Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 121?128, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Using Names and Topics for New Event Detection
Giridhar Kumaran and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{giridhar,allan}@cs.umass.edu
Abstract
New Event Detection (NED) involves
monitoring chronologically-ordered news
streams to automatically detect the stories
that report on new events. We compare
two stories by finding three cosine simi-
larities based on names, topics and the full
text. These additional comparisons sug-
gest treating the NED problem as a bi-
nary classification problem with the com-
parison scores serving as features. The
classifier models we learned show statis-
tically significant improvement over the
baseline vector space model system on all
the collections we tested, including the lat-
est TDT5 collection.
The presence of automatic speech recog-
nizer (ASR) output of broadcast news in
news streams can reduce performance and
render our named entity recognition based
approaches ineffective. We provide a so-
lution to this problem achieving statisti-
cally significant improvements.
1 Introduction
The instant and automatic detection of new events
is very useful in situations where novel informa-
tion needs to be detected from a real-time stream
of rapidly growing data. These real-life situations
occur in scenarios like financial markets, news anal-
yses, and intelligence gathering. In this paper we
focus on creating a system to immediately identify
stories reporting new events in a stream of news
- a daunting task for a human analyst given the
enormous volume of data coming in from various
sources.
The Topic Detection and Tracking (TDT) pro-
gram, a DARPA funded initiative, seeks to develop
technologies that search, organize and structure mul-
tilingual news-oriented textual materials from a va-
riety of broadcast news media. One of the tasks in
this program, New Event Detection (NED), involves
constant monitoring of streams of news stories to
identify the first story reporting topics of interest.
A topic is defined as ?a seminal event or activity,
along with directly related events and activities? (Al-
lan, 2002). An earthquake at a particular place is
an example of a topic. The first story on this topic
is the story that first carries the report on the earth-
quake?s occurrence. The other stories that make up
the topic are those discussing the death toll, the res-
cue efforts, the reactions from different parts of the
world, scientific discussions, the commercial impact
and so on. A good NED system would be one that
correctly identifies the article that reports the earth-
quake?s occurrence as the first story.
NED is a hard problem. For example, to dis-
tinguish stories about earthquakes in two different
places, a vector space model system would rely on a
tf-idf weighting scheme that will bring out the dif-
ference by weighting the locations higher. More
often then not, this doesn?t happen as the differ-
ences are buried in the mass of terms in common
between stories describing earthquakes and their af-
termath. In this paper we reduce the dependence on
tf-idf weighting by showing the utility of creating
121
three distinct representations of each story based on
named entities. This allows us to view NED as a bi-
nary classification problem - i.e., each story has to
be classified into one of two categories - old or new,
based on features extracted using the three different
representations.
The paper starts by summarizing the previous
work on NED in Section 2. In Section 3, we explain
the rationale behind our intuition. Section 4 de-
scribes the experimental setup, data pre-processing,
and our baseline NED system. We then briefly de-
scribe the evaluation methodology for NED in Sec-
tion 5. Model creation and the results of applying
these models to test data are detailed in Section 6.
In the same section, we describe the effect on perfor-
mance if the manually transcribed version of broad-
cast news is replaced with ASR output. Since its
hard to recognize named entities from ASR data,
performance expectedly deteriorates. We follow a
novel approach to work around the problem result-
ing in statistically significant improvement in per-
formance. The results are analyzed in Section 7. We
wrap up with conclusions and future work in Sec-
tion 8.
2 Previous Research
Previous approaches to NED have concentrated on
developing similarity metrics or better document
representations or both. A summer workshop on
topic-based novelty detection held at Johns Hop-
kins University extensively studied the NED prob-
lem. Similarity metrics, effect of named entities,
pre-processing of data, and language and Hidden
Markov Models were explored (Allan et al, 1999).
Combinations of NED systems were also discussed.
In the context of this paper, selective re-weighting of
named entities didn?t bring about expected improve-
ment.
Improving NED by better comparison of stories
was the focus of following papers. In an approach
to solve on-line NED, when a new document was
encountered it was processed immediately to ex-
tract features and build up a query representation
of the document?s content (Papka and Allan, 1998).
The document?s initial threshold was determined by
evaluating it with the query. If the document did not
trigger any previous query by exceeding this partic-
ular threshold, it was marked as a new event. Un-
like the previous paper, good improvements on TDT
benchmarks were shown by extending a basic in-
cremental TF-IDF model to include source-specific
models, similarity score normalization techniques,
and segmentation of documents (Brants et al, 2003).
Other researchers have attempted to build better
document models. A combination of evidence de-
rived from two distinct representations of a docu-
ment?s content was used to create a new representa-
tion for each story (Stokes and Carthy, 2001). While
one of the representations was the usual free text
vector, the other made use of lexical chains (created
using WordNet) to obtain the most prevalent topics
discussed in the document. The two vectors were
combined in a linear fashion and a marginal increase
in effectiveness was observed.
NED approaches that rely on exploiting existing
news tracking technology were proved to inevitably
exhibit poor performance (Allan et al, 2000). Given
tracking error rates, the lower and upper bounds
on NED error rates were derived mathematically.
These values were found to be good approximations
of the true NED system error rates. Since track-
ing and filtering using full-text similarity compar-
ison approaches were not likely to make the sort
of improvements that are necessary for high-quality
NED results, the paper concluded that an alternate
approach to NED was required. This led to a se-
ries of research efforts that concentrated on building
multi-stage NED algorithms and new ways to com-
bine evidence from different sources.
In the topic-conditioned novelty detection ap-
proach, documents were classified into broad top-
ics and NED was performed within these categories
(Yang et al, 2002). Additionally, named entities
were re-weighted relative to the normal words for
each topic, and a stop list was created for each topic.
The experiments were done on a corpus different
from the TDT corpus and, apparently didn?t scale
well to the TDT setting.
The DOREMI research group treated named enti-
ties like people and locations preferentially and de-
veloped a new similarity measure that utilized the
semantics classes they came up with (Makkonen et
al., 2002). They explored various definitions of the
NED task and tested their system accordingly. More
recently, they utilized a perceptron to learn a weight
122
function on the similarities between different seman-
tic classes to obtain a final confidence score for each
story (Makkonen et al, 2004).
The TDT group at UMass introduced multiple
document models for each news story and modified
similarity metrics by splitting up stories into only
named entities and only terms other than named en-
tities (Kumaran and Allan, 2004). They observed
that certain categories of news were better tackled
using only named entities, while using only topic
terms for the others helped.
In approaches similar to named entity tagging,
part-of-speech tagging (Farahat et al, 2003) has also
been successfully used to improve NED.
Papers in the TDT2003 and TDT2004 work-
shops validated the hypothesis that ensemble single-
feature classifiers based on majority voting exhibited
better performance than single classifiers working
with a number of features on the NED task (Braun
and Kaneshiro, 2003; Braun and Kaneshiro, 2004).
Examples of features they used are cosine similarity,
text tiling output and temporally-weighted tf-idf.
Probabilistic models for online clustering of doc-
uments, with a mechanism for handling creation of
new clusters have been developed. Each cluster was
assumed to correspond to a topic. Experimental re-
sults did not show any improvement over baseline
systems (Zhang et al, 2005).
3 Features for NED
Pinning down the character of new stories is a tough
process. New events don?t follow any periodic cy-
cle, can occur at any instant, can involve only one
particular type of named entity (people, places, or-
ganizations etc.) or a combination, can be reported
in any language, and can be reported as a story of
any length by any source1 . Apart from the source,
date, and time of publication or broadcast of each
news story, the TDT corpora do not contain any
other clues like placement in the webpage, the num-
ber of sources reporting the same news and so on.
Given all these factors, we decided that the best fea-
1It could be argued that articles from a source, say NYTimes,
are much longer than news stories from CNN, and hence the
length of stories is a good candidate for use as a feature. How-
ever, when there is no pattern that indicates that either of the
two sources reports new stories preferentially, the use of length
as a feature is moot.
tures to use would be those that were not particular
to the story in question only, but those that measure
differences between the story and those it is com-
pared with.
Category-specific rules that modified the baseline
confidence score assigned to each story have been
developed (Kumaran and Allan, 2004). The mod-
ification was based on additional evidence in the
form of overlap of named entities and topic terms
(terms in the document not identified as named en-
tities) with the closest story reported by a base-
line system. We decided to use these three scores:
namely the baseline confidence score, named en-
tity overlap, and topic-term overlap as features. The
named entities considered were Event, Geopolitical
Entity, Language, Location, Nationality, Organiza-
tion, Person, Cardinal, Ordinal, Date, and Time.
These named entities were detected in stories using
BBN IdentiFinderTM(Bikel et al, 1999). Irrespec-
tive of their type, all named entities were pooled to-
gether to form a single named entity vector.
The intuition behind using these features is that
we believe every event is characterized by a set of
people, places, organizations, etc. (named entities),
and a set of terms that describe the event. While
the former can be described as the who, where, and
when aspects of an event, the latter relates to the
what aspect. If two stories were on the same topic,
they would share both named entities as well as topic
terms. If they were on different, but similar, topics,
then either named entities or topic terms will match
but not both.
We illustrate the above intuition with examples.
Terms in bold face are named entities common to
both stories, while those in italics are topic terms
in common. We start with an example showing
that for old stories both the named entities as well
as topic terms overlap with a story on the same topic.
Story 1. : Story on a topic already reported
While in Croatia today, Pope John Paul II called
on the international community to help end the
fighting in the Yugoslavia?s Kosovo province.
Story 2. : Story on the same topic
Pope John Paul II is urging the international
community to quickly help the ethnic Albanians in
Kosovo. He spoke in the coastal city of Split, where
he ended a three-day visit to Croatia.
123
Story 1 is an old story about Pope John Paul II?s
visit to Yugoslavia. Story 2 was the first story on the
topic and it shares both named entities likes Pope
John Paul II and Croatia and also topic terms like
international community and help.
Our next example shows that for new stories,
either the named entities or topic terms match with
an earlier story.
Story 3. : Topic not seen before
Turkey has sent 10,000 troops to its southern border
with Syria amid growing tensions between the two
neighbors, newspapers reported Thursday. Defense
Minister Ismet Sezgin denied any troop movement
along the border, but said Turkey?s patience was
running out. Turkey accuses Syria of harboring
Turkish Kurdish rebels fighting for autonomy in
Turkey?s southeast; it says rebel leader Abdullah
Ocalan lives in Damascus.
Story 4. : Closest Story due to Named Entities
A senior Turkish government official called Mon-
day for closer military cooperation with neighboring
Bulgaria. After talks with President Petar Stoyanov
at the end of his four-day visit, Turkish Deputy Pre-
mier and National Defense Minister Ismet Sezgin
expressed satisfaction with the progress of bilateral
relations and the hope that Bulgarian-Turkish
military cooperation will be promoted.
Story 3 is a new story about the rising ten-
sions between Turkey and Syria. The closest story
as reported by a (baseline) basic vector space model
NED system using cosine similarity is Story 4,
a story about Turkish-Bulgarian relations. The
named entities Turkey and Ismet Sezgin caused
this match. We see that none of the topic terms
match. However, the system reported with a high
confidence score that Story 3 is old. This is because
of the matching of high IDF-valued named entities.
Determining that the topic terms didn?t match would
have helped the system avoid this mistake.
4 Experimental Setup and Baseline
We used the TDT2, TDT3, TDT4, and TDT5 cor-
pora for our experiments. They contain a mix of
broadcast news (bn) and newswire (nwt) stories.
Only the English stories in the multi-lingual collec-
tions were considered for the NED task. The broad-
cast news material is provided in the form of an au-
dio sampled data signal, a manual transcription of
the audio signal (bn-man), and a transcription cre-
ated using an automatic speech recognizer (bn-asr).
We used version 3.0 of the open source Lemur
system2 to tokenize the data, remove stop words,
stem and create document vectors. We used the 418
stopwords included in the stop list used by InQuery
(Callan et al, 1992), and the Krovetz-stemmer algo-
rithm implementation provided as part of Lemur.
Documents were represented as term vectors with
incremental TF-IDF weighting (Brants et al, 2003;
Yang et al, 1998). We used the cosine similarity
metric to judge the similarity of a story S with those
seen in the past.
Sim(S, X) =
?
w weight(w, S) ? weight(w, X)
?
?
w weight(w,S)
2
?
?
w weight(w, X)
2
(1)
where
weight(w, d) =tf ? idf
tf =log(termfrequency + 1.0)
idf = log((docCount+1)(documentfreq+0.5)
The maximum similarity of the story S with stories
seen in the past was taken as the confidence score
that S was old. This constituted our baseline system.
We extracted three features for each incoming
story S. The first was the confidence score reported
by the baseline system. The second and third fea-
tures were the cosine similarity between only the
named entities in S and X and the cosine similarity
between only the topic terms in S and X. We trained
a Support Vector Machine (SVM) (Burges, 1998)
classifier on these features. We chose to use SVMs
as they are considered state-of-the-art for text clas-
sification purposes (Mladeni et al, 2004), and pro-
vide us with options to consider both linear and non-
linear decision boundaries. To develop SVM models
we used SV MLight(Joachims, 1999), which is an
implementation of SVMs in C. SV MLight is an im-
plementation of Vapnik?s Support Vector Machine
(Vapnik, 1995).
For training, we used the TDT3 and TDT4 cor-
pora. There were 115 and 70 topics respectively giv-
ing us a total of 185 positive examples (new stories)
2http://www.lemurproject.org
124
and 7800 negative examples (old stories). We bal-
anced the number of positive and negative examples
by oversampling the minority class until there were
equal number of positive and negative training in-
stances. Testing was done on the TDT2 and TDT5
corpora (96 and 126 topics resp.).
5 NED Evaluation
The official TDT evaluation requires a NED system
to assign a confidence score between 0 (new) and
1 (old) to every story upon its arrival in the time-
ordered news stream. This assignment of scores
is done either immediately upon arrival or after a
fixed look-ahead window of stories. To evaluate per-
formance, the stories are sorted according to their
scores, and a threshold sweep is performed. All
stories with scores above the threshold are declared
old, while those below it are considered new. At
each threshold value, the misses and false alarms are
identified, and a cost Cdet is calculated as follows.
Cdet = Cmiss ?Pmiss ?Ptarget + CFA ? PFA ? Pnon?target
where CMiss and CFA are the costs of a Miss
and a False Alarm, respectively, PMiss and PFA
are the conditional probabilities of a Miss and a
False Alarm, respectively, and Ptarget and Pnon?target
are the a priori target probabilities (Pnon?target = 1 -
Ptarget).
The threshold that results in the least cost is se-
lected as the optimum one. Different NED systems
are compared based on their minimum cost. In other
words, the lower the Cdet score reported by a system
on test data, the better the system.
6 Results
Our first set of experiments were performed on data
consisting of newswire text and manual transcrip-
tion of broadcast news (nwt+bn-man). We used
the features mentioned in Section 3 to build SVM
models in the classification mode. We experimented
with linear, polynomial, and RBF kernels. The out-
put from the SVM classifiers was normalized to fall
within the range zero and one.
We found that using certain kernels improved per-
formance over the baseline system significantly. The
results for both corpora, TDT2 and TDT5, were
consistently and significantly improved by using the
TDT2 TDT5
Kernel Type (nwt+bn-man) (nwt)
Baseline System 0.585 0.701
Linear Kernel 0.548 0.696
Poly. of deg. 1 0.548 0.696
Poly. of deg. 2 0.543 0.688
Poly. of deg. 3 0.545 0.684
Poly. of deg. 4 0.535 0.694
Poly. of deg. 5 0.533 0.688
Poly. of deg. 6 0.534 0.693
RBF with ? = 1 0.540 0.661
RBF with ? = 5 0.530 0.699
Table 1: Summary of the results of using SVM classifier mod-
els for NED on the TDT2 and TDT5 collections. The numbers
are the minimum cost (Cdet) values (lower is better). The sign
test, with ? = 0.05, was performed to compare the baseline sys-
tem with only a classifier using RBF kernels with ? = 1. For
both collections, the improvements were found to be statisti-
cally significant (shown in bold). While there are better per-
forming kernels for TDT2, we chose to perform significance
tests for only one kernel to show that significant improvement
over the baseline can be obtained using a single kernel across
different test collections.
classification models. The 2004 NED evaluations
conducted by the National Institute of Standards and
Technology was on the TDT5 collection. The large
size of the collection and existence of a large num-
ber of topics with a single story made the task very
challenging. The best system fielded by the partici-
pating sites was the baseline system used here. Ta-
ble 1 summarizes the results we obtained.
All statistical significance testing was done using
the sign test. We counted the number of topics for
which using the SVM classifier improved over the
baseline (in terms of detecting more previously un-
detected new and old stories), and also the num-
ber of topics for which using the SVM classifier
actually converted originally correct decisions into
wrong ones. These were used as input for the sign
test. The test were used to check whether improve-
ment in performance using the classifier-based sys-
tem was spread across a significant number of top-
ics, and not confined to a few. Table 2 gives some
examples of topics and the associated improvements
in detecting them.
125
Topic ID Number of Num. detected Num. detected Improvement
old stories by baseline system by SVM classifier (Higher the better)
55105 420 407 403 -4
55010 21 21 20 -1
55023 5 5 4 -1
55089 226 226 225 -1
55125 120 114 120 6
55107 331 327 331 7
55106 808 787 795 8
55200 196 185 193 8
Table 2: Examples of improvements due to using the SVM classifier on a per-topic basis. Shown here are the
four topics each in which the greatest degradation and improvements in performance were seen. The topics
vary in size. The SVM classifier resulted in overall (statistically significant, refer Table 1) improvement as
it corrected more errors than introduced them.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
5
10
15
20
25
30
35
Confidence Score
N
um
be
r o
f S
to
rie
s
TDT5 ? New Scores
Baseline New Story Scores
Classifier New Story Scores
Figure 1: Distribution of new story scores for the
baseline and SVM model systems.
7 Analysis
The main goal of our effort was to come up with a
way to correctly identify new stories based on fea-
tures we thought characterized them. To understand
what we had actually achieved by using these mod-
els, we studied the distribution of the confidence
scores assigned to new and old stories for the base-
line and a classifier-based NED system for the TDT5
collection (Figures 1 and 2 respectively).
We observe that the scores for a small fraction
of new stories that were initially missed (between
scores 0.8 and 1) are decreased by the model-based
NED system while a larger fraction (between scores
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
500
1000
1500
2000
2500
3000
Confidence Score
N
um
be
r o
f S
to
rie
s
TDT5 ? Old Stories
Baseline Old Story Scores
Classifier Old Story Scores
Figure 2: Distribution of old story scores for the
baseline and SVM model systems.
0.1 and 0.4) is also decreased by a small amount.
However, the major impact of using SVM model-
based NED systems appears to be in detecting old
stories. We observe that the scores of a significant
number of old stories (between scores 0.2 and 0.55)
have been increased to be closer to one. This had the
effect of increasing the score difference between old
and new stories, and hence improved classification
accuracy as measured by the minimum cost.
We investigated the relative importance of the
three features by looking that the linear kernel SVM
model. While the original cosine similarity metric
CS remained the prominent feature, the contribution
126
of the third feature non-NE-CS was slightly more
than if not equal to the contribution of named en-
tities NE-CS (Table 3). This explains why simple
re-weighting of named entities alone (Allan et al,
1999) doesn?t suffice to improved performance.
Feature CS NE-CS non-NE-CS
Weight 5.4 1.58 1.83
Table 3: Weights assigned to features by the linear
kernel SVM.
If this method of harnessing named entities and
topic terms were indeed so effective, then we should
have been able to detect every old story in every
topic. However, analysis reveals that this approach
makes an assumption about the way stories in a
topic are related. Not all topics are dense, with both
named entities and topic terms threading the stories
together. Examples of such topics are natural dis-
aster topics. While the first story might report on
the actual calamity and the region it affected, suc-
cessive stories might report on individual survivor
tales. These stories might be connected to the orig-
inal story of the topic by as tenuous a link as only
the name of the calamity, or the place. Such topic
structures are very common in newswire. Hence our
approach will fail in such topics with loosely con-
nected stories. Much more advanced processing of
story content is required in such cases. Mistakes
made by the named entity recognizer also impede
performance.
Given that its impractical to expect manual tran-
scriptions of all broadcast news, we tested our base-
line and classifier systems on a version of TDT2
with newswire stories and ASR output of the broad-
cast news (nwt+bn-asr). TDT5 was left out as it
doesn?t have any broadcast news. As shown in Ta-
ble 4, the baseline system performed significantly
worse when manual transcription was replaced with
ASR output. The classifier systems did even worse
than the nwt +bn-asr baseline result. An analysis
of the named entities extracted revealed that the ac-
curacy was very poor - worse than extraction from
bn-man documents. This was primarily because the
version of IdentiFinder (IdentiFinder-man) we used
was by default trained on nwt.
To alleviate this problem we re-trained Identi-
Kernel Type TDT2 (nwt+bn-asr)
Baseline System 0.640
IdentiFinder-man IdentiFinder-asr
Linear Kernel 0.653 0.608
Poly. of deg. 1 0.654 0.608
Poly. of deg. 2 0.658 0.619
Poly. of deg. 3 0.659 0.616
Poly. of deg. 4 0.671 0.632
Poly. of deg. 5 0.676 0.640
Poly. of deg. 6 0.682 0.652
RBF with ? = 1 0.649 0.636
RBF with ? = 5 0.668 0.679
Table 4: The baseline system was the same used for
the nwt+bn-man collection. We find that using a lin-
ear kernel for the procedure using IdentiFinder-asr
to tag named entities results in statistically signifi-
cant improvement.
Finder using a simulated ASR corpus with named
entities identified correctly. Since the amount of
training data required was huge, we obtained the
training data from the bn-man version of TDT3.
We ran IdentiFinder-man on the bn-man version of
TDT3 and tagged the named entities. We then re-
moved punctuation and converted all the text to up-
percase to simulate ASR to a limited degree. We
re-trained IdentiFinder on this simulated ASR cor-
pus and used it to tag named entities in only the
bn-asr stories in TDT2. We retained the use of
IdentiFinder-man for the nwt stories. The same three
features were then extracted and we re-ran the classi-
fiers. The results are shown in Table 4 in the column
titled IdentiFinder-asr.
8 Conclusions and Future Work
We have shown the applicability of machine learn-
ing classification techniques to solve the NED prob-
lem. Significant improvements were made over the
baseline systems on all the corpora tested on. The
features we engineered made extensive use of named
entities, and reinforced the importance and need to
effectively harness their utility to solve problems in
TDT. NED requires not only detection and report-
ing of new events, but also suppression of stories
that report old events. From the study of the distri-
butions of scores assigned to stories by the baseline
127
and SVM model systems, we can see that we now do
a better job of detecting old stories (reducing false
alarms). Thus we believe that attacking the prob-
lem as ?old story detection? might be a better and
more fruitful approach. We have shown the effects
of ASR output in the news stream, and demonstrated
a procedure to alleviate the problem.
A classifier with RBF kernel with ? set to one ex-
hibited the best performance. The reason for this
superior performance over other kernels needs to be
investigated. Engineering of better features is also
a definite priority. In the future NED can also be
extended to other interesting domains like scientific
literature to detect the emerge of new topics and in-
terests.
Acknowledgements
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by NSF grant
#IIS-9907018, and in part by SPAWARSYSCEN-
SD grant number N66001-02-1-8903. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are the author(s) and do
not necessarily reflect those of the sponsor.
References
J. Allan, Hubert Jin, Martin Rajman, Charles Wayne, Daniel
Gildea, Victor Lavrenko, Rose Hoberman, and David Ca-
puto. 1999. Topic-based novelty detection. Technical re-
port, Center for Language and Speech Processing, Johns
Hopkins University. Summer Workshop Final Report.
J. Allan, Victor Lavrenko, and Hubert Jin. 2000. First story
detection in tdt is hard. In Proceedings of the Ninth Interna-
tional Conference on Information and Knowledge Manage-
ment, pages 374?381. ACM Press.
J. Allan. 2002. Topic Detection and Tracking: Event-Based
Information Organization. Kluwer Academic Publishers.
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s in a
name. Machine Learning, 34(1-3):211?231.
Thorsten Brants, Francine Chen, and Ayman Farahat. 2003. A
system for new event detection. In Proceedings of the 26th
Annual International ACM SIGIR Conference, pages 330?
337, New York, NY, USA. ACM Press.
Ronald K. Braun and Ryan Kaneshiro. 2003. Exploiting topic
pragmatics for new event detection in tdt-2004. Techni-
cal report, National Institute of Standards and Technology.
Topic Detection and Tracking Workshop.
Ronald K. Braun and Ryan Kaneshiro. 2004. Exploiting topic
pragmatics for new event detection in tdt-2004. Techni-
cal report, National Institute of Standards and Technology.
Topic Detection and Tracking Workshop.
Christopher J. C. Burges. 1998. A tutorial on support vector
machines for pattern recognition. Data Mining and Knowl-
edge Discovery, 2(2):121?167.
James P. Callan, W. Bruce Croft, and Stephen M. Harding.
1992. The INQUERY retrieval system. In Proceedings of
DEXA-92, 3rd International Conference on Database and
Expert Systems Applications, pages 78?83.
Ayman Farahat, Francine Chen, and Thorsten Brants. 2003.
Optimizing story link detection is not equivalent to optimiz-
ing new event detection. In ACL, pages 232?239.
Thorsten Joachims. 1999. Making large-scale support vector
machine learning practical. MIT Press, Cambridge, MA,
USA.
Giridhar Kumaran and J. Allan. 2004. Text classification
and named entities for new event detection. In Proceedings
of the 27th Annual International ACM SIGIR Conference,
pages 297?304, New York, NY, USA. ACM Press.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2002. Applying semantic classes in event
detection and tracking. In Proceedings of International
Conference on Natural Language Processing (ICON 2002),
pages 175?183.
Juha Makkonen, Helena Ahonen-Myka, and Marko
Salmenkivi. 2004. Simple semantics in topic detec-
tion and tracking. Information Retrieval, 7(3?4):347?368.
Dunja Mladeni, Janez Brank, Marko Grobelnik, and Natasa
Milic-Frayling. 2004. Feature selection using linear classi-
fier weights: interaction with classification models. In Pro-
ceedings of the 27th Annual International ACM SIGIR Con-
ference, pages 234?241, New York, NY, USA. ACM Press.
R. Papka and J. Allan. 1998. On-line new event detection using
single pass clustering. Technical Report UM-CS-1998-021.
Nicola Stokes and Joe Carthy. 2001. Combining semantic and
syntactic document classifiers to improve first story detec-
tion. In Proceedings of the 24th Annual International ACM
SIGIR Conference, pages 424?425, New York, NY, USA.
ACM Press.
Vladimir N. Vapnik. 1995. The nature of statistical learning
theory. Springer-Verlag New York, Inc.
Yiming Yang, Tom Pierce, and Jaime Carbonell. 1998. A study
of retrospective and on-line event detection. In Proceedings
of the 21st Annual International ACM SIGIR Conference,
pages 28?36, New York, NY, USA. ACM Press.
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun Jin.
2002. Topic-conditioned novelty detection. In Proceedings
of the 8th ACM SIGKDD International Conference, pages
688?693. ACM Press.
Jian Zhang, Zoubin Ghahramani, and Yiming Yang. 2005. A
probabilistic model for online document clustering with ap-
plication to novelty detection. In Lawrence K. Saul, Yair
Weiss, and Le?on Bottou, editors, Advances in Neural In-
formation Processing Systems 17, pages 1617?1624. MIT
Press, Cambridge, MA.
128
Proceedings of NAACL HLT 2007, pages 220?227,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Case for Shorter Queries, and Helping Users Create Them
Giridhar Kumaran and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts Amherst
Amherst, MA 01003, USA
{giridhar,allan}@cs.umass.edu
Abstract
Information retrieval systems are fre-
quently required to handle long queries.
Simply using all terms in the query or re-
lying on the underlying retrieval model
to appropriately weight terms often leads
to ineffective retrieval. We show that re-
writing the query to a version that com-
prises a small subset of appropriate terms
from the original query greatly improves
effectiveness. Targeting a demonstrated
potential improvement of almost 50% on
some difficult TREC queries and their as-
sociated collections, we develop a suite of
automatic techniques to re-write queries
and study their characteristics. We show
that the shortcomings of automatic meth-
ods can be ameliorated by some simple
user interaction, and report results that are
on average 25% better than the baseline.
1 Introduction
Query expansion has long been a focus of infor-
mation retrieval research. Given an arbitrary short
query, the goal was to find and include additional
related and suitably-weighted terms to the original
query to produce a more effective version. In this pa-
per we focus on a complementary problem ? query
re-writing. Given a long query we explore whether
there is utility in modifying it to a more concise ver-
sion such that the original information need is still
expressed.
The Y!Q beta1 search engine allows users to se-
lect large portions of text from documents and issue
them as queries. The search engine is designed to
encourage users to submit long queries such as this
example from the web site ?I need to know the gas
mileage for my Audi A8 2004 model?. The moti-
vation for encouraging this type of querying is that
longer queries would provide more information in
the form of context (Kraft et al, 2006), and this ad-
ditional information could be leveraged to provide
a better search experience. However, handling such
long queries is a challenge. The use of all the terms
from the user?s input can rapidly narrow down the
set of matching documents, especially if a boolean
retrieval model is adopted. While one would ex-
pect the underlying retrieval model to appropriately
assign weights to different terms in the query and
return only relevant content, it is widely acknowl-
edged that models fail due to a variety of reasons
(Harman and Buckley, 2004), and are not suited to
tackle every possible query.
Recently, there has been great interest in personal-
ized search (Teevan et al, 2005), where the query is
modified based on a user?s profile. The profile usu-
ally consists of documents previously viewed, web
sites recently visited, e-mail correspondence and so
on. Common procedures for using this large amount
of information usually involve creating huge query
vectors with some sort of term-weighting mecha-
nism to favor different portions of the profile.
The queries used in the TREC ad-hoc tracks con-
sist of title, description and narrative sections, of
progressively increasing length. The title, of length
1http://yq.search.yahoo.com/
220
ranging from a single term to four terms is consid-
ered a concise query, while the description is consid-
ered a longer version of the title expressing the same
information need. Almost all research on the TREC
ad-hoc retrieval track reports results using only the
title portion as the query, and a combination of the
title and description as a separate query. Most re-
ported results show that the latter is more effective
than the former, though in the case of some hard col-
lections the opposite is true. However, as we shall
show later, there is tremendous scope for improve-
ment. Formulating a shorter query from the descrip-
tion can lead to significant improvements in perfor-
mance.
In the light of the above, we believe there is great
utility in creating query-rewriting mechanisms for
handling long queries. This paper is organized in
the following way. We start with some examples
and explore ways by which we can create concise
high-quality reformulations of long queries in Sec-
tion 2. We describe our baseline system in Section 3
and motivate our investigations with experiments in
Section 4. Since automatic methods have shortfalls,
we present a procedure in Section 5 to involve users
in selecting a good shorter query from a small selec-
tion of alternatives. We report and discuss the results
of this approach in Section 6. Related work is pre-
sented in Section 7. We wrap up with conclusions
and future directions in Section 8.
2 Selecting sub-queries
Consider the following query:
Define Argentine and British international rela-
tions.
When this query was issued to a search engine,
the average precision (AP, Section 3) of the results
was 0.424. When we selected subsets of terms (sub-
queries) from the query, and ran them as distinct
queries, the performance was as shown in Table 1. It
can be observed that there are seven different ways
of re-writing the original query to attain better per-
formance. The best query, also among the shortest,
did not have a natural-language flavor to it. It how-
ever had an effectiveness almost 50% more than the
original query. This immense potential for improve-
ment by query re-writing is the motivation for this
paper.
Query AP
.... ....
international relate 0.000
define international relate 0.000
.... ....
define argentina 0.123
international relate argentina 0.130
define relate argentina 0.141
relate argentina 0.173
define britain international relate argentina 0.424
define britain international argentina 0.469
britain international relate argentina 0.490
define britain relate argentina 0.494
britain international argentina 0.528
define britain argentina 0.546
britain relate argentina 0.563
britain argentina 0.626
Table 1: The results of using all possible subsets (ex-
cluding singletons) of the original query as queries.
The query terms were stemmed and stopped.
Analysis of the terms in the sub-queries and the
relationship of the sub-queries with the original
query revealed a few interesting insights that had po-
tential to be leveraged to aid sub-query selection.
1. Terms in the original query that a human would
consider vital in conveying the type of infor-
mation desired were missing from the best sub-
queries. For example, the best sub-query for
the example was britain argentina, omitting
any reference to international relations. This
also reveals a mismatch between the user?s
query and the way terms occurred in the corpus,
and suggests that an approximate query could
at times be a better starting point for search.
2. The sub-query would often contain only terms
that a human would consider vital to the query
while the original query would also (naturally)
contain them, albeit weighted lower with re-
spect to other terms. This is a common prob-
lem (Harman and Buckley, 2004), and the fo-
cus of efforts to isolate the key concept terms
in queries (Buckley et al, 2000; Allan et al,
1996).
3. Good sub-queries were missing many of the
noise terms found in the original query. Ideally
the retrieval model would weight them lower,
but dropping them completely from the query
appeared to be more effective.
221
4. Sub-queries a human would consider as an in-
complete expression of information need some-
times performed better than the original query.
Our example illustrates this point.
Given the above empirical observations, we ex-
plored a variety of procedures to refine a long query
into a shorter one that retained the key terms. We ex-
pected the set of terms of a good sub-query to have
the following properties.
A. Minimal Cardinality: Any set that contains
more than the minimum number of terms to retrieve
relevant documents could suffer from concept drift.
B. Coherency: The terms that constitute the sub-
query should be coherent, i.e. they should buttress
each other in representing the information need. If
need be, terms that the user considered important but
led to retrieval of non-relevant documents should be
dropped.
Some of the sub-query selection methods we ex-
plored with these properties in mind are reported be-
low.
2.1 Mutual Information
Let X and Y be two random variables, with joint
distribution P (x, y) and marginal distributions P (x)
and P (y) respectively. The mutual information is
then defined as:
I(X;Y ) =
?
x
?
y
p(x, y)log p(x, y)p(x)p(y)
(1)
Intuitively, mutual information measures the infor-
mation about X that is shared by Y . If X and Y are
independent, then X contains no information about
Y and vice versa and hence their mutual information
is zero. Mutual Information is attractive because it is
not only easy to compute, but also takes into consid-
eration corpus statistics and semantics. The mutual
information between two terms (Church and Hanks,
1989) can be calculated using Equation 2.
I(x, y) = log
n(x,y)
N
n(x)
N
n(y)
N
(2)
n(x, y) is the number of times terms x and y oc-
curred within a term window of 100 terms across the
corpus, while n(x) and n(y) are the frequencies of
x and y in the collection of size N terms.
To tackle the situation where we have an arbi-
trary number of variables (terms) we extend the two-
variable case to the multivariate case. The extension,
called multivariate mutual information (MVMI) can
be generalized from Equation 1 to:
I(X1;X2;X3; ...;XN ) =
N
?
i=1
(?1)i?1
?
X?(X1,X2,X3,...,XN),|X|=k
H(X) (3)
The calculation of multivariate information using
Equation 3 was very cumbersome, and we instead
worked with the approximation (Kern et al, 2003)
given below.
I(X1;X2;X3; ...;XN ) = (4)
?
i,j={1,2,3,...,N ;i6=j}
I(Xi;Xj) (5)
For the case involving multiple terms, we calcu-
lated MVMI as the sum of the pair-wise mutual in-
formation for all terms in the candidate sub-query.
This can be also viewed as the creation of a com-
pletely connected graph G = (V,E), where the ver-
tices V are the terms and the edges E are weighted
using the mutual information between the vertices
they connect.
To select a score representative of the quality of
a sub-query we considered several options includ-
ing the sum, average, median and minimum of the
edge weights. We performed experiments on a set
of candidate queries to determine how well each of
these measures tracked AP, and found that the aver-
age worked best. We refer to the sub-query selection
procedure using the average score as Average.
2.2 Maximum Spanning Tree
It is well-known that an average is easily skewed
by outliers. In other words, the existence of one or
more terms that have low mutual information with
every other term could potentially distort results.
This problem could be further compounded by the
fact that mutual information measured using Equa-
tion 2 could have a negative value. We attempted
222
to tackle this problem by considering another mea-
sure that involved creating a maximum spanning tree
(MaxST) over the fully connected graph G, and us-
ing the weight of the identified tree as a measure rep-
resentative of the candidate query?s quality (Rijsber-
gen, 1979). We used Kruskal?s minimum spanning
tree (Cormen et al, 2001) algorithm after negating
the edge weights to obtain a MaxST. We refer to the
sub-query selection procedure using the weight of
the maximum spanning tree as MaxST.
2.3 Named Entities
Named entities (names of persons, places, organiza-
tions, dates, etc.) are known to play an important
anchor role in many information retrieval applica-
tions. In our example from Section 2, sub-queries
without Britain or Argentina will not be effective
even though the mutual information score of the
other two terms international and relations might
indicate otherwise. We experimented with another
version of sub-query selection that considered only
sub-queries that retained at least one of the named
entities from the original query. We refer to the vari-
ants that retained named entities as NE Average and
NE MasT.
3 Experimental Setup
We used version 2.3.2 of the Indri search engine, de-
veloped as part of the Lemur2 project. While the
inference network-based retrieval framework of In-
dri permits the use of structured queries, the use
of language modeling techniques provides better es-
timates of probabilities for query evaluation. The
pseudo-relevance feedback mechanism we used is
based on relevance models (Lavrenko and Croft,
2001).
To extract named entities from the queries, we
used BBN Identifinder (Bikel et al, 1999). The
named entities identified were of type Person, Lo-
cation, Organization, Date, and Time.
We used the TREC Robust 2004 and Robust 2005
(Voorhees, 2006) document collections for our ex-
periments. The 2004 Robust collection contains
around half a million documents from the Finan-
cial Times, the Federal Register, the LA Times, and
FBIS. The Robust 2005 collection is the one-million
2http://www.lemurproject.org
document AQUAINT collection. All the documents
were from English newswire. We chose these col-
lections because they and their associated queries
are known to be hard, and hence present a chal-
lenging environment. We stemmed the collections
using the Krovetz stemmer provided as part of In-
dri, and used a manually-created stoplist of twenty
terms (a, an, and, are, at, as, be, for, in, is, it, of, on,
or, that, the, to, was, with and what). To determine
the best query selection procedure, we analyzed 163
queries from the Robust 2004 track, and used 30 and
50 queries from the 2004 and 2005 Robust tracks re-
spectively for evaluation and user studies.
For all systems, we report mean average preci-
sion (MAP) and geometric mean average precision
(GMAP). MAP is the most widely used measure in
Information Retrieval. While precision is the frac-
tion of the retrieved documents that are relevant, av-
erage precision (AP) is a single value obtained by
averaging the precision values at each new relevant
document observed. MAP is the arithmetic mean of
the APs of a set of queries. Similarly, GMAP is the
geometric mean of the APs of a set of queries. The
GMAP measure is more indicative of performance
across an entire set of queries. MAP can be skewed
by the presence of a few well-performing queries,
and hence is not as good a measure as GMAP from
the perspective of measure comprehensive perfor-
mance.
4 Experiments
We first ran two baseline experiments to record the
quality of the available long query and the shorter
version. As mentioned in Section 1, we used the
description and title sections of each TREC query
as surrogates for the long and short versions re-
spectively of a query. The results are presented in
the first two rows, Baseline and Pseudo-relevance
Feedback (PRF), of Table 2. Measured in terms of
MAP and GMAP (Section3), using just the title re-
sults in better performance than using the descrip-
tion. This clearly indicates the existence of terms in
the description that while elaborating an information
need hurt retrieval performance. The result of using
pseudo-relevance feedback (PRF) on both the title
and description show moderate gains - a known fact
about this particular collection and associated train-
223
MAP GMAP
Long Query Baseline 0.243 0.136
(Description) PRF 0.270 0.124
Short Query Baseline 0.249 0.154
(Title) PRF 0.269 0.148
Best sub-query Baseline 0.342 0.270
(Combination) PRF 0.343 0.241
Table 2: Results across 163 training queries on the
Robust 2004 collection. Using the best sub-query
results in almost 50% improvement over the baseline
ing queries.
To show the potential and utility of query re-
writing, we first present results that show the upper
bound on performance that can obtained by doing
so. We ran retrieval experiments with every combi-
nation of query terms. For a query of length n, there
are 2n combinations. We limited our experiments
to queries of length n ? 12. Selecting the perfor-
mance obtained by the best sub-query of each query
revealed an upper bound in performance almost 50%
better than the baseline (Table 2).
To evaluate the automatic sub-query selection
procedures developed in Section 2, we performed
retrieval experiments using the sub-queries selected
using them. The results, which are presented in Ta-
ble 3, show that the automatic sub-query selection
process was a failure. The results of automatic se-
lection were worse than even the baseline, and there
was no significant difference between using any of
the different sub-query selection procedures.
The failure of the automatic techniques could be
attributed to the fact that we were working with the
assumption that term co-occurrence could be used
to model a user?s information need. To see if there
was any general utility in using the procedures to
select sub-queries, we selected the best-performing
sub-query from the top 10 ranked by each selection
procedure (Table 4). While the effectiveness in each
case as measured by MAP is not close to the best
possible MAP, 0.342, they are all significantly better
than the baseline of 0.243.
5 Interacting with the user
The final results we presented in the last section
hinted at a potential for user interaction. We envi-
MAP GMAP
Baseline 0.243 0.136
Average 0.172 0.025
MaxST 0.172 0.025
NE Average 0.170 0.023
NE MaxST 0.182 0.029
Table 3: Score of the highest rank sub-query by var-
ious measures.
MAP GMAP
Baseline 0.243 0.136
AverageTop10 0.296 0.167
MaxSTTop10 0.293 0.150
NE AverageTop10 0.278 0.156
NE MaxSTTop10 0.286 0.159
Table 4: Score of the best sub-query in the top 10
ranked by various measures
sioned providing the user with a list of the top 10
sub-query candidates using a good ranking proce-
dure, and asking her to select the sub-query she felt
was most appropriate. This additional round of hu-
man intervention could potentially compensate for
the inability of the ranking measures to select the
best sub-query automatically.
5.1 User interface design
We displayed the description (the long query) and
narrative portion of each TREC query in the inter-
face. The narrative was provided to help the partic-
ipant understand what information the user who is-
sued the query was interested in. The title was kept
hidden to avoid influencing the participant?s choice
of the best sub-query. A list of candidate sub-queries
was displayed along with links that could be clicked
on to display a short section of text in a designated
area. The intention was to provide an example of
what would potentially be retrieved with a high rank
if the candidate sub-query were used. The partici-
pant used this information to make two decisions -
the perceived quality of each sub-query, and the best
sub-query from the list. A facility to indicate that
none of the candidates were good was also included.
224
Percentage of candidates
better than baseline
Average 28.5%
MaxST 35.5%
NE Average 31.1%
NE MaxST 36.6%
Table 5: Number of candidates from top 10 that ex-
ceeded the baseline
5.2 User interface content issues
The two key issues we faced while determining the
content of the user interface were:
A. Deciding which sub-query selection procedure
to use to get the top 10 candidate sub-queries: To
determine this in the absence of any significant dif-
ference in performance due to the top-ranked can-
didate selected by each procedure, we looked at the
number of candidates each procedure brought into
the top 10 that were better than the baseline query,
as measured by MAP. This was guided by the belief
that greater the number of better candidates in the
top 10, the higher the probability that the user would
select a better sub-query. Table 5 shows how each of
the selection procedures compared. The NE MaxST
ranking procedure had the most number of better
sub-queries in the top 10, and hence was chosen.
B. Displaying context: Simply displaying a list
of 10 candidates without any supportive information
would make the task of the user difficult. This was in
contrast to query expansion techniques (Anick and
Tipirneni, 1999) where displaying a list of terms suf-
ficed as the task of the user was to disambiguate
or expand a short query. An experiment was per-
formed in which a single user worked with a set of
30 queries from Robust 2004, and an accompanying
set of 10 candidate sub-queries each, twice - once
with passages providing context and one with snip-
pets providing context. The top-ranked passage was
generated by modifying the candidate query into
one that retrieved passages of fixed length instead
of documents. Snippets, like those seen along with
links to top-ranked documents in the results from
almost all popular search engines, were generated
after a document-level query was used to query the
collection. The order in which the two contexts were
presented to the user was randomized to prevent the
MAP GMAP
Snippet as Context 0.348 0.170
Passage as Context 0.296 0.151
Table 6: Results showing the MAP over 19 of 30
queries that the user provided selections for using
each context type.
user from assuming a quality order. We see that pre-
senting the snippet led to better MAP that presenting
the passage (Table 6). The reason for this could be
that the top-ranking passage we displayed was from
a document ranked lower by the document-focussed
version of the query. Since we finally measure MAP
only with respect to document ranking, and the snip-
pet was generated from the top-ranked document,
we hypothesize that this led to the snippet being a
better context to display.
6 User Evaluation
We conducted an exploratory study with five par-
ticipants - four of them were graduate students in
computer science while the fifth had a background
in the social sciences and was reasonably proficient
in the use of computers and internet search engines.
The participants worked with 30 queries from Ro-
bust 2004, and 50 from Robust 20053. The baseline
values reported are automatic runs with the descrip-
tion as the query.
Table 7 shows that all five participants4 were
able to choose sub-queries that led to an improve-
ment in performance over the baseline (TREC title
query only). This improvement is not only on MAP
but also on GMAP, indicating that user interaction
helped improve a wide spectrum of queries. Most
notable were the improvements in P@5 and P@10.
This attested to the fact that the interaction tech-
nique we explored was precision-enhancing. An-
other interesting result, from # sub-queries selected
was that participants were able to decide in a large
number of cases that re-writing was either not useful
for a query, or that none of the options presented to
them were better. Showing context appears to have
helped.
3Participant 4 looked that only 34 of the 50 queries presented
4The p value for testing statistical significance of MAP im-
provement for Participant 5 was 0.053 - the result very narrowly
missed being statistically significant.
225
# Queries # sub-queries % sub-queries MAP GMAP P@5 p@10
selected better
Baseline 0.203 0.159 0. 476 0.507
1 50 26 80.7% With Interaction 0.249 0.199 0.615 0.580
Upper Bound 0.336 0.282 0.784 0.719
Baseline 0.224 0.156 0.484 0.526
2 50 19 78.9% With Interaction 0.277 0.209 0.652 0.621
Upper Bound 0.359 0.293 0.810 0.742
Baseline 0.217 0.126 0.452 0.432
3 80 53 73.5% With Interaction 0.276 0.166 0.573 0.501
Upper Bound 0.354 0.263 0.762 0.654
Baseline 0.192 0.142 0.462 0.525
4 50(34) 19 68.7% With Interaction 0.255 0.175 0.612 0.600
Upper Bound 0.344 0.310 0.862 0.800
Baseline 0.206 0.111 0.433 0.410
5 80 65 61.5% With Interaction 0.231 0.115 0.486 0.429
Upper Bound 0.341 0.245 0.738 0.640
Table 7: # Queries refers to the number of queries that were presented to the participant while # sub-queries
selected refers to the number of queries for which the participant chose a sub-query. All scores including
upper bounds were calculated only considering the queries for which the participant selected a sub-query.
An entry in bold means that the improvement in MAP is statistically significant. Statistical significance was
measured using a paired t-test, with ? set to 0.05.
7 Related Work
Our interest in finding a concise sub-query that ef-
fectively captures the information need is reminis-
cent of previous work in (Buckley et al, 2000).
However, the focus was more on balancing the ef-
fect of query expansion techniques such that differ-
ent concepts in the query were equally benefited.
Mutual information has been used previously in
(Church and Hanks, 1989) to identify collocations of
terms for identifying semantic relationships in text.
Experiments were confined to bigrams. The use of
MaST over a graph of mutual information values
to incorporate the most significant dependencies be-
tween terms was first noted in (Rijsbergen, 1979).
Extensions can be found in a different field - image
processing (Kern et al, 2003) - where multivariate
mutual information is frequently used.
Work done by (White et al, 2005) provided a ba-
sis for our decision to show context for sub-query se-
lection. The useful result that top-ranked sentences
could be used to guide users towards relevant mate-
rial helped us design an user interface that the par-
ticipants found very convenient to use.
A related problem addressed by (Cronen-
Townsend et al, 2002) was determining query qual-
ity. This is known to be a very hard problem, and
various efforts (Carmel et al, 2006; Vinay et al,
2006) have been made towards formalizing and un-
derstanding it.
Previous work (Shapiro and Taksa, 2003) in the
web environment attempted to convert a user?s natu-
ral language query into one suited for use with web
search engines. However, the focus was on merg-
ing the results from using different sub-queries, and
not selection of a single sub-query. Our approach
of re-writing queries could be compared to query re-
formulation, wherein a user follows up a query with
successive reformulations of the original. In the web
environment, studies have shown that most users
still enter only one or two queries, and conduct lim-
ited query reformulation (Spink et al, 2002). We hy-
pothesize that the techniques we have developed will
be well-suited for search engines like Ask Jeeves
where 50% of the queries are in question format
226
(Spink and Ozmultu, 2002). More experimentation
in the Web domain is required to substantiate this.
8 Conclusions
Our results clearly show that shorter reformulations
of long queries can greatly impact performance. We
believe that our technique has great potential to be
used in an adaptive information retrieval environ-
ment, where the user starts off with a more general
information need and a looser notion of relevance.
The initial query can then be made longer to express
a most focused information need.
As part of future work, we plan to conduct a more
elaborate study with more interaction strategies in-
cluded. Better techniques to select effective sub-
queries are also in the pipeline. Since we used mu-
tual information as the basis for most of our sub-
query selection procedures, we could not consider
sub-queries that comprised of a single term. We plan
to address this issue too in future work.
9 Acknowledgments
This work was supported in part by the Center
for Intelligent Information Retrieval and in part by
the Defense Advanced Research Projects Agency
(DARPA) under contract number HR0011-06-C-
0023. Any opinions, findings and conclusions or
recommendations expressed in this material are the
authors and do not necessarily reflect those of the
sponsor. We also thank the anonymous reviewers
for their valuable comments.
References
James Allan, James P. Callan, W. Bruce Croft, Lisa Ballesteros,
John Broglio, Jinxi Xu, and Hongming Shu. 1996. Inquery
at TREC-5. In TREC.
Peter G. Anick and Suresh Tipirneni. 1999. The paraphrase
search assistant: terminological feedback for iterative infor-
mation seeking. In 22nd ACM SIGIR Proceedings, pages
153?159.
Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine
Learning, 34(1-3):211?231.
Chris Buckley, Mandar Mitra, Janet Walz, and Claire Cardie.
2000. Using clustering and superconcepts within smart:
TREC 6. Information Processing and Management,
36(1):109?131.
David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg.
2006. What makes a query difficult? In 29th ACM SIGIR
Proceedings, pages 390?397.
Kenneth Ward Church and Patrick Hanks. 1989. Word associ-
ation norms, mutual information, and lexicography. In 27th
ACL Proceedings, pages 76?83.
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,
and Clifford Stein. 2001. Introduction to Algorithms, Sec-
ond Edition. The MIT Electrical Engineering and Computer
Science Series. The MIT Press.
Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. 2002.
Predicting query performance. In 25th ACM SIGIR Proceed-
ings, pages 299?306.
Donna Harman and Chris Buckley. 2004. The NRRC reliable
information access (RIA) workshop. In 27th ACM SIGIR
Proceedings, pages 528?529.
Jeffrey P. Kern, Marios Pattichis, and Samuel D. Stearns. 2003.
Registration of image cubes using multivariate mutual infor-
mation. In Thirty-Seventh Asilomar Conference, volume 2,
pages 1645?1649.
Reiner Kraft, Chi Chao Chang, Farzin Maghoul, and Ravi Ku-
mar. 2006. Searching with context. In 15th International
CIKM Conference Proceedings, pages 477?486.
Victor Lavrenko and W. Bruce Croft. 2001. Relevance based
language models. In 24th ACM SIGIR Conference Proceed-
ings, pages 120?127.
C. J. Van Rijsbergen. 1979. Information Retrieval.
Butterworth-Heinemann, Newton, MA, USA, 2 edition.
Jacob Shapiro and Isak Taksa. 2003. Constructing web search
queries from the user?s information need expressed in a nat-
ural language. In Proceedings of the 2003 ACM Symposium
on Applied Computing, pages 1157?1162.
Amanda Spink and H. Cenk Ozmultu. 2002. Characteristics of
question format web queries: An exploratory study. Infor-
mation Processing and Management, 38(4):453?471.
Amanda Spink, Bernard J. Jansen, Dietmar Wolfram, and Tefko
Saracevic. 2002. From e-sex to e-commerce: Web search
changes. Computer, 35(3):107?109.
Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Per-
sonalizing search via automated analysis of interests and ac-
tivities. In 28th ACM SIGIR Proceedings, pages 449?456.
Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and Ken
Wood. 2006. On ranking the effectiveness of searches. In
29th ACM SIGIR Proceedings, pages 398?404.
Ellen M. Voorhees. 2006. The TREC 2005 robust track. SIGIR
Forum, 40(1):41?48.
Ryen W. White, Joemon M. Jose, and Ian Ruthven. 2005. Us-
ing top-ranking sentences to facilitate effective information
access: Book reviews. JAIST, 56(10):1113?1125.
227
