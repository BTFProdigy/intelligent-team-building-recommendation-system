  
Combining Linguistic Features with Weighted Bayesian Classifier 
for Temporal Reference Processing 
 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong 
csghcao@comp.polyu.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong 
kfwong@se.cuhk.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
Abstract 
Temporal reference is an issue of determining 
how events relate to one another. Determining 
temporal relations relies on the combination of 
the information, which is explicit or implicit in 
a language. This paper reports a computational 
model for determining temporal relations in 
Chinese. The model takes into account the ef-
fects of linguistic features, such as tense/aspect, 
temporal connectives, and discourse structures, 
and makes use of the fact that events are repre-
sented in different temporal structures. A ma-
chine learning approach, Weighted Bayesian 
Classifier, is developed to map their combined 
effects to the corresponding relations. An em-
pirical study is conducted to investigate differ-
ent combination methods, including lexical-
based, grammatical-based, and role-based 
methods. When used in combination, the 
weights of the features may not be equal. Incor-
porating with an optimization algorithm, the 
weights are fine tuned and the improvement is 
remarkable. 
 
1 Introduction 
Temporal information describes changes and time 
of the changes. In a language, the time of an event 
may be specified explicitly, for example ????
1997??????????? (They solved the traf-
fic problem of the city in 1997)?; or it may be related 
to the time of another event, for example ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built?. Temporal reference describes how 
events relate to one another, which is essential to 
natural language processing (NLP). Its major appli-
cations cover syntactic structural disambiguation 
(Brent, 1990), information extraction and question 
answering (Li, 2002), language generation and ma-
chine translation (Dorr, 2002). 
Many researchers have attempted to characterize 
the nature of temporal reference in a discourse. Iden-
tifying temporal relations1 between two events de-
                                                 
1 The relations under examined include both intra-sentence and inter-
pends on a combination of information resources. 
This information is provided by explicit tense and 
aspect markers, implicit event classes or discourse 
structures. It has been used to explain semantics of 
temporal expressions (Moens, 1988; Webber, 1988), 
to constrain possible temporal interpretations 
(Hitzeman, 1995; Sing, 1997), or to generate appro-
priate temporally conjoined clauses (Dorr, 2002). 
The purpose of our work is to develop a computa-
tional model, which automatically determines tempo-
ral relations in Chinese. While temporal reference 
interpretation in English has been well studied, Chi-
nese has been rarely discussed. In our study, thirteen 
related features are identified from linguistic per-
spective. How to combine these features and how to 
map their combined effects to the corresponding rela-
tions are the critical issues to be addressed in this 
paper. 
Previous work was limited in that they just con-
structed constraint or preference rules for some rep-
resentative examples. These methods are ineffective 
for computing purpose, especially when a large 
number of the features are involved and the interac-
tion among them is unclear. Therefore, a machine 
learning approach is applied and the empirical stud-
ies are carried out in our work. 
The rest of this paper is organized as follows. Sec-
tion 2 introduces temporal relation representations. 
Section 3 provides linguistic background of temporal 
reference and investigates linguistic features for de-
termining temporal relations in Chinese. Section 4 
explains the methods used to combine linguistic fea-
tures with Bayesian Classifier. It is followed by a 
description of the optimization algorithm which is 
used for estimating feature weights in Section 5. Fi-
nally, Section 6 concludes the paper. 
2 Representing Temporal Relations 
With the growing interests to temporal information 
processing in NLP, a variety of temporal systems 
have been introduced to accommodate the character-
istics of temporal information. In order to process 
temporal reference in a discourse, a formal represen-
                                                                            
sentence relations. 
  
tation of temporal relations is required. Among those 
who worked on representing or explaining temporal 
relations, some have taken the work of Reichenbach 
(Reichenbach, 1947) as a starting point, while others 
based their works on Allen?s (Allen, 1983). 
Reichenbach proposed a point-based temporal the-
ory. Reichenbach?s representation associated English 
tenses and aspects with three time points, namely 
event time (E), speech time (S) and reference time 
(R). The reference of E-R and R-S was either before 
(or after in reverse order) or simultaneous. This the-
ory was later enhanced by Bruce who defined seven 
temporal relations (Bruce, 1972). Given two durative 
events, the interval relations between them were 
modeled by the order between the greatest lower 
bounding point and least upper bounding point of the 
two events. In the other camp, instead of adopting 
time points, Allen took intervals as temporal primi-
tives to facilitate temporal reasoning and introduced 
thirteen basic relations. In this interval-based repre-
sentation, points were relegated to a subsidiary status 
as ?meeting places? of intervals. An extension to 
Allen?s theory, which treated both points and inter-
vals as primitives on an equal footing, was later in-
vestigated by Knight and Ma (Knight, 1994). 
In natural languages, events described can be ei-
ther punctual or durative in nature. A punctual event, 
e.g., ?? (explore), occurs instantaneously. It takes 
time but does not last in a sense that it lacks of a 
process of change. It is adequate to represent a punc-
tual event with a simple point structure. Whilst, a 
durative event, e.g., ?? (built a house), is more 
complex and its accomplishment as a whole involves 
a process spreading in time. Representing a durative 
event requires an interval representation. For this 
reason, Knight and Ma?s model is adopted in our 
work (see Figure 1). Taking the sentence ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built)? as an example, the relation held between 
building the bridge (i.e., an interval) and solving the 
problem (i.e., a point) is BEFORE. 
 
Figure 1 13 relations represented with points and intervals 
3 Linguistic Background of Temporal Refer-
ence in a Discourse 
3.1 Literature Review 
There were a number of theories in the literature 
about how temporal relations between events can be 
determined in English. Most of the researches on 
temporal reference were based on Reichenbach?s 
notion of tense/aspect structure, which was known as 
Basic Tense Structure (BTS). As for relating two 
events adjoined by a temporal/causal connective, 
Hornstein (Hornstein, 1990) proposed a neo-
Reichenbach structure which organized the BTSs 
into a Complex Tense Structure (CTS). It has been 
argued that all sentences containing a matrix and an 
adjunct clause were subject to linguistic constraints 
on tense structure regardless of the lexical words in-
cluded in the sentence. Generally, constraints were 
used to support syntactic disambiguation (Brent, 
1990) or to generate acceptable sentences (Dorr, 
2002). 
In a given CTS, a past perfect clause should pre-
cede the event described by a simple past clause. 
However, the order of two events in CTS does not 
necessarily correspond to the order imposed by the 
interpretation of the connective (Dorr, 2002). Tem-
poral/casual connective, such as ?after?, ?before? or 
?because?, can supply explicit information about the 
temporal ordering of events. Passonneau (Passon-
neau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) 
determined intra-sentential relations by accounting 
for temporal or causal connectives. Dorr and Gaast-
erland (Dorr, 2002), on the other hand, studied how 
to generate the sentences which reflect event tempo-
ral relations by selecting proper connecting words. 
However, temporal connectives can be ambiguous. 
For instance, a ?when? clause permits many possible 
temporal relations. 
Several researchers have developed the models 
that incorporated aspectual types (such as those dis-
tinct from states, processes and events) to interpret 
temporal relations between clauses connected with 
?when?. Moens and Steedmen (Moens, 1988) devel-
oped a tripartite structure of events2, and emphasized 
it was the notion of causation and consequence that 
played a central role in defining temporal relations of 
events. Webber (Webber, 1988) improved upon the 
above work by specifying rules for how events are 
related to one another in a discourse and Sing and 
Sing defined semantic constraints through which 
events can be related (Sing, 1997). The importance 
of aspectual information in retrieving proper aspects 
and connectives for sentence generation was also 
recognized by Dorr and Gaasterland (Dorr, 2002). 
Some literature claimed that discourse structures 
suggested temporal relations. Lascarides and Asher 
(Lascarides, 1991) investigated various contextual 
effects on rhetorical relations (such as narration, 
elaboration, explanation, background and result). 
They corresponded each of the discourse relations to 
a kind of temporal relation. Later, Hitzeman (Hitze-
man, 1995) described a method for analyzing tempo-
ral structure of a discourse by taking into account the 
effects of tense, aspect, temporal adverbials and rhe-
                                                 
2  The structure comprises a culmination, an associated preparatory 
process and a consequence state. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY 
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
  
torical relations. A hierarchy of rhetorical and tempo-
ral relations was adopted so that they could mutually 
constrain each other.  
 To summarize, the interpretation of temporal rela-
tions draws on the combination of various informa-
tion resources, including explicit tense/aspect and 
connectives (temporal or otherwise), temporal 
classes implicit in events, or rhetorical relations hid-
den in a discourse. This conclusion, although drawn 
from the studies of English, provides the common 
understanding on what information is required for 
determining temporal relations across languages. 
3.2 Linguistic Features for Determining Tem-
poral Relations in Chinese 
Thirteen related linguistic features are recognized 
for determining Chinese temporal relations in this 
paper (See Table 1). The selected features are scat-
tered in various grammatical categories due to the 
unique nature of language, but they fall into the fol-
lowing three groups. 
(1) Tense/aspect in English is manifested by verb 
inflections. But such morphological variations are 
inapplicable to Chinese verbs. Instead, they are 
conveyed lexically. In other words, tense and as-
pect in Chinese are expressed using a combination 
of, for example, time words, auxiliaries, temporal 
position words, adverbs and prepositions, and 
particular verbs. They are known as Tense/Aspect 
Markers. 
(2) Temporal Connectives in English primarily in-
volve conjunctions, such as ?after? and ?before?, 
which are the key components in discourse struc-
tures. In Chinese, however, conjunctions, conjunc-
tive adverbs, prepositions and position words, or 
their combinations are required to represent 
connectives. A few verbs that express cause/effect 
imply a temporal relation. They are also regarded 
as a feature relating to discourse structure3. The 
words which contribute to the tense/aspect and 
temporal connective expressions are explicit in a 
sentence and generally known as Temporal Indica-
                                                 
3 The casual conjunctions such as ?because? are included in this 
group. 
tors. 
(3) Event Classes are implicit in a sentence. Events 
can be classified according to their inherent tem-
poral characteristics, such as the degree of telicity 
and atomicity. The four widespread accepted tem-
poral classes are state, process, punctual event and 
developing event (Li, 2002). Based on their 
classes, events interact with the tense/aspect of 
verbs to determine the temporal relations between 
two events. 
Temporal indicators and event classes are both re-
ferred to as Linguistic Features. Table 1 shows the 
association between a temporal indicator and its ef-
fects. Note that the association is not one-to-one. For 
example, adverbs affect tense/aspect (e.g. ?, being) 
as well as discourse structure (e.g. ?, at the same 
time). For another example, tense/aspect can be 
jointly affected by auxiliary words (e.g. ? , 
were/was), trend verbs (??, begin to), and so on. 
Obviously, it is not a simple task to map the com-
bined effects of the thirteen linguistic features to the 
corresponding relations. Therefore, a machine learn-
ing approach is proposed, which investigates how 
these features contribute to the task and how they 
should be combined. 
4 Combining Linguistic Features with Machine 
Learning Approach 
Previous efforts in corpus-based NLP have incor-
porated machine learning methods to coordinate mul-
tiple linguistic features, for example, in accent resto-
ration (Yarowsky, 1994) and event classification 
(Siegel, 1998).  
Temporal relation determination can be modeled 
as a relation classification task. We formulate the 
thirteen temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The classifica-
tion process is to assign an event pair to one class 
according to their linguistic features. There existed 
numerous classification algorithms based upon su-
pervised learning principle. One of the most effective 
classifiers is Bayesian Classifier, introduced by Duda 
and Hart (Duda, 1973) and analyzed in more detail 
by Langley and Thompson (Langley, 1992). Its pre-
dictive performance is competitive with state-of-the-
Linguistic Feature Symbol POS Tag Effect Example 
With/Without punctuations PT Not Applicable Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb objects VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing wish/hope VA TI_va Tense ??, ?, ? 
Verbs related to causality VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, De-
veloping Event, Process 
Table 1 Linguistic features: eleven temporal indicators and one event class 
  
art classifiers, such as C4.5 and SVM (Friedman, 
1997). 
4.1 Bayesian Classifier 
Given the class c , Bayesian Classifier learns from 
training data the conditional probability of each at-
tribute. Classification is performed by applying 
Bayes rule to compute the posterior probability of c  
given a particular instance x , and then predicting the 
class with the highest posterior probability ratio. Let 
],...,,,,[ 2121 nttteex = , Eee ?21 ,  are the two event 
classes and Tttt n ?,...,, 21 are the temporal indicators 
(i.e. the words). E is the set of event classes. T is the 
set of temporal indicators. Then x is classified as: 
???
?
???
?=
),...,,,,|(
),...,,,,|(
logmaxarg
2121
2121*
n
n
c ttteecP
ttteecP
c  (E1)
where c denotes the classes different from c . As-
suming event classes are independent of temporal 
indicators given c , we have: 
???
?
???
?=
???
?
???
?
)()|,...,,,,(
)()|,...,,,,(
log
),...,,,,|(
),...,,,,|(
log
2121
2121
2121
2121
cPcttteeP
cPcttteeP
ttteecP
ttteecP
n
n
n
n
                          (E2)
???
?
???
?+???
?
???
?+???
?
???
?=
)|,...,,(
)|,...,,(
log
)|,(
)|,(
log
)(
)(log
21
21
21
21
ctttP
ctttP
ceeP
ceeP
cP
cP
n
n  
Assuming temporal indicators are independent of 
each other, we have 
?
=
= n
i i
i
n
n
ctP
ctP
ctttP
ctttP
121
21
)|(
)|(
)|,...,,(
)|,...,,( ,    ( ni ,...2,1= ) (E3)
A Na?ve Bayesian Classifier assumes strict inde-
pendence among all attributes. However, this as-
sumption is not satisfactory in the context of tempo-
ral relation determination. For example, if the 
relation between 1e  and 2e  is SAME_AS, 1e  and 2e  
have to be identical. We release the independence 
assumption for 1e and 2e , and decompose the second 
part of (E2) as: 
),|()|(
),|()|(
)|,(
)|,(
121
121
21
21
ceePceP
ceePceP
ceeP
ceeP =  (E4)
Estimation of ),|( 12 ceep is motivated by Absolute 
Discounting N-Gram language model (Goodman, 
2001): 
??
??
?
=
>?=
0),,( if     )|(),(
0),,( if    
),(
),,(
),|(
1221
12
1
12
12
ceeCcePce
ceeC
ceC
DceeC
ceP e
?
 
 
(E5) 
here D is the discount factor and is set to 0.5 experi-
mentally. From the fact that 1),|(
2
12 =?
e
ceeP , we get: 
?
?
>
>
?
?
=
0),,(|
2
0),,(|
12
1
122
122
)|(1
),|(1
),(
ceeCe
ceeCe
ceP
ceeP
ce?  
 
(E6)
)|( ctp i and )|( cep i  are estimated by MLE with 
Dirichlet Smoothing method: 
?
?
+
+=
Tt
i
i
i
i
TuctC
uctC
ctP
||),(
),(
)|(      ( ni ,...2,1= )  
(E7)
?
?
+
+=
Ee
i
i
i
i
EuceC
uceC
ceP
||),(
),(
)|(     ( 2,1=i )  
(E8)
where u (=0.5) is the smoothing factor. Then, 
)|( ctp i , )|( cep i and ),|( 12 ceeP  can be estimated 
with (E5) - (E8) by substituting c  with c . 
4.2 Estimating )|,...,( 21 ctttP n  with Lexical-POS 
Information 
The effects of a temporal indicator are constrained 
by its positions in a sentence. For instance, the con-
junctive word ?? (because) may represent the dif-
ferent relations when it occurs before or after the first 
event. Therefore, in estimating )|,...,( 21 ctttp n , we 
consider an indicator located in three positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Note that 
cases (2) and (3) are ambiguous. The positions of the 
temporal indicators are the same. But it is uncertain 
whether these indicators modify the first or the sec-
ond event if there is no punctuation (such as comma, 
period, exclamation or question mark) separating 
their roles. The ambiguity is resolved by using POS 
information. We assume that an indicator modifies 
the first event if it is an auxiliary word, a trend word 
or a position word; otherwise it modifies the second. 
Thus, we rewrite )|,...,( 21 ctttP n as ,,...,( 1111 nttP  
)|,...,,...,,,...,
432 441331221
ctttttt nnn , where jn is the total 
number of the temporal indicators occurring in the 
position j . 4,3,2,1=j  represents the four positions 
and nn
j
j =?=
4
1
. Assuming jit are independent of each 
other, then ?
=
n
i
i ctP
1
)|( in (E3) is revised as 
??
= =
4
1 1
)|(
j
n
i
ji
j
ctP . Accordingly, (E7) is revised as: 
?
?
+
+=
Tt
ji
ji
ji
ji
TuctC
uctC
ctP
||),(
),(
)|(  
( 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7?)
In addition to taking positions into account, we 
further classify the temporal indicators into two 
groups according to their grammatical categories or 
semantic roles. The rationale of grouping will be 
demonstrated in Section 4.3. 
4.3 Experimental Results 
Several experiments have been designed to evalu-
ate the proposed Bayesian Classifier in combining 
linguistic features for temporal relation determination 
and to reveal the impact of linguistic features on 
learning performance. 700 instances are extracted 
from Ta Kong Pao (a local Hong Kong Chinese 
newspaper) financial version. Among them, 500 are 
used as training data, and 200 as test data, which are 
  
partitioned equally into two sets. One is similar as 
training data in class distribution, while the other is 
quite different. 209 lexical words, gathered from lin-
guistic books and corpus, are used as the temporal 
indicators and manually marked with the tags given 
in Table 1. 
4.3.1 Impact of Individual Features 
From linguistic perspective, the thirteen features 
(see Table 1) are useful for temporal relation deter-
mination. To examine the impact of each individual 
feature, we feed a single linguistic feature to the 
Bayesian Classifier learning algorithm one at a time 
and study the accuracy of the resultant classifier. The 
experimental results are given in Table 2. It shows 
that event classes have greatest accuracy, followed 
by conjunctions in the second place, and adverbs in 
the third in the close test. Since punctuation shows 
no contribution, we only use it as a syntactic feature 
to differentiate cases (2) and (3) mentioned in Sec-
tion 4.2. 
4.3.2 Features in Combination 
We now use Bayesian Classifier introduced in Sec-
tions 4.1 and 4.2 to combine all the related temporal 
indicators and event classes, since none of the fea-
tures can achieve a good result alone. The simplest 
way is to combine the features without distinction. 
The conditional probability )|( ctP ji is estimated by 
(E7?). This model is called Ungrouped Model (UG). 
However, as illustrated in table 1, the temporal in-
dicators play different roles in building temporal ref-
erence. It is not reasonable to treat them equally. We 
claim that the temporal indicators have two functions, 
i.e., representing the connections of the clauses, or 
representing the tense/aspect of the events. We iden-
tify them as connective words or tense/aspect mark-
ers and separate them into two groups. This allows 
features to be compared with those in the same group. 
Let ],[ 21 TTT = , where 1T is the set of connective 
words and 2T is the set of tense/aspect markers. We 
have 1112
1
1 ,..,, Tttt m ? and 222221 ,..,, Tttt l ? , m and l are 
the number of the connective words and the 
tense/aspect markers in a sentence respectively. We 
assume that the occurrences of the two groups are 
independent. By taking both grouping and position 
features into account, we replace ?
=
n
i
i ctP
1
)|(  with 
???
= = =
2
1
4
1 1
)|(
k j
n
i
k
ji
k
j
ctP , 2,1=k  represents the two groups 
and j
k
k
j nn =?=
2
1
. To build the grouping-based Bayes-
ian Classifier, (E7?) is modified as: 
?
?
+
+=
kk
ji Tt
kk
ji
k
jik
ji TuctC
uctC
ctP
||),(
),(
)|(  
( 2,1=k , 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7??)
4.3.3 Grouping Features by Grammatical Cate-
gories or Semantic Roles 
We partition temporal indicators into connective 
words and tense/aspect markers in two ways. One is 
simply based on their grammatical categories (i.e. 
POS information). It separates conjunctions (e.g., ?
?, after; ??, because) and verbs relating to causal-
ity (e.g., ??, cause) from others. They are assumed 
to be connective words (i.e. 1T? ), while others are 
tense/aspect markers (i.e. 2T? ). This model is called 
Grammatical Function based Grouping Model (GFG). 
Unfortunately, such a separation is ineffective. In 
comparison with UG, the performance of GFG de-
creases as shown in figure 2. This reveals the com-
plexity of Chinese in connecting expressions. It 
arises from the fact that some other words, such as 
adverbs (e.g., ???, meanwhile), prepositions (e.g., 
?, at) and position words (e.g., ??, before), can 
also serve such a connecting function (see Table 1). 
Actually, the roles of the words falling into these 
grammatical categories are ambiguous. For instance, 
the adverb? can express an event happened in the 
past, e.g., ????????? (He just finished the 
report)?. It can be also used in a connecting expres-
sion (such as ????), e.g., ??????????
??? (He went to the library after he had finished 
the report)?. 
This finding suggests that temporal indicators 
should be divided into two groups according to their 
semantic roles rather than grammatical categories. 
Therefore we propose the third model, namely 
Semantic Role based Grouping Model (SRG), in 
which the indicators are manually re-marked as 
TI_j_pos or TI_at_pos4. 
Figure 2 shows the accuracies of four models (i.e. 
DM. UG, GFG and SRG) based on the three tests. 
Test 1 is the close test carried out on training data 
and tests 2 and 3 are open tests performed on differ-
ent test data. DM (i.e., Default Model) assigns all 
incoming cases with the most likely class and it is 
used as evaluation baseline. In our case, it is 
SAME_AS, which holds 50.2% in training data. 
SRG model outperforms UG and GFG models. 
These results validate our previous assumption em-
pirically. 
                                                 
4 ?j? and ?at? are the tags representing connecting and tense/aspect 
roles respectively. ?pos? is the POS tag of the temporal indicator TI. 
Accuracy Accuracy  
Feature Close 
test 
Open 
test 1 
Open 
test 2 
 
Feature Close 
test 
Open 
test 1
Open 
test 2
VS 53.4% 48% 30% VA 57% 50% 37%
VC 56.6% 56% 49% C 62.6% 52% 45%
TR 50.2% 46% 28% U 51.8% 50% 32%
P 52.4% 49% 30% T 57.2% 48% 32%
PS 59% 53% 38% D 59.6% 55% 47%
VV 51% 49% 29% EC 72.4% 69% 68%
Table 2 Impact of each individual linguistic feature 
  
20%
30%
40%
50%
60%
70%
80%
90%
Close Test Open Test1 Open Test2
A
cc
ur
ac
y
DM UG GFG SRG
 
Figure 2 Comparing DM, UG, GFG and SRG models 
4.3.4 Impact of Semantic Roles in SRG Model 
When the temporal indicators are classified into 
two groups based on their semantic roles in SRG 
model, there are three types of linguistic features 
used in the Bayesian Classifier, i.e., tense/aspect 
markers, connective words and event classes. A set 
of experiments are conducted to investigate the im-
pacts of each individual feature type and the impacts 
when they are used in combination (shown in Table 
3). We find that the performance of methods 1 and 2 
in the open tests drops dramatically compared with 
those in the close test. But the predictive strength of 
event classes in method 3 is surprisingly high. Two 
conclusions are thus drawn. Firstly, the models using 
tense/aspect markers and connective words are more 
likely to encounter over-fitting problem with insuffi-
cient training data. Secondly, different features have 
varied weights. We then incorporate an optimization 
approach to adjust the weights of the three types of 
features, and propose an algorithm to tackle over-
fitting problem in the next section. 
Method Semantic Groups 
Close 
test 
Open 
test 1 
Open 
test 2
1 Tense/aspect markers 71% 58% 40%
2 Connective words 75% 65% 57%
3 Event classes 66.6% 69% 68%
4 1+2 84.8% 70% 56%
5 1+3 76.6% 72% 66%
6 2+3 82.4% 84% 81%
7 1+2+3 89.8% 84% 80%
8 Default 50.2% 46% 28%
Table 3: Impact of Semantic Role based Groups 
5. Weighted Bayesian Classifier  
Let 1? , 2? , 3? be the weights of event classes, con-
nective words and tense/aspect markers respectively. 
Then the Weighted Bayesian Classifier is: 
???
?
???
?
),...,,,,|(
),...,,,,|(
log
2121
2121
n
n
ttteecP
ttteecP  
???
?
???
?+???
?
???
?=
)|,(
)|,(
log
)(
)(log
21
21
1 ceeP
ceeP
cP
cP ?                             (E9)
???
?
???
?+???
?
???
?+
)|,...,,(
)|,...,,(
log
)|,...,,(
)|,...,,(
log 22
2
2
1
22
2
2
1
311
2
1
1
11
2
1
1
2 ctttP
ctttP
ctttP
ctttP
l
l
m
m ??  
In order to estimate the weights, we need a suit-
able optimization approach to search for the opti-
mal value of ],,[ 321 ???  automatically. 
5.1 Estimating Weights with Simulated Anneal-
ing Algorithm 
Quite a lot optimization approaches are available 
to compute the optimal value of ],,[ 321 ??? . Here, 
Simulated Annealing algorithm is employed to per-
form the task, which is a general and powerful opti-
mization approach with excellent global convergence 
(Kirkpatrick, 1983). Figure 3 shows the procedure of 
searching for an optimal weight vector with the algo-
rithm. 
1. 1=k , )( 1?= kk tTt  
2. Generates a random change from the current weight vec-
tor iv . The updated weight vector is denoted by jv . Then 
computes the increasement of the objective function, i.e. 
)()( ij vfvf ?=? . 
3 Accepts jv as an optimal vector and substitutes iv with the 
following accept rate: 
??
??
?
??
?
? <
>
= 0 if  )exp(
0 if             1
)(
k
ji
t
vvP  
4 If kLk < , lets 1+= kk , goes to step 2. 
5 Else if fk Tt < , goes to step 1. 
6 Else stops looping and outputs the current optimal weight 
vector.  
Figure 3 Simulated Annealing algorithm 
In Figure 3, Markov chain length 20=kL ; tem-
perature update function ttT *9.0)( = ; starting point 
],,[ 03
0
2
0
1
0 ???=v =[1,1,1]; initial temperature 200 =t  
and final temperature 810?=ft . Note that the initial 
temperature is critical for a simulated annealing algo-
rithm (Kirkpatrick, 1983). Its value should assure 
that the initial accept rate is greater than 90%. 
5.2 K-fold Cross-Validation 
The accuracy of the classifier is defined as the ob-
jective function of the Simulated Annealing algo-
rithm illustrated in Figure 3. If it is evaluated with 
the accuracy over all training data, the Weighted 
Bayesian Classifier may trap into over-fitting prob-
lem and lower the performance due to insufficient 
data. To avoid this, we employ K-fold Cross-
Validation technique. It partitions the original set of 
data into K parts. One part is selected arbitrarily as 
evaluating data and the other K-1 parts as training 
data. Then K accuracies on evaluating data are ob-
tained after K iterations and their average is used as 
the objective function. 
5.3 Experimental Results 
Table 4 shows the result of the experiment which 
compares WSRG (Weighted SRG) with SRG. We 
use error reduction to evaluate the benefit from in-
corporating weight parameters into Bayesian Classi-
fier. It is defined as: 
SRG
WSRGSRG
rateerror
rateerrorrateerror
_
__
reductionerror 
?=  
  
The experimental results show that the Weighted 
Bayesian Classifier outperforms the Bayesian Classi-
fier significantly in the two open tests and it tackles 
the over-fitting problem well. To test Simulated An-
nealing algorithm?s global convergence, we ran-
domly choose several initial values and they finally 
converge to a small area [7.2?0.09, 5.8?0.02, 
3.0?0.02]. The empirical result demonstrates that the 
output of a Simulated Annealing algorithm is a 
global optimal weighting vector. 
6 Conclusions 
Temporal reference processing has received grow-
ing attentions in last decades. However this topic has 
not been well studied in Chinese. In this paper, we 
proposed a method to determine temporal relations in 
Chinese by employing linguistic knowledge and ma-
chine learning approaches. Thirteen related linguistic 
features were recognized and temporal indicators 
were further grouped with respect to grammatical 
functions or semantic roles. This allows features to 
be compared with those in the same group. To ac-
commodate the fact that the different types of fea-
tures support varied importance, we extended Na?ve 
Bayesian Classifier to Weighted Bayesian Classifier 
and applied Simulated Annealing algorithm to opti-
mize weight parameters. To avoid over-fitting prob-
lem, K-fold Cross-Validation technique was incorpo-
rated to evaluate the objective function of the optimi-
zation algorithm. Establishing the temporal relations 
between two events could be extended to provide a 
determination of the temporal relations among multi-
ple events in a discourse. With such an extension, 
this temporal analysis approach could be incorpo-
rated into various NLP applications, such as question 
answering and machine translation. 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and CUHK 
Strategic Grant (account number 4410001). 
References 
Allen J., 1983. Maintaining Knowledge about Temporal 
Intervals. Communications of the ACM, 26(11):832-
843. 
Brent M., 1990. A Simplified Theory of Tense Repre-
sentations and Constraints on Their Composition, In 
Proceedings of the 28th Annual Conference of the As-
sociation for Computational Linguistics, pages 119-
126. Pittsburgh. 
Bruce B., 1972. A Model for Temporal References and 
its Application in Question-Answering Program. Arti-
ficial Intelligence, 3(1):1-25. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting Words 
from Temporal Expressions. submitted to Journal of 
Artificial Intelligence Research. 
Duda, R. O. and P. E. Hart, 1973. Pattern Classification 
and Scene Analysis. New York. 
Friedman N., Geiger D. and Goldszmidt M., 1997. 
Bayesian Network Classifiers. Machine Learning 
29:131-163, Kluwer Academic Publisher. 
Goodman J., 2001. A Bit of Progress in Language Mod-
eling. Microsoft Research Technical Report MSR-
TR-2001-72. 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of Dis-
course. In Proceedings of the 7th European Meeting 
of the Association for Computational Linguistics, 
pages 253-260. Dublin, Ireland.  
Hornstein N., 1990. As Time Goes By. MIT Press, Cam-
bridge, MA. 
Kirkpatrick, S., Gelatt C.D., and Vecchi M.P., 1983. 
Optimization by Simulated Annealing. Science, 
220(4598): 671-680. 
Knight B. and Ma J., 1997. Temporal Management Us-
ing Relative Time in Knowledge-based Process Con-
trol, Engineering Applications of Artificial Intelli-
gence, 10(3):269-280.  
Langley, P.W. and Thompson K., 1992. An Analysis of 
Bayesian Classifiers. In Proceedings of the 10th Na-
tional Conference on Artificial Intelligence, pages 
223?228. San Jose, CA. 
Lascarides A. and Asher N., 1991. Discourse Relations 
and Defensible Knowledge. In Proceedings of the 29th 
Meeting of the Association for Computational Lin-
guistics, pages 55-62. Berkeley, USA. 
Li W.J. and Wong K.F., 2002. A Word-based Approach 
for Modeling and Discovering Temporal Relations 
Embedded in Chinese Sentences, ACM Transaction 
on Asian Language Processing, 1(3):173-206. 
Moens M. and Steedmen M., 1988. Temporal Ontology 
and Temporal Reference. Computational Linguistics, 
14(2):15-28. 
Passonneau R., 1988. A Computational Model of the 
Semantics of Tense and Aspect. Computational Lin-
guistics, 14(2):44-60. 
Reichenbach H., 1947. The Elements of Symbolic Logic. 
The Free Press, New York. 
Siegel E.V. and McKeown K.R., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving As-
pectual Classification and Revealing Linguistic In-
sights. Computational Linguistics, 26(4):595-627. 
Singh M. and Singh M., 1997. On the Temporal Struc-
ture of Events. In Proceedings of AAAI-97 Workshop 
on Spatial and Temporal Reasoning, pages 49-54. 
Providence, Rhode Island. 
Webber B., 1988. Tense as Discourse Anaphor. Compu-
tational Linguistics, 14(2):61-73.  
Yarowsky D., 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to the Accent Restora-
tion in Spanish and French. In Proceeding of the 32nd 
Annual Meeting of the Association for Computational 
Linguistics, pages 88-95. San Francisco, CA. 
Error Rate 
Model 
Close Test Open Test1 Open Test2
SRG 10.2% 16% 20% 
WSRG 12.4%% 11% 13% 
Error Reduction -21.57% 31.25% 35% 
Table 4 Compare WSRG with SRG on error rates 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 489?496
Manchester, August 2008
PNR2: Ranking Sentences with Positive and Negative Reinforcement 
for Query-Oriented Update Summarization 
 
 Abstract 
Query-oriented update summarization is 
an emerging summarization task very 
recently. It brings new challenges to the 
sentence ranking algorithms that require 
not only to locate the important and 
query-relevant information, but also to 
capture the new information when 
document collections evolve. In this 
paper, we propose a novel graph based 
sentence ranking algorithm, namely PNR2, 
for update summarization. Inspired by the 
intuition that ?a sentence receives a 
positive influence from the sentences that 
correlate to it in the same collection, 
whereas a sentence receives a negative 
influence from the sentences that 
correlates to it in the different (perhaps 
previously read) collection?, PNR2 
models both the positive and the negative 
mutual reinforcement in the ranking 
process. Automatic evaluation on the 
DUC 2007 data set pilot task 
demonstrates the effectiveness of the 
algorithm.  
 
1 Introduction 
The explosion of the WWW has brought with it a 
vast board of information. It has become virtually 
impossible for anyone to read and understand 
large numbers of individual documents that are 
abundantly available. Automatic document 
summarization provides an effective means to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
manage such an exponentially increased 
collection of information and to support 
information seeking and condensing goals.  
The main evaluation forum that provides 
benchmarks for researchers working on 
document summarization to exchange their ideas 
and experiences is the Document Understanding 
Conferences (DUC). The goals of the DUC 
evaluations are to enable researchers to 
participate in large-scale experiments upon the 
standard benchmark and to increase the 
availability of appropriate evaluation techniques. 
Over the past years, the DUC evaluations have 
evolved gradually from single-document 
summarization to multi-document summarization 
and from generic summarization to query-
oriented summarization. Query-oriented multi-
document summarization initiated in 2005 aims 
to produce a short and concise summary for a 
collection of topic relevant documents according 
to a given query that describes a user?s particular 
interests. 
Previous summarization tasks are all targeted 
on a single document or a static collection of 
documents on a given topic. However, the 
document collections can change (actually grow) 
dynamically when the topic evolves over time. 
New documents are continuously added into the 
topic during the whole lifecycle of the topic and 
normally they bring the new information into the 
topic. To cater for the need of summarizing a 
dynamic collection of documents, the DUC 
evaluations piloted update summarization in 2007. 
The task of update summarization differs from 
previous summarization tasks in that the latter 
aims to dig out the salient information in a topic 
while the former cares the information not only 
salient but also novel.  
Up to the present, the predominant approaches 
in document summarization regardless of the 
nature and the goals of the tasks have still been 
built upon the sentence extraction framework. 
Li Wenjie1, Wei Furu1,2, Lu Qin1, He Yanxiang2 
1Department of Computing 
The Hong Kong Polytechnic University, HK 
{csfwei, cswjli, csluqin} 
@comp.polyu.edu.hk 
2Department of Computer Science 
and Technology, Wuhan University, China 
{frwei, yxhe@whu.edu.cn} 
489
Under this framework, sentence ranking is the 
issue of most concern. In general, two kinds of 
sentences need to be evaluated in update 
summarization, i.e. the sentences in an early (old) 
document collection A (denoted by SA) and the 
sentences in a late (new) document collection B 
(denoted by SB). Given the changes from SA to SB, 
an update summarization approach may be 
concerned about four ranking issues: (1) rank SA 
independently; (2) re-rank SA after SB comes; (3) 
rank SB independently; and (4) rank SB given that 
SA is provided. Among them, (4) is of most 
concern. It should be noting that both (2) and (4) 
need to consider the influence from the sentences 
in the same and different collections.  
In this study, we made an attempt to capture 
the intuition that  
?A sentence receives a positive influence from 
the sentences that correlate to it in the same 
collection, whereas a sentence receives a 
negative influence from the sentences that 
correlates to it in the different collection.? 
We represent the sentences in A or B as a text 
graph constructed using the same approach as 
was used in Erkan and Radev (2004a, 2004b). 
Different from the existing PageRank-like 
algorithms adopted in document summarization, 
we propose a novel sentence ranking algorithm, 
called PNR2 (Ranking with Positive and Negative 
Reinforcement). While PageRank models the 
positive mutual reinforcement among the 
sentences in the graph, PNR2 is capable of 
modeling both positive and negative 
reinforcement in the ranking process.  
The remainder of this paper is organized as 
follows. Section 2 introduces the background of 
the work presented in this paper, including 
existing graph-based summarization models, 
descriptions of update summarization and time-
based ranking solutions with web graph and text 
graph. Section 3 then proposes PNR2, a sentence 
ranking algorithm based on positive and negative 
reinforcement and presents a query-oriented 
update summarization model. Next, Section 4 
reports experiments and evaluation results. 
Finally, Section 5 concludes the paper. 
2 Background and Related Work 
2.1 Previous Work in Graph-based 
Document Summarization 
Graph-based ranking algorithms such as 
Google?s PageRank (Brin and Page, 1998) and 
Kleinberg?s HITS (Kleinberg, 1999) have been 
successfully used in the analysis of the link 
structure of the WWW. Now they are springing 
up in the community of document summarization. 
The major concerns in graph-based 
summarization researches include how to model 
the documents using text graph and how to 
transform existing web page ranking algorithms 
to their variations that could accommodate 
various summarization requirements. 
Erkan and Radev (2004a and 2004b) 
represented the documents as a weighted 
undirected graph by taking sentences as vertices 
and cosine similarity between sentences as the 
edge weight function. An algorithm called 
LexRank, adapted from PageRank, was applied 
to calculate sentence significance, which was 
then used as the criterion to rank and select 
summary sentences. Meanwhile, Mihalcea and 
Tarau (2004) presented their PageRank variation, 
called TextRank, in the same year. Besides, they 
reported experimental comparison of three 
different graph-based sentence ranking 
algorithms obtained from Positional Power 
Function, HITS and PageRank (Mihalcea and 
Tarau, 2005). Both HITS and PageRank 
performed excellently. 
Likewise, the use of PageRank family was also 
very popular in event-based summarization 
approaches (Leskovec et al, 2004; Vanderwende 
et al, 2004; Yoshioka and Haraguchi, 2004; Li et 
al., 2006). In contrast to conventional sentence-
based approaches, newly emerged event-based 
approaches took event terms, such as verbs and 
action nouns and their associated named entities 
as graph nodes, and connected nodes according 
to their co-occurrence information or semantic 
dependency relations. They were able to provide 
finer text representation and thus could be in 
favor of sentence compression which was 
targeted to include more informative contents in a 
fixed-length summary. Nevertheless, these 
advantages lied on appropriately defining and 
selecting event terms.  
All above-mentioned representative work was 
concerned with generic summarization. Later on, 
graph-based ranking algorithms were introduced 
in query-oriented summarization too when this 
new challenge became a hot research topic 
recently. For example, a topic-sensitive version 
of PageRank was proposed in (OtterBacher et al, 
2005). The same algorithm was followed by Wan 
et al (2006) and Lin et al (2007) who further 
investigated on its application in query-oriented 
update summarization.  
490
2.2 The DUC 2007 Update Summarization 
Task Description 
The DUC 2007 update summarization pilot task 
is to create short (100 words) multi-document 
summaries under the assumption that the reader 
has already read some number of previous 
documents. Each of 10 topics contains 25 
documents. For each topic, the documents are 
sorted in chronological order and then partitioned 
into three collections, ?A?, ?B? and ?C?. The 
participants are then required to generate (1) a 
summary for ?A?; (2) an update summary for 
?B? assuming documents in ?A? have already 
been read; and (3) an update summary for ?C? 
assuming documents in ?A? and ?B? have 
already been read. Growing out of the DUC 2007, 
the Text Analysis Conference (TAC) 2008 
planed to keep only the DUC 2007 task (1) and 
(2). 
Each topic collection in the DUC 2007 (will 
also in the TAC 2008) is accompanied with a 
query that describes a user?s interests and focuses. 
System-generated summaries should include as 
many responses relevant to the given query as 
possible. Here is a query example from the DUC 
2007 document collection ?D0703A?.  
<topic> 
<num> D0703A </num> 
<title> Steps toward introduction of the 
Euro. </title> 
<narr> Describe steps taken and worldwide 
reaction prior to introduction of the Euro on 
January 1, 1999. Include predictions and 
expectations reported in the press. </narr> 
</topic>                                          [D0703A] 
Update summarization is definitely a time-
related task. An appropriate ranking algorithm 
must be the one capable of coping with the 
change or the time issues.  
2.3 Time-based Ranking Solutions with 
Web Graph and Text Graph 
Graph based models in document summarization 
are inspired by the idea behind web graph models 
which have been successfully used by current 
search engines. As a matter of fact, adding time 
dimension into the web graph has been 
extensively studied in recent literature. 
Basically, the evolution in the web graph stems 
from (1) adding new edges between two existing 
nodes; (2) adding new nodes in the existing graph 
(consequently adding new edges between the 
existing nodes and the new nodes or among the 
new nodes); and (3) deleting existing edges or 
nodes. Berberich et al (2004 and 2005) 
developed two link analysis methods, i.e. T-Rank 
Light and T-Rank, by taking into account two 
temporal aspects, i.e. freshness (i.e. timestamp of 
most recent update) and activity (i.e. update rates) 
of the pages and the links. They modeled the web 
as an evolving graph in which each nodes and 
edges (i.e. web pages and hyperlinks) were 
annotated with time information. The time 
information in the graph indicated different kinds 
of events in the lifespan of the nodes and edges, 
such as creation, deletion and modifications. 
Then they derived a subgraph of the evolving 
graph with respect to the user?s temporal interest. 
Finally, the time information of the nodes and the 
edges were used to modify the random walk 
model as was used in PageRank. Specifically, 
they used it to modify the random jump 
probabilities (in both T-Rank Light and T-Rank) 
and the transition probabilities (in T-Rank only).  
Meanwhile, Yu et al (2004 and 2005) 
introduced a time-weighted PageRank, called 
TimedPageRank, for ranking in a network of 
scientific publications. In their approach, 
citations were weighted based on their ages. Then 
a post-processing step decayed the authority of a 
publication based on the publication?s age. Later, 
Yang et al (2007) proposed TemporalRank, 
based on which they computed the page 
importance from two perspectives: the 
importance from the current web graph snapshot 
and the accumulated historical importance from 
previous web graph snapshot. They used a kinetic 
model to interpret TemporalRank and showed it 
could be regarded as a solution to an ordinary 
differential equation.  
In conclusion, Yu et al tried to cope with the 
problem that PageRank favors over old pages 
whose in-degrees are greater than those of new 
pages. They worked on a static single snapshot of 
the web graph, and their algorithm could work 
well on all pages in the web graph. Yang et al, 
on the other hand, worked on a series of web 
graphs at different snapshots. Their algorithm 
was able to provide more robust ranking of the 
web pages, but could not alleviate the problem 
carried by time dimension at each web graph 
snapshot. This is because they directly applied 
the original PageRank to rank the pages. In other 
words, the old pages still obtained higher scores 
while the newly coming pages still got lower 
scores. Berberich et al focused their efforts on 
the evolution of nodes and edges in the web 
graph. However, their algorithms did not work 
491
when the temporal interest of the user (or query) 
was not available.  
As for graph based update summarization, 
Wan (2007) presented the TimedTextRank 
algorithm by following the same idea presented 
in the work of Yu et al Given three collections of 
chronologically ordered documents, Lin et al 
(2007) proposed to construct the TimeStamped 
graph (TSG) graph by incrementally adding the 
sentences to the graph. They modified the 
construction of the text graph, but the ranking 
algorithm was the same as the one proposed by 
OtterBacher et al  
Nevertheless, the text graph is different from 
the web graph. The evolution in the text graph is 
limited to the type (2) in the web graph. The 
nodes and edges can not be deleted or modified 
once they are inserted. In other words, we are 
only interested in the changes caused when new 
sentences are introduced into the existing text 
graph. As a result, the ideas from Berberich et al 
cannot be adopted directly in the text graph. 
Similarly, the problem in web graph as stated in 
the work of Yu et al (i.e. ?new pages, which may 
be of high quality, have few or no in-links and 
are left behind.?) does not exist in the text graph 
at all. More precisely, the new coming sentences 
are equally treated as the existing sentences, and 
the degree (in or out) of the new sentences are 
also equally accumulated as the old sentences. 
Directly applying the ideas from the work of Yu 
et al does not always make sense in the text 
graph. Recall that the main task for sentence 
ranking in update summarization is to rank SB 
given SA. So the idea from Yang et al is also not 
applicable.  
In fact, the key points include not only 
maximizing the importance in the current new 
document collection but also minimizing the 
redundancy to the old document collection when 
ranking the sentences for update summarization. 
Time dimension does contribute here, but it is not 
the only way to consider the changes. Unlike the 
web graph, the easily-captured content 
information in a text graph can provide additional 
means to analyze the influence of the changes.  
To conclude the previous discussions, adding 
temporal information to the text graph is different 
from it in the web graph. Capturing operations 
(such as addition, deletion, modification of web 
pages and hyperlinks) is most concerned in the 
web graph; however, prohibiting redundant 
information from the old documents is the most 
critical issue in the text graph. 
3 Positive and Negative Reinforcement 
Ranking for Update Summarization 
Existing document summarization approaches 
basically follow the same processes: (1) first 
calculate the significance of the sentences with 
reference to the given query with/without using 
some sorts of sentence relations; (2) then rank the 
sentences according to certain criteria and 
measures; (3) finally extract the top-ranked but 
non-redundant sentences from the original 
documents to create a summary. Under this 
extractive framework, undoubtedly the two 
critical processes involved are sentence ranking 
and sentence selection. In the following sections, 
we will first introduce the sentence ranking 
algorithm based on ranking with positive and 
negative reinforcement, and then we present the 
sentence selection strategy. 
3.1 Ranking with Positive and Negative 
Reinforcement (PNR2) 
Previous graph-based sentence ranking 
algorithms is capable to model the fact that a 
sentence is important if it correlates to (many) 
other important sentences. We call this positive 
mutual reinforcement. In this paper, we study two 
kinds of reinforcement, namely positive and 
negative reinforcement, among two document 
collections, as illustrated in Figure 1.  
 
Figure 1 Positive and Negative Reinforcement 
In Figure 1, ?A? and ?B? denote two document 
collections about the same topics (?A? is the old 
document collection, ?B? is the new document 
collection), SA and SB denote the sentences in 
?A? and ?B?. We assume: 
1. SA performs positive reinforcement on its 
own internally; 
2. SA performs negative reinforcement on SB 
externally; 
3. SB performs negative reinforcement on SA 
externally;  
4. SB performs positive reinforcement on its 
own internally. 
Positive reinforcement captures the intuition 
that a sentence is more important if it associates 
to the other important sentences in the same 
collection. Negative reinforcement, on the other 
hand, reflects the fact that a sentence is less 
A B + + 
- 
- 
492
important if it associates to the important 
sentences in the other collection, since such a 
sentence might repeat the same or very similar 
information which is supposed to be included in 
the summary generated for the other collection.  
Let RA and RB denote the ranking of the 
sentences in A and B, the reinforcement can be 
formally described as 
??
??
?
?+??+??=
?+??+??=
+
+
B
k
BBB
k
ABA
k
B
A
k
BAB
k
AAA
k
A
pRMRMR
pRMRMR
r
r
2
)(
2
)(
2
)1(
1
)(
1
)(
1
)1(
???
???
 (1) 
where the four matrices MAA, MBB, MAB and MBA 
are the affinity matrices of the sentences in SA, in 
SB, from SA to SB and from SB to SA. 
??
?
??
?
=
22
11
??
??
W  is a weight matrix to balance the 
reinforcement among different sentences. Notice 
that 0, 21 <??  such that they perform negative 
reinforcement. Ap
r
 and Bp
r
 are two bias vectors, 
with 1,0 21 << ??  as the damping factors. [ ]
1
1
?
=
n
A n
pr , where n is the order of MAA. Bp
r
 is 
defined in the same way. We will further define 
the affinity matrices in section 3.2 later. With the 
above reinforcement ranking equation, it is also 
true that 
1. A sentence in SB correlates to many new 
sentences in SB is supposed to receive a high 
ranking from RB, and 
2. A sentence in SB correlates to many old 
sentences in SA is supposed to receive a low 
ranking from RB. 
Let [ ]TBA RRR =  and [ ]TBA ppp rrr ??= 21 ?? , then 
the above iterative equation (1) corresponds to 
the linear system, 
( ) pRMI r=??                            (2) 
where, ??
?
??
?
=
BBBA
ABAA
MM
MM
M
22
11
??
??
. 
Up to now, the PNR2 is still query-independent. 
That means only the content of the sentences is 
considered. However, for the tasks of query-
oriented summarization, the reinforcement should 
obviously bias to the user?s query. In this work, 
we integrate query information into PNR2 by 
defining the vector pr  as ( )qsrelp ii |=r , where 
( )qsrel i |  denotes the relevance of the sentence si 
to the query q. 
To guarantee the solution of the linear system 
Equation (2), we make the following two 
transformations on M. First M is normalized by 
columns. If all the elements in a column are zero, 
we replace zero elements with n1  (n is the total 
number of the elements in that column). Second, 
M is multiplied by a decay factor ? ( 10 <<? ), 
such that each element in M is scaled down but 
the meaning of M will not be changed.  
Finally, Equation (2) is rewritten as, 
( ) pRMI r=??? ?                        (3) 
The matrix ( )MI ???  is a strictly diagonally 
dominant matrix now, and the solution of the 
linear system Equation (3) exists.  
3.2 Sentence Ranking based on PNR2 
We use the above mentioned PNR2 framework to 
rank the sentences in both SA and SB 
simultaneously. Section 3.2 defines the affinity 
matrices and presents the ranking algorithm. 
The affinity (i.e. similarity) between two 
sentences is measured by the cosine similarity of 
the corresponding two word vectors, i.e.  
[ ] ( )ji sssimjiM ,, =                     (4) 
where ( )
ji
ji
ji
ss
ss
sssim rr
rr
?
?
=,
. However, when 
calculating the affinity matrices MAA and MBB, the 
similarity of a sentence to itself is defined as 0, 
i.e. 
[ ] ( )
??
?
=
?
= ji
jisssimjiM ji
             0
,
,              (5) 
Furthermore, the relevance of a sentence to the 
query q is defined as 
( )
qs
qs
qsrel
i
i
i rr
rr
?
?
=,                     (6) 
Algorithm 1. RankSentence(SA, SB, q) 
Input: The old sentence set SA, the new 
sentence set SB, and the query q. 
Output: The ranking vectors R of SA and SB. 
1: Construct the affinity matrices, and set the 
weight matrix W; 
2: Construct the matrix ( )MIA ??= ? .  
3: Choose (randomly) the initial non-negative 
vectors TR ]11[)0( L= ; 
4: 0?k , 0?? ; 
5: Repeat 
6:     ( )? ?< >++ ??= ij ij kjijkjiji
ij
k
i RaRap
a
R )()1()1( 1 r ; 
7:     ( ))()1(max kk RR ??? + ; 
8:  )1( +kR is normalized such that the maximal 
element in )1( +kR is 1. 
493
9:     1+? kk ; 
10: Until ?<? 1; 
11: )(kRR ? ; 
12: Return. 
Now, we are ready to adopt the Gauss-Seidel 
method to solve the linear system Equation (3), 
and an iterative algorithm is developed to rank 
the sentences in SA and SB. 
After sentence ranking, the sentences in SB 
with higher ranking will be considered to be 
included in the final summary.  
3.3 Sentence Selection by Removing 
Redundancy 
When multiple documents are summarized, the 
problem of information redundancy is more 
severe than it is in single document 
summarization. Redundancy removal is a must. 
Since our focus is designing effective sentence 
ranking approach, we apply the following simple 
sentence selection algorithm. 
Algorithm 2. GenerateSummary(S, length) 
Input: sentence collection S (ranked in 
descending order of significance) and length 
(the given summary length limitation) 
Output: The generated summary ?  
{}?? ; 
?l length; 
For i ?  0 to |S| do 
     threshold ? ( )( )??ssssim i   ,max ; 
     If threshold <= 0.92 do 
          isU??? ; 
          ll ? - ( )islen ;  
          If ( l <= 0) break; 
     End 
End 
Return ? . 
 
4 Experimental Studies 
4.1 Data Set and Evaluation Metrics 
The experiments are set up on the DUC 2007 
update pilot task data set. Each collection of 
documents is accompanied with a query 
description representing a user?s information 
need. We simply focus on generating a summary 
for the document collection ?B? given that the 
                                                 
1
 ?  is a pre-defined small real number as the 
convergence threshold. 
2
 In fact, this is a tunable parameter in the algorithm. 
We use the value of 0.9 by our intuition. 
user has read the document collection ?A?, which 
is a typical update summarization task.  
Table 1 below shows the basic statistics of the 
DUC 2007 update data set. Stop-words in both 
documents and queries are removed 3  and the 
remaining words are stemmed by Porter 
Stemmer 4 . According to the task definition, 
system-generated summaries are strictly limited 
to 100 English words in length. We incrementally 
add into a summary the highest ranked sentence 
of concern if it doesn?t significantly repeat the 
information already included in the summary 
until the word limitation is reached. 
 A B 
Average number of documents 10 10 
Average number of sentences 237.6 177.3 
Table 1. Basic Statistics of DUC2007 Update Data Set 
As for the evaluation metric, it is difficult to 
come up with a universally accepted method that 
can measure the quality of machine-generated 
summaries accurately and effectively. Many 
literatures have addressed different methods for 
automatic evaluations other than human judges. 
Among them, ROUGE5 (Lin and Hovy, 2003) is 
supposed to produce the most reliable scores in 
correspondence with human evaluations. Given 
the fact that judgments by humans are time-
consuming and labor-intensive, and more 
important, ROUGE has been officially adopted 
for the DUC evaluations since 2005, like the 
other researchers, we also choose it as the 
evaluation criteria. 
In the following experiments, the sentences 
and the queries are all represented as the vectors 
of words. The relevance of a sentence to the 
query is calculated by cosine similarity. Notice 
that the word weights are normally measured by 
the document-level TF*IDF scheme in 
conventional vector space models. However, we 
believe that it is more reasonable to use the 
sentence-level inverse sentence frequency (ISF) 
rather than document-level IDF when dealing 
with sentence-level text processing. This has 
been verified in our early study. 
4.2 Comparison of Positive and Negative 
Reinforcement Ranking Strategy 
The aim of the following experiments is to 
investigate the different reinforcement ranking 
strategies. Three algorithms (i.e. PR(B), 
                                                 
3
 A list of 199 words is used to filter stop-words. 
4
 http://www.tartarus.org/~martin/PorterStemmer. 
5
 ROUGE version 1.5.5 is used. 
494
PR(A+B), PR(A+B/A)) are implemented as 
reference. These algorithms are all based on the 
query-sensitive LexRank (OtterBacher et al, 
2005). The differences are two-fold: (1) the 
document collection(s) used to build the text 
graph are different; and (2) after ranking, the 
sentence selection strategies are different. In 
particular, PR(B) only uses the sentences in ?B? 
to build the graph, and the other two consider the 
sentences in both ?A? and in ?B?. Only the 
sentences in ?B? are considered to be selected in 
PR(B) and PR(A+B/A), but all the sentences in 
?A? and ?B? have the same chance to be selected 
in PR(A+B). Only the sentences from B are 
considered to be selected in the final summaries 
in PNR2 as well. In the following experiments, 
the damping factor is set to 0.85 in the first three 
algorithms as the same in PageRank. The weight 
matrix W is set to ??
?
??
?
?
?
15.0
5.01
 in the proposed 
algorithm (i.e. PNR2) and 5.021 == ?? . We have 
obtained reasonable good results with the decay 
factor ?  between 0.3 and 0.8. So we set it to 0.5 
in this paper.  
Notice that the three PageRank-like graph-
based ranking algorithms can be viewed as only 
the positive reinforcement among the sentences is 
considered, while both positive and negative 
reinforcement are considered in PNR2 as 
mentioned before. Table 2 below shows the 
results of recall scores of ROUGE-1, ROUGE-2 
and ROUGE-SU4 along with their 95% 
confidential internals within square brackets.  
 ROUGE 
-1 
ROUGE 
-2 
ROUGE-
SU4 
PR(B) 0.3323 [0.3164,0.3501] 
0.0814 
[0.0670,0.0959] 
0.1165 
0.1053,0.1286] 
PR(A+B) 0.3059 [0.2841,0.3256] 
0.0746 
[0.0613,0.0893] 
0.1064 
[0.0938,0.1186] 
PR(A+B/A) 0.3376 [0.3186,0.3572] 
0.0865 
[0.0724,0.1007] 
0.1222 
[0.1104,0.1304] 
PNR2 0.3616 [0.3464,0.3756] 
0.0895 
[0.0810,0.0987] 
0.1291 
[0.1208,0.1384] 
Table 2. Experiment Results 
We come to the following three conclusions. 
First, it is not surprising that PR(B) and 
PR(A+B/A) outperform PR(A+B), because the 
update task obviously prefers the sentences from 
the new documents (i.e. ?B?). Second, 
PR(A+B/A) outperforms PR(B) because the 
sentences in ?A? can provide useful information 
in ranking the sentences in ?B?, although we do 
not select the sentences ranked high in ?A?. Third, 
PNR2 achieves the best performance. PNR2 is 
above PR(A+B/A) by 7.11% of ROUGE-1, 
3.47% of ROUGE-2, and 5.65% of ROUGE-SU4. 
This result confirms the idea and algorithm 
proposed in this work. 
4.3 Comparison with DUC 2007 Systems 
Twenty-four systems have been submitted to the 
DUC for evaluation in the 2007 update task. 
Table 3 compares our PNR2 with them. For 
reference, we present the following representative 
ROUGE results of (1) the best and worst 
participating system performance, and (2) the 
average ROUGE scores (i.e. AVG). We can then 
easily locate the positions of the proposed models 
among them. 
 PNR2 Mean Best / Worst 
ROUGE-1 0.3616 0.3262 0.3768/0.2621 
ROUGE2 0.0895 0.0745 0.1117/0.0365 
ROUGE-SU4 0.1291 0.1128 0.1430/0.0745 
Table 3. System Comparison 
4.4 Discussion 
In this work, we use the sentences in the same 
sentence set for positive reinforcement and 
sentences in the different set for negative 
reinforcement. Precisely, the old sentences 
perform negative reinforcement over the new 
sentences while the new sentences perform 
positive reinforcement over each other. This is 
reasonable although we may have a more 
comprehensive alternation. Old sentences may 
express old topics, but they may also express 
emerging new topics. Similarly, new sentences 
are supposed to express new topics, but they may 
also express the continuation of old topics. As a 
result, it will be more comprehensive to classify 
the whole sentences (both new sentences and old 
sentences together) into two categories, i.e. old 
topics oriented sentences and new topic oriented 
sentences, and then to apply these two sentence 
sets in the PNR2 framework. This will be further 
studied in our future work. 
Moreover, in the update summarization task, 
the summary length is restricted to about 100 
words. In this situation, we find that sentence 
simplification is even more important in our 
investigations. We will also work on this issue in 
our forthcoming studies. 
5 Conclusion 
In this paper, we propose a novel sentence 
ranking algorithm, namely PNR2, for update 
summarization. As our pilot study, we simply 
assume to receive two chronologically ordered 
document collections and evaluate the summaries 
495
generated for the collection given later. With 
PNR2, sentences from the new (i.e. late) 
document collection perform positive 
reinforcement among each other but they receive 
negative reinforcement from the sentences in the 
old (i.e. early) document collection. Positive and 
negative reinforcement are concerned 
simultaneously in the ranking process. As a result, 
PNR2 favors the sentences biased to the sentences 
that are important in the new collection and 
meanwhile novel to the sentences in the old 
collection. As a matter of fact, this positive and 
negative ranking scheme is general enough and 
can be used in many other situations, such as 
social network analysis etc. 
 
Acknowledgements 
The research work presented in this paper was 
partially supported by the grants from RGC of 
HKSAR (Project No: PolyU5217/07E), NSF of 
China (Project No: 60703008) and the Hong 
Kong Polytechnic University (Project No: A-
PA6L). 
 
References 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2004. G.T-Rank: Time-Aware Authority 
Ranking. In Algorithms and Models for the Web-
Graph: Third International Workshop, WAW, pp 
131-141. 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2005. Time-Aware Authority Ranking. 
Journal of Internet Mathematics, 2(3): 301-332. 
Klaus Lorenz Berberich. 2004. Time-aware and 
Trend-based Authority Ranking. Master Thesis, 
Saarlandes  University, Germany. 
Sergey Brin and Lawrence Page. 1998. The Anatomy 
of a Large-scale Hypertextual Web Search Engine. 
Computer Networks and ISDN Systems, 30(1-
7):107-117. 
Gunes Erkan and Dragomir R. Radev. 2004a. 
LexPageRank: Prestige in Multi-Document Text 
Summarization, in Proceedings of EMNLP, pp365-
371. 
Gunes Erkan and Dragomir R. Radev. 2004b. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization, Journal of Artificial 
Intelligence Research 22:457-479. 
Jon M. Kleinberg. 1999. Authoritative Sources in 
Hyperlinked Environment, Journal of the ACM, 
46(5):604-632. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of 
Document Semantic Graphs for Document 
Summarization, in Proceedings of LinkKDD 
Workshop, pp133-138. 
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Intra- 
and Inter-Event Relevance, in Proceedings of 
ACL/COLING, pp369-376. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics, in Proceedings of HLT-
NAACL, pp71-78. 
Ziheng Lin, Tat-Seng Chua, Min-Yen Kan, Wee Sun 
Lee, Long Qiu, and Shiren Ye. 2007. NUS at DUC 
2007: Using Evolutionary Models for Text. In 
Proceedings of Document Understanding 
Conference (DUC) 2007. 
Rada Mihalcea and Paul Tarau. 2004. TextRank - 
Bringing Order into Text, in Proceedings of 
EMNLP, pp404-411. 
Rada Mihalcea. 2004. Graph-based Ranking 
Algorithms for Sentence Extraction, Applied to 
Text Summarization, in Proceedings of ACL 
(Companion Volume). 
Jahna OtterBacher, Gunes Erkan, Dragomir R. Radev. 
2005. Using Random Walks for Question-focused 
Sentence Retrieval, in Proceedings of 
HLT/EMNLP, pp915-922. 
Lucy Vanderwende, Michele Banko and Arul 
Menezes. 2004. Event-Centric Summary 
Generation, in Working Notes of DUC 2004. 
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2006. 
Using Cross-Document Random Walks for Topic-
Focused Multi-Document Summarization, in 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
pp1012-1018. 
Xiaojun Wan. 2007. TimedTextRank: Adding the 
Temporal Dimension to Multi-document 
Summarization. In Proceedings of 30th ACM 
SIGIR, pp 867-868. 
Lei Yang, Lei Qi, Yan-Ping Zhao, Bin Gao, and Tie-
Yan Liu. 2007. Link Analysis using Time Series of 
Web Graphs. In Proceedings of CIKM?07. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information, in Working Notes of 
NTCIR-4. 
Philip S. Yu, Xin Li, and Bing Liu. 2004. On the 
Temporal Dimension of Search. In Proceedings of 
the 13th International World Wide Web Conference 
on Alternate Track Papers and Posters, pp 448-449. 
Philip S. Yu, Xin Li, and Bing Liu. 2005. Adding the 
Temporal Dimension to Search ? A Case Study in 
Publication Search. In Proceedings of the 2005 
IEEE/WIC/ACM International Conference on Web 
Intelligence. 
496
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985?992
Manchester, August 2008
Extractive Summarization Using Supervised and Semi-supervised 
Learning 
Kam-Fai Wong*, Mingli Wu*?  
*Department of Systems Engineering and 
Engineering Management  
The Chinese University of Hong Kong 
New Territories, Hong Kong 
{kfwong,mlwu}@se.cuhk.edu.hk
Wenjie Li? 
?Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
It is difficult to identify sentence impor-
tance from a single point of view. In this 
paper, we propose a learning-based ap-
proach to combine various sentence fea-
tures. They are categorized as surface, 
content, relevance and event features. 
Surface features are related to extrinsic 
aspects of a sentence. Content features 
measure a sentence based on content-
conveying words. Event features repre-
sent sentences by events they contained. 
Relevance features evaluate a sentence 
from its relatedness with other sentences. 
Experiments show that the combined fea-
tures improved summarization perform-
ance significantly. Although the evalua-
tion results are encouraging, supervised 
learning approach requires much labeled 
data. Therefore we investigate co-training 
by combining labeled and unlabeled data. 
Experiments show that this semi-
supervised learning approach achieves 
comparable performance to its supervised 
counterpart and saves about half of the 
labeling time cost. 
1 Introduction 
1 Automatic text summarization involves con-
densing a document or a document set to produce 
a human comprehensible summary. Two kinds of 
summarization approaches were suggested in the 
past, i.e., extractive (Radev et al, 2004; Li et al, 
2006) and abstractive summarization (Dejong, 
1978). The abstractive approaches typically need 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
to ?understand? and then paraphrase the salient 
concepts across documents. Due to the limita-
tions in natural language processing technology, 
abstractive approaches are restricted to specific 
domains. In contrast, extractive approaches 
commonly select sentences that contain the most 
significant concepts in the documents. These ap-
proaches tend to be more practical. 
Recently various effective sentence features 
have been proposed for extractive summarization, 
such as signature word, event and sentence rele-
vance. Although encouraging results have been 
reported, most of these features are investigated 
individually. We argue that it is ineffective to 
identify sentence importance from a single point 
of view. Each sentence feature has its unique 
contribution, and combing them would be advan-
tageous. Therefore we investigate combined sen-
tence features for extractive summarization. To 
determine weights of different features, we em-
ploy a supervised learning framework to identify 
how likely a sentence is important. Some re-
searchers explored learning based summarization, 
but the new emerging features are not concerned, 
such as event features (Li et. al, 2006). 
We investigate the effectiveness of different 
sentence features with supervised learning to de-
cide which sentences are important for summari-
zation. After feature vectors of sentences are ex-
amined, a supervised learning classifier is then 
employed.  Particularly, considering the length of 
final summaries is fixed, candidate sentences are 
re-ranked. Finally, the top sentences are ex-
tracted to compile the final summaries. Experi-
ments show that combined features improve 
summarization performance significantly. 
Our supervised learning approach generates 
promising results based on combined features. 
However, it requires much labeled data. As this 
procedure is time consuming and costly, we in-
vestigate semi-supervised learning to combine 
labeled data and unlabeled data. A semi-
985
supervised learning classifier is used instead of a 
supervised one in our extractive summarization 
framework. Two classifiers are co-trained itera-
tively to exploit unlabeled data. In each iteration 
step, the unlabeled training examples with top 
classifying confidence are included in the labeled 
training set, and the two classifiers are trained on 
the new training data. Experiments show that the 
performance of our semi-supervised learning 
approach is comparable to its supervised learning 
counterpart and it can reduce the labeling time 
cost by 50%. 
The remainder of this paper is organized as 
follows. Section 2 gives related work and Section 
3 describes our learning-based extractive summa-
rization framework. Section 4 outlines the vari-
ous sentence features and Section 5 describes 
supervised/semi-supervised learning approaches. 
Section 6 presents experiments and results. Fi-
nally, Section 7 concludes the paper. 
2 Related Work 
Traditionally, features for summarization were 
studied separately. Radev et al (2004) reported 
that position and length are useful surface fea-
tures. They observed that sentences located at the 
document head most likely contained important 
information. Recently, content features were also 
well studied, including centroid (Radev et al, 
2004), signature terms (Lin and Hovy, 2000) and 
high frequency words (Nenkova e t al., 2006). 
Radev et al (2004) defined centroid words as 
those whose average tf*idf score were higher 
than a threshold. Lin and Hovy (2000) identified 
signature terms that were strongly associated 
with documents based on statistics measures. 
Nenkova et al (2006) later reported that high 
frequency words were crucial in reflecting the 
focus of the document.  
Bag of words is somewhat loose and omits 
structural information. Document structure is 
another possible feature for summarization. Bar-
zilay and Elhadad (1997) constructed lexical 
chains and extracted strong chains in summaries. 
Marcu (1997) parsed documents as rhetorical 
trees and identified important sentences based on 
the trees. However, only moderate results were 
reported. On the other hand, Dejong (1978) rep-
resented documents using predefined templates. 
The procedure to create and fill the templates 
was time consuming and it was hard to adapt the 
method to different domains.  
Recently, semi-structure events (Filatovia and 
Hatzivassiloglou, 2004; Li et al, 2006; Wu, 2006) 
have been investigated by many researchers as 
they balanced document representation with 
words and structures. They defined events as 
verbs (or action nouns) plus the associated 
named entities. For instance, given the sentence 
?Yasser Arafat on Tuesday accused the United 
States of threatening to kill PLO officials?, they 
first identified ?accused?, ?threatening? and 
?kill? as event terms; and ?Yasser Arafat?, 
?United States?, ?PLO? and ?Tuesday? as event 
elements. Encouraging results based on events 
were reported for news stories.  
From another point of view, sentences in a 
document are somehow connected. Sentence 
relevance has been used as an alternative means 
to identify important sentences. Erkan and Radev 
(2004) and Yoshioka (2004) evaluate the rele-
vance (similarity) between any two sentences 
first. Then a web analysis approach, PageRank, 
was used to select important sentences from a 
sentence map built on relevance. Promising re-
sults were reported. However, the combination of 
these features is not well studied. Wu et al (2007) 
conducted preliminary research on this problem, 
but event features were not considered. 
Normally labeling procedure in supervised 
learning is very time consuming. Blum and 
Mitchell (1998) proposed co-training approach to 
exploit labeled and unlabeled data. Promising 
results were reported from their experiments on 
web page classification. A number of successful 
studies emerged thereafter for other natural lan-
guage processing tasks, such as text classification 
(Denis and Gilleron, 2003), noun phrase chunk-
ing (Pierce and Cardie, 2001), parsing (Sarkar, 
2001) and reference or relation resolution (Mul-
ler et al, 2001; Li et al, 2004). To our knowl-
edge, there is little research in the application of 
co-training techniques to extractive summariza-
tion. 
3 The Framework for Extractive Sum-
marization 
Extractive summarization can be regarded as a 
classification problem. Given the features of a 
sentence, a machine-learning based classification 
model will judge how likely the sentence is im-
portant. The classification model can be super-
vised or semi-supervised learning. Supervised 
approaches normally perform better, but require 
more labeled training data. SVMs perform well 
in many classification problems. Thus we em-
ploy it for supervised learning. For semi-
supervised learning, we co-trained a probabilistic 
986
SVM and a Na?ve Bayesian classifier to exploit 
unlabeled data. 
 
Figure 1. Learning-based Extractive Summariza-
tion Framework 
The automatic summarization procedure is 
shown in Figure 1. First, each input sentence is 
examined by going through the pre-specified fea-
ture functions. The classification model will then 
predict the importance of each sentence accord-
ing to its feature values. A re-ranking algorithm 
is then used to revise the order. Finally, the top 
sentences are included in the summaries until the 
length limitation is reached. The re-ranking algo-
rithm is crucial, as more important content are 
expected to be contained in the final summary 
with fixed length. Important sentences above a 
threshold are regarded as candidates. The one 
with less words and located at the beginning part 
of a document is ranked first. The re-ranking al-
gorithm is described as follows. 
Ranki = RankPosi + RankLengthi  
where RankPosi is the rank of sentence i accord-
ing to its position in a document (i.e. the sentence 
no.) and RankLengthi is rank of sentence i ac-
cording to its length. 
4 Sentence Features for Extractive 
Summarization 
This section provides a detailed description on 
the four types of sentence features, i.e., surface, 
content, event and relevance features, which will 
be examined systematically. 
4.1 Surface Features 
Surface features are based on structure of 
documents or sentences, including sentence 
position in the document, the number of words in 
the sentence, and the number of quoted words in 
the sentence (see Table 1).  
 
Name Description 
Position 1/sentence no. 
Doc_First Whether it is the first sentence of a document  
Para_First Whether it is the first sentence of a paragraph 
Length The number of words in a sentence 
Quote The number of quoted words in a sen-tence  
Table 1. Types of surface features 
 
The intuition with respect to the importance of 
a sentence stems from the following observations: 
(1) the first sentence in a document or a para-
graph is important; (2) the sentences in the ear-
lier parts of a document is more important than 
sentences in later parts; (3) a sentence is impor-
tant if the number of words (except stop words) 
in it is within a certain range; (4) a sentence con-
taining too many quoted words is unimportant.  
4.2 Content Features 
We integrate three well-known sentence features 
based on content-bearing words i.e., centroid 
words, signature terms, and high frequency 
words. Both unigram and bigram representations 
have been investigated. Table 2 summarizes the 
six content features we studied.  
 
Name Description 
Centroid_Uni The sum of  the weights of cen-troid uni-gram  
Centroid_Bi The sum of  the weights of cen-troid bi-grams  
SigTerm_Uni The number of signature uni-grams  
SigTerm_Bi The number of signature bi-grams 
FreqWord_Uni The sum of  the weights of fre-quent uni-grams  
FreqWord_Bi The sum of  the weights of fre-quent bi-grams  
Table 2. Types of content features 
4.3 Event Features 
An event is comprised of an event term and asso-
ciated event elements. In this study, we choose 
verbs (such as ?elect and incorporate?) and ac-
tion nouns (such as ?election and incorporation?) 
as event terms that can characterize actions. They 
relate to ?did what?. One or more associated 
named entities are considered as event elements. 
Four types of named entities are currently under 
987
consideration. The GATE system (Cunningham 
et al, 2002) is used to tag named entities, which 
are categorized as <Person>, <Organization>, 
<Location> and <Date>. They convey the infor-
mation about ?who?, ?whom?, ?when? and 
?where?. A verb or an action noun is deemed an 
event term only when it appears at least once 
between two named entities. 
Event summarization approaches based on in-
stances or concepts are investigated. An occur-
rence of an event term (or event element) in a 
document is considered as an instance, while the 
collection of the same event terms (or event ele-
ments) is considered as a concept. Given a 
document set, instances of event terms and event 
elements are identified first. An event map is 
then built based on event instances or concepts 
(Wu , 2006; Li et al, 2006). PageRank algorithm 
is used to assign weight to each node (an instance 
or concept) in the event map. The final weight of 
a sentence is the sum of weights of event in-
stances contained in the sentence. 
4.4 Relevance Features 
Relevance features are incorporated to exploit 
inter-sentence relationships. It is assumed that: (1) 
sentences related to important sentences are im-
portant; (2) sentences related to many other sen-
tences are important. The first sentence in a 
document or a paragraph is important, and other 
sentences in a document are compared with the 
leading ones. Two types of sentence relevance, 
FirstRel_Doc and FirstRel_Para (see Table 3), 
are measured by comparing pairs of sentences 
using word-based cosine similarity. 
Another way to exploit sentence relevance is 
to build a sentence map. Every two sentences are 
regarded relevant if their similarity is above a 
threshold. Every two relevant sentences are con-
nected with a unidirectional link. Based on this 
map, PageRank algorithm is applied to evaluate 
the importance of a sentence. These relevance 
features are shown in Table 3.  
 
Name Description 
FirstRel_Doc Similarity with the first sentence in the document  
FirstRel_Para Similarity with the first sentence in the paragraph  
PageRankRel PageRank value of the sentence based on the sentence map  
 
Table 3. Types of relevance features 
5 Supervised/Semi-supervised Learning 
Approaches  
To incorporate features described in Section 4, 
we investigate supervised and semi-supervised 
learning approaches. Probabilistic Support Vec-
tor Machine (PSVM)  is employed as supervised 
learning (Wu et al, 2004), while the co-training 
of PSVM and Na?ve Bayesian Classifier (NBC) 
is used for semi-supervised learning. The two 
learning-based classification approaches, PSVM 
and NBC, are described in following sections. 
5.1 Probabilistic Support Vector Machine 
(PSVM) 
For a set of training examples ( ix , iy ), 
li ,...,1= , where ix  is an instance and iy  the 
corresponding label, basic SVM requires the so-
lution of the following optimization problem. 
?
=
+
l
i
i
T
bw
Cww
1
,, 2
1  min ??  
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
 
Here the SVM classifier is expected to find a 
hyper-plane to separate testing examples as posi-
tive and negative. Wu et al (2004) extend the 
basic SVM to a probabilistic version. Its goal is 
to estimate  
 
kixiyppi ,...1 ),|( === . 
First the pairwise (one-against-one) probabilities 
) ,or  |( xjiyiyprij ==?  is estimated using 
BAfij e
r ++? 1
1
 
where A and B are estimated by minimizing the 
negative log-likelihood function using training 
data and their decision values f. Then ip  is ob-
tained by solving the following optimization 
problem 
? ?
= ?
?
k
i ijj
jijijip
prpr
1 :
2)(
2
1  min  
 
subject to    
0
1  ))(( ,
?
??+
i
ii
T
i bxwy
?
??
 
The problem can be reformulated as  
QPP T
P 2
1  min  
988
where      ??
??
?
??
== ? ?
ji if       
ji if  
Q
2
:
ij
ijji
siiss
rr
r
 
The problem is convex and the optimality condi-
tions a scalar b such that   
??
???
?=??
???
???
???
?
1
z
  
b
P
 
0Te
eQ
 
where e is the vector of all 1s and z is the vector 
of all 0s, and b is the Lagrangian multiplier of the 
equality constraint ?
=
=
k
i
ip
1
1 . 
5.2 Na?ve Bayesian Classier (NBC) 
Na?ve Bayesian Classier assumes features are 
independent. It learns prior probability and con-
ditional probability of each feature, and predicts 
the class label by highest posterior probability. 
Given a feature vector (F1, F2, F3,?, Fn), the 
classifier need to decide the label c: 
 
),...,,|(maxarg 321 n
c
FFFFcPc =  
 
By applying Bayesian rule, we have  
 
),...,,,(
)|,...,,,()(
),...,,,|(
321
321
321
n
n
n FFFFP
cFFFFPcP
FFFFcP =
 
Since the denominator does not depend on c and 
the values of Fi are given, therefore the denomi-
nator is a constant and we are only interested in 
the numerator. As features are assumed inde-
pendent,  
 
?
=
?
=
n
i
i
nn
cFPcP
cFFFFPcPFFFFcP
1
321321
)|()(
)|,...,,()(),...,,|(
 
 
where )|( cFP i is estimated with MLE from 
training data with Laplace Smoothing. 
5.3 Co-Training (COT) 
Supervised learning approaches require much 
labeled data and the labeling procedure is very 
time-consuming. Literature (Blum and Mitchell, 
1998; Collins, 1999) has suggested that unla-
beled data can be exploited together with labeled 
data by co-training two classifiers. (Blum and 
Mitchell, 1998) trained two classifiers of same 
type on different features, and (Li et al, 2004) 
trained two classifiers of different types. In this 
paper, as the number of involved features is not 
too many, we train two different classifiers, 
PSVM and NBC, on the same feature spaces. 
The co-training algorithm is described as follows. 
 
Given: 
L is the set of labeled training examples 
U is the set of unlabeled training examples 
Loop: until the unlabeled data is exhausted 
Train the first classifier C1 (PSVM) on L 
Train the second classifier C2 (NBC) on L 
For each classifier Ci 
Ci labels examples from U 
Ci chooses p positive and n negative ex-
amples E from U. These examples have 
top classifying confidence. 
Ci removes examples E from U 
Ci adds examples E with the correspond-
ing labels to L 
End 
Output: label the test examples by the optimal 
classifier which is evaluated on training data ac-
cording to the classification performance. 
6 Experiments 
DUC 20012 has been used in our experiments. It 
contains 30 clusters of relevant documents and 
308 documents in total. Each cluster deals with a 
specific topic (e.g. a hurricane) and comes with 
model summaries created by NIST assessors. 50, 
100, 200 and 400 word summaries are provided. 
Twenty-five of the thirty document clusters are 
used as training data and the remaining five are 
used as testing. The training/testing configuration 
is same in experiments of supervised learning 
and semi-supervised learning, while the differ-
ence is that some sentences in training data are 
not tagged for semi-supervised learning. 
An automatic evaluation package, i.e., 
ROUGE (Lin and Hovy, 2003) is employed to 
evaluate the summarization performance. It 
compares machine-generated summaries with 
model summaries based on the overlap. Precision 
and recall measures are used to evaluate the clas-
sification performance. For comparison, we 
evaluate our approaches on DUC 2004 data set 
also. It contains 50 clusters of documents. Only 
665-character summaries are given by assessors 
for each cluster. 
6.1 Experiments on Supervised Learning 
Approach 
We use LibSVM3 as our classification model for 
SVM classifiers normally perform better. Types 
of features presented in previous section are 
evaluated individually first. Precision measures 
                                                 
2 http://duc.nist.gov/ 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
989
the percentage of true important sentences 
among all important sentences labeled by the 
classifier. Recall measures the percentage of true 
important sentences labeled by the classifier 
among all true important sentences.  
Table 4 shows the precisions and recalls of 
different feature groups under the PSVM classi-
fier. Table 5 records the ROUGE evaluation re-
sults ? ROUGE-1, ROUGE-2 and ROUGE-L. 
They evaluate the overlap between machine-
generated summaries and model summaries 
based on unigram, bigram and long distance re-
spectively. The summary length is limited to 200 
words here.  
 
Feature Precision Recall 
Sur 0.488 0.146 
Con 0.407 0.167 
Rel 0.488 0.146 
Event 0.344 0.146 
Sur+Con 0.575 0.160 
Sur+Rel 0.488 0.146 
Con+Rel 0.588 0.139 
Sur+Event 0.600 0.125 
Con+Event 0.384 0.194 
Rel+Event 0.543 0.132 
Sur+Con+Event 0.595 0.153 
Sur+Rel+Event 0.553 0.146 
Con+Rel+Event 0.581 0.125 
Sur+Con+Rel 0.595 0.174 
Sur+Con+Rel+Event 0.579 0.153 
Table 4. Classification performance based on 
different feature groups 
 
 
Feature Rouge-1 
Rouge-
2 
Rouge-
L 
Sur 0.373 0.103 0.356 
Con 0.352 0.074 0.334 
Rel 0.373 0.103 0.356 
Event 0.344 0.064 0.325 
Sur+Con 0.380 0.109 0.363 
Sur+Rel 0.373 0.103 0.356 
Con+Rel 0.375 0.103 0.358 
Sur+Event 0.348 0.091 0.332 
Con+Event 0.344 0.071 0.330 
Rel+Event 0.349 0.089 0.356 
Sur+Con+Event 0.379 0.106 0.363 
Sur+Rel+Event 0.371 0.101 0.353 
Sur+Con+Rel 0.396 0.116 0.358 
Sur+Con+Rel+Event 0.375 0.106 0.359 
Table 5. ROUGE evaluation results for differ-
ent feature groups 
From Table 4, we can see the most useful fea-
ture groups are ?surface? and ?relevance?, i.e. 
the external characteristics of a sentence in the 
document and the relationships of a sentence 
with other sentences in a cluster. The evaluation 
scores from surface features and relevance fea-
tures are the same. We found that the reason is 
that the dominating feature in each feature group 
is about whether a sentence is the first sentence 
in a document. The influence of event features is 
not very positive. Based on our analysis the rea-
son is that not all clusters contain enough event 
terms/elements to build a good event map. 
From Table 5, it can be seen that the combina-
tion of multiple features or multiple feature 
groups outperforms individual feature or feature 
groups. When surface, content and relevance fea-
tures are employed, the best performance is 
achieved, i.e., ROUGE-1 and ROUGE-2 score 
are 0.396 and 0.116 respectively. In our prelimi-
nary experiments, we find ROUGE-1 score of a 
model summary is 0.422 (without stemming and 
filtering stop words). Therefore summaries gen-
erated by our supervised learning approach re-
ceived comparable performance with model 
summaries when evaluated by ROUGE. Al-
though ROUGE is not perfect at this time, it is 
automatic and good complement to subjective 
evaluations.  
 We also find that the Rouge scores are similar 
for variations on the feature set. Sentences from 
original documents are selected to build the final 
summaries. Normally, only four to six sentences 
are contained in one 200-word summary in our 
experiments, i.e., few sentences will be kept in a 
summary. As variations of the feature set only 
induce little change of the order of most impor-
tant sentences, the ROUGE scores change little. 
6.2 Experiments on Semi-supervised Learn-
ing Approach 
Supervised learning approaches normally 
achieve good performance but require manually 
labeled data. Recent literature (Blum and 
Mitchell, 1998; Collins, 199) has suggested that 
co-training techniques reduce the amount of la-
beled data. They trained two homogeneous clas-
sifiers based on different feature spaces. How-
ever this method is unsuitable for our application 
as the number of required features in our case is 
not too many. Therefore we develop a co-
training approach to train different classifiers 
based on same feature space. PSVM and NBC 
are applied to the combination of surface, content 
and relevance features. 
The capability of different learning approaches 
to identify important sentences is shown in Fig-
990
ure 2. The ?x? axis shows the number of labeled 
sentences employed. The remained training sen-
tences in DUC 2001 are employed as unlabeled 
training data. The y axis shows f-measures of 
important sentences identified from the test set. 
The size of the training seed set is investigated. 
For each size, three different seed sets which are 
chose randomly are used. The average evaluation 
scores are used as the final performance. This 
procedure avoids the variance of the final evalua-
tion results. The ROUGE evaluation results of 
these supervised learning approaches and semi-
supervised learning approaches are shown in Ta-
ble 6 (2000 labeled sentences). It can be seen that 
the ROUGE performance of co-trained classifiers 
is better than that of individual classifiers. 
0
0.1
0.2
0.3
0.4
50 100 200 500 1000 2000
 Number of Labeled Sentences
F
-M
ea
su
re Cotrain
Bayes
Svm
 
Figure 2. Performance of supervised learning 
and semi-supervised learning approaches 
 
Learning  
Approaches Rouge-1 Rouge-2 Rouge-L
PSVM 0.358 0.082 0.323 
NBC 0.353 0.061 0.317 
COT 0.366 0.090 0.329 
Table 6. ROUGE evaluation results of supervised 
learning and semi-supervised learning 
6.3 Experiments on Summary Length 
In DUC 2001 dataset, 50, 100, 200 and 400-word 
summaries are provided to evaluate summaries 
with different length. Our supervised approach, 
which generates the best performance in previous 
experiments, is employed. The ROUGE scores of 
evaluations on different summary length are 
shown in Table 7. Our summaries consist of ex-
tracted sentences. It can be seen that these sum-
maries achieve lower ROUGE scores when the 
length of summary is reduced. The reason is that 
when people try to write a more concise sum-
mary, condensed contents are included in the 
summaries, which may not use the original con-
tents directly. Therefore the word-overlapping 
test tool in ROUGE generates lower scores.  
We then tested the same classifier and same 
features on DUC 2004. The length of summaries 
is only 665 characters (about 100 words). 
ROUGE-1 and ROUGE-2 are 0.329 and 0.073 
respectively. It confirms that the performance of 
our approach is sensitive to the length of the 
summary.  
Sum_length Rouge-1 Rouge-2 Rouge-L
50 0.241 0.036 0.205 
100 0.309 0.085 0.277 
200 0.396 0.116 0.358 
400 0.423 0.118 0.402 
Table 7. ROUGE evaluation results for differ-
ent summary length 
7 Conclusions and Future Work 
We explore surface, content, event, relevance 
features and their combinations for extractive 
summarization with supervised learning ap-
proach. Experiments show that the combination 
of surface, content and relevance features per-
form best. The highest ROUGE-1, ROUGE-2 
scores are 0.396 and 0.116 respectively. The 
Rouge-1 score of manually generated summaries 
is 0.422. This shows the ROUGE performance of 
our supervised learning approach is comparable 
to that of manually generated summaries. The 
ROUGE-1 scores of extractive summarization 
based on centroid, signature word, high fre-
quency word and event individually are 0.319, 
0.356, 0.371 and 0.374 respectively. It can be 
seen that our summarization approach based on 
combination of features improves the perform-
ance obviously.  
Although the results of supervised learning 
approach are encouraging, it required much la-
beled data. To reduce labeling cost, we apply co-
training to combine labeled and unlabeled data. 
Experiments show that compare with supervised 
learning, semi-supervised learning approach 
saves half of the labeling cost and maintains 
comparable performance (0.366 vs 0.396). We 
also find that our extractive summarization is 
sensitive to length of the summary. When the 
length is extended, the ROUGE scores of same 
summarization method are improved. In the fu-
ture, we plan to investigate sentence compression 
to improve performance of our summarization 
approaches on short summaries. 
Acknowledgement 
The research described in this paper is partially 
supported by Research Grants Council of Hong 
Kong (RGC: PolyU5217/07E), CUHK Strategic 
Grant Scheme (No: 4410001) and Direct Grant 
Scheme (No: 2050417).  
991
References 
Regina Barzilay, and Michael Elhadad. 1997. Using 
lexical chains for text summarization. In Proceed-
ings of the 35th Annual Meeting of the Association 
for Computational Linguistics Workshop on Intel-
ligent Scalable Text Summarization, pages 10-17. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of the 11th Annual Conference on 
Computational Learning Theory, pages 92-100. 
Hamish Cunningham, Diana Maynard, Kalina 
Bontcheva, Valentin Tablan. 2002. GATE: a 
framework and graphical development environ-
ment for robust NLP tools and applications. In 
Proceedings of the 40th Annual Meeting of the As-
sociation for computational Linguistics. 
Francois Denis and Remi Gilleron. 2003. Text classi-
fication and co-training from positive and unla-
beled examples. In Proceedings of the 20th Inter-
national Conference on Machine Learning Work-
shop: the Continuum from Labeled Data to Unla-
beled Data in Machine Learning and Data Mining. 
Gunes Erkan and Dragomir R. Radev. 2004. LexPag-
eRank: prestige in multi-document text summariza-
tion. In Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 365-371. 
Elena Filatova and Vasileios Hatzivassiloglou. Event-
based extractive summarization. 2004. In Proceed-
ings of the 42nd Annual Meeting of the Association 
for Computational Linguistics Workshop, pages 
104-111. 
Gerald Francis DeJong. 1978. Fast skimming of news 
stories: the FRUMP system. Ph.D. thesis, Yale 
University. 
Wenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa 
Yuan. 2004. Applying machine learning to Chinese 
temporal relation resolution. In Proceedings of the 
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 583-589. 
Wenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu. 
2006. Extractive summarization using inter- and in-
tra- event relevance. In proceedings of Proceedings 
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics, pages 
369-376. 
Chin-Yew Lin; Eduard Hovy. 2000. The automated 
acquisition of topic signatures for text summariza-
tion. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495-
501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003 
Human Language Technology Conference of the 
North American Chapter of the Association for 
Computational Linguistics, Edmonton, Canada. 
Daniel Marcu. 1997. The rhetorical parsing of natural 
language texts. In Proceedings of the 35th Annual 
Meeting of the Association for computational Lin-
guistics, pages 96-103. 
Christoph Muller, Stefan Rapp and Michael Strube. 
2001. Applying co-training to reference resolution. 
In Proceedings of the 40th Annual Meeting on As-
sociation for Computational Linguistics.  
Ani Nenkova, Lucy Vanderwende and Kathleen 
McKeown. 2006. A compositional context sensi-
tive multi-document summarizer: exploring the 
factors that influence summarization. In Proceed-
ings of the 29th Annual International ACM SIGIR 
Conference on Research and Development in In-
formation Retrieval. 
David Pierce and Claire Cardie. 2001. Limitations of 
co-training for natural language learning from large 
datasets. In Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language Process-
ing, pages 1-9. 
Dragomir R. Radev, Timothy Allison, et al 2004. 
MEAD - a platform for multidocument multilin-
gual text summarization. In Proceedings of 4th In-
ternational Conference on Language Resources 
and Evaluation. 
Anoop Sarkar. 2001. Applying co-training methods to 
statistical parsing. In Proceedings of 2nd Meeting 
of the North American Chapter of the Association 
for Computational Linguistics on Language Tech-
nologies. 
Mingli Wu. 2006. Investigations on event-based 
summarization. In proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the Association for 
Computational Linguistics Student Research Work-
shop, pages 37-42. 
Mingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-
Fai Wong. 2007. Exploiting surface, content and 
relevance features for learning-based extractive 
summarization. In Proceedings of 2007 IEEE In-
ternational Conference on Natural Language 
Processing and Knowledge Engineering.  
Ting-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004. 
Probability estimates for multi-class classification 
by pairwise coupling. Journal of Machine Learning 
Research, 5:975-1005.  
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple news articles summarization based on 
event reference information. In Working Notes of 
the Fourth NTCIR Workshop Meeting, National In-
stitute of Informatics. 
992
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 414 ? 425, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
A Preliminary Work on Classifying Time Granularities 
of Temporal Questions 
Wei Li1, Wenjie Li1, Qin Lu1, and Kam-Fai Wong2  
1
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{cswli, cswjli, csluqin}@comp.polyu.edu.hk 
2
 Department of Systems Engineering, the Chinese University of Hong Kong, 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
Abstract. Temporal question classification assigns time granularities to tempo-
ral questions ac-cording to their anticipated answers. It is very important for an-
swer extraction and verification in the literature of temporal question answer-
ing. Other than simply distinguishing between "date" and "period", a more fine-
grained classification hierarchy scaling down from "millions of years" to "sec-
ond" is proposed in this paper. Based on it, a SNoW-based classifier, combining 
user preference, word N-grams, granularity of time expressions, special patterns 
as well as event types, is built to choose appropriate time granularities for the 
ambiguous temporal questions, such as When- and How long-like questions. 
Evaluation on 194 such questions achieves 83.5% accuracy, almost close to 
manually tagging accuracy 86.2%. Experiments reveal that user preferences 
make significant contributions to time granularity classification. 
1   Introduction 
Temporal questions, such as the questions with the interrogatives ?when?, ?how long? 
and ?which year?, seek for the occurrence time of the events or the temporal attributes 
of the entities. Temporal question classification plays an important role in the litera-
ture of question answering and temporal information processing. In the evaluation of 
TREC 10 Question-Answering (QA) track [1], more than 10% of questions in the test 
question corpus are temporal questions. Different from TREC QA track, Workshop 
TERQAS (http://www.timeml.org/terqas/) particularly investigated on temporal ques-
tion answering instead of a general one. It focused on temporal and event recognition 
in question answering systems and paid great attention to temporal relations among 
states, events and time expressions in temporal questions. TimeML (http://www.ti-
meml.org), a temporal information (e.g. time expression, tense & aspect) annotation 
standard, has also been used for temporal question answering in this workshop [2]. 
Correct understanding of a temporal question will greatly help extracting and verify-
ing its answers and certainly improve the performance of any question answering 
system. Look at the following examples. 
[Ea]. What is the birthday of Abraham Lincoln? 
[Eb]. When did the Neanderthal man live? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 415 
In a general question answering system, the question classifier commonly classifies 
temporal questions into two classes, i.e. ?date? and ?period?. With such a system, the 
above two questions are both assigned a ?date?. Whereas it is natural for the question 
[Ea] to be answered with a particular data (e.g. ?12/02/1809?), it is not the case for 
question [Eb], because a proper answer could be ?35,000 years ago?. However, if it is 
known that the time granularity concerned is ?thousands of years?, answer extraction 
turn to be more targeted. The need for a more fine-grained classification is obvious. 
Although there were different question classification hierarchies, as reported 
[3,4,12,13,14], few inclined to introducing the classification hierarchy (e.g. ?year?, 
?month? and ?day?) which could give a clearer direction to guide answer extraction 
and verification of temporal questions. In the following, we try to find out whether 
temporal questions can be further classified into finer time granularity and how to 
classify them. 
By examining a temporal question corpus consisting of 348 questions, 293 of 
which are gathered from UIUC question answering labelled data (http://l2r.cs. 
uiuc.edu/~cogcomp/Data/QA/QC), and the rest 55 from TREC 10 test corpus, we find 
two different cases. On the one hand, some questions are very straightforward in ex-
pressing the time granularities of the answers expected, e.g. the questions beginning 
with ?which year? or ?for how many years?. On the other hand, some questions are 
not so obvious, e.g. the questions headed by ?when? or ?for how long?. We call such 
questions ambiguous questions. Not surprisingly, the ambiguous When- and How 
long-like questions account for a large proportion in this temporal question corpus, 
i.e. 197 from 348 in total. 
We further investigate on those 197 ambiguous questions in order to find out 
whether they can be classified into finer time granularity. Three experimenters are 
requested to tag a time granularity to each question independently1. Answers are not 
provided. The tag with two agreements is taken as the time granularity class of the 
corresponding temporal question. Otherwise the tag ?UNKNOWN? is assigned. Ref-
erence answers for the questions are extracted from AltaVista Web Search 
(http://www.altavista.com). Comparing the time granularities tagged manually with 
those provided by the reference answers, we find that only 27 out of 197 questions are 
incorrectly tagged, in other words, the manually tagging accuracy is 86.2%. Errors 
exist though, the relatively high agreement between users? tagging and reference 
answers lights the hope of automatically determining the time granularities of tempo-
ral questions. 
Analysing the tagging results, it is revealed that the tagging errors arouse from 
three sources: insufficient world knowledge, different speaking habits and different 
expected information granularity among human. See the following examples: 
[Ec]. When did the Neanderthal man live?   
User: year; Ref.: thousands of years 
[Ed]. How long is human gestation?  
User: month; Ref.: week 
[Ee]. When was the first Wall Street Journal published?   
User: year; Ref.: day 
                                                          
1
  The granularity hierarchy and the tagging principle will be detailed later. 
416 W. Li et al 
For question [Ec], the time granularity should be ?thousands of years?, rather than 
?year?. This error could be corrected if one knows that Neanderthal man existed 
35,000 years ago. The time granularity of question [Ed] should be ?week?, but not 
?month? in accordance with the habit. For question [Ee], users? tag is ?year?, different 
from the reference answer?s tag ?day?. However, both granularities are acceptable in 
commonsense, because the different users may want coarser or finer information. This 
observation suggests that incorporating question context, world knowledge, and 
speaking habits would help determine the time granularities of temporal questions. 
In this paper, we propose a fine-grained temporal question classification scheme, 
i.e. time granularity hierarchy, consisting of sixteen non-exclusive classes and scaling 
down from ?millions of years? to ?second?. The SNoW-based classifier is then built 
to combine linguistic features (including word N-grams, granularity of time expres-
sions and special patterns), user preferences and event types, and assign one of the 
sixteen classes to each temporal question. In our work, user preference, which charac-
terizes world knowledge and speaking habits, is estimated by means of the time 
granularities of the entities and/or events involved. The SNoW-based classifier 
achieves 83.5% accuracy, almost close to 86.2% of manually tagging accuracy. Ex-
periments also show that user preference makes a great contribution to time granular-
ity classification. 
The rest of this paper is organized as follows. In the next section various related 
works in this literature are introduced. In Sect. 3, we demonstrate the time granularity 
hierarchy and principles. User preference is fully investigated in Sect. 4. Feature de-
sign is depicted in Sect. 5. Time granularity classifiers are introduced in Sect. 6 and 
the experiment results are presented in Sect. 7. We finally conclude this paper in the 
last section. 
2   Related Works 
In TREC QA track, almost every QA system joining in the evaluation has a question 
classification module. This makes question classification a hot topic. Questions can be 
classified from several aspects. Most classification hierarchies [3,4,12,13,14] adopt the 
anticipated answer types as its classification criteria. Abney et al [4] gave a coarse 
classification hierarchy with seven classes (person, location, etc.). Hovy et al [13] 
introduced a finer classification with forty-seven classes manually constructed from 
17,000 practical questions. Li et al [3] proposed a two-level classification hierarchy, a 
coarser one with six classes and a finer one with fifty classes. In all these classification 
hierarchies, temporal questions are simply classified into two classes, i.e. ?date? and 
?period?. Some works classified temporal questions from other aspects. In [2], a tem-
poral question classification hierarchy is proposed according to the temporal relation 
among state, event and time expression. In [5], temporal questions are classified into 
three types with regard to question structure: non-temporal, simple and complex. Diaz 
F. et al [6] did an interesting work on the statistics of the number of topics along time-
line. According to whether questions or topics have a clear distribution along timeline, 
they can be classified into three types: atemporal, temporal clear and temporal ambigu-
ous.  Focusing on ambiguous temporal questions, e.g. when and how long-like ques-
tions, we introduce a classification hierarchy in terms of the anticipated answer types. 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 417 
It is an extension of two classes ?date? and ?period? and includes sixteen non-
exclusive classes scaling down from ?millions of years? to ?second?. 
Related to the work of features design, Li et al [3] built the question classifier 
based on three types of features, including surface text (e.g. N-grams), syntactic fea-
tures (e.g. part-of-speech and name entity tags), and semantic related words (words 
that often occur with a specific question class). Later works of Li et al [10] intro-
duced semantic information and world knowledge from external resources such as 
WordNet. In this paper, we introduce a new feature, user preference, which is ex-
pected to imply the world knowledge in time granularity in the experiment. User 
preference is estimated from statistics with which Diaz F. et al [6] determine whether 
a question is temporal ambiguous or not. E. Saquete et al [5] suggested that questions 
had different structures, i.e. non-temporal, simple and complex, which is helpful to 
handle questions more orderly. It gives us inspiration to use question focus, i.e. 
whether a question is event-based or entity-based.  
Many machine-learning methods have been used in question classification, such 
as language model [7], SNoW [3,10], maximum entropy [15] and support vector ma-
chine [8,9]. In our experiments, language model is selected as the baseline model, and 
SNoW is selected to tackle to the large feature space and build the classifier. In fact, 
SNoW has already been used in many other fields, such as text categorization, word 
sense disambiguation and even facial feature detection. 
3   Time Granularity Hierarchy and Tagging Principles 
In traditional question answering systems, only two question types are time-related, i.e. 
?date? and ?period?. For the reasons explained in Sect. 1, we propose a more detailed 
temporal question classification scheme, namely time granularity hierarchy scaling 
down from ?millions of years? to ?second? in order to facilitate answer extraction and 
verification. The initial time granularity hierarchy includes the following twelve 
classes: ?second?, ?minute?, ?hour?, ?day?, ?week?, ?month?, ?season?, ?year?, ?dec-
ade?, ?century?,  ?thousands of years? and ?millions of years?.  
Granularity ?weekday? is added to the initial hierarchy because some temporal 
questions favor ?weekday? instead of ?day?, although both of them indicate one day. 
Some questions favour a region of time granularity. Look at the following examples. 
[Ef]. What time of year has the most air travel? 
[Eg]. What time of day did Emperor Hirohito die? 
For [Ef] question, its time granularity could be ?season?, ?month? or even ?day?; and 
for question [Eg], the time granularity could be ?hour? or ?minute?. We can only 
determine that their time granularities are less than ?year? or ?day? respectively, but 
cannot go any further. Such situations only occur to time granularity ?year? and 
?day?, so we expand the original classification hierarchy by adding another two types: 
?less than day?, ?less than year?. Besides, the questions asking for festivals are classi-
fied into ?special date?.  
Up to now, the time granularity hierarchy has sixteen classes. The less frequent 
temporal measures, such as ?microsecond? and ?billions of years? are ignored. As 
mentioned above, the class ?less than day? overlaps several granularities, e.g. ?hour? 
and ?minute?, so the time granularity hierarchy we proposed is non-exclusive.  
418 W. Li et al 
In reality, some temporal questions can be answered in several different time 
granularities. For example, question ?when was Abraham Lincoln born??,  its answers 
can be a ?day? (?12/02/1809?) or a ?year? (?1809?). To resolve this confliction, we 
adopt two principles for time granularity annotation.  
[Pa]. Assign the minimum time granularity we can determine to a given temporal 
question if several time granularities are applicable. 
[Pb]. Select the time granularity with regard to speaking habits or user preferences.  
When the two principles conflict to each other, principle [Pb] takes the priority. With 
principle [Pa], time granularity of the above question can only be ?day?. 
4   User Preference 
In general, temporal questions have two different focuses: entity-based and event-based. 
[a]. Entity-based question: temporal interrogative words + (be) + entity, e.g. 
?When was the World War II?? 
[b]. Event-based question: temporal interrogatives + event, e.g. ?When did 
Mount St. Helen last have a significant eruption?? 
Time granularities of entities (or events) have great significance to those of entity-
based (or event-based) temporal questions. So, in the following, we make estimation 
of the time granularities of entities and events from statistics, based on the intuition 
that some entities or events may favor certain types of time granularities, which is 
called user preference here.  
4.1   Estimation of Time Granularities of Entities and Events 
4.1.1   Time Granularity of Entities 
The time granularity of the entity is derived by counting the co-occurrences of the 
entity and time granularities. The statistics is gathered from AltaVista Web Search. 
The sentences containing both the entity and time expressions are extracted from the 
first one hundred results returned by AltaVista with the entity as the searching key-
word. The probability P of a time granularity class tgi on the occurrence of the entity 
is calculated as the following Equation (1).  
)(#
)(#)|(
entity
entitytg
entitytgP ii
?
=
  )|(max)( entitytgPArgentityTG itgi=          (1) 
#( ) is the number of the sentences containing the expressions between the parenthe-
sis. TG(entity) represents the time granularity of the entity. 
4.1.2   Time Granularity of Events 
The time granularities of the events are not directly extracted as what is done to the 
entities, because they have little chance to be reused on the observation that there are 
rarely two identical events in a question corpus. As an alternative, the time granularity 
of an event is estimated from a sequence of entity-verb-entity? approximating the 
event. The time granularity of the verb is determined as Equation (1) by substituting 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 419 
?verb? for ?entity?. We choose two strategies for the estimation: maximum product 
and one-win-all.  
Maximum  product: )'|()|()|(1)|( entitytgPverbtgPentitytgP
Z
eventtgP iiii =
 
                                 )|(max)( eventtgPArgeventTG itgi=                                        (2) 
TG(event) represents time granularity of event. Z is used for normalization. 
One-win-all: )}'|(),|(),|({max)( entitytgPverbtgPentitytgPArgeventTG iiitgi=               (3) 
Equation (1) is smoothed in order to avoid 0 values in Equation (2). 
                          
i
i
i tgttw
wtg
wtgP =
+
+?
= )(#
1)(#)|(                           (4) 
t is the number of the time granularity classes, w is either an entity or a verb. 
4.1.3   Experiment: Evaluating the Estimation 
In the 197 ambiguous questions, 12 questions are entity-based, and the rest 185 ques-
tions are event-based. If all the 197 questions are arbitrarily assigned a tag ?year?, the 
tagging accuracy is 48.2%. 
For each entity-based or event-based question, the time granularity of the entity or 
event within it are assumed as the time granularity of the question. Compared with the 
time granularity of the reference answer, for the entity-based questions, we achieve 
75% accuracy; for the event-based question, the accuracy of maximum product strat-
egy and one-win-all strategy are 67.0% and 64.3% respectively. It seems that maxi-
mum product strategy is more effective than one-win-all strategy in this application. 
With maximum product strategy, the overall accuracy on all the 197 ambiguous ques-
tions is 67.4%. Notice that the accuracy of arbitrarily tagging is only 48.2%, so the 
estimation of the time granularities of the entities and the events is useful for deter-
mining the time granularities of temporal questions. 
4.2   Distribution of the Time Granularity of Entities and Events 
4.2.1   Observation of Distribution 
In the experiments of estimation, we find that some entities or events tend to favor 
only one certain time granularity, some others tend to favor several time granularities, 
and the rest may have a uniform distribution almost on every time granularity. 
-1 0 1 2 3 4 5 6 7 8 9
0
20
40
60
80
100
Season
Week
Pr
o
p o
rti
o n
(%
)
Time Granularity of "gestation"
   
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
60
Century
Decade
Year
Month
Day
Pr
o
po
rti
o
n(%
)
Time Granularity of "Lincoln born"
-1 0 1 2 3 4 5 6 7 8 9
0
10
20
30
40
50
Year
Month
Weekday
Day
Hour
Pr
op
or
tio
n
(%
)
Time Granularity of "take place"
 
(a)   (b)   (c) 
Fig. 1. Distribution of the time granularities of the entities and events 
420 W. Li et al 
In Fig. 1(a), time granularity ?day? takes a preponderant proportion, i.e. more than 
80%, in the distribution of ?gestation?, which is called single-peak-distribution. In 
Fig. 1(b), both ?day? and ?year? take a large proportion, so ?Lincoln born? is multi-
peak-distributed. In Fig. 1(c), for ?take place?, all the time granularities almost take a 
similar proportion and it is a uniform distribution. 
4.2.2   Experiments on Distribution 
Assume an entity (or event) E, its possible time granularities {tgi, i=1,?t} and the 
corresponding probabilities {Pi, i=1,?t} (calculated by Equation 1 and 2).  
       ?= i iPt1? ;  ?= i iPId ),( ? ; ?
??
?
>
??
?
=
i
i
i P
P
PI
0
1),(                        (5) 
d is the number of time granularities tgi with higher probability Pi than average prob-
ability ? . For simplicity, distribution DE of the time granularity of E is determined as 
follows, 
                                              
3
31
1
>
?<
=
??
??
?
=
d
d
d
Uniform
Multi
Single
DE
                                                  (6)  
Observing the experiment results in Sect. 4.1.3, 88.7%, 56.3% and 18.9% accuracy 
are achieved on the questions within which the time granularities of the entities or 
events are estimated to be single-peak-, multi-peak-, and uniform-distributed respec-
tively. So whether the estimated time granularity of the entity or event is single-peak-, 
multi-peak-, or uniform-distributed highlights the confidence on the estimation, which 
can be taken as a feature associated with the estimation of the time granularities. 
5   Feature Design 
As described in the above section, estimation of the time granularities of the entities 
and the events is useful for determining the time granularities of temporal questions; 
whether a question is entity-based or not and the distribution of time granularities of 
the entities and events within the questions will also be taken as associated features. 
These three features are named user preference feature in total. Besides, another four 
types of features are considered. 
Word N-grams 
Word N-grams feature, e.g. unigram and bigram is the most straightforward feature 
and commonly used in question classification. In general question classification, uni-
gram ?when? indicates a temporal question. In temporal question classification, uni-
gram ?birthday? always implies a ?day? while bigram ?when ? born? is a strong 
evidence of the time granularity ?day?. From this aspect, word N-grams also reflect 
user preference on time granularity. 
Granularity of Time Expressions 
Time expressions are common in temporal questions, e.g. ?July 11, 1998? and date 
modifier ?1998? in ?1998 Superbowl?. We take the granularities of time expressions 
as features, for example, 
TG(?in 1998?) = ?year?  TG(?July 11, 1998?) = ?day? 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 421 
Granularities of time expressions impose the constraints on the time granularities of 
temporal questions. If there is a time expression whose time granularity is tg in a 
temporal question, time granularity of this question can not be tg. For example, ques-
tion ?When is the 1998 SuperBowl??, its time granularity can not be ?Year?, i.e. the 
time granularity of  ?1998?. 
Special Patterns 
In word N-gram features, words are equally processed, however, some special words 
combining with the verbs or the temporal connectives (e.g. ?when?, ?before? and 
?since?) will produce special patterns and affect the time granularities of temporal 
questions. Look at the following examples. 
[Eh]. Since when hasn?t John Sununu been able to fly on government planes for 
personal business? 
[Ei]. What time of the day does Michael Milken typically wake up? 
For question [Eh], the temporal preposition ?since? combined with ?when? highlights 
that this question is seeking for a beginning point time, which implies a finer time 
granularity; for question [Ei], ?typically? combined with verb ?wake up? indicates a 
generally occurred event, and implies that its time granularity could be ?less than 
day? or ?less than year?. 
Event Types 
In general, there are four event types: states, activities, accomplishments, and 
achievements. States and activities favour larger time granularities, while accom-
plishments and achievements favour smaller ones. For example, the activity ?stay? 
will favour larger time granularity than the accomplishment event ?take place?.  
6   Classifier Building 
In this work, we choose the Sparse Network of Winnow (SNoW) model as the time 
granularity classifier and compare it with a commonly used Language Model (LM) 
classifier. 
6.1   Language Model (LM) 
As language model has already been used in question classification [7], it is taken as 
the baseline model in the experiments. Language model mainly combines two types 
of features, i.e. unigram and bigram. Given a temporal question Q, its time granularity 
TG(Q) is calculated by Equation (7). 
              ?? =
=
+
=
=
?+=
nj
j jji
mj
j jitg wwtgPwtgPArgQTG i 1 11 )|()1()|(max)( ??              (7)  
w represents words. m and n are the numbers of unigrams and bigrams in questions 
respectively. ?  assigns different weights to unigrams and bigrams. In the experiment, 
best accuracy is achieved when 7.0=?  (see Sect. 7.3.1). 
6.2   Sparse Network of Winnow (SNoW) 
SNoW is a learning framework and applicable to the tasks with a very large number 
of features. It selects active features by updating weights of features, and learns a 
422 W. Li et al 
linear function from a corpus consisting of positive and negative examples. Let 
Ac={i1, ?, im} be the set of features that are active and linked to target class c. Let si 
be the real valued strength associated with feature in the example. Then the example?s 
class is c if and only if, 
                                                   ?
?
?
Aci
ciic sw ?,                                                     (8) 
icw , is weight of feature i connected with class c, which is learned from the training 
corpus. SNoW has already been used in question classification [3,10] and good results 
are reported. As mentioned in Sect. 5, five types of features are selected for our task. 
They are altogether counted to more than ten thousand features. Since it is a large 
feature set, SNoW is a good choice.  
7   Experiments 
7.1   Setup 
In this 348-question-corpus (see Sect. 1), time granularities of 151 questions are 
straightforward, while those of the rest 197 questions are ambiguous. For the sixteen 
time granularity classes, we only consider ten classes including more than four ques-
tions. Questions with unconsidered time granularity classes excluded, the question cor-
pus has 339 questions in total, 145 for training and 194 for testing. As a result, the task 
is to learn a model from the 145-question training corpus and classify questions in the 
194-question test corpus into ten classes: ?second?, ?minute?, ?hour?, ?day?, ?week-
day?, ?week?, ?month?, ?season?, ?year? and ?century?. The SNoW classifier is 
downloaded from UIUC (http://l2r.cs.uiuc.edu/~cogcomp/download.php?key=SNOW). 
7.2   Evaluation Criteria 
The primary evaluation standard is accuracy1, i.e. the proportion of the correct classi-
fied questions out of the test questions (see Equation 9). However, if a question seek-
ing for a finer time granularity, e.g. ?day?, has been incorrectly determined as a 
coarser one, e.g. ?year?, it should also be taken as partly correct, which is reflected in 
accuracy2 (see Equation 10).  
                                         
)(#
)(#
1 test
correctAccuracy =                                                  (9) 
#( ) is number of questions. 
       
)(#
)(
2 test
QRR
Accuracy i i?=
??
??
?
>
<
=
+?
=
)()'(
)()'(
)()'(
)1)()'((1
0
1
)(
QQ
QQ
QQ
QQ tgRtgR
tgRtgR
tgRtgR
tgRtgR
QRR
        (10) 
Qtg  and 'Qtg  are the reference and classification result respectively. )( QtgR is the 
rank of the time granularity class Qtg , scaling down from ?millions of years? to 
?second?. Rank of ?second? is 1, while rank of ?year? is 9. The ranks of the last three 
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 423 
time granularities, i.e. ?special date?, ?less than day? and ?less than year? are 14, 15 
and 16 respectively. Likewise, )'( QtgR is the rank of 'Qtg .  
7.3   Experimental Results and Analysis 
In the experiments, language model is taken as the baseline model. Performance of 
SNoW-based classifier will be compared with that of language model. Different com-
binations of features are tested in SNoW-based classifier and their performances are 
investigated. 
7.3.1   LM Classifier 
The LM classifier takes two types of features: unigram and bigram. Experiment re-
sults are presented in Fig. 2.  
Accuracy varies with different feature weight ?  and best accuracy (accuracy1 
68.0% and accuracy2 68.9%) achieves when ? =0.7. Accuracy when ? =1.0 is higher 
than that when ? =0. It indicates that, in the framework of language model, unigrams 
achieves better performance than bigrams, which accounts from the sparseness of 
bigram features. 
-0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
45
50
55
60
65
70
 Accuracy1
 Accuracy2
Ac
cu
ra
cy
(%
)
?
 
Fig. 2. Accuracy of LM classifier. Data in circle is the best performance achieved. 
7.3.2   SNOW Classifier 
Our SNoW classifier requires binary features. We then encode each feature with an 
integer label. When a feature is observed in a question, its label will appear in the 
extracted feature set of this question. There are six types of features: 15 user prefer-
ences (10 for the estimation of time granularities, 3 for the estimation distributions, 
and 2 for question focuses) (F1), 951 unigrams (F2), 9277 bigrams (F3), 10 granularity 
of time expressions (F4), 14 special patterns (F5), and 4 event types (F6). Although the 
number of all features is more than ten thousand, the features in one question are no 
more than twenty in general. Accuracies of SNoW classifier on 194 test questions are 
presented in Table 1. It shows that simply using unigram features, SNoW classifier 
has already achieved better accuracy than LM classifier (accuracy1: 69.5% vs. 68.0%; 
accuracy2: 70.3% vs. 68.9%). From this view, SNoW classifier outperforms LM clas-
sifier in handling sparse features. When all the six types of features are used, SNoW 
classifier achieves 83.5% in accuracy1 and 83.9% in accuracy2, almost close to the 
accuracy of user tagging, i.e. 86.2%. 
424 W. Li et al 
Table 1. Accuracy (%) of SNoW classifier 
Feature Set F2 F2, 3 F1~6 
Accuracy1 69.5 72.1 83.5 
Accuracy2 70.3 72.7 83.9 
Table 2. Accuracy1 (%) on different types of time granularities 
TG second minute hour day weekday 
Accuracy1 100 100 100 64.2 100 
TG week month season year century 
Accuracy1 100 60 100 90.5 66.7 
Table 3. Accuracy (%) on combination of different types of features 
Feature Set F2,3 F1,2,3 F2,3,4 F2,3,5 F2,3,6 
Accuracy1 72.1 79.8 73.7 74.7 72.6 
Accuracy2 72.7 80.6 74.7 75.2 73.1 
With all the six types of features, accuracy1 on the questions with different types of 
time granularity is illustrated in Table 2. It reveals that the classification errors mainly 
come from time granularity of ?month?, ?day? and ?century?. Low accuracy on 
?month? and ?century? accounts from absence of enough examples, i.e. examples for 
training and testing both less than five. Many ?day? questions are incorrectly classi-
fied into ?year?, which accounts for the low accuracy on ?day?.  The reason lies in 
that there are more ?year? questions than ?day? questions in the training question 
corpus (116 vs. 56).  
In general, we can extract three F1 features, one F4 feature, less than two F5 fea-
tures, and one F6 feature from one question. It is hard for SNoW classifier to train and 
test independently on each of these types of the features because of the small feature 
number in one example question. However, the numbers of F2 and F3 features in a 
question are normally more than ten. So we take unigrams (F2) and bigrams (F3) as 
the basic feature set. Table 3 presents the accuracy when the rest four types of fea-
tures are added into the basic feature set respectively. As expected user preference 
makes the most significant improvement, 7.82% in accuracy1 and 7.90% in accuracy2. 
Special patterns also play an important role, which makes 2.6% accuracy1 improve-
ment. It is strange that event type makes such a modest improvement (0.5%). After 
analyzing the experimental results, we find that as there are only four event types, it 
makes limited contribution to 10-class time granularity classification. 
8   Conclusion 
Various features for time granularity classification of temporal questions are investi-
gated in this paper. User preference is shown to make a significant contribution  
to classification performance. SNoW classifier, combining user preference, word  
 A Preliminary Work on Classifying Time Granularities of Temporal Questions 425 
N-grams, granularity of time expressions, special patterns and event types, achieves 
83.5% accuracy in classification, close to manually tagging accuracy 86.2%.  
Acknowledgement 
This project is partially supported by Hong Kong RGC CERG (Grant No: 
PolyU5181/03E), and partially by CUHK Direct Grant (No: 2050330).  
References 
1) TREC (ed.): The TREC-8 Question Answering Track Evaluation. Text Retrieval Confer-
ence TREC-8, Gaithersburg, MD (1999) 
2) Radev D. and Sundheim B.: Using TimeML in Question Answering. 
http://www.cs.brandeis.edu/~jamesp/arda/time/documentation/TimeML-use-in-qa-
v1.0.pdf, (2002) 
3) Li, X. and Roth, D.: Learning Question Classifiers. Proceedings of the 19th International 
Conference on Computational Linguistics (2002) 556-562 
4) S. Abney, M. Collins, and A. Singhal: Answer Extraction. Proceedings of the 6th ANLP 
Conference (2000) 296-301 
5) Saquete E., Mart?nez-Barco P., Mu?oz R.: Splitting Complex Temporal Questions for 
Question Answering Systems. Proceedings of the 42nd Annual Meeting of the Association 
for Computational Linguistics (2004) 567-574 
6) Diaz, F. and Jones, R.: Temporal Profiles of Queries. Yahoo! Research Labs Technical 
Report YRL-2004-022 (2004) 
7) Wei Li: Question Classification Using Language Modeling. CIIR Technical Report (2002) 
8) Dell Zhang and Wee Sun Lee: Question Classification Using Support Vector Machines. 
Proceedings of the 26th Annual International ACM SIGIR Conference on Research and 
Development in Information Retrieval (2003) 26-32 
9) Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda: Question Classification 
Using HDAG Kernel. Proceedings of Workshop on Multilingual Summarization and 
Question Answering (2003) 61-68 
10) Li X., Roth D., and Small K.: The Role of Semantic Information in Learning Question 
Classifiers. Proceedings of the International Joint Conference on Natural Language Proc-
essing (2004) 
11) Schilder, Frank & Habel, Christopher: Temporal Information Extraction for Temporal 
Question Answering. In New Directions in Question Answering. Papers from the 2003 
AAAI Spring Symposium TR SS-03-07 (2003) 34-44 
12) Rohini K. Srihari, Wei Li: A Question Answering System Supported by Information Ex-
traction. Proceedings of Association for Computational Linguistics (2000) 166-172 
13) Eduard Hovy, Laurie Geber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran: 
Towards Semantics-Based Answer Pinpointing. Proceedings of the DARPA Human Lan-
guage Technology Conference (2001) 
14) Hermjacob U.: Parsing and Question Classification for Question Answering. Proceedings 
of the Association for Computational Linguists Workshop on Open-Domain Question An-
swering (2001) 17-22 
15) Ittycheriah, Franz M., Zhu W., Ratnaparki A. and Mammone R.: Question Answering Us-
ing Maximum Entropy Components. Proceedings of the North American chapter of the 
Association for Computational Linguistics (2001) 33-39  
Integrating Collocation Features in Chinese Word Sense 
Disambiguation 
Wanyin Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswyli@comp.polyu.e
du.hk 
Qin Lu 
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
csqinlu@comp.polyu.e
du.hk 
Wenjie Li  
Department of Computing 
The Hong Kong Polytechnic 
University 
Hong Hom, Kowloon, HK 
cswjli@comp.polyu.ed
u.hk 
 
Abstract 
The selection of features is critical in pro-
viding discriminative information for clas-
sifiers in Word Sense Disambiguation 
(WSD). Uninformative features will de-
grade the performance of classifiers. Based 
on the strong evidence that an ambiguous 
word expresses a unique sense in a given 
collocation, this paper reports our experi-
ments on automatic WSD using collocation 
as local features based on the corpus ex-
tracted from People?s Daily News (PDN) 
as well as the standard SENSEVAL-3 data 
set. Using the Na?ve Bayes classifier as our 
core algorithm, we have implemented a 
classifier using a feature set combining 
both local collocation features and topical 
features. The average precision on the 
PDN corpus has 3.2% improvement com-
pared to 81.5% of the baseline system 
where collocation features are not consid-
ered. For the SENSEVAL-3 data, we have 
reached the precision rate of 37.6% by in-
tegrating collocation features into 
contextual features, to achieve 37% im-
provement  over  26.7% of precision in the 
baseline system. Our experiments have 
shown that collocation features can be used 
to reduce the size of human tagged corpus. 
1 Introduction 
WSD tries to resolve lexical ambiguity which 
refers to the fact that a word may have multiple 
meanings such as the word ?walk? in  ?Walk or 
Bike to school? and ?BBC Education Walk 
Through Time?, or the Chinese word  ???? in  
??????(?local government?) and ?????
????(?He is also partly right?). WSD tries to 
automatically assign an appropriate sense to an 
occurrence of a word in a given context.  
Various approaches have been proposed to deal 
with the word sense disambiguation problem 
including rule-based approaches, knowledge or 
dictionary based approaches, corpus-based ap-
proaches, and hybrid approaches. Among these 
approaches, the supervised corpus-based ap-
proach had been applied and discussed by many 
researches ([2-8]). According to [1], the corpus-
based supervised machine learning methods are 
the most successful approaches to WSD where 
contextual features have been used mainly to 
distinguish ambiguous words in these methods. 
However, word occurrences in the context are 
too diverse to capture the right pattern, which 
means that the dimension of contextual words 
will be very large when all words in the training 
samples are used for WSD [14]. Certain 
uninformative features will weaken the dis-
criminative power of a classifier resulting in a 
lower precision rate. To narrow down the con-
text, we propose to use collocations as contex-
tual information as defined in Section 3.1.2. It is 
generally understood that the sense of an am-
biguous word is unique in a given collocation 
[19]. For example, ???? means ?burden? but 
not ?baggage? when it appears in the collocation 
?????? (? burden of thought?). 
In this paper, we apply a classifier to combine 
the local features of collocations which contain 
the target word with other contextual features to 
discriminate the ambiguous words. The intuition 
is that when the target context captures a collo-
cation, the influence of other dimensions of
87
contextual words can be reduced or even ig-
nored. For example, in the expression ?????
?????? ? (?terrorists burned down the 
gene laboratory?), the influence of contextual 
word ???? (?gene?) should be reduced to work 
on the target word ???? because ?????? is 
a collocation whereas ???? and ???? are not 
collocations even though they do co-occur. Our 
intention is not to generally replace contextual 
information by collocation only. Rather, we 
would like to use collocation as an additional 
feature in WSD. We still make use of other  con-
textual features because of the following reasons. 
Firstly, contextual information is proven to be 
effective for WSD in the previous research 
works. Secondly, collocations may be independ-
ent on the training corpus and a sentence in con-
sideration may not contain any collocation. 
Thirdly, to fix the tie case such as ??????
?????? (?terrorists? gene checking?),  
???? means ?human? when presented in 
the collocation ??????, but ?particle? 
in the collocation ??????.  The primary 
purpose of using collocation in WSD is to im-
prove precision rate without any sacrifices in 
recall rate. We also want to investigate whether 
the use of collocation as an additional feature 
can reduce the size of hand tagged sense corpus. 
 The rest of this paper is organized as follows. 
Section 2 summarizes the existing Word Sense 
Disambiguation techniques based on annotated 
corpora. Section 3 describes the classifier and 
the features in our proposed WSD approach. 
Section 4 describes the experiments and the 
analysis of our results. Section 5 is the conclu-
sion. 
2 Related Work 
Automating word sense disambiguation tasks 
based on annotated corpora have been proposed. 
Examples of supervised learning methods for 
WSD appear in [2-4], [7-8]. The learning algo-
rithms applied including: decision tree, decision-
list [15], neural networks [7], na?ve Bayesian 
learning ([5],[11]) and maximum entropy [10]. 
Among these leaning methods, the most impor-
tant issue is what features will be used to con-
struct the classifier. It is common in WSD to use 
contextual information that can be found in the 
neighborhood of the ambiguous word in training 
data ([6], [16-18]). It is generally true that when 
words are used in the same sense, they have 
similar context and co-occurrence information 
[13]. It is also generally true that the nearby con-
text words of an ambiguous word give more ef-
fective patterns and features values than those 
far from it [12]. The existing methods consider 
features selection for context representation in-
cluding both local and topic features where local 
features refer to the information pertained only 
to the given context and topical features are sta-
tistically obtained from a training corpus. Most 
of the recent works for English corpus including 
[7] and [8], which combine both local and topi-
cal information in order to improve their per-
formance. An interesting study on feature 
selection for Chinese [10] has considered topical 
features as well as local collocational, syntactic, 
and semantic features using the maximum en-
tropy model. In Dang?s [10] work, collocational 
features refer to the local PoS information and 
bi-gram co-occurrences of words within 2 posi-
tions of the ambiguous word. A useful result 
from this work based on (about one million 
words) the tagged People?s Daily News shows 
that adding more features from richer levels of 
linguistic information such as PoS tagging 
yielded no significant improvement (less than 
1%) over using only the bi-gram co-occurrences 
information. Another similar study for Chinese 
[11] is based on the Naive Bayes classifier 
model which has taken into consideration PoS 
with position information and bi-gram templates 
in the local context. The system has a reported 
60.40% in both precision and recall based on the 
SENSEVAL-3 Chinese training data. Even 
though in both approaches, statistically signifi-
cant bi-gram co-occurrence information is used, 
they are not necessarily true collocations.  For 
example, in the express ?????????
????????????, the bi-grams in 
their system are (???,???, ???
?, ????, ?????, ????
??,? ????Some bi-grams such as 
????may have higher frequency but 
may introduce noise when considering it as fea-
tures in disambiguating the sense ?human|?? 
and ?symbol|??? like in the example case of 
?????????. In our system, we do not rely 
on co-occurrence information. Instead, we util-
ize true collocation information (???, ??) 
which fall in the window size of (-5, +5) as fea-
88
tures and the sense of ?human|?? can be de-
cided clearly using this features. The collocation 
information is a pre-prepared collocation list 
obtained from a collocation extraction system 
and verified with syntactic and semantic meth-
ods ([21], [24]).    
Yarowsky [9] used the one sense per collocation 
property as an essential ingredient for an unsu-
pervised Word-Sense Disambiguation algorithm 
to perform bootstrapping algorithm on a more 
general high-recall disambiguation. A few re-
cent research works have begun to pay attention 
to collocation features on WSD. Domminic [19] 
used three different methods called bilingual 
method, collocation method and UMLS (Unified 
Medical Language System) relation based 
method to disambiguate unsupervised English 
and German medical documents. As expected, 
the collocation method achieved a good preci-
sion around 79% in English and 82% in German 
but a very low recall which is 3% in English and 
1% in German. The low recall is due to the na-
ture of UMLS where many collocations would 
almost never occur in natural text.  To avoid this 
problem, we combine the contextual features in 
the target context with the pre-prepared colloca-
tions list to build our classifier.  
3 The Classifier With Topical Contex-
tual and Local Collocation Features 
3.1 The Feature Set 
As stated early, an important issue is what fea-
tures will be used to construct the classifier in 
WSD. Early researches have proven that using 
lexical statistical information, such as bi-gram 
co-occurrences was sufficient to produce close 
to the best results [10] for Chinese WSD. In-
stead of including bi-gram features as part of 
discrimination features, in our system, we con-
sider both topical contextual features as well as 
local collocation features. These features are 
extracted form the 60MB human sense-tagged 
People?s Daily News with segmentation infor-
mation.  
3.1.1 Topical Contextual Features 
Niu [11] proved in his experiments that Na?ve 
Bayes classifier achieved best disambiguation 
accuracy with small topical context window size 
(< 10 words).  We follow their method and set 
the contextual window size as 10 in our system.  
Each of the Chinese words except the stop 
words inside the window range will be consid-
ered as one topical feature. Their frequencies are 
calculated over the entire corpus with respect to 
each sense of an ambiguous word w.  The sense 
definitions are obtained from HowNet. 
3.1.2 Local Collocation Features 
We chose collocations as the local features. A 
collocation is a recurrent and conventional fixed 
expression of words which holds syntactic and 
semantic relations [21]. Collocations can be 
classified as fully fixed collocations, fixed col-
locations, strong collocations and loose colloca-
tions. Fixed collocations means the appearance 
of one word implies the co-occurrence of an-
other one such as ?????? (?burden of his-
tory?), while strong collocations allows very 
limited substitution of the components, for ex-
ample, ?????? (?local college?), or ? ???
?? (?local university?). The sense of ambiguous 
words can be uniquely determined in these two 
types of collocations, therefore are the colloca-
tions applied in our system. The sources of the 
collocations will be explained in Section 4.1. 
In both Niu [11] and Dang?s [10] work, topical 
features as well as the so called collocational 
features were used. However, as discussed in 
Section 2, they both used bi-gram co-
occurrences as the additional local features. 
However, bi-gram co-occurrences only indicate 
statistical significance which may not actually 
satisfy the conceptual definition of collocations. 
Thus instead of using co-occurrences of bi-
grams, we take the true bi-gram collocations 
extracted from our system and use this data to 
compare with bi-gram co-occurrences to test the 
usefulness of collocation for WSD. The local 
features in our system make use of the colloca-
tions using the template (wi, w) within a window 
size of ten (where i = ? 5). For example, ???
?????????? (?Government 
departments and local government commanded 
that?) fits the bi-gram collocation template (w, 
w1) with the value of (????). During the 
training and the testing processes, the counting 
of frequency value of the collocation feature will 
be increased by 1 if a collocation containing the 
ambiguous word occurs in a sentence. To have a 
good analysis on collocation features, we have 
also developed an algorithm using lonely 
adjacent bi-gram as locals features(named Sys-
89
adjacent bi-gram as locals features(named Sys-
tem A)  and another using collocation as local 
features(named System B). 
3.2 The Collocation Classifier 
We consider all the features in the features set F 
= Ft ?Fl = {f1, f2,  ? , fm } as independent, where 
Ft stands for the topical contextual features set, 
and Fl stands for the local collocation features 
set. For an ambiguous word w with n senses, let 
Sw = {ws1, ws2,  ? , wsn } be the sense set. For 
the contextual features, we directly apply the 
Na?ve Bayes algorithm using Add-Lambda 
Smoothing to handle unknown words: 
 
)|(log)(log)(1 sij
Ff
sisi wfpwpwscore
tj
?
?
+=   
(1) 
For each sense siw of an ambiguous word w:
 
)(
)()(
wfreq
wfreqwp sisi =                       (2) 
For each contextual feature fj respects to each 
sense siw of w : 
),(
),(
)|(
si
Ff
t
sij
sij wffreq
wffreq
wfp
tt
?
?
=   (3) 
To integrate the local collocation feature fj ? Fl  
with respect to each sense siw  of w, we use the 
follows formula: 
)()()( 21 sisisi wscorewscorewscore ?+= ?  (4) 
 
where ? is tuned from experiments (Section 4.5), 
score1( siw ) refers the score of the topical con-
textual features based on formula (1) and 
score2( siw ) refers the score of collocation fea-
tures with respect to the sense sjw  of w defined 
below. 
?
?
=
lj Ff
sjjsi wfwscore )|()(2 ?           (5) 
where ?(fj| sjw ) = 1 for fj ? Fl if the collocation 
occurs in the local context. Otherwise this term 
is set as 0. 
Finally, we choose the right skw so that 
)(maxarg sks wscores k=        (6) 
4 Experimental Results 
We have designed a set of experiments to com-
pare the classifier with and without the colloca-
tion features. In system A, the classifier is built 
with local bi-gram features and topical contex-
tual features. The classifier in system B is con-
structed from combining the local collocation 
features with topical features. 
4.1 Preparation the Data Set 
We have selected 20 ambiguous words from 
nouns and verbs with the sense number as 4 in 
average. The sense definition is taken from 
HowNet [22]. To show the effect of the algo-
rithm, we try to choose words with high degree 
of ambiguity, high frequency of use [23], and 
high frequency of constructing collocations. The 
selection of these 20 words is not completely 
random although within each criterion class we 
do try to pick word randomly. 
Based on the 20 words, we extracted 28,000 
sentences from the 60 MB People?s Daily News 
with segmentation information as our train-
ing/test set which is then manually sense-tagged.  
The collocation list is constructed from a 
combination of a digital collocation dictionary, a 
return result from a collocation automatic ex-
traction system [21], and a hand collection from 
the People?s Daily News. As we stated early, the 
sense of ambiguous words in the fixed colloca-
tions and strong collocations can be decided 
uniquely although they are not unique in loose 
collocations. For example, the ambiguous word 
???? in the collocation ??????? may 
have both the sense of ?appearance|??? or 
?reputation|???. Therefore, when labeling the 
sense of collocations, we filter out the ones 
which cannot uniquely determine the sense of 
ambiguous words inside. However, this does not 
mean that loose collocations have no contribu-
tion in WSD classification. We simply reduce its 
weight when combining it with the contextual 
features compared with the fixed and strong col-
locations. The sense and collocation distribution 
over the 20 words on the training examples can 
be found in Table 1. 
Table 1. Sense and Collocation Distribution of the 20 tar-
get words in the training corpus 
Am. 
W 
T# S1 
co# 
S2 
co# 
S3 
co# 
S4 
co# 
S5 
co# 
S6 
co# 
90
?? 31 1  1 
30 
10 NA  
  
?? 499 479  324 
18  
0 0 0 
NA  
?? 944 908  129 
1  
1 
17 
10 
18  
0 
0 NA 
?? 409 3  2 
389  
171 
17 
0 NA 
  
?? 110 3  0 
101 
36 
6  
9 NA 
  
?? 41 3  0 
37  
6 
1  
0 NA 
  
?? 4885 26  0 
34  
0 
72  
0 
4492 
1356 
261 
1 NA 
?? 3508 7  0 
48  
4 
3194 
1448 
259 
194 
NA  
?? 348 312  117 
22 
11 
14  
4 NA 
  
?? 4438 3983 721 
33  
10 
123  
37 
153 
123 
102 
23 
44 
5 
?? 1987 1712 723 
274 
10 NA  
  
?? 83 36  14 
47  
4 00 NA 
  
?? 995 168  108 
827 
513 NA  
  
?? 31 11  3 
20  
11 NA  
  
?? 2725 227 1772 
498 
49 
102 
424 
1898 
201 
NA  
?? 592 1  0 
208 
63 
367 
124 16 1 
NA  
?? 1155 756  571 
399 
135 NA  
  
?? 2792 691  98 
1765 
113 
336  
29 0 
NA  
?? 2460 82  63 
36 
11 
1231 
474 
877 
103 
NA  
?? 125 11  0 
64  
0 
15  
3 
32 
 4 
3  
0 NA 
T#: total number of sentences contain the ambiguous word 
s1- s6: sense no; co#: number of collocations in each sense 
4.2 The Effect of Collocation Features 
We recorded 6 trials with average precision over 
six-fold validation for each word. Their average 
precision for the six trials in the system A, and B 
can be found in Table 2 and Table 3. From Ta-
ble 3, regarding to precision, there are 16 words 
have improved and 4 words remained the same 
in the system B. The results from the both sys-
tem confirmed that collocation features do im-
prove the precision. Note that 4 words have the 
same precision in the two systems, which fall 
into two cases. In the first case, it can be seen 
that these words already have very high preci-
sion in the system A (over 93%) which means 
that one sense dominates all other senses. In this 
case, the additional collation information is not 
necessary. In fact, when we checked the inter-
mediate outputs, the score of the candidate 
senses of the ambiguous words contained in the 
collocations get improved. Even though, it 
would not change the result. Secondly, no collo-
cation appeared in the sentences which are 
tagged incorrectly in the system A. This is con-
firmed when we check the error files. For exam-
ple, the word ???? with the sense as ???? 
(?closeness?) appeared in 4492 examples over 
the total 4885 examples (91.9%). In the mean 
time, 99% of collocation in its collocation list 
has the same sense of ??? ? (?closeness?). 
Only one collocation ????? has the sense of 
??? ? (?power?). Therefore, the collocation 
features improved the score of sense ??? ? 
which is already the highest one based on the 
contextual features.  
As can be seen from Table 3, the collocation 
features work well for the sparse data. For ex-
ample, the word ???? in the training corpus 
has only one example with the sense ??? (?hu-
man?), the other 30 examples all have the sense 
???? (?management?). Under this situation, 
the topical contextual features failed to identify 
the right sense for the only appearance of the 
sense ??? (?human?) in the training instance 
??????????????????. How-
ever, it can be correctly identified in the system 
B because the appearance of the collocation ??
??????. 
To well show the effect of collocations on 
the accuracy of classifier for the task of WSD, 
we also tested both systems on SENSEVAL-3 
data set, and the result is recorded in the Table 4. 
From the difference in the relative improvement 
of both data sets, we can see that collocation 
features work well when the statistical model is 
not sufficiently built up such as from a small 
corpus like SENSEVAL-3. Actually, in this case, 
the training examples appear in the corpus only 
once or twice so that the parameters for such 
sparse training examples may not be accurate to 
forecast the test examples, which convinces us 
that collocation features are effective on han-
dling sparse training data even for unknown 
words. Fig. 1 shows the precision comparison in 
the system A, and B on SENVESAL-3. 
Table 2.  Average Precision (5/6 training, 1/6 test) of 
system A on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 .83 .972 
?? .90 .97 1.00 1.00 .97 .98 .972 
?? .97 .96 .96 .92 .98 .96 .958 
91
?? .94 .94 .97 .92 .97 .97 .951 
?? 1.00 1.00 .77 .94 .88 1.00 .932 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .93 .94 .89 .91 .89 .90 .91 
?? .94 .93 .86 .93 .89 .87 .903 
?? .83 .94 .89 .90 .88 .94 .897 
?? .86 .88 .92 .84 .82 .87 .865 
?? .92 .84 .92 .76 .84 .72 .833 
?? .84 .83 .88 .82 .88 .71 .827 
?? .80 .60 .80 .20 1.00 1.00 .733 
?? .68 .72 .67 .77 .70 .68 .703 
?? .51 .67 .47 .60 .68 .59 .586 
?? .70 .63 .66 .64 .64 .64 .652 
?? .57 .74 .55 .64 .72 .67 .648 
?? .65 .58 .66 .64 .54 .47 .58 
?? .55 .50 .45 .45 .45 .64 .507 
Total Average Precision 0.815 
Table 3. Average Precision (5/6 training, 1/6 test) of 
system B on People?s Daily News 
Amb. 
W T1 T2 T3 T4 T5 T6 
Ave. 
Prec. 
?? 1.00 1.00 1.00 1.00 1.00 1.00 1.00 
?? .90 .97 1.00 1.00 .97 .98 .970 
?? .96 .98 .97 .96 .98 .96 .968 
?? .94 .94 .97 .94 .97 .98 .957 
?? 1.00 1.00 .77 .94 .88 1.00 .931 
?? .83 1.00 1.00 1.00 .83 .90 .927 
?? .93 .95 .91 .92 .92 .92 .925 
?? .92 .95 .92 .92 .91 .91 .922 
?? .94 .94 .86 .93 .91 .87 .908 
?? .80 .95 .89 .93 .89 .94 .902 
?? .87 .88 .92 .84 .83 .91 .875 
?? .84 1.00 .92 .76 .84 .77 .855 
?? .88 .86 .89 .84 .90 .74 .852 
?? 1.00 .80 .80 .20 1.00 1.00 .800 
?? .69 .72 .68 .79 .75 .72 .725 
?? .69 .76 .73 .74 .82 .79 .755 
?? .58 .59 .70 .67 .64 .59 .628 
?? .68 .67 .66 .63 .65 .63 .653 
?? .65 .68 .71 .61 .70 .69 .673 
?? .60 .55 .54 .54 .54 .64 .568 
Total Average Precision 0.840 
Table 4.  Average Precision of System A & B on 
SENSEVAL-3 Data Set 
Amb. 
Word 
Total 
S 
Ave. Prec. in 
Sys A 
Ave. Prec. 
in Sys B 
?? 48 .207 .290 
?? 20 .742 .742 
? 49 .165 .325 
? 25 .325 .325 
?? 36 .260 .373 
?? 30 .167 .267 
?? 30 .192 .392 
?? 36 .635 .635 
? 57 .238 .275 
?? 36 .327 .385 
?? 31 .100 .322 
? 40 .358 .442 
?? 40 .308 .308 
? 76 .110 .123 
? 28 .308 .475 
?667. 500. 30 ? 
? 42 .165 .260 
? 57 .037 .422 
?? 28 .833 .103 
Total Ave. 
Precision .276 .376 
Fig. 1. The precision comparison in system A, and B based 
on SENSEVAL-3 
 
4.3 The Effect of Collocations on the Size 
of Training Corpus Needed 
Hwee [21] stated that a large-scale, human 
sense-tagged corpus is critical for a supervised 
learning approach to achieve broad coverage 
and high accuracy WSD. He conducted a thor-
ough study on the effect of training examples on 
the accuracy of supervised corpus based WSD. 
As the result showed, WSD accuracy continues 
to climb as the number of training examples in-
creases. Similarly, we have tested the system A, 
and B with the different size of training corpus 
based on the PDN corpus we prepared. Our ex-
periment results shown in Fig 2 follow the same 
fact.  The purpose we did the testing is that we 
hope to disclose the effect of collocations on the 
size of training corpus needed. From Fig 2, we 
can see by using the collocation features, the 
precision of the system B has increased slower 
along with the growth of training examples than 
the precision of the system A.  The result is rea-
sonable because with collocation feature, the 
statistical contextual information over the entire 
corpus becomes side effect. Actually, as can be 
seen from Fig. 2, after using collocation features 
92
in the system B, even we use 1/6 corpus as train-
ing, the precision is still higher than we use 5/6 
train corpus in the system A. 
Fig. 2. The precision variation respect to the size of   train-
ing corpus in system A, and B based on PDN corpus 
 
4.4 Investigation of Sense Distribution on 
the Effect of Collocation Features 
To investigate the sense distribution on the ef-
fect of collocation features, we selected the am-
biguous words with the number of sense varied 
from 2 to 6. In each level of the sense number, 
the words are selected randomly. Table 5 shows 
the effect of sense distribution on the effect of 
collocation features. From the table, we can see 
that the collocation features work well when the 
sense distribution is even for a particular am-
biguous word under which case the classifier 
may get confused. 
Table 5.  The Effect of Sense Distribution on the Effect of 
collocation Features 
Amb. 
word 
Prec. 
Wihtout 
coll 
Prec. 
With  
coll 
Sense 
# 
Sense 
Distri. 
?? .972 1 2 97% * 
?? .97 .97 4 96% * 
?? .957 .968 5 96% * 
?? .951 .957 3 95% * 
?? .931 .931 3 92% * 
?? .927 .927 3 90% * 
?? .925 .925 5 92% * 
?? .915 .922 4 91% * 
?? .903 .908 3 90% * 
?? .902 .902 6 90% * 
?? .865 .875 2 86% o 
?? .833 .855 3 ^ 
?? .823 .852 2 83% o 
?? .733 .8 2 ^ 
?? .706 .725 4 ^ 
?? .65 .653 4 ^ 
?? .618 .755 4 ^ 
?? .582 .628 2 ^ 
?? .563 .673 4 ^ 
?? .507 .568 5 ^ 
     *: over 90% samples fall in one dominate sense 
     ^: Even distribution over all senses  
     o: 83% to 86% samples fall in one dominate sense 
4.5 The Test of ? 
We have conducted a set of experiments based 
on both the PDN corpus and SENSEVLA-3 data 
to set the best value of ? for the formula (4) de-
scribed in Section 3.2. The best start value of ? 
is tested based on the precision rate which is 
shown in Fig. 3. It is shown from the experiment 
that ? takes the start value of 0.5 for both cor-
puses.  
Fig. 3. The best value of ? vs the precision rate 
 
5 Conclusion and the Future Work 
This paper reports a corpus-based Word Sense 
Disambiguation approach for Chinese word us-
ing local collocation features and topical contex-
tual features. Compared with the base-line 
systems in which a Na?ve Bayes classifier is 
constructed by combining the contextual fea-
tures with the bi-gram features, the new system 
achieves 3% precision improvement in average 
in Peoples? Daily News corpus and 10% im-
provement in SENSEVAL-3 data set. Actually, 
it works very well when disambiguating the 
sense with sparse distribution over the entire 
corpus under which the statistic calculation 
prone to identify it incorrectly. In the same time, 
because disambiguating using collocation fea-
93
tures does not need statistical calculation, it 
makes contribution to reduce the size of human 
tagged corpus needed which is critical and time 
consuming in corpus based approach.  
Because different types of collocations may 
play different roles in classifying the sense of an 
ambiguous word, we hope to extend this work 
by integrating collocations with different weight 
based on their types in the future, which may 
need a pre-processing job to categorize the col-
locations automatically. 
6 Acknowledgements 
We would like to present our thanks to the IR 
Laboratory in HIT University of China for shar-
ing their sense number definition automatically 
extracted from HowNet with us. 
References 
1. Hwee Tou Ng, Bin Wang, Yee Seng Chan. Exploiting 
Parallel Texts for Word Sense Disambiguation. ACL-03 
(2003) 
2. Black E.: An experiment in computational discrimina-
tion of English word senses. IBM Journal of Research 
and Development, v.32, n.2, (1988) 185-194 
3. Gale, W. A., Church, K. W. and Yarowsky, D.: A 
method for disambiguating word senses in a large cor-
pus. Computers and the Humanities, v.26, (1993) 415-
439 
4. Leacock, C., Towell, G. and Voorhees, E. M.: Corpus-
based statistical sense resolution. In Proceedings of the 
ARPA Human Languages Technology Workshop (1993) 
5. Leacock, C., Chodorow, M., & Miller G. A..Using Cor-
pus Statistics and WordNet Relations for Sense Identifi-
cation. Computational Linguistics, 24:1, (1998) 147?
165  
6. Sch?tze, H.: Automatic word sense discrimination. 
Computational Linguistics, v.24, n.1, (1998) 97-124 
7. Towell, G. and Voorhees, E. M.: Disambiguating highly 
ambiguous words. Computational Linguistics, v.24, n.1, 
(1998) 125-146 
 8. Yarowsky, D.: Decision lists for lexical ambiguity reso-
lution: Application to accent restoration in Spanish and 
French. In Proceedings of the Annual Meeting of the 
Association for Computational Linguistics, (1994) 88-
95 
9. Yarowsky, D.: Unsupervised word sense disambiguation 
rivaling supervised methods. In Proceedings of the An-
nual Meeting of the Association for Computational Lin-
guistics, (1995)189-196 
 10. Dang, H. T., Chia, C. Y., Palmer M., & Chiou, F.D., 
Simple Features for Chinese Word Sense Disambigua-
tion. In Proc. of COLING (2002) 
11. Zheng-Yu Niu, Dong-Hong Ji, Chew Lim Tan, Opti-
mizing Feature Set for Chinese Word Sense Disam-
biguation. To appear in Proceedings of the 3rd 
International Workshop on the Evaluation of Systems 
for the Semantic Analysis of Text (SENSEVAL-3). 
Barcelona, Spain (2004) 
12. Chen, Jen Nan and Jason S. Chang, A Concept-based 
Adaptive Approach to Word SenseDisambiguation, 
Proceedings of 36th Annual Meeting of the Association 
for Computational Linguistics and 17th International 
Conference on Computational linguistics. 
COLING/ACL-98 (1998) 237-243 
13.  Rigau, G., J. Atserias and E. Agirre, Combining Unsu-
pervised Lexical Knowledge Methods for Word Sense 
Disambiguation, Proceedings of joint 35th Annual 
Meeting of the Association for Computational Linguis-
tics and 8th Conference of the European Chapter of the 
Association for Computational Linguistics 
(ACL/EACL?97), Madrid, Spain (1997) 
14. Jong-Hoon Oh, and Key-Sun Choi, C02-1098.: Word 
Sense Disambiguation using Static and Dynamic Sense 
Vectors. COLING (2002) 
15. Yarowsky, D., Hierarchical Decision Lists for Word 
Sense Disambiguation. Computers and the Humanities, 
34(1-2), (2000) 179?186 
16. Agirre, E. and G. Rigau (1996) Word Sense Disam-
biguation using Conceptual Density, Proceedings of 
16th International Conference on Computational Lin-
guistics. Copenhagen, Denmark, COLING (1996) 
17. Escudero, G., L. M?rquez and G. Rigau, Boosting Ap-
plied to Word Sense Disambiguation. Proceedings of 
the 11th European Conference on Machine Learning 
(ECML 2000) Barcelona, Spain. 2000. Lecture Notes in 
Artificial Intelligence 1810. R. L. de M?ntaras and E. 
Plaza (Eds.). Springer Verlag (2000) 
18. Gruber, T. R., Subject-Dependent Co-occurrence and 
Word Sense Disambiguation. Proceedings of 29th An-
nual Meeting of the Association for Computational Lin-
guistics (1991) 
19. Dominic Widdows, Stanley Peters, Scott Cederberg, 
Chiu-Ki Chan, Diana Steffen, Paul Buitelaar, Unsuper-
vised Monolingual and Bilingual Word-Sense Disam-
biguation of Medical Documents using UMLS. 
Appeared in Natural Language Processing in Biomedi-
cine,. ACL 2003 Workshop, Sapporo, Japan (2003) 9?
16 
20. Hwee Tou Ng., Getting serious about word sense dis-
ambiguation. In Proceedings of the ACL SIGLEX 
Workshop on Tagging Text with Lexical Seman-
tics:Why, What, and How? (1997) 1?7 
21. Ruifeng Xu , Qin Lu, and Yin Li, An automatic Chi-
nese Collocation Extraction Algorithm Based On Lexi-
cal Statistics. In Proceedings of the NLPKE Workshop 
(2003) 
22.  D. Dong and Q. Dong, HowNet. 
   http://www.keenage.com, (1991) 
23.  Chih-Hao Tsai, 
 http://technology.chtsai.org/wordlist/, (1995-2004) 
24. Q. Lu, Y. Li, and R. F. Xu, Improving Xtract for Chi-
nese Collocation Extraction.  Proceedings of IEEE In-
ternational Conference on Natural Language Processing 
and Knowledge Engineering, Beijing (2003) 
 
 
94
67
68
69
70
71
72
73
74
75
Preliminary Chinese Term Classification 
for Ontology Construction 
Gaoying Cui, Qin Lu, Wenjie Li 
Department of Computing, 
Hong Kong Polytechnic University 
{csgycui, csluqin, cswjli}@comp.polyu.edu.hk 
 
 
Abstract 
An ontology can be seen as a representa-
tion of concepts in a specific domain. Ac-
cordingly, ontology construction can be re-
garded as the process of organizing these 
concepts. If the terms which are used to la-
bel the concepts are classified before build-
ing an ontology, the work of ontology con-
struction can proceed much more easily. 
Part-of-speech (PoS) tags usually carry 
some linguistic information of terms, so 
PoS tagging can be seen as a kind of pre-
liminary classification to help constructing 
concept nodes in ontology because features 
or attributes related to concepts of different 
PoS types may be different. This paper pre-
sents a simple approach to tag domain 
terms for the convenience of ontology con-
struction, referred to as Term PoS (TPoS) 
Tagging. The proposed approach makes 
use of segmentation and tagging results 
from a general PoS tagging software to pre-
dict tags for extracted domain specific 
terms. This approach needs no training and 
no context information. The experimental 
results show that the proposed approach 
achieves a precision of 95.41% for ex-
tracted terms and can be easily applied to 
different domains. Comparing with some 
existing approaches, our approach shows 
that for some specific tasks, simple method 
can obtain very good performance and is 
thus a better choice. 
Keywords: ontology construction, part-of-
speech (PoS) tagging, Term PoS (TPoS) 
tagging. 
1 Introduction 
Ontology construction has two main issues 
including the acquisition of domain concepts and 
the acquisition of appropriate taxonomies of these 
concepts. These concepts are labeled by the terms 
used in the domain which are described by 
different attributes. Since domain specific terms 
(terminology) are labels of concepts among other 
things, terminology extraction is the first and the 
foremost important step of domain concept 
acquisition. Most of the existing algorithms in 
Chinese terminology extraction only produce a list 
of terms without much linguistic information or 
classification information (Yun Li and Qiangjun 
Wang, 2001; Yan He et al, 2006; Feng Zhang et 
al., 2006). This fact makes it difficult in ontology 
construction as the fundamental features of these 
terms are missing. The acquisition of taxonomies is 
in fact the process of organizing domain specific 
concepts. These concepts in an ontology should be 
defined using a subclass hierarchy by assigning 
and defining properties and by defining 
relationship between concepts etc. (Van Rees, R., 
2003). These methods are all concept descriptions. 
The linguistic information associated with domain 
terms such as PoS tags and semantic classification 
information of terms can also make up for the 
concept related features which are associated with 
concept labels. Terms with different PoS tags 
usually carry different semantic information. For 
example, a noun is usually a word naming a thing 
or an object. A verb is usually a word denoting an 
action, occurrence or state of existence, which are 
all associated with a time and a place. Thus in 
ontology construction, noun nodes and verb nodes 
should be described using different attributes with 
different discriminating characters. With this 
information, extracted terms can then be classified 
The 6th Workshop on Asian Languae Resources, 2008
17
accordingly to help in ontology construction and 
retrieval work. Thus PoS tags can help identify the 
different features needed for concept representation 
in domain ontology construction.  
It should be pointed out that Term PoS (TPoS) 
tagging is different from the general PoS tagging 
tasks. It is designed to do PoS tagging for a given 
list of terms extracted from some terminology ex-
traction algorithms such as those presented in 
(Luning Ji et al, 2007). The granularity of general 
PoS tagging is smaller than what is targeted in this 
paper because terms representing domain specific 
concepts are more likely to be compound words 
and sometimes even phrases, such as ?????
? ?(file manager),  ????? ?(description of 
concurrency), etc.. Even though current general 
word segmentation and PoS tagging can achieve 
precision of 99.6% and 97.58%, respectively 
(Huaping Zhang et al, 2003),   its performance for 
domain specific corpus is much less satisfactory 
(Luning Ji et al, 2007), which is why terminology 
extraction algorithms need to be developed. 
In this paper, a very simple but effective method 
is proposed for TPoS tagging which needs no train-
ing process or even context information. This 
method is based on the assumption that every term 
has a headword. For a given list of domain specific 
terms which are segmented and each word in the 
term already has a PoS tag, the TPoS tagging algo-
rithm then identifies the position of the headword 
and take the tag of the headword as the tag of the 
term. Experiments show that this method is quite 
effective in giving good precision and minimal 
computing time. 
The remaining of this paper is organized as fol-
lows. Section 2 reviews the related work. Section 3 
gives the observations to the task and correspond-
ing corpus, then presents our method for TPOS 
tagging. Section 4 gives the evaluation details and 
discussions on the proposed method and reference 
methods. Section 5 concludes this paper. 
2 Related Work 
Although TPoS tagging is different from general 
PoS tagging, the general POS tagging methods are 
worthy of referencing. There are a lot of existing 
POS tagging researches which can be classified 
into following categories in general. Natural ideas 
of solving this problem were to make use of the 
information from the words themselves. A number 
of features based on prefixes and suffixes and 
spelling cues like capitalization were adopted in 
these researches (Mikheev, A, 1997; Brants, Thor-
sten, 2000; Mikheev, A, 1996). Mikheev presented 
a technique for automatically acquiring rules to 
guess possible POS tags for unknown words using 
their starting and ending segments (Mikheev, A, 
1997). Through an unsupervised process of rule 
acquisition, three complementary sets of word-
guessing rules would be induced from a general 
purpose lexicon and a raw corpus: prefix morpho-
logical rules, suffix morphological rules and end-
ing-guessing rules (Mikheev, A, 1996). Brants 
used the linear interpolation of fixed length suffix 
model for word handling in his POS tagger, named 
TnT. For example, an English word ending in the 
suffix ?able was very likely to be an adjective 
(Brants, Thorsten, 2000). 
Some existing methods are based on the analysis 
of word morphology. They exploited more features 
besides morphology or took morphology as sup-
plementary means (Toutanova et al, 2003; Huihsin 
Tseng et al, 2005; Samuelsson, Christer, 1993). 
Toutanova et al demonstrated the use of both pre-
ceding and following tag contexts via a depend-
ency network representation and made use of some 
additional features such as lexical features includ-
ing jointly conditioning on multiple consecutive 
words and other fine-grained modeling of word 
features (Toutanova et al, 2003). Huihisin et al 
proposed a variety of morphological word features, 
such as the tag sequence features from both left 
and right side of the current word for POS tagging 
and implemented them in a Maximum Entropy 
Markov model (Huihsin Tseng et al, 2005). 
Samuelsson used n-grams of letter sequences end-
ing and starting each word as word features. The 
main goal of using Bayesian inference was to in-
vestigate the influence of various information 
sources, and ways of combining them, on the abil-
ity to assign lexical categories to words. The 
Bayesian inference was used to find the tag as-
signment T with highest probability P(T|M, S) 
given morphology M (word form) and syntactic 
context S (neighboring tags) (Samuelsson, Christer, 
1993). 
Other researchers inclined to regard this POS 
tagging work as a multi-class classification prob-
lem. Many methods used in machine learning, such 
The 6th Workshop on Asian Languae Resources, 2008
18
as Decision Tree, Support Vector Machines (SVM) 
and k-Nearest-Neighbors (k-NN), were used for 
guessing possible POS tags of words (G. Orphanos 
and D. Christodoulakis, 1999; Nakagawa T, 2001; 
Maosong Sun et al, 2000). Orphanos and Christo-
doulakis presented a POS tagger for Modern Greek 
and focused on a data-driven approach for the in-
duction of decision trees used as disambiguation or 
guessing devices (G. Orphanos and D. Christodou-
lakis, 1999). The system was based on a high-
coverage lexicon and a tagged corpus capable of 
showing off the behavior of all POS ambiguity 
schemes and characteristics of words. Support 
Vector Machine is a widely used (or effective) 
classification approach for solving two-class pat-
tern recognition problems. Selecting appropriate 
features and training effective classifiers are the 
main points of SVM method. Nakagawa et al used 
substrings and surrounding context as features and 
achieve high accuracy in POS tag prediction (Na-
kagawa T, 2001). Furthermore, Sun et alpresented 
a POS identification algorithm based on k-nearest-
neighbors (k-NN) strategy for Chinese word POS 
tagging. With the auxiliary information such as 
existing tagged lexicon, the algorithm can find out 
k nearest words which were mostly similar with the 
word need tagging (Maosong Sun et al, 2000). 
3 Algorithm Design 
As pointed out earlier, TPoS tagging is different 
from the general PoS tagging tasks. In this paper, it 
is assumed that a terminology extraction algorithm 
has already obtained the PoS tags of individual 
words. For example, in the segmented and tagged 
sentence ????/n ??/n ?/v ?/f ?/w ??/n 
??/d ?/v ???/a ??/n ?/f ??/v ?/w?(In 
computer graphics, objects are usually represented 
as polygonal meshes.), the term ??????? 
(polygonal meshes) has been segmented into two 
individual words and tagged as ???? /a? 
(polygonal /a) and ??? /n? (meshes /n). The 
terminology extraction algorithm would identify 
these two words  ????/a? and ???/n? as a 
single term in a specific domain. The proposed 
algorithm is to determine the PoS of this single 
term ??????? (polygonal meshes), thus the 
algorithm is referred to as TPoS tagging. It can be 
seen that the general purpose PoS tagging and term 
PoS tagging assign tags at different granularity. In 
principle, the context information of terms can help 
TPoS tagging and the individual PoS tags may be 
good choices as classification features. 
The proposed TPoS tagging algorithm consists 
of two modules. The first module is a terminology 
extraction preprocessing module. The second 
module carries out the TPoS tag assignment. In the 
terminology extraction module, if the result of ter-
minology extraction algorithm is a list of terms 
without PoS tags, a general purpose segmenter 
called ICTCLAS1 will be used to give PoS tags to 
all individual words. ICTCLAS is developed by 
Chinese Academy of Science, the precision of 
which is 97.58% on tagging general words 
(Huaping Zhang et al, 2003). Then the output of 
this module is a list of terms, referred to as Term-
List, using algorithms such as the method de-
scribed in (Luning Ji et al, 2007). 
In this paper, two simple schemes for the term 
PoS tag assignment module are proposed. The first 
scheme is called the blind assignment scheme. It 
simply assigns the noun tag to every term in the 
TermList. This is based on the assumption that 
most of the terms in a specific domain represent 
certain concepts that are most likely to be nouns. 
Result from this blind assignment scheme can be 
considered as the baseline or the worse case sce-
nario. Even in general domain, it is observed that 
nouns are in the majority of Chinese words with 
more than 50% among all different PoS tags (Hui 
Wang, 2006).  
The second scheme is called head-word-driven 
assignment scheme. Theoretically, it will take the 
tag of the head word of one term as the tag of the 
whole term. But here it simply takes the tag of the 
last word in a term. This is based on the assump-
tion that each term has a headword which in most 
cases is the last word in a term (Hui Wang, 2006). 
One additional experiment has been done to verify 
this assumption. A manually annotated Chinese 
shallow Treebank in general domain is used for the 
statistic work (Ruifeng Xu et al, 2005). There are 
9 different structures of Chinese phrases, (Yunfang 
Wu et al, 2003), but only 3 of them do not have 
their head words in the tail, which are about 6.56% 
from all phrases. Following the examples earlier, 
   
1 Copyright ? Institute of Computing Technology, Chinese 
Academy of Sciences 
The 6th Workshop on Asian Languae Resources, 2008
19
the term ????/a ??/n? (polygonal /a meshes 
/n) will be assigned the tag ?/n? because the last 
word is labeled ?/n?. 
There are a lot of semanteme tags at the end of a 
term. For example, ?/ng? presents single character 
postfix of a noun. But it would be improper if a 
term is tagged as ?/ng?. For example, the term ??
?? ? (decision-making machine) contains two 
segments as  listed with two components ???/n? 
and ??/ng?. It is obvious that ????/ng? is in-
appropriate. Thus the head-word-driven assign-
ment scheme also includes some rules to correct 
this kind of problems. As will be discussed in the 
experiment, the current result of TPoS tagging is 
based on 2 simple induction rules applied in this 
algorithm. 
4 Experiments and Discussions 
The domain corpus used in this work contains 
16 papers selected from different Chinese IT jour-
nals between 1998 and 2000 with over 1,500,000 
numbers of characters. They cover topics in IT, 
such as electronics, software engineering, telecom, 
and wireless communication. The same corpus is 
used by the terminology extraction algorithm de-
veloped in (Luning Ji et al, 2007). In the domain 
of IT, two TermLists are used for the experiment. 
TermList1 is a manually collected and verified 
term list from the selected corpus containing a total 
of 3,343 terms. TermList1 is also referred to as the 
standard answer set to the corpus for evaluation 
purposes. TermList2 is produced by running the 
terminology extraction algorithm in (Van Rees, R, 
2003). TermList2 contains 2,660 items out of 
which 929 of them are verified as terminology and 
1,731 items are not considered terminology ac-
cording to the standard answer above. 
To verify the validity of the proposed method to 
different domains, a term list containing 366 legal 
terms obtained from Google searching results for 
??????? ?(complete dictionary of legal 
terms) is selected for comparison, which is named 
TermList3. 
4.1 Experiment on the Blind Assignment 
Scheme 
The first experiment is designed to examine the 
proportion of nouns in TermList1 and TermList3, 
to validate of the assumption of the blind assign-
ment scheme. In first part of this experiment, all 
the 3,343 terms in TermList1 are tagged as nouns. 
The result shows that the precision of the blind 
assignment scheme is between 78.79% and 84.77%. 
The reason for the range is that there are about 200 
terms in TermList1 which can be considered either 
as nouns, gerunds, or even verbs without reference 
to context. For example, the term ???????
?? (?remote access of local area network? or ?re-
mote access to local area network?) and the term 
???? (polarization or polarize), can be consid-
ered either as nouns if they are regarded as courses 
of events or as verbs if they refer to the actions for 
completing certain work. The specific type is de-
pendent on the context which is not provided with-
out the use of a corpus. However, the experiment 
result does show that in a specific domain, there is 
a much higher percentage of terms that are nouns 
than other tags in general (Hui Wang, 2006). As to 
TermList3, the precision of blind assignment is 
between 65.57% and 70.77% (19 mixed ones). 
TermList2 is the result of a terminology extraction 
algorithm and there are non-term items in the ex-
traction result, so the blind assignment scheme is 
not applied on TermList2. The blue colored bars 
(lighter color) in Figure 1 shows the result of 
TermList1 and TermList3 using the blind assign-
ment scheme which gives the two worst result 
compared to our proposed approach to be dis-
cussed in Section 4.2 
4.2 Experiments on the Head-Word-Driven 
Assignment Scheme 
The experiment in this section was designed to 
validate the proposed head-word-driven assign-
ment scheme. The same experiment is conducted 
on the three term lists respectively, as shown in 
Figure 1 in purple color (darker color). The preci-
sion for assigning TPoS tags to TermList1 is 
93.45%.  By taking the result from a terminology 
extraction algorithm without regards to its potential 
error propagations, the precision of the head-word-
driven assignment scheme for TermList2 is 
94.32%. For TermList3, the precision of PoS tag 
assignment is 90.71%. By comparing to the blind 
assignment scheme, this algorithm has reasonably 
good performance for all three term list with 
precision of over 90%. It also gives 8.7% and 
19.9% improvement for TermList1 and TermList3, 
respectively, as compared to the blind assignment 
The 6th Workshop on Asian Languae Resources, 2008
20
scheme, a reasonably good improvement without a 
heavy cost.  However, there are some abnormali-
ties in these results. Supposedly, TermList1 is a 
hand verified term list in the IT domain and thus its 
result should have less noise and thus should per-
form better than TermList2, which is not the case 
as shown in Figure 1. 
Figure 1 Performance of the Two Assignment 
Schemes on the Three Term Lists 
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
T1 T2 T3
Term Lists
Pr
ec
isi
on
blind assignment
head-driven
assignment
 
By further analyzing the error result, for exam-
ple for TermList1, among these 3,343 terms, about 
219 were given improper tags, such as the term ??
??? (Graphics). In this example, two individual 
words, ???/n? and ??/v?, form a term. So the 
output was ????/v? for taking the tag of the last 
segment. It was a wrong tag because the whole 
term was a noun. In fact, the error is caused by the 
general word PoS tagging algorithm because with-
out context, the most likely tagging of ???, a se-
manteme, is a verb. This kind of errors in se-
manteme tagging appeared in the results of all 
three term lists with 169 from TermList1, 29 from 
TermList2 and 12 from TermList3, respectively. 
This was a kind of errors which can be corrected 
by applying some simple induction rules. For ex-
ample, for all semantemes with multiple tags (in-
cluding noun as in the example), the rule can be 
?tagging terms with noun suffixes as nouns?. For 
example, terms ???/n ?/q? (reform-through-
labor camp) and ????/n ??/n ?/v? (com-
puter graphics) were given different tags using the 
head-word-driven assignment scheme. They were 
assigned as: ????/q? and ???????/v? 
which can be corrected by this rule.   Another kind 
of mistake is related to the suffix tags such as ?/ng? 
(noun suffix) and ?/vg?(verb suffix). For examples, 
???/n ??/n ?/ng? (intellectual property tri-
bunal) and ???/n ?/vg? (data set) will be tagged 
as ?????? /ng? and ???? /vg?, respec-
tively, which are obviously wrong. So, the simple 
rule of ?tagging terms with ?/ng? and ?/vg? to ?/n? 
is applied. The performance of TPoS tag assign-
ment after applying these two fine tuning induction 
rules are shown in Table 1 below. 
Table 1 Influence of Induction Rules on Different 
Term Lists 
Term 
Lists 
Precision 
of tagging
Precision 
after add-
ing induc-
tion rule 
Improve-
ment 
Percentage
TermList1 93.45% 97.03% 3.83% 
TermList2 94.32% 95.41% 1.16% 
TermList3 90.71% 93.99% 3.62% 
It is obvious that with the use of fine tuning us-
ing induction rules, the results are much better. In 
fact the result for TermList1 reached 97.03% 
which is quite close to PoS tagging of general do-
main data. The abnormality also disappeared as the 
performance of TermList1 has the best result. The 
improvement to TermList2 (1.16%) is not as obvi-
ous as that for TermList1 and TermList3, which 
are 3.83% and 3.62%, respectively. This, however, 
is reasonable as TermList2 is produced directly 
from a terminology extraction algorithm using a 
corpus, thus, the results are noisier. 
Further analysis is then conducted on the result 
of TermList2 to examine the influence of non-term 
items to this term list. The non-term items are 
items that are general words or items cannot be 
considered as terminology according to the stan-
dard answer sheet. For example, neither of the 
terms ???? (problem) and ??????? (pat-
tern training is) were considered as terms because 
the former was a general word, and the latter 
should be considered as a fragment rather than a 
word. In fact, in 2,660 items extracted by the algo-
rithm as terminology, only 929 of them are indeed 
terminology (34.92%), and rest of them do not 
qualify as domain specific terms. The result of this 
analysis is listed in Table 2. 
The 6th Workshop on Asian Languae Resources, 2008
21
Table 2 Data Distribution Analysis on TermList2 
Without Induction 
Rules 
Induction Rules 
Applied  
correct 
terms precision 
correct 
terms precision 
Terms 
(929)  879 94.62%  898 96.66%
Non-terms 
(1,731) 1,630 94.17% 1,640 94.74% 
Total 
(2,660) 2,509 94.32% 2,538 95.41% 
Results show that 31 and 50 from the 929 cor-
rect terms were assigned improper PoS tags using 
the proposed algorithm with and without the induc-
tions rules, respectively. That is, the precisions for 
correct data are comparable to that of TermList1 
(93.45% and 97.03%, respectively). For the non-
terms, 91 items and 101 items from 1,731 items 
were assigned improper tags with and without the 
induction rules, respectively. Even though the pre-
cisions for terms and non-terms without using the 
induction rules are quite the same (94.62% vs. 
94.17%), the improvement for the non-terms using 
the induction rules are much less impressive than 
that for the terms. This is the reason for the rela-
tively less impressed performance of induction 
rules for TermList2.  It is interesting to know that, 
even though the performance of the terminology 
extraction algorithm is quite poor with precision of 
only around 35% (929 out of 2,666 terms), it does 
not affect too much on the performance of the 
TPoS proposed in this paper. This is mainly be-
cause the items extracted are still legitimate words, 
compounds, or phrases which are not necessarily 
domain specific. 
The proposed algorithm in this paper use mini-
mum resources. They need no training process and 
even no context information. But the performance 
of the proposed algorithm is still quite good and 
can be directly used as a preparation work for do-
main ontology construction because of its presion 
of over 95%. Other PoS tagging algorithms reach 
good performance in processing general words. 
For example, a k-nearest-neighbors strategy to 
identify possible PoS tags for Chinese words can 
reach 90.25% for general word PoS tagging 
(Maosong Sun et al, 2000). Another method based 
on SVM method on English corpus can reach 
96.9% in PoS tagging known and unknown words 
(Nakagawa T, 2001). These results show that pro-
posed method in this paper is comparable to these 
general PoS tagging algorithms in magnitude. Of 
course, one main reason of this fact is the differ-
ence in its objectives. The proposed method is for 
the PoS tagging of domain specific terms which 
have much less ambiguity than tagging of general 
text. Domain specific terms are more likely to be 
nouns and there are some rules in the word-
formation patterns while general PoS tagging algo-
rithms usually need training process in which large 
manually labeled corpora would be involved. Ex-
periment results also show that this simple method 
can be applied to data in different domains. 
5 Conclusion and Future Work 
In this paper, a simple but effective method for 
assigning PoS tags to domain specific terms was 
presented. This is a preliminary classification work 
on terms. It needs no training process and not even 
context information. Yet it obtains a relatively 
good result. The method itself is not domain de-
pendent, thus it is applicable to different domains. 
Results show that in certain applications, a simple 
method may be more effective under similar cir-
cumstances. The algorithm can still be investigated 
over the use of more induction rules. Some context 
information, statistics of word/tag usage can also 
be explored. 
Acknowledgments 
This project is partially supported by CERG 
grants (PolyU 5190/04E and PolyU 5225/05E) and 
B-Q941 (Acquisition of New Domain Specific 
Concepts and Ontology Update). 
References 
Yun Li, Qiangjun Wang. 2001. Automatic Term Extrac-
tion in the Field of Information Technology. In the 
proceedings of The Conference of 20th Anniversary 
for Chinese Information Processing Society of China.  
Yan He, Zhifang Sui, Huiming Duan, and Shiwen Yu. 
2006. Term Mining Combining Term Component 
Bank. In Computer Engineering and Applications. 
Vol.42 No.33,4--7. 
Feng Zhang, Xiaozhong Fan, and Yun Xu. 2006. Chi-
nese Term Extraction Based on PAT Tree. Journal of 
Beijing Institute of Technology. Vol. 15, No. 2. 
Van Rees, R. 2003. Clarity in the Usage of the Terms 
Ontology, Taxonomy and Classification. CIB73. 
The 6th Workshop on Asian Languae Resources, 2008
22
Mikheev, A. 1997. Automatic Rule Induction. for Un-
known Word Guessing. In Computational Lingusitics 
Vol. 23(3), ACL.  
Toutanova, Kristina, Dan Klein, Christopher Manning, 
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Network. 
In proceedings of HLT-NAACL. 
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005.Morphological Features Help POS Tag-
ging of Unknown Words across Language Varieties. 
In proceedings of the Fourth SIGHAN Workshop on 
Chinese Language Processing. 
Samuelsson, Christer. 1993. Morphological Tagging 
Based Entirely on Bayesian Inference. In proceedings 
of NCCL 9. 
Brants, Thorsten. 2000. TnT: A Statistical Part-of-
Speech Tagger. In proceedings of ANLP 6. 
G. Orphanos, and D. Christodoulakis. 1999. POS Dis-
ambiguation and Unknown Word Guessing with De-
cision Trees. In proceedings of EACL?99, 134--141. 
H Schmid. 1994. Probabilistic Part-of-Speech Tagging 
Using Decision Trees. In proceedings of International 
Conference on New Methods in Language Processing.  
Maosong Sun, Dayang Shen, and Changning Huang. 
1997. CSeg& Tag1.0: a practical word segmenter 
and POS tagger for Chinese texts. In proceedings of 
the fifth conference on applied natural language 
processing.  
Ying Liu. 2002. Analysing Chinese with Rule-based 
Method Combined with Statistic-based Method. In 
Computer Engineering and Applications, Vol.7. 
Mikheev, A. 1996. Unsupervised Learning of Word-
Category Guessing Rules. In proceedings of ACL-96.  
Nakagawa T, Kudoh T, and Matsumoto Y. 2001. Un-
known Word Guessing and Part-of-Speech Tagging 
Using Support Vector Machines. In proceedings of 
NLPPRS 6, 325--331. 
Maosong Sun, Zhengping Zuo, and B K, TSOU. 2000. 
Part-of-Speech Identification for Unknown Chinese 
Words Based on K-Nearest-Neighbors Strategy. In 
Chinese Journal of Computers. Vol.23 No.2: 166--
170. 
Luning Ji, Mantai Sum, Qin Lu, Wenjie Li, Yirong 
Chen. 2007. Chinese Terminology Extraction using 
Window-based Contextual Information. In proceed-
ings of CICLING. 
Huaping Zhang et al 2003. HHMM-based Chinese 
Lexical Analyzer ICTCLAS. Second SIGHAN work-
shop affiliated with 41th ACL, 184--187. Sapporo 
Japan. 
Hui Wang. Last checked: 2007-08-04. Statistical studies 
on Chinese vocabulary (???????? ). 
http://www.huayuqiao.org/articles/wanghui/wanghui
06.doc. The date of publication is unknown from the 
online source. 
Ruifeng Xu, Qin Lu, Yin Li and Wanyin Li. 2005. The 
Design and Construction of the PolyU Shallow Tree-
bank. International Journal of Computational Lin-
guistics and Chinese Language Processing, V.10 N.3. 
Yunfang Wu, Baobao Chang and Weidong Zhan. 2003. 
Building Chinese-English Bilingual Phrase Database. 
Page 41-45, Vol. 4. 
The 6th Workshop on Asian Languae Resources, 2008
23
The 6th Workshop on Asian Languae Resources, 2008
24
 
Applying Machine Learning to Chinese Temporal Relation Resolution 
 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong
kfwong@se.cuhk.edu.hk 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
csghcao@comp.polyu.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
 
 
Abstract 
Temporal relation resolution involves extraction 
of temporal information explicitly or implicitly 
embedded in a language. This information is of-
ten inferred from a variety of interactive gram-
matical and lexical cues, especially in Chinese. 
For this purpose, inter-clause relations (tempo-
ral or otherwise) in a multiple-clause sentence 
play an important role. In this paper, a computa-
tional model based on machine learning and 
heterogeneous collaborative bootstrapping is 
proposed for analyzing temporal relations in a 
Chinese multiple-clause sentence. The model 
makes use of the fact that events are represented 
in different temporal structures. It takes into ac-
count the effects of linguistic features such as 
tense/aspect, temporal connectives, and dis-
course structures. A set of experiments has been 
conducted to investigate how linguistic features 
could affect temporal relation resolution.  
 
1 Introduction 
In language studies, temporal information de-
scribes changes and time of changes expressed in a 
language. Such information is critical in many typi-
cal natural language processing (NLP) applications, 
e.g. language generation and machine translation, etc. 
Modeling temporal aspects of an event in a written 
text is more complex than capturing time in a physi-
cal time-stamped system. Event time may be speci-
fied explicitly in a sentence, e.g. ???? 1997??
????????? (They solved the traffic prob-
lem of the city in 1997)?; or it may be left implicit, to 
be recovered by readers from context. For example, 
one may know that ???????????????
?????? (after the street bridge had been built, 
they solved the traffic problem of the city)?, yet 
without knowing the exact time when the street 
bridge was built. As reported by Partee (Partee, 
1984), the expression of relative temporal relations 
in which precise times are not stated is common in 
natural language. The objective of relative temporal 
relation resolution is to determine the type of rela-
tive relation embedded in a sentence. 
In English, temporal expressions have been 
widely studied. Lascarides and Asher (Lascarides, 
Asher and Oberlander, 1992) suggested that tempo-
ral relations between two events followed from dis-
course structures. They investigated various 
contextual effects on five discourse relations 
(namely narration, elaboration, explanation, back-
ground and result) and then corresponded each of 
them to a kind of temporal relations. Hitzeman et al 
(Hitzeman, Moens and Grover, 1995) described a 
method for analyzing temporal structure of a dis-
course by taking into account the effects of tense, 
aspect, temporal adverbials and rhetorical relations 
(e.g. causation and elaboration) on temporal order-
ing. They argued that rhetorical relations could be 
further constrained by event temporal classification. 
Later, Dorr and Gaasterland (Dorr and Gaasterland, 
2002) developed a constraint-based approach to 
generate sentences, which reflect temporal relations, 
by making appropriate selections of tense, aspect 
and connecting words (e.g. before, after and when). 
Their works, however, are theoretical in nature and 
have not investigated computational aspects. 
The pioneer work on Chinese temporal relation 
extraction was first reported by Li and Wong (Li and 
Wong, 2002). To discover temporal relations em-
bedded in a sentence, they devised a set of simple 
rules to map the combined effects of temporal indi-
cators, which are gathered from different grammati-
cal categories, to their corresponding relations. 
However, their work did not focus on relative tem-
poral relations. Given a sentence describing two 
temporally related events, Li and Wong only took 
the temporal position words (including before, after 
and when, which serve as temporal connectives) and 
the tense/aspect markers of the second event into 
consideration. The proposed rule-based approach 
was simple; but it suffered from low coverage and 
was particularly ineffective when the interaction be-
tween the linguistic elements was unclear. 
This paper studies how linguistic features in Chi-
nese interact to influence relative relation resolution. 
For this purpose, statistics-based machine learning 
approaches are applied. The remainder of the paper 
is structured as follows: Section 2 summarizes the 
linguistic features, which must be taken into account 
in temporal relation resolution, and introduces how 
these features are expressed in Chinese. In Section 3, 
the proposed machine learning algorithms to identify 
temporal relations are outlined; furthermore, a het-
erogeneous collaborative bootstrapping technique 
for smoothing is presented. Experiments designed 
for studying the impact of different approaches and 
linguistic features are described in Section 4. Finally, 
Section 5 concludes the paper. 
2 Modeling Temporal Relations 
2.1 Temporal Relation Representations 
As the importance of temporal information proc-
essing has become apparent, a variety of temporal 
systems have been introduced, attempting to ac-
commodate the characteristics of relative temporal 
information. Among those who worked on temporal 
relation representations, many took the work of Rei-
chenbach (Reichenbach, 1947) as a starting point, 
while some others based their works on Allen?s (Al-
len, 1981). 
Reichenbach proposed a point-based temporal 
theory. This was later enhanced by Bruce who de-
fined seven relative temporal relations (Bruce. 1972). 
Given two durative events, the interval relations be-
tween them were modeled by the order between the 
greatest lower bounding points and least upper 
bounding points of the two events. In the other camp, 
instead of adopting time points, Allen took intervals 
as temporal primitives and introduced thirteen basic 
binary relations. In this interval-based theory, points 
are relegated to a subsidiary status as ?meeting 
places? of intervals. An extension to Allen?s theory, 
which treated both points and intervals as primitives 
on an equal footing, was later investigated by Ma 
and Knight (Ma and Knight, 1994). 
In natural language, events can either be punctual 
(e.g. ?? (explore)) or durative (e.g. ?? (built a 
house)) in nature. Thus Ma and Knight?s model is 
adopted in our work (see Figure 1). Taking the sen-
tence ????????????????????? 
(after the street bridge had been built, they solved 
the traffic problem of the city)? as an example, the 
relation held between building the bridge (i.e. an 
interval) and solving the problem (i.e. a point) is 
BEFORE. 
Figure 1. Thirteen temporal relations between points and 
intervals 
2.2 Linguistic Features for Determining Relative 
Relations 
Relative relations are generally determined by 
tense/aspect, connecting words (temporal or other-
wise) and event classes.  
Tense/Aspect in English is manifested by verb in-
flections. But such morphological variations are in-
applicable to Chinese verbs; instead, they are 
conveyed lexically (Li and Wong, 2002). In other 
words, tense and aspect in Chinese are expressed 
using a combination of time words, auxiliaries, tem-
poral position words, adverbs and prepositions, and 
particular verbs. 
Temporal Connectives in English primarily in-
volve conjunctions, e.g. after, before and when (Dorr 
and Gaasterland, 2002). They are key components in 
discourse structures. In Chinese, however, conjunc-
tions, conjunctive adverbs, prepositions and position 
words are required to represent connectives. A few 
verbs which express cause and effect also imply a 
forward movement of event time. The words, which 
contribute to the tense/aspect and temporal connec-
tive expressions, are explicit in a sentence and gen-
erally known as Temporal Indicators. 
Event Class is implicit in a sentence. Events can 
be classified according to their inherent temporal 
characteristics, such as the degree of telicity and/or 
atomicity (Li and Wong, 2002). The four widespread 
accepted temporal classes1 are state, process, punc-
tual event and developing event. Based on their 
classes, events interact with the tense/aspect of verbs 
to define the temporal relations between two events. 
Temporal indicators and event classes are together 
referred to as Linguistic Features (see Table 1). For 
example, linguistic features are underlined in the 
sentence ?(??)?????(??)????????
????? after/because the street bridge had been 
built (i.e. a developing event), they solved the traffic 
problem of the city (i.e. a punctual event)?. 
                                                          
1 Temporal classification refers to aspectual classification. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
Table 1 shows the mapping between a temporal 
indicator and its effects. Notice that the mapping is 
not one-to-one. For example, adverbs affect 
tense/aspect as well as discourse structure. For an-
other example, tense/aspect can be affected by auxil-
iary words, trend verbs, etc. This shows that 
classification of temporal indicators based on part-
of-speech (POS) information alone cannot determine 
relative temporal relations. 
3 Machine Learning Approaches for Relative 
Relation Resolution 
Previous efforts in corpus-based natural language 
processing have incorporated machine learning 
methods to coordinate multiple linguistic features 
for example in accent restoration (Yarowsky, 1994) 
and event classification (Siegel and McKeown, 
1998), etc. 
Relative relation resolution can be modeled as a 
relation classification task. We model the thirteen 
relative temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The resolution 
process is to assign an event pair (i.e. the two events 
under concern)2 to one class according to their lin-
guistic features. For this purpose, we train two clas-
sifiers, a Probabilistic Decision Tree Classifier 
(PDT) and a Na?ve Bayesian Classifier (NBC). We 
then combine the results by the Collaborative Boot-
strapping (CB) technique which is used to mediate 
the sparse data problem arose due to the limited 
number of training cases. 
                                                          
2 It is an object in machine learning algorithms. 
3.1 Probabilistic Decision Tree (PDT) 
Due to two domain-specific characteristics, we 
encounter some difficulties in classification. (a) Un-
known values are common, for many events are 
modified by less than three linguistic features. (b) 
Both training and testing data are noisy. For this rea-
son, it is impossible to obtain a tree which can com-
pletely classify all training examples. To overcome 
this predicament, we aim to obtain more adjusted 
probability distributions of event pairs over their 
possible classes. Therefore, a probabilistic decision 
tree approach is preferred over conventional deci-
sion tree approaches (e.g. C4.5, ID3). We adopt a 
non-incremental supervised learning algorithm in 
TDIDT (Top Down Induction of Decision Trees) 
family. It constructs a tree top-down and the process 
is guided by distributional information learned from 
examples (Quinlan, 1993). 
3.1.1 Parameter Estimation 
Based on probabilities, each object in the PDT ap-
proach can belong to a number of classes. These 
probabilities could be estimated from training cases 
with Maximum Likelihood Estimation (MLE). Let l 
be the decision sequence, z the object and c the class. 
The probability of z belonging to c is: 
?? ?=
ll
zlplcpzclpzcp )|()|()|,()|(  (1)
let nBBBl ...21= , by MLE we have: 
)(
),(
)|()|(
n
n
n Bf
Bcf
Bcplcp =?   (2)
),( nBcf  is the count of the items whose leaf nodes 
are Bn and belonging to class c. And 
Linguistic Feature Symbol POS Tag Effect Example 
With/Without 
punctuations 
PT Not Applica-
ble 
Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb 
objects 
VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing 
wish/hope 
VA TI_va Tense ??, ?, ? 
Verbs related to 
causality 
VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, 
Developing Event, 
Process 
Table 1. Linguistic features: eleven temporal indicators and one event class 
),...|(...
),,|(),|()|()|(
11
213121
zBBBp
zBBBpzBBpzBpzlp
nn ?
=
 
 
(3)
where 
)|...(
)|...(
),...|(
121
121
121 zBBBp
zBBBBp
zBBBBp
mm
mmm
mmm
??
??
?? =
)|...(
)|...(
121
121
zBBBf
zBBBBf
mm
mmm
??
??= , ( nm ,...,3,2= ).  
An object might traverse more than one decision 
path if it has unknown attribute values. 
)|...( 121 zBBBBf mmm ??  is the count of the item z, 
which owns the decision paths from B1 to Bm. 
3.1.2 Classification Attributes 
Objects are classified into classes based on their 
attributes. In the context of temporal relation resolu-
tion, how to categorize linguistic features into classi-
fication attributes is a major design issue. We extract 
all temporal indicators surrounding an event. As-
sume m and n are the anterior and posterior window 
size. They represent the numbers of the indicators 
BEFORE and AFTER respectively. Consider the 
most extreme case where an event consists of at 
most 4 temporal indicators before and 2 after. We 
set m and n to 4 and 2 initially. Experiments show 
that learning performance drops when m>4 and n>2 
and there is only very little difference otherwise (i.e. 
when m?4 and n?2).  
In addition to temporal indicators alone, the posi-
tion of the punctuation mark separating the two 
clauses describing the events and the classes of the 
events are also useful classification attributes.  We 
will outline why this is so in Section 4.1. Altogether, 
the following 15 attributes are used to train the PDT 
and NBC classifiers: 
,,),(,,,, 2
1
1
1
1
1
2
1
3
1
4
1 1
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITI  
2
2
1
2
1
2
2
2
3
2
4
2
,),(,,,,, / 2
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITIpuncwowi  
li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before 
and the jth indicator after the event ek (k=1,2). Given 
a sentence, for example, ?/TI_d ?/E0 ?/TI_u ??
/n ?/w ?/TI_d ?/E2 ?/TI_u ??/n ?/w, the at-
tribute vector could be represented as: [0, 0, 0, ?, 
E0, ?, 0, 1, 0, 0, 0, ?, E2, ?, 0]. 
3.1.3 Attribute Selection Function 
Many similar attribute selection functions were 
used to construct a decision tree (Marquez, 2000). 
These included information gain and information 
gain ratio (Quinlan, 1993), 2? Test and Symmetrical 
Tau (Zhou and Dillon, 1991). We adopt the one pro-
posed by Lopez de Mantaraz (Mantaras, 1991) for it 
shows more stable performance than Quinlan?s 
information gain ratio in our experiments. Compared 
with Quinlan?s information gain ratio, Lopez?s dis-
tance-based measurement is unbiased towards the 
attributes with a large number of values and is capa-
ble of generating smaller trees with no loss of accu-
racy (Marquez, Padro and Rodriguez, 2000). This 
characteristic makes it an ideal choice for our work, 
where most attributes have more than 200 values. 
3.2 Na?ve Bayesian Classifier (NBC) 
NBC assumes independence among features. 
Given the class label c, NBC learns from training 
data the conditional probability of each attribute Ai 
(see Section 3.1.2). Classification is then performed 
by applying Bayes rule to compute the probability of 
c given the particular instance of A1,?,An, and then 
predicting the class with the highest posterior prob-
ability ratio. 
),...,,,|(maxarg 321
*
n
c
AAAAcscorec =  (4)
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =  (5)
Apply Bayesian rule to (5), we have: 
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =
)()|,...,,,(
)()|,...,,,(
321
321
cpcAAAAp
cpcAAAAp
n
n=
)()|(
)()|(
1
1
cpcAp
cpcAp
n
i
i
n
i
i
?
?
=
=?  
 
 
 
(6)
)|( cAp i and )|( cAp i  are estimated by MLE from 
training data with Dirichlet Smoothing method: 
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (7)
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (8)
3.3 Collaborative Bootstrapping (CB) 
PDT and NB are both supervised learning ap-
proach. Thus, the training processes require many 
labeled cases. Recent results (Blum and Mitchell, 
1998; Collins, 1999) have suggested that unlabeled 
data could also be used effectively to reduce the 
amount of labeled data by taking advantage of col-
laborative bootstrapping (CB) techniques. In previ-
ous works, CB trained two homogeneous classifiers 
based on different independent feature spaces. How-
ever, this approach is not applicable to our work 
since only a few temporal indicators occur in each 
case. Therefore, we develop an alternative CB algo-
rithm, i.e. to train two different classifiers based on 
the same feature spaces. PDT (a non-linear classifier) 
and NBC (a linear classifier) are under consideration. 
This is inspired by Blum and Mitchell?s theory that 
two collaborative classifiers should be conditionally 
independent so that each classifier can make its own 
contribution (Blum and Mitchell, 1998). The learn-
ing steps are outlined in Figure 2. 
Inputs: A collection of the labeled cases and unla-
beled cases is prepared. The labeled cases 
are separated into three parts, training 
cases, test cases and held-out cases.  
Loop: While the breaking criteria is not satisfied 
1 Build the PDT and NBC classifiers us-
ing training cases 
2 Use PDT and NBC to classify the unla-
beled cases, and exchange with the se-
lected cases which have higher 
Classification Confidence (i.e. the un-
certainty is less than a threshold). 
3 Evaluate the PDT and NBC classifiers 
with the held-out cases. If the error rate 
increases or its reduction is below a 
threshold break the loop; else go to step 
1. 
Output: Use the optimal classifier to label the test 
cases 
Figure 2. Collaborative bootstrapping algorithm 
3.4 Classification Confidence Measurement 
Classification confidence is the metric used to 
measure the correctness of each labeled case auto-
matically (see Step 2 in Figure 2). The desirable 
metric should satisfy two principles:  
? It should be able to measure the uncertainty/ cer-
tainty of the output of the classifiers; and 
? It should be easy to calculate. 
We adopt entropy, i.e. an information theory 
based criterion, for this purpose. Let x be the classi-
fied object, and },...,,,{ 321 nccccC = the set of output. 
x is classified as ci with the probability 
)|( xcp i ni ,..,3,2,1= . The entropy of the output is 
then calculated as: 
?
=
?= n
i
ii xcpxcpxCe
1
)|(log)|()|(  (9)
Once )|( xcp i is known, the entropy can be deter-
mined. These parameters can be easily determined in 
PDT, as each incoming case is classified into each 
class with a probability. However, the incoming 
cases in NBC are grouped into one class which is 
assigned the highest score. We then have to estimate 
)|( xcp i  from those scores. Without loss of general-
ity, the probability is estimated as: 
?
=
= n
j
j
i
i
xcscore
xcscore
xcp
1
)|(
)|(
 )|(   (10)
where )|( xcscore i  is the ranking score of x be-
longing to ci. 
4 Experiment Setup and Evaluation 
Several experiments have been designed to evalu-
ate the proposed learning approaches and to reveal 
the impact of linguistic features on learning per-
formance. 700 sentences are extracted from Ta Kong 
Pao (a local Hong Kong Chinese newspaper) finan-
cial version. 600 cases are labeled manually and 100 
left unlabeled. Among those labeled, 400 are used as 
training data, 100 as test data and the rest as held-out 
data. 
4.1 Use of Linguistic Features As Classification 
Attributes 
The impact of a temporal indicator is determined 
by its position in a sentence. In PDT and NBC, we 
consider an indicator located in four positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Cases (2) 
and (3) are ambiguous. The positions of the temporal 
indicators are the same. But it is uncertain whether 
these indicators modify the first or the second event 
if there is no punctuation separating their roles. We 
introduce two methods, namely NA and SAP to 
check if the ambiguity affects the two learning ap-
proaches. 
N(atural) O(rder): the temporal indicators between 
the two events are extracted and compared accord-
ing to their occurrence in the sentences regardless 
which event they modify.  
S(eparate) A(uxiliary) and P(osition) words: we 
try to resolve the above ambiguity with the gram-
matical features of the indicators. In this method, 
we assume that an indicator modifies the first 
event if it is an auxiliary word (e.g. ?), a trend 
verb (e.g. ??) or a position word (e.g. ?); oth-
erwise it modifies the second event. 
Temporal indicators are either tense/aspect or con-
nectives (see Section 2.2). Intuitively, it seems that 
classification could be better achieved if connective 
features are isolated from tense/ aspect features, 
allowing like to be compared with like. Methods 
SC1 and SC2 are designed based on this assumption. 
Table 2 shows the effect the different classification 
methods. 
SC1 (Separate Connecting words 1): it separates 
conjunctions and verbs relating to causality from 
others. They are assumed to contribute to dis-
course structure (intra- or inter-sentence structure), 
and the others contribute to the tense/aspect ex-
pressions for each individual event. They are built 
into 2 separate attributes, one for each event. 
SC2 (Separate Connecting words 2): it is the same 
as SC1 except that it combines the connecting 
word pairs (i.e. as a single pattern) into one attrib-
ute. 
EC (Event Class): it takes event classes into con-
sideration. 
Accuracy Method PDT NBC 
NO 82.00% 81.00% 
SAP 82.20% 81.50% 
SAP +SC1 80.20% 78.00% 
SAP +SC2 81.70% 79.20% 
SAP +EC 85.70% 82.25% 
Table 2. Effect of encoding linguistic features in the dif-
ferent ways 
4.2 Impact of Individual Features 
From linguistic perspectives, 13 features (see Ta-
ble 1) are useful for relative relation resolution. To 
examine the impact of each individual feature, we 
feed a single linguistic feature to the PDT learning 
algorithm one at a time and study the accuracy of the 
resultant classifier. The experimental results are 
given in Table 3. It shows that event classes have 
greatest accuracy, followed by conjunctions in the 
second place, and adverbs in the third. 
Feature Accuracy Feature Accuracy
PT 50.5% VA 56.5% 
VS 54% C 62% 
VC 54% U 51.5% 
TR 50.5% T 57.2% 
P 52.2 % D 61.7% 
PS 58.7% EC 68.2% 
VS 51.2% None 50.5% 
Table 3. Impact of individual linguistic features 
4.3 Discussions 
Analysis of the results in Tables 2 and 3 reveals 
some linguistic insights: 
1. In a situation where temporal indicators appear 
between two events and there is no punctuation 
mark separating them, POS information help re-
duce the ambiguity. Compared with NO, SAP 
shows a slight improvement from 82% to 82.2%. 
But the improvement seems trivial and is not as 
good as our prediction. This might due to the 
small percent of such cases in the corpus. 
2. Separating conjunctions and verbs relating to 
causality from others is ineffective. This reveals 
the complexity of Chinese in connecting expres-
sions. It is because other words (such as adverbs, 
proposition and position words) also serve such 
a function. Meanwhile, experiments based on 
SC1 and SC2 suggest that the connecting ex-
pressions generally involve more than one word 
or phrase. Although the words in a connecting 
expression are separated in a sentence, the action 
is indeed interactive. It would be more useful to 
regard them as one attribute. 
3. The effect of event classification is striking. 
Taking this feature into account, the accuracies 
of both PDT and NB improved significantly. As 
a matter of fact, different event classes may in-
troduce different relations even if they are con-
strained by the same temporal indicators. 
4.4 Collaborative Bootstrapping 
Table 4 presents the evaluation results of the four 
different classification approaches. DM is the default 
model, which classifies all incoming cases as the 
most likely class. It is used as evaluation baseline. 
Compare with DM, PDT and NBC show improve-
ment in accuracy (i.e. above 60% improvement). 
And CB in turn outperforms PDT and NBC. This 
proves that using unlabeled data to boost the per-
formance of the two classifiers is effective. 
Accuracy Approach Close test Open test 
DM 50.50% 55.00% 
NBC 82.25% 72.00% 
PDT 85.70% 74.00% 
CB 88.70% 78.00% 
Table 4. Evaluation of NBC, PDT and CB approaches 
5 Conclusions 
Relative temporal relation resolution received 
growing attentions in recent years. It is important for 
many natural language processing applications, such 
as information extraction and machine translation. 
This topic, however, has not been well studied, es-
pecially in Chinese. In this paper, we propose a 
model for relative temporal relation resolution in 
Chinese. Our model combines linguistic knowledge 
and machine learning approaches. Two learning ap-
proaches, namely probabilistic decision tree (PDT) 
and naive Bayesian classifier (NBC) and 13 linguis-
tic features are employed. Due to the limited labeled 
cases, we also propose a collaborative bootstrapping 
technique to improve learning performance. The 
experimental results show that our approaches are 
encouraging. To our knowledge, this is the first at-
tempt of collaborative bootstrapping, which involves 
two heterogeneous classifiers, in NLP application. 
This lays down the main contribution of our research. 
In this pilot work, temporal indicators are selected 
based on linguistic knowledge. It is time-consuming 
and could be error-prone. This suggests two direc-
tions for future studies. We will try to automate or at 
least semi-automate feature selection process. An-
other future work worth investigating is temporal 
indicator clustering. There are two methods we 
could investigate, i.e. clustering the recognized indi-
cators which occur in training corpus according to 
co-occurrence information or grouping them into 
two semantic roles, one related to tense/aspect ex-
pressions and the other to connecting expressions 
between two events. 
 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and 
CUHK Strategic Grant (account number 4410001). 
 
References 
Allen J., 1981. An Interval-based Represent Action 
of Temporal Knowledge. In Proceedings of 7th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 221-226. Los Altos, CA. 
Blum, A. and Mitchell T., 1998. Combining Labeled 
and Unlabeled Data with Co-Training. In Proceed-
ings of the Eleventh Annual Conference on Com-
putational Learning Theory, Madison, Wisconsin, 
pages 92-100 
Bruce B., 1972. A Model for Temporal References 
and its Application in Question-Answering Pro-
gram. Artificial Intelligence, 3(1):1-25. 
Collins M. and Singer Y, 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 189-196. Uni-
versity of Maryland. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting 
Words from Temporal Expressions. (submitted to 
JAIR) 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of the 7th European 
Meeting of the Association for Computational 
Linguistics, pages 253-260. Dublin, Ireland.  
Lascarides A., Asher N. and Oberlander J., 1992. 
Inferring Discourse Relations in Context. In 
Proceedings of the 30th Meeting of the 
Association for Computational Linguistics, pages 
1-8, Newark, Del. 
Li W.J. and Wong K.F., 2002. A Word-based Ap-
proach for Modeling and Discovering Temporal 
Relations Embedded in Chinese Sentences, ACM 
Transaction on Asian Language Processing, 
1(3):173-206. 
Ma J. and Knight B., 1994. A General Temporal 
Theory. The Computer Journal, 37(2):114- 123. 
M?ntaras L., 1991. A Distance-based Attribute Se-
lection Measure for Decision Tree Induction. Ma-
chine Learning, 6(1): 81?92. 
M?rquez L., Padr? L. and Rodr?guez H., 2000. A 
Machine Learning Approach to POS Tagging. 
Machine Learning, 39(1):59-91. Kluwer Aca-
demic Publishers. 
Partee, B., 1984. Nominal and Temporal Anaphora. 
Linguistics and Philosophy, 7(3):287-324. 
Quinlan J., 1993. C4.5 Programs for Machine 
Learning. Morgan Kauman Press. 
Reichenbach H., 1947. Elements of Symbolic Logic. 
Berkeley CA, University of California Press.  
Siegel E. and McKeown K., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving 
Aspectual Classification and Revealing Linguistic 
Insights. Computational Linguistics, 26(4): 595-
627. 
Wiebe, J.M., O'Hara, T.P., Ohrstrom-Sandgren, T. 
and McKeever, K.J, 1998. An Empirical Approach 
to Temporal Reference Resolution. Journal of Ar-
tificial Intelligence Research, 9:247-293. 
Wong F., Li W., Yuan C., etc., 2002. Temporal Rep-
resentation and Classification in Chinese. Interna-
tional Journal of Computer Processing of Oriental 
Languages, 15(2):211-230. 
Yarowsky D., 1994. Decision Lists for Lexical Am-
biguity Resolution: Application to the Accent Res-
toration in Spanish and French. In Proceeding of 
the 32rd Annual Meeting of ACL, San Francisco, 
CA. 
Zhou X., Dillon T., 1991. A Statistical-heuristic Fea-
ture Selection Criterion for Decision Tree Induc-
tion. IEEE Transaction on Pattern Analysis and 
Machine Intelligence, 13(8): 834-841. 
 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 369?376,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Extractive Summarization using Inter- and Intra- Event Relevance 
 
Wenjie Li, Mingli Wu and Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University 
{cswjli,csmlwu,csluqin}@comp
.polyu.edu.hk 
Wei Xu and Chunfa Yuan 
Department of Computer Science and 
Technology, Tsinghua University 
{vivian00,cfyuan}@mail.ts
inghua.edu.cn 
 
 
 
Abstract 
Event-based summarization attempts to 
select and organize the sentences in a 
summary with respect to the events or 
the sub-events that the sentences de-
scribe. Each event has its own internal 
structure, and meanwhile often relates to 
other events semantically, temporally, 
spatially, causally or conditionally. In 
this paper, we define an event as one or 
more event terms along with the named 
entities associated, and present a novel 
approach to derive intra- and inter- event 
relevance using the information of inter-
nal association, semantic relatedness, 
distributional similarity and named en-
tity clustering. We then apply PageRank 
ranking algorithm to estimate the sig-
nificance of an event for inclusion in a 
summary from the event relevance de-
rived. Experiments on the DUC 2001 
test data shows that the relevance of the 
named entities involved in events 
achieves better result when their rele-
vance is derived from the event terms 
they associate. It also reveals that the 
topic-specific relevance from documents 
themselves outperforms the semantic 
relevance from a general purpose 
knowledge base like Word-Net. 
 
 
1. Introduction 
Extractive summarization selects sentences 
which contain the most salient concepts in 
documents. Two important issues with it are 
how the concepts are defined and what criteria 
should be used to judge the salience of the con-
cepts. Existing work has typically been based on 
techniques that extract key textual elements, 
such as keywords (also known as significant 
terms) as weighed by their tf*idf score, or con-
cepts (such as events or entities) with linguistic 
and/or statistical analysis. Then, sentences are 
selected according to either the important textual 
units they contain or certain types of inter-
sentence relations they hold.  
Event-based summarization which has e-
merged recently attempts to select and organize 
sentences in a summary with respect to events or 
sub-events that the sentences describe. With re-
gard to the concept of events, people do not 
have the same definition when introducing it in 
different domains. While traditional linguistics 
work on semantic theory of events and the se-
mantic structures of verbs, studies in 
information retrieval (IR) within topic detection 
and tracking framework look at events as 
narrowly defined topics which can be 
categorized or clustered as a set of related 
documents (TDT). IR events are broader (or to 
say complex) events in the sense that they may 
include happenings and their causes, 
consequences or even more extended effects. In 
the information extraction (IE) community, 
events are defined as the pre-specified and struc-
tured templates that relate an action to its 
participants, times, locations and other entities 
involved (MUC-7). IE defines what people call 
atomic events. Regardless of their distinct perspectives, peo-
ple all agree that events are collections of activi-
ties together with associated entities. To apply 
the concept of events in the context of text sum-
marization, we believe it is more appropriate to 
consider events at the sentence level, rather than 
at the document level. To avoid the complexity 
of deep semantic and syntactic processing, we 
complement the advantages of statistical 
techniques from the IR community and struc-
tured information provided by the IE community. 
369
 We propose to extract semi-structured events 
with shallow natural language processing (NLP) 
techniques and estimate their importance for 
inclusion in a summary with IR techniques. 
Though it is most likely that documents nar-
rate more than one similar or related event, most 
event-based summarization techniques reported 
so far explore the importance of the events inde-
pendently. Motivated by this observation, this 
paper addresses the task of event-relevance 
based summarization and explores what sorts of 
relevance make a contribution. To this end, we 
investigate intra-event relevance, that is action-
entity relevance, and inter-event relevance, that 
is event-event relevance. While intra-event rele-
vance is measured with frequencies of the asso-
ciated events and entities directly, inter-event 
relevance is derived indirectly from a general 
WordNet similarity utility, distributional simi-
larity in the documents to be summarized, 
named entity clustering and so on. Pagerank 
ranking algorithm is then applied to estimate the 
event importance for inclusion in a summary 
using the aforesaid relevance.  
The remainder of this paper is organized as 
follows. Section 2 introduces related work. Sec-
tions 3 introduces our proposed event-based 
summarization approaches which make use of 
intra- and inter- event relevance. Section 4 pre-
sents experiments and evaluates different ap-
proaches. Finally, Section 5 concludes the paper. 
2. Related Work 
Event-based summarization has been investi-
gated in recent research. It was first presented in 
(Daniel, Radev and Allison, 2003), who treated 
a news topic in multi-document summarization 
as a series of sub-events according to human 
understanding of the topic. They determined the 
degree of sentence relevance to each sub-event 
through human judgment and evaluated six ex-
tractive approaches. Their paper concluded that 
recognizing the sub-events that comprise a sin-
gle news event is essential for producing better 
summaries. However, it is difficult to automati-
cally break a news topic into sub-events.  
Later, atomic events were defined as the rela-
tionships between the important named entities 
(Filatova and Hatzivassiloglou, 2004), such as 
participants, locations and times (which are 
called relations) through the verbs or action 
nouns labeling the events themselves (which are 
called connectors). They evaluated sentences 
based on co-occurrence statistics of the named 
entity relations and the event connectors in-
volved. The proposed approach claimed to out-
perform conventional tf*idf approach. Appar-
ently, named entities are key elements in their 
model. However, the constraints defining events 
seemed quite stringent.  
The application of dependency parsing, 
anaphora and co-reference resolution in recog-
nizing events were presented involving NLP and 
IE techniques more or less (Yoshioka and Hara-
guchi, 2004), (Vanderwende, Banko and Mene-
zes, 2004) and (Leskovec, Grobelnik and Fral-
ing, 2004). Rather than pre-specifying events, 
these efforts extracted (verb)-(dependent rela-
tion)-(noun) triples as events and took the triples 
to form a graph merged by relations.  
As a matter of fact, events in documents are 
related in some ways. Judging whether the sen-
tences are salient or not and organizing them in 
a coherent summary can take advantage from 
event relevance. Unfortunately, this was ne-
glected in most previous work. Barzilay and La-
pata (2005) exploited the use of the distribu-
tional and referential information of discourse 
entities to improve summary coherence. While 
they captured text relatedness with entity transi-
tion sequences, i.e. entity-based summarization, 
we are particularly interested in relevance be-
tween events in event-based summarization. 
Extractive summarization requires ranking 
sentences with respect to their importance. 
Successfully used in Web-link analysis and 
more recently in text summarization, Google?s 
PageRank (Brin and Page, 1998) is one of the 
most popular ranking algorithms. It is a kind of 
graph-based ranking algorithm deciding on the 
importance of a node within a graph by taking 
into account the global information recursively 
computed from the entire graph, rather than re-
lying on only the local node-specific infor-
mation. A graph can be constructed by adding a 
node for each sentence, phrase or word. Edges 
between nodes are established using inter-
sentence similarity relations as a function of 
content overlap or grammatically relations be-
tween words or phrases.  
The application of PageRank in sentence ex-
traction was first reported in (Erkan and Radev, 
2004). The similarity between two sentence 
nodes according to their term vectors was used 
to generate links and define link strength. The 
same idea was followed and investigated exten-
370
 sively (Mihalcea, 2005). Yoshioka and Haragu-
chi (2004) went one step further toward event-
based summarization. Two sentences were 
linked if they shared similar events. When tested 
on TSC-3, the approach favoured longer sum-
maries. In contrast, the importance of the verbs 
and nouns constructing events was evaluated 
with PageRank as individual nodes aligned by 
their dependence relations (Vanderwende, 2004; 
Leskovec, 2004).  
Although we agree that the fabric of event 
constitutions constructed by their syntactic rela-
tions can help dig out the important events, we 
have two comments. First, not all verbs denote 
event happenings. Second, semantic similarity 
or relatedness between action words should be 
taken into account. 
3. Event-based Summarization 
3.1. Event Definition and Event Map 
Events can be broadly defined as ?Who did 
What to Whom When and Where?. Both lin-
guistic and empirical studies acknowledge that 
event arguments help characterize the effects of 
a verb?s event structure even though verbs or 
other words denoting event determine the se-
mantics of an event. In this paper, we choose 
verbs (such as ?elect?) and action nouns (such as 
?supervision?) as event terms that can character-
ize or partially characterize actions or incident 
occurrences. They roughly relate to ?did What?. 
One or more associated named entities are con-
sidered as what are denoted by linguists as event 
arguments. Four types of named entities are cur-
rently under the consideration. These are <Per-
son>, <Organization>, <Location> and <Date>. 
They convey the information of ?Who?, 
?Whom?, ?When? and ?Where?. A verb or an 
action noun is deemed as an event term only 
when it presents itself at least once between two 
named entities. 
Events are commonly related with one an-
other semantically, temporally, spatially, caus-
ally or conditionally, especially when the docu-
ments to be summarized are about the same or 
very similar topics. Therefore, all event terms 
and named entities involved can be explicitly 
connected or implicitly related and weave a 
document or a set of documents into an event 
fabric, i.e. an event graphical representation (see 
Figure 1). The nodes in the graph are of two 
types. Event terms (ET) are indicated by rectan-
gles and named entities (NE) are indicated by 
ellipses. They represent concepts rather than 
instances. Words in either their original form or 
morphological variations are represented with a 
single node in the graph regardless of how many 
times they appear in documents. We call this 
representation an event map, from which the 
most important concepts can be pick out in the 
summary. 
 
 
 
Figure 1 Sample sentences and their graphical representation 
 
 
The advantage of representing with separated 
action and entity nodes over simply combining 
them into one event or sentence node is to pro-
vide a convenient way for analyzing the rele-
vance among event terms and named entities 
either by their semantic or distributional similar-
ity. More importantly, this favors extraction of 
concepts and brings the conceptual compression 
available. 
We then integrate the strength of the connec-
tions between nodes into this graphical model in 
terms of the relevance defined from different 
perspectives. The relevance is indicated by 
),( ji nodenoder , where inode  and jnode  repre-
sent two nodes, and are either event terms ( iet ) 
or named entities ( jne ). Then, the significance 
of each node, indicated by )( inodew , is calcu-
<Organization> America Online </Organization> was to buy <Organization> 
Netscape </Organization> and forge a partnership with <Organization> Sun 
</Organization>, benefiting all three and giving technological independence 
from <Organization> Microsoft </Organization>. 
371
 lated with PageRank ranking algorithm. Sec-
tions 3.2 and 3.3 address the issues of deriving 
),( ji nodenoder  according to intra- or/and inter- 
event relevance and calculating )( inodew  in de-
tail. 
3.2 Intra- and Inter- Event Relevance 
We consider both intra-event and inter-event 
relevance for summarization. Intra-event rele-
vance measures how an action itself is associ-
ated with its associated arguments. It is indi-
cated as ),( NEETR  and ),( ETNER  in Table 1 
below. This is a kind of direct relevance as the 
connections between actions and arguments are 
established from the text surface directly. No 
inference or background knowledge is required. 
We consider that when the connection between 
an event term iet  and a named entity jne  is 
symmetry, then TNEETRETNER ),(),( = . Events 
are related as explained in Section 2. By means 
of inter-event relevance, we consider how an 
event term (or a named entity involved in an 
event) associate to another event term (or an-
other named entity involved in the same or dif-
ferent events) syntactically, semantically and 
distributionally. It is indicated by ),( ETETR or 
),( NENER in Table 1 and measures an indirect 
connection which is not explicit in the event 
map needing to be derived from the external 
resource or overall event distribution. 
 Event Term 
(ET) 
Named En-
tity (NE) 
Event Term (ET) ),( ETETR  ),( NEETR  
Named Entity (NE) ),( ETNER  ),( NENER
Table 1 Relevance Matrix 
The complete relevance matrix is: 
??
???
?=
),(),(
),(),(
NENERETNER
NEETRETETR
R  
The intra-event relevance ),( NEETR can be 
simply established by counting how many times 
iet  and jne  are associated, i.e.  
),(),( jijiDocument neetfreqneetr =  (E1) 
One way to measure the term relevance is to 
make use of a general language knowledge base, 
such as WordNet (Fellbaum 1998). Word-
Net::Similarity is a freely available software 
package that makes it possible to measure the 
semantic relatedness between a pair of concepts, 
or in our case event terms, based on WordNet 
(Pedersen, Patwardhan and Michelizzi, 2004). It 
supports three measures. The one we choose is 
the function lesk. 
),(),(),( jijijiWordNet etetlesketetsimilarityetetr ==
      (E2) 
Alternatively, term relevance can be meas-
ured according to their distributions in the speci-
fied documents. We believe that if two events 
are concerned with the same participants, occur 
at same location, or at the same time, these two 
events are interrelated with each other in some 
ways. This observation motivates us to try deriv-
ing event term relevance from the number of 
name entities they share. 
|)()(|),( jijiDocument etNEetNEetetr ?=  (E3) 
Where )( ietNE is the set of named entities iet  
associate. | | indicates the number of the ele-
ments in the set. The relevance of named entities 
can be derived in a similar way. 
|)()(|),( jijiDocument neETneETnener ?=  (E4) 
The relevance derived with (E3) and (E4) are 
indirect relevance. In previous work, a cluster-
ing algorithm, shown in Figure 2, has been pro-
posed (Xu et al 2006) to merge the named en-
tity that refer to the same person (such as 
Ranariddh, Prince Norodom Ranariddh and Presi-
dent Prince Norodom Ranariddh). It is used for 
co-reference resolution and aims at joining the 
same concept into a single node in the event 
map. The experimental result suggests that 
merging named entity improves performance in 
some extend but not evidently. When applying 
the same algorithm for clustering all four types 
of name entities in DUC data, we observe that 
the name entities in the same cluster do not al-
ways refer to the same objects, even when they 
are indeed related in some way. For example, 
?Mississippi? is a state in the southeast United 
States, while ?Mississippi River? is the second-
longest rever in the United States and flows 
through ?Mississippi?. 
Step1: Each name entity is represented by 
ikiii wwwne ...21= , where iw  is the ith 
word in it. The cluster it belongs to, in-
dicated by )( ineC , is initialled by 
ikii www ...21 itself.  
Step2: For each name entity  
           ikiii wwwne ...21=  
For each name entity 
372
 jljjj wwwne ...21= , if )( ineC  is a 
sub-string of )( jneC , then 
)()( ji neCneC = . 
Continue Step 2 until no change occurs. 
Figure 2 The algorithm proposed to merge the 
named entities 
Location Person Date Organization
Mississippi 
 
Professor Sir 
Richard 
Southwood 
first six 
months of 
last year 
Long Beach 
City Council 
Sir Richard 
Southwood 
San Jose City 
Council 
Mississippi 
River 
Richard 
Southwood 
last year 
City Council 
Table 2 Some results of the named entity 
merged 
It therefore provides a second way to measure 
named entity relevance based on the clusters 
found. It is actually a kind of measure of lexical 
similarity. 
??
?=
otherwise      ,0
cluster same in the are ,      ,1
),( jijiCluster
nene
nener
     (E5) 
In addition, the relevance of the named enti-
ties can be sometimes revealed by sentence con-
text. Take the following most frequently used 
sentence patterns as examples: 
 
Figure 3 The example patterns  
Considering that two neighbouring name enti-
ties in a sentence are usually relevant, the fol-
lowing window-based relevance is also experi-
mented with. 
??
?=
otherwise      ,0
size  windowspecified-pre a within are ,      1,
),(
ji
jiPattern
nene
nener
     (E6) 
3.3 Significance of Concepts 
The significance score, i.e. the weight 
)( inodew  of each inode , is then estimated recur-
sively with PageRank ranking algorithm which 
assigns the significance score to each node ac-
cording to the number of nodes connecting to it 
as well as the strength of their connections. The 
equation calculating )( inodew using PageRank 
of a certain inode  is shown as follows. 
)
),(
)(
...
),(
)(
...
),(
)(()1()(
1
1
ti
t
ji
j
i
i
nodenoder
nodew
nodenoder
nodew
nodenoder
nodewddnodew
+++
++?=
 (E7) 
In (E7), jnode ( tj ,...2,1= , ij ? ) are the 
nodes linking to inode . d is the factor used to 
avoid the limitation of loop in the map structure. 
It is set to 0.85 experimentally. The significance 
of each sentence to be included in the summary 
is then obtained from the significance of the 
events it contains. The sentences with higher 
significance are picked up into the summary as 
long as they are not exactly the same sentences. 
We are aware of the important roles of informa-
tion fusion and sentence compression in sum-
mary generation. However, the focus of this pa-
per is to evaluate event-based approaches in ex-
tracting the most important sentences. Concep-
tual extraction based on event relevance is our 
future direction. 
4. Experiments and Discussions 
To evaluate the event based summarization ap-
proaches proposed, we conduct a set of experi-
ments on 30 English document sets provide by 
the DUC 2001 multi-document summarization 
task. The documents are pre-processed with 
GATE to recognize the previously mentioned 
four types of name entities. On average, each set 
contains 10.3 documents, 602 sentences, 216 
event terms and 148.5 name entities. 
To evaluate the quality of the generated 
summaries, we choose an automatic summary 
evaluation metric ROUGE, which has been used 
in DUCs. ROUGE is a recall-based metric for 
fixed length summaries. It bases on N-gram co-
occurrence and compares the system generated 
summaries to human judges (Lin and Hovy, 
2003). For each DUC document set, the system 
creates a summary of 200 word length and pre-
sent three of the ROUGE metrics: ROUGE-1 
(unigram-based), ROUGE-2 (bigram-based), 
and ROUGE-W (based on longest common sub-
sequence weighed by the length) in the follow-
ing experiments and evaluations.  
We first evaluate the summaries generated 
based on ),( NEETR  itself. In the pre-evaluation 
experiments, we have observed that some fre-
<Person>, a-position-name of <Organization>, 
does something. 
<Person> and another <Person> do something. 
373
 quently occurring nouns, such as ?doctors? and 
?hospitals?, by themselves are not marked by 
general NE taggers. But they indicate persons, 
organizations or locations. We compare the 
ROUGE scores of adding frequent nouns or not 
to the set of named entities in Table 3. A noun is 
considered as a frequent noun when its fre-
quency is larger than 10. Roughly 5% improve-
ment is achieved when high frequent nouns are 
taken into the consideration. Hereafter, when we 
mention NE in latter experiments, the high fre-
quent nouns are included. 
),( NEETR  NE Without High 
Frequency Nouns 
NE With High 
Frequency Nouns
ROUGE-1 0.33320 0.34859 
ROUGE-2 0.06260 0.07157 
ROUGE-W 0.12965 0.13471 
Table 3 ROUGE scores using ),( NEETR  itself 
Table 4 below then presents the summariza-
tion results by using ),( ETETR  itself. It com-
pares two relevance derivation approaches, 
WordNetR  and DocumentR . The topic-specific rele-
vance derived from the documents to be summa-
rized outperforms the general purpose Word-Net 
relevance by about 4%. This result is reasonable 
as WordNet may introduce the word relatedness 
which is not necessary in the topic-specific 
documents. When we examine the relevance 
matrix from the event term pairs with the high-
est relevant, we find that the pairs, like ?abort? 
and ?confirm?, ?vote? and confirm?, do reflect 
semantics (antonymous) and associated (causal) 
relations to some degree.  
),( ETETR  Semantic Rele-
vance from 
Word-Net 
Topic-Specific 
Relevance from 
Documents 
ROUGE-1 0.32917 0.34178 
ROUGE-2 0.05737 0.06852 
ROUGE-W 0.11959 0.13262 
Table 4 ROUGE scores using ),( ETETR  itself 
Surprisingly, the best individual result is from 
document distributional similarity DocumentR  
),( NENE  in Table 5. Looking more closely, we 
conclude that compared to event terms, named 
entities are more representative of the docu-
ments in which they are included. In other words, 
event terms are more likely to be distributed 
around all the document sets, whereas named 
entities are more topic-specific and therefore 
cluster in a particular document set more. Ex-
amples of high related named entities in rele-
vance matrix are ?Andrew? and ?Florida?, 
?Louisiana? and ?Florida?. Although their rele-
vance is not as explicit as the same of event 
terms (their relevance is more contextual than 
semantic), we can still deduce that some events 
may happen in both Louisiana and Florida, or 
about Andrew in Florida. In addition, it also 
shows that the relevance we would have ex-
pected to be derived from patterns and clustering 
can also be discovered by ),( NENERDocument . 
The window size is set to 5 experimentally in 
window-based practice.  
),( NENER Relevance 
from 
Documents
Relevance 
from 
Clustering 
Relevance 
from Window-
based Context
ROUGE-1 0.35212 0.33561 0.34466 
ROUGE-2 0.07107 0.07286 0.07508 
ROUGE-W 0.13603 0.13109 0.13523 
Table 5 ROUGE scores using ),( NENER  itself 
Next, we evaluate the integration of 
),( NEETR , ),( ETETR  and ),( NENER . As 
DUC 2001 provides 4 different summary sizes 
for evaluation, it satisfies our desire to test the 
sensibility of the proposed event-based summa-
rization techniques to the length of summaries. 
While the previously presented results are 
evaluated on 200 word summaries, now we 
move to check the results in four different sizes, 
i.e. 50, 100, 200 and 400 words. The experi-
ments results show that the event-based ap-
proaches indeed prefer longer summaries. This 
is coincident with what we have hypothesized. 
For this set of experiments, we choose to inte-
grate the best method from each individual 
evaluation presented previously. It appears that 
using the named entities relevance which is de-
rived from the event terms gives the best 
ROUGE scores in almost all the summery sizes. 
Compared with the results provided in (Filatova 
and Hatzivassiloglou, 2004) whose average 
ROUGE-1 score is below 0.3 on the same data 
set, the significant improvement is revealed. Of 
course, we need to test on more data in the fu-
ture. 
),( NENER 50 100 200 400 
ROUGE-1 0.22383 0.28584 0.35212 0.41612
ROUGE-2 0.03376 0.05489 0.07107 0.10275
ROUGE-W 0.10203 0.11610 0.13603 0.13877
),( NEETR 50 100 200 400 
ROUGE-1 0.22224 0.27947 0.34859 0.41644
ROUGE-2 0.03310 0.05073 0.07157 0.10369
ROUGE-W 0.10229 0.11497 0.13471 0.13850
),( ETETR 50 100 200 400 
374
 ROUGE-1 0.20616 0.26923 0.34178 0.41201
ROUGE-2 0.02347 0.04575 0.06852 0.10263
ROUGE-W 0.09212 0.11081 0.13262 0.13742
),( NEETR + 
),( ETETR + 
),( NENER  
 
50 
 
100 
 
200 
 
400 
ROUGE-1 0.21311 0.27939 0.34630 0.41639
ROUGE-2 0.03068 0.05127 0.07057 0.10579
ROUGE-W 0.09532 0.11371 0.13416 0.13913
Table 6 ROUGE scores using complete R matrix 
and with different summary lengths 
As discussed in Section 3.2, the named enti-
ties in the same cluster may often be relevant but 
not always be co-referred. In the following last 
set of experiments, we evaluate the two ways to 
use the clustering results. One is to consider 
them as related as if they are in the same cluster 
and derive the NE-NE relevance with (E5). The 
other is to merge the entities in one cluster as 
one reprehensive named entity and then use it in 
ET-NE with (E1). The rationality of the former 
approach is validated. 
 Clustering is 
used to derive 
NE-NE 
Clustering is used to 
merge entities and 
then to derive ET-NE
ROUGE-1 0.34072 0.33006 
ROUGE-2 0.06727 0.06154 
ROUGE-W 0.13229 0.12845 
Table 7 ROUGE scores with regard to how to 
use the clustering information 
5. Conclusion 
In this paper, we propose to integrate event-
based approaches to extractive summarization. 
Both inter-event and intra-event relevance are 
investigated and PageRank algorithm is used to 
evaluate the significance of each concept (in-
cluding both event terms and named entities). 
The sentences containing more concepts and 
highest significance scores are chosen in the 
summary as long as they are not the same sen-
tences.  
To derive event relevance, we consider the 
associations at the syntactic, semantic and con-
textual levels. An important finding on the DUC 
2001 data set is that making use of named entity 
relevance derived from the event terms they as-
sociate with achieves the best result. The result 
of 0.35212 significantly outperforms the one 
reported in the closely related work whose aver-
age is below 0.3. We are interested in the issue 
of how to improve an event representation in 
order to build a more powerful event-based 
summarization system. This would be one of our 
future directions. We also want to see how con-
cepts rather than sentences are selected into the 
summary in order to develop a more flexible 
compression technique and to know what char-
acteristics of a document set is appropriate for 
applying event-based summarization techniques.  
 
Acknowledgements 
The work presented in this paper is supported 
partially by Research Grants Council on Hong 
Kong (reference number CERG PolyU5181/03E) 
and partially by National Natural Science Foun-
dation of China (reference number: NSFC 
60573186). 
 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL 2003, pp71-78. 
Christiane Fellbaum. 1998, WordNet: An Electronic 
Lexical Database. MIT Press. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive summarization. In Pro-
ceedings of ACL 2004 Workshop on Summariza-
tion, pp104-111.  
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of Docu-
ment Semantic Graphs for Document Summariza-
tion. In LinkKDD 2004.  
Lucy Vanderwende, Michele Banko and Arul Mene-
zes. 2004. Event-Centric Summary Generation. In 
Working Notes of DUC 2004. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information. In Working Notes 
of NTCIR-4, Tokyo. 
MUC-7. http://www-nlpir.nist.gov/related_projects/ 
muc/proceeings/ muc_7_toc.html 
Naomi Daniel, Dragomir Radev and Timothy Allison. 
2003. Sub-event based Multi-document Summari-
zation. In Proceedings of the HLT-NAACL 2003 
Workshop on Text Summarization, pp9-16. 
375
 Page Lawrence, Brin Sergey, Motwani Rajeev and 
Winograd Terry. 1998. The PageRank Citation 
Ranking: Bring Order to the Web. Technical Re-
port, Stanford University. 
Rada Mihalcea. 2005. Language Independent Extrac-
tive Summarization. ACL 2005 poster. 
Regina Barzilay and Michael Elhadad. 2005. Model-
ling Local Coherence: An Entity-based Approach. 
In Proceedings of ACL, pp141-148. 
TDT. http://projects.ldc.upenn.edu/TDT. 
Ted Pedersen, Siddharth Patwardhan and Jason 
Michelizzi. 2004. WordNet::Similarity ? Measur-
ing the Relatedness of Concepts. In Proceedings of 
AAAI, pp25-29. 
Wei Xu, Wenjie Li, Mingli Wu, Wei Li and Chunfa 
Yuan. 2006. Deriving Event Relevance from the 
Ontology Constructed with Formal Concept 
Analysis, in Proceedings of CiCling?06, pp480-
489. 
 
376
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 993?1000,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Phonetic-Based Approach to Chinese Chat Text Normalization 
 
Yunqing Xia, Kam-Fai Wong 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
{yqxia, kfwong}@se.cuhk.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
Chatting is a popular communication 
media on the Internet via ICQ, chat 
rooms, etc. Chat language is different 
from natural language due to its anoma-
lous and dynamic natures, which renders 
conventional NLP tools inapplicable. The 
dynamic problem is enormously trouble-
some because it makes static chat lan-
guage corpus outdated quickly in repre-
senting contemporary chat language. To 
address the dynamic problem, we pro-
pose the phonetic mapping models to 
present mappings between chat terms and 
standard words via phonetic transcrip-
tion, i.e. Chinese Pinyin in our case. Dif-
ferent from character mappings, the pho-
netic mappings can be constructed from 
available standard Chinese corpus. To 
perform the task of dynamic chat lan-
guage term normalization, we extend the 
source channel model by incorporating 
the phonetic mapping models. Experi-
mental results show that this method is 
effective and stable in normalizing dy-
namic chat language terms. 
1 Introduction 
Internet facilitates online chatting by providing 
ICQ, chat rooms, BBS, email, blogs, etc. Chat 
language becomes ubiquitous due to the rapid 
proliferation of Internet applications. Chat lan-
guage text appears frequently in chat logs of 
online education (Heard-White, 2004), customer 
relationship management (Gianforte, 2003), etc. 
On the other hand, wed-based chat rooms and 
BBS systems are often abused by solicitors of 
terrorism, pornography and crime (McCullagh, 
2004). Thus there is a social urgency to under-
stand online chat language text. 
Chat language is anomalous and dynamic. 
Many words in chat text are anomalous to natural 
language. Chat text comprises of ill-edited terms 
and anomalous writing styles. We refer chat 
terms to the anomalous words in chat text. The 
dynamic nature reflects that chat language 
changes more frequently than natural languages. 
For example, many popular chat terms used in 
last year have been discarded and replaced by 
new ones in this year. Details on these two fea-
tures are provided in Section 2.  
The anomalous nature of Chinese chat lan-
guage is investigated in (Xia et al, 2005). Pattern 
matching and SVM are proposed to recognize 
the ambiguous chat terms. Experiments show 
that F-1 measure of recognition reaches 87.1% 
with the biggest training set. However, it is also 
disclosed that quality of both methods drops sig-
nificantly when training set is older. The dy-
namic nature is investigated in (Xia et al, 
2006a), in which an error-driven approach is pro-
posed to detect chat terms in dynamic Chinese 
chat terms by combining standard Chinese cor-
pora and NIL corpus (Xia et al, 2006b). Lan-
guage texts in standard Chinese corpora are used 
as negative samples and chat text pieces in the 
NIL corpus as positive ones. The approach calcu-
lates confidence and entropy values for the input 
text. Then threshold values estimated from the 
training data are applied to identify chat terms. 
Performance equivalent to the methods in exis-
tence is achieved consistently. However, the is-
sue of normalization is addressed in their work. 
Dictionary based chat term normalization is not a 
good solution because the dictionary cannot 
cover new chat terms appearing in the dynamic 
chat language. 
In the early stage of this work, a method based 
on source channel model is implemented for chat 
term normalization. The problem we encounter is 
addressed as follows. To deal with the anoma-
lous nature, a chat language corpus is constructed 
with chat text collected from the Internet. How-
993
ever, the dynamic nature renders the static corpus 
outdated quickly in representing contemporary 
chat language. The dilemma is that timely chat 
language corpus is nearly impossible to obtain. 
The sparse data problem and dynamic problem 
become crucial in chat term normalization. We 
believe that some information beyond character 
should be discovered to help addressing these 
two problems.  
Observation on chat language text reveals that 
most Chinese chat terms are created via phonetic 
transcription, i.e. Chinese Pinyin in our case. A 
more exciting finding is that the phonetic map-
pings between standard Chinese words and chat 
terms remain stable in dynamic chat language. 
We are thus enlightened to make use of the pho-
netic mapping models, in stead of character map-
ping models, to design a normalization algorithm 
to translate chat terms to their standard counter-
parts. Different from the character mapping 
models constructed from chat language corpus, 
the phonetic mapping models are learned from a 
standard language corpus because they attempt to 
model mappings probabilities between any two 
Chinese characters in terms of phonetic tran-
scription. Now the sparse data problem can thus 
be appropriately addressed. To normalize the 
dynamic chat language text, we extend the 
source channel model by incorporating phonetic 
mapping models. We believe that the dynamic 
problem can be resolved effectively and robustly 
because the phonetic mapping models are stable.  
The remaining sections of this paper are or-
ganized as follows. In Section 2, features of chat 
language are analyzed with evidences. In Section 
3, we present methodology and problems of the 
source channel model approach to chat term 
normalization. In Section 4, we present defini-
tion, justification, formalization and parameter 
estimation for the phonetic mapping model. In 
Section 5, we present the extended source chan-
nel model that incorporates the phonetic mapping 
models. Experiments and results are presented in 
Section 6 as well as discussions and error analy-
sis. We conclude this paper in Section 7. 
2 Feature Analysis and Evidences 
Observation on NIL corpus discloses the anoma-
lous and dynamic features of chat language. 
2.1 Anomalous 
Chat language is explicitly anomalous in two 
aspects. Firstly, some chat terms are anomalous 
entries to standard dictionaries. For example, ??
?(here, jie4 li3)? is not a standard word in any 
contemporary Chinese dictionary while it is often 
used to replace ???(here, zhe4 li3)? in chat 
language. Secondly, some chat terms can be 
found in  standard dictionaries while their mean-
ings in chat language are anomalous to the dic-
tionaries. For example, ??(even, ou3)? is often 
used to replace ??(me, wo2)? in chat text. But 
the entry that ??? occupies in standard diction-
ary is used to describe even numbers. The latter 
case is constantly found in chat text, which 
makes chat text understanding fairly ambiguous 
because it is difficult to find out whether these 
terms are used as standard words or chat terms. 
2.2 Dynamic 
Chat text is deemed dynamic due to the fact that 
a large proportion of chat terms used in last year 
may become obsolete in this year. On the other 
hand, ample new chat terms are born. This fea-
ture is not as explicit as the anomalous nature. 
But it is as crucial. Observation on chat text in 
NIL corpus reveals that chat term set changes 
along with time very quickly. 
An empirical study is conducted on five chat 
text collections extracted from YESKY BBS sys-
tem (bbs.yesky.com) within different time peri-
ods, i.e. Jan. 2004, July 2004, Jan. 2005, July 
2005 and Jan. 2006. Chat terms in each collec-
tion are picked out by hand together with their 
frequencies so that five chat term sets are ob-
tained. The top 500 chat terms with biggest fre-
quencies in each set are selected to calculate re-
occurring rates of the earlier chat term sets on the 
later ones.  
Set Jul-04 Jan-05 Jul-05 Jan-06 Avg. 
Jan-04 0.882 0.823 0.769 0.706 0.795
Jul-04 - 0.885 0.805 0.749 0.813
Jan-05 - - 0.891 0.816 0.854
Jul-05 - - - 0.875 0.875
Table 1. Chat term re-occurring rates. The rows 
represent the earlier chat term sets and the col-
umns the later ones. 
The surprising finding in Table 1 is that 29.4% 
of chat terms are replaced with new ones within 
two years and about 18.5% within one year. The 
changing speed is much faster than that in stan-
dard language. This thus proves that chat text is 
dynamic indeed. The dynamic nature renders the 
static corpus outdated quickly. It poses a chal-
lenging issue on chat language processing.  
994
3 Source Channel Model and Problems 
The source channel model is implemented as 
baseline  method in this work for chat term nor-
malization. We brief its methodology and prob-
lems as follows. 
3.1 The Model 
The source channel model (SCM) is a successful 
statistical approach in speech recognition and 
machine translation (Brown, 1990). SCM is 
deemed applicable to chat term normalization 
due to similar task nature. In our case, SCM aims 
to find the character string niicC ,...,2,1}{ ==  that 
the given input chat text njitT ,...,2,1}{ ==  is most 
probably translated to, i.e. ii ct ? , as follows. 
)(
)()|(maxarg)|(maxarg?
Tp
CpCTpTCpC
CC
==     (1) 
Since )(Tp  is a constant for C , so C?  should 
also maximize )()|( CpCTp . Now )|( TCp  is 
decomposed into two components, i.e. chat term 
translation observation model )|( CTp  and lan-
guage model )(Cp . The two models can be both 
estimated with maximum likelihood method us-
ing the trigram model in NIL corpus.  
3.2 Problems 
Two problems are notable in applying SCM in 
chat term normalization. First, data sparseness 
problem is serious because timely chat language 
corpus is expensive thus small due to dynamic 
nature of chat language. NIL corpus contains 
only 12,112 pieces of chat text created in eight 
months, which is far from sufficient to train the 
chat term translation model. Second, training 
effectiveness is poor due to the dynamic nature. 
Trained on static chat text pieces, the SCM ap-
proach would perform poorly in processing chat 
text in the future. Robustness on dynamic chat 
text thus becomes a challenging issue in our re-
search.  
Updating the corpus with recent chat text con-
stantly is obviously not a good solution to the 
above problems. We need to find some informa-
tion beyond character to help addressing the 
sparse data problem and dynamic problem. For-
tunately, observation on chat terms provides us 
convincing evidence that the underlying phonetic 
mappings exist between most chat terms and 
their standard counterparts. The phonetic map-
pings are found promising in resolving the two 
problems.  
4 Phonetic Mapping Model 
4.1 Definition of Phonetic Mapping 
Phonetic mapping is the bridge that connects two 
Chinese characters via phonetic transcription, i.e. 
Chinese Pinyin in our case. For example, ??
???? ?? )56.0,,( jiezhe ?? is the phonetic mapping con-
necting ??(this, zhe4)? and ??(interrupt, jie4)?, 
in which ?zhe? and ?jie? are Chinese Pinyin for  
??? and ??? respectively. 0.56 is phonetic 
similarity between the two Chinese characters. 
Technically, the phonetic mappings can be con-
structed between any two Chinese characters 
within any Chinese corpus. In chat language, any 
Chinese character can be used in chat terms, and 
phonetic mappings are applied to connect chat 
terms to their standard counterparts. Different 
from the dynamic character mappings, the pho-
netic mappings can be produced with standard 
Chinese corpus before hand. They are thus stable 
over time.  
4.2 Justifications on Phonetic Assumption  
To make use of phonetic mappings in normaliza-
tion of chat language terms, an assumption must 
be made that chat terms are mainly formed via 
phonetic mappings. To justify the assumption, 
two questions must be answered. First, how 
many percent of chat terms are created via pho-
netic mappings? Second, why are the phonetic 
mapping models more stable than character map-
ping models in chat language? 
Mapping type Count Percentage 
Chinese word/phrase 9370 83.3% 
English capital 2119 7.9% 
Arabic number 1021 8.0% 
Other  1034 0.8% 
Table 2. Chat term distribution in terms of  map-
ping type. 
To answer the first question, we look into chat 
term distribution in terms of mapping type in 
Table 2. It is revealed that 99.2 percent of chat 
terms in NIL corpus fall into the first four pho-
netic mapping types that make use of phonetic 
mappings. In other words, 99.2 percent of chat 
terms can be represented by phonetic mappings. 
0.8% chat terms come from the OTHER type, 
emoticons for instance. The first question is un-
doubtedly answered with the above statistics.  
To answer the second question, an observation 
is conducted again on the five chat term sets de-
scribed in Section 2.2. We create phonetic map-
995
pings manually for the 500 chat terms in each 
set. Then five phonetic mapping sets are ob-
tained. They are in turn compared against the 
standard phonetic mapping set constructed with 
Chinese Gigaword. Percentage of phonetic map-
pings in each set covered by the standard set is 
presented in Table 3.  
Set Jan-04 Jul-04 Jan-05 Jul-05 Jan-06
percentage 98.7 99.3 98.9 99.3 99.1 
Table 3. Percentages of phonetic mappings in 
each set covered by standard set.  
By comparing Table 1 and Table 3, we find 
that phonetic mappings remain more stable than 
character mappings in chat language text. This 
finding is convincing to justify our intention to 
design effective and robust chat language nor-
malization method by introducing phonetic map-
pings to the source channel model. Note that 
about 1% loss in these percentages comes from 
chat terms that are not formed via phonetic map-
pings, emoticons for example. 
4.3 Formalism 
The phonetic mapping model is a five-tuple, i.e. 
>< )|(Pr),(),(,, CTCptTptCT pm , 
which comprises of chat term character T , stan-
dard counterpart character C , phonetic transcrip-
tion of T  and C , i.e. )(Tpt  and )(Cpt , and the 
mapping probability )|(Pr CTpm  that T  is 
mapped to C  via the  phonetic mapping ( ) CT CTCptTpt pm ??????? ?? )|(Pr),(),(  (hereafter briefed by 
CT M??? ). 
As they manage mappings between any two 
Chinese characters, the phonetic mapping models 
should be constructed with a standard language 
corpus. This results in two advantages. One, 
sparse data problem can be addressed appropri-
ately because standard language corpus is used. 
Two, the phonetic mapping models are as stable 
as standard language. In chat term normalization, 
when the phonetic mapping models are used to 
represent mappings between chat term characters 
and standard counterpart characters, the dynamic 
problem can be addressed in a robust manner.   
Differently, the character mapping model used 
in the SCM (see Section 3.1) connects two Chi-
nese characters directly. It is a three-tuple, i.e.  
>< )|(Pr,, CTCT cm , 
which comprises of chat term character T , stan-
dard counterpart character C  and the mapping 
probability )|(Pr CTcm  that T  is mapped to C  
via this character mapping. As they must be con-
structed from chat language training samples, the 
character mapping models suffer from data 
sparseness problem and dynamic problem.  
4.4 Parameter Estimation 
Two questions should be answered in parameter 
estimation. First, how are the phonetic mapping 
space constructed? Second, how are the phonetic 
mapping probabilities estimated?  
To construct the phonetic mapping models, we 
first extract all Chinese characters from standard 
Chinese corpus and use them to form candidate 
character mapping models. Then we generate 
phonetic transcription for the Chinese characters 
and calculate phonetic probability for each can-
didate character mapping model. We exclude 
those character mapping models holding zero 
probability. Finally, the character mapping mod-
els are converted to phonetic mapping models 
with phonetic transcription and phonetic prob-
ability incorporated.  
The phonetic probability is calculated by 
combining phonetic similarity and character fre-
quencies in standard language as follows.  
( )
( )? ?
?=
i iislc
slc
pm
AApsAfr
AApsAfr
AAob
),()(
),()(
),(Pr    (2) 
In Equation (2) }{ iA  is the character set in 
which each element iA  is similar to character A  
in terms of phonetic transcription. )(cfrslc  is a 
function returning frequency of given character 
c  in standard language corpus and ),( 21 ccps  
phonetic similarity between character 1c  and 2c . 
Phonetic similarity between two Chinese char-
acters is calculated based on Chinese Pinyin as 
follows.  
)))(()),(((        
)))(()),(((       
))(),((),(
ApyfinalApyfinalSim
ApyinitialApyinitialSim
ApyApySimAAps
?
=
=
     (3) 
In Equation (3) )(cpy  is a function that returns 
Chinese Pinyin of given character c , and 
)(xinitial  and )(xfinal  return initial (shengmu) 
and final (yunmu) of given Chinese Pinyin x   
respectively. For example, Chinese Pinyin for the 
Chinese character ??? is ?zhe?, in which ?zh? is 
initial and ?e? is final. When initial or final is 
996
empty for some Chinese characters, we only cal-
culate similarity of the existing parts.  
An algorithm for calculating similarity of ini-
tial pairs and final pairs is proposed in (Li et al, 
2003) based on letter matching. Problem of this 
algorithm is that it always assigns zero similarity 
to those pairs containing no common letter. For 
example, initial similarity between ?ch? and ?q? 
is set to zero with this algorithm. But in fact, 
pronunciations of the two initials are very close 
to each other in Chinese speech. So non-zero 
similarity values should be assigned to these spe-
cial pairs before hand (e.g., similarity between 
?ch? and ?q? is set to 0.8). The similarity values 
are agreed by some native Chinese speakers. 
Thus Li et al?s algorithm is extended to output a 
pre-defined similarity value before letter match-
ing is executed in the original algorithm. For ex-
ample, Pinyin similarity between ?chi? and ?qi? 
is calculated as follows.  
8.018.0),(),()( =?=?= iiSimqchSimchi,qiSim  
5 Extended Source Channel Model 
We extend the source channel model by inserting 
phonetic mapping models niimM ,...,2,1}{ ==  into 
equation (1), in which chat term character it  is 
mapped to standard character ic  via im , i.e. 
i
m
i ct i??? . The extended source channel model 
(XSCM) is mathematically addressed as follows. 
)(
)()|(),|(
maxarg    
),|(maxarg?
,
,
Tp
CpCMpCMTp
TMCpC
MC
MC
=
=
   (4) 
Since )(Tp  is a constant, C?  and M?  should 
also maximize )()|(),|( CpCMpCMTp . Now 
three components are involved in XSCM, i.e. 
chat term normalization observation model 
),|( CMTp , phonetic mapping model )|( CMp  
and language model )(Cp . 
Chat Term Normalization Observation 
Model.  We assume that mappings between chat 
terms and their standard Chinese counterparts are 
independent of each other. Thus chat term nor-
malization probability can be calculated as fol-
lows. 
?= i iii cmtpCMTp ),|(),|(              (5) 
The ),|( iii cmtp ?s are estimated using maxi-
mum likelihood estimation method with Chinese 
character trigram model in NIL corpus.  
Phonetic Mapping Model. We assume that the 
phonetic mapping models depend merely on the 
current observation. Thus the phonetic mapping 
probability is calculated as follows. 
?= i ii cmpCMp )|()|(                 (6) 
in which )|( ii cmp ?s are estimated with equation 
(2) and (3) using a standard Chinese corpus.  
Language Model.  The language model )(Cp ?s 
can be estimated using maximum likelihood es-
timation method with Chinese character trigram 
model on NIL corpus.  
In our implementation, Katz Backoff smooth-
ing technique (Katz, 1987) is used to handle the 
sparse data problem, and Viterbi algorithm is 
employed to find the optimal solution in XSCM.   
6 Evaluation 
6.1 Data Description 
Training Sets 
Two types of training data are used in our ex-
periments. We use news from Xinhua News 
Agency in LDC Chinese Gigaword v.2 
(CNGIGA) (Graf et al, 2005) as standard Chi-
nese corpus to construct phonetic mapping mod-
els because of its excellent coverage of standard 
Simplified Chinese. We use NIL corpus (Xia et 
al., 2006b) as chat language corpus. To evaluate 
our methods on size-varying training data, six 
chat language corpora are created based on NIL 
corpus. We select 6056 sentences from NIL cor-
pus randomly to make the first chat language 
corpus, i.e. C#1. In every next corpus, we add 
extra 1,211 random sentences. So 7,267 sen-
tences are contained in C#2, 8,478 in C#3, 9,689 
in C#4, 10,200 in C#5, and 12,113 in C#6.  
Test Sets 
Test sets are used to prove that chat language is 
dynamic and XSCM is effective and robust in 
normalizing dynamic chat language terms. Six 
time-varying test sets, i.e. T#1 ~ T#6, are created 
in our experiments. They contain chat language 
sentences posted from August 2005 to Jan 2006. 
We randomly extract 1,000 chat language sen-
tences posted in each month. So timestamp of the 
six test sets are in temporal order, in which time-
stamp of T#1 is the earliest and that of T#6 the 
newest.  
The normalized sentences are created by hand 
and used as standard normalization answers. 
997
6.2 Evaluation Criteria 
We evaluate two tasks in our experiments, i.e. 
recognition and normalization. In recognition, 
we use precision (p), recall (r) and f-1 measure 
(f) defined as follows.  
 2        
rp
rpf
zx
xr
yx
xp +
??=+=+=      (7) 
where x denotes the number of true positives, y 
the false positives and z the true negatives.  
For normalization, we use accuracy (a), which 
is commonly accepted by machine translation 
researchers as a standard evaluation criterion. 
Every output of the normalization methods is 
compared to the standard answer so that nor-
malization accuracy on each test set is produced.  
6.3 Experiment I: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
In this experiment we investigate on quality of 
XSCM and SCM using same size-varying train-
ing data. We intend to prove that chat language is 
dynamic and phonetic mapping models used in 
XSCM are helpful in addressing the dynamic 
problem. As no standard Chinese corpus is used 
in this experiment, we use standard Chinese text 
in chat language corpora to construct phonetic 
mapping models in XSCM. This violates the ba-
sic assumption that the phonetic mapping models 
should be constructed with standard Chinese 
corpus. So results in this experiment should be 
used only for comparison purpose. It would be 
unfair to make any conclusion on general per-
formance of XSCM method based on results in 
this experiments.   
We train the two methods with each of the six 
chat language corpora, i.e. C#1 ~ C#6 and test 
them on six time-varying test sets, i.e. T#1 ~ T#6. 
F-1 measure values produced by SCM and 
XSCM in this experiment are present in Table 3.  
Three tendencies should be pointed out ac-
cording to Table 3. The first tendency is that f-1 
measure in both methods drops on time-varying 
test sets (see Figure 1) using same training chat 
language corpora. For example, both SCM and 
XSCM perform best on the earliest test set T#1 
and worst on newest T#4. We find that the qual-
ity drop is caused by the dynamic nature of chat 
language. It is thus revealed that chat language is 
indeed dynamic. We also find that quality of 
XSCM drops less than that of SCM. This proves 
that phonetic mapping models used in XSCM are 
helpful in addressing the dynamic problem. 
However, quality of XSCM in this experiment 
still drops by 0.05 on the six time-varying test 
sets. This is because chat language text corpus is 
used as standard language corpus to model the 
phonetic mappings. Phonetic mapping models 
constructed with chat language corpus are far 
from sufficient. We will investigate in Experi-
ment-II to prove that stable phonetic mapping 
models can be constructed with real standard 
language corpus, i.e. CNGIGA.  
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.829 0.805 0.762 0.701 0.739 0.705
C#2 0.831 0.807 0.767 0.711 0.745 0.715
C#3 0.834 0.811 0.774 0.722 0.751 0.722
C#4 0.835 0.814 0.779 0.729 0.753 0.729
C#5 0.838 0.816 0.784 0.737 0.761 0.737
S
C
M
C#6 0.839 0.819 0.789 0.743 0.765 0.743
C#1 0.849 0.840 0.820 0.790 0.805 0.790
C#2 0.850 0.841 0.824 0.798 0.809 0.796
C#3 0.850 0.843 0.824 0.797 0.815 0.800
C#4 0.851 0.844 0.829 0.805 0.819 0.805
C#5 0.852 0.846 0.833 0.811 0.823 0.811
X
S
C
M
C#6 0.854 0.849 0.837 0.816 0.827 0.816
Table 3. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora. 
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
 
Figure 1. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora. 
The second tendency is f-1 measure of both 
methods on same test sets drops when trained 
with size-varying chat language corpora. For ex-
ample, both SCM and XSCM perform best on 
the largest training chat language corpus C#6 and 
worst on the smallest corpus C#1. This tendency 
reveals that both methods favor bigger training 
chat language corpus. So extending the chat lan-
guage corpus should be one choice to improve 
quality of chat language term normalization.  
The last tendency is found on quality gap be-
tween SCM and XSCM. We calculate f-1 meas-
ure gaps between two methods using same train-
ing sets on same test sets (see Figure 2). Then the 
tendency is made clear. Quality gap between 
SCM and XSCM becomes bigger when test set 
998
becomes newer. On the oldest test set T#1, the 
gap is smallest, while on the newest test set T#6, 
the gap reaches biggest value, i.e. around 0.09. 
This tendency reveals excellent capability of 
XSCM in addressing dynamic problem using the 
phonetic mapping models.  
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
T#1 T#2 T#3 T#4 T#5 T#6
C#1
C#2
C#3
C#4
C#5
C#6
 
Figure 2. Tendency on f-1 measure gap in SCM 
and XSCM on six test sets with six chat language 
corpora. 
6.4 Experiment II: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
and CNGIGA 
In this experiment we investigate on quality of 
SCM and XSCM when a real standard Chinese 
language corpus is incorporated. We want to 
prove that the dynamic problem can be addressed 
effectively and robustly when CNGIGA is used 
as standard Chinese corpus.  
We train the two methods on CNGIGA and 
each of the six chat language corpora, i.e. C#1 ~ 
C#6. We then test the two methods on six time-
varying test sets, i.e. T#1 ~ T#6. F-1 measure 
values produced by SCM and XSCM in this ex-
periment are present in Table 4. 
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.849 0.840 0.820 0.790 0.735 0.703
C#2 0.850 0.841 0.824 0.798 0.743 0.714
C#3 0.850 0.843 0.824 0.797 0.747 0.720
C#4 0.851 0.844 0.829 0.805 0.748 0.727
C#5 0.852 0.846 0.833 0.811 0.758 0.734
S 
C 
M 
C#6 0.854 0.849 0.837 0.816 0.763 0.740
C#1 0.880 0.878 0.883 0.878 0.881 0.878
C#2 0.883 0.883 0.888 0.882 0.884 0.880
C#3 0.885 0.885 0.890 0.884 0.887 0.883
C#4 0.890 0.888 0.893 0.888 0.893 0.887
C#5 0.893 0.892 0.897 0.892 0.897 0.892
X 
S 
C 
M 
C#6 0.898 0.896 0.900 0.897 0.901 0.896
Table 4. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora and 
CNGIGA. 
Three observations are conducted on our re-
sults. First, according to Table 4, f-1 measure of 
SCM with same training chat language corpora 
drops on time-varying test sets, but XSCM pro-
duces much better f-1 measure consistently using 
CNGIGA and same training chat language cor-
pora (see Figure 3). This proves that phonetic 
mapping models are helpful in XSCM method. 
The phonetic mapping models contribute in two 
aspects. On the one hand, they improve quality 
of chat term normalization on individual test sets. 
On the other hand, satisfactory robustness is 
achieved consistently.  
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
`
 
Figure 3. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora and CNGIGA. 
The second observation is conducted on pho-
netic mapping models constructed with 
CNGIGA. We find that 4,056,766 phonetic map-
ping models are constructed in this experiment, 
while only 1,303,227 models are constructed 
with NIL corpus in Experiment I. This reveals 
that coverage of standard Chinese corpus is cru-
cial to phonetic mapping modeling. We then 
compare two character lists constructed with two 
corpora. The 100 characters most frequently used 
in NIL corpus are rather different from those ex-
tracted from CNGIGA. We can conclude that 
phonetic mapping models should be constructed 
with a sound corpus that can represent standard 
language.  
The last observation is conducted on f-1 meas-
ure achieved by same methods on same test sets 
using size-varying training chat language corpora. 
Both methods produce best f-1 measure with big-
gest training chat language corpus C#6 on same 
test sets. This again proves that bigger  training 
chat language corpus could be helpful to improve 
quality of chat language term normalization. One 
question might be asked whether quality of 
XSCM converges on size of the training chat 
language corpus. This question remains open due 
to limited chat language corpus available to us.  
6.5 Error Analysis 
Typical errors in our experiments belong mainly 
to the following two types.  
999
Err.1 Ambiguous chat terms 
Example-1: ??? 8?  
In this example, XSCM finds no chat term 
while the correct normalization answer is ???
??? (I still don?t understand)?. Error illus-
trated in Example-1 occurs when chat terms 
?8(eight, ba1)? and ??(meter, mi3)? appear in a 
chat sentence together. In chat language, ??? in 
some cases is used to replace ??(understand, 
ming2)?, while in other cases, it is used to repre-
sent a unit for length, i.e. meter. When number 
?8? appears before ???, it is difficult to tell 
whether they are chat terms within sentential 
context. In our experiments, 93 similar errors 
occurred. We believe this type of errors can be 
addressed within discoursal context.  
Err.2 Chat terms created in manners other 
than phonetic mapping 
Example-2: ?? ing    
In this example, XSCM does not recognize 
?ing? while the correct answer is ?(??)?? 
(I?m worrying)?. This is because chat terms cre-
ated in manners other than phonetic mapping are 
excluded by the phonetic assumption in XSCM 
method. Around 1% chat terms fall out of pho-
netic mapping types. Besides chat terms holding 
same form as showed in Example-2, we find that 
emoticon is another major exception type. Fortu-
nately, dictionary-based method is powerful 
enough to handle the exceptions. So, in a real 
system, the exceptions are handled by an extra 
component.  
7 Conclusions 
To address the sparse data problem and dynamic 
problem in Chinese chat text normalization, the 
phonetic mapping models are proposed in this 
paper to represent mappings between chat terms 
and standard words. Different from character 
mappings, the phonetic mappings are constructed 
from available standard Chinese corpus. We ex-
tend the source channel model by incorporating 
the phonetic mapping models. Three conclusions 
can be made according to our experiments. 
Firstly, XSCM outperforms SCM with same 
training data. Secondly, XSCM produces higher 
performance consistently on time-varying test 
sets.  Thirdly, both SCM and XSCM perform 
best with biggest training chat language corpus.  
Some questions remain open to us regarding 
optimal size of training chat language corpus in 
XSCM.  Does the optimal size exist? Then what 
is it? These questions will be addressed in our 
future work. Moreover, bigger context will be 
considered in chat term normalization, discourse 
for instance.  
Acknowledgement 
Research described in this paper is partially sup-
ported by the Chinese University of Hong Kong 
under the Direct Grant Scheme project 
(2050330) and Strategic Grant Scheme project 
(4410001). 
References 
Brown, P. F., J. Cocke, S. A. D. Pietra, V. J. D. Pietra, 
F. Jelinek, J. D. Lafferty, R. L. Mercer and P. S. 
Roossin. 1990.  A statistical approach to machine 
translation. Computational Linguistics, v.16 n.2, 
p.79-85. 
Gianforte, G.. 2003. From Call Center to Contact 
Center: How to Successfully Blend Phone, Email, 
Web and Chat to Deliver Great Service and Slash 
Costs. RightNow Technologies. 
Graf, D., K. Chen, J.Kong and K. Maeda. 2005. Chi-
nese Gigaword Second Edition. LDC Catalog 
Number LDC2005T14. 
Heard-White, M., Gunter Saunders and Anita Pincas. 
2004.  Report into the use of CHAT in education. 
Final report for project of Effective use of CHAT 
in Online Learning, Institute of Education, Univer-
sity of London. 
James, F.. 2000. Modified Kneser-Ney Smoothing of 
n-gram Models. RIACS Technical Report 00.07. 
Katz, S. M.. Estimation of probabilities from sparse 
data for the language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech and Signal Processing, 35(3):400-401. 
Li, H., W. He and B. Yuan. 2003. An Kind of Chinese 
Text Strings' Similarity and its Application in 
Speech Recognition. Journal of Chinese Informa-
tion Processing, 2003 Vol.17 No.1 P.60-64.  
McCullagh, D.. 2004. Security officials to spy on chat 
rooms. News provided by CNET Networks. No-
vember 24, 2004. 
Xia, Y., K.-F. Wong and W. Gao. 2005. NIL is not 
Nothing: Recognition of Chinese Network Infor-
mal Language Expressions. 4th SIGHAN Work-
shop at IJCNLP'05, pp.95-102. 
Xia, Y. and K.-F. Wong. 2006a. Anomaly Detecting 
within Dynamic Chinese Chat Text. EACL?06 
NEW TEXT workshop, pp.48-55.  
Xia, Y., K.-F. Wong and W. Li. 2006b. Constructing 
A Chinese Chat Text Corpus with A Two-Stage 
Incremental Annotation Approach. LREC?06. 
1000
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89?92,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Novel Feature-based Approach to Chinese Entity Relation Extraction
 
Wenjie Li1, Peng Zhang1,2, Furu Wei1, Yuexian Hou2 and Qin Lu1
1Department of Computing 2School of Computer Science and Technology
The Hong Kong Polytechnic University, Hong Kong Tianjin University, China 
{cswjli,csfwei,csluqin}@comp.polyu.edu.hk {pzhang,yxhou}@tju.edu.cn 
 
 
Abstract 
Relation extraction is the task of finding 
semantic relations between two entities from 
text. In this paper, we propose a novel 
feature-based Chinese relation extraction 
approach that explicitly defines and explores 
nine positional structures between two entities. 
We also suggest some correction and inference 
mechanisms based on relation hierarchy and 
co-reference information etc. The approach is 
effective when evaluated on the ACE 2005 
Chinese data set. 
1 Introduction 
Relation extraction is promoted by the ACE program. 
It is the task of finding predefined semantic relations 
between two entities from text. For example, the 
sentence ?Bill Gates is the chairman and chief 
software architect of Microsoft Corporation? conveys 
the ACE-style relation ?ORG-AFFILIATION? 
between the two entities ?Bill Gates (PER)? and 
?Microsoft Corporation (ORG)?.  
The task of relation extraction has been extensively 
studied in English over the past years. It is typically 
cast as a classification problem. Existing approaches 
include feature-based and kernel-based classification. 
Feature-based approaches transform the context of 
two entities into a liner vector of carefully selected 
linguistic features, varying from entity semantic 
information to lexical and syntactic features of the 
context. Kernel-based approaches, on the other hand, 
explore structured representation such as parse tree 
and dependency tree and directly compute the 
similarity between trees. Comparably, feature-based 
approaches are easier to implement and achieve much 
success. 
In contrast to the significant achievements 
concerning English and other Western languages, 
research progress in Chinese relation extraction is 
quite limited. This may be attributed to the different 
characteristic of Chinese language, e.g. no word 
boundaries and lack of morphologic variations, etc. In 
this paper, we propose a character-based Chinese 
entity relation extraction approach that complements 
entity context (both internal and external) character 
N-grams with four word lists extracted from a 
published Chinese dictionary. In addition to entity 
semantic information, we define and examine nine 
positional structures between two entities. To cope 
with the data sparseness problem, we also suggest 
some correction and inference mechanisms according 
to the given ACE relation hierarchy and co-reference 
information. Experiments on the ACE 2005 data set 
show that the positional structure feature can provide 
stronger support for Chinese relation extraction. 
Meanwhile, it can be captured with less effort than 
applying deep natural language processing. But 
unfortunately, entity co-reference does not help as 
much as we have expected. The lack of necessary 
co-referenced mentions might be the main reason. 
2 Related Work 
Many approaches have been proposed in the literature 
of relation extraction. Among them, feature-based and 
kernel-based approaches are most popular. 
Kernel-based approaches exploit the structure of 
the tree that connects two entities. Zelenko et al(2003) 
proposed a kernel over two parse trees, which 
recursively matched nodes from roots to leaves in a 
top-down manner. Culotta and Sorensen (2004) 
extended this work to estimate similarity between 
augmented dependency trees. The above two work 
was further advanced by Bunescu and Mooney (2005) 
who argued that the information to extract a relation 
between two entities can be typically captured by the 
shortest path between them in the dependency graph. 
Later, Zhang et al(2006) developed a composite 
kernel that combined parse tree kernel with entity 
kernel and Zhou et al(2007) experimented with a 
context-sensitive kernel by automatically determining 
context-sensitive tree spans.  
In the feature-based framework, Kambhatla (2004) 
employed ME models to combine diverse lexical, 
syntactic and semantic features derived from word, 
entity type, mention level, overlap, dependency and 
parse tree. Based on his work, Zhou et al(2005) 
89
further incorporated the base phrase chunking 
information and semi-automatically collected country 
name list and personal relative trigger word list. Jiang 
and Zhai (2007) then systematically explored a large 
space of features and evaluated the effectiveness of 
different feature subspaces corresponding to sequence, 
syntactic parse tree and dependency parse tree. Their 
experiments showed that using only the basic unit 
features within each feature subspace can already 
achieve state-of-art performance, while over-inclusion 
of complex features might hurt the performance. 
Previous approaches mainly focused on English 
relations. Most of them were evaluated on the ACE 
2004 data set (or a sub set of it) which defined 7 
relation types and 23 subtypes. Although Chinese 
processing is of the same importance as English and 
other Western language processing, unfortunately few 
work has been published on Chinese relation 
extraction. Che et al(2005) defined an improved edit 
distance kernel over the original Chinese string 
representation around particular entities. The only 
relation they studied is PERSON-AFFLIATION. The 
insufficient study in Chinese relation extraction drives 
us to investigate how to find an approach that is 
particularly appropriate for Chinese. 
3 A Chinese Relation Extraction Model 
Due to the aforementioned reasons, entity relation 
extraction in Chinese is more challenging than in 
English. The system segmented words are already not 
error free, saying nothing of the quality of the 
generated parse trees. All these errors will 
undoubtedly propagate to the subsequent processing, 
such as relation extraction. It is therefore reasonable to 
conclude that kernel-based especially tree-kernel 
approaches are not suitable for Chinese, at least at 
current stage. In this paper, we study a feature-based 
approach that basically integrates entity related 
information with context information. 
3.1 Classification Features  
The classification is based on the following four types 
of features. 
z Entity Positional Structure Features  
We define and examine nine finer positional 
structures between two entities (see Appendix). They 
can be merged into three coarser structures. 
z Entity Features 
Entity types and subtypes are concerned.  
z Entity Context Features 
These are character-based features. We consider 
both internal and external context. Internal context 
includes the characters inside two entities and the 
characters inside the heads of two entities. External 
context involves the characters around two entities 
within a given window size (it is set to 4 in this study). 
All the internal and external context characters are 
transformed to Uni-grams and Bi-grams. 
z Word List Features 
Although Uni-grams and Bi-grams should be able 
to cover most of Chinese words given sufficient 
training data, many discriminative words might not be 
discovered by classifiers due to the severe sparseness 
problem of Bi-grams. We complement character- 
based context features with four word lists which are 
extracted from a published Chinese dictionary. The 
word lists include 165 prepositions, 105 orientations, 
20 auxiliaries and 25 conjunctions. 
3.2 Correction with Relation/Argument 
Constraints and Type/Subtype Consistency Check 
An identified relation is said to be correct only when 
its type/subtype (R) is correct and at the same time its 
two arguments (ARG-1 and ARG-2) must be of the 
correct entity types/subtypes and of the correct order. 
One way to improve the previous feature-based 
classification approach is to make use of the prior 
knowledge of the task to find and rectify the incorrect 
results. Table 1 illustrates the examples of possible 
relations between PER and ORG. We regard possible 
relations between two particular types of entity 
arguments as constraints. Some relations are 
symmetrical for two arguments, such as PER_ 
SOCIAL.FAMILY, but others not, such as ORG_AFF. 
EMPLOYMENT. Argument orders are important for 
asymmetrical relations.  
 PER ORG 
PER PER_SOCIAL.BUS, PER_SOCIAL.FAMILY, ? 
ORG_AFF.EMPLOYMENT, 
 ORG_AFF.OWNERSHIP, ? 
ORG  PART_WHOLE.SUBSIDIARY, ORG_AFF.INVESTOR/SHARE, ?
Table 1 Possible Relations between ARG-1 and ARG-2 
Since our classifiers are trained on relations instead 
of arguments, we simply select the first (as in adjacent 
and separate structures) and outer (as in nested 
structures) as the first argument. This setting works at 
most of cases, but still fails sometimes. The correction 
works in this way. Given two entities, if the identified 
type/subtype is an impossible one, it is revised to 
NONE (it means no relation at all). If the identified 
type/subtype is possible, but the order of arguments 
does not consist with the given relation definition, the 
order of arguments is adjusted.  
Another source of incorrect results is the 
inconsistency between the identified types and 
subtypes, since they are typically classified separately. 
90
This type of errors can be checked against the 
provided hierarchy of relations, such as the subtypes 
OWNERSHIP and EMPLOYMENT must belong to 
the ORG_AFF type. There are existing strategies to 
deal with this problem, such as strictly bottom-up (i.e. 
use the identified subtype to choose the type it belongs 
to), guiding top-down (i.e. to classify types first and 
then subtypes under a certain type). However, these 
two strategies lack of interaction between the two 
classification levels. To insure consistency in an 
interactive manner, we rank the first n numbers of the 
most likely classified types and then check them 
against the classified subtype one by one until the 
subtype conforms to a type. The matched type is 
selected as the result. If the last type still fails, both 
type and subtype are revised to NONE. We call this 
strategy type selection. Alternatively, we can choose 
the most likely classified subtypes, and check them 
with the classified type (i.e. subtype selection 
strategy). Currently, n is 2. 
3.2 Inference with Co-reference Information and 
Linguistic Patterns 
Each entity can be mentioned in different places in 
text. Two mentions are said to be co-referenced to one 
entity if they refers to the same entity in the world 
though they may have different surface expressions. 
For example, both ?he? and ?Gates? may refer to ?Bill 
Gates of Microsoft?. If a relation ?ORG- 
AFFILIATION? is held between ?Bill Gates? and 
?Microsoft?, it must be also held between ?he? and 
?Microsoft?. Formally, given two entities E1={EM11, 
EM12, ?, EM1n} and E2={EM21, EM22, ?, EM2m} (Ei 
is an entity, EMij is a mention of Ei), it is true that 
R(EM11, EM21)? R(EM1l, EM2k). This nature allows 
us to infer more relations which may not be identified 
by classifiers.  
Our previous experiments show that the 
performance of the nested and the adjacent relations is 
much better than the performance of other structured 
relations which suffer from unbearable low recall due 
to insufficient training data. Intuitively we can follow 
the path of ?Nested ? Adjacent ? Separated ? 
Others? (Nested, Adjacent and Separated structures 
are majority in the corpus) to perform the inference. 
But soon we have an interesting finding. If two related 
entities are nested, almost all the mentions of them are 
nested. So basically inference works on ?Adjacent ? 
Separated??. 
When considering the co-reference information, we 
may find another type of inconsistency, i.e. the one 
raised from co-referenced entity mentions. It is 
possible that R(EM11, EM21) ? R(EM12, EM22) when R 
is identified based on the context of EM. Co-reference 
not only helps for inference but also provides the 
second chance to check the consistency among entity 
mention pairs so that we can revise accordingly. As the 
classification results of SVM can be transformed to 
probabilities with a sigmoid function, the relations of 
lower probability mention pairs are revised according 
to the relation of highest probability mention pairs. 
The above inference strategy is called coreference- 
based inference. Besides, we find that pattern-based 
inference is also necessary. The relations of adjacent 
structure can infer the relations of separated structure 
if there are certain linguistic indicators in the local 
context. For example, given a local context ?EM1 and 
EM2 located EM3?, if the relation of EM2 and EM3 has 
been identified, EM1 and EM3 will take the relation 
type/subtype that EM2 and EM3 holds. Currently, the 
only indicators under consideration are ?and? and ?or?. 
However, more patterns can be included in the future. 
4 Experimental Results 
The experiments are conducted on the ACE 2005 
Chinese RDC training data (with true entities) where 6 
types and 18 subtypes of relations are annotated. We 
use 75% of it to train SVM classifiers and the 
remaining to evaluate results.  
The aim of the first set of experiments is to examine 
the role of structure features. In these experiments, a 
?NONE? class is added to indicate a null type/subtype. 
With entity features and entity context features and 
word list features, we consider three different 
classification contexts: (1), only three coarser 
structures 1 , i.e. nested, adjacent and separated, are 
used as feature, and a classifier is trained for each 
relation type and subtype; (2) similar to (1) but all nine 
structures are concerned; and (3) similar to (2) but the 
training data is divided into 9 parts according to 
structure, i.e. type and subtype classifiers are trained 
on the data with the same structures. The results 
presented in Table 2 show that 9-structure is much 
more discriminative than 3-structure. Also, the 
performance can be improved significantly by 
dividing training data based on nine structures. 
Type / Subtype Precision Recall F-measure 
3-Structure 0.7918/0.7356 0.3123/0.2923 0.4479/0.4183
9-Structure 0.7533/0.7502 0.4389/0.3773 0.5546/0.5021
9-Structure_Divide 0.7733/0.7485 0.5506/0.5301 0.6432/0.6209
Table 2 Evaluation on Structure Features 
Structure Positive Class Negative Class Ratio 
Nested 6332 4612 1 : 0.7283
Adjacent 2028 27100 1 : 13.3629
                                                     
1 Nine structures are combined to three by merging (b) and (c) to (a), (e) 
and (f) to (d), (h) and (i) to (g). 
91
Separated 939 79989 1 : 85.1853
Total 9299 111701 1 : 12.01 
Table 3 Imbalance Training Class Problem 
In the experiments, we find that the training class 
imbalance problem is quite serious, especially for the 
separated structure (see Table 3 above where 
?Positive? and ?Negative? mean there exists a relation 
between two entities and otherwise). A possible 
solution to alleviate this problem is to detect whether 
the given two entities have some relation first and if 
they do then to classify the relation types and subtypes 
instead of combining detection and classification in 
one process. The second set of experiment is to 
examine the difference between these two 
implementations. Against our expectation, the 
sequence implementation does better than the 
combination implementation, but not significantly, as 
shown in Table 4 below.  
Type / Subtype Precision Recall F-measure 
Combination 0.7733/0.7485 0.5506/0.5301 0.6432/0.6206
Sequence 0.7374/0.7151 0.5860/0.5683 0.6530/0.6333
Table 4 Evaluation of Two Detection and Classification Modes 
Based on the sequence implementation, we set up 
the third set of experiments to examine the correction 
and inference mechanisms. The results are illustrated 
in Table 5. The correction with constraints and 
consistency check is clearly contributing. It improves 
F-measure 7.40% and 6.47% in type and subtype 
classification respectively. We further compare four 
possible consistency check strategies in Table 6 and 
find that the strategies using subtypes to determine or 
select types perform better than top down strategies. 
This can be attributed to the fact that correction with 
relation/argument constraints in subtype is tighter than 
the ones in type.  
Type / Subtype Precision Recall F-measure 
Seq. + Cor. 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Seq. + Cor. + Inf. 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 5 Evaluation of Correction and Inference Mechanisms 
Type / Subtype Precision Recall F-measure 
Guiding Top-Down 0.7644/0.7853 0.6074/0.5783 0.6770/0.6661
Subtype Selection 0.8069/0.7738 0.6065/0.5817 0.6925/0.6641
Strictly Bottom-Up 0.8120/0.7798 0.6146/0.5903 0.6996/0.6719
Type Selection 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734
Table 6 Comparison of Different Consistency Check Strategies 
Finally, we provide our findings from the fourth set 
of experiments which looks at the detailed 
contributions from four feature types. Entity type 
features themselves do not work. We incrementally 
add the structures, the external contexts and internal 
contexts, Uni-grams and Bi-grams, and at last the 
word lists on them. The observations are: Uni-grams 
provide more discriminative information than 
Bi-grams; external context seems more useful than 
internal context; positional structure provides stronger 
support than other individual recognized features such 
as entity type and context; but word list feature can not 
further boost the performance.  
Type / Subtype Precision Recall F-measure 
Entity Type + Structure 0.7288/0.6902 0.4876/0.4618 0.5843/0.5534
+ External (Uni-) 0.7935/0.7492 0.5817/0.5478 0.6713/0.6321
+ Internal (Uni-) 0.8137/0.7769 0.6113/0.5836 0.6981/0.6665
+ Bi- (Internal & External) 0.8144/0.7828 0.6141/0.5902 0.7002/0.6730
+ Wordlist 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741
Table 6 Evaluation of Feature and Their Combinations 
5 Conclusion 
In this paper, we study feature-based Chinese relation 
extraction. The proposed approach is effective on the 
ACE 2005 data set. Unfortunately, there is no result 
reported on the same data so that we can compare. 
6 Appendix: Nine Positional Structures  
 
Acknowledgments 
This work was supported by HK RGC (CERG PolyU5211/05E) 
and China NSF (60603027). 
References 
Razvan Bunescu and Raymond Mooney. 2005. A Shortest Path 
Dependency Tree Kernel for Relation Extraction, In Proceedings of 
HLT/EMNLP, pages 724-731.  
Aron Culotta and Jeffrey Sorensen. 2004. Dependency Tree Kernels for 
Relation Extraction, in Proceedings of ACL, pages 423-429. 
Jing Jiang, Chengxiang Zhai. 2007. A Systematic Exploration of the 
Feature Space for Relation Extraction. In proceedings of 
NAACL/HLT, pages 113-120. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, and Semantic 
Features with Maximum Entropy Models for Extracting Relations. 
In Proceedings of ACL, pages 178-181.  
Dmitry Zelenko, Chinatsu Aone and Anthony Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of Machine 
Learning Research 3:1083-1106 
Min Zhang, Jie Zhang, Jian Su and Guodong Zhou. 2006. A Composite 
Kernel to Extract Relations between Entities with both Flat and 
Structured Features, in Proceedings of COLING/ACL, pages 
825-832. 
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring 
Various Knowledge in Relation Extraction. In Proceedings of ACL, 
pages 427-434. 
GuoDong Zhou, Min Zhang, Donghong Ji and Qiaoming Zhu. 2007. 
Tree Kernel-based Relation Extraction with Context-Sensitive 
Structured Parse Tree Information. In Proceedings of EMNLP, 
pages 728-736. 
Wanxiang Che et al 2005. Improved-Edit-Distance Kernel for Chinese 
Relation Extraction. In Proceedings of IJCNLP, pages 132-137. 
92
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 113?116,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
An Integrated Multi-document Summarization Approach based on 
Word Hierarchical Representation 
You Ouyang, Wenji Li, Qin Lu 
Department of Computing 
The Hong Kong Polytechnic University  
{csyouyang,cswjli,csluqin}@comp.polyu.edu.hk 
 
Abstract 
This paper introduces a novel hierarchical 
summarization approach for automatic multi-
document summarization. By creating a 
hierarchical representation of the words in the 
input document set, the proposed approach is 
able to incorporate various objectives of multi-
document summarization through an 
integrated framework. The evaluation is 
conducted on the DUC 2007 data set. 
1 Introduction and Background 
Multi-document summarization requires creating 
a short summary from a set of documents which 
concentrate on the same topic. Sometimes an 
additional query is also given to specify the 
information need of the summary. Generally, an 
effective summary should be relevant, concise 
and fluent. It means that the summary should 
cover the most important concepts in the original 
document set, contain less redundant information 
and should be well-organized.  
Currently, most successful multi-document 
summarization systems follow the extractive 
summarization framework. These systems first 
rank all the sentences in the original document 
set and then select the most salient sentences to 
compose summaries for a good coverage of the 
concepts. For the purpose of creating more 
concise and fluent summaries, some intensive 
post-processing approaches are also appended on 
the extracted sentences. For example, 
redundancy removal (Carbonell and Goldstein, 
1998) and sentence compression (Knight and 
Marcu, 2000) approaches are used to make the 
summary more concise. Sentence re-ordering 
approaches (Barzilay et al, 2002) are used to 
make the summary more fluent. In most systems, 
these approaches are treated as independent steps. 
A sequential process is usually adopted in their 
implementation, applying the various approaches 
one after another. 
In this paper, we suggest a new summarization 
framework aiming at integrating multiple 
objectives of multi-document summarization. 
The main idea of the approach is to employ a 
hierarchical summarization process which is 
motivated by the behavior of a human 
summarizer. While the document set may be 
very large in multi-document summarization, the 
length of the summary to be generated is usually 
limited. So there are always some concepts that 
can not be included in the summary. A natural 
thought is that more general concepts should be 
considered first. So, when a human summarizer 
faces a set of many documents, he may follow a 
general-specific principle to write the summary. 
The human summarizer may start with finding 
the core topic in a document set and write some 
sentences to describe this core topic. Next he 
may go to find the important sub-topics and 
cover the subtopics one by one in the summary, 
then the sub-sub-topics, sub-sub-sub-topics and 
so on. By this process, the written summary can 
convey the most salient concepts. Also, the 
general-specific relation can be used to serve 
other objectives, i.e. diversity, coherence and etc.  
Motivated by this experience, we propose a 
hierarchical summarization approach which 
attempts to mimic the behavior of a human 
summarizer. The approach includes two phases. 
In the first phase, a hierarchical tree is 
constructed to organize the important concepts in 
a document set following the general-to-specific 
order. In the second phase, an iterative algorithm 
is proposed to select the sentences based on the 
constructed hierarchical tree with consideration 
of the various objectives of multi-document 
summarization. 
2 Word Hierarchical  Representation 
2.1 Candidate Word Identification 
As a matter of fact, the concepts in the original 
document set are not all necessary to be included 
in the summary. Therefore, before constructing 
the hierarchical representation, we first conduct a 
113
filtering process to remove the unnecessary 
concepts in the document set in order to improve 
the accuracy of the hierarchical representation. In 
this study, concepts are represented in terms of 
words. Two types of unnecessary words are 
considered. One is irrelevant words that are not 
related to the given query. The other is general 
words that are not significant for the specified 
document set. The two types of words are 
filtered through two features, i.e. query-
relevance and topic-specificity.  
The query-relevance of a word is defined as 
the proportion of the number of sentences that 
contains both the word and at least one query 
word to the number of sentences that contains the 
word. If a feature value is large, it means that the 
co-occurrence rate of the word and the query is 
high, thus it is more related to the query. The 
topic-specificity of a word is defined as the 
entropy of its frequencies in different document 
sets. If the feature value is large, it means that the 
word appears uniformly in document sets, so its 
significance to a specified document set is low. 
Thus, the words with very low query-relevance 
or with very high topic-specificity are filtered 
out1. 
2.2 Word Relation Identification and 
Hierarchical Representation 
To construct a hierarchical representation for the 
words in a given document set, we follow the 
idea introduced by Lawrie et al (2001) who use 
the subsuming relation to express the general-to-
specific structure of a document set. A 
subsumption is defined as an association of two 
words if one word can be regarded as a sub-
concept of the other one. In our approach, the 
pointwise mutual information (PMI) is used to 
identify the subsumption between words. 
Generally, two words with a high PMI is 
regarded as related. Using the identified relations, 
the word hierarchical tree is constructed in a top-
bottom manner. Two constraints are used in the 
tree construction process: 
(1) For two words related by a subsumption 
relation, the one which appears more frequently 
in the document set serves as the parent node in 
the tree and the other one serves as the child 
node. 
(2) For a word, its parent node in the hierarchical 
tree is defined as the most related word, which is 
identified by PMI.  
                                                 
1 Experimental thresholds are used on the evaluated data.  
2 http://duc.nist.gov/ 
The construction algorithm is detailed below. 
Algorithm 1: Hierarchical Tree Construction 
1: Sort the identified key words by their 
frequency in the document set in descending 
order, denoted as T = {t1, t2 ,?, tn} 
2: For each ti, i from 1 to n, find the most 
relevant word tj from all the words before ti in T, 
as Ti = {t1, t2 ,?, ti-1}. Here the relevance of two 
words is calculated by their PMI, i.e. 
)()(
*),(
log),(
ji
ji
ji tfreqtfreq
Nttfreq
ttPMI   
If the coverage rate of word ti by word tj 
2.0
)(
),(
)|( t 
i
ji
ji tfreq
ttfreq
ttP , ti is regarded as 
being subsumed by tj. Here freq(ti) is the 
frequency of ti in the document set and  freq(ti, 
ti) is the co-occurrence of ti and tj in the same 
sentences of the document set. N is the total 
number of tokens in the document set. 
4: After all the subsumption relations are found, 
the tree is constructed by connecting the related 
words from the first word t1. 
An example of a tree fragment is demonstrated 
below. The tree is constructed on the document 
set D0701A from DUC 20072, the query of this 
document set is ?Describe the activities of 
Morris Dees and the Southern Poverty Law 
Center?. 
 
3 Summarization based on Word 
Hierarchical Representation 
3.1 Word Significance Estimation 
In order to include the most significant concepts 
into the summary, before using the hierarchical 
tree to create an extract, we need to estimate the 
significance of the words on the tree first. 
Initially, a rough estimation of the significance of 
a word is given by its frequency in the document 
set. However, this simple frequency-based 
measure is obviously not accurate. One thing we 
observe from the constructed hierarchical tree is 
that a word which subsumes many other words is 
usually very important, though it may not appear 
Center 
Dee Law group 
Morris hatePoverty Southern
lawyer organizationcivil Klan
114
frequently in the document set. The reason is that 
the word covers many key concepts so it is 
dominant in the document set. Motivated by this, 
we develop a bottom-up algorithm which 
propagates the significance of the child nodes in 
the hierarchical tree backward to their parent 
nodes to boost the significance of nodes with 
many descendants. 
Algorithm 2: Word Scoring Theme 
1: Set the initial score of each word in T as its 
log-frequency, i.e. score(ti) =log freq(ti). 
2: For ti from n to 1, propagate an importance 
score to its parent node par(ti) (if exists) 
according to their relevance, i.e. score(par(ti)) = 
score(par(ti)) + log freq(ti, par(ti)).  
3.2 Sentence Selection  
Based on the word hierarchical tree and the 
estimated word significance, we propose an 
iterative algorithm to select sentences which is 
able to integrate the multiple objectives for 
composing a relevant, concise and fluent 
summary. The algorithm follows a general-to-
specific order to select sentences into the 
summary. In the implementation, the idea is 
carried out by following a top-down order to 
cover the words in the hierarchical tree. In the 
beginning, we consider several ?seed? words 
which are in the top-level of the tree (these 
words are regarded as the core concepts in the 
document set). Once some sentences have been 
extracted according to these ?seed? words, the 
algorithm moves to down-level words through 
the subsumption relations between the words. 
Then new sentences are added according to the 
down-level words and the algorithm continues 
moving to lower levels of the tree until the whole 
summary is generated. For the purpose of 
reducing redundancy, the words already covered 
by the extracted sentences will be ignored while 
selecting new sentences. To improve the fluency 
of the generated summary, after a sentence is 
selected, it is inserted to the position according to 
the subsumption relation between the words of 
this sentence and the sentences which are already 
in the summary. The detailed process of the 
sentence selection algorithm is described below. 
Algorithm 3: Summary Generation  
1: For the words in the hierarchical tree, set the 
initial states of the top n words3 as ?activated? 
and the states of other words as ?inactivated?. 
2: For all the sentences in the document set, 
                                                 
3 n is set to 3 experimentally on the evaluation data set. 
select the sentence with the largest score 
according to the ?activated? word set. The 
score of a sentence s is defined as 
? )(|| 1)( itscoressscore  where ti is a word 
belongs to s and the state of ti should be 
?activated?. | s | is the number of words in s. 
3: For the selected sentence sk, the subsumption 
relations between it and the existing sentences 
in the current summary are calculated and the 
most related sentence sl is selected. sk is then 
inserted to the position right behind sl. 
4: For each word ti belongs to the selected 
sentence sk, set its state to ?inactivated?; for 
each word tj which is subsumed by ti, set its 
state to ?activated?. 
5: Repeat step 2-4 until the length limit of the 
summary is exceeded. 
4 Experiment  
Experiments are conducted on the DUC 2007 
data set which contains 45 document sets. Each 
document set consists of 25 documents and a 
topic description as the query. In the task 
definition, the length of the summary is limited 
to 250 words. In our summarization system, pre-
processing includes stop-word removal and word 
stemming (conducted by GATE4). 
One of the DUC evaluation methods, ROUGE 
(Lin and Hovy, 2003), is used to evaluate the 
content of the generated summaries. ROUGE is a 
state-of-the-art automatic evaluation method 
based on N-gram matching between system 
summaries and human summaries. In the 
experiment, our system is compared to the top 
systems in DUC 2007. Moreover, a baseline 
system which considers only the frequencies of 
words but ignores the relations between words is 
included for comparison. Table 1 below shows 
the average recalls of ROUGE-1, ROUGE-2 and 
ROUGE-SU4 over the 45 DUC 2007document 
sets. In the experiment, the proposed 
summarization system outperforms the baseline 
system, which proves the benefit of considering 
the relations between words. Also, the system 
ranks the 6th among the 32 submitted systems in 
DUC 2007. This shows that the proposed 
approach is competitive. 
  ROUGE-1 ROUGE-2 ROUGE-SU4
S15 0.4451 0.1245 0.1771 
S29 0.4325 0.1203 0.1707 
S4 0.4342 0.1189 0.1699 
S24 0.4526 0.1179 0.1759 
                                                 
4 http://gate.ac.uk/ 
115
S13 0.4218 0.1117 0.1644 
Ours 0.4257 0.1110 0.1608 
Baseline 0.4088 0.1040 0.1542 
Table 1. ROUGE Evaluation Results 
To demonstrate the advantage of the proposed 
approach, i.e. its ability to incorporate multiple 
summarization objectives, the fragments of the 
generated summaries on the data set D0701A are 
also provided below as a case study. 
The summary produced by our system 
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists. 
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala. 
Dees, founder of the Southern Poverty Law 
Center, has won a series of civil right suits against 
the Ku Klux Klan and other racist organizations in 
a campaign to drive them out of business. 
In 1987, Dees won a $7 million verdict against a 
Ku Klux Klan organization over the slaying of a 
19-year-old black man in Mobile, Ala. 
The summary produced by the baseline system
Morris Dees, co-founder of the Southern Poverty 
Law Center in Montgomery, Ala.  
The Southern Poverty Law Center tracks hate 
groups, and Intelligence Report covers right-wing 
extremists.  
The Southern Poverty Law Center previously 
recorded a 20-percent increase in hate groups 
from 1996 to 1997.  
The verdict was obtained by lawyers for the 
Southern Poverty Law Center, a nonprofit 
organization in Birmingham, Ala. 
Comparing the generated summaries of the 
two systems, we can see that the summary 
generated by the proposed approach is better in 
coherence and fluency since these factors are 
considered in the integrated summarization 
framework. Various summarization approaches, 
i.e. sentence ranking, redundancy removal and 
sentence re-ordering, are all implemented in the 
sentence selection algorithm based on the word 
hierarchical tree. However, we also observe that 
the proposed approach fails to generate better 
summaries on some document sets. The main 
problem is that the quality of the constructed 
hierarchical tree is not always satisfied. In the 
proposed summarization approach, we mainly 
rely on the PMI between the words to construct 
the hierarchical tree. However, a single PMI-
based measure is not enough to characterize the 
word relation. Consequently the constructed tree 
can not always well represent the concepts for 
some document sets. Another problem is that the 
two constraints used in the tree construction 
algorithm are not always right in real data. So we 
regard developing better tree construction 
approaches as of primary importance. Also, there 
are other places which can be improved in the 
future, such as the word significance estimation 
and sentence inserting algorithms. Nevertheless, 
we believe that the idea of incorporating the 
multiple summarization objectives into one 
integrated framework is meaningful and worth 
further study. 
5 Conclusion  
We introduced a summarization framework 
which aims at integrating various summarization 
objectives. By constructing a hierarchical tree 
representation for the words in the original 
document set, we proposed a summarization 
approach for the purpose of generating a relevant, 
concise and fluent summary. Experiments on 
DUC 2007 showed the advantages of the 
integrated framework.  
Acknowledgments 
The work described in this paper was partially 
supported by Hong Kong RGC Projects (No. 
PolyU 5217/07E) and partially supported by The 
Hong Kong Polytechnic University internal 
grants (A-PA6L and G-YG80). 
 
References  
R. Barzilay, N. Elhadad, and K. R. McKeown. 2002. 
Inferring strategies for sentence ordering in 
multidocument news summarization. Journal of 
Artificial Intelligence Research, 17:35-55, 2002. 
J. Carbonell and J. Goldstein. 1998. The Use of MMR, 
Diversity-based Reranking for Reordering 
Documents and Producing Summaries. In 
Proceedings of ACM SIGIR 1998, pp 335-336. 
K. Knight and D. Marcu. 2000. Statistics-based 
summarization --- step one: Sentence compression. 
In Proceeding of The American Association for 
Artificial Intelligence Conference (AAAI-2000), 
pp 703-710. 
D. Lawrie, W. B. Croft and A. Rosenberg. 2001. 
Finding topic words for hierarchical 
summarization. In Proceedings of ACM SIGIR 
2001, pp 349-357. 
C. Lin and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurance statistics. 
In Proc. of HLT-NAACL 2003, pp 71-78. 
116
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 117?120,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 
Co-Feedback Ranking for Query-Focused Summarization 
Furu Wei1,2,3  Wenjie Li1 and Yanxiang He2 
1 The Hong Kong Polytechnic University, Hong Kong 
{csfwei,cswjli}@comp.polyu.edu.hk 
2 Wuhan University, China 
{frwei,yxhe}@whu.edu.cn
3 IBM China Research Laboratory, Beijing, China 
 
Abstract 
In this paper, we propose a novel ranking 
framework ? Co-Feedback Ranking (Co-
FRank), which allows two base rankers to 
supervise each other during the ranking 
process by providing their own ranking results 
as feedback to the other parties so as to boost 
the ranking performance. The mutual ranking 
refinement process continues until the two 
base rankers cannot learn from each other any 
more. The overall performance is improved by 
the enhancement of the base rankers through 
the mutual learning mechanism. We apply this 
framework to the sentence ranking problem in 
query-focused summarization and evaluate its 
effectiveness on the DUC 2005 data set. The 
results are promising.  
1 Introduction and Background 
Sentence ranking is the issue of most concern in 
extractive summarization. Feature-based 
approaches rank the sentences based on the 
features elaborately designed to characterize the 
different aspects of the sentences. They have 
been extensively investigated in the past due to 
their easy implementation and the ability to 
achieve promising results. The use of feature-
based ranking has led to many successful (e.g. 
top five) systems in DUC 2005-2007 query-
focused summarization (Over et al, 2007). A 
variety of statistical and linguistic features, such 
as term distribution, sentence length, sentence 
position, and named entity, etc., can be found in 
literature. Among them, query relevance, 
centroid (Radev et al, 2004) and signature term 
(Lin and Hovy, 2000) are most remarkable.  
There are two alternative approaches to 
integrate the features. One is to combine features 
into a unified representation first, and then use it 
to rank the sentences. The other is to utilize rank 
fusion or rank aggregation techniques to combine 
the ranking results (orders, ranks or scores) 
produced by the multiple ranking functions into a 
unified rank. The most popular implementation 
of the latter approaches is to linearly combine the 
features to obtain an overall score which is then 
used as the ranking criterion. The weights of the 
features are either experimentally tuned or 
automatically derived by applying learning-based 
mechanisms. However, both of the above-
mentioned ?combine-then-rank? and ?rank-then-
combine? approaches have a common drawback. 
They do not make full use of the information 
provided by the different ranking functions and 
neglect the interaction among them before 
combination. We believe that each individual 
ranking function (we call it base ranker) is able 
to provide valuable information to the other base 
rankers such that they learn from each other by 
means of mutual ranking refinement, which in 
turn results in overall improvement in ranking. 
To the best of our knowledge, this is a research 
area that has not been well addressed in the past.  
The inspiration for the work presented in this 
paper comes from the idea of Co-Training (Blum 
and Mitchell, 1998), which is a very successful 
paradigm in the semi-supervised learning 
framework for classification. In essence, co-
training employs two weak classifiers that help 
augment each other to boost the performance of 
the learning algorithms. Two classifiers mutually 
cooperate with each other by providing their own 
labeling results to enrich the training data for the 
other parties during the supervised learning 
process. Analogously, in the context of ranking, 
although each base ranker cannot decide the 
overall ranking well on itself, its ranking results 
indeed reflect its opinion towards the ranking 
from its point of view. The two base rankers can 
then share their own opinions by providing the 
ranking results to each other as feedback. For 
each ranker, the feedback from the other ranker 
contains additional information to guide the 
refinement of its ranking results if the feedback 
is defined and used appropriately. This process 
continues until the two base rankers can not learn 
from each other any more. We call this ranking 
paradigm Co-Feedback Ranking (Co-FRank). 
The way how to use the feedback information 
117
 varies depending on the nature of a ranking task. 
In this paper, we particularly consider the task of 
query-focused summarization. We design a new 
sentence ranking algorithm which allows a 
query-dependent ranker and a query-independent 
ranker mutually learn from each other under the 
Co-FRank framework. 
2 Co-Feedback Ranking for Query-
Focused Summarization 
2.1 Co-Feedback Ranking Framework 
Given a set of objects O, one can define two base 
ranker f1 and f2:     Ooofof ??o?o ,, 21 . The 
ranking results produced by f1 and f2 individually 
are by no means perfect but the two rankers can 
provide relatively reasonable ranking 
information to supervise each other so as to 
jointly improve themselves. One way to do Co-
Feedback ranking is to take the most confident 
ranking results (e.g. highly ranked instances 
based on orders, ranks or scores) from one base 
ranker as feedback to update the other?s ranking 
results, and vice versa. This process continues 
iteratively until the termination condition is 
reached, as depicted in Procedure 1. While the 
standard Co-Training algorithm requires two 
sufficient and redundant views, we suggest f1 and 
f2 be two independent rankers which emphasize 
two different aspects of the objects in O. 
Procedure 1. Co-FRank(f1, f2, O) 
1:  Rank O with f1 and obtain the ranking results r1; 
2:  Rank O with f2 and obtain the ranking results r2; 
3:  Repeat  
4:  Select the top N ranked objects 1W  from r1 as 
feedback to supervise f2, and re-rank O using f2 
and 1W ; Update r2; 
5:  Select the top N ranked objects 2W  from r2 as 
feedback to supervise f1, and re-rank O using f1 
and 2W ; Update r1; 
5:  Until I(O). 
The termination condition I(O) can be defined 
according to different application scenarios. For 
example, I(O) may require the top K ranked 
objects in r1 and r2 to be identical if one is 
particularly interested in the top ranked objects. 
It is also very likely that r1 and r2 do not change 
any more after several iterations (or the top K 
objects do not change). In this case, the two base 
rankers can not learn from each other any more, 
and the Co-Feedback ranking process should 
terminate either. The final ranking results can be 
easily determined by combining the two base 
rankers without any parameter, because they 
have already learnt from each other and can be 
equally treated.  
2.2 Query-Focused Summarization based 
on Co-FRank  
The task of query-focused summarization is to 
produce a short summary (250 words in length) 
for a set of related documents D with respect to 
the query q that reflects a user?s information 
need. We follow the traditional extractive 
summarization framework in this study, where 
the two critical processes involved are sentence 
ranking and sentence selection, yet we focus 
more on the sentence ranking algorithm based on 
Co-FRank. As for sentence selection, we 
incrementally add into the summary the highest 
ranked sentence if it doesn?t significantly repeat1 
the information already included in the summary 
until the word limitation is reached. 
In the context of query-focused summarization, 
two kinds of features, i.e. query-dependent and 
query-independent features are necessary and 
they are supposed to complement each other. We 
then use these two kinds of features to develop 
the two base rankers. The query-dependent 
feature (i.e. the relevance of the sentence s to the 
query q) is defined as the cosine similarity 
between s and q.  
    qsqsqsqsrelf ?x  ? /,cos,1  (1) 
The words in s and q vectors are weighted by 
tf*isf. Meanwhile, the query-independent feature 
(i.e. the sentence significance based on word 
centroid) is defined as 
    swcscf
sw
/2 ? ? ?   (2) 
where c(w) is the centroid weight of the word w 
in s and     DSDs ws Nisftfwc w? ? ? . DSN  is the total 
number of the sentences in D, s
w
tf  is the 
frequency of w in s, and  wDw sfNisf Slog  is the 
inverse sentence frequency (ISF) of w, where sfw  
is the sentence frequency of w in D. The sentence 
ranking algorithm based on Co-FRank is detailed 
in the following Algorithm 1.  
Algorithm 1. Co-FRank(f1, f2, D, q) 
1:  Extract sentences S={s1, ? sm} from D;  
2:  Rank S with f1 and obtain the ranking results r1; 
3:  Rank S with f2 and obtain the ranking results r2; 
4:  Normalize r1,            11111 minmaxmin rrrsrsr ii  ;
5:  Normalize r2,            22222 minmaxmin rrrsrsr ii  ; 
6:  Repeat  
                                                 
1 A sentence is discarded if the cosine similarity of it to any 
sentence already selected into the summary is greater than 
0.9. 
118
 7:  Select the top N ranked sentences at round n n
1
W  
from r1 as feedback for f2, and re-rank S using f2 
and n
1
W ,                                                
        nssims n
k
k
ii /,
1
2 1? m WS ,     22 222 minmax
min
SS
SSS 
  
            iii ssfsr 222 1 SKK ??m                         (3)
8: Select the top N ranked sentences at round n n2W  
from r2 as feedback for f1, and re-rank S using f1
and n2W ;  
         nssims n
k
k
ii /,
1
1 2? m WS ,     11 111 minmax
min
SS
SSS 
    
            iii ssfsr 111 1 SKK ??m                              (4) 
9: Until the top K sentences in r1 and r2 are the same, 
both r1 and r2 do not change any more, or 
maximum iteration round is achieved. 
10: Calculate the final ranking results, 
            221 iii srsrsr  .                                        (5) 
The update strategies used in Algorithm 1, as 
formulated in Formulas (3) and (4), are designed 
based on the intuition that the new ranking of the 
sentence s from one base ranker (say f1) consists 
of two parts. The first part is the initial ranking 
produced by f1. The second part is the similarity 
between s and the top N feedback provided by 
the other ranker (say f2), and vice versa. The top 
K ranked sentences by f2 are supposed to be 
highly supported by f2. As a result, a sentence 
that is similar to those top ranked sentences 
should deserve a high rank as well.  nissim 2,W  
captures the effect of such feedback at round n 
and the definition of it may vary with regard to 
the application background. For example, it can 
be defined as the maximum, the minimum or the 
average similarity value between si and a set of 
feedback sentences in 2W . Through this mutual 
interaction, the two base rankers supervise each 
other and are expected as a whole to produce 
more reliable ranking results.  
We assume each base ranker is most confident 
with its first ranked sentence and set N to 1. 
Accordingly,  nissim 2,W is defined as the similarity 
between si and the one sentence in n
2
W . K  is a 
balance factor which can be viewed as the 
proportion of the dependence of the new ranking 
results on its initial ranking results. K is set to 10 
as 10 sentences are basically sufficient for the 
summarization task we work on. We carry out at 
most 5 iterations in the current implementation. 
3 Experimental Study   
We take the DUC 2005 data set as the evaluation 
corpus in this preliminary study. ROUGE (Lin 
and Hovy, 2003), which has been officially 
adopted in the DUC for years is used as the 
evaluation criterion. For the purpose of 
comparison, we implement the following two 
basic ranking functions and the linear 
combination of them for reference, i.e. the query 
relevance based ranker (denoted by QRR, same 
as f1) and the word centroid based ranker 
(denoted by WCR, same as f2), and the linear 
combined ranker, LCR= O QRR+(1- O )WCR, 
where O  is a combination parameter. QRR and 
WCR are normalized by    minmaxmin x , 
where x, max and min denote the original ranking 
score, the maximum ranking score and minimum 
ranking score produced by a ranker, respectively. 
Table 1 shows the results of the average recall 
scores of ROUGE-1, ROUGE-2 and ROUGE-
SU4 along with their 95% confidence intervals 
included within square brackets. Among them, 
ROUGE-2 is the primary DUC evaluation 
criterion.  
 ROUGE-1 ROUGE-2 ROUGE-SU4
QRR 0.3597 [0.3540, 0.3654]
0.0664 
[0.0630, 0.0697] 
0.1229 
[0.1196, 0.1261]
WCR 0.3504 [0.3436, 0.3565]
0.0644 
[0.0614, 0.0675] 
0.1171 
[0.1138, 0.1202]
LCR* 0.3513 [0.3449, 0.3572]
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209]
Co- 
FRank+
0.3769 
[0.3712, 0.3829]
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
LCR** 
0.3753 
[0.3692, 0.3813]
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340]
Co- 
FRank++
0.3783 
[0.3719, 0.3852]
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360]
* The worst results produced by LCR when O  = 0.1 
+ The worst results produced by Co-FRank when K  = 0.6 
** The best results produced by LCR when O  = 0.4 
++ The best results produced by Co-FRank when K  = 0.8 
Table 1 Compare different ranking strategies 
Note that the improvement of LCR over QRR 
and WCR is rather significant if the combination 
parameter O  is selected appropriately. Besides, 
Co-FRank is always superior to LCR regardless 
of the best or the worst ouput, and the 
improvement is visible. The reason is that both 
QRR and WCR are enhanced step by step in Co-
FRank, which in turn results in the increased 
overall performance. The trend of the 
improvement has been clearly observed in the 
experiments. This observation validates our 
motivation and the rationality of the algorithm 
proposed in this paper and motivates our further 
investigation on this topic.  
We continue to examine the parameter settings 
in LCR and Co-FRank. Table 2 shows the results 
of LCR when the value of O  changes from 0.1 to 
119
 1.0, and Table 3 shows the results of Co-FRank 
with K  ranging from 0.5 to 0.9. Notice that K  is 
not a combination parameter. We believe that a 
base ranker should have at least half belief in its 
initial ranking results and thus the value of the K  
should be greater than 0.5. We find that LCR 
heavily depends on O . LCR produces relatively 
good and stable results with O  varying from 0.4 
to 0.6. However, the ROUGE scores drop 
apparently when O  heading towards its two end 
values, i.e. 0.1 and 1.0. 
O  ROUGE-1 ROUGE-2 ROUGE-SU4
0.1 0.3513 [0.3449, 0.3572] 
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209] 
0.2 0.3623 [0.3559, 0.3685] 
0.0699 
[0.0662, 0.0736] 
0.1235 
[0.1197, 0.1271] 
0.3 0.3721 [0.3660, 0.3778] 
0.0741 
[0.0706, 0.0778] 
0.1281 
[0.1246, 0.1318] 
0.4 0.3753 [0.3692, 0.3813] 
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340] 
0.5 0.3756 [0.3698, 0.3814] 
0.0755 
[0.0717, 0.0793] 
0.1307 
[0.1272, 0.1342] 
0.6 0.3770 [0.3710, 0.3826] 
0.0754 
[0.0716, 0.0791] 
0.1323 
[0.1286, 0.1357] 
0.7 0.3698 [0.3636, 0.3759] 
0.0718 
[0.0680, 0.0756] 
0.1284 
[0.1246, 0.1318] 
0.8 0.3672 [0.3613, 0.3730] 
0.0706 
[0.0669, 0.0743] 
0.1271 
[0.1234, 0.1305] 
0.9 0.3651 [0.3591, 0.3708] 
0.0689 
[0.0652, 0.0726] 
0.1258 
[0.1220, 0.1293] 
Table 2 LCR with different O  values 
As shown in Table 3, the Co-FRank can 
always produce stable and promising results 
regardless of the change of K . More important, 
even the worst result produced by Co-FRank still 
outperforms the best result produced by LCR. 
K  ROUGE-1 ROUGE-2 ROUGE-SU4
0.5 0.3750 [0.3687, 0.3810] 
0.0766 
[0.0727, 0.0804] 
0.1308 
[0.1270, 0.1344] 
0.6 0.3769 [0.3712, 0.3829] 
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
0.7 0.3775 [0.3713, 0.3835] 
0.0763 
[0.0724, 0.0801] 
0.1319 
[0.1282, 0.1354]
0.8 0.3783 [0.3719, 0.3852] 
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360] 
0.9 0.3779 [0.3722, 0.3835] 
0.0765 
[0.0728, 0.0803] 
0.1319 
[0.1285, 0.1354 
Table 3 Co-FRank with different K  values 
We then compare our results to the DUC 
participating systems. We present the following 
representative ROUGE results of (1) the top 
three DUC participating systems according to 
ROUGE-2 scores (S15, S17 and S10); and (2) 
the NIST baseline which simply selects the first 
sentences from the documents. 
 ROUGE-1 ROUGE-2 ROUGE-SU4
Co-FRank 0.3783 0.0775 0.1323 
S15 - 0.0725 0.1316 
S17 - 0.0717 0.1297 
S10 - 0.0698 0.1253 
Baseline   0.0403 0.0872 
Table 4 Compare with DUC participating systems 
It is clearly shown in Table 4 that Co-FRank 
can produce a very competitive result, which 
significantly outperforms the NIST baseline and 
meanwhile it is superior to the best participating 
system in the DUC 2005. 
4 Conclusion and Future Work 
In this paper, we propose a novel ranking 
framework, namely Co-Feedback Ranking (Co-
FRank), and examine its effectiveness in query-
focused summarization. There is still a lot of 
work to be done on this topic. Although we show 
the promising achievements of Co-Frank from 
the perspective of experimental studies, we 
expect a more theoretical analysis on Co-FRank. 
Meanwhile, we would like to investigate more 
appropriate techniques to use feedback, and we 
are interested in applying Co-FRank to the other 
applications, such as opinion summarization 
where the integration of opinion-biased and 
document-biased ranking is necessary. 
Acknowledgments 
The work described in this paper was supported 
by the Hong Kong Polytechnic University 
internal the grants (G-YG80 and G-YH53) and 
the China NSF grant (60703008). 
References  
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-Training. In 
Proceedings of the Eleventh Annual Conference on 
Computational Learning Theory, pp92-100. 
Chin-Yew Lin and Eduard Hovy. 2000. The 
Automated Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING, 
pp495-501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL, pp71-78. 
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based 
Summarization of Multiple Documents. 
Information Processing and Management, 40:919-
938. 
Paul Over, Hoa Dang and Donna Harman. 2007. DUC 
in Context. Information Processing and 
Management, 43(6):1506-1520. 
120
An Indexing Method Based on Sentences* 
Li Li 1, Chunfa Yuan1 , K.F. Wong2, and Wenjie Li3 
1State Key Laboratory of Intelligent Technology and System 
1Dept. of Computer Science & Technology, Tsinghua University, Beijing 100084 
 Email: lili97@mails.tsinghua.edu.cn; cfyuan@tsinghua.edu.cn 
2D e p t. o f  S y s te m  E n g in e e r in g  &  E n g in e e r in g  M a n a g e m e n t, T h e  C h in e se  U n iv e rs ity  o f H o n g  K o n g , H o n g  K o n g . 
Email: kfwong@se.cuhk.edu.hk 
3
 Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong. 
Email: cswjli@comp.polyu.edu.hk  
 
                                                          
*Supported by Natural Science Foundation of China(Proceedings of the Linguistic Annotation Workshop, pages 61?68,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating Chinese Collocations with Multi Information 
 
Ruifeng Xu1,     Qin Lu1,     Kam-Fai Wong2,    Wenjie Li1 
? 
?1 Department of Computing,                       2 Department of Systems Engineering and 
?                                                  Engineering Management 
 The Hong Kong Polytechnic University,        The Chinese University of Hong Kong,   
?Kowloon, Hong Kong                                      N.T., Hong Kong 
{csrfxu,csluqin,cswjli}@comp.polyu.edu.hk  kfwong@se.cuhk.edu.hk 
? 
 
Abstract 
This paper presents the design and construc-
tion of an annotated Chinese collocation bank 
as the resource to support systematic research 
on Chinese collocations. With the help of 
computational tools, the bi-gram and n-gram 
collocations corresponding to 3,643 head-
words are manually identified. Furthermore, 
annotations for bi-gram collocations include 
dependency relation, chunking relation and 
classification of collocation types. Currently, 
the collocation bank annotated 23,581 bi-
gram collocations and 2,752 n-gram colloca-
tions extracted from  a 5-million-word corpus. 
Through statistical analysis on the collocation 
bank, some characteristics of Chinese bi-
gram collocations are examined which is es-
sential to collocation research, especially for 
Chinese. 
1 Introduction 
Collocation is a lexical phenomenon in which two 
or more words are habitually combined and com-
monly used in a language to express certain seman-
tic meaning. For example, in Chinese, people will 
say ??-?? (historical baggage) rather than ?
? -?? (historical luggage) even though ??
(baggage) and ?? (luggage) are synonymous. 
However, no one can argue why ?? must collo-
cate with??. Briefly speaking, collocations are 
frequently used word combinations. The collocated 
words always have syntactic or semantic relations 
but they cannot be generated directly by syntactic 
or semantic rules. Collocation can bring out differ-
ent meanings a word can carry and it plays an in-
dispensable role in expressing the most appropriate 
meaning in a given context. Consequently, colloca-
tion knowledge is widely employed in natural lan-
guage processing tasks such as word sense disam-
biguation, machine translation, information re-
trieval and natural language generation (Manning 
et al 1999).  
Although the importance of collocation is well 
known, it is difficult to compile a complete collo-
cation dictionary. There are some existing corpus 
linguistic researches on automatic extraction of 
collocations from electronic text (Smadja 1993; 
Lin 1998; Xu and Lu 2006). These techniques are 
mainly based on statistical techniques and syntactic 
analysis. However, the performances of automatic 
collocation extraction systems are not satisfactory 
(Pecina 2005). A problem is that collocations are 
word combinations that co-occur within a short 
context, but not all such co-occurrences are true 
collocations. Further examinations is needed to 
filter out pseudo-collocations once co-occurred 
word pairs are identified.  A collocation bank with 
true collocations annotated is naturally an indis-
pensable resource for collocation research. (Kosho 
et al 2000) presented their works of collocation 
annotation on Japanese text. Also, the Turkish 
treebank, (Bedin 2003) included collocation anno-
tation as one step in its annotation. These two col-
location banks provided collocation identification 
and co-occurrence verification information. (Tutin 
2005) used shallow analysis based on finite state 
transducers and lexicon-grammar to identify and 
annotate collocations in a French corpus. This col-
location bank further provided the lexical functions 
of the collocations. However to this day, there is 
no reported Chinese collocation bank available. 
61
In this paper, we present the design and con-
struction of a Chinese collocation bank (acrony-
med CCB). This is the first attempt to build a 
large-scale Chinese collocation bank as a Chinese 
NLP resource with multiple linguistic information 
for each collocation including:  (1) annotating the 
collocated words for each given headword; (2) dis-
tinguishing n-gram and bi-gram collocations for 
the headword; (3) for bi-gram collocations, CCB 
provides their syntactic dependencies, chunking 
relation and classification of collocation types 
which is proposed by (Xu and Lu 2006). In addi-
tion, we introduce the quality assurance mecha-
nism used for CCB. CCB currently contains for 
3,643 common headwords taken from ?The Dic-
tionary of Modern Chinese Collocations? (Mei 
1999) with 23,581 unique bi-gram collocations and 
2,752 unique n-gram collocations extracted from a 
five-million-word segmented and chunked Chinese 
corpus (Xu and Lu, 2005). 
The rest of this paper is organized as follows. 
Section 2 presents some basic concepts. Section 3 
describes the annotation guideline. Section 4 de-
scribes the practical issues in the annotation proc-
ess including corpus preparation, headword prepa-
ration, annotation flow, and the quality assurance 
mechanism. Section 5 gives current status of CCB 
and characteristics analysis of the annotated collo-
cations. Section 6 concludes this paper. 
2 Basic Concepts 
Although collocations are habitual expressions in 
natural language use and they can be easily under-
stood by people, a precise definition of collocation 
is still far-reaching (Manning et al 1999). In this 
study, we define a collocation as a recurrent and 
conventional expression of two or more content 
words that holds syntactic and semantic relation. 
Content words in Chinese include noun, verb, ad-
jective, adverb, determiner, directional word, and 
gerund. Collocations with only two words are 
called bi-gram collocations and others are called n-
gram collocations. 
From a linguistic view point, collocations have a 
number of characteristics. Firstly, collocations are 
recurrent as they are of habitual use. Collocations 
occur frequently in similar contexts and they ap-
pear in certain fixed patterns. However, they can-
not be described by the same set of syntactic or 
semantic rules. Secondly, free word combinations 
which can be generated by linguistic rules are 
normally considered compositional.  In contrast, 
collocations should be limited compositional 
(Manning et al 1999) and they usually carry addi-
tional meanings when used as a collocation. 
Thirdly, collocations are also limited substitutable 
and limited modifiable. Limited substitutable here 
means that a word cannot be freely substituted by 
other words with similar linguistic functions in the 
same context such as synonyms. Also, many collo-
cations cannot be modified freely by adding modi-
fiers or through grammatical transformations. 
Lastly, collocations are domain-dependent (Smadja 
1993) and language-dependent.  
3 Annotation Guideline Design 
The guideline firstly determines the annotation 
strategy. 
(1) The annotation of CCB follows the head-
word-driven strategy. The annotation uses selected 
headwords as the starting point. In each circle, the 
collocations corresponding to one headword are 
annotated. Headword-driven strategy makes a 
more efficient annotation as it is helpful to estimate 
and compare the relevant collocations. 
(2) CCB is manually annotated with the help of 
automatic estimation of computational features, i.e. 
semi-automatic software tools are used to generate 
parsing and chunking candidates and to estimate 
the classification features. These data are present to 
the annotators for determination. The use of assis-
tive tools is helpful to produce accurate annota-
tions with efficiency. 
The guideline also specifies the information to 
be annotated and the labels used in the annotation. 
For a given headword, CCB annotates both bi-
gram collocations and n-gram collocations. Con-
sidering the fact that n-gram collocations consist-
ing of continuous significant bi-grams as a whole 
and, the n-gram annotation is based on the identifi-
cation and verification of bi-gram word combina-
tions and is prior to the annotation of bi-gram col-
locations.  
For bi-gram annotation, which is the major in-
terest  in collocation research, three kinds of in-
formation are annotated. The first one is the syn-
tactic dependency of the headword and its co-word 
in a bi-gram collocation . A syntactic dependency 
normally consists of one word as the governor (or 
head), a dependency type and another word serves 
62
as dependent (or modifier) (Lin 1998).Totally, 10 
types of dependencies are annotated in CCB. They 
are listed in Table 1 below. 
 
 Dependency Description Example 
ADA Adjective and its adverbial modifier ??/d ??/a  greatly painful 
ADV Predicate and its adverbial modifier in 
which the predicate serves as head 
??/ad ??/v heavily strike 
AN Noun and its adjective modifier ??/a ??/n lawful incoming 
CMP Predicate and its complement in which 
the predicate serves as head 
??/v??/v ineffectively treat
NJX Juxtaposition structure ??/a??/a fair and reasonable
NN Noun and its nominal modifier ??/n ??/n personal safety 
SBV Predicate and its subject ??/n ??/v property transfer 
VO Predicate and its object in which the 
predicate serves as head 
??/v ??/n change mechanism
VV Serial verb constructions which indi-
cates that there are serial actions  
??/v ??/v trace and report 
OT Others   
Table 1. The dependency categories 
The second one is the syntactic chunking informa-
tion (a chunk is defined as a minimum non-nesting 
or non-overlapping phrase) (Xu and Lu, 2005). 
Chunking information identifies all the words for a 
collocation within the context of an enclosed 
chunk. Thus, it is a way to identify its proper con-
text at the most immediate syntactic structure. 11 
types of syntactic chunking categories given in (Xu 
and 2006) are used as listed in Table 2.  
 
 Description Examples 
BNP Base noun phrase [??/n ??/n]NP     market economy 
BAP Base adjective phrase  [??/a??/a]BAP   fair and reasonable
BVP Base verb phrase [??/a??/v]BVP   successfully start 
BDP Base adverb phrase [?/d ??/d]BDP      no longer 
BQP Base quantifier phrase [?? /m ? /q]BQP ?? /n several thou-
sand soldiers 
BTP Base time phrase [??/t ??/t]BTP 8:00 in the morning 
BFP Base position phrase [??/ns ???/f]BFP Northeast of Mon-
golia 
BNT Name of an organization [??/ns ??/n]BNT Yantai University 
BNS Name of a place [??/ns ??/ns]BNS Tongshan, Jiangsu 
Province 
BNZ Other proper noun phrase [???/nr?/n]BNZ The Nobel Prize 
BSV S-V structure [??/n ??/a]BSV  territorial integrity 
Table 2. The chunking categories 
The third one is the classification of collocation 
types. Collocations cover a wide spectrum of ha-
bitual word combinations ranging from idioms to 
free word combinations. Some collocations are 
very rigid and some are more flexible. (Xu and Lu 
2006) proposed a scheme to classify collocations 
into four types according to the internal association 
of collocations including compositionality, non-
substitutability, non-modifiability, and statistical 
significance. They are,  
Type 0: Idiomatic Collocation 
Type 0 collocations are fully non-compositional 
as its meaning cannot be predicted from the mean-
ings of its components such as???? (climbing 
a tree to catch a fish, which is a metaphor for a 
fruitless endeavour). Some terminologies are also 
Type 0 collocations such as ?  ?(Blue-tooth ) 
which refers to a wireless communication protocol. 
Type 0 collocations must have fixed forms. Their 
components are non-substitutable and non-
modifiable allowing no syntactic transformation 
and no internal lexical variation. This type of col-
locations has very strong internal associations and 
co-occurrence statistics is not important.  
Type 1: Fixed Collocation 
Type 1 collocations are very limited composi-
tional with fixed forms which are non-substitutable 
and non-modifiable. However, this type can be 
compositional. None of the words in a Type 1 col-
location can be substituted by any other words to 
retain the same meaning such as in??/n ???
/n (diplomatic immunity). Finally, Type 1 colloca-
tions normally have strong co-occurrence statistics 
to support them. 
Type 2: Strong Collocation 
Type 2 collocations are limitedly compositional. 
They allow very limited substitutability. In other 
words, their components can only be substituted by 
few synonyms and the newly generated word com-
binations have similar meaning, e.g., ??/v ??
/n (alliance formation) and ??/v ??/n (alliance 
formation). Furthermore, Type 2 collocations al-
low limited modifier insertion and the order of 
components must be maintained. Type2 colloca-
tions normally have strong statistical support.  
Type 3: Loose Collocation 
Type 3 collocations have loose restrictions. 
They are nearly compositional. Their components 
may be substituted by some of their synonyms and 
the newly generated word combinations usually 
have very similar meanings. Type 3 collocations 
are modifiable meaning that they allow modifier 
insertions. Type 3 collocations have weak internal 
associations and they must have statistically sig-
nificant co-occurrence.  
The classification represents the strength of in-
ternal associations of collocated words. The anno-
tation of these three kinds of information is essen-
tial to all-rounded characteristic analysis of collo-
cations. 
63
4 Annotation of CCB 
4.1 Data Preparation 
CCB is based on the PolyU chunk bank (Xu and 
Lu, 2005) which contains chunking information on 
the People?s Daily corpus with both segmentation 
and part-of-speech tags. The accuracies of word 
segmentation and POS tagging are claimed to be 
higher than 99.9% and 99.5%, respectively (Yu et 
al. 2001). The use of this popular and accurate raw 
resource helped to reduce the cost of annotation 
significantly, and ensured maximal sharing of our 
output.  
The set of 3, 643 headwords are selected from 
?The Dictionary of Modern Chinese Collocation? 
(Mei 1999) among about 6,000 headwords in the 
dictionary. The selection  was based both on the 
judgment by linguistic experts as well as the statis-
tical information that they are commonly used. 
4.2 Corpus Preprocessing 
The CCB annotations are represented in XML. 
Since collocations are practical word combinations 
and word is the basic unit in collocation research, a 
preprocessing module is devised to transfer the 
chunked sentences in the PolyU chunk bank to 
word sequences with the appropriate labels to indi-
cate the corresponding chunking information. This 
preprocessing module indexes the words and 
chunks in the sentences and encodes the chunking 
information of each word in two steps. Consider 
the following sample sentence extracted from the 
PolyU chunk bank: 
??/v[??/n??/n]BNP?/u[??/n??/n??
/an ]BNP 
(ensure life and property safety of the people) 
The first step in preprocessing is to index each 
word and the chunk in the sentence by giving in-
cremental word ids and chunk ids from left to right. 
That is,, 
[W1]??/v [W2]??/n [W3]??/n [W4]?/u  
[W5]??/n [W6]??/n [W7]??/an [C1]BNP [C2]BNP 
where, [W1] to [W7] are the words and [C1] to [C2] 
are chunks although chunking positions are not 
included in this step. One Chinese word may occur 
in a sentence for more than one times, the unique 
word ids are helpful to avoid ambiguities in the 
collocation annotation on these words. 
The second step is to represent the chunking in-
formation of each word. Chunking boundary in-
formation is labeled by following initial/final rep-
resentation scheme. Four labels, O/B/I/E, are used 
to mark the isolated words outsides any chunks, 
chunk-initial words, words in the middle of chunks, 
and chunk-final words, respectively. Finally, a la-
bel H is used to mark the identified head of chunks 
and N to mark the non-head words. 
The above sample sentence is then transferred to 
a sequence of words with labels as shown below, 
<labeled> [W1][O_O_N][O]??/v [W2][B_BNP_N][C1]
??/n [W3][E_BNP_H][C1]??/n [W4][O_O_N][O]?/u 
[W5][B_BNP_N][C2]??/n [W6][I_BNP_N][C2]??/n 
[W7][E_BNP_N][C2]??/an </labeled> 
For each word, the first label is the word ID. The 
second one is a hybrid tag for describing its chunk-
ing status. The hybrid tags are ordinal with respect 
to the chunking status of boundary, syntactic cate-
gory and head, For example, B_BNP_N indicates 
that current word is the beginning or a BNP and 
this word is not the head of this chunk. The third 
one is the chunk ID if applicable. For the word out 
of any chunks, a fixed chunk ID O is given. 
4.3 Collocation Annotation 
Collocation annotation is conducted on one head-
word at a time. For a given headword, an annota-
tors examines its context to determine if its co-
occurred word(s) forms a collocation with it and if 
so, also annotate the collocation?s dependency, 
chunking and classification information. The anno-
tation procedure, requires three passes. We use a 
headword ??/an (safe), as an illustrative exam-
ple.  
Pass 1. Concordance and dependency identifica-
tion 
In the first pass, the concordance of the given 
headword is performed. Sentences containing the 
headwords are obtained, e.g.  
S1: ??/v [??/v  ??/an]BVP  ?/u  ??/n 
(follow the principles for ensuring the safety) 
S2: ??/v [??/n ??/n]BNP ?/u[??/n ??/n ?
?/an]BNP 
(ensure life and property safety of people) 
S3: ??/v  ??/ns  [??/an  ??/v]BVP 
(ensure the flood pass through Yangzi River safely) 
With the help of an automatic dependency pars-
er, the annotator determines all syntactically and 
semantically dependent words in the chunking con-
text of the observing headword. The annotation 
output of S1 is given below in which XML tags are 
used for the dependency annotation.  
S1:<sentence>??/v [??/v  ??/an]BVP  ?/u  ??/n 
64
<labeled> [W1][O_O_N][O]??/v [W2][B_BVP_H][C1]
??/v [W3][E_BNP_N][C1]??/an [W4][O_O_N][O]?
/u  [W5][O_O_N][O]??/n </labeled> 
<dependency no="1" observing="??/an" head="??
/v" head_wordid="W2" head_chunk ="B_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="E_BVP_N" 
modifer_chunkid="C1" relation="VO" > </dependency> 
</sentence> 
Dependency of word combination is annotated 
with the tag <dependency> which includes the fol-
lowing attributes: 
-<dependency> indicates an identified depend-
ency   
-no is the id of identified dependency within cur-
rent sentence according to ordinal sequence 
-observing indicates the current observing 
headword 
-head indicates the head of the identified word 
dependency 
-head_wordid is the word id of the head 
-head_chunk is the hybrid tags for labeling the 
chunking information of the head 
-head_chunkid is the chunk id of the head 
-modifier indicates the modifier of the identified 
dependency 
-modifier_wordid is the word id of the modifier 
-modifier_chunk is the hybrid tags for labeling 
chunking information of the modifier 
-modifier_chunkid is the chunk id of the modi-
fier 
-relation gives the syntactic dependency rela-
tions labeled according to the dependency labels 
listed in Table 1. 
In S1 and S2, the word combination ??/v??
/an has direct dependency, and in S3, such a de-
pendency does not exist as??/v only determines
??/v and ??/an depends on ??/v. The qual-
ity of CCB highly depends on the accuracy of de-
pendency annotation. This is very important for 
effective characteristics analysis of collocations 
and for the collocation extraction algorithms.  
Pass 2. N-gram collocations annotation 
It is relatively easy to identify n-gram colloca-
tions since an n-gram collocation is of habitual and 
recurrent use of a series of bi-grams. This means 
that n-gram collocations can be identified by find-
ing consecutive occurrence of significant bi-grams 
in certain position. In the second pass, the annota-
tors focus on the sentences where the headword 
has more than one dependency. The percentage of 
all appearances of each dependent word at each 
position around the headword is estimated with the 
help of a program (Xu and Lu, 2006). Finally, 
word dependencies frequently co-occurring in con-
secutive positions in a fixed order are extracted as 
n-gram collocations.   
For the headword, an n-gram collocation??/n 
? ? /n ? ? /an is identified since the co-
occurrence percentage of dependency??/-NN-?
?/an and dependency??/n-NN-??/an is 0.74 
is greater than a empirical threshold suggest in (Xu 
and Lu, 2006). This n-gram is annotated in S2 as 
follows: 
<ncolloc observing="??/an" w1="??/n" w2="??/n" 
w3="??/an" start_wordid="5"> </ncolloc> 
where, 
-<ncolloc> indicates an n-gram collocation  
-w1, w2,..wn give the components of the n-gram 
collocation according to the ordinal sequence.  
-start_wordid  indicates the word id of the first 
component of the n-gram collocation. 
Since n-gram collocation is regarded as a whole, 
its internal dependencies are ignored in the output 
file of pass 2. That is, if the dependencies of sev-
eral components are associated with an n-gram 
collocation in one sentence, the n-gram collocation 
is annotated and these dependencies are filtered out 
so as not to disturb the bi-gram dependencies.  
Pass 3. Bi-gram collocations annotation 
In this pass, all the word dependencies are ex-
amined to identify bi-gram collocations. Further-
more, if a dependent word combination is regarded 
as a collocation by the annotators, it will be further 
labeled based on the type determined. The identifi-
cation is based on expert knowledge combined 
with the use of several computational features as 
discussed in (Xu and Lu, 2006). 
An assistive tool is developed to estimate the 
computational features. We use the program to ob-
tain feature data based on two sets of data. The 
first data set is the annotated dependencies in the 
5-million-word corpus which is obtained through 
Pass 1 and Pass 2 annotations. Because the de-
pendent word combinations are manually identi-
fied and annotated in the first pass, the statistical 
significance is helpful to identify whether the word 
combination is a collocation and to determine its 
type. However, data sparseness problem must be 
considered since 5-million-word is not large 
enough. Thus, another set of statistical data are 
65
collected from a 100-million segmented and tagged 
corpus (Xu and Lu, 2006). With this large corpus, 
data sparseness is no longer a serious problem. But, 
the collected statistics are quite noisy since they 
are directly retrieved from text without any verifi-
cation. By analyzing the statistical features from 
both sets, the annotator can use his/her professional 
judgment to determine whether a bi-gram is a col-
location and its collocation type. 
In the example sentences, two collocations are 
identified. Firstly, ??/an ??/v is classified as a 
Type 1 collocation as they have only one peak co-
occurrence, very low substitution ratio and their 
co-occurrence order nearly never altered. Secondly, 
??/v ??/an is identified as a collocation. They 
have frequent co-occurrences and they are always 
co-occurred in fixed order among the verified de-
pendencies. However, their co-occurrences are dis-
tributed evenly and they have two peak co-
occurrences. Therefore, ??/v ??/an is classi-
fied as a Type 3 collocation. These bi-gram collo-
cations are annotated as illustrated below, 
<bcolloc observing="??/an" col="??/v" head="??
/v" type= "1" relation="ADV">  
<dependency no="1" observing="??/an" head="??/v" 
head_wordid="W4" head_chunk ="E_BVP_H" 
head_chunkid="C1" modifier=" ? ? /an" modi-
fier_wordid="W3" modifier _chunk="B_BVP_N" 
modifer_chunkid="C1" relation="ADV" 
></dependency></bcolloc> 
where, 
-<bcolloc> indicates a bi-gram collocation. 
-col is for  the collocated word. 
-head indicates the head of an identified colloca-
tion 
-type is the classified collocation type. 
-relation gives the syntactic dependency rela-
tions of this bi-gram collocation. 
Note that the dependency annotations within the 
bi-gram collocations are reserved. 
4.4 Quality Assurance 
The annotators of CCB are three post-graduate stu-
dents majoring in linguistics. In the first annotation 
stage, 20% headwords of the whole set was anno-
tated in duplicates by all three of them. Their out-
puts were checked by a program. Annotated collo-
cation including classified dependencies and types 
accepted by at least two annotators are reserved in 
the final data as the Golden Standard while the 
others are considered incorrect. The inconsisten-
cies between different annotators were discussed to 
clarify any misunderstanding in order to come up 
with the most appropriate annotations. In the sec-
ond annotation stage, 80% of the whole annota-
tions were then divided into three parts and sepa-
rately distributed to the annotators with 5% dupli-
cate headwords were distributed blindly. The du-
plicate annotation data were used to estimate the 
annotation consistency between annotators.  
5 Collocation Characteristic Analysis 
5.1 Progress and Quality of CCB 
Up to now, the first version of CCB is completed. 
We have obtained 23,581 unique bi-gram colloca-
tions and 2,752 unique n-gram collocations corre-
sponding to the 3,643 observing headwords. 
Meanwhile, their occurrences in the corpus are an-
notated and verified. With the help of a computer 
program, the annotators manually classified bi-
gram collocations into three types. The numbers of 
Type 0/1, Type 2 and Type 3 collocations are 152, 
3,982 and 19,447, respectively.  
For the 3,643 headwords in The Dictionary of 
Modern Chinese Collocations (Mei 1999) with 
35,742 bi-gram collocations,  20,035 collocations 
appear in the corpus. We call this collection as 
Mei?s Collocation Collection (MCC). There are 
19,967 common entries in MCC and CCB, which 
means 99.7% collocations in MCC appear in CCB 
indicating a good linguistic consistency. Further-
more, 3,614 additional collocations are found in 
CCB which enriches the static collocation diction-
ary.
 
5.2 Dependencies Numbers Statistics of Col-
locations 
Firstly, we study the statistics of how many types 
of dependencies a bi-gram collocation may have. 
The numbers of dependency types with respect to 
different collocation types are listed in Table 3.  
 
Collocations 1 type 2 types >2 types Total 
Type 0/1 152 0 0 152 
Type 2 3970 12 0 3982 
Type 3 17282 2130 35 19447 
Total 21404 2142 35 23581 
Table 3. Collocation classification versus number 
of dependency types 
66
It is observed that about 90% bi-gram collocations 
have only one dependency type. This indicates that 
a collocation normally has only one fixed syntactic 
dependency. It is also observed that about 10% bi-
gram collocations have more than one dependency 
type, especially Type 3 collocations. For example, 
two types of dependencies are identified in the bi-
gram collocation ??/an-??/n. They are ??
/an-AN-??/n (a safe nation) which indicates the 
dependency of a noun and its nominal modifier 
where ??/n serves as the head, and??/n-NN-
?? /an (national security) which indicates the 
dependency of a noun and its nominal modifier 
where ??/an serves as the head. It is attributed to 
the fact that the use of Chinese words is flexible. A 
Chinese word may support different part-of-speech. 
A collocation with different dependencies results 
in different distribution trends and most of these 
collocations are classified as Type 3. On the other 
hand, Type 0/1 and Type 2 collocations seldom 
have more than one dependency type. 
5.3 Syntactic Dependency Statistics of Collo-
cations 
The statistics of the 10 types of syntactic depend-
encies with respect to different types of bi-gram 
collocations are shown in Table 4. No. is the num-
ber of collocations with a given dependency type  
D and a given collocation type T. The percentage 
of No. among all collocations with the same collo-
cation type T is labeled as P_T, and the percentage 
of No. among all of the collocations with the same 
dependency D is labeled as P_D. 
 
 Type 0/1  Type 2  Type 3  Total 
 No. P_T P_D No. P_T P_D No. P_T P_D No. P_T
ADA 1 0.7 0.1 212 5.3 11.5 1637 7.6 88.5 1850 7.2
ADV 9 5.9 0.3 322 8.1 11.2 2555 11.8 88.5 2886 11.2
AN 20 13.2 0.4 871 21.8 15.4 4771 22.0 84.3 5662 22.0
CMP 12 7.9 2.2 144 3.6 26.9 379 1.8 70.8 535 2.1
NJX 8 5.3 3.2 42 1.1 16.9 198 0.9 79.8 248 1.0
NN 44 28.9 0.9 1036 25.9 21.6 3722 17.2 77.5 4802 18.6
SBV 4 2.6 0.2 285 7.1 11.1 2279 10.5 88.7 2568 10.0
VO 26 17.1 0.5 652 16.3 12.5 4545 21.0 87.0 5223 20.2
VV 3 2.0 0.2 227 5.7 13.4 1464 6.8 86.4 1694 6.6
OT 25 16.4 7.7 203 5.1 62.5 97 0.4 29.8 325 1.3
Total 152 100.0 0.6 3994 100.0 15.5 21647 100.0 83.9 25793 100.0
Table 4. The statistics of collocations with dif-
ferent collocation type and dependency 
 
Corresponding to 23,581 bi-gram collocations, 
25,793 types of dependencies are identified (some 
collocations have more than one types of depend-
ency). In which, about 82% belongs to five major 
dependency types. They are AN, VO, NN, ADV and 
SBV. It is note-worthy that the percentage of NN 
collocation is much higher than that in English. 
This is because nouns are more often used in paral-
lel to serve as one syntactic component in Chinese 
sentences than in English. 
The percentages of Type 0/1, Type 2 and Type 3 
collocations in CCB are 0.6%, 16.9% and 82.5%, 
respectively. However, the collocations with dif-
ferent types of dependencies have shown their own 
characteristics with respect to different collocation 
types. The collocations with CMP, NJX and NN 
dependencies on average have higher percentage to 
be classified into Type 0/1 and Type 2 collocations. 
This indicates that CMP, NJX and NN collocations 
in Chinese are always used in fixed patterns and 
these kinds of collocations are not freely modifi-
able and substitutable. In the contrary, many ADV 
and AN collocations are classified as Type 3. This 
is partially due to the special usage of auxiliary 
words in Chinese.  Many AN Chinese collocations 
can be inserted by a meaningless auxiliary word?
/u and many ADV Chinese collocations can be in-
serted by an auxiliary word?/u. This means that 
many AN and ADV collocations can be modified 
and thus, they always have two peak co-
occurrences. Therefore, they are classified as Type 
3 collocations. 7.7% and 62.5% of the collocations 
with dependency OT are classified as Type 0/1 and 
Type2 collocations, respectively. Such percentages 
are much higher than the average. This is attributed 
by the fact that some Type 0/1 and Type 2 colloca-
tions have strong semantic relations rather than 
syntactic relations and thus their dependencies are 
difficult to label. 
5.4 Chunking Statistics of Collocations 
The chunking characteristic for the collocations 
with different types and different dependencies are 
examined. In most cases, Type 0/1/2 collocations 
co-occur within one chunk or between neighboring 
chunks. Therefore, their chunking characteristics 
are not discussed in detail. The percentage of the 
occurrences of Type 3 collocations with different 
chunking distances are given in Table 5. If a collo-
cation co-occurs within one chunk, the chunking 
distance is 0. If a collocation co-occurs between 
neighboring chunks, or between neighboring words, 
or between a word and a neighboring chunk, the 
chunking distance is 1, and so on. 
67
 
 ADA ADV AN CMP NJX NN SBV VO VV OT
0 chunk 56.8 53.1 65.7 48.5 70.2 62.4 46.5 41.1 47.2 86.4
1 chunk 38.2 43.7 28.5 37.2 15.4 27.9 41.2 35.7 41.1 13.5
2 chunks 5.0 3.2 3.7 14.2 14.4 9.7 11.0 17.6 9.6 0.1
>2chunks 0.0 0.0 2.1 0.1 0.0 0.0 1.3 5.6 2.1 0.0
Table 5. Chunking distances of Type 3 collocations  
 
It is shown that the co-occurrence of collocations 
decreases with increased chunking distance. Yet, 
the behavior for decrease is different for colloca-
tions with different dependencies. Generally speak-
ing, the ADA, ADV, CMP, NJX, NN and OT collo-
cations seldom co-occur cross two words or two 
chunks. Furthermore, the occurrences of AN, NJX 
and OT collocations quickly drops when the 
chunking distance is greater than 0, i.e. these col-
locations tends to co-occur within the same chunk. 
In the contrary, the co-occurrences of ADA, ADV, 
CMP, SBV and VV collocations corresponding to 
chunking distance equals 0 and 1 decrease steadily. 
It means that these four kinds of collocations are 
more evenly distributed within the same chunk or 
between neighboring words or chunks. The occur-
rences of VO collocations corresponding to chunk-
ing distance from 0 to 3 with a much flatter reduc-
tion. This indicates that a verb may govern its ob-
ject in a long range. 
6 Conclusions 
This paper describes the design and construction of 
a manually annotated Chinese collocation bank. 
Following a set of well-designed annotation guide-
line, the collocations corresponding to 3,643 
headwords are identified from a chunked five-
million word corpus. 2,752 unique n-gram colloca-
tions and 23,581 unique bi-gram collocations are 
annotated. Furthermore, each bi-gram collocation 
is annotated with its syntactic dependency informa-
tion, classification information and chunking in-
formation. Based on CCB, characteristics of collo-
cations with different types and different depend-
encies are examined. The obtained result is essen-
tial for improving research related to Chinese col-
location. Also, CCB may be used as a standard an-
swer set for evaluating the performance of differ-
ent collocation extraction algorithms. In the future, 
collocations of all unvisited headwords will be an-
notated to produce a complete 5-million-word Chi-
nese collocation bank. 
Acknowledgement 
This research is supported by The Hong Kong 
Polytechnic University (A-P203), CERG Grant 
(5087/01E) and Chinese University of Hong Kong 
under the Direct Grant Scheme project (2050330) 
and Strategic Grant Scheme project (4410001). 
References 
Bedin N. et al 2003. The Annotation Process in the 
Turkish Treebank. In Proc. 11th Conference of the 
EACL-4th Linguistically Interpreted Corpora Work-
shop- LINC. 
Kosho S. et al 2000. Collocations as Word Co-
occurrence Restriction Data - An Application to 
Japanese Word Processor. In Proc. Second Interna-
tional Conference on Language Resources and 
Evaluation 
Lin D.K. 1998. Extracting collocations from text cor-
pora. In Proc. First Workshop on Computational 
Terminology, Montreal  
Manning, C.D., Sch?tze, H. 1999: Foundations of Sta-
tistical Natural Language Processing, MIT Press  
Mei J.J. 1999. Dictionary of Modern Chinese Colloca-
tions, Hanyu Dictionary Press  
Pecina P. 2005. An Extensive Empirical Study of Collo-
cation Extraction Methods. In Proc. 2005 ACL Stu-
dent Research Workshop. 13-18 
Smadja. F. 1993. Retrieving collocations from text: 
Xtract, Computational Linguistics. 19. 1.  143-177 
Tutin A. 2005. Annotating Lexical Functions in Corpora: 
Showing Collocations in Context. In Proc. 2nd Inter-
national Conference on the Meaning ? Text Theory 
Xu R. F. and Lu Q. 2005. Improving Collocation Ex-
traction by Using Syntactic Patterns, In Proc. IEEE 
International Conference on Natural Language Proc-
essing and Knowledge Engineering. 52-57 
Xu, R.F. and Lu, Q. 2006. A Multi-stage Chinese Col-
location Extraction System. Lecture Notes in Com-
puter Science, Vol. 3930, Springer-Verlag. 740-749 
Yu S.W. et al 2001. Guideline of People?s Daily Cor-
pus Annotation, Technical Report, Peking University  
 
 
68
An Algorithm for Situation Classification 
of Chinese Verbs 
Xiaodan Zhu, Chunfa Yuan 
State Key Laboratory for Intelligent 
Technology and System, DepL of Computer 
Science & Technology, Tsinghua 
University, Beijing 100084, P.R.C. 
K.F.Wong, Wenjie.Li 
Dept. of System Engineenng and Engineering 
Management, Chinese University of 
HongKong 
Abst rac t  
Temporal information analysis is very 
important for Chinese Information Process. 
Comparing with English, Chinese is quite 
different in temporal information 
expression. Based on the feature of Chinese 
a phase-based method is proposed to deal 
with Chinese temporal information. To this 
end, an algorithm is put forward to classify 
verbs into different situation types 
automatically. About 2981 verbs were 
tested. The result has shown that the 
algorithm is effective. 
1.*Introduction 
We are now launching a research 
project on Events Extraction from Chinese 
Financial News, which requires us to extract 
the related temporal information from news. 
Temporal expressions in Chinese form a 
complex system. We cannot fully 
understand the temporal information only by 
extracting the verbs, adverbs, auxiliary 
words and temporal phrases. Instead, more 
profound analysis is needed. In this paper, 
we first introduce the temporal system of 
Chinese, then we put forward a method in 
dealing with Chinese temporal information, 
in which situation types is very important. 
Therefore, an algorithm is rendered to 
classify verbs into several situation types. 
1.1 Temporal System of Chinese 
Commonly, Chinese linguists \[3\]\[4\] 
think that the temporal system of Chinese 
includes three parts: phase, tense and aspect. 
Each of these represents ome profile of 
temporal expression (these definitions are a 
little different from linguistic theory of 
English). 
(1) Phase. A sentence may describe a static 
state or an action; an action may be durative 
or instantaneous; a durative action may 
indicate a terminal or not. All of these are 
the research fields of phase. So, static vs. 
dynamic, durative vs. instantaneous, telic vs. 
non-telic are three pairs of phase features. 
Phase depends fully on meaning. According 
to phase features, we can classify the verbs 
into different situation types. 
(2) Tense. Tense describes the relations 
between an event (E), reference time (R) and 
speaking time (S). First, taking S as the 
origin, we can get three reIations between R 
and S: if R is before S, the sentence 
describes past; if R is the same time as S, it 
describes present; if R is after S, it desci'ibes 
future. This is called primary tense. 
Secondly, we can get three relations 
between E and S: If E is before R, we call it 
anterior; if E is the same time as R, we call it 
simple; otherwise we call it posterior. This is 
called secondary tense. Therefore, there are 
nine tenses including anterior past, anterior 
future, simple future, posterior present, etc. 
* Supported by National Natural Science Foundation 
of China (69975008) and 973 project (G 1998030507) 
140 
(3) Aspect . Aspect reflects the way we 
observe an event. For the same event, there 
are many perspectives. We can take the 
event as atomic and not consider its inner 
structure, and call it perfective. We can 
consider it being in process, and call it 
imperfective. For imperfective, we can 
observe it at a"position before it, at the 
beginning of it, in the middle of it, etc. 
Different perspectives lead to different 
expressions in the language. 
Phase, tense and aspect are not 
independent even though they are different 
conceptions; each of them can influence and 
restrict he others, ultimately building up the 
complex temporal system of Chinese. 
1.2 Phase-based Chinese temporal 
information analysis 
Most languages express temporal 
information through phase, tense and aspect, 
however, for different languages, the 
relative importance of the three parts is 
different. A very important feature of 
English is that tense and aspect are 
expressed by variation of predicates. But for 
Chinese, predicates keep the same form no 
matter how the tense and aspect are 
different. 
Therefore, in English, temporal 
information analysis mainly considers tense 
and aspect, as well as temporal adjective and 
time words and phrases. But in Chinese, 
tense and aspect of a sentence are not very 
clear, verbs do not vary in form with the 
change of tense and aspect. So we suggest 
basing temporal information analysis on 
phase. We mainly perceive the situation type 
of a sentence, then roughly acquire tense 
from adverbs and auxiliary words. After 
considering the temporal phrases, we can 
understand the temporal information of 
single event fully. Finally, according to the 
absolute temporal information of single 
event, we can get the temporal relation 
between two events. Phase-based temporal 
information analysis has been used in our 
research on Event Extraction from Financial 
News, in which the most important and 
fundamental problem is to acquire the 
situation types of a sentence. 
1.3 Situation Classification of Chinese 
Verbs 
In the West, research on situation has a 
long history. The earliest can be traced to 
the times of Aristotle. In resent years, 
Western researchers have published a large 
volume of papers, which present many 
points of view. The most important are 
Vendler(1967), Bache(1982), and Smith 
(1985) They approximately classify the 
situation as four types: 
state, activity, accomplishment, and 
achievement. 
Chinese researchers have also done 
considerable work, among which the most 
typical research were done by Chen\[3\] and 
Ma\[5\]. 
Ma\[5\] stated that the situation of a 
sentence is fully determined by the situation 
of the main verb of the sentence. He use 
three phases: static, durative, telic to classify 
verbs into four situational types 
V1,V2,V3,V4. 
Static Durative Telic 
VI + + + 
V2 + + 
V3 + 
V4 + 
Table 1.1 
Chen\[3\] stated that the situation of a 
sentence not only depends on the main verb 
of the sentence but also on other parts of the 
sentence. That is, although the main verb is 
the most important in determining a 
sentence situation, other parts such as 
adverbs also have effect. Cheri s 
classification is more detailed. 
141 
NO. 
(1) 
(2) 
(3) 
(4) 
(5) 
(6) 
(7) 
(8) 
(9) 
00) 
Verb types 
Attribute 
Mental state 
Position 
Action and 
Mental Activity 
Verb-object Structure 
Change 
Directional Action 
Instantaneous Change 
Instantaneous Action 
Verb-verb or 
Verb-adjective 
Instances 
:E(be), ~(equal) 
;~:l~'~(believe), ~l~J~(re~'et) 
~.~i(stand), ~(sit), J\]~j 
(lie) 
gf~jump), ,~. (think), 
~i=~q~uess) 
i~t~(read (books)), 
I1~(sing (songs)) 
(become) 
/EgE(run up), ~,_J2(climb on) 
~.(die), ~l ie) ,  IS(snap) ..  
~t~(sit), ~td/(stand) 
~J(push down), 
~,TJ~..(smash (into pieces)) 
Table 1.2 
Static 
Situation types 
State + 
Activity 
Accomplishment 
Simple change 
Complex change 
Dura- Telic verb types 
tive (table above) 
(1) (2) 
(3) 
+ (3) (4) 
(5) 
+ + (3) (4) 
+ (6) (7) 
(S) (9) 
(10) 
Table 1.3 
From the tables above, we can find that 
some words(such as (3) and (4) in table 1.3) 
can belong to more than one category, so 
Chen use modifiers, auxiliary words and 
prepositions to eliminate the ambiguity. 
State Acti- 
vity 
Ell l~ l  I \ [  
~1~ +V 
v~ 
~v 
v+(y) 
+TQP+ 
~act) 
V+(T) + + 
+TQP+ 
~m,e)  
TQP: Time Quantity Phrase, (-) : 
? 
(-1 ? ? 
(-) (-) + 
? 
Accom- Complex Simple 
plishment change change 
+ 
? ? 
? ? 
? ? ? 
in most case, it is 
Table 1.4 
2. Our Classification Algorithm for 
Verbg Situation 
2.1 Guiding Thoughts 
(1) Our algorithm is for information 
processing 
eMa\[5\] uses three pairs of phase features in 
classifying, but from which we can not get 
an automatic classification algorithm for 
computers; the classification can only be 
done manually. 
eln linguistics, telicity is a phase feature 
used in classifying. In table 1.1 the 
difference between category V2 and V3, in 
table 1.3, the difference between "activivJ' 
and "accomplishmenf', are attributed to 
telicity. But in information process, we need 
not distinguish whether an event is telic or 
not. For example, 
Exp. 1 
~)~t\]~'j~, (He is playing the flute) 
'~ .  (He is playing a song '%iangzhu " )
Chen\[3\] thinks that in Exp. 1, the first 
sentence has the features: dynamicity, and 
durativity, and non-telicity; it belongs 
to "activi~' . The second sentence has the 
features dynamicity, durativity, and telicity, 
because in the second sentence, there is a 
default terminal . . . .  when the song 
~angzhd' is over, the action '~la~ is 
over, so the sentence belongs to 
'hccomplishmenf instead of  "activity. 
However we think such discrimination is 
useless for information extraction, because 
telicity is an ambiguous concept itself. What 
we need is to acquire the exact duration of 
the event. So if we knew the event is 
durative or not, and got the temporal 
phrases, we can know terminal time of the 
event. Besides, whether an event is telic or 
not can not be attributed to collocation and 
only can be done manually(as the exp 1 
shows). For these reasons, we consider the 
two verbs in Exp. 1 belonging to the same 
situational type, that is, we do not use 
talicity as a phase feature to classifying 
verbs. 
142 
(2) Separate classification of the verb 
situation from classification of the sentence 
situation. 
Chen\[3\] points that some verbs belong 
to more than one category, and gives a 
method to distinguish these cases. To make 
the ideal more clear, we use two steps to 
complete the seritence situation recognition. 
In this paper, we render an algorithm to 
classify verbs into different categories, 
which is the basis of another research ... .  
recognition of sentence situation, which will 
be discussed in future work. 
'Men(MentalityJ' can follow '~1~ (very). 
Verbs in the "AmlS' category can followed 
by "~-~(preposition-objec0' structures, etc. 
The following is the set of collocational 
features. 
Verb+T 
~ll~+Verb 
Verb+~ 
:i-+Verb 
Verb+~ 
Static verbs Amb Act Ins 
Att Men 
+ ? + 
+ 
(-) + (+) 
(-) (-) + 
? 
Table 2.1 
2.2 Classification Method 
We classify the verbs into five categories , 
Att(Attribute), Men(Mentality), Act 
(Activity) , Ins(Instantaneous) , Amb 
(Ambiguous). 
Att: ~(be), ~'(equal), '~'(include), m~(accord with) 
Men: ~.~J~(like), ~.,(belittle), ~(love), ~ff~ 
(be satisfied with) 
Act: ~(draw), ~l~l(gab), ~(drink), ~( run)  
In;  ~?~(explore), ~l;~(extinguish), I~(snap), 
(discovery) 
Amb: ~.~(sit), ~i(stand), Jig(lie), ~(kneel), ~:(bring), 
~(hang), ~(wear), ~-~(install) 
Amb(Ambiguous) include those words 
which describe different situations in 
different context. For example: 
Exp. 2: 
(they hung the picture on the wall. ) 
(Picture is hanging on the wall.) 
In Exp. 2, the two sentences have the 
same predicate '~  (hang). In the first 
sentence, '~  descnbes an instantaneous 
action, but the second sentence describes a
state. In English, forms of these two 
predicates are different; while in Chinese, 
they are the same. For this reason, we 
consider it ambiguous and indistinguishable 
without context. 
We have pointed out previously that 
phase depends only on meaning. However 
different situational types collocate with 
different words. So the essence of our 
algorithm is replace semantic judgement 
with collocational judgement. For example, 
2.3 Implementation of the algorithm 
According to table 2.1, a classification 
algorithm was designed, and we use two 
resources to implement our algorithm: The 
Contemporary Chinese Cihai \[11\] (which 
we will refer to as the Cihai below) 
dictionary and the Machine Tractable 
Dictionary of Contemporary Chinese 
Predicate Verbs \[12\](which we will refer to 
as the predicate dictionary below). The 
Cihai dictionary includes 12,000 entries and 
700,000 collocation instances, predicate 
dictionary includes about 3000 verbs with 
their semantic information, case relations 
and detailed collocation information. These 
two dictionaries both include some of the 
collocation information that the algorithm 
needs. 
Considenng the features of these two 
dictionaries, we adjust part of our algorithm: 
(1) In predicate dictionary, there is a slot 
named "verb typ? , which includes 
'transitive verlY , 'fntransifive verB' , 
~ttribufive verlS", 'linking vertt' etc. So, at 
the beginning of the algorithm, we judge if 
the verb is a "linking verlS' (
~(be~', ~(equa l~ '  ,etc) or a "possessive 
verlS' ('~-q~J' (have)). If it is, we directly 
classify the verb as "att(attribute~' without 
further processing. 
(2) The predicate dictionary provides the 
case relation of verbs, and their semantic 
ategodes. We restrict the agent of verbs in 
the "Men(mentality~' tobelong to one of: 
143 
"{ .)kI"(people), "{ )l,.,~} "(human), "1 
)l,~} "(multitude) , {~s:} 
(collectivity)'; "{:~:-~?~}(creatures)'; ,,~- ~,-ii,~,~,jtr 
bel ieff ,  "{gJJl~}(animal)" (3) Because 
Cihai includes collocation instances instead 
of collocation relations, we should consider 
synonyms. To be exact, when we determine 
whether a verb belongs to "Men(Mentality~' 
or not, we judge if it can follow 
(very) and synonyms such as 
'trY'P; 
However, some seldom seen instances were 
included. All these cause some errors. 
The final algorithm is as follows: 
if (a verb is labeled as" linking verb" or  '~:~ossessive vertf 
in predicate dictionary ) 
then the verb belongs to "Att(Attribute)" 
else if (the verb can follow ":~\[~"(very) and synonyms "~.~,I~", 
"1-?~"; '~l"~ '~71~2', "~" )  and (its agenf s 
semantic belongs to setl*) 
then the verb belongs to "Men(Mentality)" 
else if (it can follow ";t~E") or (be followed by"~") 
then if (it can be followed by "preposition-object" 
structure) 
then the verb belongs to "Amb(Ambiguous)" 
else the verb belongs to "Act(Activity)" 
else if (it can be followed by"T") 
then the verb belongs to"Ins(Instantaneous)" 
else the verb belongs tff unknown" 
*setl={human, multitude, collectivity, creatures, belief, 
animal } 
3. Resu l ts  and  Ana lys i s  
3.1 Results 
We use the algorithm above to classify 
the 2981 words in predicate dictionary, at 
the same time, we do the classification 
manually, Table 3.1 is the result: 
Att Men Amb Ins Act Un- Totel 
known 
Human 20 112 500 662 1683 4* 2981 
Algo- 20 111 537 691 1519 i 103 2981 
Rithm i 
*this 4 words are not verb. 
Table 3.1 
Table 2.2 shows the details: 
by a lgo -  Classifying b human 
rithm Att Men Amb Ins Act Non- Totel 
verb 
Art 20 0 0 0 0 0 20 
Men 0 99 1 1 10 0 111 
Amb 0 0 473 9 55 0 537 
Ins 0 0 3 637 51 0 691 
Act 0 1 12 2 1504 1519 
Un- 0 12 11 13 63 103 
known 
Totel 20 112 500 662 1683 4 2981 
Table 3.2 
Table 3.3 shows precision and recall: 
Att Men Amb Ins Act Average 
Precision 100.0 89.9 88.1 92.2 99.0 93.8 
Recall 100.0 88.4 94.6 96.2 89.4 93.7 
Table 3.3 
3.2 Analysis 
We mainly analyze the errors: 
(1) Failure of algorithm 
Chinese .is a very complex language. 
Replacing semantic judgment by using 
collocations has limitation itself. For 
example, in most cases, whether a verb is 
durative or not can be decided by whether it 
can be used in such structure "verb-g ~"  (In 
most case, "~ represents an action in 
progress). But some instantaneous verbs 
such as ~.(knocky, can also be used in 
such structure to express a repeated action. 
(2) Errors caused by the resources 
(2.1) Collocation incompleteness in 
Cihai: for example, '~(d isagreeJ '  can 
collocate with '~  (very), but this 
collocation is not included in Cihai. 
(2.2)Errors caused by predicate dictionary: 
It is obvious that a certain proportion of 
dictionary errors is inevitable. For example, 
though '~  (beg) can follow '~i~E to 
represent the action is in progress, it is not 
included in the corresponding slot of 
predicate dictionary. 
(2.3) The inconsistency between the two 
dictionaries: For example, 
NN (admire), (mind), and ( 
belittle) are included in predicate dictionary 
but not in Cihai. Although ~ (regret) is 
included in Cihai, it is taken as an adjective 
instead of a verb. 
144 
4. Conclusion 
In this paper, we advance a phase-based 
method to analyze temporal information in 
Chinese. For this purpose, an algorithm is 
rendered to classify verbs into different 
situation types. Because a verl5 s situation 
depends on the meaning of the verb, the 
essence of our algorithm takes advantage of 
collocations to avoid semantics. The result 
shows the algorithm is successful. We also 
believe that if the errors caused by resources 
were eliminated, the result would be 
improved significantly. 
Although the five categories are defined 
by us, they can describe basic situations of 
Chinese. The classification algorithm itself 
is independent of resources, so it can be 
applied to other resources (dictionaries) if
these resources include sufficient collocation 
information. Furthermore, Discarding 
dictionaries and doing classification directly 
on large-scale real corpus, especially in 
certain domain, deserve the future research. 
Our algorithm is very useful for the 
future analysis of sentence situation for 
Information Extraction system and for 
dictionary construction. 
\[5\]Ma Qingzhu. Time Quantity Phrase and 
Categories of Verbs: China Chinese. 
Vol.2,1981. 
\[6\]Hu Yushu & Fan Xiao. Research on 
Verbs, Henan Univ. Press, 1995 
\[7\]Hwang C.H. & Schubert L. K. 
Interpreting Tense, Aspect and Time 
Adverbials: A Compositional, Unified 
Approach : Proceeding of 1st International 
Conference in Temporal Logic, Bonn, 
Germany, July 1994. 
\[8\]Allen J.F. Towards a General Theory of 
Action and Time: Artificial Intelligence, 
23,123-154. 
\[9\]Allen J.F. & George, F.. Action and 
Events in Interval Temporal Logic: Journal 
of Logic and Computation, Special Issue on 
Actions and Processes, 1994. 
\[10lion Androutsopoulos, Graeme Ritche & 
Peter Thanisch. Time ,Tense and Aspect 
in Natural Language Database Interface, 
CMP-LG Mar 1998. 
\[l l\]Ni Wenjie . Contemporary Chinese 
Cihai, People China Press, 1994. 
\[ 12\]Chen Qunxiu Designing and 
implement of Machine Tractable Dictionary 
of Contemporary Chinese Predicate 
Verbs, Proceedings of ICCC96, Singapore, 
Jun, 1996. 
References 
\[ 1 \]Message Understanding Conference 
Website, http://www.muc.saic.com. 
\[2\]Message Understanding Evaluation and 
Conference: Proceedings of 3rd-6th APRA 
Workshops, Morgan Kaufmann Publishers 
Inc., 1996. 
\[3\]Chen ping. Discussion On Temporal 
System of Contemporary Chinese: China 
Chinese Vol.6,1998. 
\[4\]Gong Qianyan. Phase, Tense and Aspect 
of Chinese, Commercial Press. 
145 
A Model for Processing Temporal References in Chinese
Wenjie Li, Kam-Fai Wong
Department of Systems Engineering
and Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
fwjli, kfwongg@se.cuhk.edu.hk
Chunfa Yuan
Department of Computer Science
and Technology
Tsinghua University,
Beijing, 100084, P.R. China
ycf@s1000e.cs.tsinghua.edu.cn
Abstract
Conventional information systems can-
not cater for temporal information eec-
tively. For this reason, it is useful to cap-
ture and maintain the temporal knowl-
edge (especially the relative knowledge)
associated to each action in an informa-
tion system. In this paper, we propose a
model to mine and organize temporal re-
lations embedded in Chinese sentences.
Three kinds of event expressions are ac-
counted for, i.e. single event, multiple
events and declared event(s). Experi-
ments are conducted to evaluate the min-
ing algorithm using a set of news reports
and the results are signicant. Error
analysis has also been performed open-
ing up new doors for future research.
1 Introduction
Information Extraction (IE) is an upcoming chal-
lenging research area to cope with the increas-
ing volume of unwieldy distributed information re-
sources, such as information over WWW. Among
them, temporal information is regarded as an
equally, if not more, important piece of infor-
mation in domains where the task of extract-
ing and tracking information over time occurs
frequently, such as planning, scheduling and
question-answering. It may be as simple as an ex-
plicit or direct expression in a written language,
such as \the company closed down in May, 1997";
or it may be left implicit, to be recovered by read-
ers from the surrounding texts. For example, one
may know the fact that \the company has closed
down before the earthquake", yet without know-
ing the exact time of the bankruptcy. Relative
temporal knowledge such as this where the pre-
cise time is unavailable is typically determined by
human. An information system which does not
account for this properly is thus rather restrictive.
It is hard to separate temporal information (in
particular refers to temporal relations in this pa-
per) discovery from natural language processing.
In English, tenses and aspects reected by dier-
ent verb forms are important elements in a sen-
tence for expressing temporal reference (Steed-
man, 97) and for transforming situations into
temporal logic operators (Bruce, 72). The pio-
neer work of Reichenbach (Reichenbach, 47) on
tenses forms the basis of many subsequent re-
search eorts in temporal natural language pro-
cessing, e.g. the work of Prior in tense logic (Prior,
67), and of Hwang et alin tense tree (Hwang
92) and temporal adverbial analysis (Hwang 94),
etc. Reichenbach argued that the tense system
provided predication over three underlying times,
namely S (speech time), R (reference time), and
E (event time). Later, a multiple temporal refer-
ences model was introduced by Bruce (Bruce, 72).
He dened the set (S
1
; S
2
; :::; S
n
), which is an el-
ement of a tense. S
1
corresponds to the time of
speech. Each S
i
(i = 2; :::; n  1) is a time of ref-
erence, and S
n
, the time of an event. To facilitate
logic manipulation, Bruce proposed seven rst or-
der logic relations based on time intervals and a
method to map nine English tenses into tempo-
ral rst order logic expressions
1
. His work laid
down the foundation of temporal logic in natu-
ral language. These relations were then gradually
expanded to nine in (Allen, 81) and further to
thirteen in (Allen, 83)
2
.
In contrast, Chinese verbs appear in only one
1
The seven relations are symbolized as R(A;B) for
relation R and time intervals A and B, where R in-
cludes before, after, during, contains, same-time, over-
laps or overlapped-by.
2
meet, met-by, starts, started-by, nishes and
nished-by are added into temporal relations.
form. The lack of regular morphological tense
markers renders Chinese temporal expressions
complicated. For quite a long time, linguists ar-
gued whether tenses existed in Chinese; and if
they did how are they expressed. We believe that
Chinese do have tenses. But they are determined
with the assistance of temporal adverbs and aspect
auxiliary words. For example, ? ...  (being), .
? ... ? (was/been) and  ... (will be) express an
ongoing action, a situation started or nished in
the past, and a situation which will occur in the
future, respectively. Therefore, the conventional
theory to determine temporal information based
on verb axations is inapplicable. Over the past
years, there has been considerable progress in the
areas of information extraction and temporal logic
in English (Antony, 87; Bruce, 72; Kaufmann, 97).
Nevertheless, only a few researchers have investi-
gated these areas in Chinese.
The objective of our research is to design and
develop a temporal information extraction sys-
tem. For practical and cultural reason, the appli-
cation target is on-line nancial news in Chinese.
The nal system, referred to as TICS (Tempo-
ral Information-extraction from Chinese Sources),
will accept a series of Chinese nancial texts as in-
put, analyze each sentence one by one to extract
the desirable temporal information, represent each
piece of information in a concept frame, link all
frames together in chronological order based on
inter- or intra-event relations, and nally apply
this linked knowledge to fulll users' queries.
In this paper, we introduce a fundamental
model of TICS, which is designed to mine and
organize temporal relations embedded in Chinese
sentences. Three kinds of event expressions are
accounted for, i.e. single event, multiple events
and declared event(s). This work involved four
major parts, (1) built temporal model; (2) con-
structed rules sets; (3) developed the algorithm;
and (4) set up the experiments and performed the
evaluation.
2 A Model for Temporal Relation
Discovery
2.1 Temporal Concept Frame
In IE, it is impossible as well as impractical to
extract all the information from an incoming doc-
ument. For this reason, all IE systems are geared
for specic application domains. The domain is
determined by a pre-specied concept dictionary.
Then a certain concept is triggered by several lex-
ical items and activated in the specic linguistic
contexts. Each concept denition contains a set
of slots for the extracted information. In addition,
it contains a set of enabling conditions which are
constraints that must be satised in order for the
concept to be activated. Due to its versatility, a
frame structure is generally used to represent con-
cepts (as shown in Figure 1).
Slots in a temporal concept frame are divided
into two types: activity-related and time-related.
Activity-related slots provide the descriptions of
objects and actions concerning the concept. For
example, company predator, company target and
purchase value are the attributes of the concept
(B?, TAKEOVER). Meanwhile, time-related
slots provide information related to when a con-
cept begins or nishes, how long does it last and
how does it relate to another concept, etc.
referenced concept frame
......
(temporal relations
among activities)
{AND, OR}
Absolute relation
relation 1
relation 
time
time
relation n reference
relative relation
relation 1 reference
Delare frame
speaksman
location
reliability
absolute relation
relative relation
source
absolute relation
publish frame
n
......
......
......
......
Temporal slots
absolute relation
declare
duration
reliability
Activity slots
company
Concept frame
......
concept type
publish
company frame
company name
employees no.
turnover
parent company
company name
employees no.
turnover
parent company
company frame
among entities)(static relations
relative relation
Figure 1: Temporal concept frame construction
2.2 Temporal Relations
The system is designed with two sets of temporal
relations, namely absolute and relative relations.
The role of absolute temporal relations is to posi-
tion situation occurrences on a time axis. These
relations depict the beginning and/or ending time
bounds of an occurrence or its relevance to refer-
ence times, see TR(T ) in Section 2.3. Absolute
relations are organized by Time Line in the sys-
tem, see Figure 2.
r:                                   t:                                d:absolute relation      time parameter        duration
13/6/99
Time_Line
17/6/99 21/6/99 3/7/9929/6/99
New World demands
the payment of 10-
million-dollar debt
from CTI
Nan Hua takes over
Si Hai Travel with
1st Pacific takes over
from Lin Shao Liang
40% Yin Duo Fu?s stock
Jing Kuang sells DC
holdings to Dong Fong
Hong
Ba Ling buys 30%
Lian
stock from Zhong
o o ooo
Event B Time specification Event C Time specificationTime specification
t r r t dtr
25/6/99
23 million dollar
Event A
d d ......
Figure 2: The T ime Line organization for abso-
lute relations in TICS
In many cases, the time when an event takes
place may not be known. But its relevance to
another occurrence time is given. Relative tem-
poral knowledge such as this is manifested by rel-
ative relations. Allen has proposed thirteen re-
lations. The same is adopted in our system, see
TR(E
i
; E
j
) in Section 2.3. The relative relations
are derived either directly from a sentence describ-
ing two situations, or indirectly from the absolute
relations of two individual situations. They are or-
ganized by Relational Chains, as shown in Figure
3.
o                                       o
before
o                                       o
before
R d R d R d
R d R d R d
same_as
o o
o
contains
before
before
Event A
Si Tong parent
stock
Event CEvent A
Hua Ruen places
company reorgnizes
Event B
decreases by 1.3%
The price of Hua Ruen
Event D
Event E Event F
CKI acquires 20% of 
Envestra Limited from Richard Li for $3.8 billion
Event G
Ba Ling buye 30%
stock from Zhong Lian
R d
Event B Event C Event D
Event B Event E Event F
R d R d
Event C Event G
Si Tong parent
company goes public 
Tricom purchases 60% of PCC
R:                                     d:relative relation       duration
Figure 3: The Relational Chain organization for
relative relations in TICS
2.3 Temporal Model
This section describes our temporal model for
discovering relations from Chinese sentences.
Suppose TR indicates a temporal relation, E
indicates an event and T indicates time. The
absolute and relative relations are symbolized as:
OCCUR(E
i
; TR(T ))
3
and TR(E
i
; E
j
), respec-
tively. The sets of TR are:
TR(T ) = fON; BEGIN; END; PAST ,
FUTUER; ONGOING; CONTINUEDg
TR(E
i
; E
j
) = fBEFORE; AFTER; MEETS,
METBY; OV ERLAPS; OV ERLAPPED,
DURING; CONTAINS; STAREDBY ,
STARTS; FINISHES; FINISHEDBY ,
SAME ASg
For an absolute relation of a single event, T is
an indispensable parameter, which includes event
time t
e
, reference time t
r
4
and speech time t
s
:
3
OCCUR is a predicate for the happening of a sin-
gle event. Under the situations where there are no am-
biguity, E
i
can be omitted. The OCCUR(E
i
; TR(T )
is simplied as TR(T ).
4
There maybe exist more than one reference time
in a statement.
T = ft
e
; t
r
; t
s
g
Some Chinese words can function as the tem-
poral indicators. These include time word (TW ),
time position word (F ), temporal adverb (ADV ),
auxiliary word (AUX), preposition word (P ),
auxiliary verb (V A), trend verb (V C) and some
special verbs (V V ). They are all regarded as the
elements of the temporal indicator TI :
TI = fTW; F; ADV; AUX; V A; V C; V V; Pg
Each type of the indicators, e.g. TW , con-
tains a set of words, such as TW = twlist =
ftw
1
; tw
2
; :::tw
n
g, with each word having an tem-
poral attribute, indicated by ATT .
The core of the model is thus a rule set R
which maps the combinational eects of all the
indicators, TI , in a sentence to its corresponding
temporal relation, TR,
R : TI !
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 185?188,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extractive Summarization Based on Event Term Clustering 
Maofu Liu1,2, Wenjie Li1, Mingli Wu1 and Qin Lu1 
 
1Department of Computing 
The Hong Kong Polytechnic University 
{csmfliu, cswjli, csmlwu, 
csluqin}@comp.polyu.edu.hk 
2College of Computer Science and Technology 
Wuhan University of Science and Technology 
mfliu_china@hotmail.com 
 
Abstract 
Event-based summarization extracts and 
organizes summary sentences in terms of 
the events that the sentences describe. In 
this work, we focus on semantic relations 
among event terms. By connecting terms 
with relations, we build up event term 
graph, upon which relevant terms are 
grouped into clusters. We assume that each 
cluster represents a topic of documents. 
Then two summarization strategies are 
investigated, i.e. selecting one term as the 
representative of each topic so as to cover 
all the topics, or selecting all terms in one 
most significant topic so as to highlight the 
relevant information related to this topic. 
The selected terms are then responsible to 
pick out the most appropriate sentences 
describing them. The evaluation of 
clustering-based summarization on DUC 
2001 document sets shows encouraging 
improvement over the well-known 
PageRank-based summarization. 
1 Introduction 
Event-based extractive summarization has emerged 
recently (Filatova and Hatzivassiloglou, 2004). It 
extracts and organizes summary sentences in terms 
of the events that sentences describe.  
We follow the common agreement that event 
can be formulated as ?[Who] did [What] to [Whom] 
[When] and [Where]? and ?did [What]? denotes 
the key element of an event, i.e. the action within 
the formulation. We approximately define the 
verbs and action nouns as the event terms which 
can characterize or partially characterize the event 
occurrences. 
Most existing event-based summarization 
approaches rely on the statistical features derived 
from documents and generally associated with 
single events, but they neglect the relations among 
events. However, events are commonly related 
with one another especially when the documents to 
be summarized are about the same or very similar 
topics. Li et al(2006) report that the improved 
performance can be achieved by taking into 
account of event distributional similarities, but it 
does not benefit much from semantic similarities. 
This motivated us to further investigate whether 
event-based summarization can take advantage of 
the semantic relations of event terms, and most 
importantly, how to make use of those relations. 
Our idea is grouping the terms connected by the 
relations into the clusters, which are assumed to 
represent some topics described in documents. 
In the past, various clustering approaches have 
been investigated in document summarization. 
Hatzivassiloglou et al(2001) apply clustering 
method to organize the highly similar paragraphs 
into tight clusters based on primitive or composite 
features. Then one paragraph per cluster is selected 
to form the summary by extraction or by 
reformulation. Zha (2002) uses spectral graph 
clustering algorithm to partition sentences into 
topical groups. Within each cluster, the saliency 
scores of terms and sentences are calculated using 
mutual reinforcement principal, which assigns high 
salience scores to the sentences that contain many 
terms with high salience scores. The sentences and 
key phrases are selected by their saliency scores to 
generate the summary. The similar work based on 
topic or event is also reported in (Guo and Stylios, 
2005).
The granularity of clustering units mentioned 
above is rather coarse, either sentence or paragraph. 
In this paper, we define event term as clustering 
185
unit and implement a clustering algorithm based on 
semantic relations. We extract event terms from 
documents and construct the event term graph by 
linking terms with the relations. We then regard a 
group of closely related terms as a topic and make 
the following two alterative assumptions:  
(1) If we could find the most significant topic as 
the main topic of documents and select all terms in 
it, we could summarize the documents with this 
main topic.  
(2) If we could find all topics and pick out one 
term as the representative of each topic, we could 
obtain the condensed version of topics described in 
the documents.  
Based on these two assumptions, a set of cluster 
ranking, term selection and ranking and sentence 
extraction strategies are developed. The remainder 
of this paper is organized as follows. Section 2 
introduces the proposed extractive summarization 
approach based on event term clustering. Section 3 
presents experiments and evaluations. Finally, 
Section 4 concludes the paper. 
2 Summarization Based on Event Term 
Clustering 
2.1 Event Term Graph 
We introduce VerbOcean (Chklovski and Pantel, 
2004), a broad-coverage repository of semantic 
verb relations, into event-based summarization. 
Different from other thesaurus like WordNet, 
VerbOcean provides five types of semantic verb 
relations at finer level. This just fits in with our 
idea to introduce event term relations into 
summarization. Currently, only the stronger-than 
relation is explored. When two verbs are similar, 
one may denote a more intense, thorough, 
comprehensive or absolute action. In the case of 
change-of-state verbs, one may denote a more 
complete change. This is identified as the stronger-
than relation in (Timothy and Patrick, 2004). In 
this paper, only stronger-than is taken into account 
but we consider extending our future work with 
other applicable relations types.  
The event term graph connected by term 
semantic relations is defined formally as 
, where V is a set of event terms and E 
is a set of relation links connecting the event terms 
in V. The graph is directed if the semantic relation 
has the characteristic of the asymmetric. Otherwise, 
it is undirected. Figure 1 shows a sample of event 
term graph built from one DUC 2001 document set. 
It is a directed graph as the stronger-than relation 
in VerbOcean exhibits the conspicuous asymmetric 
characteristic. For example, ?fight? means to 
attempt to harm by blows or with weapons, while 
?resist? means to keep from giving in. Therefore, a 
directed link from ?fight? to ?resist? is shown in 
the following Figure 1.  
),( EVG =
Relations link terms together and form the event 
term graph. Based upon it, term significance is 
evaluated and in turn sentence is judged whether to 
be extracted in the summary. 
 
Figure 1. Terms connected by semantic relations 
2.2 Event Term Clustering 
Note that in Figure 1, some linked event terms, 
such as ?kill?, ?rob?, ?threaten? and ?infect?, are 
semantically closely related. They may describe 
the same or similar topic somehow. In contrast, 
?toler?, ?resist? and ?fight? are clearly involved in 
another topic; although they are also reachable 
from ?kill?. Based on this observation, a clustering 
algorithm is required to group the similar and 
related event terms into the cluster of the topic.  
In this work, event terms are clustered by the 
DBSCAN, a density-based clustering algorithm 
proposed in (Easter et al 1996). The key idea 
behind it is that for each term of a cluster the 
neighborhood of a given radius has to contain at 
least a minimum number of terms, i.e. the density 
in the neighborhood has to exceed some threshold. 
By using this algorithm, we need to figure out 
appropriate values for two basic parameters, 
namely, Eps (denoting the searching radius from 
each term) and MinPts (denoting the minimum 
number of terms in the neighborhood of the term). 
We assign one semantic relation step to Eps since 
there is no clear distance concept in the event term 
186
graph. The value of Eps is experimentally set in 
our experiments. We also make some modification 
on Easter?s DBSCAN in order to accommodate to 
our task.  
Figure 2 shows the seven term clusters 
generated by the modified DBSCAN clustering 
algorithm from the graph in Figure 1. We represent 
each cluster by the starting event term in bold font.  
fight
resist
consider
expect
announce
offer
list public
accept
honor
publish study
found
place
prepare
toler
pass
fear
threaten
kill
feel suffer
live
survive
undergo
ambush
rob
infect
endure
run
moverush
report
investigate
file
satisfy
please
manage
accept
Figure 2. Term clusters generated from Figure 1 
2.3 Cluster Ranking 
The significance of the cluster is calculated by  
? ??
? ??
=
CC Ct
t
Ct
ti
i ii
ddCsc /)(  
where  is the degree of the term t  in the term 
graph. C  is the set of term clusters obtained by the 
modified DBSCAN clustering algorithm and  is 
the ith one. Obviously, the significance of the 
cluster is calculated from global point of view, i.e. 
the sum of the degree of all terms in the same 
cluster is divided by the total degree of the terms in 
all clusters. 
td
iC
2.4 Term Selection  and Ranking 
Representative terms are selected according to the 
significance of the event terms calculated within 
each cluster (i.e. from local point of view) or in all 
clusters (i.e. from global point of view) by  
LOCAL:  or ?
?
=
ict
tt ddtst /)(
GLOBAL:  ? ?
? ?
=
Cc ct
tt
i i
ddtst /)(
Then two strategies are developed to select the 
representative terms from the clusters.  
(1) One Cluster All Terms (OCAT) selects all 
terms within the first rank cluster. The selected 
terms are then ranked according to their 
significance.  
(2) One Term All Cluster (OTAC) selects one 
most significant term from each cluster. Notice that 
because terms compete with each other within 
clusters, it is not surprising to see )()( 21 tsttst <  
even when , . To 
address this problem, the representative terms are 
ranked according to the significance of the clusters 
they belong to.  
)()( 21 csccsc > ),( 2211 ctct ??
2.5 Sentence Evaluation and Extraction 
A representative event term may associate to more 
than one sentence. We extract only one of them as 
the description of the event. To this end, sentences 
are compared according to the significance of the 
terms in them. MAX compares the maximum 
significance scores, while SUM compares the sum 
of the significance scores. The sentence with either 
higher MAX or SUM wins the competition and is 
picked up as a candidate summary sentence. If the 
sentence in the first place has been selected by 
another term, the one in the second place is chosen. 
The ranks of these candidates are the same as the 
ranks of the terms they are selected for. Finally, 
candidate sentences are selected in the summary 
until the length limitation is reached. 
3 Experiments 
We evaluate the proposed approaches on DUC 
2001 corpus which contains 30 English document 
sets. There are 431 event terms on average in each 
document set. The automatic evaluation tool, 
ROUGE (Lin and Hovy, 2003), is run to evaluate 
the quality of the generated summaries (200 words 
in length). The tool presents three values including 
unigram-based ROUGE-1, bigram-based ROUGE-
2 and ROUGE-W which is based on longest 
common subsequence weighted by the length. 
Google?s PageRank (Page and Brin, 1998) is 
one of the most popular ranking algorithms. It is 
also graph-based and has been successfully applied 
in summarization. Table 1 lists the result of our 
implementation of PageRank based on event terms. 
We then compare it with the results of the event 
term clustering-based approaches illustrated in 
Table 2. 
 PageRank  
ROUGE-1 0.32749 
187
ROUGE-2 0.05670 
ROUGE-W 0.11500 
Table 1. Evaluations of PageRank-based 
Summarization 
LOCAL+OTAC MAX SUM 
ROUGE-1 0.32771 0.33243
ROUGE-2 0.05334 0.05569
ROUGE-W 0.11633 0.11718
GLOBAL+OTAC MAX SUM 
ROUGE-1 0.32549 0.32966
ROUGE-2 0.05254 0.05257
ROUGE-W 0.11670 0.11641
LOCAL+OCAT MAX SUM 
ROUGE-1 0.33519 0.33397
ROUGE-2 0.05662 0.05869
ROUGE-W 0.11917 0.11849
GLOBAL+OCAT MAX SUM 
ROUGE-1 0.33568 0.33872
ROUGE-2 0.05506 0.05933
ROUGE-W 0.11795 0.12011
Table 2. Evaluations of Clustering-based  
Summarization 
The experiments show that both assumptions are 
reasonable. It is encouraging to find that our event 
term clustering-based approaches could outperform 
the PageRank-based approach. The results based 
on the second assumption are even better. This 
suggests indeed there is a main topic in a DUC 
2001 document set. 
4 Conclusion 
In this paper, we put forward to apply clustering 
algorithm on the event term graph connected by 
semantic relations derived from external linguistic 
resource. The experiment results based on our two 
assumptions are encouraging. Event term 
clustering-based approaches perform better than 
PageRank-based approach. Current approaches 
simply utilize the degrees of event terms in the 
graph. In the future, we would like to further 
explore and integrate more information derived 
from documents in order to achieve more 
significant results using the event term clustering-
based approaches. 
Acknowledgments 
The work described in this paper was fully 
supported by a grant from the Research Grants 
Council of the Hong Kong Special Administrative 
Region, China (Project No. PolyU5181/03E). 
References 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries using N-gram 
Cooccurrence Statistics. In Proceedings of HLT/ 
NAACL 2003, pp71-78. 
Elena Filatova and Vasileios Hatzivassiloglou. 2004. 
Event-based Extractive Summarization. In 
Proceedings of ACL 2004 Workshop on 
Summarization, pp104-111. 
Hongyuan Zha. 2002. Generic Summarization and 
keyphrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In Proceedings 
of the 25th annual international ACM SIGIR 
conference on Research and development in 
information retrieval, 2002. pp113-120.   
Lawrence Page and Sergey Brin, Motwani Rajeev 
and Winograd Terry. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Martin Easter, Hans-Peter Kriegel, J?rg Sander, et al 
1996. A Density-Based Algorithm for Discovering 
Clusters in Large Spatial Databases with Noise. In 
Proceedings of the 2nd International Conference 
on Knowledge Discovery and Data Mining, Menlo 
Park, CA, 1996. 226-231.  
Lawrence Page, Sergey Brin, Rajeev Motwani and 
Terry Winograd. 1998. The PageRank 
CitationRanking: Bring Order to the Web. 
Technical Report,Stanford University. 
Timothy Chklovski and Patrick Pantel. 2004. 
VerbOcean: Mining the Web for Fine-Grained 
Semantic Verb Relations. In Proceedings of 
Conference on Empirical Methods in Natural 
Language Processing, 2004. 
Vasileios Hatzivassiloglou, Judith L. Klavans, 
Melissa L. Holcombe, et al 2001. Simfinder: A 
Flexible Clustering Tool for Summarization. In 
Workshop on Automatic Summarization, NAACL, 
2001. 
Wenjie Li, Wei Xu, Mingli Wu, et al 2006. 
Extractive Summarization using Inter- and Intra- 
Event Relevance. In Proceedings of ACL 2006, 
pp369-376. 
Yi Guo and George Stylios. 2005. An intelligent 
summarization system based on cognitive 
psychology. Journal of Information Sciences, 
Volume 174, Issue 1-2, Jun. 2005, pp1-36. 
188
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 134?142,
Beijing, August 2010
Simultaneous Ranking and Clustering of Sentences: A Reinforcement 
Approach to Multi-Document Summarization 
1Xiaoyan Cai, 1Wenjie Li, 1You Ouyang, 2Hong Yan 
1Department of Computing, The Hong Kong Polytechnic University 
{csxcai,cswjli,csyouyang}@comp.polyu.edu.hk 
2Department of Logistics and Maritime Studies, The Hong Kong Polytechnic University 
lgthyan@polyu.edu.hk 
 
 
Abstract 
Multi-document summarization aims to 
produce a concise summary that contains 
salient information from a set of source 
documents. In this field, sentence ranking 
has hitherto been the issue of most concern. 
Since documents often cover a number of 
topic themes with each theme represented 
by a cluster of highly related sentences, 
sentence clustering was recently explored in 
the literature in order to provide more 
informative summaries. Existing cluster-
based ranking approaches applied clustering 
and ranking in isolation. As a result, the 
ranking performance will be inevitably 
influenced by the clustering result. In this 
paper, we propose a reinforcement approach 
that tightly integrates ranking and clustering 
by mutually and simultaneously updating 
each other so that the performance of both 
can be improved. Experimental results on 
the DUC datasets demonstrate its 
effectiveness and robustness. 
1 Introduction 
Automatic multi-document summarization has 
drawn increasing attention in the past with the 
rapid growth of the Internet and information 
explosion. It aims to condense the original text 
into its essential content and to assist in 
filtering and selection of necessary information. 
So far extractive summarization that directly 
extracts sentences from documents to compose 
summaries is still the mainstream in this field. 
Under this framework, sentence ranking is the 
issue of most concern. 
Though traditional feature-based ranking 
approaches and graph-based approaches 
employed quite different techniques to rank 
sentences, they have at least one point in 
common, i.e., all of them focused on sentences 
only, but ignored the information beyond the 
sentence level (referring to Figure 1(a)). 
Actually, in a given document set, there 
usually exist a number of themes (or topics) 
with each theme represented by a cluster of 
highly related sentences (Harabagiu and 
Lacatusu, 2005; Hardy et al, 2002). These 
theme clusters are of different size and 
especially different importance to assist users 
in understanding the content in the whole 
document set. The cluster level information is 
supposed to have foreseeable influence on 
sentence ranking.  
 
Figure 1. Ranking vs. Clustering 
In order to enhance the performance of 
summarization, recently cluster-based ranking 
approaches were explored in the literature 
(Wan and Yang, 2006; Sun et al 2007; Wang 
et al 2008a,b; Qazvinian and Radev, 2008). 
Normally these approaches applied a clustering 
algorithm to obtain the theme clusters first and 
then ranked the sentences within each cluster 
or by exploring the interaction between 
sentences and obtained clusters (referring to 
Figure 1(b)). In other words, clustering and 
ranking are regarded as two independent 
processes in these approaches although the 
cluster-level information has been incorporated 
into the sentence ranking process. As a result, 
Ranking Ranking 
Clustering 
Ranking 
Clustering
(a)                           (b)                           (c) 
134
the ranking performance is inevitably 
influenced by the clustering result.  
To help alleviate this problem, we argue in 
this paper that the quality of ranking and 
clustering can be both improved when the two 
processes are mutually enhanced (referring to 
Figure 1(c)). Based on it, we propose a 
reinforcement approach that updates ranking 
and clustering interactively and iteratively to 
multi-document summarization. The main 
contributions of the paper are three-fold: (1) 
Three different ranking functions are defined 
in a bi-type document graph constructed from 
the given document set, namely global, within-
cluster and conditional rankings, respectively. 
(2) A reinforcement approach is proposed to 
tightly integrate ranking and clustering of 
sentences by exploring term rank distributions 
over the clusters. (3) Thorough experimental 
studies are conducted to verify the 
effectiveness and robustness of the proposed 
approach. 
The rest of this paper is organized as follows. 
Section 2 reviews related work in cluster-based 
ranking. Section 3 defines ranking functions 
and explains reinforced ranking and clustering 
process and its application in multi-document 
summarization. Section 4 presents experiments 
and evaluations. Section 5 concludes the paper.  
2 Related Work 
Clustering has become an increasingly 
important topic with the explosion of 
information available via the Internet. It is an 
important tool in text mining and knowledge 
discovery. Its ability to automatically group 
similar textual objects together enables one to 
discover hidden similarity and key concepts, as 
well as to summarize a large amount of text 
into a small number of groups (Karypis et al, 
2000).  
To summarize a scientific paper, Qazvinian 
and Radev (2008) presented two sentence 
selection strategies based on the clusters which 
were generated by a hierarchical 
agglomeration algorithm applied in the citation 
summary network. One was called C-RR, 
which started with the largest cluster and 
extracted the first sentence from each cluster in 
the order they appeared until the summary 
length limit was reached. The other was called 
C-LexRank, which was similar to C-RR but 
adopted LexRank to rank the sentences within 
each cluster and chose the most salient one. 
Meanwhile, Wan and Yang (2008) proposed 
two models to incorporate the cluster-level 
information into the process of sentence 
ranking for generic summarization. While the 
Cluster-based Conditional Markov Random 
Walk model (ClusterCMRW) incorporated the 
cluster-level information into the text graph 
and manipulated clusters and sentences equally, 
the Cluster-based HITS model (ClusterHITS) 
treated clusters and sentences as hubs and 
authorities in the HITS algorithm.  
Besides, Wang et al (2008) proposed a 
language model to simultaneously cluster and 
summarize documents. Nonnegative 
factorization was performed on the term-
document matrix using the term-sentence 
matrix as the base so that the document-topic 
and sentence-topic matrices could be 
constructed, from which the document clusters 
and the corresponding summary sentences 
were generated simultaneously. 
3 A Reinforcement Approach to 
Multi-document Summarization 
3.1 Document Bi-type Graph 
First of all, let?s introduce the sentence-term 
bi-type graph model for a set of given 
documents D, based on which the algorithm of 
reinforced ranking and clustering is developed. 
Let >=< WEVG ,, , where V is the set of 
vertices that consists of the sentence set 
},,,{ 21 nsssS ?=  and the term set 
},,{ 21 mtttT ,?= , i.e., TSV ?= , E is the set of 
edges that connect the vertices, i.e., 
},|,{ VvvvvE jiji ?><= . W is the adjacency 
matrix in which the element ijw  represents the 
weight of the edge connecting iv  and jv . 
Formally, W can be decomposed into four 
blocks, i.e., SSW , STW , TSW  and TTW , each 
representing a sub-graph of the textual objects 
indicated by the subscripts. W can be written as 
???
?
???
?=
TTTS
STSS
WW
WW
W ,       
where ),( jiWST  is the number of times the 
term jt  appears in the sentence is . )(i,jWSS  is 
135
the number of common terms in the sentences 
is  and js . TSW  is equal to 
T
STW  as the 
relationships between terms and sentences are 
symmetric. For simplification, in this study we 
assume there is no direct relationships between 
terms, i.e., 0=TTW . In the future, we will 
explore effective ways to integrate term 
semantic relationships into the model.  
3.2 Basic Ranking Functions 
Recall that our ultimate goal is sentence 
ranking. As an indispensable part of the 
approach, the basic ranking functions need to 
be defined first.  
3.2.1 Global Ranking (without Clustering) 
Let )( isr  (i=1, 2, ?, n) and )( jtr  (j=1, 2, ?, 
m) denote the ranking scores of the sentence is  
and the term jt  in the whole document set, 
respectively. Based on the assumptions that 
?Highly ranked terms appear in highly ranked 
sentences, while highly ranked sentences 
contain highly ranked terms. Moreover, a 
sentence is ranked higher if it contains many 
terms that appear in many other highly ranked 
sentences.? 
we define  
)(),()1()(),()(
11
j
n
j
SS
m
j
jSTi srjiWtrjiWsr ??
==
???+??= ?? (1) 
and  
)(),()(
1
i
n
i
TSj srijWtr ?
=
?= .      (2) 
For calculation purpose, )( isr  and )( jtr  are 
normalized by  
?
=
?
n
i
i
i
i
sr
sr
sr
1'
' )(
)(
)(  and 
?
=
?
m
j
j
j
j
tr
tr
tr
1'
' )(
)(
)( . 
Equations (1) and (2) can be rewritten using 
the matrix form, i.e.,  
???
???
?
?
?=
?
???+?
??=
||)(||
)(
)(
||)(||
)(
)1(
||)(||
)(
)(
SrW
SrW
Tr
SrW
SrW
TrW
TrW
Sr
TS
TS
SS
SS
ST
ST ??
. (3) 
We call )(Sr  and )(Tr  the ?global ranking 
functions?, because at this moment sentence 
clustering is not yet involved and all the 
sentences/terms in the whole document set are 
ranked together. 
Theorem: The solution to )(Sr  and )(Tr  
given by Equation (3) is the primary 
eigenvector of SSTSST WWW ??+?? )1( ??  and 
STSSTS WWIW ????? ?1))1(( ?? , respectively. 
Proof: Combine Equations (1) and (2), we get 
||)(||
)(
)1(
||)(||
)(
||)(||
)(
)1(
||
||)(||
)(
||
||)(||
)(
SrW
SrW
SrWW
SrWW
SrW
SrW
SrW
SrW
W
SrW
SrW
W
Sr
SS
SS
TSST
TSST
SS
SS
TS
TS
ST
TS
TS
ST
?
???+??
???=
?
???+
?
??
?
??
?=
??
??)(
 
As the iterative process is a power method, 
it is guaranteed that )(Sr  converges to the 
primary eigenvector of +?? TSST WW?  
SSW?? )1( ? . Similarly,  )(Tr  is guaranteed to 
converge to the primary eigenvector of 
STSSTS WWIW ????? ?1))1(( ?? .                      ? 
3.2.2 Local Ranking (within Clusters) 
Assume now K theme clusters have been 
generated by certain clustering algorithm, 
denoted as },,,{ 21 KCCCC ?=  where kC  (k=1, 
2, ?, K) represents a cluster of highly related 
sentences )( kC CS k ?  which contain the terms 
)( kC CT k ? . The sentences and terms within 
the cluster kC  form a cluster bi-type graph 
with the adjacency matrix 
kCW . Let )( kk CC Sr  
and )(
kk CC Tr  denote the ranking scores of kCS  
and 
kCT  within kC . They are calculated by an 
equation similar to Equation (3) by replacing 
the document level adjacency matrix W  with 
the cluster level adjacency matrix 
kCW . We 
call )(
kk CC Sr  and )( kk CC Tr  the ?within-
cluster ranking functions? with respect to the 
cluster kC . They are the local ranking 
functions, in contrast to )(Sr  and )(Tr  that 
rank all the sentences and terms in the whole 
document set D. We believe that it will benefit 
sentence overall ranking when knowing more 
details about the ranking results at the finer 
granularity of theme clusters, instead of at the 
coarse granularity of the whole document set. 
136
3.2.3 Conditional Ranking (across Clusters) 
To facilitate the discovery of rank distributions 
of terms and sentences over all the theme 
clusters, we further define two ?conditional 
ranking functions? )|( kCSr  and )|( kCTr . 
These rank distributions are necessary for the 
parameter estimation during the reinforcement 
process introduced later. The conditional 
ranking score of the term jt  on the cluster kC , 
i.e., )|( kCTr  is directly derived from kCT , i.e., 
=)|( kj Ctr )( jC tr k  if kj Ct ? , and 0)|( =kj Ctr  
otherwise. It is further normalized as  
? =
=
m
j kj
kj
kj
Ctr
Ctr
Ctr
1
)|(
)|(
)|( .   (4) 
Then the conditional ranking score of the 
sentence is  on the cluster kC  is deduced from 
the terms that are included in is , i.e.,  
? ?
?
= =
=
?
?
=
n
i
m
j kjST
m
j kjST
ki
CtrjiW
CtrjiW
Csr
1 1
1
)|(),(
)|(),(
)|( . (5) 
Equation (5) can be interpreted as that the 
conditional rank of is  on kC  is higher if many 
terms in is  are ranked higher in kC . Now we 
have sentence and term conditional ranks over 
all the theme clusters and are ready to 
introduce the reinforcement process.  
3.3 Reinforcement between Within-
Cluster Ranking and Clustering  
The conditional ranks of the term jt  across the 
K theme clusters can be viewed as a rank 
distribution. Then the rank distribution of the 
sentence is  can be considered as a mixture 
model over K conditional rank distributions of 
the terms contained in the sentence is . And the 
sentence is  can be represented as a K-
dimensional vector in the new measure space, 
in which the vectors can be used to guide the 
sentence clustering update. Next, we will 
explain the mixture model of sentence and use 
EM algorithm (Bilmes, 1997) to get the 
component coefficients of the model. Then, we 
will present the similarity measure between 
sentence and cluster, which is used to adjust 
the clusters that the sentences belong to and in 
turn modify within-cluster ranking for the 
sentences in the updated clusters.  
3.3.1 Sentence Mixture Model  
For each sentence  is , we assume that it 
follows the distribution )|( isTr  to generate the 
relationship between the sentence is  and the 
term set T. This distribution can be considered 
as a mixture model over K component 
distributions, i.e. the term conditional rank 
distributions across K theme clusters. We use 
ki,?  to denote the probability that is  belongs 
to kC , then )|( isTr  can be modeled as: 
?
=
?=
K
k
kki CTrsTr
1
i, )|()|( ?  and ?
=
=
K
k
k
1
i, 1? . (6) 
ki,?  can be explained as )|( ik sCp  and 
calculated by the Bayesian equation 
?? )|()|( kiik CspsCp )( kCp , where )|( ki Csp  
is assumed to be )|( ki Csr  obtained from the 
conditional rank of is  on kC  as introduced 
before and )( kCp  is the prior probability. 
3.3.2 Parameter Estimation 
We use EM algorithm to estimate the 
component coefficients ki,?  along with 
)}({ kCp . A hidden variable zC , },,2,1{ Kz ??  
is used to denote the cluster label that a 
sentence term pair ),( ji ts  are from. In addition, 
we make the independent assumption that the 
probability of is  belonging to kC  and the 
probability of jt  belonging to kC  are 
independent, i.e., ?= )|()|,( kikji CspCtsp  
)|( kj Ctp , where )|,( kji Ctsp is the probability 
of is  and jt  both belonging to kC . Similarly, 
)|( kj Ctp  is assumed to be )|( kj Ctr . 
Let ?  be the parameter matrix, which is a 
Kn?  matrix }{ ,kiKn ?=? ?  ;,,1( ni ?=  
),,1 Kk ?= . The best ?  is estimated from the 
relationships observed in the document bi-type 
graph, i.e., STW  and SSW . The likelihood of 
generating all the relationships under the 
parameter ?  can be calculated as:  
????
= == =
???=
???=?
n
i
n
j
jiW
ji
n
i
m
j
jiW
ji
SSSTSSST
SSST ssptsp
WpWpWWL
1 1
),(
1 1
),(
'
)|,()|,(
)|()|(),|(
 
137
where )|,( ?ji tsp  is the probability that is  
and jt  both belong to the same cluster, given 
the current parameter. As )|,( ?ji ssp  does not 
contain variables from ? , we only need to 
consider maximizing the first part of the 
likelihood in order to get the best estimation of 
? . Let )|( STWL ?  be the first part of 
likelihood.  
Taking into account the hidden variable zC , 
the complete log-likelihood can be written as  
( )
( )
( )??
??
??
= =
= =
= =
???=
???=
?=?
n
i
m
j
zjiZST
n
i
m
j
zzji
n
i
m
j
jiW
zjiZST
CptspjiW
CpCtsp
CtspCWL
jiSTW
ST
1 1
1 1
1 1
),(
)|(),(log),(
)|(),|,(log
)|,,(log),|(log
),( . 
In the E-step, given the initial parameter 0? , 
which is set to Kki
10
, =?  for all i and k, the 
expectation of log-likelihood under the current 
distribution of ZC  is: 
???
???
= = =
= = =
?
?=??=?
+?=??=
?=??
n
i
K
k
m
j
jikzkzST
K
k
n
i
m
j
jikzjikST
ZSTWCf
tsCCpCCpjiW
tsCCptspjiW
CWLEQ
STZ
1 1 1
0
1 1 1
0
),|(
0
),,|())|(log(),(
),,|()),(log(),(
),|((log),( 0
 
The conditional distribution in the above 
equation, i.e., ),,|( 0?= jikz tsCCp , can be 
calculated using the Bayesian rule as follows: 
)()|()|(
)|(),|,(
),,|(
000
00
0
kzkjki
kzkzji
jikz
CCpCtpCsp
CCpCCtsp
tsCCp
=?
?=?=?
?=
. (7) 
In the M-Step, we first get the estimation of 
)( kz CCp =  by maximizing the expectation 
),( 0??Q . By introducing a Lagrange 
multiplier ? , we get the equation below. 
?=?=+??=?
? ?
=
0)]1)((),([
)( 1
0
K
k
kz
kz
CCpQ
CCp
?
??
= =
=+?==
n
i
m
j
jikz
kz
ST tsCCpCCp
jiW
1 1
0 0),,|(
)(
1
),( ?  
Thus, the estimation of )( kz CCp =  given 
previous 0?  is  
??
??
= =
= =
?=
==
n
i
m
j
ST
n
i
m
j
jikzST
kz
jiW
tsCCpjiW
CCp
1 1
1 1
0
),(
),,|(),(
)( . (8) 
Then, the parameters ki,?  can be calculated 
with the Bayesian rule as 
?
=
=
==
K
l
lzli
kzki
ki
CCpCsp
CCpCsp
1
,
)()|(
)()|(? .  (9) 
By setting ?=?0 , the whole process can 
be repeated. The updating rules provided in 
Equations (7)-(9) are applied at each iteration. 
Finally ?  will converge to a local maximum. 
A similar estimation process has been adopted 
in (Sun et al, 2009), which was used to 
estimate the component coefficients for author-
conference networks.  
3.3.3 Similarity Measure 
After we get the estimations of the component 
coefficients ki,?  for is  , is  will be represented 
as a K dimensional vector ,,,( 2,1, ?iiis ??=  
),Ki? . The center of each cluster can thus be 
calculated accordingly, which is the mean of 
is  for all is  in the same cluster, i.e., 
|| k
Cs
i
C
C
s
Center kik
?
?= ,      
where || kC  is the size of kC .  
Then the similarity between each sentence 
and each cluster can be calculated as the cosine 
similarity between them, i.e.,  
??
?
==
==
K
l C
K
l i
K
l Ci
ki
lCenterls
lCenterls
Cssim
k
k
1
2
1
2
1
))())(
)()(
),( . (10) 
Finally, each sentence is re-assigned to a 
cluster that is the most similar to the sentence. 
Based on the updated clusters, within-cluster 
ranking is updated accordingly, which triggers 
the next round of clustering refinement. It is 
expected that the quality of clusters should be 
improved during this iterative update process 
since the similar sentences under new 
attributes will be grouped together, and 
meanwhile the quality of ranking will be 
improved along with the better clusters and 
138
thus offers better attributes for further 
clustering.  
3.4 Ensemble Ranking 
The overall sentence ranking function f is 
defined as the ensemble of all the sentence 
conditional ranking scores on the K clusters.  
?
=
?=
K
k
kiki Csrsf
1
)|()( ? ,  (11) 
where k?  is a coefficient evaluating the 
importance of kC . It can be formulated as the 
normalized cosine similarity between a theme 
cluster and the whole document set for generic 
summarization, or between a theme cluster and 
a given query for query-based summarization. 
]1,0[?k?  and ?
=
=
K
k
k
1
1? . 
Figure 2 below summarizes the whole 
process that determines the overall sentence 
ensemble ranking scores.  
Input: The bi-type document graph >=< WETSG ,,? , 
ranking functions, the cluster number K, 1=? , 
001.0=Tre , 10=IterNum . 
Output: sentence final ensemble ranking vector )(Sf . 
1. 0?t ; 
2. Get the initial partition for S, i.e. tkC , Kk ?,2,1= , 
calculate cluster centers t
kC
Center accordingly.  
3. For (t=1; t<IterNum && Tre>? ; t++) 
4.     Calculate the within-cluster ranking )(
kk CC Tr
, 
)(
kCkC
Sr  and the conditional ranking )|( ki Csr ; 
5.     Get new attribute is  for each sentence is , and 
new attribute t
kCCenter  for each cluster 
t
kC ; 
6.     For each sentence is in S 
7.          For k=1 to K 
8.               Calculate similarity value ),( tki Cssim  
9.          End For 
10.        Assign is to 10
+t
kC , ),(maxarg0
t
kik Cssimk =  
11.   End For 
12.   ||max 1 t
kC
t
kCk
CenterCenter ?= +?  
13.   1+? tt  
14. End For 
15. For each sentence is  in S 
16.        For k=1 to K 
17.             ?
=
?=
K
k
kiki Csrsf
1
)|()( ?  
18.        End For 
19. End For 
Figure 2. The Overall Sentence Ranking Algorithm  
3.5 Summary Generation 
In multi-document summarization, the number 
of documents to be summarized can be very 
large. This makes information redundancy 
appears to be more serious in multi-document 
summarization than in single-document 
summarization. Redundancy control is 
necessary. We apply a simple yet effective 
way to choose summary sentences. Each time, 
we compare the current candidate sentence to 
the sentences already included in the summary. 
Only the sentence that is not too similar to any 
sentence in the summary (i.e., the cosine 
similarity between them is lower than a 
threshold) is selected into the summary. The 
iteration is repeated until the length of the 
sentences in the summary reaches the length 
limitation. In this paper, the threshold is set to 
0.7 as always in our past work. 
4 Experiments and Evaluations 
We conduct the experiments on the DUC 2004 
generic multi-document summarization dataset 
and the DUC 2006 query-based multi-
document summarization dataset. According to 
task definitions, systems are required to 
produce a concise summary for each document 
set (without or with a given query description) 
and the length of summaries is limited to 665 
bytes in DUC 2004 and 250 words in DUC 
2006. 
A well-recognized automatic evaluation 
toolkit ROUGE (Lin and Hovy, 2003) is used 
in evaluation. It measures summary quality by 
counting overlapping units between system-
generated summaries and human-written 
reference summaries. We report two common 
ROUGE scores in this paper, namely ROUGE-
1 and ROUGE-2, which base on Uni-gram 
match and Bi-gram match, respectively. 
Documents and queries are pre-processed by 
segmenting sentences and splitting words. Stop 
words are removed and the remaining words 
are stemmed using Porter stemmer.  
4.1 Evaluation of Performance  
In order to evaluate the performance of 
reinforced clustering and ranking approach, we 
compare it with the other three ranking 
approaches: (1) Global-Rank, which does not 
apply clustering and simply relies on the 
139
sentence global ranking scores to select 
summary sentences; (2) Local-Rank, which 
clusters sentences first and then rank sentences 
within each cluster. A summary is generated in 
the same way as presented in (Qazvinian and 
Radev, 2008). The clusters are ordered by 
decreasing size; (3) Cluster-HITS, which also 
clusters sentences first, but then regards 
clusters as hubs and sentences as authorities in 
the HITS algorithm and uses the obtained 
authority scores to rank and select sentences. 
The classical clustering algorithm K-means is 
used where necessary. For query-based 
summarization, the additional query-relevance 
(i.e. the cosine similarity between sentences 
and query) is involved to re-rank the candidate 
sentences chosen by the ranking approaches 
for generic summarization. 
Note that K-means requires a predefined 
cluster number K. To avoid exhaustive search 
for a proper cluster number for each document 
set, we employ the spectra approach 
introduced in (Li et al, 2007) to predict the 
number of the expected clusters. Based on the 
sentence similarity matrix using the 
normalized 1-norm, for its eigenvalues i?  
(i=1,2, ?, n), the ratio )1(/ 21 ?= + ???? ii   is 
defined. If 05.01 >? +ii ??  and i?  is still close 
to 1, then set K=i+1. Tables 1 and 2 below 
compare the performance of the four 
approaches on DUC 2004 and 2006 according 
to the calculated K.  
DUC 2004 ROUGE-1 ROUGE-2 
Reinforced 0.37082 0.08351 
Cluster-HITS 0.36463 0.07632 
Local-Rank 0.36294 0.07351 
Global-Rank 0.35729 0.06893 
Table 1. Results on the DUC 2004 dataset 
DUC 2006 ROUGE-1 ROUGE-2 
Reinforced 0.39531 0.08957 
Cluster-HITS 0.38315 0.08632 
Local-Rank 0.38104 0.08841 
Global-Rank 0.37478 0.08531 
Table 2. Results on the DUC 2006 dataset 
It is not surprised to find that ?Global-Rank? 
shows the poorest performance, when it 
utilizes the sentence level information only 
whereas the other three approaches all 
integrate the additional cluster level 
information in various ways. In addition, as 
results illustrate, the performance of ?Cluster-
HITS? is better than the performance of 
?Local-Rank?. This can be mainly credited to 
the ability of ?Cluster-HITS? to consider not 
only the cluster-level information, but also the 
sentence-to-cluster relationships, which are 
ignored in ?Local-Rank?. It is happy to see that 
the proposed reinforcement approach, which 
simultaneously updates clustering and ranking 
of sentences, consistently outperforms the 
other three approaches. 
4.2 Analysis of Cluster Quality 
Our original intention to propose the 
reinforcement approach is to hope to generate 
more accurate clusters and ranking results by 
mutually refining within-cluster ranking and 
clustering. In order to check and monitor the 
variation trend of the cluster quality during the 
iterations, we define the following measure 
?
?=
?= ??
?=
K
k
K
kll
ji
CsCs
ki
Cs
sssim
Cssim
quan
ljki
ki
1
,1 ,
)
),(min
),(min
( , (12) 
where ),(min ki
Cs
Cssim
ki?
 denotes the distance 
between the cluster center and the border 
sentence in a cluster that is the farthest away 
from the center. The larger it is, the more 
compact the cluster is. ),(min
,
ji
CsCs
sssim
ljki ??
, on 
the other hand, denotes the distance between 
the most distant pair of sentences, one from 
each cluster. The smaller it is, the more 
separated the two clusters are. The distance is 
measured by cosine similarity. As a whole, the 
larger quan means the better cluster quality. 
Figure 3 below plots the values of quan in each 
iteration on the DUC 2004 and 2006 datasets. 
Note that the algorithm converges in less than 
6 rounds and 5 rounds on the DUC 2004 and 
2006 datasets, respectively. The curves clearly 
show the increasment of quan and thus the 
improved cluster quality. 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
6.5
7
7.5
1 2 3 4 5 6IterNum
Q
ua
n
DUC2004 DUC2006
 
Figure 3. Cluster Quality on DUC 2004 and 2006  
140
While quan directly evaluate the quality of 
the generated clusters, we are also quite 
interested in whether the improved clusters 
quality can further enhance the quality of 
sentence ranking and thus consequently raise 
the performance of summarization. Therefore, 
we evaluate the ROUGEs in each iteration as 
well. Figure 4 below illustrates the changes of 
ROUGE-1 and ROUGE-2 result on the DUC 
2004 and 2006 datasets, respectively. Now, we 
have come to the positive conclusion. 
0.29
0.3
0.31
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.4
1 2 3 4 5 6
IterNum
R
O
U
G
E
-1
DUC2004 DUC2006
0.045
0.05
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
1 2 3 4 5 6
IterNum
R
O
U
G
E-
2
 
Figure 4. ROUGEs on DUC 2004 and 2006  
4.3 Impact of Cluster Numbers 
In previous experiments, the cluster number is 
predicted through the eigenvalues of 1-norm 
normalized sentence similarity matrix. This 
number is just the estimated number. The 
actual number is hard to predict accurately. To 
further examine how the cluster number 
influences summarization, we conduct the 
following additional experiments by varying 
the cluster number. Given a document set, we 
let S denote the sentence set in the document 
set, and set K in the following way: 
|| SK ?= ? ,   (13) 
where )1,0(??  is a ratio controlling the 
expected cluster number. The larger ?  is, the 
more clusters will be produced. ?  ranges from 
0.1 to 0.9 in the experiments. Due to page 
limitation, we only provide the ROUGE-1 and 
ROUGE-2 results of the proposed approach, 
?Cluster-HITS? and ?Local-Rank? on the DUC 
2004 dataset in Figure 5. The similar curves 
are also observed on the 2006 dataset. 
0.355
0.36
0.365
0.37
0.375
0.38
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-1
Cluster-HITS Local Rank Reinforced
?  
0.072
0.075
0.078
0.081
0.084
0.087
0.09
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
R
O
U
G
E
-2
?  
Figure 5. ROUGEs vs.? on DUC 2004 
It is shown that (1) the proposed approach 
outperforms ?Cluster-HITS? and ?Local-
Rank? in almost all the cases no matter how 
the cluster number is set; (2) the performances 
of ?Cluster-HITS? and ?Local-Rank? are more 
sensitive to the cluster number and a large 
number of clusters appears to deteriorate the 
performances of both. This is reasonable. 
Actually when ?  getting close to 1, ?Local-
Rank? approaches to ?Global-Rank?. These 
results demonstrate the robustness of the 
proposed approach. 
5 Conclusion 
In this paper, we present a reinforcement 
approach that tightly integrates ranking and 
clustering together by mutually and 
simultaneously updating each other. 
Experimental results demonstrate the 
effectiveness and the robustness of the 
proposed approach. In the future, we will 
explore how to integrate term semantic 
relationships to further improve the 
performance of summarization. 
Acknowledgement 
The work described in this paper was 
supported by an internal grant from the Hong 
Kong Polytechnic University (G-YG80). 
 
141
References 
J. Bilmes. 1997. A Gentle Tutorial on the em 
Algorithm and Its Application to Parameter 
Wstimation for Gaussian Mixture and Hidden 
Markov Models. Technical Report ICSI-TR-97-
02, University of Berkeley. 
Brin, S., and Page, L. 1998. The Anatomy of a 
Large-scale Hypertextual Web Search Engine. In 
Proceedings of WWW1998.. 
Harabagiu S. and Lacatusu F. 2005. Topic Themes 
for Multi-Document Summarization. In 
Proceedings of SIGIR2005. 
Hardy H., Shimizu N., Strzalkowski T., Ting L., 
Wise G. B., and Zhang X. 2002. Cross-
Document Summarization by Concept 
Classification. In Proceedings of SIGIR2002. 
Jon M. Kleinberg. 1999. Authoritative Sources in a 
Hyperlinked Environment. In Proceedings of the 
9th ACM-SIAM Symposium on Discrete 
Algorithms.  
Karypis, George, Vipin Kumar and Michael 
Steinbach. 2000. A Comparison of Document 
Clustering Techniques. KDD workshop on Text 
Mining. 
Lin, C. Y. and Hovy, E. 2000. The Automated 
Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING2000.  
Li W.Y., Ng W.K., Liu Y.  and Ong K.L. 2007. 
Enhancing the Effectiveness of Clustering with 
Spectra Analysis. IEEE Transactions on 
Knowledge and Data Engineering (TKDE). 
19(7): 887-902.  
Li, F., Tang, Y., Huang, M., Zhu, X. 2009. 
Answering Opinion Questions with Random 
Walks on Graphs. In Proceedings of ACL2009. 
Otterbacher J., Erkan G. and Radev D. 2005. Using 
RandomWalks for Question-focused Sentence 
Retrieval. In Proceedings of HLT/EMNLP 2005. 
Qazvinian  V. and Radev D. R. 2008. Scientific 
paper summarization using citation summary 
networks. In Proceedings of COLING2008. 
Sun P., Lee J.H., Kim D.H., and Ahn C.M. 2007. 
Multi-Document Using Weighted Similarity 
Between Topic and Clustering-Based Non-
negative Semantic Feature. APWeb/WAIM 
2007. 
Sun Y., Han J., Zhao P., Yin Z., Cheng H., and Wu 
T. 2009. Rankclus: Integrating Clustering with  
 
Ranking for Heterogenous Information Network 
Analysis. In Proceedings of EDBT 2009. 
Wang D.D., Li T., Zhu S.H., Ding Chris. 2008a 
Multi-Document Summarization via Sentence-
Level Semantic Analysis and Symmetric Matrix 
Factorization. In Proceedings of SIGIR2008. 
Wang D.D., Zhu S.H., Li T., Chi Y., and Gong Y.H. 
2008b. Integrating Clustering and Multi-
Document Summarization to Improve Document 
Understanding. In Proceedings of CIKM 2008. 
Wan X. and Yang J. 2006. Improved Affinity Graph 
based Multi-Document Summarization. In 
Proceedings of HLT-NAACL2006. 
Zha H. 2002. Generic Summarization and Key 
Phrase Extraction using Mutual Reinforcement 
Principle and Sentence Clustering. In 
Proceedings of SIGIR2002. 
142
Coling 2010: Poster Volume, pages 919?927,
Beijing, August 2010
A Study on Position Information in Document Summarization 
You Ouyang       Wenjie Li       Qin Lu       Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University 
{csyouyang,cswjli,csluqin,csrzhang}@comp.polyu.edu.hk 
Abstract 
Position information has been proved to 
be very effective in document 
summarization, especially in generic 
summarization. Existing approaches 
mostly consider the information of 
sentence positions in a document, based 
on a sentence position hypothesis that 
the importance of a sentence decreases 
with its distance from the beginning of 
the document. In this paper, we consider 
another kind of position information, i.e., 
the word position information, which is 
based on the ordinal positions of word 
appearances instead of sentence 
positions. An extractive summarization 
model is proposed to provide an 
evaluation framework for the position 
information. The resulting systems are 
evaluated on various data sets to 
demonstrate the effectiveness of the 
position information in different 
summarization tasks. Experimental 
results show that word position 
information is more effective and 
adaptive than sentence position 
information. 
1 Introduction 
Position information has been frequently used in 
document summarization. It springs from 
human?s tendency of writing sentences of 
greater topic centrality at particular positions in 
a document. For example, in newswire 
documents, topic sentences are usually written 
earlier. A sentence position hypothesis is then 
given as: the first sentence in a document is the 
most important and the importance decreases as 
the sentence gets further away from the 
beginning. Based on this sentence position 
hypothesis, sentence position features are 
defined by the ordinal position of sentences. 
These position features have been proved to be 
very effective in generic document 
summarization. In more recent summarization 
tasks, such as query-focused and update 
summarization tasks, position features are also 
widely used.  
Although in these tasks position features may 
be used in different ways, they are all based on 
the sentence position hypothesis. So we regard 
them as providing the sentence position 
information. In this paper, we study a new kind 
of position information, i.e., the word position 
information. The motivation of word position 
information comes from the idea of assigning 
different importance to multiple appearances of 
one word in a document.  
As to many language models such as the bag-
of-words model, it is well acknowledged that a 
word which appears more frequently is usually 
more important. If we take a closer look at all 
the appearances of one word, we can view this 
as a process that the different appearances of the 
same word raise the importance of each other. 
Now let?s also take the order of the appearances 
into account. When reading a document, we can 
view it as a word token stream from the first 
token to the last. When a new token is read, we 
attach more importance to previous tokens that 
have the same lemma because they are just 
repeated by the new token. Inspired by this, we 
postulate a word position hypothesis here: for 
all the appearances of a fixed word, the 
importance of each appearance depends on all 
its following appearances. Therefore, the first 
appearance of a word is the most important and 
the importance decreases with the ordinal 
919
positions of the appearances. Then, a novel kind 
of position features can be defined for the word 
appearances based on their ordinal positions. 
We believe that these word position features 
have some advantages when compared to 
traditional sentence position features. According 
to the sentence position hypothesis, sentence 
position features generally prefer earlier 
sentences in a document. As to the word 
position features that attempt to differentiate 
word appearances instead of sentences, a 
sentence which is not the first one in the 
document may still not be penalized as long as 
its words do not appear in previous sentences. 
Therefore, word position features are able to 
discover topic sentences in deep positions of the 
document. On the other hand, the assertion that 
the first sentence is always the most important is 
not true in actual data. It depends on the writing 
style indeed. For example, some authors may 
like to write some background sentences before 
topic sentences. In conclusion, we can expect 
word position features  to be more adaptive to 
documents with different structures.  
In the study of this paper, we define several 
word position features based on the ordinal 
positions of word appearances. We also develop 
a word-based summarization system to evaluate 
the effectiveness of the proposed word position 
features on a series of summarization data sets. 
The main contributions of our work are: 
(1) representation of word position information, 
which is a new kind of position information in 
document summarization area. 
(2) empirical results on various data sets that 
demonstrate the impact of position information 
in different summarization tasks. 
2 Related Work 
The use of position information in document 
summarization has a long history. In the seminal 
work by (Luhn, 1958), position information was 
already considered as a good indicator of 
significant sentences. In (Edmundson, 1969), a 
location method was proposed that assigns 
positive weights to the sentences to their ordinal 
positions in the document. Position information 
has since been adopted by many successful 
summarization systems, usually in the form of 
sentence position features. For example, Radev 
et al (2004) developed a feature-based system 
MEAD based on word frequencies and sentence 
positions. The position feature was defined as a 
descending function of the sentence position. 
The MEAD system performed very well in the 
generic multi-document summarization task of 
the DUC 2004 competition. Later, position 
information is also applied to more 
summarization tasks. For example, in query-
focused task, sentence position features are 
widely used in learning-based summarization 
systems as a component feature for calculating 
the composite sentence score (Ouyang et al 
2007; Toutanova et al 2007). However, the 
effect of position features alone was not studied 
in these works.  
There were also studies aimed at analyzing 
and explaining the effectiveness of position 
information. Lin and Hovy (1997) provided an 
empirical validation on the sentence position 
hypothesis. For each position, the sentence 
position yield was defined as the average value 
of the significance of the sentences with the 
fixed position. It was observed that the average 
significance at earlier positions was indeed 
larger. Nenkova (2005) did a conclusive 
overview on the DUC 2001-2004 evaluation 
results. It was reported that position information 
is very effective in generic summarization. In 
generic single-document summarization, a lead-
based baseline that simply takes the leading 
sentences as the summary can outperform most 
submitted summarization system in DUC 2001 
and 2002. As in multi-document summarization, 
the position-based baseline system is 
competitive in generating short summaries but 
not in longer summaries. Schilder and 
Kondadadi (2008) analyzed the effectiveness of 
the features that are used in their learning-based 
sentence scoring model for query-focused 
summarization. By comparing the ROUGE-2 
results of each individual feature, it was 
reported that position-based features are less 
effective than frequency-based features. In 
(Gillick et al, 2009), the effect of position 
information in the update summarization task 
was studied. By using ROUGE to measure the 
density of valuable words at each sentence 
position, it was observed that the first sentence 
of newswire document was especially important 
for composing update summaries. They defined 
a binary sentence position feature based on the 
920
observation and the feature did improve the 
performance on the update summarization data. 
3 Methodology 
In the section, we first describe the word-based 
summarization model. The word position 
features are then defined and incorporated into 
the summarization model. 
3.1 Basic Summarization Model 
To test the effectiveness of position information 
in document summarization, we first propose a 
word-based summarization model for applying 
the position information. The system follows a 
typical extractive style that constructs the target 
summary by selecting the most salient sentences.  
Under the bag-of-words model, the 
probability of a word w in a document set D can 
be scaled by its frequency, i.e., p(w)=freq(w)/|D|, 
where freq(w) indicates the frequency of w in D 
and |D| indicates the total number of words in D. 
The probability of a sentence s={w1, ?, wN} is 
then calculated as the product of the word 
probabilities, i.e., p(s)=?i p(wi). Moreover, the 
probability of a summary consisting a set of 
sentences, denoted as S={s1, ?, sM}, can be 
calculated by the product of the sentence 
probabilities, i.e., p(S)=?j p(sj). To obtain the 
optimum summary, an intuitive idea is to select 
the sentences to maximize the overall summary 
probability p(S), equivalent to maximizing 
log(p(S)) = ?j?i log(p(wji)) = ?j?i (logfreq(wji)- 
log|D|) = ?j?i log freq(wji) - |S|?log |D|,  
where wji indicates the ith word in sj and |S| 
indicates the total number of words in S. As to 
practical summarization tasks, a maximum 
summary length is usually postulated. So here 
we just assume that the length of the summary 
is fixed. Then, the above optimization target is 
equivalent to maximizing ?j?i logfreq(wji). 
From the view of information theory, the sum 
can also be interpreted as a simple measure on 
the total information amount of the summary. In 
this interpretation, the information of a single 
word wji is measured by log freq(wji) and the 
summary information is the sum of the word 
information. So the optimization target can also 
be interpreted as including the most informative 
words to form the most informative summary 
given the length limit.  
In extractive summarization, summaries are 
composed by sentence selection. As to the 
above optimization target, the sentence scoring 
function for ranking the sentences should be 
calculated as the average word information, i.e., 
score(s) = ?i log freq(wi) / |s|. 
After ranking the sentences by their ranking 
scores, we can select the sentences into the 
summary by the descending order of their score 
until the length limit is reached. By this process, 
the summary with the largest  p(S) can be 
composed.  
3.2 Word Position Features 
With the above model, word position features 
are defined to represent the word position 
information and are then incorporated into the 
model. According to the motivation, the features 
are defined by the ordinal positions of word 
appearances, based on the position hypothesis 
that earlier appearances of a word are more 
informative. Formally, for the ith appearance 
among the total n appearances of a word w, four 
position features are defined based on i and n 
using different formulas as described below. 
(1) Direct proportion (DP) With the word 
position hypothesis, an intuitive idea is to regard 
the information degree of the first appearance as 
1 and the last one as 1/n, and then let the degree 
decrease linearly to the position i. So we can 
obtain the first position feature defined by the 
direct proportion function, i.e., f(i)=(n-i+1)/n. 
(2) Inverse proportion (IP). Besides the linear 
function, other functions can also be used to 
characterize the relationship between the 
position and the importance. The second 
position feature adopts another widely-used 
function, the inversed proportion function, i.e., 
f(i)=1/i. This measure is similar to the above 
one, but the information degree decreases by the 
inverse proportional function. Therefore, the 
degree decreases more quickly at smaller 
positions, which implies a stronger preference 
for leading sentences. 
(3) Geometric sequence (GS). For the third 
feature, we make an assumption that the degree 
of every appearance is the sum of the degree of 
all the following appearances, i.e., f(i) = f(i+1)+ 
f(i+2)+?+ f(n). It can be easily derived that the 
sequence also satisfies f(i) = 2?f(i-1). That is, the 
information degree of each new appearance is 
921
halved. Then the feature value of the ith 
appearance can be calculated as f(i) = (1/2)i-1.  
(4) Binary function (BF). The final feature is a 
binary position feature that regards the first 
appearance as much more informative than the 
all the other appearances, i.e., f(i)=1, if i=1; ? 
else, where ? is a small positive real number.  
3.3 Incorporating the Position Features  
To incorporate the position features into the 
word-based summarization model, we use them 
to adjust the importance of the word appearance. 
For the ith appearance of a word w, its original 
importance is multiplied by the position feature 
value, i.e., log freq(w)?pos(w, i), where pos(w, i) 
is calculated by one of the four position features 
introduced above. By this, the position feature is 
also incorporated into the sentence scores, i.e., 
score?(s) = ?i [log freq(wi) ? pos(wi)] / |s| 
3.4 Sentence Position Features 
In our study, another type of position features, 
which model sentence position information, is 
defined for comparison with the word position 
features. The sentence position features are also 
defined by the above four formulas. However, 
for each appearance, the definition of i and n in 
the formulas are changed to the ordinal position 
of the sentence that contains this appearance 
and the total number of sentences in the 
document respectively. In fact, the effects of the 
features defined in this way are equivalent to 
traditional sentence position features. Since i 
and n are now defined by sentence positions, the 
feature values of the word tokens in the same 
sentence s are all equal. Denote it by pos(s), and 
the sentence score with the position feature can 
be written as  
score?(s) = ( ?w in slogfreq(w) ? pos(s))/|s|  
= pos(s)?(? logw in s freq(w)/|s|), 
which can just be viewed as the product of the 
original score and a sentence position feature. 
3.5 Discussion 
By using the four functions to measure word or 
sentence position information, we can generate 
a total of eight position features. Among the 
four functions, the importance drops fastest 
under the binary function and the order is BF > 
GS > IP > DP. Therefore, the features based on 
the binary function are the most biased to the 
leading sentences in the document and the 
features based on the direct proportion function 
are the least. On the other hand, as mentioned in 
the introduction, sentence-based features have 
larger preferences for leading sentences than 
word-based position features.  
An example is given below to illustrate the 
difference between word and sentence position 
features. This is a document from DUC 2001. 
1. GENERAL ACCIDENT, the leading British 
insurer, said yesterday that insurance claims 
arising from Hurricane Andrew could 'cost it as 
much as Dollars 40m.'  
2. Lord Airlie, the chairman who was 
addressing an extraordinary shareholders' 
meeting, said: 'On the basis of emerging 
information, General Accident advise that the 
losses to their US operations arising from 
Hurricane Andrew, which struck Florida and 
Louisiana, might in total reach the level at 
which external catastrophe reinsurance covers 
would become exposed'.  
3. What this means is that GA is able to pass on 
its losses to external reinsurers once a certain 
claims threshold has been breached.  
4. It believes this threshold may be breached in 
respect of Hurricane Andrew claims.  
5. However, if this happens, it would suffer a 
post-tax loss of Dollars 40m (Pounds 20m).  
6. Mr Nelson Robertson, GA's chief general 
manager, explained later that the company has a  
1/2 per cent share of the Florida market.  
7. It has a branch in Orlando.  
8. The company's loss adjusters are in the area 
trying to estimate the losses.  
9. Their guess is that losses to be faced by all 
insurers may total more than Dollars 8bn.  
10. Not all damaged property in the area is 
insured and there have been estimates that the 
storm caused more than Dollars 20bn of 
damage.  
11. However, other insurers have estimated that 
losses could be as low as Dollars 1bn in total. 
12 Mr Robertson said: 'No one knows at this 
time what the exact loss is'. 
For the word ?threshold? which appears 
twice in the document, its original importance is 
log(2), for the appearance of ?threshold? in the 
4th sentence, the modified score based on word 
position feature with the direct proportion 
function is 1/2?log(2). In contrast, the score 
based on sentence position feature with the 
922
same function is 9/12?log(2), which is larger. 
For the appearance of the word ?estimate? in the 
8th sentence, its original importance is log(3) 
(the three boldfaced tokens are regarded as one 
word with stemming). The word-based and 
sentence-based scores are log(3) and 5/12?log(3) 
respectively. So its importance is larger under 
word position feature. Therefore, the system 
with word position features may prefer the 8th 
sentence that is in deeper positions but the 
system with sentence position feature may 
prefer the 4th sentence. As for this document, the 
top 5 sentences selected by sentence position 
feature are {1, 4, 3, 5, 2} and the those selected 
by the word position features are {1, 8, 3, 6, 9}. 
This clearly demonstrates the difference 
between the position features. 
4 Experimental Results 
4.1 Experiment Settings 
We conduct the experiments on the data sets 
from the Document Understanding Conference 
(DUC) run by NIST. The DUC competition 
started at year 2001 and has successfully 
evaluated various summarization tasks up to 
now. In the experiments, we evaluate the 
effectiveness of position information on several 
DUC data sets that involve various 
summarization tasks. One of the evaluation 
criteria used in DUC, the automatic 
summarization evaluation package ROUGE, is 
used to evaluate the effectiveness of the 
proposed word position features in the context 
of document summarization1. The recall scores 
of ROUGE-1 and ROUGE-2, which are based 
on unigram and bigram matching between 
system summaries and reference summaries, are 
adopted as the evaluation criteria.  
In the data sets used in the experiments, the 
original documents are all pre-processed by 
sentence segmentation, stop-word removal and 
word stemming. Based on the word-based 
summarization model, a total of nine systems 
are evaluated in the experiments, including the 
system with the original ranking model (denoted 
as None), four systems with each word position 
feature (denoted as WP) and four systems with 
each sentence position feature (denoted as SP). 
                                                 
1 We run ROUGE-1.5.5 with the parameters ?-x -m -
n 2 -2 4 -u -c 95 -p 0.5 -t 0? 
For reference, the average ROUGE scores of all 
the human summarizers and all the submitted 
systems from the official results of NIST are 
also given (denoted as Hum and NIST 
respectively).  
4.2 Redundancy Removal 
To reduce the redundancy in the generated 
summaries, we use an approach similar to the 
maximum marginal relevance (MMR) approach 
in the sentence selection process (Carbonell and 
Goldstein, 1998). In each round of the sentence 
selection, the candidate sentence is compared 
against the already-selected sentences. The 
sentence is added to the summary only if it is 
not significantly similar to any already-selected 
sentence, which is judged by the condition that 
the cosine similarity between the two sentences 
is less than 0.7. 
4.3 Generic Summarization 
In the first experiment, we use the DUC 2001 
data set for generic single-document 
summarization and the DUC 2004 data set for 
generic multi-document summarization. The 
DUC 2001 data set contains 303 document-
summary pairs; the DUC 2004 data set contains 
45 document sets, with each set consisting of 10 
documents. A summary is required for each 
document set. Here we need to adjust the 
ranking model for the multi-document task, i.e., 
the importance of a word is calculated as its 
total frequency in the whole document set 
instead of a single document. For both tasks, the 
summary length limit is 100 words. 
Table 1 and 2 below provide the average 
ROUGE-1 and ROUGE-2 scores (denoted as R-
1 and R-2) of all the systems. Moreover, we 
used paired two sample t-test to calculate the 
significance of the differences between a pair of 
word and sentence position features. The bolded 
score in the tables indicates that that score is 
significantly better than the corresponding 
paired one. For example, in Table 1, the bolded 
R-1 score of system WP DP means that it is 
significantly better than the R-1 score of system 
SP DP. Besides the ROUGE scores, two 
statistics, the number of ?first sentences 2 ? 
among the selected sentences (FS-N) and the 
                                                 
2 A ?first sentence? is the sentence at the fist position 
of a document.  
923
average position of the selected sentences (A-
SP), are also reported in the tables for analysis.  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.4473 0.1942 301 4.00 
SP DP 0.4396 0.1844 300 3.69 
WP IP 0.4543 0.2023 290 4.30 
SP IP 0.4502 0.1964 303 3.08 
WP GS 0.4544 0.2041 278 4.50 
SP GS 0.4509 0.1974 303 2.93 
WP BF 0.4544 0.2036 253 5.57 
SP BF 0.4239 0.1668 303 9.64 
None 0.4193 0.1626 265 10.06
NIST 0.4445 0.1865 - - 
Hum 0.4568 0.1740 - - 
Table 1. Results on the DUC 2001 data set  
 
System R-1 R-2 FS-N A-SP 
WP DP 0.3728 0.0911 89 4.16 
SP DP 0.3724 0.0908 112 2.68 
WP IP 0.3756 0.0912 108 3.77 
SP IP 0.3690 0.0905 201 1.01 
WP GS 0.3751 0.0916 110 3.67 
SP GS 0.3690 0.0905 201 1.01 
WP BF 0.3740 0.0926 127 3.14 
SP BF 0.3685 0.0903 203 1 
None 0.3550 0.0745 36 10.98
NIST 0.3340 0.0686 - - 
Hum 0.4002 0.0962 - - 
Table 2. Results on the DUC 2004 data set 
 
From Table 1 and Table 2, it is observed that 
position information is indeed very effective in 
generic summarization so that all the systems 
with position features performed better than the 
system None which does not use any position 
information. Moreover, it is also clear that the 
proposed word position features consistently 
outperform the corresponding sentence position 
features. Though the gaps between the ROUGE 
scores are not large, the t-tests proved that word 
position features are significantly better on the 
DUC 2001 data set. On the other hand, the 
advantages of word position features over 
sentence position features are less significant on 
the DUC 2004 data set. One reason may be that 
the multiple documents have provided more 
candidate sentences for composing the summary. 
Thus it is possible to generate a good summary 
only from the leading sentences in the 
documents. According to Table 2, the average-
sentence-position of system SP BF is 1, which 
means that all the selected sentences are ?first 
sentences?. Even under this extreme condition, 
the performance is not much worse. 
The two statistics also show the different 
preferences of the features. Compared to word 
position features, sentence position features are 
likely to select more ?first sentences? and also 
have smaller average-sentence-positions. The 
abnormally large average-sentence-position of 
SP BF in DUC 2001 is because it does not 
differentiate all the other sentences except the 
first one. The corresponding word-position-
based system WP BF can differentiate the 
sentences since it is based on word positions, so 
its average-sentence-position is not that large. 
4.4 Query-focused Summarization 
Since year 2005, DUC has adopted query-
focused multi-document summarization tasks 
that require creating a summary from a set of 
documents to a given query. This task has been 
specified as the main evaluation task over three 
years (2005-2007). The data set of each year 
contains about 50 DUC topics, with each topic 
including 25-50 documents and a query. In this 
experiment, we adjust the calculation of the 
word importance again for the query-focused 
issue. It is changed to the total number of the 
appearances that fall into the sentences with at 
least one word in the query. Formally, given the 
query which is viewed as a set of words 
Q={w1, ?, wT}, a sentence set SQ is defined as 
the set of sentences that contain at least one wi 
in Q. Then the importance of a word w is 
calculated by its frequency in SQ. For the query-
focused task, the summary length limit is 250 
words. 
Table 3 below provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
DUC 2005-2007 data sets. The boldfaced terms 
in the tables indicate the best results in each 
column. According to the results, on query-
focused summarization, position information 
seems to be not as effective as on generic 
summarization. The systems with position 
features can not outperform the system None. In 
fact, this is reasonable due to the requirement 
specified by the pre-defined query. Given the 
query, the content of interest may be in any 
924
position of the document and thus the position 
information becomes less meaningful.  
On the other hand, we find that though the 
systems with word position features cannot 
outperform the system None, it does 
significantly outperform the systems with 
sentence position features. This is also due to 
the role of the query. Since it may refer to the 
specified content in any position of the 
documents, sentence position features are more 
likely to fail in discovering the desired 
sentences since they always prefer leading 
sentences. In contrast, word position features 
are less sensitive to this problem and thus 
perform better. Similarly, we can see that the 
direct proportion (DP), which has the least bias 
for leading sentences, has the best performance 
among the four functions. 
System 2005 2006 2007 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3791 0.0805 0.3909 0.0917 0.4158 0.1135 
SP DP 0.3727 0.0776 0.3832 0.0869 0.4118 0.1103 
WP IP 0.3772 0.0791 0.3830 0.0886 0.4106 0.1121 
SP IP 0.3618 0.0715 0.3590 0.0739 0.3909 0.1027 
WP GS 0.3767 0.0794 0.3836 0.0879 0.4109 0.1119 
SP GS 0.3616 0.0716 0.3590 0.0739 0.3909 0.1027 
WP BF 0.3740 0.0741 0.3642 0.0796 0.3962 0.1037 
SP BF 0.3647 0.0686 0.3547 0.0742 0.3852 0.1013 
NONE 0.3788 0.0791 0.3936 0.0924 0.4193 0.1140 
NIST 0.3353 0.0592 0.3707 0.0741 0.0962 0.3978 
Hum 0.4392 0.1022 0.4532 0.1101 0.4757 0.1402 
Table 3. Results on the DUC 2005 - 2007 data sets 
 
System 2008 A 2008 B 2009 A 2009 B R-1 R-2 R-1 R-2 R-1 R-2 R-1 R-2 
WP DP 0.3687 0.0978 0.3758 0.1036 0.3759 0.1015 0.3693 0.0922 
SP DP 0.3687 0.0971 0.3723 0.1011 0.3763 0.1031 0.3704 0.0946 
WP IP 0.3709 0.1014 0.3741 0.1058 0.3758 0.1030 0.3723 0.0906 
SP IP 0.3619 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP GS 0.3705 0.1004 0.3732 0.1048 0.3770 0.1051 0.3731 0.0917 
SP GS 0.3625 0.0975 0.3723 0.1037 0.3693 0.0994 0.3690 0.0956 
WP BF 0.3661 0.0975 0.3678 0.0992 0.3720 0.1069 0.3650 0.0936 
SP BF 0.3658 0.0965 0.3674 0.0980 0.3683 0.1043 0.3654 0.0945 
NONE 0.3697 0.0978 0.3656 0.0915 0.3653 0.0934 0.3595 0.0834 
NIST 0.3389 0.0799 0.3192 0.0676 0.3468 0.0890 0.3315 0.0761 
Hum 0.4105 0.1156 0.3948 0.1134 0.4235 0.1249 0.3901 0.1059 
Table 4. Results on the TAC 2008 - 2009 data sets 
 
4.5 Update Summarization 
Since year 2008, the DUC summarization track 
has become a part of the Text Analysis 
Conference (TAC). In the update summarization 
task, each document set is divided into two 
ordered sets A and B. The summarization target 
on set A is the same as the query-focused task in 
DUC 2005-2007. As to the set B, the target is to 
write an update summary of the documents in 
set B, under the assumption that the reader has 
already read the documents in set A. The data 
set of each year contains about 50 topics, and 
each topic includes 10 documents for set A, 10 
documents for set B and an additional query. 
For set A, we follow exactly the same method 
used in section 4.4; for set B, we make an 
additional novelty check for the sentences in B 
with the MMR approach. Each candidate 
sentence for set B is now compared to both the 
selected sentences in set B and in set A to 
925
ensure its novelty. In the update task, the 
summary length limit is 100 words.  
Table 4 above provides the average ROUGE-
1 and ROUGE-2 scores of all the systems on the 
TAC 2008-2009 data sets. The results on set A 
and set B are shown individually. For the task 
on set A which is almost the same as the DUC 
2005-2007 tasks, the results are also very 
similar. A small difference is that the systems 
with position features perform slightly better 
than the system None on these two data sets. 
Also, the difference between word position 
features and sentence position features becomes 
smaller. One reason may be that the shorter 
summary length increases the chance of 
generating good summaries only from the 
leading sentences. This is somewhat similar to 
the results reported in (Nenkova, 2005) that 
position information is more effective for short 
summaries. 
For the update set B, the results show that 
position information is indeed very effective. In 
the results, all the systems with position features 
significantly outperform the system None. We 
attribute the reason to the fact that we are more 
concerned with novel information when 
summarizing update set B. Therefore, the effect 
of the query is less on set B, which means that 
the effect of position information may be more 
pronounced in contrast. On the other hand, 
when comparing the position features, we can 
see that though the difference of the position 
features is quite small, word position features 
are still better in most cases.  
4.6 Discussion 
Based on the experiments, we briefly conclude 
the effectiveness of position information in 
document summarization. In different tasks, the 
effectiveness varies indeed. It depends on 
whether the given task has a preference for the 
sentences at particular positions. Generally, in 
generic summarization, the position hypothesis 
works well and thus the ordinal position 
information is effective. In this case, those 
position features that are more distinctive, such 
as GS and BF, can achieve better performances. 
In contrast, in the query-focused task that relates 
to specified content in the documents, ordinal 
position information is not so useful. Therefore, 
the more distinctive a position feature is, the 
worse performance it leads to. However, in the 
update summarization task that also involves 
queries, position information becomes effective 
again since the role of the query is less 
dominant on the update document set.   
On the other hand, by comparing the sentence 
position features and word position features on 
all the data sets, we can draw an overall 
conclusion that word position features are 
consistently more appreciated. For both generic 
tasks in which position information is effective 
and query-focused tasks in which it is not so 
effective, word position features show their 
advantages over sentence position features. This 
is because of the looser position hypothesis 
postulated by them. By avoiding arbitrarily 
regarding the leading sentences as more 
important, they are more adaptive to different 
tasks and data sets. 
5 Conclusion and Future Work 
In this paper, we proposed a novel kind of word 
position features which consider the positions of 
word appearances instead of sentence positions. 
The word position features were compared to 
sentence position features under the proposed 
sentence ranking model. From the results on a 
series of DUC data sets, we drew the conclusion 
that the word position features are more 
effective and adaptive than traditional sentence 
position features. Moreover, we also discussed 
the effectiveness of position information in 
different summarization tasks. 
In our future work, we?d like to conduct more 
detailed analysis on position information. 
Besides the ordinal positions, more kinds of 
position information can be considered to better 
model the document structures. Moreover, since 
position hypothesis is not always correct in all 
documents, we?d also like to consider a pre-
classification method, aiming at identifying the 
documents for which position information is 
more suitable. 
 
Acknowledgement The work described in 
this paper was supported by Hong Kong RGC 
Projects (PolyU5217/07E). We are grateful to 
professor Chu-Ren Huang for his insightful 
suggestions and discussions with us. 
926
References
Edmundson, H. P.. 1969. New methods in automatic 
Extracting. Journal of the ACM, volume 16, issue 
2, pp 264-285. 
Gillick, D., Favre, B., Hakkani-Tur, D., Bohnet, B., 
Liu, Y., Xie, S.. 2009. The ICSI/UTD 
Summarization System at TAC 2009. Proceedings 
of Text Analysis Conference 2009.  
Jaime G. Carbonell and Jade Goldstein. 1998. The 
use of MMR, diversity-based reranking for 
reordering documents and producing summaries. 
Proceedings of the 21st annual international ACM 
SIGIR conference on Research and development 
in information retrieval, pp 335-336. 
Lin, C. and Hovy, E.. 1997. Identifying Topics by 
Position. Proceedings of the fifth conference on 
Applied natural language processing 1997, pp 
283-290. 
Luhn, H. P.. 1958. The automatic creation of 
literature abstracts. IBM J. Res. Develop. 2, 2, pp 
159-165. 
Nenkova. 2005. Automatic text summarization of 
newswire: lessons learned from the document 
understanding conference. Proceedings of the 
20th National Conference on Artificial 
Intelligence, pp 1436-1441. 
Ouyang, Y., Li, S., Li, W.. 2007. Developing 
learning strategies for topic-based summarization. 
Proceedings of the sixteenth ACM conference on 
Conference on information and knowledge 
management, pp 79-86. 
Radev, D., Jing, H., Sty?s, M. and Tam, D.. 2004. 
Centroid-based summarization of multiple 
documents. Information Processing and 
Management, volume 40, pp 919?938. 
Schilder, F., Kondadadi, R.. 2008. FastSum: fast and 
accurate query-based multi-document 
summarization. Proceedings of the 46th Annual 
Meeting of the Association for Computational 
Linguistics on Human Language Technologies, 
short paper session, pp 205-208. 
Toutanova, K. et al 2007. The PYTHY 
summarization system: Microsoft research at 
DUC 2007. Proceedings of Document 
Understanding Conference 2007.  
 
927
Coling 2010: Poster Volume, pages 1489?1497,
Beijing, August 2010
Sentence Ordering with Event-Enriched Semantics and Two-
Layered Clustering for Multi-Document News Summarization
Renxian Zhang                Wenjie Li                   Qin Lu      
Department of Computing, the Hong Kong Polytechnic University
{csrzhang,cswjli,csluqin}@comp.polyu.edu.hk
Abstract
We propose an event-enriched model to 
alleviate the semantic deficiency 
problem in the IR-style text processing 
and apply it to sentence ordering for 
multi-document news summarization.
The ordering algorithm is built on event 
and entity coherence, both locally and 
globally. To accommodate the event-
enriched model, a novel LSA-integrated 
two-layered clustering approach is 
adopted. The experimental result shows 
clear advantage of our model over 
event-agonistic models.
1 Introduction
One of the crucial steps in multi-document 
summarization (MDS) is information ordering, 
right after content selection and before sentence 
realization (Jurafsky and Martin, 2009:832?
834). Problems with this step are the culprit for 
much of the dissatisfaction with automatic 
summaries. While textual order may guide the 
ordering in single-document summarization, no 
such guidance is available for MDS ordering. 
A sensible solution is ordering sentences by 
enhancing coherence since incoherence is the 
source of disorder. Recent researches in this 
direction mostly focus on local coherence by 
studying lexical cohesion (Conroy et al, 2006) 
or entity overlap and transition (Barzilay and 
Lapata, 2008). But global coherence, i.e., 
coherence between sentence groups with the 
whole text in view, is largely unaccounted for 
and few efforts are made at levels higher than 
entity or word in measuring sentence coherence.
On the other hand, event as a high-level 
construct has proved useful in MDS content 
selection (Filatova and Hatzivassiloglou, 2004; 
Li et al, 2006). But the potential of event in 
summarization has not been fully gauged and 
few publications report using event in MDS 
information ordering. We will argue that event 
is instrumental for MDS information ordering, 
especially multi-document news summarization 
(MDNS). Ordering algorithms based on event 
and entity information outperform those based 
only on entity information.
After related works are surveyed in section 2, 
we will discuss in section 3 the problem of 
semantic deficiency in IR-based text processing, 
which motivates building event information into 
sentence representation. The details of such 
representation are provided in section 4. In 
section 5, we will explicate the ordering 
algorithms, including layered clustering and 
cluster-based ordering. The performance of the 
event-enriched model will be extensively 
evaluated in section 6. Section 7 will conclude 
the work with directions to future work.
2 Related Work
In MDS, information ordering is often realized 
on the sentence level and treated as a coherence 
enhancement task. A simple ordering criterion 
is the chronological order of the events 
represented in the sentences, which is often 
augmented with other ordering criteria such as 
lexical overlap (Conroy et al, 2006), lexical
cohesion (Barzilay et al, 2002) or syntactic 
features (Lapata 2003).
A different way to capture local coherence in 
sentence ordering is the Centering Theory (CT, 
Grosz et al 1995)-inspired entity-transition 
approach, advocated by Barzilay and Lapata 
(2005, 2008). In their entity grid model, 
syntactic roles played by entities and transitions 
between these syntactic roles underlie the 
coherence patterns between sentences and in the 
1489
whole text. An entity-parsed corpus can be used 
to train a model that prefers the sentence 
orderings that comply with the optimal entity 
transition patterns.
Another important clue to sentence ordering 
is the sentence positional information in a 
source document, or ?precedence relation?, 
which is utilized by Okazaki et al (2004) in 
combination with topical clustering.
Those works are all relevant to the current 
work because we seek ordering clues from 
chronological order, lexical cohesion, entity 
transition, and sentence precedence. But we also 
add an important member to the panoply ? event.  
Despite its intuitive and conceptual appeal, 
event is not as extensively used in 
summarization as term or entity. Filatova and 
Hatzivassiloglou (2004) use ?atomic events? as 
conceptual representations in MDS content 
selection, followed by Li et al (2006) who treat 
event terms and named entities as graph nodes 
in their PageRank algorithm. Yoshioka and 
Haraguchi (2004) report an event reference-
based approach to MDS content selection for 
Japanese articles. Although ?sentence 
reordering? is a component of their model, it 
relies merely on textual and chronological order. 
Few published works report using event 
information in MDS sentence ordering.
Our work will represent text content at two 
levels: event vectors and sentence vectors. This 
is close in spirit to Bromberg?s (2006) enriched 
LSA-coherence model, where both sentence and 
word vectors are used to compute a centroid as 
the topic of the text. 
3 Semantic Deficiency in IR-Style Text 
Processing
As automatic summarization traces its root to 
Information Retrieval (IR), it inherits the vector 
space model (VSM) of text representation,
according to which a sentence is treated as a bag 
of words or stoplist-filtered terms. The order or 
relation among the terms is ignored. For 
example,
1a) The storm killed 120,000 people in Jamaica 
and five in the Dominican Republic before moving 
west to Mexico.
1b) [Dominican, Mexico, Jamaica, Republic, five,
kill, move, people, storm, west]
1c) [Dominican Republic, Mexico, Jamaica,
people, storm]
1b) and 1c) are the term-based and entity-
based representations of 1a) respectively. They
only indicate what the sentence is about (i.e., 
some happening, probably a storm, in some 
place that affects people), but ?aboutness? is a 
far cry from informativeness. For instance, no 
message about ?people in which place, Mexico 
or Jamaica, are affected? or ?what moves to 
where? can be gleaned from 1b) although such 
message is clearly conveyed in 1a). In other 
words, the IR-style text representation is 
semantically deficient. 
We argue that a natural text, especially a 
news article, is not only about somebody or 
something. It also tells what happened to 
somebody or something in a temporal-spatial 
manner. A natural approach to meeting the 
?what happened? requirement is to introduce 
event.
4 Event-Enriched SentenceRepresentation 
In summarization, an event is an activity or 
episode associated with participants, time, place, 
and manner. Conceptually, event bridges 
sentence and term/entity and partially fills the 
semantic gap in the sentence representation.
4.1 Event Structure and Extraction
Following (Li et al 2006), we define an event E
as a structured semantic unit consisting of one 
event term Term(E) and a set of event entities 
Entity(E). In the news domain, event terms are 
typically action verbs or deverbal nouns. Light 
verbs such as ?take?, ?give?, etc. (Tan et al,
2006) are removed.
Event entities include named entities and 
high-frequency entities. Named entities denote 
people, locations, organizations, dates, etc. 
High-frequency entities are common nouns or 
NPs that frequently participate in news events. 
Filatova and Hatzivassiloglou (2004) take the 
top 10 most frequent entities and Li et al (2006)
take the entities with frequency > 10. Rather 
than using a fixed threshold, we reformulate 
?high-frequency? as relative statistics based on 
(assumed) Gaussian distribution of the entities 
and consider those with z-score > 1 as candidate 
event entities. 
Event extraction begins with shallow parsing 
and named entity recognition, analyzing each 
1490
sentence S into ordered lists of event terms {t1,
t2, ?}. Low-frequency common entities are 
removed. If a noun is decided to be an event 
term, it cannot be (the head noun of) an entity.
The next step is to identify events with event 
terms and entities. Filatova and 
Hatzivassiloglou (2003) treat events as triplets 
with two event entities sandwiching one 
connector (event term). But the number 
restriction on entities is counterintuitive and is 
dropped in our method. We first identify n + 1
Segi segmented by n event terms tj.
? t1 ? ? tj-1 ? tj ? tj+1 ? ? tn ?
Figure 1. Segments among Event Terms
For each tj, the corresponding event Ej are 
extracted by taking tj and the event entities in its 
nearest entity-containing Segp and Segq.
Ej = [tj, Entity(Segp)?Entity(Segq)]            (Eq. 1)
where p = argmax?????????????(????) ? ? and q
= argmin?????????????(????) ? ? if such p and q
exist. 1d) is the event-extracted result of 1a).
1d) {[killed, [storm, people, Jamaica, Dominican
Republic]], [moving, [people, Jamaica, Dominican
Republic, west, Mexico]]}
From this representation, it is easy to identify 
the two events in sentence 1a) led by the event 
terms ?killed? and ?moving?. Unlike the triplets 
(two named entities and one connector) in 
(Filatova and Hatzivassiloglou 2003), an event 
in our model can have an unlimited number of 
event entities, as is often the real case. 
Moreover, we can tell that the ?killing? involves
?people?, ?storm?, ?Jamaica?, etc. and the 
?moving? involves ?Jamaica?, ?Dominique 
Republic?, etc.
The shallow parsing-based approach is 
admittedly coarse-grade (e.g., ?storm? is 
missing from the ?moving? event), but the 
extracted event-enriched representations help to 
alleviate the semantic deficiency problem in IR.
4.2 Event Relations
The relations between two events include event 
term relation and event entity relation. Two 
events are similar if their event terms are similar 
and/or their event entities are similar. Such
similarities are in turn defined on the word level. 
For event terms, we first find the root verbs of 
deverbal nouns and then measure verb similarity 
by using the fine-grained relations provided by 
VerbOcean (Chklovski and Pantel, 2004), 
which has proved useful in summarization (Liu 
et al, 2007). But unlike (Liu et al, 2007), we 
count in all the verb relations except antonymy
because considering two antonymous verbs as 
similar is counterintuitive. The other four 
relations ? similarity, strength, enablement,
before ? are all considered in our measurement 
of verb similarity. If we denote the normalized 
score of two verbs on relation i as VOi(V1, V2)
with i = 1, 2, 3, 4 corresponding to the above 
four relations, the term similarity of two events
?t(E1, E2) is defined as in Eq. 2, where ? is a 
small number to suppress zeroes. ? = 0.01 if
VOi(V1, V2) = 1 and otherwise ? = 0.
?t(E1, E2) = ?t(Term(E1), Term(E2)) = 1 ?
? (1 ? ? ??(???)???),???)???)) + ????? ) (Eq. 2)
Entity similarity is measured by the shared 
entities between two events. Li et al (2006) 
define entity similarity as the number of shared 
entities, which may unfairly assign high scores 
to events with many entities in our model. So 
we decide to use the normalized result as shown 
in Eq. 3, where ?e(E1, E2) denotes the event 
entity-based similarity between events E1 and E2.
?e(E1, E2) = 
|??????(??)???????(??)|
|??????(??)???????(??)|
(Eq. 3)
?(E1, E2), the score of event similarity, is a 
linear combination of ?t(E1, E2) and ?e(E1, E2).
?(E1, E2) = ?1 ? ?t(E1, E2) + (1 ? ?1) ? ?e(E1, E2) (Eq. 4)
4.3 Statistical Evidence for News Events
In this work, we introduce events as a middle-
layer representation between words and 
sentences under the assumptions that 1) events 
are widely distributed in a text and that 2) they 
are natural clusters of salient information in a 
text. They guarantee the relevance of event to 
our task ? summaries are condensed collections 
of salient information in source documents.
In order to confirm them, we scan the whole 
dataset in our experiment, which consists of 42 
200w human extracts and 39 400w human 
extracts for the DUC 02 multi-document extract 
task. Detailed information about the dataset can 
be found in Section 6. Table 1 lists the statistics.
200w 400w
200w +
400w
Source
Docs
Entity/Sent 8.78 8.48 8.47 6.01
Entity/Word 0.34 0.33 0.33 0.30
Event/Sent 2.43 2.26 2.28 1.42
SegnSegj-1 SegjSeg0
1491
Event/Word 0.09 0.09 0.09 0.07
Sents with
events/Sents
86.9% 85.1% 84.6% 71.3%
Table 1. Statistics from DUC 02 Dataset
There are on average 1.42 events per sentence 
in the source documents, and more than 70% of 
all the sentences contain events. The high event 
density confirms our first assumption about the 
distribution of events. For the 200w+400w 
category consisting of all the human-selected 
sentences, there are on average 2.28 events per
sentence, a 60% increase from the same ratio in 
the source documents. The proportion of event-
containing sentences reaches 84.6%, 13% 
higher than that in the source documents. Such 
is evidence that events count into the extract-
worthiness of sentences, which confirms our 
second assumption about the relevance of 
events to summarization. The data also show 
higher entity density in the extracts than in the 
source documents. As entities are still reliable 
and domain-independent clues of salient content,
we will consider both event and entity in the 
following ordering algorithm.
5 MDS Sentence Ordering with Event 
and Entity Coherence
In this section, we discuss how event can 
facilitate MDS sentence ordering with layered 
clustering on the event and sentence levels and 
then how event and entity information can be 
integrated in a coherence-based algorithm to 
order sentences based on sentence clusters.
5.1 Two-layered Clustering
After sentences are represented as collections of 
events, we need to vectorize events and 
sentences to facilitate clustering and cluster-
based sentence ordering. 
For a document set, event vectorization 
begins with aggregating all the event terms and 
entities in a set of event units (eu). Given m
distinct event terms, n distinct named entities, 
and p distinct high-frequency common entities, 
the m + n + p eu?s are a concatenation of the 
event terms and entities such that eui is an event 
term for 1 ? i ? m, a named entity for m + 1 ? i
? m + n, and a high-frequency entity for m + n +
1 ? i ? m + n + p. The eu?s define the m + n + p
dimensions of an event vector in an eu-by-event 
matrix E = [eij], as shown in Figure 2.
?
?
?
?
?
?
?
??? ? ???
? ? ?
??? ? ???
? ?
????,? ? ????,?
? ?
??????,? ? ??????,??
?
?
?
?
?
?
Figure 2. eu-by-Event Matrix
We further define EntityN(Ej) and EntityH(Ej)
to be the set of named entities and set of high-
frequency entities of Ej. Then,
??(???,???)???)) 1 ? i ? m
eij =
? ??(???,?)?????????(??)
????????(??)?
m + 1 ? i ? m + n
? ??(???,?)?????????(??)
????????(??)?
m + n + 1 ? i ?
m + n + p (Eq. 5)
                     2 w1 is identical to w2
?n(w1, w2) =  1 w1 (w2) is a part of w2 (w1) or they 
are in a hypernymy / holonymy 
relationship
             0 otherwise                          (Eq. 6)
1 w1 is identical to w2
?h(w1, w2) = 0.5 w1 are w2 are synonyms
0 otherwise                       (Eq. 7)
In Eq. 5, ?t(w1, w2) is defined as in Eq. 2.
Both the entity-based ?n(w1, w2) and ?h(w1, w2)
are measured in terms of total equivalence 
(identity) and partial equivalence. For named 
entities, partial equivalence applies to structural 
subsumption (e.g., ?Britain? and ?Great Britain?) 
and hypernymy/holonymy (e.g., ?South Africa? 
and ?Zambia?). For common entities, it applies 
to synonymy (e.g., ?security? and ?safety?). 
Partial equivalence is considered because of the 
lexical variations frequently employed in 
journalist writing. The named entity scores are 
doubled because they represent the essential 
elements of a news story.
Since the events are represented as vectors, 
sentence vectorization based on events is not as 
straightforward as on entities or terms. In this 
work we propose a novel approach of two-
layered clustering for the purpose. The basic 
idea is clustering events at the first layer and 
then using event clusters as a feature to 
vectorize and cluster sentences at the second 
E1, E2, ? Eq
eu1
?
eum
?
eum+n
...
eum+n+p
1492
layer. Hard clustering of events, such as K-
means, not only results in binary values in event 
vectors and data sparseness but also is 
inappropriate. For example, if EC1 clusters 
events all with event terms similar to t* and EC2
clusters events all with event entity sets similar 
to e* (a set), what about event {t*, e*}? 
Assigning it to either EC1 or EC2 is problematic
as it is partially similar to both. So we decide to 
do soft clustering at the first layer.
A well-studied soft clustering technique is the 
Expectation-Maximization (EM) algorithm 
which iteratively estimates the unknown 
parameters in a probability mixture model. We 
assume a Gaussian mixture model for the q
event vectors V1, V2, ?, Vq, with hidden 
variables Hi, initial means Mi, priors ?i, and 
covariance matrix Ci. The E-step is to calculate 
the hidden variables ??
? for each Vt and the M-
step re-estimates the new priors ?i?, means Mi?,
and covariance matrix Ci
?. We iterate the two 
steps until the log-likelihood converges within a 
threshold = 10-6. The performance of the EM 
algorithm is sensitive to the initial means, which 
are pre-computed by a conventional K-means.
In a preliminary study, we found that the 
event vectors display pronounced sparseness. A 
solution to this problem in an effort to leverage 
the latent ?event topics? among eu?s is the 
Latent Semantic Analysis (LSA, Landauer and 
Dumais, 1997) approach. We apply LSA-style 
dimensionality reduction to the eu-by-event 
matrix E by doing Singular Value 
Decomposition (SVD). A problem is with the 
number h of the largest singular values, which 
affects the performance of dimensionality 
reduction. In this work, we adopt a utility-based 
metric to find the best h* by maximizing intra-
cluster similarity (?h) and minimizing inter-
cluster similarity (?h) corresponding to the h-
dimensionality reduction
h* = argmax? ?? ???                               (Eq. 8)
?h is defined as the mean of average cluster 
similarities measured by cosine distance and ?h
is the mean of cluster centroid similarities. 
Because the EM clustering assigns a probability 
to every event vector, we also take those 
probabilities into account when calculating ?h
and ?h.
Based on the EM clustering of events, we 
vectorize a sentence by summing up the 
probabilities of its constituent event vectors 
over all event clusters (ECs) and obtaining an 
EC-by-sentence (Sn) matrix S = [sij].
                     ?
??? ? ???
? ? ?
??? ? ???
?
Figure 3. EC-by-Sentence Matrix
sij = ? P(????????????)????? where ?????? is Er?s vector.
At the sentence layer, hard clustering is 
sufficient because we need definitive, not 
probabilistic, membership information for the 
next step ? sentence ordering. We use K-means 
for the purpose. The LSA-style dimensionality 
reduction is still in order as possible 
performance gain is expected from the 
discovery of latent EC ?topics?. The decision of 
the best dimensionality is the same as before,
except that no probabilities are included.
5.2 Coherence-Based Sentence Ordering
Our ordering algorithm is based on sentence 
clusters, which is designed on the observation
that human writers and summarizers organize 
sentences by blocks (paragraphs). Sentences 
within a block are conceptually close to each 
other and adjacent sentences cohere with each 
other. Local coherence is thus realized within 
blocks. On the other hand, blocks are not 
randomly ordered. Two blocks are put next to 
each other if their contents are close enough to 
ensure text-level coherence. So text-level, or 
global coherence is realized among blocks. 
We believe in MDNS, the block-style 
organization is a sensible strategy taken by 
human extractors to sort sentences from 
different sources. Sentence clusters are 
simulations of such blocks and our ordering 
algorithm will be based on local coherence and 
global coherence described above. 
First we have to pinpoint the leading sentence 
for an extract. Using the heuristic of time and 
textual precedence, we first generate a set of 
possible leading sentences L = {Li} as the 
intersection of the document-leading extract 
sentence set LDoc and the time-leading sentence 
set LTime. Note that |LDoc| = the number of 
documents, LTime is in fact a sentence collection 
of time-leading documents, and LDoc ? LTime ? ?.
S1, S2, ? Sn
EC1
?
ECm
1493
If L is a singleton, finding the leading 
sentence SL is trivial. If not, SL is decided to be 
the sentence in L most similar to all the other 
sentences in the extract sentence set P so that it 
qualifies as a good topic sentence.
SL = argmax???? ? ??????(?? ,??)????\{??} (Eq. 9)
where ??????(??, ??) is the similarity between S1
and S2 in terms of their event similarity ?(S1, S2)
and entity similarity ?(S1, S2). ?(S1, S2) is an 
extended version of ?(E1, E2) (Eq. 4) by 
averaging the ?t(Ei, Ej) and ?e(Ei, Ej) for all (Ei,
Ej) pairs in S1 ?S2.
?(S1, S2) = ?2 ?
? ??(??,??)?????,?????
|?????(??)??????(??)|
+
(1 ? ?2)?
? ??(??,??)?????,?????
|?????(??)??????(??)|
              (Eq. 10)
where Event(S) is the set of all events in S. Next, 
?(S1, S2) is the cosine similarity between their 
entity vectors ?????? and ?????? with entity weights 
constructed according to Eq. 6 and 7. Then,
??????(??, ??) = ?3??(S1, S2) +(1 ? ?3)??(S1, S2) (Eq. 11)
After the leading sentence is determined, we 
identify the leading cluster it belongs to and our 
local coherence-based ordering starts with this 
cluster. We adopt a greedy algorithm, which 
selects each time from the unordered sentence 
set a sentence that best coheres with the 
sentence just selected, called anchor sentence.
Matching each candidate sentence with the 
anchor sentence only in terms of ?????? would 
assume that the sentences are isolated and 
decontextualized. But the anchor sentence did 
not come from nowhere and in order to find its 
best successor, we should also seek clues from 
its source context, which is inspired by the 
?sentence precedence? by Okazaki et al (2004).
More formally, given an anchor sentence Si at 
the end of the ordered sentence list, we select 
the next best sentence Si+1 according to their 
associative similarity and substitutive 
similarity, two crucial measures invented by us.
Associative similarity SimASS(Si, Sj) measures 
how Si and Sj associate with each other in terms 
of their event and entity coherence, which 
almost is ?????????, ???. But to better capture the 
transition between entities and the flow of topic, 
we also consider a topic-continuity score tc(Si,
Sj) according to the Centering Theory. If the 
topic continuity is measured in terms of entity 
change, local coherence can be captured by the 
centering transitions (CB and CP) in adjacent 
sentences. Based on (Taboada and Wiesemann,
2009), we assign 0.2 to the Establish and 
Continue transitions, 0.1 to Smooth Shift and 
Retain, and 0 to other centering transitions.
Since tc(Si, Sj) only applies to entities, it is 
treated as a bonus affiliated to ?(Si, Sj).
??????? ??, ??? = ?4 ? ?(Si, Sj) + (1 ? ?4) ? ?(Si, Sj)
? (1 + tc(Si, Sj))                                                 (Eq. 12)
Substitutive similarity accommodates what 
we earlier emphasized about the ?source context?
of the extracted sentences by measuring to what 
degree Si and Sj resemble each other?s relevant 
source context. More formally, let LC(Si) and 
RC(Si) be the left and right source contexts of Si
respectively, and the substitutive similarity 
SimSUB(Si, Sj) is defined as follows.
??????? ??, S?? = ??????? ??, ??( ??)? +
?????????( ??), S??                                       (Eq. 13)
In this work, we simply take LC(Si) and RC(Si)
to be the left adjacent sentence and right 
adjacent sentence of Si in the source document. 
Note that tc(Si, Sj) does not apply here. In view 
of the chronological order widely accepted in 
MDS ordering, a time penalty, tp(Si, Sj), is used 
to discount the score by 0.8 if Si?s document 
date is later than Sj?s document date. Finally, Eq.
14 summarizes our intra-cluster ordering 
method in a sentence cluster SCk.
Si+1 = argmax??????\{??} ??? ? ??????? ??, ??? +
(1 ? ??) ? ??????? ??, ???? ? ??( ??, ??) (Eq. 14)
After all the sentences in the current sentence 
cluster are ordered, we move on by considering 
the similarity of sentence clusters. Given a 
processed sentence cluster SCi, the next best 
sentence cluster SCi+1 is the one that maximizes 
the cluster similarity SimCLU(SCi, SCj) among 
the set of all clusters U. Since clusters are 
collections of sentences, their similarity is the 
mean of cross-cluster pairwise sentence 
similarities, each calculated according to Eq. 14.
Eq. 15 shows how SCi+1 is computed.
SCi+1=argmax?????\{???}??????(??? , ???) (Eq. 15)
This is how we incorporate (block-style) 
global coherence into MDS sentence ordering. 
Starting from the second chosen sentence 
cluster, we choose the first sentence in the 
current cluster with reference to the last 
sentence in the previous processed cluster and 
apply Eq. 14. We continue the whole process 
until all the extract sentences are ordered.
1494
6 Evaluation
In this section, we report the experimental result 
on the DUC 02 dataset.
6.1 Data
We use the dataset of the DUC 02 
summarization track for MDS because it 
includes an extraction task for which model 
extracts are provided. For every document set, 2 
model extracts are provided each for the 200w 
and 400w length categories. We use 1 randomly 
chosen model extract per document set per 
length category as the gold standard.
We intended to use all the 59 document sets 
on DUC 02 but found that for some categories, 
both model extracts contain material from 
sections such as the title, lead, or even byline.
Those extracts are incompatible with our design 
tailored for news body extracts. Therefore we 
have to filter them and retain only those extracts 
with all units selected from the news body. As a 
result, we collect 42 200w extracts and 39 400w 
extracts as our experimental dataset.
6.2 Peer Orderings
We evaluate the role played by various key 
elements in our approach, including event, topic 
continuity, time penalty, and LSA-style 
dimensionality reduction. In addition, we 
produce a random ordering and a baseline 
ordering according to chronological and textual 
order only. Table 2 lists the 9 peer orderings to 
be evaluated, with their codes.
A Random
B Baseline (time order + textual order)
C Entity only (no LSA)
D Event only (no LSA)
E Entity + Event ? topic continuity (no LSA)
F Entity + Event ? time penalty (no LSA)
G Entity + Event (no LSA)
H Entity + Event (event clustering LSA)
I Entity + Event (event + sentence clustering LSA)
Table 2. Peer Orderings
6.3 Metrics
A popular metric used in sequence evaluation 
is Kendall?s ? (Lapata, 2006), which measures 
ordering differences in terms of the number of 
adjacent sentence inversions necessary to 
convert a test ordering to the reference ordering.
? = 4m/(n(n ? 1))             (Eq. 16)
where m is the number of inversions described 
above and n is the total number of sentences.
The second metric we use is the Average 
Continuity (AC) developed by Bollegala et al
(2006), which captures the intuition that the 
ordering quality can be estimated by the number 
of correctly arranged continuous sentences.
?? = exp( ?
???
? log( ?? + ?)????                (Eq. 17)
where k is the maximum number of continuous 
sentences, ? is a small value in case Pn = 1. Pn,
the proportion of continuous sentences of length 
n in an ordering, is defined as m/(N ? n + 1) 
where m is the number of continuous sentences 
of length n in both the test and reference 
orderings and N is the total number of sentences. 
We set k = 4 and ? = 0.01.
6.4 Result
We empirically determine all the parameters (?
i
)
and produce all the peer orderings. Table 3 lists
the result, where we also show the statistical 
significance between the full model peer
ordering ?I? and all other versions, marked by * 
(p < .05) and ** (p < .01) on a two-tailed t-test.
Peer 
Code
200w 400w
Kendall?s ? AC Kendall?s ? AC
A 0.014** 0.009** -0.019** 0.004**
B 0.387 0.151* 0.259** 0.151*
C 0.369* 0.128* 0.264* 0.156*
D 0.380 0.163 0.270* 0.158*
E 0.375* 0.156* 0.267* 0.157*
F 0.388 0.159* 0.264* 0.157*
G 0.385 0.158* 0.269* 0.162
H 0.384 0.164 0.292* 0.170
I 0.395 0.170 0.350 0.176
Table 3. Evaluation Result
Almost all versions with entity and event 
information outperform the baseline. The LSA-
style dimensionality reduction proves effective 
for our task, as the full model (Peer I) ranks first 
and significantly beats versions without event
information, topic continuity, or LSA. Applying
LSA to both event and sentence clustering is 
better than applying it only to event clustering
(Peer H), which produces unstable results and is 
sometimes outperformed by no-LSA versions
(Peer G).
Event (Peer D) proves to be more valuable 
than entity (Peer C) as the event-only versions 
outperform the entity-only version in all 
categories, which is predicable because events
1495
are high-level constructs that incorporate most 
of the document-level important entities.
When entity is used, extra bonus can be 
gained from topic continuity concerns from CT
(Peer E vs. Peer G) because the centering 
transition effectively captures the coherence 
pattern between adjacent sentences. The effect 
of the chronological order seems less clear (Peer 
F vs. P) as removing it hurts longer extracts 
rather than short extracts. Therefore
chronological clues are more valuable for 
arranging more sentences from the same source 
document.
Our ordering algorithm achieves even better 
result with long extracts because the importance 
of order and coherence grows with text length. 
Measured by Kendall?s ?, the full model 
ordering in the 400w category is significantly
better than all other orderings.
For a qualitative evaluation, we select the 
200w extract d080ae and list all the sentences in 
Figure 4. The event terms are boldfaced and the 
event entities are underlined.
Limited by space, let?s focus on the baseline
(1 2 3 4 5 6), entity-only (3 5 2 4 6 1), and full-
model versions (3 5 4 2 1 6). The news extract 
is about the acquitting of child molesters. Both 
the ?acquitting? and ?molesting? events are 
found in 1) and 3) but only the latter qualifies as
the topic sentence because it contains important 
event entities. Choosing 3) instead of 1) as the 
leading sentence shows the advantage of our 
event-enriched model over the baseline. The
same choice is made by the entity-only version 
because 3) happens to be also entity-intensive. 
In order to see the advantage of the full model 
over the entity-only model, let?s consider 2) and 
4). 2) is chosen by the entity-only model after 5) 
because of the heavy entity overlap between 5) 
a
because of the heavy entity overlap between 5) 
and 2). But semantically, 2) is not as close to 5) 
as 4) because only 4) contains entities for both 
the ?acquitting? (?juror?) and ?molesting?
(?children?) events and intuitively, 4) continues 
the main trial-acquittal event topic but 2) 
supplies only secondary information. We
examined the sentence clusters before the 
ordering and found that 3), 5), and 4) are 
clustered together only by the full model,
leading to better coherence, locally and globally.
7 Conclusion and Future Work
We set out by realizing the semantic deficiency 
of IR and propose a low-cost approach of 
building event semantics into sentence 
representation. Event extraction relies on 
shallow parsing and external knowledge sources. 
Then we propose a novel approach of two-
layered clustering to use event information,
coupled with LSA-style dimensionality
reduction. MDS sentence ordering is guided by 
local and global coherence to simulate the 
block-style writing and is realized by a greedy 
algorithm. The evaluation shows clear 
advantage of our event-enriched model over
baseline and event-agonistic models, 
quantitatively and qualitatively.
The extraction approach can be refined by 
deep parsing and rich verb (frame) semantics. In 
a follow-up project, we will expand our dataset 
and experiment with more data and incorporate 
human evaluation in comparative tasks.
Acknowledgment
The work described in this paper was partially 
supported by a grant from the HK RGC (Project 
Number: PolyU5217/07E).
1) Thursday's acquittals in the McMartin Pre-School molestation case outraged parents who said prosecutors botched it, 
while those on the defense side proclaimed a triumph of justice over hysteria and hype.
2) Originally, there were seven defendants, including Raymond Buckey's sister, Peggy Ann Buckey, and Virginia McMartin, 
the founder of the school, mother of Mrs. Buckey and grandmother of Raymond Buckey.
3) Seven jurors who spoke with reporters in a joint news conference after acquitting Raymond Buckey and his mother, 
Peggy McMartin Buckey, on 52 molestation charges Thursday said they felt some children who testified may have been 
molested _ but not at the family-run McMartin Pre-School.
4) ``The children were never allowed to say in their own words what happened to them,'' said juror John Breese.
5) Ray Buckey and his mother, Peggy McMartin Buckey, were found not guilty Thursday of molesting children at the 
family-run McMartin Pre-School in Manhattan Beach, a verdict which brought to a close the longest and costliest criminal 
trial in history .
6) As it becomes apparent that McMartin cases will stretch out for years to come, parents and the former criminal defendants
alike are trying to resign themselves to the inevitability that the matter may be one they can never leave behind.
Figure 4. Extract sentences of d80ae, 200w
1496
References
Barzilay, R., Elhadad, N., and McKeown, K. 2002. 
Inferring Strategies for Sentence Ordering in 
Multidocument News Summarization. Journal of 
Artificial Intelligence Research, 17:35?55.
Barzilay, R., and Lapata, M. 2005. Modeling Local 
Coherence: An Entity-based Approach. In 
Proceedings of the 43rd Annual Meeting of the 
ACL, 141-148. Ann Arbor.
Barzilay, R., and Lapata, M. 2008. Modeling Local 
Coherence: An Entity-Based Approach. 
Computational Linguistics, 34:1?34.
Bollegala, D, Okazaki, N., and Ishizuka, M. 2006. A 
Bottom-up Approach to Sentence Ordering for 
Multi-document Summarization. In Proceedings 
of the 21st International Conference on 
Computational Linguistics and 44th Annual 
Meeting of the ACL, 385?392. Sydney, Australia.
Bromberg, I. 2006. Ordering Sentences According to 
Topicality. Presented at the Midwest 
Computational Linguistics Colloquium.
Chklovski, T., and Pantel, P. 2004. VerbOcean: 
Mining the Web for Fine-Grained Semantic Verb 
Relations. In Proceedings of Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-04). 11?13. Barcelona, 
Spain.
Conroy, J. M., Schlesinger, J. D., and Goldstein, J. 
2006. CLASSY Tasked Based Summarization: 
Back to Basics. In proceedings of the Document 
Understanding Conference (DUC-06).
Filatova, E., and Hatzivassiloglou, V. 2003. Domain-
independent detection, extraction, and labeling of 
atomic events. In Proceedings of RANLP, 145?
152, Borovetz, Bulgaria.
Filatova, E., and Hatzivassiloglou, V. 2004. Event-
Based Extractive Summarization. In Proceedings 
of the ACL-04, 104?111.
Grosz, B. J., Aravind K. J., and Scott W. 1995. 
Centering: A framework for Modeling the Local 
Coherence of Discourse. Computational 
Linguistics, 21(2):203?225.
Jurafsky D., and Martin, J. H. 2009. Speech and 
Language Processing, Second Edition. Upper 
Saddle River, NJ: Pearson Education International.
Landauer, T., and Dumais, S. 1997. A solution to 
Plato?s problem: The latent semantic analysis 
theory of the acquisition, induction, and 
representation of knowledge. Psychological 
Review, 104.
Lapata, M. 2003. Probabilistic Text Structuring: 
Experiments with Sentence Ordering. In 
Proceedings of the Annual Meeting of ACL, 545-
552. Sapporo, Japan.
Li, W., Wu, M., Lu, Q., Xu, W., and Yuan, C. 2006. 
Extractive Summarization Using Inter- and Intra-
Event Relevance. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the ACL,
369?376. Sydney.
Liu, M., Li, W., Wu, M., and Lu, Q. 2007. Extractive 
Summarization Based on Event Term Clustering. 
In Proceedings of the ACL 2007 Demo and Poster
Sessions, 185?188. Prague.
Okazaki, N., Matsuo, Y., and Ishizuka, M. 2004. 
Improving Chronological Ordering by Precedence 
Relation. In Proceedings of 20th International 
Conference on Computational Linguistics 
(COLING 04), 750?756.
Taboada, M., and Wiesemann, L., Subjects and 
topics in conversation. Journal of Pragmatics
(2009), doi:10.1016/j.pragma.2009.04.009.
Tan, Y. F., Kan, M., and Cui, H. 2006. Extending 
corpus-based identification of light verb 
constructions using a supervised learning 
framework. In Proceedings of the EACL 2006 
Workshop on Multi-word-expressions in a 
multilingual context, 49?56, Trento, Italy.
Yoshioka, M., and Haraguchi, M. 2004. Multiple 
News Articles Summarization Based on Event 
Reference Information. In Working Notes of 
NTCIR-4, Tokyo.
1497
Feature-Frequency?Adaptive On-line
Training for Fast and Accurate Natural
Language Processing
Xu Sun?
Peking University
Wenjie Li??
Hong Kong Polytechnic University
Houfeng Wang?
Peking University
Qin Lu?
Hong Kong Polytechnic University
Training speed and accuracy are two major concerns of large-scale natural language processing
systems. Typically, we need to make a tradeoff between speed and accuracy. It is trivial to improve
the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.
Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time,
which is the target of this work. To reach this target, we present a new training method, feature-
frequency?adaptive on-line training, for fast and accurate training of natural language process-
ing systems. It is based on the core idea that higher frequency features should have a learning rate
that decays faster. Theoretical analysis shows that the proposed method is convergent with a fast
convergence rate. Experiments are conducted based on well-known benchmark tasks, including
named entity recognition, word segmentation, phrase chunking, and sentiment analysis. These
tasks consist of three structured classification tasks and one non-structured classification task,
with binary features and real-valued features, respectively. Experimental results demonstrate
that the proposed method is faster and at the same time more accurate than existing methods,
achieving state-of-the-art scores on the tasks with different characteristics.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: xusun@pku.edu.cn.
?? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: cswjli@comp.polyu.edu.hk.
? Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, Beijing, China,
and School of EECS, Peking University, Beijing, China. E-mail: wanghf@pku.edu.cn.
? Department of Computing, Hong Kong Polytechnic University, Hung Hom, Kowloon 999077, Hong
Kong. E-mail: csluqin@comp.polyu.edu.hk.
Submission received: 27 December 2012; revised version received: 30 May 2013; accepted for publication:
16 September 2013.
doi:10.1162/COLI a 00193
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Training speed is an important concern of natural language processing (NLP) systems.
Large-scale NLP systems are computationally expensive. In many real-world applica-
tions, we further need to optimize high-dimensional model parameters. For example,
the state-of-the-art word segmentation system uses more than 40 million features (Sun,
Wang, and Li 2012). The heavyNLPmodels together with high-dimensional parameters
lead to a challenging problem onmodel training, whichmay require week-level training
time even with fast computing machines.
Accuracy is another very important concern of NLP systems. Nevertheless, usually
it is quite difficult to build a system that has fast training speed and at the same time
has high accuracy. Typically we need to make a tradeoff between speed and accuracy,
to trade training speed for higher accuracy or vice versa. In this work, we have tried
to overcome this problem: to improve the training speed and the model accuracy at the
same time.
There are twomajor approaches for parameter training: batch and on-line. Standard
gradient descent methods are normally batch training methods, in which the gradient
computed by using all training instances is used to update the parameters of the model.
The batch training methods include, for example, steepest gradient descent, conjugate
gradient descent (CG), and quasi-Newtonmethods like limited-memory BFGS (Nocedal
and Wright 1999). The true gradient is usually the sum of the gradients from each
individual training instance. Therefore, batch gradient descent requires the training
method to go through the entire training set before updating parameters. This is why
batch training methods are typically slow.
On-line learning methods can significantly accelerate the training speed compared
with batch training methods. A representative on-line training method is the stochastic
gradient descent method (SGD) and its extensions (e.g., stochastic meta descent) (Bottou
1998; Vishwanathan et al. 2006). The model parameters are updated more frequently
compared with batch training, and fewer passes are needed before convergence. For
large-scale data sets, on-line training methods can be much faster than batch training
methods.
However, we find that the existing on-line training methods are still not good
enough for training large-scale NLP systems?probably because those methods are
not well-tailored for NLP systems that have massive features. First, the convergence
speed of the existing on-line training methods is not fast enough. Our studies show that
the existing on-line training methods typically require more than 50 training passes
before empirical convergence, which is still slow. For large-scale NLP systems, the
training time per pass is typically long and fast convergence speed is crucial. Second,
the accuracy of the existing on-line training methods is not good enough. We want to
further improve the training accuracy. We try to deal with the two challenges at the
same time. Our goal is to develop a new training method for faster and at the same time
more accurate natural language processing.
In this article, we present a new on-line training method, adaptive on-line gradient
descent based on feature frequency information (ADF),1 for very accurate and fast
on-line training of NLP systems. Other than the high training accuracy and fast train-
ing speed, we further expect that the proposed training method has good theoretical
1 ADF source code and tools can be obtained from http://klcl.pku.edu.cn/member/sunxu/index.htm.
564
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
properties. We want to prove that the proposed method is convergent and has a fast
convergence rate.
In the proposed ADF training method, we use a learning rate vector in the on-line
updating. This learning rate vector is automatically adapted based on feature frequency
information in the training data set. Each model parameter has its own learning rate
adapted on feature frequency information. This proposal is based on the simple intu-
ition that a feature with higher frequency in the training process should have a learning
rate that decays faster. This is because a higher frequency feature is expected to be
well optimized with higher confidence. Thus, a higher frequency feature is expected to
have a lower learning rate. We systematically formalize this intuition into a theoretically
sound training algorithm, ADF.
The main contributions of this work are as follows:
r On the methodology side, we propose a general purpose on-line training
method, ADF. The ADF method is significantly more accurate than
existing on-line and batch training methods, and has faster training speed.
Moreover, theoretical analysis demonstrates that the ADF method is
convergent with a fast convergence rate.
r On the application side, for the three well-known tasks, including named
entity recognition, word segmentation, and phrase chunking, the proposed
simple method achieves equal or even better accuracy than the existing
gold-standard systems, which are complicated and use extra resources.
2. Related Work
Our main focus is on structured classification models with high dimensional features.
For structured classification, the conditional random fields model is widely used. To
illustrate that the proposed method is a general-purpose training method not limited to
a specific classification task or model, we also evaluate the proposal for non-structured
classification tasks like binary classification. For non-structured classification, the max-
imum entropy model (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996)
is widely used. Here, we review the conditional random fields model and the related
work of on-line training methods.
2.1 Conditional Random Fields
The conditional random field (CRF) model is a representative structured classification
model and it is well known for its high accuracy in real-world applications. The CRF
model is proposed for structured classification by solving ?the label bias problem?
(Lafferty, McCallum, and Pereira 2001). Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a global feature vector f, the probability
of a label sequence y conditioned on the observation sequence x is modeled as follows
(Lafferty, McCallum, and Pereira 2001):
P(y|x,w) =
exp {w>f (y,x)}
?
?y? exp {w>f (y? ,x)}
(1)
wherew is a parameter vector.
565
Computational Linguistics Volume 40, Number 3
Given a training set consisting of n labeled sequences, zi = (xi,yi), for i = 1 . . .n,
parameter estimation is performed by maximizing the objective function,
L(w) =
n
?
i=1
logP(yi|xi,w)? R(w) (2)
The first term of this equation represents a conditional log-likelihood of training
data. The second term is a regularizer for reducing overfitting. We use an L2 prior,
R(w) = ||w||
2
2?2 . In what follows, we denote the conditional log-likelihood of each sample
as logP(yi|xi,w) as `(zi,w). The final objective function is as follows:
L(w) =
n
?
i=1
`(zi,w)?
||w||2
2?2
(3)
2.2 On-line Training
The most representative on-line training method is the SGD method (Bottou 1998;
Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013). The SGD method uses a
randomly selected small subset of the training sample to approximate the gradient of
an objective function. The number of training samples used for this approximation is
called the batch size. By using a smaller batch size, one can update the parameters
more frequently and speed up the convergence. The extreme case is a batch size of 1,
and it gives the maximum frequency of updates, which we adopt in this work. In this
case, the model parameters are updated as follows:
wt+1 = wt + ?t?wtLstoch(zi,wt) (4)
where t is the update counter, ?t is the learning rate or so-called decaying rate, and
Lstoch(zi,wt) is the stochastic loss function based on a training sample zi. (More details
of SGD are described in Bottou [1998], Tsuruoka, Tsujii, and Ananiadou [2009], and
Sun et al. [2013].) Following the most recent work of SGD, the exponential decaying
rate works the best for natural language processing tasks, and it is adopted in our
implementation of the SGD (Tsuruoka, Tsujii, and Ananiadou 2009; Sun et al. 2013).
Other well-known on-line training methods include perceptron training (Freund
and Schapire 1999), averaged perceptron training (Collins 2002), more recent devel-
opment/extensions of stochastic gradient descent (e.g., the second-order stochastic
gradient descent training methods like stochastic meta descent) (Vishwanathan et al.
2006; Hsu et al. 2009), and so on. However, the second-order stochastic gradient descent
method requires the computation or approximation of the inverse of the Hessian matrix
of the objective function, which is typically slow, especially for heavily structured classi-
fication models. Usually the convergence speed based on number of training iterations
is moderately faster, but the time cost per iteration is slower. Thus the overall time cost
is still large.
Compared with the related work on batch and on-line training (Jacobs 1988;
Sperduti and Starita 1993; Dredze, Crammer, and Pereira 2008; Duchi, Hazan, and
Singer 2010; McMahan and Streeter 2010), our work is fundamentally different. The
proposedADF trainingmethod is based on feature frequency adaptation, and to the best
of our knowledge there is no prior work on direct feature-frequency?adaptive on-line
566
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
training. Compared with the confidence-weighted (CW) classification method and its
variation AROW (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza, and Dredze
2009), the proposed method is substantially different. While the feature frequency
information is implicitly modeled via a complicated Gaussian distribution framework
in Dredze, Crammer, and Pereira (2008) and Crammer, Kulesza, and Dredze (2009),
the frequency information is explicitly modeled in our proposal via simple learning
rate adaptation. Our proposal is more straightforward in capturing feature frequency
information, and it has no need to use Gaussian distributions and KL divergence,
which are important in the CW and AROW methods. In addition, our proposal is a
probabilistic learning method for training probabilistic models such as CRFs, whereas
the CW and AROW methods (Dredze, Crammer, and Pereira 2008; Crammer, Kulesza,
and Dredze 2009) are non-probabilistic learning methods extended from perceptron-
style approaches. Thus, the framework is different. This work is a substantial extension
of the conference version (Sun, Wang, and Li 2012). Sun, Wang, and Li (2012) focus on
the specific task of word segmentation, whereas this article focuses on the proposed
training algorithm.
3. Feature-Frequency?Adaptive On-line Learning
In traditional on-line optimization methods such as SGD, no distinction is made for
different parameters in terms of the learning rate, and this may result in slow conver-
gence of the model training. For example, in the on-line training process, suppose the
high frequency feature f1 and the low frequency feature f2 are observed in a training
sample and their corresponding parameters w1 and w2 are to be updated via the same
learning rate ?t. Suppose the high frequency feature f1 has been updated 100 times
and the low frequency feature f2 has only been updated once. Then, it is possible that
the weight w1 is already well optimized and the learning rate ?t is too aggressive for
updating w1. Updating the weight w1 with the learning rate ?t may make w1 be far
from the well-optimized value, and it will require corrections in the future updates. This
causes fluctuations in the on-line training and results in slow convergence speed. On
the other hand, it is possible that the weight w2 is poorly optimized and the same learn-
ing rate ?t is too conservative for updating w2. This also results in slow convergence
speed.
To solve this problem, we propose ADF. In spite of the high accuracy and fast
convergence speed, the proposed method is easy to implement. The proposed method
with feature-frequency?adaptive learning rates can be seen as a learning method with
specific diagonal approximation of the Hessian information based on assumptions of
feature frequency information. In this approximation, the diagonal elements of the
diagonal matrix correspond to the feature-frequency?adaptive learning rates. Accord-
ing to the aforementioned example and analysis, it assumes that a feature with higher
frequency in the training process should have a learning rate that decays faster.
3.1 Algorithm
In the proposed ADF method, we try to use more refined learning rates than traditional
SGD training. Instead of using a single learning rate (a scalar) for all weights, we extend
the learning rate scalar to a learning rate vector, which has the same dimension as the
weight vector w. The learning rate vector is automatically adapted based on feature
567
Computational Linguistics Volume 40, Number 3
frequency information. By doing so, each weight has its own learning rate, and we will
show that this can significantly improve the convergence speed of on-line learning.
In the ADF learning method, the update formula is:
wt+1 = wt +?t ? gt (5)
The update term gt is the gradient term of a randomly sampled instance:
gt = ?wtLstoch(zi,wt) = ?wt
{
`(zi,wt)?
||wt||2
2n?2
}
In addition, ?t ? R
f
+ is a positive vector-valued learning rate and ? denotes the
component-wise (Hadamard) product of two vectors.
The learning rate vector ?t is automatically adapted based on feature frequency
information in the updating process. Intuitively, a feature with higher frequency in the
training process has a learning rate that decays faster. This is because a weight with
higher frequency is expected to be more adequately trained, hence a lower learning
rate is preferable for fast convergence. We assume that a high frequency feature should
have a lower learning rate, and a low frequency feature should have a relatively higher
learning rate in the training process.We systematically formalize this idea into a theoret-
ically sound training algorithm. The proposedmethodwith feature-frequency?adaptive
learning rates can be seen as a learning method with specific diagonal approximation
of the inverse of the Hessian matrix based on feature frequency information.
Given awindow size q (number of samples in awindow), we use a vector v to record
the feature frequency. The kth entry vk corresponds to the frequency of the feature k in
this window. Given a feature k, we use u to record the normalized frequency:
u = vk/q
For each feature, an adaptation factor ? is calculated based on the normalized frequency
information, as follows:
? = ?? u(?? ?)
where ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1. Intu-
itively, the upper bound ? corresponds to the adaptation factor of the lowest frequency
features, and the lower bound ? corresponds to the adaptation factor of the highest
frequency features. The optimal values of ? and ? can be tuned based on specific real-
world tasks, for example, via cross-validation on the training data or using held-out
data. In practice, via cross-validation on the training data of different tasks, we found
that the following setting is sufficient to produce adequate performance for most of the
real-world natural language processing tasks: ? around 0.995, and ? around 0.6. This
indicates that the feature frequency information has similar characteristics across many
different natural language processing tasks.
As we can see, a feature with higher frequency corresponds to a smaller scalar via
linear approximation. Finally, the learning rate is updated as follows:
?k ? ??k
568
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
ADF learning algorithm
1: procedure ADF(Z,w, q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample zi at random from the data set Z
5: . v ? UPDATEFEATUREFREQ(v, zi)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATELEARNRATE(?, v)
8: . . v ? 0
9: . g ??wLstoch(zi,w)
10: . w ? w +? ? g
11: . t? t+ 1
12: returnw
13:
14: procedure UPDATEFEATUREFREQ(v, zi)
15: for k ? features used in sample zi
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATELEARNRATE(?, v)
20: for k ? all features
21: . u? vk/q
22: . ?? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1
The proposed ADF on-line learning algorithm. In the algorithm, Z is the training data set; q, c, ?,
and ? are hyper-parameters; q is an integer representing window size; c is for initializing the
learning rates; and ? and ? are the upper and lower bounds of a scalar, with 0 < ? < ? < 1.
With this setting, different features correspond to different adaptation factors based
on feature frequency information. Our ADF algorithm is summarized in Figure 1.
The ADF training method is efficient because the only additional computation
(compared with traditional SGD) is the derivation of the learning rates, which is simple
and efficient. As we know, the regularization of SGD can perform efficiently via the opti-
mization based on sparse features (Shalev-Shwartz, Singer, and Srebro 2007). Similarly,
the derivation of ?t can also perform efficiently via the optimization based on sparse
features. Note that although binary features are common in natural language processing
tasks, the ADF algorithm is not limited to binary features and it can be applied to real-
valued features.
3.2 Convergence Analysis
We want to show that the proposed ADF learning algorithm has good convergence
properties. There are two steps in the convergence analysis. First, we show that the
ADF update rule is a contraction mapping. Then, we show that the ADF training is
asymptotically convergent, and with a fast convergence rate.
To simplify the discussion, our convergence analysis is based on the convex loss
function of traditional classification or regression problems:
L(w) =
n
?
i=1
`(xi, yi,w ? f i)?
||w||2
2?2
569
Computational Linguistics Volume 40, Number 3
where f i is the feature vector generated from the training sample (xi, yi). L(w) is a func-
tion in w ? f i, such as 12 (yi ?w ? f i)2 for regression or log[1+ exp(?yiw ? f i)] for binary
classification.
To make convergence analysis of the proposed ADF training algorithm, we need to
introduce several mathematical definitions. First, we introduce Lipschitz continuity:
Definition 1 (Lipschitz continuity)
A function F : X ? R is Lipschitz continuous with the degree of D if |F(x)? F(y)| ?
D|x? y| for ?x, y ? X . X can be multi-dimensional space, and |x? y| is the distance
between the points x and y.
Based on the definition of Lipschitz continuity, we give the definition of the
Lipschitz constant ||F||Lip as follows:
Definition 2 (Lipschitz constant)
||F||Lip := inf{D where |F(x)? F(y)| ? D|x? y| for ?x, y}
In other words, the Lipschitz constant ||F||Lip is the lower bound of the continuity degree
that makes the function F Lipschitz continuous.
Further, based on the definition of Lipschitz constant, we give the definition of
contraction mapping as follows:
Definition 3 (Contraction mapping)
A function F : X ? X is a contraction mapping if its Lipschitz constant is smaller than
1: ||F||Lip < 1.
Then, we can show that the traditional SGD update is a contraction mapping.
Lemma 1 (SGD update rule is contraction mapping)
Let ? be a fixed low learning rate in SGD updating. If ? ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1,
the SGD update rule is a contraction mapping in Euclidean space with Lipschitz con-
tinuity degree 1? ?/?2.
The proof can be extended from the relatedwork on convergence analysis of parallel
SGD training (Zinkevich et al. 2010). The stochastic training process is a one-following-
one dynamic update process. In this dynamic process, if we use the same update rule F,
we havewt+1 = F(wt) andwt+2 = F(wt+1). It is only necessary to prove that the dynamic
update is a contraction mapping restricted by this one-following-one dynamic process.
That is, for the proposed ADF update rule, it is only necessary to prove it is a dynamic
contraction mapping. We formally define dynamic contraction mapping as follows.
Definition 4 (Dynamic contraction mapping)
Given a function F : X ? X , suppose the function is used in a dynamic one-following-
one process: xt+1 = F(xt) and xt+2 = F(xt+1) for ?xt ? X . Then, the function F is a
dynamic contraction mapping if ?D < 1, |xt+2 ? xt+1| ? D|xt+1 ? xt| for ?xt ? X .
We can see that a contraction mapping is also a dynamic contraction mapping, but
a dynamic contraction mapping is not necessarily a contraction mapping. We first show
570
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
that the ADF update rule with a fixed learning rate vector of different learning rates is
a dynamic contraction mapping.
Theorem 1 (ADF update rule with fixed learning rates)
Let ? be a fixed learning rate vector with different learning rates. Let ?max be the max-
imum learning rate in the learning rate vector ?: ?max := sup{?i where ?i ? ?}. Then
if ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, the ADF update rule is a dynamic contraction
mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Further, we need to prove that the ADF update rule with a decaying learning
rate vector is a dynamic contraction mapping, because the real ADF algorithm has a
decaying learning rate vector. In the decaying case, the condition that ?max ? (||x2i || ?
||?y? `(xi, yi, y?)||Lip)?1 can be easily achieved, because ? continues to decay with an
exponential decaying rate. Even if the ? is initialized with high values of learning rates,
after a number of training passes (denoted as T) ?T is guaranteed to be small enough so
that ?max := sup{?i where ?i ? ?T} and ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Without
losing generality, our convergence analysis starts from the pass T and we take ?T as ?0
in the following analysis. Thus, we can show that the ADF update rule with a decaying
learning rate vector is a dynamic contraction mapping:
Theorem 2 (ADF update rule with decaying learning rates)
Let ?t be a learning rate vector in the ADF learning algorithm, which is decaying
over the time t and with different decaying rates based on feature frequency infor-
mation. Let ?t start from a low enough learning rate vector ?0 such that ?max ?
(||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1, where ?max is the maximum element in?0. Then, the ADF
update rule with decaying learning rate vector is a dynamic contraction mapping in
Euclidean space with Lipschitz continuity degree 1? ?max/?2.
The proof is sketched in Section 5.
Based on the connections between ADF training and contraction mapping, we
demonstrate the convergence properties of the ADF training method. First, we prove
the convergence of the ADF training.
Theorem 3 (ADF convergence)
ADF training is asymptotically convergent.
The proof is sketched in Section 5.
Further, we analyze the convergence rate of the ADF training. When we have the
lowest learning rate?t+1 = ??t, the expectation of the obtainedwt is as follows (Murata
1998; Hsu et al. 2009):
E(wt) = w? +
t
?
m=1
(I ??0?mH(w?))(w0 ?w?)
where w? is the optimal weight vector, and H is the Hessian matrix of the objective
function. The rate of convergence is governed by the largest eigenvalue of the function
Ct =
?t
m=1(I ??0?mH(w?)). Following Murata (1998) and Hsu et al. (2009), we can
derive a bound of rate of convergence, as follows.
571
Computational Linguistics Volume 40, Number 3
Theorem 4 (ADF convergence rate)
Assume ? is the largest eigenvalue of the function Ct =
?t
m=1(I ??0?mH(w?)). For the
proposed ADF training, its convergence rate is bounded by ?, and we have
? ? exp {?0???? 1}
where ? is the minimum eigenvalue ofH(w?).
The proof is sketched in Section 5.
The convergence analysis demonstrates that the proposed method with feature-
frequency-adaptive learning rates is convergent and the bound of convergence rate
is analyzed. It demonstrates that increasing the values of ?0 and ? leads to a lower
bound of the convergence rate. Because the bound of the convergence rate is just an
up-bound rather than the actual convergence rate, we still need to conduct automatic
tuning of the hyper-parameters, including ?0 and ?, for optimal convergence rate in
practice. The ADF training method has a fast convergence rate because the feature-
frequency-adaptive schema can avoid the fluctuations on updating the weights of high
frequency features, and it can avoid the insufficient training on updating the weights of
low frequency features. In the following sections, we perform experiments to confirm
the fast convergence rate of the proposed method.
4. Evaluation
Our main focus is on training heavily structured classification models. We evaluate the
proposal on three NLP structured classification tasks: biomedical named entity recogni-
tion (Bio-NER), Chinese word segmentation, and noun phrase (NP) chunking. For the
structured classification tasks, the ADF training is based on the CRF model (Lafferty,
McCallum, and Pereira 2001). Further, to demonstrate that the proposed method is
not limited to structured classification tasks, we also perform experiments on a non-
structured binary classification task: sentiment-based text classification. For the non-
structured classification task, the ADF training is based on themaximum entropymodel
(Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1996).
4.1 Biomedical Named Entity Recognition (Structured Classification)
The biomedical named entity recognition (Bio-NER) task is from the BIONLP-2004
shared task. The task is to recognize five kinds of biomedical named entities, including
DNA, RNA, protein, cell line, and cell type, on the MEDLINE biomedical text mining
corpus (Kim et al. 2004). A typical approach to this problem is to cast it as a sequential
labeling task with the BIO encoding.
This data set consists of 20,546 training samples (from 2,000 MEDLINE article
abstracts, with 472,006 word tokens) and 4,260 test samples. The properties of the data
are summarized in Table 1. State-of-the-art systems for this task include Settles (2004),
Finkel et al. (2004), Okanohara et al. (2006), Hsu et al. (2009), Sun, Matsuzaki, et al.
(2009), and Tsuruoka, Tsujii, and Ananiadou (2009).
Following previous studies for this task (Okanohara et al. 2006; Sun, Matsuzaki,
et al. 2009), we use word token?based features, part-of-speech (POS) based features,
and orthography pattern?based features (prefix, uppercase/lowercase, etc.), as listed in
Table 2. With the traditional implementation of CRF systems (e.g., the HCRF package),
the edges features usually contain only the information of yi?1 and yi, and ignore the
572
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 1
Summary of the Bio-NER data set.
#Abstracts #Sentences #Words
Train 2,000 20,546 (10/abs) 472,006 (23/sen)
Test 404 4,260 (11/abs) 96,780 (23/sen)
Table 2
Feature templates used for the Bio-NER task. wi is the current word token on position i. ti is the
POS tag on position i. oi is the orthography mode on position i. yi is the classification label on
position i. yi?1yi represents label transition. A? B represents a Cartesian product between
two sets.
Word Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?2, ti?1, ti, ti+1, ti+2, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
Orthography Pattern?based Features:
{oi?2, oi?1, oi, oi+1, oi+2, oi?2oi?1, oi?1oi, oioi+1, oi+1oi+2}
?{yi, yi?1yi}
information of the observation sequence (i.e., x). The major reason for this simple real-
ization of edge features in traditional CRF implementation is to reduce the dimension
of features. To improve the model accuracy, we utilize rich edge features following Sun,
Wang, and Li (2012), in which local observation information of x is combined in edge
features just like the implementation of node features. A detailed introduction to rich
edge features can be found in Sun, Wang, and Li (2012). Using the feature templates,
we extract a high dimensional feature set, which contains 5.3? 107 features in total.
Following prior studies, the evaluation metric for this task is the balanced F-score
defined as 2PR/(P+ R), where P is precision and R is recall.
4.2 Chinese Word Segmentation (Structured Classification)
Chinese word segmentation aims to automatically segment character sequences into
word sequences. Chinese word segmentation is important because it is the first step
for most Chinese language information processing systems. Our experiments are based
on the Microsoft Research data provided by The Second International Chinese Word
Segmentation Bakeoff. In this data set, there are 8.8? 104 word-types, 2.4? 106 word-
tokens, 5? 103 character-types, and 4.1? 106 character-tokens. State-of-the-art systems
for this task include Tseng et al. (2005), Zhang, Kikui, and Sumita (2006), Zhang and
Clark (2007), Gao et al. (2007), Sun, Zhang, et al. (2009), Sun (2010), Zhao et al. (2010),
and Zhao and Kit (2011).
The feature engineering follows previous work on word segmentation (Sun, Wang,
and Li 2012). Rich edge features are used. For the classification label yi and the label
transition yi?1yi on position i, we use the feature templates as follows (Sun, Wang, and
Li 2012):
r Character unigrams located at positions i? 2, i? 1, i, i+ 1, and i+ 2.
573
Computational Linguistics Volume 40, Number 3
r Character bigrams located at positions i? 2, i? 1, i and i+ 1.
r Whether xj and xj+1 are identical, for j = i? 2, . . . , i+ 1.
r Whether xj and xj+2 are identical, for j = i? 3, . . . , i+ 1.
r The character sequence xj,i if it matches a word w ? U, with the constraint
i? 6 < j < i. The item xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected from the training data.
r The character sequence xi,k if it matches a word w ? U, with the constraint
i < k < i+ 6.
r The word bigram candidate [xj,i?1, xi,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
B represents the word bigram dictionary collected from the training data.
r The word bigram candidate [xj,i, xi+1,k] if it hits a word bigram
[wi, wj] ? B, and satisfies the aforementioned constraints on j and k.
All feature templates are instantiated with values that occurred in training samples.
The extracted feature set is large, and there are 2.4? 107 features in total. Our evaluation
is based on a closed test, and we do not use extra resources. Following prior studies, the
evaluation metric for this task is the balanced F-score.
4.3 Phrase Chunking (Structured Classification)
In the phrase chunking task, the non-recursive cores of noun phrases, called base NPs,
are identified. The phrase chunking data is extracted from the data of the CoNLL-2000
shallow-parsing shared task (Sang and Buchholz 2000). The training set consists of 8,936
sentences, and the test set consists of 2,012 sentences. We use the feature templates
based on word n-grams and part-of-speech n-grams, and feature templates are shown
in Table 3. Rich edge features are used. Using the feature templates, we extract 4.8? 105
features in total. State-of-the-art systems for this task include Kudo and Matsumoto
(2001), Collins (2002), McDonald, Crammer, and Pereira (2005), Vishwanathan et al.
(2006), Sun et al. (2008), and Tsuruoka, Tsujii, and Ananiadou (2009). Following prior
studies, the evaluation metric for this task is the balanced F-score.
4.4 Sentiment Classification (Non-Structured Classification)
To demonstrate that the proposed method is not limited to structured classification, we
select a well-known sentiment classification task for evaluating the proposed method
on non-structured classification.
Table 3
Feature templates used for the phrase chunking task. wi, ti, and yi are defined as before.
Word-Token?based Features:
{wi?2, wi?1, wi, wi+1, wi+2, wi?1wi, wiwi+1}
?{yi, yi?1yi}
Part-of-Speech (POS)?based Features:
{ti?1, ti, ti+1, ti?2ti?1, ti?1ti, titi+1, ti+1ti+2, ti?2ti?1ti, ti?1titi+1, titi+1ti+2}
?{yi, yi?1yi}
574
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Generally, sentiment classification classifies user review text as a positive or neg-
ative opinion. This task (Blitzer, Dredze, and Pereira 2007) consists of four subtasks
based on user reviews from Amazon.com. Each subtask is a binary sentiment clas-
sification task based on a specific topic. We use the maximum entropy model for
classification. We use the same lexical features as those used in Blitzer, Dredze, and
Pereira (2007), and the total number of features is 9.4? 105. Following prior work, the
evaluation metric is binary classification accuracy.
4.5 Experimental Setting
As for training, we perform gradient descent with the proposed ADF training method.
To compare with existing literature, we choose four popular training methods, a rep-
resentative batch training method, and three representative on-line training methods.
The batch training method is the limited-memory BFGS (LBFGS) method (Nocedal and
Wright 1999), which is considered to be one of the best optimizers for log-linear models
like CRFs. The on-line training methods include the SGD training method, which we
introduced in Section 2.2, the structured perceptron (Perc) training method (Freund
and Schapire 1999; Collins 2002), and the averaged perceptron (Avg-Perc) training
method (Collins 2002). The structured perceptron method and averaged perceptron
method are non-probabilistic training methods that have very fast training speed due
to the avoidance of the computation on gradients (Sun, Matsuzaki, and Li 2013). All
training methods, including ADF, SGD, Perc, Avg-Perc, and LBFGS, use the same set
of features.
We also compared the ADF method with the CW method (Dredze, Crammer, and
Pereira 2008) and the AROW method (Crammer, Kulesza, and Dredze 2009). The CW
and AROW methods are implemented based on the Confidence Weighted Learning
Library.2 Because the current implementation of the CW and AROW methods do not
utilize rich edge features, we removed the rich edge features in our systems to make
more fair comparisons. That is, we removed rich edge features in the CRF-ADF setting,
and this simplified method is denoted as ADF-noRich. The second-order stochastic
gradient descent training methods, including the SMD method (Vishwanathan et al.
2006) and the PSA method (Hsu et al. 2009), are not considered in our experiments
because we find those methods are quite slow when running on our data sets with high
dimensional features.
We find that the settings of q, ?, and ? in the ADF training method are not sensitive
among specific tasks and can be generally set. We simply set q = n/10 (n is the number
of training samples). It means that feature frequency information is updated 10 times
per iteration. Via cross-validation only on the training data of different tasks, we find
that the following setting is sufficient to produce adequate performance for most of
the real-world natural language processing tasks: ? around 0.995 and ? around 0.6.
This indicates that the feature frequency information has similar characteristics across
many different natural language processing tasks.
Thus, we simply use the following setting for all tasks: q = n/10, ? = 0.995, and
? = 0.6. This leaves c (the initial value of the learning rates) as the only hyper-parameter
that requires careful tuning. We perform automatic tuning for c based on the training
data via 4-fold cross-validation, testing with c = 0.005, 0.01, 0.05, 0.1, respectively, and
the optimal c is chosen based on the best accuracy of cross-validation. Via this automatic
2 http://webee.technion.ac.il/people/koby/code-index.html.
575
Computational Linguistics Volume 40, Number 3
tuning, we find it is proper to set c = 0.005, 0.1, 0.05, 0.005, for the Bio-NER, word
segmentation, phrase chunking, and sentiment classification tasks, respectively.
To reduce overfitting, we use an L2 Gaussian weight prior (Chen and Rosenfeld
1999) for the ADF, LBFGS, and SGD training methods. We vary the ? with different
values (e.g., 1.0, 2.0, and 5.0) for 4-fold cross validation on the training data of different
tasks, and finally set ? = 5.0 for all training methods in the Bio-NER task; ? = 5.0 for
all training methods in the word segmentation task; ? = 5.0, 1.0, 1.0 for ADF, SGD,
and LBFGS in the phrase chunking task; and ? = 1.0 for all training methods in the
sentiment classification task. Experiments are performed on a computer with an Intel(R)
Xeon(R) 2.0-GHz CPU.
4.6 Structured Classification Results
4.6.1 Comparisons Based on Empirical Convergence. First, we check the experimental re-
sults of different methods on their empirical convergence state. Because the perceptron
training method (Perc) does not achieve empirical convergence even with a very large
number of training passes, we simply report its results based on a large enough number
of training passes (e.g., 200 passes). Experimental results are shown in Table 4.
As we can see, the proposed ADF method is more accurate than other training
methods, either the on-line ones or the batch one. It is a bit surprising that the ADF
method performs even more accurately than the batch training method (LBFGS). We
notice that some previous work also found that on-line training methods could have
Table 4
Results for the Bio-NER, word segmentation, and phrase chunking tasks. The results and the
number of passes are decided based on empirical convergence (with score deviation of adjacent
five passes less than 0.01). For the non-convergent case, we simply report the results based on a
large enough number of training passes. As we can see, the ADF method achieves the best
accuracy with the fastest convergence speed.
Bio-NER Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 67.69 70.20 68.92 400 152,811.34
SGD (on-line) 70.91 72.69 71.79 91 76,549.21
Perc (on-line) 65.37 66.95 66.15 200 20,436.69
Avg-Perc (on-line) 68.76 72.56 70.61 37 3,928.01
ADF (proposal) 71.71 72.80 72.25 35 27,490.24
Segmentation Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 97.46 96.86 97.16 102 13,550.68
SGD (on-line) 97.58 97.11 97.34 27 6,811.15
Perc (on-line) 96.99 96.03 96.50 200 8,382.606
Avg-Perc (on-line) 97.56 97.05 97.30 16 716.87
ADF (proposal) 97.67 97.31 97.49 15 4,260.08
Chunking Prec Rec F-score Passes Train-Time (sec)
LBFGS (batch) 94.57 94.09 94.33 105 797.04
SGD (on-line) 94.48 94.04 94.26 56 903.88
Perc (on-line) 93.66 93.31 93.48 200 543.51
Avg-Perc (on-line) 94.34 94.04 94.19 12 33.45
ADF (proposal) 94.66 94.38 94.52 17 282.17
576
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
better performance than batch training methods such as LBFGS (Tsuruoka, Tsujii, and
Ananiadou 2009; Schaul, Zhang, and LeCun 2012). The ADF training method can
achieve better results probably because the feature-frequency?adaptive training schema
can produce more balanced training of features with diversified frequencies. Traditional
SGD training may over-train high frequency features and at the same time may have
insufficient training of low frequency features. The ADF training method can avoid
such problems. It will be interesting to perform further analysis in future work.
We also performed significance tests based on t-tests with a significance level of
0.05. Significance tests demonstrate that the ADF method is significantly more accurate
than the existing training methods in most of the comparisons, whether on-line or
batch. For the Bio-NER task, the differences between ADF and LBFGS, SGD, Perc,
and Avg-Perc are significant. For the word segmentation task, the differences between
ADF and LBFGS, SGD, Perc, and Avg-Perc are significant. For the phrase chunking
task, the differences between ADF and Perc and Avg-Perc are significant; the differences
between ADF and LBFGS and SGD are non-significant.
Moreover, as we can see, the proposed method achieves a convergence state with
the least number of training passes, and with the least wall-clock time. In general, the
ADF method is about one order of magnitude faster than the LBFGS batch training
method and several times faster than the existing on-line training methods.
4.6.2 Comparisons with State-of-the-Art Systems. The three tasks are well-known bench-
mark tasks with standard data sets. There is a large amount of published research on
those three tasks. We compare the proposed method with the state-of-the-art systems.
The comparisons are shown in Table 5.
As we can see, our system is competitive with the best systems for the Bio-NER,
word segmentation, and NP-chunking tasks. Many of the state-of-the-art systems use
extra resources (e.g., linguistic knowledge) or complicated systems (e.g., voting over
Table 5
Comparing our results with some representative state-of-the-art systems.
Bio-NER Method F-score
(Okanohara et al. 2006) Semi-Markov CRF + global features 71.5
(Hsu et al. 2009) CRF + PSA(1) training 69.4
(Tsuruoka, Tsujii, and Ananiadou 2009) CRF + SGD-L1 training 71.6
Our Method CRF + ADF training 72.3
Segmentation Method F-score
(Gao et al. 2007) Semi-Markov CRF 97.2
(Sun, Zhang, et al. 2009) Latent-variable CRF 97.3
(Sun 2010) Multiple segmenters + voting 96.9
Our Method CRF + ADF training 97.5
Chunking Method F-score
(Kudo and Matsumoto 2001) Combination of multiple SVM 94.2
(Vishwanathan et al. 2006) CRF + SMD training 93.6
(Sun et al. 2008) Latent-variable CRF 94.3
Our Method CRF + ADF training 94.5
577
Computational Linguistics Volume 40, Number 3
multiple models). Thus, it is impressive that our single model?based system without
extra resources achieves good performance. This indicates that the proposed ADF
training method can train model parameters with good generality on the test data.
4.6.3 Training Curves. To study the detailed training process and convergence speed, we
show the training curves in Figures 2?4. Figure 2 focuses on the comparisons between
the ADF method and the existing on-line training methods. As we can see, the ADF
method converges faster than other on-line training methods in terms of both training
passes and wall-clock time. The ADF method has roughly the same training speed per
pass compared with traditional SGD training.
Figure 3 (Top Row) focuses on comparing the ADF method with the CW method
(Dredze, Crammer, and Pereira 2008) and the AROW method (Crammer, Kulesza, and
Dredze 2009). Comparisons are based on similar features. As discussed before, the ADF-
noRich method is a simplified system, with rich edge features removed from the CRF-
ADF system. As we can see, the proposed ADF method, whether with or without rich
edge features, outperforms the CWandAROWmethods. Figure 3 (BottomRow) focuses
on the comparisons with different mini-batch (the training samples in each stochastic
update) sizes. Representative results with a mini-batch size of 10 are shown. In general,
we find larger mini-batch sizes will slow down the convergence speed. Results demon-
strate that, compared with the SGD training method, the ADF training method is less
sensitive to mini-batch sizes.
Figure 4 focuses on the comparisons between the ADF method and the batch
training method LBFGS. As we can see, the ADF method converges at least one order
0 20 40 60 80 10066
67
68
69
70
71
72
73
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 20 40 60 80 10092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Number of Passes
)
 
 
ADFSGDPerc
0 1 2 3 4 5 6x 104
66
67
68
69
70
71
72
73
Training Time (sec)
)
 
 
ADFSGDPerc
0 2,000 4,000 6,000 8,000 10,00096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
0 200 400 600 80092.5
93
93.5
94
94.5 Chunking (ADF vs. on-line)
Training Time (sec)
)
 
 
ADFSGDPerc
Figure 2
Comparisons among the ADF method and other on-line training methods. (Top Row)
Comparisons based on training passes. As we can see, the ADF method has the best accuracy
and with the fastest convergence speed based on training passes. (Bottom Row) Comparisons
based on wall-clock time.
578
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
0 20 40 60 80 10060
62
64
66
68
70
72
Bio?NER (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092
93
94
95
96
97
98Segmentation (ADF vs. CW/AROW)
Number of Passes
F?sco
re  (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10092.5
93
93.5
94
94.5
95 Chunking (ADF vs. CW/AROW)
Number of Passes
F?sco
re (%)
 
 
ADFADF?noRichCWAROW
0 20 40 60 80 10068
69
70
71
72
73 Bio?NER (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
0 20 40 60 80 10096.4
96.6
96.8
97
97.2
97.4
97.6
Segmentation (MiniBatch=10)
Number of Passes
F?sco
re  (%)
 
 
ADFSGD
0 20 40 60 80 10093.2
93.4
93.6
93.8
94
94.2
94.4
94.6
Chunking (MiniBatch=10)
Number of Passes
F?sco
re (%)
 
 
ADFSGD
Figure 3
(Top Row) Comparing ADF and ADF-noRich with CW and AROW methods. As we can see,
both the ADF and ADF-noRich methods work better than the CW and AROW methods.
(Bottom Row) Comparing different methods with mini-batch = 10 in the stochastic learning
setting.
magnitude faster than the LBFGS training in terms of both training passes and wall-
clock time. For the LBFGS training, we need to determine the LBFGSmemory parameter
m, which controls the number of prior gradients used to approximate the Hessian
information. A larger value of m will potentially lead to more accurate estimation
of the Hessian information, but at the same time will consume significantly more
memory. Roughly, the LBFGS training consumes m times more memory than the ADF
on-line training method. For most tasks, the default setting of m = 10 is reasonable. We
set m = 10 for the word segmentation and phrase chunking tasks, and m = 6 for the
Bio-NER task due to the shortage of memory for m > 6 cases in this task.
4.6.4 One-Pass Learning Results. Many real-world data sets can only observe the training
data in one pass. For example, some Web-based on-line data streams can only appear
once so that the model parameter learning should be finished in one-pass learning (see
Zinkevich et al. 2010). Hence, it is important to test the performance in the one-pass
learning scenario.
In the one-pass learning scenario, the feature frequency information is computed
?on the fly? during on-line training. As shown in Section 3.1, we only need to have
a real-valued vector v to record the cumulative feature frequency information, which
is updated when observing training instances one by one. Then, the learning rate
vector ? is updated based on the v only and there is no need to observe the training
instances again. This is the same algorithm introduced in Section 3.1 and no change is
required for the one-pass learning scenario. Figure 5 shows the comparisons between
the ADF method and baselines on one-pass learning. As we can see, the ADF method
579
Computational Linguistics Volume 40, Number 3
0 100 200 300 40066
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 100 200 300 40096
96.296.4
96.696.8
9797.2
97.497.6
Segmentation (ADF vs. batch)
Number of Passes
F?sco
re  (%)
 
 
ADFLBFGS
0 100 200 300 400
93.6
93.8
94
94.2
94.4
Chunking (ADF vs. batch)
Number of Passes
F?sco
re (%)
 
 
ADFLBFGS
0 5 10 15x 104
66
67
68
69
70
71
72
73 Bio?NER (ADF vs. batch)
Training Time (sec)
F?sco
re (%)
 
 
ADFLBFGS
0 1 2 3 4 5x 104
9696.2
96.496.6
96.897
97.297.4
97.6
Segmentation (ADF vs. batch)
Training Time (sec)
F?sco
re  (%)
 
 
ADFLBFGS
0 1,000 2,000 3,00092.5
93
93.5
94
94.5 Chunking (ADF vs. batch)
Training Time (sec)
)
 
 
ADFLBFGS
Figure 4
Comparisons between the ADF method and the batch training method LBFGS. (Top Row)
Comparisons based on training passes. As we can see, the ADF method converges much faster
than the LBFGS method, and with better accuracy on the convergence state. (Bottom Row)
Comparisons based on wall-clock time.
consistently outperforms the baselines. This also reflects the fast convergence speed of
the ADF training method.
4.7 Non-Structured Classification Results
In previous experiments, we showed that the proposed method outperforms existing
baselines on structured classification. Nevertheless, we want to show that the ADF
method also has good performance on non-structured classification. In addition, this
task is based on real-valued features instead of binary features.
Figure 5
Comparisons among different methods based on one-pass learning. As we can see, the ADF
method has the best accuracy on one-pass learning.
580
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Table 6
Results on sentiment classification (non-structured binary classification).
Accuracy Passes Train-Time (sec)
LBFGS (batch) 87.00 86 72.20
SGD (on-line) 87.13 44 55.88
Perc (on-line) 84.55 25 5.82
Avg-Perc (on-line) 85.04 46 12.22
ADF (proposal) 87.89 30 57.12
Experimental results of different training methods on the convergence state are
shown in Table 6. As we can see, the proposed method outperforms all of the on-line
and batch baselines in terms of binary classification accuracy. Here again we observe
that the ADF and SGD methods outperform the LBFGS baseline.
The training curves are shown in Figure 6. As we can see, the ADF method con-
verges quickly. Because this data set is relatively small and the feature dimension is
much smaller than previous tasks, we find the baseline training methods also have
fast convergence speed. The comparisons on one-pass learning are shown in Fig-
ure 7. Just as for the experiments for structured classification tasks, the ADF method
0 20 40 60 80 10082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Number of Passes
Accur
acy (%
)
 
 
ADFSGDPerc
0 20 40 60 8082
83
84
85
86
87
88
89 Sentiment (ADF vs. on-line)
Training Time (sec)
Accur
acy (%
)
 
 
ADFSGDPerc
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Number of Passes
Accur
acy (%
)
 
 
ADFLBFGS
0 50 100 15082
83
84
85
86
87
88
89 Sentiment (ADF vs. batch)
Training Time (sec)
Accu
racy (%
)
 
 
ADFLBFGS
Figure 6
F-score curves on sentiment classification. (Top Row) Comparisons among the ADF method and
on-line training baselines, based on training passes and wall-clock time, respectively. (Bottom
Row) Comparisons between the ADF method and the batch training method LBFGS, based on
training passes and wall-clock time, respectively. As we can see, the ADF method outperforms
both the on-line training baselines and the batch training baseline, with better accuracy and
faster convergence speed.
581
Computational Linguistics Volume 40, Number 3
Figure 7
One-pass learning results on sentiment classification.
outperforms the baseline methods on one-pass learning, with more than 12.7% error
rate reduction.
5. Proofs
This section gives proofs of Theorems 1?4.
Proof of Theorem 1 Following Equation (5), the ADF update rule is F(wt) := wt+1 =
wt +? ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |F(wt+1)?wt+1|
= |wt+1 +? ? gt+1 ?wt+1|
= |? ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |?maxgt+1|
= |FSGD(wt+1)? FSGD(wt)|
(6)
where ai and bi are the ith elements of the vector ? and gt+1, respectively. FSGD is the
SGD update rule with the fixed learning rate ?max such that ?max := sup{?i where ?i ?
?}. In other words, for the SGD update rule FSGD, the fixed learning rate ?max is derived
from the ADF update rule. According to Lemma 1, the SGD update rule FSGD is a
contraction mapping in Euclidean space with Lipschitz continuity degree 1? ?max/?2,
given the condition that ?max ? (||x2i || ? ||?y? `(xi, yi, y?)||Lip)?1. Hence, it goes to
|FSGD(wt+1)? FSGD(wt)| ? (1? ?max/?2)|wt+1 ?wt| (7)
Combining Equations (6) and (7), it goes to
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
Thus, according to the definition of dynamic contraction mapping, the ADF update rule
is a dynamic contraction mapping in Euclidean space with Lipschitz continuity degree
1? ?max/?2. ut
582
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
Proof of Theorem 2 As presented in Equation (5), the ADF update rule is F(wt) :=
wt+1 = wt +?t ? gt. For ?wt ? X ,
|F(wt+1)? F(wt)|
= |?t+1 ? gt+1|
= [(a1b1)2 + (a2b2)2 + ? ? ?+ (af bf )2]1/2
? [(?maxb1)2 + (?maxb2)2 + ? ? ?+ (?maxbf )2]1/2
= |FSGD(wt+1)? FSGD(wt)|
(8)
where ai is the ith element of the vector ?t+1. bi and FSGD are the same as before.
Similar to the analysis of Theorem 1, the third step of Equation (8) is valid because ?max
is the maximum learning rate at the beginning and all learning rates are decreasing
when t is increasing. The proof can be easily derived following the same steps in the
proof of Theorem 1. To avoid redundancy, we do not repeat the derivation. ut
Proof of Theorem 3 Let M be the accumulative change of the ADF weight vectorwt:
Mt :=
?
t?=1,2,...,t
|wt?+1 ?wt? |
To prove the convergence of the ADF, we need to prove the sequence Mt converges as
t??. Following Theorem 2, we have the following formula for the ADF training:
|F(wt+1)? F(wt)| ? (1? ?max/?2)|wt+1 ?wt|
where ?max is the maximum learning rate at the beginning. Let d0 := |w2 ?w1| and
q := 1? ?max/?2, then we have:
Mt =
?
t?=1,2,...,t
|wt?+1 ?wt? |
? d0 + d0q+ d0q2 + ? ? ?+ d0qt?1
= d0(1? qt)/(1? q)
(9)
When t??, d0(1? qt)/(1? q) goes to d0/(1? q) because q < 1. Hence, we have:
Mt ? d0/(1? q)
Thus, Mt is upper-bounded. Because we know that Mt is a monotonically increasing
function when t??, it follows that Mt converges when t??. This completes the
proof. ut
583
Computational Linguistics Volume 40, Number 3
Proof of Theorem 4 First, we have
eigen(Ct) =
t
?
m=1
(1??0?m?)
? exp
{
??0?
t
?
m=1
?m
}
Then, we have
0 ?
n
?
j=1
(1? aj) ?
n
?
j=1
e?aj = e?
?n
j=1 aj
This is because 1? aj ? e?aj given 0 ? aj < 1. Finally, because
?t
m=1 ?m ?
?
1?? when
t??, we have
eigen(Ct) ? exp
{
??0?
t
?
m=1
?m
}
? exp
{
??0??
1? ?
}
This completes the proof. ut
6. Conclusions
In this work we tried to simultaneously improve the training speed andmodel accuracy
of natural language processing systems. We proposed the ADF on-line training method,
based on the core idea that high frequency features should result in a learning rate that
decays faster. We demonstrated that the ADF on-line training method is convergent
and has good theoretical properties. Based on empirical experiments, we can state the
following conclusions. First, the ADF method achieved the major target of this work:
faster training speed and higher accuracy at the same time. Second, the ADF method
was robust: It had good performance on several structured and non-structured classifi-
cation tasks with very different characteristics. Third, the ADF method worked well on
both binary features and real-valued features. Fourth, the ADF method outperformed
existing methods in a one-pass learning setting. Finally, our method achieved state-
of-the-art performance on several well-known benchmark tasks. To the best of our
knowledge, our simple method achieved a much better F-score than the existing best
reports on the biomedical named entity recognition task.
Acknowledgments
This work was supported by the National
Natural Science Foundation of China
(no. 61300063, no. 61370117), the Doctoral
Fund of Ministry of Education of China
(no. 20130001120004), a Hong Kong
Polytechnic University internal grant
(4-ZZD5), a Hong Kong RGC Project
(no. PolyU 5230/08E), the National High
Technology Research and Development
Program of China (863 Program,
no. 2012AA011101), and the Major National
Social Science Fund of China (no. 12&ZD227).
This work is a substantial extension of the
conference version presented at ACL 2012
(Sun, Wang, and Li 2012).
584
Sun et al. Feature-Frequency?Adaptive On-line Training for Natural Language Processing
References
Berger, Adam L., Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39?71.
Blitzer, John, Mark Dredze, and Fernando
Pereira. 2007. Biographies, Bollywood,
boom-boxes and blenders: Domain
adaptation for sentiment classification.
In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics,
pages 440?447, Prague.
Bottou, Le?on. 1998. Online algorithms
and stochastic approximations. In
D. Saad, editor. Online Learning and Neural
Networks. Cambridge University Press,
pages 9?42.
Chen, Stanley F. and Ronald Rosenfeld.
1999. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMU-CS-99-108, Carnegie
Mellon University.
Collins, Michael. 2002. Discriminative
training methods for hidden Markov
models: Theory and experiments with
perceptron algorithms. In Proceedings of
EMNLP?02, pages 1?8, Philadelphia, PA.
Crammer, Koby, Alex Kulesza, and Mark
Dredze. 2009. Adaptive regularization of
weight vectors. In NIPS?09, pages 414?422,
Vancouver.
Dredze, Mark, Koby Crammer, and
Fernando Pereira. 2008. Confidence-
weighted linear classification. In
Proceedings of ICML?08, pages 264?271,
Helsinki.
Duchi, John, Elad Hazan, and Yoram Singer.
2010. Adaptive subgradient methods
for online learning and stochastic
optimization. Journal of Machine
Learning Research, 12:2,121?2,159.
Finkel, Jenny, Shipra Dingare, Huy Nguyen,
Malvina Nissim, Christopher Manning,
and Gail Sinclair. 2004. Exploiting context
for biomedical entity recognition: From
syntax to the Web. In Proceedings of
BioNLP?04, pages 91?94, Geneva.
Freund, Yoav and Robert Schapire. 1999.
Large margin classification using the
perceptron algorithm. Machine Learning,
37(3):277?296.
Gao, Jianfeng, Galen Andrew, Mark Johnson,
and Kristina Toutanova. 2007. A comparative
study of parameter estimation methods for
statistical natural language processing. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics
(ACL?07), pages 824?831, Prague.
Hsu, Chun-Nan, Han-Shen Huang, Yu-Ming
Chang, and Yuh-Jye Lee. 2009. Periodic
step-size adaptation in second-order
gradient descent for single-pass on-line
structured learning. Machine Learning,
77(2-3):195?224.
Jacobs, Robert A. 1988. Increased rates of
convergence through learning rate
adaptation. Neural Networks, 1(4):295?307.
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa
Tsuruoka, and Yuka Tateisi. 2004.
Introduction to the bio-entity recognition
task at JNLPBA. In Proceedings of
BioNLP?04, pages 70?75, Geneva.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of NAACL?01, pages 1?8,
Pittsburgh, PA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence
data. In ICML?01, pages 282?289,
Williamstown, MA.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Flexible text
segmentation with structured multilabel
classification. In Proceedings of HLT/
EMNLP?05, pages 987?994, Vancouver.
McMahan, H. Brendan and Matthew J.
Streeter. 2010. Adaptive bound
optimization for online convex
optimization. In Proceedings of COLT?10,
pages 244?256, Haifa.
Murata, Noboru. 1998. A statistical study
of on-line learning. In D. Saad, editor.
Online Learning in Neural Networks.
Cambridge University Press, pages 63?92.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical optimization. Springer.
Okanohara, Daisuke, Yusuke Miyao,
Yoshimasa Tsuruoka, and Jun?ichi Tsujii.
2006. Improving the scalability of
semi-Markov conditional random
fields for named entity recognition.
In Proceedings of COLING-ACL?06,
pages 465?472, Sydney.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech tagging.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing
1996, pages 133?142, Pennsylvania.
Sang, Erik Tjong Kim and Sabine Buchholz.
2000. Introduction to the CoNLL-2000
shared task: Chunking. In Proceedings
of CoNLL?00, pages 127?132, Lisbon.
Schaul, Tom, Sixin Zhang, and Yann LeCun.
2012. No more pesky learning rates. CoRR,
abs/1206.1106.
585
Computational Linguistics Volume 40, Number 3
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random
fields and rich feature sets. In Proceedings
of BioNLP?04, pages 104?107, Geneva.
Shalev-Shwartz, Shai, Yoram Singer, and
Nathan Srebro. 2007. Pegasos: Primal
estimated sub-gradient solver for SVM.
In Proceedings of ICML?07, pages 807?814,
Corvallis, OR.
Sperduti, Alessandro and Antonina Starita.
1993. Speed up learning and network
optimization with extended back
propagation. Neural Networks, 6(3):365?383.
Sun, Weiwei. 2010. Word-based and
character-based word segmentation
models: Comparison and combination.
In COLING?10 (Posters), pages 1,211?1,219,
Beijing.
Sun, Xu, Takuya Matsuzaki, and Wenjie Li.
2013. Latent structured perceptrons
for large-scale learning with hidden
information. IEEE Transactions on
Knowledge and Data Engineering,
25(9):2,063?2,075.
Sun, Xu, Takuya Matsuzaki, Daisuke
Okanohara, and Jun?ichi Tsujii. 2009.
Latent variable perceptron algorithm for
structured classification. In Proceedings
of the 21st International Joint Conference
on Artificial Intelligence (IJCAI 2009),
pages 1,236?1,242, Pasadena, CA.
Sun, Xu, Louis-Philippe Morency, Daisuke
Okanohara, and Jun?ichi Tsujii. 2008.
Modeling latent-dynamic in shallow
parsing: A latent conditional model with
improved inference. In Proceedings of
COLING?08, pages 841?848, Manchester.
Sun, Xu, Houfeng Wang, and Wenjie Li. 2012.
Fast online training with frequency-
adaptive learning rates for Chinese word
segmentation and new word detection.
In Proceedings of ACL?12, pages 253?262,
Jeju Island.
Sun, Xu, Yaozhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2009. A discriminative
latent variable Chinese segmenter with
hybrid word/character information.
In Proceedings of NAACL-HLT?09,
pages 56?64, Boulder, CO.
Sun, Xu, Yao Zhong Zhang, Takuya
Matsuzaki, Yoshimasa Tsuruoka, and
Jun?ichi Tsujii. 2013. Probabilistic Chinese
word segmentation with non-local
information and stochastic training.
Information Processing & Management,
49(3):626?636.
Tseng, Huihsin, Pichuan Chang, Galen
Andrew, Daniel Jurafsky, and Christopher
Manning. 2005. A conditional random
field word segmenter for SIGHAN bakeoff
2005. In Proceedings of the Fourth SIGHAN
Workshop, pages 168?171, Jeju Island.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and
Sophia Ananiadou. 2009. Stochastic
gradient descent training for
l1-regularized log-linear models with
cumulative penalty. In Proceedings of
ACL?09, pages 477?485, Suntec.
Vishwanathan, S. V. N., Nicol N.
Schraudolph, Mark W. Schmidt, and
Kevin P. Murphy. 2006. Accelerated
training of conditional random
fields with stochastic meta-descent.
In Proceedings of ICML?06, pages 969?976,
Pittsburgh, PA.
Zhang, Ruiqiang, Genichiro Kikui, and
Eiichiro Sumita. 2006. Subword-based
tagging by conditional random fields for
Chinese word segmentation. In Proceedings
of the Human Language Technology
Conference of the NAACL, Companion
Volume: Short Papers, pages 193?196,
New York City.
Zhang, Yue and Stephen Clark. 2007.
Chinese segmentation with a word-based
perceptron algorithm. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 840?847,
Prague.
Zhao, Hai, Changning Huang, Mu Li,
and Bao-Liang Lu. 2010. A unified
character-based tagging framework for
Chinese word segmentation. ACM
Transactions on Asian Language Information
Processing, 9(2): Article 5.
Zhao, Hai and Chunyu Kit. 2011. Integrating
unsupervised and supervised word
segmentation: The role of goodness
measures. Information Sciences,
181(1):163?183.
Zinkevich, Martin, Markus Weimer,
Alexander J. Smola, and Lihong Li.
2010. Parallelized stochastic gradient
descent. In Proceedings of NIPS?10,
pages 2,595?2,603, Vancouver.
586
Proceedings of the ACL 2010 Conference Short Papers, pages 296?300,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Key Phrase Extraction Approach: Learning from 
Title Phrases through a Document Semantic Network 
 
Decong Li1, Sujian Li1, Wenjie Li2, Wei Wang1, Weiguang Qu3 
1Key Laboratory of Computational Linguistics, Peking University  
2Department of Computing, The Hong Kong Polytechnic University 
3 School of Computer Science and Technology, Nanjing Normal University 
{lidecong,lisujian, wwei }@pku.edu.cn   cswjli@comp.polyu.edu.hk  wgqu@njnu.edu.cn 
  
  
Abstract 
It is a fundamental and important task to ex-
tract key phrases from documents. Generally, 
phrases in a document are not independent in 
delivering the content of the document. In or-
der to capture and make better use of their re-
lationships in key phrase extraction, we sug-
gest exploring the Wikipedia knowledge to 
model a document as a semantic network, 
where both n-ary and binary relationships 
among phrases are formulated. Based on a 
commonly accepted assumption that the title 
of a document is always elaborated to reflect 
the content of a document and consequently 
key phrases tend to have close semantics to the 
title, we propose a novel semi-supervised key 
phrase extraction approach in this paper by 
computing the phrase importance in the se-
mantic network, through which the influence 
of title phrases is propagated to the other 
phrases iteratively. Experimental results dem-
onstrate the remarkable performance of this 
approach. 
1 Introduction 
Key phrases are defined as the phrases that ex-
press the main content of a document. Guided by 
the given key phrases, people can easily under-
stand what a document describes, saving a great 
amount of time reading the whole text. Conse-
quently, automatic key phrase extraction is in 
high demand. Meanwhile, it is also fundamental 
to many other natural language processing appli-
cations, such as information retrieval, text clus-
tering and so on.  
Key phrase extraction can be normally cast as 
a ranking problem solved by either supervised or 
unsupervised methods. Supervised learning re-
quires a large amount of expensive training data, 
whereas unsupervised learning totally ignores 
human knowledge. To overcome the deficiencies 
of these two kinds of methods, we propose a 
novel semi-supervised key phrase extraction ap-
proach in this paper, which explores title phrases 
as the source of knowledge.  
It is well agreed that the title has a similar role 
to the key phrases. They are both elaborated to 
reflect the content of a document. Therefore, 
phrases in the titles are often appropriate to be 
key phrases. That is why position has been a 
quite effective feature in the feature-based key 
phrase extraction methods (Witten, 1999), i.e., if 
a phrase is located in the title, it is ranked higher.  
However, one can only include a couple of 
most important phrases in the title prudently due 
to the limitation of the title length, even though 
many other key phrases are all pivotal to the un-
derstanding of the document. For example, when 
we read the title ?China Tightens Grip on the 
Web?, we can only have a glimpse of what the 
document says. On the other hand, the key 
phrases, such as ?China?, ?Censorship?, ?Web?, 
?Domain name?, ?Internet?, and ?CNNIC?, etc. 
can tell more details about the main topics of the 
document. In this regard, title phrases are often 
good key phrases but they are far from enough. 
If we review the above example again, we will 
find that the key phrase ?Internet? can be in-
ferred from the title phrase ?Web?. As a matter 
of fact, key phrases often have close semantics to 
title phrases. Then a question comes to our minds: 
can we make use of these title phrases to infer 
the other key phrases?  
To provide a foundation of inference, a seman-
tic network that captures the relationships among 
phrases is required. In the previous works (Tur-
dakov and Velikhov, 2008), semantic networks 
are constructed based on the binary relations, and 
the semantic relatedness between a pair of phras-
es is formulated by the weighted edges that con-
nects them. The deficiency of these approaches is 
the incapability to capture the n-ary relations 
among multiple phrases. For example, a group of 
296
phrases may collectively describe an entity or an 
event.  
In this study, we propose to model a semantic 
network as a hyper-graph, where vertices 
represent phrases and weighted hyper-edges 
measure the semantic relatedness of both binary 
relations and n-ary relations among phrases. We 
explore a universal knowledge base ? Wikipedia 
? to compute the semantic relatedness. Yet our 
major contribution is to develop a novel semi-
supervised key phrase extraction approach by 
computing the phrase importance in the semantic 
network, through which the influence of title 
phrases is propagated to the other phrases itera-
tively.  
The goal of the semi-supervised learning is to 
design a function that is sufficiently smooth with 
respect to the intrinsic structure revealed by title 
phrases and other phrases. Based on the assump-
tion that semantically related phrases are likely 
to have similar scores, the function to be esti-
mated is required to assign title phrases a higher 
score and meanwhile locally smooth on the con-
structed hyper-graph. Zhou et al?s work (Zhou 
2005) lays down a foundation for our semi-
supervised phrase ranking algorithm introduced 
in Section 3. Experimental results presented in 
Section 4 demonstrate the effectiveness of this 
approach. 
2 Wikipedia-based Semantic Network 
Construction  
Wikipedia1 is a free online encyclopedia, which 
has unarguably become the world?s largest col-
lection of encyclopedic knowledge. Articles are 
the basic entries in the Wikipedia, with each ar-
ticle explaining one Wikipedia term. Articles 
contain links pointing from one article to another. 
Currently, there are over 3 million articles and 90 
million links in English Wikipedia. In addition to 
providing a large vocabulary, Wikipedia articles 
also contain a rich body of lexical semantic in-
formation expressed via the extensive number of 
links. During recent years, Wikipedia has been 
used as a powerful tool to compute semantic re-
latedness between terms in a good few of works 
(Turdakov 2008).   
We consider a document composed of the 
phrases that describe various aspects of entities 
or events with different semantic relationships. 
We then model a document as a semantic net-
work formulated by a weighted hyper-graph 
                                               
1 www.wikipedia.org 
 
G=(V, E, W), where each vertex vi?V (1?i?n) 
represents a phrase, each hyper-edge ej?E 
(1?j?m) is a subset of V, representing binary re-
lations or n-ary relations among phrases, and the 
weight w(ej) measures the semantic relatedness 
of ej.  
By applying the WSD technique proposed by 
(Turdakov and Velikhov, 2008), each phrase is 
assigned with a single Wikipedia article that de-
scribes its meaning. Intuitively, if the fraction of 
the links that the two articles have in common to 
the total number of the links in both articles is 
high, the two phrases corresponding to the two 
articles are more semantically related. Also, an 
article contains different types of links, which are 
relevant to the computation of semantic related-
ness to different extent. Hence we adopt the 
weighted Dice metric proposed by (Turdakov 
2008) to compute the semantic relatedness of 
each binary relation, resulting in the edge weight  
w(eij), where eij is an edge connecting the phrases 
vi and vj. 
To define the n-ary relations in the semantic 
network, a proper graph clustering technique is 
needed. We adopt the weighted Girvan-Newman 
algorithm (Newman 2004) to cluster phrases (in-
cluding title phrases) by computing their bet-
weenness centrality. The advantage of this algo-
rithm is that it need not specify a pre-defined 
number of clusters. Then the phrases, within 
each cluster, are connected by a n-ary relation. n-
ary relations among the phrases in the same clus-
ter are then measured based on binary relations. 
The weight of a hyper-edge e is defined as: 
( ) ( )| | ij ije e
w e w ee
?
?
? ?
  (1) 
where |e| is the number of the vertices in e, eij is 
an edge with two vertices included in e and ? ? 0 
is a parameter balancing the relative importance 
of n-ary hyper-edges compared with binary ones.  
3 Semi-supervised Learning from Title 
Given the document semantic network 
represented as a phrase hyper-graph, one way to 
make better use of the semantic information is to 
rank phrases with a semi-supervised learning 
strategy, where the title phrases are regarded as 
labeled samples, while the other phrases as unla-
beled ones. That is, the information we have at 
the beginning about how to rank phrases is that 
the title phrases are the most important phrases. 
Initially, the title phrases are assigned with a pos-
itive score of 1 indicating its importance and oth-
297
er phrases are assigned zero. Then the impor-
tance scores of the phrases are learned iteratively 
from the title phrases through the hyper-graph. 
The key idea behind hyper-graph based semi-
supervised ranking is that the vertices which 
usually belong to the same hyper-edges should 
be assigned with similar scores. Then, we have 
the following two constraints: 
1. The phrases which have many incident hy-
per-edges in common should be assigned similar 
scores. 
2. The given initial scores of the title phrases 
should be changed as little as possible. 
Given a weighted hyper-graph G, assume a 
ranking function f over V, which assigns each 
vertex v an importance score f(v). f can be 
thought as a vector in Euclid space R|V|. For the 
convenience of computation, we use an inci-
dence matrix H to represent the hypergraph, de-
fined as: 
0, if ( , ) 1, if 
v eh v e v e
??? ? ??
   (2) 
Based on the incidence matrix, we define the 
degrees of the vertex v and the hyper-edge e as 
   (3) 
and 
   (4) 
Then, to formulate the above-mentioned con-
straints, let  denote the initial score vector, then 
the importance scores of the phrases are learned 
iteratively by solving the following optimization 
problem: 
| |
2arg min { ( ) }Vf R f f y?? ? ? ?
 (5) 
2
{ , }
1 1 ( ) ( )( ) ( )2 ( ) ( ) ( )e E u v e
f u f vf w ee d u d v?? ?
? ?? ? ?? ?
? ?? ?
 (6) 
where ?> 0 is the parameter specifying the 
tradeoff between the two competitive items. Let 
Dv and De denote the diagonal matrices contain-
ing the vertex and the hyper-edge degrees re-
spectively, W denote the diagonal matrix con-
taining the hyper-edge weights, f* denote the so-
lution of (6).  Zhou has given the solution (Zhou, 
2005) as. 
* * (1 )f f y? ?? ? ? ?   (7) 
where 
1/2 1 1/2Tv e vD HWD H D? ? ?? ?  and 1/ ( 1)? ?? ? . 
Using an approximation algorithm (e.g. Algo-
rithm 1), we can finally get a vector f 
representing the approximate phrase scores. 
Algorithm 1: PhraseRank(V, T, a, b) 
Input: Title phrase set = {v1,v2,?,vt},the set of other 
phrases ={vt+1,vt+2,?,vn}, parameters ? and ?, con-
vergence threshold ? 
Output: The approximate phrase scores f  
Construct a document semantic network for all the 
phrases {v1,v2,?,vn} using the method described  in 
section 2. 
Let 
1/2 1 1/2Tv e vD HWD H D? ? ? ?? ;  
Initialize the score vector y as 1,1iy i t? ? ? , and  
0,jy t j n? ? ?
; 
Let , k = 0; 
REPEAT  
1 (1 )k kf f y?? ?? ? ? ?; 
, ; 
; 
UNTIL  
END 
Finally we rank phrases in descending order of 
the calculated importance scores and select those 
highest ranked phrases as key phrases. Accord-
ing to the number of all the candidate phrases, 
we choose an appropriate proportion, i.e. 10%, of 
all the phrases as key phrases. 
4 Evaluation 
4.1 Experiment Set-up  
We first collect all the Wikipedia terms to com-
pose of a dictionary. The word sequences that 
occur in the dictionary are identified as phrases. 
Here we use a finite-state automaton to accom-
plish this task to avoid the imprecision of pre-
processing by POS tagging or chunking. Then, 
we adopt the WSD technique proposed by (Tur-
dakov and Velikhov 2008) to find the corres-
ponding Wikipedia article for each phrase. As 
mentioned in Section 2, a document semantic 
network in the form of a hyper-graph is con-
structed, on which Algorithm 1 is applied to rank 
the phrases.  
To evaluate our proposed approach, we select 
200 pieces of news from well-known English 
media. 5 to 10 key phrases are manually labeled 
in each news document and the average number 
of the key phrases is 7.2 per document. Due to 
the abbreviation and synonymy phenomena, we 
construct a thesaurus and convert all manual and 
automatic phrases into their canonical forms 
when evaluated. The traditional Recall, Precision 
and F1-measure metrics are adopted for evalua-
tion. This section conducts two sets of experi-
ment: (1) to examine the influence of two para-
meters: ? and ?, on the key phrase extraction 
performance; (2) to compare with other well 
known state-of-art key phrase extraction ap-
proaches. 
298
4.2 Parameter tuning  
The approach involves two parameters: ? (??0) 
is a relation factor balancing the influence of n-
ary relations and binary relations; ? (0???1) is a 
learning factor tuning the influence from the title 
phrases. It is hard to find a global optimized so-
lution for the combination of these two factors. 
So we apply a gradient search strategy. At first, 
the learning factor is set to ?=0.8. Different val-
ues of ? ranging from 0 to 3 are examined. Then, 
given that ? is set to the value with the best per-
formance, we conduct experiments to find an 
appropriate value for ?. 
4.2.1 ?: Relation Factor 
First, we fix the learning factor ? as 0.8 random-
ly and evaluate the performance by varying ? 
value from 0 to 3. When ?=0, it means that the 
weight of n-ary relations is zero and only binary 
relations are considered. As we can see from 
Figure 1, the performance is improved in most 
cases in terms of F1-measure and reaches a peak 
at ?=1.8. This justifies the rational to incorpo-
rate n-ary relations with binary relations in the 
document semantic network. 
 
Figure 1. F1-measures with ? in [0 3] 
4.2.2 ?: Learning factor  
Next, we set the relation factor ?=1.8, we in-
spect the performance with the learning factor ? 
ranging from 0 to 1. ?=1 means that the ranking 
scores learn from the semantic network without 
any consideration of title phrases. As shown in 
Figure 2, we find that the performance almost 
keep a smooth fluctuation as ? increases from 0 
to 0.9, and then a diving when ?=1. This proves 
that title phrases indeed provide valuable infor-
mation for learning.  
 
Figure 2. F1-measure with ? in [0,1] 
4.3 Comparison with Other Approaches  
Our approach aims at inferring important key 
phrases from title phrases through a semantic 
network. Here we take a method of synonym 
expansion as the baseline, called WordNet ex-
pansion here. The WordNet2 expansion approach 
selects all the synonyms of the title phrases in the 
document as key phrases. Afterwards, our ap-
proach is evaluated against two existing ap-
proaches, which rely on the conventional seman-
tic network and are able to capture binary rela-
tions only. One approach combines the title in-
formation into the Grineva?s community-based 
method (Grineva et al, 2009), called title-
community approach. The title-community ap-
proach uses the Girvan-Newman algorithm to 
cluster phrases into communities and selects 
those phrases in the communities containing the 
title phrases as key phrases. We do not limit the 
number of key phrases selected. The other one is 
based on topic-sensitive LexRank (Otterbacher et 
al., 2005), called title-sensitive PageRank here. 
The title-sensitive PageRank approach makes use 
of title phrases to re-weight the transitions be-
tween vertices and picks up 10% top-ranked 
phrases as key phrases.  
Approach Precision Recall F1 
Title-sensitive Pa-
geRank (d=0.15) 
34.8% 39.5% 37.0% 
Title-community 29.8% 56.9% 39.1% 
Our approach 
(?=1.8, ?=0.5) 
39.4% 44.6% 41.8% 
WordNet expansion 
(baseline) 
7.9%  32.9% 12.5% 
Table 1. Comparison with other approaches 
Table 1 summarizes the performance on the 
test data. The results presented in the table show 
that our approach exhibits the best performance 
among all the four approaches. It follows that the 
key phrases inferred from a document semantic 
network are not limited to the synonyms of title 
phrases. As the title-sensitive PageRank ap-
                                               
2 http://wordnet.princeton.edu 
299
proach totally ignores the n-ary relations, its per-
formance is the worst. Based on binary relations, 
the title-community approach clusters phrases 
into communities and each community can be 
considered as an n-ary relation. However, this 
approach lacks of an importance propagation 
process. Consequently, it has the highest recall 
value but the lowest precision. In contrast, our 
approach achieves the highest precision, due to 
its ability to infer many correct key phrases using 
importance propagation among n-ary relations.  
5 Conclusion  
This work is based on the belief that key phrases 
tend to have close semantics to the title phrases. 
In order to make better use of phrase relations in 
key phrase extraction, we explore the Wikipedia 
knowledge to model one document as a semantic 
network in the form of hyper-graph, through 
which the other phrases learned their importance 
scores from the title phrases iteratively. Experi-
mental results demonstrate the effectiveness and 
robustness of our approach. 
 
Acknowledgments 
The work described in this paper was partially 
supported by NSFC programs (No: 60773173, 
60875042 and 90920011), and Hong Kong RGC 
Projects (No: PolyU5217/07E).  We thank the 
anonymous reviewers for their insightful com-
ments. 
References  
David Milne, Ian H. Witten. 2008. An Effective, 
Low-Cost Measure of Semantic Relatedness 
Obtained from Wikipedia Links. In Wikipedia 
and AI workshop at the AAAI-08 Conference, 
Chicago, US. 
Dengyong Zhou, Jiayuan Huang and Bernhard 
Sch?lkopf. 2005. Beyond Pairwise Classifica-
tion and Clustering Using Hypergraphs. MPI 
Technical Report, T?bingen, Germany. 
Denis Turdakov and Pavel Velikhov. 2008. Semantic 
relatedness metric for wikipedia concepts 
based on link analysis and its application to 
word sense disambiguation. In Colloquium on 
Databases and Information Systems (SYRCoDIS). 
Ian H. Witten, Gordon W. Paynter, Eibe Frank , Carl 
Gutwin , Craig G. Nevill-Manning. 1999.  KEA: 
practical automatic keyphrase extraction, In 
Proceedings of the fourth ACM conference on Dig-
ital libraries, pp.254-255, California, USA. 
Jahna Otterbacher, Gunes Erkan and Dragomir R. 
Radev. 2005. Using Random Walks for Ques-
tion-focused Sentence Retrieval. In Proceedings 
of HLT/EMNLP 2005, pp. 915-922, Vancouver, 
Canada. 
Maria Grineva, Maxim Grinev and Dmitry Lizorkin. 
2009. Extracting key terms from noisy and 
multitheme documents, In Proceedings of the 
18th international conference on World wide web, 
pp. 661-670, Madrid, Spain.  
Michael Strube and Simone Paolo Ponzetto. 
2006.WikiRelate! Computing Semantic Rela-
tedness using Wikipedia. In Proceedings of the 
21st National Conference on Artificial Intelligence, 
pp. 1419?1424, Boston, MA. 
M. E. J. Newman. 2004. Analysis of Weighted Net-
works.  Physical Review E 70, 056131. 
 
300
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253?262,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Fast Online Training with Frequency-Adaptive Learning Rates for Chinese
Word Segmentation and New Word Detection
Xu Sun?, Houfeng Wang?, Wenjie Li?
?Department of Computing, The Hong Kong Polytechnic University
?Key Laboratory of Computational Linguistics (Peking University), Ministry of Education, China
{csxsun, cswjli}@comp.polyu.edu.hk wanghf@pku.edu.cn
Abstract
We present a joint model for Chinese
word segmentation and new word detection.
We present high dimensional new features,
including word-based features and enriched
edge (label-transition) features, for the joint
modeling. As we know, training a word
segmentation system on large-scale datasets
is already costly. In our case, adding high
dimensional new features will further slow
down the training speed. To solve this
problem, we propose a new training method,
adaptive online gradient descent based on
feature frequency information, for very fast
online training of the parameters, even given
large-scale datasets with high dimensional
features. Compared with existing training
methods, our training method is an order
magnitude faster in terms of training time, and
can achieve equal or even higher accuracies.
The proposed fast training method is a general
purpose optimization method, and it is not
limited in the specific task discussed in this
paper.
1 Introduction
Since Chinese sentences are written as continuous
sequences of characters, segmenting a character
sequence into words is normally the first step
in the pipeline of Chinese text processing. The
major problem of Chinese word segmentation
is the ambiguity. Chinese character sequences
are normally ambiguous, and new words (out-
of-vocabulary words) are a major source of the
ambiguity. A typical category of new words
is named entities, including organization names,
person names, location names, and so on.
In this paper, we present high dimensional
new features, including word-based features and
enriched edge (label-transition) features, for the
joint modeling of Chinese word segmentation
(CWS) and new word detection (NWD). While most
of the state-of-the-art CWS systems used semi-
Markov conditional random fields or latent variable
conditional random fields, we simply use a single
first-order conditional random fields (CRFs) for
the joint modeling. The semi-Markov CRFs and
latent variable CRFs relax the Markov assumption
of CRFs to express more complicated dependencies,
and therefore to achieve higher disambiguation
power. Alternatively, our plan is not to relax
Markov assumption of CRFs, but to exploit more
complicated dependencies via using refined high-
dimensional features. The advantage of our choice
is the simplicity of our model. As a result, our
CWS model can be more efficient compared with
the heavier systems, and with similar or even higher
accuracy because of using refined features.
As we know, training a word segmentation system
on large-scale datasets is already costly. In our
case, adding high dimensional new features will
further slow down the training speed. To solve this
challenging problem, we propose a new training
method, adaptive online gradient descent based on
feature frequency information (ADF), for very fast
word segmentation with new word detection, even
given large-scale datasets with high dimensional
features. In the proposed training method, we try
to use more refined learning rates. Instead of using
a single learning rate (a scalar) for all weights,
we extend the learning rate scalar to a learning
rate vector based on feature frequency information
in the updating. By doing so, each weight has
253
its own learning rate adapted on feature frequency
information. We will show that this can significantly
improve the convergence speed of online learning.
We approximate the learning rate vector based
on feature frequency information in the updating
process. Our proposal is based on the intuition
that a feature with higher frequency in the training
process should be with a learning rate that is decayed
faster. Based on this intuition, we will show the
formalized training algorithm later. We will show in
experiments that our solution is an order magnitude
faster compared with exiting learning methods, and
can achieve equal or even higher accuracies.
The contribution of this work is as follows:
? We propose a general purpose fast online
training method, ADF. The proposed training
method requires only a few passes to complete
the training.
? We propose a joint model for Chinese word
segmentation and new word detection.
? Compared with prior work, our system
achieves better accuracies on both word
segmentation and new word detection.
2 Related Work
First, we review related work on word segmentation
and new word detection. Then, we review popular
online training methods, in particular stochastic
gradient descent (SGD).
2.1 Word Segmentation and New Word
Detection
Conventional approaches to Chinese word
segmentation treat the problem as a sequential
labeling task (Xue, 2003; Peng et al, 2004; Tseng
et al, 2005; Asahara et al, 2005; Zhao et al,
2010). To achieve high accuracy, most of the state-
of-the-art systems are heavy probabilistic systems
using semi-Markov assumptions or latent variables
(Andrew, 2006; Sun et al, 2009b). For example,
one of the state-of-the-art CWS system is the latent
variable conditional random field (Sun et al, 2008;
Sun and Tsujii, 2009) system presented in Sun et al
(2009b). It is a heavy probabilistic model and it is
slow in training. A few other state-of-the-art CWS
systems are using semi-Markov perceptron methods
or voting systems based on multiple semi-Markov
perceptron segmenters (Zhang and Clark, 2007;
Sun, 2010). Those semi-Markov perceptron systems
are moderately faster than the heavy probabilistic
systems using semi-Markov conditional random
fields or latent variable conditional random fields.
However, a disadvantage of the perceptron style
systems is that they can not provide probabilistic
information.
On the other hand, new word detection is also one
of the important problems in Chinese information
processing. Many statistical approaches have been
proposed (J. Nie and Jin, 1995; Chen and Bai, 1998;
Wu and Jiang, 2000; Peng et al, 2004; Chen and
Ma, 2002; Zhou, 2005; Goh et al, 2003; Fu and
Luke, 2004; Wu et al, 2011). New word detection
is normally considered as a separate process from
segmentation. There were studies trying to solve this
problem jointly with CWS. However, the current
studies are limited. Integrating the two tasks would
benefit both segmentation and new word detection.
Our method provides a convenient framework for
doing this. Our new word detection is not a stand-
alone process, but an integral part of segmentation.
2.2 Online Training
The most representative online training method
is the SGD method. The SGD uses a small
randomly-selected subset of the training samples to
approximate the gradient of an objective function.
The number of training samples used for this
approximation is called the batch size. By using a
smaller batch size, one can update the parameters
more frequently and speed up the convergence. The
extreme case is a batch size of 1, and it gives the
maximum frequency of updates, which we adopt in
this work. Then, the model parameters are updated
in such a way:
wt+1 = wt + ?t?wtLstoch(z i,wt), (1)
where t is the update counter, ?t is the learning rate,
and Lstoch(z i,wt) is the stochastic loss function
based on a training sample z i.
There were accelerated versions of SGD,
including stochastic meta descent (Vishwanathan
et al, 2006) and periodic step-size adaptation
online learning (Hsu et al, 2009). Compared with
those two methods, our proposal is fundamentally
254
different. Those two methods are using 2nd-order
gradient (Hessian) information for accelerated
training, while our accelerated training method
does not need such 2nd-order gradient information,
which is costly and complicated. Our ADF training
method is based on feature frequency adaptation,
and there is no prior work on using feature frequency
information for accelerating online training.
Other online training methods includes averaged
SGD with feedback (Sun et al, 2010; Sun et al,
2011), latent variable perceptron training (Sun et al,
2009a), and so on. Those methods are less related to
this paper.
3 System Architecture
3.1 A Joint Model Based on CRFs
First, we briefly review CRFs. CRFs are proposed
as a method for structured classification by solving
?the label bias problem? (Lafferty et al, 2001).
Assuming a feature function that maps a pair of
observation sequence x and label sequence y to a
global feature vector f , the probability of a label
sequence y conditioned on the observation sequence
x is modeled as follows (Lafferty et al, 2001):
P (y|x,w) =
exp
{
w?f (y,x)
}
?
?y? exp
{
w?f (y? ,x)
} , (2)
wherew is a parameter vector.
Given a training set consisting of n labeled
sequences, z i = (xi, y i), for i = 1 . . . n, parameter
estimation is performed by maximizing the objective
function,
L(w) =
n
?
i=1
logP (y i|xi,w)?R(w). (3)
The first term of this equation represents a
conditional log-likelihood of a training data. The
second term is a regularizer for reducing overfitting.
We employed an L2 prior, R(w) = ||w||
2
2?2 . In what
follows, we denote the conditional log-likelihood of
each sample logP (y i|xi,w) as ?(z i,w). The final
objective function is as follows:
L(w) =
n
?
i=1
?(z i,w)?
||w||2
2?2 . (4)
Since no word list can be complete, new word
identification is an important task in Chinese NLP.
New words in input text are often incorrectly
segmented into single-character or other very short
words (Chen and Bai, 1998). This phenomenon
will also undermine the performance of Chinese
word segmentation. We consider here new word
detection as an integral part of segmentation,
aiming to improve both segmentation and new word
detection: detected new words are added to the
word list lexicon in order to improve segmentation.
Based on our CRF word segmentation system,
we can compute a probability for each segment.
When we find some word segments are of reliable
probabilities yet they are not in the existing word
list, we then treat those ?confident? word segments
as new words and add them into the existing word
list. Based on preliminary experiments, we treat
a word segment as a new word if its probability
is larger than 0.5. Newly detected words are re-
incorporated into word segmentation for improving
segmentation accuracies.
3.2 New Features
Here, we will describe high dimensional new
features for the system.
3.2.1 Word-based Features
There are two ideas in deriving the refined
features. The first idea is to exploit word features
for node features of CRFs. Note that, although our
model is a Markov CRFmodel, we can still use word
features to learn word information in the training
data. To derive word features, first of all, our system
automatically collect a list of word unigrams and
bigrams from the training data. To avoid overfitting,
we only collect the word unigrams and bigrams
whose frequency is larger than 2 in the training set.
This list of word unigrams and bigrams are then used
as a unigram-dictionary and a bigram-dictionary to
generate word-based unigram and bigram features.
The word-based features are indicator functions that
fire when the local character sequence matches a
word unigram or bigram occurred in the training
data. The word-based feature templates derived for
the label yi are as follows:
? unigram1(x, yi) ? [xj,i, yi], if the
character sequence xj,i matches a word w ? U,
255
with the constraint i ? 6 < j < i. The item
xj,i represents the character sequence xj . . . xi.
U represents the unigram-dictionary collected
from the training data.
? unigram2(x, yi) ? [xi,k, yi], if the
character sequence xi,k matches a wordw ? U,
with the constraint i < k < i + 6.
? bigram1(x, yi) ? [xj,i?1, xi,k, yi], if
the word bigram candidate [xj,i?1, xi,k] hits
a word bigram [wi, wj ] ? B, and satisfies
the aforementioned constraints on j and k. B
represents the word bigram dictionary collected
from the training data.
? bigram2(x, yi) ? [xj,i, xi+1,k, yi], if
the word bigram candidate [xj,i, xi+1,k] hits a
word bigram [wi, wj ] ? B, and satisfies the
aforementioned constraints on j and k.
We also employ the traditional character-based
features. For each label yi, we use the feature
templates as follows:
? Character unigrams locating at positions i? 2,
i? 1, i, i + 1 and i + 2
? Character bigrams locating at positions i ?
2, i? 1, i and i + 1
? Whether xj and xj+1 are identical, for j = i?
2, . . . , i + 1
? Whether xj and xj+2 are identical, for j = i?
3, . . . , i + 1
The latter two feature templates are designed
to detect character or word reduplication, a
morphological phenomenon that can influence word
segmentation in Chinese.
3.2.2 High Dimensional Edge Features
The node features discussed above are based on
a single label yi. CRFs also have edge features
that are based on label transitions. The second idea
is to incorporate local observation information of
x in edge features. For traditional implementation
of CRF systems (e.g., the HCRF package), usually
the edges features contain only the information
of yi?1 and yi, and without the information of
the observation sequence (i.e., x). The major
reason for this simple realization of edge features
in traditional CRF implementation is for reducing
the dimension of features. Otherwise, there can
be an explosion of edge features in some tasks.
For example, in part-of-speech tagging tasks, there
can be more than 40 labels and more than 1,600
types of label transitions. Therefore, incorporating
local observation information into the edge feature
will result in an explosion of edge features, which
is 1,600 times larger than the number of feature
templates.
Fortunately, for our task, the label set is quite
small, Y = {B,I,E}1. There are only nine possible
label transitions: T = Y ? Y and |T| = 9.2 As
a result, the feature dimension will have nine times
increase over the feature templates, if we incorporate
local observation information of x into the edge
features. In this way, we can effectively combine
observation information of x with label transitions
yi?1yi. We simply used the same templates of
node features for deriving the new edge features.
We found adding new edge features significantly
improves the disambiguation power of our model.
4 Adaptive Online Gradient Descent based
on Feature Frequency Information
As we will show in experiments, the training of the
CRF model with high-dimensional new features is
quite expensive, and the existing training method is
not good enough. To solve this issue, we propose a
fast online training method: adaptive online gradient
descent based on feature frequency information
(ADF). The proposed method is easy to implement.
For high convergence speed of online learning, we
try to use more refined learning rates than the SGD
training. Instead of using a single learning rate (a
scalar) for all weights, we extend the learning rate
scalar to a learning rate vector, which has the same
dimension of the weight vector w. The learning
rate vector is automatically adapted based on feature
frequency information. By doing so, each weight
1B means beginning of a word, I means inside a word, and
E means end of a word. The B,I,E labels have been widely
used in previous work of Chinese word segmentation (Sun et
al., 2009b).
2The operator ? means a Cartesian product between two
sets.
256
ADF learning algorithm
1: procedure ADF(q, c, ?, ?)
2: w ? 0, t? 0, v ? 0, ? ? c
3: repeat until convergence
4: . Draw a sample z i at random
5: . v ? UPDATE(v , z i)
6: . if t > 0 and t mod q = 0
7: . . ? ? UPDATE(? , v)
8: . . v ? 0
9: . g ? ?wLstoch(z i,w)
10: . w ? w + ? ? g
11: . t? t + 1
12: returnw
13:
14: procedure UPDATE(v , z i)
15: for k ? features used in sample z i
16: . vk ? vk + 1
17: return v
18:
19: procedure UPDATE(? , v)
20: for k ? all features
21: . u? vk/q
22: . ? ? ?? u(?? ?)
23: . ?k ? ??k
24: return ?
Figure 1: The proposed ADF online learning algorithm.
q, c, ?, and ? are hyper-parameters. q is an integer
representing window size. c is for initializing the learning
rates. ? and ? are the upper and lower bounds of a scalar,
with 0 < ? < ? < 1.
has its own learning rate, and we will show that this
can significantly improve the convergence speed of
online learning.
In our proposed online learning method, the
update formula is as follows:
wt+1 = wt + ? t ? gt. (5)
The update term gt is the gradient term of a
randomly sampled instance:
gt = ?wtLstoch(z i,wt) = ?wt
{
?(z i,wt)?
||wt||2
2n?2
}
.
In addition, ? t ? Rf+ is a positive vector-
valued learning rate and ? denotes component-wise
(Hadamard) product of two vectors.
We learn the learning rate vector ? t based
on feature frequency information in the updating
process. Our proposal is based on the intuition that a
feature with higher frequency in the training process
should be with a learning rate that decays faster. In
other words, we assume a high frequency feature
observed in the training process should have a small
learning rate, and a low frequency feature should
have a relatively larger learning rate in the training.
Our assumption is based on the intuition that a
weight with higher frequency is more adequately
trained, hence smaller learning rate is preferable for
fast convergence.
Given a window size q (number of samples in a
window), we use a vector v to record the feature
frequency. The k?th entry vk corresponds to the
frequency of the feature k in this window. Given
a feature k, we use u to record the normalized
frequency:
u = vk/q.
For each feature, an adaptation factor ? is calculated
based on the normalized frequency information, as
follows:
? = ?? u(?? ?),
where ? and ? are the upper and lower bounds of
a scalar, with 0 < ? < ? < 1. As we can see,
a feature with higher frequency corresponds to a
smaller scalar via linear approximation. Finally, the
learning rate is updated as follows:
?k ? ??k.
With this setting, different features will correspond
to different adaptation factors based on feature
frequency information. Our ADF algorithm is
summarized in Figure 1.
The ADF training method is efficient, because
the additional computation (compared with SGD) is
only the derivation of the learning rates, which is
simple and efficient. As we know, the regularization
of SGD can perform efficiently via the optimization
based on sparse features (Shalev-Shwartz et al,
2007). Similarly, the derivation of ? t can also
perform efficiently via the optimization based on
sparse features.
4.1 Convergence Analysis
Prior work on convergence analysis of existing
online learning algorithms (Murata, 1998; Hsu et
257
Data Method Passes Train-Time (sec) NWD Rec Pre Rec CWS F-score
MSR Baseline 50 4.7e3 72.6 96.3 95.9 96.1
+ New features 50 1.2e4 75.3 97.2 97.0 97.1
+ New word detection 50 1.2e4 78.2 97.5 96.9 97.2
+ ADF training 10 2.3e3 77.5 97.6 97.2 97.4
CU Baseline 50 2.9e3 68.5 94.0 93.9 93.9
+ New features 50 7.5e3 68.0 94.4 94.5 94.4
+ New word detection 50 7.5e3 68.8 94.8 94.5 94.7
+ ADF training 10 1.5e3 68.8 94.8 94.7 94.8
PKU Baseline 50 2.2e3 77.2 95.0 94.0 94.5
+ New features 50 5.2e3 78.4 95.5 94.9 95.2
+ New word detection 50 5.2e3 79.1 95.8 94.9 95.3
+ ADF training 10 1.2e3 78.4 95.8 94.9 95.4
Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge
features), new word detection, and ADF training (replacing SGD training with ADF training). Number of passes is
decided by empirical convergence of the training methods.
#W.T. #Word #C.T. #Char
MSR 8.8? 104 2.4? 106 5? 103 4.1? 106
CU 6.9? 104 1.5? 106 5? 103 2.4? 106
PKU 5.5? 104 1.1? 106 5? 103 1.8? 106
Table 1: Details of the datasets. W.T. represents word
types; C.T. represents character types.
al., 2009) can be extended to the proposed ADF
training method. We can show that the proposed
ADF learning algorithm has reasonable convergence
properties.
When we have the smallest learning rate ? t+1 =
?? t, the expectation of the obtainedwt is
E(wt) = w? +
t
?
m=1
(I ? ?0?mH (w?))(w0 ?w?),
wherew? is the optimal weight vector, andH is the
Hessian matrix of the objective function. The rate of
convergence is governed by the largest eigenvalue of
the functionC t =
?t
m=1(I ? ?0?mH (w?)). Then,
we can derive a bound of rate of convergence.
Theorem 1 Assume ? is the largest eigenvalue of
the function C t =
?t
m=1(I ? ?0?mH (w?)). For
the proposed ADF training, its convergence rate is
bounded by ?, and we have
? ? exp
{?0??
? ? 1
}
,
where ? is the minimum eigenvalue ofH (w?).
5 Experiments
5.1 Data and Metrics
We used benchmark datasets provided by the second
International Chinese Word Segmentation Bakeoff
to test our proposals. The datasets are from
Microsoft Research Asia (MSR), City University
of Hongkong (CU), and Peking University (PKU).
Details of the corpora are listed in Table 1. We
did not use any extra resources such as common
surnames, parts-of-speech, and semantics.
Four metrics were used to evaluate segmentation
results: recall (R, the percentage of gold standard
output words that are correctly segmented by the
decoder), precision (P , the percentage of words in
the decoder output that are segmented correctly),
balanced F-score defined by 2PR/(P + R), and
recall of new word detection (NWD recall). For
more detailed information on the corpora, refer to
Emerson (2005).
5.2 Features, Training, and Tuning
We employed the feature templates defined in
Section 3.2. The feature sets are huge. There are
2.4 ? 107 features for the MSR data, 4.1 ? 107
features for the CU data, and 4.7 ? 107 features for
the PKU data. To generate word-based features, we
extracted high-frequency word-based unigram and
bigram lists from the training data.
As for training, we performed gradient descent
258
0 10 20 30 40 5095
95.5
96
96.5
97
97.5
MSR
Number of Passes
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 10 20 30 40 5092
92.5
93
93.5
94
94.5
95
CU
Number of Passes
F?
sco
re
0 10 20 30 40 5094
94.5
95
95.5
PKU
Number of Passes
F?
sco
re
0 2000 4000 600095
95.5
96
96.5
97
97.5
MSR
Training time (sec)
F?
sco
re
 
 
ADF
SGD
LBFGS (batch)
0 1000 2000 3000 400092
92.5
93
93.5
94
94.5
95
CU
Training time (sec)
F?
sco
re
0 1000 2000 3000 400094
94.5
95
95.5
PKU
Training time (sec)
F?
sco
re
Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.
with our proposed training method. To compare
with existing methods, we chose two popular
training methods, a batch training one and an
online training one. The batch training method
is the Limited-Memory BFGS (LBFGS) method
(Nocedal and Wright, 1999). The online baseline
training method is the SGD method, which we have
introduced in Section 2.2.
For the ADF training method, we need to tune the
hyper-parameters q, c, ?, and ?. Based on automatic
tuning within the training data (validation in the
training data), we found it is proper to set q = n/10
(n is the number of training samples), c = 0.1,
? = 0.995, and ? = 0.6. To reduce overfitting,
we employed an L2 Gaussian weight prior (Chen
and Rosenfeld, 1999) for all training methods. We
varied the ? with different values (e.g., 1.0, 2.0, and
5.0), and finally set the value to 1.0 for all training
methods.
5.3 Results and Discussion
First, we performed incremental evaluation in this
order: Baseline (word segmentation model with
SGD training); Baseline + New features; Baseline
+ New features + New word detection; Baseline +
New features + New word detection + ADF training
(replacing SGD training). The results are shown in
Table 2.
As we can see, the new features improved
performance on both word segmentation and new
word detection. However, we also noticed that
the training cost became more expensive via
adding high dimensional new features. Adding
new word detection function further improved the
segmentation quality and the new word recognition
recall. Finally, by using the ADF training method,
the training speed is much faster than the SGD
training method. The ADF method can achieve
empirical optimum in only a few passes, yet
with better segmentation accuracies than the SGD
training with 50 passes.
To get more details of the proposed training
method, we compared it with SGD and LBFGS
training methods based on an identical platform,
by varying the number of passes. The comparison
was based on the same platform: Baseline + New
features + New word detection. The F-score curves
of the training methods are shown in Figure 2.
Impressively, the ADF training method reached
empirical convergence in only a few passes, while
the SGD and LBFGS training converged much
slower, requiring more than 50 passes. The ADF
training is about an order magnitude faster than
the SGD online training and more than an order
magnitude faster than the LBFGS batch training.
Finally, we compared our method with the state-
259
Data Method Prob. Pre Rec F-score
MSR Best05 (Tseng et al, 2005)
?
96.2 96.6 96.4
CRF + rule-system (Zhang et al, 2006)
?
97.2 96.9 97.1
Semi-Markov perceptron (Zhang and Clark, 2007) ? N/A N/A 97.2
Semi-Markov CRF (Gao et al, 2007)
?
N/A N/A 97.2
Latent-variable CRF (Sun et al, 2009b)
?
97.3 97.3 97.3
Our method (A Single CRF)
?
97.6 97.2 97.4
CU Best05 (Tseng et al, 2005)
?
94.1 94.6 94.3
CRF + rule-system (Zhang et al, 2006)
?
95.2 94.9 95.1
Semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 95.1
Latent-variable CRF (Sun et al, 2009b)
?
94.7 94.4 94.6
Our method (A Single CRF)
?
94.8 94.7 94.8
PKU Best05 (Chen et al, 2005) N/A 95.3 94.6 95.0
CRF + rule-system (Zhang et al, 2006)
?
94.7 95.5 95.1
semi-perceptron (Zhang and Clark, 2007) ? N/A N/A 94.5
Latent-variable CRF (Sun et al, 2009b)
?
95.6 94.8 95.2
Our method (A Single CRF)
?
95.8 94.9 95.4
Table 3: Comparing our method with the state-of-the-art CWS systems.
of-the-art systems reported in the previous papers.
The statistics are listed in Table 3. Best05 represents
the best system of the Second International Chinese
Word Segmentation Bakeoff on the corresponding
data; CRF + rule-system represents confidence-
based combination of CRF and rule-based models,
presented in Zhang et al (2006). Prob. indicates
whether or not the system can provide probabilistic
information. As we can see, our method achieved
similar or even higher F-scores, compared with the
best systems reported in previous papers. Note that,
our system is a single Markov model, while most of
the state-of-the-art systems are complicated heavy
systems, with model-combinations (e.g., voting of
multiple segmenters), semi-Markov relaxations, or
latent-variables.
6 Conclusions and Future Work
In this paper, we presented a joint model for
Chinese word segmentation and newword detection.
We presented new features, including word-based
features and enriched edge features, for the joint
modeling. We showed that the new features can
improve the performance on the two tasks.
On the other hand, the training of the model,
especially with high-dimensional new features,
became quite expensive. To solve this problem,
we proposed a new training method, ADF training,
for very fast training of CRFs, even given large-
scale datasets with high dimensional features. We
performed experiments and showed that our new
training method is an order magnitude faster than
existing optimization methods. Our final system can
learn highly accurate models with only a few passes
in training. The proposed fast learning method
is a general algorithm that is not limited in this
specific task. As future work, we plan to apply
this fast learning method on other large-scale natural
language processing tasks.
Acknowledgments
We thank Yaozhong Zhang and Weiwei Sun
for helpful discussions on word segmentation
techniques. The work described in this paper was
supported by a Hong Kong RGC Project (No. PolyU
5230/08E), National High Technology Research and
Development Program of China (863 Program) (No.
2012AA011101), and National Natural Science
Foundation of China (No.91024009, No.60973053).
References
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
260
In Proceedings of EMNLP?06, pages 465?472.
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takahashi Tsuzuki. 2005. Combination of
machine learning methods for optimum chinese word
segmentation. In Proceedings of The Fourth SIGHAN
Workshop, pages 134?137.
K.J. Chen and M.H. Bai. 1998. Unknown word
detection for chinese by a corpus-based learning
method. Computational Linguistics and Chinese
Language Processing, 3(1):27?44.
Keh-Jiann Chen and Wei-Yun Ma. 2002. Unknown word
extraction for chinese documents. In Proceedings of
COLING?02.
Stanley F. Chen and Ronald Rosenfeld. 1999. A
gaussian prior for smoothing maximum entropy
models. Technical Report CMU-CS-99-108, CMU.
Aitao Chen, Yiping Zhou, Anne Zhang, and Gordon Sun.
2005. Unigram language model for chinese word
segmentation. In Proceedings of the fourth SIGHAN
workshop, pages 138?141.
Thomas Emerson. 2005. The second international
chinese word segmentation bakeoff. In Proceedings
of the fourth SIGHAN workshop, pages 123?133.
Guohong Fu and Kang-Kwong Luke. 2004. Chinese
unknown word identification using class-based lm. In
Proceedings of IJCNLP?04, volume 3248 of Lecture
Notes in Computer Science, pages 704?713. Springer.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL?07), pages 824?831.
Chooi-Ling Goh, Masayuki Asahara, and Yuji
Matsumoto. 2003. Chinese unknown word
identification using character-based tagging and
chunking. In Kotaro Funakoshi, Sandra Kbler, and
Jahna Otterbacher, editors, Proceedings of ACL
(Companion)?03, pages 197?200.
Chun-Nan Hsu, Han-Shen Huang, Yu-Ming Chang, and
Yuh-Jye Lee. 2009. Periodic step-size adaptation in
second-order gradient descent for single-pass on-line
structured learning. Machine Learning, 77(2-3):195?
224.
M. Hannan J. Nie and W. Jin. 1995. Unknown
word detection and segmentation of chinese using
statistical and heuristic knowledge. Communications
of the Chinese and Oriental Languages Information
Processing Society, 5:47C57.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference on
Machine Learning (ICML?01), pages 282?289.
Noboru Murata. 1998. A statistical study of on-line
learning. In On-line learning in neural networks,
Cambridge University Press, pages 63?92.
Jorge Nocedal and Stephen J. Wright. 1999. Numerical
optimization. Springer.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of
Coling 2004, pages 562?568, Geneva, Switzerland,
Aug 23?Aug 27. COLING.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML?07.
Xu Sun and Jun?ichi Tsujii. 2009. Sequential labeling
with latent variables: An exact inference algorithm
and its efficient approximation. In Proceedings of
EACL?09, pages 772?780, Athens, Greece, March.
Xu Sun, Louis-Philippe Morency, Daisuke Okanohara,
and Jun?ichi Tsujii. 2008. Modeling latent-dynamic
in shallow parsing: A latent conditional model with
improved inference. In Proceedings of COLING?08,
pages 841?848, Manchester, UK.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun?ichi Tsujii. 2009a. Latent variable perceptron
algorithm for structured classification. In Proceedings
of the 21st International Joint Conference on Artificial
Intelligence (IJCAI 2009), pages 1236?1242.
Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa
Tsuruoka, and Jun?ichi Tsujii. 2009b. A
discriminative latent variable chinese segmenter with
hybrid word/character information. In Proceedings
of NAACL-HLT?09, pages 56?64, Boulder, Colorado,
June.
Xu Sun, Hisashi Kashima, Takuya Matsuzaki, and
Naonori Ueda. 2010. Averaged stochastic gradient
descent with feedback: An accurate, robust, and
fast training method. In Proceedings of the 10th
International Conference on Data Mining (ICDM?10),
pages 1067?1072.
Xu Sun, Hisashi Kashima, Ryota Tomioka, and Naonori
Ueda. 2011. Large scale real-life action recognition
using conditional random fields with stochastic
training. In Proceedings of the 15th Pacific-Asia
Conf. on Knowledge Discovery and Data Mining
(PAKDD?11).
Weiwei Sun. 2010. Word-based and character-
based word segmentation models: Comparison and
combination. In Chu-Ren Huang and Dan Jurafsky,
editors, COLING?10 (Posters), pages 1211?1219.
Chinese Information Processing Society of China.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A
261
conditional random field word segmenter for sighan
bakeoff 2005. In Proceedings of The Fourth SIGHAN
Workshop, pages 168?171.
S.V.N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
meta-descent. In Proceedings of ICML?06, pages 969?
976.
A. Wu and Z. Jiang. 2000. Statistically-enhanced new
word identification in a rule-based chinese system.
In Proceedings of the Second Chinese Language
Processing Workshop, page 46C51, Hong Kong,
China.
Yi-Lun Wu, Chaio-Wen Hsieh, Wei-Hsuan Lin, Chun-
Yi Liu, and Liang-Chih Yu. 2011. Unknown
word extraction from multilingual code-switching
sentences (in chinese). In Proceedings of ROCLING
(Posters)?11, pages 349?360.
Nianwen Xue. 2003. Chinese word segmentation
as character tagging. International Journal of
Computational Linguistics and Chinese Language
Processing, 8(1):29?48.
Yue Zhang and Stephen Clark. 2007. Chinese
segmentation with a word-based perceptron algorithm.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 840?
847, Prague, Czech Republic, June. Association for
Computational Linguistics.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proceedings
of the Human Language Technology Conference of
the NAACL, Companion Volume: Short Papers, pages
193?196, New York City, USA, June. Association for
Computational Linguistics.
Hai Zhao, Changning Huang, Mu Li, and Bao-Liang Lu.
2010. A unified character-based tagging framework
for chinese word segmentation. ACM Trans. Asian
Lang. Inf. Process., 9(2).
Guodong Zhou. 2005. A chunking strategy
towards unknown word detection in chinese word
segmentation. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Yee Kwong, editors, Proceedings
of IJCNLP?05, volume 3651 of Lecture Notes in
Computer Science, pages 530?541. Springer.
262
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 567?571,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Sequential Summarization: A New Application for Timely Updated 
Twitter Trending Topics 
Dehong Gao, Wenjie Li, Renxian Zhang 
Department of Computing, the Hong Kong Polytechnic University, Hong Kong 
{csdgao, cswjli, csrzhang}@comp.polyu.edu.hk 
 
Abstract 
The growth of the Web 2.0 technologies has 
led to an explosion of social networking 
media sites. Among them, Twitter is the most 
popular service by far due to its ease for real-
time sharing of information. It collects 
millions of tweets per day and monitors what 
people are talking about in the trending topics 
updated timely. Then the question is how 
users can understand a topic in a short time 
when they are frustrated with the 
overwhelming and unorganized tweets. In 
this paper, this problem is approached by 
sequential summarization which aims to 
produce a sequential summary, i.e., a series 
of chronologically ordered short sub-
summaries that collectively provide a full 
story about topic development. Both the 
number and the content of sub-summaries are 
automatically identified by the proposed 
stream-based and semantic-based approaches. 
These approaches are evaluated in terms of 
sequence coverage, sequence novelty and 
sequence correlation and the effectiveness of 
their combination is demonstrated.  
1 Introduction and Background 
Twitter, as a popular micro-blogging service, 
collects millions of real-time short text messages 
(known as tweets) every second. It acts as not 
only a public platform for posting trifles about 
users? daily lives, but also a public reporter for 
real-time news. Twitter has shown its powerful 
ability in information delivery in many events, 
like the wildfires in San Diego and the 
earthquake in Japan. Nevertheless, the side effect 
is individual users usually sink deep under 
millions of flooding-in tweets. To alleviate this 
problem, the applications like whatthetrend 1 
have evolved from Twitter to provide services 
that encourage users to edit explanatory tweets 
about a trending topic, which can be regarded as 
topic summaries. It is to some extent a good way 
to help users understand trending topics. 
                                                          
1 whatthetrend.com 
There is also pioneering research in automatic 
Twitter trending topic summarization. (O'Connor 
et al, 2010) explained Twitter trending topics by 
providing a list of significant terms. Users could 
utilize these terms to drill down to the tweets 
which are related to the trending topics. (Sharifi 
et al, 2010) attempted to provide a one-line 
summary for each trending topic using phrase 
reinforcement ranking. The relevance model 
employed by (Harabagiu and Hickl, 2011) 
generated summaries in larger size, i.e., 250-
word summaries, by synthesizing multiple high 
rank tweets. (Duan et al, 2012) incorporate the 
user influence and content quality information in 
timeline tweet summarization and employ 
reinforcement graph to generate summaries for 
trending topics. 
Twitter summarization is an emerging 
research area. Current approaches still followed 
the traditional summarization route and mainly 
focused on mining tweets of both significance 
and representativeness. Though, the summaries 
generated in such a way can sketch the most 
important aspects of the topic, they are incapable 
of providing full descriptions of the changes of 
the focus of a topic, and the temporal information 
or freshness of the tweets, especially for those 
newsworthy trending topics, like earthquake and 
sports meeting. As the main information 
producer in Twitter, the massive crowd keeps 
close pace with the development of trending 
topics and provide the timely updated 
information. The information dynamics and 
timeliness is an important consideration for 
Twitter summarization. That is why we propose 
sequential summarization in this work, which 
aims to produce sequential summaries to capture 
the temporal changes of mass focus. 
Our work resembles update summarization 
promoted by TAC 2  which required creating 
summaries with new information assuming the 
reader has already read some previous 
documents under the same topic. Given two 
chronologically ordered documents sets about a 
topic, the systems were asked to generate two 
                                                          
2 www.nist.gov/tac 
567
summaries, and the second one should inform the 
user of new information only. In order to achieve 
this goal, existing approaches mainly emphasized 
the novelty of the subsequent summary (Li and 
Croft, 2006; Varma et al, 2009; Steinberger and 
Jezek, 2009). Different from update 
summarization, we focus more on the temporal 
change of trending topics. In particular, we need 
to automatically detect the ?update points? 
among a myriad of related tweets.  
It is the goal of this paper to set up a new 
practical summarization application tailored for 
timely updated Twitter messages. With the aim 
of providing a full description of the focus 
changes and the records of the timeline of a 
trending topic, the systems are expected to 
discover the chronologically ordered sets of 
information by themselves and they are free to 
generate any number of update summaries 
according to the actual situations instead of a 
fixed number of summaries as specified in 
DUC/TAC. Our main contributions include 
novel approaches to sequential summarization 
and corresponding evaluation criteria for this 
new application. All of them will be detailed in 
the following sections. 
2 Sequential Summarization 
Sequential summarization proposed here aims to 
generate a series of chronologically ordered sub-
summaries for a given Twitter trending topic. 
Each sub-summary is supposed to represent one 
main subtopic or one main aspect of the topic, 
while a sequential summary, made up by the sub-
summaries, should retain the order the 
information is delivered to the public. In such a 
way, the sequential summary is able to provide a 
general picture of the entire topic development. 
2.1 Subtopic Segmentation 
One of the keys to sequential summarization is 
subtopic segmentation. How many subtopics 
have attracted the public attention, what are they, 
and how are they developed? It is important to 
provide the valuable and organized materials for 
more fine-grained summarization approaches. 
We proposed the following two approaches to 
automatically detect and chronologically order 
the subtopics. 
2.1.1 Stream-based Subtopic Detection and 
Ordering 
Typically when a subtopic is popular enough, it 
will create a certain level of surge in the tweet 
stream. In other words, every surge in the tweet 
stream can be regarded as an indicator of the 
appearance of a subtopic that is worthy of being 
summarized. Our early investigation provides 
evidence to support this assumption. By 
examining the correlations between tweet content 
changes and volume changes in randomly 
selected topics, we have observed that the 
changes in tweet volume can really provide the 
clues of topic development or changes of crowd 
focus.  
The stream-based subtopic detection approach 
employs the offline peak area detection (Opad) 
algorithm (Shamma et al, 2010) to locate such 
surges by tracing tweet volume changes. It 
regards the collection of tweets at each such 
surge time range as a new subtopic.  
Offline Peak Area Detection (Opad) Algorithm 
1: Input: TS (tweets stream, each twi with timestamp ti); 
peak interval window ?? (in hour), and time 
step? (? ? ??); 
2: Output: Peak Areas PA. 
3: Initial: two time slots: ?? = ? = ?0 + ??;  
Tweet numbers: ?? = ? = ?????(?) 
4: while (?? = ? + ?) < ???1 
5:      update ?? = ?? + ?? and ?
? = ?????(??) 
6:      if (?? < ? And up-hilling)  
7: output one peak area ???  
8: state of down-hilling 
9:      else  
10: update ? = ?? and ? =  ?? 
11: state of up-hilling 
12: 
13: function ?????(?) 
14: Count tweets in time interval T 
The subtopics detected by the Opad algorithm 
are naturally ordered in the timeline. 
2.1.2 Semantic-based Subtopic Detection and 
Ordering 
Basically the stream-based approach monitors 
the changes of the level of user attention. It is 
easy to implement and intuitively works, but it 
fails to handle the cases where the posts about 
the same subtopic are received at different time 
ranges due to the difference of geographical and 
time zones. This may make some subtopics 
scattered into several time slots (peak areas) or 
one peak area mixed with more than one 
subtopic. 
In order to sequentially segment the subtopics 
from the semantic aspect, the semantic-based 
subtopic detection approach breaks the time 
order of tweet stream, and regards each tweet as 
an individual short document. It takes advantage 
of Dynamic Topic Modeling (David and Michael, 
2006) to explore the tweet content.  
568
DTM in nature is a clustering approach which 
can dynamically generate the subtopic 
underlying the topic. Any clustering approach 
requires a pre-specified cluster number. To avoid 
tuning the cluster number experimentally, the 
subtopic number required by the semantic-based 
approach is either calculated according to 
heuristics or determined by the number of the 
peak areas detected from the stream-based 
approach in this work. 
Unlike the stream-based approach, the 
subtopics formed by DTM are the sets of 
distributions of subtopic and word probabilities. 
They are time independent. Thus, the temporal 
order among these subtopics is not obvious and 
needs to be discovered. We use the probabilistic 
relationships between tweets and topics learned 
from DTM to assign each tweet to a subtopic that 
it most likely belongs to. Then the subtopics are 
ordered temporally according to the mean values 
of their tweets? timestamps. 
2.2 Sequential Summary Generation 
Once the subtopics are detected and ordered, the 
tweets belonging to each subtopic are ranked and 
the most significant one is extracted to generate 
the sub-summary regarding that subtopic. Two 
different ranking strategies are adopted to 
conform to two different subtopic detection 
mechanisms. 
For a tweet in a peak area, the linear 
combination of two measures is considered to 
evaluate its significance to be a sub-summary: (1) 
subtopic representativeness measured by the 
cosine similarity between the tweet and the 
centroid of all the tweets in the same peak area; 
(2) crowding endorsement measured by the times 
that the tweet is re-tweeted normalized by the 
total number of re-tweeting. With the DTM 
model, the significance of the tweets is evaluated 
directly by word distribution per subtopic.  
MMR (Carbonell and Goldstein, 1998) is used 
to reduce redundancy in sub-summary generation.  
3 Experiments and Evaluations 
The experiments are conducted on the 24 Twitter 
trending topics collected using Twitter APIs 3 . 
The statistics are shown in Table 1. 
Due to the shortage of gold-standard 
sequential summaries, we invite two annotators 
to read the chronologically ordered tweets, and 
write a series of sub-summaries for each topic 
                                                          
3https://dev.twitter.com/ 
independently. Each sub-summary is up to 140 
characters in length to comply with the limit of 
tweet, but the annotators are free to choose the 
number of sub-summaries. It ends up with 6.3 
and 4.8 sub-summaries on average in a 
sequential summary written by the two 
annotators respectively. These two sets of 
sequential summaries are regarded as reference 
summaries to evaluate system-generated 
summaries from the following three aspects. 
 
Category #TT 
Trending Topic 
Examples 
Tweets 
Number 
News 6 
Minsk, Libya 
Release 
25145 
Sports 6 
#bbcf1, 
Lakers/Heat 
17204 
Technology 5 Google Fiber 13281 
Science 2 AH1N1, Richter 10935 
Entertainment 2 Midnight Club, 6573 
Meme 2 
#ilovemyfans, 
Night Angels 
14595 
Lifestyle 1 Goose Island 6230 
Total 24 ------------ 93963 
Table 1. Data Set 
? Sequence Coverage 
Sequence coverage measures the N-gram match 
between system-generated summaries and 
human-written summaries (stopword removed 
first). Considering temporal information is an 
important factor in sequential summaries, we 
propose the position-aware coverage measure by 
accommodating the position information in 
matching. Let S={s1, s2, ?, sk} denote a 
sequential summary and si the ith sub-summary, 
N-gram coverage is defined as: 
????????
=
1
|???|
?
? ? ??????????(?-????)?-???????,????????
??? ? ? ? ?????(?-????)?-???????????????????
 
where,  ??? = |? ? ?| + 1, i and j denote the serial 
numbers of the sub-summaries in the system-
generated summary ???  and the human-written 
summary ??? , respectively. ?  serves as a 
coefficient to discount long-distance matched 
sub-summaries. We evaluate unigram, bigram, 
and skipped bigram matches. Like in ROUGE 
(Lin, 2004), the skip distance is up to four words. 
? Sequence Novelty 
Sequence novelty evaluates the average novelty 
of two successive sub-summaries. Information 
content (IC) has been used to measure the 
novelty of update summaries by (Aggarwal et al, 
2009). In this paper, the novelty of a system-
569
generated sequential summary is defined as the 
average of IC increments of two adjacent sub-
summaries,  
??????? =
1
|?| ? 1
?(???? ? ????, ???1)
?>1
 
where |?| is the number of sub-summaries in the 
sequential summary. ???? = ? ??????? . ????, ???1 =
? ???????????1  is the overlapped information in the 
two adjacent sub-summaries. ??? = ????  ?
?????????(?, ???) where w is a word, ???? is the 
inverse tweet frequency of w, and ??? is all the 
tweets in the trending topic. The relevance 
function is introduced to ensure that the 
information brought by new sub-summaries is 
not only novel but also related to the topic.  
? Sequence Correlation 
Sequence correlation evaluates the sequential 
matching degree between system-generated and 
human-written summaries. In statistics, 
Kendall?s tau coefficient is often used to measure 
the association between two sequences (Lapata, 
2006). The basic idea is to count the concordant 
and discordant pairs which contain the same 
elements in two sequences. Borrowing this idea, 
for each sub-summary in a human-generated 
summary, we find its most matched sub-
summary (judged by the cosine similarity 
measure) in the corresponding system-generated 
summary and then define the correlation 
according to the concordance between the two 
matched sub-summary sequences. 
???????????
=
2(|#???????????????| ? |#???????????????|)
?(? ? 1)
 
where n is the number of human-written sub-
summaries.  
Tables 2 and 3 below present the evaluation 
results. For the stream-based approach, we set 
?t=3 hours experimentally. For the semantic-
based approach, we compare three different 
approaches to defining the sub-topic number K: 
(1) Semantic-based 1: Following the approach 
proposed in (Li et al, 2007), we first derive the 
matrix of tweet cosine similarity. Given the 1-
norm of eigenvalues  ??
???? (? = 1, 2, ? , ?) of the 
similarity matrix and the ratios ?? = ??
????/?2 , 
the subtopic number ? = ? + 1  if ?? ? ??+1 > ? 
(? = 0.4 ). (2) Semantic-based 2: Using the rule 
of thumb in (Wan and Yang, 2008), ? = ?? , 
where n is the tweet number. (3) Combined: K is 
defined as the number of the peak areas detected 
from the Opad algorithm, meanwhile we use the 
tweets within peak areas as the tweets of DTM. 
This is our new idea. 
The experiments confirm the superiority of the 
semantic-based approach over the stream-based 
approach in summary content coverage and 
novelty evaluations, showing that the former is 
better at subtopic content modeling. The sub-
summaries generated by the stream-based 
approach have comparative sequence (i.e., order) 
correlation with the human summaries. 
Combining the advantages the two approaches 
leads to the best overall results.  
 
Coverage Unigram  Bigram  
Skipped 
Bigram 
Stream-
based(?t=3) 
0.3022 0.1567 0.1523 
Semantic-
based1(?=0.5) 
0.3507 0.1684 0.1866 
Semantic-based 2 0.3112 0.1348 0.1267 
Combined(?t=3) 0.3532 0.1699 0.1791 
Table 2. N-Gram Coverage Evaluation 
Approaches Novelty Correlation 
Stream-based (?t=3) 0.3798 0.3330 
Semantic-based 1 (?=0.4) 0.7163 0.3746 
Semantic-based 2 0.7017 0.3295 
Combined (?t=3) 0.7793 0.3986 
Table 3. Novelty and Correlation Evaluation 
4 Concluding Remarks 
We start a new application for Twitter trending 
topics, i.e., sequential summarization, to reveal 
the developing scenario of the trending topics 
while retaining the order of information 
presentation. We develop several solutions to 
automatically detect, segment and order 
subtopics temporally, and extract the most 
significant tweets into the sub-summaries to 
compose sequential summaries. Empirically, the 
combination of the stream-based approach and 
the semantic-based approach leads to sequential 
summaries with high coverage, low redundancy, 
and good order. 
Acknowledgments 
The work described in this paper is supported by 
a Hong Kong RGC project (PolyU No. 5202/12E) 
and a National Nature Science Foundation of 
China (NSFC No. 61272291). 
References  
Aggarwal Gaurav, Sumbaly Roshan and Sinha Shakti. 
2009. Update Summarization. Stanford: CS224N 
Final Projects. 
570
Blei M. David and Jordan I. Michael. 2006. Dynamic 
topic models. In Proceedings of the 23rd 
international conference on Machine learning, 113-
120.  Pittsburgh, Pennsylvania. 
Carbonell Jaime and Goldstein Jade. 1998. The use of 
MMR, diversity based reranking for reordering 
documents and producing summaries. In 
Proceedings of the 21st Annual International 
Conference on Research and Development in 
Information Retrieval, 335-336. Melbourne, 
Australia. 
Duan Yajuan, Chen Zhimin, Wei Furu, Zhou Ming 
and Heung-Yeung Shum. 2012. Twitter Topic 
Summarization by Ranking Tweets using Social                
Influence and Content Quality. In Proceedings of 
the 24th International Conference on Computational 
Linguistics, 763-780. Mumbai, India. 
Harabagiu Sanda and Hickl Andrew. 2011. Relevance 
Modeling for Microblog Summarization. In 
Proceedings of 5th International AAAI Conference 
on Weblogs and Social Media. Barcelona, Spain. 
Lapata Mirella. 2006. Automatic evaluation of 
information ordering: Kendall?s tau. Computational 
Linguistics, 32(4):1-14.  
Li Wenyuan, Ng Wee-Keong, Liu Ying and Ong 
Kok-Leong. 2007. Enhancing the Effectiveness of 
Clustering with Spectra Analysis. IEEE 
Transactions on Knowledge and Data Engineering, 
19(7):887-902. 
Li Xiaoyan and Croft W. Bruce. 2006. Improving 
novelty detection for general topics using sentence 
level information patterns. In Proceedings of the 
15th ACM International Conference on Information 
and Knowledge Management, 238-247. New York, 
USA. 
Lin Chin-Yew. 2004. ROUGE: a Package for 
Automatic Evaluation of Summaries. In 
Proceedings of the ACL Workshop on Text 
Summarization Branches Out, 74-81. Barcelona, 
Spain. 
Liu Fei, Liu Yang and Weng Fuliang. 2011. Why is 
?SXSW ? trending? Exploring Multiple Text 
Sources for Twitter Topic Summarization. In 
Proceedings of the ACL Workshop on Language in 
Social Media, 66-75. Portland, Oregon. 
O'Connor Brendan, Krieger Michel and Ahn David. 
2010. TweetMotif: Exploratory Search and Topic 
Summarization for Twitter. In Proceedings of the 
4th International AAAI Conference on Weblogs 
and Social Media, 384-385. Atlanta, Georgia. 
Shamma A. David, Kennedy Lyndon and Churchill F. 
Elizabeth. 2010. Tweetgeist: Can the Twitter 
Timeline Reveal the Structure of Broadcast Events? 
In Proceedings of the 2010 ACM Conference on 
Computer Supported Cooperative Work, 589-593. 
Savannah, Georgia, USA. 
Sharifi Beaux, Hutton Mark-Anthony and Kalita Jugal. 
2010. Summarizing Microblogs Automatically. In 
Human Language Technologies: the 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 685-
688. Los Angeles, California. 
Steinberger Josef and Jezek Karel. 2009. Update 
summarization based on novel topic distribution. In 
Proceedings of the 9th ACM Symposium on 
Document Engineering, 205-213. Munich, 
Germany. 
Varma Vasudeva, Bharat Vijay, Kovelamudi Sudheer, 
Praveen Bysani, Kumar K. N, Kranthi Reddy, 
Karuna Kumar and Nitin Maganti. 2009. IIIT 
Hyderabad at TAC 2009. In Proceedings of the 
2009 Text Analysis Conference. GaithsBurg, 
Maryland. 
Wan Xiaojun and Yang Jianjun. 2008. Multi-
document summarization using cluster-based link 
analysis. In Proceedings of the 31st Annual 
International Conference on Research and 
Development in Information Retrieval, 299-306. 
Singapore, Singapore. 
 
571
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 25?35,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Text-level Discourse Dependency Parsing 
 
Sujian Li1 Liang Wang1 Ziqiang Cao1 Wenjie Li2 
1 Key Laboratory of Computational Linguistics, Peking University, MOE, China 
2 Department of Computing, The Hong Kong Polytechnic University, HongKong 
{lisujian,intfloat,ziqiangyeah}@pku.edu.cn 
cswjli@comp.polyu.edu.hk 
  
 
Abstract 
Previous researches on Text-level discourse 
parsing mainly made use of constituency 
structure to parse the whole document into 
one discourse tree. In this paper, we present 
the limitations of constituency based dis-
course parsing and first propose to use de-
pendency structure to directly represent the 
relations between elementary discourse 
units (EDUs). The state-of-the-art depend-
ency parsing techniques, the Eisner algo-
rithm and maximum spanning tree (MST) 
algorithm, are adopted to parse an optimal 
discourse dependency tree based on the arc-
factored model and the large-margin learn-
ing techniques. Experiments show that our 
discourse dependency parsers achieve a 
competitive performance on text-level dis-
course parsing.  
1 Introduction 
It is widely agreed that no units of the text can be 
understood in isolation, but in relation to their 
context. Researches in discourse parsing aim to 
acquire such relations in text, which is funda-
mental to many natural language processing ap-
plications such as question answering, automatic 
summarization and so on. 
One important issue behind discourse parsing 
is the representation of discourse structure. Rhe-
torical Structure Theory (RST) (Mann and 
Thompson, 1988), one of the most influential 
discourse theories, posits a hierarchical genera-
tive tree representation, as illustrated in Figure 1. 
The leaves of a tree correspond to contiguous 
text spans called Elementary Discourse Units 
(EDUs)1. The adjacent EDUs are combined into 
                                                          
1 EDU segmentation is a relatively trivial step in discourse 
parsing. Since our work focus here is not EDU segmenta-
tion but discourse parsing. We assume EDUs are already 
known. 
the larger text spans by rhetorical relations (e.g., 
Contrast and Elaboration) and the larger text 
spans continue to be combined until the whole 
text constitutes a parse tree. The text spans 
linked by rhetorical relations are annotated as 
either nucleus or satellite depending on how sali-
ent they are for interpretation. It is attractive and 
challenging to parse the whole text into one tree.  
Since such a hierarchical discourse tree is 
analogous to a constituency based syntactic tree 
except that the constituents in the discourse trees 
are text spans, previous researches have explored 
different constituency based syntactic parsing 
techniques (eg. CKY and chart parsing) and var-
ious features (eg. length, position et al) for dis-
course parsing (Soricut and Marcu, 2003; Joty et 
al., 2012; Reitter, 2003; LeThanh et al, 2004; 
Baldridge and Lascarides, 2005; Subba and Di 
Eugenio, 2009; Sagae, 2009; Hernault et al, 
2010b; Feng and Hirst, 2012). However, the ex-
isting approaches suffer from at least one of the 
following three problems. First, it is difficult to 
design a set of production rules as in syntactic 
parsing, since there are no determinate genera-
tive rules for the interior text spans. Second, the 
different levels of discourse units (e.g. EDUs or 
larger text spans) occurring in the generative 
process are better represented with different fea-
tures, and thus a uniform framework for dis-
course analysis is hard to develop. Third, to 
reduce the time complexity of the state-of-the-art 
constituency based parsing techniques, the ap-
proximate parsing approaches are prone to trap 
in local maximum. 
In this paper, we propose to adopt the depend-
ency structure in discourse representation to 
overcome the limitations mentioned above. Here 
is the basic idea: the discourse structure consists 
of EDUs which are linked by the binary, asym-
metrical relations called dependency relations. A 
dependency relation holds between a subordinate 
EDU called the dependent, and another EDU on 
25
which it depends called the head, as illustrated in 
Figure 2. Each EDU has one head. So, the de-
pendency structure can be seen as a set of head-
dependent links, which are labeled by functional 
relations. Now, we can analyze the relations be-
tween EDUs directly, without worrying about 
any interior text spans. Since dependency trees 
contain much fewer nodes and on average they 
are simpler than constituency based trees, the 
current dependency parsers can have a relatively 
low computational complexity. Moreover, con-
cerning linearization, it is well known that de-
pendency structures can deal with non-projective 
relations, while constituency-based models need 
the addition of complex mechanisms like trans-
formations, movements and so on. In our work, 
we adopt the graph based dependency parsing 
techniques learned from large sets of annotated 
dependency trees. The Eisner (1996) algorithm 
and maximum spanning tree (MST) algorithm 
are used respectively to parse the optimal projec-
tive and non-projective dependency trees with 
the large-margin learning technique (Crammer 
and Singer, 2003). To the best of our knowledge, 
we are the first to apply the dependency structure 
and introduce the dependency parsing techniques 
into discourse analysis.  
The rest of this paper is organized as follows. 
Section 2 formally defines discourse dependency 
structure and introduces how to build a discourse 
dependency treebank from the existing RST cor-
pus. Section 3 presents the discourse parsing ap-
proach based on the Eisner and MST algorithms. 
Section 4 elaborates on the large-margin learning 
technique as well as the features we use. Section 
5 discusses the experimental results. Section 6 
introduces the related work and Section 7 con-
cludes the paper. 
1
e2
e1-e2
*
e3
e1- 2
*
-e3
e1 e2
e1-e2
*
e3
e1-e2-e3
*
e1 e2
e1
*
-e2
e3
e1-e2-e3
*
e1
e2
e1
*
-e2
e3
e1
*
-e2-e3
e1 e2
e2
*
-e3
e3
e1-e2
*
-e3
e1 e2
e3 e1 e2
e3
e1
*
-e2-e3
e2
*
-e3 e2-e3
*
e1
*
-e2-e3
1 2 3 4
5 6 7 8
e1
e2 e3
e1- 2- 3
*
e2-e3
*
 
Figure 1: Headed Constituency based Discourse Tree Structure (e1,e2 and e3 denote three EDUs, 
and * denotes the NUCLEUS constituent) 
e1 e2 e3 e1 e2 e3 e1 e2 e3 e1 e2 e3
e1 e2 e3e1 e2 e3 e1 e2 e3e1 e2 e3
1' 2' 3' 4'
5' 6' 7' 8' 9'
e1 e2 e3
e0e0 e0 e0
e0 e0 e0 e0 e0
 
Figure 2: Discourse Dependency Tree Structures (e1,e2 and e3 denote three EDUS, and the directed 
arcs  denote one dependency relations. The artificial e0 is also displayed here. ) 
2 Discourse Dependency Structure and 
Tree Bank 
2.1 Discourse Dependency Structure 
Similar to the syntactic dependency structure 
defined by McDonald (2005a, 2005b), we insert 
an artificial EDU e0 in the beginning for each 
document and label the dependency relation link-
ing from e0 as ROOT. This treatment will sim-
plify both formal definitions and computational 
implementations. Normally, we assume that each 
EDU should have one and only one head except 
for e0. A labeled directed arc is used to represent 
the dependency relation from one head to its de-
pendent. Then, discourse dependency structure 
can be formalized as the labeled directed graph, 
where nodes correspond to EDUs and labeled 
arcs correspond to labeled dependency relations. 
26
We assume that the text2  T is composed of 
n+1 EDUs including the artificial e0. That is 
T=e0 e1 e2 ? en. Let R={r1,r2, ? ,rm} denote a 
finite set of functional relations that hold be-
tween two EDUs. Then a discourse dependency 
graph can be denoted by G=<V, A> where V de-
notes a set of nodes and A denotes a set of la-
beled directed arcs, such that for the text T=e0 e1 
e2 ? en and the label set R the following holds: 
(1) V = { e0, e1, e2, ? en } 
(2) A ? V? R ? V, where <ei, r, ej>?A represents 
an arc from the head ei to the dependent ej 
labeled with the relation r. 
(3) If <ei, r, ej>?A then <ek, r?, ej>?A for all k?i  
(4) If <ei, r, ej>?A then <ei, r?, ej>?A for all r??r 
The third condition assures that each EDU has 
one and only one head and the fourth tells that 
only one kind of dependency relation holds be-
tween two EDUs. According to the definition, 
we illustrate all the 9 possible unlabeled depend-
ency trees for a text containing three EDUs in 
Figure 2. The dependency trees 1? to 7? are pro-
jective while 8? and 9? are non-projective with 
crossing arcs. 
2.2 Our Discourse Dependency Treebank  
To automatically conduct discourse dependency 
parsing, constructing a discourse dependency 
treebank is fundamental. It is costly to manually 
construct such a treebank from scratch. Fortu-
nately, RST Discourse Treebank (RST-DT) 
(Carlson et al, 2001) is an available resource to 
help with.  
A RST tree constitutes a hierarchical structure 
for one document through rhetorical relations. A 
total of 110 fine-grained relations (e.g. Elabora-
tion-part-whole and List) were used for tagging 
RST-DT. They can be categorized into 18 classes 
(e.g. Elaboration and Joint). All these relations 
can be hypotactic (?mononuclear?) or paratactic 
(?multi-nuclear?). A hypotactic relation holds 
between a nucleus span and an adjacent satellite 
span, while a paratactic relation connects two or 
more equally important adjacent nucleus spans. 
For convenience of computation, we convert the 
n-ary (n>2) RST trees3 to binary trees through 
adding a new node for the latter n-1 nodes and 
assume each relation is connected to only one 
nucleus4. This departure from the original theory 
                                                          
2 The two terms ?text? and ?document? are used inter-
changeably and represent the same meaning. 
3 According to our statistics, there are totally 381 n-ary rela-
tions in RST-DT.  
4 We set the first nucleus as the only nucleus. 
is not such a major step as it may appear, since 
any nucleus is known to contribute to the essen-
tial meaning. Now, each RST tree can be seen as 
a headed constituency based binary tree where 
the nuclei are heads and the children of each 
node are linearly ordered. Given three EDUs5, 
Figure 1 shows the possible 8 headed constituen-
cy based trees where the superscript * denotes 
the heads (nuclei). We use dependency trees to 
simulate the headed constituency based trees.  
Contrasting Figure 1 with Figure 2, we use 
dependency tree 1? to simulate binary trees 1 and 
8, and dependency tress 2?- 7? to simulate binary 
trees 2-7 correspondingly. The rhetorical rela-
tions in RST trees are kept as the functional rela-
tions which link the two EDUs in dependency 
trees. With this kind of conversion, we can get 
our discourse dependency treebank. It is worth 
noting that the non-projective trees like 8? and 9? 
do not exist in our dependency treebank, though 
they are eligible according to the definition of 
discourse dependency graph.  
3 Discourse Dependency Parsing 
3.1 System Overview 
As stated above, T=e0 e1 ?en represents an input 
text (document) where ei denotes the i
th EDU of 
T. We use V to denote all the EDU nodes and 
V?R?V-0 (V-0 =V-{e0}) denote all the possible 
discourse dependency arcs. The goal of discourse 
dependency parsing is to parse an optimal span-
ning tree from V?R?V-0. Here we follow the arc 
factored method and define the score of a de-
pendency tree as the sum of the scores of all the 
arcs in the tree. Thus, the optimal dependency 
tree for T is a spanning tree with the highest 
score and obtained through the function DT(T,w): 
0
0
0
, ,
, ,
( , )( , )
( , , )
( , , )f
T
T
i j T
T
i j T
G V R V
G V R V i j
e r e G
G V R V i j
e r e G
TDT T argmax
argmax e r e
argmax e r
s ore T G
e
c
?
?
?
?
? ? ?
? ? ?
? ??
? ? ?
? ??
?
?
? ?
?
?
w
w
where GT means a possible spanning tree with 
( , )Tscore T G  and ?(       ) denotes the score of 
the arc <ei, r, ej> which is calculated according to 
its feature representation f(ei,r,ej) and a weight 
vector w. 
Next, two basic problems need to be solved: 
how to find the dependency tree with the highest 
                                                          
5 We can easily get al possible headed binary trees for one 
more complex text containing more than three EDUs, by 
extending the 8 possible situations for three EDUs.  
27
score for T given all the arc scores (i.e. a parsing 
problem), and how to learn and compute the 
scores of arcs according to a set of arc features 
(i.e. a learning problem).  
The following of this section addresses the 
first problem. Given the text T, we first reduce 
the multi-digraph composed of all possible arcs 
to the digraph. The digraph keeps only one arc 
<ei, r, ej> between two nodes which satisfies 
?(       )                  . Thus, we can 
proceed with a reduction from labeled parsing to 
unlabeled parsing. Next, two algorithms, i.e. the 
Eisner algorithm and MST algorithm, are pre-
sented to parse the projective and non-projective 
unlabeled dependency trees respectively. 
3.2 Eisner Algorithm 
It is well known that projective dependency pars-
ing can be handled with the Eisner algorithm 
(1996) which is based on the bottom-up dynamic 
programming techniques with the time complexi-
ty of O(n3). The basic idea of the Eisner algo-
rithm is to parse the left and right dependents of 
an EDU independently and combine them at a 
later stage. This reduces the overhead of index-
ing heads. Only two binary variables, i.e. c and d, 
are required to specify whether the heads occur 
leftmost or rightmost and whether an item is 
complete. 
 
Eisner(T,  ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1   Instantiate E[i, i, d, c]=0.0 for all i, d, c 
2   For m := 1 to n 
3       For i := 1 to n 
4          j = i + m 
5          if j> n then break;  
6          # Create subgraphs with c=0 by adding arcs 
7         E[i, j, 0, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ej,ei)) 
8         E[i, j, 1, 0]=maxi?q?j (E[i,q,1,1]+E[q+1,j,0,1]+?(ei,ej)) 
9          # Add corresponding left/right subgraphs 
10        E[i, j, 0, 1]=maxi?q?j (E[i,q,0,1]+E[q,j,0,0] 
11        E[i, j, 1, 1]=maxi?q?j (E[i,q,1,0]+E[q,j,1,1]) 
Figure 3: Eisner Algorithm 
Figure 3 shows the pseudo-code of the Eisner 
algorithm. A dynamic programming table 
E[i,j,d,c] is used to represent the highest scored 
subtree spanning ei to ej. d indicates whether ei is 
the head (d=1) or ej is head (d=0). c indicates 
whether the subtree will not take any more de-
pendents (c=1) or it needs to be completed (c=0). 
The algorithm begins by initializing all length-
one subtrees to a score of 0.0. In the inner loop, 
the first two steps (Lines 7 and 8) are to construct 
the new dependency arcs by taking the maximum 
over all the internal indices (i?q?j) in the span, 
and calculating the value of merging the two sub-
trees and adding one new arc. The last two steps 
(Lines 10 and 11) attempt to achieve an optimal 
left/right subtree in the span by adding the corre-
sponding left/right subtree to the arcs that have 
been added previously. This algorithm considers 
all the possible subtrees. We can then get the 
optimal dependency tree with the score 
E[0,n,1,1] . 
3.3 Maximum Spanning Tree Algorithm  
As the bottom-up Eisner Algorithm must main-
tain the nested structural constraint, it cannot 
parse the non-projective dependency trees like 8? 
and 9? in Figure 2. However, the non-projective 
dependency does exist in real discourse. For ex-
ample, the earlier text mainly talks about the top-
ic A with mentioning the topic B, while the latter 
text gives a supplementary explanation for the 
topic B. This example can constitute a non-
projective tree and its pictorial diagram is exhib-
ited in Figure 4. Following the work of McDon-
ald (2005b), we formalize discourse dependency 
parsing as searching for a maximum spanning 
tree (MST) in a directed graph. 
... ...
A A AB B
...
 
Figure 4: Pictorial Diagram of Non-projective 
Trees 
Chu and Liu (1965) and Edmonds (1967) in-
dependently proposed the virtually identical al-
gorithm named the Chu-Liu/Edmonds algorithm, 
for finding MSTs on directed graphs (McDonald 
et al 2005b). Figure 5 shows the details of the 
Chu-Liu/Edmonds algorithm for discourse pars-
ing. Each node in the graph greedily selects the 
incoming arc with the highest score. If one tree 
results, the algorithm ends. Otherwise, there 
must exist a cycle. The algorithm contracts the 
identified cycle into a single node and recalcu-
lates the scores of the arcs which go in and out of 
the cycle. Next, the algorithm recursively call 
itself on the contracted graph. Finally, those arcs 
which go in or out of one cycle will recover 
themselves to connect with the original nodes in 
V. Like McDonald et al (2005b), we adopt an 
efficient implementation of the Chu-
Liu/Edmonds algorithm that is proposed by Tar-
jan (1997) with O(n2) time complexity. 
 
28
Chu-Liu-Edmonds(G, ?) 
Input: Text T=e0 e1? en; Arc scores ?(ei,ej) 
1      A? = {<ei, ej>| ei = argmax ?(ei,ej); 1?j?|V|} 
2      G? = (V, A?) 
3      If G? has no cycles, then return G?  
4      Find an arc set AC that is a cycle in G? 
5      <GC, ep> = contract(G, AC, ?) 
6      G = (V, A)=Chu-Liu-Edmonds(GC, ?) 
7      For the arc <ei,eC> where ep(ei,eC)=ej: 
8              A=A?AC?{<ei,ej)}-{<ei,eC>, <a(ej),ej>} 
9      For the arc <eC, ei> where ep(eC ,ei)=ej:  
10            A=A?{<ej,ei>}-{<eC,ei>} 
11    V = V 
12    Return G 
Contract(G=(V,A), AC, ?) 
1   Let GC be the subgraph of G excluding nodes in C 
2   Add a node eC to GC denoting the cycle C 
3   For ej ?V-C : ?ei?C <ei,ej>?A 
4        Add arc <eC,ej> to GC with  
ep(eC,ej)=          ?(ei,ej) 
5        ?(eC,ej) = ?(ep(eC,ej),ej) 
6    For ei ?V-C: ?ej?C   (ei,ej)?A 
7         Add arc <ei,eC> to GC with 
                  ep(ei,eC)= =           [?(ei,ej)-?(a(ei),ej)] 
8         ?(ei,eC) =?(ei,ej)-?(a(ei),ej)+score(C) 
9   Return <GC, ep> 
Figure 5: Chu-Liu/Edmonds MST Algorithm 
4 Learning 
In Section 3, we assume that the arc scores are 
available. In fact, the score of each arc is calcu-
lated as a linear combination of feature weights. 
Thus, we need to determine the features for arc 
representation first. With referring to McDonald 
et al (2005a; 2005b), we use the Margin Infused 
Relaxed Algorithm (MIRA) to learn the feature 
weights based on a training set of documents 
annotated with dependency structures ? ?? ? 1, Ni iT ?iy  
where yi denotes the correct dependency tree for 
the text Ti. 
4.1 Features 
Following (Feng and Hirst, 2012; Lin et al, 2009; 
Hernault et al, 2010b), we explore the following 
6 feature types combined with relations to repre-
sent each labeled arc <ei, r, ej> . 
(1) WORD: The first one word, the last one 
word, and the first bigrams in each EDU, the pair 
of the two first words and the pair of the two last 
words in the two EDUs are extracted as features. 
(2) POS: The first one and two POS tags in each 
EDU, and the pair of the two first POS tags in 
the two EDUs are extracted as features. 
(3) Position: These features concern whether the 
two EDUs are included in the same sentence, and 
the positions where the two EDUs are located in 
one sentence, one paragraph, or one document. 
(4) Length: The length of each EDU.  
(5) Syntactic:  POS tags of the dominating nodes 
as defined in Soricut and Marcu (2003) are ex-
tracted as features. We use the syntactic trees 
from the Penn Treebank to find the dominating 
nodes,. 
(6) Semantic similarity: We compute the se-
mantic relatedness between the two EDUs based 
on WordNet. The word pairs are extracted from 
(ei, ej) and their similarity is calculated. Then, we 
can get a weighted complete bipartite graph 
where words are deemed as nodes and similarity 
as weights. From this bipartite graph, we get the 
maximum weighted matching and use the aver-
aged weight of the matches as the similarity be-
tween ei and ej. In particular, we use 
path_similarity, wup_similarity, res_similarity, 
jcn_similarity and lin_similarity provided by the 
nltk.wordnet.similarity (Bird et. al., 2009) pack-
age for calculating word similarity. 
As for relations, we experiment two sets of 
relation labels from RST-DT. One is composed 
of 19 coarse-grained relations and the other 111 
fine-grained relations6.  
4.2 MIRA based Learning 
Margin Infused Relaxed Algorithm (MIRA) is an 
online algorithm for multiclass classification and 
is extended by Taskar et al (2003) to cope with 
structured classification.  
 
MIRA   Input: a training set ? ?? ? 1, Ni iT ?iy  
1      w0 = 0; v = 0; j = 0  
2      For iter := 1 to K 
3            For i := 1 to N 
4                   update w according to ? ?,iT iy : 
1min j j? ?w w
 
                  s.t.  ( , ) ( , ') ( , ')
where  ' ( , )
i i i i i i
j
i i
s T s T L
DT T
? ?
?
y y y y
y w
 
5                      v = v + wj ; 
6                      j = j+1   
7       w = v/(K*N) 
Figure 6: MIRA based Learning 
Figure 6 gives the pseudo-code of the MIRA 
algorithm (McDonld et al, 2005b). This algo-
rithm is designed to update the parameters w us-
ing a single training instance ? ?,iT iy  in each 
iteration. On each update, MIRA attempts to 
keep the norm of the change to the weight vector 
                                                          
6 19 relations include the original 18 relation in RST-DT 
plus one artificial ROOT relation. The 111 relations also 
include the ROOT relation. 
29
as small as possible, which is subject to con-
structing the correct dependency tree under con-
sideration with a margin at least as large as the 
loss of the incorrect dependency trees. We define 
the loss of a discourse dependency tree 'iy  (de-
noted by ( , ')i iL y y  ) as the number of the EDUs 
that have incorrect heads. Since there are expo-
nentially many possible incorrect dependency 
trees and thus exponentially many margin con-
straints, here we relax the optimization and stay 
with a single best dependency tree 
' ( , )ji iDT T?y w  which is parsed under the weight 
vector wj. In this algorithm, the successive up-
dated values of w are accumulated and averaged 
to avoid overfitting.  
5 Experiments 
5.1 Preparation 
We test our methods experimentally using the 
discourse dependency treebank which is built as 
in Section 2. The training part of the corpus is 
composed of 342 documents and contains 18,765 
EDUs, while the test part consists of 38 docu-
ments and 2,346 EDUs. The number of EDUs in 
each document ranges between 2 and 304. Two 
sets of relations are adopted. One is composed of 
19 relations and Table 1 shows the number of 
each relation in the training and test corpus. The 
other is composed of 111 relations. Due to space 
limitation, Table 2 only lists the 10 highest-
distributed relations with regard to their frequen-
cy in the training corpus.  
The following experiments are conducted: (1) 
to measure the parsing performance with differ-
ent relation sets and different feature types; (2) to 
compare our parsing methods with the state-of-
the-art discourse parsing methods.  
 
Relations Train Test Relations Train Test 
Elaboration 6879 796 Temporal 426 73 
Attribution 2641 343 ROOT 342 38 
Joint 1711 212 Compari. 273 29 
Same-unit 1230 127 Condition 258 48 
Contrast 944 146 Manner. 191 27 
Explanation 849 110 Summary 188 32 
Background 786 111 Topic-Cha. 187 13 
Cause 785 82 Textual 147 9 
Evaluation 502 80 TopicCom. 126 24 
Enablement 500 46 Total 18765 2346 
Table 1: Coarse-grained Relation Distribution 
 
 
Relations Train Test 
Elaboration-additional 2912 312 
Attribution 2474 329 
Elaboration-object-attribute-e 2274 250 
List 1690 206 
Same-unit 1230 127 
Elaboration-additional-e 747 69 
Circumstance 545 80 
Explanation-argumentative 524 70 
Purpose 430 43 
Contrast 358 64 
Table 2: 10 Highest Distributed Fine-grained 
Relations 
5.2 Feature Influence on Two Relation Sets 
So far, researches on discourse parsing avoid 
adopting too fine-grained relations and the rela-
tion sets containing around 20 labels are widely 
used. In our experiments, we observe that adopt-
ing a fine-grained relation set can even be helpful 
to building the discourse trees. Here, we conduct 
experiments on two relation sets that contain 19 
and 111 labels respectively. At the same time, 
different feature types are tested their effects on 
discourse parsing.  
Method Features Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3602 0.2651 
1+2+3 0.7310 0.4855 
1+2+3+4 0.7370 0.4868 
1+2+3+4+5 0.7447 0.4957 
1+2+3+4+5+6 0.7455 0.4983 
MST 1+2 0.1957 0.1479 
1+2+3 0.7246 0.4783 
1+2+3+4 0.7280 0.4795 
1+2+3+4+5 0.7340 0.4915 
1+2+3+4+5+6 0.7331 0.4851 
Table 3: Performance Using Coarse-grained Re-
lations. 
Method Feature types Unlabeled 
Acc. 
Labeled 
Acc. 
Eisner 1+2 0.3743 0.2421 
1+2+3 0.7451 0.4079 
1+2+3+4 0.7472 0.4041 
1+2+3+4+5 0.7506 0.4254 
1+2+3+4+5+6 0.7485 0.4288 
MST 1+2 0.2080 0.1300 
1+2+3 0.7366 0.4054 
1+2+3+4 0.7468 0.4071 
1+2+3+4+5 0.7494 0.4288 
1+2+3+4+5+6 0.7460 0.4309 
Table 4: Performance Using Fine-grained Rela-
tions. 
Based on the MIRA leaning algorithm, the 
Eisner algorithm and MST algorithm are used to 
parse the test documents respectively. Referring 
to the evaluation of syntactic dependency parsing, 
30
we use unlabeled accuracy to calculate the ratio 
of EDUs that correctly identify their heads, la-
beled accuracy the ratio of EDUs that have both 
correct heads and correct relations. Table 3 and 
Table 4 show the performance on two relation 
sets. The numbers (1-6) represent the corre-
sponding feature types described in Section 4.1.  
From Table 3 and Table 4, we can see that the 
addition of more feature types, except the 6th fea-
ture type (semantic similarity), can promote the 
performance of relation labeling, whether using 
the coarse-grained 19 relations and the fine-
grained 111 relations. As expected, the first and 
second types of features (WORD and POS) are 
the ones which play an important role in building 
and labeling the discourse dependency trees. 
These two types of features attain similar per-
formance on two relation sets. The Eisner algo-
rithm can achieve unlabeled accuracy around 
0.36 and labeled accuracy around 0.26, while 
MST algorithm achieves unlabeled accuracy 
around 0.20 and labeled accuracy around 0.14. 
The third feature type (Position) is also very 
helpful to discourse parsing. With the addition of 
this feature type, both unlabeled accuracy and 
labeled accuracy exhibit a marked increase. Es-
pecially, when applying MST algorithm on dis-
course parsing, unlabeled accuracy rises from 
around 0.20 to around 0.73. This result is con-
sistent with Hernault?s work (2010b) whose ex-
periments have exhibited the usefulness of those 
position-related features. The other two types of 
features which are related to length and syntactic 
parsing, only promote the performance slightly.  
As we employed the MIRA learning algorithm, 
it is possible to identify which specific features 
are useful, by looking at the weights learned to 
each feature using the training data. Table 5 se-
lects 10 features with the highest weights in ab-
solute value for the parser which uses the coarse-
grained relations, while Table 6 selects the top 
10 features for the parser using the fine-grained 
relations. Each row denotes one feature: the left 
part before the symbol ?&? is from one of the 6 
feature types and the right part denotes a specific 
relation. From Table 5 and Table 6, we can see 
that some features are reasonable. For example, 
The sixth feature in Table 5 represents that the 
dependency relation is preferred to be labeled 
Explanation with the fact that ?because? is the 
first word of the dependent EDU. From these 
two tables, we also observe that most of the 
heavily weighted features are usually related to 
those highly distributed relations. When using 
the coarse-grained relations, the popular relations 
(eg. Elaboration, Attribution and Joint) are al-
ways preferred to be labeled. When using the 
fine-grained relations, the large relations includ-
ing List and Elaboration-object-attribute-e are 
given the precedence of labeling. This phenome-
non is mainly caused by the sparseness of the 
training corpus and the imbalance of relations. 
To solve this problem, the augment of training 
corpus is necessary. 
 
 Feature description Weight 
1 
Last two words in dependent EDU are  
?appeals court?  & Joint 
0.475 
2 
First word in dependent EDU is ?racked? 
& Elaboration 
0.445 
3 
First two words in head EDU are ?I ?d? 
& Attribution 
0.324 
4 
Last word in dependent EDU is ?in?  
& Elaboration 
-0.323 
5 
The res_similarity between two EDUs is 0  
& Elaboration 
0.322 
6 
First word in dependent EDU is ?because? 
& Explanation 
0.306 
7 First POS in head EDU is ?DT? & Joint -0.299 
8 
First two words in dependent EDU are ?that 
required? & Elaboration 
0.287 
9 
First two words in dependent EDU are ?that 
the? & Elaboration 
0.277 
10 
First word in dependent EDU is ?because? 
& Cause 
0.265 
Table 5: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
 Features Weight 
1 Last two words in dependent EDU are ?ap-
peals court?  & List 
0.576 
2 First two words in head EDU are ?I ?d?  
& Attribution 
0.385 
3 First two words in dependent EDU is ?that 
the? & Elaboration-object-attribute-e 
0.348 
4 First POS in head EDU is ?DT? & List -0.323 
5 Last word in dependent EDU is ?in? & List -0.286 
6 First word in dependent EDU is ?racked? & 
Elaboration-object-attribute-e 
0.445 
7 First two word pairs are <?In an?,?But 
even?>  & List 
-0.252 
8 Dependent EDU has a dominating node 
tagged ?CD?& Elaboration-object-attribute-e 
-0.244 
9 First two words in dependent EDU are ?pa-
tents disputes? & Purpose 
0.231 
10 First word in dependent EDU is ?to?  
& Purpose 
0.230 
Table 6: Top 10 Feature Weights for Coarse-
grained Relation Labeling (Eisner Algorithm) 
Unlike previous discourse parsing approaches, 
our methods combine tree building and relation 
labeling into a uniform framework naturally. 
This means that relations play a role in building 
the dependency tree structure. From Table 3 and 
Table 4, we can see that fine-grained relations 
are more helpful to building unlabeled discourse 
31
trees more than the coarse-grained relations. The 
best result of unlabeled accuracy using 111 rela-
tions is 0.7506, better than the best performance 
(0.7447) using 19 relations. We can also see that 
the labeled accuracy using the fine-grained rela-
tions can achieve 0.4309, only 0.06 lower than 
the best labeled accuracy (0.4915) using the 
coarse-grained relations. 
In addition, comparing the MST algorithm 
with the Eisner algorithm, Table 3 and Table 4 
show that their performances are not significant-
ly different from each other. But we think that 
MST algorithm has more potential in discourse 
dependency parsing, because our converted dis-
course dependency treebank contains only pro-
jective trees and somewhat suppresses the MST 
algorithm to exhibit its advantage of parsing non-
projective trees. In fact, we observe that some 
non-projective dependencies produced by the 
MST algorithm are even reasonable than what 
they are in the dependency treebank. Thus, it is 
important to build a manually labeled discourse 
dependency treebank, which will be our future 
work. 
5.3 Comparison with Other Systems  
The state-of-the-art discourse parsing methods 
normally produce the constituency based dis-
course trees. To comprehensively evaluate the 
performance of a labeled constituency tree, the 
blank tree structure (?S?), the tree structure with 
nuclearity indication (?N?), and the tree structure 
with rhetorical relation indication but no nuclear-
ity indication (?R?) are evaluated respectively 
using the F measure (Marcu 2000).  
To compare our discourse parsers with others, 
we adopt MIRA and Eisner algorithm to conduct 
discourse parsing with all the 6 types of features 
and then convert the produced projective de-
pendency trees to constituency based trees 
through their correspondence as stated in Section 
2. Our parsers using two relation sets are named 
Our-coarse and Our-fine respectively. The in-
putted EDUs of our parsers are from the standard 
segmentation of RST-DT. Other text-level dis-
course parsing methods include: (1) Percep-
coarse: we replace MIRA with the averaged per-
ceptron learning algorithm and the other settings 
are the same with Our-coarse; (2) HILDA-
manual and HILDA-seg are from Hernault 
(2010b)?s work, and their inputted EDUs are 
from RST-DT and their own EDU segmenter 
respectively; (3) LeThanh indicates the results 
given by LeThanh el al. (2004), which built a 
multi-level rule based parser and used 14 rela-
tions evaluated on 21 documents from RST-DT; 
(4) Marcu denotes the results given by Mar-
cu(2000)?s decision-tree based parser which used 
15 relations evaluated on unspecified documents.  
Table 7 shows the performance comparison 
for all the parsers mentioned above. Human de-
notes the manual agreement between two human 
annotators. From this table, we can see that both 
our parsers perform better than all the other 
parsers as a whole, though our parsers are not 
developed directly for constituency based trees. 
Our parsers do not exhibit obvious advantage 
than HILDA-manual on labeling the blank tree 
structure, because our parsers and HILDA-
manual all perform over 94% of Human and this 
performance level somewhat reaches a bottle-
neck to promote more. However, our parsers 
outperform the other parsers on both nuclearity 
and relation labeling. Our-coarse achieves 94.2% 
and 91.8% of the human F-scores, on labeling 
nuclearity and relation respectively, while Our-
fine achieves 95.2% and 87.6%. We can also see 
that the averaged perceptron learning algorithm, 
though simple, can achieve a comparable per-
formance, better than HILDA-manual. The 
parsers HILDA-seg, LeThanh and Marcu use 
their own automatic EDU segmenters and exhibit 
a relatively low performance. This means that 
EDU segmentation is important to a practical 
discourse parser and worth further investigation. 
  
 S N R 
Our-coarse 82.9 73.0 60.6 
Our-fine 83.4 73.8 57.8 
Percep-coarse 82.3 72.6 59.4 
HILDA-manual 83.0 68.4 55.3 
HILDA-seg 72.3 59.1 47.8 
LeThanh 53.7 47.1 39.9 
Marcu 44.8 30.9 18.8 
Human 88.1 77.5 66.0 
Table 7: Full Parser Evaluation 
 MAFS WAFS Acc 
Our-coarse 0.454 0.643 66.84 
Percep-coarse 0.438 0.633 65.37 
Feng 0.440 0.607 65.30 
HILDA-manual 0.428 0.604 64.18 
Baseline - - 35.82 
Table 8: Relation Labeling Performance  
To further compare the performance of rela-
tion labeling, we follow Hernault el al. (2010a) 
and use Macro-averaged F-score (MAFS) to 
evaluate each relation. Due to space limitation, 
we do not list the F scores for each relation. 
Macro-averaged F-score is not influenced by the 
number of instances that are contained in each 
32
relation. Weight-averaged F-score (WAFS) 
weights the performance of each relation by the 
number of its existing instances. Table 8 com-
pares our parser Our-coarse with other parsers 
HILDA-manual, Feng (Feng and Hirst, 2012) 
and Baseline. Feng (Feng and Hirst, 2012) can 
be seen as a strengthened version of HILDA 
which adopts more features and conducts feature 
selection. Baseline always picks the most fre-
quent relation (i.e. Elaboration). From the results, 
we find that Our-coarse consistently provides 
superior performance for most relations over 
other parsers, and therefore results in higher 
MAFS and WAFS.  
6 Related Work 
So far, the existing discourse parsing techniques 
are mainly based on two well-known treebanks. 
One is the Penn Discourse TreeBank (PDTB) 
(Prasad et al, 2007) and the other is RST-DT.  
PDTB adopts the predicate-arguments repre-
sentation by taking an implicit/explicit connec-
tive as a predication of two adjacent sentences 
(arguments). Then the discourse relation between 
each pair of sentences is annotated independently 
to characterize its predication. A majority of re-
searches regard discourse parsing as a classifica-
tion task and mainly focus on exploiting various 
linguistic features and classifiers when using 
PDTB (Wellner et al, 2006; Pitler et al, 2009; 
Wang et al, 2010). However, the predicate-
arguments annotation scheme itself has such a 
limitation that one can only obtain the local dis-
course relations without knowing the rich context. 
In contrast, RST and its treebank enable peo-
ple to derive a complete representation of the 
whole discourse. Researches have begun to in-
vestigate how to construct a RST tree for the 
given text. Since the RST tree is similar to the 
constituency based syntactic tree except that the 
constituent nodes are different, the syntactic 
parsing techniques have been borrowed for dis-
course parsing (Soricut and Marcu, 2003; 
Baldridge and Lascarides, 2005; Sagae, 2009; 
Hernault et al, 2010b; Feng and Hirst, 2012). 
Soricut and Marcu (2003) use a standard bottom-
up chart parsing algorithm to determine the dis-
course structure of sentences. Baldridge and Las-
carides (2005) model the process of discourse 
parsing with the probabilistic head driven parsing 
techniques. Sagae (2009) apply a transition based 
constituent parsing approach to construct a RST 
tree for a document. Hernault et al (2010b) de-
velop a greedy bottom-up tree building strategy 
for discourse parsing. The two adjacent text 
spans with the closest relations are combined in 
each iteration. As the extension of Hernault?s 
work, Feng and Hirst (2012) further explore var-
ious features aiming to achieve better perfor-
mance. However, as analyzed in Section 1, there 
exist three limitations with the constituency 
based discourse representation and parsing. We 
innovatively adopt the dependency structure, 
which can be benefited from the existing RST-
DT, to represent the discourse. To the best of our 
knowledge, this work is the first to apply de-
pendency structure and dependency parsing 
techniques in discourse analysis. 
7 Conclusions 
In this paper, we present the benefits and feasi-
bility of applying dependency structure in text-
level discourse parsing. Through the correspond-
ence between constituency-based trees and de-
pendency trees, we build a discourse dependency 
treebank by converting the existing RST-DT. 
Based on dependency structure, we are able to 
directly analyze the relations between the EDUs 
without worrying about the additional interior 
text spans, and apply the existing state-of-the-art 
dependency parsing techniques which have a 
relatively low time complexity. In our work, we 
use the graph based dependency parsing tech-
niques learned from the annotated dependency 
trees. The Eisner algorithm and the MST algo-
rithm are applied to parse the optimal projective 
and non-projective dependency trees respectively 
based on the arc-factored model. To calculate the 
score for each arc, six types of features are ex-
plored to represent the arcs and the feature 
weights are learned based on the MIRA learning 
technique. Experimental results exhibit the effec-
tiveness of the proposed approaches. In the fu-
ture, we will focus on non-projective discourse 
dependency parsing and explore more effective 
features. 
Acknowledgments 
This work was partially supported by National 
High Technology Research and Development 
Program of China (No. 2012AA011101), Na-
tional Key Basic Research Program of China (No. 
2014CB340504), National Natural Science 
Foundation of China (No. 61273278), and Na-
tional Key Technology R&D Program (No: 
2011BAH10B04-03). We also thank the three 
anonymous reviewers for their helpful comments. 
33
References 
Jason Baldridge and Alex Lascarides. 2005. Probabil-
istic Head-driven Parsing for Discourse Structure. 
In Proceedings of the Ninth Conference on Com-
putational Natural Language Learning, pages 96?
103. 
Steven Bird, Ewan Klein, and Edward Loper. 2009. 
Natural Language Processing with Python ? Ana-
lyzing Text with the Natural Language Toolkit. 
O?Reilly. 
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 
2001. Building a Discourse-tagged Corpus in the 
Framework of Rhetorical Structure Theory. Pro-
ceedings of the Second SIGdial Workshop on Dis-
course and Dialogue-Volume 16, pages 1?10. 
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On the 
Shortest Arborescence of a Directed Graph, Sci-
ence Sinica, v.14, pp.1396-1400.  
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative Online Algorithms for Multiclass Prob-
lems. JMLR. 
Jack Edmonds. 1967. Optimum Branchings, J. Re-
search of the National Bureau of Standards, 71B, 
pp.233-240.  
Jason Eisner. 1996. Three New Probabilistic Models 
for Dependency Parsing: An Exploration. In Proc. 
COLING. 
Vanessa Wei Feng and Graeme Hirst. Text-level Dis-
course Parsing with Rich Linguistic Features, Pro-
ceedings of the 50th Annual Meeting of the 
Association for Computational Linguistics, pages 
60?68, Jeju, Republic of Korea, 8-14 July 2012. 
Hugo Hernault, Danushka Bollegala, and Mitsuru 
Ishizuka. 2010a. A Semi-supervised Approach to 
Improve Classification of Infrequent Discourse Re-
lations Using Feature Vector Extension. In Pro-
ceedings of the 2010 Conference on Empirical 
Methods in Natural Language Processing, pages 
399?409, Cambridge, MA, October. Association 
for Computational Linguistics. 
Hugo Hernault, Helmut Prendinger, David A. duVerle, 
and Mitsuru Ishizuka. 2010b. HILDA: A Discourse 
Parser Using Support Vector Machine Classifica-
tion. Dialogue and Discourse, 1(3):1?33. 
Shafiq Joty, Giuseppe Carenini and Raymond T. Ng. 
A Novel Discriminative Framework for Sentence-
level Discourse Analysis. EMNLP-CoNLL '12 
Proceedings of the 2012 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and Computational Natural Language Learning 
Stroudsburg, PA, USA. 
Huong LeThanh, Geetha Abeysinghe, and Christian 
Huyck. 2004. Generating Discourse Structures for 
Written Texts. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics, 
pages 329? 335. 
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. 
Recognizing Implicit Discourse Relations in the 
Penn Discourse Treebank. In Proceedings of the 
2009 Conference on Empirical Method in Natural 
Language Processing, Vol. 1, EMNLP?09, pages 
343-351. 
William Mann and Sandra Thompson. 1988. Rhetori-
cal Structure Theory: Toward a Functional Theory 
of Text Organization. Text, 8(3):243?281. 
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press, 
Cambridge, MA, USA. 
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005a. Online Large-Margin Training of De-
pendency Parsers, 43rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
2005) .  
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and 
Jan Hajic. 2005b. Non-projective Dependency 
Parsing using Spanning Tree Algorithms, Proceed-
ings of HLT/EMNLP 2005. 
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. 
Automatic Sense Prediction for Implicit Discourse 
Relations in Text, In Proc. of the 47th ACL. pages 
683-691.  
Rashmi Prasad, Eleni Miltsakaki, Nikhil Dinesh, Alan 
Lee, Aravind Joshi, Livio Robaldo, and Bonnie 
Webber. 2007. The Penn Discourse Treebank 2.0 
Annotation Manual. The PDTB Research Group, 
December. 
David Reitter. 2003. Simple Signals for Complex 
Rhetorics: On Rhetorical Analysis with Rich-
feature Support Vector Models. LDV Forum, 
18(1/2):38?52. 
Kenji Sagae. 2009. Analysis of discourse structure 
with syntactic dependencies and data-driven shift-
reduce parsing. In Proceedings of the 11th Interna-
tional Conference on Parsing Technologies, pages 
81-84. 
Radu Soricut and Daniel Marcu. 2003. Sentence level 
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference 
34
of the North American Chapter of the Association 
for Computational Linguistics on Human Lan-
guage Technology, Volume 1, pages 149?156. 
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic in-
formation. In Proceedings of Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics, pages 566?574. 
Robert Endre Tarjan, 1977. Finding Optimum 
Branchings, Networks, v.7, pp.25-35. 
Ben Taskar, Carlos Guestrin and Daphne Koller. 2003. 
Max-margin Markov Networks. In Proc. NIPS. 
Bonnie Webber. 2004. D-LTAG: Extending Lexical-
ized TAG to Discourse. Cognitive Science, 
28(5):751?779. 
Wen Ting Wang, Jian Su and Chew Lim Tan. 2010. 
Kernel based Discourse Relation Recognition with 
Temporal Ordering Information, In Proc. of 
ACL?10. pages 710-719. 
Ben Wellner, James Pustejovsky, Catherine Havasi, 
Anna Rumshisky and Roser Sauri. 2006. Classifi-
cation of Discourse Coherence Relations: an Ex-
ploratory Study Using Multiple Knowledge 
Sources. In Proc.of the 7th SIGDIAL Workshop on 
Discourse and Dialogue. pages 117-125. 
 
35
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 142?145,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
273. Task 5. Keyphrase Extraction Based on Core Word                 
Identification and Word Expansion 
You Ouyang        Wenjie Li        Renxian Zhang 
The Hong Kong Polytechnic University 
{csyouyang,cswjli,csrzhang}@comp.polyu.edu.hk 
Abstract 
This paper provides a description of the Hong 
Kong Polytechnic University (PolyU) System 
that participated in the task #5 of SemEval-2, 
i.e., the Automatic Keyphrase Extraction from 
Scientific Articles task. We followed a novel 
framework to develop our keyphrase 
extraction system, motivated by differentiating 
the roles of the words in a keyphrase. We first 
identified the core words which are defined as 
the most essential words in the article, and 
then expanded the identified core words to the 
target keyphrases by a word expansion 
approach.  
1 Introduction 
The task #5 in SemEval-2 requires extracting the 
keyphrases for scientific articles. According to 
the task definition, keyphrases are the words that 
capture the main topic of the given document. 
Currently, keyphrase extraction is usually carried 
out by a two-stage process, including candidate 
phrase identification and key phrase selection. 
The first stage is to identify the candidate phrases 
that are potential keyphrases. Usually, it is 
implemented as a process that filters out the 
obviously unimportant phrases. After the 
candidate identification stage, the target 
keyphrases can then be selected from the 
candidates according to their importance scores, 
which are usually estimated by some features, 
such as word frequencies, phrase frequencies, 
POS-tags, etc.. The features can be combined 
either by heuristics or by learning models to 
obtain the final selection strategy. 
In most existing keyphrase extraction methods, 
the importance of a phrase is estimated by a 
composite score of the features. Different 
features indicate preferences to phrases with 
specific characteristics. As to the common 
features, the phrases that consist of important and 
correlated words are usually preferred. Moreover, 
it is indeed implied in these features that the 
words are uniform in the phrase, that is, their 
degrees of importance are evaluated by the same 
criteria. However, we think that this may not 
always be true. For example, in the phrase ?video 
encoding/decoding?, the word ?video? appears 
frequently in the article and thus can be easily 
identified by simple features, while the word 
?encoding/decoding? is very rare and thus is very 
hard to discover. Therefore, a uniform view on 
the words is not able to discover this kind of 
keyphrases. On the other hand, we observe that 
there is usually at least one word in a keyphrase 
which is very important to the article, such as the 
word ?video? in the above example. In this paper, 
we call this kind of words core words. For each 
phrase, there may be one or more core words in 
it, which serve as the core component of the 
phrase. Moreover, the phrase may contain some 
words that support the core words, such as 
?encoding/decoding? in the above example. 
These words may be less important to the article, 
but they are highly correlated with the core word 
and are able to form an integrated concept with 
the core words. Motivated by this, we consider a 
new keyphrase extraction framework, which 
includes two stages: identifying the core words 
and expanding the core words to keyphrases. The 
methodology of the proposed approaches and the 
performance of the resulting system are 
introduced below. We also provide further 
discussions and modifications.  
2 Methodology 
According to our motivation, our extraction 
framework consists of three processes, including 
(1) The pre-processing to obtain the necessary 
information for the following processes; 
(2) The core word identification process to 
discover the core words to be expanded; 
(3) The word expansion process to generate the 
final keyphrases.  
 In the pre-processing, we first identify the text 
fields for each scientific article, including its title, 
abstract and main text (defined as all the section 
titles and section contents). The texts are then 
processed by the language toolkit GATE 1  to 
carry out sentence segmentation, word stemming 
and POS (part-of-speech) tagging. Stop-words 
                                                 
1 Publicly available at http://gate.ac.uk/gate 
142
are not considered to be parts of the target 
keyphrases. 
2.1 Core Word Identification 
Core words are the words that represent the 
dominant concepts in the article. To identify the 
core words, we consider the features below.  
Frequencies: In a science article, the words with 
higher frequencies are usually more important. 
To differentiate the text fields, in our system we 
consider three frequency-based features, i.e., 
Title-Frequency (TF), Abstract-Frequency 
(AF) and MainText-Frequency (MF), to 
represent the frequencies of one word in different 
text fields. For a word w in an article t, the 
frequencies are denoted by 
TF(w) = Frequency of  w in the title of t;  
AF(w) = Frequency of w in the abstract of t;  
MF(w) = Frequency of w in the main text of t. 
POS tag: The part-of-speech tag of a word is a 
good indicator of core words. Here we adopt a 
simple constraint, i.e., only nouns or adjectives 
can be potential core words. 
In our system, we use a progressive algorithm 
to identify all the core words. The effects of 
different text fields are considered to improve the 
accuracy of the identification result. First of all, 
for each word w in the title, it is identified to be a 
core word when satisfying  
{ TF(w)> 0 ? AF(w) > 0 } 
Since the abstract is usually less indicative 
than the title, we use stricter conditions for the 
words in the abstract by considering their co-
occurrence with the already-identified core 
words in the title. For a word w in the abstract, a 
co-occurrence-based feature COT(w) is defined 
as |S(w)|, where S(w) is the set of sentences 
which contain both w and at least one title core 
word. For a word w in the abstract, it is identified 
as an abstract core word when satisfying 
{ AF(w)> 0 ? MF(w) > ?1 ? COT (w) > ?2} 
Similarly, for a word w in the main text, it is 
identified as a general core word when satisfying 
{ MF(w) > ?1 ? COTA (w) >?2} 
where COTA (w) = |S?(w)| and S?(w) is the set of 
sentences which contain both w and at least one 
identified title core word or abstract core word. 
With this progressive algorithm, new core 
words can be more accurately identified with the 
previously identified core words. In the above 
heuristics, the parameters ? and ? are pre-defined 
thresholds, which are manually assigned2.  
                                                 
2 (?1, ?2, ?1, ?2) = (10, 5, 20, 10) in the system 
As a matter of fact, this heuristic-based 
identification approach is simple and preliminary. 
More sophisticated approaches, such as training 
machine learning models to classify the words, 
can be applied for better performance. Moreover, 
more useful features can also be considered. 
Nevertheless, we adopted the heuristic-based 
implementation to test the applicability of the 
framework as an initial study.  
An example of the identified core words is 
illustrated in Table 1 below: 
Type Core Word 
Title grid, service, discovery, UDDI 
Abstract distributed, multiple, web, computing, 
registry, deployment, scalability, DHT, 
DUDE, architecture 
Main proxy, search, node, key, etc. 
Table 1: Different types of core words 
2.2 Core Word Expansion 
Given the identified core words, the keyphrases 
can then be generated by expanding the core 
words. An example of the expansion process is 
illustrated below as 
grid ? grid service ? grid service discovery ? 
scalable grid service discovery  
For a core word, each appearance of it can be 
viewed as a potential expanding point. For each 
expanding point of the word, we need to judge if 
the context words can form a keyphrase along 
with it. Formally, for a candidate word w and the 
current phrase e (here we assume that w is the 
previous word, the case for the next word is 
similar), we consider the following features to 
judge if e should be expanded to w+e. 
Frequencies: the frequency of w (denoted by 
Freq(w)) and the frequency of the combination 
of w and e (denoted by phraseFreq(w, e)) which 
reflects the degree of w and e forming an 
integrated phrase. 
POS pattern: The part-of-speech tag of the 
word w is also considered here, i.e., we only try 
to expand w to w+e when w is a noun, an 
adjective or the specific conjunction ?of?. 
A heuristic-based approach is adopted here 
again. We intend to define some loose heuristics, 
which prefer long keyphrases. The heuristics 
include (1) If w and e are in the title or abstract, 
expand e to e+w when w satisfies the POS 
constraint and Freq(w) > 1; (2) If w and e are in 
the main text, expand e to e+w when w satisfies 
the POS constraint and phraseFreq(w, e) >1.  
More examples are provided in Table 2 below. 
 
 
143
Core Word Expanded Key Phrase 
grid scalable grid service discovery, 
grid computing 
UDDI UDDI registry, UDDI key 
web web service,  
scalability Scalability issue 
DHT DHT node 
Table 2: Core words and corresponding key phrases 
3 Results 
3.1 The Initial PolyU System in SemEval-2 
In the Semeval-2 test set, a total of 100 articles 
are provided. Systems are required to generate 15 
keyphrases for each article. Also, 15 keyphrases 
are generated by human readers as standard 
answers. Precision, recall and F-value are used to 
evaluate the performance. 
To generate exactly 15 keyphrases with the 
framework, we expand the core words in the title, 
abstract and main text in turn. Moreover, the core 
words in one fixed field are expanded following 
the descending order of frequency. When 15 
keyphrases are obtained, the process is stopped.  
For each new phrase, a redundancy check is 
also conducted to make sure that the final 15 
keyphrases can best cover the core concepts of 
the article, i.e.,  
(1) the new keyphrase should contain at least one 
word that is not included in any of the selected 
keyphrases; 
(2) if a selected keyphrase is totally covered by 
the new keyphrase, the covered keyphrase will 
be substituted by the new keyphrase. 
    The resulting system based on the above 
method is the one we submitted to SemEval-2. 
3.2 Phrase Filtering and Ranking 
Initially, we intend to use just the proposed 
framework to develop our system, i.e., using the 
expanded phrases as the keyphrases. However, 
we find out later that it must be adjusted to suit 
the requirement of the SemEval-2 task. In our 
subsequent study, we consider two adjustments, 
i.e., phrase filtering and phrase ranking.  
In SemEval-2, the evaluation criteria require 
exact match between the phrases. A phrase that 
covers a reference keyphrase but is not equal to it 
will not be counted as a successful match. For 
example, the candidate phrase ?scalable grid 
service discovery? is not counted as a match 
when compared to the reference keyphrase ?grid 
service discovery?. We call this the ?partial 
matching problem?. In our original framework, 
we followed the idea of ?expanding the phrase as 
much as possible? and adopted loose conditions. 
Consequently, the partial matching problem is 
indeed very serious. This unavoidably affects its 
performance under the criteria in SemEval-2 that 
requires exact matches. Therefore, we consider a 
simple filtering strategy here, i.e., filtering any 
keyphrase which only appears once in the article.  
Another issue is that the given task requires a 
total of exactly 15 keyphrases. Naturally we need 
a selection process to handle this. As to our 
framework, a keyphrase ranking process is 
necessary for discovering the best 15 keyphrases, 
not the best 15 core words. For this reason, we 
also try a simple method that re-ranks the 
expanded phrases by their frequencies. The top 
15 phrases are then selected finally. 
3.3 Results 
Table 3 below shows the precision, recall and F-
value of our submitted system (PolyU), the best 
and worst systems submitted to SemEval-2 and 
the baseline system that uses simple TF-IDF 
statistics to select keyphrases. 
On the SemEval-2 test data, the performance 
of the PolyU system was not good, just a little 
better than the baseline. A reason is that we just 
developed the PolyU system with our past 
experiences but did not adjust it much for better 
performance (since we were focusing on 
designing the new framework). After the 
competition, we examined two refined systems 
with the methods introduced in section 3.2. 
First, the PolyU system is adapted with the 
phrase filtering method. The performance of the 
resulting system (denoted by PolyU+) is given in 
Table 4. As shown in Table 4, the performance is 
much better just with this simple refinement to 
meet the requirement on extract matches for the 
evaluation criteria. Then, the phrase ranking 
method is also incorporated into the system. The 
performance of the resulting system (denoted by 
PolyU++) is also provided in Table 4. The 
performance is again much improved with the 
phrase ranking process. 
3.4 Discussion 
In our participation in SemEval-2, we submitted 
the PolyU system with the proposed extraction 
framework, which is based on expanding the 
core words to keyphrases. However, the PolyU 
system did not perform well in SemEval-2. 
However, we also showed later that the 
framework can be much improved after some
144
Simple but necessary refinements are made 
according to the given task. The final PolyU++ 
system with two simple refinements is much 
better. These refinements, including phrase 
filtering and ranking, are similar to traditional 
techniques. So it seems that our expansion-based 
framework is more applicable along with some 
traditional techniques. Though this conflicts our 
initial objective to develop a totally novel 
framework, the framework shows its ability of 
finding those keyphrases which contain different 
types of words. As to the PolyU++ system, when 
adapted with just two very simple post-
processing methods, the extracted candidate 
phrases can already perform quite well in 
SemEval-2. This may suggest that the framework 
can be considered as a new way for candidate 
keyphrase identification for the traditional 
extraction process. 
4 Conclusion and future work 
In this paper, we introduced our system in our 
participation in SemEval-2. We proposed a new 
framework for the keyphrase extraction task, 
which is based on expanding core words to 
keyphrases. Heuristic approaches are developed 
to implement the framework. We also analyzed 
the errors of the system in SemEval-2 and 
conducted some refinements. Finally, we 
concluded that the framework is indeed 
appropriate as a candidate phrase identification 
method. Another issue is that we just consider 
some simple information such as frequency or 
POS tag in this initial study. This indeed limits 
the power of the resulting systems. In future 
work, we?d like to develop more sophisticated 
implementations to testify the effectiveness of 
the framework. More syntactic and semantic 
features should be considered. Also, learning 
models can be applied to improve both the core 
word identification approach and the word 
expansion approach. 
 
Acknowledgments 
The work described in this paper is supported by 
Hong Kong RGC Projects (PolyU5217/07E and 
PolyU5230/08E). 
References  
Frank, E., Paynter, G.W., Witten, I., Gutwin, C. and 
Nevill-Manning, C.G.. 1999. Domain Specific 
Keyphrase Extraction. Proceedings of the IJCAI 
1999, pp.668--673. 
Medelyan, O. and Witten, I. H.. 2006. Thesaurus 
based automatic keyphrase indexing. Proceedings 
of the JCDL 2006, Chapel Hill, NC, USA. 
Medelyan, O. and Witten, I. H.. 2008. Domain 
independent automatic keyphrase indexing with 
small training sets. Journal of American Society for 
Information Science and Technology. Vol. 59 (7), 
pp. 1026-1040 
SemEval-2. Evaluation Exercises on Semantic 
Evaluation. http://semeval2.fbk.eu/ 
Turney, P.. 1999. Learning to Extract Keyphrases 
from Text. National Research Council, Institute for 
Information Technology, Technical Report ERB-
1057. (NRC \#41622), 1999. 
Wan, X. Xiao, J.. 2008. Single document keyphrase 
extraction using neighborhood knowledge. In 
Proceedings of AAAI 2008, pp 885-860. 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
Best 34.6% 14.4% 20.3% 26.1% 21.7% 23.7% 21.5% 26.7% 23.8%
Worst 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
Baseline 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
Table 3: Results from SemEval-2 
 
System 5 Keyphrases 10 Keyphrases 15 Keyphrases P R F P R F P R F 
PolyU 13.6% 5.65% 7.98% 12.6% 10.5% 11.4% 12.0% 15.0% 13.3%
PolyU+ 21.2% 8.8% 12.4% 16.9% 14.0% 15.3% 13.9% 17.3% 15.4%
PolyU++ 31.2% 13.0% 18.3% 22.1% 18.4% 20.1% 20.3% 20.6% 20.5%
Table 4: The performance of the refined systems 
 
145
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 102?109,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Using Deep Belief Nets for Chinese Named Entity Categorization 
Yu Chen1, You Ouyang2, Wenjie Li2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{csyouyang, cswjli}@comp.polyu.edu.hk
Abstract 
Identifying named entities is essential in 
understanding plain texts. Moreover, the 
categories of the named entities are indicative 
of their roles in the texts. In this paper, we 
propose a novel approach, Deep Belief Nets 
(DBN), for the Chinese entity mention 
categorization problem. DBN has very strong 
representation power and it is able to 
elaborately self-train for discovering 
complicated feature combinations. The 
experiments conducted on the Automatic 
Context Extraction (ACE) 2004 data set 
demonstrate the effectiveness of DBN. It 
outperforms the state-of-the-art learning 
models such as SVM or BP neural network. 
1 Introduction 
Named entities (NE) are defined as the names of 
existing objects, such as persons, organizations 
and etc. Identifying NEs in plain texts provides 
structured information for semantic analysis. 
Hence the named entity recognition (NER) task 
is a fundamental task for a wide variety of 
natural language processing applications, such as 
question answering, information retrieval and etc. 
In a text, an entity may either be referred to by a 
common noun, a noun phrase, or a pronoun. 
Each reference of the entity is called a mention. 
NER indeed requires the systems to identify 
these entity mentions from plain texts. The task 
can be decomposed into two sub-tasks, i.e., the 
identification of the entities in the text and the 
classification of the entities into a set of pre-
defined categories. In the study of this paper, we 
focus on the second sub-task and assume that the 
boundaries of all the entity mentions to be 
categorized are already correctly identified. 
In early times, NER systems are mainly based 
on handcrafted rule-based approaches. Although 
rule-based approaches achieved reasonably good 
results, they have some obvious flaws. First, they 
require exhausted handcraft work to construct a 
proper and complete rule set, which partially 
expressing the meaning of entity. Moreover, 
once the interest of task is transferred to a 
different domain or language, rules have to be 
revised or even rewritten. The discovered rules 
are indeed heavily dependent on the task 
interests and the particular corpus. Finally, the 
manually-formatted rules are usually incomplete 
and their qualities are not guaranteed. 
Recently, more attentions are switched to the 
applications of machine learning models with 
statistic information. In this camp, entity 
categorization is typically cast as a multi-class 
classification process, where the named entities 
are represented by feature vectors. Usually, the 
vectors are abstracted by some lexical and 
syntactic features instead of semantic feature. 
Many learning models, such as Support Vector 
Machine (SVM) and Neural Network (NN), are 
then used to classify the entities by their feature 
vectors. 
Entity categorization in Chinese attracted less 
attention when compared to English or other 
western languages. This is mainly because the 
unique characteristics of Chinese. One of the 
most common problems is the lack of boundary 
information in Chinese texts. For this problem, 
character-based methods are reported to be a 
possible substitution of word-based methods. As 
to character-based methods, it is important to 
study the implicit combination of characters.  
In our study, we explore the use of Deep 
Belief Net (DBN) in character-based entity 
categorization. DBN is a neural network model 
which is developed under the deep learning 
architecture. It is claimed to be able to 
automatically learn a deep hierarchy of the input 
features with increasing levels of abstraction for 
the complex problem. In our problem, DBN is 
used to automatically discover the complicated 
composite effects of the characters to the NE 
categories from the input data. With DBN, we 
need not to manually construct the character 
combination features for expressing the semantic 
relationship among characters in entities. 
Moreover, the deep structure of DBN enables the 
possibility of discovering very sophisticated 
102
combinations of the characters, which may even 
be hard to discover by human. 
The rest of this paper is organized as follow. 
Section 2 reviews the related work on name 
entity categorization. Section 3 introduces the 
methodology of the proposed approach. Section 
4 provides the experimental results. Finally, 
section 5 concludes the whole paper. 
2 Related work 
Over the past decades, NER has evolved from 
simple rule-based approaches to adapted self-
training machine learning approaches. 
As early rule-based approaches, MacDonald 
(1993) utilized local context, which implicate 
internal and external evidence, to aid on 
categorization. Wacholder (1997) employed an 
aggregation of classification method to capture 
internal rules. Both used hand-written rules and 
knowledge bases. Later, Collins (1999) adopted 
the AdaBoost algorithm to find a weighted 
combination of simple classifiers. They reported 
that the combination of simple classifiers can 
yield some powerful systems with much better 
performances. As a matter of fact, these methods 
all need manual studies on the construction of the 
rule set or the simple classifiers. 
Machine learning models attract more 
attentions recently. Usually, they train 
classification models based on context features. 
Various lexical and syntactic features are 
considered, such as N-grams, Part-Of-Speech 
(POS), and etc. Zhou and Su (2002) integrated 
four different kinds of features, which convey 
different semantic information, for a 
classification model based on the Hidden 
Markov Model (HMM). Koen (2006) built a 
classifier with the Conditional Random Field 
(CRF) model to classify noun phrases in a text 
with the WordNet SynSet. Isozaki and Kazawa 
(2002) studied the use of SVM instead. 
There were fewer studies in Chinese entity 
categorization. Guo and Jiang (2005) applied 
Robust Risk Minimization to classify the named 
entities. The features include seven traditional 
lexical features and two external-NE-hints based 
features. An important result they reported is that 
character-based features can be as good as word-
based features since they avoid the Chinese word 
segmentation errors. In (Jing et al, 2003), it was 
further reported that pure character-based models 
can even outperform word-based models with 
character combination features.  
Deep Belief Net is introduced in (Hinton et al, 
2006). According to their definition, DBN is a 
deep neural network that consists of one or more 
Restricted Boltzmann Machine (RBM) layers 
and a Back Propagation (BP) layer. This multi-
layer structure leads to a strong representation 
power of DBN. Moreover, DBN is quite efficient 
by using RBM to implement the middle layers, 
since RBM can be learned very quickly by the 
Contrastive Divergence (CD) approach. 
Therefore, we believe that DBN is very suitable 
for the character-level Chinese entity mention 
categorization approach. It can be used to solve 
the multi-class categorization problem with just 
simple binary features as the input. 
3 Deep Belief Network for Chinese 
Entity Categorization 
3.1 Problem Formalization 
An Entity mention categorization is a process of 
classifying the entity mentions into different 
categories. In this paper, we assume that the 
entity mentions are already correctly detected 
from the texts. Moreover, an entity mention 
should belong to one and only one predefined 
category. Formally, the categorization function 
of the name entities is 
( ( ))if V e C?            (1) 
where 
ie  is an entity mention from all the 
mention set E, ( )iV e  is the binary feature 
vector of 
ie , C={C1, C2, ?, CM} is the pre-
defined categories. Now the question is to find a 
classification function : Df R C?  which maps 
the feature vector V(ei) of an entity mention to its 
category. Generally, this classification function 
is learned from training data consisting of entity 
mentions with labeled categories. The learned 
function is then used to predict the category of 
new entity mentions by their feature vectors. 
3.2 Character-based Features 
As mentioned in the introduction, we intend to 
use character-level features for the purpose of 
avoiding the impact of the Chinese word 
segmentation errors. Denote the character 
dictionary as D={d1, d2, ?, dN}. To an e, it?s 
feature vector is V(e)={ v1, v2, ?, vN }. Each unit 
vi can be valued as Equation 2. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (2) 
103
For example, there is an entity mention ??
? ?Clinton?. So its feature vector is a vector 
with the same length as the character dictionary, 
in which all the dimensions are 0 except the three 
dimensions standing for ?, ?, and ?. The 
representation is clearly illustrated in Figure 1 
below. Since our objective is to test the 
effectiveness of DBN for this task. Therefore, we 
do not involve any other feature. 
 
Fig. 1. Generating the character-level features 
Characters compose the named entity and 
express its meaning. As a matter of fact, the 
composite effect of the characters to the 
mention category is quite complicated. For 
example, ?? ?Mr. Li? and ?? ?Laos? both 
have character ?, but ?? ?Mr. Li? indicates 
a person but ?? ?Laos? indicates a country. 
These are totally different NEs. Another 
example is ????? ?Capital of Paraguay? 
and ??? ?Asuncion?. They are two entity 
mentions point to the same entity despite that 
the two entities do not have any common 
characters. In such case, independent character 
features are not sufficient to determine the 
categories of the entity mentions. So we should 
also introduce some features which are able to 
represent the combinational effects of the 
characters. However, such kind of features is 
very hard to discover. Meanwhile, a complete 
set of combinations is nearly impossible to be 
found manually due to the exponential number 
of all the possible combinations. As in our 
study, we adopt DBN to automatically find the 
character combinations.  
3.3 Deep Belief Nets 
Deep Belief Network (DBN) is a complicated 
model which combines a set of simple models 
that are sequentially connected (Ackley, 1985). 
This deep architecture can be viewed as multiple 
layers. In DBN, upper layers are supposed to 
represent more ?abstract? concepts that explain 
the input data whereas lower layers extract ?low-
level features? from the data. DBN often consists 
of many layers, including multiple Restricted 
Boltzmann Machine (RBM) layers and a Back 
Propagation (BP) layer.  
 
Fig. 2.  The structure of a DBN. 
As illustrated in Figure 2, when DBN receives 
a feature vector, the feature vector is processed 
from the bottom to the top through several RBM 
layers in order to get the weights in each RBM 
layer, maintaining as many features as possible 
when they are transferred to the next layer. RBM 
deals with feature vectors only and omits the la-
bel information. It is unsupervised. In addition, 
each RBM layer learns its parameters indepen-
dently. This makes the parameters optimal for 
the relevant RBM layer but not optimal for the 
whole model. To solve this problem, there is a 
supervised BP layer on top of the model which 
fine-tunes the whole model in the learning 
process and generates the output in the inference 
process. After the processing of all these layers, 
the final feature vector consists of some sophisti-
cated features, which reflect the structured in-
formation among the original features. With this 
new feature vector, the classification perfor-
mance is better than directly using the original 
feature vector. 
None of the RBM is capable of guaranteeing 
that all the information conveyed to the output is 
accurate or important enough. However the 
learned information produced by preceding RBM 
layer will be continuously refined through the 
next RBM layer to weaken the wrong or insigni-
ficant information in the input. Each layer can 
detect feature in the relevant spaces. Multiple 
layers help to detect more features in different 
spaces. Lower layers could support object detec-
tion by spotting low-level features indicative of 
object parts. Conversely, information about ob-
jects in the higher layers could resolve lower-
level ambiguities. The units in the final layer 
share more information from the data. This in-
creases the representation power of the whole 
model. It is certain that more layers mean more 
computation time. 
104
DBN has some attractive features which make 
it very suitable for our problem. 
1) The unsupervised process can detect the 
structures in the input and automatically ob-
tain better feature vectors for classification. 
2) The supervised BP layer can modify the 
whole network by back-propagation to im-
prove both the feature vectors and the classi-
fication results. 
3) The generative model makes it easy to in-
terpret the distributed representations in the 
deep hidden layers. 
4) This is a fast learning algorithm that can 
find a fairly good set of parameters quickly 
and can ensure the efficiency of DBN. 
3.3.1 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which is 
the core component of DBN. RBM is Boltzmann 
Machine with no connection within the same 
layer. An RBM is constructed with one visible 
layer and one hidden layer. Each visible unit in 
the visible layer V  is an observed variable iv  
while each hidden unit in the hidden layer H  is 
a hidden variable 
jh
. Its joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (3) 
In RBM, the parameters that need to be esti-
mated are ( , , )W b c? ?  and 2( , ) {0,1}v h ? . 
To learn RBM, the optimum parameters are 
obtained by maximizing the above probability on 
the training data (Hinton, 1999). However, the 
probability is indeed very difficult in practical 
calculation. A traditional way is to find the gra-
dient between the initial parameters and the re-
spect parameters. By modifying the previous pa-
rameters with the gradient, the expected parame-
ters can gradually approximate the target para-
meters as 
0
( 1) ( ) ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (4) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging to 
the target. 
Traditionally, the Markov chain Monte Carlo 
method (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(5) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the av-
erage over the data states and its relevant sample 
in hidden unit. 
h v? ?
 denotes the multiplication 
of the average over the model states in visible 
unit and its relevant sample in hidden unit. 
However, MCMC requires estimating an ex-
ponential number of terms. Therefore, it typically 
takes a long time to converge to 
h v? ?
. Hinton 
(2002) introduced an alternative algorithm, i.e., 
the contrastive divergence (CD) algorithm, as a 
substitution. It is reported that CD can train the 
model much more efficiently than MCMC. To 
estimate the distribution ( )p x , CD considers a 
series of distributions { ( )np x } which indicate the 
distributions in n steps. It approximates the gap 
of two different Kullback-Leiler divergences 
(Kullback, 1987) as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (6) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution p?  
defined by the model. In each step, the gap is 
approximately minimized so that we can obtain 
the final distribution which has the smallest 
Kullback-Leiler divergence with the fantasy dis-
tribution.  
After n steps, the gradient can be estimated 
and used in Equation 4 to adjust the weights of 
RBM. In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust the 
weight of RBM. In this case, the estimate of the 
gradient is just the gap between the products of 
the visual layer and the hidden layer, i.e., 
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (7) 
Figure 3 below illustrates the process of learning 
RBM with CD-based gradient estimation. 
 
105
Fig. 3.  Learning RBM with CD-based gradient 
estimation 
3.3.2 Back-propagation (BP) 
The RBM layers provide an unsupervised analy-
sis on the structures of data set. They automati-
cally detect sophisticated feature vectors. The 
last layer in DBN is the BP layer. It takes the 
output from the last RBM layer and applies it in 
the final supervised learning process. In DBN, 
not only is the supervised BP layer used to gen-
erate the final categories, but it is also used to 
fine-tune the whole network. Specifically speak-
ing, when the parameters in BP layer are 
changed during its iterating process, the changes 
are passed to the other RBM layers in a top-to-
bottom sequence. 
The BP algorithm has a feed-forward step and 
a back-propagation step. In the feed-forward step, 
the input values are propagated to obtain the out-
put values. In the back-propagation step, the out-
put values are compared to the real category la-
bels and used them to modify the parameters of 
the model. We consider the weight
ijw  
which 
indicates the edge pointing from the i-th node in 
one RBM layer to the j-th node in its upper layer. 
The computation in feed-forward is 
i ijo w , 
where 
io  is the stored output for the unit i. In 
the back-propagation step, we compute the error 
E in the upper layers and also the gradient with 
respect to this error, i.e., 
i ijE o w? ?
. Then the 
weight
ijw  
will be adjusted by the gradient des-
cent. 
ij i i j
i ij
Ew o oo w? ? ?
?? ? ? ? ??
 (8) 
where ??  is used to control the length of the 
moving step. 
3.3.3 DBN-based Entity Mention Categori-
zation 
For each entity mention, it is represented by the 
character feature vector as introduced in section 
3.2 and then fed to DBN. The training procedure 
can be divided into two phases. The first phase is 
the parameter estimation process of the RBMs on 
all the inputted feature vectors. When a feature 
vector is fed to DBN, the first RBM layer is 
adjusted automatically according to this vector. 
After the first RBM layer is ready, its output 
becomes the input of the second RBM layer. The 
weights of the second RBM layer are also 
adjusted. The similar procedure is carried out on 
all the RBM layers. Then DBN will operates in 
the second phase, the back-propagation 
algorithm. The labeled categories of the entity 
mention are used to tune the parameters of the 
BP layer. Moreover, the changes of the BP layer 
are also fed back to the RBM layers. The 
procedure will iterate until the terminating 
condition is met. It can be a fixed number of 
iterations or a pre-given precision threshold. 
Once the weights of all the layers in DBN are 
obtained, the estimated model could be used to 
prediction. 
 
Fig. 4.  The mention categorization process 
of DBN 
Figure 4 illustrates the classification process of 
DBN. In prediction, for an entity mention e, we 
first calculate its feature vector V(e) and used as 
the input of DBN. V(e) is passed through all the 
layers to get the outputs for all RBM layers and 
last back-propagation layer. In the ith RBM layer, 
the dimensions in the input vector Vinput_i(e) are 
combined to yield the dimensions of the next 
feature vector Voutput_i(e) as input of the next layer. 
After the feature vector V(e) goes through all the 
RBM layers, it is indeed transformed to another 
feature vector V?(e) which consists of 
complicated combinations of the original 
character features and contains rich structured 
information between the characters. This feature 
vector is then fed into the BP layer to get the 
final category c(e). 
4 Experiments 
4.1 Experiment Setup 
In our experiment, we use the ACE 2004 corpus 
to evaluate our approach. The objective of this 
study is that the correctly detected Chinese entity 
mentions categorization using DBN from the text 
and figure out the suitability of DBN on this task. 
Moreover, an entity mention should belong to 
one and only one category. 
106
According to the guideline of the ACE04 task, 
there are five categories for consideration in total, 
i.e., Person, Organization, Geo-political entity, 
Location, and Facility. Moreover, each entity 
mention is expressed in two forms, i.e., the head 
and the extent. For example, ??????? 
?President Clinton of USA? is the extent of an 
entity mention and ???  ?Clinton? is the 
corresponding head. The two phrases both point 
to a named entity whose name is Clinton and he 
is the president of USA.  Here we make the 
?breakdown? strategy mentioned in Li et al 
(2007) that only the entity head is considered to 
generate the feature vector, considering that the 
information from the entity head refines the 
name entity. Although the entity extent includes 
more information, it also brings many noises 
which may make the learning process much 
more difficult. 
   In our experiments, we test the machine 
learning models under a 4-flod cross-validation. 
All entity mentions are divided into four parts 
randomly where three parts are used for training 
and one for test. In total, 7746 mentions are used 
for training and 2482 mentions are used for 
testing at each round. Precision is chosen as the 
evaluation criterion, calculated by the proportion 
of the number of correctly categorized instances 
and the number of total instances. Since all the 
instances should be classified, the recall value is 
equal to the precision value. 
4.2 Evaluation on Named Entity categoriza-
tion 
First of all, we provide some statistics of the data 
set. The distribution of entity mentions in each 
category is given in table 1. The size of the 
character dictionary in the corpus is 1185, so 
does the dimension of each feature vector. 
Type Quantity 
Person 4197 
Organization 1783 
Geo-political entity 287 
Location 3263 
Facility 399 
Table 1.  Number of entity mentions in each 
category 
In the first experiment, we compare the 
performance of DBN with some popular 
classification algorithms, including Support 
Vector Machine (labeled by SVM) and a 
traditional BP neutral network (labeled by NN 
(BP)). To implement the models, we use the 
LibSVM toolkit1 for SVM and the neural neutral 
network toolbox in Matlab2 for BP. The DBN in 
this experiment includes two RBM layers and 
one BP layer. Results of the first experiment are 
given in Table 2.  
Learning Model Precision 
DBN 91.45% 
SVM 90.29% 
NN(BP) 87.23% 
Table 2.  Performances of the systems with 
different classification models 
In this experiment, the DBN has three RBM 
layers and one BP layer. And the numbers of 
units in each RBM layer are 900, 600 and 300 
respectively. NN (BP) has the same structure as 
DBN. As for SVM, we choose the linear kernel 
with the penalty parameter C=1 and set the other 
parameters as default after comparing different 
kernels and parameters. 
In the results, DBN achieved better 
performance than both SVM and BP neural 
network. This clearly proved the advantages of 
DBN. The deep architecture of DBN yields 
stronger representation power which makes it 
able to detect more complicated and efficient 
features, thus better performance is achieved.  
In the second experiment, we intend to 
examine the performance of DBN with different 
number of RBM layers, from one RBM layer 
plus one BP layer to three RBM layers plus one 
BP layer. The amount of the units in the first 
RBM layer is set 900 and the amount in the 
second RBM layer is 600, if the second layer 
exists. As for the third RBM layers, the amount 
of units is set to 300. 
Construction of Neural Network Precision 
Three RBMs and One BP 91.45% 
Two RBMs and One BP 91.42% 
One RBM and one BP 91.05% 
Table 3.  Performance of DBNs with different 
number s of RBM layers 
Results in Table 3 show that the performance 
tends to be better when more RBM layers are 
incorporated. More RBM layers do enhance the 
representation power of DBN. However, it is 
also noted that the improvement is not significant 
from two layers to three layers. The reason may 
                                                 
1 available at http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
2 available at 
http://www.mathworks.com/access/helpdesk/help/toolbox
/nnet/backprop.html 
107
be that two-RBM DBN already has enough 
representation power for modeling this data set 
and thus one more RBM layer brings 
insignificant improvement. It is also mentioned 
in Hinton (2006) that more than three RBM 
layers are indeed not necessary. Another 
important result in Table 3 is that the DBN with 
One RBM and one BP performs much better than 
the neutral network with only BP in Table 1. 
This clearly showed the effectiveness of feature 
combination by the RBM layer again. 
As to the amount of units in each RBM layer, 
it is manually fixed in upper experiments. This 
number certainly affects the representation 
power of an RBM layer, consequently the 
representation power of the whole DBN. In this 
set of experiment, we intend to study the 
effectiveness of the unit size to the performance 
of DBN. A series of DBNs with only one RBM 
layer and different unit numbers for this RBM 
layer is evaluated. The results are provided in 
Table 4 below. 
Construction of Neural Network Precision 
one RBM(300 units) + one BP 90.61% 
one RBM(600 units) + one BP 90.69% 
one RBM(900 units) + one BP 91.05% 
one RBM(1200 units) + one BP 90.98% 
one RBM(1500 units) + one BP 90.61% 
one RBM(1800 units) + one BP 90.57% 
Table 4.  Performance of One-RBM DBNs 
with different number of units 
Based on the results, we can see that the 
performance is quite stable with different unit 
numbers. But the numbers that are closer to the 
original feature size seem to be some better. This 
could suggest that we should not decrease or 
increase the dimension of the vector feature too 
much when casting the vector transformation by 
RBM layers. 
Finally, we show the results of the individual 
categories. For each category, the Precision-
Recall-F values are provided in table 5, in which 
the F-measure is calculated by 
2*Precision*Recall-measure= Precision+RecallF
    (9) 
Type P R F 
Person 91.26% 96.26% 93.70% 
Organization 89.86% 89.04% 89.45% 
Location 77.58% 59.21% 76.17% 
Geo-political 
entity 
93.60% 91.89% 92.74% 
Facility 77.43% 63.72% 69.91% 
Table 5.  Performances of the system on each 
category 
5 Conclusions 
In this paper we presented our recent work on 
applying a novel machine learning model, the 
Deep Belief Nets, on Chinese entity mention 
categorization. It is demonstrated that DBN is 
very suitable for character-level mention 
categorization approaches due to its strong 
representation power and the ability on 
discovering complicated feature combinations. 
We conducted a series of experiments to prove 
the benefits of DBN. Experimental results 
clearly showed the advantages of DBN that it 
obtained better performance than existing 
approaches such as SVM and traditional BP 
neutral network. 
References  
David Ackley, Geoffrey Hinton, and Terrence 
Sejnowski. 1985. A learning algorithm for 
Boltzmann machines. Cognitive Science. 9. 
David MacDonald. 1993. Internal and external 
evidence in the identification and semantic 
categorization of proper names. Corpus 
Processing for Lexical Acquisition, MIT Press, 61-
76. 
Geoffrey Hinton. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Geoffrey Hinton. 2002. Training products of experts 
by minimizing contrastive divergence. Neural 
Computation, 14, 1771?1800. 
Geoffrey Hinton, Simon Osindero, and Yee-Whey 
Teh. 2006. A fast learning algorithm for deep 
belief nets. Neural Computation. 18, 1527?1554 . 
GuoDong Zhou and Jian Su. 2002. Named entity 
recognition using an hmm-based chunk tagger. In 
proceedings of ACL. 473-480. 
Hideki Isozaki and Hideto Kazawa. 2002. Efficient 
support vector classifiers for named entity 
recognition. In proceedings of IJCNLP. 1-7. 
Honglei Guo, Jianmin Jiang, Guang Hu and Tong 
Zhang. 2005. Chinese named entity recognition 
based on multilevel linguistics features. In 
pr ceedings of IJCNLP. 90-99. 
Jing, Hongyan, Radu Florian, Xiaoqiang Luo, Tong 
Zhang and Abraham Ittycheriah. 2003. How to get 
a Chinese name (entity): Segmentation and 
combination issues. In proceedings of EMNLP. 
200-207. 
Koen Deschacht and Marie-Francine Moens. 2006, 
Efficient Hierarchical Entity Classifier Using 
Conditional Random Field. In Proceedings of the 
108
2nd Workshop on Ontology Learning and 
Population. 33-40. 
Michael Collins and Yoram Singer. 1999. 
Unsupervised models for named entity 
classification. In Proceedings of EMNLP'99. 
Nina Wacholder, Yael Ravin and Misook Choi. 1997. 
Disambiguation of Proper Names in Text. In 
Proceedings of the Fifth Conference on Applied 
Natural Language Processing. 
Solomon Kullback. 1987. Letter to the Editor: The 
Kullback-Leibler distance. The American 
Statistician 41 (4): 340?341. 
Wenjie Li and Donglei Qian. 2007. Detecting, 
Categorizing and Clustering Entity Mentions in 
Chinese Text, in Proceedings of the 30th Annual 
International ACM SIGIR Conference (SIGIR?07). 
647-654. 
Yoshua Bengio and Yann LeCun. 2007. Scaling 
learning algorithms towards ai. Large-Scale Ker-
nel Machines. MIT Press. 
 
109
Exploring Deep Belief Network for Chinese Relation Extraction 
Yu Chen1, Wenjie Li2, Yan Liu2, Dequan Zheng1, Tiejun Zhao1 
1School of Computer Science and Technology, Harbin Institute of Technology, China 
{chenyu, dqzheng, tjzhao}@mtlab.hit.edu.cn 
2Department of Computing, The Hong Kong Polytechnic University, Hong Kong 
{cswjli, csyliu}@comp.polyu.edu.hk 
Abstract 
Relation extraction is a fundamental 
task in information extraction that 
identifies the semantic relationships 
between two entities in the text. In this 
paper, a novel model based on Deep 
Belief Network (DBN) is first 
presented to detect and classify the 
relations among Chinese entities. The 
experiments conducted on the 
Automatic Content Extraction (ACE) 
2004 dataset demonstrate that the 
proposed approach is effective in 
handling high dimensional feature 
space including character N-grams, 
entity types and the position 
information. It outperforms the state-
of-the-art learning models such as 
SVM or BP neutral network. 
1 Introduction 
Information Extraction (IE) is to automatically 
pull out the structured information required by 
the users from a large volume of plain text. It 
normally includes three sequential tasks, i.e., 
entity extraction, relation extraction and event 
extraction. In this paper, we limit our focus on 
relation extraction.  
In early time, pattern-based approaches were 
the main focus of most research studies in 
relation extraction. Although pattern-based 
approaches achieved reasonably good results, 
they have some obvious flaws. It requires 
expensive handcraft work to assemble patterns 
and not all relations can be identified by a set 
of reliable patterns (Willy Yap, 2009). Also, 
once the interest of task is transferred to a 
different domain or a different language, 
patterns have to be revised or even rewritten. 
That is to say, the discovered patterns are 
heavily dependent on the task in a specific 
domain or on a particular corpus. 
Naturally, a vast amount of work was spent 
on feature-based machine learning approaches 
in later years. In this camp, relation extraction 
is typically cast as a classification problem, 
where the most important issue is to train a 
model to scale and measure the similarity of 
features reflecting relation instances. The 
entity semantic information expressing relation 
was often formulated as the lexical and 
syntactic features, which are identical to a 
certain linear vector in high dimensions. Many 
learning models are capable of self-training 
and classifying these vectors according to 
similarity, such as Support Vector Machine 
(SVM) and Neural Network (NN).  
Recently, kernel-based approaches have 
been developing rapidly. These approaches 
involved kernels of structure representations, 
like parse tree or dependency tree, in similarity 
calculation. In fact, feature-based approaches 
can be viewed as the special and simplified 
kinds of kernel-based approaches. They used 
dot-product as the kernel function and did not 
range over the intricate structure information 
(Ji, et al 2009). 
Relation extraction in Chinese received 
quite limited attention as compared to English 
and other western languages. The main reason 
is the unique characteristic of Chinese, such as 
more flexible grammar, lack of boundary 
information and morphological variations etc 
(Sun and Dong, 2009). Especially, the existing 
Chinese syntactic analysis tools at current 
stage are not yet reliable to capture the 
valuable structured information. It is urgent to 
develop approaches that are in particular 
suitable for Chinese relation extraction. 
In this paper, we explore the use of Deep 
Belief Network (DBN), a new feature-based 
machine learning model for Chinese relation 
extraction. It is a neural network model 
developed under the deep learning architecture 
that is claimed by Hinton (2006) to be able to 
automatically learn a deep hierarchy of 
features with increasing levels of abstraction 
for the complex problems like natural language 
processing (NLP). It avoids assembling 
patterns that express the semantic relation 
information and meanwhile it succeeds to 
produce accurate model that is not confined to 
the parsing results.  
The rest of this paper is structured in the 
following manner. Section 2 reviews the 
previous work on relation extraction. Section 3 
presents task definition, briefly introduces the 
DBN model and the feature construction. 
Section 4 provides the experimental results. 
Finally, Section 5 concludes the paper.  
2 Related Work 
Over the past decades, relation extraction had 
come to a significant progress from simple 
pattern-based approaches to adapted self-
training machine learning approaches. 
Brin (1998) used Dual Iterative Pattern 
Relation Expansion, a bootstrapping-based 
system, to find the largest common substrings 
as patterns. It had the ability of searching 
patterns automatically and was good for large 
quantity of uniform contexts. Chen (2006) 
proposed graph algorithm called label 
propagation, which transferred the pattern 
similarity to probability of propagating the 
label information from any vertex to its nearby 
vertices. The label matrix indicated the relation 
type. 
Feature-based approaches utilized the linear 
vector of carefully chosen lexical and syntactic 
features derived from different levels of text 
analysis and ranging from part-of-speech (POS) 
tagging to full parsing and dependency parsing 
(Zhang 2009). Jing and Zhai (2007) defined a 
unified graphic representation of features that 
served as a general framework in order to 
systematically explore the information at 
diverse levels in three subspaces and finally 
estimated the effectiveness of these features. 
They reported that the basic unit feature was 
generally sufficient to achieve state-of-art 
performance. Meanwhile, over-inclusion 
complex features were harmful. 
Kernel-based approaches utilize kernel 
functions on structures between two entities, 
such as sequences and trees, to measure the 
similarity between two relation instances. 
Zelenok (2003) applied parsing tree kernel 
function to distinguish whether there was an 
existing relationship between two entities. 
However, they limited their task on Person-
affiliation and organization-location.  
The previous work mainly concentrated on 
relation extraction in English. Relatively, less 
attention was drawn on Chinese relation 
extraction. However, its importance is being 
gradually recognized. For instance, Zhang et al 
(2008) combined position information, entity 
type and context features in a feature-based 
approach and Che (2005) introduced the edit 
distance kernel over the original Chinese string 
representation.  
DBN is a new feature-based approach for 
NLP tasks. According to the work by Hinton 
(2006), DBN consisted of several layers 
including multiple Restricted Boltzmann 
Machine (RBM) layers and a Back 
Propagation (BP) layer. It was reported to 
perform very well in many classification 
problems (Ackley, 1985), which is from the 
origin of its ability to scale gracefully and be 
computationally tractable when applied to high 
dimensional feature vectors. Furthermore, to 
against the combinations of feature were 
intricate, it detected invariant representations 
from local translations of the input by deep 
architecture.  
3 Deep Belief Network for Chinese 
Relation Extraction 
3.1 Task Definition 
Relation extraction, promoted by the 
Automatic Content Extraction (ACE) program, 
is a task of finding predefined semantic 
relations between pairs of entities from the 
texts. According to the ACE program, an entity 
is an object or a set of objects in the world 
while a relation is an explicitly or implicitly 
stated relationship between entities. The task 
can be formalized as:  
1 2( , , )e e s r?       (1) 
where 
1e  and 2e  are the two entities in a 
sentence s  under concern and r  is the relation 
between them. We call the triple 
1 2( , , )e e s  the 
relation candidate. According to the ACE 2004 
guideline 1 , five relation types are defined. 
They are: 
Role: it represents an affiliation between a 
Person entity and an Organization, Facility, 
or GPE (a Geo-political entity) entities. 
Part: it represents the part-whole relationship 
between Organization, Facility and GPE 
entities. 
At: it represents that a Person, Organization, 
GPE, or Facility entity is location at a 
Location entities. 
Near: it represents the fact that a Person, 
Organization, GPE or Facility entity is near 
(but not necessarily ?At?) a Location or 
GPE entities. 
Social: it represents personal and professional 
affiliations between Person entities. 
3.2 Deep Belief Networks (DBN) 
DBN often consists of several layers, 
including multiple RBM layers and a BP layer. 
As illustrated in Figure 1, each RBM layer 
learns its parameters independently and 
unsupervisedly. RBM makes the parameters 
optimal for the relevant RBM layer and detect 
complicated features, but not optimal for the 
whole model. There is a supervised BP layer 
on top of the model which fine-tunes the whole 
model in the learning process and generates the 
output in the inference process. RBM keeps 
information as more as possible when it 
transfers vectors to next layer. It makes 
networks to avoid local optimum. RBM is also 
adopted to ensure the efficiency of the DBN 
model. 
 
Fig. 1.  The structure of a DBN. 
                                                 
1 available at http://www.nist.gov/speech/tests/ace/. 
Deep architecture of DBN represents many 
functions compactly. It is expressible by 
integrating different levels of simple functions 
(Y. Bengio and Y. LeCun). Upper layers are 
supposed to represent more ?abstract? concepts 
that explain the input data whereas lower 
layers extract ?low-level features? from the 
data. In addition, none of the RBM guarantees 
that all the information conveyed to the output 
is accurate or important enough. The learned 
information produced by preceding RBM layer 
will be continuously refined through the next 
RBM layer to weaken the wrong or 
insignificant information in the input. Multiple 
layers filter valuable features. The units in the 
final layer share more information from the 
data. This increases the representation power 
of the whole model. The final feature vectors 
used for classification consist of sophisticated 
features which reflect the structured 
information, promote better classification 
performance than direct original feature vector.  
3.3 Restricted Boltzmann Machine (RBM) 
In this section, we will introduce RBM, which 
is the core component of DBN. RBM is 
Boltzmann Machine with no connection within 
the same layer. An RBM is constructed with 
one visible layer and one hidden layer. Each 
visible unit in the visible layer V  is an 
observed variable 
iv  while each hidden unit in 
the hidden layer H  is a hidden variable 
jh
. Its 
joint distribution is 
( , ) exp( ( , )) T T Th Wv b x c hp v h E v h e ? ?? ? ? (2) 
In RBM, 2( , ) {0,1}v h ? and ( , , )W b c? ? are 
the parameters that need to be estimated?W  
is the weight tying visible layer and hidden 
layer. b is the bias of units v and c is the bias of 
units h. 
To learn RBM, the optimum parameters are 
obtained by maximizing the joint distribution 
( , )p v h  on the training data (Hinton, 1999). A 
traditional way is to find the gradient between 
the initial parameters and the expected 
parameters. By modifying the previous 
parameters with the gradient, the expected 
parameters can gradually approximate the 
target parameters as 
0
( 1) ( ) log ( )
W
P vW W W ?
? ? ?? ?? ? ?
 (3) 
where ?  is a parameter controlling the leaning 
rate. It determines the speed of W converging 
to the target. 
Traditionally, the Monte Carlo Markov 
chain (MCMC) is used to calculate this kind of 
gradient. 
0 0log ( , )p v h h v h vw ? ?
? ? ??       
(4) 
where log ( , )p v h  is the log probability of the 
data. 
0 0h v
 denotes the multiplication of the 
average over the data states and its relevant 
sample in hidden unit. 
h v? ?
 denotes the 
multiplication of the average over the model 
states in visible units and its relevant sample in 
hidden units. 
  
Fig. 2.  Learning RBM with CD-based 
gradient estimation 
However, MCMC requires estimating an 
exponential number of terms. Therefore, it 
typically takes a long time to converge to 
h v? ?
. Hinton (2002) introduced an alternative 
algorithm, i.e., the contrastive divergence (CD) 
algorithm, as a substitution. It is reported that 
CD can train the model much more efficiently 
than MCMC. To estimate the distribution ( )p x , 
CD considers a series of distributions { ( )np x } 
which indicate the distributions in n steps. It 
approximates the gap of two different 
Kullback-Leiler divergences as 
0( || ) ( || )n nCD KL p p KL p p? ?? ?     (5) 
Maximizing the log probability of the data is 
exactly the same as minimizing the Kullback?
Leibler divergence between the distribution of 
the data 
0p  and the equilibrium distribution 
p?  defined by the model.  
In our experiments, we set n to be 1. It 
means that in each step of gradient calculation, 
the estimate of the gradient is used to adjust 
the weight of RBM as Equation 6.  
0 0 1 1log ( , )p v h h v h vW
? ? ??
 (6) 
Figure 2 below illustrates the process of 
learning RBM with CD-based gradient 
estimation. 
3.4 Back-Propagation (BP) 
The RBM layers provide an unsupervised 
analysis on the structures of data set. They 
automatically detect sophisticated feature 
vectors. The last layer in DBN is the BP layer. 
It takes the output from the last RBM layer and 
applies it in the final supervised learning 
process. In DBN, not only is the supervised BP 
layer used to generate the final categories, but 
it is also used to fine-tune the whole network. 
Specifically speaking, when the BP layer is 
changed during its iterating process, the 
changes are passed to the other RBM layers in 
a top-to-bottom sequence. 
3.5 The Feature Set 
DBN is able to detect high level hidden 
features from lexical, syntactic and/or position 
characteristic. As mentioned in related work, 
over-inclusion complex features are harmful. 
We therefore involve only three kinds of low 
level features in this study. They are described 
below. 
3.5.1 Character-based Features 
Since Chinese text is written without word 
boundaries, the word-level features are limited 
by the efficiency of word segmentation results. 
In the paper presented by H. Jing (2003) and 
some others, they observed that pure character-
based models can even outperform word-based 
models. Li et al?s (2008) work relying on 
character-based features also achieved 
significant performance in relation extraction. 
We denote the character dictionary as D={d1, 
d2, ?, dN}. In our experiment, N is 1500. To 
an e, it?s character-based feature vector is 
V(e)={ v1, v2, ?, vN }. Each unit vi can be 
valued as Equation 8. 
??
??
?
?
??
      0
    1
ed
edv
i
i
i
          (7) 
3.5.2 Entity Type Features 
According to the ACE 2004 guideline, there 
are five entity types in total, including Person, 
Organization, GPE, Location, and Facility. We 
recognize and classify the relation between the 
recognized entities. The entities in ACE 2004 
corpus were labeled with these five types. 
Type features are distinctive for classification. 
For example, the entities of Location cannot 
appear in the Role relation.  
3.5.3 Relative Position Features 
We define three types of position features 
which depict the relative structures between 
the two entities, including Nested, Adjacent 
and Separated. For each relation candidate 
triple 
1 2( , , )e e s , let .starte  and .ende  denote 
the starting and end positions of e  in a 
document. Table 1 summarizes the conditions 
for each type, where }2,1{, ?ji  and ji ? .  
Type Condition 
Nested ( .start, .end) ( .start, .end)i i j je e e e?
 
Adjacent .end= .start-1i je e
 
Separated ( .start< .start)&( .end+1< .start)i j i je e e e
 
Table 1. The internal postion structure features 
between two named entities 
We combine the character-based features of 
two entities, their type information and 
position information as the feature vector of 
relation candidate.  
3.6 Order of Entity Pair 
A relation is basically an order pair. For 
example, ?Bank of China in Hong Kong? 
conveys the ACE-style relation ?At? between 
two entities ?Bank of China (Organization)? 
and ?Hong Kong (Location)?. We can say that 
Bank of China can be found in Hong Kong, 
but not vice verse. The identified relation is 
said to be correct only when both its type and 
the order of the entity pair are correct. We 
don?t explicitly incorporate such order 
restriction as an individual feature but use the 
specified rules to sort the two entities in a 
relation once the relation type is recognized. 
As for those symmetric relation types, the 
order needs not to be concerned. Either order is 
considered correct in the ACE standard. As for 
those asymmetric relation types, we simply 
select the first (in adjacent and separated 
structure) or outer (in nested structures) as the 
first entity. In most cases, this treatment leads 
to the correct order. We also make use of 
entity types to verify (and rectify if necessary) 
this default order. For example, considering 
?At? is a relation between a Person, 
Organization, GPE, or Facility entity and a 
Location entity, the Location entity must be 
placed after the Person, Organization, GPE, or 
Facility entity in a relation. 
4 Experiments and Evaluations 
4.1 Experiment Setup 
The experiments are conducted on the ACE 
2004 Chinese relation extraction dataset, 
which consists of 221 documents selected from 
broadcast news and newswire reports. There 
are 2620 relation instances and 11800 pairs of 
entities have no relationship in the dataset. The 
size of the feature space is 3017.  
We examine the proposed DBN model 
using 4-fold cross-validation. The performance 
is measured by precision, recall, and F-
measure. 
2*Precision*Recall-measure= Precision+RecallF
    (8) 
In the following experiments, we plan to test 
the effectiveness of the DBN model in three 
ways: 
Detection Only: For each relation candidate, 
we only recognize whether there is a certain 
relationship between the two entities, no 
matter what type of relation they hold.  
Detection and Classification in Sequence: 
For each relation candidate, when it is 
detected to be an instance of relation, it 
proceeds to detect the type of the relation 
the two entities hold. 
Detection and Classification in Combination: 
We define N+1 relation label, N for relation 
types defined by ACE and one for NULL 
indicating there is no relationship between 
the two entities. In this way, the processes 
of detection and classification are combined. 
We will compare DBN with a well-known 
Support Vector Machine model (labeled as 
SVM in the tables) and a traditional BP neutral 
network model (labeled as NN (BP only)). 
Among them, SVM has been successfully 
applied in many classification applications. We 
use the LibSVM toolkit 2  to implement the 
SVM model. 
4.2 Evaluation on Detection Only 
We first evaluate relation detection, where 
only two output classes are concerned, i.e. 
NULL (which means no relation recognized) 
and RELATION. The parameters used in DBN, 
SVM and NN (BP only) are tuned 
experimentally and the results with the best 
parameter settings are presented in Table 2. In 
each of our experiments, we test many 
parameters of SVM and chose the best set of 
that to show below. 
Regarding the structure of DBN, we 
experiment with different combinations of unit 
numbers in the RBM layers. Finally we choose 
DBN with three RBM layers and one BP layer. 
And the numbers of units in each RBM layer 
are 2400, 1800 and 1200 respectively, which is 
the best size of each layer in our experiment. 
Our empirical results showed that the numbers 
of units in adjoining layers should not decrease 
the dimension of feature vector too much when 
casting the vector transformation. NN has the 
same structure as DBN. As for SVM, we 
choose the linear kernel with the penalty 
parameter C=0.3, which is the best penalty 
coefficient, and set the other parameters as 
default after comparing different kernels and 
parameter values.  
Model Precision Recall F-measure 
DBN 67.8% 70.58% 69.16% 
SVM 73.06% 52.42% 61.04% 
NN (BP 
only) 
51.51% 61.77% 56.18% 
Table 2. Performances of DBN, SVM and NN 
models for detection only 
As showed in Table 2, with their best 
parameter settings, DBN performs much better 
                                                 
2 http://www.csie.ntu.edu.tw/~cjlin/libsvm/  
than both SVM and NN (BP only) in terms of 
F-measure. It tells that DBN is quite good in 
this binary classification task. Since RBM is a 
fast approach to approximate global optimum 
of networks, its advantage over NN (BP only) 
is clearly demonstrated in their results.  
4.3 Evaluation on Detection and 
Classification in Sequence 
In the next experiment, we go one step further. 
If a relation is detected, we classified it into 
one of the 5 pre-defined relation types. For 
relation type classification, DBN and NN (BP 
only) have the same structures as they are in 
the first experiment. We adopt SVM linear 
kernel again and set C to 0.09 and other 
parameters as default. The overall performance 
of detection and classification of three models 
are illustrated in Table 3 below. DBN again is 
more effective than SVM and NN. 
Model Precision Recall F-measure 
DBN 63.67% 59% 61.25% 
SVM 67.78% 47.43% 55.81% 
NN  61% 45.62% 52.2% 
Table 3. Performances of DBN and other 
classification models for detection and 
classification in sequence 
4.4 Evaluation on Detection and 
Classification in Combination 
In the third experiment, we unify relation 
detection and relation type classification into 
one classification task. All the candidates are 
directly classified into one of the 6 classes, 
including 5 relation types and a NULL class. 
Parameter settings of the three models in this 
experiment are identical to those in the second 
experiment, except that C in SVM is set to 0.1. 
Model Precision Recall F-measure 
DBN 65.8% 59.15% 62.3% 
SVM 75.25% 44.07% 55.59% 
NN (BP 
only) 
63.2% 45.7% 53.05% 
Table 4. Performances of DBN, SVM and NN 
models for detection and classification in 
combination 
As demonstrated, DBN outperforms both 
SVM and NN (BP only) in all these three 
experiments consistently. In this regard, the 
advantages of DBN over the other two models 
are apparent. RBM approximates expected 
parameters rapidly and the deep DBN 
architecture yields stronger representativeness 
of complicated, efficient features.  
Comparing the results of the second and the 
third experiments, SVM perform better 
(although not quite significantly) when 
detection and classification are in sequence 
than in combination. This finding is consistent 
with our previous work (to be added later). It 
can possibly be that preceding detection helps 
to deal with the severe unbalance problem, i.e. 
there are much more relation candidates that 
don?t hold pre-defined relations. However, 
DBN obtaining the opposite result cause by 
that the amount of examples we have is not 
sufficient for DBN to self-train itself well for 
type classification. We will further exam this 
issue in our feature work. 
4.5 Evaluation on DBN Structure 
Next, we compare the performance of DBN 
with different structures by changing the 
number of RBM layers. All the candidates are 
directly classified into 6 types in this 
experiment.  
DBN  Precision Recall F-measure 
3 RBMs + 
BP 
65.8% 59.15% 62.3% 
2 RBMs + 
BP 
65.22% 57.1% 60.09% 
1 RBM + 
BP 
64.35% 55.5% 59.6% 
Table 5. Performance RBM with different 
layers 
The results provided in Table 5 show that 
the performance can be improved when more 
RBM layers are incorporated. Multiple RBM 
layers enhance representation power. Since it 
was reported by Hinton (2006) that three RBM 
layer is enough to detect the complex features 
and more RBM layer are of less help, we do 
not try to go beyond the three layers in this 
experiment. Note that the improvement is more 
obvious from two layers to three layers than 
from one layer to two layers. 
4.6 Error Analysis 
Finally, we provide the test results for 
individual relation types in Table 6. We can 
see that the proposed model performs better on 
?Role? and ?Part? relations. When taking a 
closer look at their relation instance 
distributions, the instances of these two types 
comprise over 63% percents of all the relation 
instances in the dataset. Clearly their better 
results benefit from the amount of training data. 
It further implies that if we have more training 
data, we should be able to train a more 
powerful DBN. The same characteristic is also 
observed in Table 7 which shows the 
distributions of the identified relations against 
the gold standard.  However, the sizes of ?At? 
relation instances and ?Role? relation instances 
are similar, its result is much worse. We 
believe it is from the origin of that the position 
feature is not distinctive for ?At? relation, as 
shown in Table 8. ?Near? and ?Social? are two 
symmetric relation types. Ideally, they should 
have better results. But due to quite small 
number of training examples, you can see that 
they are actually the types with the worst F-
measure. 
Type Precision Recall F-measure 
Role 65.19% 69.2% 67.14% 
Part 67.86% 71.43% 69.59% 
At 51.15% 60% 55.22% 
Near 15.38% 33.33% 20.05% 
Social 25% 35.71% 29.41% 
Table 6. Performance of DBN for each 
relation type 
 R P A N S Null 
Role (R) 191 1 5 0 0 96 
Part (P) 1 95 12 0 0 32 
At (A) 4 8 111 2 1 91 
Near (N) 0 1 0 2 0 10 
Social (S) 1 0 0 0 5 14 
Table 7. Distribution of the identified relations 
Type Adjacent  Separated Nested  
Role 7 63 223 
Part 1 17 122 
At 21 98 98 
Near 0 8 5 
Social 10 10 10 
           Identified 
Standard 
Table 8.  Statistic of position feature 
The main mistakes observed in Table 7 are 
wrongly classifying a ?Part? relation as a ?At? 
relations. We further inspect these 12 mistakes 
and find that it is indeed difficult to distinct the 
two types for the given entity pairs. Here is a 
typical example: entity 1: ?????  (the 
Democratic Party of the United States, defined 
as an organization entity), entity 2: ?? (the 
United States, defined as a GPE entity). 
Therefore, the major problem we have to face 
is how to effectively recall more relations. 
Given the limited training resources, it is 
needed to well explore the appropriate external 
knowledge or the Web resources. 
5 Conclusions 
In this paper we present our recent work on 
applying a novel machine learning model, 
namely Deep Belief Network, to Chinese 
relation extraction. DBN is demonstrated to 
be effective for Chinese relation extraction 
because of its strong representativeness. We 
conduct a series of experiments to prove the 
benefits of DBN. Experimental results clearly 
show the strength of DBN which obtains 
better performance than other existing models 
such as SVM and the traditional BP neutral 
network. In the future, we will explore if it is 
possible to incorporate the appropriate 
external knowledge in order to recall more 
relation instances, given the limited training 
resource. 
References 
Ackley D., Hinton G. and Sejnowski T. 1985. A 
learning algorithm for Boltzmann machines, 
Cognitive Science, 9. 
Brin Sergey. 1998. Extracting patterns and relations 
from world wide web, In Proceedings of 
WebDB Workshop at 6th International 
Conference on Extending Database 
Technology (WebDB?98), 172-183. 
Che W.X. Improved-Edit-Distance Kernel for 
Chinese Relation Extraction, In Dale, R.,Wong, 
K.-F., Su, J., Kwong, O.Y. (eds.) IJCNLP 
2005.LNCS(LNAI). vol. 2651. 
H. Jing, R. Florian, X. Luo, T. Zhang, A. 
Ittycheriah. 2003. How to get a Chinese name 
(entity): Segmentation and combination issues. 
In proceedings of EMNLP. 200-207. 
Hinton, G.. 1999. Products of experts. In 
Proceedings of the Ninth International. 
Conference on Artificial Neural Networks 
(ICANN). Vol. 1, 1?6. 
Hinton, G. E. 2002. Training products of experts by 
minimizing contrastive divergence, Neural 
Computation, 14(8), 1711?1800. 
Hinton G. E., Osindero S. and Teh Y. 2006. A fast 
learning algorithm for deep belief nets, Neural 
Computation, 18. 1527?1554. 
Ji Zhang, You Ouyang, Wenjie Li and Yuexian 
Hou. 2009. A Novel Composite Kernel 
Approach to Chinese Entity Relation 
Extraction. in Proceedings of the 22nd 
International Conference on the Computer 
Processing of Oriental Languages, Hong Kong, 
pp240-251. 
Ji Zhang, You Ouyang, Wenjie Li, and Yuexian 
Hou. 2009. Proceedings of the 22nd 
International Conference on Computer 
Processing of Oriental Languages. 236-247.  
Jiang J. and Zhai C. 2007. A Systematic 
Exploration of the Feature Space for Relation 
Extraction, In Proceedings of NAACL/HLT, 
113?120. 
Jinxiu Chen, Donghong Ji, Chew L., Tan and 
Zhengyu Niu. 2006. Relation extraction using 
label propagation based semi-supervised 
learning, In Proceedings of ACL?06, 129?136. 
Li W.J., Zhang P., Wei F.R., Hou Y.X. and Lu, Q. 
2008. A Novel Feature-based Approach to 
Chinese Entity Relation Extraction, In 
Proceeding of ACL 2008 (Companion Volume), 
89?92 
Sun Xia and Dong Lehong, 2009. Feature-based 
Approach to Chinese Term Relation Extraction. 
International Conference on Signal Processing 
Systems. 
Willy Yap and Timothy Baldwin. 2009. 
Experiments on Pattern-based Relation 
Learning. Proceeding of the 18th ACM 
conference on Information and knowledge 
management. 1657-1660.  
Y. Bengio and Y. LeCun. 2007. Scaling learning 
algorithms towards ai. Large-Scale Kernel 
Machines. MIT Press. 
Zelenko D. Aone C and Richardella A. 2003. 
Kernel Methods for Relation Extraction, 
Journal of Machine Learning Research 
2003(2), 1083?1106. 
Zhang P., Li W.J., Wei F.R., Lu Q. and Hou Y.X. 
2008. Exploiting the Role of Position Feature 
in Chinese Relation Extraction, In Proceedings 
of the 6th International Conference on 
Language Resources and Evaluation (LREC). 
 
 The Chinese Persons Name Disambiguation Evaluation: Exploration of 
Personal Name Disambiguation in Chinese News 
 
 
Ying Chen*, Peng Jin?, Wenjie Li?,Chu-Ren Huang? 
* China Agricultural University ?Leshan Teachers? College ?The Hong Kong Polytechnic University 
chenying3176@gmail.com jandp@pku.edu.cn cswjli@comp.polyu.edu.hk 
  churenhuang@gmail.com 
   
 
 
 
 
 
 
Abstract 
Personal name disambiguation becomes hot as it 
provides a way to incorporate semantic under-
standing into information retrieval. In this cam-
paign, we explore Chinese personal name 
disambiguation in news. In order to examine how 
well disambiguation technologies work, we con-
centrate on news articles, which is well-formatted 
and whose genre is well-studied. We then design a 
diagnosis test to explore the impact of Chinese 
word segmentation to personal name disambigua-
tion. 
1 Introduction 
Incorporating semantic understanding technolo-
gies from the field of NLP becomes one of further 
directions for information retrieval. Among them, 
named entity disambiguation, which intends to 
use state-of-the-art named entity processing to 
enhance a search engine, is a hot research issue. 
Because of the popularity of personal names in 
queries, more efforts are put on personal name 
disambiguation. The personal name disambigua-
tion used both in Web Personal Search (WePS1) 
and our campaign is defined as follow. Given 
documents containing a personal name in interest, 
the task is to cluster them according to which en-
tity the name in a document refers to.  
WePS, which explores English personal name 
disambiguation, has been held twice (Artiles et al, 
                                                           
1
 http://nlp.uned.es/weps/ 
2007, 2009). Compared to the one in English, per-
sonal name disambiguation in Chinese has special 
issues, such as Chinese text processing and Chi-
nese personal naming system. Therefore, we hold 
Chinese personal name disambiguation (CPND) to 
explore those problems. In this campaign, we 
mainly examine the relationships between Chinese 
word segmentation and Chinese personal name 
disambiguation.  
Moreover, from our experiences in WePS 
(Chen et al, 2007, 2009), we notice that webpages 
are so noisy that text pre-processing that extracts 
useful text for disambiguation needs much effort. 
In fact, text pre-processing for webpages is rather 
complicated, such as deleting of HTML tags, the 
detection of JavaScript codes and so on. Therefore, 
the final system performance in the WePS cam-
paign sometimes does not reflect the disambigua-
tion power of the system, and instead it shows the 
comprehensive result of text pre-processing as 
well as disambiguation. In order to focus on per-
sonal name disambiguation, we choose news 
documents in CPND. 
The paper is organized as follows. Section 2 de-
scribes our formal test including datasets and 
evaluation. Section 3 introduces the diagnosis test, 
which explores the impact of Chinese word seg-
mentation to personal name disambiguation. Sec-
tion 4 describes our campaign, and Section 5 
presents the results of the participating systems. 
Finally, Section 6 concludes our main findings in 
this campaign. 
 
2 The Formal Test 
2.1 Datasets 
To avoid the difficulty to clean a webpage, we 
choose news articles in this campaign. Given a 
full name in Chinese, we search the character-
based personal name string in all documents of 
Chinese Gigaword Corpus, a large Chinese news 
collection. If a document contains the name, it is 
belonged to the dataset of this name. To ensure 
the popularity of a personal name, we keep only a 
personal name whose corresponding dataset com-
prises more than 100 documents. In addition, if 
there are more than 300 documents in that dataset, 
we randomly select 300 articles to annotate. Fi-
nally, there are totally 58 personal names and 
12,534 news articles used in our data, where 32 
names are in the development data and 26 names 
are in the test data, as shown Appendix Table 4 
and 5 separately.   
From Table 4 and 5, we can find that the ambi-
guity (the document number per cluster) distribu-
tion is much different between the development 
data and the test data.  In fact, the ambiguity var-
ies with a personal name in interest, such as the 
popularity of the name in the given corpus, the 
celebrity degree of the name, and so on.    
2.2 Evaluation  
In WePS, Artiles et al (2009) made an intensive 
study of clustering evaluation metrics, and found 
that B-Cubed metric is an appropriate evaluation 
approach. Moreover, in order to handle overlap-
ping clusters (i.e. a personal name in a document 
refers to more than one person entity in reality), 
we extend B-Cubed metric as Table 1, where S = 
{S1, S2, ?} is a system clustering  and R = {R1, 
R2, ?} is a gold-standard clustering. The final 
performance of a system clustering for a personal 
name is the F score (?= 0.5), and the final per-
formance of a system is the Mac F score, the aver-
age of the F scores of all personal names. 
Moreover, Artiles et al (2009) also discuss 
three cheat systems: one-in-one, all-in-one, and 
the hybrid cheat system. One-in-one assigns each 
document into a cluster, and in contrast, all-in-one 
put all documents into one cluster. The hybrid 
cheat system just incorporates all clusters both in 
one-in-one and all-in-one clustering. Although the 
hybrid cheat system can achieve fairly good per-
formance, it is not useful for real applications. In 
the formal test, these three systems serve as the 
baseline.  
 
 Formula 
 
Precision 
?
? ?
?
? ? ??
?
S
S d dR
 i
 i i j j
S
i
S S i
ji
R;R
|S|
|S|
|RS|
 
max
 
 
Recall 
?
? ?
?
? ? ??
?
R
R d dS
 i
 i i j j
R
i
R i
ji
R S;S
|R|
|R|
|SR|
 
max
 
Table 1: the formula of the modified B-cubed 
metrics 
3 The Diagnosis Test 
Because of no word delimiter, Chinese text proc-
essing often needs to do Chinese word segmenta-
tion first. In order to explore the relationship 
between personal name disambiguation and word 
segmentation, we provide a diagnosis data which 
attempts to examine the impact of word segmenta-
tion to disambiguation.  
Firstly, for each personal name, its correspond-
ing dataset will be manually divided into three 
groups as follows. The disambiguation system 
then runs for each group of documents. The three 
clustering outputs are merged into the final clus-
tering for that personal name.  
(1) Exactly matching: news articles contain-
ing personal names that exactly match 
the query personal name. 
(2) Partially matching: news articles contain-
ing personal names that are super-strings 
of the query personal name. For instance, 
an article that has a person named with 
????? (Gao Jun Tian)  is retrieved 
for the query personal name ???? (Gao 
Jun).  
(3) Discarded: news articles containing 
character sequences that match the query 
personal name string and however in fact 
are not a personal name. For instance, an 
article that has the string ??????
?? (Zui Gao Jun Shi Fa Yuan: supreme 
military court) is also retrieved for the  
personal name ???? (Gao Jun).  
 
This diagnosis test is designed to simulate the 
realistic scenario where Chinese word segmenta-
tion works before personal name disambiguation. 
If a Chinese word segmenter works perfectly, a 
word-based matching can be used to retrieve the 
documents containing a personal name, and arti-
cles in Groups (2) and (3) should not be returned. 
The personal name disambiguation task that is 
limited to the documents in Group (1) should be 
simpler. 
Moreover, in this diagnosis test, we propose a 
baseline based on the gold-standard word segmen-
tation as follows, namely the word-segment sys-
tem.  
1) All articles in the ?exactly matching? 
group are merged into a cluster, and all 
articles in the ?discarded? group are 
merged into a cluster. 
2) In the ?partially matching? group, enti-
ties exactly sharing the same personal 
name are merged into a cluster.  For ex-
ample, all articles containing ????? 
(Gao Jun Tian) are merged into a cluster, 
and all articles containing???? (Gao 
Jun Hua) are merged into another cluster. 
4 Campaign Design  
4.1 The Participants 
The task of Chinese personal name disambigua-
tion in news has attracted the participation of 10 
teams. As a team can submit at most 2 results, 
there are 17 submissions from the 10 teams in the 
formal test, and there are 11 submissions from 7 
teams in the diagnosis.  
4.2 System descriptions 
Regarding system architecture, all systems are 
based on clustering, and most of them comprise 
two components: feature extraction and clustering. 
However, NEU-1 and HITSZ_CITYU develop a 
different clustering, which in fact is a cascaded 
clustering. Taking the advantage of the properties 
of a news article, both systems first divide the 
dataset for a personal name into two groups ac-
cording to whether the person in question is a re-
porter of the news. They then choose a different 
strategy to make further clustering for each group.    
In terms of feature extraction, we find that all 
systems except SoochowHY use word segmenta-
tion as pre-processing. Moreover, most systems 
choose named entity detection to enhance their 
feature extraction. In addition, character-based 
bigrams are also used in some systems.  In Ap-
pendix Table 6, we give the summary of word 
segmentation and named entity detection used in 
the participating systems. 
Regarding clustering algorithms, agglomerative 
hierarchical clustering is popular in the submis-
sions. Moreover, we find that weight learning is 
very crucial for similarity matrix, which has a big 
impact to the final clustering performance. Be-
sides the popular Boolean and TFIDF weighting 
schemes, SoochowHY and NEU-2 use different 
weighting learning. NEU-2 manually assigns 
weights to different kinds of features. So-
ochowHY develops an algorithm that iteratively 
learns a weight for a character-based n-gram.  
5 Results  
We first provide the performances of the formal 
test, and make some analysis. We then present and 
discuss the performances of the diagnosis test.  
5.1 Results of the Formal test 
For the formal test, we show the performances of 
11 submissions from 10 teams in Table 2. For 
each team, we keep only the better result except 
the NEU team because they use different tech-
nologies in their two submissions (NEU_1 and 
NEU_2).  
From Table 2, we first observe that 7 submis-
sions perform better than the hybrid cheat system. 
In contrast, in Artiles et al (2009), only 3 teams 
can beat the hybrid system. From our analysis, 
this may attribute to the following facts.  
1) Personal name disambiguation on Chinese 
may be easier than the one on English. For 
example, one of key issues in personal name 
disambiguation is to capture the occurrences 
of a query name in text. However, various 
personal name expressions, such as the use of 
 Precision Recall Macro F  
NEU_1 95.76 88.37 91.47 
NEU_2 95.08 88.62 91.15 
HITSZ_CITYU 83.99 93.29 87.42 
ICL_1 83.68 92.23 86.94 
DLUT_1 82.69 91.33 86.36 
BUPT_1 80.33 94.52 85.79 
XMU 90.55 84.88 85.72 
Hybrid cheat system 73.48 100 82.37 
HIT_ITNLP_2 91.08 62.75 71.03 
BIT 80.2 68.75 68.4 
ALL_IN_ONE 52.54 100 61.74 
BUPT_pris02 72.39 58.35 57.68 
SoochowHY_2 84.51 44.17 51.42 
ONE_IN_ONE 94.42 14.41 21.07 
Table 2: The B-Cubed performances of the formal test  
  
 Precision Recall Macro F  
NEU_1 95.6 89.74 92.14 
NEU_2 94.53 89.99 91.66 
XMU 89.84 89.84 89.08 
ICL_1 84.53 93.42 87.96 
BUPT_1 80.43 95.41 86.18 
Word_segment system 71.11 100 80.92 
BUPT_pris01 77.91 75.09 74.25 
BIT 94.62 63.32 72.48 
SoochowHY 87.22 58.52 61.85 
Table 3: The B-Cubed performances of the diagnosis test   
 
middle names in English, cause many prob-
lems during recognizing of the occurrences 
of a personal name in interest. 
2) We works on news articles, which have less 
noisy information compared to webpages 
used in Artiles et al (2009). More efforts are 
put on the exploration directly on disam-
biguation, not on text pre-processing. Fur-
thermore, most of systems extract features 
based on some popular NLP techniques, such 
as Chinese word segmentation, named entity 
recognition and POS tagger. As those tools 
usually are developed based on news corpora, 
they should extract high-quality features for 
disambiguation in our task.  
 
We then notice that the NEU team achieves the 
best performance. From their system description, 
we find that they make some special processing 
just for this task. For example, they develop a per-
sonal name recognition system to detect the occur-
rences of a query name in a news article, and a 
cascaded clustering for different kinds of persons. 
5.2 Results of the Diagnosis test 
We present the performances of 8 submissions for 
the diagnosis test from 7 teams in Table 3 as the 
format of Table 2. Meanwhile, we use the word-
segment system as the baseline.  
  Comparing Table 2 and 3, we first find that the 
word-segment system has a lower performance 
than the hybrid cheat system although the word-
segment system is more useful for real applica-
tions. This implies the importance to develop an 
appropriate evaluation method for clustering. 
From Table 3, five submissions achieve better 
performances than the word-segment system.  
Given the gold-standard word segmentation on 
personal names in the diagnosis test, from Table 3, 
our total impression is that the top systems take 
less advantages, and the bottom systems take 
more. This indicates that bottom systems suffer 
from their low-quality word segmentation and 
named entity detection. For example, 
BUPT_pris01 increases ~22% F score (from 
52.81% to 74.25%). 
6 Conclusions 
This campaign follows the work of WePS, and 
explores Chinese personal name disambiguation 
on news. We examine two issues: one is for Chi-
nese word segmentation, and the other is noisy 
information. As Chinese word segmentation usu-
ally is a pre-processing for most NLP processing, 
we investigate the impact of word segmentation to 
disambiguation. To avoid noisy information for 
disambiguation, such as HTML tags in webpage 
used in WePS, we choose news article to work on 
so that we can capture how good the state-of-the-
art disambiguation technique is. 
References  
 Artiles, Javier, Julio Gonzalo and Satoshi Sekine.2007. 
The SemEval-2007 WePS Evaluation: Establishing 
a benchmark for the Web People Search Task. In 
Proceedings of Semeval 2007, Association for Com-
putational Linguistics. 
Artiles, Javier, Julio Gonzalo and Satoshi Sekine. 2009. 
WePS 2 Evaluation Campaign: overview of the Web 
People Search Clustering Task. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Bagga, Amit and Breck Baldwin.1998. Entity-based 
Cross-document Co-referencing Using the Vector 
Space Model. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics.  
Chen, Ying and James H. Martin. 2007. CU-COMSEM: 
Exploring Rich Features for Unsupervised Web Per-
sonal Name Disambiguation. In Proceedings of Se-
meval 2007, Association for Computational 
Linguistics.  
Chen, Ying, Sophia Yat Mei Lee and Chu-Ren Huang. 
2009. PolyUHK: A Robust Information Extraction 
System for Web Personal Names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th 
WWW Conference.  
 
 
Appendix 
 
name document #  cluster # document #  per cluster 
?? 
155 37 4.19 
?? 
301 42 7.17 
?? 
300 5 60 
?? 
105 30 3.5 
?? 
156 42 3.71 
??? 
350 15 23.33 
?? 
269 70 3.84 
?? 
257 8 32.13 
?? 
211 109 1.94 
?? 
177 36 4.92 
?? 
358 165 2.17 
?? 
300 20 15 
?? 
140 57 2.46 
?? 
300 27 11.11 
?? 
296 73 4.05 
?? 
135 75 1.8 
?? 
297 14 21.21 
?? 
110 24 4.58 
?? 
207 68 3.04 
?? 
131 26 5.04 
?? 
145 22 6.59 
?? 
164 15 10.93 
??? 
247 20 12.35 
?? 
173 34 5.09 
??? 
171 21 8.14 
?? 
170 34 5 
?? 
195 32 6.09 
?? 
301 22 13.68 
?? 
318 76 4.18 
?? 
234 117 2 
?? 
134 9 14.89 
?? 
123 7 17.57 
 
6930 1352 5.13 
Table 4: The training data distribution 
 
 
name document #  cluster # document #  per cluster 
?? 
190 96 1.99 
?? 
191 5 38.2 
?? 
258 16 16.13 
??? 
224 32 7 
??? 
118 29 4.07 
?? 
239 21 11.38 
?? 
208 43 4.84 
??? 
201 17 11.82 
??? 
317 3 105.67 
??? 
151 6 25.17 
?? 
188 61 3.08 
??? 
200 2 100 
?? 
213 69 3.09 
??? 
182 5 36.4 
?? 
278 11 25.27 
?? 
180 4 45 
??? 
286 1 286 
?? 
206 38 5.42 
?? 
193 16 12.06 
??? 
172 9 19.11 
?? 
174 5 34.8 
?? 
299 39 7.67 
?? 
233 90 2.59 
?? 
300 13 23.08 
?? 
141 25 5.64 
??? 
262 13 20.15 
 
5604 669 8.38 
Table5: The test data distribution 
 
 
 Word segmentation Named Entity 
NEU Name: Neucsp 
Source: 1998 People's Daily   
Name: in-house 
HITSZ_CITYU   
ICL Name: LTP 
F score: 96.5% 
Source:  2nd SIGHAN 
Name: LTP   
 
DLUT   
BUPT Name: in-house 
F score: 96.5% 
Source: SIGHAN 2010 
 
XMU Name: in-house 
Source: 1998 People's Daily 
F score: 97.8% 
 
HIT_ITNLP Name: IRLAS 
Source: 1998 People's Daily 
F score: 97.4% 
Name: IRLAS 
 
BIT Name: ICTCLAS2010   
Precision: ~97% 
Source: 1998 People's Daily   
Name: ICTCLAS2010   
 
BUPT_pris Name: LTP 
 
Name: LTP 
SoochowHY None None 
Table 6: The summary of word segmentation and named entity detection used in the participants 
 
* LTP(Language Technology Platform) 
 
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 18?27,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Towards Scalable Speech Act Recognition in Twitter:  
Tackling Insufficient Training Data 
 
 
Renxian Zhang Dehong Gao Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
{csrzhang, csdgao, cswjli}@comp.polyu.edu.hk 
 
 
 
 
Abstract 
Recognizing speech act types in Twitter is of 
much theoretical interest and practical use. 
Our previous research did not adequately 
address the deficiency of training data for this 
multi-class learning task. In this work, we set 
out by assuming only a small seed training set 
and experiment with two semi-supervised 
learning schemes, transductive SVM and 
graph-based label propagation, which can 
leverage the knowledge about unlabeled data. 
The efficacy of semi-supervised learning is 
established by our extensive experiments, 
which also show that transductive SVM is 
more suitable than graph-based label 
propagation for our task. The empirical 
findings and detailed evidences can 
contribute to scalable speech act recognition 
in Twitter. 
1. Introduction 
The social media platform of Twitter makes 
available a plethora of data to probe the 
communicative act of people in a social network 
woven by interesting events, people, topics, etc. 
Communicative acts such as disseminating 
information, asking questions, or expressing 
feelings all fall in the purview of ?speech act?, a 
long established area in pragmatics (Austin 
1962). The automatic recognition of speech act 
in tons of tweets has both theoretical and 
practical appeal. Practically, it helps tweeters to 
find topics to read or tweet about based on 
speech act compositions. Theoretically, it 
introduces a new dimension to study social 
media content as well as providing real-life data 
to validate or falsify claims in the speech act 
theory. 
Different taxonomies of speech act have been 
proposed by linguists and computational 
linguists, ranging from a few to over a hundred 
types. In this work, we adopt the 5 types of 
speech act used in our previous work (Zhang et 
al. 2011), which are in turn inherited from 
(Searle 1975): statement, question, suggestion, 
comment, and miscellaneous. Our choice is 
based on the fact that unlike face-to-face 
communication, twittering is more in a 
?broadcasting? style than on a personal basis. 
Statement and comment, which are usually 
intended to make one?s knowledge, thought, and 
sentiment known, thus befit Twitter?s 
communicative style. Question and suggestion 
on Twitter are usually targeted at other tweeters 
in general or one?s followers. More interpersonal 
speech acts such as ?threat? or ?thank? as well as 
rare speech acts in Twitter (Searle?s (1975) 
?commissives? and ?declaratives?) are relegated 
to ?miscellaneous?. Some examples from our 
experimental datasets are provided in Table 1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
18
Tweet Speech Act 
Libya Releases 4 Times 
Journalists - 
http://www.photozz.com/?104k 
Statement 
#sincewebeinghonest why u so 
obsessed with what me n her 
do?? Don't u got ya own 
man???? Oh wait..... 
Question 
RT @NaonkaMixon: I will 
donate 10 $ to the Red Cross 
Japan Earthquake fund for 
every person that retweets this! 
#PRAYFORJAPAN 
Suggestion 
is enjoying this new season of 
#CelebrityApprentice.... Nikki 
Taylor = Yum!! 
Comment 
65. I want to get married to 
someone i meet in highschool. 
#100factsaboutme 
Miscellaneous 
Table 1. Example Tweets with Speech acts 
 
Assuming one tweet demonstrates only one 
speech act, the automatic recognition of those 
speech act types in Twitter is a multi-class 
classification task. We concede that this 
assumption may not always hold in real 
situations. But given the short length of tweets, 
multi-speech act tweets are rare and we find this 
simplifying assumption effective in reducing the 
complexity of our problem. A major problem 
with this task is the deficiency of training data. 
Tweeters as well as face-to-face interlocutors do 
not often identify their speech acts; human 
annotation is costly and time-consuming. 
Although our previous research (Zhang et al 
2011) sheds light on the preparation of training 
data, it did not adequately address this problem. 
Our contribution in this work is to directly 
address the problem of training data deficiency 
by using two well-known semi-supervised 
learning techniques that leverage the relationship 
between a small seed of training data and a large 
body of unlabeled data: transductive SVM and 
graph-based label propagation. The empirical 
results show that the knowledge about unlabeled 
data provides promising solutions to the data 
deficiency problem, and that transductive SVM 
is more competent for our task. Our exploration 
with different training/unlabeled data ratios for 
three major Twitter categories and a mixed-type 
category provides solid evidential support for 
future research. 
The rest of the paper is organized as follows. 
Section 2 reviews works related to speech act 
recognition and semi-supervised learning; 
Section 3 briefly discusses supervised learning of 
speech act types developed in our earlier work 
and complementing the previous findings with 
learning curves. The technical details of semi-
supervised learning are presented in Section 4. 
Then we report and discuss the results of our 
experiments in Section 5. Finally, Section 6 
concludes the paper and outlines future 
directions. 
2. Related Work  
The automatic recognition of speech act, also 
known as ?dialogue act?, has attracted sustained 
interest in computational linguistics and speech 
technology for over a decade (Searle 1975; 
Stolcke et al 2000). A few annotated corpora 
such as Switchboard-DAMSL (Jurafsky et al 
1997) and Meeting Recorder Dialog Act (Dhillon 
et al 2004) are widely used, with data 
transcribed from telephone or face-to-face 
conversation. 
Prior to the flourish of microblogging services 
such as Twitter, speech act recognition has been 
extended to electronic media such as email and 
discussion forum (Cohen et al 2004; Feng et al 
2006) in order to study the behavior of email or 
message senders. 
The annotated corpora for ordinary verbal 
communications and the methods developed for 
email, or discussion forum cannot be directly 
used for our task because Twitter text has a 
distinctive Netspeak style that is situated 
between speech and text but resembles neither 
(Crystal 2006, 2011). Compared with email or 
forum post, it is rife with linguistic noises such 
as spelling mistakes, random coinages, mixed 
use of letters and symbols. 
Speech act recognition in Twitter is a fairly 
new task. In our pioneering work (Zhang et al 
2011), we show that Twitter text normalization is 
unnecessary and even counterproductive for this 
task. More importantly, we propose a set of 
useful features and draw empirical conclusion 
about the scope of this task, such as recognizing 
speech act on the coarse-grade category level 
works as well as on the fine-grade topic level. In 
this work, we continue to adopt this framework 
including other learning details (speech act types 
and feature selection for tweets), but the new 
quest starts where the old one left: tackling 
insufficient training data. 
19
As in many practical applications, sufficient 
annotated data are hard to obtain. Therefore, 
unsupervised and semi-supervised learning 
methods are actively pursued. While 
unsupervised sentence classification is rule-based 
and domain-dependent (Deshpande et al 2010), 
semi-supervised methods that both alleviate the 
data deficiency problem and leverage the power 
of state-of-the-art classifiers hold more promises 
for different domains (Medlock and Briscoe 
2007; Erkan et al 2007). 
In the machine learning literature, a classic 
semi-supervised learning scheme is proposed by 
Yarowsky (1995), which is a classical self-
teaching process that makes no use of labeled 
data before they are classified. More theoretical 
analyses are made by (Culp and Michailidis 2007) 
and (Haffari and Sarkar 2007).  
Transductive SVM (Joachims 1999) extends 
the state-of-the-art inductive SVM by explicitly 
considering the relationship between labeled and 
unlabeled data. The graph-based label 
propagation model (Zhu et al 2003; Zhou et al 
2004) using a harmonic function also 
accommodates the knowledge about unlabeled 
data. We will adapt both of them to our multi-
class classification task. 
Jeong et al (2009) report a semi-supervised 
approach to classifying speech acts in emails and 
online forums. But their subtree-based method is 
not applicable to our task because Twitter?s noisy 
textual quality cannot be found in the much 
cleaner email or forum texts. 
3. Supervised Learning of Speech Act 
Types  
Supervised learning of speech act types in 
Twitter relies heavily on a good set of features 
that capture the textual characteristics of both 
Twitter and speech act utterances. As in our 
previous work, we use speech act-specific cues, 
special words (abbreviations and acronyms, 
opinion words, vulgar words, and emoticons), 
and special characters (Twitter-specific 
characters and a few punctuations). Tweet-
external features such as tweeter profile may also 
help, but that is beyond the focus of this paper. 
Although it has been empirically shown that 
speech act recognition in Twitter can be done 
without using training data specific to topics or 
even categories, it is not clear how much training 
data is needed to achieve desirable performance. 
In order to answer this question, we adopt the 
same experimental setup and datasets as reported 
in (Zhang et al 2011) and plot the learning 
curves shown in Figure 1. 
 
 
Figure 1. Learning Curves of Each Category and 
All Tweets 
 
For all individual experiments, the test data are 
a randomly sampled 10% set of all annotated 
data. When training data reach 90%, we actually 
duplicate the reported results. However, Figure 1 
shows that it is unnecessary to use so much 
training data to achieve good classification 
performance. For News and Entity, the 
classification makes little noticeable 
improvement after the training data ratio reaches 
40% (training : test = 4 : 1). For Mixed (the 
aggregate of the News, Entity, LST datasets) and 
LST, performance peaks even earlier at 20% 
training data (training : test = 2 : 1) and 10% 
(training : test = 1 : 1).  
It is delightful to see that only a moderate 
number of annotated data are needed for speech 
act recognition. But even that number (for the 
Mixed dataset, 10% training data are over 800 
annotated tweets) may not be available and in 
many situations, test data may be much more 
than training data. Taking this challenge is the 
next important step we make. 
4. Semi-Supervised Learning of Speech 
Act Types  
The problem setting of a small seed training 
(labeled) set and a much larger test (labeled) set 
fits the semi-supervised learning scheme. Classic 
semi-supervised learning approaches such as 
self-teaching methods (e.g., Yarowsky 1995) are 
mainly concerned with incrementing high-
confidence labeled data in each round of training. 
They do not, however, directly take into account 
the knowledge about unlabeled data. The recent 
research emphasis is on leveraging knowledge 
about unlabeled data during training. In this 
section, we discuss two such approaches. 
20
4.1 Transductive SVM 
The standard SVM classifier popularly used in 
text classification is also known as inductive 
SVM as a model is induced from training data. 
The model is solely dependent on the training 
data and agnostic about the test data. In contrast, 
transductive SVM (Vapnik 1998; Joachims 1999) 
predicts test labels by using the knowledge about 
test data. In the case of test (unlabeled) data far 
outnumbering training (labeled) data, 
transductive SVM provides a feasible scheme of 
semi-supervised learning. 
For a single-class classification problem {xi, yi} 
that focuses on only one speech act type, where 
xi is the ith tweet and yi is the corresponding 
label and { 1, 1}iy ? ? ?  denotes whether xi 
contains the speech act or not, inductive SVM is 
formulated to find an optimal hyperplane 
sign(w?xi ? b) to maximize the soft margin 
between positive and negative objects, or to 
minimize: 
 
21/ 2 iiC ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable. Adopting the same 
formulation, transductive SVM further considers 
test data xi* during training by finding a labeling 
yj* and a hyperplane to maximize the soft margin 
between both training and test data, or to 
minimize: 
 
2
1 21/ 2 i ii iC C? ?? ?? ?w
 
s.t. ( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
      * *( ) 1i i iy b ?? ? ? ?x w , 0i? ?  
 
where
i? is a slack variable for the test data. In 
fact, labeling test data is done during training. 
As the maximal margin approach proves very 
effective for text classification, its transductive 
variant that effectively uses the knowledge about 
test data holds promises of handling the 
deficiency of labeled data. 
4.2 Graph-based Label Propagation 
An alternative way of using unlabeled data in 
semi-supervised learning is based on the intuition 
that similar objects should belong to the same 
class, which can be translated into label 
smoothness on a graph with weights indicating 
object similarities. This is the idea underlying 
Zhu et al?s (2003) graph-based label propagation 
model using Gaussian random fields.   
We again focus on a single-class classification 
problem. Formally, {x1, ? xN} are N tweets, 
having their actual speech act labels y = {y1, ? 
yL, ? yN} (yi ?{1, 0} denoting whether xi 
contains the speech act or not) with the first L of 
them known, and f = {f1, ? fL, ? fN} are their 
predicted labels. Let L = {x1, ? xL} and U = 
{xL+1, ? xN} and the task is to determine 
{fL+1, ? fN} for U. We further define a graph G = 
(V, E), where V = L?U and E is weighted by W 
= [wij]N?N  with wij denoting the similarity 
between xi and xj. Preferring label smoothness on 
G and preserving the given labels, we want to 
minimize the loss function: 
 
2
,
( ) 1/ 2 ( ) Tij i j
i j L U
E w f f
? ?
? ? ??f f ?f
 
s.t. fi = yi (i = 1, ?, L) 
 
where ? = D ? W is the combinatorial graph 
Laplacian with D being a diagonal matrix [dij]N?N 
and 
ii ijj
d w??
. 
This can be expressed as a harmonic function, 
h = argmin fL = yLE(f), which satisfies the 
smoothness property on the graph: 
( ) 1/ ( ( ))ii ikkh i d w h k? ?
. If we define 
/ij ij ikkp w w? ?
and collect pij and h(i) into 
matrix P and column vector h, solving ?h = 0 s.t. 
hL = yL is equivalent to solving h = Ph. 
To find the solution, we can use L and U to 
partition h and P: 
L
U
? ?? ? ?? ?
hh h
, ,
,
LL LU
UL UU
? ?? ? ?? ?
P PP P P
 
and it can be shown that 1( )U UU UL L?? ?h I P P y. 
To get the final classification result, those 
elements in hU that are greater than a threshold 
(0.5) become 1 and the others become 0. 
This approach propagates labels from labeled 
data to unlabeled data on the principle of label 
smoothness. If the assumption about similar 
tweets having same speech acts holds, it should 
work well for our problem. 
4.3 Multi-class Classification 
In the previous formulations, we emphasized 
?single-class classification? because both 
21
transductive SVM and graph-based label 
propagation are inherently one class-oriented. 
Since our problem is a multi-class one, we 
transform the problem to single-class 
classifications by using the one-vs-all scheme.  
Specifically, for each class (speech act type) ci, 
we label all training instances belonging to ci as 
+1 and all those belonging to other classes as ?1 
and then do binary classification. For our 
problem with 5 speech act types, we make 5 such 
transformations. The final prediction is made by 
choosing the class with the highest classification 
score from the 5 binary classifiers. Both 
transductive SVM and graph-based label 
propagation produce real-valued classification 
scores and are amenable to this scheme. 
5. Experiments  
Our experiments are designed to answer two 
questions: 1) How useful is semi-supervised 
speech act learning in comparison with 
supervised learning? 2) Which semi-supervised 
learning approach is more appropriate for our 
problem? 
5.1 Experimental Setup 
We use the 6 datasets in our previous study1 , 
which fall into 3 categories: News, Entity, Long-
standing Topic (LST). Each of the total 8613 
tweets is labeled with one of the following 
speech act types: sta (statement), que (question), 
sug (suggestion), com (comment), mis 
(miscellaneous). In addition, we randomly select 
1000 tweets from each of the categories to create 
a Mixed category of 3000 tweets. Figures 2 to 5 
illustrate the distributions of the speech act types 
in the 3 original categories and the Mixed 
category. 
 
 
Figure 2. Speech Act Distribution (News) 
 
                                                          
1 http://www4.comp.polyu.edu.hk/~csrzhang 
 
Figure 3. Speech Act Distribution (Entity) 
 
 
Figure 4. Speech Act Distribution (LST) 
 
 
Figure 5. Speech Act Distribution (Mixed) 
 
For each category, we use two 
labeled/unlabeled data settings, with labeled data 
accounting for 5% and 10% of the total so that 
the labeled/unlabeled ratios are set at 
approximately 1:19 and 1:9. The labeled data in 
each category are randomly selected in a 
stratified way: using the same percentage to 
select labeled data with each speech act type. The 
stratified selection is intended to keep the speech 
act distributions in both labeled and unlabeled 
data. Table 2 and Table 3 list the details of data 
splitting using the two settings. 
 
Category # Labeled # Unlabeled Total 
News 155 2995 3150 
Entity 72 1391 1463 
LST 198 3802 4000 
Mixed 147 2853 3000 
Table 2. Stratified Data Splitting with 5% as 
Labeled 
 
22
Category # Labeled # Unlabeled Total 
News 312 2838 3150 
Entity 144 1319 1463 
LST 399 3601 4000 
Mixed 298 2702 3000 
Table 3. Stratified Data Splitting with 10% as 
Labeled 
 
For comparison with supervised learning, we 
also use inductive SVM. The inductive and 
transductive SVM classifications are 
implemented by using the SVMlight tool2 with a 
linear kernel. For the graph-based label 
propagation method, we populate the similarity 
matrix W with weights calculated by a Gaussian 
function. Given two tweets xi and xj,  
 
2
2exp( )2
i j
ijw ?
?? ? x x
 
 
where ?.? is the L2 norm. Empirically, the 
Gaussian function measure leads to better results 
than other measures such as cosine. Then we 
convert the graph to an ?NN graph (Zhu and 
Goldberg 2009) by removing edges with weight 
less than a threshold because the ?NN graph 
empirically outperforms the fully connected 
graph. The threshold is set to be ? + ?, the mean 
of all weights plus one standard deviation. 
5.2 Results 
To better evaluate the performance of semi-
supervised learning on speech act recognition in 
Twitter, we report the classification scores for 
both multi-class and individual classes, as well as 
confusion matrices. 
 
Multi-class Evaluation 
Table 4 lists the macro-average F scores and 
weighted average F scores for all classifiers and 
all categories at the 5% labeled data setting. 
Macro-average F is chosen because it gives equal 
weight to all classes. Since some classes (e.g., sta) 
have much more instances than others (e.g., que), 
macro-average F ensures that significant score 
change on minority classes will not be 
overshadowed by small score change on majority 
classes. In contrast, weighted average F is 
calculated according to class instance numbers, 
which is chosen mainly because we want to 
compare the result with supervised learning 
(reported in Zhang et al 2011 and Figure 1). In 
                                                          
2 http://svmlight.joachims.org/ 
this and the following tables, iSVM, tSVM, and 
GLP denote inductive SVM, transductive SVM, 
and graph-based label propagation. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .374 .502 .285 .702 .759 .643 
Entity .312 .395 .329 .493 .534 .436 
LST .295 .360 .216 .433 .501 .376 
Mixed .383 .424 .245 .539 .537 .391 
Table 4. Multi-class F scores (5% labeled data) 
 
Almost without exception, transductive SVM 
achieves the best performance. Measured by 
macro-average F, it outperforms inductive SVM 
with a gain of 10.7% (Mixed) to 34.2% (News). 
Consistent with supervised learning results, 
semi-supervised learning results degrade with 
News > Entity > LST, indicating that both semi-
supervised learning and supervised learning are 
sensitive to dataset characteristics. More uniform 
tweet set (e.g., News) leads to better 
classification and greater improvement by semi-
supervised learning. That also explains why the 
Mixed category, composed of the most 
diversified tweets, benefits least from semi-
supervised learning. 
Conversely, supervised learning (inductive 
SVM) on the Mixed category benefits from the 
data hodgepodge even though the test data are 19 
times the training data. Its macro-average F is 
higher than the other categories although it does 
not have the most training data. Its weighted-
average F using inductive SVM is even higher 
than using transductive SVM. 
It is a little surprising to find that the graph-
based label propagation performs very poorly. In 
all but one place, the GLP score is lower than its 
iSVM counterpart. This may indicate that the 
graph method cannot adapt well to the multi-
class scenario and we will show more evidences 
in the next two sections. 
To understand the effectiveness of semi-
supervised learning, a better way than doing 
numerical calculation is juxtaposing semi-
supervised data settings with their comparable 
supervised data settings, which is shown in Table 
5. The supervised data settings are of those with 
the closest weighted average F (waF) to the 
semi-supervised (tSVM) waF from our previous 
results (Figure 1). 
 
23
 # labeled labeled :unlabeled waF 
 Semi-supervised (tSVM) 
News 155 1 : 19 .759 
Entity 72 1 : 19 .534 
LST 198 1 : 19 .501 
Mixed 147 1 : 19 .537 
 Supervised (with closest waF) 
News 945 1 : 0.3 .768 
Entity 146 1 : 1 .589 
LST 800 1 : 0.5 .501 
Mixed 861 1 : 1 .596 
 
Table 5. Semi-supervised Learning vs. 
Supervised Learning 
 
Obviously semi-supervised learning by 
transductive SVM can achieve classification 
performance comparable to supervised learning 
by inductive SVM, with less training data and 
much lower labeled/unlabeled ratio. This shows 
that semi-supervised learning such as 
transductive SVM holds much promise for 
scalable speech act recognition in Twitter. 
It is tempting to think that with more labeled 
data and higher labeled/unlabeled ratio, semi-
supervised learning performance should improve. 
To put this conjecture to test, we double the 
labeled data (from 5% to 10%) and 
labeled/unlabeled ratio (from 1/19 to 1/9), with 
results in Table 6. 
 
 
Macro-average F Weighted average F 
iSVM tSVM GLP iSVM tSVM GLP 
News .403 .524 .298 .731 .762 .647 
Entity .441 .440 .311 .587 .575 .406 
LST .335 .397 .216 .459 .512 .384 
Mixed .435 .463 .284 .557 .553 .415 
Table 6. Multi-class F scores (10% labeled data) 
 
Compared with Table 4, increased labeled data 
does lead to some improvement, but not much as 
we would expect, the largest gain being 15.9% 
(macro-average F on Mixed, using GLP). Note 
that this is achieved at the cost of labeling twice 
as much data and predicting half as much. In 
contrast, the inductive SVM performance is 
improved by as much as 41.3% (macro-average 
F on Entity). Such evidence shows that semi-
supervised learning of speech acts in Twitter 
benefits disproportionately little from increased 
labeled data, or at least the gain is not worth the 
pain. In fact, this is good news for scalable 
speech act recognition. 
 
Individual Class Evaluation 
For more microscopic inspection, we also report 
the classification results on individual classes for 
all categories. In Table 7, we list the rankings of 
F measures by each classifier for each speech act 
type and each category. The one-letter notations i, 
t, g are short for iSVM, tSVM, and GLP. 
Therefore, t > g > i means tSVM outperforms 
GLP, which outperforms iSVM, in terms of F 
measure. The labeled data are 5%. 
 
 Sta Que Sug Com Mis 
News t >g>i t >i>g t >i>g t >i>g t >g>i 
Entity t >g>i t >i>g g >t>i i >t>g t >g>i 
LST i >g>t t >i>g i >t>g t >i>g t >g>i 
Mixed i >t>g t >i>g t >i>g i >t>g t >g>i 
Table 7. Classifier Rankings for Each Speech 
Act Type and Category (5% Labeled Data) 
 
In 15 out of the 20 rankings, transductive 
SVM or graph-based label propagation beats 
inductive SVM, which shows the efficacy of 
semi-supervised learning in this class-based 
perspective. Transductive SVM is the champion, 
claiming 14 top places.  
We also find that the overall performance of 
graph-based label propagation is the poorest, 
claiming 12 out of 20 bottom places. After 
inspecting the data, we observe that the 
underlying assumption of GLP that similar 
objects belong to the same class is questionable 
for speech act recognition in Twitter. Tweets 
with different speech acts (e.g., question and 
comment) may appear very similar on the graph. 
The maximal margin approach is apparently 
more appropriate for our problem.  
On the other hand, the GLP performance 
evaluated on individual classes is better than 
evaluated on the multi-class if we compare Table 
7 and Table 4, where GLP is almost always the 
lowest achiever. This indicates that in multi-class 
classification, GLP suffers further from the one-
vs-all converting scheme, a point we will make 
clearer in the following. 
 
 
 
24
Confusion matrices 
Confusion matrix provides another perspective to 
understand the multi-class classification 
performance. For brevity?s sake, we present the 
confusion matrices of the three classifiers on the 
News category with 5% labeled data in Figure 6 
to Figure 8. Similar patterns are also observed for 
the other categories and with 10% labeled data. 
Note that the rows represent true classes and the 
columns represent predicted classes. 
 
 Sta Que Sug Com Mis 
Sta 2043 0 5 14 0 
Que 46 7 2 9 0 
Sug 211 1 61 21 0 
Com 276 2 10 164 0 
Mis 120 0 1 2 0 
Figure 6. Confusion Matrix of iSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1848 4 56 90 64 
Que 19 17 7 20 1 
Sug 95 0 158 31 10 
Com 143 5 19 275 10 
Mis 94 3 4 15 7 
Figure 7. Confusion Matrix of tSVM (News, 5% 
Labeled Data) 
 
 Sta Que Sug Com Mis 
Sta 1852 0 4 11 195 
Que 19 6 0 0 39 
Sug 123 0 25 2 144 
Com 134 0 0 47 271 
Mis 102 0 0 1 20 
Figure 8. Confusion Matrix of GLP (News, 5% 
Labeled Data) 
 
The News category is typically biased towards 
the statement speech act, which accounts for 
69% of the total tweets according to Figure 2. As 
a result, the iSVM tends to classify tweets of the 
other speech acts as statement. Figure 6 also 
shows that the prediction accuracy is correlated 
with the training amount. The two classes with 
the least training data, question and 
miscellaneous, demonstrate the lowest accuracy. 
Clearly, supervised learning suffers from training 
data deficiency. 
Both tSVM and GLP show the effect of 
leveraging unlabeled data as they assign new 
labels to some instances wrongly classified as 
statement. Transductive SVM is more successful 
in that it moves most of the Sug and Com 
instances to the diagonal. The situation for Que 
and Mis is also better, though the prediction 
accuracy still suffers from lack of training data. 
Figure 8, however, reveals an intrinsic problem 
of applying graph-based label propagation to 
multi-class classification. Most instances are 
predicted as either Sta or Mis. The wrong 
prediction as Mis cannot be explained by 
imbalance of training data. Rather, it is due to the 
fact that the single-class scores for Mis after 
smoothing on the graph are generally higher than 
those for Que, Sug, or Com. In other words, the 
graph-based method is highly sensitive to class 
differences when multi-class prediction is 
converted from single-class predictions on a 
scheme like one-vs-all. 
In contrast, transductive SVM does not suffer 
much from class differences according to Figure 
7, proving to be more suitable for multi-class 
classification than graph-based label propagation. 
5.3 Summary 
For the task of recognizing speech acts in Twitter, 
we have made some interesting findings from the 
extensive empirical study. To wrap up, let?s 
summarize the most important of them in the 
following. 
1) Semi-supervised learning approaches, 
especially transductive SVM, perform 
comparably to supervised learning approaches, 
such as inductive SVM, with considerably less 
training data and lower training/test ratio. 
Increasing training data cannot improve 
performance proportionately. 
2) Transductive SVM proves to be more 
effective than graph-based label propagation for 
our task. The performance of the latter is hurt by 
two factors: a) the inappropriate assumption 
about similar tweets having the same speech act 
and b) its vulnerability to class differences under 
the one-vs-all multi-class conversion scheme. 
3) For supervised learning as well as semi-
supervised learning for multi-class classification, 
training data imbalance poses no lesser threat 
than training data deficiency. 
25
6. Conclusion and Future Work  
Speech act recognition in Twitter facilitates 
content-based user behavior study. Realizing that 
it is obsessed with insufficient training data, we 
start where previous research left. 
We are not aware of previous study of semi-
supervised learning of speech acts in Twitter and 
in this paper we contribute to scalable speech act 
recognition by drawing conclusions from 
extensive experiments. Specifically, we 
1) extend the work of (Zhang et al 2011) by 
establishing the practicality of semi-supervised 
learning that leverages the knowledge of 
unlabeled data as a promising solution to 
insufficient training data;  
2) show that transductive SVM is more 
effective than graph-based label propagation for 
our problem, which aptly extends the maximal 
margin approach to unlabeled data and is more 
amenable to the multi-class scenario; 
3) provide detailed empirical evidences of 
multi-class and single-class results, which can 
inform future extensions in this direction and 
design of practical systems. 
At this stage, we are not sure whether the one-
vs-all scheme is a bottleneck to one class-
oriented classifiers (it appears to be so for the 
graph-based method). Therefore we will next 
explore other multi-class conversion schemes 
and also consider semi-supervised learning using 
inherently multi-class classifiers such as Na?ve 
Bayes or Decision Tree. In the future, we will 
also explore unsupervised approaches to 
recognizing speech acts in Twitter. 
Acknowledgments 
The work described in this paper was supported 
by the grants GRF PolyU 5217/07E and PolyU 
5230/08E. 
 
26
References  
Austin, J. 1962. How to Do Things with Words. 
Oxford: Oxford University Press. 
Cohen, W., Carvalho, V., and Mitchell, T. 2004. 
Learning to Classify Email into ?Speech Acts?. In 
Proceedings of Empirical Methods in Natural 
Language Processing (EMNLP-04), 309?316.  
Crystal, D. 2006. Language and the Internet, 2nd 
edition. Cambridge, UK: Cambridge University 
Press. 
Crystal, D. 2011. Internet linguistics. London: 
Routledge. 
Culp M. and Michailidis, G. 2007. An Iterative 
Algorithm for Extending Learners to a 
Semisupervised Setting. In The 2007 Joint 
Statistical Meetings (JSM). 
Deshpande S. S., Palshikar, G. K., and Athiappan, G. 
2010. An Unsupervised Approach to Sentence 
Classification, In International Conference on 
Management of Data (COMAD 2010), Nagpur, 
India. 
Dhillon, R., Bhagat, S., Carvey, H., and Shriberg, E. 
2004. Meeting Recorder Project: Dialog Act 
Labeling Guide. Technical report, International 
Computer Science Institute. 
Erkan, G., ?zg?r, A., and Radev, D. 2007. Semi-
Supervised Classification for Extracting Protein 
Interaction Sentences Using Dependency Parsing. 
In Proceedings of the 2007 Joint Conference on 
Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 228?237. 
Feng, D., Shaw, E., Kim, J., and Hovy. E. H. 2006. 
Learning to Detect Conversation Focus of 
Threaded Discussions. In Proceedings of HLT-
NAACL, 208?215. 
Haffari G.R. and Sarkar. A. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. 
In 23rd Conference on Uncertainty in Artificial 
Intelligence (UAI). 
Jeong, M., Lin, C-Y., and Lee, G. 2009. Semi-
supervised Speech Act Recognition in Emails and 
Forums. In Proceedings of EMNLP, pages 1250?
1259. 
Joachims, T. 1999. Transductive Inference for Text 
Classification using Support Vector Machines. In 
Proceedings of the 16th International Conference 
on Machine Learning (ICML). 
Jurafsky, D., Shriberg, E., and Biasca, D. 1997. 
Switchboard SWBD-DAMSL Labeling Project 
Coder?s Manual, Draft 13. Technical report, 
University of Colorado Institute of Cognitive 
Science. 
Medlock, B., and Briscoe, T. 2007. Weakly 
Supervised Learning for Hedge Classification in 
Scientific Literature. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, 992?999. 
Searle, J. 1975. Indirect speech acts. In P. Cole and J. 
Morgan (eds.), Syntax and semantics, vol. iii: 
Speech acts (pp. 59?82). New York: Academic 
Press. 
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., Martin, R. Van Ess-
Dykema, C., and Meteer, M. 2000. Dialogue Act 
Modeling for Automatic Tagging and Recognition 
of Conversational Speech. Computational 
Linguistics, 26(3):339?373. 
Vapnik, V. 1998. Statistical Learning Theory. New 
York: John Wiley & Sons. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL-
1995), 189?196. 
Zhang, R., Gao, D., and Li, W. 2011. What Are 
Tweeters Doing: Recognizing Speech Acts in 
Twitter. In AAAI-11 Workshop on Analyzing 
Microtext. 
Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and 
Scholkopf, B. 2004. Learning with Local and 
Global Consistency. Advances in Neural 
Information Processing Systems (NIPS), vol. 16, 
Cambridge, MA: MIT Press. 
Zhu, X., Ghahramani, Z., and Lafferty, J. D. 2003. 
Semi-supervised Learning Using Gaussian Fields 
and Harmonic Functions. In Proceedings of the 
Twentieth International Conference on Machine 
Learning (ICML), 912?919, Washington, DC. 
Zhu, X. and Goldberg, A. B., 2009. Introduction to 
Semi-Supervised Learning. Morgan & Claypool 
Publishers. 
 
27
