Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1682?1693, Dublin, Ireland, August 23-29 2014.
Ensemble-Based Medical Relation Classification
Jennifer D?Souza and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{jld082000,vince}@hlt.utdallas.edu
Abstract
Despite the successes of distant supervision approaches to relation extraction in the news do-
main, the lack of a comprehensive ontology of medical relations makes it difficult to apply such
approaches to relation classification in the medical domain. In light of this difficulty, we propose
an ensemble approach to this task where we exploit human-supplied knowledge to guide the de-
sign of members of the ensemble. Results on the 2010 i2b2/VA Challenge corpus show that our
ensemble approach yields a 19.8% relative error reduction over a state-of-the-art baseline.
1 Introduction
Medical relation (MR) classification, an information extraction task in the clinical domain that was re-
cently defined in the 2010 i2b2/VA Challenge (Uzuner et al., 2011), involves determining the relation
between a pair of medical concepts (problems, treatments, or tests). The ability to classify MRs is indis-
pensable to sound automatic analysis of patient health records.
While MR classification is a relatively new task, there has been a lot of work on extracting semantic
relations from news articles. Supervised approaches train classifiers on data annotated with the target
relation types, typically using a rich feature set (Zhou et al., 2005; Surdeanu and Ciaramita, 2007; Zhou et
al., 2007). Since obtaining annotated data is a time-consuming and labor-intensive process, researchers
have considered unsupervised approaches (Shinyama and Sekine, 2006; Banko et al., 2007). While
unsupervised approaches can use a large amount of unannotated data and extract a large number of
relations, it may not be easy to map the resulting relations to those needed for a given knowledge base.
One way to mitigate this problem is semi-supervised learning: starting from a given set of seed instances,
a bootstrapping algorithm is used to iteratively learn extraction patterns and extract instances (Brin,
1999; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et
al., 2005; Pantel and Pennacchiotti, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).
However, the resulting patterns often suffer from semantic drift and low precision. Recent years have
seen a surge of interest in distant supervision for relation extraction (Mintz et al., 2009; Nguyen and
Moschitti, 2011; Krause et al., 2012; Min et al., 2013). The idea is to automatically create annotated
relation instances by extracting their labels from relation instances in a knowledge base such as Freebase
(Bollacker et al., 2008) and YAGO (Suchanek et al., 2007).
Our goal in this paper is to advance the state of the art in MR classification. One of the major chal-
lenges in MR classification is the scarcity of labeled data. At first glance, we can mitigate this problem
using distant supervision approaches. However, there is difficulty in applying these approaches to MR
classification: only one of the relation types defined in the 2010 i2b2 Challenge is represented in the
Unified Medical Language System
1
, the most comprehensive medical ontology available to date.
In light of this difficulty, we propose an ensemble approach to MR classification, where we exploit
human-supplied knowledge to guide the design of different members of the ensemble. Unlike state-
of-the-art supervised approaches to this task, which represent contextual information largely as flat (i.e.,
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
www.nlm.nih.gov/research/umls/
1682
discrete- or real-valued) features (de Bruijn et al., 2011; Rink et al., 2011) or structured tree features (Zhu
et al., 2013), we represent contexts as sequences, specifically word sequences and dependency sequences,
and use them to derive lexical and dependency patterns. Our ensemble approach exploits human-supplied
knowledge in three ways. First, while existing approaches employ similarity functions already defined
in off-the-shelf learning algorithms (e.g., linear kernel (Rink et al., 2011), tree kernel (Zhu et al., 2013))
to compute the similarity between two relation instances, we define functions to compare the similarity
between two patterns. Second, to complement the automatically induced patterns, we hand-craft patterns
based on manual observations made on the training set, specifically by having a human identify the
contexts of two concepts that are strongly indicative of a medical relation class. Finally, we employ
human knowledge to identify the constraints on the classification of different relation instances, and
enforce the resulting constraints in an integer linear programming (ILP) framework. Evaluation results
on the 2010 i2b2/VA Challenge corpus (henceforth the i2b2 corpus) show that our ensemble approach
yields a 19.8% relative error reduction over a state-of-the-art system.
The rest of the paper is organized as follows. Section 2 provides an overview of the i2b2 corpus. Sec-
tion 3 describes the baseline systems. Sections 4 and 5 describe our new components and our ensemble
approach. Section 6 discusses our constraints for enforcing global consistency. We present evaluation
results in Section 7, conduct an error analysis in Section 8, and conclude in Section 9.
2 Corpus
For evaluation, we use the i2b2 corpus, which comprises 426 de-identified discharge summaries. We
adopt the i2b2 organizers? partition of the 426 summaries into a training set (170 summaries) and a test
set (256 summaries). As many of the algorithms in our approach require parameter tuning, we reserve
30 of the 170 summaries in the training set for development purposes.
In each discharge summary, the concepts and the medical relation between each pair of concepts
are marked up. Each concept is annotated with a type attribute that indicates whether it is a TEST,
a PROBLEM, or a TREATMENT. In addition, a PROBLEM concept has an assertion attribute, which
specifies whether the problem was present, absent, or possible in the patient, conditionally present in
the patient under certain circumstances, hypothetically present in the patient at some future point, or
mentioned in the patient report but associated with someone other than the patient.
Eleven types of intra-sentential pairwise relations are annotated. A brief description of these relation
types and the relevant statistics are provided in Table 1. As we can see from the table, each medical
relation has a type and is defined only on intra-sentence TREATMENT-PROBLEM, TEST-PROBLEM, and
PROBLEM-PROBLEM concept pairs. Also, while there are 11 relation types, three of them, namely
Relations 6, 9, and 11, denote the absence of a medical relation between the corresponding concepts.
The purpose of having ?no relation? classes is to ensure that every pair of TEST/PROBLEM/TREATMENT
concepts is annotated, whether or not a medical relation exists between them.
3 Baseline MR Classification Systems
We employ two supervised MR classification systems as baselines. The first baseline is a state-of-the-art
system that achieved the best performance in the official 2010 i2b2 evaluation. The second baseline is a
tree kernel-based system, motivated by the fact that tree kernels are frequently used in relation extraction
(e.g., Zhou et al. (2007), Zhu et al. (2013)).
3.1 SVM with Flat Features
Our first baseline, Rink et al.?s (2011) system, employs an SVM classifier trained on a set of flat features
(i.e., features that are discrete- or real-valued).
Following Rink et al., we create training instances as follows. First, we form training instances be-
tween every pair of (PROBLEM, TEST, and TREATMENT) concepts in the training documents, labeling
an instance with its relation type. Since the instances belonging to the three ?no relation? classes signifi-
cantly outnumber those belonging to the remaining eight classes, we reduce data skew by downsampling
1683
Id Relation Example Total (%)
1 TrIP: Treatment improves medical problem Her pain resolved after surgery 203 (0.6)
2 TrWP: Treatment worsens medical problem treated with Zofran with no relief 133 (0.4)
3 TrCP: Treatment causes medical problem Transdermal nitroglycerin caused headache 526 (1.8)
4 TrAP: Treatment is administered for medical
problem
start on Decadron 4 mg q6 to prevent swelling 2613 (8.9)
5 TrNAP: Treatment is not administered be-
cause of medical problem
His Avandia was discontinued secondary to the
side effect profile
174 (0.6)
6 NTrP: No relation between treatment and
problem
with sutures intact and no erythema or puru-
lence noted .
4462 (15.2)
7 TeRP: Test reveals medical problem A postoperative MRI revealed no remarkable
findings
3051 (10.4)
8 TeCP: Test conducted to investigate medical
problem
An ultrasound was done to rule out cholestasis 504 (1.7)
9 NTeP: No relation between test and problem Throughout the stay his labs remained normal
and his pain controlled .
2964 (10.1)
10 PIP: Medical problem indicates medical prob-
lem
with a moderate-sized , dense , fixed inferior
defect indicative of scar
2202 (7.5)
11 NPP: No relation between paired medical
problems
He is somewhat cantankerous and demanding
of the nurses .
12503 (42.6)
Table 1: The 11 relation types for medical relation classification. Each relation type is defined on an
ordered pair where concepts in the pair are as specified by the relation. The ?Total? and ?%? columns
show the number and percentage of instances annotated with the corresponding relation type over all 426
discharge summaries, respectively.
instances belonging to the three ?no relation? classes.
2
Specifically, we downsample the instances be-
longing to the three ?no relation? classes (i.e., NTrP, NTeP, and NPP) by ensuring that (1) the ratio of
the number of NTrP instances to the number of TREATMENT-PROBLEM instances is 0.06; (2) the ratio
of the number of NTeP instances to the number of TEST-PROBLEM instances is 0.03; and (3) the ratio
of the number of NPP instances to the number of PROBLEM-PROBLEM instances is 0.5. These ratios are
selected using our 30-summary development set, as described in Section 2. As mentioned above, each
instance corresponds to a pair of concepts, c
1
and c
2
, and is represented using 37 groups of features that
can be divided into five categories:
3
Context (13 groups). The words, the POS tags, the bigrams, the string of words, the sequence of
phrase chunk types, and the concept types used between c
1
and c
2
; the word preceding c
1
/c
2
; any of the
three words succeeding c
1
/c
2
; the predicates associated with c
1
/c
2
; the predicates associated with both
concepts; and a feature that indicates whether a conjunction regular expression matched the string of
words between c
1
and c
2
.
Similarity (5 groups). We find the concept pairs in the training set that are most similar to the (c
1
,c
2
)
pair (i.e., its nearest neighbors), and create features that encode the statistics collected from these nearest
neighbors. To find the nearest neighbors, we (1) represent each pair in the training set as a sequence; (2)
define the number of nearest neighbors to use; and (3) define a similarity metric to compute the similarity
of two sequences.
Following Rink et al. (2011), we employ five methods to represent a pair. The five methods are: (1) as a
sequence of POS tags for the entire sentence containing the pair; (2) as a phrase chunk sequence between
the two concepts; (3) as a word lemma sequence beginning the two words before the first concept, up to
and including the second word following the second concept in the pair; (4) as a concept type sequence
for all the concepts found in the sentence containing the pair; and (5) as a shortest dependency path
sequence connecting the two concepts. Table 2 shows an example of these five methods of generating
sequences from the TEST concept her exam and the PROBLEM concept her hyperreflexia in the sentence
2
Other methods for addressing class imbalance, such as over-sampling (Chawla et al., 2002) and cost-sensitive learning
(Turney, 1995), can also be employed.
3
To compute the features, we use (1) the Stanford CoreNLP tool (Manning et al., 2014) to obtain POS tags, word lemmas,
and dependency structures; (2) GENIA (http://www.nactem.ac.uk/tsujii/GENIA/tagger) to obtain phrase
chunks; and (3) SENNA (Collobert et al., 2011) to obtain predicate-argument structures.
1684
Generation Method Sequence
(1) RB VB , test
c1
RB VBD RB IN problem
c2
.
(2) ADVP VP ADVP PP
(3) postop , test
c1
only improve slightly in problem
c2
.
(4) test
c1
problem
c2
(5) test
c1
?nsubj?> prep <?pobj?problem
c2
Table 2: Examples of the five methods of sequence generation.
Postop, her exam only improved slightly in her hyperreflexia . Note that for better generalization, the
two concepts are replaced with their concept type (i.e., her exam and her hyperreflexia are replaced
with test
c1
and problem
c2
respectively) before sequence generation. Like Rink et al., we seek different
numbers of nearest neighbors for the five methods of generating sequences. For the first method, we use
100 nearest neighbors; for the second method, 15 neighbors; for the third method, 20 neighbors; for the
fourth method, 100 neighbors; and for the fifth method, 20 neighbors. We use the Levenshtein distance
(Levenshtein, 1966) as the similarity metric.
After finding the nearest neighbors for each of the five methods of sequence representation, we create
features as follows. For each method, we compute the percentage of nearest neighbors belonging to each
of the 11 relation types, and then create 11 features whose values are these 11 numbers.
Single concept (11 groups). Any word lemma from c
1
/c
2
; any word used to describe c
1
/c
2
; the concept
type for c
1
/c
2
; the string of words in c
1
/c
2
; the concatenation of assertion types for both concepts; and the
sentiment category (i.e., positive or negative) of c
1
/c
2
obtained from the General Inquirer lexicon (Stone
et al., 1968).
Wikipedia (6 groups). Six features are computed based on the Wikipedia articles, their categories, and
the links between them. The first feature encodes whether neither c
1
nor c
2
contains any substring that
may be matched against the title of an article. The second feature encodes whether the links between
the articles retrieved based on the two concepts are absent. The next two features encode whether a link
exists from the article pertaining to c
1
(c
2
) to the article pertaining to c
2
(c
1
). The fifth feature encodes
whether there are links between the articles pertaining to both concepts. The last feature encodes whether
both concepts have the same concept type according to their Wikipedia categories.
Vicinity (2 groups). The concatenation of the type of c
1
and the type of the closest concept preceding
c
1
; and the concatenation of the type of c
2
and the type of the closest concept succeeding c
2
.
After creating the training instances, we train a 11-class classifier on them using SVM
multiclass
(Tsochantaridis et al., 2004). We set C, the regularization parameter, to 10,000, since preliminary exper-
iments indicate that preferring generalization to overfitting (by setting C to a small value) tends to yield
poorer classification performance. The remaining learning parameters are set to their default values. Af-
ter training, we use the resulting classifier to make predictions on the test instances, which are generated
in the same way as the training instances.
3.2 SVM with Structured Feature
In this framework, each instance is represented using a single structured feature computed from the
parse tree of the sentence containing the concept pair. Since publicly available SVM learners capable
of handling structured features can only make binary predictions, we train 11 SVM classifiers, one for
representing each medical relation. In each classifier?s training data, a positive instance is one whose
class value matches the medical relation class value of the classifier, and a negative instance is one with
other class values applicable to the given concept pair. Since the negative instances significantly out-
number the positive instances in each of these binary classifiers, we reduce data skew by downsampling
the negative instances. Following the order of the 11 relations listed in Table 1, the optimal ratios of
negative-to-positive instances according to our 30-summary development set are 0.2, 0.2, 0.06, 0.2, 0.5,
1, 1, 0.3, 0.06, 0.06, and 0.09, respectively. We set C to 100 based on the development data.
While we want to use a parse tree directly as a feature for representing an instance, we do not want
to use the entire parse tree as a feature. Specifically, while using the entire parse tree enables a richer
1685
representation of the syntactic context of the two concepts than using a partial parse tree, the increased
complexity of the tree also makes it more difficult for the SVM learner to make generalizations.
To strike a better balance between having a rich representation of the context and improving the
learner?s ability to generalize, we extract a subtree from a parse tree and use it as the value of the struc-
tured feature of an instance. Specifically, given two concepts in an instance and the associated syntactic
parse tree T , we retain as our subtree the portion of T that covers (1) all the nodes lying on the shortest
path between the two entities, and (2) all the immediate children of these nodes that are not the leaves of
T . This subtree is known as a simple expansion tree.
After training the 11 tree kernel-based relation classifiers, we can apply them to classify a test instance.
The class value of an instance is determined based on the classifier with the maximum classification
confidence, where the confidence value of an instance is its signed distance from the SVM hyperplane.
4 Exploiting Sequences for MR Classification
Unlike the two baselines, which exploit flat features and parse-based structured features for MR classifi-
cation, in this section we describe three MR classification systems that exploit sequences.
4.1 Dependency-Based Sequences
The first system is based on sequences of dependency relations. To see why dependency relations could
be useful for MR classification, consider the sentences in Table 3:
(1)
(2)
Table 3: Example dependency paths.
In sentences (1) and (2), the PROBLEM concepts His pain and The patient?s pain occur as the subject
of the verb controlled and the TREATMENT concepts oral medication and Motrin occur as objects of the
prepositional with modifier of the same verb controlled. In other words, intuitively, the verb controlled
cues that the PROBLEM concept is being controlled, and together with the preposition with it cues that the
TREATMENT concept is doing the controlling. Note that in each case the relation between the PROBLEM
and TREATMENT is TrIP, which can now be easily inferred given the dependency relations of the concept
pairs with the verb controlled. These examples suggest that the verb closest to each of the two concepts
is an important word as it cues the relation.
Given the usefulness of dependency structures and the verb closest to each concept for MR classifica-
tion, we represent each training/test instance as a paired dependency sequence with separate dependency
paths traced from each concept in the pair to its closest verb. To reduce data sparsity, for the argument
words found in a dependency path, we replace them with their POS tags. For example, given sentence (1),
the path extracted from His pain is ?nsubjpass ( controlled NN )? and from oral medication is ?prep (
controlled with ) pobj ( with NN )?.
4
Next, we describe how to classify a test instance inst. First, we identify the set of training instances T
that satisfy two conditions: (a) the ancestor verb pair in the training instance is the same as that in inst,
and (b) each of the two dependency sequences in the training instance either is the same as, or contains,
or is contained in the corresponding dependency sequences in inst. Second, we find for inst its nearest
neighbor in T by employing the following similarity function:
Similarity(train, test) = cosine(path
train
c
1
, path
test
c
1
) ? cosine(path
train
c
2
, path
test
c
2
) (1)
4
Note that sometimes a dependency path cannot be traced (e.g., a verb does not exist, which is not uncommon in a discharge
report) for a given concept pair. If this happens, no instance will be generated from the concept pair.
1686
where cosine(x, y) is a function that computes the cosine similarity of x and y.
5
Finally, if the similarity
between inst and its nearest neighbor in T is greater than a threshold, we classify inst using the class
value of its nearest neighbor.
6
Otherwise, this system will leave inst unclassified. In other words, this
system is precision-oriented, classifying only those instances it can classify confidently.
4.2 Lexical Patterns
In our second system, we represent each concept pair as a lexical pattern. Specifically, we employ
Generation Method 3 as described in the Similarity features in Section 3.1 to generate a lexical pattern
from a concept pair. To classify a test instance inst, we employ the one-nearest-neighbor algorithm. To
identify the nearest neighbor of inst, we employ the Levenshtein distance as the similarity metric.
Two questions naturally arise. First, since these lexical patterns have already been used to generate
features in the flat-feature baseline, why do we still employ them in a separate system? To answer
this question, note that although these lexical patterns were used to generate features for training the
flat-feature baseline classifier, we have no control over whether these features are deemed useful by the
learning algorithm and are subsequently used by the resulting classifier. Having a separate system that
employs these patterns ensures that they will be used when making the final classification decision.
Second, given that we described five methods to generate sequences in Section 3.1, why do we employ
Generation Method 3 but not the remaining methods? In principle, we can employ the remaining four
generation methods for generating lexical patterns as well: all we need to do is to create four additional
systems, each of which makes use of the patterns created by exactly one of the four methods. In prac-
tice, however, not all generation methods are equally good: if a method does not generate patterns that
adequately capture context, then employing the resulting patterns may yield poor-performing systems.
Consequently, we employ only the system corresponding to the generation method that yields the best
performance on the development data, which turns out to be the system corresponding to Method 3.
4.3 Rules
In the previous subsection, we employ automatically induced patterns. In contrast, our third system em-
ploys patterns that are hand-crafted based on manual observations made on the training set. Specifically,
we ask a human to identify the contexts of two concepts that are strongly indicative of a relation class.
Like the automatically induced patterns, each hand-crafted pattern is composed of the types of the two
concepts involved and the context in which they occur. For example, in the pattern due to PROBLEM by
TREATMENT, the TREATMENT is likely to cause the PROBLEM and therefore it will be labeled as TrCP.
As another example, in the pattern attributed to PROBLEM as a result of PROBLEM, the two PROBLEMs
are likely to have an indicative relation and therefore it will be labeled as PIP. At the end of this process,
we end up with 136 manually labeled patterns, which we will subsequently refer to as a ruleset.
Next, we order these rules in decreasing order of accuracy, where the accuracy of a rule is defined
as the number of times it yields the correct MR type divided by the number of times it is applied, as
measured on the training set.
Given this ruleset, we can classify a test instance using the first applicable rule in it. If no rules are
applicable, the test instance will remain unclassified.
7
5 The Ensemble
In the previous section, we described three systems for MR classification. Together with the two baseline
systems, we have five systems for MR classification. A natural way to make use of all of them for MR
classification is to include them in an ensemble. The question, then, is: how do we classify a test instance
using this ensemble? The simplest approach is perhaps majority voting, but that presumes that each
5
To apply cosine similarity, we represent each path as a frequency-count vector, where each dimension in the vector corre-
sponds to a dependency type or an argument word appearing in the path.
6
Based on development set experiments, the similarity threshold values for each concept pair type are: T
Treatment?Problem
= 0.85; T
Test?Problem
= 0.75; and T
Problem?Problem
= 0.75.
7
Space limitations preclude a complete listing of these rules. See our website at https://www.hlt.utdallas.edu/
?
jld082000/medical-relations/ for the complete list of rules.
1687
member of the ensemble is equally important. In practice, however, some members are more important
than the others, so the votes cast by these members should have higher weights.
To model this observation, we combine the (probabilistic) votes of the members in a weighted fashion
using the following formula:
P
combined
(c) = w
1
? P
tree
(c) + w
2
? P
flat
(c) + w
3
? P
dependency
(c) + w
4
? P
word
(c) + w
5
? P
rules
(c)
(2)
where w
i
(i = 1, . . . , 5) is a combination weight, and P
x
(c) is the probability that the test instance
belongs to class c according to system x.
Two questions naturally arise. First, how can the combination weights be determined? We perform an
exhaustive search on held-out development data to find the combination of weights that jointly maximizes
overall accuracy on the development set. We allow each weight to vary between 0 and 1 in steps of 0.1,
subject to the constraint that the five weights sum to 1.
Second, how can P
x
(c) be computed? In other words, how can each system compute the probability
that a given test instance belongs to a certain class? To answer this question, we have to convert the
output of each system for each test instance into a 11-element probability vector, which is used to encode
the probability that the given test instance belongs to each of the 11 relation types.
We perform the conversion as follows. For the two baseline systems, the SVM outputs a confidence
value for each class. Hence, to obtain the probability vector, we first normalize the confidence value
associated with each class so that it falls within the [0,1] range, and then normalize the resulting values
so that they sum to 1. For the systems employing lexical patterns and dependency-based sequences, the
class chosen by each system receives a probability of 0.6, and each of the other classes applicable to
the test instance under consideration receives an equal share of the remaining probability mass. For the
rule-based system, we take the rule that is used to classify the test instance and apply this rule to each
instance in the training set to estimate the probability that the rule is correct with respect to each of the
11 classes. We can then use the resulting 11 probabilities to create the 11-element probability vector.
Finally, recall that some of these systems are not applicable to all of the test instances. If this happens,
the corresponding system(s) will return a vector in which all of its elements are set to 0.
6 Enforcing Global Consistency
So far we have had an ensemble that, given a test instance, returns the probability that it belongs to each
of the 11 classes. Since the test instances are classified independently of each other, there is no guarantee
that the resulting classifications are globally consistent. To enforce global consistency, we employ global
constraints implemented in the Integer Linear Programming (ILP) framework (Roth and Yih, 2004).
Since our constraints are intra-sentential, we formulate one ILP program for each sentence s in each
training summary. Each ILP program contains 11?N
s
variables, whereN
s
is the number of test instances
formed from the concept pairs in s. In other words, there is one binary indicator variable x
i,j,r
for each
relation class r of each test instance inst formed from concept i and concept j, which will be set to 1 by
the ILP solver if and only if it thinks inst should belong to class r.
Our objective is to maximize the linear combination of these variables and their corresponding proba-
bilities given by the ensemble (see (3) below) subject to two types of constraints, the integrity constraints
and the consistency constraints. The integrity constraints ensure that each concept pair is assigned ex-
actly one relation type (see the equality constraint in (4)). The consistency constraints ensure consistency
between the predictions made for different instances in the same sentence.
Maximize:
?
(i,j)?R
?
r?L
p
i,j,r
x
i,j,r
(3)
subject to:
?
r?L
x
i,j,r
= 1 ? (i, j) ? R (4)
1688
Relation Relations in Conflict
TrIP(tr
i
,p
j
) TrWP(tr
i
,p
k
),TrCP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrWP(tr
i
,p
j
) TrIP(tr
i
,p
k
),TrCP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrCP(tr
i
,p
j
) TrIP(tr
i
,p
k
),TrWP(tr
i
,p
m
),TrNAP(tr
i
,p
n
)
TrAP(tr
i
,p
j
) TrNAP(tr
i
,p
k
)
TrNAP(tr
i
,p
j
) TrAP(tr
i
,p
k
),TrIP(tr
i
,p
m
),TrWP(tr
i
,p
n
),TrCP(tr
i
,p
o
)
TeRP(te
i
,p
j
) TeCP(te
i
,p
k
)
TeCP(te
i
,p
j
) TeRP(te
i
,p
k
)
Table 4: Constraints on relation types.
and consistency constraints.
Note that (1) p
i,j,r
is the probability that the instance formed from concept i and concept j belongs to
relation type r according to the ensemble; (2) L denotes the set of unique relation types; and (3) R is the
set of instances in the sentence under consideration.
The consistency constraints are listed in Table 4. Each row of the table represents a constraint and can
be interpreted as follows. If the relation in the first column holds, then none of the relations in the second
column can hold. Consider, for instance, the constraint in the first row of the table, which says that if
TREATMENT tr
i
improves PROBLEM p
j
, then tr
i
cannot worsen, cause, or be administered for any other
PROBLEM. At first glance, it may not seem intuitive that a treatment that improves one problem cannot
also worsen or cause other problems. This can be attributed to the way a patient discharge summary
is written: while the constraint can be violated for concept pairs in different sentences, there is no case
in which the constraint is violated for concept pairs in the same sentence in the training set. These
constraints can be implemented as linear constraints in ILP. For example, the constraint ?if TREATMENT
tr
i
improves PROBLEM p
j
, then tr
i
cannot worsen PROBLEM p
k
? can be implemented as follows.
x
i,j,TrIP
? 1? x
i,k,TrWP
(5)
7 Evaluation
7.1 Experimental Setup
Following the 2010 i2b2/VA evaluation scheme, we assume that (1) gold concepts and their types are
given, and (2) a medical relation classification system is evaluated on all but the ?no relation? types. In
other words, a system will not be directly rewarded if it correctly identifies a ?no relation? instance, but
will be penalized if it misclassifies a ?no relation? instance as one of the eight relation types.
As mentioned before, we use 170 training summaries from the 2010 i2b2/VA corpus for classifier
training and reserve 256 test summaries for evaluating system performance. Thirty training summaries
are used for development purposes in all experiments that require parameter tuning.
7.2 Results and Discussion
Table 5 shows the 8-class classification results for our MR classification task, where results are expressed
in terms of recall (R), precision (P), and micro F-score (F).
Row 1 and row 2 show the results of the flat-feature baseline and the structured-feature baseline,
respectively. As we can see, the flat-feature baseline performs significantly better than the structured-
feature baseline.
8
It is worth mentioning that since the dataset available to the research community which
we are using contains a subset of the summaries from the dataset that was available to the shared task
participants, we were unable to directly compare our system?s performance with theirs. Nevertheless, we
believe that the results of our reimplementation of Rink et al.?s (2011) system in row 1 can be taken to be
roughly the state of the art results on this dataset.
Rows 3?5 show the results of the three systems we introduced. As we can see from row 3, by using
simple lexical patterns in combination with the Levenshtein similarity metric, we achieve an F-score that
is significantly better than that of the structured-feature baseline but significantly worse (at p < 0.01) than
8
All statistical significance tests are paired t-tests with p < 0.05 unless otherwise stated.
1689
Individual System R P F
1 Flat 66.7 58.1 62.1
2 Tree 64.3 55.6 59.6
3 Lexical Patterns 63.9 59.2 61.4
4 Dependencies 4.3 82.9 8.2
5 Rules 11.9 84.4 9.1
Ensemble System R P F
6 Ensemble
(1+2)
69.2 61.3 65.0
7 Ensemble
(1+2+3)
70.4 63.1 66.6
8 Ensemble
(1+2+3+4)
70.0 64.7 67.2
9 Ensemble
(1+2+3+4+5)
71.1 64.8 67.8
10 Ensemble
(1+2+3+4+5)
+ ILP 72.9 66.7 69.6
Single Classifier R P F
11 Single
(1+2)
53.0 73.6 61.7
12 Single
(1+2+3)
54.4 74.7 63.0
13 Single
(1+2+3+4)
56.4 73.7 63.9
14 Single
(1+2+3+4+5)
56.3 74.5 64.1
15 Single
(1+2+3+4+5)
+ ILP 58.9 75.0 66.0
Bagged System R P F
16 Bagging
(1+2)
54.6 73.6 62.7
17 Bagging
(1+2+3)
54.5 73.8 62.7
18 Bagging
(1+2+3+4)
56.9 73.2 64.0
19 Bagging
(1+2+3+4+5)
56.7 73.9 64.2
20 Bagging
(1+2+3+4+5)
+ ILP 59.2 75.5 66.4
Table 5: Medical relation classification results.
that of the flat-feature baseline. On the other hand, the remaining two systems are precision-oriented:
they classify an instance only if they can do so confidently, thus resulting in poor recall.
Rows 6?10 show the results of our ensemble approach when the individual MR classification systems
are added incrementally to the flat-feature baseline. Except for the addition of the dependency-based
system and the hand-crafted rules, which yielded insignificant improvements in F-score, the addition of
all other components yielded significant improvements. In fact, every significant improvement in F-score
is accompanied by a simultaneous rise in recall and precision. The best-performing system is the one
that comprises all of our components, achieving an F-score of 69.6. This translates to a relative error
reduction of 19.8% and a highly significant improvement (p < 0.001) over our reimplementation of
Rink et al.?s (2011) state-of-the-art baseline. The weights learned for the members of the ensemble are
indeed different: both baselines have a weight of 0.3, the rule-based system and the lexical patterns have
a weight of 0.1, and the remaining weight goes to the dependency-based component.
7.3 Additional Comparisons
Given the above results, a natural question is: is an ensemble approach ever needed to combine the
knowledge sources exploited by different systems in order to obtain these improvements? In other words,
can we achieve similar performance by training a single classifier using a feature set containing all the
features currently exploited by different members of the ensemble?
To answer this question, we repeat the experiments in rows 6?10 of Table 5, except that in each ex-
periment we train a single classifier on a feature set formed from the union of those features employed
by all the members of the corresponding ensemble. Results are shown in rows 11?15 of Table 5. In each
of these five experiments the F-score obtained by our ensemble approach is significantly better than that
achieved by the corresponding single-classifier approach. In addition, although we see improvements
in F-score as we add the individual extensions (including ILP) incrementally to the flat-feature base-
line, none of these improvements is statistically significant. Nevertheless, when applied in combination,
these extensions yield a system that is significantly better than the flat-feature baseline. Overall, these
results provide suggestive evidence that to achieve the same level of performance we cannot replace our
ensemble approach with a simpler setup that relies on a single classifier.
Given that our ensemble approach performs better than a single-classifier approach, a relevant question
is: do we have to use our ensemble approach, or can we still achieve similar performance by replacing it
with a generic ensemble learning method such as bagging (Breiman, 1996)?
To answer this question, we repeat the experiments in rows 6-10 of Table 5, except that we train a
committee of classifiers using bagging. Recall that in bagging each classifier in the committee is trained
on a bootstrap sample created by randomly sampling instances with replacement from the training data
until the size of the bootstrap sample is equal to that of the training data. In our implementation, we
train 20 multi-class SVM classifiers using SVM
multiclass
. Given a test instance, each member of the
committee will independently cast a probabilistic vote, and the class that receives the largest number
of probabilistic votes from the committee members will be assigned to the test instance. Results are
shown in rows 16?20 of Table 5. In each of these five experiments, the F-score obtained by bagging
1690
is significantly worse that that achieved by our ensemble approach. In fact, comparing bagging and the
single-classifier approach, their results are statistically indistinguishable in all but one case (row 11 vs.
row 16), where bagging achieves significantly better performance. Like in the single-classifier experi-
ments, in the bagging experiments we see improvements in F-score as we add the individual extensions
(including ILP) incrementally to the flat-feature baseline, although the improvements are significant only
with the addition of ILP and the dependency-based system. Nevertheless, when applied in combination,
these extensions yield a system that is significantly better than the flat-feature baseline. Overall, these
results provide suggestive evidence that to achieve the same level of performance we cannot replace our
ensemble approach with bagging.
8 Error Analysis
To gain additional insights into our ensemble approach and to provide directions for future work, we
conduct an error analysis of our best-performing system.
NTeP confused as TeRP. This is a frequent type of confusion where 34% of the TEST?PROBLEM pairs
that do not have a relation are misclassified as having a ?Test Reveals Problem? relation. Below are two
subcategories of errors commonly made by the system in this confusion category.
? TEST with numeric results followed by PROBLEM concepts in written text
The following example illustrates this confusion:
. . . [
test
mean gradient] 33 mm , [
problem
decreased disc motion] , [
problem
mobile mass in LVOT] ,
[
problem
mild AI] , [
problem
mild to moderate MR] . . .
In sentences like the one above where a TEST concept has a numeric result (result of TEST mean
gradient is 33 mm), since the TEST concept is already associated with its result, it has no relation with
any other concepts in the sentence. While in some cases the system is able to correctly classify the
relation between the TEST concept and the first following PROBLEM concept, in almost all cases, it fails
to propagate this no relation class down through the other PROBLEMs listed in a series following the
TEST concept. For the sentence above, it incorrectly classifies the relation between TEST concept mean
gradient and each of the PROBLEM concepts mobile mass in LVOT, mild AI, and mild to moderate MR
as TeRP instead of NTeP.
? TEST reveals PROBLEM that is consistent with other PROBLEM
This is a common error where a TEST concept is classified as revealing two consistent PROBLEM
concepts when in actuality it only reveals one of the PROBLEMs. Consider the following sentence:
[
test
Radiograph] revealed [
problem
bilateral diffuse granular pattern] consistent with [
problem
surfactant
deficiency] .
In this sentence, PROBLEM concept bilateral diffuse granular pattern is described as being consistent
with another PROBLEM concept surfactant deficiency. While the system correctly classifies the pair (Ra-
diograph, bilateral diffuse granular pattern) as TeRP, it misclassifies the pair (Radiograph, surfactant
deficiency) as TeRP. In case of the second pair, the TEST concept has no relation with the PROBLEM
concept. From this common error type, an insight one can derive is that the system is currently missing
knowledge of the association of the two PROBLEMs w.r.t. each other, and thus in turn cannot make an
informed decision of which of the two PROBLEMs the TEST concept actually reveals.
PIP confused as NPP. The second major confusion in the system?s output concerns misclassifying
PROBLEM concept pairs that are indicative of each other as having ?no relation?. We observe that 39.5%
of the PIP instances get classified as NPP.
? PROBLEM without another PROBLEM
In a sentence, if a PROBLEM concept is actually said to be without another PROBLEM, then such a pair is
commonly misclassified by the classifier into the no-relation class NPP instead of the has-relation class
PIP. An example of this can be found in the sentence ?[
problem
Angio site] was clean , dry , and intact
without [
problem
bleeding] or [
problem
drainage] .?, where PROBLEM Angio site is classified as NPP with
1691
both concepts bleeding and drainage, respectively. Such cases call for domain-specific knowledge that
can aid in identifying attributes of PROBLEMs, like that the PROBLEM concepts bleeding and drainage
are commonly associated attributes of the PROBLEM concept Angio site. With this information the
system is better equipped to recognize that PROBLEM Angio site is related to its attributes.
9 Conclusion
We investigated a new approach to the medical relation classification task, where we employed human-
supplied knowledge to assist the construction of relation classification systems based on sequences, com-
bined them via an ensemble, and then enforced global consistency using constraints in an ILP framework.
Experimental results on the i2b2 corpus show a significant relative error reduction of 19.8% over a state-
of-the-art baseline.
Acknowledgments
We thank the three anonymous reviewers for their detailed and insightful comments on an earlier draft
of this paper. This work was supported in part by NSF Grants IIS-1147644 and IIS-1219142.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In
Proceedings of the Fifth ACM Conference on Digital Libraries, pages 85?94.
Michele Banko, Michael Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction for the Web. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collabora-
tively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD
International Conference on Management of Data, pages 1247?1250.
Leo Breiman. 1996. Bagging predictors. Machine Learning, 24:123?140.
Sergey Brin. 1999. Extracting patterns and relations from the World Wide Web. In The World Wide Web and
Databases, pages 172?183. Springer.
Razvan Bunescu and Raymond Mooney. 2007. Learning to extract relations from the Web using minimal su-
pervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages
576?583.
Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. SMOTE: Synthetic
minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321?357.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011.
Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machine-learned
solutions for three stages of clinical information extraction: The state of the art at i2b2 2010. Journal of the
American Medical Informatics Association, 18(5):557?562.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S
Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the Web: An experimental
study. Artificial Intelligence, 165(1):91?134.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-scale learning of relation-extraction rules
with distant supervision from the Web. In Proceedings of the International Semantic Web Conference, pages
263?278.
Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10(8):707?710.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant supervision for relation
extraction with an incomplete knowledge base. In Proceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 777?782.
1692
Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics: System Demonstrations, pages 55?60.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky. 2009. Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language Processing of the AFNLP, pages 1003?1011.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision
from external semantic repositories. In Proceedings of the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies, pages 277?282.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting
semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Computational Linguistics, pages 113?120.
Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In
Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 474?479.
Bryan Rink, Sanda Harabagiu, and Kirk Roberts. 2011. Automatic extraction of relations between medical
concepts in clinical texts. Journal of the American Medical Informatics Association, 18(5):594?600.
Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language
tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning, pages 1?8.
Benjamin Rozenfeld and Ronen Feldman. 2008. Self-supervised relation extraction from the Web. Knowledge
and Information Systems, 17(1):17?33.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Technology Conference of the North American Chapter of the
Association for Computational Linguistics, pages 304?311.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith, and Daniel M. Ogilvie. 1966. The General Inquirer: A
Computer Approach to Content Analysis. MIT Press, Cambridge, Massachusetts.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. In
Proceedings of the 16th International World Wide Web Conference, pages 697?706.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Robust information extraction with perceptrons. In Proceed-
ings of the NIST 2007 Automatic Content Extraction Workshop.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine
learning for interdependent and structured output spaces. In Proceedings of the 21st International Conference
on Machine Learning, pages 104?112.
Peter Turney. 1995. Cost-sensitive classification: Empirical evaluation of a hybrid genetic decision tree induction
algorithm. Journal of Artificial Intelligence Research, 2:369?409.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 i2b2/VA Challenge on concepts,
assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552?
556.
GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang. 2005. Exploring various knowledge in relation extraction.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 427?434.
GuoDong Zhou, Min Zhang, DongHong Ji, and QiaoMing Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In Proceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning, pages 728?736.
Xiaodan Zhu, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Berry de Bruijn. 2013. Detecting concept
relations in clinical text: Insights from a state-of-the-art model. Journal of Biomedical Informatics, 46(2):275?
285.
1693
Proceedings of NAACL-HLT 2013, pages 918?927,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Classifying Temporal Relations with Rich Linguistic Knowledge
Jennifer D?Souza and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{jld082000,vince}@hlt.utdallas.edu
Abstract
We examine the task of temporal relation clas-
sification. Unlike existing approaches to this
task, we (1) classify an event-event or event-
time pair as one of the 14 temporal relations
defined in the TimeBank corpus, rather than
as one of the six relations collapsed from the
original 14; (2) employ sophisticated linguis-
tic knowledge derived from a variety of se-
mantic and discourse relations, rather than fo-
cusing on morpho-syntactic knowledge; and
(3) leverage a novel combination of rule-based
and learning-based approaches, rather than re-
lying solely on one or the other. Experiments
with the TimeBank corpus demonstrate that
our knowledge-rich, hybrid approach yields
a 15?16% relative reduction in error over a
state-of-the-art learning-based baseline sys-
tem.
1 Introduction
Recent years have seen a surge of interest in tem-
poral information extraction (IE). Temporal relation
classification, one of the most important temporal
IE tasks, involves classifying a given event-event
pair or event-time pair as one of a set of predefined
temporal relations. The creation of the TimeBank
corpus (Pustejovsky et al, 2003) and the organiza-
tion of the TempEval-1 (Verhagen et al, 2007) and
TempEval-2 (Verhagen et al, 2010) evaluation ex-
ercises have facilitated the development and evalua-
tion of temporal relation classification systems.
Our goal in this paper is to advance the state of
the art in temporal relation classification. Our work
differs from existing work with respect to both the
complexity of the task we are addressing and the ap-
proach we adopt. Regarding task complexity, rather
than focus on six temporal relations as is typically
done in previous work (see Section 2 for more infor-
mation), we address an arguably more challenging
version of the task where we consider all the 14 re-
lations originally defined in the TimeBank corpus.
Our approach to temporal relation classification
can be distinguished from existing approaches in
two respects. The first involves a large-scale ex-
pansion of the linguistic features made available
to the classification system. Recall that exist-
ing approaches have relied primarily on morpho-
syntactic features as well as a few semantic fea-
tures extracted from WordNet synsets and VerbO-
cean?s (Chklovski and Pantel, 2004) semantic rela-
tions. On the other hand, we propose not only novel
lexical and grammatical features, but also sophis-
ticated features involving semantics and discourse.
Most notably, we propose (1) semantic features en-
coding a variety of semantic relations, including
PropBank-style predicate-argument relations as well
as those extracted from the Merriam-Webster dictio-
nary, and (2) discourse features encoding automat-
ically computed Penn Discourse TreeBank (PDTB)
style (Prasad et al, 2008) discourse relations.
Second, while the vast majority of existing ap-
proaches to temporal relation classification are
learning-based, we propose a system architecture in
which we combine a learning-based approach and a
rule-based approach. Our motivation behind adopt-
ing a hybrid approach stems from two hypotheses.
First, a rule-based method could better handle the
skewed class distribution underlying the dataset for
918
our 14-class classification problem. Second, better
decision rules could be formed by leveraging hu-
man insights to combine the available linguistic fea-
tures than by using fully automatic machine learn-
ing methods. Note that while rule-based approaches
have been shown to underperform learning-based
approaches on this task (Mani et al, 2006), to our
knowledge they have not been used in combination
with learning-based approaches. Moreover, while
the rules employed in previous work are created
based on intuition (e.g., Mani et al (2006), Pus?cas?u
(2007)), our rules are created in a data-driven man-
ner via a manual inspection of the annotated tempo-
ral relations in the TimeBank corpus.
Experiments on the TimeBank corpus demon-
strate the effectiveness of our knowledge-rich, hy-
brid approach to temporal relation classification: it
yields a 15?16% relative reduction in error over a
state-of-the-art learning-based baseline system.
To our knowledge, we are the first to (1) report re-
sults for the 14-class temporal relation classification
task on the TimeBank (v1.2) corpus; (2) success-
fully employ automatically computed PDTB-style
discourse relations to improve performance on this
task; and (3) show that a hybrid approach to this
task can yield better results than either a rule-based
or learning-based approach. Note that hybrid ap-
proaches in this spirit were popular in the natural
language processing community back in the mid-90s
(Klavans and Resnik, 1994). We believe that they
are among the most competitive approaches to lan-
guage processing tasks that require complex reason-
ing and should be given more attention in the com-
munity. We release the complete set of rules that we
mined from the TimeBank corpus and used in our
rule-based approach in hopes that our insights into
how features can be combined as decision rules can
benefit researchers interested in this task.
The rest of the paper is organized as follows. Sec-
tion 2 provides an overview of the TimeBank cor-
pus. Sections 3 and 4 describe the baseline system
and our approach, respectively. We present evalua-
tion results in Section 5 and conclude in Section 6.
2 Corpus
For evaluation, we use the TimeBank (v1.2) cor-
pus, which consists of 183 newswire articles. In
each article, the events, times, and their temporal re-
lations are marked up. An event, which can be a
tensed verb, adjective, or nominal, contains various
attributes, including the class of event, tense, aspect,
polarity, and modality. A time expression has a class
attribute, which specifies whether it is a date, time,
duration, or set, and its value is normalized based on
TIMEX3. A temporal relation can be an order rela-
tion, which orders two events (as in sentence (1)), or
an anchor relation, which anchors an event to a time
expression (as in sentence (2)).
(1) A steep rise in world oil prices fol-
lowed the Kuwait invasion.
(2) We are there to stay for a long period.
Each temporal relation has a type. For example,
the relation defined on rise and invasion in (1) has
type After, whereas the relation defined on stay and
period in (2) has type During. Note that a temporal
relation is defined on an ordered pair. For exam-
ple, in (1), the pair (rise, invasion) has type After,
whereas the pair (invasion, rise) has type Before).
14 relation types are defined and used to annotate
the temporal relations in the TimeBank corpus. Ta-
ble 1 provides a brief description of these relation
types and the relevant statistics.
In our experiments, we assume that our tempo-
ral relation classification system is given an event-
event or event-time pair that is known to belong to
one of the 14 relation types defined in TimeBank and
aims to determine its relation type. Following pre-
vious evaluations of the temporal relation classifica-
tion task on the TimeBank corpus (e.g., Mani et al
(2006), Chambers et al (2007)) and in TempEval-
1/2, we assume as input gold events and time ex-
pressions.
Unlike Mani et al (2006) and Chambers et al
(2007), who focus on six relation types (Simul-
taneous, Before, IBefore, Begins, Ends, and In-
cludes), we report results on 14 relation types. Note
that the aforementioned six relation types are cho-
sen by (1) discarding During, During Inv, and
Identity, and (2) combining the two relation types
in each of the five pairs, namely (Before, After),
(IBefore, IAfter), (Includes, Is Included), (Be-
gins, Begun By), and (Ends, Ended By), into a sin-
gle type because they are inverses of each other. In
other words, if a relation instance (e1, e2) is anno-
919
Id Relation Description Total % E-E E-T
1 Simultaneous e1 and e2 happen at the same time or are temporally distinguishable 660 (13.3) 599 61
2 Identity e1 and e2 are coreferent 702 (14.1) 696 6
3 Before e1 happens before e2 in time 689 (13.9) 639 50
4 After e1 happens after e2 in time 744 (15) 681 63
5 IBefore e1 happens immediately before e2 in time 39 (0.8) 38 1
6 IAfter e1 happens immediately after e2 in time 28 (0.6) 25 3
7 Includes As in Ed arrived in Seoul last Sunday (e1=last Sunday; e2=arrived) 758 (15.3) 318 440
8 Is Included As in Ed arrived in Seoul last Sunday (e1=arrived; e2=last Sunday) 762 (15.3) 201 561
9 During e1 persists throughout duration e2 102 (2.1) 19 83
10 During Inv e2 persists throughout duration e1 124 (2.5) 44 80
11 Begins e1 marks the beginning of e2 66 (1.3) 44 22
12 Begun By e2 marks the beginning of e1 61 (1.2) 32 29
13 Ends e1 marks the end of e2 66 (1.3) 21 45
14 Ended By e2 marks the end of e1 170 (3.42) 93 77
Table 1: The 14 temporal relations and their frequency of occurrences in TimeBank (v1.2). Each relation is defined
on an ordered event-event or event-time pair (e1,e2). The ?Total? and ?%? columns show the number and percentage
of instances annotated with the corresponding relation in the corpus, respectively, and the ?E-E? and ?E-T? columns
show the breakdown by the number of event-event pairs and event-time pairs.
tated as After, it is replaced with the instance (e2,
e1) with class Before, and subsequently a relation
classifier is presented with (e2, e1) but not (e1, e2).
On the other hand, our 14-class task is arguably
more challenging since our system has to further dis-
tinguish a relation type from its inverse given an in-
stance in which the two elements are in arbitrary or-
der.
3 Baseline Temporal Relation Classifier
Since the currently best-performing systems for
temporal relation classification are learning-based,
we will employ a learning-based system as our base-
line. Below we describe how we train this baseline.
Without loss of generality, assume that (e1,e2) is
an event-event/event-time pair such that (1) e1 pre-
cedes e2 in the associated text and (2) (e1,e2) be-
longs to one of the 14 TimeBank temporal rela-
tion types. We create one training instance for each
event-event/event-time pair in a training document
that satisfies the two conditions above, labeling it
with the relation type that exists between e1 and e2.
To build a strong baseline, we represent each
instance using 68 linguistic features modeled af-
ter the top-performing temporal relation classifica-
tion systems on TimeBank (e.g., Mani et al (2006),
Chambers et al (2007)) and in the TempEval shared
tasks (e.g., Min et al (2007), Pus?cas?u (2007), Ha et
al. (2010), Llorens et al (2010), Mirroshandel and
Ghassem-Sani (2011)).1 These features can be di-
vided into six categories, as described below.
Lexical (5). The strings of e1 and e2, the head
words of e1 and e2, and a binary feature indicating
whether e1 and e2 have the same string.
Grammatical (33). The POS tags of the head
words of e1 and e2, the POS tags of the five to-
kens preceding and following e1 and e2, the POS
bigram formed from the head word of e1 and its pre-
ceding token, the POS bigram formed from the head
word of e2 and its preceding token, the POS tag pair
formed from the head words of e1 and e2, the prepo-
sitional lexeme of the prepositional phrase (PP) if e1
is headed by a PP (Chambers et al, 2007), the prepo-
sitional lexeme of the PP if e2 is headed by a PP, the
prepositional lexeme of the PP if e1 is governed by
a PP (Mirroshandel and Ghassem-Sani, 2011), the
prepositional lexeme of the PP if e2 is governed by
a PP, the POS of the head of the verb phrase (VP) if
e1 is governed by a VP, the POS of the head of the
VP if e2 is governed by a VP, whether e1 syntacti-
cally dominates e2 (Chambers et al, 2007), and the
shortest path from e1 to e2 in the associated syntac-
tic parse tree. We obtain parse trees and POS tags
using the Stanford CoreNLP tool.2
1Note, however, that these features were designed for the
arguably simpler 6-class temporal relation classification tasks.
2http://nlp.stanford.edu/software/
corenlp.shtml
920
Entity attributes (13). The tense, aspect, modal-
ity, polarity, and event type of e1 and e2 if they are
events (if one of them is a time expression, then the
class attribute will be set to its class and the rest of
them will have the value NULL), pairwise features
formed by pairing up the tense values, the aspect
values, and the class values of e1 and e2.
Semantic (7). The subordinating temporal role to-
ken of e1 if it appears within a temporal semantic
role argument (Llorens et al, 2010), the subordinat-
ing temporal role token of e2 if it appears within a
temporal semantic role argument, the first WordNet
synset to which e1 belongs, the first WordNet synset
to which e2 belongs, and whether e1 and e2 are in the
happens-before, happens-after, and similar relation
according to VerbOcean.3
Distance (1). Are e1 and e2 in the same sentence?
DCT related (3). The temporal relation type be-
tween e1 and the document creation time (DCT) [its
value can be one of the 14 relation types, or NULL
if no relation exists], the temporal relation type be-
tween e2 and the DCT, and whether e1 and e2 have
different relation types with the DCT.
After creating the training instances, we train
a 14-class classifier on them using SVMmulticlass
(Tsochantaridis et al, 2004).4 We then use it to
make predictions on the test instances, which are
generated in the same way as the training instances.
4 Our Hybrid Approach
In this section, we describe our hybrid learning-
based and rule-based approach to temporal relation
classification. Section 4.1 describes our novel fea-
tures, which will be used to augment the baseline
feature set (see Section 3) to train a temporal rela-
tion classifier. Section 4.2 outlines our manual rule
creation process. Section 4.3 discusses how we com-
bine our hand-crafted rules and the learned classifier
to make predictions in our hybrid approach.
3happens-after is not a relation in VerbOcean: we create this
relation simply by inverting the happens-before relation.
4For all the experiments involving SVMmulticlass, we set C,
the regularization parameter, to 10,000, since preliminary ex-
periments indicate that preferring generalization to overfitting
(by setting C to a small value) tends to yield poorer classifica-
tion performance. The remaining learning parameters are set to
their default values.
4.1 Six Types of New Features
4.1.1 Pairwise Features
Recall that some of the features in the baseline fea-
ture set are computed based on either e1 or e2 but
not both. Since our task is to predict the relation be-
tween them, we hypothesize that pairwise features,
which are computed based on both elements, could
better capture the relationship between them.
Specifically, we introduce pairwise versions of the
head word feature and the two prepositional lexeme-
based features in the baseline. In addition, we create
two quadruple-wise features, one by pairing up the
tense and class attribute values of e1 with those of
e2, and the other by pairing up their tense and as-
pect values. Next, we create two trace features, one
based on prepositions and the other on verbs, since
prepositions and verb tenses have been shown to
play an important role in temporal relation classifi-
cation The preposition trace feature is computed by
(1) collecting the list of prepositions along the path
from e1/e2 to the root of its syntactic parse trees, and
(2) concatenating the resulting lists computed from
e1 and e2. The verb trace feature is computed in a
similar manner, except that we collect the POS tags
of the verbs appearing in the corresponding paths.
4.1.2 Dependency Relations
We introduce features computed based on de-
pendency parse trees obtained via the Stanford
CoreNLP tool, motivated by our observation that
some dependency relation types are more closely
associated with certain temporal relation types than
with others. Let us illustrate with an example:
(3) Ed changed his plans as the mood took
him.
In (3), there is a adverbial clause modifier depen-
dency between changed and took, because took ap-
pears in an adverbial clause (headed by as) modify-
ing changed. Intuitively, if the two events partici-
pate in this type of dependency relation and the ad-
verbial clause is headed by as and there is a tempo-
ral relation between them, then it is likely that this
temporal relation is Simultaneous. While the tem-
poral relation type is dependent on the connective
heading the adverbial clause, in general an adverbial
clause modifier dependency between two events im-
plies that their temporal relation is likely to be Si-
921
multaneous, Before, or After.
Given the potential usefulness of dependency re-
lations for temporal relation classification, we cre-
ate dependency-based features as follows. For each
of the 25 dependency relation types produced by
the Stanford parser, we create four binary features:
whether e1/e2 is the governing entity in the relation,
and whether e1/e2 is the dependent in the relation.
4.1.3 Webster Relations
Some events are not connected by a dependency re-
lation but by a lexical relation. We hypothesize that
some of these lexical relations could be useful for
temporal relation classification. Consider the fol-
lowing example.
(4) The phony war has finished and the real
referendum campaign has begun.
In this sentence, the two events, finished and be-
gun, are connected by an antonym relation. Statisti-
cally speaking, if (1) two events are in two clauses
connected by a coordinating conjunction (e.g., and),
(2) one is an antonym of the other, and (3) there is
a temporal relation between them, then the temporal
relation is likely to be Simultaneous.
Given the potential usefulness of lexical rela-
tions for temporal relation classification, we cre-
ate features based on four types of lexical re-
lations present in Webster?s online thesaurus5,
namely synonyms, related-words, near-antonyms,
and antonyms. Specifically, for each event e appear-
ing in TimeBank, we first use the head word of e to
retrieve four lists, which are the lists corresponding
to the synonyms, related words, near-antonyms, and
antonyms of e. Then, given a training/test instance
involving e1 and e2, we create eight binary features:
whether e1 appears in e2?s list of synonyms/related
words/near-antonyms/antonyms, and whether e2 ap-
pears in e1?s list of synonyms/related words/near-
antonyms/antonyms.
4.1.4 WordNet Relations
Previous uses of WordNet for temporal relation clas-
sification are limited to synsets (e.g., Llorens et al
(2010)). We hypothesize that other WordNet lexical
relations could also be useful for the task. Specif-
ically, we employ four types of WordNet relations,
5http://www.merriam-webster.com/
namely hypernyms, hyponyms, troponyms, and sim-
ilar, to create eight binary features for temporal rela-
tion classification. These eight features are created
from the four WordNet relations in the same way as
the eight features were created from the four Web-
ster relations in the previous subsection.
4.1.5 Predicate-Argument Relations
So far we have exploited lexical and dependency
relations for temporal relation classification. We
hypothesize that semantic relations, in particular
predicate-argument relations, could be useful for the
task. Consider the following example.
(5) ?What sector is stepping forward to
pick up the slack?? he asked.
Using SENNA (Collobert et al, 2011), a PropBank-
style semantic role labeler, we know that forward is
in the directional argument of the predicate stepping.
This enables us to infer that an Includes relation ex-
ists between stepping and forward since intuitively
an action includes a direction.
As another example, consider another PropBank-
style predicate-argument relation, cause. Assuming
that e2 is in e1?s cause argument, we can infer that
e2 occurs Before e1 since intuitively the cause of an
action precedes the action.
Consequently, we create features for tempo-
ral relation classification based on four types
of PropBank-style predicate-argument relations,
namely directional, manner, temporal, and cause.
Specifically, using SENNA?s output, we create four
binary features that encode whether argument e2 is
related to predicate e1 through the four types of rela-
tions, and we create another four binary features that
encode whether argument e1 is related to predicate
e2 through the four types of relations.
4.1.6 Discourse Relations
Rhetorical relations such as causation, elaboration
and enablement could aid in tracking the temporal
progression of the discourse (Hitzeman et al, 1995).
Hence, unlike syntactic dependencies and predicate-
argument relations through which we can identify
intra-sentential temporal relations, discourse rela-
tions can potentially be exploited to discover both
inter-sentential and intra-sentential temporal rela-
tions. However, no recent work has attempted to
use discourse relations for temporal relation clas-
922
(6) { Arg1 Hewlett-Packard Co. said it raised its stake in Octel Communications Corp. to 8.5% of the
common shares outstanding. Arg1} { Arg2 RESTATEMENT In a Securities and Exchange Commis-
sion filing, Hewlett-Packard said it now holds 1,384,119 Octel common shares Arg2}.
(7) { Arg1 Reports said that Saudi Arabia told U.S. oil companies of a 15?20 percent cutback in its oil
supply in September. Arg1} { Conn SYNCHRONY Meanwhile Conn} { Arg2 Egypt?s Middle East
Agency said Thursday that Saddam was the target of an assassination attempt. Arg2}
Table 2: Examples illustrating the usefulness of discourse relations for temporal relation classification.
sification. In this subsection, we examine whether
we can improve a temporal relation identifier via
explicit and implicit PDTB-style discourse relations
automatically extracted by Lin et al?s (2013) end-to-
end discourse parser.
Let us first review PDTB-style discourse rela-
tions. Each relation is represented by a triple (Arg1,
sense, Arg2), where Arg1 and Arg2 are the two ar-
guments of the relation and sense is the sense/type
of the relation. A discourse relation can be explicit
or implicit. An explicit relation is triggered by a dis-
course connective. On the other hand, an implicit
relation is not triggered by a discourse connective,
and may exist only between two consecutive sen-
tences. Generally, implicit relations are much harder
to identify than their explicit counterparts.
Next, to motivate why discourse relations can be
useful for temporal relation classification, we use
two examples (see Table 2), one involving an im-
plicit relation (Example (6)) and the other an explicit
relation (Example (7)). For convenience, both sen-
tences are also annotated using Lin et al?s (2013)
discourse parser, which marks up the two arguments
with the Arg1 and Arg2 tags and outputs the rela-
tion sense next to the beginning of Arg2.
In (6), we aim to determine the order relation be-
tween the reporting event said and the occurrence
event filing. The parser determines that a RESTATE-
MENT implicit relation exists between the two sen-
tences. Intuitively, if no asynchronous relations can
be found among the events in two discourse units
connected by the RESTATEMENT relation, then the
temporal relation between two temporally linked
events within these units is likely to be either Iden-
tity or Simultaneous. In this case, we can rule out
Identity: since said and filing belong to different
event classes, they are not coreferent.
In (7), we aim to determine the anchor relation
between the reporting event said and the date Thurs-
day. The parser determines that a SYNCHRONY
explicit relation triggered by Meanwhile exists be-
tween the two sentences. Intuitively, if a temporally
related reporting event and date occur within differ-
ent discourse units connected by the SYNCHRONY
relation, then it is likely that the event Is Included
in the date. Note that without this discourse relation,
it could be difficult for a machine to confidently as-
sociate a reporting event with a date occurring in a
different discourse segment.
Given the potential usefulness of discourse rela-
tions for temporal relation classification, we create
four features based on discourse relations. In the
first feature, if e1 is in Arg1, e2 is in Arg2, and Arg1
and Arg2 possess an explicit relation with sense s,
then its feature value is s; otherwise its value is
NULL. In the second feature, if e2 is in Arg1, e1 is in
Arg2, and Arg1 and Arg2 possess a explicit relation
with sense s, then its feature value is s; otherwise
its value is NULL. The third and fourth features are
computed in the same way as the first two features,
except that they are computed over implicit rather
than explicit relations.
4.2 Manual Rule Creation
As noted before, we adopt a hybrid learning-based
and rule-based approach to temporal relation clas-
sification. Hence, in addition to training a tempo-
ral relation classifier, we also manually design a set
of rules in which each rule returns a temporal rela-
tion type for a given test instance. We hypothesize
that a rule-based approach can complement a purely
learning-based approach, since a human could com-
bine the available linguistic features into rules using
commonsense knowledge that may not be accessible
to a learning algorithm.
The design of the rules is partly based on intu-
923
ition and partly data-driven: we first use our intu-
ition to come up with a rule and then manually re-
fine it based on the observations we made on the
TimeBank data. For this purpose, we partition the
TimeBank documents into five folds of roughly the
same size, reserving three folds for developing our
rules and using the remaining two folds for evaluat-
ing final system performance. We order these rules
in decreasing order of accuracy, where the accuracy
of a rule is defined as the number of times the rule
yields the correct temporal relation type divided by
the number of times it is applied, as measured on the
three development folds. A new instance is classi-
fied using the first applicable rule in the ruleset.
Some of these rules were shown in the previ-
ous subsection when we motivated each feature type
with examples. The complete set of rules can be ac-
cessed via our website.6
4.3 Combining Rules and Machine Learning
We investigate three ways to combine the hand-
crafted rules and the machine-learned classifier.
In the first method, we employ all of the rules as
additional features for training the classifier. The
value of each such feature is the temporal relation
type predicted by the corresponding rule.
The second method can be viewed as an extension
of the first one. Given a test instance, we first apply
to it the ruleset composed only of rules that are at
least 80% accurate. If none of the rules is applicable,
we classify it using the classifier employed in the
first method.7
The third method is essentially the same as the
second, except we do not employ the rules as fea-
tures when training the classifier.
5 Evaluation
5.1 Experimental Setup
Dataset. As mentioned before, we partition the
183 documents in the TimeBank (v1.2) corpus into
five folds of roughly the same size, reserving three
folds (say Folds 1?3) for manual rule development
6http://www.hlt.utdallas.edu/
?
jld082000/
temporal-relations/
7Although this classifier is applied to only those test in-
stances that the rules cannot handle, we did not retrain it on
only those training instances that the rules cannot handle.
and using the remaining two folds (say Folds 4?5)
for testing. We perform two-fold cross-validation
experiments using the two test folds. In the first fold
experiment, we train a temporal relation classifier on
Folds 1?4 and test on Fold 5; and in the second fold
experiment, we train the classifier on all but Fold 4
and test on Fold 4. The results reported in the rest of
the paper are averaged over the two test folds.
Evaluation metrics. We employ accuracy (Acc)
and macro F-score (Fma). Accuracy is the per-
centage of correctly classified test instances, and is
the standard evaluation metric for temporal relation
classification. Since each test instance belongs to
one of the 14 temporal relation types, accuracy is the
same as micro F-score. On the other hand, macro F-
score is rarely used to evaluate this task. We chose it
because it could provide insights into how well our
approach performs on the minority classes.
5.2 Results and Discussion
Table 3 shows the two-fold cross-validation results
for our 14-class temporal relation classification task.
The six columns of the table correspond to six dif-
ferent system architectures. The ?Feature? column
corresponds to a purely learning-based architecture
where the results are obtained simply by training a
temporal relation classifier using the available fea-
tures. The next two columns correspond to two
purely rule-based architectures, differing by whether
all rules are used regardless of their accuracy or
whether only high-accuracy rules (i.e., those that are
at least 80% accurate) are used. The rightmost three
columns correspond to the three ways of combining
rules and machine learning described in Section 4.3.
On the other hand, the rows of the table differ in
terms of what features are available to a system. In
row 1, only the baseline features are available. In the
subsequent rows, the six types of features discussed
in Section 4 are added incrementally to the baseline
feature set. This means that the last row corresponds
to the case where all feature types are used.
A point merits clarification. It may not be imme-
diately clear how to interpret the results under, for
instance, the ?All Rules? column. In other words,
it may not be clear what it means to add the six
types of features incrementally to a rule-based sys-
tem. Recall that one of our goals is to compare
a purely learning-based system with a purely rule-
924
Features All Rules All Rules with Features + Rules + Rules + Features +
accuracy ? 0.8 Rules as Features Features Rules as Features
Feature Type Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma Acc Fma
1 Baseline 45.3 24.9 ? ? ? ? ? ? ? ? ? ?
2 + Pairwise 46.5 25.8 37.6 26.5 5.1 13.9 46.7 26.5 48.0 31.9 48.2 32.1
3 + Dependencies 47.0 25.9 39.0 27.8 6.9 15.7 47.2 26.7 49.2 32.3 49.2 32.6
4 + WordNet 46.9 26.0 43.5 30.4 6.9 15.7 47.5 26.8 49.2 32.3 49.5 32.8
5 + Webster 46.9 25.8 43.3 29.9 6.9 15.7 48.1 26.8 49.2 32.0 50.1 33.1
6 + PropBank 47.2 26.0 44.3 30.5 8.1 16.6 48.0 26.8 49.5 32.2 50.0 33.0
7 + Discourse 48.1 26.6 47.5 35.1 12.8 23.3 48.9 27.5 53.0 36.0 53.4 36.6
Table 3: Two-fold cross-validation accuracies and macro F-scores as features are added incrementally to the baseline.
based system, since we hypothesized that humans
may be better at combining the available features
to form rules than a learning algorithm would be.
To facilitate this comparison, all and only those fea-
tures that are available to a learning-based system in
a given row can be used in hand-crafting the rules
of the rule-based system in the same row. The other
columns involving the use of rules can be interpreted
in a similar manner.
The highest accuracy and macro F-score are
achieved when all types of features are used in
combination with the ?Rules + Features + Rules
as Features? architecture. Specifically, this system
achieves an accuracy of 53.4% and a macro F-score
of 36.6% on the 2000-instance test set. This trans-
lates to a relative error reduction of 15?16% in com-
parison to the baseline result shown in row 1. A
closer examination of these results reveals that the
hand-crafted rules used by the system correctly clas-
sify 239 of the 305 test instances to which they are
applicable. In other words, the rules achieve a preci-
sion of 78.3% and a recall of 15.3% on the test data.
Our results suggest that the rules are effective at
improving performance when they are used to make
classification decisions prior to the application of
the classifier, as the performance of the ?Rules +
Features + Rules as Features? architecture is sig-
nificantly better than that of the ?Features + Rules
as Features? architecture.8 On the other hand, the
?Rules + Features + Rules as Features? architecture
does not benefit from the use of rules as features,
as its performance is statistically indistinguishable
from that of the ?Rules + Features? architecture.
Nevertheless, both ?Rules + Features + Rules as
Features? and ?Rules + Features? are significantly
8Unless otherwise stated, all statistical significance tests are
paired t-tests, with p < 0.05.
better than the remaining four architectures. This
suggests that the best-performing approach for our
14-class temporal relation classification task is the
hybrid approach where high-accuracy rules are first
applied and then the learned classifier is used to clas-
sify those cases that cannot be handled by the rules.
Among the remaining four architectures, ?All
Rules with accuracy ? 0.8?, the version of the rule-
based architecture where only the high-accuracy
rules are used, performs significantly worse than the
others, presumably because the coverage of the rule-
set is low. The results of the two feature-based archi-
tectures, ?Features? and ?Features + Rules as Fea-
tures?, are statistically indistinguishable from each
other at the p < 0.01 level. At the p < 0.05
level, however, their results are mixed: ?Features +
Rules as Features? is better than ?Features? accord-
ing to accuracy, whereas the reverse is true accord-
ing to macro F-score. Combining these results with
those we discussed above concerning the ?Rules +
Features? and ?Rules + Features + Rules as Fea-
tures? architectures, we can conclude that the fea-
tures encoding the hand-crafted rules are (mildly)
useful only when used in combination with a weak-
performing system. Finally, comparing the ?Fea-
tures? architecture and the ?All Rules? architecture,
we also see mixed results: ?Features? is better than
?All Rules? according to accuracy, whereas the re-
verse is true according to macro F-score. These
results confirm our earlier hypothesis that the rule-
based system is indeed better at identifying instances
of minority relation types.
Next, to determine whether the addition of a par-
ticular type of features to the feature set is use-
ful, we apply the paired t-test to each pair of ad-
jacent rows in Table 3. We found that adding
pairwise features, dependency relations, and most
925
Event-Event Event-Time
Feature Type Acc Fma Acc Fma
1 Baseline 36.7 15.6 63.3 19.2
2 + Pairwise 40.4 25.4 64.7 24.2
3 + Dependencies 42.4 28.4 64.9 25.4
4 + WordNet 42.6 28.1 64.7 25.3
5 + Webster 43.0 29.7 64.6 25.3
6 + PropBank 43.2 28.6 64.3 25.1
7 + Discourse 46.8 36.3 65.4 26.4
Table 4: Event-event and event-time classification results
of our best system (Rules + Features+ Rules as features).
importantly, discourse relations significantly im-
proves both accuracy and macro F-score (p < 0.05).
Adding the Webster relations improves accuracy at a
slightly lower significance level (p < 0.07) but does
not significantly improve macro F-score. Some-
what counter-intuitively, the WordNet and predicate-
argument relations are not useful. We speculate that
their failure to improve performance could be at-
tributed to the fact that these relations are extracted
by imperfect analyzers. Additional experiments in-
volving the use of gold-standard quality features are
needed to precisely determine the reason.
Recall that the results shown in Table 3 were com-
puted over both the order (i.e., event-event) and an-
chor (i.e., event-time) temporal relations. To gain
additional insights into our best-performing system,
we show in Table 4 its performance on classify-
ing event-event and event-time relations separately.
In comparison to the baseline, both accuracy and
macro F-score increase significantly when our sys-
tem is used in combination with all feature types.
In particular, our system yields a relative error re-
duction of 16?25% for event-event classification and
6?9% for event-time classification over the base-
line. The pairwise features, as well as dependency
relations and discourse relations, contribute signif-
icantly to the classification of both event-event and
event-time relations.
Finally, we show in Table 5 the per-class results
of the baseline system and our best-performing sys-
tem. As we can see, our system performs signifi-
cantly better than the baseline on all relation types,
owing to a simultaneous rise in recall and precision.
6 Conclusions
We have investigated a knowledge-rich, hybrid ap-
proach to the 14-class temporal relation classifica-
Baseline Our System
Relation R P F R P F
Simultaneous 22.5 30.5 25.9 29.5 39.5 33.8
Identity 56.5 51.5 53.9 59.0 57.5 58.2
Before 39.5 38.5 39.0 50.5 50.5 50.5
After 50.5 35.0 41.4 59.5 44.5 50.9
IBefore 0.0 0.0 0.0 32.5 85.5 47.1
IAfter 0.0 0.0 0.0 5.5 50.0 9.9
Includes 54.5 50.5 52.4 61.0 55.5 58.1
Is Included 71.5 64.5 67.8 74.5 65.0 69.4
During 11.0 31.0 16.2 19.0 34.5 24.5
During Inv 14.0 20.0 16.5 19.5 40.5 26.3
Begins 4.5 10.0 6.2 37.0 43.5 40.0
Begun By 6.5 14.5 9.0 35.0 44.0 39.0
Ends 6.5 10.0 7.9 23.5 70.0 35.2
Ended By 9.0 10.0 9.5 29.0 26.5 27.7
Table 5: Per-class results of the baseline system and our
best system (Rules + Features+ Rules as features).
tion task. Results on the TimeBank corpus show
that our approach achieves a relative error reduction
of 15?16% over a learning-based baseline that em-
ploys a state-of-the-art feature set. Our results sug-
gest that (1) the pairwise features, dependency rela-
tions, and discourse relations are useful for temporal
relation classification; and (2) hand-crafted rules can
better handle the skewed class distribution underly-
ing our dataset via improving minority class predic-
tion. To our knowledge, we are the first to (1) re-
port results for the 14-class temporal relation clas-
sification task on TimeBank; (2) successfully em-
ploy PDTB-style discourse relations to improve this
task; and (3) show that a hybrid approach to this task
can yield better results than either a rule-based or
learning-based approach. To stimulate research on
this task, we make our complete set of hand-crafted
rules available to other researchers. We believe that
hybrid rule-based and learning-based approaches are
promising approaches to language processing tasks
that require complex reasoning and hope that they
will be given more attention in the community.
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier draft
of the paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142. Any
opinions, findings, or conclusions expressed in this
paper are those of the authors and do not necessarily
reflect the views or official policies of NSF.
926
References
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Companion Vol-
ume: Proceedings of the Demo and Poster Sessions,
pages 173?176.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 33?40.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Eun Young Ha, Alok Baikadi, Carlyle Licata, and James
Lester. 2010. NCSU: Modeling temporal relations
with markov logic and lexical ontology. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 341?344.
Janet Hitzeman, Marc Moens, and Claire Grover. 1995.
Algorithms for analysing the temporal structure of dis-
course. In Proceedings of the 7th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 253?260.
Judith Klavans and Philip Resnik, editors. 1994. The
Balancing Act: Combining Symbolic and Statistical
Approaches to Language. Association for Computa-
tional Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2013.
A PDTB-styled end-to-end discourse parser. Natural
Language Engineering (to appear).
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. TIPSem (English and Spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluation, pages 284?291.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 753?760.
Congmin Min, Munirathnam Srikanth, and Abraham
Fowler. 2007. LCC-TE: A hybrid approach to tem-
poral relation identification in news text. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007), pages 219?222.
Seyed Abolghasem Mirroshandel and Gholamreza
Ghassem-Sani. 2011. Temporal relation extraction
using expectation maximization. In Proceedings of the
International Conference Recent Advances in Natural
Language Processing 2011, pages 218?225.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation.
Georgiana Pus?cas?u. 2007. WVALI: Temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 484?487.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew
See, David Day, Lisa Ferro, Robert Gaizauskas, Mar-
cia Lazo, Andrea Setzer, and Beth Sundheim. 2003.
The TimeBank corpus. In Corpus Linguistics, pages
647?656.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning, pages 104?112.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 Task 15: TempEval tempo-
ral relation identification. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 Task 13:
TempEval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62.
927
