Modular MT with a learned bilingual dictionary: rapid deployment
of a new language pair
Jessie Pinkham Martine Smets
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{jessiep martines}@microsoft.com
Abstract
The MT system described in this paper
combines hand-built analysis and generation
components with automatically learned
example-based transfer patterns. Up to now,
the transfer component used a traditional
bilingual dictionary to seed the transfer
pattern learning process and to provide
fallback translations at runtime. This paper
describes an improvement to the system by
which the bilingual dictionary used for these
purposes is instead learned automatically
from aligned bilingual corpora, making the
system?s transfer knowledge entirely
derivable from corpora. We show that this
system with a fully automated transfer
process performs better than the system
with a hand-crafted bilingual dictionary.
More importantly, this has enabled us to
create in less than one day a new language
pair, French-Spanish, which, for a technical
domain, surpasses the quality bar of the
commercial system chosen for comparison.
1 Introduction
The phrase ?MT in a day? is strongly associated
with research in statistical MT. In this paper we
demonstrate that ?MT in a day? is possible with
a non-statistical MT system provided that the
transfer component is learned from aligned
bilingual corpora (bi-texts), and does not rely on
any large hand-crafted bilingual resource. We
propose instead to use a bilingual dictionary
learned only from the same bi-texts. Section 4.2
describes the creation of the new language pair,
French-Spanish, and gives evaluation results.
Section 4.1 examines the impact of the learned
dictionary on our existing French-English
system.
2 Previous work
Commercial systems and other large-scale
systems have traditionally relied heavily on the
knowledge encoded in their bilingual
dictionaries. Gerber & Yang (1997) clearly
state that Systran?s translation capabilities are
dependent on ?large, carefully encoded, high-
quality dictionaries?. With the advent of bi-
texts, efforts to derive bilingual lexicons have
led to substantial research (Melamed 1996,
Moore 2001 for discussion), including resources
for semi-automatic creation of bilingual lexica
such as SABLE (Melamed 1997), used for
instance in Palmer et al (1998). Statistical MT
systems have relied on bi-texts to automatically
create word-alignments; in many statistical MT
systems however, the authors state that use of a
conventional bilingual dictionary enhances the
performance of the system (Al-Onaizan et al
1999, Koehn & Knight 2001). We find then,
that in spite of the movement to create bilingual
dictionaries automatically, there is still a heavy
reliance on hand-crafted and hand-edited
resources. We found no full-scale MT system
that relied only on learned bilingual dictionaries
and certainly none that was found better in
performance for doing so.
Rapid deployment of a new language pair
has been one of the strong features of statistical
MT systems. For example, ?MT in a day? was a
stated goal of the workshop on statistical MT
(Al-Onaizan et al 1999). The system deployed
was of low quality, in part because of the small
size of the corpus used, and the difficulty of the
language pair chosen (Chinese to English). We
have chosen French-Spanish, because we are
constrained by the availability of well-
developed analysis and generation components
in our experiment. Those, needless to say, were
not created in one day, nor were the large size
monolingual dictionaries that they rely on. But
given the assumption that these modules are
available and of good quality, we demonstrate
that training the transfer dictionary1 and
example base on bi-texts is sufficient to create a
new language pair which is of comparable
quality to others based on the same source
language. This, to our knowledge, has not been
done before in the context of a large hybrid MT
system
3 System overview
The MT system discussed here uses a source
language broad coverage analyzer, a large multi-
purpose source language dictionary, an
application-independent natural language
generation component which can access a full
monolingual dictionary for the target language,
and a transfer component. The transfer
component, described in detail in Menezes
(2001), consists of high-quality transfer patterns
automatically acquired from sentence-aligned
bilingual corpora.
The innovation of this work is the use of an
unedited, automatically created dictionary which
contains translation pairs and parts of speech,
without any use of a broad domain, general
purpose hand-crafted dictionary resource. The
architecture of the MT system as described
elsewhere (Richardson et al 2001) used both a
traditional bilingual dictionary and an
automatically derived word-association file at
training time, but it used only the traditional
bilingual dictionary at runtime. We refer to this
below as the HanC system, because it uses a
Hand-crafted Dictionary2. We changed this so
that a learned dictionary consisting of word-
associations (Moore 2001) with parts of speech
and a function word only bilingual dictionary
(prepositions, conjunctions and pronouns)
replaces the previous combination both at
training and at runtime3. We refer to this as the
1 In both French-English and French-Spanish, we use
a hand-crafted bilingual function word dictionary of
about 500 entries. It includes conjunctions,
prepositions and pronouns; see section 4.1.4.
2 The dictionaries are automatically converted from
electronic dictionaries acquired from publishers, and
are updated by hand over time.
3 The same statistical techniques identify certain
multi-word terms for parsing and transfer. This
LeaD system (Learned Dictionary). We
demonstrate that this change improves sentences
that differ between both systems, and show that
we can now adapt quickly to new language pairs
with excellent results.
Analysis of the consequences of removing
the standard hand-crafted bilingual dictionary
from the system (and having no dictionary as a
fallback at all) are provided in Pinkham &
Smets (2002). It proved important to have a
dictionary containing parts of speech to use as a
fallback, motivating the work described here.
4 Experiments
We conducted two experiments. In the first one,
we compared the performance of the HanC
(Hand-Crafted dictionary) MT system to the
performance of our LeaD (Learned Dictionary)
system. The French-English system is trained
on 200,000 sentences in the computer domain,
and tested on unseen sentences from the same
domain.
In the second experiment, we created a new
language pair, French-Spanish, in less than 8
hours. The French-Spanish system was trained
on 220,000 sentences from the same computer
domain, and also tested on unseen computer
domain data.
4.1 French-English translation
with a learned bilingual
dictionary
4.1.1 Comparing HanC to LeaD
In this first experiment, we compare the
performance of the HanC system and the LeaD
system for French-English versus the same
competitor.
Translations produced by the two versions of
our system differ in 30% of the cases. Out of
the 2000 sentences in our test set, only 595 were
translated differently. In about half of these
cases, there was an overt difference in the word
chosen as a fallback translation at runtime. In
the other half, the translation example-base
patterns were different.
learned dictionary stays constant during the French-
English experiments.
We evaluated 400 of the 595 ?diff? sentences
mentioned. A complete description of the
evaluation method is given in Richardson
(2001), and repeated in Appendix A. Evaluation
for each version of the system was conducted
against the competitor system, which we use as
a benchmark of quality. Our current benchmark
for French-English is Systran4, which uses
relevant dictionaries available but has not been
otherwise customized to the domain in any way.
Scores Signif. Size
HanC system
(diffs only)
-.1777 +/-.087 > .999 400
LeaD system
(diffs only)
-.0735 +/-.182 .97 400
French-English
HanC system
+.2626 +/- .103 > .999 400
French-English
LeaD system
+.2804 +/-.115 > .999 400
Table 1: LeaD vs. HanC for FE
We also evaluated a set of 400 sentences
taken randomly from the 2000 test sentence set.
They were translated with both the HanC system
and the LeaD system, and evaluated against the
same competitor, Systran.
4.1.2 Results
The random test has a score representative of
the quality of the system (December 2001
system), and is significantly better than the
competitor given the score of +0.2804 (0 means
the systems are the same, -1 the competitor is
better, 1 the competitor is worse). See Table 1.
Sentences whose translations differ between
the HanC and LeaD versions of our system are
less well translated overall. Through
examination of the data, we have found that
reliance on the fallback translation at runtime
tends to indicate a failure to learn or apply
transfer patterns from the example-base, both of
which are often due to faulty analysis of the
source sentence. There are also cases where
4 Systran was chosen on the basis of its ranking as
the best FE system in the IDC report (Flanagan &
McClure, 2000)
translations are not learned because of sparse
data, but these tend to be rare in our technical
corpus.
More importantly, we see that the LeaD
version of the system has a significantly higher
score than the HanC version (p=0.002 in a one-
tailed t-test). Replacing the conventional
bilingual dictionary with the learned bilingual
dictionary combined with the small function
word dictionary has led to significant
improvement in quality when measured on
?diff? sentences, i.e. cases where all the
sentences are different. However, when we take
400 random sentences, the difference between
the two versions only affects 30% of the
sentences (133 or thereabouts) and therefore
does not result in a significant difference
(p=0.13 in a one tailed t-test).
4.1.3 Translation examples
In this section, we give examples of translation
with both versions of our system, and compared
to Systran. The LeaD version of our system
uses the correct translation of ?casiers?, in this
specific context, while both our HanC version of
the system and Systran use terms inappropriate
for this domain. By using a learned dictionary,
the LeaD system is better suited to the domain.
Source Le finisseur est trait? comme trois
casiers individuels,
Reference The Finisher is addressed as three
individual bins
LeaD The finisher is processed like three
individual bins.
HanC The finisher is processed like three
individual pigeonholes.
Systran The finisher is treated like three
individual racks,
4.1.4 Creation of the learned bilingual
dictionary
The learned dictionary with parts of speech was
created by the same method (Moore, 2001) as
the previously used word-association file, with
the exception that parts of speech were
appended to lemmas in the first step of the
process. We are easily able to modify the input
this way, because we use the output of the
analysis of the training data to create the file that
is the input to the word alignment process.
Appending the part of speech disambiguates
homographs such as ?use?, causing them to be
treated as separate entities in the word-
association process:
use^^Verb
use^^Noun
The word-association process assigns scores
to each pair of words. We have established a
threshold below which the pairs are discarded.
Here are the top word pairs in the learned
dictionary for this domain:
utiliser^^Verb use^^Verb
fichier^^Noun file^^Noun
serveur^^Noun server^^Noun
Because the input to the learning process is
derived from Logical Forms (the output of our
analysis systems), and because this format no
longer includes lemmas for function words,
there are no function words in the learned
dictionaries. This is the primary reason why we
complemented the learned dictionary with a
function word dictionary. See the future work
section for ideas on learning the function words
as well.
Both the French-English and the French-
Spanish were arbitrarily cut off at the same
threshold, and were not edited in any way,
resulting in a file with 24,000 translation pairs
for French-English and 28,000 translation pairs
for French-Spanish. The dictionary for function
words contains about 500 word pairs. The
traditional French-English dictionary had
approximately 40,000 entries.
4.2 French-Spanish
4.2.1 Creating French-Spanish
Our group currently has both a French-English
system and an English-Spanish system. In
choosing the new language pair to develop, we
were constrained by the availability of good
quality analysis and generation systems. This is
a limiting factor, but will become less so once
we have more generation modules available for
use5, as we currently have seven fully developed
analysis modules. We were fortunate to have
220,000 aligned sentences for French-Spanish
from the technical domain (manuals, help files),
5 Members of our group (Corston-Oliver et al) are
developing an automatic generation component.
This could speed up the development of generation
modules, giving us a potential of 42 different
language-pairs trainable on bi-texts.
which enabled the construction of the learned
bilingual dictionaries, and the automatic
creation of the transfer pattern example base.
For reasons explained above, our first
learned dictionary made no attempt to learn
function word translations. We needed,
therefore, to complement the learned French-
Spanish dictionary with a French-Spanish
function word bilingual dictionary, which was
bootstrapped from our French-English and
English-Spanish bilingual dictionaries. All the
translations for prepositions, conjunctions and
pronouns were created using both of these, and
hand-edited by a lexicographer bilingual in
French and Spanish.
The creation process, including the hand-
editing work, took less than 8 hours.
4.2.2 Results
The test was conducted on 250 test sentences
from the same technical domain as the training
corpus, using the methodology described in
Appendix A. All test data is distinct from
training data and unseen by developers. The
Sail Labs French-Spanish system is the
benchmark used as comparison. The technical
domain dictionary on the website was applied to
the Sail Labs translation, but it was not
otherwise customized to the domain.
The Sail Labs translation included brackets
around unfound words, which were thought to
interfere with the raters? ability to compare the
sentences; the brackets were removed for the
evaluation.
Condition Scores Signif Size
FS LeaD +.2278
+/- .117
> .999 250
French-English +.2804
+/- .114
> .999 400
Table 2: French Spanish results
As seen in Table 2, where the French-
Spanish system is ranked at +0.228, it is
significantly better than the Sail Labs French-
Spanish system in this technical domain. The
score is very similar to the French-English score
as measured against Systran (+.2804). Since
these are being compared against different
competitors, we also wanted to measure their
absolute quality. On a scale of 1 to 4, where 4 is
the best, we found that both Systran and Sail
Labs were comparable in quality, and that our
system scored slightly higher in both cases, but
not significantly so, if one considers the
confidence measures (Table 3). The details of
the scoring for absolute evaluations are given in
Appendix B. As a brief illustration, the LeaD
French-English translation in 4.1.3 has a score
of 3, while the LeaD French-Spanish translation
in 4.2.3 received a score of 2.5.
Absolute score
FS LeaD 2.676 +/- .329 250
FS Sail Labs 2.444 +/- .339 250
French-English 2.321 +/- .21 400
FE Systran 2.259 +/- .291 250
Table 3: Absolute scores FS and FE
4.2.3 Translation Example for French-
Spanish
This section gives examples of translation from
French into Spanish. The LeaD translation has
the correct translation for domain specific terms
such as ?hardware? and ?casilla de
verificaci?n?, while Sails Labs translation does
not in spite of the use of a domain bilingual
dictionary.
Source Si la case ? cocher Supprimer de ce
profil mat?riel est activ?e, le
p?riph?rique est supprim? du profil
mat?riel.
Reference Si la casilla de verificaci?n Quitar este
perfil de hardware est? activada, se ha
quitado el dispositivo del perfil de
hardware.
LeaD Si se activa la casilla de verificaci?n
Eliminar de este perfil de hardware, el
dispositivo se quita del perfil de
hardware.
Sails Labs Si la coloca a marcar Suprimir de este
perfil material es activada, el perif?rico
se suprime del perfil material.
5 Future Work
We are planning to experiment with lowering
the threshold for the cutoff of information in the
learned bilingual dictionary, in an attempt to
include more word pairs (some words remain
untranslated).
To further validate the Learned Dictionary
approach, we are experimenting with other
domains. One might assume, for instance, that
as the domain becomes broader, learned
dictionaries would be less effective due to
sparse data. We have preliminary experiments
on Hansard French-English data which indicate
that this is not the case.
6 Conclusion
We have demonstrated that we can replace the
traditional bilingual dictionary with a
combination of a small bilingual function word
dictionary and a bilingual dictionary learned
from bi-texts. This removes the reliance on
acquired or hand-built bilingual dictionaries,
which can be expensive and time-consuming to
create. One can estimate that for any new
domain application, this could save as much as
1-2 person years of customization. This also
removes a major obstacle to quick deployment
of a new language pair.
We believe that high-quality linguistic
analysis is a necessary ingredient for successful
MT. In our system, it has enabled automation of
the transfer component, both in the learning of
the bilingual dictionary and in the creation of
example-based patterns.
Appendix A: Relative Evaluation Method
For each version of the system to be tested,
seven evaluators were asked to evaluate the
same set of blind test sentences. For each
sentence, raters were presented with a reference
sentence, the original English sentence from
which the human French translation was
derived. In order to maintain consistency among
raters who may have different levels of fluency
in the source language, raters were not shown
the original French sentence. Raters were also
shown two machine translations, one from the
system with the component being tested, and
one from the comparison system (Systran for
French-English, Sails Lab for French-Spanish).
Because the order of the two machine
translation sentences was randomized on each
sentence, evaluators could not determine which
sentence was from which system. The order of
presentation of sentences was also randomized
for each rater in order to eliminate any ordering
effect.
The raters were asked to make a three-way
choice. For each sentence, the raters were to
determine which of the two automatically
translated sentences was the better translation of
the (unseen) source sentence, assuming that the
reference sentence was a perfect translation,
with the option of choosing ?neither? if the
differences were negligible. Raters were
instructed to use their best judgment about the
relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the relative
judgments of quality. The three-way scale also
allowed sentences to be rated on the same scale,
regardless of whether the differences between
output from system 1 and system 2 were
substantial or relatively small; and regardless of
whether either version of the system produced
an adequate translation.
The scoring system was similarly simple;
each judgment by a rater was represented as 1
(sentence from our system judged better), 0
(neither sentence judged better), or -1 (sentence
from Systran or Sails Labs judged better). The
score for each version of the system was the
mean of the scores of all sentences for all raters.
The significance of the scores was calculated in
two ways. First, we determined the range around
the mean which we could report with 95%
confidence (i.e. a confidence interval at .95),
taking into account both variations in the
sentences and variations across the raters'
judgments. In order to determine the effects of
each stage of development on the overall quality
of the system, we calculated the significance of
the difference in the scores across the different
versions of the system to determine whether the
difference between them was statistically
meaningful. We used a one-tailed t-test, since
our a priori hypothesis was that the system with
more development would show improvement
(that is, a statistically meaningful change in
quality with respect to the competitor).
Appendix B: Absolute Evaluation
Method
At the same time as the relative evaluations are
made, all the raters enter scores from 1 to 4
reflecting the absolute quality of the translation,
as compared to the reference translation given.
The grading is done according to these
guidelines:
1 unacceptable:
Absolutely not comprehensible and/or little
or no information transferred accurately
2 possibly acceptable:
Possibly comprehensible (given enough context
and/or time to work it out); some information
transferred accurately
3 acceptable:
Not perfect (stylistically or grammatically odd),
but definitely comprehensible, AND with
accurate transfer of all important information
4 ideal:
Not necessarily a perfect translation, but
grammatically correct, and with all information
accurately transferred
References
Al-Onaizan, Y & Curin, J. & Jahr, M. & Knight
K. & Lafferty, J. & Melamed, D. & Och, F-J,
& Purdy, D. & Smith, N. A. & Yarowsky, D.
(1999). Statistical Machine Translation: Final
Report, Johns Hopkins University 1999
Summer Workshop on Language
Engineering, Center for Speech and Language
Processing, Baltimore, MD.
Corston-Oliver, S., M. Gamon, E. Ringger, R.
Moore. 2002. An overview of Amalgam: A
machine-learned generation module. To
appear in Proceedings of the International
Natural Language Generation Conference.
New York, USA
Flanagan, M and McClure, S. (2000) Machine
Translation Engines: An Evaluation of Output
Quality, IDC publication 22722.
Gerber, L. & Yang,J. (1997) Systran MT
Dictionary Development in the Proceedings
of the MT Summit V, San Diego.
Koehn, P. & Knight, K. (2001) Knowledge
Sources for Word-Level Translation Models,
Proceedings of the conference on Empirical
Methods in Natural Language Processing
(EMNLP)
Melamed, D. (1998). Empirical Methods for MT
Lexicon Construction, in L. Gerber and D.
Farwell, Eds., Machine Translation and the
Information Soup, Springer-Verlag.
Melamed, D. (1997). A Scalable Architecture
for Bilingual Lexicography, Dept. of
Computer and Information Science Technical
Report #MS-CIS-91-01.
Melamed, D. (1996). Automatic Construction of
Clean Broad-Coverage Translation Lexicons,
Proceeding of the 2nd Conference of the
Association for Machine Translation in the
Americas (AMTA'96), Montreal, Canada.
Menezes, A. & Richardson, S. (2001). A Best-
First Alignment Algorithm for Automatic
Extraction of Transfer Mappings from
Bilingual Corpora. In Proceedings of the
Workshop on Data-Driven Machine
Translation, ACL Conference, June 2001.
Moore, R.C. (2001). Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Between Words. In
Proceedings of the Workshop on Data-Driven
Machine Translation, ACL Conference, June
2001.
Pinkham, J & Smets, M (2002) Machine
Translation without a bilingual dictionary
Proceedings of the TMI conference, Kyoto,
Japan.
Palmer, M. & Rambow, O. & Nasr, A. (1998).
Rapid Prototyping of Domain-Specific
Machine Translation Systems, in Proceedings
of the AMTA ?98.
Richardson, S. & Dolan, W. & Menezes, A. &
Corston-Oliver, M. (2001). Overcoming the
Customisation Bottleneck Using Example-
Based MT. In Proceedings of the Workshop
on Data-Driven Machine Translation, ACL
Conference, June 2001.
Adding Domain Specificity to an MT system
Jessie Pinkham
Microsoft Research
One Microsoft Way
Redmond, WA 98152
jessiep@microsoft.com
Monica Corston-Oliver
Butler Hill Group
moco@butlerhill.com
Abstract
In the development of a machine
translation system, one important issue
is being able to adapt to a specific
domain without requiring time-
consuming lexical work. We have
experimented with using a statistical
word-alignment algorithm to derive
word association pairs (French-English)
that complement an existing multi-
purpose bilingual dictionary. This word
association information is added to the
system at the time of the automatic
creation of our translation pattern
database, thereby making this database
more domain specific. This technique
significantly improves the overall
quality of translation, as measured in an
independent blind evaluation.
1 Introduction
The machine translation system described
here is a French-English translation system
which uses a French broad coverage analyzer, a
large multi-purpose French dictionary, a large
French-English bilingual lexicon, an application
independent English natural language generation
component and a transfer component. The
transfer component consists of high-quality
transfer patterns automatically acquired from
sentence-aligned bilingual corpora using an
alignment grammar and algorithm described in
detail in Menezes (2001) (see Figure 1 for an
overview of the French-English MT system).
The transfer component consists only of
correspondences learned during the alignment
process. Training takes place on aligned
sentences which have been analyzed by the
French and English analysis systems to yield
dependency structures specific to our system
entitled Logical Forms (LF). The LF structures,
when aligned, allow the extraction of lexical and
structural translation correspondences which are
stored for use at runtime in the transfer database.
The transfer database can also be thought of as
an example-base of conceptual structure
representations. See Figure 2 for an illustration
of the training process.
The transfer database for French-English was
trained on approximately 200,000 pairs of
aligned sentences from computer manuals and
help files. In these aligned pairs, the French text
was produced by human translators from the
original English version.
Sample sentences from the training set are:
French training sentence:
Dans le menu D?marrer, pointez sur
Programmes, sur Outils d'administration
(commun), puis cliquez sur Gestionnaire des
utilisateurs pour les domaines.
English training sentence:
On the Start menu, point to Programs, point
to Administrative Tools (Common), and then
click User Manager for Domains.
The French-English lexicon is used during
the training period of the transfer component to
establish initial, tentative, word correspondences
during the alignment process. The sources for
the bilingual dictionary were: Cambridge
University Press English-French, Soft-Art
English-French, and Langenscheidt French-
English and English-French dictionaries. The
English-French translation data was reversed to
create French-English pairs in order to augment
the size of the dictionary, with a final translation
count of 75,000 pairs.
However, quick examination of the sample
sentence above shows that many terms are
highly specific to the domain, e.g menu
D?marrer <-> Start menu. To further add to
the specificity of the vocabulary available to the
alignment process, we added translation pairs
extracted from the actual domain, using
statistical word/phrase assignment, as described
below. This resulted in one file of automatically
created French English translation
correspondences, or word associations (WA),
and a second file of specialized multi-word
translation correspondences which we term Title
Associations (TA). These files, of size 30,000
and 2600 respectively, added to the quality of
the alignments and to overall translation quality.
2 Domain Specificity
2.1 Word-Association list
Moore (2001) describes a method for learning
translation relationship between words from
bilingual corpora. The five step process is
restated here:
1. Extract word lemmas from the Logical
Form created by parsing the raw
training data.
2. Compute association scores for
individual lemmas.
3. Hypothesize occurrences of compounds
in the training data, replacing lemmas
constituting hypothesized occurrences
of a compound with a single token
representing the compound.
4. Recompute association scores for
compounds and remaining individual
individual lemmas.
5. Recompute association scores, taking
into account only co-occurrences such
that there is no equally strong or
stronger association for either item in
the aligned logical-form pair.
The word-association list (WA) was created by
applying this method to our training data set of
200,000 aligned French-English sentences of
computer manual and help file data. A French
linguist determined the best cutoff for the raw
data, i.e. determined the association score which
would determine the cutoff, and otherwise left
the file unedited for inclusion in the transfer
training stage. For internal reasons, we used
only associations which are conceptually single
word to single word, where a single word can be
defined as an item returned as one unit by the
analyzer, even though it might be a multi-word
item in the source text, e.g base_de_donn?e <->
database. The files included 30,000 pairs, which
in their totality, were judged to be 60%
accurate1.
Figure 2
The word association file was used only in
training (see Figure 2) to enhance the
opportunity for alignment during the detection
of transfer patterns.
Examples of WA pairings :
? cliquer click
? processeur CPU
? ?clairage lighting
? http://www.mcafee.com
http://www.mcafee.com
? nettoyer scavenge
? conversion translation
? Requ?te/?dition query/edit
2.2 Title Association list
The second file used was a specialized file
created using the same algorithm, but allowing
multi-word titles that are all in capitals in
English to associate with multiple words in
French that have mixed capitalization on major
content words. Because these phrases are
identified by using capitalization, they are also
referred to as Captoids (Moore, 2001). Items
such as Organizational Units, which occur with
complete capitalization in English, are
1 The size of of the WA file of 42,486 reported in Moore
2001 includes multiple word associations which were not
used in this experiment.
associated with the French translation, Unit?s d'
organisation, a unit which is less easily
identified on its own, due to the mixed case.
The information yields approximately 2600
pairs of this type:
Unit?s d' organisation <->
Organizational Units
Voir aussi <-> Related Topics
This title association file (TA) is used in
training of the transfer patterns but are also
added to the processing of the French training
text; they are treated as multi-word lexical
entries similar to any French dictionary entry.
They become part of the translation dictionary
as well. The inclusion of Voir aussi as a lexical
noun phrase at the analysis stage (French)
allows it to parse correctly, and permits the
correct translation. Many of the occurrences of
Title association pairs are menu names which
are syntactically verb phrases (Voir aussi) and
would have parsed less well without the TA file.
(1)
Source: Pour plus d'informations sur l'utilisation
du Gestionnaire de p?riph?riques, consultez
Voir aussi.
Reference: For more information about using
Device Manager, see Related Topics.
ALL translation: For more information about
using of the manager of devices, see Related
Topics.
NONE translation: See for more information
on using of the Device Manager; also See.
However, the evaluation shows that the
overall effect of title associations is much less
than that of word associations, presumably
because the frequency of these items is low in
the overall test set.
3 Experiment and Methodology
In order to evaluate the relative quality of the
translations with and without the word
association and title association strategies, we
performed several evaluations of machine
translation quality. These evaluations were
performed by an independent organization that
provides support for NL application
development; the evaluators are completely
independent of development activities.
We performed two separate sets of
evaluations. In the first, we evaluated the full
version of our system with the Word
Association and Title Association components
against versions of the system from which we
had removed those components. We thus
expected that versions of the system with the
WA and TA components would outperform
those without.
In the second evaluation, we tested the
versions of our system with and without the WA
and TA components against a benchmark
system (the latest release of the French-English
Systran system, run with settings appropriate for
the computer domain) to see whether the
addition of the combination of these components
would significantly improve our scores with
respect to that benchmark.
3.1 Evaluation design
For each condition to be tested, seven
evaluators were asked to evaluate the same set
of 250 blind test sentences. For each sentence,
raters were presented with a reference sentence,
the original English translation from which the
human French translation was derived. In order
to maintain consistency among raters who may
have different levels of fluency in the source
language, raters were not shown the original
French sentence (for similar methodologies, see
Ringger et al, 2001; White et al, 1993). Raters
were also shown two machine translations, one
from the system with the component being
tested (System 1), and one from the comparison
system (System 2). Because the order of the two
machine translation sentences was randomized
on each sentence, evaluators could not
determine which sentence was from System 1.
The order of presentation of sentences was also
randomized for each rater in order to eliminate
any ordering effect.
The raters were asked to make a three-way
choice. For each sentence, the raters were to
determine which of the two automatically
translated sentences was the better translation of
the (unseen) source sentence, assuming that the
reference sentence was a perfect translation,
with the option of choosing ?neither? if the
differences were negligible. Raters were
instructed to use their best judgment about the
relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the relative
importance of these parameters for subjective
judgments of quality. The three-way scale also
allows sentences to be rated on the same scale,
regardless of whether the differences between
output from system 1 and system 2 were
substantial or relatively small; and regardless of
whether either version of the system produced
an adequate translation.
The scoring system is similarly simple; each
judgment by a rater was represented as 1
(sentence from System 1 judged better), 0
(neither sentence judged better), or -1 (System 2
judged better). The score for each condition is
the mean of the scores of all sentences for all
raters.
4 Results
4.1 Results with multiple versions of our
system
In order to isolate the effects of the WA and TA
components on the system as a whole, we built 3
new versions of the system:
? NONE: Includes neither TA nor WA.
? No TA: Includes WA but not TA.
? No WA: Includes TA but not WA.
We evaluated each of these versions of the
system against our baseline system (ALL),
which contains both the WA and TA
components. Our hypothesis was that the
removal of each of the two components would
cause the experimental systems to significantly
underperform the ALL system.
We evaluated 250 sentences2 in each
condition in which the output strings for System
1 (ALL) and System 2 (NONE, NoWA, and
NoTA, respectively) were not identical. In other
words, this analysis shows the amount of
improvement between the systems in only those
sentences which show any change at all in each
condition. For each condition, we calculated the
statistical significance of the hypothesis that ALL
system is better than the comparison system (e.g.
that the score is greater than 0), taking into
account both variations in the sentence sample,
and variations across the judgments of individual
raters.
2 The data used for testing is blind, i.e. withheld from
development and not included in the training set.
Conditio
n
Score Sample
Size
Significance
ALL/NONE 0.233 +/- .095 250 > .99999
ALL/NoWA 0.267 +/- .09 250 > .99999
ALL/NoTA 0.063 +/- .093 250 .91
Table 1: Results with differences only
The results show that, for sentences affected by
the combination of the WA and TA components,
the ALL condition is significantly better than the
NONE condition, at a significance level of 0.95.
In addition, for sentences affected by the
presence of the WA component only, the ALL
condition is significantly better than the No WA
condition. However, the ALL condition is not
significantly better the NoTA condition.
Another question of interest is the effect of
the experimental components on the corpus as a
whole, rather than just on the sentences that
changed; it is possible that the effects we found
might have become diluted below the
significance threshold because of sparsity of the
differences across the whole corpus. Rather than
do additional evaluations, we determined the
proportion of differences in each condition, and
extrapolated a larger sample, assuming that
sentences which were absolutely identical would
receive a score of 0, using the same 250
judgments as in the previous analysis.
Condition iffs
hecked
otal diffs
n test set
f 2965
ercent
f diffs
n test set
Projected
sample
size to get
250 diffs
NONE /ALL 250 1307 19.13 567
NoWA/ALL 250 1170 21.37 634
NoTA/ALL 250 280 89.29 2647
Table 2: Projected sample sizes
As expected, the results using the projected
sample were still positive, though the scores
were lower due to the larger sample size. Again,
the improvements in the NONE/ALL and
NoWA/ALL conditions are significant across
the whole data set.
Condition Score Sample
size
Significance
NONE /ALL 0.103 +/-
.04
567 > .99999
NoWA/ALL 0.105 +/-
.035
634 > .99999
NoTA/ALL 0.006 +/-
.008
2647 .90
Table 3: Results across whole sample
4.2 Results against benchmark system
In a second analysis, we tested to see if the
experimental changes to the system improved
the performance of our system against our
regular benchmark. We selected a random
sample of 250 sentences, and translated them
using first the ALL, and then the NONE,
versions of our system. We also translated them
using the benchmark system. We predicted that
sentences translated using the ALL system
would be significantly better than the sentences
translated using the NONE system in its
performance against the benchmark.
Condition Score Sample
size
NONE /benchmark -0.18 +/- .1 250
ALL/benchmark -0.14 +/- .11 250
Table 4: Results against Benchmark system.
The difference between these two scores is on
the border of significance using a one-tailed
paired t-test (p = .051825; t = -1.6334).
5 Discussion
The premise of the experiment described here
was that pairs of translations which were
automatically derived from the training data
would increase the number of transfer pairings
found and improve the quality of translation.
The results show that the combination of the
word association list and title association list
does in fact give us an improvement in quality
of translation.
We have measured the change in size in the
transfer database, and found that the database
shows increased numbers of transfer patterns
retained (transfer patterns seen only once were
discarded) when the word association file is
used, for instance:
Condition Unique transfers kept
NONE 316518
ALL 368853
Table 5: Increase in patterns kept
We have found from informal observation
that increased number of transfers in the transfer
database correlates with better performance,
particularly if the translation correspondence
includes more than one word.
Whereas the WA and TA files have been
judged elsewhere on the quality of the
translation pairs themselves (Moore 2001), we
are primarily interested in whether the data
interacts in a positive way with a full-scale
automatic alignment process. The result might
appear disappointing at first glance, since it is
barely significant. However, our experience is
that a gain of .04 against the benchmark
represents a noticeable difference in quality
translation from the user?s perspective.
It is important to note as well that this result
was achieved even in the presence of a sizeable
translation dictionary. We found that the
combination of the bilingual dictionary and the
structural mapping in the alignment process had
already enabled a number of ?domain specific?
translation correspondences, e.g. journal <-> log
as in example (2) below. In a sense, the
alignment algorithm had been able to overcome
some domain specific lexical gaps on its own.
The evaluation results give us a number of
illustrations of improved transfer patterns. The
only difference between the output categorized
as NONE and the output categorized as ALL is
the use of a transfer database trained with both
the WA and TA files included.
(2)
Source: Le tableau ci-dessous explique la
fonction des diff?rentes options disponibles
dans l'onglet Journal des transactions de la
bo?te de dialogue Propri?t?s de la base de
donn?es.
Reference: This table shows the options and
their functions available on the Transaction
Log tab of the Database Properties dialog
box.
ALL translation: The table explains the
function of different options available in the
Transaction Log tab of the dialog Properties
box of the database below.
NONE translation: The table explains the
function of different options available in the
tab transactions Log of the dialog Properties
box of the database below.
The pattern which caused the improvement is
the correspondence (Journal des transactions
<-> Transaction Log) was learned on different
pairs of sentences during the alignment phase
due to the presence of the word log introduced
by the word association file.
Without the addition of log at alignment time,
the alignment process mapped Journal to Log,
but not the more complex mapping for Journal
des transactions. Compare the translations from
the FE dictionary to the pairs from the word
association file (where ordering represents
frequency of each translation). Note that the
WA list has learned the most relevant technical
translation (log), which was lacking in the FE
dictionary, but also the most frequent general
translation (journal):
FE dictionary
(journal)=(journal magazine diary newspaper)
Word association list
(journal)=(log journal newspaper)
A similar case below (3) shows that the
inclusion of the word push as a translation of
?mission in the word association file allows for a
correct pattern in the transfer database:
r?plication par ?mission
push replication
(3)
Source: Pour configurer un serveur WINS afin
d'utiliser une r?plication par ?mission, vous
pouvez faire votre choix parmi plusieurs
options configurables de la console WINS.
Reference: To configure a WINS server to use
push replication, you can choose from
several WINS console configurable options.
ALL translation: For configuring a WIN server
to use a push replication, you can do your
choice among options configurable of the
WIN console.
NONE translation: For configuring a WIN
server to use a replication by program, you
can do your choice among options
configurable of the console WIN.
FE dictionary: (?mission)=(program
transmission broadcasting emission
broadcast issue uttering)
Word association list: (?mission)=(issue push
Transmit transmit issuance)
This example is quite interesting, because the
link of push to ?mission is helpful, even though
is would be judged incorrect in a standard
evaluation of the pairings themselves
We have described the improvements so
far as increases in domain specificity, but the
effect is more wide-spread. We find that the
added information allows for creation and
retention of such generally better patterns as
those in example (4):
(4)
Source: Assurez-vous qu'il y a du papier dans
l'imprimante.
Reference: Make sure there is paper in your
printing device.
ALL translation: Make sure that there is a
paper in the printer.
NONE translation: Provide that a paper in the
printer becomes.
We note the improved transfer patterns for Make
sure and there is.
The incidence of faulty translation patterns
learned because of incorrect word-associations
has been difficult to measure, but appears to be
low. One instance was the learned
correspondence of ?teindre <-> off (instead of
turn_off). We believe this could be avoided by
more accurate preservation of information from
our Logical Form representation in step one of
the Moore algorithm.
5.1 Future improvements
The experiment presented here is the first
step in our search for techniques that contribute
to the quality of the translations by providing
domain specific additions.
We are working to find the most productive
method for pruning low accuracy pairs (but still
without hand-editing). We have already seen
that if the data is truncated to maximize the
accuracy of the word associations, the impact on
the translation quality drops off, presumably
because the high frequency pairs in the word
association file contribute fewer unknown
translations than the larger noisier file. This
suggests that in the process of seeding an
automatic alignment process such as ours, recall
is more important than precision.
.
References
Frederking, Robert, and Ralf Brown. 1996. The
Pangloss-Lite Machine Translation System.
In Proceedings of the Conference of the
Association for Machine Translation in the
Americas. 268-272.
Frederking, Robert, et al 1994. Integrating
Translations From Multiple Sources Within
the Pangloss Mark III Machine Translation
System. In Technology Partnerships for
Crossing the Language Barrier: Proceedings
of the First Conference of the Association of
Machine Translation in the Americas. 73-80.
Melamed, I. Dan. 1996. Automatic Construction
of Clean Broad-Coverage Translation
Lexicons. In Proceedings of the Second
Conference of the Association for Machine
Translation in the Americas. 125-134.
Menezes, Arul and Steve Richardson. 2001. A
Best-First Alignment Algorithm for
Automatic Extraction of Transfer Mappings
from Bilingual Corpora. In Proceedings of the
Data-Driven MT workshop, ACL 2001.
Moore, Robert C. 2001. Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Between Words. In
Proceedings of the Data-Driven MT
workshop, ACL 2001.
Ringger, Eric K., Monica Corston-Oliver, and
Robert C. Moore. 2001. Using Word-
Perplexity for Automatic Evaluation of
Machine Translation. Unpublished ms.
Hideo Watanabe, Sadao Kurohashi and Eiji
Aramaki. 2000. Finding Structural
Correspondences from Bilingual Parsed
Corpus for Corpus-based Translation. In
Proceedings of COLING: The 18th
International Conference on Computational
Linguistics. 906-912.
White, John S., Theresa A. O'Connell, and Lynn
M. Carlson. 1993. Evaluation of machine
translation. In Human Language Technology:
Proceedings of a Workshop (ARPA). 206-
210.
Machine Translation as a testbed for multilingual analysis 
Richard Campbell, Carmen Lozano, Jessie Pinkham and Martine Smets* 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052  USA 
{richcamp, clozano, jessiep, martines}@microsoft.com 
*to whom all correspondence should be addressed 
 
 
Abstract 
We propose that machine translation (MT) is a 
useful application for evaluating and deriving 
the development of NL components, 
especially in a wide-coverage analysis system. 
Given the architecture of our MT system, 
which is a transfer system based on linguistic 
modules, correct analysis is expected to be a 
prerequisite for correct translation, suggesting 
a correlation between the two, given relatively 
mature transfer and generation components.  
We show through error analysis that there is 
indeed a strong correlation between the quality 
of the translated output and the subjectively 
determined goodness of the analysis.  We use 
this correlation as a guide for development of 
a coordinated parallel analysis effort in 7 
languages. 
1  Introduction 
The question of how to test natural language 
analysis systems has been central to all natural 
language work in the past two decades.  It is a 
difficult question, for which researchers have 
found only partial answers.  The most common 
answer is component testing, where the component 
is compared against a standard of goodness, 
usually the Penn Treebank for English (Marcus et 
al., 1993),  allowing a numerical score of precision 
and recall (e.g. Collins, 1997). 
 Such methods have limitations, however, and 
need to be supplemented by additional methods.  
One limitation is the availability of annotated 
corpora, which do not exist for all languages.  
Secondly, comparison to an annotated corpus can 
only measure how well a system produces the kind 
of analysis for which the corpus is annotated, e.g. 
labeled bracketing of surface syntax.  Evaluation 
of analysis of deeper, more semantically 
descriptive, levels requires additional annotated 
corpora, which may not exist.  A more 
fundamental limitation of such methods is that 
they measure the goodness of a grammar without 
taking into account what the grammar is good for.  
This limitation is overcome, we claim, only by 
measuring the goodness of a grammar by its 
success in real-world applications. 
 We propose that machine translation (MT) is a 
good application to evaluate and drive the 
development of analysis components when the 
transfer component is based on linguistic modules.  
Multi-lingual applications such as MT allow 
evaluation of system components that overcomes 
the limitations mentioned above, and therefore 
serves as a useful complement to other evaluation 
techniques.  Another significant advantage to 
using MT as a testbed for the analysis system is 
that it prioritizes analysis problems, highlighting 
those problems that have the greatest negative 
effect on translation output. 
 In this paper, we give an overview of 
NLPWin, a multi-application natural language 
analysis and generation system under development 
at Microsoft Research (Jensen et al, 1993; Gamon 
et al, 1997; Heidorn 2000), incorporating analysis 
systems for 7 languages (Chinese, English, French, 
German, Japanese, Korean and Spanish). Our 
discussion focuses on a description of the three 
components of the analysis system (called sketch, 
portrait and logical form) with a particular 
emphasis on the logical form derived as the end-
product, which serves as the medium for transfer 
in our MT system.  
 We also give an overview of the architecture 
of the MSR-MT system, and of the evaluation we 
use to measure correctness of the translations. We 
demonstrate the correlation between the scores 
assigned to translation outputs and the correctness 
of the analysis, using as illustration two language-
pairs at different stages of development:  Spanish-
English (SE) translation, as a testbed for the 
Spanish analysis system, and French-English (FE) 
translation, as a testbed for the French analysis 
system.   
2  Overview of the analysis component of 
NLPWin 
Analysis produces three representations for the 
input sentence: sketch, portrait and logical form1.  
Sketch is the initial tree representation for the 
sentence, along with its associated attribute-value 
structure. An example of sketch is given in Figure 
1, which shows the sketch tree for sentence (1). 
 
(1) 
Ce  format est pris   en charge par Windows 2000 
this format is   taken in charge by  Windows 2000 
?This format is supported by Windows 2000? 
 
 
Figure 1 :  Sketch analysis of (1) 
 
Attachment sites for post-modifiers are not 
determined in sketch.  In most cases, the 
information available as the syntactic tree is built 
is not sufficient to determine where e.g. 
prepositional phrases or relative clauses should be 
attached. Post-modifiers are thus systematically 
attached to the closest possible attachment site, 
and reattached, if necessary, by the reattachment 
module, a set of heuristic rules. 
 Reattachment rules apply to the sketch to 
produce the portrait; the portrait analysis of (1) is 
given in Figure 2, where the PP expressing the 
agent of the passive construction, originally 
attached to PP1 in sketch (see Figure 1) has been 
reattached at the sentence level. 
 
                                                     
1 The presentation of the analysis module is very 
simplified, but sufficient for our current discussion. 
More details can be found in the references.  
 
Figure 2: Portrait analysis of (1) 
 
 The portrait is the input to the computation of 
the logical form (LF), a labeled directed unordered 
graph representing the deep syntactic relations 
among the content words of the sentence (i.e., 
basic predicate-argument structure), along with 
some semantic information, such as functional 
relations expressed by certain prepositions.2 At 
this level, the difference between active and 
passive constructions is normalized; control 
relations and long-distance dependencies, such as 
subjects of infinitives, arguments associated with 
gaps, etc., are resolved.  The LF of (1) is shown in 
Figure 3.  Note that the surface subject of the 
passive is rendered as the Dobj (deep object) in 
LF, and the par-phrase as the Dsub (deep subject). 
 
 
Figure 3 :  LF analysis of (1) 
 
 Modifications to any of the analysis 
components are tested using monolingual 
regression files containing thousands of analyzed 
sentences; differences caused by the modification 
are examined manually by the linguist responsible 
for the change (Suzuki, 2002).  This process serves 
as an initial screening to ensure that modifications 
to the analysis have the desired effect. 
3 MSR-MT 
In this section we review the basics of the MSR-
MT translation system and its evaluation.  The 
reader is referred to Pinkham et al (2001) and 
Richardson et al (2001) for further details on the 
French and Spanish versions of the system. The 
overall architecture and basic component structure 
                                                     
2   LF as described here corresponds to the PAS 
representation of Campbell and Suzuki (2002). 
are the same for both the FE and SE versions of 
the system. 
3.1 Overview 
MSR-MT uses the broad coverage analysis system 
described in Section 2, a large multi-purpose 
source-language dictionary, a learned bilingual 
dictionary, an application independent target-
language generation component and a transfer 
component. 
 The transfer component consists of transfer 
patterns automatically acquired from sentence-
aligned bilingual corpora (described below) using 
an alignment algorithm described in detail in 
Menezes and Richardson (2001). Training takes 
place on aligned sentences which have been 
analyzed by the source- and target-language 
analysis systems to yield logical forms. The 
logical form structures, when aligned, allow the 
extraction of lexical and structural translation 
correspondences which are stored for use at 
runtime in the transfer database. See Figure 4 for 
an overview of the training process. 
 The transfer database is trained on 350,000 
pairs of aligned sentences from computer manuals 
for SE, and 500,000 pairs of aligned Canadian 
parliamentary data (the Hansard corpus) for FE.  
 
 
Figure 4:  MSR-MT training phase 
3.2   Evaluation of MSR-MT 
Seven evaluators are asked to evaluate the same 
set of sentences. For each sentence, raters are 
presented with a reference sentence, the original 
English sentence from which the human French 
and Spanish translations were derived, and MSR-
MT?s machine translation.3 In order to maintain 
                                                     
3 Microsoft manuals are written in English and 
translated by hand into other languages. We use these 
translations as input to our system, and translate them 
back into English. 
consistency among raters who may have different 
levels of fluency in the source language, raters are 
not shown the original French or Spanish sentence 
(for similar methodologies, see Ringger et al, 
2001; White et al, 1993).  
 All the raters enter scores reflecting the 
absolute quality of the translation as compared to 
the reference translation given. The overall score 
of a sentence is the average of the scores given by 
the seven raters. Scores range from 1 to 4, with 1 
meaning unacceptable (not comprehensible), 2 
meaning possibly acceptable (some information is 
transferred accurately), 3 meaning acceptable (not 
perfect, but accurate transfer of all important 
information, and 4 meaning ideal (grammatically 
correct and all the important information is 
transferred).  
4 Examples from FE and SE 
In this section we discuss specific examples to 
illustrate how results from MT evaluation help us 
to test and develop the analysis system. 
4.1  FE translation: the Hansard corpus 
The evaluation we are discussing in this section 
was performed in January 2002, at the beginning 
of our effort on the Hansard corpus. The 
evaluation was performed on a corpus of 250 
sentences, of which 55.6% (139 sentences) were 
assigned a score of 2 or lower, 30.4% (76 
sentences) were assigned a score greater than 2 but 
not greater than 3, and 14% (35 sentences) were 
assigned a score greater than 3. 
 Examination of French sentences receiving 
low-score translations led to the identification of 
some classes of analysis problems, such as the 
following: 
- mis-identification of vocatives 
- clefts not represented correctly 
- mis-analysis of ce qui / ce que free relatives 
- bad representation of complex inversion 
(pronoun-doubling of inverted subject) 
- no treatment of reflexives 
- fitted parses (i.e., not spanning the sentence) 
Most of the problematic structures are 
characteristic of spoken language as opposed to 
more formal, written styles (vocatives, clefts, 
direct questions), and had not been encountered in 
our previous work, which had involved mostly 
translation of technical manuals. Other problems 
(free relatives, reflexives) are analysis issues that 
we had not yet addressed. Fitted parses are parses 
that do not span the whole sentence, but are pieced 
together by the parser from partial parses; fitted 
parses usually result in poor translations. 
 Examples of translations together with their 
score are given in Table I. The source sentences 
are the French sentences, the reference sentence is 
the human translation to which the translation is 
compared by the evaluators, and the translation is 
the output of MSR-MT. Each of the three 
categories considered above is illustrated by an 
example. 
 Sentence (2) (with a score of 1.5) is a direct 
question with complex inversion and the doubled 
subject typical of that construction. In the LF for 
(2), les ministres des finances is analyzed as a 
modifier, because the verb r?unir already has a 
subject, the pronoun ils ?they?.  There are a couple 
of additional problems with this sentence: si is 
analyzed as the adverb meaning ?so? instead of as 
the conjunction meaning ?if?, and a direct question 
is analyzed as a complement clause; the sketch and 
LF analyses of this sentence are given in the 
Appendix..  The MSR-MT translation of this 
sentence has a very low score, reflecting the 
severity of the analysis problems. 
 The two other sentences, on the other hand, do 
not have analysis problems: the poor translation of 
(3) (score 2.16) is caused by bad alignment (droit 
translates as right instead of law), and the 
translation of (4) (score 3) is not completely fluent, 
but this is due to an English generation problem, 
rather than to a French analysis problem. This last 
sentence is the most correct with appropriate 
lexical items and has the highest score of the three. 
 Of the 139 sentences with score 2 or lower, 
73% were due to analysis problems, and 24% to 
alignment problems. Most of the rest had bugs 
related to the learned dictionary.  There were a few 
cases of very free translations, where the reference 
translation was very far from the French sentence, 
and our translation, based on the source sentence, 
was therefore penalized.  
 These figures show that, at this stage of 
development of our system, most of the problems 
in translation come from analysis. Translation can 
be improved by tackling analysis problems 
exhibited by the lowest scoring sentences, and, 
conversely, analysis issues can be discovered by 
looking at the sentences with the lowest translation 
score.  
 The next section gives examples of issues with 
the SE system, which is more mature than the FE 
system. 
 
4.2  SE translation: Technical manuals 
An evaluation of the Spanish-English MT system 
was also performed in January 2002, after work on 
the MT system had been progressing for 
approximately a year and a half.  The SE system 
was developed and tested using a corpus of 
sentences from Microsoft technical manuals.  A 
set of 600 unseen sentences was used for the 
evaluation.  
 Out of a total of 600 sentences, the number of 
sentences with a score from 3 to 4 was 251 (42%), 
the number of sentences with a score greater than 
2 but less than 3 was 186 (31%), and the 
remaining 163 sentences, (27%) had a score of 2 
or lower. Of these 163 sentences with the lowest 
scores, 50% (82 sentences) had analysis problems, 
and 17% of them (29 sentences) had fitted parses.  
A few of the fitted parses, 7 sentences out of 29, 
had faulty input, e.g. input that contained unusual 
characters or punctuation, typos, or sentence 
fragments.   
 Typical analysis problems that led to poor 
translation in the SE system include the following: 
- incorrect analysis of arguments in relative 
clauses,  especially those with a single 
argument (and a possible non-overt subject) 
- failure to identify the referent of clitic le (i.e. 
usted ?you?) in imperative sentences in LF 
- mis-analysis of Spanish reflexive or 
se constructions in LF 
- incorrect syntactic analysis of homographs 
- incorrect analysis of coordination  
- mis-identification of non-overt or controlled 
subjects  
- fitted parses  
 Table II contains sample sentences from the 
SE evaluation.  For each row, the second column 
displays the Spanish source sentence with the 
reference sentence in the next column, the 
translation produced by the MT system is in the 
fourth column, and the score for the translation 
assigned by the human evaluators in the last 
column.    
# Source  Reference Translation Score
(2) Si tel n'?tait pas le cas, pourquoi les 
ministres des Finances des provinces se 
seraient-ils r?unis hier pour essayer de 
s'entendre sur un programme commun ? 
soumettre au ministre des Finances? 
If that were not the case, 
why were the finance 
ministers of the provinces 
coalescing yesterday to try 
and come up with a joint 
program to bring to the 
finance minister?. 
Not was the case that they have 
the ministers met why 
yesterday Finances of the 
provinces trying to agree on a 
common program to bring 
Finances for the minister this so 
like? 
1.5 
(3) Nous ne pouvons pas appuyer cette 
motion apr?s que le Bloc qu?b?cois ait 
refus? de reconna?tre la primaut? du droit 
et de la d?mocratie pour  tous. 
 
We cannot support this 
motion after seeing the 
Bloc Quebecois refuse to 
recognize the rule of law 
and the principle of 
democracy for all. 
We cannot support this motion 
after the Bloc Quebecois has 
refused to recognize the rule of 
the right and democracy for all. 
2.16 
(4) En tant que membre de l'opposition 
officielle, je continuerai d'exercer des 
pressions sur le gouvernement pour qu'il 
tienne ses promesses ? cet ?gard. 
As a member of the official 
opposition I will continue 
to pressure the government 
to fulfil its promises in this 
regard. 
As member of the official 
opposition, I will continue to 
exercise pressures on the 
government for it to keep its 
promises in this regard. 
3 
Table I:  Examples of FE translation 
# Source Reference Translation Score
(5) Este procedimiento s?lo es aplicable si 
est? ejecutando una versi?n de idioma de 
Windows 2000 que no coincida con el 
idioma en el que desee escribir. 
This procedure applies only 
if you are running a 
language version of 
Windows 2000 that doesn't 
match the language you 
want to type 
This procedure only applies if 
you are running a Windows 
2000 language version that does 
not match the language that you 
want to type. 
3.8 
(6) Repita este proceso hasta que haya 
eliminado todos los componentes de red 
desde las propiedades de Red, haga clic 
en Aceptar y, a continuaci?n, haga clic 
en S? cuando se le pregunte si desea 
reiniciar el equipo. 
Repeat this process until 
you have deleted all of the 
network components from 
Network properties, click 
OK, and then click Yes 
when you are prompted to 
restart your computer. 
Repeat this process until you 
have deleted all of the network 
components from the Network 
properties, you click OK, and 
you click Yes then when asking 
that to restart the computer is 
wanted for him. 
2.0 
(7) En el siguiente ejemplo se muestra el 
nombre de la presentaci?n que se est? 
ejecutando en la ventana de presentaci?n 
con diapositivas uno. 
The following example 
displays the name of the 
presentation that's currently 
running in slide show 
window one. 
In the following example, the 
display name that is being run 
in the slide show window is 
displayed I join. 
1.4 
Table II:  Examples of SE translation 
 
 In the evaluation process, human evaluators 
compared the MT translation to the reference 
sentence, in the manner described in Section 4.1.   
 Example (5), with a score of 3.8, illustrates the 
fact that human evaluators considered the 
translation ?a Windows 2000 language version? to 
be a slightly worse translation than ?a language 
version of Windows 2000? for una version de 
idioma de Windows 2000; however the difference 
is so slight as to not be considered an analysis 
problem. 
 Example (6) illustrates the failure to identify 
usted ?you? (understood as the subject of the 
imperative) as the referent of the pronominal clitic 
le; as mentioned above, this is a common source of 
bad SE translations.  The last example (7) is a 
sentence with a fitted parse due to misanalysis of a 
word as its homograph :  uno is analyzed as the 
first person singular present form of the verb unir 
?join? instead of as the noun uno ?one?; the LF of 
this sentence is given in the Appendix. 
4.3 Discussion 
The examples discussed in this section are typical:  
The sentences for which MSR-MT produces better 
translations tend to be the ones with fewer analysis 
errors, while those which are misanalyzed tend to 
be mistranslated. 
 In this way, evaluation of MT output serves as 
one way to prioritize analysis problems; that is, to 
decide which among the many different analysis 
problems lead to the most serious problems.  For 
example, the poor quality of the translation of (2) 
highlights the need for an improved analysis of 
complex inversion in the French grammar, which 
will need to be incorporated into the sketch and/or 
LF components.  Similarly, the poor translation of 
(7) indicates the need to deal better with 
homographs in the Spanish morphological or 
sketch   component. 
 More generally, the analysis of FE and SE 
translation problems has led to the lists of analysis 
problems given in Sections 4.1 and 4.2, 
respectively.  Analysis problems identified in this 
way then become priorities for grammar/LF 
development. 
5 Conclusion 
We have outlined how the output of MT can be 
used as testbed for linguistic analysis in the source 
language, supplementing other methods.  The 
main advantage of this approach, in our view, is 
that it helps to prioritize analysis problems, 
highlighting those which have the most direct 
bearing on the application(s), the correct 
functioning of which is the main goal of the 
system. 
Acknowledgements 
This paper represents the work of many people in 
the NLP group at MSR; we acknowledge their 
contributions. 
References  
Campbell, R. and H. Suzuki.  2002.  Language-neutral 
representation of syntactic structure.  In R. Malaka, 
R. Porzel and M. Stube, eds., Proceedings of the First 
International Workshop on Scalable Natural 
Language Understanding. 
Collins, M. 1997. Three generative, lexicalised models 
for statistical parsing. Proceedings of the 35th Annual 
Meeting of the ACL, Madrid.  
Gamon, M., C. Lozano, J. Pinkham and T. Reutter. 
1997. Practical experience with grammar sharing in 
multilingual NLP. In Burstein J., Leacock C., eds, 
Proceedings of the Workshop on Making NLP Work, 
ACL Conference, Madrid. 
Heidorn, G.  2000.  Intelligent writing assistance.  In R. 
Dale, H. Moisl and H. Somers, eds., Handbook of 
Natural Language Processing. 
Jensen, K., G. Heidorn and S. Richardson, eds. 1993. 
Natural Language Processing: The PLNLP Approach, 
Boston, Kluwer. 
Marcus, M., B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Menezes, A. and S. Richardson. 2001. A Best-First 
Alignment Algorithm for Automatic Extraction of 
Transfer Mappings from Bilingual Corpora. In 
Proceedings of the Data-Driven MT workshop, ACL 
2001. 
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro, 2001.  Rapid assembly of a large-scale 
French-English MT system. In Proceedings of the 
2001 MT Summit. 
Richardson, S., W.B. Dolan, A. Menezes and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods.  In 
Proceedings of the 2001 MT Summit. 
Ringger, E.K., M. Corston-Oliver, and R.C. Moore. 
2001. Using Word-Perplexity for Automatic 
Evaluation of Machine Translation. Unpublished ms. 
Suzuki, H.  2002.  A development environment for 
large-scale multi-lingual parsing systems.  Workshop 
on Grammar Engineering and Evaluation, COLING 
2002. 
White, J.S., T.A. O'Connell, and L.M. Carlson. 1993. 
Evaluation of machine translation. In Human 
Language Technology: Proceedings of a Workshop 
(ARPA). 206-210. 
Appendix 
 
Figure 5 :  Sketch analysis of (2) 
 
 
Figure 6 :  LF analysis of (2) 
 
 
Figure 7 :  LF analysis of (7)
 
