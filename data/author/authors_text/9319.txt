Extraposition: A case study in German sentence realization 
Michael GAMON?, Eric RINGGER?, Zhu ZHANG?, 
Robert MOORE?, Simon CORSTON-OLIVER? 
 
?Microsoft Research 
Microsoft Corporation 
Redmond, WA 98052 
{mgamon, ringger, bobmoore, 
simonco}@microsoft.com 
 
 
?University of Michigan 
Ann Arbor, MI 48109 
zhuzhang@umich.edu 
 
Abstract 
We profile the occurrence of clausal 
extraposition in corpora from different 
domains and demonstrate that extraposition 
is a pervasive phenomenon in German that 
must be addressed in German sentence 
realization. We present two different 
approaches to the modeling of extraposition, 
both based on machine learned decision tree 
classifiers. The two approaches differ in their 
view of the movement operation: one 
approach models multi-step movement 
through intermediate nodes to the ultimate 
target node, while the other approach models 
one-step movement to the target node. We 
compare the resulting models, trained on data 
from two domains and discuss the 
differences between the two types of models 
and between the results obtained in the 
different domains. 
Introduction 
Sentence realization, the last stage in natural 
language generation, derives a surface string 
from a more abstract representation. Numerous 
complex operations are necessary to produce 
fluent output, including syntactic aggregation, 
constituent ordering, word inflection, etc. We 
argue that for fluent output from German 
sentence realization, clausal extraposition needs 
to be included. We show how to accomplish this 
task by applying machine learning techniques. 
A comparison between English and German 
illustrates that it is possible in both languages to 
extrapose clausal material to the right periphery 
of a clause, as the following examples show: 
Relative clause extraposition: 
English: A man just left who had come to 
ask a question. 
German: Der Mann ist gerade 
weggegangen, der gekommen war, um 
eine Frage zu stellen. 
Infinitival clause extraposition: 
English: A decision was made to leave 
the country. 
German: Eine Entscheidung wurde 
getroffen, das Land zu verlassen. 
Complement clause extraposition: 
English: A rumor has been circulating 
that he is ill. 
German: Ein Ger?cht ging um, dass er 
krank ist. 
Unlike obligatory movement phenomena such as 
Wh-movement, extraposition is subject to 
pragmatic variability. A widely-cited factor 
influencing extraposition is clausal heaviness; in 
general, extraposition of heavy clauses is 
preferred over leaving them in place. Consider 
the following example from the technical 
domain: 
German: Es werden Datenstrukturen 
verwendet, die f?r die Benutzer nicht 
sichtbar sind. 
English: Data structures are used which 
are not visible to the user. 
This perfectly fluent sentence contains an 
extraposed relative clause. If the relative clause is 
left in place, as in the following example, the 
result is less fluent, though still grammatical: 
? Es werden Datenstrukturen, die f?r die 
Benutzer nicht sichtbar sind, verwendet. 
Data structures which are not visible to 
the users are used. 
Table 1 presents a quantitative analysis of the 
frequency of extraposition in different corpora in 
both English and German. This analysis is based 
on automatic data profiling using the NLPWin 
system (Heidorn 2000). The technical manual 
corpus consists of 100,000 aligned 
English-German sentence pairs from Microsoft 
technical manuals. The Encarta corpora consist 
of 100,000 randomly selected sentences from the 
Encarta encyclopedia in both English and 
German. The output of the parser was 
post-processed to identify relative clauses 
(RELCL), infinitival clauses (INFCL), and 
complement clauses (COMPCL) that have been 
moved from a position adjacent to the term they  
modify. According to this data profile, 
approximately one third of German relative 
clauses are extraposed in technical writing, while 
only 0.22% of English relative clauses are 
extraposed in the corresponding sentence set. The 
high number of extraposed relative clauses in 
German is corroborated by numbers from the 
German hand-annotated NEGRA corpus. In 
NEGRA, 26.75% of relative clauses are 
extraposed. Uszkoreit et al (1998) report 24% of 
relative clauses being extraposed in NEGRA, but 
their number is based on an earlier version of 
NEGRA, which is about half the size of the 
current NEGRA corpus. 
We also used the NEGRA corpus to verify the 
accuracy of our data profiling with NLPWin. 
These results are presented in Table 2. We only 
took into account sentences that received a 
complete parse in NLPWin. Of the 20,602 
sentences in NEGRA, 17,756 (86.19%) fell into 
that category. The results indicate that NLPWin 
is sufficiently reliable for the identification of 
relative clauses to make our conclusions 
noteworthy and to make learning from 
NLPWin-parsed data compelling. 
Extraposition is so rare in English that a sentence 
realization module may safely ignore it and still 
yield fluent output. The fluency of sentence 
realization for German, however, will suffer from 
the lack of a good extraposition mechanism.
 
 
German  
technical 
manuals 
English  
technical 
manuals 
German  
Encarta 
English  
Encarta 
RELCL 34.97% 0.22% 18.97% 0.30% 
INFCL 3.2% 0.53% 2.77% 0.33% 
COMPCL 1.50% 0.00% 2.54% 0.15% 
Table 1: Percentage of extraposed clauses in English and German corpora 
Relative clause 
identification overall 
Identification of 
extraposed relative clauses 
Identification of non-
extraposed relative clauses 
Recall Precision Recall Precision Recall Precision 
94.55 93.40 74.50 90.02 94.64 87.76 
Table 2: NLPWin recall and precision for relative clauses on the NEGRA corpus 
 
This evidence makes it clear that any serious 
sentence realization component for German 
needs to be able to produce extraposed relative 
clauses in order to achieve reasonable fluency. In 
the German sentence realization module, 
code-named Amalgam (Gamon et al 2002, 
Corston-Oliver et al 2002), we have successfully 
implemented both extraposition models as 
described here. 
1 Two strategies for modeling 
extraposition 
The linguistic and pragmatic factors involved in 
clause extraposition are inherently complex. We 
use machine learning techniques to leverage large 
amounts of data for discovering the relevant 
conditioning features for extraposition. As a 
machine learning technique for the problem at 
hand, we chose decision tree learning, a practical 
approach to inductive inference in widespread 
use. We employ decision tree learning to 
approximate discrete-valued functions from large 
feature sets that are robust to noisy data. Decision 
trees provide an easily accessible inventory of the 
selected features and some indication of their 
relative importance in predicting the target 
features in question. The particular tool we used 
to build our decision trees is the WinMine toolkit 
(Chickering et al, 1997, n.d.). Decision trees 
built by WinMine predict a probability 
distribution over all possible target values. 
We consider two different strategies for the 
machine-learned modeling of extraposition. The 
two strategies are a series of movements versus a 
single reattachment. 
1.1 Multi-step movement 
In the multi-step movement approach, the 
question to model for each potential attachment 
site of an extraposable clause is whether the 
clause should move up to its grandparent (a ?yes? 
answer) or remain attached to its current parent (a 
?no? answer). In other words, we have cast the 
problem as a staged classification task. At 
generation runtime, for a given extraposable 
clause, the movement question is posed, and if 
the DT classifier answers ?yes?, then the clause is 
reattached one level up, and the question is posed 
again. The final attachment site is reached when 
the answer to the classification task is ?no?, and 
hence further movement is barred. Figure 1 
illustrates the multi-step movement of a clause 
(lower triangle) through two steps to a new 
landing site (the reattached clause is the upper 
triangle). Note that in both Figure 1 and Figure 2 
linear order is ignored; only the hierarchical 
aspects of extraposition are represented. 
 
Figure 1: Multi-step movement 
1.2 One-step movement 
Modeling extraposition as a one-step movement 
involves a classification decision for each node in 
the parent chain of an extraposable clause. The 
classification task can be formulated as ?should 
the extraposable clause move up to this target 
from its base position??. Figure 2 shows the 
one-step movement approach to extraposition in 
the same structural configuration as in Figure 1. 
In this example, out of the three potential landing 
sites, only one qualifies. At generation runtime, if 
more than one node in the parent chain qualifies 
as a target for extraposition movement, the node 
with the highest probability of being a target is 
chosen. In the event of equally likely target nodes, 
the target node highest in the tree is chosen. 
 
Figure 2: One-step movement 
2 Data and features 
We employed two different sets of data to build 
the models for German: the 100,000 sentence 
technical manual corpus, and the 100,000 
sentence Encarta corpus. The data were split 
70/30 for training and parameter tuning purposes, 
respectively. We extracted features for each data 
point, using the syntactic and semantic analysis 
provided by the Microsoft NLPWin system (see 
Gamon et al 2002 for more details). We only 
considered sentences for feature extraction which 
received a complete spanning parse in NLPwin. 
85.14% of the sentences in the technical domain, 
and 88.37% of the sentences in the Encarta 
corpus qualified. The following features were 
extracted: 
? syntactic label of the node under 
consideration (i.e., the starting node for a 
single-step movement), its parent and 
grandparent, and the extraposable clause 
? semantic relation to the parent node of 
the node under consideration, the parent 
and the grandparent, and the 
extraposable clause 
? status of the head of the node under 
consideration as a separable prefix verb, 
the same for the parent and the 
grandparent 
? verb position information (verb-second 
versus verb-final) for the node under 
consideration, the parent and grandparent 
? all available analysis features and 
attributes in NLPWin (see Gamon et al 
2002 for a complete list of the currently 
used features and attributes) on the node 
under consideration, the parent and 
grandparent, and on the extraposable 
clause and its parent and grandparent 
? two features indicating whether the 
extraposable node has any verbal 
ancestor node with verb-final or 
verb-second properties 
? ?heaviness? of extraposable clause as 
measured in both number of words and 
number of characters 
? ?heaviness? of the whole sentence as 
measured in both number of words and 
number of characters 
A total of 1397 features were extracted for the 
multi-step movement model. For the single-step 
movement model, we extracted an additional 21 
features. Those features indicate for each of the 
21 labels for non-terminal nodes whether a node 
with that label intervenes between the parent of 
the extraposable clause and the putative landing 
site. 
Another linguistic feature commonly cited as 
influencing extraposition is the length and 
complexity of the part of the structure between 
the original position and the extraposed clause. 
Since in the Amalgam generation module 
extraposition is applied before word and 
constituent order is established, length of 
intervening strings is not accessible as a feature.  
For each training set, we built decision trees at 
varying levels of granularity (by manipulating the 
prior probability of tree structures to favor 
simpler structures) and selected the model with 
maximal accuracy on the corresponding 
parameter tuning data set. 
Since the syntactic label of the extraposable 
clause is one of the extracted features, we decided 
to build one general extraposition model, instead 
of building separate models for each of the three 
extraposable clause types (complement clause 
COMPCL, infinitival clause INFCL, and relative 
clause RELCL). If different conditions apply to 
the three types of extraposition, the decision tree 
model is expected to pick up on the syntactic 
label of the extraposable clause as a predictive 
feature. If, on the other hand, conditions for 
extraposition tend to be neutral with respect to the 
type of extraposable clause, the modeling of 
INFCL and COMPCL extraposition can greatly 
benefit from the much larger set of data points in 
relative clause extraposition. 
3 Comparison  
To compare the one-step and multi-step models, 
we processed a new blind test set of 10,000 
sentences from each domain, Microsoft technical 
manuals and Encarta, respectively. These 
sentences were extracted randomly from data in 
these domains that were neither included in the 
training nor in the parameter tuning set. For each 
extraposable clause, three different outputs were 
computed: the observed behavior, the prediction 
obtained by iteratively applying the multi-step 
model as described in Section 1.1, and the 
prediction obtained by applying the one-step 
model. The values for these outputs were either 
?no extraposition? or a specific target node. If 
either the general extraposition prediction or the 
predicted specific target node did not match the 
observed behavior, this was counted as an error. 
3.1 One-step versus multi-step in the 
technical domain 
Accuracy data on a blind set of 10,000 sentences 
from the technical manuals domain are presented 
in Table 3. 
 One-step Multi-step Baseline 
RELCL 81.56% 83.87% 60.93% 
INFCL 93.70% 92.02% 93.70% 
COMPCL 98.10% 98.57% 94.29% 
Overall 84.42% 86.12% 67.58% 
Table 3: Accuracy numbers for the two models in 
the technical domain 
The baseline score is the accuracy for a system 
that never extraposes. Both models outperform 
the overall baseline by a large margin; the 
multi-step movement model achieves an 
accuracy 1.7% higher than the one-step model. 
The baselines in INFCL and COMPCL 
extraposition are very high. In the test set there 
were only 15 cases of extraposed INFCLs and 12 
cases of extraposed COMPCLs, making it 
impossible to draw definite conclusions. 
3.2 One-step versus multi-step in the 
Encarta domain 
Results from a blind test set of 10,000 sentences 
from the Encarta domain are presented in Table 
4. 
 One-step Multi-step Baseline 
RELCL 87.59% 88.45% 80.48% 
INFCL 97.73% 97.48% 95.72% 
COMPCL 97.32% 97.32% 95.97% 
Overall 89.99% 90.61% 84.15% 
Table 4: Accuracy numbers for the two models in 
the Encarta domain 
As in the technical domain, the multi-step model 
outperforms the one-step model, and both 
outperform the baseline significantly. Again, 
extraposed COMPCLs and INFCLs are rare in 
the dataset (there were only 17 and 6 instances, 
respectively), making the results on these types of 
clauses inconclusive. 
3.3 Domain-specificity of the models 
Since we have data from two very different 
domains we considered the extent to which the 
domain-specific models overlapped. This is a 
linguistically interesting question: from a 
linguistic perspective one would expect both 
universal properties of extraposition as well as 
domain specific generalizations to emerge from 
such a comparison. 
3.3.1 Feature selection in the technical domain 
versus Encarta 
Of the 1397 features that were extracted for the 
multi-step model, the best model for the technical 
domain was created by the WinMine tools by 
selecting 60 features. In the Encarta domain, 49 
features were selected. 27 features are shared by 
the two models. This overlap in selected features 
indicates that the models indeed capture 
linguistic generalizations that are valid across 
domains. The shared features fall into the 
following categories (where node refers to the 
starting node for multi-step movement): 
? features relating to verbal properties of 
the node 
o a separable prefix verb as 
ancestor node 
o tense and mood of ancestor 
nodes 
o presence of a verb-final or 
verb-second VP ancestor 
o presence of Modals attribute 
(indicating the presence of a 
modal verb) on ancestors 
o verb-position in the current node 
and ancestors 
? ?heaviness?-related features on the 
extraposable clause and the whole 
sentence: 
o sentence length in characters 
o number of words in the 
extraposable clause 
? syntactic labels 
? the presence of a prepositional relation 
? the presence of semantic subjects and 
objects on the node and ancestors 
? definiteness features 
? the presence of modifiers on the parent 
? person and number features 
? some basic subcategorization features 
(e.g., transitive versus intransitive) 
Interestingly, the features that are not shared (33 
in the model for the technical domain and 27 in 
the model for the Encarta domain) fall roughly 
into the same categories as the features that are 
shared. To give some examples: 
? The Encarta model refers to the presence 
of a possessor on the parent node, the 
technical domain model does not. 
? The technical domain model selects more 
person and number features on ancestors 
of the node and ancestors of the 
extraposable clause than the Encarta 
model. 
For the one-step model, 1418 total features were 
extracted. Of these features, the number of 
features selected as being predictive is 49 both in 
the Encarta and in the technical domain. 
Twenty-eight of the selected features are shared 
by the models in the two domains. Again, this 
overlap indicates that the models do pick up on 
linguistically relevant generalizations. 
The shared features between the one-step models 
fall into the same categories as the shared features 
between the multi-step models. 
The results from these experiments suggest that 
the categories of selected features are 
domain-independent, while the choice of 
individual features from a particular category 
depends on the domain. 
3.3.2 Model complexity 
In order to assess the complexity of the models, 
we use the simple metric of number of branching 
nodes in the decision tree. The complexity of the 
models clearly differs across domains. Table 5 
illustrates that for both multi-step and one-step 
movement the model size is considerably smaller 
in the Encarta domain versus the technical 
domain. 
 One-step Multi-step 
Encarta 68 82 
Technical 87 116 
Table 5: Number of branching nodes in the 
decision trees 
We hypothesize that this difference in model 
complexity may be attributable to the fact that 
NLPWin assigns a higher percentage of spanning 
parses to the Encarta data, indicating that in 
general, the Encarta data may yield more reliable 
parsing output. 
3.3.3 Cross-domain accuracy 
The results in Table 3 and Table 4 above show 
that the models based on the Encarta domain 
achieve a much higher overall accuracy (89.99% 
and 90.61%) than the models based on the 
technical domain (84.42% and 86.12%), but they 
are also based on a much higher baseline of 
non-extraposed clauses (84.15% versus 67.58% 
in the technical domain). To quantify the domain 
specificity of the models, we applied the models 
across domains; i.e., we measured the 
performance of the Encarta models on the 
technical domain and vice versa. The results 
contrasted with the in-domain overall accuracy 
from Table 3 and Table 4 are given in Table 6. 
Encarta Model Technical Model  
1-step Multi 1-step Multi 
On 
Enc. 
89.99% 90.61% 84.42% 86.12% 
On 
Tech. 
79.39% 83.03% 88.54% 89.20% 
Table 6: Cross-domain accuracy of the models 
The results show that for both one-step and 
multi-step models, the models trained on a given 
domain will outperform the models trained on a 
different domain. These results are not surprising; 
they confirm domain-specificity of the 
phenomenon. Viewed from a linguistic 
perspective, this indicates that the generalizations 
governing clausal extraposition cannot be 
formulated independently of the text domain. 
Conclusion 
We have shown that it is possible to model 
extraposition in German using decision tree 
classifiers trained on automatic linguistic 
analyses of corpora. This method is particularly 
effective for extraposed relative clauses, which 
are pervasive in German text in domains as 
disparate as news, technical manuals, and 
encyclopedic text. Both one-step and multi-step 
models very clearly outperform the baseline in 
the two domains in which we experimented. This 
in itself is a significant result, given the 
complexity of the linguistic phenomenon of 
clausal extraposition. The machine learning 
approach to extraposition has two clear 
advantages: it eliminates the need for 
hand-coding of complex conditioning 
environments for extraposition, and it is 
adaptable to new domains. The latter point is 
supported by the cross-domain accuracy 
experiment and the conclusion that extraposition 
is governed by domain-specific regularities. 
We have shown that across domains, the 
multi-step model outperforms the one-step model. 
In the German sentence realization system 
code-named Amalgam (Corston-Oliver et al 
2002, Gamon et al 2002), we have experimented 
with implementations of both the one-step and 
multi-step extraposition models, and based on the 
results reported here we have chosen the 
multi-step model for inclusion in the end-to-end 
system. 
As we have shown, extraposed relative clauses 
outnumber other extraposed clause types by a 
large margin. Still, the combined model for 
clausal extraposition outperforms the baseline 
even for infinitival clauses and complement 
clauses, although the conclusions here are not 
very firm, given the small number of relevant 
data points in the test corpus. Since the syntactic 
label of the extraposed clause is one of the 
features extracted from the training data, 
however, the setup that we have used will adapt 
easily once more training data (especially for 
infinitival and complement clauses) become 
available. The models will automatically pick up 
distinctions between the generalizations covering 
relative clauses versus infinitival/complement 
clauses when they become relevant, by selecting 
the syntactic label feature as predictive. 
Finally, evaluation of the types of features that 
were selected by the extraposition models show 
that besides the ?heaviness? of the extraposed 
clause, a number of other factors from the 
structural context enter the determination of 
likelihood of extraposition. This, in itself, is an 
interesting result: it shows how qualitative 
inspection of a machine learned model can yield 
empirically based linguistic insights. 
Acknowledgements 
Our thanks go to Max Chickering for his 
assistance with the WinMine toolkit and to the 
anonymous reviewers for helpful comments. 
References  
Chickering D. M., Heckerman D. and Meek C. (1997). 
A Bayesian approach to learning Bayesian 
networks with local structure. In "Uncertainty in 
Artificial Intelligence: Proceedings of the 
Thirteenth Conference", D. Geiger and P. Punadlik 
Shenoy, ed., Morgan Kaufman, San Francisco, 
California,  pp. 80-89. 
Chickering, D. Max. nd. WinMine Toolkit Home Page. 
http://research.microsoft.com/~dmax/WinMine/To
oldoc.htm 
Corston-Oliver S., Gamon M., Ringger E. and Moore 
R. (2002). An overview of Amalgam: a 
machine-learned generation module. To appear in 
Proceedings of the Second International Natural 
Language Generation Conference 2002, New York. 
Gamon M., Ringger E., Corston-Oliver S.. (2002). 
Amalgam: A machine-learned generation module. 
Microsoft Technical Report MSR-TR-2002-57. 
Heidorn, G. E. (2000): Intelligent Writing Assistance. 
In "A Handbook of Natural Language Processing: 
Techniques and Applications for the Processing of 
Language as Text", R. Dale, H. Moisl, and H. 
Somers (ed.), Marcel Dekker, New York, pp. 
181-207. 
Uszkoreit, H., Brants T., Duchier D., Krenn B., 
Konieczny L., Oepen S. and Skut W. (1998). 
Aspekte der Relativsatzextraposition im Deutschen. 
Claus-Report Nr.99, Sonderforschungsbereich 378, 
Universit?t des Saarlandes, Saarbr?cken, Germany. 
NewsInEssence: A System For Domain-Independent,
Real-Time News Clustering and Multi-Document
Summarization
Dragomir R. Radev?y, Sasha Blair-Goldensohn?, Zhu Zhang?, Revathi Sundara Raghavany
?School of Information
yDepartment of EECS
University of Michigan
Ann Arbor, MI 48109
fradev,sashabg,zhuzhang,rsundarag@umich.edu
1. INTRODUCTION
NEWSINESSENCE is a system for finding, visualizing and sum-
marizing a topic-based cluster of news stories. In the generic sce-
nario for NEWSINESSENCE, a user selects a single news story from
a news Web site. Our system then searches other live sources of
news for other stories related to the same event and produces sum-
maries of a subset of the stories that it finds, according to parame-
ters specified by the user.
2. THE NEWSINESSENCE SYSTEM
NewsInEssence?s search agent, NewsTroll, runs in two phases.
First, it looks for related articles by traversing links from the page
containing the seed article. Using the seed article and any related
articles it finds in this way, the agent then decides on a set of key-
words for further search. In the second phase, it attempts to add
to the cluster of related articles by going to the search engines of
various news websites and using the keywords which it found in
the first phase as search terms.
In both phases, NewsTroll selectively follows hyperlinks with
the aim of reaching pages which contain related stories and/or fur-
ther hyperlinks to related stories pages.
Both general and site-specific rules help NewsTroll determine
which URLs are likely to be useful. Only if NewsTroll determines
that a URL is ?interesting?, will it go to the Internet to fetch the new
page. A more stringent set of rules are applied to determine whether
the URL is likely to be a news story itself. If so, the similarity
of its text to that of the original seed page is computed using an
IDF-weighted vector measure. If the similarity is above a certain
threshold, the page is considered to contain a related article and
added to the cluster. The user may use our web interface (Figure 2)
to adjust the similarity threshold used in a given search.
Using several levels of filtering, NewsTroll is able to screen out
large numbers web pages quite efficiently. The expensive opera-
tion of testing lexical similarity is reserved for the small number of
.
pages which NewsTroll finds interesting. Consequently, the agent
can return useful results in real time.
3. ANNOTATED SAMPLE RUN
The example begins when we find a news article we would like
to read more about. In this case we pick a story is about a break-
ing story regarding one of President-Elect Bush?s cabinet nominees
(see Figure 1).
We input the URL using the web interface of the NEWSINESSENCE
system, then select our search options, click ?Proceed? and wait for
our results (see Figure 2).
In response to the user query, NewsTroll begins looking for re-
lated articles linked from the chosen start page. In a selection from
the agent?s output log in Figure 3, we can see that it extracts and
tests links from the page, and decides to test one which looks like a
news article. We then see that it tests this article and determines it
to be related. This article is added to the initial cluster, from which
the list of top keywords is drawn.
In its secondary phase, NewsTroll inputs its keywords to the
search engines of news sites and lets them do the work of find-
ing stories. Since we have selected good keywords, most of the
links seen by NewsTroll in this part of the search are indeed related
articles (see Figure 4). Upon exiting, NewsTroll reports the num-
ber of links it has considered, followed, tested, and retrieved (see
Figure 4).
The system?s web interface reports its progress to the user in real
time and provides a link to the visualization GUI once the cluster
is complete (Figure 5). Using the GUI, the user can select which of
the articles to summarize (see Figures 6 and 7). Figure 8 shows the
output of the cluster summarizer.
4. FUTURE WORK
We are currently working on the integration of Cross-Document
structure theory (CST) [1] with NEWSINESSENCE. CST is used to
describe relations between textual units in multi-document clusters.
It is used for example to identify which portions of a cluster contain
background information, which sections are redundant, and which
ones contain additional information about an event.
5. REFERENCES
[1] Dragomir Radev. A common theory of information fusion
from multiple text sources, step one: Cross-document
structure. In Proceedings, 1st ACL SIGDIAL Workshop on
Discourse and Dialogue, Hong Kong, October 2000.
Figure 1: Seed article.
Figure 2: User interface.
Figure 3: Run-time log (part I).
Figure 4: Run-time log (part II).
Figure 5: System progress.
Figure 6: Cluster visualization.
Figure 7: Selected articles.
Figure 8: Summarization interface.
Mining Inter-Entity Semantic Relations Using
Improved Transductive Learning
Zhu Zhang
School of Information and Department of EECS,
University of Michigan, Ann Arbor, MI 48105, U.S.A
Abstract. This paper studies the problem of mining relational data
hidden in natural language text. In particular, it approaches the relation
classification problem with the strategy of transductive learning. Dif-
ferent algorithms are presented and empirically evaluated on the ACE
corpus. We show that transductive learners exploiting various lexical and
syntactic features can achieve promising classification performance. More
importantly, transductive learning performance can be significantly im-
proved by using an induced similarity function.
1 Introduction
The world today is full of various information sources, with different ways of
representing the same information. One common problem that arises in the data
management community is that data existing in one format may be needed in a
different format for another purpose. An instance of this general problem is that
relational data don?t always exist in the form of relational tables; lots of them
are hidden in natural language text. For example, (author, book) pairs can
be instantiated as ?. . .Shakespeare?s famous work Hamlet . . . ? or ?. . .A Brief
History of Time was written by Stephen Hawking . . . ? in text.
On the other hand, within the information retrieval and natural language
processing community, Information Extraction (IE) systems are understood as
techniques for automatically extracting information from text, specifically, iden-
tifying relevant information (usually of pre-defined types) from text documents
in a certain domain. Once extracted, the information can be used for purposes
such as database population and text indexing. While significant progress has
been made in IE research, stimulated in particular by the Message Understand-
ing Conferences (MUC) 1 and the recent ACE (Automatic Content Extraction)
program 2 organized by the LDC (Linguistic Data Consortium), it is generally
agreed that many barriers exist to the wider use of IE technologies due to the
difficulties in adapting systems to new applications and domains. Keeping track
of dynamic information sources (e.g., web pages) is challenging as well.
To address these challenges, there has been a recent trend shift in the research
community from knowledge-based approaches to machine learning techniques.
Moreover, due to the cost related to acquiring large amount of labeled training
1 http://www.itl.nist.gov/iaui/894.02/related projects/muc/
2 http://www.ldc.upenn.edu/Projects/ACE/
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 402?413, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Mining Inter-Entity Semantic Relations 403
data, researchers have been looking at various learning algorithms exploiting
cheaply available unlabeled data (usually in much larger amounts), which aim
at minimizing the need for labeled data while still achieving comparable results.
According to the scope of ACE, current IE research has three main objectives:
Entity Detection and Tracking (EDT), Relation Detection and Characterization
(RDC), and Event Detection and Characterization (EDC). This study focuses on
the second subproblem, RDC. In particular, the goal is to automatically classify
binary relations between entities, i.e., to decide in which relational table to put
each entity pair, using transductive learning algorithms. We propose an improved
transductive learner and empirically compare it with the baseline learner on the
ACE corpus.
2 Related Work
The current paper draws upon previous work in NLP and machine learning.
2.1 Relation Extraction and Classification
Within the realm of information extraction, there are several representative sys-
tems that use machine learning for extracting relations.
Snowball [1] is a bootstrapping-based system that requires only a handful
of training examples of tuples of interest. These examples are used to generate
extraction patterns, which in turn result in new tuples being extracted from
the document collection. At each iteration of the extraction process, Snowball
evaluates the quality of these patterns and tuples without human intervention,
and keeps only the most reliable ones for the next iteration. A scalable evaluation
methodology is also developed for the task. The approach was illustrated on the
problem of extracting (organization, headquarter location) pairs from a
collection of more than 300, 000 newspaper documents.
DIPRE (Dual Iterative Pattern Relation Expansion) [2] is another technique
that exploits the duality between sets of patterns and relations to grow the
target relation starting from a small sample. The technique was used to extract
(author, title) pairs from the World Wide Web.
In [3], an application of kernel methods to extracting relations from natural
language text is presented. The authors introduce kernels defined over shallow
parse representations of text, and design efficient algorithms for computing the
kernels. The devised kernels are used in conjunction with SVM and Voted Per-
ceptron learning algorithms for the task of extracting person-affiliation and
organization-location relations from text. The proposed methods are com-
pared with feature-based learning algorithms, with promising results.
More recently, Zhang [4] investigates the relation classification problem by
bootstrapping from a small amount of labeled data. Bootstrapping procedures
are built on top of SVM classifiers and evaluated on the ACE corpus.
Rosario and Hearst [5] examine the problem of distinguishing among seven
relation types that can occur between the entities ?treatment? and ?disease?
in bioscience text, and the problem of identifying such entities. Five different
404 Z. Zhang
generative graphical models and a neural network model using lexical, syntactic,
and semantic features are compared. The authors find that the neural network
helps achieve high classification accuracy.
2.2 Transductive Learning
Almost all work above falls into the realm of ?inductive learning?, in the sense that
a ?model? is first induced from the labeled (training) data and then used to predict
unseen data. The beauty of this approach is that once the classification function
(model) is generalized (assuming a ?good? generalization algorithm), it can be
used for prediction independently of the labeled data on which it was trained.
In many domains, including NLP, there is usually a large amount of unlabeled
data but only limited amount of labeled training data. If a generalized model
is preferred, one can still follow the inductive learning paradigm, which entails
work such as bootstrapping [6]. On the other hand, we might encounter the
following situation:
? we are only concerned about performance on a particular pool of data,
? and we don?t care about generalizability,
? and data points can be effectively queried/accessed
If all the conditions above are true, the learner can observe the test data
and potentially exploit structures in their distribution. In other words, there
is really no difference between ?unlabeled data? and ?test data?, and the re-
search question is: ?given some labeled data and a large set of (unlabeled) test
data, can properties of the entire data set be used to make predictions?? This
is the motivation behind transductive learning. The setting itself, specifically,
transductive SVMs, was first introduced by Vapnik [7], and then later refined
by [8] and [9]. Other approaches are based on s ? t cuts [10,11] or multi-way
cuts [12]. Joachims [13] presents Spectral Graph Transducer (SGT), which is a
transductive version of the k nearest-neighbor classifier.
3 Problem Definition
The research problem of this paper is classification of relations between entities.
In other words, the task is to determine the appropriate relational table into
which one should put a given pair of related entities. To be more precise,
? We only focus on binary relations, i.e., ones between pairs of entities.
? We only deal with intra-sentence explicit relations in this study. In other
words, the (two) EDT mentions of the entity arguments of a relation must
occur within a common syntactic construction, in this case a sentence. The
relations also have to be ?explicit? in the sense that they should have explicit
textual support and don?t require further reasoning based on understanding
of the context?s meaning.
? We don?t actually ?detect? relations. Rather, the goal is to classify the type
of relation between two entities (or, in other words, to put the entity pair
into the correct relational table), given that they are known to be related.
Mining Inter-Entity Semantic Relations 405
? It is also assumed that entity recognition already takes place beforehand,
hence all entity-related information is available.
We use the five high-level relations defined in ACE RDC Annotation Guide-
lines V3.6 as the target set of classes of the classification task (in other words,
they define the five candidate relational tables into which the entity pairs will
be dispatched). These are:
ROLE affiliation between people and organizations, facilities, and GPEs (Geo-
Political Entities). This includes employment, office holder, ownership,
founder, member, and nationality relationships, etc.
PART part-whole relationships between organizations, facilities and GPEs.
AT location of a Person, Organization, GPE, or Facility entity. For example,
a person is at a Location, GPE or Facility if the context indicates that the
person was, is or will be there. An Organization is in a Location/GPE if it
has a branch there.
NEAR indicates that an entity is explicitly near a location, but not actually in
that location or part of that location.
SOC personal or professional relationships between people, such as relative,
associate, etc.
First, for each relation r in the list above, we learn the following classifier:
Cr : (cpr, e1, cm, e2, cpt) ? l
where a sentence is a concatenation of five parts, with e1 and e2 representing
the entities, and cpr, cm, and cpt representing the pre-, mid-, and post-context
respectively. A label l ? {0, 1} is assigned to the five-tuple. For example, in the
following sentence,
Shares of Disney, parent company of ABC, are up five eighths.
?Disney? and ?ABC? are the two ?ORGANIZATION? entities, and they divide
the whole sentence into three context windows (the pre-context before ?Disney?,
the post-context after ?ABC?, and the mid-context between the two entities).
With regard to the ?PART? relation, the label is ?1?, and ?0? for other relations.
Then we combine the multiple binary classifiers and get a single classifier
C(cpr, e1, cm, e2, cpt) = arg max
ri
Cri(cpr, e1, cm, e2, cpt)
In the example above, a label ?PART? is eventually assigned to the tuple.
4 Approach: Learning Similarity Functions for
Transductive Learning
4.1 Formalization of Different Learning Paradigms
Assuming we have
? Input (instance) space X and output (label) space Y
406 Z. Zhang
? Labeled data set L and unlabeled data set U (as mentioned before, no dis-
tinction is made between ?unlabeled? and ?test? data in the transductive
learning setting)
One could distinguish three types of learning paradigms:
? Induction
(XL, YL) ? f (1)
where f represents the induced model
? Induction with unlabeled data
(XL, YL) ? XU ? f (2)
? Transduction
(XL, YL) ? XU ? YU (3)
The three learning paradigms clearly have different advantages and different
application scenarios. However, when it comes to exploiting unlabeled data, the
tradeoff between the last two is not yet well understood. In this paper, we focus
on the last learning paradigm, i.e., transductive learning.
4.2 Transductive Learning with Learned Similarity Function
A general approach to transductive learning is to construct a graph of all data
points based on distance or similarity among them, and then to use the ?known?
labels to perform some type of graph partitioning or label propagation.
In this study, we use the Spectral Graph Transducer (implemented in SGT-
light) [13] as our baseline transductive learner, which exactly follows the trans-
ducitve learning paradigm defined by Equation (3). The basic idea of SGT is to
construct a similarity weighted undirected k nearest-neighbor (kNN) graph G
on X with adjacency matrix A (defined below), and then run spectral partition-
ing on it.
Aij =
{
similarity(xi,xj)
?
xk?knn(xi)
similarity(xi,xk)
xj ? knn(xi)
0 otherwise
(4)
Notice that what takes a crucial role in shaping the structure of graph is
the similarity function, as which SGT uses the cosine value between feature
vectors. However, there might exist other choices for similarity functions. Our
hypothesis is:
If we can learn (induce) a similarity function from part of the labeled
data and use it to construct a new weighted graph G? over the unlabeled
data and the remaining labeled data, a transductive learner on G? will
outperform the baseline transductive learner that works on G.
Mining Inter-Entity Semantic Relations 407
This defines the following modified version of the transductive learning
paradigm:
(XL1 , YL1) ? fL1
fL1(XL2 ? XU ) ? G?
((XL2 , YL2) ? XU )G
? ? YU
(5)
in which L1 ? L2 = L and L1 ? L2 = ?.
Below is a very straightforward (yet effective, as the readers will see from
experimental results) way of defining the ?learned? similarity function. Suppose
the induced model fL1 assigns a confidence score confidencefL1 (xi) to each data
points based on its model trained on the the labeled data, then the similarity
function in G? can be defined as:
similarity(xi, xj) = e?distance(xi,xj) (6)
where the ?distance? between two data points is defined as
distance(xi, xj) = |confidencefL1 (xi) ? confidencefL1 (xj)| (7)
Simply put: the more different the confidence scores, the further away two
instances are from each other; the further away, the less similar they are.
4.3 Features
We extract the following lexical and syntactic features (all categorical features
are binarized) from the linguistic context in which the two entities co-occur:
Lexical features. Surface tokens of the two entities and three context windows.
Shallow-syntactic features. Part-Of-Speech tags (e.g., ?noun?, etc.) corre-
sponding to all tokens in the two entities and three context windows.
Deep-syntactic features. To capture the syntactic dependencies between en-
tities, the following features are extracted from the chunklink representation
(flattened parse trees):
? Chunk tags of the two entities and three context windows. This informa-
tion is not explicitly present in the treebank format. For example, the
?O? tag means that the current word is outside of any chunk; the ?I-XP?
tag means that this word is inside an XP chunk; the ?B-XP? by default
means that the word is at the beginning of an XP chunk.
? Grammatical function tags of the two entities and three context windows.
The last word in each chunk is its head, and the function of the head is
the function of the whole chunk. For example, ?NP-SBJ? means an NP
chunk as the subject of the sentence. The other words in a chunk that
are not the head have ?NOFUNC? as their function.
? IOB-chains of the heads of the two entities, each of which is a lexicalized
path, in other words, a concatenation of the syntactic categories of all
408 Z. Zhang
the constituents on the path from the root node to this leaf node of the
tree (e.g., ?S/VP/NP/NN?).
Other features. Miscellaneous information including:
? An ordering flag that indicates the relative position of the two entity
arguments of a relation.
? Types of the two entities, such as ?PERSON? or ?GPE?.
The context windows are defined as the following:
? Mid-context: everything between the two entities.
? Pre- (post-) context: up to two words before (after) the corresponding entity.
5 Experiments and Results
5.1 Data
We use the ACE corpus for our task. Specifically, ACE-2 version 1.0 is used,
which contains 519 files from sources including broadcast, newswire, and news-
paper. The corpus contains 5, 260 manually tagged relations (a small number
of additional relations are dropped out due to data preprocessing errors). A
breakdown of the data by different relation type is given in Table 1. We treat
the ?training? and ?devtest? portions of the corpus as a whole and perform our
split on the data in the experiments.
Table 1. Number of relations: break-down by relation type
Relation type Training Devtest
ROLE 1964 472
PART 549 123
AT 1249 328
NEAR 78 31
SOC 398 68
Total 4238 1022
The following steps are taken to process the data:
1. Parse the ACE data in XML format; extract and index entities and relations.
2. Segment the text into sentences using the sentence segmenter provided by
the DUC competition 3.
3. Parse the sentences using the Charniak parser [14].
4. Convert the parse trees into chunklink format using chunklink.pl [15].
5. Extract and compute features from the chunklink format.
3 http://duc.nist.gov/past duc/duc2003/software/
Mining Inter-Entity Semantic Relations 409
5.2 Experimental Setup and Evaluation Metrics
To test the superiority of the learned similarity function in the transductive
setting, we experiment the following three scenarios:
? A vanilla SGT learner that uses a labeled set of size 2, 000 and an unlabeled
set (by hiding the labels) of size 3, 260.
? A modified SGT learner (SVM-SGT) that uses SVM-light [16] as the induc-
tive learner for similarity functions. (In this case, the confidence score for
each data point is the value of the decision function.)
? Another modified SGT learner (SNoW-SGT) that uses SNoW [17] with the
Winnow updating rule [18] as the inductive learner for similarity functions.
(In this case, the confidence score for each data point is the softmax normal-
ized activation for the positive label.)
For both the SVM-SGT and SNoW-SGT learners, we use the same amount
of labeled and unlabeled data as for the vanilla SGT learner, with half of the
labeled data (1, 000 data points) used for inducing the similarity function, and
the other half used for SGT learning on the modified graph/matrix. All three
experiments are run with 10 random splits of the whole data set, which contains
5, 260 data points.
In all three scenarios, the final combination of multiple classifiers is done by
assigning the label for which the corresponding binary classifier has the highest
confidence score (i.e., the solution of the spectral optimization problem in SGT).
To evaluate the performance of learning algorithms, we compute overall clas-
sification accuracy, and for each class, the precision, recall, and F-measure.
5.3 Experimental Results: Effect of Induced Similarity Measure
We experimented different values of k, ranging from 20 to 120, for kNN graph.
Empirically, they do not seem to make a lot of difference. All the performance
numbers reported below are based on 100-NN graphs.
With the vanilla SGT learner, we get a 70.34% accuracy, and the class-specific
performance is summarized in Table 2.
With the SVM-SGT learner, we get a 78.04% accuracy, and the class-specific
performance is summarized in Table 3.
With the SNoW-SGT learner, we get a 76.02% accuracy, and the class-specific
performance is summarized in Table 4.
Table 2. Performance of vanilla SGT learner (full)
Relation type Precision Recall F-measure
ROLE 73.72% 83.31% 78.19%
PART 63.34% 42.32% 49.93%
AT 67.43% 72.88% 69.95%
NEAR 65.92% 7.36% 12.71%
SOC 71.87% 47.81% 56.96%
410 Z. Zhang
Table 3. Performance of SVM-SGT learner (full)
Relation type Precision Recall F-measure
ROLE 82.87% 84.01% 83.41%
PART 63.31% 57.49% 60.13%
AT 77.16% 79.69% 78.36%
NEAR 4.81% 0.62% 5.32%
SOC 76.23% 88.88% 81.93%
Table 4. Performance of SNow-SGT learner (full)
Relation type Precision Recall F-measure
ROLE 81.47% 79.61% 80.44%
PART 62.72% 62.34% 62.35%
AT 74.57% 81.15% 77.64%
NEAR 0.59% 0.13% NA
SOC 73.73% 77.92% 75.96%
The most important result of interest is that both modified SGT learners
consistently outperforms the vanilla SGT learner across all random runs, and
the differences are statically significant (p << 0.01). This justifies our hypothesis
that a learned similarity function between data points, as opposed to naive cosine
similarity, can significantly improve the performance of transductive learners.
5.4 Experimental Results: Comparison with Supervised Inductive
Learners
To get a sense of the empirical difference between transductive, improved trans-
ductive, and inductive learning algorithms, we also present the performance of
a few supervised inducitve learners on the same number of training examples
(2, 000). Results are also averaged over 10 random runs.
With the supervised SVM learner, we get a 82.31% accuracy, and the class-
specific performance is summarized in Table 5.
With the supervised SNoW learner, we get a 77.37% accuracy, and the class-
specific performance is summarized in Table 6.
Table 5. Performance of supervised SVM learner (full)
Relation type Precision Recall F-measure
ROLE 86.27% 85.96% 86.11%
PART 75.90% 58.36% 65.89%
AT 78.87% 88.65% 83.46%
NEAR 83.96% 3.57% 8.41%
SOC 82.13% 94.29% 87.74%
Mining Inter-Entity Semantic Relations 411
Table 6. Performance of supervised SNoW learner (full)
Relation type Precision Recall F-measure
ROLE 85.46% 80.30% 82.19%
PART 64.93% 64.40% 64.50%
AT 77.07% 79.77% 77.77%
NEAR 28.94% 27.95% 24.56%
SOC 79.69% 84.32% 81.21%
Table 7. Performance of supervised naive bayes learner (full)
Relation type Precision Recall F-measure
ROLE 52.63% 97.56% 68.37%
PART 0% 0% 0%
AT 78.13% 35.83% 48.82%
NEAR 0% 0% 0%
SOC 0% 0% 0%
With the supervised Naive Bayes learner, we get a 56.10% accuracy, and the
class-specific performance is summarized in Table 7.
If we compare the performance presented in this subsection with those of the
corresponding transductive learners in the previous subsection, we observe the
following pattern:
NB < SGT < SNoW-SGT < SNoW < SVM-SGT < SVM
With regard to the purpose of this study, again, it is most important to no-
tice that the induction-aided transductive learners significantly outperform the
?pure? transductive learner. On the other hand, it is reasonable to expect that
with improvement of the fundamental algorithm (e.g., spectral partioning), the
transductive learners (with or without induced similarity measures) may out-
perform the best inductive learners.
6 Conclusions and Future Work
This paper approaches the relation classification problem with improved trans-
ductive learning. Specifically, we learned the following:
? Application of transductive learning on NLP problems, including informa-
tion extraction, has been under-explored. This paper makes the attempt to
show that binary relations hidden in natural language text can be effectively
classified by using transductive learning.
? It is shown that an improved transductive learner using similarity functions
induced from a small amount of labeled data outperforms its naive trans-
ductive counterpart.
412 Z. Zhang
? Further more, the general idea of inducing similarity functions for trans-
ductive learning are potentially applicable to other classification problems,
since it doesn?t have any specific characteristics tied to the current relation
classification problem.
In the future, we are interested in pursuing the following directions:
? The current work only deals with binary relations. The algorithms presented
should be generalized so that they can work on higher-order relations.
? In this study, we only used a randomly selected portion of the labeled data
available as the seed labeled set for inducing similarity functions. It is con-
ceivable that if we anchor the seed data points more intelligently (e.g., using
clustering or in other unsupervised fashion), better classification performance
of the modified transductive learner can be expected.
? This chapter presents one particular way of inducing the similarity function
for transductive learning, which is simple yet effective. However, it may be
worth the effort to investigate other alternatives.
? In the machine learning community, how to exploit unlabeled data remains
largely an open question. In the long run, it would be very interesting and
useful to investigate, both theoretically and empirically, the tradeoff between
induction with unlabeled data vs. transduction (including ?induction-aided?
transduction discussed in this paper).
References
1. Agichtein, E., Gravano, L.: Snowball: Extracting relations from large plain-text
collections. In: Proceedings of the Fifth ACM International Conference on Digital
Libraries. (2000)
2. Brin, S.: Extracting patterns and relations from the world wide web. In: WebDB
Workshop at 6th International Conference on Extending Database Technology,
EDBT?98. (1998)
3. Zelenko, D., Aone, C., Richardella, A.: Kernel methods for relation extraction. J.
Mach. Learn. Res. 3 (2003) 1083?1106
4. Zhang, Z.: Weakly-supervised relation classification for information extraction. In:
Proceedings of the 13th International Conference on Information and Knowledge
Management CIKM 2004, Washington DC (2004)
5. Rosario, B., Hearst, M.: Classifying semantic relations in bioscience text. In:
Proceedings of the 42nd Annual Meeting of the Association for Computational
Linguistics. (2004)
6. Abney, S.: Understanding the Yarowsky algorithm. Computational Linguistics 30
(2004)
7. Vapnik, V.N.: Statistical learning theory. John Wiley, NY (1998)
8. Joachims, T.: Transductive inference for text classification using support vector
machines. In Bratko, I., Dzeroski, S., eds.: Proceedings of ICML-99, 16th Interna-
tional Conference on Machine Learning, Bled, SL, Morgan Kaufmann Publishers,
San Francisco, US (1999) 200?209
9. Bennett, K.: Combining support vector and mathematical programming methods
for classification. In Sch?lkopf, B., Burges, C., Smola, A., eds.: Advances in Kernel
Methods - Support Vector Learning. MIT-Press (1999)
Mining Inter-Entity Semantic Relations 413
10. Blum, A., Chawla, S.: Learning from labeled and unlabeled data using graph
mincuts. In: ICML ?01: Proceedings of the Eighteenth International Conference on
Machine Learning, San Francisco, CA, USA, Morgan Kaufmann Publishers Inc.
(2001) 19?26
11. Blum, A., Lafferty, J., Rwebangira, M.R., Reddy, R.: Semi-supervised learning
using randomized mincuts. In: ICML ?04: Twenty-first international conference on
Machine learning, New York, NY, USA, ACM Press (2004)
12. Kleinberg, J., Tardos, E.: Approximation algorithms for classification problems
with pairwise relationships: metric labeling and Markov random fields. In: Pro-
ceedings of the 40th Annual Symposium on Foundations of Computer Science.
(1999) 14?23
13. Joachims, T.: Transductive learning via spectral graph partitioning. In: Proceed-
ings of The Twentieth International Conference on Machine Learning (ICML).
(2003)
14. Charniak, E.: A maximum-entropy-inspired parser. Technical Report CS-99-12,
Computer Scicence Department, Brown University (1999)
15. Buchholz, S.: The chunklink script. (2000) Software available at
http://ilk.uvt.nl/~sabine/chunklink/.
16. Joachims, T.: Making large-scale support vector machine learning practical. In:
Advances in kernel methods: support vector learning. MIT Press, Cambridge, MA,
USA (1999) 169?184
17. Carlson, A., Cumby, C., Rosen, J., Roth, D.: The SNoW learning architec-
ture. Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Depart-
ment (1999)
18. Littlestone, N.: Learning quickly when irrelevant attributes abound: A new linear-
threshold algorithm. Mach. Learn. 2 (1988) 285?318
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 885 ? 895, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Tense Tagging for Verbs in Cross-Lingual Context:  
A Case Study 
Yang Ye1 and Zhu Zhang2 
1
 Department of Linguistics, University of Michigan, USA 
2
 School of Information and Department of Electrical Engineering and Computer Science,  
University of Michigan, USA 
yye@umich.edu, zhuzhang@umich.edu 
Abstract. The current work applies Conditional Random Fields to the problem 
of temporal reference mapping from Chinese text to English text. The learning 
algorithm utilizes a moderate number of linguistic features that are easy and in-
expensive to obtain. We train a tense classifier upon a small amount of manu-
ally labeled data. The evaluation results are promising according to standard 
measures as well as in comparison with a pilot tense annotation experiment in-
volving human judges. Our study exhibits potential value for full-scale machine 
translation systems and other natural language processing tasks in a cross-
lingual scenario. 
1   Introduction 
Temporal resolution is a crucial dimension in natural language processing. The fact that 
tense does not necessarily exist as a grammatical category in many languages poses a 
challenge on cross-lingual applications, e.g. machine translation. The fact that English 
tenses and Chinese aspect markers align at word level on one hand and sub-word level 
on the other hand poses a challenge for temporal reference distinction translation in a 
statistical machine translation (MT) system. A word-based alignment algorithm will not 
be able to capture the temporal reference distinction when mapping between Chinese 
and English. Being able to successfully map the temporal reference distinction in Chi-
nese text through disparate features onto the most appropriate tenses for the parallel 
English text is an important criterion for good translation quality. Languages have vari-
ous levels of time reference distinction representation: some have finer grained tenses 
than others, as typological studies have shown. When facing the unbalanced levels of 
temporal reference distinction between a pair of languages, we have to optimize the 
mapping between the two temporal systems through intelligent learning. Most machine 
translation systems do not have a separate temporal reference resolution module, but if 
we can integrate a special module into them, the temporal reference resolution of the 
system could be corrected accordingly and yield a better translation. Other than machine 
translation, in cross-lingual question answering (CLQA) with English as the target lan-
guage, the ability to successfully formulate queries and maintain the temporal reference 
information in the original questions is desirable. 
886 Y. Ye and Z. Zhang 
2   Related Work 
2.1   Temporal Reference Modeling in Cross-Lingual Scenario 
The nature of being past, present or future is highly relative and hence the information 
contained in tenses is often referred to as temporal reference distinction. While there 
is a large body of research on temporal reference in formal semantics and logic as 
well as in other disciplines of Linguistics, works in cross-lingual temporal reference 
mapping remain inadequate.  
Campbell et. al. [1] proposed a language-neutral framework for representing se-
mantic tense. This framework is called the Language Neutral Syntax (LNS). Based on 
the observation that grammatical or morphological tenses in different languages do 
not necessarily mean the same thing, they interpret semantic tense to be largely a rep-
resentation of event sequence; their work did not attempt direct and explicit represen-
tations of tenses. The tense node in the LNS tree contains either global tense feature 
(also known as ?absolute tense?) or anchorable tense feature (also known as ?relative 
tense?). This work treated compound tenses as being represented by primary and sec-
ondary tense features. The tense in an embedded clause is anchored to the tense in the 
matrix clause. Campbell?s work attempted neither a strict nor a deep semantic repre-
sentation of tenses, but rather a syntactic representation that is language-neutral. In 
addition, similar to most of its peer works in tense modeling, it only attacked the 
problem in a scope of individual sentences. 
Pustejovsky et. al. [2] reported an annotation scheme, the TimeML metadata for 
markup of events and their anchoring in documents. The challenge of human labeling 
of links among eventualities were discussed to the full fledge in their paper showing 
that inter-annotator consistency for links is a hard-to-reach ideal. The automatic 
?time-stamping? was attempted earlier on a small sample of text in an earlier work of 
Mani [3]. The result was not particularly promising showing need for bigger size of 
training data as well as more predictive features, especially on the discourse level. At 
the word level, semantic representation of tenses could be approached in various ways 
depending on different applications. None of the previous works were designed par-
ticularly for cross-lingual temporal reference distinction mapping and the challenges 
of this mapping for some language pairs have not received full attention. 
2.2   Temporal Reference Mapping Between Chinese and English 
Since temporal reference distinction mapping is of particular interest of cross-lingual 
natural language processing tasks, the pilot works for tense classification in Chinese 
were naturally motivated by machine translation scenario. Olsen et. al. [4] attacked 
tense reconstructing for Chinese text in the scenario of Chinese to English MT. On 
top of the more overt features, their work made use of the telicity information en-
coded in the lexicons through the use of Lexical Conceptual Structures (LCS). Based 
on the dichotomy of grammatical aspect and lexical aspect, they proposed that past 
tense corresponds to the telic LCS which is either inherently telic or derived telic. 
While grammatical aspect markings supersede the LCS, in the absence of grammati-
cal aspect marking, verbs that have telic LCS are translated into past tense and present 
tense otherwise. This work, while pushing tense reconstruction one step further to-
wards the semantics embedded in the events, is subject to the risk of adopting one-to-
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 887 
one mapping between grammatical aspect markings and tenses hence oversimplifies 
the temporal reference situation in Chinese text. Additionally, their binary tense tax-
onomy is oversimplifying the rich temporal reference system that exists in Chinese. 
Li et. al. [5] proposed a computational model based on machine learning and het-
erogeneous collaborative bootstrapping for analyzing temporal relations in a Chinese 
multiple-clause sentence. The core model is a set of rules that map the combinational 
effects of a set of linguistic features to one class of temporal relations for one event 
pair. Their work showed promising results for combining machine learning algorithms 
and linguistic features to achieve temporal relation resolution, but did not directly ad-
dress cross-lingual temporal reference information mapping. The nature of the task 
they were attacking is B Series temporal resolution in Mctaggart?s terminology.   
3   Problem Definition 
3.1   The Taxonomy of Tenses 
In the current literature, the taxonomy of tenses typically includes the three basic 
tenses (present, past and future) plus their combination with the progressive and per-
fect grammatical aspects, because in English tense and aspect are morphologically 
merged. This yields a taxonomy of 13 tenses. We collapse these 13 tenses into a tax-
onomy of three classes: present, past and future. The reason for this collapse is two-
fold: linguistically, this three-class taxonomy conforms more strictly with the well  
defined tripartite temporal reference distinction [6]; and in practice, only nine tenses 
occurred  in  our  data  set:  simple past, simple future, simple present, present perfect, 
 
Fig. 1. Tense Taxonomy 
 
1. Present 
E,R,S 
2. Past 
E,R S 
3. Future 
E,R S 
Perfect 
4. 
E R,S 5. 
E 
R,S 
6. 
E SR 7. 
E 
R S 
8. 
E 
S R 
9. 
E 
S R 
Progressive 
E 
10. 
R,S 
E 
11. 
R S 
12. 
S R 
E
13. 
R S E 
888 Y. Ye and Z. Zhang 
present progressive, past perfect, past progressive, past future and present perfect pro-
gressive. Some tenses are very sparse in the data set yielding little value from the 
learning perspective. Figure 11 shows the tense taxonomy. In the graph, for each of 
the thirteen tenses, we provide the timeline representation for the configuration of the 
three time points under Reichenbachian system. E stands for the event time, R stands 
for the reference time and S stands for the speech time. It is observed that in terms of 
the relationship between the speech time and the event time, the thirteen tenses could 
be grouped into three categories: tense 1 and tense 5 have the event time overlapping 
with the speech time; tense 2, 4, 6, 7, 10, 11 and 13 have the event time being prior to 
the speech time; tense 3, 8, 9 and 12 have the event time being later than the speech 
time. These three categories form our collapsed tense taxonomy. 
3.2   Problem Formulation 
In general, the tense tagging problem for verbs can be formalized as a standard classi-
fication or labeling problem, in which we try to learn a classifier 
C: V?T 
where V is the set of verbs (each described by a feature vector), and T is the set of 
possible tense tags (defined by the taxonomy above). 
This is, however, a somewhat simplistic view of the picture. Just as temporal 
events are usually sequentially correlated, verbs in adjacent linguistic utterances are 
not independent. Therefore the problem should be further formalized as a sequential 
learning problem, where we try tag a sequence of verbs (V1, ?, Vn) with a sequence 
of tense tags (t1, ?, tn). This formalization shares similarities with many other prob-
lems inside and outside the computational linguistics community, such as information 
extraction from web pages, part-of-speech tagging, protein and DNA sequence analy-
sis, and computer intrusion detection. 
4   Data 
4.1   Data Summary 
We use 52 pairs of parallel Chinese-English articles from LDC release. The 52 Chi-
nese articles from Xinhua News Service consist of 20626 Chinese characters in total 
with each article containing between about 340 and 400 Chinese characters The Chi-
nese documents are in Chinese Treebank format with catalog number LDC2001T11. 
The parallel English articles are from Multiple-Translation Chinese (MTC) Corpus 
from LDC with catalog number LDC2002T01. We use the best human translations 
out of 10 translation teams2 as our gold-standard parallel English data. 
                                                          
1
  For tense 13, it is controversial whether the event time precedes or succeeds the speech time. 
(e.g. for ?I was going to ask him at that time?, it is not clear whether the asking event has 
happened by the speech time.) This graph only represents the authors? hunch about the tense 
taxonomy for this particular project. 
2
  Two LDC personnel, one a Chinese-dominant bilingual and the other an English-dominant bi-
lingual, performed this ranking. There was overall agreement on the ranking between the two 
and minor discrepancies were resolved through discussion and comparison of additional files. 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 889 
4.2   Obtaining Tense Tags from the Data 
The decision of the granularity level of the data points in the current project is a non-
trivial issue. Recently it has been argued that tense should be regarded as a category 
of the whole sentence, or in logical terms of the whole proposition, since it relates to 
the truth value of the proposition as a whole, rather than just some property of the 
verb. While we agree with this assertion, in the interest of focusing on our immediate 
goal of assigning an appropriate tense tag to the parallel verb in the target language, 
we adopt the more traditional analysis of tense as a category of the verb on the basis 
of its morphological attachment to the verb.  
There are a total of 1542 verbs in the 52 Chinese source articles. We manually 
aligned these verbs in the Chinese source article with their corresponding verbs in 
English; this yields a subset of 712 verbs out of the 1542 verbs being translated into 
English as verbs. We see a dramatic nominalization (i.e. verbal expressions in Chi-
nese are translated into nominal phrases in English) process in Chinese-to-English 
translation through the dramatic contrast between these two numbers. We excluded 
the verbs that are not translated as verbs into the parallel English text. This exclusion 
is based on the rationale that another choice of syntactic structure might retain the 
verbal status in the target English sentence, but the tense of those potential English 
verbs would be left to the joint decision of a set of disparate features. Those tenses are 
unknown in our training data. 
5   Tense Tagging by Learning 
5.1   Temporal Reference Distinction in Chinese Text 
Assigning accurate tense tags to the English verbs in Chinese-to-English Machine 
Translation is equivalent to understanding temporal reference distinction in the source 
Chinese text. Since there are no morphologically realized tenses in Chinese, the tempo-
ral reference distinction in Chinese is encoded in disparate linguistic features. Figure 2 
shows how various features in simple Chinese sentences jointly represent the temporal 
reference distinction information. For complex sentences with an embedding structure, 
these  features  will  behave  in  a more complicated way in that the anaphoric relations  
 
Sentence-
final modal 
particle 
   verb + Post-verbal marker + 
Temporal 
adverbial 
+ 
Lexical properties (Vendler, 1967): 
activity, state, achievement and 
accomplishment  
 
Fig. 2. Temporal Structure for a Simple Chinese Sentence 
890 Y. Ye and Z. Zhang 
between the reference time and speech time hold differently for main verbs and verbs 
in embedded structure. While world knowledge is beyond the scope of our computa-
tional capacity at this stage, we expect that the various linguistic features will be able 
to approximately reconstruct the temporal reference distinction for Chinese verbs. 
5.2   The Feature Space 
There are a big variety of heterogeneous features that contribute to the temporal refer-
ence semantics of Chinese verbs. Tenses in English, while manifesting temporal ref-
erence distinction, do not always reflect the distinction at the semantic level, as is 
shown in the sentence ?I will leave when he comes.? Hornstein [7] accounted for this 
type of phenomenon by proposing the Constraints on Derived Tense Structures. 
Hence the feature space we propose to use consists of the features that contribute to 
the semantic level temporal reference construction as well as those contributing to the 
tense generation from that semantic level. 
The feature space includes the following 11 features: 
feature1: whether the current sentence contains a temporal noun phrase, a 
 temporal location phrase or a temporal prepositional phrase; 
feature2: whether or not the current verb is in quoted speech; 
feature3: whether the current verb appears in relative clause or sentential  
     complement; 
feature4: whether or not the current verb is in news headlines; 
feature5: previous word's POS; 
feature6: current verb's POS, there are three types of verbs in the corpora: the  
regular verbs (VV); the copula ?shi4?3 (VC) and the verb ?you3? (VE); 
feature7: next word's POS; 
feature8: whether or not the verb is followed by the aspect marker ?le?; 
feature9: whether or not the verb is followed by the aspect marker ?zhe?; 
feature10: whether or not the verb is followed by the aspect marker ?guo?; 
feature11: whether or not the verb is a main verb; 
The above 11 features include lexical features as well as syntactic features. None 
of the above features is expensive to obtain. We aim to show that the temporal refer-
ence distinction classe, as a semantic feature of the verb, could be predicted by learn-
ing from inexpensive linguistic features that are easily available. Feature 11 is moti-
vated by the observation that tense in English is used to inform the reader (listener) of 
when the event associating with the main verb occurs with respect to the time of ut-
terance while the tense of an embedded verb does not necessarily indicate this rela-
tionship directly. In the current paper, we have a different definition for main verb: 
any verb that is not in embedded structure is treated as a main verb including those 
verbs appearing in adjunct clauses. 
5.3   Learning Algorithm: Conditional Random Field 
Conditional Random Fields (CRF) is a formalism well-suited for learning and predic-
tion on sequential data. It is a probabilistic framework proposed by Lafferty [8] for 
                                                          
3
  The digit at the end of the syllable here indicates the tone. ?Shi4? means ?be? and ?you3? 
means ?have?. 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 891 
labeling and segmenting structured data, such as sequences, trees and lattices. The 
conditional nature of CRFs relaxes the independence assumptions required by tradi-
tional Hidden Markov Models (HMMs); CRFs also avoid the label bias problem ex-
hibited by maximum entropy Markov models (MEMMs) and other conditional 
Markov models based on directed graphical models. CRFs have been shown to per-
form well on a number of real-world problems, in particular, NLP problems such as 
shallow parsing [9], table extraction [10], and named entity recognition [11]. 
For our experiments, we use the off-the-shelf implementation of CRFs provided by 
MALLET [12].  
6   Experiments and Evaluation 
6.1   Preliminary Experiment with Tense Annotation by Human Judges 
In order to evaluate the empirical challenge of tense generation in a Chinese-to-
English Machine Translation system, a pilot experiment of tense annotation for Chi-
nese text by native judges was carried. The annotation experiment was carried out on 
20 news articles from LDC Xinhua News release with category number 
LDC2001T11. The articles were divided into 4 groups with 5 articles in each group. 
For each group, three native Chinese speakers annotated the tense of the verbs in the 
articles. Prior to annotating the data, the judges underwent brief training during which 
they were asked to read an example of a Chinese sentence for each tense and make 
sure they understand the examples. During the annotation, the judges were asked to 
read whole articles first and then select a tense tag based on the context of each verb. 
In cases where the judges were unable to decide the tense of a verb, they were in-
structed to tag it as ?unknown?.  
Kappa scores were calculated for the three human judges? annotation results. 
Kappa score is the de facto standard for evaluating inter-judge agreement on tagging 
tasks. It is defined by the following formula (1), where P(A) is the observed agree-
ment among the judges and P(E) is the expected agreement: 
)(1
)()(
EP
EPAPk
?
?
=  (1) 
The annotation was originally carried out on the taxonomy of 13 tenses. We col-
lapsed these 13 tenses into three tenses as discussed in section 3.1. Table 1 summa-
rizes the kappa statistics for the human annotation results after we collapse the tenses: 
Table 1. Kappa Scores for Human Tense Annotation for Xinhua News on Collapsed Tense 
Classes 
 
Xinhua 
news 1 
Xinhua 
news 2 
Xinhua 
news 3 
Xinhua 
news 4 
Kappa score 
for 3 judges 0.409 0.440 0.317 0.325 
892 Y. Ye and Z. Zhang 
There are different interpretations as to what is a good level of agreement and what 
kappa scores are considered low. But generally, a kappa score of lower than 0.40 falls 
into the lower range of agreement4. Even if we consider the meta-linguistic nature of 
the task, the kappa scores we observe belong to the poor-fair range of agreement, il-
lustrating the challenge of temporal reference mapping across Chinese and English. 
The difficulty of tense classification demonstrated by these experiments with human 
judges provides an upper bound on the performance of automatic machine classifica-
tion. As challenging a task as it is, tense generation for English verbs in a Chinese-to-
English Machine Translation system must address this cross-lingual mapping problem 
in order to obtain an accurate translation result. 
6.2   Experimental Setup and Evaluation Metrics 
It is conceivable that the granularity of sequences may matter in learning from data 
with sequential relationship, and in the context of verb tense tagging, it naturally maps 
to the granularity of discourse. Based on this conjecture, we experiment with two dif-
ferent sequential granularities: 
? Sentence-level sequence: each sentence is treated as a sequence; 
? Paragraph-level sequence: each sentence is treated as a sequence, and there is no 
boundary between sentences within the paragraph. 
All results are obtained by 5-fold cross validation. The classifier?s performance is 
evaluated against the tenses from the best-ranked human translation parallel English 
text. 
To evaluate the performance of classifiers, we measure the standard classification 
accuracy where accuracy is defined as in equation (2): 
spredictionofnumbertotal
spredictioncorrectofnumber
accuracy =  (2) 
To measure how well the classifier does on each class respectively, we compute 
precision, recall, and F-measure, which are defined respectively in equation (3), (4) 
and (5): 
hitsofnumbertotal
hitscorrectofnumber
ecision =Pr  (3) 
hitlistperfectofsize
hitscorrectofnumber
call =Re  (4) 
callecision
callecision
measureF
RePr
RePr2
+
??
=?  (5) 
                                                          
4
 http://www.childrens-mercy.org/stats/definitions/kappa.htm 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 893 
6.3   Experimental Results 
The evaluation is carried on the collapsed tense taxonomy that consists of three tense 
classes: present, past and future. This collapse is motivated by two reasons: linguisti-
cally, this collapse reflects the accommodation of the ?gray area? that exists in the 13-
way tense taxonomy; practically, the collapse helps to alleviate the sparse data prob-
lem. Ideally, with a large enough data set that could cover the less-common tenses, 
the full-fledged tense taxonomy is desirable given that the ?gray area? could be ana-
lyzed and included into the evaluation. The CRF-based tense classifier yielded the 
performance in Table 2 and Table 3: 
Table 2. Sentence-level sequence: overall accuracy 58.21% 
 Precision Recall F-measure 
Present 42.50% 27.48% 32.07% 
Past 67.57% 79.55% 72.10% 
Future 29.66% 25.56% 21.56% 
Table 3. Paragraph-level sequence: overall accuracy 58.05% 
 Precision Recall F-measure 
Present 38.79% 32.44% 33.96% 
Past 69.12% 75.72% 71.59% 
Future 33.16% 30.25% 26.59% 
An accuracy of around 60% seems not satisfactory if viewed in isolation, but when 
contrasted with the kappa score of human tense annotating discussed above, the cur-
rent evaluation indicates promising results for our algorithm. Even though the human 
judges underwent only minimal training, their poor-to-fair kappa scores indicate that 
this is a very hard problem. Therefore while there is certainly room for improvement, 
the tagging performance of our algorithm is quite promising. 
It is noticed that the granularity of sequences does not seem to yield significantly 
different performance based on the current data. However, whether this is true in gen-
eral remains an open question. 
7   Discussions 
There are four important dimensions for any natural language processing tasks: 
? The data: ideally, the data used should be as representative as possible of a 
wide range of genres unless the target application is focused on a certain nar-
row domain; 
? The feature space: ideally, the features should be easily available and have 
wide coverage over the predicting space of the target problem; the more so-
894 Y. Ye and Z. Zhang 
phisticated and the more expensive the features are, the less we could claim to 
gain from the learning algorithms. 
? The learning algorithm: nowadays, various machine-learning algorithms have 
been proposed and applied in different natural language task domains. A learn-
ing algorithm should be chosen to appropriately explore the feature space. 
? The evaluation: ideally, evaluation from multiple perspectives is desired to re-
solve disagreements. 
Reflecting upon these dimensions for the current paper, from the data perspective, 
we focused on news report genre where the temporal thread progression is relatively 
simpler than many other genres. When facing temporal reference classification for 
more complicated genres, larger amounts of training data would be necessary for 
learning a more sophisticated classifier. Fortunately, the amount of accessible parallel 
data is growing and it is always possible to obtain the tense tags for the Chinese verbs 
automatically using an off-the-shelf aligning tool although this might introduce a cer-
tain amount of noise.  
As for the choice of the predicting features, the current project does not utilize any 
lexical semantic features owing to the limited lexical semantic knowledge resources 
for Chinese. We expect such knowledge resources, if available, would enhance the 
feature vector and boost the classification performance. Additionally, it is observed 
that for a Chinese-to-English MT system, tense generation in English is significantly 
subject to the syntactic constraints. Hence when integrating into a MT system, the 
current learning algorithm might have opportunity to employ additional features from 
other parts of the system, for example, syntactic features for English could be added 
to the current feature space. 
Regarding the choice of learning algorithm, we chose CRFs, a learning algorithm 
for sequential data, based on the fact that tenses for verbs in a certain discourse unit 
are not independent of each other.  
From the evaluation point of view, the current work evaluates the classifier against 
the tenses from a certain human translation team. The frequent disagreements among 
the human annotators illustrate the difficulty of constructing a gold standard against 
which to evaluate the performance of our classifier. Lastly, measuring BLEU score 
change brought about by integrating the current classifier into a statistical MT system 
would be desirable, such that we can better understand the practical implications of 
this study for MT systems. 
8   Conclusions and Future Work 
The current work has shown how a moderate set of shallow and inexpensive linguistic 
features can be combined with a standard machine learning algorithm for learning a 
tense classifier trained on a moderate number of data points, with promising results. A 
tense resolution module built upon the current framework could enhance a MT system 
with its temporal reference distinction resolution.  
Several issues to be explored in future work are the following: First, our current 
training corpus of Xinhua News articles is rather homogeneous, hence the classifier 
trained exclusively on this data set may not be robust when carried over to data from 
different source. This will become particularly important if we want to integrate the 
 Tense Tagging for Verbs in Cross-Lingual Context: A Case Study 895 
current work into a general-domain MT system. Secondly, related to the homogeneity 
of our training data, we only explored a limited number of features, while the feature 
space could be expanded to include a richer and wider scope. For example, discourse 
structure features have not been explored. Finally, we are very interested in evaluating 
our work against existing MT systems with regard to temporal mapping. 
References 
1. Campbell, R., Aikawa, T., Jiang, Z., Lozano, C., Melero, M and Wu, A.: A Language-
Neutral Representation of Temporal Information. In Proceedings of the Workshop on An-
notation Standards for Tempora Information in Natural Language, LREC 2002, Las Pal-
mas de Gran Canaria, Spain (2002) 13-21.  
2. Pustejovsky, J., Ingria, B., Sauri, R., Castano, J., Littman, J., Gaizauskas, R., Setzer, A., 
Katz, G. and Mani, I.: The Specification Language TimeML. In Mani, I., Pustejovsky, J., 
and Gaizauskas, R (eds.). (2004) The Language of Time: A Reader. Oxford University 
Press, to appear 
3. Mani, I.: "Recent Developments in Temporal Information Extraction (Draft)", In Nicolov, 
N., and Mitkov, R. Proceedings of RANLP'03, John Benjamins, to appear. 
4. Olson, M., Traum, D., Van-ess Dykema, C. and Weinberg, A.: Implicit Cues for Explicit 
Generation: Using Telicity as a Cue for Tense Structure in a Chinese to English MT System, 
in proceedings Machine Translation Summit VIII, Santiago de Compostela (Spain) (2001)  
5. Li, W., Wong, K. F., Hong, C. and Yuan, C.: Applying Machine Learning to Chinese 
Temporal Relation Resolution, Proceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (2004) 582-588 
6. Reichenbach, H.: Elements of Symbolic Logic, The Macmillan Company (1947) 
7. Dorr, B. J. and Gaasterland, T.: "Constraints on the Generation of Tense, Aspect, and Con-
necting Words from Temporal Expressions," Technical Report CS-TR-4391, UMIACS-TR-
2002-71, LAMP-TR-091, University of Maryland, College Park, MD (2002) 
8. Lafferty, J., McCallum, A. and Pereira, F.: Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In Proceedings of ICML-01, (2001) 282-289 
9. Sha, F. and Pereira, F.: Shallow Parsing with Conditional Random Fields, In Proceedings 
of the 2003 Human Language Technology Conference and North American Chapter of the 
Association for Computational Linguistics (HLT/NAACL-03) (2003) 
10. Pinto, D., McCallum, A., Lee, X. and Croft, W. B.: Table Extraction Using Conditional 
Random Fields. In Proceedings of the 26th Annual International ACM SIGIR Conference 
on Research and Development in Information Retrieval (SIGIR 2003) (2003)  
11. McCallum, A. and Li, W.: Early Results for Named Entity Recognition with Conditional 
Random Fields, Feature Induction and Web-Enhanced Lexicons. In Proceedings of the 
Seventh Conference on Natural Language Learning (CoNLL) (2003) 
12. McCallum, A. K.: MALLET: A Machine Learning for Language Toolkit 
http://mallet.cs.umass.edu. (2002) 
