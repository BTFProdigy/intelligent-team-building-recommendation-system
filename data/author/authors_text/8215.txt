Noun Phrase Recognition by System Combination 
Er ik  F .  T jong  K im Sang 
Center for Dutch  Language and Speech  
Univers i ty  of Antwerp  
er ikt@uia,  ua. ac. be 
Abstract  
The performance ofmachine learning algorithms can 
be improved by combining the output of different 
systems. In this paper we apply this idea to the 
recognition of noun phrases. We generate different 
classifiers by using different representations of the 
data. By combining the results with voting tech- 
niques described in (Van Halteren et al, 1998) we 
manage to improve the best reported performances 
on standard ata sets for base noun phrases and ar- 
bitrary noun phrases. 
1 Int roduct ion 
(Van Halteren et al, 1998) and (Brill and Wu, 1998) 
describe a series of successful experiments for im- 
proving the performance of part-of-speech taggers. 
Their results have been obtained by combining the 
output of different aggers with system combination 
techniques uch as majority voting. This approach 
cancels errors that are made by the minority of the 
taggers. With the best voting technique, the com- 
bined results decrease the lowest error rate of the 
component taggers by as much as 19% (Van Hal- 
teren et al, 1998). The fact that combination of 
classifiers leads to improved performance has been 
reported in a large body of machine learning work. 
We would like to know what improvement combi- 
nation techniques would cause in noun phrase recog- 
nition. For this purpose, we apply a single memory- 
based learning technique to data that has been rep- 
resented in different ways. We compare various com- 
bination techniques on a part of the Penn Treebank 
and use the best method on standard ata sets for 
base noun phrase recognition and arbitrary noun 
phrase recognition. 
2 Methods  and exper iments  
In this section we start with a description of our task: 
recognizing noun phrases. After this we introduce 
the different data representations we use and our 
machine learning algorithms. We conclude with an 
outline of techniques for combining classifier esults. 
2.1 Task description 
Noun phrase recognition can be divided in two tasks: 
recognizing base noun phrases and recognizing arbi- 
trary noun phrases. Base noun phrases (baseNPs) 
are noun phrases which do not contain another noun 
phrase. For example, the sentence 
In \[ early trading \] in \[ Hong Kong \] 
\[ Monday \] , \[ gold \] was quoted at 
\[ $ 366.50 \] \[ an ounce \] . 
contains six baseNPs (marked as phrases between 
square brackets). The phrase $ 366.50 an ounce  
is a noun phrase as well. However, it is not a 
baseNP since it contains two other noun phrases. 
Two baseNP data sets have been put forward by 
(Ramshaw and Marcus, 1995). The main data set 
consist of four sections (15-18) of the Wall Street 
Journal (WSJ) part of the Penn Treebank (Marcus 
et al, 1993) as training material and one section 
(20) as test material 1. The baseNPs in this data are 
slightly different from the ones that can be derived 
from the Treebank, most notably in the attachment 
of genitive markers. 
The recognition task involving arbitrary noun 
phrases attempts to find both baseNPs and noun 
phrases that contain other noun phrases. A stan- 
dard data set for this task was put forward at the 
CoNLL-99 workshop. It consist on the same parts 
of the Penn Treebank as the main baseNP data set: 
WSJ sections 15-18 as training data and section 20 
as test data 2. The noun phrases in this data set 
are the same as in the Treebank and therefore the 
baseNPs in this data set are slightly different from 
the ones in the (Ramshaw and Marcus, 1995) data 
sets. 
In both tasks, performance is measured with three 
scores. First, with the percentage of detected noun 
phrases that are correct (precision). Second, with 
the percentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
1This (Ramshaw and Marcus, 1995) baseNP data set is 
available via ftp://ftp.cis.upenn.edu/pub/chunker/ 
2Software for generating the data is available from 
http://lcg-www.uia.ac.be/conl199/npb/ 
50 
with the FZ=I rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate has 
been used as the target for optimization. 
2.2 Data  representat ion  
In our example sentence in section 2.1, noun phrases 
are represented by bracket structures. Both (Mufioz 
et al, 1999) and (Tjong K im Sang and Veenstra, 
1999) have shown how classifiers can process bracket 
structures. One  classifier can be trained to recog- 
nize open brackets (O) while another will process 
close brackets (C). Their results can be converted to 
baseNPs by making pairs of open and close brackets 
with large probability scores (Mufioz et al, 1999) or 
by regarding only the shortest phrases between open 
and close brackets as baseNPs (Tjong K im Sang and 
Veenstra, 1999). We have used the bracket repre- 
sentation (O+C)  in combination with the second 
baseNP construction method. 
An  alternative representation for baseNPs has 
been put forward by (Ramshaw and Marcus, 1995). 
They  have defined baseNP recognition as a tagging 
task: words can be inside a baseNP (1) or outside of 
baseNPs (O). In the case that one baseNP immedi- 
ately follows another baseNP, the first word in the 
second baseNP receives tag B. Example: 
Ino earlyi tradingr ino Hongl Kongz 
MondayB ,o goldz waso quotedo ato $r 
366.50z anB ounce/ -o 
This set of three tags is sufficient for encoding 
baseNP structures ince these structures are non- 
recursive and nonoverlapping. 
(Tjong Kim Sang and Veenstra, 1999) have pre- 
sented three variants of this tagging representation. 
First, the B tag can be used for the first word of 
every noun phrase (IOB2 representation). Second, 
instead of the B tag an E tag can be used to mark the 
last word of a baseNP immediately before another 
baseNP (IOE1). And third, the E tag can be used 
for every noun phrase final word (IOE2). They have 
used the (Ramshaw and Marcus, 1995) representa- 
tion as well (IOB1). We will use these four tagging 
representations a  well as the O+C representation. 
2.3 Mach ine  learn ing a lgor i thms 
We have used the memory-based learning algorithm 
IBI-IG which is part of TiMBL package (Daelemans 
et al, 1999b). In memory-based learning the train- 
ing data is stored and a new item is classified by the 
most frequent classification among training items 
which are closest o this new item. Data items are 
represented as sets of feature-value pairs. In IBI-IG 
each feature receives a weight which is based on the 
amount of information which it provides for com- 
puting the classification of the items in the training 
data. These feature weights are used for computing 
the distance between a pair of data items (Daele- 
mans et al, 1999b). ml-IG has been used success- 
fully on a large variety of natural anguage process- 
ing tasks. 
Beside IBI - IG,  we have used IGTREE in the combi- 
nation experiments. IGTREE is a decision tree vari- 
ant of II31-IG (Daelemans et al, 1999b). It uses the 
same feature weight method as IBI-IG. Data items 
are stored in a tree with the most important features 
close to the root node. A new item is classified by 
traveling down from the root node until a leaf node 
is reached or no branch is available for the current 
feature value. The most frequent classification of the 
current node will be chosen. 
2.4 Combinat ion  techn iques  
Our experiments will result in different classifica- 
tions of the data and we need to find out how to 
combine these. For this purpose we have evaluated 
different voting mechanisms, effectively the voting 
methods as described in (Van Halteren et al, 1998). 
All combination methods assign some weight to the 
results of the individual classifier. For each input to- 
ken, they pick the classification score with the high- 
est total score. For example, if five classifiers have 
weights 0.9, 0.4, 0.8, 0.6 and 0.6 respectively and 
they classify some token as npstart, null, npstart, 
null and null, then the combination method will pick 
npstart since it has a higher total score (1.7) than 
null (1.6). The values of the weights are usually es- 
timated by processing a part of the training data, 
the tuning data, which has been kept separate as 
training data for the combination process. 
In the first voting method, each of the five classi- 
tiers receives the same weight (majority). The sec- 
ond method regards as the weight of each individual 
classification algorithm its accuracy on the tuning 
data (TotPrecision). The third voting method com- 
putes the precision of each assigned tag per classifier 
and uses this value as a weight for the classifier in 
those cases that it chooses the tag (TagPrecision). 
The fourth method uses the tag precision weights 
as well but it subtracts from them the recall val- 
ues of the competing classifier esults. Finally, the 
fifth method uses not only a weight for the current 
classification but it also computes weights for other 
possible classifications. The other classifications are 
determined by examining the tuning data and reg- 
istering the correct values for every pair of classifier 
results (pair-wise voting). 
Apart from these five voting methods we have also 
processed the output streams with two classifiers: 
IBI-IG (memory-based) and IGTREE (decision tree). 
This approach is called classifier stacking. Like (Van 
Halteren et al, 1998), we have used different input 
versions: one containing only the classifier output 
and another containing both classifier output and 
a compressed representation f the classifier input. 
51 
train 
All correct 
Majority correct 
Minority correct 
All wrong 
0 C 
96.21% 96.66% 
1.98% 1.64% 
0.88% 0.75% 
0.93% 0.95% 
Table 1: Token classification agreement between the 
five classifiers applied to the baseNP training data 
after conversion to the open bracket (O) and the 
close bracket representation (C). 
For the latter purpose we have used the part-of- 
speech tag of the current word. 
3 Resu l t s  
Our first goal was to find out whether system combi- 
nation could improve performance of baseNP recog- 
nition and, if this was the fact, to select the best 
combination technique. For this purpose we per- 
formed a 10-fold cross validation experiment on the 
baseNP training data, sections 15-18 of the WSJ 
part of the Penn Treebank (211727 tokens). Like 
the data used by (Ramshaw and Marcus, 1995), 
this data was retagged by the Brill tagger in or- 
der to obtain realistic part-of-speech (POS) tags 3. 
The data was segmented into baseNP parts and non- 
baseNP parts in a similar fashion as the data used 
by (Ramshaw and Marcus, 1995). 
The data was converted to the five data represen- 
tations (IOB1, IOB2, IOE1, IOE2 and O+C) and 
IBI-IG was used to classify it by using 10-fold cross 
validation. This means that the data was divided 
in ten consecutive parts of about the same size af- 
ter which each part was used as test data with the 
other nine parts as training data. The standard pa- 
rameters of IBI-IG have been used except for k, the 
number of examined nearest neighbors, which was 
set to three. Each word in the data was represented 
by itself and its POS tag and additionally a left and 
right context of four word-POS tag pairs. For the 
first four representations, wehave used a second pro- 
cessing stage as well. In this stage, a word was repre- 
sented by itself, its POS tag, a left and right context 
of three word-POS tag pairs and a left and right 
context of two classification results of the first pro- 
cessing stage (see figure 1). The second processing 
stage improved the FZ=I scores with almost 0.7 on 
average. 
The classifications of the IOB1, IOB2, IOE1 and 
IOE2 representations were converted to the open 
bracket (O) and close bracket (C) representations. 
aNo perfect Penn Treebank POS tags will be available for 
novel texts. If we would have used the Treebank POS tags 
for NP recognition, our performance rates would have been 
unrealistically high. 
train 
Representation 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
98.01% 
97.8O% 
97.97% 
97.89% 
97.92% 
98.19% 
98.19% 
98.19% 
98.19% 
C 
98.14% 
98.08% 
98.04% 
98.08% 
98.13% 
98.30% 
98.30% 
98.30% 
98.30% 
Pairwise Voting 
TagPair 98.19% 98.30% 
Memory-Based  
Tags 98.19% 98.34% 
Tags + POS 98.19% 98.35% 
Decision Trees 
Tags 98.17% 98.34% 
Tags + POS 98.17% 98.34% 
Table 2: Open and close bracket accuracies for the 
baseNP training data (211727 tokens). Each com- 
bination performs significantly better than any of 
the five individual classifiers listed under Represen- 
tation. The performance differences between the 
combination methods are not significant. 
After this conversion step we had five O results and 
five C results. In the bracket representations, to- 
kens can be classified as either being the first token 
of an NP (or the last in the C representation) or not. 
The results obtained with these representations have 
been measured with accuracy rates: the percentage 
of tokens that were classified correctly. Only about 
one in four tokens are at a baseNP boundary so 
guessing that a text does not contains baseNPs will 
already give us an accuracy of 75%. Therefore the 
accuracy rates obtained with these representations 
are high and the room for improvement is small (see 
table 1). However, because of the different treatment 
of neighboring chunks, the five classifiers disagree in 
about 2.5% of the classifications. It seems useful to 
use combination methods for finding the best classi- 
fication for those ambiguous cases. 
The five O results and the five C results were pro- 
cessed by the combination techniques described in 
section 2.4. The accuracies per input token for the 
combinations can be found in table 2. For both 
data representations, all combinations perform sig- 
nificantly better than the best individual classifier 
(p<0.001 according to a X 2 test) 4. Unlike in (Van 
4We have performed significance computat ions  on the 
bracket accuracy rates because we have been unable to find 
a satisfactory method for comput ing significance scores for 
52 
trading/NN in/IN Hong/NNP Kong/NNP Monday/NNP ,/, gold/NN was/VBD quoted/VBN 
in/IN Hong/NNP/I Kong/NNP/I Monday/NNP ,/,/O gold/NN/I was/VBD 
Figure 1: Example of the classifier input features used for classifying Monday in the example sentence. The 
first processing stage (top) contains a word and POS context of four left and four right while the second 
processing stage (bottom) contains a word and POS context of three and a chunk tag context of two. 
section 20 
Majority voting 
(Mufioz et al, 1999) 
(Tjong Kim Sang and Veenstra~ 1999) 
(Ramshaw and Marcus, 1995) 
(Argarnon et al, 1998) 
accuracy precision 
O:98.10% C:98.29% 93.63% 
O:98.1% C:98.2% 93.1% 
97.58% 92.50% 
97.37% 91.80% 
91.6% 
recall FZ=I 
92.89% 93.26 
92.4% 92.8 
92.25% 92.37 
92.27% 92.03 
91.6% 91.6 
section 00 accuracy precision 
Majority voting 0:98.59% C:98.65% 95.04% 
r (Tjong Kim Sang and Veenstra, 1999) 98.04% 93.71% 
(Ramshaw and Marcus, 1995) 97.8% 93.1% 
recall FB=I 
94.75% 94.90 
93.90% 93.81 
93.5% 93.3 
Table 3: The results of majority voting of different data representations applied to the two standard ata 
sets put forward by (Ramshaw and Marcus, 1995) compared with earlier work. The accuracy scores indicate 
how often a word was classified correctly with the representation used (O, C or IOB1). The training data 
for WSJ section 20 contained 211727 tokens while section 00 was processed with 950028 tokens of training 
data. Majority voting outperforms all earlier eported results for the two data sets. 
Halteren et al, 1998), the best voting technique did 
not outperform the best stacked classifier. Further- 
more the performance differences between the com- 
bination methods are not significant (p>0.05). To 
our surprise the five voting techniques performed the 
same. We assume that this has happened because 
the accuracies of the individual classifiers do not dif- 
fer much and because the classification i volves a 
binary choice. 
Since there is no significant difference between the 
combination methods, we can use any of them in the 
remaining experiments. We have chosen to use ma- 
jority voting because it does not require tuning data. 
We have applied it to the two data sets mentioned 
in (Ramshaw and Marcus, 1995). The first data set 
uses WSJ sections 15-18 as training data (211727 
tokens) and section 20 as test data (47377 tokens). 
The second one uses sections 02-21 of the same cor- 
pus as training data (950028 tokens) and section 00 
as test data (46451 tokens). All data sets were pro- 
cessed in the same way as described earlier. The 
results of these experiments can be found in table 3. 
With section 20 as test set, we managed to reduce 
the error of the best result known to us with 6% with 
the error rate dropping from 7.2% to 6.74%, and for 
section 00 this difference was almost 18% with the 
FB= 1 rates. 
error rate dropping from 6.19% to 5.10% (see table 
3). 
We have also applied majority voting to the NP 
data set put forward on the CoNLL-99 workshop. 
In this task the goal is to recognize all NPs. We 
have approached this as repeated baseNP recogni- 
tion. A first stage detects the baseNPs. The recog- 
nized NPs are replaced by their presumed head word 
with a special POS tag and the result is send to a 
second stage which recognizes NPs with one level of 
embedding. The output of this stage is sent to a 
third stage and this stage finds NPs with two levels 
of embedding and so on. 
In the first processing stage we have used the five 
data representations with majority voting. This ap- 
proach did not work as well for other stages. The 
O+C representation utperformed the other four 
representations by a large margin for the valida- 
tion data 5. This caused the combined output of 
all five representations being worse than the O+C 
result. Therefore we have only used the O+C repre- 
sentation for recognizing nombaseNPs. The overall 
system reached an F~=I score of 83.79 and this is 
slightly better than the best rate reported at the 
5The validation data  is the test set we have used for esti- 
mat ing  the best parameters  for the CoNLL experiment: WSJ  
section 21. 
53
CoNLL-99 workshop (82.98 (CoNLL-99, 1999), an 
error reduction of 5%). 
4 Re la ted  work  
(Abney, 1991) has proposed to approach parsing by 
starting with finding correlated chunks of words. 
The chunks can be combined to trees by a sec- 
ond processing stage, the attacher. (Ramshaw 
and Marcus, 1995) have build a chunker by apply- 
ing transformation-based learning to sections of the 
Penn Treebank. Rather than working with bracket 
structures, they have represented the chunking task 
as a tagging problem. POS-like tags were used to 
account for the fact that words were inside or out- 
side chunks. They have applied their method to two 
segments of the Penn Treebank and these are still 
being used as benchmark data sets. 
Several groups have continued working with the 
Ramshaw and Marcus data sets for base noun 
phrases. (Argamon et al, 1998) use Memory-Based 
Sequence Learning for recognizing both NP chunks 
and VP chunks. This method records POS tag se- 
quences which contain chunk boundaries and uses 
these sequences to classify the test data. Its per- 
formance is somewhat worse than that of Ramshaw 
and Marcus (F~=1=91.6 vs. 92.0) but it is the best 
result obtained without using lexical information 6. 
(Cardie and Pierce, 1998) store POS tag sequences 
that make up complete chunks and use these se- 
quences as rules for classifying unseen data. This 
approach performs worse than the method of Arga- 
mon et al (F~=1=90.9). 
Three papers mention having used the memory- 
based learning method IBI-IG. (Veenstra, 1998) in- 
troduced cascaded chunking, a two-stage process in 
which the first stage classifications are used to im- 
prove the performance in a second processing stage. 
This approach reaches the same performance l vel 
as Argamon et al but it requires lexical informa- 
tion. (Daelemans et al, 1999a) report a good per- 
formance for baseNP recognition but they use a dif- 
ferent data set and do not mention precision and 
recall rates. (Tjong Kim Sang and Veenstra, 1999) 
compare different data representations forthis task. 
Their baseNP results are slightly better than those 
of Ramshaw and Marcus (F~=1=92.37). 
(XTAG, 1998) describes a baseNP chunker built 
from training data by a technique called supertag- 
ging. The performance of the chunker was an 
improvement of the Ramshaw and Marcus results 
(Fz=I =92.4). (Mufioz et al, 1999) use SNOW, a net- 
work of linear units, for recognizing baseNP phrases 
6We have applied majority voting of five data represen- 
tations to the Ramshaw and Marcus data set without using 
lexical information and the results were: accuracy O: 97.60%, 
accuracy C: 98.10%, precision: 92.19%, recall: 91.53% and 
F~=I: 91.86. 
and SV phrases. They compare two data representa- 
tions and report that a representation with bracket 
structures outperforms the IOB tagging representa- 
tion introduced by (Ramshaw and Marcus, 1995). 
SNoW reaches the best performance on this task 
(Fz=I =92.8). 
There has been less work on identifying eneral 
noun phrases than on recognizing baseNPs. (Os- 
borne, 1999) extended a definite clause grammar 
with rules induced by a learner that was based upon 
the maximum description length principle. He pro- 
cessed other parts of the Penn Treebank than we 
with an F~=I rate of about 60. Our earlier effort 
to process the CoNLL data set was performed in 
the same way as described in this paper but with- 
out using the combination method for baseNPs. We 
obtained an F~=I rate of 82.98 (CoNLL-99, 1999). 
5 Conc lud ing  remarks  
We have put forward a method for recognizing noun 
phrases by combining the results of a memory-based 
classifier applied to different representations of the 
data. We have examined ifferent combination tech- 
niques and each of them performed significantly bet- 
ter than the best individual classifier. We have cho- 
sen to work with majority voting because it does 
not require tuning data and thus enables the indi- 
vidual classifiers to use all the training data. This 
approach was applied to three standard ata sets 
for base noun phrase recognition and arbitrary noun 
phrase recognition. For all data sets majority voting 
improved the best result for that data set known to 
US. 
Varying data representations is not the only way 
for generating different classifiers for combination 
purposes. We have also tried dividing the training 
data in partitions (bagging) and working with artifi- 
cial training data generated by a crossover-like oper- 
ator borrowed from genetic algorithm theory. With 
our memory-based classifier applied to this data, we 
have been unable to generate a combination which 
improved the performance of its best member. An- 
other approach would be to use different classifica- 
tion algorithms and combine the results. We are 
working on this but we are still to overcome the prac- 
tical problems which prevent us from obtaining ac- 
ceptable results with the other learning algorithms. 
Acknowledgements  
We would like to thank the members of the CNTS 
group in Antwerp, Belgium, the members of the ILK 
group in Tilburg, The Netherlands and three anony- 
mous reviewers for valuable discussions and com- 
ments. This research was funded by the European 
TMR network Learning Computational Grammars ~.
7 http://lcg-www.uia.ac.be/ 
55. 
References 
Steven Abney. 1991. Parsing by chunks. In Principle- 
Based Parsing. Kluwer Academic Publishers. 
Shlomo Argamon, Ido Dagan, and Yuval Krymolowski. 
1998. A memory-based approach to learning shal- 
low natural language patterns. In Proceedings of 
COLING-ACL '98. Association for Computational 
Linguistics. 
Eric Brill and Jun Wu. 1998. Classifier combination 
for improved lexical disambiguation. In Proceedings 
of COLING-ACL '98. Association for Computational 
Linguistics. 
Claire Cardie and David Pierce. 1998. Error-driven 
pruning of treebank grammars for base noun phrase 
identification. In Proceedings of COLING-ACL '98. 
Association for Computational Linguistics. 
CoNLL-99. 1999. Conll-99 home page. http://lcg- 
www.uia.ac.be/conl199/. 
Walter Daelemans, Antal van den Bosch, and Jakub Za- 
vrel. 1999a. Forgetting exceptions i harmful in lan- 
guage learning. Machine Learning, 34. 
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and 
Antal van den Bosch. 1999b. TiMBL: Tilburg Mem- 
ory Based Learner, version 2.0, Reference Guide. 
ILK Technical Report 99-01. http://ilk.kub.nl/-ilk/ 
papers/ilk9901.ps.gz. 
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a large annotated cor- 
pus of English: the Penn Treebank. Computational 
Linguistics, 19. 
Marcia Mufioz, Vasin Punyakanok, Dan Roth, and Day 
Zimak. 1999. A learning approach to shallow parsing. 
In Proceedings of EMNLP-WVLC'99. Association for 
Computational Linguistics. 
Miles Osborne. 1999. MDL-based DCG induction 
for NP identification. In Miles Osborne and Erik 
Tjong Kim Sang, editors, CoNLL-99 Computational 
Natural Language Learning. Association for Compu- 
tational Linguistics. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text chunking using transformation-based l arning. 
In Proceedings of the Third ACL Workshop on Very 
Large Corpora. Association for Computational Lin- 
guistics. 
Erik F. Tjong Kim Sang and Jorn Veenstra. 1999. Rep- 
resenting text chunks. In Proceedings of EACL '99. As- 
sociation for Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter Daele- 
mans. 1998. Improving data driven wordclass tagging 
by system combination. In Proceedings of COLING- 
A CL'98. Association for Computational Linguistics. 
Jorn Veenstra. 1998. Fast NP chunking using memory- 
based learning techniques. In BENELEARN-98: Pro- 
ceedings of the Eighth Belgian-Dutch Conference on 
Machine Learning. ATO-DLO, Wageningen, report 
352. 
The XTAG Research Group. 1998. A Lexicalized 
Tree Adjoining Grammar for English. IRCS Tech 
Report 98-18, University of Pennsylvania. (also 
cs.CL/9809024). 
55
Applying System Combinat ion to Base Noun Phrase Identif ication 
Er ik  F. T jong  K im Sang",  Wal ter  Dae lemans  '~, Herv6  D6 jean  ~, 
Rob  Koel ingT,  Yuva l  Krymolowski /~,  Vas in  Punyakanok  '~, Dan Roth" 
~University of Antwert) 
Uifiversiteitsplohl 1 
13.-261.0 Wilri jk, Belgium 
{erikt,daelem}@uia.ua.ac.be 
r Unive.rsitiil; Tii l) ingen 
Kleine Wilhehnstrat./e 113 
I)-72074 T/il)ingen, Germany 
(lejean((~sl:q, ni)hil.ulfi-l;uebingen.de, 
7S1{,I Cambridge 
23 Millers Yard,Mil l  Lane 
Cambridge, CB2 ll{Q, UK 
koeling@caln.sri.coIn 
;~Bal'-Ilan University 
lbunat  Gan, 52900, Israel 
yuwdk(c~)macs. 1)iu.ac.il 
"University of Illinois 
1304: W. Sl)ringfield Ave. 
Url)ana, IL 61801, USA 
{lmnyakan,(lanr} ((~cs.uiuc.edu 
A1)s t rac t  
We us('. seven machine h;arning algorithms tbr 
one task: idenl;it~ying l)ase holm phrases. The 
results have 1)een t)rocessed by ditt'erent system 
combination methods and all of these (mtt)er- 
formed the t)est individual result. We have ap- 
t)lied the seven learners with the best (:omt)ina- 
tot, a majority vote of the top tive systenls, to a 
standard (lata set and lllallage(1 I;O ilnl)rov(', 1;11(' 
t)est pul)lished result %r this (lata set. 
1 In t roduct ion  
Van Haltor(m eta\ ] .  (1998) and Brill and Wu 
(1998) show that part-ofst)ee(:h tagger l)erfor- 
mance can 1)e iml)roved 1)y (:oml)ining ditl'erent 
tatters. By using te(:hni(tues su(:h as majority 
voting, errors made l)y 1;11(; minority of the tag- 
gers can 1)e r(;moved. Van Ilaltere, n et al (1998) 
rel)ort that the results of such a ('oml)ined al)- 
proach can improve ll\])Oll the aCcllracy error of 
the best individual system with as much as 19%. 
Tim positive (;tl'e(:t of system combination tbr 
non-language t)ro(:essing tasks has t)een shown 
in a large l)o(ly of mac\]fine l arning work. 
In this 1)aper we will use system (:omt)ination 
for identifying base noun 1)hrases (1)aseNt)s). 
W(; will at)l)ly seven machine learning algo- 
rithms to the same 1)aseNP task. At two l)oints 
we will al)ply confl)ination methods. We will 
start with making the systems process five out- 
trot representations and combine the l'esults t)y 
(:hoosing the majority of the outtmt tL'atures. 
Three of the seven systems use this al)l)roaeh. 
Afl, er this w(; will make an overall eoml)ination 
of the results of the seven systems. There we 
will evaluate several system combination meth- 
()(Is. The 1)est l)erforming method will 1)e at)- 
t)lied to a standard ata set tbr baseNP identi- 
tication. 
2 Methods  and exper iments  
in this se(:tion we will describe our lem:ning task: 
recognizing 1)ase noun phrases. After this we 
will (tes(:ril)e the data representations we used 
and the ma('hine learning algorithms that we 
will at)l)ly to the task. We will con(:ludc with 
an overview of the (:ombination metllo(ls that 
we will test. 
2.1 Task descript ion 
Base noun \])hrases (1)aseNPs) are n(mn phrases 
whi(:h do not (:ontain another noun l)hrase. \]?or 
cxamt)le , the sentence 
In \[early trading\] in \[ IIong Kong\] 
\[ Mo,l,tay \], \[ g,,la \] was q, loted at 
\[ $ 366. 0 \] \ [a .  o1,,,.(; \] .  
contains six baseN1)s (marked as phrases be- 
tween square 1)rackets). The phrase $ 266.50  
an  ounce  ix a holm phrase as well. However, it 
is not a baseNP since it contains two other noun 
phrases. Two baseNP data sets haw.' been put 
forward by Ramshaw and Marcus (1995). The 
main data set consist of tbur sections of the Wall 
Street Journal (WSJ) part of the Penn Tree- 
bank (Marcus et al, 1.993) as training mate- 
rial (sections 15-18, 211727 tokens) and one sec- 
tion aS test material (section 20, 47377 tokens)5. 
The data contains words, their part-of-speech 
1This Ramshaw and Marcus (1995) bascNP data set 
is availal)le via ffp://fti).cis.upe,m.edu/pub/chunker/ 
857 
(POS) tags as computed by the Brill tagger and 
their baseNP segmentation asderived from the 
%'eebank (with some modifications). 
In the baseNP identitication task, perfor- 
mance is measured with three rates. First, 
with the percentage of detected noun phrases 
that are correct (precision). Second, with the 
1)ercentage of noun phrases in the data that 
were found by the classifier (recall). And third, 
with the F#=~ rate which is equal to (2*preci- 
sion*recall)/(precision+recall). The latter rate 
has been used as the target for optimization. 
2.2 Data representat ion 
In our example sentence in section 2.1, noun 
phrases are represented by bracket structures. 
It has been shown by Mufioz et al (1999) 
that for baseNP recognition, the representa- 
tion with brackets outperforms other data rep- 
resentations. One classifier can be trained to 
recognize open brackets (O) and another can 
handle close brackets (C). Their results can be 
combined by making pairs of open and close 
brackets with large probability scores. We have 
used this bracket representation (O+C) as well. 
However, we have not used the combination 
strategy from Mufioz et al (1999) trot in- 
stead used the strategy outlined in Tjong Kim 
Sang (2000): regard only the shortest possi- 
ble phrases between candidate open and close 
brackets as base noun phrases. 
An alternative representation for baseNPs 
has been put tbrward by Ramshaw and Mar- 
cus (1995). They have defined baseNP recog- 
nition as a tagging task: words can be inside a 
baseNP (I) or outside a baseNP (O). In the case 
that one baseNP immediately follows another 
baseNP, the first word in the second baseNP 
receives tag B. Example: 
Ino early1 trading1 ino Hongi Kongi 
MondayB ,o gold1 waso quotedo ato 
$I 366.501 anu ounce1 .o 
This set of three tags is sufficient for encod- 
ing baseNP structures since these structures are 
nonrecursive and nonoverlapping. 
Tjong Kiln Sang (2000) outlines alternative 
versions of this tagging representation. First, 
the B tag can be used for tile first word of ev- 
ery baseNP (IOB2 representation). Second, in- 
stead of the B tag an E tag can be used to 
nlark the last word of a baseNP immediately 
before another baseNP (IOE1). And third, the 
E tag call be used for every noun phrase final 
word (IOE2). He used the Ramshaw and Mar- 
cus (1995) representation as well (IOB1). We 
will use these tbur tagging representations and 
the O+C representation for the system-internal 
combination experiments. 
2.a Machine learning algorithms 
This section contains a brief description of tile 
seven machine learning algorithms that we will 
apply to the baseNP identification task: AL- 
LiS, c5.0, IO~?ee, MaxEnt, MBL, MBSL and 
SNOW. 
ALLiS 2 (Architecture for Learning Linguistic 
Structures) is a learning system which uses the- 
ory refinement in order to learn non-recursive 
NP and VP structures (Ddjean, 2000). ALLiS 
generates a regular expression grammar which 
describes the phrase structure (NP or VP). This 
grammar is then used by the CASS parser (Ab- 
hey, 1996). Following the principle of theory re- 
finement, tile learning task is composed of two 
steps. The first step is the generation of an 
initial wa, mmar. The generation of this grmn- 
mar uses the notion of default values and some 
background knowledge which provides general 
expectations concerning the immr structure of 
NPs and VPs. This initial grammar provides 
an incomplete and/or incorrect analysis of tile 
data. The second step is the refinement of this 
grammar. During this step, the validity of the 
rules of the initial grammar is checked and the 
rules are improved (refined) if necessary. This 
refinement relies on the use of two operations: 
the contextualization (i which contexts uch a 
tag always belongs to the phrase) and lexical- 
ization (use of information about the words and 
not only about POS). 
05.0 a, a commercial version of 04.5 (Quin- 
lan, 1993), performs top-do,vn induction of de- 
cision trees (TDIDT). O,1 the basis of an in- 
stance base of examples, 05.0 constructs a deci- 
sion tree which compresses the classification i - 
formation in the instance base by exploiting dif- 
tbrences in relative importance of different fea- 
tures. Instances are stored in the tree as paths 
2A demo f the NP and VP ctmnker is available at 
ht;t:p: / /www.sfb441.unituebingen.de/~ dej an/chunker.h 
tml 
aAvailable fl'om http://www.rulequest.com 
858 
of commcted nodes ending in leaves which con- 
tain classification information. Nodes are con- 
nected via arcs denoting feature wflues. Feature 
inff)rmation gain (nmt;ual inforniation 1)etween 
features and class) is used to determine the or- 
der in which features are mnt)loyed as tests at all 
levels of the tree (Quinlan, 1993), With the full 
inlmt representation (words and POS tags)~ we 
were not able to run comt)lete xperiments. We 
therefore xperimented only with the POS tags 
(with a context of two left; and right). We have 
used the default parameter setting with decision 
trees coml)ined with wflue groul)ing. 
We have used a nearest neighbor algoritlml 
(IBI.-1G, here listed as MBL) and a decision tree 
algoritlmi (llG\[lh:ee) from the TiMBL learning 
package (Da(flmnans et al, 19991)). Both algo- 
rithms store the training data and ('lassi(y new 
it;eros by choosing the most frequent (:lassiti(:a- 
lion among training items which are closest to 
this new item. l)ata it(uns rare rel)resented as 
sets of thature-vahu; 1)airs. Each ti;ature recc'ives 
a weight which is t)ased on the amount of in- 
formation whi(:h it t/rovides fi)r comtmting the 
classification of t;t1(; items in the training data. 
IBI-IG uses these weights tbr comt)uting the dis- 
lance l)etween a t)air of data items and IGTree 
uses them fi)r deciding which feature-value de- 
cisions shouM t)e made in the top nod(;s of the 
decision tree (l)a(;lenJans et al, 19991)). We 
will use their det, mlt pm:amet('a:s excel)t for the 
IBI-IG t)arameter for the numl)er of exmnine(t 
m',arest n(,ighl)ors (k) whi('h we h~ve s(,t to 3 
(Daelemans et al, 1999a). The classifiers use a 
left and right context of four words and part- 
ofsl)eech tags. t~i)r |;lie four IO representations 
we have used a second i)rocessing stage which 
used a smaller context lint which included in- 
formation at)out the IO tags 1)redicted by the 
first processing phase (Tjong Kim Sang, 2000). 
When /)uilding a classifier, one must gather 
evidence ti)r predicting the correct class of an 
item from its context. The Maxinmm Entropy 
(MaxEnt) fl:mnework is especially suited tbr 
integrating evidence tiom various inti)rmal;ion 
sources. Frequencies of evidence/class combi~ 
nations (called features) are extracted fl'om a 
sample corlms and considere(t to be t)roperties 
of the classification process. Attention is con- 
strained to models with these l)roperties. The 
MaxEnt t)rinciph; now demands that among all 
1;11(; 1)robability distributions that obey these 
constraints, the most mfiform is chosen, l)ur- 
ing training, features are assigned weights in 
such a way that, given the MaxEnt principle, 
the training data is matched as well as possible. 
During evaluation it is tested which features are 
active (i.e. a feature is active when the context 
meets the requirements given by t;11(', feature). 
For every class the weights of the active fea- 
tures are combined and the best scoring class 
is chosen (Berger et al, 1996). D)r the classi- 
tier built here the surromlding words, their POS 
tags and lmseNP tags predicted for the previous 
words are used its evidence. A mixture of simple 
features (consisting of one of the mentioned in- 
formation sources) and complex features (com- 
binations thereof) were used. The left context 
never exceeded 3 words, the right context was 
maximally 2 words. The model wits (:ah:ulated 
using existing software (l)ehaspe, 1997). 
MBSL (Argalnon et al, 1999) uses POS data 
in order to identit~y t/aseNPs, hfferenee re- 
lies on a memory which contains all the o(:- 
cm:rences of P()S sequences which apt)ear in 
the t)egimfing, or the end, of a 1)aseNl? (in- 
(:hiding complete t)hrases). These sequences 
may include a thw context tags, up to a 1)re- 
st)ecifi('d max_(:ont<~:t. \])uring inti',rence, MBSL 
tries to 'tile' each POS string with parts of 
noun-l)hrases from l;he memory. If the string 
coul(1 l)e fully covered t)y the tiles, il; becomes 
l)art of a (:andidate list, anfl)iguities 1)etween 
candidates are resolved by a constraint )ropa- 
gation algorithm. Adding a (:ontext extends the 
possil)ilities for tiling, thereby giving more op- 
portunities to 1)etter candidates. The at)t)roaeh 
of MBSL to the i)rot)lem of identifying 1)aseNPs 
is sequence-1)ased rather than word-based, that 
is, decisions are taken per POS sequence, or per 
candidate, trot not for a single word. In addi- 
tion, the tiling l)rocess gives no preference to 
any (tirection in the sentence. The tiles may 1)e 
of any length, up to the maximal ength of a 
1)hrase in the training (ILl;L, which gives MBSL 
a generalization power that compensates for the 
setup of using only POS tags. The results t)re- 
seated here were obtained by optimizing MBSL 
parameters based on 5-fold CV on the training 
data. 
SNoW uses the Open/Close model, described 
in Mufioz et al (1999). As is shown there, this 
859 
section 21 
IOB1 
IOB2 
IOE1 
IOE2 
O+C 
0 
97.81% 
97.63% 
97.80% 
97.72% 
97.72% 
MBL 
Majority 98.04% 98.20% 
C Ffl=l 
97.97% 91.68 
97.96% 91.79 
97.92% 91.54 
97.94% 92.06 
98.04% 92.03 
92.82 
MaxEnt 
O C 
97.90% 98.11% 
97.81% 98.14% 
97.88% 98.12% 
97.84% 98.12% 
97.82% 98.15% 
97.94% 98.24% 
Ffl=l 
92.43 
92.14 
92.37 
92.13 
92.26 
92.60 
IGTree 
O C 
96.62% 96.89% 
97.27% 97.30% 
95.88% 96.01% 
97.19% 97.62% 
96.89% 97.49% 
97.70% 97.99% 
F\[~=1 
87.88 
90.03 
82.80 
89.98 
89.37 
91.92 
Table 1: The effects of system-internal combination by using different output representations. A 
straight-forward majority vote of the output yields better bracket accuracies and Ffl=l rates than 
any included individual classifier. The bracket accuracies in the cohmms O and C show what 
percentage of words was correctly classified as baseNP start, baseNP end or neither. 
model produced better results than the other 
paradigm evaluated there, the Inside/Outside 
paradigm. The Open/Close model consists of 
two SNoW predictors, one of which predicts the 
beginning of baseNPs (Open predictor), and the 
other predicts the end of the ptlrase (Close pre- 
dictor). The Open predictor is learned using 
SNoW (Carlson el; al., 1999; Roth, 1998) as a 
flmction of features that utilize words and POS 
tags in the sentence and, given a new sentence, 
will predict for each word whether it is the first 
word in the phrase or not. For each Open, the 
Close predictor is learned using SNoW as a func- 
tion of features that utilize the words ill the sen- 
tence, the POS tags and the open prediction. It 
will predict, tbr each word, whether it Call be 
the end of" the I)hrase, given the previously pre- 
dicted Open. Each pair of predicted Open mid 
Close forms a candidate of a baseNP. These can- 
didates may conflict due to overlapping; at this 
stage, a graph-based constraint satisfaction al- 
gorithm that uses the confidence values SNoW 
associates with its predictions i elnployed. This 
algorithln ("the combinator') produces tile list 
of" the final baseNPs fbr each sentence. Details 
of SNOW, its application in shallow parsing and 
the combinator% Mgorithm are in Mufioz et al 
(1999). 
2.4 Combinat ion techniques 
At two points in our noun phrase recognition 
process we will use system combination. We will 
start with system-internal combination: apply 
the same learning algorithm to variants of the 
task and combine the results. The approach 
we have chosen here is the same as in Tjong 
Kim Sang (2000): generate different variants 
of the task by using different representations 
of the output (IOB1, IOB2, IOE1, IOE2 and 
O+C). The five outputs will converted to the 
open bracket representation (O) and the close 
bracket; representation (C) and M'ter this, tile 
most frequent of the five analyses of each word 
will chosen (inajority voting, see below). We 
expect the systems which use this combination 
phase to perform better than their individuM 
members (Tjong Kim Sang, 2000). 
Our seven learners will generate different clas- 
sifications of tile training data and we need to 
find out which combination techniques are most 
appropriate. For the system-external combi- 
nation experiment, we have evaluated itfi;rent 
voting lllechanisms~ effectively the voting meth- 
ods as described in Van Halteren et al (1998). 
In the first method each classification receives 
the same weight and the most frequent classifi- 
cation is chosen (Majority). The second nmthod 
regards as tile weight of each individual clas- 
sification algorithm its accuracy on solne part 
of the data, tile tuning data (TotPrecision). 
The third voting method computes the preci- 
sion of each assigned tag per classifer and uses 
this value as a weight for tile classifier in those 
cases that it chooses the tag (TagPrecision). 
The fourth method uses both the precision of 
each assigned tag and tile recall of the com- 
peting tags (Precision-Recall). Finally, tile fifth 
lnethod uses not only a weight for tile current 
classification but it also computes weights tbr 
other possible classifications. The other classi- 
fications are deternfined by exalnining the tun- 
860 
ing data and registering the correct wflues for 
(;very pair of classitier esults (pair-wise voting, 
see Van Halteren et al (1998) tbr an elaborate 
explanation). 
Apart from these five voting methods we have 
also processed the output streams with two clas- 
sifters: MBL and IG%'ee. This approach is 
called classifier stacking. Like Van Halteren et 
al. (1998), we have used diff'erent intmt ver- 
sions: olle containing only the classitier Otltl)ut 
and another containing both classifier outlmt 
and a compressed representation of the data 
item tamer consideration. \]?or the latter lmr- 
pose we have used the part-of-speech tag of the 
carrent word. 
3 Resul ts  4 
We want to find out whether system combi- 
nation could improve performmlce of baseNP 
recognition and, if this is the fact, we want to 
seJect the best confl)ination technique. For this 
lmrpose we have pertbrmed an experiment with 
sections 15-18 of the WSJ part of the Prom %'ee- 
bank as training data (211727 tokens) and sec- 
tion 21 as test data (40039 tokens). Like the 
data used by Ramshaw and Marcus (1995), this 
data was retagged by the Brill tagger in order 
to obtain realistic part-of  speech (POS) tags 5. 
The data was seglnente.d into baseNP parts and 
non-lmseNP t)arts ill a similar fitshion as the 
data used 1)y Ramshaw and Marcus (1995). Of 
the training data, only 90% was used for train- 
ing. The remaining 10% was used as laming 
data for determining the weights of the combi- 
nation techniques. 
D)r three classifiers (MBL, MaxEnt and 
IGTree) we haw; used system-internal coral)i- 
nation. These learning algorithms have pro- 
cessed five dittbrent representations of the out- 
put (IOB1, IOB2, IOE1, IOE2 and O-t-C) and 
the results have been combined with majority 
voting. The test data results can 1)e fimnd in 
Table 1. In all cases, the combined results were 
better than that of the best included system. 
Tile results of ALLiS, 05.0, MB SL and SNoW 
have tmen converted to the O and the C repre- 
4Detailed results of our experiments me available on 
http: / /lcg-www.uia.ae.be/-erikt /np('oml,i / 
SThe retagging was necessary to assure that the per- 
formance rates obtained here would be similar to rates 
obtained for texts for which no Treebank POS tags are 
available. 
section 21 
Classifier 
ALLiS 
05.0 
IGTree 
MaxEnt 
MBL 
MBSL 
SNoW 
Simple Voting 
Majority 
TotPrecision 
TagPrecision 
Precision-Recall 
0 
97.87% 
97.05% 
97.70% 
97.94% 
98.04% 
97.27% 
97.78% 
98.08% 
98.08% 
98.08% 
98.08% 
C FS=j 
98.08% 92.15 
97.76% 89.97 
97.99% 91.92 
98.24% 92.60 
98.20% 92.82 
97.66% 90.71 
97.68% 91.87 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
98.21% 92.95 
Pairwise Voting 
TagPair 98.13% 98.23% 
Memory-Based 
Tags 98.24% 98.35% 
Tags 4- P()S 98.14% 98.33% 
Deeision Trees 
Tags 98.24% 98.35% 
Tags + POS 98.13% 98.32% 
93.07 
93.39 
93.24 
93.39 
93.21 
Table 2: Bracket accuracies and Ff~=l scores 
for section WSJ 21 of the Penn ~15'eebank with 
seve, n individual classifiers and combinations of 
them. Each combination t)erforms t)etter than 
its best individual me, tuber. The stacked classi- 
tiers without COllte, xt intbrmation perform best. 
sentation. Together with the bracket; ret)resen- 
tations of the other three techniques, this gave 
us a total of seven O results and seven C results. 
These two data streams have been combined 
with the combination techniques described in 
section 2.4. After this, we built baseNPs from 
the, O and C results of each combinatkm tech- 
nique, like, described in section 2.2. The bracket 
accuracies and tile F~=I scores tbr test data can 
be found in Table 2. 
All combinations iml)rove the results of the 
best individual classifier. The best results were 
obtained with a memory-based stacked classi- 
ter. This is different from the combination re- 
sults presented in Van Ilalteren et al (1998), 
in which pairwise voting pertbrmed best. How- 
eves, in their later work stacked classifiers out- 
perIbrm voting methods as well (Van Halteren 
et al, to appear). 
861 
section 20 accuracy precision recall 
Best-five combination 0:98.32% C:98.41% 94.18% 93.55% 
Tjong Kim Sang (2000) O:98.10% C:98.29% 93.63% 92.89% 
Mufioz et al (1999) O:98.1% C:98.2% 92.4% 93.1% 
Ramshaw and Marcus (1995) IOB1:97.37% 91.80% 92.27% 
Argamon et al (1999) - 91.6% 91.6% 
F/3=1 
93.86 
93.26 
92.8 
92.03 
91.6 
Table 3: The overall pertbrmance of the majority voting combination of our best five systems 
(selected on tinting data perfbrnmnce) applied to the standard data set pnt tbrward by Ramshaw 
and Marcus (1995) together with an overview of earlier work. The accuracy scores indicate how 
often a word was classified correctly with the representation used (O, C or IOB1). The combined 
system outperforms all earlier reported results tbr this data set. 
Based on an earlier combination study 
(Tjong Kim Sang, 2000) we had expected the 
voting methods to do better. We suspect hat 
their pertbrmance is below that of the stacked 
classifiers because the diflhrence between tile 
best and the worst individual system is larger 
than in our earlier study. We assume that the 
voting methods might perform better if they 
were only applied to the classifiers that per- 
form well on this task. In order to test this 
hypothesis, we have repeated the combination 
experiments with the best n classitiers, where 
n took vahms from 3 to 6 and the classifiers 
were ranked based on their performance on the 
tnning data. The t)est pertbrmances were ob- 
tained with five classifiers: F/~=1=93.44 for all 
five voting methods with tile best stacked classi- 
tier reaching 93.24. With the top five classifiers, 
tile voting methods outpertbrm the best; combi- 
nation with seven systems G. Adding extra clas- 
sification results to a good combination system 
should not make overall performance worse so 
it is clear that there is some room left for im- 
provement of our combination algorithms. 
We conclude that the best results ill this 
task can be obtained with tile simplest voting 
method, majority voting, applied to the best 
five of our classifiers. Our next task was to 
apply the combination apt)roach to a standard 
data set so that we could compare our results 
with other work. For this purpose we have used 
6V~re are unaware of a good method for determining 
the significance of F~=I differences but we assume that 
this F~=I difference is not significant. However, we be- 
lieve that the fact that more colnbination methods per- 
tbrm well, shows that it easier to get a good pertbrmmlce 
out of the best; five systems than with all seven. 
tile data put tbrward by ll,amshaw and Marcus 
(1995). Again, only 90% of the training data 
was used tbr training while the remaining 11)% 
was reserved tbr ranking the classifiers. The 
seven learners were trained with the same pa- 
rameters as in the previous experiment. Three 
of the classifiers (MBL, MaxEnt and iG%'ee) 
used system-internal combination by processing 
different output representations. 
The classifier output was converted to the 
O and the C representation. Based on the 
tuning data performance, the classifiers ALLiS, 
IGTREE, MaxEnt, MBL and SNoW were se- 
lected for being combined with majority vot- 
ing. After this, the resulting O and C repre- 
sentations were combined to baseNPs by using 
the method described in section 2.2. The re- 
sults can be found in Table 3. Our combined 
system obtains an F/~=I score of 93.86 which 
corresponds to an 8% error reduction compared 
with tile best published result tbr this data set 
(93.26). 
4 Conc lud ing  remarks  
In this paper we have examined two methods for 
combining the results of machine learuing algo- 
rithms tbr identii}cing base noun phrases. Ill the 
first Inethod, the learner processed ifferent out- 
put data representations and tile results were 
combined by majority voting. This approach 
yielded better results than the best included 
classifier. Ill the second combination approach 
we have combined the results of seven learning 
systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, 
MBSL and SNOW). Here we have tested d i f  
ferent confl)ination methods. Each coilfl)ination 
862 
nmthod outt)erformed the best individual learn- 
ing algorithm and a majority vote of the tol) 
five systems peribrmed best. We, have at}i}lie, d 
this approach of system-internal nd system- 
external coral}|nation to a standard ata set for 
base noun phrase identification and the 1}ertbr- 
mance of our system was 1)etter than any other 
tmblished result tbr this data set. 
Our study shows that the c, omt)ination meth- 
(}{Is that we have tested are sensitive for the in- 
clusion of classifier esults of poor quality. This 
leaves room for imt)rovement of our results t}y 
evaluating other coml}inators. Another interest- 
ing apl)roach which might lead to a l}etter t)er- 
f{}rmance is taking into a{-com~t more context 
inibrmation, for example by coral)in|rig com- 
plete 1}hrases instead of indet}endent t}ra{:kets. 
It would also be worthwhile to evaluate using 
more elaborate me, thods lbr building baseNPs 
out of ot}en and close t}ra{:ket (:an{ti{tates. 
Acknowledgements  
l)djean, Koeling and 'l?jong Kim Sang are 
funded by the TMII. 1\]etwork Learning (Jompu- 
tational Grammars r. 1}unyakanok and Roth are 
SUl)t}orted by NFS grants IIS-98{}1638 an{t SBR- 
9873450. 
Re ferences  
Steven Alm{',y. 1996. Partial t)a\]'sing via finite- 
state cascades. In l'n, l}~wce, di'ngs of the /~,gS- 
LLI '95 l?,obust 1)arsi'n9 Worlcsh, op. 
SMomo Argam(m, Ido l)agan, an(l YllV~t\] Kry- 
molowsld. 1999. A memory-1}ased at}proach 
to learning shalh}w natural anguage patterns. 
Journal of E:rperimental and Th, eovetical AL 
11(3). 
Adam L. Berge, r, SteI}hen A. l)ellaPietra, and 
Vincent J. DellaPietra. 1996. A inaximum 
entrol)y apI)roach to natural language pro- 
cessing. Computational Linguistics, 22(1). 
Eric Bri\]l and ,lun Wu. 1998. Classifier com- 
bination tbr improved lexical disaml)iguation. 
In P~vccedings o.f COLING-A 6'15 '98. Associ- 
ation for Computational Linguistics. 
A. Carlson, C. Cunfl)y, J. Rosen, and 
D. l/,oth. 1.999. The SNoW learning archi- 
tecture. Technical Report UIUCDCS-11,-99- 
2101, UIUC Computer Science Department, 
May. 
r httl): / /lcg-www.ui',,.ac.be~/ 
Walter Daelemans, A.ntal van den Bosch, and 
Jakub Zavrel. 1999a. \])brgetting exceptions 
is harmflll in language learning. Machine 
Learning, 34(1). 
Walter Daelemans, Jakub Zavrel, Ko wmder  
Sloot, and Antal van den Bosch. 1999b. 
TiMBL: Tilb'arg Memory Bused Learner, ver- 
sion 2.G Rqfi;rence Guide. ILK Te(:hnical 
th',port 99-01. http:// i lk.kub.nl/.  
Luc Dehaspe. 1997. Maximum entropy model- 
ing with clausal constraints, in PTvcecdings oJ' 
th, c 7th, 1}l, ternational Workshop on ind'uctivc 
Logic Programming. 
Hervd Ddjean. 200(I. Theory refinement and 
natural language processing. In Proceedings 
of the ColingEO00. Association for Computa- 
tional Linguistics. 
Mitchell 17 }. Marcus, Beatrice Santorini, and 
Mary Aim Marcinkiewicz. 1993. Building a 
large mmotated corpus of english: the penn 
treebank. Computational Linguistics, 19(2). 
Marcia Munoz,  Vasin Punyakanok, l)an l l,oth, 
and Day Zimak. 1999. A learning ap- 
t}roa(:h to shallow t)arsing. In P~vceedings of 
EMNLP-WVLC'99.  Asso('iation for Coml)u- 
tational Linguisti(:s. 
J. Ross Quinlan. 1993. c/t.5: Programs for Ma- 
th,|he Learning. Morgan Kauflnann. 
Lance A. Ramshaw and Mitchell P. Marcus. 
1995. Text chunking using transformation- 
l)ase{t learn|Jig. In 1}roceeding s o\[ the Th, i'rd 
A CL Worksh, op on Ve, r~.l LacTic Corpora. As- 
sociation for Comlmtational Linguistics. 
D. Roth. 1.9!t8. Learning to resolve natural an- 
guage aml}iguities: A unified approach. In 
AAAL98.  
Erik F. Tjong Kim Sang. 2000. N{mn phrase 
recognition by system {:ombination. In Pro- 
ceedings of th, e ANLP-NAA CL-2000. Seattle, 
Washington, USA. Morgan Kauflnan Pub- 
lishers. 
Hans van Halteren, Jakub Zavrel, and Wal- 
ter Daelemans. 1998. Iml)roving data driven 
wordclass tagging by system corot}|nation. In
P~veeedings of COLING-ACL '98. Associa- 
tion tbr Computational Linguistics. 
Hans van Halteren, Jakub Zavrel, and Walter 
Daelemans. to appear, hnproving accuracy 
ill nlp through coati)|nation ofmachine learn- 
ing systems. 
863 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 127-132, Lisbon, Portugal, 2000. 
Introduction to the CoNLL-2000 Shared Task: Chunking 
Erik F. Tjong Kim Sang 
CNTS - Language Technology Group 
University of Antwerp 
er ik tOu ia ,  ua.  ac. be 
Sabine Buchho lz  
ILK, Computat iona l  Linguistics 
Ti lburg University 
s. buchholz@kub, nl 
Abst rac t  
We describe the CoNLL-2000 shared task: 
dividing text into syntactically related non- 
overlapping groups of words, so-called text 
chunking. We give background information on 
the data sets, present a general overview of the 
systems that have taken part in the shared task 
and briefly discuss their performance. 
1 In t roduct ion  
Text chunking is a useful preprocessing step 
for parsing. There has been a large inter- 
est in recognizing non-overlapping oun phrases 
(Ramshaw and Marcus (1995) and follow-up pa- 
pers) but relatively little has been written about 
identifying phrases of other syntactic ategories. 
The CoNLL-2000 shared task attempts to fill 
this gap. 
2 Task  descr ip t ion  
Text chunking consists of dividing a text into 
phrases in such a way that syntactically re- 
lated words become member of the same phrase. 
These phrases are non-overlapping which means 
that one word can only be a member of one 
chunk. Here is an example sentence: 
\[NP He \] \[vP reckons \] \[NP the current 
account deficit \] \[vP will narrow \] 
\[pp to \] \[NP only ? 1.8 billion \] 
\[pp in \]\[NP September \] .  
Chunks have been represented as groups of 
words between square brackets. A tag next to 
the open bracket denotes the type of the chunk. 
As far as we know, there are no annotated cor- 
pora available which contain specific informa- 
tion about dividing sentences into chunks of 
words of arbitrary types. We have chosen to 
work with a corpus with parse information, the 
Wall  Street Journal WSJ  part of the Penn Tree- 
bank  II corpus (Marcus et al, 1993), and to ex- 
tract chunk information from the parse trees in 
this corpus. We will give a global description of 
the various chunk types in the next section. 
3 Chunk  Types  
The  chunk types are based on the syntactic cat- 
egory part (i.e. without function tag) of the 
bracket label in the Treebank (cf. Bies (1995) 
p.35). Roughly, a chunk contains everything to 
the left of and including the syntactic head of 
the constituent of the same name. Some Tree- 
bank  constituents do not have related chunks. 
The  head of S (simple declarative clause) for ex- 
ample is normally thought to be the verb, but 
as the verb is already part of the VP  chunk, no 
S chunk exists in our example  sentence. 
Besides the head, a chunk also contains pre- 
modifiers (like determiners and adjectives in 
NPs), but no postmodifiers or arguments. This 
is why  the PP  chunk only contains the preposi- 
tion, and not the argument  NP,  and the SBAR 
chunk consists of only the complementizer. 
There are several difficulties when converting 
trees into chunks. In the most  simple case, a 
chunk is just a syntactic constituent without 
any further embedded constituents, like the NPs  
in our examples. In some cases, the chunk con- 
tains only what  is left after other chunks have 
been removed from the constituent, cf. " (VP  
loves (NP  Mary))"  above, or ADJPs  and PPs  
below. We will discuss some special cases dur- 
ing the following description of the individual 
chunk types. 
3.1 NP  
Our  NP  chunks are very similar to the ones of 
Ramshaw and Marcus  (1995). Specifically, pos- 
sessive NP  constructions are split in front of 
the possessive marker  (e.g. \[NP Eastern Air- 
lines \] \ [NP ' creditors \]) and the handling of co- 
127 
ordinated NPs follows the Treebank annotators. 
However, as Ramshaw and Marcus do not de- 
scribe the details of their conversion algorithm, 
results may differ in difficult cases, e.g. involv- 
ing NAC and NX. 1 
An ADJP constituent inside an NP con- 
stituent becomes part of the NP chunk: 
(NP The (ADJP most volatile) form) 
\[NP the most volatile form \] 
3.2 VP  
In the Treebank, verb phrases are highly embed- 
ded; see e.g. the following sentence which con- 
tains four VP constituents. Following Ramshaw 
and Marcus' V-type chunks, this sentence will 
only contain one VP chunk: 
((S (NP-SBJ-3 Mr. Icahn) (VP may 
not (VP want (S (NP-SBJ *-3) (VP to 
(VP sell ...))))) . )) 
\[NP Mr. Icahn \] \[vP may not want 
to sell \] ... 
It is still possible however to have one VP chunk 
directly follow another: \[NP The impression \] 
\[NP I \ ]  \[VP have got  \] \[vP is \] \[NP they\]  \[vP 'd 
love to do \] \[PRT away \] \[pp with \] \[NP it \]. In this 
case the two VP constituents did not overlap in 
the Treebank. 
Adverbs/adverbial phrases becorae part of 
the VP chunk (as long as they are in front of 
the main verb): 
(VP could (ADVP very well) (VP 
show . . .  ) )  
"-+ \[ve could very well show \] ... 
In contrast to Ramshaw and Marcus (1995), 
predicative adjectives of the verb are not part 
of the VP chunk, e.g. in "\[NP they \] \[vP are \] 
\[ADJP unhappy \]'. 
In inverted sentences, the auxiliary verb is not 
part of any verb phrase in the Treebank. Con- 
sequently it does not belong to any VP chunk: 
((S (SINV (CONJP Not only) does 
(NP-SBJ-1 your product) (VP have (S 
IE.g. (NP-SBJ (NP Robin Leigh-Pemberton) , (NP 
(NAC Bank (PP of (NP England))) governor) ,) which 
we convert to \[NP Robin Leigh-Pemberton \] , Bank 
\[pp of \] \[NP England \] \[NP governor \] whereas Ramshaw 
and Marcus state that ' "governor" is not included in 
any baseNP chunk'. 
(NP-SBJ *-1) (VP to (VP be (ADJP- 
PRD excellent)))))) , but ... 
\[CONJP Not only \] does \[NP your 
product \] \[vP have to be \] \[ADJP ex- 
cellent \] , but ... 
3.3 ADVP and  ADJP  
ADVP chunks mostly correspond to ADVP con- 
stituents in the Treebank. However, ADVPs in- 
side ADJPs or inside VPs if in front of the main 
verb are assimilated into the ADJP respectively 
VP chunk. On the other hand, ADVPs that 
contain an NP make two chunks: 
(ADVP-TMP (NP a year) earlier) 
-+ \[NP a year \] \[ADVP earlier \] 
ADJPs inside NPs are assimilated into the NP. 
And parallel to ADVPs, ADJPs that contain an 
NP make two chunks: 
(ADJP-PRD (NP 68 years) old) 
\[NP 68 years \] \[ADJP old \] 
It would be interesting to see how chang- 
ing these decisions (as can be done in the 
Treebank-to-chunk conversion script 2) infiu- 
ences the chunking task. 
3.4 PP  and  SBAR 
Most PP  chunks just consist of one word (the 
preposition) with the part-of-speech tag IN. 
This does not mean, though, that finding PP  
chunks is completely trivial. INs can also con- 
stitute an SBAR chunk (see below) and some 
PP  chunks contain more than one word. This 
is the case with fixed multi-word prepositions 
such as such as, because of, due to, with prepo- 
sitions preceded by a modifier: well above, just 
after, even in, particularly among or with coor- 
dinated prepositions: inside and outside. We 
think that PPs behave sufficiently differently 
from NPs in a sentence for not wanting to group 
them into one class (as Ramshaw and Marcus 
did in their N-type chunks), and that on the 
other hand tagging all NP chunks inside a PP 
as I -PP would only confuse the chunker. We 
therefore chose not to handle the recognition of 
true PPs (prep.+NP) during this first chunking 
step. 
~The Treebank-to-chunk conversion script is available 
from http://ilk.kub.nl/-sabine/chunklink/ 
128 
SBAR Chunks mostly consist of one word (the 
complementizer) with the part-of-speech tag IN, 
but like multi-word prepositions, there are also 
multi-word complementizers: even though, so 
that, just as, even if, as if, only if. 
3.5 CONJP ,  PRT,  INT J ,  LST, UCP 
Conjunctions can consist of more than one word 
as well: as well as, instead of, rather than, not 
only, but also. One-word conjunctions (like and, 
or) are not annotated as CONJP in the Tree- 
bank, and are consequently no CONJP chunks 
in our data. 
The Treebank uses the PRT constituent to 
annotate verb particles, and our PRT chunk 
does the same. The only multi-word particle 
is on and off. This chunk type should be easy 
to recognize as it should coincide with the part- 
of-speech tag RP, but through tagging errors it 
is sometimes also assigned IN (preposition) or 
RB (adverb). 
INTJ is an interjection phrase/chunk li e no, 
oh, hello, alas, good grief!. It is quite rare. 
The list marker LST is even rarer. Examples 
are 1., 2 ,  3., .first, second, a, b, c. It might con- 
sist of two words: the number and the period. 
The UCP chunk is reminiscent of the UCP 
(unlike coordinated phrase) constituent in the 
Treebank. Arguably, the conjunction is the 
head of the UCP, so most UCP chunks consist 
of conjunctions like and and or. UCPs are the 
rarest chunks and are probably not very useful 
for other NLP tasks. 
3.6 Tokens outs ide 
Tokens outside any chunk are mostly punctua- 
tion signs and the conjunctions inordinary coor- 
dinated phrases. The word not may also be out- 
side of any chunk. This happens in two cases: 
Either not is not inside the VP constituent in 
the Treebank annotation e.g. in 
... (VP have (VP told (NP-1 clients) 
(S (NP-SBJ *-1) not (VP to (VP ship 
(NP anything)))))) 
or not is not followed by another verb (because 
the main verb is a form of to be). As the right 
chunk boundary is defined by the chunk's head, 
i.e. the main verb in this case, not is thenin fact 
a postmodifier and as such not included in the 
chunk: "... \[SBAR that \] \[NP there \] \[vP were \] 
n't \[NP any major problems \]." 
3.7 P rob lems 
All chunks were automatically extracted from 
the parsed version of the Treebank, guided by 
the tree structure, the syntactic onstituent la- 
bels, the part-of-speech tags and by knowledge 
about which tags can be heads of which con- 
stituents. However, some trees are very complex 
and some annotations are inconsistent. What 
to think about a VP in which the main verb is 
tagged as NN (common oun)? Either we al- 
low NNs as heads of VPs (not very elegant but 
which is what we did) or we have a VP without 
a head. The first solution might also introduce 
errors elsewhere... As Ramshaw and Marcus 
(1995) already noted: "While this automatic 
derivation process introduced a small percent- 
age of errors on its own, it was the only practi- 
cal way both to provide the amount of training 
data required and to allow for fully-automatic 
testing." 
4 Data  and Eva luat ion  
For the CoNLL shared task, we have chosen 
to work with the same sections of the Penn 
Treebank as the widely used data set for base 
noun phrase recognition (Ramshaw and Mar- 
cus, 1995): WSJ sections 15-18 of the Penn 
Treebank as training material and section 20 
as test material 3. The chunks in the data 
were selected to match the descriptions in the 
previous section. An overview of the chunk 
types in the training data can be found in ta- 
ble 1. De data sets contain tokens (words and 
punctuation marks), information about the lo- 
cation of sentence boundaries and information 
about chunk boundaries. Additionally, a part- 
of-speech (POS) tag was assigned to each token 
by a standard POS tagger (Brill (1994) trained 
on the Penn Treebank). We used these POS 
tags rather than the Treebank ones in order to 
make sure that the performance rates obtained 
for this data are realistic estimates for data for 
which no treebank POS tags are available. 
In our example sentence in section 2, we have 
used brackets for encoding text chunks. In the 
data sets we have represented chunks with three 
types of tags: 
3The text chunking data set is available at http://lcg- 
www.uia.ac.be/conll2000/chunking/ 
129 
count % 
55081 51% 
21467 20% 
21281 20% 
4227 4% 
2207 2% 
2060 2% 
556 1% 
56 O% 
31 0% 
10 O% 
2 0% 
type 
NP (noun phrase) 
VP (verb phrase) 
PP (prepositional phrase) 
ADVP (adverb phrase) 
SBAR (subordinated clause) 
ADJP (adjective phrase) 
PRT (particles) 
CONJP (conjunction phrase 
INTJ (interjection) 
LST (list marker) 
UCP (unlike coordinated phrase) 
Table 1: Number of chunks per phrase type 
in the training data (211727 tokens, 106978 
chunks). 
B-X 
I-X 
0 
first word of a chunk of type X 
non-initial word in an X chunk 
word outside of any chunk 
This representation type is based on a repre- 
sentation proposed by Ramshaw and Marcus 
(1995) for noun phrase chunks. The three tag 
groups are sufficient for encoding the chunks in 
the data since these are non-overlapping. Using 
these chunk tags makes it possible to approach 
the chunking task as a word classification task. 
We can use chunk tags for representing our ex- 
ample sentence in the following way: 
He/B-NP reckons/B-VP the/B-NP 
current/I-NP account/I-NP 
deficit/I-NP will/B-VP narrow/I-VP 
to/B-PP only/B-NP #/ I -NP  
1.8/I-NP billion/B-NP in/B-PP 
September/B-NP ./O 
The output of a chunk recognizer may contain 
inconsistencies in the chunk tags in case a word 
tagged I-X follows a word tagged O or I-Y, with 
X and Y being different. These inconsistencies 
can be resolved by assuming that such I-X tags 
start a new chunk. 
The performance on this task is measured 
with three rates. First, the percentage of 
detected phrases that are correct (precision). 
Second, the percentage of phrases in the 
data that were found by the chunker (recall). 
And third, the FZ=i rate which is equal to 
(f12 + 1)*precision*recall / (~2,precision+recall) 
with ~=1 (van Rijsbergen, 1975). The lat- 
ter rate has been used as the target for 
optimization 4. 
5 Resul ts  
The eleven systems that have been applied to 
the CoNLL-2000 shared task can be divided in 
four groups: 
1. Rule-based systems: Villain and Day; Jo- 
hansson; D6jean. 
2. Memory-based systems: Veenstra nd Van 
den Bosch. 
3. Statistical systems: Pla, Molina and Pri- 
eto; Osborne; Koeling; Zhou, Tey and Su. 
4. Combined systems: Tjong Kim Sang; Van 
Halteren; Kudoh and Matsumoto. 
Vilain and Day (2000) approached the shared 
task in three different ways. The most success- 
ful was an application of the Alembic parser 
which uses transformation-based rules. Johans- 
son (2000) uses context-sensitive and context- 
free rules for transforming part-of-speech (POS) 
tag sequences to chunk tag sequences. D6jean 
(2000) has applied the theory refinement sys- 
tem ALLiS to the shared task. In order to ob- 
tain a system which could process XML format- 
ted data while using context information, he 
has used three extra tools. Veenstra and Van 
den Bosch (2000) examined ifferent parame- 
ter settings of a memory-based learning algo- 
rithm. They found that modified value differ- 
ence metric applied to POS information only 
worked best. 
A large number of the systems applied to 
the CoNLL-2000 shared task uses statistical 
methods. Pla, Molina and Prieto (2000) use 
a finite-state version of Markov Models. They 
started with using POS information only and 
obtained a better performance when lexical 
information was used. Zhou, Tey and Su 
(2000) implemented a chunk tagger based on 
HMMs. The initial performance of the tag- 
ger was improved by a post-process correction 
method based on error driven learning and by 
4In the literature about related tasks sometimes the 
tagging accuracy is mentioned as well. However, since 
the relation between tag accuracy and chunk precision 
and recall is not very strict, tagging accuracy is not a 
good evaluation measure for this task. 
130 
test data 
Kudoh and Matsumoto 
Van Halteren 
Tjong Kim Sang 
Zhou, Tey and Su 
D@jean 
Koeling 
Osborne 
Veenstra nd Van den Bosch 
Pla, Molina and Prieto 
Johansson 
Vilain and Day 
baseline 
precision 
93.45% 
93.13% 
94.04% 
91.99% 
91.87% 
92.08% 
91.65% 
91.05% 
90.63% 
86.24% 
88.82% 
72.58% 
recall 
93.51% 
93.51% 
91.00% 
92.25% 
91.31% 
91.86% 
92.23% 
92.03% 
89.65% 
88.25% 
82.91% 
82.14% 
F~=i  
93.48 
93.32 
92.50 
92.12 
92.09 
91.97 
91.94 
91.54 
90.14 
87.23 
85.76 
77.07 
Table 2: Performance of the eleven systems on the test data. The baseline results have been 
obtained by selecting the most frequent chunk tag for each part-of-speech tag. 
incorporating chunk probabilities generated by 
a memory-based learning process. The two 
other statistical systems use maximum-entropy 
based methods. Osborne (2000) trained Ratna- 
parkhi's maximum-entropy POS tagger to out- 
put chunk tags. Koeling (2000) used a stan- 
dard maximum-entropy learner for generating 
chunk tags from words and POS tags. Both 
have tested different feature combinations be- 
fore finding an optimal one and their final re- 
sults are close to each other. 
Three systems use system combination. 
Tjong Kim Sang (2000) trained and tested five 
memory-based learning systems to produce dif- 
ferent representations of the chunk tags. A 
combination of the five by majority voting per- 
formed better than the individual parts. Van 
Halteren (2000) used Weighted Probability Dis- 
tribution Voting (WPDV) for combining the 
results of four WPDV chunk taggers and a 
memory-based chunk tagger. Again the com- 
bination outperformed the individual systems. 
Kudoh and Matsumoto (2000) created 231 sup- 
port vector machine classifiers to predict the 
unique pairs of chunk tags. The results of the 
classifiers were combined by a dynamic pro- 
gramming algorithm. 
The performance of the systems can be found 
in Table 2. A baseline performance was ob- 
tained by selecting the chunk tag most fre- 
quently associated with a POS tag. All systems 
outperform the baseline. The majority of the 
systems reached an F~=i score between 91.50 
and 92.50. Two approaches performed a lot 
better: the combination system WPDV used by 
Van Halteren and the Support Vector Machines 
used by Kudoh and Matsumoto. 
6 Re la ted  Work  
In the early nineties, Abney (1991) proposed 
to approach parsing by starting with finding 
related chunks of words. By then, Church 
(1988) had already reported on recognition 
of base noun phrases with statistical meth- 
ods. Ramshaw and Marcus (1995) approached 
chunking by using a machine learning method. 
Their work has inspired many others to study 
the application of learning methods to noun 
phrase chunking 5. Other chunk types have not 
received the same attention as NP chunks. The 
most complete work is Buchholz et al (1999), 
which presents results for NP, VP, PP, ADJP 
and ADVP chunks. Veenstra (1999) works with 
NP, VP and PP chunks. Both he and Buchholz 
et al use data generated by the script that pro- 
duced the CoNLL-2000 shared task data sets. 
Ratnaparkhi (1998) has recognized arbitrary 
chunks as part of a parsing task but did not re- 
port on the chunking performance. Part of the 
Sparkle project has concentrated on finding var- 
ious sorts of chunks for the different languages 
~An elaborate overview of the work done on noun 
phrase chunking can be found on http://lcg-www.uia. 
ac.be/- erikt/reseaxch/np-chunking.html 
131 
(Carroll et al, 1997). 
'7 Conc lud ing  Remarks  
We have presented an introduction to the 
CoNLL-2000 shared task: dividing text into 
syntactically related non-overlapping groups of 
words, so-called text chunking. For this task we 
have generated training and test data from the 
Penn Treebank. This data has been processed 
by eleven systems. The best performing system 
was a combination of Support Vector Machines 
submitted by Taku Kudoh and Yuji Matsumoto. 
It obtained an FZ=i score of 93.48 on this task. 
Acknowledgements  
We would like to thank the members of 
the CNTS - Language Technology Group in 
Antwerp, Belgium and the members of the ILK 
group in Tilburg, The Netherlands for valuable 
discussions and comments. Tjong Kim Sang is 
funded by the European TMR network Learn- 
ing Computational Grammars. Buchholz is sup- 
ported by the Netherlands Organization for Sci- 
entific Research (NWO). 
Re ferences  
Steven Abney. 1991. Parsing by chunks. In 
Principle-Based Parsing. Kluwer Academic Pub- 
lishers. 
Ann Bies, Mark Ferguson, Karen Katz, and Robert 
MacIntyre. 1995. Bracket Guidelines /or Tree- 
bank H Style Penn Treebank Project. Penn Tree- 
bank II cdrom. 
Eric Brill. 1994. Some advances in rule-based 
part of speech tagging. In Proceedings o\] the 
Twelfth National Con/erence on Artificial Intel- 
ligence (AAAI-9~). Seattle, Washington. 
Sabine Buchholz, Jorn Veenstra, and Walter Daele- 
mans. 1999. Cascaded grammatical :relation as- 
signment. In Proceedings o\] EMNLP/VLC-99. 
Association for Computational Linguistics. 
John Carroll, Ted Briscoe, Glenn Carroll, Marc 
Light, Dethleff Prescher, Mats Rooth, Stefano 
Federici, Simonetta Montemagni, Vito Pirrelli, 
Irina Prodanof, and Massimo Vanocchi. 1997. 
Phrasal Parsing Software. Sparkle Work Package 
3, Deliverable D3.2. 
Kenneth Ward Church. 1988. A stochastic parts 
program and noun phrase parser for unrestricted 
text. In Second Con\]erence on Applied Natural 
Language Processing. Austin, Texas. 
H@rve D@jean. 2000. Learning syntactic structures 
with xml. In Proceedings o/ CoN~LL-2000 and 
LLL-2000. Lisbon, Portugal. 
Christer Johansson. 2000. A context sensitive max- 
imum likelihood approach to chunking. In Pro- 
ceedings o\] CoNLL-2000 and LLL-2000. Lisbon, 
Portugal. 
Rob Koeling. 2000. Chunking with maximum en- 
tropy models. In Proceedings o/ CoNLL-2000 and 
LLL-2000. Lisbon, Portugal. 
Taku Kudoh and Yuji Matsumoto. 2000. Use of sup- 
port vector learning for chunk identification. In 
Proceedings o~ CoNLL-2000 and LLL-2000. Lis- 
bon, Portugal. 
Mitchell P. Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building a large 
annotated corpus of english: the penn treebank. 
Computational Linguistics, 19(2). 
Miles Osborne. 2000. Shallow parsing as part-of- 
speech tagging. In Proceedings o\] CoNLL-2000 
and LLL-2000. Lisbon, Portugal. 
Ferran Pla, Antonio Molina, and Natividad Pri- 
eto. 2000. Improving chunking by means of 
lexical-contextual information in statistical lan- 
guage models. In Proceedings o\] CoNLL-2000 and 
LLL-2000. Lisbon, Portugal. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text chunking using transformation-based learn- 
ing. In Proceedings o\] the Third A CL Workshop 
on Very Large Corpora. Association for Compu- 
tational Linguistics. 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models \]or Natural Language Ambiguity Resolu- 
tion. PhD thesis Computer and Information Sci- 
ence, University of Pennsylvania. 
Erik F. Tjong Kim Sang. 2000. Text chunking by 
system combination. In Proceedings of CoNLL- 
2000 and LLL-2000. Lisbon, Portugal. 
Hans van Halteren. 2000. Chunking with wpdv 
models. In Proceedings o/ CoNLL-2000 and LLL- 
2000. Lisbon, Portugal. 
C.J. van Rijsbergen. 1975. In/ormation Retrieval. 
Buttersworth. 
Jorn Veenstra and Antal van den Bosch. 2000. 
Single-classifier memory-based phrase chunking. 
In Proceedings o\] CoNLL-2000 and LLL-2000. 
Lisbon, Portugal. 
Jorn Veenstra. 1999. Memory-based text chunking. 
In Nikos Fakotakis, editor, Machine learning in 
human language technology, workshop at ACAI 
99. 
Marc Vilain and David Day. 2000. Phrase parsing 
with rule sequence processors: an application to 
the shared conll task. In Proceedings o/CoNLL- 
2000 and LLL-2000. Lisbon, Portugal. 
GuoDong Zhou, Jian Su, and TongGuan Tey. 2000. 
Hybrid text chunking. In Proceedings o/CoNLL- 
2000 and LLL-2000. Lisbon, Portugal. 
132 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 151-153, Lisbon, Portugal, 2000. 
Text Chunking by System Combination 
Erik F. Tjong Kim Sang 
CNTS - Language Technology Group 
University of Antwerp 
er ik t@uia ,  ua.  ac.  be 
1 In t roduct ion  
We will apply a system-internal combination 
of memory-based learning classifiers to the 
CoNLL-2000 shared task: finding base chunks. 
Apart from testing different combination meth- 
ods, we will also examine if dividing the chunk- 
ing process in a boundary recognition phase and 
a type identification phase would aid perfor- 
mance. 
2 Approach  
Tjong Kim Sang (2000) describes how a system- 
internal combination of memory-based learners 
can be used for base noun phrase (baseNP) 
recognition. The idea is to generate different 
chunking models by using different chunk rep- 
resentations. Chunks can be represented with 
bracket structures but alternatively one can use 
a tagging representation which classifies words 
as being inside a chunk (I), outside a chunk 
(O) or at a chunk boundary (B) (Ramshaw and 
Marcus, 1995). There are four variants of this 
representation. The B tags can be used for the 
first word of chunks that immediately follow an- 
other chunk (the IOB1 representation) or they 
can be used for every chunk-initial word (IOB2). 
Alternatively an E tag can be used for labeling 
the final word of a chunk immediately preced- 
ing another chunk (IOE1) or it can be used for 
every chunk-final word (IOE2). Bracket struc- 
tures can also be represented as tagging struc- 
tures by using two streams of tags which de- 
fine whether words start a chunk or not (O) 
or whether words are at the end of a chunk or 
not (C). We need both for encoding the phrase 
structure and hence we will treat the two tag 
streams as a single representation (O+C). A 
combination of baseNP classifiers that use the 
five representation performs better than any of 
the included systems (Tjong Kim Sang, 2000). 
We will apply such a classifier combination to 
the CoNLL-2000 shared task. 
The individual classifiers will use the 
memory-based learning algorithm IBi-IG 
(Daelemans et al, 1999) for determining 
the most probable tag for each word. In 
memory-based learning the training data is 
stored and a new item is classified by the most 
frequent classification among training items 
which are closest o this new item. Data items 
are represented as sets of feature-value pairs. 
Features receive weights which are based on 
the amount of information they provide for 
classifying the training data (Daelemans et al, 
1999). 
We will evaluate nine different methods for 
combining the output of our five chunkers (Van 
Halteren et al, 1998). Five are so-called voting 
methods. They assign weights to the output of 
the individual systems and use these weights to 
determine the most probable output tag. Since 
the classifiers generate different output formats, 
all classifier output has been converted to the 
O and the C representations. The most sim- 
ple voting method assigns uniform weights and 
picks the tag that occurs most often (Majority). 
A more advanced method is to use as a weight 
the accuracy of the classifier on some held-out 
part of the training data, the tuning data (Tot- 
Precision). One can also use the precision ob- 
tained by a classifier for a specific output value 
as a weight (TagPrecision). Alternatively, we 
use as a weight a combination of the precision 
score for the output tag in combination with 
the recall score for competing tags (Precision- 
Recall). The most advanced voting method ex- 
amines output values of pairs of classifiers and 
assigns weights to tags based on how often they 
appear with this pair in the tuning data (Tag- 
Pair, Van Halteren et al, (1998)). 
151 
Apart from these voting methods we have also 
applied two memory-based learners t;o the out- 
put of the five chunkers: IBI-IG and IGTREE, a 
decision tree variant of IBI-IG (Daelemans et 
al., 1999). This approach is called classifier 
stacking. Like with the voting algorithms, we 
have tested these meta-classifiers with the out- 
put of the first classification stage. Unlike the 
voting algorithms, the classifiers do not require 
a uniform input. Therefore we have tested if 
their performance can be improved by supply- 
ing them with information about the input of 
the first classification stage. For this purpose 
we have used the part-of-speech tag of the cur- 
rent word as compressed representation f the 
first stage input (Van Halteren et al, 1998). 
The combination methods will generate a list 
of open brackets and a list of close brackets. We 
have converted these to phrases by only using 
brackets which could be matched with the clos- 
est matching candidate and ignoring the others. 
For example, in the structure \[NP a \[NP b \]gg 
\[VP c \]pg d \]vg, we would accept \[NP b \]NP 
as a noun phrase and ignore all other brackets 
since they cannot be matched with their clos- 
est candidate for a pair, either because of type 
inconsistencies or because there was some other 
bracket in between them. 
We will examine three processing strategies 
in order to test our hypothesis that chunking 
performance can be increased by making a dis- 
tinction between finding chunk boundaries and 
identifying chunk types. The first is the single- 
pass method. Here each individual classifier at- 
tempts to find the correct chunk tag for each 
word in one step. A variant of this is the double- 
pass method. It processes the data twice: first 
it searches for chunks boundaries and then it 
attempts to identify the types of the chunks 
found. The third processing method is the n- 
pass method. It contains as many passes as 
there are different chunk types. In each pass, 
it attempts to find chunks of a single type. In 
case a word is classified as belonging to more 
than one chunk type, preference will be given 
to the chunk type that occurs most often in the 
training data. We expect he n-pass method to 
outperform the other two methods. However, 
we are not sure if the performance difference 
will be large enough to compensate for the extra 
computation that is required for this processing 
method. 
3 Resu l ts  
In order to find out which of the three process- 
ing methods and which of the nine combination 
methods performs best, we have applied them 
to the training data of the CoNLL-2000 shared 
task (Tjong Kim Sang and Buchholz, 2000) in a 
10-fold cross-validation experiment (Weiss and 
Kulikowski, 1991). For the single-pass method, 
we trained IBI-IG classifiers to produce the most 
likely output tags for the five data representa- 
tions. In the input of the classifiers a word was 
represented as itself, its part-of-speech tag and 
a context of four left and four right word/part- 
of-speech tag pairs. For the four IO represen- 
tations we used a second phase with a lim- 
ited input context (3) but with additionally the 
two previous and the two next chunk tags pre- 
dicted by the first phase. The classifier out- 
put was converted to the O representation (open 
brackets) and the C representation (close brack- 
ets) and the results were combined with the 
nine combination methods. In the double-pass 
method finding the most likely tag for each word 
was split in finding chunk boundaries and as- 
signing types to the chunks. The n-pass method 
divided this process into eleven passes each of 
which recognized one chunk type. 
For each processing strategy, all combination 
results were better than those obtained with the 
five individual classifiers. The differences be- 
tween combination results within each process- 
ing strategy were small and between the three 
strategies the best results were not far apart: 
the best FZ=i rates were 92.40 (single-pass), 
92.35 (double-pass) and 92.75 (n-pass). 
Since the three processing methods reach a 
similar performances, we can choose any of 
them for our remaining experiments. The n- 
pass method performed best but it has the 
disadvantage of needing as many passes as 
there are chunk types. This will require a 
lot of computation. The single-pass method 
was second-best but in order to obtain good 
results with this method, we would need to 
use a stacked classifier because those performed 
better (F~=1=92.40) than the voting methods 
(Fz=1=91.98). This stacked classifier equires 
preprocessed combinator training data which 
can be obtained by processing the original train- 
152 
ing data with 10-fold cross-validation. Again 
this will require a lot of work for new data sets. 
We have chosen for the double-pass method 
because in this processing strategy it is possi- 
ble to obtain good results with majority vot- 
ing. The advantage of using majority voting is 
that it does not require extra preprocessed com- 
binator training data so by using it we avoid 
the extra computation required for generating 
this data. We have applied the double-pass 
method with majority voting to the CoNLL- 
2000 test data while using the complete train- 
ing data. The results can be found in table 1. 
The recognition method performs well for the 
most frequently occurring chunk types (NP, VP 
and PP) and worse for the other seven (the test 
data did not contain UCP chunks). The recog- 
nition rate for NP chunks (F~=1=93.23) is close 
to the result for a related standard baseNP data 
set obtained by Tjong Kim Sang (2000) (93.26). 
Our method outperforms the results mentioned 
in Buchholz et al (1999) in four of the five 
cases (ADJP, NP, PP and VP); only for ADVP 
chunks it performs lightly worse. This is sur- 
prising given that Buchholz et al (1999) used 
956696 tokens of training data and we have used 
only 211727 (78% less). 
4 Conc lud ing  Remarks  
We have evaluated three methods for recogniz- 
ing non-recursive non-overlapping text chunks 
of arbitrary syntactical categories. In each 
method a memory-based learner was trained 
to recognize chunks represented in five differ- 
ent ways. We have examined nine different 
methods for combining the five results. A 10- 
fold cross-validation experiment on the train- 
ing data of the CoNLL-2000 shared task re- 
vealed that (1) the combined results were better 
than the individual results, (2) the combination 
methods perform equally well and (3) the best 
performances of the three processing methods 
were similar. We have selected the double-pass 
method with majority voting for processing the 
CoNLL-2000 shared task data. This method 
outperformed an earlier text chunking study for 
most chunk types, despite the fact that it used 
about 80% less training data. 
Re ferences  
Sabine Buchholz, Jorn Veenstra, and Walter Daele- 
mans. 1999. Cascaded grammatical relation as- 
test data 
ADJP 
ADVP 
CONJP 
INTJ 
LST 
NP 
PP 
PRT 
SBAR 
VP 
all 
precision 
85.25% 
85.03% 
42.86% 
100.00% 
0.00% 
94.14% 
96.45% 
79.49% 
89.81% 
93.97% 
94.04% 
recall 
59.36% 
71.48% 
33.33% 
50.00% 
0.00% 
92.34% 
96.59% 
58.49% 
72.52% 
91.35% 
91.00% 
Ffl=l 
69.99 
77.67 
37.50 
66.67 
0.00 
93.23 
96.52 
67.39 
80.25 
92.64 
92.50 
Table h The results per chunk type of process- 
ing the test data with the double-pass method 
and majority voting. Our method outper- 
forms most chunk type results mention in Buch- 
holz et al (1999) (FAD jR=66.7, FADVp=77.9 
FNp=92.3, Fpp=96.8, FNp=91.8) despite the 
fact that we have used about 80% less training 
data. 
signment. In Proceedings o\] EMNLP/VLC-99. 
Association for Computational Linguistics. 
Walter Daelemans, Jakub Zavrel, Ko van der 
Sloot, and Antal van den Bosch. 1999. TiMBL: 
Tilburg Memory Based Learner, version 2.0, 
Reference Guide. ILK Technical Report 99-01. 
http://ilk.kub.nl/. 
Lance A. Ramshaw and Mitchell P. Marcus. 1995. 
Text chunking using transformation-based learn- 
ing. In Proceedings of the Third A CL Workshop 
on Very Large Corpora. Association for Compu- 
tational Linguistics. 
Erik F. Tjong Kim Sang and Sabine Buchholz. 
2000. Introduction to the CoNLL-2000 shared 
task: Chunking. In Proceedings of the CoNLL- 
2000. Association for Computational Linguistics. 
Erik F. Tjong Kim Sang. 2000. Noun phrase recog- 
nition by system combination. In Proceedings of 
the ANLP-NAACL 2000. Seattle, Washington, 
USA. Morgan Kaufman Publishers. 
Hans van Halteren, Jakub Zavrel, and Walter Daele- 
mans. 1998. Improving data driven wordclass 
tagging by system combination. In Proceedings 
of COLING-ACL '98. Association for Computa- 
tional Linguistics. 
Sholom M. Weiss and Casimir A. Kulikowski. 1991. 
Computer Systems That Learn: Classification and 
Prediction Methods ~rom Statistics, Neural Nets, 
Machine Learning and Expert Systems. Morgan 
Kaufmann. 
153 
Combining a self-organising map with memory-based learning
James Hammerton
james.hammerton@ucd.ie
Dept of Computer Science
University College Dublin
Ireland
Erik Tjong Kim Sang
erikt@uia.ua.ac.be
CNTS ? Language Technology Group
University of Antwerp
Belgium
Abstract
Memory-based learning (MBL) has
enjoyed considerable success in
corpus-based natural language process-
ing (NLP) tasks and is thus a reliable
method of getting a high-level of per-
formance when building corpus-based
NLP systems. However there is a
bottleneck in MBL whereby any novel
testing item has to be compared against
all the training items in memory base.
For this reason there has been some
interest in various forms of memory
editing whereby some method of
selecting a subset of the memory base
is employed to reduce the number of
comparisons. This paper investigates
the use of a modified self-organising
map (SOM) to select a subset of the
memory items for comparison. This
method involves reducing the number
of comparisons to a value proportional
to the square root of the number of
training items. The method is tested on
the identification of base noun-phrases
in the Wall Street Journal corpus,
using sections 15 to 18 for training and
section 20 for testing.
1 Introduction
Currently, there is considerable interest in ma-
chine learning methods for corpus-based lan-
guage learning. A promising technique here is
memory-based learning1 (MBL) (Daelemans et
al., 1999a), where a task is redescribed as a classi-
fication problem. The classification is performed
by matching an input item to the most similar of
a set of training items and choosing the most fre-
quent classification of the closest item(s). Sim-
ilarity is computed using an explicit similarity
metric.
MBL performs well by bringing all the training
data to bear on the task. This is done at the cost, in
the worst case, of comparing novel items to all of
the training items to find the closest match. There
is thus some interest in developing memory edit-
ing techniques to select a subset of the items for
comparison.
This paper investigates whether a self-
organising map (SOM) can be used to perform
memory editing without reducing performance.
The system is tested on base noun-phrase (NP)
chunking using the Wall Street Journal corpus
(Marcus et al, 1993).
2 The Self-Organising Map (SOM)
The SOM was developed originally by Kohonen
(1990) and has found a wide range of uses from
classification to storing a lexicon. It operates as
follows (see Figure 1). The SOM consists of two
layers, an input layer and the map. Each unit in
the map has a vector of weights associated with
it that is the same size as that of the input layer.
When an input is presented to the SOM, the unit
whose weight vector is closest to the input vector
is selected as a winner.
1Also known as instance based learning.
INPUTS
MAP
Figure 1: The Self-Organising Map. The map
units respond to the inputs. The map unit whose
weight vector is closest to the input vector be-
comes the winner. During training, after a winner
is chosen, the weight vectors of the winner and
a neighbourhood of surrounding units are nudged
towards the current input.
During training, the weight vectors of winning
unit and a set of units within the neighbourhood
of the winner are nudged, by an amount deter-
mined by the learning rate, towards the input vec-
tor. Over time the size of the neighbourhood is
decreased. Sometimes the learning rate may be
too. At the end of training, the units form a map
of the input space that reflects how the input space
was sampled in the training data. In particular ar-
eas of input space in which there were a lot of
inputs will be mapped in finer detail, using more
units than areas where the inputs were sparsely
distributed.
3 Why use a SOM for memory editing
The SOM was chosen because the input is
matched to the unit with the closest weight vec-
tor. Thus it is motivated by the same principle as
used to find the closest match in MBL. It is thus
hoped that the SOM can minimise the risk of fail-
ing to select the closest match, since the subset
will be chosen according to similarity.
However, Daelemans et al(1999b) claim that,
in language learning, pruning the training items
is harmful. When they removed items from the
memory base on the basis of their typicality (i.e.
the extent to which the items were representative
of other items belonging to the same class) or
their class prediction strength (i.e. the extent to
which the item formed a predictor for its class),
the generalisation performance of the MBL sys-
tem dropped across a range of language learning
tasks.
The memory editing approach used by Daele-
mans et alremoves training items independently
of the novel items, and the remainder are used for
matching with all novel items. If one selects a
different subset for each novel item based on sim-
ilarity to the novel item, then maybe the risk of
degrading performance in memory editing will be
reduced. This work aims to achieve precisely this.
4 A hybrid SOM/MBL classifier
4.1 Labelled SOM and MBL (LSOMMBL)
A modified SOM was developed called Labelled
SOM. Training proceeds as follows:
  Each training item has an associated label.
Initially all the map units are unlabelled.
  When an item is presented, the closest unit
out of those with the same label as the input
and those that are unlabelled is chosen as the
winner. Should an unlabelled unit be chosen
it gets labelled with the input?s label.
  The weights for neighbouring units are up-
dated as with the standard SOM if they share
the same label as the input or are unlabelled.
  When training ends, all the training inputs
are presented to the SOM and the winners
for each training input noted. Unused units
are discarded.
Testing proceeds as follows:
  When an input is presented a winning unit is
found for each category.
  The closest match is selected from the train-
ing items associated with each of the win-
ning units found.
  The most frequent classification for that
match is chosen.
It is thus hoped that the closest matches for each
category are found and that these will include the
closest match amongst them.
Assuming each unit is equally likely to be cho-
sen, the average number of comparisons here is
given by
	

where

is the number of
categories,

is the number of units in the map
and

is the number of training items. Choosing
  
minimises comparisons to 
 
. In
the experiments the size of the map was chosen
to be close to
 
. This system is referred to as
LSOMMBL.
4.2 SOM and MBL (SOMMBL)
In the experiments, a comparison with using
the standard SOM in a similar manner was per-
formed. Here the SOM is trained as normal on
the training items.
At the end of training each item is presented to
the SOM and the winning unit noted as with the
modified SOM above. Unused units are discarded
as above.
During testing, a novel item is presented to the
SOM and the top C winners chosen (i.e. the 
closest map units), where C is the number of cat-
egories. The items associated with these win-
ners are then compared with the novel item and
the closest match found and then the most fre-
quent classification of that match is taken as be-
fore. This system is referred to as SOMMBL.
5 The task: Base NP chunking
The task is base NP chunking on section 20 of the
Wall Street Journal corpus, using sections 15 to
18 of the corpus as training data as in (Ramshaw
and Marcus, 1995). For each word in a sentence,
the POS tag is presented to the system which out-
puts whether the word is inside or outside a base
NP, or on the boundary between 2 base NPs.
Training items consist of the part of speech
(POS) tag for the current word, varying amounts
of left and right context (POS tags only) and the
classification frequencies for that combination of
tags. The tags were represented by a set of vec-
tors. 2 sets of vectors were used for comparison.
One was an orthogonal set with a vector of all
zeroes for the ?empty? tag, used where the con-
text extends beyond the beginning/end of a sen-
tence. The other was a set of 25 dimensional vec-
tors based on a representation of the words en-
coding the contexts in which each word appears
in the WSJ corpus. The tag representations were
obtained by averaging the representations for the
words appearing with each tag. Details of the
method used to generate the tag representations,
known as lexical space, can be found in (Zavrel
and Veenstra, 1996). Reilly (1998) found it ben-
eficial when training a simple recurrent network
on word prediction.
The self-organising maps were trained as fol-
lows:
  For maps with 100 or more units, the train-
ing lasted 250 iterations and the neighbour-
hood started at a radius of 4 units, reducing
by 1 unit every 50 iterations to 0 (i.e. where
only the winner?s weights are modified). The
learning rate was constant at 0.1.
  For the maps with 6 units, the training lasted
90 iterations, with an initial neighbourhood
of 2, reducing by one every thirty iterations.
  For the maps with 30 units, the training
lasted 150 iterations, with an initial neigh-
bourhood of 2, reduced by one every 50 iter-
ations. A single training run is reported for
each network since the results did not vary
significantly for different runs.
These map sizes were chosen to be close to the
square root of the number of items in the training
set. No attempt was made to systematically in-
vestigate whether these sizes would optimise the
performance of the system. They were chosen
purely to minimise the number of comparisons
performed.
6 Results
Table 1 gives the results of the experiments. The
columns are as follows:
 
?features?. This column indicates how the
features are made up.
?lex? means the features are the lexical
space vectors representing the POS tags.
?orth? means that orthogonal vectors are
used. ?tags? indicates that the POS tags
themselves are used. MBL uses a weighted
overlap similarity metric while SOMMBL
and LSOMMBL use the euclidean distance.
features window Chunk Chunk tag Max
fscore accuracy comparisons (% of items)
LSOMMBL lex 0-0 79.99 94.48% 87 (197.7%)
LSOMMBL lex 1-0 86.13 95.77% 312 (30.3%)
LSOMMBL lex 1-1 89.51 96.76% 2046 (20.4%)
LSOMMBL lex 2-1 88.91 96.42% 2613 (6.8%)
LSOMMBL orth 0-0 79.99 94.48% 87 (197.7%)
LSOMMBL orth 1-0 86.09 95.76% 702 (68.1%)
LSOMMBL orth 1-1 89.39 96.75% 1917 (19.1%)
LSOMMBL orth 2-1 88.71 96.52% 2964 (7.7%)
SOMMBL lex 0-0 79.99 94.48% 51 (115.9%)
SOMMBL lex 1-0 86.11 95.77% 327 (31.7%)
SOMMBL lex 1-1 89.47 96.74% 1005 (10.0%)
SOMMBL lex 2-1 88.98 96.48% 1965 (5.1%)
SOMMBL orth 0-0 79.99 94.48% 42 (95.5%)
SOMMBL orth 1-0 86.08 95.75% 306 (29.7%)
SOMMBL orth 1-1 89.38 96.77% 1365 (13.6%)
SOMMBL orth 2-1 88.61 96.45% 2361 (6.1%)
MBL tags 0-0 79.99 94.48% 44 (100.0%)
MBL tags 1-0 86.14 95.78% 1031 (100.0%)
MBL tags 1-1 89.57 96.80% 10042 (100.0%)
MBL tags 2-1 89.81 96.83% 38465 (100.0%)
MBL lex 0-0 79.99 94.70% 44 (100.0%)
MBL lex 1-0 86.14 95.95% 1031 (100.0%)
MBL lex 1-1 89.57 96.93% 10042 (100.0%)
MBL lex 2-1 89.81 96.96% 38465 (100.0%)
MBL orth 0-0 79.99 94.70% 44 (100.0%)
MBL orth 1-0 86.12 95.94% 1031 (100.0%)
MBL orth 1-1 89.46 96.89% 10042 (100.0%)
MBL orth 2-1 89.55 96.87% 38465 (100.0%)
Table 1: Results of base NP chunking for section 20 of the WSJ corpus, using SOMMBL, LSOMMBL
and MBL, training was performed on sections 15 to 18. The fscores of the best performers in each case
for LSOMMBL, SOMMBL and MBL have been highlighted in bold.
 
?window?. This indicates the amount of con-
text, in the form of ?left-right? where ?left?
is the number of words in the left context,
and ?right? is the number of words in the
right context.
 
?Chunk fscore? is the fscore for finding base
NPs. The fscore (  ) is computed as  

 
	ffLearning Computational Grammars
John Nerbonne   , Anja Belz  , Nicola Cancedda  , Herve? De?jean  ,
James Hammerton

, Rob Koeling

, Stasinos Konstantopoulos   ,
Miles Osborne   , Franck Thollard

and Erik Tjong Kim Sang 
Abstract
This paper reports on the LEARNING
COMPUTATIONAL GRAMMARS (LCG)
project, a postdoc network devoted to
studying the application of machine
learning techniques to grammars suit-
able for computational use. We were in-
terested in a more systematic survey to
understand the relevance of many fac-
tors to the success of learning, esp. the
availability of annotated data, the kind
of dependencies in the data, and the
availability of knowledge bases (gram-
mars). We focused on syntax, esp. noun
phrase (NP) syntax.
1 Introduction
This paper reports on the still preliminary, but al-
ready satisfying results of the LEARNING COM-
PUTATIONAL GRAMMARS (LCG) project, a post-
doc network devoted to studying the application
of machine learning techniques to grammars suit-
able for computational use. The member insti-
tutes are listed with the authors and also included
ISSCO at the University of Geneva. We were im-
pressed by early experiments applying learning
to natural language, but dissatisfied with the con-
centration on a few techniques from the very rich
area of machine learning. We were interested in

University of Groningen,  nerbonne,konstant  @let.
rug.nl, osborne@cogsci.ed.ac.uk
	
SRI Cambridge, anja.belz@cam.sri.com, Rob.Koe-
ling@netdecisions.co.uk

XRCE Grenoble, nicola.cancedda@xrce.xerox.com

University of Tu?bingen, Herve.Dejean@xrce.xerox.
com, thollard@sfs.nphil.uni-tuebingen.de

University College Dublin, james.hammerton@ucd.ie

University of Antwerp, erikt@uia.ua.ac.be
a more systematic survey to understand the rele-
vance of many factors to the success of learning,
esp. the availability of annotated data, the kind
of dependencies in the data, and the availability
of knowledge bases (grammars). We focused on
syntax, esp. noun phrase (NP) syntax from the
beginning. The industrial partner, Xerox, focused
on more immediate applications (Cancedda and
Samuelsson, 2000).
The network was focused not only by its sci-
entific goal, the application and evaluation of
machine-learning techniques as used to learn nat-
ural language syntax, and by the subarea of syn-
tax chosen, NP syntax, but also by the use of
shared training and test material, in this case ma-
terial drawn from the Penn Treebank. Finally, we
were curious about the possibility of combining
different techniques, including those from statisti-
cal and symbolic machine learning. The network
members played an important role in the organi-
sation of three open workshops in which several
external groups participated, sharing data and test
materials.
2 Method
This section starts with a description of the three
tasks that we have worked on in the framework of
this project. After this we will describe the ma-
chine learning algorithms applied to this data and
conclude with some notes about combining dif-
ferent system results.
2.1 Task descriptions
In the framework of this project, we have worked
on the following three tasks:
1. base phrase (chunk) identification
2. base noun phrase recognition
3. finding arbitrary noun phrases
Text chunks are non-overlapping phrases which
contain syntactically related words. For example,
the sentence:
 
He 
 
reckons 
 
the current
account deficit 
 
will narrow 
 
to 
 
only  1.8 billion 
 
in 
 
September  .
contains eight chunks, four NP chunks, two VP
chunks and two PP chunks. The latter only con-
tain prepositions rather than prepositions plus the
noun phrase material because that has already
been included in NP chunks. The process of
finding these phrases is called CHUNKING. The
project provided a data set for this task at the
CoNLL-2000 workshop (Tjong Kim Sang and
Buchholz, 2000)1. It consists of sections 15-18 of
the Wall Street Journal part of the Penn Treebank
II (Marcus et al, 1993) as training data (211727
tokens) and section 20 as test data (47377 tokens).
A specialised version of the chunking task is
NP CHUNKING or baseNP identification in which
the goal is to identify the base noun phrases. The
first work on this topic was done back in the
eighties (Church, 1988). The data set that has
become standard for evaluation machine learn-
ing approaches is the one first used by Ramshaw
and Marcus (1995). It consists of the same train-
ing and test data segments of the Penn Treebank
as the chunking task (respectively sections 15-18
and section 20). However, since the data sets
have been generated with different software, the
NP boundaries in the NP chunking data sets are
slightly different from the NP boundaries in the
general chunking data.
Noun phrases are not restricted to the base lev-
els of parse trees. For example, in the sentence In
early trading in Hong Kong Monday , gold was
quoted at $ 366.50 an ounce ., the noun phrase
  $ 366.50 an ounce  contains two embedded
noun phrases
  $ 366.50  and   an ounce  .
In the NP BRACKETING task, the goal is to find
all noun phrases in a sentence. Data sets for this
task were defined for CoNLL-992. The data con-
sist of the same segments of the Penn Treebank as
1Detailed information about chunking, the CoNLL-
2000 shared task, is also available at http://lcg-
www.uia.ac.be/conll2000/chunking/
2Information about NP bracketing can be found at
http://lcg-www.uia.ac.be/conll99/npb/
the previous two tasks (sections 15-18) as train-
ing material and section 20 as test material. This
material was extracted directly from the Treebank
and therefore the NP boundaries at base levels are
different from those in the previous two tasks.
In the evaluation of all three tasks, the accu-
racy of the learners is measured with three rates.
We compare the constituents postulated by the
learners with those marked as correct by experts
(gold standard). First, the percentage of detected
constituents that are correct (precision). Second,
the percentage of correct constituents that are de-
tected (recall). And third, a combination of pre-
cision and recall, the F ff  	
ffIntroduction to the CoNLL-2002 Shared Task:
Language-Independent Named Entity Recognition
Erik F. Tjong Kim Sang
CNTS - Language Technology Group
University of Antwerp
erikt@uia.ua.ac.be
Abstract
We describe the CoNLL-2002 shared task:
language-independent named entity recogni-
tion. We give background information on the
data sets and the evaluation method, present a
general overview of the systems that have taken
part in the task and discuss their performance.
1 Introduction
Named entities are phrases that contain the
names of persons, organizations, locations,
times and quantities. Example:
[PER Wol ] , currently a journalist in
[LOC Argentina ] , played with [PER
Del Bosque ] in the nal years of the
seventies in [ORG Real Madrid ] .
This sentence contains four named entities:
Wol and Del Bosque are persons, Argentina
is a location and Real Madrid is a organiza-
tion. The shared task of CoNLL-2002 concerns
language-independent named entity recogni-
tion. We will concentrate on four types of
named entities: persons, locations, organiza-
tions and names of miscellaneous entities that
do not belong to the previous three groups. The
participants of the shared task have been oered
training and test data for two European lan-
guages: Spanish and Dutch. They have used the
data for developing a named-entity recognition
system that includes a machine learning compo-
nent. The organizers of the shared task were es-
pecially interested in approaches that make use
of additional nonannotated data for improving
their performance.
2 Data and Evaluation
The CoNLL-2002 named entity data consists of
six les covering two languages: Spanish and
Dutch
1
. Each of the languages has a training
le, a development le and a test le. The
learning methods will be trained with the train-
ing data. The development data can be used
for tuning the parameters of the learning meth-
ods. When the best parameters are found, the
method can be trained on the training data and
tested on the test data. The results of the dif-
ferent learning methods on the test sets will be
compared in the evaluation of the shared task.
The split between development data and test
data has been chosen to avoid that systems are
being tuned to the test data.
All data les contain one word per line with
empty lines representing sentence boundaries.
Additionally each line contains a tag which
states whether the word is inside a named entity
or not. The tag also encodes the type of named
entity. Here is a part of the example sentence:
Wol B-PER
, O
currently O
a O
journalist O
in O
Argentina B-LOC
, O
played O
with O
Del B-PER
Bosque I-PER
Words tagged with O are outside of named
entities. The B-XXX tag is used for the rst
word in a named entity of type XXX and I-
XXX is used for all other words in named en-
tities of type XXX. The data contains enti-
1
The data les are available from http://lcg-www.
uia.ac.be/conll2002/ner/
ties of four types: persons (PER), organiza-
tions (ORG), locations (LOC) and miscella-
neous names (MISC). The tagging scheme is a
variant of the IOB scheme originally put for-
ward by Ramshaw and Marcus (1995). We as-
sume that named entities are non-recursive and
non-overlapping. In case a named entity is em-
bedded in another named entity usually only
the top level entity will be marked.
The Spanish data is a collection of news wire
articles made available by the Spanish EFE
News Agency. The articles are from May 2000.
The annotation was carried out by the TALP
Research Center
2
of the Technical University of
Catalonia (UPC) and the Center of Language
and Computation (CLiC
3
) of the University of
Barcelona (UB), and funded by the European
Commission through the NAMIC project (IST-
1999-12392). The data contains words and en-
tity tags only. The training, development and
test data les contain 273037, 54837 and 53049
lines respectively.
The Dutch data consist of four editions of the
Belgian newspaper "De Morgen" of 2000 (June
2, July 1, August 1 and September 1). The data
was annotated as a part of the Atranos project
4
at the University of Antwerp in Belgium, Eu-
rope. The annotator has followed the MITRE
and SAIC guidelines for named entity recogni-
tion (Chinchor et al, 1999) as well as possible.
The data consists of words, entity tags and part-
of-speech tags which have been derived by a
Dutch part-of-speech tagger (Daelemans et al,
1996). Additionally the article boundaries in
the text have been marked explicitly with lines
containing the tag -DOCSTART-. The training,
development and test data les contain 218737,
40656 and 74189 lines respectively.
The performance in this task is mea-
sured with F
=1
rate which is equal to
(
2
+1)*precision*recall / (
2
*precision+recall)
with =1 (van Rijsbergen, 1975). Precision is
the percentage of named entities found by the
learning system that are correct. Recall is the
percentage of named entities present in the cor-
pus that are found by the system. A named
entity is correct only if it is an exact match of
the corresponding entity in the data le.
2
http://www.talp.upc.es/
3
http://clic.l.ub.es/
4
http://atranos.esat.kuleuven.ac.be/
3 Results
Twelve systems have participated in this shared
task. The results for the test sets for Spanish
and Dutch can be found in Table 1. A baseline
rate was computed for both sets. It was pro-
duced by a system which only identied entities
which had a unique class in the training data. If
a phrase was part of more than one entity, the
system would select the longest one. All sys-
tems that participated in the shared task have
outperformed the baseline system.
McNamee and Mayeld (2002) have applied
support vector machines to the data of the
shared task. Their system used many binary
features for representing words (almost 9000).
They have evaluated dierent parameter set-
tings of the system and have selected a cascaded
approach in which rst entity boundaries were
predicted and then entity classes (Spanish test
set: F
=1
=60.97; Dutch test set: F
=1
=59.52).
Black and Vasilakopoulos (2002) have evalu-
ated two approaches to the shared task. The
rst was a transformation-based method which
generated in rules in a single pass rather than
in many passes. The second method was a
decision tree method. They found that the
transformation-based method consistently out-
performed the decision trees (Spanish test set:
F
=1
=67.49; Dutch test set: F
=1
=56.43)
Tsukamoto, Mitsuishi and Sassano (2002)
used a stacked AdaBoost classier for nding
named entities. They found that cascading clas-
siers helped improved performance. Their -
nal system consisted of a cascade of ve learners
each of which performed 10,000 boosting rounds
(Spanish test set: F
=1
=71.49; Dutch test set:
F
=1
=60.93)
Malouf (2002) tested dierent models with
the shared task data: a statistical baseline
model, a Hidden Markov Model and maximum
entropy models with dierent features. The lat-
ter proved to perform best. The maximum en-
tropy models beneted from extra feature which
encoded capitalization information, positional
information and information about the current
word being part of a person name earlier in
the text. However, incorporating a list of per-
son names in the training process did not help
(Spanish test set: F
=1
=73.66; Dutch test set:
F
=1
=68.08)
Jansche (2002) employed a rst-order Markov
model as a named entity recognizer. His system
used two separate passes, one for extracting en-
tity boundaries and one for classifying entities.
He evaluated dierent features in both subpro-
cesses. The categorization process was trained
separately from the extraction process but that
did not seem to have harmed overall perfor-
mance (Spanish test set: F
=1
=73.89; Dutch
test set: F
=1
=69.68)
Patrick, Whitelaw and Munro (2002) present
SLINERC, a language-independent named en-
tity recognizer. The system uses tries as well
as character n-grams for encoding word-internal
and contextual information. Additionally, it re-
lies on lists of entities which have been com-
piled from the training data. The overall sys-
tem consists of six stages, three regarding entity
recognition and three for entity categorization.
Stages use the output of previous stages for ob-
taining an improved performance (Spanish test
set: F
=1
=73.92; Dutch test set: F
=1
=71.36)
Tjong Kim Sang (2002) has applied a
memory-based learner to the data of the shared
task. He used a two-stage processing strategy
as well: rst identifying entities and then clas-
sifying them. Apart from the base classier,
his system made use of three extra techniques
for boosting performance: cascading classiers
(stacking), feature selection and system combi-
nation. Each of these techniques were shown to
be useful (Spanish test set: F
=1
=75.78; Dutch
test set: F
=1
=70.67).
Burger, Henderson and Morgan (2002) have
evaluated three approaches to nding named
entities. They started with a baseline system
which consisted of an HMM-based phrase tag-
ger. They gave the tagger access to a list
of approximately 250,000 named entities and
the performance improved. After this several
smoothed word classes derived from the avail-
able data were incorporated into the training
process. The system performed better with the
derived word lists than with the external named
entity lists (Spanish test set: F
=1
=75.78;
Dutch test set: F
=1
=72.57).
Cucerzan and Yarowsky (2002) approached
the shared task by using word-internal and con-
textual information stored in character-based
tries. Their system obtained good results by us-
ing part-of-speech tag information and employ-
ing the one sense per discourse principle. The
authors expect a performance increase when the
system has access to external entity lists but
have not presented the results of this in detail
(Spanish test set: F
=1
=77.15; Dutch test set:
F
=1
=72.31).
Wu, Ngai, Carpuat, Larsen and Yang (2002)
have applied AdaBoost.MH to the shared
task data and compared the performance with
that of a maximum entropy-based named
entity tagger. Their system used lexical
and part-of-speech information, contextual and
word-internal clues, capitalization information,
knowledge about entity classes of previous oc-
currences of words and a small external list
of named entity words. The boosting tech-
niques operated on decision stumps, decision
trees of depth one. They outperformed the
maximum entropy-based named entity tagger
(Spanish test set: F
=1
=76.61; Dutch test set:
F
=1
=75.36).
Florian (2002) employed three stacked
learners for named entity recognition:
transformation-based learning for obtain-
ing base-level non-typed named entities, Snow
for improving the quality of these entities
and the forward-backward algorithm for nd-
ing categories for the named entities. The
combination of the three algorithms showed
a substantially improved performance when
compared with a single algorithm and an
algorithm pair (Spanish test set: F
=1
=79.05;
Dutch test set: F
=1
=74.99).
Carreras, Marquez and Padro (2002) have ap-
proached the shared task by using AdaBoost
applied to xed-depth decision trees. Their sys-
tem used many dierent input features contex-
tual information, word-internal clues, previous
entity classes, part-of-speech tags (Dutch only)
and external word lists (Spanish only). It pro-
cessed the data in two stages: rst entity recog-
nition and then classication. Their system
obtained the best results in this shared task
for both the Spanish and Dutch test data sets
(Spanish test set: F
=1
=81.39; Dutch test set:
F
=1
=77.05).
4 Concluding Remarks
We have described the CoNLL-2002 shared
task: language-independent named entity
recognition. Twelve dierent systems have been
applied to data covering two Western European
languages: Spanish and Dutch. A boosted deci-
sion tree method obtained the best performance
on both data sets (Carreras et al, 2002).
Acknowledgements
Tjong Kim Sang is supported by IWT STWW
as a researcher in the ATRANOS project.
References
William J. Black and Argyrios Vasilakopoulos. 2002.
Language-independent named entity classication
by modied transformation-based learning and by
decision tree induction. In Proceedings of CoNLL-
2002. Taipei, Taiwan.
John D. Burger, John C. Henderson, and William T.
Morgan. 2002. Statistical named entity recog-
nizer adaptation. In Proceedings of CoNLL-2002.
Taipei, Taiwan.
Xavier Carreras, Llus Marques, and Llus Padro.
2002. Named entity extraction using adaboost.
In Proceedings of CoNLL-2002. Taipei, Taiwan.
Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty
Robinson. 1999. 1999 Named Entity Recognition
Task Denition. MITRE and SAIC.
Silviu Cucerzan and David Yarowsky. 2002. Lan-
guage independent ner using a unied model of
internal and contextual evidence. In Proceedings
of CoNLL-2002. Taipei, Taiwan.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. Mbt: A memory-based part
of speech tagger-generator. In Proceedings of the
Fourth Workshop on Very Large Corpora, pages
14{27. Copenhagen, Denmark.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classier stacking. In Proceedings
of CoNLL-2002. Taipei, Taiwan.
Martin Jansche. 2002. Named entity extraction
with conditional markov models and classiers.
In Proceedings of CoNLL-2002. Taipei, Taiwan.
Robert Malouf. 2002. Markov models for language-
independent named entity recognition. In Pro-
ceedings of CoNLL-2002. Taipei, Taiwan.
Paul McNamee and James Mayeld. 2002. Entity
extraction without language-specic resources. In
Proceedings of CoNLL-2002. Taipei, Taiwan.
Jon Patrick, Casey Whitelaw, and Robert Munro.
2002. Slinerc: The sydney language-independent
named entity recogniser and classier. In Proceed-
ings of CoNLL-2002. Taipei, Taiwan.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learn-
ing. In Proceedings of the Third ACL Workshop
on Very Large Corpora, pages 82{94. Cambridge,
MA, USA.
Erik F. Tjong Kim Sang. 2002. Memory-based
Spanish test precision recall F
=1
Carreras et.al. 81.38% 81.40% 81.39
Florian 78.70% 79.40% 79.05
Cucerzan et.al. 78.19% 76.14% 77.15
Wu et.al. 75.85% 77.38% 76.61
Burger et.al. 74.19% 77.44% 75.78
Tjong Kim Sang 76.00% 75.55% 75.78
Patrick et.al. 74.32% 73.52% 73.92
Jansche 74.03% 73.76% 73.89
Malouf 73.93% 73.39% 73.66
Tsukamoto 69.04% 74.12% 71.49
Black et.al. 68.78% 66.24% 67.49
McNamee et.al. 56.28% 66.51% 60.97
baseline
5
26.27% 56.48% 35.86
Dutch test precision recall F
=1
Carreras et.al. 77.83% 76.29% 77.05
Wu et.al. 76.95% 73.83% 75.36
Florian 75.10% 74.89% 74.99
Burger et.al. 72.69% 72.45% 72.57
Cucerzan et.al. 73.03% 71.62% 72.31
Patrick et.al. 74.01% 68.90% 71.36
Tjong Kim Sang 72.56% 68.88% 70.67
Jansche 70.11% 69.26% 69.68
Malouf 70.88% 65.50% 68.08
Tsukamoto 57.33% 65.02% 60.93
McNamee et.al. 56.22% 63.24% 59.52
Black et.al. 62.12% 51.69% 56.43
baseline
5
64.38% 45.19% 53.10
Table 1: Overall precision, recall and F
=1
rates
obtained by the twelve participating systems on
the test data sets for the two languages in the
CoNLL-2002 shared task.
named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu
Sassano. 2002. Learning with multiple stacking
for named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
C.J. van Rijsbergen. 1975. Information Retrieval.
Buttersworth.
Dekai Wu, Grace Ngai, Marine Carpuat, Jeppe
Larsen, and Yongsheng Yang. 2002. Boosting
for named entity recognition. In Proceedings of
CoNLL-2002. Taipei, Taiwan.
5
Due to some harmful annotation errors in the train-
ing data, the baseline system performs less well than
expected. Without the errors, the baseline F
=1
rates
would have been 62.49 for Spanish and 57.59 for Dutch.
  	
ffIntroduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition
Erik F. Tjong Kim Sang and Fien De Meulder
CNTS - Language Technology Group
University of Antwerp
{erikt,fien.demeulder}@uia.ua.ac.be
Abstract
We describe the CoNLL-2003 shared task:
language-independent named entity recog-
nition. We give background information on
the data sets (English and German) and
the evaluation method, present a general
overview of the systems that have taken
part in the task and discuss their perfor-
mance.
1 Introduction
Named entities are phrases that contain the names
of persons, organizations and locations. Example:
[ORG U.N. ] official [PER Ekeus ] heads for
[LOC Baghdad ] .
This sentence contains three named entities: Ekeus
is a person, U.N. is a organization and Baghdad is
a location. Named entity recognition is an impor-
tant task of information extraction systems. There
has been a lot of work on named entity recognition,
especially for English (see Borthwick (1999) for an
overview). The Message Understanding Conferences
(MUC) have offered developers the opportunity to
evaluate systems for English on the same data in a
competition. They have also produced a scheme for
entity annotation (Chinchor et al, 1999). More re-
cently, there have been other system development
competitions which dealt with different languages
(IREX and CoNLL-2002).
The shared task of CoNLL-2003 concerns
language-independent named entity recognition. We
will concentrate on four types of named entities:
persons, locations, organizations and names of
miscellaneous entities that do not belong to the pre-
vious three groups. The shared task of CoNLL-2002
dealt with named entity recognition for Spanish and
Dutch (Tjong Kim Sang, 2002). The participants
of the 2003 shared task have been offered training
and test data for two other European languages:
English and German. They have used the data
for developing a named-entity recognition system
that includes a machine learning component. The
shared task organizers were especially interested in
approaches that made use of resources other than
the supplied training data, for example gazetteers
and unannotated data.
2 Data and Evaluation
In this section we discuss the sources of the data
that were used in this shared task, the preprocessing
steps we have performed on the data, the format of
the data and the method that was used for evaluating
the participating systems.
2.1 Data
The CoNLL-2003 named entity data consists of eight
files covering two languages: English and German1.
For each of the languages there is a training file, a de-
velopment file, a test file and a large file with unanno-
tated data. The learning methods were trained with
the training data. The development data could be
used for tuning the parameters of the learning meth-
ods. The challenge of this year?s shared task was
to incorporate the unannotated data in the learning
process in one way or another. When the best pa-
rameters were found, the method could be trained on
the training data and tested on the test data. The
results of the different learning methods on the test
sets are compared in the evaluation of the shared
task. The split between development data and test
data was chosen to avoid systems being tuned to the
test data.
The English data was taken from the Reuters Cor-
pus2. This corpus consists of Reuters news stories
1Data files (except the words) can be found on
http://lcg-www.uia.ac.be/conll2003/ner/
2http://www.reuters.com/researchandstandards/
English data Articles Sentences Tokens
Training set 946 14,987 203,621
Development set 216 3,466 51,362
Test set 231 3,684 46,435
German data Articles Sentences Tokens
Training set 553 12,705 206,931
Development set 201 3,068 51,444
Test set 155 3,160 51,943
Table 1: Number of articles, sentences and tokens in
each data file.
between August 1996 and August 1997. For the
training and development set, ten days? worth of data
were taken from the files representing the end of Au-
gust 1996. For the test set, the texts were from De-
cember 1996. The preprocessed raw data covers the
month of September 1996.
The text for the German data was taken from the
ECI Multilingual Text Corpus3. This corpus consists
of texts in many languages. The portion of data that
was used for this task, was extracted from the Ger-
man newspaper Frankfurter Rundshau. All three of
the training, development and test sets were taken
from articles written in one week at the end of Au-
gust 1992. The raw data were taken from the months
of September to December 1992.
Table 1 contains an overview of the sizes of the
data files. The unannotated data contain 17 million
tokens (English) and 14 million tokens (German).
2.2 Data preprocessing
The participants were given access to the corpus af-
ter some linguistic preprocessing had been done: for
all data, a tokenizer, part-of-speech tagger, and a
chunker were applied to the raw data. We created
two basic language-specific tokenizers for this shared
task. The English data was tagged and chunked by
the memory-based MBT tagger (Daelemans et al,
2002). The German data was lemmatized, tagged
and chunked by the decision tree tagger Treetagger
(Schmid, 1995).
Named entity tagging of English and German
training, development, and test data, was done by
hand at the University of Antwerp. Mostly, MUC
conventions were followed (Chinchor et al, 1999).
An extra named entity category called MISC was
added to denote all names which are not already in
the other categories. This includes adjectives, like
Italian, and events, like 1000 Lakes Rally, making it
a very diverse category.
3http://www.ldc.upenn.edu/
English data LOC MISC ORG PER
Training set 7140 3438 6321 6600
Development set 1837 922 1341 1842
Test set 1668 702 1661 1617
German data LOC MISC ORG PER
Training set 4363 2288 2427 2773
Development set 1181 1010 1241 1401
Test set 1035 670 773 1195
Table 2: Number of named entities per data file
2.3 Data format
All data files contain one word per line with empty
lines representing sentence boundaries. At the end
of each line there is a tag which states whether the
current word is inside a named entity or not. The
tag also encodes the type of named entity. Here is
an example sentence:
U.N. NNP I-NP I-ORG
official NN I-NP O
Ekeus NNP I-NP I-PER
heads VBZ I-VP O
for IN I-PP O
Baghdad NNP I-NP I-LOC
. . O O
Each line contains four fields: the word, its part-
of-speech tag, its chunk tag and its named entity
tag. Words tagged with O are outside of named en-
tities and the I-XXX tag is used for words inside a
named entity of type XXX. Whenever two entities of
type XXX are immediately next to each other, the
first word of the second entity will be tagged B-XXX
in order to show that it starts another entity. The
data contains entities of four types: persons (PER),
organizations (ORG), locations (LOC) and miscel-
laneous names (MISC). This tagging scheme is the
IOB scheme originally put forward by Ramshaw and
Marcus (1995). We assume that named entities are
non-recursive and non-overlapping. When a named
entity is embedded in another named entity, usually
only the top level entity has been annotated.
Table 2 contains an overview of the number of
named entities in each data file.
2.4 Evaluation
The performance in this task is measured with F?=1
rate:
F? =
(?2 + 1) ? precision ? recall
(?2 ? precision + recall)
(1)
lex pos aff pre ort gaz chu pat cas tri bag quo doc
Florian + + + + + + + - + - - - -
Chieu + + + + + + - - - + - + +
Klein + + + + - - - - - - - - -
Zhang + + + + + + + - - + - - -
Carreras (a) + + + + + + + + - + + - -
Curran + + + + + + - + + - - - -
Mayfield + + + + + - + + - - - + -
Carreras (b) + + + + + - - + - - - - -
McCallum + - - - + + - + - - - - -
Bender + + - + + + + - - - - - -
Munro + + + - - - + - + + + - -
Wu + + + + + + - - - - - - -
Whitelaw - - + + - - - - + - - - -
Hendrickx + + + + + + + - - - - - -
De Meulder + + + - + + + - + - - - -
Hammerton + + - - - + + - - - - - -
Table 3: Main features used by the the sixteen systems that participated in the CoNLL-2003 shared task
sorted by performance on the English test data. Aff: affix information (n-grams); bag: bag of words; cas:
global case information; chu: chunk tags; doc: global document information; gaz: gazetteers; lex: lexical
features; ort: orthographic information; pat: orthographic patterns (like Aa0); pos: part-of-speech tags; pre:
previously predicted NE tags; quo: flag signing that the word is between quotes; tri: trigger words.
with ?=1 (Van Rijsbergen, 1975). Precision is the
percentage of named entities found by the learning
system that are correct. Recall is the percentage of
named entities present in the corpus that are found
by the system. A named entity is correct only if it
is an exact match of the corresponding entity in the
data file.
3 Participating Systems
Sixteen systems have participated in the CoNLL-
2003 shared task. They employed a wide variety of
machine learning techniques as well as system com-
bination. Most of the participants have attempted
to use information other than the available train-
ing data. This information included gazetteers and
unannotated data, and there was one participant
who used the output of externally trained named en-
tity recognition systems.
3.1 Learning techniques
The most frequently applied technique in the
CoNLL-2003 shared task is the Maximum Entropy
Model. Five systems used this statistical learning
method. Three systems used Maximum Entropy
Models in isolation (Bender et al, 2003; Chieu and
Ng, 2003; Curran and Clark, 2003). Two more
systems used them in combination with other tech-
niques (Florian et al, 2003; Klein et al, 2003). Max-
imum Entropy Models seem to be a good choice for
this kind of task: the top three results for English
and the top two results for German were obtained
by participants who employed them in one way or
another.
Hidden Markov Models were employed by four of
the systems that took part in the shared task (Flo-
rian et al, 2003; Klein et al, 2003; Mayfield et al,
2003; Whitelaw and Patrick, 2003). However, they
were always used in combination with other learning
techniques. Klein et al (2003) also applied the re-
lated Conditional Markov Models for combining clas-
sifiers.
Learning methods that were based on connection-
ist approaches were applied by four systems. Zhang
and Johnson (2003) used robust risk minimization,
which is a Winnow technique. Florian et al (2003)
employed the same technique in a combination of
learners. Voted perceptrons were applied to the
shared task data by Carreras et al (2003a) and
Hammerton used a recurrent neural network (Long
Short-Term Memory) for finding named entities.
Other learning approaches were employed less fre-
quently. Two teams used AdaBoost.MH (Carreras
et al, 2003b; Wu et al, 2003) and two other groups
employed memory-based learning (De Meulder and
Daelemans, 2003; Hendrickx and Van den Bosch,
2003). Transformation-based learning (Florian et
al., 2003), Support Vector Machines (Mayfield et al,
2003) and Conditional Random Fields (McCallum
and Li, 2003) were applied by one system each.
Combination of different learning systems has
proven to be a good method for obtaining excellent
results. Five participating groups have applied sys-
tem combination. Florian et al (2003) tested dif-
ferent methods for combining the results of four sys-
tems and found that robust risk minimization worked
best. Klein et al (2003) employed a stacked learn-
ing system which contains Hidden Markov Models,
Maximum Entropy Models and Conditional Markov
Models. Mayfield et al (2003) stacked two learners
and obtained better performance. Wu et al (2003)
applied both stacking and voting to three learners.
Munro et al (2003) employed both voting and bag-
ging for combining classifiers.
3.2 Features
The choice of the learning approach is important for
obtaining a good system for recognizing named en-
tities. However, in the CoNLL-2002 shared task we
found out that choice of features is at least as impor-
tant. An overview of some of the types of features
chosen by the shared task participants, can be found
in Table 3.
All participants used lexical features (words) ex-
cept for Whitelaw and Patrick (2003) who imple-
mented a character-based method. Most of the sys-
tems employed part-of-speech tags and two of them
have recomputed the English tags with better tag-
gers (Hendrickx and Van den Bosch, 2003; Wu et al,
2003). Othographic information, affixes, gazetteers
and chunk information were also incorporated in
most systems although one group reports that the
available chunking information did not help (Wu et
al., 2003) Other features were used less frequently.
Table 3 does not reveal a single feature that would
be ideal for named entity recognition.
3.3 External resources
Eleven of the sixteen participating teams have at-
tempted to use information other than the training
data that was supplied for this shared task. All in-
cluded gazetteers in their systems. Four groups ex-
amined the usability of unannotated data, either for
extracting training instances (Bender et al, 2003;
Hendrickx and Van den Bosch, 2003) or obtaining
extra named entities for gazetteers (De Meulder and
Daelemans, 2003; McCallum and Li, 2003). A rea-
sonable number of groups have also employed unan-
notated data for obtaining capitalization features for
words. One participating team has used externally
trained named entity recognition systems for English
as a part in a combined system (Florian et al, 2003).
Table 4 shows the error reduction of the systems
G U E English German
Zhang + - - 19% 15%
Florian + - + 27% 5%
Chieu + - - 17% 7%
Hammerton + - - 22% -
Carreras (a) + - - 12% 8%
Hendrickx + + - 7% 5%
De Meulder + + - 8% 3%
Bender + + - 3% 6%
Curran + - - 1% -
McCallum + + - ? ?
Wu + - - ? ?
Table 4: Error reduction for the two develop-
ment data sets when using extra information like
gazetteers (G), unannotated data (U) or externally
developed named entity recognizers (E). The lines
have been sorted by the sum of the reduction per-
centages for the two languages.
with extra information compared to while using only
the available training data. The inclusion of ex-
tra named entity recognition systems seems to have
worked well (Florian et al, 2003). Generally the sys-
tems that only used gazetteers seem to gain more
than systems that have used unannotated data for
other purposes than obtaining capitalization infor-
mation. However, the gain differences between the
two approaches are most obvious for English for
which better gazetteers are available. With the ex-
ception of the result of Zhang and Johnson (2003),
there is not much difference in the German results
between the gains obtained by using gazetteers and
those obtained by using unannotated data.
3.4 Performances
A baseline rate was computed for the English and the
German test sets. It was produced by a system which
only identified entities which had a unique class in
the training data. If a phrase was part of more than
one entity, the system would select the longest one.
All systems that participated in the shared task have
outperformed the baseline system.
For all the F?=1 rates we have estimated sig-
nificance boundaries by using bootstrap resampling
(Noreen, 1989). From each output file of a system,
250 random samples of sentences have been chosen
and the distribution of the F?=1 rates in these sam-
ples is assumed to be the distribution of the perfor-
mance of the system. We assume that performance
A is significantly different from performance B if A
is not within the center 90% of the distribution of B.
The performances of the sixteen systems on the
two test data sets can be found in Table 5. For En-
glish, the combined classifier of Florian et al (2003)
achieved the highest overall F?=1 rate. However,
the difference between their performance and that
of the Maximum Entropy approach of Chieu and Ng
(2003) is not significant. An important feature of the
best system that other participants did not use, was
the inclusion of the output of two externally trained
named entity recognizers in the combination process.
Florian et al (2003) have also obtained the highest
F?=1 rate for the German data. Here there is no sig-
nificant difference between them and the systems of
Klein et al (2003) and Zhang and Johnson (2003).
We have combined the results of the sixteen sys-
tem in order to see if there was room for improve-
ment. We converted the output of the systems to
the same IOB tagging representation and searched
for the set of systems from which the best tags for
the development data could be obtained with ma-
jority voting. The optimal set of systems was de-
termined by performing a bidirectional hill-climbing
search (Caruana and Freitag, 1994) with beam size 9,
starting from zero features. A majority vote of five
systems (Chieu and Ng, 2003; Florian et al, 2003;
Klein et al, 2003; McCallum and Li, 2003; Whitelaw
and Patrick, 2003) performed best on the English
development data. Another combination of five sys-
tems (Carreras et al, 2003b; Mayfield et al, 2003;
McCallum and Li, 2003; Munro et al, 2003; Zhang
and Johnson, 2003) obtained the best result for the
German development data. We have performed a
majority vote with these sets of systems on the re-
lated test sets and obtained F?=1 rates of 90.30 for
English (14% error reduction compared with the best
system) and 74.17 for German (6% error reduction).
4 Concluding Remarks
We have described the CoNLL-2003 shared task:
language-independent named entity recognition.
Sixteen systems have processed English and German
named entity data. The best performance for both
languages has been obtained by a combined learn-
ing system that used Maximum Entropy Models,
transformation-based learning, Hidden Markov Mod-
els as well as robust risk minimization (Florian et al,
2003). Apart from the training data, this system also
employed gazetteers and the output of two externally
trained named entity recognizers. The performance
of the system of Chieu et al (2003) was not signif-
icantly different from the best performance for En-
glish and the method of Klein et al (2003) and the
approach of Zhang and Johnson (2003) were not sig-
nificantly worse than the best result for German.
Eleven teams have incorporated information other
than the training data in their system. Four of them
have obtained error reductions of 15% or more for
English and one has managed this for German. The
resources used by these systems, gazetteers and ex-
ternally trained named entity systems, still require a
lot of manual work. Systems that employed unanno-
tated data, obtained performance gains around 5%.
The search for an excellent method for taking advan-
tage of the fast amount of available raw text, remains
open.
Acknowledgements
Tjong Kim Sang is financed by IWT STWW as a
researcher in the ATraNoS project. De Meulder is
supported by a BOF grant supplied by the University
of Antwerp.
References
Oliver Bender, Franz Josef Och, and Hermann Ney.
2003. Maximum Entropy Models for Named En-
tity Recognition. In Proceedings of CoNLL-2003.
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. PhD thesis,
New York University.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2003a. Learning a Perceptron-Based Named En-
tity Chunker via Online Recognition Feedback. In
Proceedings of CoNLL-2003.
Xavier Carreras, Llu??s Ma`rquez, and Llu??s Padro?.
2003b. A Simple Named Entity Extractor using
AdaBoost. In Proceedings of CoNLL-2003.
Rich Caruana and Dayne Freitag. 1994. Greedy At-
tribute Selection. In Proceedings of the Eleventh
International Conference on Machine Learning,
pages 28?36. New Brunswick, NJ, USA, Morgan
Kaufman.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named
Entity Recognition with a Maximum Entropy Ap-
proach. In Proceedings of CoNLL-2003.
Nancy Chinchor, Erica Brown, Lisa Ferro, and Patty
Robinson. 1999. 1999 Named Entity Recognition
Task Definition. MITRE and SAIC.
James R. Curran and Stephen Clark. 2003. Lan-
guage Independent NER using a Maximum En-
tropy Tagger. In Proceedings of CoNLL-2003.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2002. MBT: Memory-
Based Tagger, version 1.0, Reference Guide. ILK
Technical Report ILK-0209, University of Tilburg,
The Netherlands.
Fien De Meulder and Walter Daelemans. 2003.
Memory-Based Named Entity Recognition using
Unannotated Data. In Proceedings of CoNLL-
2003.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and
Tong Zhang. 2003. Named Entity Recognition
through Classifier Combination. In Proceedings of
CoNLL-2003.
James Hammerton. 2003. Named Entity Recogni-
tion with Long Short-Term Memory. In Proceed-
ings of CoNLL-2003.
Iris Hendrickx and Antal van den Bosch. 2003.
Memory-based one-step named-entity recognition:
Effects of seed list features, classifier stacking, and
unannotated data. In Proceedings of CoNLL-2003.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christo-
pher D. Manning. 2003. Named Entity Recogni-
tion with Character-Level Models. In Proceedings
of CoNLL-2003.
James Mayfield, Paul McNamee, and Christine Pi-
atko. 2003. Named Entity Recognition using Hun-
dreds of Thousands of Features. In Proceedings of
CoNLL-2003.
Andrew McCallum and Wei Li. 2003. Early results
for Named Entity Recognition with Conditional
Random Fields, Feature Induction and Web-
Enhanced Lexicons. In Proceedings of CoNLL-
2003.
Robert Munro, Daren Ler, and Jon Patrick.
2003. Meta-Learning Orthographic and Contex-
tual Models for Language Independent Named En-
tity Recognition. In Proceedings of CoNLL-2003.
Eric W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses. John Wiley & Sons.
Lance A. Ramshaw and Mitchell P. Marcus.
1995. Text Chunking Using Transformation-Based
Learning. In Proceedings of the Third ACL Work-
shop on Very Large Corpora, pages 82?94. Cam-
bridge, MA, USA.
Helmut Schmid. 1995. Improvements in Part-of-
Speech Tagging with an Application to German.
In Proceedings of EACL-SIGDAT 1995. Dublin,
Ireland.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of
CoNLL-2002, pages 155?158. Taipei, Taiwan.
C.J. van Rijsbergen. 1975. Information Retrieval.
Buttersworth.
English test Precision Recall F?=1
Florian 88.99% 88.54% 88.76?0.7
Chieu 88.12% 88.51% 88.31?0.7
Klein 85.93% 86.21% 86.07?0.8
Zhang 86.13% 84.88% 85.50?0.9
Carreras (a) 84.05% 85.96% 85.00?0.8
Curran 84.29% 85.50% 84.89?0.9
Mayfield 84.45% 84.90% 84.67?1.0
Carreras (b) 85.81% 82.84% 84.30?0.9
McCallum 84.52% 83.55% 84.04?0.9
Bender 84.68% 83.18% 83.92?1.0
Munro 80.87% 84.21% 82.50?1.0
Wu 82.02% 81.39% 81.70?0.9
Whitelaw 81.60% 78.05% 79.78?1.0
Hendrickx 76.33% 80.17% 78.20?1.0
De Meulder 75.84% 78.13% 76.97?1.2
Hammerton 69.09% 53.26% 60.15?1.3
Baseline 71.91% 50.90% 59.61?1.2
German test Precision Recall F?=1
Florian 83.87% 63.71% 72.41?1.3
Klein 80.38% 65.04% 71.90?1.2
Zhang 82.00% 63.03% 71.27?1.5
Mayfield 75.97% 64.82% 69.96?1.4
Carreras (a) 75.47% 63.82% 69.15?1.3
Bender 74.82% 63.82% 68.88?1.3
Curran 75.61% 62.46% 68.41?1.4
McCallum 75.97% 61.72% 68.11?1.4
Munro 69.37% 66.21% 67.75?1.4
Carreras (b) 77.83% 58.02% 66.48?1.5
Wu 75.20% 59.35% 66.34?1.3
Chieu 76.83% 57.34% 65.67?1.4
Hendrickx 71.15% 56.55% 63.02?1.4
De Meulder 63.93% 51.86% 57.27?1.6
Whitelaw 71.05% 44.11% 54.43?1.4
Hammerton 63.49% 38.25% 47.74?1.5
Baseline 31.86% 28.89% 30.30?1.3
Table 5: Overall precision, recall and F?=1 rates ob-
tained by the sixteen participating systems on the
test data sets for the two languages in the CoNLL-
2003 shared task.
Casey Whitelaw and Jon Patrick. 2003. Named En-
tity Recognition Using a Character-based Proba-
bilistic Approach. In Proceedings of CoNLL-2003.
Dekai Wu, Grace Ngai, and Marine Carpuat. 2003.
A Stacked, Voted, Stacked Model for Named En-
tity Recognition. In Proceedings of CoNLL-2003.
Tong Zhang and David Johnson. 2003. A Robust
Risk Minimization based Named Entity Recogni-
tion System. In Proceedings of CoNLL-2003.
Memory-based semantic role labeling:
Optimizing features, algorithm, and output
Antal van den Bosch, Sander Canisius,
Iris Hendrickx
ILK / Computational Linguistics
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{Antal.vdnBosch,S.V.M.Canisius,
I.H.E.Hendrickx}@uvt.nl
Walter Daelemans,
Erik Tjong Kim Sang
CNTS / Department of Linguistics
University of Antwerp, Universiteitsplein 1,
B-2610 Antwerpen, Belgium
{Walter.Daelemans,
Erik.TjongKimSang}@ua.ac.be
1 Introduction
In this paper we interpret the semantic role labeling prob-
lem as a classification task, and apply memory-based
learning to it in an approach similar to Buchholz et al
(1999) and Buchholz (2002) for grammatical relation la-
beling. We apply feature selection and algorithm parame-
ter optimization strategies to our learner. In addition, we
investigate the effect of two innovations: (i) the use of
sequences of classes as classification output, combined
with a simple voting mechanism, and (ii) the use of iter-
ative classifier stacking which takes as input the original
features and a pattern of outputs of a first-stage classifier.
Our claim is that both methods avoid errors in sequences
of predictions typically made by simple classifiers that
are unaware of their previous or subsequent decisions in
a sequence.
2 Data and Features
The CoNLL-2004 shared task (Carreras and Ma`rquez,
2004) supplied data sets for the semantic role labeling
task with several levels of annotation apart from the role
labels to be predicted. Central to our approach is the
choice to adopt the instance encoding analogous to Buch-
holz et al (1999) to have our examples represent relations
between pairs of verbs and chunks. That is, we transform
the semantic role labeling task to a classification task in
which we decide for all pairs of verbs and chunks whether
they stand in a semantic role relation. Afterwards we con-
sider all adjacent chunks to which the same role label is
assigned by our classifier as belonging to the same argu-
ment. All results reported below use this task representa-
tion. Processing focuses on one verb at a time; verbs are
treated independently.
We did not employ the provided Propbank data nor
the verb sense information available, nor did we use any
other external source of information.
Apart from the provided words and the predicted PoS
tags, chunk labels, clause labels, and named-entity labels,
provided beforehand, we have considered an additional
set of automatically derived features:
1. attenuated words (Eisner, 1996), i.e. wordforms
occurring below a frequency threshold (of 10) are
converted to a string capturing some of the original
word form?s features (capitalization, whether it con-
tains numbers or a hyphen, or suffix letters);
2. the distance between the candidate role word and the
verb, measured in intervening words, chunks, NP
chunks or VP chunks (negative if the word is to the
left, positive if it is to the right of the verb);
3. preceding preposition: a feature containing the head
word of the previous chunk if that was labeled as
preposition;
4. passive main verb: a binary feature which is on if
the main verb is used in a passive construction;
5. current clause: a binary feature which is on if the
current word is in the same clause as the main verb;
6. role pattern: the most frequently occurring role pat-
tern of the main verb in the training data (contains
the order of V and A0-A5).
For every target verb in every sentence, the data sup-
plied to the learners contains instances for every head
word of non-VP chunks and for all words in VP chunks,
and all words in all chunks containing a target verb (i.e.,
more instances than chunks, to account for the fact that
some roles are contained within chunks). Here is an ex-
ample instance for the second chunk of the training data:
expect -2 -1 0 morph-cap in IN
NN PP passive clause A0VA1 A1
This instance contains 12 features: the verb (1), dis-
tance to the verb measured in chunks (2), NP chunks (3)
and VP chunks (4), attenuated words (5?6), PoS tags (7?
8), a chunk tag (9), passive main verb (10), current clause
(11) and role pattern (12). The final item of the line is
the required output class. Our choice of instance format
is only slightly harmful for performance: with a perfect
classifier we can still obtain a maximal F?=1 score of 99.1
on the development data.
3 Approach
In this section we describe our approach to semantic role
labeling. The core part of our system is a memory-based
learner. During the development of the system we have
used feature selection and parameter optimization by it-
erative deepening. Additionally we have evaluated three
extensions of the basic memory-based learning method:
class n-grams, i.e. complex classes composed of se-
quences of simple classes, iterative classifier stacking and
automatic output post-processing.
3.1 Memory-based learning
Memory-based learning is a supervised inductive algo-
rithm for learning classification tasks based on the k-
nn algorithm (Cover and Hart, 1967; Aha et al, 1991)
with various extensions for dealing with nominal features
and feature relevance weighting. Memory-based learn-
ing stores feature representations of training instances in
memory without abstraction and classifies new (test) in-
stances by matching their feature representation to all in-
stances in memory, finding the most similar instances.
From these ?nearest neighbors?, the class of the test item
is extrapolated. See Daelemans et al (2003) for a de-
tailed description of the algorithms and metrics used in
our experiments. All memory-based learning experi-
ments were done with the TiMBL software package1.
In previous research, we have found that memory-
based learning is rather sensitive to the chosen features
and the particular setting of its algorithmic parameters
(e.g. the number of nearest neighbors taken into account,
the function for extrapolation from the nearest neighbors,
the feature relevance weighting method used, etc.). In or-
der to minimize the effects of this sensitivity, we have put
much effort in trying to find the best set of features and
the optimal learner parameters for this particular task.
3.2 Feature selection
We have employed bi-directional hill-climbing (Caruana
and Freitag, 1994) for finding the features that were most
suited for this task. This wrapper approach starts with the
empty set of features and evaluates the learner for every
individual feature on the development set. The feature
associated with the best performance is selected and the
process is repeated for every pair of features that includes
the best feature. For every next best set of features, the
1We used TiMBL version 5.0, available freely for research
from http://ilk.uvt.nl.
system evaluates each set that contains one extra feature
or has one feature less. This process is repeated until the
local search does not lead to a performance gain.
3.3 Parameter optimization
We used iterative deepening (ID) as a heuristic way of
searching for optimal algorithm parameters. This tech-
nique combines classifier wrapping (using the training
material internally to test experimental variants) (Kohavi
and John, 1997) with progressive sampling of training
material (Provost et al, 1999). We start with a large pool
of experiments, each with a unique combination of algo-
rithmic parameter settings. Each settings combination is
applied to a small amount of training material and tested
on a small held-out set alo taken from the training set.
Only the best settings are kept; the others are removed
from the pool of competing settings. In subsequent itera-
tions, this step is repeated, retaining the best-performing
settings, with an exponentially growing amount of train-
ing and held-out data ? until all training data is used or
one best setting is left. Selecting the best settings at each
step is based on classification accuracy on the held-out
data; a simple one-dimensional clustering on the ranked
list of accuracies determines which group of settings is
selected for the next iteration.
3.4 Class n-grams
Alternative to predicting simple classes, sequential tasks
can be rephrased as mappings from input examples to
sequences of classes. Instead of predicting just A1 in
the example given earlier, it is possible to predict a tri-
gram of classes. The second example in the training data
which we used earlier, is now labeled with the trigram
A1 A1 A1, indicating that the chunk in focus has an A1
relation with the verb, along with its left and right neigh-
bor chunks (which are all part of the same A1 argument).
expect -2 -1 0 morph-cap in
IN NN PP passive clause A0VA1
A1 A1 A1
Predicting class trigrams offers two potential benefits.
First, the classifier is forced to predict ?legal? sequences
of classes; this potentially fixes a problem with simple
classifiers which are blind to their previous or subsequent
simple classifications in sequences, potentially resulting
in impossible sequences such as A1 A0 A1. Second,
if the classifier predicts the trigrams example by exam-
ple, it produces a sequence of overlapping trigrams which
may contain information that can boost classification ac-
curacy. Effectively, each class is predicted three times, so
that a simple majority voting can be applied: we simply
take the middle prediction as the actual classification of
the example unless the two other votes together suggest
another class label.
Prec. Recall F?=1 method
a 51.6% 51.9% 51.8 feature selection
b 57.3% 52.7% 54.9 parameter optimization
c 58.8% 54.2% 56.4 feature selection
d 59.5% 53.9% 56.5 parameter optimization
e 64.3% 54.2% 58.8 classifier stacking
f 66.3% 56.3% 60.9 parameter optimization
g 66.5% 56.3% 60.9 feature selection
h 68.1% 56.8% 61.9 classifier stacking
i 68.3% 57.5% 62.4 feature selection
j 68.9% 57.8% 62.9 classifier stacking
k 69.1% 57.8% 63.0 classifier stacking
50.6% 30.3% 37.9 baseline
Table 1: Effects of cascaded feature selection, parameter
optimization and classifier stacking on the performance
measured on the development data set.
3.5 Iterative classifier stacking
Stacking (Wolpert, 1992) refers to a class of meta-
learning systems that learn to correct errors made by
lower-level classifiers. We implement stacking by adding
a windowed sequence of previous and subsequent output
class labels to the original input features. To generate
the training material, we copy these windowed (unigram)
class labels into the input, excluding the focus class label
(which is a perfect predictor of the output class). To gen-
erate test material, the output of the first-stage classifier
trained on the original data is used.
Stacking can be repeated; an nth-stage classifier can be
built on the output of the n-1th-stage classifier. We im-
plemented this by replacing the class features in the input
of each nth-stage classifier by the output of the previous
classifier.
3.6 Automatic output post-processing
Even while employing n-gram output classes and clas-
sifier stacking, we noticed that our learner made sys-
tematic errors caused by the lack of broader (sentential)
contextual information in the instances and the classes.
The most obvious of these errors was having multiple in-
stances of arguments A0-A5 in one sentence. Although
sentences with multiple A0-A3 arguments appear in the
training data, they are quite rare (0.17%). When the
learner assigns an A0 role to three different arguments
in a sentence, most likely at least two of these are wrong.
In order to reflect this fact, we have restricted the sys-
tem to outputting at most one phrase of type A0-A5. If
the learner predicts multiple arguments then only the one
closest to the main verb is kept.
Features a-b c-d e-f g-h i-k
words -1?0 -2?1 -2?1 -2?1 -2?1
PoS tags 0?1 0?1 0?1 -1?1 -1?1
chunk tags 0 0?2 0?2 -1?1 -1?1
NE tags ? ? ? ? ?
output classes NA NA -3?3 -3?3 -3?3
distances cNV cNVw cNVw Vw cNV
main verb + + + + +
role pattern + + + + +
passive verb + + + + +
current clause + + + + +
previous prep. ? + + + ?
Total 12 18 24 23 24
Table 2: Features used in the different runs mentioned
in Table 1. The numbers mentioned for words, part-of-
speech tags, chunk tags, named entity tags and output
classes show the position of the tokens with respect to
the focus token (0). Distances are measured in chunks,
NP chunks, VP chunks and words. In all other table en-
tries, + denotes selection and ? omission.
Parameters a b-c d-e f-k
algorithm IB1 IB1 IB1 IB1
distance metric O M J O
switching threshold NA 2 2 NA
feature weighting nw nw nw nw
neighborhood size 1 15 19 1
class weights Z ED1 ED1 Z
Table 3: Parameters of the machines learner that were
used in the different runs mentioned in Table 1. More
information about the parameters and their values can be
found in Daelemans et al (2003).
4 Results
We started with a feature selection process with the fea-
tures described in section 2. This experiment used a
basic k-nn classifier without feature weighting, a near-
est neighborhood of size 1, attenuated words, and output
post-processing. We evaluated the effect of trigram out-
put classes by performing an experiment with and with-
out them. The feature selection experiment without tri-
gram output classes selected 10 features and obtained an
F?=1 score of 46.3 on the development data set. The ex-
periment that made use of combined classes selected 12
features and reached a score of 51.8.
We decided to continue using trigram output classes.
Subsequently, we optimized the parameters of our ma-
chine learner based on the features in the second experi-
ment and performed another feature selection experiment
with these parameters. The performance effects can be
found in Table 1 (rows b and c). An additional parameter
optimization step did not have a substantial effect (Ta-
ble 1, row d).
After training a stacked classifier while using the out-
put of the best first stage learner, performance went
up from 56.5 to 58.8. Additional feature selection
and parameter optimization were useful at this level
(F?=1=60.9, see Table 1). Most of our other performance
gain was obtained by a continued process of classifier
stacking. Parameter optimization did not result in im-
proved performance when stacking more than one classi-
fier. Feature selection was useful for the third-stage clas-
sifier but not for the next one. Our final system obtained
an F?=1 score of 63.0 on the development data (Table 1)
and 60.1 on the test set (Table 4).
5 Conclusion
We have described a memory-based semantic role labeler.
In the development of the system we have used feature
selection through bi-directional hill-climbing and param-
eter optimization through iterative deepening search. We
have evaluated n-gram output classes, classifier stacking
and output post-processing, all of which increased per-
formance. An overview of the performance of the system
on the test data can be found in Table 4.
Acknowledgements
Sander Canisius, Iris Hendrickx, and Antal van den
Bosch are funded by NWO (Netherlands Organisation for
Scientific Research). Erik Tjong Kim Sang is funded by
IWT STWW as a researcher in the ATraNoS project.
References
D. W. Aha, D. Kibler, and M. Albert. 1991. Instance-
based learning algorithms. Machine Learning, 6:37?
66.
S. Buchholz, J. Veenstra, and W. Daelemans. 1999. Cas-
caded grammatical relation assignment. In EMNLP-
VLC?99, the Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and Very
Large Corpora, June.
S. Buchholz. 2002. Memory-Based Grammatical Rela-
tion Finding. PhD thesis, University of Tilburg.
X. Carreras and L. Ma`rquez. 2004. Introduction to the
conll-2004 shared task: Semantic role labe ling. In
Proceedings of CoNLL-2004. Boston, MA, USA.
R. Caruana and D. Freitag. 1994. Greedy attribute se-
lection. In Proceedings of the Eleventh International
Conference on Machine Learning, pages 28?36, New
Brunswick, NJ, USA. Morgan Kaufman.
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
Precision Recall F?=1
Overall 67.12% 54.46% 60.13
A0 80.41% 70.18% 74.95
A1 62.04% 59.67% 60.83
A2 46.29% 35.85% 40.41
A3 59.42% 27.33% 37.44
A4 67.44% 58.00% 62.37
A5 0.00% 0.00% 0.00
AM-ADV 25.00% 4.56% 7.71
AM-CAU 0.00% 0.00% 0.00
AM-DIR 33.33% 12.00% 17.65
AM-DIS 58.38% 50.70% 54.27
AM-EXT 53.85% 50.00% 51.85
AM-LOC 38.79% 19.74% 26.16
AM-MNR 48.00% 18.82% 27.04
AM-MOD 97.11% 89.61% 93.21
AM-NEG 74.67% 88.19% 80.87
AM-PNC 44.44% 4.71% 8.51
AM-PRD 0.00% 0.00% 0.00
AM-TMP 58.84% 32.53% 41.90
R-A0 80.26% 76.73% 78.46
R-A1 78.95% 42.86% 55.56
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AA 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 66.67% 14.29% 23.53
V 97.93% 97.93% 97.93
Table 4: The performance of our system measured on the
test data.
tronics Engineers Transactions on Information Theory,
13:21?27.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van
den Bosch. 2003. TiMBL: Tilburg memory based
learner, version 5.0, reference guide. ILK Techni-
cal Report 03-08, Tilburg University. available from
http://ilk.uvt.nl/downloads/pub/papers/ilk.0308.ps.
J. Eisner. 1996. An Empirical Comparison of Probabil-
ity Models for Dependency Grammar. Technical Re-
port IRCS-96-11, Institute for Research in Cognitive
Science, University of Pennsylvania.
R. Kohavi and G. John. 1997. Wrappers for feature
subset selection. Artificial Intelligence Journal, 97(1?
2):273?324.
F. Provost, D. Jensen, and T. Oates. 1999. Efficient pro-
gressive sampling. In Proceedings of the Fifth Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 23?32.
D. H. Wolpert. 1992. On overfitting avoidance as bias.
Technical Report SFI TR 92-03-5001, The Santa Fe
Institute.
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Lexical Patterns or Dependency Patterns:
Which Is Better for Hypernym Extraction?
Erik Tjong Kim Sang
Alfa-informatica
University of Groningen
e.f.tjong.kim.sang@rug.nl
Katja Hofmann
ISLA, Informatics Institute
University of Amsterdam
khofmann@science.uva.nl
Abstract
We compare two different types of extraction
patterns for automatically deriving semantic
information from text: lexical patterns, built
from words and word class information, and
dependency patterns with syntactic informa-
tion obtained from a full parser. We are partic-
ularly interested in whether the richer linguis-
tic information provided by a parser allows for
a better performance of subsequent informa-
tion extraction work. We evaluate automatic
extraction of hypernym information from text
and conclude that the application of depen-
dency patterns does not lead to substantially
higher precision and recall scores than using
lexical patterns.
1 Introduction
For almost a decade, automatic sentence parsing
systems with a reasonable performance (90+% con-
stituent precision/recall) have been available for En-
glish (Charniak, 1999). In recent years there has
been an increase in linguistic applications which use
parsing as a preprocessing step, e.g. Snow et al
(2006) and Surdeanu et al (2008). One of the boosts
for these new applications was the increasing power
of desktop computers, which allows for an easier ac-
cess to the computing-intensive parsing results. An-
other is the increased popularity of dependency pars-
ing of which the results can easily be incorporated
into followup systems.
Although there is a consensus about the fact that
the richness of the dependency structures should, in
principle, enable better performance than lexical in-
formation or shallow parsing results, it is not clear if
these better results can also be obtained in practice.
A performance of 90% precision and recall at con-
stituent level still leaves an average of one error in
a medium-length sentence of ten words. These er-
rors could degrade the performance of any approach
which relies heavily on parser output.
The question of whether to include a full parser as
a preprocessor for natural language processing task,
has led to a heated discussion between the two au-
thors of the paper. One of us argues that full parsers
are slow and make too many errors, and relies on
shallow techniques like part-of-speech tagging for
preprocessing. The other points at the decreasing
costs of computing and improvements in the reliabil-
ity of parsers, and recommends dependency parsers
as preprocessing tools.
While no automatic text preprocessing method is
free of errors, it is indeed true that approaches other
than full parsing, like for example shallow parsing,
offer useful information at a considerably cheaper
processing cost. The choice between using a heavy
full parser or a light shallow language analyzer is
one that developers of language processing systems
frequently have to make. The expected performance
boost of parsed data could be an important motiva-
tion for choosing for full syntactic analysis. How-
ever, we do not know how big the difference be-
tween the two methods will be. In order to find this
out, we designed an experiment in which we com-
pared the effects of preprocessing with and without
using information generated by a full parser.
In this paper, we compare two text preprocess-
ing approaches for a single language processing
task. The first of the two methods is shallow lin-
174
guistic processing, a robust and fast text analysis
method which only uses information from words,
like lemma information and part-of-speech classes.
The second method is dependency parsing which in-
cludes information about the syntactic relations be-
tween words. The natural language processing task
which we will use for assessing the usability of the
two processing methods is automatic extraction of
hypernym information from text. The language of
the text documents is Dutch. We expect that the find-
ings of this study would have been similar if any
other Germanic language (including English) was
used.
The contribution of this paper is a thorough and
fair comparison of the involved preprocessing tech-
niques. There have been earlier studies of hyper-
nym extraction with either lexical or dependency ex-
traction patterns. However, these studies applied the
techniques to a variety of different data sets and used
different evaluation techniques. We will apply the
two methods to the same data, evaluate the results in
a consistent manner and examine the differences.
After this introduction, we will describe the task,
the preprocessing methods and the evaluation setting
in more detail. In the third section, we will show
how our experiments were set up and present the re-
sults. Section four contains a detailed discussion of
the two methods and their effect on the extraction
task. In the final section of the paper, we will present
some concluding remarks.
2 Task and methods
We will apply two different preprocessing methods
to the task of extracting lexical information from
text. In the next sections we describe this task, dis-
cuss different methods for preprocessing the data
and outline the method used for evaluating the re-
sults.
2.1 Extracting hypernym relations
We will concentrate on extracting a single type of
lexical relation: hypernymy. Word A is a hypernym
of word B if the meaning of A both covers the mean-
ing of B and is broader. For example, color is a hy-
pernym of red which in turn is a hypernym of scar-
let. If A is a hypernym of B than B is a hyponym of
A.
There has been quite a lot of work on extracting
hypernymy pairs from text. The pioneering work
of Hearst (1992) applied fixed patterns like NP1 ,
especially NP2 to derive that NP1 is a hypernym
of NP2. Lately there has been a lot of interest in
acquiring such text patterns using a set of hyper-
nymy examples, e.g. Pantel et al (2004) and Snow
(2006). Application of such techniques has not been
restricted to English but also involved other lan-
guages such as Dutch (Tjong Kim Sang and Hof-
mann, 2007). Recent work has also examined ex-
tracting hypernym information from structured data,
like Wikipedia (Sumida and Torisawa, 2008).
For our extraction work, we will closely follow
the approach described in Snow et al (2006):
1. Collect from a text corpus phrases (consecutive
word sequences from a single sentence) that
contain a pair of nouns
2. Mark each phrase as containing a hypernym
pair or a non-hypernym pair according to a lex-
ical resource
3. Remove the noun pair from the phrases and
register how often each phrase is associated by
hypernym pairs and by non-hypernym pairs
4. Use this information for training a machine
learning system to predict whether two nouns
are a hypernym-hyponym pair based on the
phrases in which they occur in a text corpus
For example, we find two phrases: colors such as
cyan and colors such as birds, both of which contain
the basic phrase such as. We mark the first phrase
as a hypernym phrase (color is a hypernym of cyan)
while the second is marked as non-hypernym (color
is not a hypernym of bird). Thus the pattern such as
will receive a positive point and a negative point. A
machine learning algorithm can deduce from these
numbers that two other nouns occurring in the same
pattern will have an estimated probability of 50% of
being related according to hypernymy. The learner
can use information from other patterns to obtain a
better estimation of this probability.
2.2 Lexical patterns
We use two different text preprocessing methods
which automatically assign linguistic information to
sentences. The first preprocessing method has the
175
advantage of offering a fast analysis of the data but
its results are less elaborate than those of the second
method. The first method consists of three steps:
? Tokenization: sentence boundaries are detected
and punctuation signs are separated from words
? Part-of-speech tagging: part-of-speech classes
like noun and verb are assigned to words
? Lemmatization: words are reduced to their ba-
sic form (lemma)
The analysis process would convert a phrase like
Large cities in northern England such as Liverpool
are beyond revival. to lemmas and their associated
part-of-speech tags: large/JJ city/NN in/IN north/JJ
England/NNP such/DT as/IN Liverpool/NNP be/VB
beyond/IN revival/NN ./.
Like in the work of Snow et al (2005), the tar-
get phrases for hypernym extraction are two noun
phrases, with a maximum of three tokens in be-
tween and one or two optional extra tokens (a non-
head token of the first noun phrase and/or one of
the second noun phrase). The lexical preprocessing
method uses two basic regular expressions for find-
ing noun phrases: Determiner? Adjective* Noun+
and ProperNoun+. It assumes that the final token
of the matched phrase is the head. Here is one set of
four patterns which can be derived from the example
sentence:
1. NP in NP
2. large NP in NP
3. NP in north NP
4. large NP in north NP
The patterns contain the lemmas rather than the
words of the sentence in order to allow for general
patterns. For the same reason, the noun phrases have
been replaced by the token NP. Each of the four pat-
terns will be used as evidence for a possible hyper-
nymy relation between the two noun phrase heads
city and England. As a novel extension to the work
of Snow et al, we included two additional variants
of each pattern in which either the first NP or the
second NP was replaced by its head:
5. city in NP
6. NP in England
This enabled us to identify among others appositions
as patterns: president NP.
2.3 Dependency patterns
A dependency analysis contains the same three steps
used for finding lexical patterns: tokenization, part-
of-speech tagging and lemmatization. Additionally,
it includes a fourth step:
? Dependency parsing: find the syntactic depen-
dency relations between the words in each sen-
tence
The syntactic analysis is head-based which means
that for each word in the sentence it finds another
word that dominates it. Here is a possible analysis
of the previous example sentence:
large:JJ:MOD:NN:city
city:NN:SUBJ:VBD:be
in:IN:MOD:NN:city
north:JJ:MOD:NNP:England
England:NNP:OBJ1:IN:in
such:DT:MOD:IN:as
as:IN:MOD:NN:city
Liverpool:NNP:OBJ1:IN:as
be:VB:?:?:?
beyond:IN:MOD:VB:be
revival:NN::OBJ1:IN:beyond
Each line contains a lemma, its part-of-speech tag,
the relation between the word and its head, the part-
of-speech tag of its head and the lemma of the head
word. Our work with dependency patterns closely
follows the work of Snow et al (2005). Patterns are
defined as dependency paths with at most three in-
termediate nodes between the two focus nouns. Ad-
ditional satellite nodes can be present next to the two
nouns. The dependency patterns contain more infor-
mation than the lexical patterns. Here is one of the
patterns that can be derived for the two noun phrases
large cities and northern England in the example
sentence:
NP1:NN:SUBJ:VBD:
in:IN:MOD:NN:NP1
NP2:NNP:OBJ1:IN:in
The pattern defines a path from the head lemma city
via in, to England. Note that lemma information
linking outside this pattern (be at the end of the first
line) has been removed and that lemma information
176
from the target noun phrases has been replaced by
the name of the noun phrase (NP1 at the end of the
second line). For each dependency pattern, we build
six variants similar to the six variants of the lexical
patterns: four with additional information from the
two noun phrases and two more with head informa-
tion of one of the two target NPs.
Both preprocessing methods can identify phrases
like N such as N1 , N2 and N3 as well. Such phrases
produce evidence for each of the pairs (N,N1),
(N,N2) and (N,N3). These three noun pairs will be
included in the data collected for the patterns that
can be derived from the phrase.
We expect that an important advantage of using
dependency patterns over lexical patterns will be
that the former offer a wider coverage. In the ex-
ample sentence, no lexical pattern will associate city
with Liverpool because there are too many words in
between. However, a dependency pattern will cre-
ate a link between these two words, via the word
as. This will enable the dependency patterns to find
out that city is a hypernym of Liverpool, where the
lexical patterns are not able to do this based on the
available information.
The two preprocessing methods generate a large
number of noun pairs associated by patterns. Like
Snow et al (2005), we keep only noun pairs which
are associated by at least five different patterns. The
same constraint is enforced on the extraction pat-
terns: we keep only the patterns which are associ-
ated by at least five different noun pairs. The data
is converted to binary feature vectors representing
noun pairs. These are training data for a Bayesian
Logistic Regression system, BBRtrain (Genkin et
al., 2004). We use the default settings of the learn-
ing system and test its prediction capability in a bi-
nary classification task: whether two nouns are re-
lated according to hypernymy or not. Evaluation is
performed by 10-fold cross validation.
2.4 Evaluation
For parameter optimization we need an automatic
evaluation procedure, since repeated manual checks
of results generated by different versions of the
learner require too much time. We have adopted the
evaluation method of Snow et al(2006): compare
the generated hypernyms with hypernyms present in
a lexical resource, in our case the Dutch part of Eu-
roWordNet (1998).
This choice results in two restrictions. First, we
will only consider pairs of known words (words that
are present in the lexical resource) for evaluation.
We have no information about other words so we
make no assumptions about them. Second, if two
words appear in the lexical resource but not in the
hypernym relation of that same resource then we
will assume that they are unrelated. In other words,
we assume the hypernymy relation specified in the
lexical resource as complete (like in the work of
Snow et al (2006)).
We use standard evaluation scores. We will com-
pute precision and recall for the candidate hyper-
nyms, as well as the related F?=1 rate, the harmonic
mean between precision and recall. Precision will be
computed against all chosen candidate hypernyms.
However, recall will only be computed against the
positive noun pairs which occur in the phrases se-
lected by the examined method. The different pre-
processing methods may cause different numbers of
positive pairs to be selected. Only these pairs will
be used for computing recall scores. Others will be
ignored. For this reason we will report the selected
number of positive target pairs in the result tables as
well1.
3 Experiments and results
We have applied the extraction techniques to two
different Dutch corpora. The first is a collection of
texts from the news domain. It consists of texts from
five different Dutch news papers from the Twente
News Corpus collection. Two versions of this cor-
pus exist. We have worked with the version which
contains the years 1997-2005 (26 million sentences
and 450 million tokens). The second corpus is the
Dutch Wikipedia. Here we used a version of Octo-
ber 2006 (5 million sentences and 58 million words).
Syntactic preprocessing of the material was done
with the Alpino parser, the best available parser for
Dutch with a labeled dependency accuracy of 89%
(Van Noord, 2006). Rather than performing the
parsing task ourselves, we have relied on an avail-
able parsed treebank which included the text corpora
1In a seperate study we have shown that the observed differ-
ences between the two methods remain the same when recall is
computed over sets of similar sizes (Tjong Kim Sang, 2009).
177
that we wanted to use (Van Noord, 2009).
The parser also performs part-of-speech tagging
and lemmatization, tasks which are useful for the
lexical preprocessing methods. However, taking fu-
ture real-time applications in mind, we did not want
the lexical processing to be dependent on the parser.
Therefore we have developed an in-house part-of-
speech tagger and lemmatizer based on the material
created in the Corpus Spoken Dutch project (Eynde,
2005). The tagger achieved an accuracy of 96% on
test data from the same project while the lemmatizer
achieved 98%.
We used the Dutch part of EuroWordNet (Vossen,
1998) as the gold standard lexical resource, both for
training and testing. In the lexicon, many nouns have
different senses. This can cause problems for the
pattern extraction process. For example, if a noun
N1 with sense X is related to another noun N2 then
the appearance of N1 with sense Y with N2 in the
text may be completely accidental and say nothing
about the relation between the two words. In that
case it would be wrong to regard the context of the
two words as an interesting extraction pattern.
There are several ways to deal with this prob-
lem. One is to automatically assign senses to words.
However we do not have a reliable sense tagger for
Dutch at our disposal. Another method was pro-
posed by Snow et al(2005): assume that every word
bears its most frequent sense. But this is also in-
formation which we lack for Dutch: our lexical re-
source does not contain frequency information for
word senses. We have chosen the approach sug-
gested by Hofmann and Tjong Kim Sang (2007):
remove all nouns with multiple senses from the data
set and use only the monosemous words for find-
ing good extraction patterns. This restriction is only
imposed in the training phase. We consider both
monosemous words and polysemous words in the
evaluation process.
We imposed two additional restrictions on the lex-
ical resource. First, we removed the top noun of
the hypernymy hierarchy (iets) from the list of valid
hypernyms. This word is a valid hypernym of any
other noun. It is not an interesting suggestion for
the extraction procedure to put forward. Second, we
restricted the extraction procedure to propose only
known hypernyms as candidate hypernyms. Nouns
that appeared in the lexical resources only as hy-
lexical patterns
Data source Targ. Prec. Recall F?=1
AD 620 55.8% 27.9% 37.2
NRC 882 50.4% 23.8% 32.3
Parool 462 51.8% 21.9% 30.8
Trouw 607 54.1% 25.9% 35.0
Volkskrant 970 49.7% 24.1% 32.5
Newspapers 3307 43.1% 26.7% 33.0
Wikipedia 1288 63.4% 44.3% 52.1
dependency patterns
Data source Targ. Prec. Recall F?=1
AD 706 42.9% 30.2% 35.4
NRC 1224 26.2% 25.3% 25.7
Parool 584 31.2% 23.8% 27.0
Trouw 760 35.3% 29.0% 31.8
Volkskrant 1204 29.2% 25.5% 27.2
Newspapers 3806 20.7% 29.1% 24.2
Wikipedia 1580 61.9% 47.0% 53.4
Table 1: Hypernym extraction scores for the five news-
papers in the Twente News Corpus (AD, NRC, Parool,
Trouw and Volkskrant) and for the Dutch Wikipedia.
The Targets column shows the number of unique posi-
tive word pairs in each data set. The Dutch Wikipedia
contains about as much data as one of the newspaper sec-
tions.
ponyms (leaf nodes of the hypernymy tree) were
never proposed as candidate hypernyms. This made
sense for our evaluation procedure which is only
aimed at finding known hypernym-hyponym pairs.
We performed two hypernym extraction experi-
ments, one which used lexical extraction patterns
and one which used dependency patterns2. The re-
sults from the experiments can be found in Table
1. The newspaper F-scores obtained with lexical
patterns are similar to those reported for English
(Snow et al, 2005, 32.0) but the dependency pat-
terns perform worse. Both approaches perform well
on Wikipedia data, most likely because of the more
repeated sentence structures and the presence of
many definition sentences. For newspaper data, lex-
ical patterns outperform dependency patterns both
for precision and F?=1. For Wikipedia data the dif-
ferences are smaller and in fact the dependency pat-
2The software used in these experiment has been made avail-
able at http://www.let.rug.nl/erikt/cornetto/D08.zip
178
terns obtain the best F-score. For all data sets, the
dependency patterns suggest more related pairs than
the lexical patterns (column Targets). The differ-
ences between the two pattern types are significant
(p < 0.05) for all evaluation measures for Newspa-
pers and for positive targets and recall for Wikipedia.
4 Result analysis
In this section, we take a closer look at the results de-
scribed in the previous section. We start with look-
ing for an explanation for the differences between
the scores obtained with lexical patterns and depen-
dency patterns. First we examine the results for
Wikipedia data and then the results for newspaper
data. Finally, we perform an error analysis to find
out the strengths and weaknesses of each of the two
methods.
4.1 Wikipedia data
The most important difference between the two pat-
tern types for Wikipedia data is the number of posi-
tive targets (Table 1). Dependency patterns find 23%
more related pairs in the Wikipedia data than lexi-
cal patterns (1580 vs. 1288). This effect can also
be simulated by changing the size of the corpus. If
we restrict the data set of the dependency patterns
to 70% of its current size then the patterns retrieve a
similar number of positive targets as the lexical pat-
terns, 1289, with comparable precision, recall and
F?=1 scores (62.5%, 46.6% and 53.4). So we expect
that the effect of applying the dependency patterns
is the same as applying the lexical patterns to 43%
more data.
4.2 Newspaper data
Performance-wise there seems to be only a small
difference between the two preprocessing methods
when applied to the Wikipedia data set. However,
when we examine the scores obtained on the news-
paper data (Table 1) then we find larger differences.
Dependency patterns remain finding more positive
targets and obtaining a larger recall score but their
precision score is disappointing. However, when we
examine the precision-recall plots of the two meth-
ods (Figure 1, obtained by varying the acceptance
threshold of the machine learner), they are almost
indistinguishable. The performance line for lexical
patterns extends further to the left than the one of
Figure 1: Performance of individual hypernym extraction
patterns applied to the combination of five newspapers
and Wikipedia. Each + in the graphs represent a differ-
ent extraction pattern. The precision-recall graphs for the
machine learner (lines) are identical for each data source
except for the extended part of the performance line for
lexical patterns.
179
lexical patterns applied to Newspapers
Key Phrase Targ. Prec. Recall F?=1
N and other N 376 22.0% 11.4% 15.0
N such as N 222 25.1% 6.7% 10.6
N like N 579 7.6% 17.5% 10.6
N , such as N 263 15.6% 8.0% 10.5
N ( N 323 7.5% 9.8% 8.5
dependency patterns applied to Newspapers
Key Phrase Targ. Prec. Recall F?=1
N and other N 420 21.1% 11.0% 14.5
N be a N 451 8.2% 11.8% 9.7
N like N 205 27.3% 5.4% 9.0
N be N 766 5.7% 20.1% 8.8
N such as N 199 22.4% 5.2% 8.5
lexical patterns applied to Wikipedia
Key Phrase Targ. Prec. Recall F?=1
N be a N 294 40.8% 22.8% 29.3
N be N 418 22.9% 32.5% 26.9
a N be N 185 53.3% 14.4% 22.6
N such as N 161 57.5% 12.5% 20.5
N ( N 188 21.2% 14.6% 17.3
dependency patterns applied to Wikipedia
Key Phrase Targ. Prec. Recall F?=1
N be N 609 33.6% 38.5% 35.9
N be a N 452 44.3% 28.6% 34.8
the N be N 258 34.0% 16.3% 22.1
a N be N 184 44.7% 11.6% 18.5
N N 234 16.6% 14.8% 15.6
Table 2: Best performing extraction patterns according to
F-scores.
the dependency patterns but the remainder of the two
graphs overlap. The measured performances in Ta-
ble 1 are different because the machine learner put
the acceptance level for extracted pairs at different
points of the graph: the performance lines in both
newspaper graphs contain (recall,precision) points
(26.7%,43.1%) and (29.1%,20.7%).
We are unable to find major differences in the re-
sults of the two approaches. We conclude that, apart
from an effect which can be simulated with some
extra data, there is no difference between prepro-
cessing text with shallow methods and with a full
56 ? covered by other patterns
12 48% required full parsing
6 24% lemmatization errors
3 12% omitted for lack of support
3 12% pos tagging errors
1 4% extraction pattern error
81 100%
45 ? covered by other patterns
38 64% parsing errors
10 17% lemmatization errors
7 12% extraction pattern errors
3 5% omitted for lack of support
1 2% pos tagging error
104 100%
Table 3: Primary causes of recall errors made by the lex-
ical pattern N such as N (top) and the best performing
corresponding dependency pattern (bottom).
dependency parser.
4.3 Error analysis
Despite the lack of performance differences between
the two preprocessing methods, there are still inter-
nal differences which cause one method to generate
different related word pairs than the other. We will
now examine in detail two extraction patterns and
specify their distinct effects on the output results.
We hope that by carefully examining their output we
can learn about the strengths and weaknesses of the
two approaches.
We take a closer look at extraction pattern N such
as N for Newspaper data (second best for lexical pat-
terns and fifth best for dependency patterns, see Ta-
ble 2). The lexical pattern found 222 related word
pairs while the dependency pattern discovered 199.
118 of these pairs were found by both patterns which
means that the lexical pattern missed 81 of the pairs
while the dependency pattern missed 104.
An overview of the cause of the recall errors can
be found in Table 3. The two extraction patterns
do not overlap completely. The dependency parser
ignored punctuation signs and therefore the depen-
dency pattern covers both phrases with and without
punctuation. However, these phrase variants result
in different lexical patterns. This is the cause for
56 hypernyms being missed by the lexical pattern.
180
Meanwhile there is a difference between a depen-
dency pattern without the conjunction and and one
with the conjunction, while there is a unified lexi-
cal pattern processing both phrases with and without
conjunctions. This caused the dependency pattern to
miss 45 hypernyms. However, all of these ?missed?
hypernyms are handled by other patterns.
The main cause of the recall differences between
the two extraction patterns was the parser. The de-
pendency pattern found twelve hypernyms which
the lexical pattern missed because they required an
analysis which was beyond part-of-speech tagging
and the basic noun phrase identifier used by the lex-
ical preprocessor. Six hypernyms required extend-
ing a noun phrase with a prepositional phrase, five
needed noun phrase extension with a relative clause
and one involved appositions. An example of such
a phrase is illnesses caused by vitamin deficits, like
scurvy and beriberi.
However, the syntactic information that was avail-
able to the dependency pattern did also have a neg-
ative effect on its recall. 38 of the hypernyms de-
tected by the lexical pattern were missed by the de-
pendency pattern because there was a parsing error
in the relevant phrase. In more than half of the cases,
this involved attaching the phrase starting with such
as at an incorrect position. We found that a phrase
like N1 such as N2 , N3 and N4 could have been split
at any position. We even found some cases of prepo-
sitional phrases and relative clauses incorrectly be-
ing moved from other positions in the sentence into
the target phrase.
Other recall error causes appear less frequently.
The two preprocessing methods used different
lemmatization algorithms which also made different
errors. The effects of this were visible in the errors
made by the two patterns. Some hypernyms that
were found by both patterns but were not present
in both results because of insufficient support from
other patterns (candidate hypernyms should be sup-
ported by at least five different patterns). The ef-
fect of errors in part-of-speech tags was small. Our
data analysis also revealed some inconsistencies in
the extraction patterns which should be examined.
5 Concluding remarks
We have evaluated the effects of two different pre-
processing methods for a natural language process-
ing task: automatically identifying hypernymy in-
formation. The first method used lexical patterns
and relied on shallow processing techniques like
part-of-speech tagging and lemmatization. The sec-
ond method used dependency patterns which re-
lied on additional information obtained from depen-
dency parsing.
In earlier work, McCarthy et al (2007) found
that for word sense disambiguation using the-
sauri generated from dependency relations perform
only slightly better than thesauri generated from
proximity-based relations. Jijkoun et al (2004)
showed that information obtained from dependency
patterns significantly improved the performance of a
question answering system. Li and Roth (2001) re-
port that preprocessing by shallow parsing allows for
a more accurate post-processing of ill-formed sen-
tences than preprocessing with full parsing.
Our study supports the findings of McCarthy et
al. (2007). We found only minor differences in per-
formances between the two preprocessing methods.
The most important difference: about 20% extra
positive cases that were identified by the dependency
patterns applied to Wikipedia data, can be overcome
by increasing the data set of the lexical patterns by
half. We believe that obtaining more data may often
be easier than dealing with the extra computing time
required for parsing the data. For example, in the
course of writing this paper, we had to refrain from
using a recent version of Wikipedia because pars-
ing the data would have taken 296 days on a single
processor machine compared with a single hour for
tagging the data.
References
Eugene Charniak. 1999. A maximum-entropy inspired
parser. Technical Report CS-99-12, Brown University.
Frank Van Eynde. 2005. Part of Speech Tagging en Lem-
matisering van het Corpus Gesproken Nederlands.
K.U. Leuven. (in Dutch).
Alexander Genkin, David D. Lewis, and David Madigan.
2004. Large-Scale Bayesian Logistic Regression for
Text Categorization. Technical report, Rutgers Uni-
versity, New Jersey.
181
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
ACL-92. Newark, Delaware, USA.
Katja Hofmann and Erik Tjong Kim Sang. 2007. Au-
tomatic extension of non-english wordnets. In Pro-
ceedings of SIGIR?07. Amsterdam, The Netherlands
(poster).
Valentin Jijkoun, Maarten de Rijke, and Jori Mur. 2004.
Information extraction for question answering: Im-
proving recall through syntactic patterns. In Proceed-
ings of Coling?04. Geneva, Switzerland.
Xin Li and Dan Roth. 2001. Exploring evidence for shal-
low parsing.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Caroll. 2007. Unsupervised acquisition of predomi-
nant word senses. Computational Linguistics, 33(4).
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition.
In Proceedings of COLING 2004, pages 771?777.
Geneva, Switzerland.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS 2005. Vancouver, Canada.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006. Sydney,
Australia.
Asuka Sumida and Kentaro Torisawa. 2008. Hacking
wikipedia for hyponymy relation acquisition. In Pro-
ceedings of IJCNLP 2008. Hyderabad, India.
Mihai Surdeanu, Richard Johansson, Llu??s Ma`rquez,
Adam Meyers, and Joakim Nivre. 2008. The conll-
2008 shared task on joint learning of syntactic and se-
mantic dependencies. In Proceedings of CoNLL-2008.
Manchester, UK.
Erik Tjong Kim Sang and Katja Hofmann. 2007. Au-
tomatic extraction of dutch hypernym-hyponym pairs.
In Proceedings of CLIN-2006. Leuven, Belgium.
Erik Tjong Kim Sang. 2009. To use a treebank or not ?
which is better for hypernym extraction. In Proceed-
ings of the Seventh International Workshop on Tree-
banks and Linguistic Theories (TLT 7). Groningen,
The Netherlands.
Gertjan Van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, Cedrick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, TALN06. Verbum Ex
Machina. Actes de la 13e conference sur le traitement
automatique des langues naturelles.
Gertjan Van Noord. 2009. Huge parsed corpora in lassy.
In Proceedings of TLT7. LOT, Groningen, The Nether-
lands.
Piek Vossen. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publisher.
182
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1124?1128,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Constraint Satisfaction Approach to Dependency Parsing
Sander Canisius
ILK / Communication and Information Science
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
S.V.M.Canisius@uvt.nl
Erik Tjong Kim Sang
ISLA, University of Amsterdam,
Kruislaan 403, NL-1098 SJ Amsterdam,
The Netherlands
erikt@science.uva.nl
Abstract
We present an adaptation of constraint satis-
faction inference (Canisius et al, 2006b) for
predicting dependency trees. Three differ-
ent classifiers are trained to predict weighted
soft-constraints on parts of the complex out-
put. From these constraints, a standard
weighted constraint satisfaction problem can
be formed, the solution to which is a valid
dependency tree.
1 Introduction
Like the CoNLL-2006 shared task, the 2007 shared
task focuses on dependency parsing and aims at
comparing state-of-the-art machine learning algo-
rithms applied to this task (Nivre et al, 2007). For
our official submission, we used the dependency
parser described by Canisius et al (2006a). In this
paper, we present a novel approach to dependency
parsing based on constraint satisfaction. The method
is an adaptation of earlier work using constraint sat-
isfaction techniques for predicting sequential out-
puts (Canisius et al, 2006b). We evaluated our ap-
proach on all ten data sets of the 2007 shared task1.
In the remainder of this paper, we will present the
new constraint satisfaction method for dependency
parsing in Section 2. The method is evaluated in
Section 3, in which we will also present a brief error
1Hajic? et al (2004), Aduriz et al (2003), Mart?? et al (2007),
Chen et al (2003), Bo?hmova? et al (2003), Marcus et al
(1993), Johansson and Nugues (2007), Prokopidis et al (2005),
Csendes et al (2005), Montemagni et al (2003), Oflazer et al
(2003)
analysis. Finally, Section 4 presents our main con-
clusions.
2 Constraint Satisfaction Inference for
Dependency Trees
The parsing algorithm we used is an adaptation for
dependency trees of the constraint satisfaction in-
ference method for sequential output structures pro-
posed by Canisius et al (2006b). The technique
uses standard classifiers to predict a weighted con-
straint satisfaction problem, the solution to which is
the complete dependency tree. Constraints that are
predicted each cover a small part of the complete
tree, and overlap between them ensures that global
output structure is taken into account, even though
the classifiers only make local predictions in isola-
tion of each other.
A weighted constraint satisfaction problem (W-
CSP) is a tuple (X,D,C,W ). Here, X =
{x1, x2, . . . , xn} is a finite set of variables. D(x)
is a function that maps each variable to its domain,
and C is a set of constraints on the values assigned
to the variables. For a traditional (non-weighted)
constraint satisfaction problem, a valid solution is
an assignment of values to the variables that (1) are
a member of the corresponding variable?s domain,
and (2) satisfy all constraints in the set C . Weighted
constraint satisfaction, however, relaxes this require-
ment to satisfy all constraints. Instead, constraints
are assigned weights that may be interpreted as re-
flecting the importance of satisfying that constraint.
The optimal solution to a W-CSP is the solution that
assigns those values that maximise the sum of the
weights of satisfied constraints.
1124
Figure 1: Dependency tree for the sentence No it
wasn?t Black Monday
To adapt this framework to predicting a depen-
dency tree for a sentence, we construct a constraint
satisfaction problem by first introducing one vari-
able xi for each token of the sentence. This vari-
able?s value corresponds to the dependency relation
that token is the modifier of, i.e. it should specify a
relation type and a head token. The constraints of the
CSP are predicted by a classifier, where the weight
for a constraint corresponds to the classifier?s confi-
dence estimate for the prediction.
For the current study, we trained three classifiers
to predict three different types of constraints.
1. Cdep(head, modifier, relation), i.e. the re-
sulting dependency tree should have a
dependency arc from head to modifier la-
belled with type relation. For the example
tree in Figure 1, among others the constraint
Cdep(head=was, modifier=No, relation=VMOD)
should be predicted.
2. Cdir(modifier, direction), the relative posi-
tion (i.e. to its left or to its right) of
the head of modifier. The tree in Fig-
ure 1 will give rise to constraints such as
Cdir(modifier=Black, direction=RIGHT).
3. Cmod(head, relation), in the dependency
tree, head should be modified by a relation
of type relation. The constraints gener-
ated for the word was in Figure 1 would
be Cmod(head=was, relations=SBJ), and
Cmod(head=was, relations=VMOD).
Predicting constraints of type Cdep is essentially
what is done by Canisius et al (2006a); a classi-
fier is trained to predict a relation label, or a sym-
bol signalling the absence of a relation, for each
pair of tokens in a sentence2. The training data
for this classifier consists of positive examples of
constraints to generate, e.g. was, No, VMOD, and
negative examples, of constraints not to generate,
e.g. was, Black, NONE, but also No, was, NONE. In
the aforementioned paper, it is shown that downsam-
pling the negative class in the classifier?s training
data improves the recall for predicted constraints.
The fact that improved recall comes at the cost of a
reduced precision is compensated for by our choice
for the weighted constraint satisfaction framework:
an overpredicted constraint may still be left unsatis-
fied if other, conflicting constraints outweigh its own
weight.
In addition to giving rise to a set of constraints,
this classifier differs from the other two in the sense
that it is also used to predict the domains of the vari-
ables, i.e. any dependency relation not predicted by
this classifier will not be considered for inclusion in
the output tree.
Whereas the Cdep classifier classifies instances
for each pair of words, the classifiers for Cdir and
Cmod only classify individual tokens. The features
for these classifiers have been kept simple and the
same for both classifiers: a 5-slot wide window of
both tokens and part-of-speech tags, centred on the
token currently being classified. The two classifiers
differ in the classes they predict. For Cdir , there are
only three possible classes: LEFT, RIGHT, NONE.
Instances classified as LEFT, or RIGHT give rise to
constraints, whereas NONE implies that no Cdir con-
straint is added for that token.
For Cmod there is a rather large class space; a
class label reflects all modifying relations for the to-
ken, e.g. SBJ+VMOD. From this label, as many con-
straints are generated as there are different relation
types in the label.
With the above, a weighted constraint satisfaction
problem can be formulated that, when solved, de-
scribes a dependency tree. As we formulated our
problem as a constraint satisfaction problem, any
off-the-shelf W-CSP solver could be used to obtain
the best dependency parse. However, in general such
solvers have a time complexity exponential in the
2For reasons of efficiency and to avoid having too many neg-
ative instances in the training data, we follow the approach of
Canisius et al (2006a) of limiting the maximum distance be-
tween a potential head and modifier.
1125
Language LAS ?06 UAS ?06
Arabic 60.36 +1.2 78.61 +1.7
Basque 64.23 +1.1 72.24 +2.1
Catalan 77.33 +1.9 84.73 +3.1
Chinese 71.73 +1.3 77.29 +2.5
Czech 57.58 +1.4 75.61 +3.5
English 79.47 +2.2 81.05 +2.8
Greek 62.32 +2.0 76.42 +4.0
Hungarian 66.86 +2.6 72.52 +4.7
Italian 77.04 +1.5 81.24 +2.2
Turkish 67.80 -0.3 75.58 +0.4
Table 1: Performance of the system applied to the
test data for each language. The ?06 columns show
the gain/loss with respect to the parser of Canisius et
al. (2006a).
number of variables, and thus in the length of the
sentence. As a more efficient alternative we chose to
use the CKY algorithm for dependency parsing (Eis-
ner, 2000) for computing the best solution, which
has only cubic time complexity, but comes with the
disadvantage of only considering projective trees as
candidate solutions.
3 Results and discussion
We tested our system on all ten languages of the
shared task. The three constraint classifiers have
been implemented with memory-based learning. No
language-specific parameter optimisation or feature
engineering has been performed, but rather the exact
same system has been applied to all languages. La-
belled and unlabelled attachment scores are listed in
Table 1. In addition, we show the increase/decrease
in performance when compared with the parser of
Canisius et al (2006a); for all languages but Turk-
ish, there is a consistent increase, mostly somewhere
between 1.0 and 2.0 percent in labelled attachment
score.
The parser by Canisius et al (2006a) can be
considered a rudimentary implementation of con-
straint satisfaction inference that only uses Cdep con-
straints. The parser described in this paper elabo-
rates this by adding (1) the Cmod and Cdir soft con-
straints, and (2) projectivity and acyclicity hard con-
straints, enforced implicitly by the CKY algorithm.
To evaluate the effect of each of these constraints,
Language ?06 Cdep Cmod/dep C
dir/
dep all
Arabic 59.13 +0.3 +0.9 +0.9 +1.2
Basque 63.17 +0.3 +0.4 +0.9 +1.1
Catalan 75.44 +0.8 +1.2 +1.4 +1.9
Chinese 70.45 +0.4 +1.2 +0.4 +1.3
Czech 56.14 +0.5 +0.5 +1.1 +1.4
English 77.27 +0.4 +1.4 +1.2 +2.2
Greek 60.35 +0.4 +0.6 +1.6 +2.0
Hungarian 64.31 +1.9 +1.3 +2.8 +2.6
Italian 75.57 +0.2 +1.0 +1.1 +1.5
Turkish 68.09 -0.2 -0.3 -0.3 -0.3
Table 2: Performance of the parser by Canisius et al
(2006a) and the performance gain of the constraint
satisfaction inference parser with various constraint
configurations.
Table 2 shows the labelled attachment scores for
several parser configurations; starting with the 2006
parser, i.e. a parser with only Cdep constraints, then
the CKY-driven Cdep parser, i.e. with acyclicity and
projectivity constraints, then with Cmod, and Cdir
separately, and finally, the full parser based on all
constraints. It can be seen that supplementing the
Cdep-only parser with hard constraints for acyclicity
and projectivity already gives a small performance
improvement. For some languages, such as Ital-
ian (+0.2), this improvement is rather small, how-
ever for Hungarian 1.9 is gained only by using CKY.
The remaining columns show that adding more con-
straints improves performance, and that for all lan-
guages but Turkish and Hungarian, using all con-
straints works best.
While in comparison with the system of Canisius
et al (2006a) the addition of extra constraints has
clearly shown its use, we expect the Cdep classifier
still to be the performance bottleneck of the sys-
tem. This is mainly due to the fact that this classifier
is also responsible for defining the domains of the
CSP variables, i.e. which dependency relations will
be considered for inclusion in the output. For this
reason, we performed an error analysis of the out-
put of the Cdep classifier and the effect it has on the
performance of the complete system.
In our error analysis, we distinguish three types of
errors: 1) label errors, a correct dependency arc was
added to the tree, but its label is incorrect, 2) recall
1126
Cdep prec. rec.
Language prec. rec. %OOD %OOD
Arabic 54.90 73.66 78.83 77.95
Basque 55.82 74.10 85.05 83.66
Catalan 65.19 87.25 80.29 80.00
Chinese 65.10 76.49 83.79 82.94
Czech 53.64 74.35 81.16 80.27
English 59.37 90.08 67.51 66.63
Greek 53.24 76.29 79.96 79.08
Hungarian 44.71 78.64 69.08 67.45
Italian 71.70 82.57 87.97 87.32
Turkish 64.92 72.79 89.11 88.51
Table 3: Columns two and three: precision and re-
call on dependency predictions by the Cdep classi-
fier. Columns four and five: percentage of depen-
dency arc precision and recall errors caused by out-
of-domain errors.
errors, the true dependency tree contains an arc that
is missing from the predicted tree, and 3) precision
errors, the predicted tree contains a dependency arc
that is not part of the true dependency parse.
Label errors are always a direct consequence of
erroneous Cdep predictions. If the correct arc was
predicted, but with an incorrect label, then by defi-
nition, the correct arc with the correct label cannot
have been predicted at the same time. In case of the
other two types of errors, the correct constraints may
well have been predicted, but afterwards outweighed
by other, conflicting constraints. Nevertheless, pre-
cision and recall errors may also be caused by the
fact that the Cdep classifier simply did not predict a
dependency arc where it should have. We will refer
to those errors as out-of-domain errors, since the do-
main of at least one of the CSP variables does not
contain the correct value. An out-of-domain error
is a direct consequence of a recall error made by
the Cdep classifier. To illustrate these interactions,
Table 3 shows for all languages the precision and
recall of the Cdep classifier, and the percentage of
dependency precision and recall errors that are out-
of-domain errors.
The table reveals several interesting facts. For En-
glish, which is the language for which our system at-
tains its highest score, the percentage of dependency
precision and recall errors caused by Cdep recall er-
rors is the lowest of all languages. This can directly
be related to the 90% recall of the English Cdep clas-
sifier. Apparently, the weak precision (59%), caused
by down-sampling the training data, is compensated
for in the subsequent constraint satisfaction process.
For Italian, the percentage of out-of-domain-
related errors is much higher than for English. At
the same time, the precision and recall of the Cdep
classifier are much more in balance, i.e. a higher
precision, but a lower recall. We tried breaking this
balance in favour of a higher recall by applying an
even stronger down-sampling of negative instances,
and indeed the parser benefits from this. Labelled
attachment increases from 77.04% to 78.41%. The
precision and recall of this new Cdep classifier are
58.65% and 87.15%, respectively.
The lowest Cdep precision has been observed for
Hungarian (44.71), which unfortunately is not mir-
rored by a high recall score. Remarkably however,
after English, Hungarian has the lowest percentage
of dependency errors due to Cdep recall errors (69.08
and 67.45). It is therefore hypothesised that not
the low recall, but the low precision is the main
cause for errors made on Hungarian. With this in
mind, we briefly experimented with weaker down-
sampling ratios in order to boost precision, but so
far we did not manage to attain better results.
4 Concluding remarks
We have presented a novel dependency parsing
method based on a standard constraint satisfaction
framework. First results on a set of ten different lan-
guages have been promising, but so far no extensive
optimisation has been performed, which inevitably
reflects upon the scores attained by the system. Fu-
ture work will focus on tuning the many parameters
our system has, as well as on experimenting with dif-
ferent types of constraints to supplement or replace
one or more of the three types used in this study.
Acknowledgements
The authors wish to thank Antal van den Bosch for
discussions and suggestions. This research is funded
by NWO, the Netherlands Organization for Scien-
tific Research under the IMIX programme.
1127
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(Abeille?, 2003), chapter 7, pages 103?127.
S. Canisius, T. Bogers, A. van den Bosch, J. Geertzen,
and E. Tjong Kim Sang. 2006a. Dependency parsing
by inference over high-recall dependency predictions.
In Proceedings of CoNLL-X. New York, NY, USA.
S. Canisius, A. van den Bosch, and W. Daelemans.
2006b. Constraint Satisfaction Inference: Non-
probabilistic Global Inference for Sequence Labelling.
Proceedings of the EACL 2006 Workshop on Learning
Structured Information in Natural Language Applica-
tions, pages 9?16.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(Abeille?, 2003), chapter 13, pages 231?248.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. Advances in Probabilistic
and Other Parsing Technologies, pages 29?62.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (Abeille?, 2003), chap-
ter 11, pages 189?210.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille?
(Abeille?, 2003), chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
1128
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 165?168,
Prague, June 2007. c?2007 Association for Computational Linguistics
Extracting Hypernym Pairs from the Web
Erik Tjong Kim Sang
ISLA, Informatics Institute
University of Amsterdam
erikt@science.uva.nl
Abstract
We apply pattern-based methods for collect-
ing hypernym relations from the web. We
compare our approach with hypernym ex-
traction from morphological clues and from
large text corpora. We show that the abun-
dance of available data on the web enables
obtaining good results with relatively unso-
phisticated techniques.
1 Introduction
WordNet is a key lexical resource for natural lan-
guage applications. However its coverage (currently
155k synsets for the English WordNet 2.0) is far
from complete. For languages other than English,
the available WordNets are considerably smaller,
like for Dutch with a 44k synset WordNet. Here, the
lack of coverage creates bigger problems. A man-
ual extension of the WordNets is costly. Currently,
there is a lot of interest in automatic techniques for
updating and extending taxonomies like WordNet.
Hearst (1992) was the first to apply fixed syn-
tactic patterns like such NP as NP for extracting
hypernym-hyponym pairs. Carballo (1999) built
noun hierarchies from evidence collected from con-
junctions. Pantel, Ravichandran and Hovy (2004)
learned syntactic patterns for identifying hypernym
relations and combined these with clusters built
from co-occurrence information. Recently, Snow,
Jurafsky and Ng (2005) generated tens of thousands
of hypernym patterns and combined these with noun
clusters to generate high-precision suggestions for
unknown noun insertion into WordNet (Snow et al,
2006). The previously mentioned papers deal with
English. Little work has been done for other lan-
guages. IJzereef (2004) used fixed patterns to ex-
tract Dutch hypernyms from text and encyclopedias.
Van der Plas and Bouma (2005) employed noun dis-
tribution characteristics for extending the Dutch part
of EuroWordNet.
In earlier work, different techniques have been ap-
plied to large and very large text corpora. Today,
the web contains more data than the largest available
text corpus. For this reason, we are interested in em-
ploying the web for the extraction of hypernym re-
lations. We are especially curious about whether the
size of the web allows to achieve meaningful results
with basic extraction techniques.
In section two we introduce the task, hypernym
extraction. Section three presents the results of our
web extraction work as well as a comparison with
similar work with large text corpora. Section four
concludes the paper.
2 Task and Approach
We examine techniques for extending WordNets. In
this section we describe the relation we focus on,
introduce our evaluation approach and explain the
query format used for obtaining web results.
2.1 Task
We concentrate on a particular semantic relation:
hypernymy. One term is a hypernym of another if
its meaning both covers the meaning of the second
term and is broader. For example, furniture is a hy-
pernym of table. The opposite term for hypernym is
hyponym. So table is a hyponym of furniture. Hy-
pernymy is a transitive relation. If term A is a hyper-
nym of term B while term B is a hypernym of term
165
C then term A is also a hypernym of term C.
In WordNets, hypernym relations are defined be-
tween senses of words (synsets). The Dutch Word-
Net (Vossen, 1998) contains 659,284 of such hy-
pernym noun pairs of which 100,268 are immedi-
ate links and 559,016 are inherited by transitivity.
More importantly, the resource contains hypernym
information for 45,979 different nouns. A test with
a Dutch newspaper text revealed that the WordNet
only covered about two-thirds of the noun lemmas
in the newspaper (among the missing words were
e-mail, euro and provider). Proper names pose an
even larger problem: the Dutch WordNet only con-
tains 1608 words that start with a capital character.
2.2 Collecting evidence
In order to find evidence for the existence of hyper-
nym relations between words, we search the web for
fixed patterns like H such as A, B and C. Following
Snow et al (2006), we derive two types of evidence
from these patterns:
? H is a hypernym of A, B and C
? A, B and C are siblings of each other
Here, sibling refers to the relative position of the
words in the hypernymy tree. Two words are sib-
lings of each other if they share a parent.
We compute a hypernym evidence score S(h,w)
for each candidate hypernym h for word w. It is the
sum of the normalized evidence for the hypernymy
relation between h and w, and the evidence for sib-
ling relations between w and known hyponyms s of
h:
S(h,w) = fhw?
x fxw
+
?
s
gsw
?
y gyw
where fhw is the frequency of patterns that predictthat h is a hypernym of w, gsw is the frequency ofpatterns that predict that s is a sibling of w, and x
and y are arbitrary words from the WordNet. For
each word w, we select the candidate hypernym h
with the largest score S(h,w).
For each hyponym, we only consider evidence
for hypernyms and siblings. We have experimented
with different scoring schemes, for example by in-
cluding evidence from hypernyms of hypernyms and
remote siblings, but found this basic scoring scheme
to perform best.
2.3 Evaluation
We use the Dutch part of EuroWordNet (DWN)
(Vossen, 1998) for evaluation of our hypernym ex-
traction methods. Hypernym-hyponym pairs that are
present in the lexicon are assumed to be correct. In
order to have access to negative examples, we make
the same assumption as Snow et al (2005): the hy-
pernymy relations in the WordNets are complete for
the terms that they contain. This means that if two
words are present in the lexicon without the target
relation being specified between them, then we as-
sume that this relation does not hold between them.
The presence of positive and negative examples al-
lows for an automatic evaluation in which precision,
recall and F values are computed.
We do not require our search method to find the
exact position of a target word in the hypernymy
tree. Instead, we are satisfied with any ancestor. In
order to rule out identification methods which sim-
ply return the top node of the hierarchy for all words,
we also measure the distance between the assigned
hypernym and the target word. The ideal distance is
one, which would occur if the suggested ancestor is
a parent. Grandparents are associated with distance
two and so on.
2.4 Composing web queries
In order to collect evidence for lexical relations, we
search the web for lexical patterns. When working
with a fixed corpus on disk, an exhaustive search can
be performed. For web search, however, this is not
possible. Instead, we rely on acquiring interesting
lexical patterns from text snippets returned for spe-
cific queries. The format of the queries has been
based on three considerations.
First, a general query like such as is insufficient
for obtaining much interesting information. Most
web search engines impose a limit on the number
of results returned from a query (for example 1000),
which limits the opportunities for assessing the per-
formance of such a general pattern. In order to ob-
tain useful information, the query needs to be more
specific. For the pattern such as, we have two op-
tions: adding the hypernym, which gives hypernym
such as, or adding the hyponym, which results in
such as hyponym.
Both extensions of the general pattern have their
166
limitations. A pattern that includes the hypernym
may fail to generate enough useful information if the
hypernym has many hyponyms. And patterns with
hyponyms require more queries than patterns with
hypernyms (one per child rather than one per par-
ent). We chose to include hyponyms in the patterns.
This approach models the real world task in which
one is looking for the meaning of an unknown entity.
The final consideration regards which hyponyms
to use in the queries. Our focus is on evaluating the
approach via comparison with an existing WordNet.
Rather than submitting queries for all 45,979 nouns
in the lexical resource to the web search engine, we
will use a random sample of nouns.
3 Hypernym extraction
We describe our web extraction work and compare
the results with our earlier work with extraction from
a text corpus and hypernym prediction from mor-
phological information.
3.1 Earlier work
In earlier work (Tjong Kim Sang and Hofmann,
2007), we have applied different methods for ob-
taining hypernym candidates for words. First,
we extracted hypernyms from a large text corpus
(300Mwords) following the approach of Snow et
al. (2006). We collected 16728 different contexts
in which hypernym-hyponym pairs were found and
evaluated individual context patterns as well as a
combination which made use of Bayesian Logistic
Regression. We also examined a single pattern pre-
dicting only sibling relations: A en(and) B.
Additionally, we have applied a corpus-indepen-
dent morphological approach which takes advantage
of the fact that in Dutch, compound words often
have the head in the final position (like blackbird in
English). The head is a good hypernym candidate
for the compound and therefore long words which
end with a legal Dutch word often have this suffix as
hypernym (Sabou et al, 2005).
The results of the approaches can be found in Ta-
ble 1. The corpus approaches achieve reasonable
precision rates. The recall scores are low because
we attempt to retrieve a hypernym for all nouns in
the WordNet. Surprisingly enough the basic mor-
phological approach outperforms all corpus meth-
Method Prec. Recall F Dist.
corpus: N zoals N 0.22 0.0068 0.013 2.01
corpus: combined 0.36 0.020 0.038 2.86
corpus: N en N 0.31 0.14 0.19 1.98
morphological approach 0.54 0.33 0.41 1.19
Table 1: Performances measured in our earlier work
(Tjong Kim Sang and Hofmann, 2007) with a mor-
phological approach and patterns applied to a text
corpus (single hypernym pattern, combined hyper-
nym patterns and single conjunctive pattern). Pre-
dicting valid suffixes of words as their hypernyms,
outperforms the corpus approaches.
ods, both with respect to precision and recall.
3.2 Extraction from the web
For our web extraction work, we used the same in-
dividual extraction patterns as in the corpus work:
zoals (such as) and en (and), but not the com-
bined hypernym patterns because the expected per-
formance did not make up for the time complexity
involved. We added randomly selected candidate
hyponyms to the queries to improve the chance to
retrieve interesting information.
This approach worked well. As Table 2 shows, for
both patterns the recall score improved in compari-
son with the corpus experiments. Additionally, the
single web hypernym pattern zoals outperformed the
combination of corpus hypernym patterns with re-
spect to recall and distance. Again, the conjunctive
pattern outperformed the hypernym pattern. We as-
sume that the frequency of the two patterns plays an
important role (the frequency of pages with the con-
junctive pattern is five times the frequency of pages
with zoals).
Finally, we combined word-internal information
with the conjunctive pattern approach by adding the
morphological candidates to the web evidence be-
fore computing hypernym pair scores. This ap-
proach achieved the highest recall score at only
slight precision loss (Table 2).
3.3 Error analysis
We have inspected the output of the conjunctive web
extraction with word-internal information. For this
purpose we have selected the ten most frequent hy-
pernym pairs (Table 3), the ten least frequent and
the ten pairs exactly between these two groups. 40%
167
Method Prec. Recall F Dist.
web: N zoals N 0.23 0.089 0.13 2.06
web: N en N 0.39 0.31 0.35 2.04
morphological approach 0.54 0.33 0.41 1.19
web: en + morphology 0.48 0.45 0.46 1.64
Table 2: Performances measured in the two web ex-
periments and a combination of the best web ap-
proach with the morphological approach. The con-
junctive web pattern N en N rates best, because of its
high frequency. The recall rate can be improved by
supplying the best web approach with word-internal
information.
of the pairs were correct, 47% incorrect and 13%
were plausible but contained relations that were not
present in the reference WordNet. In the center
group of ten pairs all errors are caused by the mor-
phological approach while all other errors originate
from the web extraction method.
4 Concluding remarks
The contributions of this paper are two-fold. First,
we show that the large quantity of available web data
allows basic patterns to perform better on hyper-
nym extraction than a combination of extraction pat-
terns applied to a large corpus. Second, we demon-
strate that the performance of web extraction can be
improved by combining its results with those of a
corpus-independent morphological approach.
The described approach is already being applied
in a project for extending the coverage of the Dutch
WordNet. However, we remain interested in obtain-
ing a better performance levels especially in higher
recall scores. There are some suggestions on how
we could achieve this. First, our present selection
method, which ignores all but the first hypernym
suggestion, is quite strict. We expect that the lower-
ranked hypernyms include a reasonable number of
correct candidates as well. Second, a combination
of web patterns most likely outperforms individual
patterns. Obtaining results for many different web
pattens will be a challenge given the restrictions on
the number of web queries we can currently use.
References
Sharon A. Caraballo. 1999. Automatic construction ofa hypernym-labeled noun hierarchy from text. In Pro-
+/- score hyponym hypernym
- 912 buffel predator
+ 762 trui kledingstuk
? 715 motorfiets motorrijtuig
+ 697 kruidnagel specerij
- 680 concours samenzijn
+ 676 koopwoning woongelegenheid
+ 672 inspecteur opziener
? 660 roller werktuig
? 654 rente verdiensten
? 650 cluster afd.
Table 3: Example output of the the conjunctive web
system with word-internal information. Of the ten
most frequent pairs, four are correct (+). Four others
are plausible but are missing in the WordNet (?).
ceedings of ACL-99. Maryland, USA.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
ACL-92. Newark, Delaware, USA.
Leonie IJzereef. 2004. Automatische extractie van hy-
perniemrelaties uit grote tekstcorpora. MSc thesis,University of Groningen.
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.2004. Towards terascale knowledge acquisition.
In Proceedings of COLING 2004, pages 771?777.Geneva, Switzerland.
Lonneke van der Plas and Gosse Bouma. 2005. Auto-
matic acquisition of lexico-semantic knowledge for qa.In Proceedings of the IJCNLP Workshop on Ontolo-
gies and Lexical Resources. Jeju Island, Korea.
Marta Sabou, Chris Wroe, Carole Goble, and GiladMishne. 2005. Learning domain ontologies for web
service descriptions: an experiment in bioinformat-ics. In 14th International World Wide Web Conference
(WWW2005). Chiba, Japan.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.Learning syntactic patterns for automatic hypernymdiscovery. In NIPS 2005. Vancouver, Canada.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006. Sydney,Australia.
Erik Tjong Kim Sang and Katja Hofmann. 2007. Au-tomatic extraction of dutch hypernym-hyponym pairs.In Proceedings of the Seventeenth Computational Lin-
guistics in the Netherlands. Katholieke UniversiteitLeuven, Belgium.
Piek Vossen. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. KluwerAcademic Publisher.
168
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 229?232, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Applying spelling error correction techniques
for improving semantic role labelling
Erik Tjong Kim Sang
Informatics Institute
University of Amsterdam, Kruislaan 403
NL-1098 SJ Amsterdam, The Netherlands
erikt@science.uva.nl
Sander Canisius, Antal van den Bosch, Toine Bogers
ILK / Computational Linguistics and AI
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{S.V.M.Canisius,Antal.vdnBosch,
A.M.Bogers}@uvt.nl
1 Introduction
This paper describes our approach to the CoNLL-
2005 shared task: semantic role labelling. We do
many of the obvious things that can be found in the
other submissions as well. We use syntactic trees
for deriving instances, partly at the constituent level
and partly at the word level. On both levels we edit
the data down to only the predicted positive cases
of verb-constituent or verb-word pairs exhibiting a
verb-argument relation, and we train two next-level
classifiers that assign the appropriate labels to the
positively classified cases. Each classifier is trained
on data in which the features have been selected to
optimize generalization performance on the particu-
lar task. We apply different machine learning algo-
rithms and combine their predictions.
As a novel addition, we designed an automatically
trained post-processing module that attempts to cor-
rect some of the errors made by the base system.
To this purpose we borrowed Levenshtein-distance-
based correction, a method from spelling error cor-
rection to repair mistakes in sequences of labels. We
adapted the method to our needs and applied it for
improving semantic role labelling output. This pa-
per presents the results of our approach.
2 Data and features
The CoNLL-2005 shared task data sets provide sen-
tences in which predicate?argument relations have
been annotated, as well as a number of extra anno-
tations like named entities and full syntactic parses
(Carreras and Ma`rquez, 2005). We have used the
parses for generating machine learning instances for
pairs of predicates and syntactic phrases. In princi-
ple each phrase can have a relation with each verb
in the same sentence. However, in order to keep
the number of instances at a reasonable number, we
have only built instances for verb?phrase pairs when
the phrase parent is an ancestor of the verb (400,128
training instances). A reasonable number of ar-
guments are individual words; these do not match
with phrase boundaries. In order to be able to label
these, we have also generated instances for all pairs
of verbs and individual words using the same con-
straint (another 542,217 instances). The parent node
constraint makes certain that embedded arguments,
which do not occur in these data sets, cannot be pre-
dicted by our approach.
Instances which are associated with verb?
argument pairs receive the label of the argument as
class while others in principle receive a NULL class.
In an estimated 10% of the cases, the phrase bound-
aries assigned by the parser are different from those
in the argument annotation. In case of a mismatch,
we have always used the argument label of the first
word of a phrase as the class of the corresponding
instance. By doing this we attempt to keep the posi-
tional information of the lost argument in the train-
ing data. Both the parser phrase boundary errors as
well as the parent node constraint restrict the num-
ber of phrases we can identify. The maximum recall
score attainable with our phrases is 84.64% for the
development data set.
We have experimentally evaluated 30 features
based on the previous work in semantic role la-
belling (Gildea and Jurafsky, 2002; Pradhan et al,
2004; Xue and Palmer, 2004):
? Lexical features (5): predicate (verb), first
phrase word, last phrase word and words im-
mediately before and after the phrase.
? Syntactic features (14): part-of-speech tags
(POS) of: first phrase word, last phrase word,
229
word immediately before phrase and word im-
mediately after phrase; syntactic paths from
word to verb: all paths, only paths for words
before verb and only paths for words after verb;
phrase label, label of phrase parent, subcate-
gorisation of verb parent, predicate frame from
PropBank, voice, head preposition for preposi-
tional phrases and same parents flag.
? Semantic features (2): named entity tag for
first phrase word and last phrase word.
? Positional features (3): position of the phrase
with respect to the verb: left/right, distance in
words and distance in parent nodes.
? Combination features (6): predicate + phrase
label, predicate + first phrase word, predicate
+ last phrase word, predicate + first phrase
POS, predicate + last phrase POS and voice +
left/right.
The output of two parsers was available. We have
briefly experimented with the Collins parses includ-
ing the available punctuation corrections but found
that our approach reached a better performance with
the Charniak parses. We report only on the results
obtained with the Charniak parses.
3 Approach
This section gives a brief overview of the three main
components of our approach: machine learning, au-
tomatic feature selection and post-processing by a
novel procedure designed to clean up the classifier
output by correcting obvious misclassifications.
3.1 Machine learning
The core machine learning technique employed, is
memory-based learning, a supervised inductive al-
gorithm for learning classification tasks based on the
k-nn algorithm. We use the TiMBL system (Daele-
mans et al, 2003), version 5.0.0, patch-2 with uni-
form feature weighting and random tiebreaking (op-
tions: -w 0 -R 911). We have also evaluated two al-
ternative learning techniques. First, Maximum En-
tropy Models, for which we employed Zhang Le?s
Maximum Entropy Toolkit, version 20041229 with
default parameters. Second, Support Vector Ma-
chines for which we used Taku Kudo?s YamCha
(Kudo and Matsumoto, 2003), with one-versus-all
voting and option -V which enabled us to ignore pre-
dicted classes with negative distances.
3.2 Feature selection
In previous research, we have found that memory-
based learning is rather sensitive to the chosen fea-
tures. In particular, irrelevant or redundant fea-
tures may lead to reduced performance. In order
to minimise the effects of this sensitivity, we have
employed bi-directional hill-climbing (Caruana and
Freitag, 1994) for finding the features that were most
suited for this task. This process starts with an empty
feature set, examines the effect of adding or remov-
ing one feature and then starts a new iteration with
the set associated with the best performance.
3.3 Automatic post-processing
Certain misclassifications by the semantic role-
labelling system described so far lead to unlikely and
impossible relation assignments, such as assigning
two indirect objects to a verb where only one is pos-
sible. Our proposed classifier has no mechanism to
detect these errors. One solution is to devise a post-
processing step that transforms the resulting role as-
signments until they meet certain basic constraints,
such as the rule that each verb may have only sin-
gle instances of the different roles assigned in one
sentence (Van den Bosch et al, 2004).
We propose an alternative automatically-trained
post-processing method which corrects unlikely role
assignments either by deleting them or by replacing
them with a more likely one. We do not do this by
knowledge-based constraint satisfaction, but rather
by adopting a method for error correction based on
Levenshtein distance (Levenshtein, 1965), or edit
distance, as used commonly in spelling error correc-
tion. Levenshtein distance is a dynamically com-
puted distance between two strings, accounting for
the number of deletions, insertions, and substitu-
tions needed to transform the one string into the
other. Levenshtein-based error correction typically
matches a new, possibly incorrect, string to a trusted
lexicon of assumedly correct strings, finds the lex-
icon string with the smallest Levenshtein distance
to the new string, and replaces the new string with
the lexicon string as its likely correction. We imple-
mented a roughly similar procedure. First, we gener-
ated a lexicon of semantic role labelling patterns of
A0?A5 arguments of verbs on the basis of the entire
training corpus and the PropBank verb frames. This
230
lexicon contains entries such as abandon A0 V A1,
and categorize A1 V A2 ? a total of 43,033 variable-
length role labelling patterns.
Next, given a new test sentence, we consider all
of its verbs and their respective predicted role la-
bellings, and compare each with the lexicon, search-
ing the role labelling pattern with the same verb at
the smallest Levenshtein distance (in case of an un-
known verb we search in the entire lexicon). For
example, in a test sentence the pattern emphasize A0
V A1 A0 is predicted. One closest lexicon item is
found at Levenshtein distance 1, namely emphasize
A0 V A1, representing a deletion of the final A0. We
then use the nearest-neighbour pattern in the lexicon
to correct the likely error, and apply all deletions
and substitutions needed to correct the current pat-
tern according to the nearest-neighbour pattern from
the trusted lexicon. We do not apply insertions, since
the post-processor module does not have the infor-
mation to decide which constituent or word would
receive the inserted label. In case of multiple possi-
ble deletions (e.g. in deleting one out of two A1s in
emphasize A0 V A1 A1), we always delete the argu-
ment furthest from the verb.
4 Results
In order to perform the optimisation of the seman-
tic role labelling process in a reasonable amount of
time, we have divided it in four separate tasks: prun-
ing the data for individual words and the data for
phrases, and labelling of these two data sets. Prun-
ing amounts to deciding which instances correspond
with verb-argument pairs and which do not. This
resulted in a considerable reduction of the two data
sets: 47% for the phrase data and 80% for the word
data. The remaining instances are assumed to de-
fine verb-argument pairs and the labelling tasks as-
sign labels to them. We have performed a sepa-
rate feature selection process in combination with
the memory-based learner for each of the four tasks.
First we selected the best feature set based on task
accuracy. As soon as a working module for each of
the tasks was available, we performed an extra fea-
ture selection process for each of the modules, opti-
mising overall system F?=1 while keeping the other
three modules fixed.
The effect of the features on the overall perfor-
Words Phrases
Features prune label prune label
predicate -0.04 +0.05 -0.25 -0.52
first word +0.38 +0.16 -0.17 +1.14
last word ? ? -0.01 +1.12
previous word -0.06 +0.02 -0.05 +0.74
next word -0.04 -0.08 +0.44 -0.16
part-of-speech first word -0.01 -0.02 -0.07 -0.11
part-of-speech last word ? ? -0.14 -0.45
previous part-of-speech -0.12 -0.06 +0.22 -1.14
next part-of-speech -0.08 -0.12 -0.01 -0.21
all paths +0.42 +0.10 +0.84 +0.75
path before verb +0.00 -0.02 +0.00 +0.27
path after verb -0.01 -0.01 -0.01 -0.06
phrase label -0.01 -0.02 +0.13 -0.02
parent label +0.03 -0.02 -0.03 +0.00
voice +0.02 -0.04 -0.04 +1.85
subcategorisation -0.01 +0.00 -0.02 +0.03
PropBank frame -0.12 -0.03 -0.16 +1.04
PP head +0.00 +0.00 -0.06 +0.08
same parents -0.02 -0.01 +0.03 -0.05
named entity first word +0.00 +0.00 +0.05 -0.11
named entity last word ? ? -0.04 -0.12
absolute position +0.00 +0.00 +0.00 -0.02
distance in words +0.34 +0.04 +0.16 -0.96
distance in parents -0.02 -0.02 +0.06 -0.04
predicate + label -0.05 -0.07 -0.22 -0.47
predicate + first word -0.05 +0.00 +0.13 +0.97
predicate + last word ? ? -0.03 +0.08
predicate + first POS -0.05 -0.06 -0.20 -0.50
predicate + last POS ? ? -0.13 -0.40
voice + position +0.02 -0.04 -0.05 -0.04
Table 1: Effect of adding a feature to the best feature
sets when memory-based learning is applied to the
development set (overall F?=1). The process con-
sisted of four tasks: pruning data sets for individual
words and phrases, and labelling these two data sets.
Selected features are shown in bold. Unfortunately,
we have not been able to use all promising features.
mance can be found in Table 1. One feature (syntac-
tic path) was selected in all four tasks but in general
different features were required for optimal perfor-
mance in the four tasks. Changing the feature set
had the largest effect when labelling the phrase data.
We have applied the two other learners, Maximum
Entropy Models and Support Vector Machines to the
two labelling tasks, while using the same features as
the memory-based learner. The performance of the
three systems on the development data can be found
in Table 3. Since the systems performed differently
we have also evaluated the performance of a com-
bined system which always chose the majority class
assigned to an instance and the class of the strongest
system (SVM) in case of a three-way tie. The com-
bined system performed slightly better than the best
231
Precision Recall F?=1
Development 76.79% 70.01% 73.24
Test WSJ 79.03% 72.03% 75.37
Test Brown 70.45% 60.13% 64.88
Test WSJ+Brown 77.94% 70.44% 74.00
Test WSJ Precision Recall F?=1
Overall 79.03% 72.03% 75.37
A0 85.65% 81.73% 83.64
A1 76.97% 71.89% 74.34
A2 71.07% 58.20% 63.99
A3 69.29% 50.87% 58.67
A4 75.56% 66.67% 70.83
A5 100.00% 40.00% 57.14
AM-ADV 64.36% 51.38% 57.14
AM-CAU 75.56% 46.58% 57.63
AM-DIR 48.98% 28.24% 35.82
AM-DIS 81.88% 79.06% 80.45
AM-EXT 87.50% 43.75% 58.33
AM-LOC 62.50% 50.96% 56.15
AM-MNR 64.52% 52.33% 57.78
AM-MOD 96.76% 97.64% 97.20
AM-NEG 97.38% 96.96% 97.17
AM-PNC 45.98% 34.78% 39.60
AM-PRD 50.00% 20.00% 28.57
AM-REC 0.00% 0.00% 0.00
AM-TMP 80.52% 70.75% 75.32
R-A0 81.47% 84.38% 82.89
R-A1 74.00% 71.15% 72.55
R-A2 60.00% 37.50% 46.15
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 100.00% 25.00% 40.00
R-AM-EXT 100.00% 100.00% 100.00
R-AM-LOC 86.67% 61.90% 72.22
R-AM-MNR 33.33% 33.33% 33.33
R-AM-TMP 64.41% 73.08% 68.47
V 97.36% 97.36% 97.36
Table 2: Overall results (top) and detailed results on
the WSJ test (bottom).
individual system.
5 Conclusion
We have presented a machine learning approach to
semantic role labelling based on full parses. We
have split the process in four separate tasks: prun-
ing the data bases of word-based and phrase-based
examples down to only the positive verb-argument
cases, and labelling the two positively classified data
sets. A novel automatic post-processing procedure
based on spelling correction, comparing to a trusted
lexicon of verb-argument patterns from the training
material, was able to achieve a performance increase
by correcting unlikely role assignments.
Learning algorithm Precision Recall F?=1
without post-processing:
Maximum Entropy Models 70.78% 70.03% 70.40
Memory-Based Learning 70.70% 69.85% 70.27
Support Vector Machines 75.07% 69.15% 71.98
including post-processing:
Maximum Entropy Models 74.06% 69.84% 71.89
Memory-Based Learning 73.84% 69.88% 71.80
Support Vector Machines 77.75% 69.11% 73.17
Combination 76.79% 70.01% 73.24
Table 3: Effect of the choice of machine learning
algorithm, the application of Levenshtein-distance-
based post-processing and the use of system combi-
nation on the performance obtained for the develop-
ment data set.
Acknowledgements
This research was funded by NWO, the Netherlands
Organisation for Scientific Research, and by Senter-
Novem IOP-MMI.
References
X. Carreras and L. Ma`rquez. 2005. Introduction to the CoNLL-
2005 Shared Task: Semantic Role Labeling. In Proceedings
of CoNLL-2005. Ann Arbor, MI, USA.
R. Caruana and D. Freitag. 1994. Greedy attribute selection.
In Proceedings of the Eleventh International Conference on
Machine Learning, pages 28?36, New Brunswick, NJ, USA.
Morgan Kaufman.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2003. TiMBL: Tilburg memory based learner, ver-
sion 5.0, reference guide. ILK Technical Report 03-10,
Tilburg University.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 28(3):245?288.
T. Kudo and Y. Matsumoto. 2003. Fast methods for kernel-
based text analysis. In Proceedings of ACL-2003. Sapporo,
Japan.
V. Levenshtein. 1965. Binary codes capable of correcting
deletions, insertions and reversals. Doklady Akademii Nauk
SSSR, 163(4):845?848.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky.
2004. Shallow semantic parsing using support vector ma-
chines. In Proceedings of the HLT/NAACL 2004. Boston,
MA.
A. van den Bosch, S. Canisius, W. Daelemans, I Hendrickx,
and E. Tjong Kim Sang. 2004. Memory-based semantic
role labeling: Optimizing features, algorithm, and output. In
Proceedings of the CoNLL-2004, Boston, MA, USA.
N. Xue and M. Palmer. 2004. Calibrating features for semantic
role labeling. In Proceedings of EMNLP-2004. Barcelona,
Spain.
232
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 176?180, New York City, June 2006. c?2006 Association for Computational Linguistics
Dependency Parsing by Inference over High-recall Dependency Predictions
Sander Canisius, Toine Bogers,
Antal van den Bosch, Jeroen Geertzen
ILK / Computational Linguistics and AI
Tilburg University, P.O. Box 90153,
NL-5000 LE Tilburg, The Netherlands
{S.V.M.Canisius,A.M.Bogers,
Antal.vdnBosch,J.Geertzen}@uvt.nl
Erik Tjong Kim Sang
Informatics Institute
University of Amsterdam, Kruislaan 403
NL-1098 SJ Amsterdam, The Netherlands
erikt@science.uva.nl
1 Introduction
As more and more syntactically-annotated corpora
become available for a wide variety of languages,
machine learning approaches to parsing gain inter-
est as a means of developing parsers without having
to repeat some of the labor-intensive and language-
specific activities required for traditional parser de-
velopment, such as manual grammar engineering,
for each new language. The CoNLL-X shared task
on multi-lingual dependency parsing (Buchholz et
al., 2006) aims to evaluate and advance the state-of-
the-art in machine learning-based dependency pars-
ing by providing a standard benchmark set compris-
ing thirteen languages1. In this paper, we describe
two different machine learning approaches to the
CoNLL-X shared task.
Before introducing the two learning-based ap-
proaches, we first describe a number of baselines,
which provide simple reference scores giving some
sense of the difficulty of each language. Next, we
present two machine learning systems: 1) an ap-
proach that directly predicts all dependency relations
in a single run over the input sentence, and 2) a cas-
cade of phrase recognizers. The first approach has
been found to perform best and was selected for sub-
mission to the competition. We conclude this paper
with a detailed error analysis of its output for two of
the thirteen languages, Dutch and Spanish.
1The data sets were extracted from various existing tree-
banks (Hajic? et al, 2004; Simov et al, 2005; Simov and Osen-
ova, 2003; Chen et al, 2003; Bo?hmova? et al, 2003; Kromann,
2003; van der Beek et al, 2002; Brants et al, 2002; Kawata and
Bartels, 2000; Afonso et al, 2002; Dz?eroski et al, 2006; Civit
Torruella and Mart?? Anton??n, 2002; Nilsson et al, 2005; Oflazer
et al, 2003; Atalay et al, 2003)
2 Baseline approaches
Given the diverse range of languages involved in
the shared task, each having different characteristics
probably requiring different parsing strategies, we
developed four different baseline approaches for as-
signing labeled dependency structures to sentences.
All of the baselines produce strictly projective struc-
tures. While the simple rules implementing these
baselines are insufficient for achieving state-of-the-
art performance, they do serve a useful role in giving
a sense of the difficulty of each of the thirteen lan-
guages. The heuristics for constructing the trees and
labeling the relations used by each of the four base-
lines are described below.
Binary right-branching trees The first baseline
produces right-branching binary trees. The first to-
ken in the sentence is marked as the top node with
HEAD 0 and DEPREL ROOT. For the rest of the
tree, token n ? 1 serves as the HEAD of token n.
Figure 1 shows an example of the kind of tree this
baseline produces.
Binary left-branching trees The binary left-
branching baseline mirrors the previous baseline.
The penultimate token in the sentence is marked as
the top node with HEAD 0 and DEPREL ROOT
since punctuation tokens can never serve as ROOT2.
For the rest of the tree, the HEAD of token n is token
n+1. Figure 2 shows an example of a tree produced
by this baseline.
2We simply assume the final token in the sentence to be
punctuation.
176
Inward-branching trees In this approach, the
first identified verb3 is marked as the ROOT node.
The part of the sentence to the left of the ROOT is
left-branching, the part to the right of the ROOT is
right-branching. Figure 3 shows an example of a
tree produced by this third baseline.
Nearest neighbor-branching trees In our most
complex baseline, the first verb is marked as the
ROOT node and the other verbs (with DEPREL vc)
point to the closest preceding verb. The other to-
kens point in the direction of their nearest neighbor-
ing verb, i.e. the two tokens at a distance of 1 from
a verb have that verb as their HEAD, the two tokens
at a distance of 2 have the tokens at a distance of 1
as their head, and so on until another verb is a closer
neighbor. In the case of ties, i.e. tokens that are
equally distant from two different verbs, the token is
linked to the preceding token. Figure 4 clarifies this
kind of dependency structure in an example tree.
verb verb punct
ROOT
Figure 1: Binary right-branching tree for an example
sentence with two verbs.
verb verb punct
ROOT
Figure 2: Binary left-branching tree for the example
sentence.
verb verb punct
ROOT
Figure 3: Binary inward-branching tree for the ex-
ample sentence.
3We consider a token a verb if its CPOSTAG starts with a
?V?. This is an obviously imperfect, but language-independent
heuristic choice.
ROOT
verb verb punct
Figure 4: Nearest neighbor-branching tree for the
example sentence.
Labeling of identified relations is done using a
three-fold back-off strategy. From the training set,
we collect the most frequent DEPREL tag for each
head-dependent FORM pair, the most frequent DE-
PREL tag for each FORM, and the most frequent
DEPREL tag in the entire training set. The rela-
tions are labeled in this order: first, we look up if the
FORM pair of a token and its head was present in
the training data. If not, then we assign it the most
frequent DEPREL tag in the training data for that
specific token FORM. If all else fails we label the
token with the most frequent DEPREL tag in the en-
tire training set (excluding punct4 and ROOT).
language baseline unlabeled labeled
Arabic left 58.82 39.72
Bulgarian inward 41.29 29.50
Chinese NN 37.18 25.35
Czech NN 34.70 22.28
Danish inward 50.22 36.83
Dutch NN 34.07 26.87
German NN 33.71 26.42
Japanese right 67.18 64.22
Portuguese right 25.67 22.32
Slovene right 24.12 19.42
Spanish inward 32.98 27.47
Swedish NN 34.30 21.47
Turkish right 49.03 31.85
Table 1: The labeled and unlabeled scores for the
best performing baseline for each language (NN =
nearest neighbor-branching).
The best baseline performance (labeled and un-
labeled scores) for each language is listed in Table
1. There was no single baseline that outperformed
the others on all languages. The nearest neighbor
baseline outperformed the other baselines on five
of the thirteen languages. The right-branching and
4Since the evaluation did not score on punctuation.
177
inward-branching baselines were optimal on four
and three languages respectively. The only language
where the left-branching trees provide the best per-
formance is Arabic.
3 Parsing by inference over high-recall
dependency predictions
In our approach to dependency parsing, a machine
learning classifier is trained to predict (directed) la-
beled dependency relations between a head and a de-
pendent. For each token in a sentence, instances are
generated where this token is a potential dependent
of each of the other tokens in the sentence5. The
label that is predicted for each classification case
serves two different purposes at once: 1) it signals
whether the token is a dependent of the designated
head token, and 2) if the instance does in fact corre-
spond to a dependency relation in the resulting parse
of the input sentence, it specifies the type of this re-
lation, as well.
The features we used for encoding instances for
this classification task correspond to a rather simple
description of the head-dependent pair to be clas-
sified. For both the potential head and dependent,
there are features encoding a 2-1-2 window of words
and part-of-speech tags6; in addition, there are two
spatial features: a relative position feature, encoding
whether the dependent is located to the left or to the
right of its potential head, and a distance feature that
expresses the number of tokens between the depen-
dent and its head.
One issue that may arise when considering each
potential dependency relation as a separate classifi-
cation case is that inconsistent trees are produced.
For example, a token may be predicted to be a de-
pendent of more than one head. To recover a valid
dependency tree from the separate dependency pre-
dictions, a simple inference procedure is performed.
Consider a token for which the dependency relation
is to be predicted. For this token, a number of clas-
sification cases have been processed, each of them
5To prevent explosion of the number of classification cases
to be considered for a sentence, we restrict the maximum dis-
tance between a token and its potential head. For each language,
we selected this distance so that, on the training data, 95% of the
dependency relations is covered.
6More specifically, we used the part-of-speech tags from the
POSTAG column of the shared task data files.
indicating whether and if so how the token is related
to one of the other tokens in the sentence. Some of
these predictions may be negative, i.e. the token is
not a dependent of a certain other token in the sen-
tence, others may be positive, suggesting the token
is a dependent of some other token.
If all classifications are negative, the token is as-
sumed to have no head, and consequently no depen-
dency relation is added to the tree for this token; the
node in the dependency tree corresponding to this
token will then be an isolated one. If one of the clas-
sifications is non-negative, suggesting a dependency
relation between this token as a dependent and some
other token as a head, this dependency relation is
added to the tree. Finally, there is the case in which
more than one prediction is non-negative. By defi-
nition, at most one of these predictions can be cor-
rect; therefore, only one dependency relation should
be added to the tree. To select the most-likely can-
didate from the predicted dependency relations, the
candidates are ranked according to the classification
confidence of the base classifier that predicted them,
and the highest-ranked candidate is selected for in-
sertion into the tree.
For our base classifier we used a memory-based
learner as implemented by TiMBL (Daelemans et
al., 2004). In memory-based learning, a machine
learning method based on the nearest-neighbor rule,
the class for a given test instance is predicted by per-
forming weighted voting over the class labels of a
certain number of most-similar training instances.
As a simple measure of confidence for such a pre-
diction, we divide the weight assigned to the major-
ity class by the total weight assigned to all classes.
Though this confidence measure is a rather ad-hoc
one, which should certainly not be confused with
any kind of probability, it tends to work quite well
in practice, and arguably did so in the context of
this study. The parameters of the memory-based
learner have been optimized for accuracy separately
for each language on training and development data
sampled internally from the training set.
The base classifier in our parser is faced with a
classification task with a highly skewed class dis-
tribution, i.e. instances that correspond to a depen-
dency relation are largely outnumbered by those that
do not. In practice, such a huge number of nega-
tive instances usually results in classifiers that tend
178
to predict fairly conservatively, resulting in high pre-
cision, but low recall. In the approach introduced
above, however, it is better to have high recall, even
at the cost of precision, than to have high precision at
the cost of recall. A missed relation by the base clas-
sifier can never be recovered by the inference proce-
dure; however, due to the constraint that each token
can only be a dependent of one head, excessive pre-
diction of dependency relations can still be corrected
by the inference procedure. An effective method for
increasing the recall of a classifier is down-sampling
of the training data. In down-sampling, instances
belonging to the majority class (in this case the neg-
ative class) are removed from the training data, so
as to obtain a more balanced distribution of negative
and non-negative instances.
Figure 5 shows the effect of systematically re-
moving an increasingly larger part of the negative in-
stances from the training data. First of all, the figure
confirms that down-sampling helps to improve re-
call, though it does so at the cost of precision. More
importantly however, it also illustrates that this im-
proved recall is beneficial for the performance of the
dependency parser. The shape of the performance
curve of the dependency parser closely follows that
of the recall. Remarkably, parsing performance con-
tinues to improve with increasingly stronger down-
sampling, even though precision drops considerably
as a result of this. This shows that the confidence
of the classifier for a certain prediction is a suffi-
ciently reliable indication of the quality of that pre-
diction for fixing the over-prediction of dependency
relations. Only when the number of negative train-
ing instances is reduced to equal the number of pos-
itive instances, the performance of the parser is neg-
atively affected. Based on a quick evaluation of var-
ious down-sampling ratios on a 90%-10% train-test
split of the Dutch training data, we decided to down-
sample the training data for all languages with a ratio
of two negative instances for each positive one.
Table 2 lists the unlabeled and labeled attachment
scores of the resulting system for all thirteen lan-
guages.
4 Cascaded dependency parsing
One of the alternative strategies explored by us was
modeling the parsing process as a cascaded pair of
 0
 20
 40
 60
 80
 100
 2 4 6 8 10
Sampling ratio
PrecisionRecallSystem LAS
Figure 5: The effect of down-sampling on precision
and recall of the base classifier, and on labeled ac-
curacy of the dependency parser. The x-axis refers
to the number of negative instances for each posi-
tive instance in the training data. Training and test-
ing was performed on a 90%-10% split of the Dutch
training data.
basic learners. This approach is similar to Yamada
and Matsumoto (2003) but we only use their Left
and Right reduction operators, not Shift. In the first
phase, each learner predicted dependencies between
neighboring words. Dependent words were removed
and the remaining words were sent to the learners for
further rounds of processing until all words but one
had been assigned a head. Whenever crossing links
prevented further assignments of heads to words, the
learner removed the remaining word requiring the
longest dependency link. When the first phase was
finished another learner assigned labels to pairs of
words present in dependency links.
Unlike in related earlier work (Tjong Kim Sang,
2002), we were unable to compare many different
learner configurations. We used two different train-
ing files for the first phase: one for predicting the
dependency links between adjacent words and one
for predicting all other links. As a learner, we used
TiMBL with its default parameters. We evaluated
different feature sets and ended up with using words,
lemmas, POS tags and an extra pair of features with
the POS tags of the children of the focus word. With
this configuration, this cascaded approach achieved
a labeled score of 62.99 on the Dutch test data com-
pared to 74.59 achieved by our main approach.
179
language unlabeled labeled
Arabic 74.59 57.64
Bulgarian 82.51 78.74
Chinese 82.86 78.37
Czech 72.88 60.92
Danish 82.93 77.90
Dutch 77.79 74.59
German 80.01 77.56
Japanese 89.67 87.41
Portuguese 85.61 77.42
Slovene 74.02 59.19
Spanish 71.33 68.32
Swedish 85.08 79.15
Turkish 64.19 51.07
Table 2: The labeled and unlabeled scores for the
submitted system for each of the thirteen languages.
5 Error analysis
We examined the system output for two languages
in more detail: Dutch and Spanish.
5.1 Dutch
With a labeled attachment score of 74.59 and an
unlabeled attachment score of 77.79, our submitted
Dutch system performs somewhat above the average
over all submitted systems (labeled 70.73, unlabeled
75.07). We review the most notable errors made by
our system.
From a part-of-speech (CPOSTAG) perspective,
a remarkable relative amount of head and depen-
dency errors are made on conjunctions. A likely
explanation is that the tag ?Conj? applies to both co-
ordinating and subordinating conjunctions; we did
not use the FEATS information that made this dis-
tinction, which would have likely solved some of
these errors.
Left- and right-directed attachment to heads is
roughly equally successful. Many errors are made
on relations attaching to ROOT; the system appears
to be overgenerating attachments to ROOT, mostly
in cases when it should have generated rightward
attachments. Unsurprisingly, the more distant the
head is, the less accurate the attachment; especially
recall suffers at distances of three and more tokens.
The most frequent attachment error is generat-
ing a ROOT attachment instead of a ?mod? (mod-
ifier) relation, often occurring at the start of a sen-
tence. Many errors relate to ambiguous adverbs such
as bovendien (moreover), tenslotte (after all), and
zo (thus), which tend to occur rather frequently at
the beginning of sentences in the test set, but less
so in the training set. The test set appears to con-
sist largely of formal journalistic texts which typi-
cally tend to use these marked rhetorical words in
sentence-initial position, while the training set is a
more mixed set of texts from different genres plus
a significant set of individual sentences, often man-
ually constructed to provide particular examples of
syntactic constructions.
5.2 Spanish
The Spanish test data set was the only data set on
which the alternative cascaded approach (72.15) out-
performed our main approach (68.32). A detailed
comparison of the output files of the two systems
has revealed two differences. First, the amount of
circular links, a pair of words which have each other
as head, was larger in the analysis of the submitted
system (7%) than in the cascaded analysis (3%) and
the gold data (also 3%). Second, the number of root
words per sentence (always 1 in the gold data) was
more likely to be correct in the cascaded analysis
(70% correct; other sentences had no root) than in
the submitted approach (40% with 20% of the sen-
tences being assigned no roots and 40% more than
one root). Some of these problems might be solvable
with post-processing
Acknowledgements
This research is funded by NWO, the Netherlands
Organization for Scientific Research under the IMIX
programme, and the Dutch Ministry for Economic
Affairs? IOP-MMI programme.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc. of the Tenth Conf. on Computational Natural Lan-
guage Learning (CoNLL-X). SIGNLL.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2004. TiMBL: Tilburg memory based learner, ver-
sion 5.1, reference guide. Technical Report ILK 04-02, ILK
Research Group, Tilburg University.
Erik Tjong Kim Sang. 2002. Memory-based shallow parsing.
Journal of Machine Learning Research, 2(Mar):559?594.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 8th In-
ternational Workshop of Parsing Technologies (IWPT2003).
Nancy, France.
180
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 148?150,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Baseline Approach for Detecting Sentences Containing Uncertainty
Erik Tjong Kim Sang
University of Groningen
erikt@xs4all.nl
Abstract
We apply a baseline approach to the
CoNLL-2010 shared task data sets on
hedge detection. Weights have been as-
signed to cue words marked in the train-
ing data based on their occurrences in
certain and uncertain sentences. New
sentences received scores that correspond
with those of their best scoring cue word,
if present. The best acceptance scores for
uncertain sentences were determined us-
ing 10-fold cross validation on the training
data. This approach performed reasonably
on the shared task?s biological (F=82.0)
and Wikipedia (F=62.8) data sets.
1 Introduction
CoNLL-2010 offered two shared tasks which in-
volve finding text parts which express uncertainty
or unreliability (Farkas et al, 2010). We focus
on Task 1, identifying sentences which contain
statements which can be considered uncertain or
unreliable. We train a basic statistical model on
the training data supplied for the task, apply the
trained model to the test data and discuss the re-
sults. The next section describes the format of
the data and introduces the model that was used.
Section three discusses the experiments with the
model and their results. Section four concludes
the paper.
2 Data and model
The CoNLL-2010 shared task training data sets
contain sentences which are classified as either
certain or uncertain. Sentences of the uncertain
class contain one or more words which have been
marked as indicator of uncertainty, the so-called
hedge cues. Here is an example of such a sentence
with the hedge cues written in bold font:
These results indicate that in mono-
cytic cell lineage, HIV-1 could mimic
some differentiation/activation stimuli
allowing nuclear NF-KB expression.
CoNLL-2010 offers two shared tasks: classify-
ing sentences in running text as either certain or
uncertain (Task 1) and finding hedge cues in sen-
tences classified as uncertain together with their
scopes (Task 2). We have only participated in
Task 1.
We built a basic model for the training data, tak-
ing advantage of the fact that the hedge cues were
marked explicitly. We estimated the probability of
each training data word appearing in a hedge cue
with unigram statistics:
P (w in cue) = f(w in cue)f(w)
where P (w in cue) is the probability that word w
appears in a hedge cue, f(w) is frequency of the
word w in the data and f(w in c) is the frequency
of the word inside hedge cues. We performed only
little text preprocessing, converting all words to
lower case and separating six common punctua-
tion signs from the words.
In the classification stage, we assigned to each
word the estimated hedge cue probability accord-
ing to the training data. Next, we assigned a score
to each sentence that was equal to one minus the
highest individual score of its words:
P (s is certain) = 1? argmax
w in s
P (w in cue)
P (s is certain) is the estimated probability that
the sentence s is certain, and it is equal to one mi-
nus the highest probability of any of its words be-
ing part of a hedge cue. So a sentence contain-
ing only words that never appeared as a hedge cue
would receive score 1.0. Meanwhile a sentence
148
with a single word that had appeared in a hedge
cue in the training data would receive one minus
the probability associated with that word. This
model ignores any relations between the words
of the sentence. We experimented with combin-
ing the scores of the different words but found the
minimum word score to perform best.
3 Experiments
Apart from the word probabilities, we needed to
obtain a good threshold score for deciding whether
to classify a sentence as certain or uncertain.
For this purpose, we performed a 10-fold cross-
validation experiment on each of the two training
data files (biological andWikipedia) and measured
the effect of different threshold values. The results
can be found in Figure 1.
The model performed well on the biological
training data, with F scores above 80 for a large
range of threshold values (0.15?0.85). It per-
formed less well on the Wikipedia training data,
with a maximum F score of less than 60 and 50+
scores being limited to the threshold range 0.45?
0.85. The maximum F scores were reached for
threshold values 0.55 and 0.65 for biological data
(F=88.8) and Wikipedia data (F=59.4), respec-
tively. We selected the threshold value 0.55 for
our further work because the associated precision
and recall values were closer to each other than for
value 0.65.
We build domain-specific models with the bio-
logical data (14,541 sentences) and the Wikipedia
data (11,111 sentences) and applied the models to
the related training data. We obtained an F score
of 80.2 on the biological data (13th of 20 partici-
pants) and a score of 54.4 on the Wikipedia data
(9th of 15 participants). The balance between pre-
cision and recall scores that we strived for when
processing the training data, was not visible in the
test results. On the biological test data the sys-
tem?s recall score was 13 points higher than the
precision score while on the Wikipedia test data
precision outperformed recall by 31 points (see
Table 1).
Next, we tested the effect of increasing the data
sets with data from another domain. We repeated
the cross-validation experiments with the training
data, this time adding the available data of the
other domain to each of the sets of nine folds used
as training data. Unfortunately, this did not re-
sult in a performance improvement. The best per-
train-test thre. Precis. Recall F?=1
bio-bio .55 74.3% 87.1% 80.2?1.0
wik-wik .55 74.0% 43.0% 54.4?0.9
all-bio .55 69.3% 74.6% 71.8?1.2
all-wik .55 69.0% 44.6% 54.2?1.0
Table 1: Performances of the models for different
combinations of training and test data sets with the
associated acceptance threshold values. Training
and testing with data from the same domain pro-
duces the best scores. Higher recall scores were
obtained for biological data than for Wikipedia
data. Standard deviations for F scores were esti-
mated with bootstrap resampling (Yeh, 2000).
formance for the biological data dropped to F =
84.2 (threshold 0.60) while the top score for the
Wikipedia data dropped to F = 56.5 (0.70).
We kept the threshold value of 0.55, built a
model from all available training data and tested
its performance on the two test sets. In both cases
the performances were lower than the ones ob-
tained with domain dependent training data: F =
71.8 for biological data and F = 54.2 for Wikipedia
data (see Table 1).
As post-deadline work, we added statistics for
word bigrams to the model, following up work
by Medlock (2008), who showed that considering
word bigrams had a positive effect on hedge detec-
tion. We changed the probability estimation score
of words appearing in a hedge cue to
P (wi?1wi in cue) = f(wi?1wi in cue)f(wi?1wi)
where wi?1wi is a bigram of successive words in a
sentence. Bigrams were considered to be part of a
hedge cue when either or both words were inside
the hedge cue. Unigram probabilities were used
as backoff for known words that appeared outside
known bigrams while unknown words received the
most common score for known words (0). Sen-
tences received a score which is equal to one mi-
nus the highest score of their word bigrams:
P (s is certain) = 1? argmax
wi?1wi in s
P (wi?1wi in cue)
We repeated the threshold estimation experiments
and found that new bigram scores enabled the
models to perform slightly better on the training
149
Figure 1: Precision-recall plot (left) and F plot (right) for different values of the certainty acceptance
thresholds measured by 10-fold cross-validation experiments on the two shared task training data sets
(biological and Wikipedia). The best attained F scores were 88.8 for biological data (threshold 0.55) and
59.4 for Wikipedia data (0.65).
data. The maximum F score for biological training
data improved from 88.8 to 90.1 (threshold value
0.35) while the best F score for the Wikipedia
training data moved up slightly to 59.8 (0.65).
We applied the bigram models with the two op-
timal threshold values for the training data to the
test data sets. For the biological data, we obtained
an F score of 82.0, a borderline significant im-
provement over the unigram model score. The
performance on the Wikipedia data improved sig-
nificantly, by eight points, to F = 62.8 (see Table
2). This is also an improvement of the official
best score for this data set (60.2). We believe that
the improvement originates from using the bigram
model as well as applying a threshold value that
is better suitable for the Wikipedia data set (note
that in our unigram experiments we used the same
threshold value for all data sets).
4 Concluding remarks
We applied a baseline model to the sentence clas-
sification part of the CoNLL-2010 shared task on
hedge detection. The model performed reason-
ably on biological data (F=82.0) but less well on
Wikipedia data (F=62.8). The model performed
best when trained and tested on data of the same
domain. Including additional training data from
another domain had a negative effect. Adding bi-
gram statistics to the model, improved its perfor-
mance on Wikipedia data, especially for recall.
Although the model presented in this paper per-
forms reasonably on the hedge detection tasks, it
is probably too simple to outperform more com-
plex models. However, we hope to have shown its
train-test thre. Precis. Recall F?=1
bio-bio .35 79.8% 84.4% 82.0?1.1
wik-wik .65 62.2% 63.5% 62.8?0.8
all-bio .50 73.2% 77.7% 75.4?1.2
all-wik .60 63.5% 57.9% 60.6?0.9
Table 2: Performances of bigram models for dif-
ferent combinations of training and test data sets.
The bigram models performed better than the uni-
gram models (compare with Table 1).
usefulness as baseline and as possible feature for
more advanced models. We were surprised about
the large difference in performance of the model
on the two data sets. However, similar perfor-
mance differences were reported by other partic-
ipants in the shared task, so they seem data-related
rather than being an effect of the chosen model.
Finding the origin of the performance differences
would be an interesting goal for future work.
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proc.
of the Computational Natural Language Learning
(CoNLL-2010): Shared Task, pages 1?12.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41:636?654.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th International Conference on
Computational Linguistics, pages 947?953.
150
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 53?60,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Predicting the 2011 Dutch Senate Election Results with Twitter
Erik Tjong Kim Sang and Johan Bos
Alfa-informatica
University of Groningen
Groningen, The Netherlands
{e.f.tjong.kim.sang,johan.bos}@rug.nl
Abstract
To what extend can one use Twitter in opin-
ion polls for political elections? Merely
counting Twitter messages mentioning po-
litical party names is no guarantee for ob-
taining good election predictions. By im-
proving the quality of the document col-
lection and by performing sentiment anal-
ysis, predictions based on entity counts in
tweets can be considerably improved, and
become nearly as good as traditionally ob-
tained opinion polls.
1 Introduction
Predicting the future is one of human?s great-
est desires. News companies are well aware of
this, and try to predict tomorrow?s weather and
changes on the stock markets. Another case in
point are the opinion polls, of which the news
is abundant in the period before political elec-
tions. Such polls are traditionally based on ask-
ing a (representative) sample of voters what they
would vote on the day of election.
The question we are interested in, is whether
opinion polls could be conducted on the basis
of the information collected by Twitter, a popu-
lar microblog website, used by millions to broad-
cast messages of no more than 140 characters,
known as tweets. Over the last two years, we have
collected a multi-billion-word corpus of Dutch
1The data and software used for the experiments de-
scribed in this paper can be retrieved from http://
ifarm.nl/ps2011/p2011.zip
tweets, with the general aim of developing nat-
ural language processing tools for automatically
analyzing the content of the messages in this new
social medium, which comes with its own chal-
lenges. When the Dutch Senate elections took
place in 2011, we took this as an opportunity to
verify the predictive power of tweets.
More concretely, we wanted to test whether by
simply counting Twitter messages mentioning po-
litical party names we could accurately predict the
election outcome. Secondly, we wanted to inves-
tigate factors that influence the predictions based
on the Dutch tweets.
In this paper we present the results of our exper-
iments. We first summarize related work in Sec-
tion 2. Then we outline our data collection pro-
cess (Section 3). The methods we used for pre-
dicting election results and the obtained results,
are presented in Sections 4, 5 and 6. We discuss
the results of the experiments in Section 7 and
conclude in Section 8.
2 Related work
Tumasjan et al (2010) investigate how Twitter
is used in political discourse and check if polit-
ical sentiment on Twitter reflects real-life senti-
ments about parties and politicians. As a part of
their study, they compare party mentions on Twit-
ter with the results of the 2009 German parliament
election. They conclude that the relative number
of tweets mentioning a party is a good predictor
for the number of votes of that party in an elec-
tion. A similar finding was earlier reported by
Jean Ve?ronis in a series of blogposts: the number
53
Figure 1: Overview of our collection of Dutch tweets of the year 2011. The data set contains almost 700 million
tweets. Both the number of tweets (about two million per day) and the number of unique users (about one
million) increase almost every month. The collection is estimated to contain about 37% of the total volume of
Dutch tweets.
of times a French presidential candidate was men-
tioned in the press was a good prediction for his or
her election results (Ve?ronis, 2007). This predic-
tion task involved only two candidates, so it was
easier than predicting the outcome of a multiparty
election.
Jungherr et al (2011) criticize the work of Tu-
masjan et al (2010). They argue that the choice
of included parties in the evaluation was not well
motivated and show that the inclusion of a seventh
party, the Pirate Party, would have had a large neg-
ative effect on accuracy of the predictions. Fur-
thermore, Jungherr et al question the time period
which was used by Tumasjan et al for collecting
the tweets and show that including the tweets of
the week right before the election would also have
had a significant negative effect on the prediction
accuracy.
Using Twitter data for predicting election re-
sults was popular in 2010 and 2011. Chung
and Mustafaraj (2011) found that merely count-
ing tweets is not enough to obtain good predic-
tions and measure the effect of sentiment analysis
and spam filtering. O?Connor et al (2010) dis-
covered that while volumes of mentions of obama
on Twitter before the US presidential election of
2008 correlated with high poll ratings for Barack
Obama, volumes of mentions of his rival mccain
also correlated with high poll ratings of the elec-
tion winner. Gayo-Avello et al (2011) show that
predictions based on Twitter only predicted half
of the winners of US congressional elections with
two candidates correctly, a performance which is
not better than chance.
3 Data collection
We collect Dutch Twitter messages (tweets) with
the filter stream provided by Twitter. We continu-
ously search for messages that contain at least one
of a list of about a hundred high-frequent Dutch
words and a dozen frequent Dutch subject tags
(hashtags). The results of this process also con-
tain some false positives: tweets that contain ap-
parent Dutch words but are actually written in an-
other language. In order to get rid of these mes-
sages, we apply a language guesser developed by
Thomas Mangin (Mangin, 2007). It ranks lan-
guages by comparing character n-grams of an in-
put text to n-gram models of texts in known lan-
guages. We use a set of 74 language models de-
veloped by our students in 2007.
In order to estimate the coverage of our selec-
tion with respect to all tweets in Dutch, we col-
lected all tweets of one month from 1,017 ran-
domly selected users which predominantly post
messages in Dutch. We compared the two data
streams and found that the first contained 37% of
the data found in the second. This suggests that
we collect about 37% of all Dutch tweets. Our
data collection process contains two filters: one is
based on a word list and the other is the language
guesser. The first filter lost 62% of the data while
the second lost another 1%.
54
Short Long Seats Seats Seats Average
Party name name Total Twitter PB MdH polls
PVV 2226 1 2227 18 12 12 12
VVD 1562 0 1562 13 14 16 15
CDA 1504 0 1504 12 9 10 9.5
PvdA 1056 1 1057 9 13 13 13
SP 839 0 839 7 8 7 7.5
GL 243 505 748 6 5 3 4
D66 610 0 610 5 6 5 5.5
CU 159 79 238 2 3 3 3
PvdD 103 51 154 1 1 1 1
SGP 139 0 139 1 2 2 2
50+ 6 43 49 0 1 2 1.5
OSF - - - 1 1 1 1
offset 21 4 4 -
Table 1: Frequencies of tweets mentioning one of 11 main political parties from one day, Wednesday 16 February
2011, converted to Senate seats (column Seats Twitter) and compared with the predictions of two polls from the
same week: from Politieke Barometer of 17 February (Synovate.nl, 2011b) and from Maurice de Hond of 15
February (Peil.nl, 2011b). The offset value is the sum of the differences between the Twitter predictions and the
average poll predictions. The OSF group is a cooperation of 11 local parties which were not tracked on Twitter.
4 Counting party names
The Dutch Senate elections are held once ev-
ery four years. The elections are preceded by
the Dutch Provincial Election in which the vot-
ers choose 566 representatives for the States-
Provincial. Three months later the new repre-
sentatives elect the new Senate. In the second
election, each of the representatives has a weight
which is proportional to the number of people
he or she represents. The 2011 Dutch provincial
elections were held on Wednesday 2 March 2011
and the corresponding Senate elections were held
on Monday 23 May 2011. In the Senate elections
75 seats are contested.
Our work on predicting the results of this elec-
tion was inspired by the work of Tumasjan et al
(2010), who report that basic counts of tweets
mentioning a political party provided good pre-
dictions for the results of the 2009 German parlia-
ment election. We decided to replicate their work
for the Dutch Senate Elections of 2011.
We started with examining the Dutch tweets of
Wednesday 16 February 2011, two weeks prior
to the Provincial elections. This data set con-
sisted of 1.7 million tweets. From this data set
we extracted the tweets containing names of po-
litical parties. This resulted in 7,000 tweets. This
number was lower than we had expected. Origi-
nally we had planned to use the tweets for predict-
ing local election results. However, further filter-
ing of the tweets to require location information
would have left us with a total of about 70 polit-
ical tweets per day, far too few to make reliable
predictions for twelve different provinces.
In the data, we searched for two variants of
each party: the abbreviated version and the full
name, allowing for minor punctuation and capi-
talization variation. For nearly all parties, the ab-
breviated name was used more often on Twitter
than the full name. The two exceptions are Groen-
Links/GL and 50Plus/50+ (Table 1). Party names
could be identified with a precision close to 100%
except for the party ChristenUnie: its abbreviation
CU is also used as slang for see you. This was the
case for 11% of the tweets containing the phrase
CU. In this paper, the 11% of tweets have already
been removed from the counts of this party.
Apart from the eleven regular parties shown in
Table 1, there was a twelfth party with a chance
of winning a Senate seat: the Independent Senate
Group (OSF), a cooperation of 11 regional par-
55
ties. These parties occur infrequently in our Twit-
ter data (less than five times per party per day),
too infrequent to allow for a reliable base for pre-
dicting election results. Therefore we decided to
use a baseline prediction for them. We assumed
that the group would win exactly one Senate seat,
just like in the two previous elections.
We converted the counts of the party names on
Twitter to Senate seats by counting every tweet
mentioning a party name as a vote for that party.
The results can be found in the column Seats Twit-
ter in Table 1. The predicted number of seats
were compared with the results of two polls of the
same week: one by the polling company Politieke
Barometer of 17 February (Synovate.nl, 2011b)
and another from the company Peil.nl, commonly
referred to as Maurice de Hond, from 15 February
(Peil.nl, 2011b). The predicted numbers of seats
by Twitter were reasonably close to the numbers
of the polling companies. However, there is room
for improvement: for the party PVV, tweets pre-
dicted a total of 18 seats while the polling com-
panies only predicted 12 and for the party 50+,
Twitter predicted no seats while the average of the
polling companies was 1.5 seats.
5 Normalizing party counts
The differences between the Twitter prediction
and prediction of the polling companies could
have been caused by noise. However, the differ-
ences could also have resulted from differences
between the methods for computing the predic-
tions. First, in the polls, like in an election, every-
one has one vote. In the tweet data set this is not
the case. One person may have send out multiple
tweets or may have tweeted about different politi-
cal parties. This problem of the data is easy to fix:
we can keep only one political tweet per user in
the data set and remove all others.
A second problem is that not every message
containing a party name is necessarily positive
about the party. For example:
Wel triest van de vvd om de zondagen
nu te schrappen wat betreft het shop-
pen, jammer! Hierbij dus een #fail
Sadly, the VVD will ban shopping on
Sundays, too bad! So here is a #fail
One party One tweet Both
Party per tweet per user constraints
PVV 22 17 19
VVD 12 13 13
CDA 12 12 12
PvdA 8 8 8
SP 6 8 7
GL 6 7 7
D66 5 5 5
CU 1 2 2
PvdD 1 1 1
SGP 1 1 0
50+ 0 0 0
OSF 1 1 1
offset 29 22 25
Table 2: Senate seat predictions based on normalized
tweets: keeping only tweets mentioning one party,
keeping only the first tweet of each user and keeping of
each user only the first tweet which mentioned a single
party. The offset score is the seat difference between
the predictions and the average poll prediction of Ta-
ble 1.
While the tweet is mentioning a political party,
the sender does not agree with the policy of the
party and most likely will not vote for the party.
These tweets need to be removed as well.
A third problem with the data is that the demo-
graphics of Dutch Twitter users are probably quite
different from the demographics of Dutch voters.
Inspection of Dutch tweets revealed that Twitter is
very popular among Dutch teens but they are not
eligible to vote. User studies for other countries
have revealed that senior citizens are underrepre-
sented on the Internet (Fox, 2010) but this group
has a big turnout in elections (Epskamp and van
Rhee, 2010). It would be nice if we could as-
sign weights to tweets based on the representa-
tiveness of certain groups of users. Unfortunately
we cannot determine the age and gender of indi-
vidual Twitter users because users are not required
to specify this information in their profile.
Based on the previous analysis, we tested two
normalization steps for the tweet data. First, we
removed all tweets that mentioned more than one
party name. Next, we kept only the first tweet of
each user. Finally we combined both steps: keep-
56
ing of each user only the first tweet which men-
tioned a single political party. We converted all
the counts to party seats and compared them with
the poll outcomes. The results can be found in
Table 2. The seat predictions did not improve. In
fact, the offsets of the three methods proved to be
larger than the corresponding number of the base-
line approach without normalization (29, 25 and
22 compared to 21). Still, we believe that normal-
ization of the tweet counts is a good idea.
Next, we determined the sentiments of the
tweets. Since we do not have reliable automatic
sentiment analysis software for Dutch, we de-
cided to build a corpus of political tweets with
manual sentiment annotation. Each of the two au-
thors of this paper manually annotated 1,678 po-
litical tweets, assigning one of two classes to each
tweet: negative towards the party mentioned in
the tweet or nonnegative. The annotators agreed
on the sentiment of 1,333 tweets (kappa score:
0.59).
We used these 1,333 tweets with unanimous
class assignment for computing sentiment scores
per party. We removed the tweets that mentioned
more than one party and removed duplicate tweets
of users that contributed more than one tweet. 534
nonnegative tweets and 227 negative tweets were
left. Then we computed weights per party by di-
viding the number of nonnegative tweets per party
by the associated total number of tweets. For ex-
ample, there were 42 negative tweets for the VVD
party and 89 nonnegative, resulting in a weight of
89/(42+89) = 0.68. The resulting party weights
can be found in Table 3.
We multiplied the weights with the tweet
counts obtained after the two normalization steps
and converted these to Senate seats. As a result
the difference with the poll prediction dropped
from 25 to 23 (see Table 3). Incorporating sen-
timent analysis improved the results of the pre-
diction.
After sentiment analysis, the tweets still did not
predict the same number of seats as the polls for
any party. For nine parties, the difference was
two and a half seats or lower but the difference
was larger for two parties: GL (5) and PvdA (6).
A possible cause for these differences is a mis-
match between the demographics of Twitter users
Tweet Sentiment Seats
Party count weight Twitter
PVV 811 0.49 13
VVD 552 0.68 13
CDA 521 0.70 12
PvdA 330 0.69 7
SP 314 0.90 9
GL 322 0.81 9
D66 207 0.94 6
CU 104 0.67 2
PvdD 63 1.00 2
SGP 39 0.86 1
50+ 17 0.93 0
OSF - - 1
offset 23
Table 3: Sentiment weights per party resulting from
a manual sentiment analysis, indicating what fraction
of tweets mentioning the party is nonnegative and the
resulting normalized seat predictions after multiplying
tweet counts with these weights. The second column
contains the number of tweets per party after the nor-
malization steps of Table 2.
and the Dutch population. We have no data de-
scribing this discrepancy. We wanted to build a
model for this difference so we chose to model the
difference by additional correction weights based
on the seats differences between the two predic-
tions. We based the expected number of seats on
the two poll results of the same time period as
the tweets (Synovate.nl, 2011b; Peil.nl, 2011b).
For example, after normalization, there were 811
tweets mentioning the PVV party. The party has a
sentiment weight of 0.49 so the adjusted number
of tweets is 0.49*811 = 397. The polls predicted
12 of 74 seats for this party. The associated pop-
ulation weight is equal to the average number of
poll seats divided by the total number of seats di-
vided by the adjusted number of tweets divided
by the total number of adjusted tweets (2,285):
(12/74)/(397/2285) is 0.93.
The population weights can be found in Table
4. They corrected most predicted seat numbers
of Twitter to the ones predicted by the polls. A
drawback of this approach is that we have tuned
the prediction system to the results of polls rather
than to the results of elections. It would have been
57
Population Seats Average
Party weight Twitter polls
PVV 0.93 12 12
VVD 1.23 15 15
CDA 0.80 10 9.5
PvdA 1.76 13 13
SP 0.82 8 7.5
GL 0.47 4 4
D66 0.87 5 5.5
CU 1.33 3 3
PvdD 0.49 1 1
SGP 1.84 2 2
50+ 2.93 1 1.5
OSF - 1 1
offset 2 -
Table 4: Population weights per party resulting from
dividing the percentage of the predicted poll seats
(Synovate.nl, 2011b; Peil.nl, 2011b) by the percent-
age of nonnegative tweets (Table 3), and the associated
seat predictions from Twitter, which are now closer to
the poll predictions. Offsets are measured by compar-
ing with the average number of poll seats from Table 1.
better to tune the system to the results of past elec-
tions but we do not have associated Twitter data
for these elections. Adjusting the results of the
system to get them as close to the poll predictions
as possible, is the best we can do at this moment.
6 Predicting election outcomes
The techniques described above were applied to
Dutch political tweets collected in the week be-
fore the election: 23 February 2011 ? 1 March
2011: 64,395 tweets. We used a week of data
rather than a day because we expected that using
more data would lead to better predictions. We
chose for a week of tweets rather than a month
because we assumed that elections were not an
important discussion topic on Twitter one month
before they were held.
After the first two normalization steps, one
party per tweet and one tweet per user, 28,704
tweets were left. The parties were extracted from
the tweets, and counted, and the counts were mul-
tiplied with the sentiment and population weights
and converted to Senate seats. The results are
shown in Table 5 together with poll predictions
Seats Seats Seats
Party Result PB MdH Twitter
VVD 16 14 16 14
PvdA 14 12 11 16
CDA 11 9 9 8
PVV 10 11 12 10
SP 8 9 9 6
D66 5 7 5 8
GL 5 4 4 3
CU 2 3 3 3
50+ 1 2 2 2
SGP 1 2 2 2
PvdD 1 1 2 2
OSF 1 1 0 1
offset - 14 14 18
Table 5: Twitter seat prediction for the 2 March 2011
Dutch Senate elections compared with the actual re-
sults (Kiesraad.nl, 2012a) and the predictions of two
polling companies of 1 March 2011: PB: Politieke
Barometer (Synovate.nl, 2011a) and MdH: Maurice de
Hond (Peil.nl, 2011a).
(Synovate.nl, 2011a; Peil.nl, 2011a) and the re-
sults of the elections of 2 March 2011 (Kies-
raad.nl, 2012a).
The seat numbers predicted by the tweets were
close to the election results. Twitter predicted
the correct number of seats for the party PVV
while the polling companies predicted an in-
correct number. However the companies pre-
dicted other seat numbers correctly and they had
a smaller total error: 14 seats compared to 18 for
our approach.
In Dutch elections, there is no strict linear rela-
tion between the number of votes for a party and
the number seats awarded to a party. Seats that
remain after truncating seat numbers are awarded
to parties by a system which favors larger par-
ties (Kiesraad.nl, 2012b). Furthermore, in 2011
there was a voting incident in the Senate elections
which caused one party (D66) to loose one of its
seats to another party (SP). In our evaluation we
have compared seat numbers because that is the
only type of data that we have available from the
polling companies. The election results allow a
comparison based on percentages of votes. This
comparison is displayed in Table 6.
58
Party Result Twitter offset
VVD 19.6% 17.3% -2.3%
PvdA 17.3% 20.8% +3.5%
CDA 14.1% 11.0% -3.1%
PVV 12.4% 13.3% +0.9%
SP 10.2% 8.5% -1.7%
D66 8.4% 10.1% +1.7%
GL 6.3% 4.8% -1.5%
CU 3.6% 4.0% +0.4%
50+ 2.4% 3.1% +0.7%
SGP 2.4% 3.1% +0.7%
PvdD 1.9% 2.7% +0.8%
OSF 1.4% 1.3% -0.1%
offset - 17.4%
Table 6: Twitter vote prediction for the 2 March 2011
Dutch Provincial elections compared with the actual
results in percentages2.
With the exception of the three largest par-
ties, all predicted percentages are within 1.7%
of the numbers of the election. The percentages
might prove to be more reliable than seat num-
bers as a base for a election prediction method.
We hope to use percentage figures when the pre-
dicting the outcome of next parliament elections:
one of the polling companies publishes such fig-
ures with their predictions of parliament elections.
7 Discussion
Although we are happy about the accuracy ob-
tained by the Twitter predictions, we have some
concerns about the chosen approach. In Table 4,
we introduced poll-dependent weights to correct
the demographic differences between the Twitter
users and the Dutch electorate. This was neces-
sary because we did not have information about
the demographics of Twitter users, for example
about their gender and age. As already men-
tioned, this choice led to tuning the system to
predicting poll results rather than election results.
But do the population weights not also minimize
the effect that tweet counts have on the predic-
tions? Does the system still use the tweet counts
2CU and SGP were awarded an additional 0.3% and 0.2%
for the 0.5% they won as an alliance.
Seats Population
Party Result Twitter weight
VVD 16 16 2.23
PvdA 14 13 1.93
CDA 11 10 1.41
PVV 10 12 1.78
SP 8 7 1.11
D66 5 5 0.82
GL 5 4 0.59
CU 2 3 0.45
50+ 1 1 0.22
SGP 1 2 0.30
PvdD 1 1 0.15
OSF 1 1 -
offset - 8
Table 7: Seat prediction for the 2 March 2011 Dutch
Senate elections based on an uniform distribution of
tweets mentioning political parties.
for the election prediction?
In order to answer the latter question, we de-
signed an additional experiment. Suppose the
tweets per party were uniformly distributed such
that each party name appeared in the same number
of tweets each day. This would make tweet counts
uninteresting for predicting elections. However,
how would our system deal with this situation?
The results of this experiment are shown in Ta-
ble 7.
Since we did not have data to base sentiment
weights on, we assumed that all the sentiment
weights had value 1.0. Since the tweet counts
were different from those in the earlier exper-
iments, we needed to compute new population
weights (see Table 7). The seat numbers predicted
by the system were equal to the average of the seat
numbers of the two polls in Table 4 plus or mi-
nus a half in case the two numbers added up to
an odd number. The VVD party gained one seat,
as a consequence of the system of awarding re-
mainder seats to larger parties. We assume that
the tweet distribution will be uniform at all times
and this means that the system will always predict
the seat distribution. The offset of the new predic-
tion was 3 seats for the test distribution of Table 4
and 8 seats for the election results (see Table 7), a
59
smaller error than either of the polling companies
(compare with Table 5).
This experiment has produced a system which
generates the average of the predictions of the
two polling companies from the week of 16/17
February as an election prediction. It does not re-
quire additional input. This is not a good method
for predicting election outcome but by chance it
generated a better prediction than our earlier ap-
proach and those of two polling companies. We
are not sure what conclusions to draw from this.
Is the method of using population weights flawed?
Is our evaluation method incorrect? Are tweets
bad predictors of political sentiment? Is the mar-
gin of chance error large? It would be good to test
whether the measured differences are statistically
significant but we do not know how to do that for
this data.
8 Concluding remarks
We have collected a large number of Dutch Twit-
ter messages (hundreds of millions) and showed
how they can be used for predicting the results of
the Dutch Senate elections of 2011. Counting the
tweets that mention political parties is not suffi-
cient to obtain good predictions. We tested the
effects of improving the quality of the data col-
lection by removing certain tweets: tweets men-
tioning more than one party name, multiple tweets
from a single user and tweets with a negative sen-
timent. Despite having no gold standard training
data, the total error of our final system was only
29% higher than that of two experienced polling
companies (Table 5). We hope to improve these
results in the future, building on the knowledge
we have obtained in this study.
Acknowledgements
We would like to thank the two reviewers of this
paper for valuable comments.
References
Jessica Chung and Eni Mustafaraj. 2011. Cam col-
lective sentiment expressed on twitter predict po-
litical elections? In Proceedings of the Twenty-
Fifth AAAI Conference on Artificial Intelligence.
San Francisco, CA, USA.
Martijn Epskamp and Marn van Rhee. 2010. Analyse
opkomst gemeenteraadsverkiezingen 2010.
Susannah Fox. 2010. Four in ten seniors go online.
Pew Research Center, http://www.pewinternet.org
/Commentary/2010/January/38-of-adults-age-65-
go-online.aspx (Retrieved 8 March 2012).
Daniel Gayo-Avello, Panagiotis Metaxas, and Eni
Mustafaraj. 2011. Limits of electoral predictions
using social media data. In Proceedings of the In-
ternational AAAI Conference on Weblogs and So-
cial Media. Barcelona, Spain.
Andreas Jugherr, Pascal Ju?rgens, and Harald Schoen.
2011. Why the pirate party won the german elec-
tion of 2009 or the trouble with predictions: A re-
sponse to tumasjan, a., sprenger, t. o., sander, p.
g., & welpe, i. m. ?predicting elections with twit-
ter: What 140 characters reveal about political sen-
timent?. Social Science Computer Review.
Kiesraad.nl. 2012a. Databank verkiezingsuitslagen.
http://www.verkiezingsuitslagen.nl/Na1918/Verkie-
zingsuitslagen.aspx?VerkiezingsTypeId=2 (retrie-
ved 27 February 2012).
Kiesraad.nl. 2012b. Toewijzing zetels. http://www.
kiesraad.nl/nl/Onderwerpen/Uitslagen/Toewijzing
zetels.html (retrieved 27 February 2012).
Thomas Mangin. 2007. ngram: Textcat implementa-
tion in python. http://thomas.mangin.me.uk/.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In Proceedings of the
International AAAI Conference on Weblogs and
Social Media. Washington DC, USA.
Peil.nl. 2011a. Nieuw haags peil 1 maart 2011.
http://www.peil.nl/?3182 (retrieved 5 March 2012).
Peil.nl. 2011b. Nieuw haags peil 15 februari 2011.
http://www.peil.nl/?3167 (retrieved 1 March 2012).
Synovate.nl. 2011a. Nieuws 2011 - peiling eerste
kamer - week 9. http://www.synovate.nl/con-
tent.asp? targetid=721 (retrieved 5 March 2012).
Synovate.nl. 2011b. Peiling eerste kamer - week 7.
http://www.synovate.nl/content.asp?targetid=713
(retrieved 5 March 2012).
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
twitter: What 140 characters reveal about political
sentiment. In Proceedings of the Fourth AAAI con-
ference on Weblogs and Social Media, pages 178?
185.
Jean Ve?ronis. 2007. 2007: La presse fait a? nouveau
mieux que les sondeurs. http://blog.veronis.fr
/2007/05/2007-la-presse-fait-nouveau-mieux-
que.html.
60
