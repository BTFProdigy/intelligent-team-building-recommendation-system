Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 625?632
Manchester, August 2008
Exploring Domain Differences for the Design of Pronoun Resolution
Systems for Biomedical Text
Ngan L.T. Nguyen Jin-Dong Kim
Department of Computer Science, University of Tokyo, Hongo 7-3-1, Tokyo, Japan
{nltngan, jdkim}@is.s.u-tokyo.ac.jp
Abstract
Much effort in the research community has
been spent on solving the anaphora resolu-
tion or pronoun resolution problem, and in
particular for news texts. In order to selec-
tively inherit the previous works and solve
the same problem for a new domain, we
carried out a comparative study with three
different corpora: MUC, ACE for the news
texts, and GENIA for bio-medical papers.
Our corpus analysis and experimental re-
sults show the significant differences in the
use of pronouns in the two domains, thus
by properly considering the characteristics
of a domain, we can improve the perfor-
mance of pronoun resolution for that do-
main.
1 Introduction
Pronoun resolution is the task of determining the
antecedent of an anaphoric pronoun, or a pro-
noun pointing back to some previously mentioned
item in a text. For example, in the sentence, ?The
IL-2 gene displays both T cell specific and in-
ducible expression: it is only expressed in CD4+ T
cells after antigenic or mitogenic stimulation,? the
pronoun ?it? should be resolved to refer to ?the
IL-2 gene,? and thus, we have an anaphora link.
Pronoun resolution is an important task in the
family of reference resolution tasks, including
anaphora resolution and co-reference resolution,
which are known as significant parts of text un-
derstanding systems. Recently the need to have
more powerful information extraction systems for
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
biomedical technical papers has motivated re-
searchers to solve the same task for the biomed-
ical domain. Castano (Castano et al, 2002) re-
solved the sortal and pronominal anaphora, by us-
ing a salience measure, which is the sum of all fea-
ture scores. Kim and Park (Kim and C.Park, 2004)
introduced BioAR, a biomedical anaphora resolu-
tion system that relates entity mentions in text with
their corresponding Swiss-Prot entries. This sys-
tem resolves anaphoric pronouns by using heuris-
tic rules and seven patterns for parallelism. How-
ever, the sizes of the data sets used in their exper-
iments were small. In the former system, 46 and
54 MEDLINE abstracts were used for the devel-
opment set, and the test set respectively, and the
test set in the latter work contained only sixteen
anaphoric pronouns. Contrary to their work, in this
work we made use of GENIA, a large co-reference
annotated corpus for the bio domain, containing
1999 MEDLINE abstracts.
While there are quite a few works on this task
for the bio-medical domain, for other domains, and
especially for the news domain, a myriad of works
on pronoun resolution has been carried out by
the NLP researchers (Mitkov, 2002). Since Soon
(Soon et al, 2001) started the trend of using the
machine learning approach by using a binary clas-
sifier in a pairwise manner for solving co-reference
resolution problem, many machine learning-based
systems have been built, using both supervised
and, unsupervised learning methods (Haghighi and
Klein, 2007). Such methods were claimed to be
comparable with traditional methods. However,
the problems caused by domain differences, which
strongly affect a deep-semantics related task like
pronoun resolution, have not yet been studied well
enough.
In order to recognize the important factors in
625
building an effective machine learning-based pro-
noun resolution system, and in particular for the
bio-domain, we have built a machine learning-
based pronoun resolver and observed the contribu-
tions of different features in the pronoun resolution
process. In our experiments for the news domain,
we used the MUC-7 and ACE corpora, and for the
biomedical domain, we employed the GENIA co-
reference corpus.
Section 2 describes the noticeable issues related
to the corpora, and their preprocessing. Section 3
describes the implementation of our pronoun reso-
lution system, including the resolution model and
the features used. Our experiment settings, eval-
uation scheme, and experimental results are pre-
sented in Section 4. Finally, we conclude our paper
in Section 5.
2 Corpora
In this section, we briefly introduce three corpora
used in our experiments: MUC-7, ACE, and GE-
NIA, and discuss the differences in their annota-
tion schemes. Afterwards, we analyzed the major
differences in the distributions of anaphoric pro-
nouns in these data sets, which provide important
information for the design of features used in the
pronoun resolution process.
The MUC-7 co-reference corpus is a collection
of news wire articles from the source for North
American News Text Corpora. It contains the
training, dry run test, and formal run test sets. The
dry run and formal run have different domains; the
dry run (and training) consists of aircrash scenar-
ios, while the formal run consists of missile launch
scenarios. The ACE (phase 2) corpus for named
entity detection contains three data sets: news wire
(NWIRE), broadcast news (BNEWS), and news-
paper (NPAPER). Each data set is divided into
2 parts for training (train), and for development
testing (devtest). For the bio-domain, we use the
GENIA co-reference corpus, containing 1999 ab-
stracts selected from MEDLINE: a huge source of
bio-domain scientific papers.
These three corpora are all manually annotated
with co-reference information; i.e., the informa-
tion where mentions refer to the same entities.
However, since the annotation schemes used are
not the same, these corpora contain some signif-
icant differences, which may affect our reference
resolution systems.
Figure 1: The symmetric and asymmetric annota-
tion schemes. The dotted lines represent implicit
links between the elements.
2.1 Variations in co-reference annotation
schemes
We started by introducing some important ter-
minologies together with some noticeable issues
related to the common co-reference annotation
scheme. Later, we mention the differences among
the annotation schemes of the three corpora used
in our experiments.
There are three main elements in the co-
reference corpus annotation: the anaphoric expres-
sions, which are anaphoric pronouns in the case
of the pronominal anaphora, their antecedents, and
the referred concepts. Depending on either the
asymmetric scheme employed in MUC (Lynette,
1997) and GENIA (Hong, 2004) or the symmetric
scheme in ACE (NIST, 2003), the annotation task
is defined as either an anaphor-antecedent linking,
or mention-concept linking task, correspondingly
(See Figure 1). Moreover, each annotation scheme
provides its own guidelines for recognizing and an-
notating these three elements, causing the varia-
tions across different co-reference annotated cor-
pora.
In the annotation schemes, mentions which may
join in the co-reference relationship are called
markable. All of the three annotation schemes
record both a maximal and a minimal boundary of
markables, in concerning the evaluation schemes.
However, the types of markables to be annotated,
and the ways to decide their maximal boundary,
are not the same in every annotation scheme.
Table 1 shows the concepts annotated for each
corpora according to the annotation schemes.
While the number of concepts in the ACE corpus
is limited to only 5 entity types, the GENIA and
MUC annotation schemes do not clearly specify
the concept types. This means that every possible
concept in the text domains can join the anaphora
relations; i.e., can be annotated as markables. This
in turn makes the resolution task become more dif-
ficult.
626
Table 1: Possible concepts according to the anno-
tation schemes
GENIA ACE MUC
(Not specified
explicitly)
-Bio entities
5 types of enti-
ties
-Person
-Organization
-Facility
-Location
-GPE(Geo-
political Entity)
(Not specified
explicitly)
-Person
-Organization
-Location
-Date
-Time
-Money
-Percent
Table 2: Possible types of anaphor according to the
annotation schemes (O: allowed, X: not allowed,
U: unspecified)
TYPE GENIA ACE MUC
Personal pronoun O O O
Demonstrative pronoun O O O
Possessive pronoun O O O
Reflexive pronoun O O U
Indefinite pronoun (e.g.,
both)
O U U
Pleonastic pronoun it X U U
Bound anaphor X U O
Mention with empty head
(e.g., five of)
X U U
here, there U O U
The possible types of annotated anaphoric pro-
nouns are given in Table 2. O denotes the type of
pronoun, which may be annotated as markable, in
contrast to X, which denotes the type of pronoun,
which is not allowed to be annotated as markable.
The notation U represents the annotation scheme
that does not state how a type should be treated
because that type is not popular in the domain, or
the scheme does not allow the annotation of such a
type implicitly.
Using the similar notations as in Table 2, Ta-
ble 3 shows the possible syntactic structures of
antecedents according to the annotation schemes,
which are also the structures of markables in real
annotations. In practice, such structural varia-
tions may cause troubles for automatically mark-
able recognition, so in the experiments with pro-
noun resolution, gold markables are often used to
eliminate error-prone problems.
2.2 Corpus preprocessing
Our objective anaphoric pronouns are limited to
the following types: personal pronouns (all cases),
possessive pronouns, and demonstrative pronouns,
which have a nominal antecedent. In addition
Table 3: Possible types of antecedent according
to the annotation schemes (O: allowed, X: not al-
lowed, U: unspecified)
TYPE GENIA ACE MUC
Pronominal X O O
Noun used as a modifier (em-
bedded in NP)
X O O
Name, named entity (embedded
in NP)
X O O
Gerund U U X
NP with a head noun (definite
and indefinite)
O O O
Conjoint NP (with more than
one head)
O X O
Coordinated NP O O O
Predicate nominal X O O
NP with a restrictive appositive
phrase
X O O
NP with a non-restrictive ap-
positive phrase
X O O
NP with a restrictive preposi-
tional phrase
O O O
NP with a non-restrictive
prepositional phrase
X O O
NP with a restrictive relative
clause
O O O
NP with a non-restrictive rela-
tive clause
O O O
Infinitive clause O U U
Date, Currency expressions,
and percentages
U U O
Proper adjective (e.g.,French) U O U
here, there X O U
to these types of pronouns, the annotated cor-
pora contain other types of pronominal anaphora,
including ?both,? ?one,? numeric mentions (GE-
NIA), and bound anaphora (ACE). However, anal-
ysis statistics show that such pronouns occupy less
than 5% of the total pronouns in the GENIA cor-
pus, thus we have ignored them.
In the preprocessing step, for each corpus, we
extract the gold pronominal anaphora links, which
link the anaphoric pronouns with their antecedents.
Although MUC and GENIA used the same asym-
metric annotation schemes, picking one gold an-
tecedent in a set of co-referenced mentions is not
straightforward, since pronouns in GENIA are not
allowed to be linked with a pronomial antecedent,
while in the MUC corpus, this kind of link is al-
lowed. In order to achieve the fairest compara-
tive experimental results, we uniformly choose the
nearest item in the co-reference chain of a pro-
noun, and make a gold anaphora link. This pol-
icy is best suited for ACE, thanks to the symmetric
scheme used.
627
Table 4: Sizes of the data sets (number of
anaphoric pronoun)
GENIA ACE MUC
Training set 1442 2427 371
Test set 357 633 240
Figure 2: Analysis of anaphoric pronoun in differ-
ent data sets
2.3 Statistics
In the following step, we analyze the extracted
anaphora links for the three corpora. The analy-
sis statistics in Figure 2 show the differences of the
distributions of pronoun types and pronoun prop-
erties in three data sets: MUC-7, GENIA, and
BNEWS from ACE. Note that only four major
types out of the nine types of anaphoric pronouns
mentioned in the previous section are counted. In
particular, the chosen types correspond to those
rows in Table 2 that contain at least two O.
We can see that all of the anaphoric pronouns in
GENIA are neutral-gender and third-person pro-
nouns. Another difference is that the number of
demonstrative pronouns in GENIA comes to about
20%, which is much more than in other data sets.
As each type of pronoun has its own referen-
tial characteristics, such differences in the distribu-
tions of pronouns can significantly affect the pro-
noun resolution. This will be shown in our experi-
ments, and analysis of the experimental results will
be given in the following section.
3 Implementation
3.1 Pronoun resolution model
We built a machine learning based pronoun res-
olution engine using a Maximum Entropy ranker
model (Berger et al, 1996), similar with Denis and
Baldridge?s model (Denis and Baldridge, 2007).
For every anaphoric pronoun ?, the ranker selects
the most likely antecedent candidate ?, from a set
of k candidate markables.
P
r
(?
j
|?) =
exp (
?
n
i=1
?
i
f
i
(?, ?
j
))
?
k
exp (
?
n
i=1
?
i
f
i
(?, ?
k
))
(1)
We constructed the training examples in the fol-
lowing way: for each gold anaphora link in the
training corpus, we created a positive instance, and
negative training instances are created by pairing
the pronoun with all of the other markables ap-
pearing in a window of w preceding sentences. In
all the experiments on ACE and MUC, we set w
to 10 sentences, while for GENIA, w is set to 5.
This setting is based on our corpus analysis show-
ing that many of the gold antecedents in the bio-
domain texts are in at most three sentences from
their anaphors. In the resolution phase, the same
method for collecting instances was also applied.
3.2 Features
Table 5 shows the primitive features used in our
system, which are grouped into feature groups ac-
cording to the type of information that they carry.
Note that the actual features used by the ranker are
distance features (sdist, and tdist), and not only the
primitive features themselves, but also the combi-
nations of these primitive features. The pronoun
resolution model makes use of the discriminative
power of these combinatory features. For exam-
ple, the combination of P num and C num tests the
agreement in number between the anaphoric pro-
noun and its candidate. Such agreements in num-
ber and gender are one of the constraints in the
anaphora phenomenon, and have been exploited in
almost all machine learning-based pronoun resolu-
tion frameworks (Soon et al, 2001).
Each primitive feature is from a layer of text
analysis (see Layer), which can be morphological
(mor.), syntactic (syn.), or semantic (sem.). The
second column represents the feature sets that are
used in our experiments. The explanation column
in the table shows the way we extract feature val-
ues from texts, with the exception of the primitive
628
feature P semw, reflecting the context information
of the anaphoric pronoun. This feature value is de-
termined in the following way. If the pronoun is
a subject, then P semw is its governing head verb,
and if it is a possessive adjective, then P semw is
the head noun of the noun phrase containing that
pronoun. A default value is used if the pronoun
belongs to neither of the above cases.
The last column of this table shows an exam-
ple of the feature characterization for the anaphora
link PMA-its in this discourse: ?By comparison,
PMA is a very inefficient inducer of the jun gene
family in Jurkat cells. Similar to its effect on the
induction of AP1 by okadaic acid, PMA inhibits
the induction of c-jun mRNA by okadaic acid.?
We divided the feature groups into 3 feature sets:
fundamental, baseline and additional. The funda-
mental feature set contains the indispensable fea-
tures for solving pronoun resolution. The base-
line feature set mostly includes morphological fea-
tures, reflecting the properties of text mentions,
and in particular the pronoun properties such as
gender, number, etc. The features in the addi-
tional feature set are used to exploit higher levels
of knowledge through more semantic and syntactic
features. We also include in this feature set the fea-
tures that have been used in some previous work in
order to clarify their contributions in our system.
4 Experiments
4.1 Experiment setting and evaluation
scoring
For each corpus, we trained our resolver on the
training set, and then applied it to the develop-
ment test set. For the case of the ACE corpus, we
only used the train part of the BNEWS data set for
training, and applied on the corresponding devtest
data set. We randomly splitted the GENIA cor-
pus it into 2 parts: the train, and the heldout data
sets, which contain 1599 and 400 abstracts, respec-
tively. For the MUC corpus, we used the dryrun
part for training, and the formal part for testing.
Similar to previous works, all of the experimen-
tal results in this paper are reported in success rate
(Mitkov, 2002), calculated using the following for-
mula.
Success rate = Number of successfully resolved anaphors
Number of all anaphors
(2)
The input of our resolver are the gold mentions
annotated in the corpora. The output anaphora
links of a pronoun resolution system are evalu-
ated following two criteria. In criterion 1, the
recognized antecedent of an anaphoric pronoun is
considered correct only when it matches the an-
tecedent in the gold anaphora link of that pro-
noun. Criterion 2 is a bit looser when the recog-
nized antecedent just needs to match one of the
antecedents of a pronoun in its co-reference chain.
This criterion has been used by most of the pre-
vious works, including Denis and Baldridge?s sys-
tem (Denis and Baldridge, 2007).
4.2 Baseline resolver
In this experiment, we use the baseline feature set
presented in section 3.2. One of the reasons in
choosing these features for the baseline system, is
that they are basic features that have been used by
most of the previous reference resolution systems.
Moreover, we wanted to see how these features
contribute to the resolution process for different
corpora, presented in the next section.
Our baseline system achieved a 71.41% success
rate on the BNEWS data set (Table 6, criterion
2), which is comparable to the result of Denis and
Baldridge?s system on the same data set (Denis and
Baldridge, 2007). Moreover, we can see that the
differences caused by the two criteria are not the
same for every data set. For the news domain data
sets, the differences vary from 4.17% (MUC) to
6.8% (ACE), which is high in comparison with the
percentages of GENIA, which were less than two
percent. This can be explained by the fact that pro-
nouns in news texts are used more repeatedly than
those in bio-medical texts. Because bio-entities are
neutral-gender mentions, and are referred by the
neutral gender and third person pronouns, the re-
peated use of pronouns may increase the ambiguity
of the text, and confuse the readers.
To prevent the confusion of the readers, we
chose just one data set BNEWS to represent the
ACE corpus and present our further analysis ex-
periments on these three data sets: GENIA, ACE
(BNEWS), and MUC (MUC-7).
4.3 Contributions of the features in the
baseline resolver
In order to observe the effects of the features in the
baseline pronoun resolver, we omitted each fea-
ture group from the whole feature set, retrained
our resolution models with the new feature set,
and applied them to the three data sets: GENIA,
629
Table 5: Features used in the pronoun resolver
Layer Feature set Group Primitive Feature Explanation Example
mor.
fundamental mention type P type pronoun type possessive pronounC type candidate mention type proper name
baseline
sdist CP sdis distance in sentence 1
tdist CP tdis normalized distance in token 17
numb P numb number of p singularC numb number of c unknown
pers P pers person of p third personC pers person of c third person
gend P gend gender of p neutralC gend gender of c neutral
pfam P pfam family of p itC pfam family of c null
string P word pronoun string its
syn.
C head candidate head string PMA
additional
pos
P lpos POS of the left word of p TO
P rpos POS of the right word of p NN
C lpos POS of the left word of c COMMA
C rpos POS of the right word of c VBZ (is)
parg P parg argument role of p nullC parg argument role of c arg1
sem. netype C netype entity type of c null
mor. last3c C last3c the last 3 characters of c pma
syn.
comb P semw see Section 3.2 effect
other C 1stnp first NP in a sentence or not false
Table 6: Baseline system evalutation (C1: criterion
1, C2: criterion 2, D: difference between criterion
1 and 2)
GENIA ACE MUC
C1 70.31 64.61 57.08
C2 71.43 71.41 61.25
Diff 1.12 6.80 4.17
BNEWS, and MUC. Pronoun type and mention
type are the most significant features, and thus, are
not omitted in this experiment.
Table 7 shows the experimental results: the first
column is the feature group name, and the follow-
ing three columns show the resolution accuracy of
the three corpora. The figures in the parentheses
show the degradation, when we exclude the corre-
sponding group from the baseline feature set. Our
data analysis show some noticeable issues:
Number features (numb) :
The number-combination features are the most
significant features in bio-texts while they are not
so effective on ACE, and even perform negatively
on MUC. One of the reasons behind this, is that
in the bio-texts, all of the anaphoric pronouns
have a deterministic number; i.e., either singular
or plural (Section 2.3), while the news texts con-
tain first- and second-person pronouns whose num-
bers are unspecified. Another reason emerges from
the non-pronominal types of mentions, which play
a role as antecedents. The number property of
these mentions is characterized in the markable
detection phase based on the part-of-speech tag,
the head noun, and the phrase structure of those
mentions. In particular, the MUC corpus con-
tains many coordinated-structured mentions (Sec-
tion 2.1), which are difficult for markable charac-
terization.
Person features and pronoun family (pers
and pfam) :
The absence of the pers features caused the
biggest loss for the resolution success rate on the
ACE corpus, because the co-reference chains in
this corpus contain a lot of pronouns, and it is
easier for the pronoun resolver to determine a
pronominal antecedent than to determine a non-
pronominal antecedent. The same phenomena can
be observed with pfam features. The bio-text only
contains third-person anaphoric pronouns (Section
2.3), so the person features do not have any profits.
Distance features (sdist and tdist) :
Our baseline resolver again confirmed that the
sentence distance is an indispensable feature in
pronoun resolution. However, the token-based dis-
tance did not show any improvements on the MUC
corpus. Analyzing the MUC anaphora links, we
found that these tdist features resulted in 10 cor-
rect anaphora links, but also mis-recognized 10 an-
tecedents.
630
Table 7: Feature contributions in the baseline sys-
tem (evaluation criterion 1)
Excluded GENIA ACE MUC
none 70.31 64.61 57.08
-sdist 67.23(?3.08) 63.51(?1.10) 51.67(?5.41)
-tdist 70.03(?0.28) 59.56(?5.05) 57.08(+0.00)
-numb 65.83(?4.48) 61.77(?2.84) 58.33(+1.25)
-pers 70.31(+0.00) 57.19(?7.42) 55.42(?1.66)
-gend 69.75(?0.56) 64.45(?0.16) 56.67(?0.41)
-pfam 71.15(+0.84) 63.51(?1.10) 57.92(+0.84)
-string 68.07(?2.24) 61.93(?2.68) 55.83(?1.25)
4.4 Contributions of additional features to
the baseline feature set
In addition to the baseline feature set, we enhanced
our resolver with more features. Among them,
there are two noticeable features: the grammati-
cal role of pronouns or antecedent candidates, and
the named entity type of the candidates. The other
feature groups are used in Denis and Baldridge?s
system, which we also want to test in our system.
Table 8 shows the resolution results and the
increase when adding the corresponding feature
group. With the exception of the last3c features,
the others significantly improved the resolution
success rate on bio-texts, although they did not
have clear contributions to the news domain data
sets. The following is our further analysis to see
the way that these features can contribute to the
pronoun resolution process.
Semantic features (netype)
The first feature we would like to observe is
the combination of C netype and P semw features,
which contributed to the increase by 3.64 points.
We further conducted a small test by excluding this
combination from the netype feature group, but the
success rate remained unchanged from the baseline
result. This signifies that this combination con-
tributed the most to the above increase.
The combination of C netype and P semw fea-
tures exploits the co-ocurrence of the semantic
type of the candidate antecedent and the context
word, which appears in some relationship with the
pronoun. This combination feature uses the infor-
mation similar to the semantic compatibility fea-
tures proposed by Yang (Yang et al, 2005) and
Bergsma (Bergsma and Lin, 2006). Depending
on the pronoun type, the feature extractor decides
which relationship is used. For example, the re-
solver successfully recognizes the antecedent of
the pronoun its in this discourse: ?HSF3 is con-
stitutively expressed in the erythroblast cell line
HD6 , the lymphoblast cell line MSB , and em-
bryo fibroblasts , and yet its DNA-binding activ-
ity is induced only upon exposure of HD6 cells to
heat shock ,? because HSF3 was detected as a Pro-
tein entity, which has a strong association with the
governing head noun activity of the pronoun.
Another example is the correct anaphora link
between ?it? and ?the viral protein? in the fol-
lowing sentence, which the other features failed to
detect. ?Tax , the viral protein , is thought to be
crucial in the development of the disease , since
it transforms healthy T cells in vitro and induces
tumors in transgenic animals.? The correct an-
tecedent was recognized due to the bias given to
the association of the Protein entity type, and the
governing verb, ?transform? of the pronoun. The
experimental results show the contribution of the
domain knowledge to the pronoun resolution, and
the potential combination use of such knowledge
with the syntactic features.
Parse features (parg)
The combinations of the primitive features of
grammatical roles significantly improved the per-
formance of our resolver. The following examples
show the correct anaphora links resulting from us-
ing the parse features:
? ?By comparison, PMA is a very inefficient in-
ducer of the jun gene family in Jurkat cells.
Similar to its effect on the induction of AP1
by okadaic acid, PMA inhibits the induction
of c-jun mRNA by okadaic acid.?
In this example, the possessive pronoun ?its? in
the second sentence corefers to ?PMA?, the sub-
ject of the preceding sentence.
Among the combination features in this group,
one noticeable feature is the combination of
C parg, Sdist, and P type which contains the as-
sociation of the grammatical role of the candidate,
the sentence-based distance, and the pronoun type.
The idea of adding this combination is based on
the Centering theory (Walker et al, 1998), a the-
ory of discourse successfully used in pronoun res-
olution. This simple feature shows the potential of
encoding centering theory in the machine learning
features, based on the parse information.
Feature integration
Finally, we integrated all of the positive fea-
ture groups for each data set in the above experi-
ments, and tested this combining feature set. Table
631
Table 8: Additional features and their contribu-
tions (evaluation criterion 1)
Added GENIA ACE MUC
none 70.31 64.61 57.08
+pos 75.63(+5.32) 62.88(?1.73) 57.50(+0.42)
+parg 73.67(+3.36) 63.82(?0.79) 58.75(+1.67)
+netype 73.95(+3.64) 64.30(?0.31) 58.33(+1.25)
+last3c 67.51(?2.80) 62.09(?2.52) 56.67(?0.41)
+comb 72.83(+2.52) 63.82(?0.79) 56.25(?0.83)
Table 9: Feature integration
GENIA ACE MUC
C1 79.55 (+9.24) 64.61 (+0.00) 60.42 (+3.34)
C2 80.95 (+9.52) 71.41 (+0.00) 66.25 (+5.00)
9 shows a significant increase in the performance
of the resolver on GENIA and MUC.
5 Conclusion and future work
Through the differences in the corpus annotation
schemes, in the corpora themselves, and in contri-
butions of resolution factors to the pronoun resolu-
tion process, we can see that adapting pronoun res-
olution for a different domain is not an easy task. A
good study on the types of anaphoric pronouns and
entity mention structures beforehand can help de-
sign a better feature set for our machine learning-
based pronoun resolution system and thus, can
save much time and labor.
As shown in this paper, for the news do-
main, the properties of anaphoric pronouns contain
rich information about their antecedents, which is
very useful in the resolution process. While in
biomedical text, it is more important to capture
the information to connect a pronoun and its an-
tecedent from their surrounding context, because
the anaphoric pronouns themselves contain almost
no information of their antecedents with the excep-
tion of the numbers.
As a future work, it would be interesting to see
how the system performs in other domains. More
experiments should be designed to make the influ-
ences of annotation schemes on the pronoun reso-
lution process clearer.
References
Berger, Adam L., Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional Linguistics, 22(1):39?71.
Bergsma, Shane and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 33?40.
Castano, Jose, Jason Zhang, and James Pusterjovsky.
2002. Anaphora resolution in biomedical literature.
In Int?l Symposium Reference Resolution in NLP.
Denis, Pascal and J. Baldridge. 2007. A ranking ap-
proach to pronoun resolution. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence (IJCAI07).
Haghighi, Aria and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848?855.
Hong, Huaqing. 2004. Coreference annotation scheme
for medco corpus.
Kim, Jung-Jae and Jong C.Park. 2004. Bioar:
Anaphora resolution for relating protein names to
proteome database entries. In Proceedings of the
ACL 2004: Workshop on Reference Resolution and
its Applications, pages 79?86.
Lynette, Hirschman. 1997. Muc-7 coreference task
definition.
Mitkov, Ruslan. 2002. Anaphora resolution. Pearson
Education, London, Great Britain.
NIST. 2003. Entity detection and tracking - phrase
1 edt and metonymy annotation guidelines version
2.5.1 20030502.
Soon, W., H. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Walker, Marilyn A., Aravind K. Joshi, and Ellen F.
Prince. 1998. Centering Theory in Discourse.
Clarendon Press, Oxford.
Yang, Xiaofeng, Jian Su, and Chew-Lim Tan. 2005.
Improving pronoun resolution using statistics-based
semantic compatibility information. In Proceedings
of the 43rd Annual meeting of the Association for
Computational Linguistics (ACL05), pages 427?434.
632
TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of BioNLP Shared Task 2011 Workshop, pages 1?6,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Sampo Pyysalo
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
smp@is.s.u-tokyo.ac.jp
Tomoko Ohta
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
okap@is.s.u-tokyo.ac.jp
Robert Bossy
National Institute for Agricultural Research
78352 Jouy en Josas, Cedex
Robert.Bossy@jouy.inra.fr
Ngan Nguyen
University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
The BioNLP Shared Task 2011, an informa-
tion extraction task held over 6 months up to
March 2011, met with community-wide par-
ticipation, receiving 46 final submissions from
24 teams. Five main tasks and three support-
ing tasks were arranged, and their results show
advances in the state of the art in fine-grained
biomedical domain information extraction and
demonstrate that extraction methods success-
fully generalize in various aspects.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
series represents a community-wide move toward
fine-grained information extraction (IE), in particu-
lar biomolecular event extraction (Kim et al, 2009;
Ananiadou et al, 2010). The series is complemen-
tary to BioCreative (Hirschman et al, 2007); while
BioCreative emphasizes the short-term applicability
of introduced IE methods for tasks such as database
curation, BioNLP-ST places more emphasis on the
measurability of the state-of-the-art and traceabil-
ity of challenges in extraction through an approach
more closely tied to text.
These goals were pursued in the first event,
BioNLP-ST 2009 (Kim et al, 2009), through high
quality benchmark data provided for system devel-
opment and detailed evaluation performed to iden-
tify remaining problems hindering extraction perfor-
mance. Also, as the complexity of the task was high
and system development time limited, we encour-
aged focus on fine-grained IE by providing gold an-
notation for named entities as well as various sup-
porting resources. BioNLP-ST 2009 attracted wide
attention, with 24 teams submitting final results. The
task setup and data since have served as the basis
for numerous studies (Miwa et al, 2010b; Poon and
Vanderwende, 2010; Vlachos, 2010; Miwa et al,
2010a; Bjo?rne et al, 2010).
As the second event of the series, BioNLP-ST
2011 preserves the general design and goals of the
previous event, but adds a new focus on variabil-
ity to address a limitation of BioNLP-ST 2009: the
benchmark data sets were based on the Genia corpus
(Kim et al, 2008), restricting the community-wide
effort to resources developed by a single group for
a small subdomain of molecular biology. BioNLP-
ST 2011 is organized as a joint effort of several
groups preparing various tasks and resources, in
which variability is pursued in three primary direc-
tions: text types, event types, and subject domains.
Consequently, generalization of fine grained bio-IE
in these directions is emphasized as the main theme
of the second event.
This paper summarizes the entire BioNLP-ST
2011, covering the relationships between tasks and
similar broad issues. Each task is presented in detail
in separate overview papers and extraction systems
in papers by participants.
1
2 Main tasks
BioNLP-ST 2011 includes four main tracks (with
five tasks) representing fine-grained bio-IE.
2.1 Genia task (GE)
The GE task (Kim et al, 2011) preserves the task
definition of BioNLP-ST 2009, arranged based on
the Genia corpus (Kim et al, 2008). The data repre-
sents a focused domain of molecular biology: tran-
scription factors in human blood cells. The purpose
of the GE task is two-fold: to measure the progress
of the community since the last event, and to eval-
uate generalization of the technology to full papers.
For the second purpose, the provided data is com-
posed of two collections: the abstract collection,
identical to the BioNLP-ST 2009 data, and the new
full paper collection. Progress on the task is mea-
sured through the unchanged task definition and the
abstract collection, while generalization to full pa-
pers is measured on the full paper collection. In this
way, the GE task is intended to connect the entire
event to the previous one.
2.2 Epigenetics and post-translational
modification task (EPI)
The EPI task (Ohta et al, 2011) focuses on IE for
protein and DNA modifications, with particular em-
phasis on events of epigenetics interest. While the
basic task setup and entity definitions follow those of
the GE task, EPI extends on the extraction targets by
defining 14 new event types relevant to task topics,
including major protein modification types and their
reverse reactions. For capturing the ways in which
different entities participate in these events, the task
extends the GE argument roles with two new roles
specific to the domain, Sidechain and Contextgene.
The task design and setup are oriented toward the
needs of pathway extraction and curation for domain
databases (Wu et al, 2003; Ongenaert et al, 2008)
and are informed by previous studies on extraction
of the target events (Ohta et al, 2010b; Ohta et al,
2010c).
2.3 Infectious diseases task (ID)
The ID task (Pyysalo et al, 2011a) concerns the ex-
traction of events relevant to biomolecular mecha-
nisms of infectious diseases from full-text publica-
tions. The task follows the basic design of BioNLP-
ST 2009, and the ID entities and extraction targets
are a superset of the GE ones. The task extends
considerably on core entities, adding to PROTEIN
four new entity types, including CHEMICAL and
ORGANISM. The events extend on the GE defini-
tions in allowing arguments of the new entity types
as well as in introducing a new event category for
high-level biological processes. The task was im-
plemented in collaboration with domain experts and
informed by prior studies on domain information ex-
traction requirements (Pyysalo et al, 2010; Anani-
adou et al, 2011), including the support of systems
such as PATRIC (http://patricbrc.org).
2.4 Bacteria track
The bacteria track consists of two tasks, BB and BI.
2.4.1 Bacteria biotope task (BB)
The aim of the BB task (Bossy et al, 2011) is to ex-
tract the habitats of bacteria mentioned in textbook-
level texts written for non-experts. The texts are
Web pages about the state of the art knowledge about
bacterial species. BB targets general relations, Lo-
calization and PartOf , and is challenging in that
texts contain more coreferences than usual, habitat
references are not necessarily named entities, and,
unlike in other BioNLP-ST 2011 tasks, all entities
need to be recognized by participants. BB is the first
task to target phenotypic information and, as habi-
tats are yet to be normalized by the field community,
presents an opportunity for the BioNLP community
to contribute to the standardization effort.
2.4.2 Bacteria interaction task (BI)
The BI task (Jourde et al, 2011) is devoted to the ex-
traction of bacterial molecular interactions and reg-
ulations from publication abstracts. Mainly focused
on gene transcriptional regulation in Bacillus sub-
tilis, the BI corpus is provided to participants with
rich semantic annotation derived from a recently
proposed ontology (Manine et al, 2009) defining
ten entity types such as gene, protein and deriva-
tives as well as DNA sites/motifs. Their interactions
are described through ten relation types. The BI
corpus consists of the sentences of the LLL corpus
(Ne?dellec, 2005), provided with manually checked
linguistic annotations.
2
Task Text Focus #
GE abstracts, full papers domain (HT) 9
EPI abstracts event types 15
ID full papers domain (TCS) 10
BB web pages domain (BB) 2
BI abstracts domain (BS) 10
Table 1: Characteristics of BioNLP-ST 2011 main tasks.
?#?: number of event/relation types targeted. Domains:
HT = human transcription factors in blood cells, TCS
= two-component systems, BB = bacteria biology, BS =
Bacillus subtilis
2.5 Characteristics of main tasks
The main tasks are characterized in Table 1. From
the text type perspective, BioNLP-ST 2011 gener-
alizes from abstracts in 2009 to full papers (GE and
ID) and web pages (BB). It also includes data collec-
tions for a variety of specific subject domains (GE,
ID, BB an BI) and a task (EPI) whose scope is not
defined through a domain but rather event types. In
terms of the target event types, ID targets a superset
of GE events and EPI extends on the representation
for PHOSPHORYLATION events of GE. The two bac-
teria track tasks represent an independent perspec-
tive relatively far from other tasks in terms of their
target information.
3 Supporting tasks
BioNLP-ST 2011 includes three supporting tasks
designed to assist in primary the extraction tasks.
Other supporting resources made available to par-
ticipants are presented in (Stenetorp et al, 2011).
3.1 Protein coreference task (CO)
The CO task (Nguyen et al, 2011) concerns the
recognition of coreferences to protein references. It
is motivated from a finding from BioNLP-ST 2009
result analysis: coreference structures in biomedical
text hinder the extraction results of fine-grained IE
systems. While finding connections between event
triggers and protein references is a major part of
event extraction, it becomes much harder if one is
replaced with a coreferencing expression. The CO
task seeks to address this problem. The data sets for
the task were produced based on MedCO annotation
(Su et al, 2008) and other Genia resources (Tateisi
et al, 2005; Kim et al, 2008).
Event Date Note
Sample Data 31 Aug. 2010
Support. Tasks
Train. Data 27 Sep. 2010 7 weeks for development
Test Data 15 Nov. 2010 4 days for submission
Submission 19 Nov. 2010
Evaluation 22 Nov. 2010
Main Tasks
Train. Data 1 Dec. 2010 3 months for development
Test Data 1 Mar. 2011 9 days for submission
Submission 10 Mar. 2011 extended from 8 Mar.
Evaluation 11 Mar. 2011 extended from 10 Mar.
Table 2: Schedule of BioNLP-ST 2011
3.2 Entity relations task (REL)
The REL task (Pyysalo et al, 2011b) involves the
recognition of two binary part-of relations between
entities: PROTEIN-COMPONENT and SUBUNIT-
COMPLEX. The task is motivated by specific chal-
lenges: the identification of the components of pro-
teins in text is relevant e.g. to the recognition of
Site arguments (cf. GE, EPI and ID tasks), and re-
lations between proteins and their complexes rele-
vant to any task involving them. REL setup is in-
formed by recent semantic relation tasks (Hendrickx
et al, 2010). The task data, consisting of new anno-
tations for GE data, extends a previously introduced
resource (Pyysalo et al, 2009; Ohta et al, 2010a).
3.3 Gene renaming task (REN)
The REN task (Jourde et al, 2011) objective is to ex-
tract renaming pairs of Bacillus subtilis gene/protein
names from PubMed abstracts, motivated by dis-
crepancies between nomenclature databases that in-
terfere with search and complicate normalization.
REN relations partially overlap several concepts:
explicit renaming mentions, synonymy, and renam-
ing deduced from biological proof. While the task
is related to synonymy relation extraction (Yu and
Agichtein, 2003), it has a novel definition of renam-
ing, one name permanently replacing the other.
4 Schedule
Table 2 shows the task schedule, split into two
phases to allow the use of supporting task results in
addressing the main tasks. In recognition of their
higher complexity, a longer development period was
arranged for the main tasks (3 months vs 7 weeks).
3
Team GE EPI ID BB BI CO REL REN
UTurku 1 1 1 1 1 1 1 1
ConcordU 1 1 1 1 1 1
UMass 1 1 1
Stanford 1 1 1
FAUST 1 1 1
MSR-NLP 1 1
CCP-BTMG 1 1
Others 8 0 2 2 0 4 2 1
SUM 15 7 7 3 1 6 4 3
Table 3: Final submissions to BioNLP-ST 2011 tasks.
5 Participation
BioNLP-ST 2011 received 46 submissions from 24
teams (Table 3). While seven teams participated in
multiple tasks, only one team, UTurku, submitted fi-
nal results to all the tasks. The remaining 17 teams
participated in only single tasks. Disappointingly,
only two teams (UTurku, and ConcordU) performed
both supporting and main tasks, and neither used
supporting task analyses for the main tasks.
6 Results
Detailed evaluation results and analyses are pre-
sented in individual task papers, but interesting ob-
servations can be obtained also by comparisons over
the tasks. Table 4 summarizes best results for vari-
ous criteria (Note that the results shown for e.g. GEa,
GEf and GEp may be from different teams).
The community has made a significant improve-
ment in the repeated GE task, with an over 10%
reduction in error from ?09 to GEa. Three teams
achieved better results than M10, the best previously
reported individual result on the ?09 data. This in-
dicates a beneficial role from focused efforts like
BioNLP-ST. The GEf and ID results show that
generalization to full papers is feasible, with very
modest loss in performance compared to abstracts
(GEa). The results for PHOSPHORYLATION events
in GE and EPI are comparable (GEp vs EPIp), with
the small drop for the EPI result, suggesting that
the removal of the GE domain specificity does not
compromise extraction performance. EPIc results
indicate some challenges in generalization to simi-
lar event types, and EPIf suggest substantial further
challenges in additional argument extraction. The
complexity of ID is comparable to GE, also reflected
to their final results, which further indicate success-
Task Evaluation Results
BioNLP-ST 2009 (?09) 46.73 / 58.48 / 51.95
Miwa et al (2010b) (M10) 48.62 / 58.96 / 53.29
LLL 2005 (LLL) 53.00 / 55.60 / 54.30
GE abstracts (GEa) 50.00 / 67.53 / 57.46
GE full texts (GEf) 47.84 / 59.76 / 53.14
GE PHOSPHORYLATION (GEp) 79.26 / 86.99 / 82.95
GE LOCALIZATION (GEl) 37.88 / 77.42 / 50.87
EPI full task (EPIf) 52.69 / 53.98 / 53.33
EPI core task (EPIc) 68.51 / 69.20 / 68.86
EPI PHOSPHORYLATION (EPIp) 86.15 / 74.67 / 80.00
ID full task (IDf) 48.03 / 65.97 / 55.59
ID core task (IDc) 50.62 / 66.06 / 57.32
BB 45.00 / 45.00 / 45.00
BB PartOf (BBp) 32.00 / 83.00 / 46.00
BI 71.00 / 85.00 / 77.00
CO 22.18 / 73.26 / 34.05
REL 50.10 / 68.00 / 57.70
REN 79.60 / 95.90 / 87.00
Table 4: Best results for various (sub)tasks (recall / preci-
sion / f-score (%)). GEl: task 2 without trigger detection.
ful generalization to a new subject domain as well
as to new argument (entity) types. The BB task is
in part comparable to GEl and involves a represen-
tation similar to REL, with lower results likely in
part because BB requires entity recognition. The BI
task is comparable to LLL Challenge, though BI in-
volves more entity and event types. The BI result
is 20 points above the LLL best result, indicating a
substantial progress of the community in five years.
7 Discussion and Conclusions
Meeting with wide participation from the commu-
nity, BioNLP-ST 2011 produced a wealth of valu-
able resources for the advancement of fine-grained
IE in biology and biomedicine, and demonstrated
that event extraction methods can successfully gen-
eralize to new text types, event types, and domains.
However, the goal to observe the capacity of sup-
porting tasks to assist the main tasks was not met.
The entire shared task period was very long, more
than 6 months, and the complexity of the task was
high, which could be an excessive burden for partic-
ipants, limiting the application of novel resources.
There have been ongoing efforts since BioNLP-ST
2009 to develop IE systems based on the task re-
sources, and we hope to see continued efforts also
following BioNLP-ST 2011, especially exploring
the use of supporting task resources for main tasks.
4
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology.
Sophia Ananiadou, Dan Sullivan, William Black, Gina-
Anne Levow, Joseph J. Gillespie, Chunhong Mao,
Sampo Pyysalo, BalaKrishna Kolluru, Junichi Tsujii,
and Bruno Sobral. 2011. Named entity recognition
for bacterial type IV secretion systems. PLoS ONE,
6(3):e14780.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid ?O. Se?aghdha, Sebastian Pado?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakow-
icz. 2010. Semeval-2010 task 8: Multi-way classi-
fication of semantic relations between pairs of nom-
inals. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, SemEval ?10, pages 33?
38, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Lynette Hirschman, Martin Krallinger, and Alfonso Va-
lencia, editors. 2007. Proceedings of the Second
BioCreative Challenge Evaluation Workshop. CNIO
Centro Nacional de Investigaciones Oncolo?gicas.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
A.P. Manine, E. Alphonse, and Bessie`res P. 2009. Learn-
ing ontological rules to extract multiple relations of
genic interactions from text. International Journal of
Medical Informatics, 78(12):e31?38.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Ne?dellec. 2005. Learning Language in Logic ? Genic
Interaction Extraction Challenge. In Proceedings of
4th Learning Language in Logic Workshop (LLL?05),
pages 31?37.
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, Jin-Dong Kim, and
Jun?ichi Tsujii. 2010a. A re-evaluation of biomedical
named entity-term relations. Journal of Bioinformat-
ics and Computational Biology (JBCB), 8(5):917?928.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, Jin-Dong
Kim, and Jun?ichi Tsujii. 2010b. Event extraction
for post-translational modifications. In Proceedings of
BioNLP?10, pages 19?27.
Tomoko Ohta, Sampo Pyysalo, Makoto Miwa, and
Jun?ichi Tsujii. 2010c. Event extraction for dna
methylation. In Proceedings of SMBM?10.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Mate? Ongenaert, Leander Van Neste, Tim De Meyer,
Gerben Menschaert, Sofie Bekaert, and Wim
Van Criekinge. 2008. PubMeth: a cancer methylation
database combining text-mining and expert annota-
tion. Nucleic Acids Research, 36(suppl 1):D842?846.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Sampo Pyysalo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static Relations: a Piece
5
in the Biomedical Information Extraction Puzzle.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9, Boulder, Colorado. Association for Computa-
tional Linguistics.
Sampo Pyysalo, Tomoko Ohta, Han-Cheol Cho, Dan Sul-
livan, Chunhong Mao, Bruno Sobral, Jun?ichi Tsujii,
and Sophia Ananiadou. 2010. Towards event extrac-
tion from full texts on infectious diseases. In Proceed-
ings of BioNLP?10, pages 132?140.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Proceedings of the IJCNLP 2005,
Companion volume, pages 222?227.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
Cathy H. Wu, Lai-Su L. Yeh, Hongzhan Huang, Leslie
Arminski, Jorge Castro-Alvear, Yongxing Chen,
Zhangzhi Hu, Panagiotis Kourtesis, Robert S. Led-
ley, Baris E. Suzek, C.R. Vinayaka, Jian Zhang, and
Winona C. Barker. 2003. The Protein Information
Resource. Nucleic Acids Research, 31(1):345?347.
H. Yu and E. Agichtein. 2003. Extracting synony-
mous gene and protein terms from biological litera-
ture. Bioinformatics, 19(suppl 1):i340.
6
Proceedings of BioNLP Shared Task 2011 Workshop, pages 74?82,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of the Protein Coreference task in BioNLP Shared Task 2011
Ngan Nguyen
University of Tokyo
Hongo 7-3-1, Bunkyoku, Tokyo
nltngan@is.s.u-tokyo.ac.jp
Jin-Dong Kim
Database Center for Life Science
Yayoi 2-11-16, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Jun?ichi Tsujii
Microsoft Research Asia
5 Dan Ling Street, Haiian District, Beijing
jtsujii@microsoft.com
Abstract
This paper summarizes the Protein Coref-
erence Resolution task of BioNLP Shared
Task 2011. After 7 weeks of system devel-
opment period, the task received final sub-
missions from 6 teams. Evaluation results
show that state-of-the-art performance on the
task can find 22.18% of protein coreferences
with the precision of 73.26%. Analysis of
the submissions shows that several types of
anaphoric expressions including definite ex-
pressions, which occupies a significant part of
the problem, have not yet been solved.
1 Introduction
While named entity recognition (NER) and relation
or event extraction are regarded as standard tasks
of information extraction (IE), coreference resolu-
tion (Ng, 2010; Bejan and Harabagiu, 2010) is more
and more recognized as an important component of
IE for a higher performance. Without coreference
resolution, the performance of IE is often substan-
tially limited due to an abundance of coreference
structures in natural language text, i.e. information
pieces written in text with involvement of a corefer-
ence structure are hard to be captured (Miwa et al,
2010). There have been several attempts for coref-
erence resolution, particularly for newswire texts
(Strassel et al, 2008; Chinchor, 1998). It is also one
of the lessons from BioNLP Shared Task (BioNLP-
ST, hereafter) 2009 that coreference structures in
biomedical text substantially hinder the progress of
fine-grained IE (Kim et al, 2009).
To address the problem of coreference resolution
in molecular biology literature, the Protein Corefer-
ence (COREF) task is arranged in BioNLP-ST 2011
as a supporting task. While the task itself is not
an IE task, it is expected to be a useful compo-
nent in performing the main IE tasks more effec-
tively. To establish a stable evaluation and to observe
the effect of the results of the task to the main IE
tasks, the COREF task particularly focuses on find-
ing anaphoric protein references.
The benchmark data sets for developing and test-
ing coreference resolution system were developed
based on various manual annotations made to the
Genia corpus (Ohta et al, 2002). After 7 weeks of
system development phase, for which training and
development data sets with coreference annotation
were given, six teams submitted their prediction of
coreferences for the test data. The best system ac-
cording to our primary evaluation criteria is evalu-
ated to find 22.18% of anaphoric protein references
at the precision of 73.26%.
This paper presents overall explanation of the
COREF task, which includes task definition (Sec-
tion 2), data preparation (Section 4), evaluation
methods (Section 5), results (Section 7), and thor-
ough analyses (Section 8) to figure out what are
remaining problems for coreference resolution in
biomedical text.
2 Problem Definition
This section provides an explanation of the corefer-
ence resolution task in our focus, through examples.
Figure 1 shows an example text segmented into
four sentences, S2 - S5, where anaphoric corefer-
ences are illustrated with colored extends and ar-
rows. In the figure, protein names are highlighted in
purple, T4 - T10, and anaphoric protein references,
e.g. pronouns and definite noun phrases, are high-
lighted in red, T27, T29, T30, T32, of which the an-
74
Figure 1: Protein coreference annotation
tecedents are indicated by arrows if found in the text.
In the example, the definite noun phrase (NP), this
transcription factor (T32), is a coreference to p65
(T10). Without knowing the coreference structure,
it becomes hard to capture the information written
in the phrase, nuclear exclusion of this transcription
factor, which is localization of p65 (out of nucleus)
according to the framework of BioNLP-ST.
A standard approach would include a step to find
candidate anaphoric expressions that may refer to
proteins. In this task, pronouns, e.g. it or they, and
definite NPs that may refer to proteins, e.g. the tran-
scription factor or the inhibitor are regarded as can-
didates of anaphoric protein references. This step
corresponds to markable detection and anaphoric-
ity determination steps in the jargon of MUC. The
next step would be to find the antecedents of the
anaphoric expressions. This step corresponds to
anaphora resolution in the jargon of MUC.
3 Task Setting
In the task, the training, development and test data
sets are provided in three types of files: the text, the
protein annotation, and the coreference annotation
files. The text files contain plain texts which are tar-
get of annotation. The protein annotation files pro-
vide gold annotation for protein names in the texts,
and the coreference annotation files provide gold an-
notation for anaphoric references to those protein
names. The protein annotation files are given to the
participants, together with all the training, develop-
ment and test data sets. The coreference annotation
files are not given with the test data set, and the task
for the participants is to produce them automatically.
In protein annotation files, annotations for protein
names are given in a stand-off style encoding. For
example, those highlighted in purple in Figure 1 are
protein names, which are given in protein annotation
files as follows:
T4 Protein 275 278 p65
T5 Protein 294 297 p50
T6 Protein 367 372 v-rel
T7 Protein 406 409 p65
T8 Protein 597 600 p50
T9 Protein 843 848 MAD-3
T10 Protein 879 882 p65
The first line indicates there is a protein reference
in the span that begins at 275th character and ends
before 278th character, of which the text is ?p65?,
and the annotation is identified by the id, ?T4?
The coreference annotation files include three sort
of annotations. First, annotations for anaphoric pro-
tein references are given. For example, those in red
in Figure 1 are anaphoric protein references:
T27 Exp 179 222 the N.. 215 222 complex
T29 Exp 307 312 which
T30 Exp 459 471 this .. 464 471 complex
T32 Exp 1022 1047 this .. 1027 1047 tra..
The first line indicates that there is an anaphoric
protein reference in the specified span, of which the
text is ?the NF-kappa B transcription factor com-
plex? (truncated due to limit of space), and that its
minimal expression is ?complex?. Second, noun
phrases that are antecedents of the anaphoric refer-
ences are also given in the coreference annotation
file. For example, T28 and T31 (highlighted in blue)
are antecedents of T29 and T32, respectively, and
thus given in the file:
T28 Exp 264 297 NF-ka..
T31 Exp 868 882 NF-ka..
Third, the coreference relation between the
anaphoric expressions and their antecedents are
given in predicate-argument expressions1:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
The first line indicates there is a coreference rela-
tion, R1, of which the anaphor is T29 and the an-
tecedent is T28, and the relation involves two protein
names, T5 and T4.
Note that, sometimes, an anaphoric expression,
e.g. which (T29), is connected to more than one
protein names, e.g. p65 (T4) and p50 (T5). Some-
times, coreference structures do not involve any spe-
cific protein names, e.g. T30 and T27. In order
1Due to limitation of space, argument names are abbrevi-
ated, e.g. ?Ana? for ?Anaphora?, and ?Ant? for ?Antecedent?
75
to establish a stable evaluation, our primary evalu-
ation will focus only on coreference structures that
involve specific protein names, e.g. T29 and T28,
and T32 and T31. Among the three, only two, R1
and R3, involves specific protein references, T4 and
T5, and T10. Thus, finding of R2 will be ignored
in the primary evaluation. However, those not in-
volving specific protein references are also provided
in the training data to help system development,
and will be considered in the secondary evaluation
mode. See section 5 for more detail.
4 Data Preparation
The data sets for the COREF task are produced
based on three resources: MedCO coreference an-
notation (Su et al, 2008), Genia event annotation
(Kim et al, 2008), and Genia Treebank (Tateisi et
al., 2005). Although the three have been developed
independently from each other, they are annotations
made to the same corpus, the Genia corpus (Kim et
al., 2008). Since COREF was focused on finding
anaphoric references to proteins (or genes), only rel-
evant annotations were extracted from the MedCO
corpus though the following process:
1. From MedCo annotation, coreference entities that
were pronouns and definite base NPs were ex-
tracted, which became candidate anaphoric expres-
sions. The base NPs were determined by consulting
Genia Tree Bank.
2. Among the candidate anaphoric expressions, those
that could not be protein references were filtered
out. This process was done by checking the head
noun of NPs. For example, definite NPs with ?cell?
as their head noun were filtered out. The remaining
ones became candidate protein coreferences.
3. The candidate protein coreferences and their an-
tecedents according to MedCo annotation were in-
cluded in the data files for COREF task.
4. The protein name annotations from Genia event
annotation were added to the data files to deter-
mine which coreference expressions involve protein
name references.
Table 1 summarizes the coreference entities in the
training, development, and test sets for COREF task.
In the table, the anaphoric entities are classified into
four types as follows:
RELAT indicates relative pronouns or relative adjec-
tives, e.g. that, which, or whose.
PRON indicates pronouns, e.g. it.
Type Train Dev Test
RELAT 1193 254 349
PRON 738 149 269
Anaphora DNP 296 58 91
APPOS 9 1 3
N/C 11 1 2
Antecedent 2116 451 674
TOTAL 4363 914 1388
Table 1: Statistics of coreference entities in COREF data
sets: N/C = not-classified.
DNP indicates definite NPs or demonstrative NPs, e.g.
NPs that begin with the, this, etc.
APPOS indicates coreferences in apposition.
5 Evaluation
The coreference resolution performance is evaluated
in two modes.
The Surface coreference mode evaluates the per-
formance of finding anaphoric protein references
and their antecedents, regardless whether the an-
tecedents actually embed protein names or not. In
other words, it evaluates the ability to predict the
coreference relations as provided in the gold coref-
erence annotation file, which we call surface coref-
erence links.
The protein coreference mode evaluates the per-
formance of finding anaphoric protein references
with their links to actual protein names (protein
coreference links). In the implementation of the
evaluation, the chain of surface coreference linkes
is traced until an antecedent embedding a protein
name is found. If a protein-name-embedding an-
tecedent is connected to an anaphora through only
one surfs link, we call the antecedent a direct pro-
tein antecedent. If a protein-name-embedding an-
teceden is connected to an anaphora through more
than one surface link, we call it an indirect protein
antecedent, and the antecedents in the middle of the
chain intermediate antecedents. The performance
evaluated in this mode may be directly connected
to the potential performance in main IE tasks: the
more the (anaphoric) protein references are found,
the more the protein-related events may be found.
For this reason, the protein coreference mode is cho-
sen as the primary evaluation mode.
Evaluation results for both evaluation modes are
76
given in traditional precision, recall and f-score,
which are similar to (Baldwin, 1997).
5.1 Surface coreference
A response expression is matched with a gold ex-
pression following partial match criterion. In par-
ticular, a response expression is considered cor-
rect when it covers the minimal boundary, and is
included in the maximal boundary of expression.
Maximal boundary is the span of expression anno-
tation, and minimal boundary is the head of ex-
pression, as defined in MUC annotation schemes
(Chinchor, 1998). A response link is correct when
its two argument expressions are correctly matched
with those of a gold link.
5.2 Protein coreference
This is the primary evaluation perspective of the pro-
tein coreference task. In this mode, we ignore coref-
erence links that do not reference to proteins. Inter-
mediate antecedents are also ignored.
Protein coreference links are generated from the
surface coreference links. A protein coreference link
is composed of an anaphoric expression and a pro-
tein reference that appears in its direct or indirect
antecedent. Below is an example.
Example:
R1 Coref Ana:T29 Ant:T28 [T5, T4]
R2 Coref Ana:T30 Ant:T27
R3 Coref Ana:T32 Ant:T31 [T10]
R4 Coref Ana:T33 Ant:T32
In this example, supposing that there are four surface
links in the coreference annotation file (T29,T28),
(T30,T27), (T32,T31), and (T33, T32), in which
T28 contains two protein mentions T5, T4, and T31
contains one protein mention T10; thus, the protein
coreference links generated from these surface links
are (T29,T4), (T29,T5), (T32,T10), and (T33, T10).
Notice that T33 is connected with T10 through the
intermediate expression T32.
Response expressions and generated response re-
sult links are matched with gold expressions and
links correspondingly in a way similar to the surface
coreference evaluation mode.
6 Participation
We received submissions from six teams. Each team
was requested to submit a brief description of their
team, which was summarized in Table 2.
Team Member Approach & Tools
UU 1 NLP ML (Yamcha SVM,
Reconcile)
UZ 5 NLP RB (-)
CU 2 NLP RB (-)
UT 1 biochemist ML (SVM-Light)
US 2 AI ML (SVM-Light)
UC 3 NLP, 1 BioNLP ML (Weka SVM)
Table 2: Participation. UU = UofU, UZ = UZH,
CU=ConcordU, UT = UTurku, UZ = UZH, US =
Uszeged, UC = UCD SCI, RB = Rule-based, ML = Ma-
chine learning-based.
TEAM RESP C P R F
UU 86 63 73.26 22.18 34.05
UZ 110 61 55.45 21.48 30.96
CU 87 55 63.22 19.37 29.65
UT 61 41 67.21 14.44 23.77
US 259 9 3.47 3.17 3.31
UC 794 2 0.25 0.70 0.37
Table 3: Protein coreference results. Total num-
ber of gold link = 284. RESP=response, C=correct,
P=precision, R=recall, F=fscore
The tool column shows the external tools used
in resolution processing. Among these tools,
there is only one team used an external coref-
erence resolution framework, Reconcile, which
achieved the state-of-the-art performance for super-
vised learning-based coreference resolution (Stoy-
anov et al, 2010b).
7 Results
7.1 Protein coreference results
Evaluation results in the protein coreference mode
are shown in Table 3. The UU team got the high-
est f-score 34.05%. The UZ and CU teams are
the second- and third-best teams with 30.96% and
29.65% f-score correspondingly, which are compa-
rable to each other. Unfortunately, two teams, US
and UC could not produce meaningful results, and
the other four teams show performance optimized
for high precision. It was expected that the 22.18%
of protein coreferences may contribute to improve
the performance on main task, which was not ob-
served this time, unfortunately.
The first ranked system by UU utilized Recon-
77
TEAM RESP C P R F
UU 360 43 11.94 20.48 15.09
UZ 736 51 6.93 24.29 10.78
CU 365 36 9.86 17.14 12.52
UT 452 50 11.06 23.81 15.11
US 259 4 1.54 1.90 1.71
UC 797 1 0.13 0.48 0.20
Table 4: Surface coreference results. Total num-
ber of gold link = 210. RESP=response, C=correct,
P=precision, R=recall, F=fscore
UU UT
S-correct & P-missing 8 29
S-missing & P-correct 16 5
Table 5: Count of anaphors that have different status in
different evaluation modes. S = surface coreference eval-
uation mode, P = protein coreference evaluation mode
cile which was originally developed for newswire
domain. It supports the hypothesis that machine
learning-based coreference resolution tool trained
on different domains can be helpful for the bio med-
ical domain; however, it still requires some adapta-
tions.
7.2 Surface coreference results
Table 4 shows the evaluation results in the surface
link mode. The overall performances of all the sys-
tems are low, in which recalls are much higher than
the precisions. One possible reason of the low re-
sults is because most of the teams focus on resolv-
ing pronominal coreference; however, they failed to
solve some difficult types of pronoun such as ?it?,
?its?, ?these?, ?them?, and ?which?, which occupy
the majority of anaphoric pronominal expressions
(Table 1). Definite anaphoric expressions were ig-
nored by almost all of the systems (except one sub-
mission).
The results show that the protein coreference res-
olution is not a trivial task; and many parts remains
challenging. In next section, we analyze about po-
tential reason of the low results, and discuss possible
directions for further improvement.
Ex 1 GOLD
T5 DQalpha and DQbeta trans heterodimeric
HLA-DQ molecules
T6 such trans-dimers
T7 which
R1 T6 T5 [T3, T4]
R2 T7 T6
RESP
T5 such trans-dimers
T6 which
R1 T6 T5
Ex 2 GOLD
T18 Five members of this family
(MYC, SCL, TAL-2, LYL-1 and E2A)
T20 their
R3 T20 T18 [T3, T2, T5, T4]
RESP
T19 Five members
T20 their
R2 T20 T19
Table 6: Example of surface-correct & protein-missing
cases. Protein names are underlined, and the min-values
are in italic.
8 Analysis
8.1 Why the rankings based on the two
evaluation methods are not the same?
Comparing with the protein coreference mode, we
can see the rankings based on two evaluation meth-
ods are different. In order to find out what led to
this interesting difference, we further analyzed the
submissions from the two teams UT and UU. The
UT team achieved the highest f-score in the surface
evaluation mode, but was in the fourth rank in the
protein evaluation mode. Meanwhile, the score of
UU team was slightly less than the UT team in the
former mode, but got the highest in the later (Table
3 and Table 4). In other words, there is no clear cor-
relation between the two evaluation results.
Because the two precisions in surface evaluation
mode are not much different, the recalls were the
main contribution in the difference of f-score. An-
alyzing the correct and missing examples in both
evaluation modes, we found that there are anaphors
whose surface links are correct, while the protein
links with the same anaphors are evaluated as miss-
ing; and vice versa with missing surface links and
correct protein links. Counts of anaphors of each
78
type are shown in Table 5. In this table, the cell
at column UT and row S-correct and P-missing can
be interpreted as following. There are 29 anaphors
in the UT response whose surface links are correct
but protein links are missing, which contributes pos-
itively to the recall in surface coreference mode, and
negatively to that in protein coreference mode.
Table 6 shows two examples of S-correct and
P-missing. In the first example, we can see that
the gold antecedent proteins are contained in an in-
direct antecedent. Therefore, when the interme-
diate antecedent is correctly detected by the sur-
face link R1, but the indirect antecedent is not de-
tected, the anaphor is not linked to it antecedent
proteins ?DQalpha? and ?DQbeta?. Another reason
is because response antecedents do not include an-
tecedent proteins. This is actually the problem of
expression boundary detection. An example of this
is example 2 (Table 6), in which the response sur-
face link R2 is correct, but the protein links to the
four proteins are not detected, because the response
antecedent ?five members? does not include the pro-
tein mentions ?SCL, TAL-2, LYL-1 and E2A?. How-
ever, the response antecedent expression is correct
because it contains the minimal boundary ?mem-
bers?.
For S-missing and P-correct, we found that
anaphors are normally directly linked to antecedent
proteins. In other words, expression boundary is
same as protein boundary. Another case is that re-
sponse antecedents contain the antecedent proteins,
but are evaluated as incorrect because the expres-
sion boundary of the response expression is larger
than the gold expression. An example is shown in
Table 7 where the response expression ?a second
GCR, termed GCRbeta? includes the gold expres-
sion ?GCRbeta?. Therefore, although the surface
link is incorrect because the response expression is
evaluated as incorrect, the protein coreference link
receives a full score .
The difference reflects the characteristics of the
two evaluation methods. The analysis result also
shows the affect of markable detection or expression
detection on the resolution evaluation result.
8.2 Protein coreference analysis
We want to see how well each system performs on
each type of anaphor. However, the type information
Ex 3 GOLD
T17 GCRbeta
T18 which
R2 T18 T17 [T4]
RESP
T16 a second GCR, termed GCRbeta
T19 which
R2 T19 T16
Table 7: Examples of S-missing and P-correct
is not explicitly included in the response, so it has
to be induced automatically. We done this by find-
ing the first word of anaphoric expression; then, we
combine it with 1 if the expression is a single-word
expression, or 2 if the expression is multi-word, to
create a sub type value for each anaphor of both
gold and response anaphors. After that, subtypes are
mapped with the anaphor types specified in Section
4 using the mapping in Table 10.
Protein coreference resolution results by sub type
are given in Table 9 and 8. It can be easily seen in
Table 9 which team performed well on which type
of anaphor. In particular, the CU system was good at
resolving the RELAT, APPOS and other types. The
UU team performed well on the DNP type. And for
the PRON type, UZ was the best team. In theory,
knowing this, we can combine strengths of the teams
to tackle all the types.
We analyzed false positive protein anaphora links
to see what types of anaphora are solved by each
system. The recalls in Table 11 are calculated based
on the anaphor type information manually annotated
in the gold data. Comparing with those in Table 9,
there is a small difference due to the automatic in-
duction of anaphoric types based on sub types. It
can be seen in the table 11 that only 77.5 percent of
RELAT-typed anaphora links were resolved (by CU
team), although this type is supposed to be the eas-
iest type. Examining the output data, we found that
the system tends to choose the nearest expression
as the antecedent of a relative pronoun; however,
this is not always correct, as in the following exam-
ples from the UofU submission: ?We also identified
functional Aiolos-binding sites1a in the Bcl-2 pro-
moter1b, which1 are able to activate the luciferase
reporter gene.?, and ?Furthermore, the analysis of
IkappaBalpha turnover demonstrated an increased
79
PRON P- P- P- P- P- P- DNP D- RELAT R-
both-2 it-1 its-1 one-2 that-1 their-1 these-2 this-2 those-1 which-1 whose-1 N/C
UU 36.4 64.4 2 13.3 18.2 62 5 30.8
UZ 46.2 35.7 53.3 7.1 12.5 5.4 59 66.7 15.4
CU 62 70.9 5 42.1
UT 9.5 36.8 10 34.6 9.5 5 30.8
US 13.9 22.9
UC 28.6 9.1
Table 8: Fine-grained results (f-score, %)
Team PRON P- P- DNP D- D- RELAT R- R- Others O- O-
P R F P R F P R F P R F
UU 79.0 11.5 20.1 66.7 5.9 10.8 71.3 56.0 62.7 100.0 18.3 30.8
UZ 62.9 16.9 26.7 12.5 4.4 6.5 71.4 46.7 56.5 50.0 9.1 15.4
CU 64.6 68.0 66.2 50.0 36.4 42.1
UT 72.7 12.3 21.1 14.3 1.5 2.7 73.3 29.3 41.9 100.0 18.2 30.8
US 27.3 6.9 11.0
UC 9.1 1.5 2.6
Table 9: Protein coreference results by coreference type (fscore, %). P = precision, R = recall, F = f-score. O = Others.
TEAM A R D P O
UU 0.0 62.0 5.7 11.1 0.0
UZ 0.0 49.3 4.3 17.0 0.0
CU 0.0 77.5 0.0 0.0 0.0
UT 0.0 32.4 1.4 11.9 14.3
US 0.0 0.0 0.0 6.7 0.0
UC 0.0 0.0 1.4 0.7 0.0
Table 11: Exact recalls by anaphor type, based on man-
ual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=OTHER
degradation of IkappaBalpha2a in HIV-1-infected
cells2b that2 may account for the constitutive DNA
binding activity.?. Expressions with the same index
are coreferential expressions. The a subscript indi-
cates correct antecedent, and b subscript indicates
the wrong one. In these examples, the relative pro-
noun that and which are incorrectly linked with the
nearest expression, which is actually part of post-
modifier or the correct antecedent expression.
For the DNP type, recall of the best system is less
than 6 percent (Table 11), although it is an impor-
tant type which occupies almost one fifth of all pro-
tein links (Table 1). There is only one team, the UC
team, attempted to tackle the anaphor; however, it
resulted in many spurious links. The other teams
did not make any prediction on this type. A possi-
ble reason of this is because there are much more
non-anaphoric definite noun phrases than anaphoric
ones, which making it difficult to train an effective
classier for anaphoricity determination. We have to
seek for a better method for solving the DNP links,
in order to significantly improve protein coreference
resolution system.
Concerning the PRON type, Table 8 shows that
except for that-1, no other figures are higher than
50 percent f-score. This is an interesting obser-
vation because pronominal anaphora problem has
been reported with much higher results on other
domains(Raghunathan et al, 2010), and also on
other bio data (hsiang Lin and Liang, 2004). One
of the reasons for the low recall is because target
anaphoric pronouns in the bio domain are neutral-
gender and third-person pronouns(Nguyen and Kim,
2008), which are difficult to resolve than other types
of pronouns(Stoyanov et al, 2010a).
8.3 Protein coreference analysis - Intermediate
antecedent
As mentioned in the task setting, anaphors can di-
rectly link to their antecedent, or indirectly link via
one or more intermediate antecedents. We counted
the numbers of correct direct and indirect protein
coreference links in each submission (Table 12).
80
Sub type Type Count Sub type Type Count Sub type Type Count
both 1 PRON 2 both 2 PRON 4 either 1 PRON 0
it 1 PRON 17 its 1 PRON 61 one 2 PRON 1
such 2 DNP 2 that 1 RELAT 37 the 2 DNP 20
their 1 PRON 27 them 1 PRON 1 these 1 PRON 1
these 2 DNP 26 they 1 PRON 5 this 1 PRON 1
this 2 DNP 20 those 1 PRON 9 which 1 RELAT 37
whose 1 RELAT 1 whose 2 RELAT 0 (others) N/C 11
Table 10: Mapping from sub type to coreference type. Count = number of anaphors
TEAM A R R D D P P O
Di Di In Di In Di In Di
UU 44 4 15
UZ 35 2 1 23
CU 54 1
UT 22 1 1 16 1
US 8 1
UC 1 1
Total 1 64 7 65 5 126 9 7
Table 12: Numbers of correct protein coreference links
by anaphor type and by number of antecedents, based on
manual type annotation. A=APPOS, R=RELAT, D=DNP,
P=PRON, O=Others. Di=direct, In=indirect.
APPOS and Others types do not have any intermedi-
ate antecedent, thus there is only one column marked
with D (direct protein coreference link). We can
see in this table that very few indirect links were
detected. Therefore, there is place to improve our
resolution system by focusing on detection of such
links.
8.4 Surface coreference results
Because inclusion of all expressions was not a re-
quirement of shared task submission, the submitted
results may not contain expressions that do not in-
volve in any coreference links. Therefore, it is un-
fair to evaluate expression detection based on the re-
sponse expressions.
Evaluation results for anaphoricity determination
are shown in Table 13. The calculation is performed
as following. Supposing that every anaphor has a
response link, the number of anaphors is number
of distinct anaphoric expressions inferred from the
response links, which is given in the first column.
The total number of gold anaphors are also calcu-
lated in similar way. Since response expressions
are lined with gold expressions before evaluation,
Team Resp Align P R F
UU 360 94.2 19.4 33.3 24.6
UZ 736 75.8 22.0 77.1 34.2
CU 365 89.6 15.3 26.7 19.5
UT 452 92.0 18.1 39.0 24.8
US 259 9.3 6.2 7.6 6.8
UC 797 6.8 1.1 4.3 1.8
Table 13: Anaphoricity determination results. Total num-
ber of gold anaphors = 210. Resp = number of response
anchors, Align = alignment rate(%), P = precision (%), R
= recall (%), F = f-score (%)
we provided the alignment rate for reference in the
second column of the table. The third and forth
columns show the precisions and recalls. In theory,
low anaphoricity determination precision results in
many spurious response links, while low recall be-
comes the bottle neck for the overall coreference
resolution recall. Therefore, we can conclude that
the low performance of anaphoricity determination
contribute to the low coreference evaluation results
(Table 4, Table 3).
9 Conclusion
The coreference resolution supporting task of
BioNLP Shared Task 2011 has drawn attention from
researchers of different interests. Although the over-
all results are not good enough to be helpful for the
main shared tasks as expected, the analysis results in
this paper shows the coreference types which have
and have not yet been successfully solved. Tack-
ling the remained problems in expression bound-
ary detection, anaphoricity determination and reso-
lution algorithms for difficult types of anaphors such
as definite noun phrases should be the future work.
Then, it would be interesting to see how much coref-
erence can contribute to event extraction.
81
References
B. Baldwin. 1997. Cogniac: High precision with limited
knowledge and linguistic resources. In Proceedings of
the ACL?97/EACL?97 Workshop on Operational Fac-
tors in Practical, Robust Anaphora Resolution, pages
38?45, Madrid, Spain.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguis-
tic features. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1412?1422, Uppsala, Sweden, July. Association
for Computational Linguistics.
Nancy Chinchor. 1998. Overview of MUC-7/MET-2.
In Message Understanding Conference (MUC-7) Pro-
ceedings.
Yu hsiang Lin and Tyne Liang. 2004. Pronominal and
sortal anaphora resolution for biomedical literature.
In In Proceedings of ROCLING XVI: Conference on
Computational Linguistics and Speech Processing.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
ACL, pages 1396?1411.
Ngan Nguyen and Jin-Dong Kim. 2008. Exploring do-
main differences for the design of a pronoun resolution
system for biomedical texts. In Proceedings of 22nd
International Conference on Computational Linguis-
tics (COLING-2008).
T Ohta, Y Tateisi, H Mima, and J Tsujii. 2002. Ge-
nia corpus: an annotated research abstract corpus in
molecular biology domain. Proceedings of the Hu-
man Language Technology Conference (HLT 2002),
San Diego, California, pages 73?77.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 492?501, October.
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010a. Coreference resolution with
reconcile. In Proceedings of the Conference of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2010).
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,
and D. Hysom. 2010b. Reconcile: A coreference res-
olution platform. In Tech Report - Cornell University.
Stephanie Strassel, Mark Przybocki, Kay Peterson, Zhiyi
Song, and Kazuaki Maeda. 2008. Linguistic Re-
sources and Evaluation Techniques for Evaluation of
Cross-Document Automatic Content Extraction. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC 2008).
Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,
and Jun?ichi Tsujii. 2008. Coreference Resolution in
Biomedical Texts: a Machine Learning Approach. In
Ontologies and Text Mining for Life Sciences?08.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax annotation for the genia
corpus. In International Joint Conference on Natu-
ral Language Processing, pages 222?227, Jeju Island,
Korea, October.
82
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 19?27,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Utilizing State-of-the-art Parsers to Diagnose Problems in Treebank
Annotation for a Less Resourced Language
Quy T. Nguyen
University of Information
Technology, Ho Chi Minh City
quynt@uit.edu.vn
Ngan L.T. Nguyen
National Institute
of Informatics, Tokyo
ngan@nii.ac.jp
Yusuke Miyao
National Institute
of Informatics, Tokyo
yusuke@nii.ac.jp
Abstract
The recent success of statistical pars-
ing methods has made treebanks become
important resources for building good
parsers. However, constructing high-
quality annotated treebanks is a challeng-
ing task. We utilized two publicly avail-
able parsers, Berkeley and MST parsers,
for feedback on improving the quality of
part-of-speech tagging for the Vietnamese
Treebank. Analysis of the treebank and
parsing errors revealed how problems with
the Vietnamese Treebank influenced the
parsing results and real difficulties of Viet-
namese parsing that required further im-
provements to existing parsing technolo-
gies.
1 Introduction
Treebanks, corpora annotated with syntactic struc-
tures, have become more and more important
for language processing. The Vietnamese Tree-
bank (VTB) has been built as part of the national
project ?Vietnamese language and speech process-
ing (VLSP)? to strengthen automatic processing of
the Vietnamese language (Nguyen et al, 2009).
However, when we trained the Berkeley parser
(Petrov et al, 2006) in our preliminary experiment
with VTB and evaluated it using the corpus, the
parser only achieved an F-score of 72.1%. This
percentage was far lower than the state-of-the-art
performance reported for the Berkeley parser on
the English Penn Treebank of 90.2% (Petrov et
al., 2006). There are two possible reasons for this.
First, the quality of VTB is not good enough to
construct a good parser that included the quality of
the annotation scheme, the annotation guidelines,
and the annotation process. Second, parsing Viet-
namese is a difficult problem on its own, and we
need to seek new solutions to this.
Nguyen et al (2012) proposed methods of
improving the annotations of word segmentation
(WS) for VTB. They also evaluated different WS
criteria in two applications, i.e., machine trans-
lation and text classification. This paper focuses
on improving the quality of parts-of-speech (POS)
annotations by using state-of-the-art parsers to
provide feedback for this process.
The difficulties with Vietnamese POS tag-
ging have been recognized by many researchers
(Nghiem et al, 2008; Le et al, 2010). There is lit-
tle consensus as to the methodology for classifying
words. Polysemous words, words with the same
surface form but having different meanings and
grammar functions, are very popular in the Viet-
namese language. For example, the word ?c??
can be a noun that means neck/she, or an adjec-
tive that means ancient depending on the context.
This characteristic makes it difficult to tag POSs
for Vietnamese, both manually and automatically.
The rest of this paper is organized as follows:
a brief introduction to VTB and its annotation
schemes are provided in Section 2. Then, previ-
ous work is summarized in Section 3. Section 4
describes our methods of detecting and correcting
inconsistencies in POSs in the VTB corpus. Eval-
uations of these methods are described in Section
5. Finally, Section 6 explains our evaluations of
the Berkeley parser and Minimum-Spanning Tree
(MST) parser on different versions of the VTB
corpus, which were created by using detected in-
consistencies. These results from evaluations are
considered to be a way of measuring the effect
of automatically detected and corrected inconsis-
tencies. We could observe difficulties with Viet-
namese that affected the quality of parsers by ana-
lyzing the results from parsing.
Our experiences in using state-of-the-art parsers
for treebank annotation, which are presented in
this paper, should not only benefit the Vietnamese
language, but also other languages with similar
19
Label Name Example
N Common noun nh?n d?n {people}
Np Proper noun Vi?t Nam {Vietnam}
Nc Classifer noun con, c?i, b?c {*}
Nu Unit noun m?t {meter}
V Verb ng?i {sit}
A Adjective t?t {good}
P Pronoun t?i {I}, h?n {he}
L Determiner m?i {every}, nh?ng {*}
M Number m?t {one}
R Adverb ??, s?, ?ang {*}
E Preposition tr?n {on}
C Conjunction tuy nhi?n {however}
I Exclamation ?i, chao, a ha {*}
T Particle ?, ?y, ch?ng {*}
B Foreign word internet, email
Y Abbreviation APEC, WTO, HIV
S Affix b?t, v?, ?a {*}
X Other
Table 1: VTB part-of-speech tag set
characteristics.
2 Brief introduction to VTB
The VTB corpus contains 10.433 sentences
(274.266 tokens), semi-manually annotated with
three layers of WS, POS tagging, and bracketing.
The first annotation is produced for each annota-
tion layer by using automatic tools. Then, the an-
notators revise these data. The WS and POS an-
notation schemes were introduced by Nguyen et
al. (2012). This section briefly introduces POS tag
set and a bracketing annotation scheme.
VTB specifies the 18 different POS tags sum-
marized in Table 1 (Nguyen et al, 2010a). Each
unit in this table goes with several example words.
English translations of these words are included in
braces. However, as we could not find any appro-
priate English translations for some words, these
empty translations have been denoted by asterisks
(*).
The VTB corpus is annotated with three syn-
tactic tag types: constituency tags, functional
tags, and null-element tags. There are 18 con-
stituency tags in VTB. The functional tags are
used to enrich information for syntactic trees, such
as where functional tag ?SUB? is combined with
constituency tag ?NP?, which is presented as ?NP-
SUB? to indicate this noun phrase is a subject.
There are 17 functional tags in VTB. The head
word of a phrase is annotated with functional tag
?H?.
The phrase structures of Vietnamese include
three positions: <pre-head>, <head>, and <post-
head> (Vietnamese grammar, 1983; Nguyen et al,
2010c). The head word of the phrase is in the
<head> position. The words that are in the <pre-
head> and <post-head> positions are modifiers of
the head word.
There is a special type of noun in Vietnamese
that we have called Nc-noun in this paper. Nc-
nouns can be classifier nouns or common nouns
depending on their modifiers. For example, the
Nc-noun ?con? is a classifier noun if its modifier
is the word ?c? {fish}? (?con c??, which means
a specific fish, similar to ?the fish? in English).
However, the Nc-noun ?con {child}? is a common
noun if its modifier is the word ?gh?? (?con gh??,
which means ?stepchild? in English). We found
that Nc-nouns always appeared in the head posi-
tions of noun phrases by investigating the VTB
corpus. There is currently little consensus as to
the methodology for annotating Nc-nouns (Hoang,
1998; Nguyen et al, 2010b; Nguyen et al, 2010a).
3 Summarization of previous work
Nguyen et al (2012) described methods of detect-
ing and correcting WS inconsistencies in the VTB
corpus. These methods focused on two types of
WS inconsistency, variation and structural incon-
sistency, which are defined below.
Variation inconsistency: is a sequence of tokens
that has more than one way of being segmented in
the corpus.
Structural inconsistency: occurs when different
sequences have similar structures, and thus should
be split in the same way, but are segmented in dif-
ferent ways in the corpus. Nguyen et al (2012)
pointed out three typical cases of structural in-
consistency that were analyzed as classifier nouns
(Nc), affixes (S), and special characters.
Nguyen et al (2012) analyzed N-gram se-
quences and phrase structures to detect WS in-
consistencies. Then, the detected WS inconsis-
tencies were classified into several patterns of in-
consistencies, parts of which were manually fixed
to improve the quality of the corpus. The rest
were used to create different versions of the VTB
corpus. These data sets were evaluated on auto-
matic WS and its applications to text classification
and English-Vietnamese statistical machine trans-
lations to find appropriate criteria for automatic
WS and its applications.
Their experiments revealed that the
VAR_FREQ data set achieved excellent re-
sults in these applications. The VAR_FREQ data
20
set was the original VTB corpus with manually
corrected structural inconsistencies in special
characters and selected segmentations with higher
frequencies in all detected variations. There-
fore, we used the VAR_FREQ data set in our
experiments.
4 Methods of detecting and correcting
inconsistencies in POS annotations
We propose two kinds of methods of detecting
and correcting inconsistencies. They correspond
to two different types of POS inconsistency that
we call multi-POS inconsistency (MI) and Nc in-
consistency (NcI), which are defined as follows.
Multi-POS inconsistency: is a word that is not
Nc-noun and has more than one POS tag at each
position in each phrase category.
Nc inconsistency: is a sequence of Nc-noun and
modifier, in which Nc-noun has more than one
way of POS annotation in the VTB corpus.
We separated the POS inconsistencies into these
two types of inconsistencies because Nc-nouns
are special types of words in Vietnamese. The
methods of detecting and correcting NcIs were
language-specific methods developed based on the
characteristics of Vietnamese. However, as the
methods for MIs are rather general, they can be
applied to other languages.
4.1 General method for multi-POS
inconsistencies
Detection method (MI_DM)
Our main problem was to distinguish MIs
from polysemous words, since polysemous words
should not be considered inconsistent annotations.
Our method was based on the position of words in
phrases and phrase categories. This idea resulted
from the observation that polysemous words have
many POS tags; however, each word usually has
only one true POS tag at each position in each
phrase category. For example, when a phrase cat-
egory is a verb phrase, the word ?can? in the pre-
head position of the verb phrase ?(VP (MD can)
(VB can))? should be a modal, but the word ?can?
in the head position should be a verb. Further, the
word ?cut? in the head position of a noun phrase
?(NP (DT a) (JJ further) (NN cut))? should be a
noun, but the word ?cut? in the head position of
the verb phrase ?(VP (VB cut) (NP (NNS costs)))?
should be a verb. This may be more frequent in
Vietnamese because it is not an inflectional lan-
guage i.e., the word form does not change accord-
ing to tenses, word categories (e.g., nouns, verbs,
and adjectives), or number (singular and plural).
The method involved three steps. First, we
extracted words in the same position for each
phrase category. Second, we counted the num-
ber of different POS tags of each word. Words
that had more than one POS tag were determined
to be multi-POS inconsistencies. For example, in
the following two preposition phrases, ?(PP (E-
H c?a) (P ch?ng_t?i1)) {of us}? and ?(PP (C-H
c?a) (P h?i_ngh?)) {of conference}?, the words
?c?a {of}? appear at the head positions of both
phrases, but they are annotated with different POS
tags, preposition (E) and conjunction (C). There-
fore, they are MIs according to our method.
It should be noted that this method was applied
to words that were direct children of a phrase.
Embedded phrases, such as ?(PP (E c?a) (P
ch?ng_t?i))? in ?(NP (M hai) (Nc-H con) (N m?o)
(PP (E c?a) (P ch?ng_t?i))) {our two cats}?, were
considered separately.
Correction method (MI_CM)
A multi-POS inconsistency detected with the
MI_DM method is denoted by ?w|P1-f1|P2-
f2|...|Pn-fn AC?, where Pi (i = 1, 2, ..., n) is a POS
tag of word w, fi is the frequency of POS tag Pi,
and AC is applying condition of w. Our method
of correcting the POS tag for POS inconsistency
?w|P1-f1|P2-f2|...|Pn-fn AC? involves two steps.
First, we select the POS tag with the highest fre-
quency of all POS tags of ?w|P1-f1|P2-f2|...|Pn-fn
AC? (Pmax). Second, we replace POS tags Pi of
all instances (w|Pi) satisfying condition AC with
POS tag Pmax. For MIs, the AC of word w is its
phrase category and position in the phrase.
For example, ?to?n b?|L-27|P-2? is a multi-
POS inconsistency in the pre-head position of a
noun phrase. The frequency of POS tag ?L? is 27
and the frequency of POS tag ?P? is 2. There-
fore, ?L? is the POS tag that was selected by the
MI_CM method. We replace all POS tags Pi of
instances ?to?n b?|Pi? in the pre-head positions
of noun phrases with POS tag ?L?.
4.2 Language-specific method for classifier
nouns
Detection method
As mentioned in Section 2, an Nc-noun can be
1We used underscore ?_? to link syllables of Vietnamese
compound words.
21
annotated with POS tag ?Nc? or ?N? depending
on the modifier that follows that Nc-noun. Ana-
lyzing the VTB corpus revealed that Nc-nouns had
two characteristics. First, an Nc-noun that is fol-
lowed by the same word at each occurrence is usu-
ally annotated with the same POS tag. Second, an
Nc-noun that is followed by a phrase or nothing at
each occurrence is annotated with the same POS
tag. Based on these two cases, we propose two
methods of detecting NcIs, which we have called
NcI_DM1 and NcI_DM2. They are described be-
low.
NcI_DM1: We counted Nc-nouns in VTB that
had two or more ways of POS annotation, satis-
fying the condition that Nc-nouns are followed by
a phrase or nothing. For example, the Nc-noun
?con? in ?(NP (M 2) (N-H con)) {2 children}? is
followed by nothing or it is followed by a prepo-
sitional phrase as in ?(NP (L c?c) (N-H con) (PP
(E-H c?a) (P t?i))) {my children}?.
NcI_DM2: We counted two-gram sequences
beginning with an Nc-noun in VTB that had two
or more ways of POS annotation of the Nc-noun,
satisfying the conditions that two tokens were all
in the same phrase and and they all had the same
depth in a phrase. For example, the Nc-noun
?con? in the two-gram ?con g?i {daughter}? was
sometimes annotated ?Nc?, and sometimes anno-
tated ?N? in VTB; in addition, as ?con? and ?g?i?
in the structure ?(NP (Nc-H con) (N g?i) (PP (E-
H c?a) (P t?i))) {my daughter}? were in the same
phrase and have the same depth, ?con? was an
NcI.
Correction method
We denoted NcIs with ?w|P1-f1|P2-f2|...|Pn-fn
AC? similarly to MIs. We also replaced the POS
tag of Nc-nouns with the highest frequency tag.
The only differences were the applying conditions
that varied according to the previous two cases of
NcIs.
? For Nc inconsistencies detected by the
NcI_DM1 method, AC is defined as follows:
w is an Nc-noun that is followed by nothing
or a phrase.
? For Nc inconsistencies detected by the
NcI_DM2 method, AC is defined as follows:
w is an Nc-noun that must be followed by a
word, m.
5 Results and evaluation
We detected and corrected MIs and NcIs based
on the two data sets, ORG and VAR_FREQ. The
ORG data set was the original VTB corpus and
VAR_FREQ was the original corpus with modifi-
cations to WS annotation. This setting was made
similar to that used by Nguyen et al (2012) to
enable comparison.
There are a total of 128,871 phrases in the VTB
corpus. The top five types of phrases are noun
phrases (NPs) (representing 49.6% of the total
number of phrases), verb phrases (VPs), preposi-
tional phrases (PPs), adjectival phrases (ADJPs),
and quantity phrases (QPs), representing 99.1% of
the total number of phrases in the VTB corpus. We
analyzed the VTB corpus based on these five types
of phrases.
5.1 Results for detected POS inconsistencies
Tables 2 and 3 show the overall statistics for
MIs and NcIs for each phrase category. The sec-
ond and third columns in these tables indicate the
numbers of inconsistencies and their instances that
were detected in the ORG data set. The fourth and
fifth columns indicate the numbers of inconsisten-
cies and their instances that were detected in the
VAR_FREQ data set. The rows in Table 3 indicate
the number of NcIs and the number of instances
detected with the NcI_DM1 and NcI_DM2 meth-
ods.
According to Table 2, most of the MIs occurred
in noun phrases, representing more than 72% of
the total number of MIs. All NcIs in Table 3 are
also in noun phrases. There are two possible rea-
sons for this. First, noun phrases represent the ma-
jority of phrases in VTB (represent 49.6% of the
total number of phrases in the VTB corpus). Sec-
ond, nouns are sub-divided into many other types
(common noun (N), classifier noun (Nc), proper
noun (Np), and unit noun (Nu)) (mentioned in Sec-
tion 2), which may confuse annotators in anno-
tating POS tags for nouns. In addition, the high
number of NcIs in Table 3 indicate that it is diffi-
cult to distinguish between Nc and other types of
nouns. Therefore, we need to have clearer annota-
tion guidelines for this.
5.2 Evaluation of methods to detect and
correct inconsistencies
We estimated the accuracy of our methods which
detected and corrected inconsistencies in POS tag-
22
Phrase
ORG VAR_FREQ
Inc Ins Inc Ins
NP 792 28,423 752 27,067
VP 221 10,158 139 10,110
ADJP 64 1,302 61 1,257
QP 4 22 4 22
PP 14 5,649 13 5,628
Total 1,095 45,554 969 44,084
Table 2: Statistics for multi-POS inconsistencies
for each phrase category in VTB. Number of In-
consistencies (Inc) and Number of Instances (Ins).
Detection method
ORG VAR_FREQ
Inc Ins Inc Ins
NcI_DM1 52 3,801 51 3,792
NcI_DM2 338 2,468 326 2,412
Total 390 6,269 377 6,204
Table 3: Statistics for Nc inconsistencies in head
positions of noun phrases in VTB.
ging by manually inspecting inconsistent annota-
tions. We manually inspected the two data sets
of ORG_EVAL and ORG_POS_EVAL. To cre-
ate ORG_EVAL, we randomly selected 100 sen-
tences which contained instances of POS incon-
sistencies in the ORG data set. ORG_EVAL con-
tained 459 instances of 157 POS inconsistencies.
ORG_POS_EVAL was the ORG_EVAL data set
with corrections made to multi-POS inconsisten-
cies and Nc inconsistencies with our methods of
correction above.
Detection: We manually checked POS incon-
sistencies and found that 153 cases out of 157 POS
inconsistencies (97.5%) were actual inconsisten-
cies. There were four cases that our method de-
tected as multi-POS inconsistencies, but they were
actually ambiguities in Vietnamese POS tagging.
They were polysemous words whose meanings
and POS tags depended on surrounding words, but
did not depend on their positions in phrases. For
example, the word ?s?ng? in the post-head posi-
tions of the verb phrases VP1 and VP2 below, can
be a noun that means morning in English, or it can
be an adjective that means bright, depending on
the preceding verb.
VP1: (VP (V-H th?p) (A s?ng) {lighten bright}
VP2: (VP (V-H ?i) (N s?ng) {go in the morning}
Correction: Table 4 shows results of com-
parison of the POS tags for 459 instances in
ORG_EVAL and those in ORG_POS_EVAL.
These results indicate that there are instances
whose POS tags are incorrect in ORG_EVAL
but correct in ORG_POS_EVAL (the third row
ORG_EVAL ORG_POS_EVAL No. of Instances
correct correct 404
incorrect correct 41
correct incorrect 11
incorrect incorrect 3
Total 459
Table 4: Comparison of POS tags for 459
instances in ORG_EVAL with those in
ORG_POS_EVAL.
PoPOS Counts Examples
Nc-N 385 ng??i {the, person}
N-V 186 m?t m?t {loss}
N-Np 176 H?i {association}
N-A 144 kh? kh?n {difficult}
V-A 92 ph?i {must, right}
Table 5: Top five pairs of confusing POS tags.
in Table 4), and there are instances whose POS
tags are correct in ORG_EVAL but incorrect in
ORG_POS_EVAL (the fourth row in Table 4).
The results in Table 4 indicate that, the number
of correct POS tags in ORG_POS_EVAL (445 in-
stances, representing 96.9% of the total number of
instances) is higher than that in ORG_EVAL (415
instances, representing 90.4% of the total number
of instances). This means our methods of correct-
ing inconsistencies in POS tagging improved the
quality of treebank annotations.
5.3 Analysis of detected inconsistencies
We analyzed the detected POS inconsistencies to
find the reasons for inconsistent POS annotations.
We classified the detected POS inconsistencies ac-
cording to pairs of their POS tags. There were
a total of 85 patterns of pairs of POS tags. Ta-
ble 5 lists the top five confusing patterns (PoPOS),
their counts of inconsistencies (Counts), and ex-
amples. It also seemed to be extremely confus-
ing for the annotators to distinguish types of nouns
(Nc and N, and N and Np) and distinguish nouns
from other types of words (such as verbs, adjec-
tives, and pronouns).
We investigated POS inconsistencies and the
annotation guidelines (Nguyen et al, 2010b;
Nguyen et al, 2010a; Nguyen et al, 2010c) to
find why common nouns were sometimes tagged
as classifier nouns and vice versa, and verbs were
sometimes tagged as common nouns and vice
versa, and so on. We found that these POS in-
consistencies belonged to polysemous words that
were difficult to tag.
The difficulties with tagging polysemous words
23
were due to four main reasons: (1) The POS of a
polysemous word changes according to the func-
tion of that polysemous word in each phrase cate-
gory or changes according to the meaning of sur-
rounding words. Although polysemous words are
annotated with different POS tags, they do not
change their word form. (2) The way polysemous
words are tagged according to their context is not
completely clear in the POS tagging guidelines.
(3) Annotators referred to a dictionary that had
been built as part of the VLSP project (Nguyen et
al., 2009) (VLSP dictionary) to annotate the VTB
corpus. However, this dictionary lacked various
words and did not cover all contexts for the words.
For example, ?h?n {more than}? in Vietnamese is
an adjective when it is the head word of an adjec-
tival phrase, but ?h?n {over}? is an adverb when it
is the modifier of a quantifier noun (such as ?h?n
200 sinh vi?n {over 200 students}?). However, the
VLSP dictionary only considered ?h?n? to be an
adjective (?t?i h?n n? hai tu?i {I am more than
him two years old}?). No cases where ?h?n? was
an adverb were mentioned in this dictionary. (4)
There are several overlapping but conflicting in-
structions across the annotation guidelines for dif-
ferent layers of the treebank. For example, the
combinations of affixes and words they modify to
create compound words are clear in the WS guide-
lines, but POS tagging guidelines treat affixes as
words and they are annotated as POS tags ?S?.
For words modifying quantifier nouns, such as
?h?n and g?n {over and about}?, the POS tagging
guidelines treat them as adjectives, but the brack-
eting guidelines treat them as adverbs. Therefore,
our method detected multi-POS inconsistencies as
?h?n|A-135|R-51?, ?g?n|A-102|R-5? at the pre-
head positions of noun phrases. Since the frequen-
cies of the adjective tags were greater than those of
adverb tags (fA > fR), these words were automati-
cally assigned to adjective POS tags (A) according
to our method of correction. These were POS in-
consistencies that our method of correction could
not be applied to, because the frequency of incor-
rect POS tags was higher than that of actual POS
tags.
6 Evaluation of state-of-the-art parsers
on VTB
We carried out experiments to evaluate two pop-
ular parsers, a syntactic parser and a dependency
parser, on different versions of the VTB corpus.
Some of these data sets were made the same as the
data settings for WS in Nguyen et al (2012). The
other data sets contained changes in POS annota-
tions following our methods of correcting incon-
sistencies presented in Section 4. We could ob-
serve how the problems with WS and POS tag-
ging influenced the quality of Vietnamese parsing
by analyzing the parsing results.
6.1 Experimental settings
Data. Nine configurations of the VTB corpus
were created as follows:
? ORG: The original VTB corpus.
? BASE, STRUCT_AFFIX, STRUCT_NC,
VAR_SPLIT, VAR_COMB, and
VAR_FREQ correspond to different set-
tings for WS described in Nguyen et
al. (2012).
? ORG_POS: The ORG data set with correc-
tions for multi-POS inconsistencies and Nc
inconsistencies by using the methods in Sec-
tion 4.1 and 4.2.
? VAR_FREQ_POS: The VAR_FREQ data set
with corrections for multi-POS inconsisten-
cies and Nc inconsistencies by using the
methods in Section 4.1 and 4.2.
Each of the nine data sets was randomly split
into two subsets for training and testing our parser
models. The training set contained 9,443 sen-
tences, and the testing set contained 1,000 sen-
tences.
Tools
We used the Berkeley parser (Petrov et al,
2006) to evaluate the syntactic parser on VTB.
This parser has been used in experiments in En-
glish, German, and Chinese and achieved an F1 of
90.2% on the English Penn Treebank.
We used the conversion tool built by Johans-
son et al (2007) to convert VTB into dependency
trees.
We used the MST parser to evaluate the depen-
dency parsing on VTB. This parser was evaluated
on the English Penn Treebank (Mcdonald et al,
2006a) and 13 other languages (Mcdonald et al,
2006b). Its accuracy achieved 90.7% on the En-
glish Penn Treebank.
We made use of the bracket scoring program
EVALB, which was built by Sekine et al (1997),
24
Data sets Bracketing F-measures
ORG 72.10
BASE 72.20
STRUCT_AFFIX 72.60
STRUCT_NC 71.92
VAR_SPLIT 72.03
VAR_COMB 72.46
VAR_FREQ 72.34
ORG_POS 72.72
VAR_FREQ_POS 73.21
Table 6: Bracketing F-measures of Berkeley
parser on nine configurations of VTB corpus.
Data set UA LA
ORG 50.51 46.14
BASE 53.90 50.14
STRUCT_AFFIX 54.00 50.25
STRUCT_NC 53.88 49.96
VAR_SPLIT 53.95 50.14
VAR_COMB 53.93 50.27
VAR_FREQ 54.21 50.41
ORG_POS 54.20 50.37
VAR_FREQ_POS 57.87 53.19
Table 7: Dependency accuracy of MSTParser on
nine configurations of VTB corpus. Unlabeled
Accuracy (UA), Labeled Accuracy (LA).
to evaluate the performance of the Berkeley parser.
As an evaluation tool was included in the MST
parser tool, we used it to evaluate the MST parser.
6.2 Experimental results
The bracketing F-measures of the Berkeley parser
on nine configurations of the VTB corpus are
listed in Table 6. The dependency accuracies of
the MST parser on nine configurations of the VTB
corpus are shown in Table 7. These results indicate
that the quality of the treebank strongly affected
the quality of the parsers.
According to Table 6, all modifications to WS
inconsistencies improved the performance of the
Berkeley parser except for STRUCT_NC and
VAR_SPLIT. More importantly, the ORG_POS
model achieved better results than the ORG
model, and the VAR_FREQ_POS model achieved
better results than the VAR_FREQ model, which
indicates that the modifications to POS inconsis-
tencies improved the performance of the Berkeley
parser. The VAR_FREQ_POS model scored 1.11
point higher than ORG, which is a significant im-
provement.
Dependency accuracies of the MST parser
in Table 7 indicate that all modifications to
POS inconsistencies improved the performance
of the MST parser. All modifications to WS
APSs CCTs and Freq
A M N NP-79|ADJP-27
A V VP-56|ADJP-78|NP-2
Table 8: Examples of ambiguous POS sequences
(APSs), their CCTs, and frequency of each CCT
(Freq)
inconsistencies also improved the performance
of the MST parser except for STRUCT_NC.
The VAR_FREQ_POS model scored 7.36 points
higher than ORG, which is a significant improve-
ment.
6.3 Analysis of parsing results
The results for the Berkeley parser and MST
parser trained on the POS-modified versions of
VTB were better than those trained on the origi-
nal VTB corpus, but they were still much lower
than the performance of the same parsers on
the English language. We analyzed error based
on the output data of the best parsing results
(VAR_FREQ_POS) for the Berkeley parser, and
found that the unmatched annotations between
gold and test data were caused by ambiguous POS
sequences in the VTB corpus.
An ambiguous POS sequence is a sequence of
POS tags that has two or more constituency tags.
For example, there are the verb phrase ?(VP (R
?ang) (A c?m_c?i) (V l?m)) {* (be) painstak-
ingly doing}? and the adjectival phrase ?(ADJP (R
r?t) (A d?) (V th?c_hi?n)) {very easy (to) imple-
ment}? in the training data of VAR_FREQ_POS.
As these two phrases have the same POS sequence
?R A V?, ?R A V? is an ambiguous POS se-
quence, and VP and ADJP are confusing con-
stituency tags (CCTs). We found 42,373 occur-
rences of 213 ambiguous POS sequences (repre-
senting 37.02% of all phrases) in the training data
of VAR_FREQ_POS. We also found 1,065 oc-
currences of 13 ambiguous POS sequences in the
parsing results for VAR_FREQ_POS. Some ex-
amples of ambiguous POS sequences, their CCTs,
and the number of occurrences of each CCT in the
training data of VAR_FREQ_POS are listed in Ta-
ble 8.
We classified the detected ambiguous POS se-
quences according to pairs of different CCTs to
find the reasons for ambiguity in each pair. There
were a total of 42 pairs of CCTs, whose top three
pairs, along with their counts of types of am-
biguous POS sequences, and examples of ambigu-
25
Pairs of CCTs Counts Examples
NP-VP 61 P V N, ...
VP-ADJP 54 R A V, A V N, ...
ADJP-NP 52 A M N, ...
Table 9: Top three pairs of confusing constituency
tags
Pairs of CCTs 1 2
NP-VP M, L ,R ,V N, R, M, P, A
VP-ADJP A, R N, R
ADJP-NP N, R R, M, A, L
Table 10: Statistics for POS tags at pre-head posi-
tion of each phrase category.
ous POS sequences are listed in Table 9. We
extracted different POS tags at each position of
each phrase category for each pair of CCTs, based
on the ambiguous POS sequences. For example,
the third row in Table 9 has ?R A V? and ?A V
N?, which are two ambiguous POS sequences that
were sometimes annotated as VP and sometimes
annotated as ADJP. The different POS tags that
were extracted from the pre-head positions of VPs
based on these two POS sequences were ?R, A?
and ?R? was the POS tag that was extracted from
the pre-head positions of ADJPs based on these
two POS sequences. These POS tags are important
clues to finding reasons for ambiguities in POS se-
quences.
Table 10 summarizes the extracted POS tags at
pre-head positions for the top three pairs of CCTs.
For example, the POS tags in row NP-VP and col-
umn 1 are in the pre-head positions of NP and the
POS tags in row NP-VP and column 2 are in the
pre-head positions of VP. By comparing these re-
sults with the structures of the pre-head positions
of phrase categories in VTB bracketing guidelines
(Nguyen et al, 2010c), we found many cases that
were not annotated according to instructions in the
VTB bracketing guidelines, such as those accord-
ing to Table 10, where an adjective (A) is in the
pre-head position of VP, but according to the VTB
bracketing guidelines, the structure of the pre-head
position of VB only includes adverb (R).
We investigated cases that had not been anno-
tated according to the guidelines, and found two
possible reasons that caused ambiguous POS se-
quences. First, although our methods improved
the quality of the VTB corpus, some POS anno-
tation errors remained in the VTB corpus. These
POS annotation errors were cases to which our
methods could not be applied (mentioned in Sec-
tion 5). Second, there were ambiguities in POS
sequences caused by Vietnamese characteristics,
such as the adjectival phrase ?(ADJP (R ?ang)
(N ng?y_??m) (A ?au_??n)) {* day-and-night
painful}? and the noun phrase ?(NP (R c?ng) (N
sinh_vi?n) (A gi?i)) {also good student}? that had
the same POS sequence of ?R N A?.
Therefore, POS annotation errors need to be
eliminated from the VTB corpus to further im-
prove its quality and that of the Vietnamese parser.
We not only need to eliminate overlapping but
conflicting instructions, which were mentioned in
Section 5.3, from the guidelines, but we also have
to complete annotation instructions for cases that
have not been treated (or not been clearly treated)
in the guidelines. We may also need to improve
POS tag set because adverbs modifying adjectives,
verbs and nouns are all presently tagged as ?R?,
which caused ambiguous POS sequences, such as
the ambiguous POS sequence ?R N A? mentioned
above. If we use different POS tags for the adverb
??ang?, which modifies the adjective ??au ??n
{painful}?, and the adverb ?c?ng?, which modi-
fies the noun ?sinh vi?n {student}?, we can elimi-
nate ambiguous POS sequences in these cases.
7 Conclusion
We proposed several methods of improving the
quality of the VTB corpus. Our manual evalua-
tion revealed that our methods improved the qual-
ity of the VTB corpus by 6.5% with correct POS
tags. Analysis of inconsistencies and the annota-
tion guidelines suggested that: (1) better instruc-
tions should be added to the VTB guidelines to
help annotators to distinguish difficult POS tags,
(2) overlapping but conflicting instructions should
be eliminated from the VTB guidelines, and (3)
annotations that referred to dictionaries should be
avoided.
To the best of our knowledge, this paper is the
first report on evaluating state-of-the-art parsers
used on the Vietnamese language. The results ob-
tained from evaluating these two parsers were used
as feedback to improve the quality of treebank an-
notations. We also thoroughly analyzed the pars-
ing output, which revealed challenging issues in
treebank annotations and in the Vietnamese pars-
ing problem itself.
26
References
Anna M. D. Sciullo and Edwin Williams. 1987. On
the definition of word. The MIT Press.
Fei Xia. 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
Minh Nghiem, Dien Dinh and Mai Nguyen. 2008. Im-
proving Vietnamese POS tagging by integrating a
rich feature set and Support Vector Machines. Pro-
ceedings of RIVF 2008, pages: 128?133.
Phe Hoang. 1998. Vietnamese Dictionary. Scientific
& Technical Publishing.
Phuong H. Le, Azim Roussanaly, Huyen T. M. Nguyen
and Mathias Rossignol. 2010. An empirical study of
maximum entropy approach for part-of-speech tag-
ging of Vietnamese texts. Proceedings of TALN
2010 Conference. Montreal, Canada.
Quy T. Nguyen, Ngan L.T. Nguyen and Yusuke Miyao.
2012. Comparing Different Criteria for Vietnamese
Word Segmentation. Proceedings of 3rd Workshop
on South and Southeast Asian Natural Language
Processing (SANLP), pages: 53?68.
Richard Johansson and Pierre Nugues. 2007. Extended
Constituent-to-dependency Conversion for English.
Proceedings of NODALIDA, Tartu, Estonia, pages:
105?112.
Ryan Mcdonald and Fernando Pereira. 2006a. On-
line Learning of Approximate Dependency Parsing
Algorithms. Proceedings of 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics: EACL 2006, pages: 81?88.
Ryan Mcdonald, Kevin Lerman and Fernando Pereira.
2006b. Multilingual Dependency Analysis with
a Two-Stage Discriminative Parser. Proceedings
of Tenth Conference on Computational Natural
Language Learning (CoNLL-X), Bergan, Norway,
pages: 216?220.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. Proceedings of 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages: 433?440.
Thai P. Nguyen, Luong X. Vu and Huyen T.M. Nguyen.
2010a. VTB part-of-speech tagging guidelines.
Thai P. Nguyen, Luong X. Vu and Huyen T.M. Nguyen.
2010b. VTB word segmentation guidelines.
Thai P. Nguyen, Luong X. Vu, Huyen T.M. Nguyen,
Hiep V. Nguyen and Phuong H. Le. 2009. Build-
ing a large syntactically-annotated corpus of Viet-
namese. Proceedings of Third Linguistic Annota-
tion Workshop, pages: 182?185.
Thai P. Nguyen, Luong X. Vu, Huyen T.M. Nguyen,
Thu M. Dao, Ngoc T.M. Dao and Ngan K. Le.
2010c. VTB bracketing guidelines.
Vietnamese grammar. 1983. Social Sciences Publish-
ers.
27
