Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 931?938, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Automatically Evaluating Answers to Definition Questions
Jimmy Lin1,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
Following recent developments in the au-
tomatic evaluation of machine translation
and document summarization, we present
a similar approach, implemented in a mea-
sure called POURPRE, for automatically
evaluating answers to definition questions.
Until now, the only way to assess the cor-
rectness of answers to such questions in-
volves manual determination of whether
an information nugget appears in a sys-
tem?s response. The lack of automatic
methods for scoring system output is an
impediment to progress in the field, which
we address with this work. Experiments
with the TREC 2003 and TREC 2004 QA
tracks indicate that rankings produced by
our metric correlate highly with official
rankings, and that POURPRE outperforms
direct application of existing metrics.
1 Introduction
Recent interest in question answering has shifted
away from factoid questions such as ?What city is
the home to the Rock and Roll Hall of Fame??,
which can typically be answered by a short noun
phrase, to more complex and difficult questions.
One interesting class of information needs con-
cerns so-called definition questions such as ?Who is
Vlad the Impaler??, whose answers would include
?nuggets? of information about the 16th century
warrior prince?s life, accomplishments, and legacy.
Actually a misnomer, definition questions can be
better paraphrased as ?Tell me interesting things
about X.?, where X can be a person, an organiza-
tion, a common noun, etc. Taken another way, defi-
nition questions might be viewed as simultaneously
asking a whole series of factoid questions about the
same entity (e.g., ?When was he born??, ?What was
his occupation??, ?Where did he live??, etc.), except
that these questions are not known in advance; see
Prager et al (2004) for an implementation based on
this view of definition questions.
Much progress in natural language processing and
information retrieval has been driven by the creation
of reusable test collections. A test collection con-
sists of a corpus, a series of well-defined tasks, and
a set of judgments indicating the ?correct answers?.
To complete the picture, there must exist meaning-
ful metrics to evaluate progress, and ideally, a ma-
chine should be able to compute these values auto-
matically. Although ?answers? to definition ques-
tions are known, there is no way to automatically
and objectively determine if they are present in a
given system?s response (we will discuss why in
Section 2). The experimental cycle is thus tortuously
long; to accurately assess the performance of new
techniques, one must essentially wait for expensive,
large-scale evaluations that employ human assessors
to judge the runs (e.g., the TREC QA track). This
situation mirrors the state of machine translation and
document summarization research a few years ago.
Since then, however, automatic scoring metrics such
as BLEU and ROUGE have been introduced as stop-
gap measures to facilitate experimentation.
Following these recent developments in evalua-
931
1 vital 32 kilograms plutonium powered
2 vital seven year journey
3 vital Titan 4-B Rocket
4 vital send Huygens to probe atmosphere of Titan, Saturn?s largest moon
5 okay parachute instruments to planet?s surface
6 okay oceans of ethane or other hydrocarbons, frozen methane or water
7 vital carries 12 packages scientific instruments and a probe
8 okay NASA primary responsible for Cassini orbiter
9 vital explore remote planet and its rings and moons, Saturn
10 okay European Space Agency ESA responsible for Huygens probe
11 okay controversy, protest, launch failure, re-entry, lethal risk, humans, plutonium
12 okay Radioisotope Thermoelectric Generators, RTG
13 vital Cassini, NASA?S Biggest and most complex interplanetary probe
14 okay find information on solar system formation
15 okay Cassini Joint Project between NASA, ESA, and ASI (Italian Space Agency)
16 vital four year study mission
Table 1: The ?answer key? to the question ?What is the Cassini space probe??
tion research, we propose POURPRE, a technique for
automatically evaluating answers to definition ques-
tions. Like the abovementioned metrics, POURPRE
is based on n-gram co-occurrences, but has been
adapted for the unique characteristics of the question
answering task. This paper will show that POUR-
PRE can accurately assess the quality of answers
to definition questions without human intervention,
allowing experiments to be performed with rapid
turnaround. We hope that this will enable faster ex-
ploration of the solution space and lead to acceler-
ated advances in the state of the art.
This paper is organized as follows: In Section 2,
we briefly describe how definition questions are cur-
rently evaluated, drawing attention to many of the
intricacies involved. We discuss previous work in
Section 3, relating POURPRE to evaluation metrics
for other language applications. Section 4 discusses
metrics for evaluating the quality of an automatic
scoring algorithm. The POURPRE measure itself is
outlined in Section 5; POURPRE scores are corre-
lated with official human-generated scores in Sec-
tion 6, and also compared to existing metrics. In
Section 7, we explore the effect that judgment vari-
ability has on the stability of definition question
evaluation, and its implications for automatic scor-
ing algorithms.
2 Evaluating Definition Questions
To date, NIST has conducted two formal evaluations
of definition questions, at TREC 2003 and TREC
2004.1 In this section, we describe the setup of the
task and the evaluation methodology.
Answers to definition questions are comprised of
an unordered set of [document-id, answer string]
pairs, where the strings are presumed to provide
some relevant information about the entity being
?defined?, usually called the target. Although no
explicit limit is placed on the length of the answer
string, the final scoring metric penalizes verbosity
(discussed below).
To evaluate system responses, NIST pools answer
strings from all systems, removes their association
with the runs that produced them, and presents them
to a human assessor. Using these responses and re-
search performed during the original development of
the question, the assessor creates an ?answer key??
a list of ?information nuggets? about the target. An
information nugget is defined as a fact for which the
assessor could make a binary decision as to whether
a response contained that nugget (Voorhees, 2003).
The assessor also manually classifies each nugget as
1TREC 2004 questions were arranged around ?topics?; def-
inition questions were implicit in the ?other? questions.
932
[XIE19971012.0112] The Cassini space probe, due to be launched from Cape Canaveral in Florida of
the United States tomorrow, has a 32 kilogram plutonium fuel payload to power its seven year journey
to Venus and Saturn.
Nuggets assigned: 1, 2
[NYT19990816.0266] Early in the Saturn visit, Cassini is to send a probe named Huygens into the
smog-shrouded atmosphere of Titan, the planet?s largest moon, and parachute instruments to its hidden
surface to see if it holds oceans of ethane or other hydrocarbons over frozen layers of methane or water.
Nuggets assigned: 4, 5, 6
Figure 1: Examples of judging actual system responses.
either vital or okay. Vital nuggets represent con-
cepts that must be present in a ?good? definition;
on the other hand, okay nuggets contribute worth-
while information about the target but are not essen-
tial; cf. (Hildebrandt et al, 2004). As an example,
nuggets for the question ?What is the Cassini space
probe?? are shown in Table 1.
Once this answer key of vital/okay nuggets is cre-
ated, the assessor then manually scores each run. For
each system response, he or she decides whether or
not each nugget is present. Assessors do not sim-
ply perform string matches in this decision process;
rather, this matching occurs at the conceptual level,
abstracting away from issues such as vocabulary
differences, syntactic divergences, paraphrases, etc.
Two examples of this matching process are shown
in Figure 1: nuggets 1 and 2 were found in the top
passage, while nuggets 4, 5, and 6 were found in the
bottom passage. It is exactly this process of concep-
tually matching nuggets from the answer key with
system responses that we attempt to capture with an
automatic scoring algorithm.
The final F-score for an answer is calculated in
the manner described in Figure 2, and the final score
of a run is simply the average across the scores of all
questions. The metric is a harmonic mean between
nugget precision and nugget recall, where recall is
heavily favored (controlled by the ? parameter, set
to five in 2003 and three in 2004). Nugget recall is
calculated solely on vital nuggets, while nugget pre-
cision is approximated by a length allowance given
based on the number of both vital and okay nuggets
returned. Early on in a pilot study, researchers dis-
covered that it was impossible for assessors to con-
sistently enumerate the total set of nuggets contained
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F (?) = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004.
Figure 2: Official definition of F-measure.
in a system response, given that they were usually
extracted text fragments from documents (Voorhees,
2003). Thus, a penalty for verbosity serves as a sur-
rogate for precision.
3 Previous Work
The idea of employing n-gram co-occurrence statis-
tics to score the output of a computer system against
one or more desired reference outputs was first suc-
cessfully implemented in the BLEU metric for ma-
chine translation (Papineni et al, 2002). Since then,
the basic method for scoring translation quality has
been improved upon by others, e.g., (Babych and
Hartley, 2004; Lin and Och, 2004). The basic idea
has been extended to evaluating document summa-
rization with ROUGE (Lin and Hovy, 2003).
933
Recently, Soricut and Brill (2004) employed n-
gram co-occurrences to evaluate question answer-
ing in a FAQ domain; unfortunately, the task differs
from definition question answering, making their re-
sults not directly applicable. Xu et al (2004) applied
ROUGE to automatically evaluate answers to defi-
nition questions, viewing the task as a variation of
document summarization. Because TREC answer
nuggets were terse phrases, the authors found it nec-
essary to rephrase them?two humans were asked
to manually create ?reference answers? based on the
assessors? nuggets and IR results, which was a labor-
intensive process. Furthermore, Xu et al did not
perform a large-scale assessment of the reliability of
ROUGE for evaluating definition answers.
4 Criteria for Success
Before proceeding to our description of POURPRE, it
is important to first define the basis for assessing the
quality of an automatic evaluation algorithm. Cor-
relation between official scores and automatically-
generated scores, as measured by the coefficient of
determination R2, seems like an obvious metric for
quantifying the performance of a scoring algorithm.
Indeed, this measure has been employed in the eval-
uation of BLEU, ROUGE, and other related metrics.
However, we believe that there are better mea-
sures of performance. In comparative evaluations,
we ultimately want to determine if one technique
is ?better? than another. Thus, the system rank-
ings produced by a particular scoring method are
often more important than the actual scores them-
selves. Following the information retrieval litera-
ture, we employ Kendall?s ? to capture this insight.
Kendall?s ? computes the ?distance? between two
rankings as the minimum number of pairwise adja-
cent swaps necessary to convert one ranking into the
other. This value is normalized by the number of
items being ranked such that two identical rankings
produce a correlation of 1.0; the correlation between
a ranking and its perfect inverse is ?1.0; and the ex-
pected correlation of two rankings chosen at random
is 0.0. Typically, a value of greater than 0.8 is con-
sidered ?good?, although 0.9 represents a threshold
researchers generally aim for. In this study, we pri-
marily focus on Kendall?s ? , but also report R2 val-
ues where appropriate.
5 POURPRE
Previously, it has been assumed that matching
nuggets from the assessors? answer key with sys-
tems? responses must be performed manually be-
cause it involves semantics (Voorhees, 2003). We
would like to challenge this assumption and hypoth-
esize that term co-occurrence statistics can serve as
a surrogate for this semantic matching process. Ex-
perience with the ROUGE metric has demonstrated
the effectiveness of matching unigrams, an idea we
employ in our POURPRE metric. We hypothesize
that matching bigrams, trigrams, or any other longer
n-grams will not be beneficial, because they primar-
ily account for the fluency of a response, more rele-
vant in a machine translation task. Since answers to
definition questions are usually document extracts,
fluency is less important a concern.
The idea behind POURPRE is relatively straight-
forward: match nuggets by summing the unigram
co-occurrences between terms from each nugget and
terms from the system response. We decided to start
with the simplest possible approach: count the word
overlap and divide by the total number of terms in
the answer nugget. The only additional wrinkle is to
ensure that all words appear within the same answer
string. Since nuggets represent coherent concepts,
they are unlikely to be spread across different an-
swer strings (which are usually different extracts of
source documents). As a simple example, let?s say
we?re trying to determine if the nugget ?A B C D? is
contained in the following system response:
1. A
2. B C D
3. D
4. A D
The match score assigned to this nugget would be
3/4, from answer string 2; no other answer string
would get credit for this nugget. This provision re-
duces the impact of coincidental term matches.
Once we determine the match score for every
nugget, the final F-score is calculated in the usual
way, except that the automatically-derived match
scores are substituted where appropriate. For exam-
ple, nugget recall now becomes the sum of the match
scores for all vital nuggets divided by the total num-
ber of vital nuggets. In the official F-score calcula-
934
POURPRE ROUGE
Run micro, cnt macro, cnt micro, idf macro, idf +stop ?stop
TREC 2004 (? = 3) 0.785 0.833 0.806 0.812 0.780 0.786
TREC 2003 (? = 3) 0.846 0.886 0.848 0.876 0.780 0.816
TREC 2003 (? = 5) 0.890 0.878 0.859 0.875 0.807 0.843
Table 2: Correlation (Kendall?s ? ) between rankings generated by POURPRE/ROUGE and official scores.
POURPRE ROUGE
Run micro, cnt macro, cnt micro, idf macro, idf +stop ?stop
TREC 2004 (? = 3) 0.837 0.929 0.904 0.914 0.854 0.871
TREC 2003 (? = 3) 0.919 0.963 0.941 0.957 0.876 0.887
TREC 2003 (? = 5) 0.954 0.965 0.957 0.964 0.919 0.929
Table 3: Correlation (R2) between values generated by POURPRE/ROUGE and official scores.
tion, the length allowance?for the purposes of com-
puting nugget precision?was 100 non-whitespace
characters for every okay and vital nugget returned.
Since nugget match scores are now fractional, this
required some adjustment. We settled on an al-
lowance of 100 non-whitespace characters for every
nugget match that had non-zero score.
A major drawback of this basic unigram over-
lap approach is that all terms are considered equally
important?surely, matching ?year? in a system?s re-
sponse should count for less than matching ?Huy-
gens?, in the example about the Cassini space
probe. We decided to capture this intuition using in-
verse document frequency, a commonly-used mea-
sure in information retrieval; idf(ti) is defined as
log(N/ci), where N is the number of documents in
the collection, and ci is the number of documents
that contain the term ti. With scoring based on idf,
term counts are simply replaced with idf sums in
computing the match score, i.e., the match score of
a particular nugget is the sum of the idfs of match-
ing terms in the system response divided by the sum
of all term idfs from the answer nugget. Finally,
we examined the effects of stemming, i.e., matching
stemmed terms derived from the Porter stemmer.
In the next section, results of experiments with
submissions to TREC 2003 and TREC 2004 are re-
ported. We attempted two different methods for ag-
gregating results: microaveraging and macroaverag-
ing. For microaveraging, scores were calculated by
computing the nugget match scores over all nuggets
for all questions. For macroaveraging, scores for
each question were first computed, and then aver-
aged across all questions in the testset. With mi-
croaveraging, each nugget is given equal weight,
while with macroaveraging, each question is given
equal weight.
As a baseline, we revisited experiments by Xu
et al (2004) in using ROUGE to evaluate definition
questions. What if we simply concatenated all the
answer nuggets together and used the result as the
?reference summary? (instead of using humans to
create custom reference answers)?
6 Evaluation of POURPRE
We evaluated all definition question runs submitted
to the TREC 20032 and TREC 2004 question an-
swering tracks with different variants of our POUR-
PRE metric, and then compared the results with the
official F-scores generated by human assessors. The
Kendall?s ? correlations between rankings produced
by POURPRE and the official rankings are shown in
Table 2. The coefficients of determination (R2) be-
tween the two sets of scores are shown in Table 3.
We report four separate variants along two different
parameters: scoring by term counts only vs. scoring
by term idf, and microaveraging vs. macroaveraging.
Interestingly, scoring based on macroaveraged term
2In TREC 2003, the value of ? was arbitrarily set to five,
which was later determined to favor recall too heavily. As a
result, it was readjusted to three in TREC 2004. In our experi-
ments with TREC 2003, we report figures for both values.
935
Figure 3: Scatter graph of official scores plotted
against the POURPRE scores (macro, count) for
TREC 2003 (? = 5).
counts outperformed any of the idf variants.
A scatter graph plotting official F-scores against
POURPRE scores (macro, count) for TREC 2003
(? = 5) is shown in Figure 3. Corresponding graphs
for other variants appear similar, and are not shown
here. The effect of stemming on the Kendall?s ? cor-
relation between POURPRE (macro, count) and of-
ficial scores in shown in Table 4. Results from the
same stemming experiment on the other POURPRE
variants are similarly inconclusive.
For TREC 2003 (? = 5), we performed an anal-
ysis of rank swaps between official and POURPRE
scores. A rank swap is said to have occurred if the
relative ranking of two runs is different under dif-
ferent conditions?they are significant because rank
swaps might prevent researchers from confidently
drawing conclusions about the relative effectiveness
of different techniques. We observed 81 rank swaps
(out of a total of 1431 pairwise comparisons for 54
runs). A histogram of these rank swaps, binned by
the difference in official score, is shown in Figure 4.
As can be seen, 48 rank swaps (59.3%) occurred
when the difference in official score is less than
0.02; there were no rank swaps observed for runs
in which the official scores differed by more than
0.061. Since measurement error is an inescapable
fact of evaluation, we need not be concerned with
rank swaps that can be attributed to this factor. For
TREC 2003, Voorhees (2003) calculated this value
to be approximately 0.1; that is, in order to conclude
with 95% confidence that one run is better than an-
Run unstemmed stemmed
TREC 2004 (? = 3) 0.833 0.825
TREC 2003 (? = 3) 0.886 0.897
TREC 2003 (? = 5) 0.878 0.895
Table 4: The effect of stemming on Kendall?s ? ; all
runs with (macro, count) variant of POURPRE.
Figure 4: Histogram of rank swaps for TREC 2003
(? = 5), binned by difference in official score.
other, an absolute F-score difference greater than 0.1
must be observed. As can be seen, all the rank swaps
observed can be attributed to error inherent in the
evaluation process.
From these results, we can see that evaluation
of definition questions is relatively coarse-grained.
However, TREC 2003 was the first formal evalua-
tion of definition questions; as methodologies are re-
fined, the margin of error should go down. Although
a similar error analysis for TREC 2004 has not been
performed, we expect a similar result.
Given the simplicity of our POURPRE metric,
the correlation between our automatically-derived
scores and the official scores is remarkable. Starting
from a set of questions and a list of relevant nuggets,
POURPRE can accurately assess the performance of
a definition question answering system without any
human intervention.
6.1 Comparison Against ROUGE
We choose ROUGE over BLEU as a baseline for
comparison because, conceptually, the task of an-
swering definition questions is closer to summariza-
tion than it is to machine translation, in that both are
recall-oriented. Since the majority of question an-
936
swering systems employ extractive techniques, flu-
ency (i.e., precision) is not usually an issue.
How does POURPRE stack up against using
ROUGE3 to directly evaluate definition questions?
The Kendall?s ? correlations between rankings pro-
duced by ROUGE (with and without stopword re-
moval) and the official rankings are shown in Ta-
ble 2; R2 values are shown in Table 3. In all cases,
ROUGE does not perform as well.
We believe that POURPRE better correlates with
official scores because it takes into account special
characteristics of the task: the distinction between
vital and okay nuggets, the length penalty, etc. Other
than a higher correlation, POURPRE offers an advan-
tage over ROUGE in that it provides a better diag-
nostic than a coarse-grained score, i.e., it can reveal
why an answer received a particular score. This al-
lows researchers to conduct failure analyses to iden-
tify opportunities for improvement.
7 The Effect of Variability in Judgments
As with many other information retrieval tasks,
legitimate differences in opinion about relevance
are an inescapable fact of evaluating definition
questions?systems are designed to satisfy real-
world information needs, and users inevitably dis-
agree on which nuggets are important or relevant.
These disagreements manifest as scoring variations
in an evaluation setting. The important issue, how-
ever, is the degree to which variations in judgments
affect conclusions that can be drawn in a compar-
ative evaluation, i.e., can we still confidently con-
clude that one system is ?better? than another? For
the ad hoc document retrieval task, research has
shown that system rankings are stable with respect to
disagreements about document relevance (Voorhees,
2000). In this section, we explore the effect of judg-
ment variability on the stability and reliability of
TREC definition question answering evaluations.
The vital/okay distinction on nuggets is one major
source of differences in opinion, as has been pointed
out previously (Hildebrandt et al, 2004). In the
Cassini space probe example, we disagree with the
assessors? assignment in many cases. More impor-
tantly, however, there does not appear to be any op-
3We used ROUGE-1.4.2 with n set to 1, i.e. unigram match-
ing, and maximum matching score rating.
Figure 5: Distribution of rank placement using ran-
dom judgments (for top two runs from TREC 2004).
erationalizable rules for classifying nuggets as either
vital or okay. Without any guiding principles, how
can we expect our systems to automatically recog-
nize this distinction?
How do differences in opinion about vital/okay
nuggets impact the stability of system rankings? To
answer this question, we measured the Kendall?s ?
correlation between the official rankings and rank-
ings produced by different variations of the answer
key. Three separate variants were considered:
? all nuggets considered vital
? vital/okay flipped (all vital nuggets become
okay, and all okay nuggets become vital)
? randomly assigned vital/okay labels
Results are shown in Table 5. Note that this exper-
iment was conducted with the manually-evaluated
system responses, not our POURPRE metric. For the
last condition, we conducted one thousand random
trials, taking into consideration the original distri-
bution of the vital and okay nuggets for each ques-
tion using a simplified version of the Metropolis-
Hastings algorithm (Chib and Greenberg, 1995); the
standard deviations are reported.
These results suggest that system rankings are
sensitive to assessors? opinion about what consti-
tutes a vital or okay nugget. In general, the Kendall?s
? values observed here are lower than values com-
puted from corresponding experiments in ad hoc
document retrieval (Voorhees, 2000). To illustrate,
the distribution of ranks for the top two runs from
937
Run everything vital vital/okay flipped random judgments
TREC 2004 (? = 3) 0.919 0.859 0.841 ? 0.0195
TREC 2003 (? = 3) 0.927 0.802 0.822 ? 0.0215
TREC 2003 (? = 5) 0.920 0.796 0.808 ? 0.0219
Table 5: Correlation (Kendall?s ? ) between scores under different variations of judgments and the official
scores. The 95% confidence interval is presented for the random judgments case.
TREC 2004 (RUN-12 and RUN-8) over the one
thousand random trials is shown in Figure 5. In 511
trials, RUN-12 was ranked as the highest-scoring
run; however, in 463 trials, RUN-8 was ranked as
the highest-scoring run. Factoring in differences of
opinion about the vital/okay distinction, one could
not conclude with certainty which was the ?best? run
in the evaluation.
It appears that differences between POURPRE and
the official scores are about the same as (or in some
cases, smaller than) differences between the official
scores and scores based on variant answer keys (with
the exception of ?everything vital?). This means that
further refinement of the metric to increase correla-
tion with human-generated scores may not be par-
ticularly meaningful; it might essentially amount to
overtraining on the whims of a particular human as-
sessor. We believe that sources of judgment variabil-
ity and techniques for managing it represent impor-
tant areas for future study.
8 Conclusion
We hope that POURPRE can accomplish for defini-
tion question answering what BLEU has done for
machine translation, and ROUGE for document sum-
marization: allow laboratory experiments to be con-
ducted with rapid turnaround. A much shorter ex-
perimental cycle will allow researchers to explore
different techniques and receive immediate feedback
on their effectiveness. Hopefully, this will translate
into rapid progress in the state of the art.4
9 Acknowledgements
This work was supported in part by ARDA?s
Advanced Question Answering for Intelligence
(AQUAINT) Program. We would like to thank
4A toolkit implementing the POURPRE metric can be down-
loaded at http://www.umiacs.umd.edu/?jimmylin/downloads/
Donna Harman and Bonnie Dorr for comments on
earlier drafts of this paper. In addition, we would
like to thank Kiri for her kind support.
References
Bogdan Babych and Anthony Hartley. 2004. Extend-
ing the BLEU MT evaluation method with frequency
weightings. In Proc. of ACL 2004.
Siddhartha Chib and Edward Greenberg. 1995. Under-
standing the Metropolis-Hastings algorithm. Ameri-
can Statistician, 49(4):329?345.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proc. of HLT/NAACL 2004.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proc. of HLT/NAACL 2003.
Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: A
method for evaluating automatic evaluation metrics for
machine translation. In Proc. of COLING 2004.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of ACL 2002.
John Prager, Jennifer Chu-Carroll, and Krzysztof Czuba.
2004. Question answering using constraint satisfac-
tion: QA?by?Dossier?with?Constraints. In Proc. of
ACL 2004.
Radu Soricut and Eric Brill. 2004. A unified framework
for automatic evaluation using n-gram co-occurrence
statistics. In Proc. of ACL 2004.
Ellen M. Voorhees. 2000. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
Information Processing and Management, 36(5):697?
716.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
question answering track. In Proc. of TREC 2003.
Jinxi Xu, Ralph Weischedel, and Ana Licuanan. 2004.
Evaluation of an extraction-based approach to answer-
ing definition questions. In Proc. of SIGIR 2004.
938
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 383?390,
New York, June 2006. c?2006 Association for Computational Linguistics
Will Pyramids Built of Nuggets Topple Over?
Jimmy Lin1,2,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
The present methodology for evaluating
complex questions at TREC analyzes an-
swers in terms of facts called ?nuggets?.
The official F-score metric represents the
harmonic mean between recall and pre-
cision at the nugget level. There is an
implicit assumption that some facts are
more important than others, which is im-
plemented in a binary split between ?vi-
tal? and ?okay? nuggets. This distinc-
tion holds important implications for the
TREC scoring model?essentially, sys-
tems only receive credit for retrieving vi-
tal nuggets?and is a source of evalua-
tion instability. The upshot is that for
many questions in the TREC testsets, the
median score across all submitted runs is
zero. In this work, we introduce a scor-
ing model based on judgments from mul-
tiple assessors that captures a more refined
notion of nugget importance. We demon-
strate on TREC 2003, 2004, and 2005 data
that our ?nugget pyramids? address many
shortcomings of the present methodology,
while introducing only minimal additional
overhead on the evaluation flow.
1 Introduction
The field of question answering has been moving
away from simple ?factoid? questions such as ?Who
invented the paper clip?? to more complex informa-
tion needs such as ?Who is Aaron Copland?? and
?How have South American drug cartels been using
banks in Liechtenstein to launder money??, which
cannot be answered by simple named-entities. Over
the past few years, NIST through the TREC QA
tracks has implemented an evaluation methodology
based on the notion of ?information nuggets? to as-
sess the quality of answers to such complex ques-
tions. This paradigm has gained widespread accep-
tance in the research community, and is currently be-
ing applied to evaluate answers to so-called ?defini-
tion?, ?relationship?, and ?opinion? questions.
Since quantitative evaluation is arguably the sin-
gle biggest driver of advances in language technolo-
gies, it is important to closely examine the charac-
teristics of a scoring model to ensure its fairness, re-
liability, and stability. In this work, we identify a
potential source of instability in the nugget evalua-
tion paradigm, develop a new scoring method, and
demonstrate that our new model addresses some of
the shortcomings of the original method. It is our
hope that this more-refined evaluation model can
better guide the development of technology for an-
swering complex questions.
This paper is organized as follows: Section 2
provides a brief overview of the nugget evaluation
methodology. Section 3 draws attention to the vi-
tal/okay nugget distinction and the problems it cre-
ates. Section 4 outlines our proposal for building
?nugget pyramids?, a more-refined model of nugget
importance that combines judgments from multiple
assessors. Section 5 describes the methodology for
evaluating this new model, and Section 6 presents
our results. A discussion of related issues appears in
Section 7, and the paper concludes with Section 8.
383
2 Evaluation of Complex Questions
To date, NIST has conducted three large-scale eval-
uations of complex questions using a nugget-based
evaluation methodology: ?definition? questions in
TREC 2003, ?other? questions in TREC 2004 and
TREC 2005, and ?relationship? questions in TREC
2005. Since relatively few teams participated in
the 2005 evaluation of ?relationship? questions, this
work focuses on the three years? worth of ?defini-
tion/other? questions. The nugget-based paradigm
has been previously detailed in a number of pa-
pers (Voorhees, 2003; Hildebrandt et al, 2004; Lin
and Demner-Fushman, 2005a); here, we present
only a short summary.
System responses to complex questions consist of
an unordered set of passages. To evaluate answers,
NIST pools answer strings from all participants, re-
moves their association with the runs that produced
them, and presents them to a human assessor. Us-
ing these responses and research performed during
the original development of the question, the asses-
sor creates an ?answer key? comprised of a list of
?nuggets??essentially, facts about the target. Ac-
cording to TREC guidelines, a nugget is defined as
a fact for which the assessor could make a binary
decision as to whether a response contained that
nugget (Voorhees, 2003). As an example, relevant
nuggets for the target ?AARP? are shown in Table 1.
In addition to creating the nuggets, the assessor also
manually classifies each as either ?vital? or ?okay?.
Vital nuggets represent concepts that must be in a
?good? definition; on the other hand, okay nuggets
contribute worthwhile information about the target
but are not essential. The distinction has important
implications, described below.
Once the answer key of vital/okay nuggets is cre-
ated, the assessor goes back and manually scores
each run. For each system response, he or she de-
cides whether or not each nugget is present. The
final F-score for an answer is computed in the man-
ner described in Figure 1, and the final score of a
system run is the mean of scores across all ques-
tions. The per-question F-score is a harmonic mean
between nugget precision and nugget recall, where
recall is heavily favored (controlled by the ? param-
eter, set to five in 2003 and three in 2004 and 2005).
Nugget recall is computed solely on vital nuggets
vital 30+ million members
okay Spends heavily on research & education
vital Largest seniors organization
vital Largest dues paying organization
vital Membership eligibility is 50+
okay Abbreviated name to attract boomers
okay Most of its work done by volunteers
okay Receives millions for product endorsements
okay Receives millions from product endorsements
Table 1: Answer nuggets for the target ?AARP?.
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, the F? = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004, 2005.
Figure 1: Official definition of F-score.
(which means no credit is given for returning okay
nuggets), while nugget precision is approximated by
a length allowance based on the number of both vi-
tal and okay nuggets returned. Early in a pilot study,
researchers discovered that it was impossible for as-
sessors to enumerate the total set of nuggets con-
tained in a system response (Voorhees, 2003), which
corresponds to the denominator in the precision cal-
culation. Thus, a penalty for verbosity serves as a
surrogate for precision.
Note that while a question?s answer key only
needs to be created once, assessors must manually
determine if each nugget is present in a system?s re-
sponse. This human involvement has been identified
as a bottleneck in the evaluation process, although
we have recently developed an automatic scoring
metric called POURPRE that correlates well with hu-
man judgments (Lin and Demner-Fushman, 2005a).
384
Testset # q?s 1 vital 2 vital
TREC 2003 50 3 10
TREC 2004 64 2 15
TREC 2005 75 5 16
Table 2: Number of questions with few vital nuggets
in the different testsets.
3 What?s Vital? What?s Okay?
Previously, we have argued that the vital/okay dis-
tinction is a source of instability in the nugget-
based evaluation methodology, especially given the
manner in which F-score is calculated (Hildebrandt
et al, 2004; Lin and Demner-Fushman, 2005a).
Since only vital nuggets figure into the calculation
of nugget recall, there is a large ?quantization ef-
fect? for system scores on topics that have few vital
nuggets. For example, on a question that has only
one vital nugget, a system cannot obtain a non-zero
score unless that vital nugget is retrieved. In reality,
whether or not a system returned a passage contain-
ing that single vital nugget is often a matter of luck,
which is compounded by assessor judgment errors.
Furthermore, there does not appear to be any reliable
indicators for predicting the importance of a nugget,
which makes the task of developing systems even
more challenging.
The polarizing effect of the vital/okay distinction
brings into question the stability of TREC evalua-
tions. Table 2 shows statistics about the number of
questions that have only one or two vital nuggets.
Compared to the size of the testset, these numbers
are relatively large. As a concrete example, ?F16? is
the target for question 71.7 from TREC 2005. The
only vital nugget is ?First F16s built in 1974?. The
practical effect of the vital/okay distinction in its
current form is the number of questions for which
the median system score across all submitted runs is
zero: 22 in TREC 2003, 41 in TREC 2004, and 44
in TREC 2005.
An evaluation in which the median score for many
questions is zero has many shortcomings. For one,
it is difficult to tell if a particular run is ?better? than
another?even though they may be very different in
other salient properties such as length, for exam-
ple. The discriminative power of the present F-score
measure is called into question: are present systems
that bad, or is the current scoring model insufficient
to discriminate between different (poorly perform-
ing) systems?
Also, as pointed out by Voorhees (2005), a score
distribution heavily skewed towards zero makes
meta-analysis of evaluation stability hard to per-
form. Since such studies depend on variability in
scores, evaluations would appear more stable than
they really are.
While there are obviously shortcomings to the
current scheme of labeling nuggets as either ?vital?
or ?okay?, the distinction does start to capture the
intuition that ?not all nuggets are created equal?.
Some nuggets are inherently more important than
others, and this should be reflected in the evaluation
methodology. The solution, we believe, is to solicit
judgments from multiple assessors and develop a
more refined sense of nugget importance. However,
given finite resources, it is important to balance the
amount of additional manual effort required with the
gains derived from those efforts. We present the idea
of building ?nugget pyramids?, which addresses the
shortcomings noted here, and then assess the impli-
cations of this new scoring model against data from
TREC 2003, 2004, and 2005.
4 Building Nugget Pyramids
As previously pointed out (Lin and Demner-
Fushman, 2005b), the question answering and sum-
marization communities are converging on the task
of addressing complex information needs from com-
plementary perspectives; see, for example, the re-
cent DUC task of query-focused multi-document
summarization (Amigo? et al, 2004; Dang, 2005).
From an evaluation point of view, this provides op-
portunities for cross-fertilization and exchange of
fresh ideas. As an example of this intellectual dis-
course, the recently-developed POURPRE metric for
automatically evaluating answers to complex ques-
tions (Lin and Demner-Fushman, 2005a) employs
n-gram overlap to compare system responses to ref-
erence output, an idea originally implemented in the
ROUGE metric for summarization evaluation (Lin
and Hovy, 2003). Drawing additional inspiration
from research on summarization evaluation, we
adapt the pyramid evaluation scheme (Nenkova and
Passonneau, 2004) to address the shortcomings of
385
the vital/okay distinction in the nugget-based evalu-
ation methodology.
The basic intuition behind the pyramid
scheme (Nenkova and Passonneau, 2004) is
simple: the importance of a fact is directly related
to the number of people that recognize it as such
(i.e., its popularity). The evaluation methodology
calls for assessors to annotate Semantic Content
Units (SCUs) found within model reference sum-
maries. The weight assigned to an SCU is equal
to the number of annotators that have marked the
particular unit. These SCUs can be arranged in a
pyramid, with the highest-scoring elements at the
top: a ?good? summary should contain SCUs from a
higher tier in the pyramid before a lower tier, since
such elements are deemed ?more vital?.
This pyramid scheme can be easily adapted for
question answering evaluation since a nugget is
roughly comparable to a Semantic Content Unit.
We propose to build nugget pyramids for answers
to complex questions by soliciting vital/okay judg-
ments from multiple assessors, i.e., take the original
reference nuggets and ask different humans to clas-
sify each as either ?vital? or ?okay?. The weight as-
signed to each nugget is simply equal to the number
of different assessors that deemed it vital. We then
normalize the nugget weights (per-question) so that
the maximum possible weight is one (by dividing
each nugget weight by the maximum weight of that
particular question). Therefore, a nugget assigned
?vital? by the most assessors (not necessarily all)
would receive a weight of one.1
The introduction of a more granular notion of
nugget importance should be reflected in the calcu-
lation of F-score. We propose that nugget recall be
modified to take into account nugget weight:
R =
?
m?Awm
?
n?V wn
Where A is the set of reference nuggets that are
matched within a system?s response and V is the set
of all reference nuggets; wm and wn are the weights
of nuggetsm and n, respectively. Instead of a binary
distinction based solely on matching vital nuggets,
all nuggets now factor into the calculation of recall,
1Since there may be multiple nuggets with the highest score,
what we?re building is actually a frustum sometimes. :)
subjected to a weight. Note that this new scoring
model captures the existing binary vital/okay dis-
tinction in a straightforward way: vital nuggets get
a score of one, and okay nuggets zero.
We propose to leave the calculation of nugget pre-
cision as is: a system would receive a length al-
lowance of 100 non-whitespace characters for ev-
ery nugget it retrieved (regardless of importance).
Longer answers would be penalized for verbosity.
Having outlined our revisions to the standard
nugget-based scoring method, we will proceed to
describe our methodology for evaluating this new
model and demonstrate how it overcomes many of
the shortcomings of the existing paradigm.
5 Evaluation Methodology
We evaluate our methodology for building ?nugget
pyramids? using runs submitted to the TREC 2003,
2004, and 2005 question answering tracks (2003
?definition? questions, 2004 and 2005 ?other? ques-
tions). There were 50 questions in the 2003 testset,
64 in 2004, and 75 in 2005. In total, there were 54
runs submitted to TREC 2003, 63 to TREC 2004,
and 72 to TREC 2005. NIST assessors have man-
ually annotated nuggets found in a given system?s
response, and this allows us to calculate the final F-
score under different scoring models.
We recruited a total of nine different assessors for
this study. Assessors consisted of graduate students
in library and information science and computer sci-
ence at the University of Maryland as well as volun-
teers from the question answering community (ob-
tained via a posting to NIST?s TREC QA mailing
list). Each assessor was given the reference nuggets
along with the original questions and asked to clas-
sify each nugget as vital or okay. They were pur-
posely asked to make these judgments without refer-
ence to documents in the corpus in order to expedite
the assessment process?our goal is to propose a re-
finement to the current nugget evaluation methodol-
ogy that addresses shortcomings while minimizing
the amount of additional effort required. Combined
with the answer key created by the original NIST
assessors, we obtained a total of ten judgments for
every single nugget in the three testsets.2
2Raw data can be downloaded at the following URL:
http://www.umiacs.umd.edu/?jimmylin
386
2003 2004 2005
Assessor Kendall?s ? zeros Kendall?s ? zeros Kendall?s ? zeros
0 1.00 22 1.00 41 1.00 44
1 0.908 20 0.933 36 0.888 43
2 0.896 21 0.916 43 0.900 41
3 0.903 21 0.917 38 0.897 39
4 0.912 20 0.914 42 0.879 56
5 0.873 23 0.926 40 0.841 53
6 0.889 29 0.908 32 0.894 39
7 0.900 22 0.930 37 0.890 54
8 0.909 18 0.932 29 0.891 35
9 0.879 26 0.908 49 0.877 58
average 0.896 22.2 0.920 38.7 0.884 46.2
Table 3: Kendall?s ? correlation between system scores generated using ?official? vital/okay judgments and
each assessor?s judgments. (Assessor 0 represents the original NIST assessors.)
We measured the correlation between system
ranks generated by different scoring models using
Kendall?s ? , a commonly-used rank correlation mea-
sure in information retrieval for quantifying the sim-
ilarity between different scoring methods. Kendall?s
? computes the ?distance? between two rankings as
the minimum number of pairwise adjacent swaps
necessary to convert one ranking into the other. This
value is normalized by the number of items being
ranked such that two identical rankings produce a
correlation of 1.0; the correlation between a rank-
ing and its perfect inverse is ?1.0; and the expected
correlation of two rankings chosen at random is
0.0. Typically, a value of greater than 0.8 is con-
sidered ?good?, although 0.9 represents a threshold
researchers generally aim for.
We hypothesized that system ranks are relatively
unstable with respect to individual assessor?s judg-
ments. That is, how well a given system scores
is to a large extent dependent on which assessor?s
judgments one uses for evaluation. This stems from
an inescapable fact of such evaluations, well known
from studies of relevance in the information retrieval
literature (Voorhees, 1998). Humans have legitimate
differences in opinion regarding a nugget?s impor-
tance, and there is no such thing as ?the correct an-
swer?. However, we hypothesized that these varia-
tions can be smoothed out by building ?nugget pyra-
mids? in the manner we described. Nugget weights
reflect the combined judgments of many individual
assessors, and scores generated with weights taken
into account should correlate better with each indi-
vidual assessor?s opinion.
6 Results
To verify our hypothesis about the instability of us-
ing any individual assessor?s judgments, we calcu-
lated the Kendall?s ? correlation between system
scores generated using the ?official? vital/okay judg-
ments (provide by NIST assessors) and each individ-
ual assessor?s judgments. This is shown in Table 3.
The original NIST judgments are listed as ?assessor
0? (and not included in the averages). For all scoring
models discussed in this paper, we set ?, the param-
eter that controls the relative importance of preci-
sion and recall, to three.3 Results show that although
official rankings generally correlate well with rank-
ings generated by our nine additional assessors, the
agreement is far from perfect. Yet, in reality, the
opinions of our nine assessors are not any less valid
than those of the NIST assessors?NIST does not
occupy a privileged position on what constitutes a
good ?definition?. We can see that variations in hu-
man judgments do not appear to be adequately cap-
tured by the current scoring model.
Table 3 also shows the number of questions for
which systems? median score was zero based on
each individual assessor?s judgments (out of 50
3Note that ? = 5 in the official TREC 2003 evaluation.
387
2003 2004 2005
0 0.934 0.943 0.901
1 0.962 0.940 0.950
2 0.938 0.948 0.952
3 0.938 0.947 0.950
4 0.936 0.922 0.914
5 0.916 0.956 0.887
6 0.916 0.950 0.958
7 0.949 0.933 0.927
8 0.964 0.972 0.953
9 0.912 0.899 0.881
average 0.936 0.941 0.927
Table 4: Kendall?s ? correlation between system
rankings generated using the ten-assessor nugget
pyramid and those generated using each individual
assessor?s judgments. (Assessor 0 represents the
original NIST assessors.)
questions for TREC 2003, 64 for TREC 2004, and
75 for TREC 2005). These numbers are worrisome:
in TREC 2004, for example, over half the questions
(on average) have a median score of zero, and over
three quarters of questions, according to assessor 9.
This is problematic for the various reasons discussed
in Section 3.
To evaluate scoring models that combine the opin-
ions of multiple assessors, we built ?nugget pyra-
mids? using all ten sets of judgments in the manner
outlined in Section 4. All runs submitted to each
of the TREC evaluations were then rescored using
the modified F-score formula, which takes into ac-
count a finer-grained notion of nugget importance.
Rankings generated by this model were then com-
pared against those generated by each individual as-
sessor?s judgments. Results are shown in Table 4.
As can be seen, the correlations observed are higher
than those in Table 3, meaning that a nugget pyramid
better captures the opinions of each individual asses-
sor. A two-tailed t-test reveals that the differences in
averages are statistically significant (p << 0.01 for
TREC 2003/2005, p < 0.05 for TREC 2004).
What is the effect of combining judgments from
different numbers of assessors? To answer this
question, we built ten different nugget pyramids
of varying ?sizes?, i.e., combining judgments from
one through ten assessors. The Kendall?s ? corre-
 0.86
 0.88
 0.9
 0.92
 0.94
 0.96
 0.98
 1
 1  2  3  4  5  6  7  8  9  10
Ke
nd
al
l's
 ta
u
Number of assessors
TREC 2003
TREC 2004
TREC 2005
Figure 2: Average agreement (Kendall?s ? ) between
individual assessors and nugget pyramids built from
different numbers of assessors.
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 1  2  3  4  5  6  7  8  9  10
Fr
ac
tio
n 
of
 q
ue
st
io
ns
 w
ho
se
 m
ed
ia
n 
sc
or
e 
is 
ze
ro
Number of assessors
TREC 2003
TREC 2004
TREC 2005
Figure 3: Fraction of questions whose median score
is zero plotted against number of assessors whose
judgments contributed to the nugget pyramid.
lations between scores generated by each of these
and scores generated by each individual assessor?s
judgments were computed. For each pyramid, we
computed the average across all rank correlations,
which captures the extent to which that particular
pyramid represents the opinions of all ten assessors.
These results are shown in Figure 2. The increase
in Kendall?s ? that comes from adding a second as-
sessor is statistically significant, as revealed by a
two-tailed t-test (p << 0.01 for TREC 2003/2005,
p < 0.05 for TREC 2004), but ANOVA reveals no
statistically significant differences beyond two as-
sessors.
From these results, we can conclude that adding
a second assessor yields a scoring model that is sig-
nificantly better at capturing the variance in human
relevance judgments. In this respect, little is gained
beyond two assessors. If this is the only advantage
388
provided by nugget pyramids, then the boost in rank
correlations may not be sufficient to justify the ex-
tra manual effort involved in building them. As we
shall see, however, nugget pyramids offer other ben-
efits as well.
Evaluation by our nugget pyramids greatly re-
duces the number of questions whose median score
is zero. As previously discussed, a strict vital/okay
split translates into a score of zero for systems that
do not return any vital nuggets. However, nugget
pyramids reflect a more refined sense of nugget im-
portance, which results in fewer zero scores. Fig-
ure 3 shows the number of questions whose median
score is zero (normalized as a fraction of the en-
tire testset) by nugget pyramids built from varying
numbers of assessors. With four or more assessors,
the number of questions whose median is zero for
the TREC 2003 testset drops to 17; for TREC 2004,
23 for seven or more assessors; for TREC 2005, 27
for nine or more assessors. In other words, F-scores
generated using our methodology are far more dis-
criminative. The remaining questions with zero me-
dians, we believe, accurately reflect the state of the
art in question answering performance.
An example of a nugget pyramid that combines
the opinions of all ten assessors is shown in Table 5
for the target ?AARP?. Judgments from the original
NIST assessors are also shown (cf. Table 1). Note
that there is a strong correlation between the original
vital/okay judgments and the refined nugget weights
based on the pyramid, indicating that (in this case,
at least) the intuition of the NIST assessor matches
that of the other assessors.
7 Discussion
In balancing the tradeoff between advantages pro-
vided by nugget pyramids and the additional man-
ual effort necessary to create them, what is the opti-
mal number of assessors to solicit judgments from?
Results shown in Figures 2 and 3 provide some an-
swers. In terms of better capturing different asses-
sors? opinions, little appears to be gained from going
beyond two assessors. However, adding more judg-
ments does decrease the number of questions whose
median score is zero, resulting in a more discrim-
inative metric. Beyond five assessors, the number
of questions with a zero median score remains rela-
1.0 vital Largest seniors organization
0.9 vital Membership eligibility is 50+
0.8 vital 30+ million members
0.7 vital Largest dues paying organization
0.2 okay Most of its work done by volunteers
0.1 okay Spends heavily on research & education
0.1 okay Receives millions for product endorsements
0.1 okay Receives millions from product endorsements
0.0 okay Abbreviated name to attract boomers
Table 5: Answer nuggets for the target ?AARP? with
weights derived from the nugget pyramid building
process.
tively stable. We believe that around five assessors
yield the smallest nugget pyramid that confers the
advantages of the methodology.
The idea of building ?nugget pyramids? is an ex-
tension of a similarly-named evaluation scheme in
document summarization, although there are impor-
tant differences. Nenkova and Passonneau (2004)
call for multiple assessors to annotate SCUs, which
is much more involved than the methodology pre-
sented here, where the nuggets are fixed and asses-
sors only provide additional judgments about their
importance. This obviously has the advantage of
streamlining the assessment process, but has the po-
tential to miss other important nuggets that were not
identified in the first place. Our experimental results,
however, suggest that this is a worthwhile tradeoff.
The explicit goal of this work was to develop scor-
ing models for nugget-based evaluation that would
address shortcomings of the present approach, while
introducing minimal overhead in terms of additional
resource requirements. To this end, we have been
successful.
Nevertheless, there are a number of issues that
are worth mentioning. To speed up the assessment
process, assessors were instructed to provide ?snap
judgments? given only the list of nuggets and the tar-
get. No additional context was provided, e.g., docu-
ments from the corpus or sample system responses.
It is also important to note that the reference nuggets
were never meant to be read by other people?NIST
makes no claim for them to be well-formed de-
scriptions of the facts themselves. These answer
389
keys were primarily note-taking devices to assist in
the assessment process. The important question,
however, is whether scoring variations caused by
poorly-phrased nuggets are smaller than the varia-
tions caused by legitimate inter-assessor disagree-
ment regarding nugget importance. Our experiments
appear to suggest that, overall, the nugget pyramid
scheme is sound and can adequately cope with these
difficulties.
8 Conclusion
The central importance that quantitative evaluation
plays in advancing the state of the art in language
technologies warrants close examination of evalua-
tion methodologies themselves to ensure that they
are measuring ?the right thing?. In this work, we
have identified a shortcoming in the present nugget-
based paradigm for assessing answers to complex
questions. The vital/okay distinction was designed
to capture the intuition that some nuggets are more
important than others, but as we have shown, this
comes at a cost in stability and discriminative power
of the metric. We proposed a revised model that in-
corporates judgments from multiple assessors in the
form of a ?nugget pyramid?, and demonstrated how
this addresses many of the previous shortcomings. It
is hoped that our work paves the way for more ac-
curate and refined evaluations of question answering
systems in the future.
9 Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE), and has
greatly benefited from discussions with Ellen
Voorhees, Hoa Dang, and participants at TREC
2005. We are grateful for the nine assessors who
provided nugget judgments. The first author would
like to thank Esther and Kiri for their loving support.
References
Enrique Amigo?, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An empirical study
of information synthesis task. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004).
Hoa Dang. 2005. Overview of DUC 2005. In Proceed-
ings of the 2005 Document Understanding Conference
(DUC 2005) at NLT/EMNLP 2005.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proceedings of the 2004 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2004).
Jimmy Lin and Dina Demner-Fushman. 2005a. Auto-
matically evaluating answers to definition questions.
In Proceedings of the 2005 Human Language Technol-
ogy Conference and Conference on Empirical Methods
in Natural Language Processing (HLT/EMNLP 2005).
Jimmy Lin and Dina Demner-Fushman. 2005b. Evalu-
ating summaries and answers: Two sides of the same
coin? In Proceedings of the ACL 2005 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2003).
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the 2004 Human Lan-
guage Technology Conference and the North American
Chapter of the Association for Computational Linguis-
tics Annual Meeting (HLT/NAACL 2004).
Ellen M. Voorhees. 1998. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
In Proceedings of the 21st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR 1998).
Ellen M. Voorhees. 2003. Overview of the TREC
2003 question answering track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
Ellen M. Voorhees. 2005. Using question series to eval-
uate question answering system effectiveness. In Pro-
ceedings of the 2005 Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP 2005).
390
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 841?848,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Answer Extraction, Semantic Clustering, and Extractive Summarization
for Clinical Question Answering
Dina Demner-Fushman1,3 and Jimmy Lin1,2,3
1Department of Computer Science
2College of Information Studies
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
demner@cs.umd.edu, jimmylin@umd.edu
Abstract
This paper presents a hybrid approach
to question answering in the clinical
domain that combines techniques from
summarization and information retrieval.
We tackle a frequently-occurring class of
questions that takes the form ?What is
the best drug treatment for X?? Starting
from an initial set of MEDLINE citations,
our system first identifies the drugs un-
der study. Abstracts are then clustered us-
ing semantic classes from the UMLS on-
tology. Finally, a short extractive sum-
mary is generated for each abstract to pop-
ulate the clusters. Two evaluations?a
manual one focused on short answers and
an automatic one focused on the support-
ing abstracts?demonstrate that our sys-
tem compares favorably to PubMed, the
search system most widely used by physi-
cians today.
1 Introduction
Complex information needs can rarely be ad-
dressed by single documents, but rather require the
integration of knowledge from multiple sources.
This suggests that modern information retrieval
systems, which excel at producing ranked lists of
documents sorted by relevance, may not be suffi-
cient to provide users with a good overview of the
?information landscape?.
Current question answering systems aspire to
address this shortcoming by gathering relevant
?facts? from multiple documents in response to
information needs. The so-called ?definition?
or ?other? questions at recent TREC evalua-
tions (Voorhees, 2005) serve as good examples:
?good answers? to these questions include inter-
esting ?nuggets? about a particular person, organi-
zation, entity, or event.
The importance of cross-document information
synthesis has not escaped the attention of other re-
searchers. The last few years have seen a conver-
gence between the question answering and sum-
marization communities (Amigo? et al, 2004), as
highlighted by the shift from generic to query-
focused summaries in the 2005 DUC evalua-
tion (Dang, 2005). Despite a focus on document
ranking, different techniques for organizing search
results have been explored by information retrieval
researchers, as exemplified by techniques based on
clustering (Hearst and Pedersen, 1996; Dumais et
al., 2001; Lawrie and Croft, 2003).
Our work, which is situated in the domain of
clinical medicine, lies at the intersection of ques-
tion answering, information retrieval, and summa-
rization. We employ answer extraction to identify
short answers, semantic clustering to group sim-
ilar results, and extractive summarization to pro-
duce supporting evidence. This paper describes
how each of these capabilities contributes to an in-
formation system tailored to the requirements of
physicians. Two separate evaluations demonstrate
the effectiveness of our approach.
2 Clinical Information Needs
Although the need to answer questions related
to patient care has been well documented (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
1999), studies have shown that existing search sys-
tems, e.g., PubMed, the U.S. National Library of
Medicine?s search engine, are often unable to sup-
ply physicians with clinically-relevant answers in
a timely manner (Gorman et al, 1994; Cham-
bliss and Conley, 1996). Clinical information
841
Disease: Chronic Prostatitis
I anti-microbial
1. [temafloxacin] Treatment of chronic bacterial prostatitis with temafloxacin. Temafloxacin 400 mg b.i.d. adminis-
tered orally for 28 days represents a safe and effective treatment for chronic bacterial prostatitis.
2. [ofloxacin] Ofloxacin in the management of complicated urinary tract infections, including prostatitis. In chronic
bacterial prostatitis, results to date suggest that ofloxacin may be more effective clinically and as effective micro-
biologically as carbenicillin.
3. ...
I Alpha-adrenergic blocking agent
1. [terazosine] Terazosin therapy for chronic prostatitis/chronic pelvic pain syndrome: a randomized, placebo con-
trolled trial. CONCLUSIONS: Terazosin proved superior to placebo for patients with chronic prostatitis/chronic
pelvic pain syndrome who had not received alpha-blockers previously.
2. ...
Table 1: System response to the question ?What is the best drug treatment for chronic prostatitis??
systems for decision support represent a poten-
tially high-impact application. From a research
perspective, the clinical domain is attractive be-
cause substantial knowledge has already been cod-
ified in the Unified Medical Language System
(UMLS) (Lindberg et al, 1993). The 2004 version
of the UMLS Metathesaurus contains information
about over 1 million biomedical concepts and 5
million concept names. This and related resources
allow us to explore knowledge-based techniques
with substantially less upfront investment.
Naturally, physicians have a wide spectrum of
information needs, ranging from questions about
the selection of treatment options to questions
about legal issues. To make the retrieval problem
more tractable, we focus on a subset of therapy
questions taking the form ?What is the best drug
treatment for X??, where X can be any number of
diseases. We have chosen to tackle this class of
questions because studies of physicians? behavior
in natural settings have revealed that such ques-
tions occur quite frequently (Ely et al, 1999). By
leveraging the natural distribution of clinical in-
formation needs, we can make the greatest impact
with the least effort.
Our research follows the principles of evidence-
based medicine (EBM) (Sackett et al, 2000),
which provides a well-defined model to guide the
process of clinical question answering. EBM is
a widely-accepted paradigm for medical practice
that involves the explicit use of current best ev-
idence, i.e., high-quality patient-centered clinical
research reported in the primary medical literature,
to make decisions about patient care. As shown
by previous work (Cogdill and Moore, 1997; De
Groote and Dorsch, 2003), citations from the
MEDLINE database (maintained by the U.S. Na-
tional Library of Medicine) serve as a good source
of clinical evidence. As a result of these findings,
our work focuses on MEDLINE abstracts as the
source for answers.
3 Question Answering Approach
Conflicting desiderata shape the characteristics of
?answers? to clinical questions. On the one hand,
conciseness is paramount. Physicians are always
under time pressure when making decisions, and
information overload is a serious concern. Fur-
thermore, we ultimately envision deploying ad-
vanced retrieval systems in portable packages such
as PDAs to serve as tools in bedside interac-
tions (Hauser et al, 2004). The small form factor
of such devices limits the amount of text that can
be displayed. However, conciseness exists in ten-
sion with completeness. For physicians, the im-
plications of making potentially life-altering deci-
sions mean that all evidence must be carefully ex-
amined in context. For example, the efficacy of a
drug is always framed in the context of a specific
sample population, over a set duration, at some
fixed dosage, etc. A physician simply cannot rec-
ommend a particular course of action without con-
sidering all these factors.
Our approach seeks to balance conciseness and
completeness by providing hierarchical and inter-
842
active ?answers? that support multiple levels of
drill-down. A partial example is shown in Fig-
ure 1. Top-level answers to ?What is the best drug
treatment for X?? consist of categories of drugs
that may be of interest to the physician. Each cat-
egory is associated with a cluster of abstracts from
MEDLINE about that particular treatment option.
Drilling down into a cluster, the physician is pre-
sented with extractive summaries of abstracts that
outline the clinical findings. To obtain more detail,
the physician can pull up the complete abstract
text, and finally the electronic version of the en-
tire article (if available). In the example shown in
Figure 1, the physician can see that two classes of
drugs (anti-microbial and alpha-adrenergic block-
ing agent) are relevant for the disease ?chronic
prostatitis?. Drilling down into the first cluster, the
physician can see summarized evidence for two
specific types of anti-microbials (temafloxacin and
ofloxacin) extracted from MEDLINE abstracts.
Three major capabilities are required to produce
the ?answers? described above. First, the system
must accurately identify the drugs under study in
an abstract. Second, the system must group ab-
stracts based on these substances in a meaningful
way. Third, the system must generate short sum-
maries of the clinical findings. We describe a clin-
ical question answering system that implements
exactly these capabilities (answer extraction, se-
mantic clustering, and extractive summarization).
4 System Implementation
Our work is primarily concerned with synthesiz-
ing coherent answers from a set of search results?
the actual source of these results is not important.
For convenience, we employ MEDLINE citations
retrieved by the PubMed search engine (which
also serves as a baseline for comparison). Given
an initial set of citations, answer generation pro-
ceeds in three phases, described below.
4.1 Answer Extraction
Given a set of abstracts, our system first identi-
fies the drugs under study; these later become the
short answers. In the parlance of evidence-based
medicine, drugs fall into the category of ?interven-
tions?, which encompasses everything from surgi-
cal procedures to diagnostic tests.
Our extractor for interventions relies on
MetaMap (Aronson, 2001), a program that au-
tomatically identifies entities corresponding to
UMLS concepts. UMLS has an extensive cov-
erage of drugs, falling under the semantic type
PHARMACOLOGICAL SUBSTANCE and a few oth-
ers. All such entities are identified as candidates
and each is scored based on a number of features:
its position in the abstract, its frequency of occur-
rence, etc. A separate evaluation on a blind test
set demonstrates that our extractor is able to accu-
rately recognize the interventions in a MEDLINE
abstract; see details in (Demner-Fushman and Lin,
2005; Demner-Fushman and Lin, 2006 in press).
4.2 Semantic Clustering
Retrieved MEDLINE citations are organized into
semantic clusters based on the main interventions
identified in the abstract text. We employed a
variant of the hierarchical agglomerative cluster-
ing algorithm (Zhao and Karypis, 2002) that uti-
lizes semantic relationships within UMLS to com-
pute similarities between interventions.
Iteratively, we group abstracts whose interven-
tions fall under a common ancestor, i.e., a hyper-
nym. The more generic ancestor concept (i.e., the
class of drugs) is then used as the cluster label.
The process repeats until no new clusters can be
formed. In order to preserve granularity at the
level of practical clinical interest, the tops of the
UMLS hierarchy were truncated; for example, the
MeSH category ?Chemical and Drugs? is too gen-
eral to be useful. This process was manually per-
formed during system development. We decided
to allow an abstract to appear in multiple clusters
if more than one intervention was identified, e.g.,
if the abstract compared the efficacy of two treat-
ments. Once the clusters have been formed, all
citations are then sorted in the order of the origi-
nal PubMed results, with the most abstract UMLS
concept as the cluster label. Clusters themselves
are sorted in decreasing size under the assumption
that more clinical research is devoted to more per-
tinent types of drugs.
Returning to the example in Figure 1, the ab-
stracts about temafloxacin and ofloxacin were
clustered together because both drugs are hy-
ponyms of anti-microbials within the UMLS on-
tology. As can be seen, this semantic resource pro-
vides a powerful tool for organizing search results.
4.3 Extractive Summarization
For each MEDLINE citation, our system gener-
ates a short extractive summary consisting of three
elements: the main intervention (which is usu-
843
ally more specific than the cluster label); the ti-
tle of the abstract; and the top-scoring outcome
sentence. The ?outcome?, another term from
evidence-based medicine, asserts the clinical find-
ings of a study, and is typically found towards
the end of a MEDLINE abstract. In our case,
outcome sentences state the efficacy of a drug in
treating a particular disease. Previously, we have
built an outcome extractor capable of identifying
such sentences in MEDLINE abstracts using su-
pervised machine learning techniques (Demner-
Fushman and Lin, 2005; Demner-Fushman and
Lin, 2006 in press). Evaluation on a blind held-
out test set shows high classification accuracy.
5 Evaluation Methodology
Given that our work draws from QA, IR, and sum-
marization, a proper evaluation that captures the
salient characteristics of our system proved to be
quite challenging. Overall, evaluation can be de-
composed into two separate components: locating
a suitable resource to serve as ground truth and
leveraging it to assess system responses.
It is not difficult to find disease-specific pharma-
cology resources. We employed Clinical Evidence
(CE), a periodic report created by the British Med-
ical Journal (BMJ) Publishing Group that summa-
rizes the best known drugs for a few dozen dis-
eases. Note that the existence of such secondary
sources does not obviate the need for automated
systems because they are perpetually falling out of
date due to rapid advances in medicine. Further-
more, such reports are currently created by highly-
experienced physicians, which is an expensive and
time-consuming process.
For each disease, CE classifies drugs into one of
six categories: beneficial, likely beneficial, trade-
offs (i.e., may have adverse side effects), un-
known, unlikely beneficial, and harmful. Included
with each entry is a list of references?citations
consulted by the editors in compiling the resource.
Although the completeness of the drugs enumer-
ated in CE is questionable, it nevertheless can be
viewed as ?authoritative?.
5.1 Previous Work
How can we leverage a resource such as CE to as-
sess the responses generated by our system? A
survey of evaluation methodologies reveals short-
comings in existing techniques.
Answers to factoid questions are automatically
scored using regular expression patterns (Lin,
2005). In our application, this is inadequate
for many reasons: there is rarely an exact string
match between system output and drugs men-
tioned in CE, primarily due to synonymy (for ex-
ample, alpha-adrenergic blocking agent and ?-
blocker refer to the same class of drugs) and on-
tological mismatch (for example, CE might men-
tion beta-agonists, while a retrieved abstract dis-
cusses formoterol, which is a specific represen-
tative of beta-agonists). Furthermore, while this
evaluation method can tell us if the drugs proposed
by the system are ?good?, it cannot measure how
well the answer is supported by MEDLINE cita-
tions; recall that answer justification is important
for physicians.
The nugget evaluation methodology (Voorhees,
2005) developed for scoring answers to com-
plex questions is not suitable for our task, since
there is no coherent notion of an ?answer text?
that the user reads end?to?end. Furthermore, it
is unclear what exactly a ?nugget? in this case
would be. For similar reasons, methodologies for
summarization evaluation are also of little help.
Typically, system-generated summaries are either
evaluated manually by humans (which is expen-
sive and time-consuming) or automatically using
a metric such as ROUGE, which compares sys-
tem output against a number of reference sum-
maries. The interactive nature of our answers vio-
lates the assumption that systems? responses are
static text segments. Furthermore, it is unclear
what exactly should go into a reference summary,
because physicians may want varying amounts of
detail depending on familiarity with the disease
and patient-specific factors.
Evaluation methodologies from information re-
trieval are also inappropriate. User studies have
previously been employed to examine the effect
of categorized search results. However, they often
conflate the effectiveness of the interface with that
of the underlying algorithms. For example, Du-
mais et al (2001) found significant differences in
task performance based on different ways of using
purely presentational devices such as mouseovers,
expandable lists, etc. While interface design is
clearly important, it is not the focus of our work.
Clustering techniques have also been evaluated
in the same manner as text classification algo-
rithms, in terms of precision, recall, etc. based
on some ground truth (Zhao and Karypis, 2002).
844
This, however, assumes the existence of stable,
invariant categories, which is not the case since
our output clusters are query-specific. Although
it may be possible to manually create ?reference
clusters?, we lack sufficient resources to develop
such a data set. Furthermore, it is unclear if suffi-
cient interannotator agreement can be obtained to
support meaningful evaluation.
Ultimately, we devised two separate evaluations
to assess the quality of our system output based
on the techniques discussed above. The first is
a manual evaluation focused on the cluster labels
(i.e., drug categories), based on a factoid QA eval-
uation methodology. The second is an automatic
evaluation of the retrieved abstracts using ROUGE,
drawing elements from summarization evaluation.
Details of the evaluation setup and results are pre-
ceded by a description of the test collection we
created from CE.
5.2 Test Collection
We were able to mine the June 2004 edition of
Clinical Evidence to create a test collection for
system evaluation. We randomly selected thirty
diseases, generating a development set of five
questions and a test set of twenty-five questions.
Some examples include: acute asthma, chronic
prostatitis, community acquired pneumonia, and
erectile dysfunction. CE listed an average of 11.3
interventions per disease; of those, 2.3 on average
were marked as beneficial and 1.9 as likely benefi-
cial. On average, there were 48.4 references asso-
ciated with each disease, representing the articles
consulted during the compilation of CE itself. Of
those, 34.7 citations on average appeared in MED-
LINE; we gathered all these abstracts, which serve
as the reference summaries for our ROUGE-based
automatic evaluation.
Since the focus of our work is not on retrieval al-
gorithms per se, we employed PubMed to fetch an
initial set of MEDLINE citations and performed
answer synthesis using those results. The PubMed
citations also serve as a baseline, since it repre-
sents a system commonly used by physicians.
In order to obtain the best possible set of ci-
tations, the first author (an experienced PubMed
searcher), manually formulated queries, taking
advantage of MeSH (Medical Subject Headings)
terms when available. MeSH terms are controlled
vocabulary concepts assigned manually by trained
medical indexers (based on the full text of the ar-
ticles), and encode a substantial amount of knowl-
edge about the contents of the citation. PubMed
allows searches on MeSH terms, which usually
yield accurate results. In addition, we limited re-
trieved citations to those that have theMeSH head-
ing ?drug therapy? and those that describe a clin-
ical trial (another metadata field). Finally, we re-
stricted the date range of the queries so that ab-
stracts published after our version of CE were ex-
cluded. Although the query formulation process
currently requires a human, we envision automat-
ing this step using a template-based approach in
the future.
6 System Evaluation
We adapted existing techniques to evaluate our
system in two separate ways: a factoid-style man-
ual evaluation focused on short answers and an
automatic evaluation with ROUGE using CE-cited
abstracts as the reference summaries. The setup
and results for both are detailed below.
6.1 Manual Evaluation of Short Answers
In our manual evaluation, system outputs were as-
sessed as if they were answers to factoid ques-
tions. We gathered three different sets of answers.
For the baseline, we used the main intervention
from each of the first three PubMed citations. For
our test condition, we considered the three largest
clusters, taking the main intervention from the first
abstract in each cluster. This yields three drugs
that are at the same level of ontological granularity
as those extracted from the unclustered PubMed
citations. For our third condition, we assumed the
existence of an oracle which selects the three best
clusters (as determined by the first author, a med-
ical doctor). From each of these three clusters,
we extracted the main intervention of the first ab-
stracts. This oracle condition represents an achiev-
able upper bound with a human in the loop. Physi-
cians are highly-trained professionals that already
have significant domain knowledge. Faced with a
small number of choices, it is likely that they will
be able to select the most promising cluster, even
if they did not previously know it.
This preparation yielded up to nine drug names,
three from each experimental condition. For short,
we refer to these as PubMed, Cluster, and Oracle,
respectively. After blinding the source of the drugs
and removing duplicates, each short answer was
presented to the first author for evaluation. Since
845
Clinical Evidence Physician
B LB T U UB H N Good Okay Bad
PubMed 0.200 0.213 0.160 0.053 0.000 0.013 0.360 0.600 0.227 0.173
Cluster 0.387 0.173 0.173 0.027 0.000 0.000 0.240 0.827 0.133 0.040
Oracle 0.400 0.200 0.133 0.093 0.013 0.000 0.160 0.893 0.093 0.013
Table 2: Manual evaluation of short answers: distribution of system answers with respect to CE cat-
egories (left side) and with respect to the assessor?s own expertise (right side). (Key: B=beneficial,
LB=likely beneficial, T=tradeoffs, U=unknown, UB=unlikely beneficial, H=harmful, N=not in CE)
the assessor had no idea from which condition an
answer came, this process guarded against asses-
sor bias.
Each answer was evaluated in two different
ways: first, with respect to the ground truth in CE,
and second, using the assessor?s own medical ex-
pertise. In the first set of judgments, the asses-
sor determined which of the six categories (ben-
eficial, likely beneficial, tradeoffs, unknown, un-
likely beneficial, harmful) the system answer be-
longed to, based on the CE recommendations. As
we have discussed previously, a human (with suf-
ficient domain knowledge) is required to perform
this matching due to synonymy and differences in
ontological granularity. However, note that the as-
sessor only considered the drug name when mak-
ing this categorization. In the second set of judg-
ments, the assessor separately determined if the
short answer was ?good?, ?okay? (marginal), or
?bad? based both on CE and her own experience,
taking into account the abstract title and the top-
scoring outcome sentence (and if necessary, the
entire abstract text).
Results of this manual evaluation are presented
in Table 2, which shows the distribution of judg-
ments for the three experimental conditions. For
baseline PubMed, 20% of the examined drugs fell
in the beneficial category; the values are 39% for
the Cluster condition and 40% for the Oracle con-
dition. In terms of short answers, our system
returns approximately twice as many beneficial
drugs as the baseline, a marked increase in answer
accuracy. Note that a large fraction of the drugs
evaluated were not found in CE at all, which pro-
vides an estimate of its coverage. In terms of the
assessor?s own judgments, 60% of PubMed short
answers were found to be ?good?, compared to
83% and 89% for the Cluster and Oracle condi-
tions, respectively. From a factoid QA point of
view, we can conclude that our system outper-
forms the PubMed baseline.
6.2 Automatic Evaluation of Abstracts
A major limitation of the factoid-based evaluation
methodology is that it does not measure the qual-
ity of the abstracts from which the short answers
were extracted. Since we lacked the necessary
resources to manually gather abstract-level judg-
ments for evaluation, we sought an alternative.
Fortunately, CE can be leveraged to assess the
?goodness? of abstracts automatically. We assume
that references cited in CE are examples of high
quality abstracts, since they were used in gener-
ating the drug recommendations. Following stan-
dard assumptions made in summarization evalu-
ation, we considered abstracts that are similar in
content with these ?reference abstracts? to also be
?good? (i.e., relevant). Similarity in content can
be quantified with ROUGE.
Since physicians demand high precision, we as-
sess the cumulative relevance after the first, sec-
ond, and third abstract that the clinician is likely
to have examined (where the relevance for each
individual abstract is given by its ROUGE-1 pre-
cision score). For the baseline PubMed condition,
the examined abstracts simply correspond to the
first three hits in the result set. For our test system,
we developed three different orderings. The first,
which we term cluster round-robin, selects the first
abstract from the top three clusters (by size). The
second, which we term oracle cluster order, selects
three abstracts from the best cluster, assuming the
existence of an oracle that informs the system. The
third, which we term oracle round-robin, selects
the first abstract from each of the three best clus-
ters (also determined by an oracle).
Results of this evaluation are shown in Table 3.
The columns show the cumulative relevance (i.e.,
ROUGE score) after examining the first, second,
and third abstract, under the different ordering
conditions. To determine statistical significance,
we applied the Wilcoxon signed-rank test, the
846
Rank 1 Rank 2 Rank 3
PubMed Ranked List 0.170 0.349 0.523
Cluster Round-Robin 0.181 (+6.3%)? 0.356 (+2.1%)? 0.526 (+0.5%)?
Oracle Cluster Order 0.206 (+21.5%)M 0.392 (+12.6%)M 0.597 (+14.0%)N
Oracle Round-Robin 0.206 (+21.5%)M 0.396 (+13.6%)M 0.586 (+11.9%)N
Table 3: Cumulative relevance after examining the first, second, and third abstracts, according to different
orderings. (? denotes n.s., M denotes sig. at 0.90, N denotes sig. at 0.95)
standard non-parametric test for applications of
this type. Due to the relatively small test set (only
25 questions), the increase in cumulative relevance
exhibited by the cluster round-robin condition is
not statistically significant. However, differences
for the oracle conditions were significant.
7 Discussion and Related Work
According to two separate evaluations, it appears
that our system outperforms the PubMed baseline.
However, our approach provides more advantages
over a linear result set that are not highlighted in
these evaluations. Although difficult to quantify,
categorized results provide an overview of the in-
formation landscape that is difficult to acquire by
simply browsing a ranked list?user studies of cat-
egorized search have affirmed its value (Hearst
and Pedersen, 1996; Dumais et al, 2001). One
main advantage we see in our application is bet-
ter ?redundancy management?. With a ranked list,
the physician may be forced to browse through
multiple redundant abstracts that discuss the same
or similar drugs to get a sense of the different
treatment options. With our cluster-based ap-
proach, however, potentially redundant informa-
tion is grouped together, since interventions dis-
cussed in a particular cluster are ontologically re-
lated through UMLS. The physician can examine
different clusters for a broad overview, or peruse
multiple abstracts within a cluster for a more thor-
ough review of the evidence. Our cluster-based
system is able to support both types of behaviors.
This work demonstrates the value of semantic
resources in the question answering process, since
our approach makes extensive use of the UMLS
ontology in all phases of answer synthesis. The
coverage of individual drugs, as well as the rela-
tionship between different types of drugs within
UMLS enables both answer extraction and seman-
tic clustering. As detailed in (Demner-Fushman
and Lin, 2006 in press), UMLS-based features are
also critical in the identification of clinical out-
comes, on which our extractive summaries are
based. As a point of comparison, we also im-
plemented a purely term-based approach to clus-
tering PubMed citations. The results are so inco-
herent that a formal evaluation would prove to be
meaningless. Semantic relations between drugs,
as captured in UMLS, provide an effective method
for organizing results?these relations cannot be
captured by keyword content alone. Furthermore,
term-based approaches suffer from the cluster la-
beling problem: it is difficult to automatically gen-
erate a short heading that describes cluster content.
Nevertheless, there are a number of assump-
tions behind our work that are worth pointing
out. First, we assume a high quality initial re-
sult set. Since the class of questions we examine
translates naturally into accurate PubMed queries
that can make full use of human-assigned MeSH
terms, the overall quality of the initial citations
can be assured. Related work in retrieval algo-
rithms (Demner-Fushman and Lin, 2006 in press)
shows that accurate relevance scoring of MED-
LINE citations in response to more general clin-
ical questions is possible.
Second, our system does not actually perform
semantic processing to determine the efficacy of a
drug: it only recognizes ?topics? and outcome sen-
tences that state clinical findings. Since the sys-
tem by default orders the clusters based on size, it
implicitly equates ?most popular drug? with ?best
drug?. Although this assumption is false, we have
observed in practice that more-studied drugs are
more likely to be beneficial.
In contrast with the genomics domain, which
has received much attention from both the IR and
NLP communities, retrieval systems for the clin-
ical domain represent an underexplored area of
research. Although individual components that
attempt to operationalize principles of evidence-
based medicine do exist (Mendonc?a and Cimino,
2001; Niu and Hirst, 2004), complete end?to?
end clinical question answering systems are dif-
847
ficult to find. Within the context of the PERSI-
VAL project (McKeown et al, 2003), researchers
at Columbia have developed a system that lever-
ages patient records to rerank search results. Since
the focus is on personalized summaries, this work
can be viewed as complementary to our own.
8 Conclusion
The primary contribution of this work is the de-
velopment of a clinical question answering system
that caters to the unique requirements of physi-
cians, who demand both conciseness and com-
pleteness. These competing factors can be bal-
anced in a system?s response by providing mul-
tiple levels of drill-down that allow the informa-
tion space to be viewed at different levels of gran-
ularity. We have chosen to implement these capa-
bilities through answer extraction, semantic clus-
tering, and extractive summarization. Two sepa-
rate evaluations demonstrate that our system out-
performs the PubMed baseline, illustrating the ef-
fectiveness of a hybrid approach that leverages se-
mantic resources.
9 Acknowledgments
This work was supported in part by the U.S. Na-
tional Library of Medicine. The second author
thanks Esther and Kiri for their loving support.
References
E. Amigo?, J. Gonzalo, V. Peinado, A. Pen?as, and
F. Verdejo. 2004. An empirical study of informa-
tion synthesis task. In ACL 2004.
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In AMIA 2001.
M. Chambliss and J. Conley. 1996. Answering clinical
questions. The Journal of Family Practice, 43:140?
144.
K. Cogdill and M. Moore. 1997. First-year medi-
cal students? information needs and resource selec-
tion: Responses to a clinical scenario. Bulletin of
the Medical Library Association, 85(1):51?54.
D. Covell, G. Uman, and P. Manning. 1985. Informa-
tion needs in office practice: Are they being met?
Annals of Internal Medicine, 103(4):596?599.
H. Dang. 2005. Overview of DUC 2005. In DUC
2005 Workshop at HLT/EMNLP 2005.
S. De Groote and J. Dorsch. 2003. Measuring use
patterns of online journals and databases. Journal of
the Medical Library Association, 91(2):231?240.
D. Demner-Fushman and J. Lin. 2005. Knowledge ex-
traction for clinical question answering: Preliminary
results. In AAAI 2005 Workshop on QA in Restricted
Domains.
D. Demner-Fushman and J. Lin. 2006, in press. An-
swering clinical questions with knowledge-based
and statistical techniques. Comp. Ling.
S. Dumais, E. Cutrell, and H. Chen. 2001. Optimizing
search by showing results in context. In CHI 2001.
J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,
M. Chambliss, and E. Evans. 1999. Analysis of
questions asked by family doctors regarding patient
care. BMJ, 319:358?361.
P. Gorman, J. Ash, and L. Wykoff. 1994. Can pri-
mary care physicians? questions be answered using
the medical journal literature? Bulletin of the Medi-
cal Library Association, 82(2):140?146, April.
S. Hauser, D. Demner-Fushman, G. Ford, and
G. Thoma. 2004. PubMed on Tap: Discovering
design principles for online information delivery to
handheld computers. In MEDINFO 2004.
M. Hearst and J. Pedersen. 1996. Reexaming the clus-
ter hypothesis: Scatter/gather on retrieval results. In
SIGIR 1996.
D. Lawrie and W. Croft. 2003. Generating hierarchical
summaries for Web searches. In SIGIR 2003.
J. Lin. 2005. Evaluation of resources for question an-
swering evaluation. In SIGIR 2005.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32(4):281?291.
K. McKeown, N. Elhadad, and V. Hatzivassiloglou.
2003. Leveraging a common representation for per-
sonalized search and summarization in a medical
digital library. In JCDL 2003.
E. Mendonc?a and J. Cimino. 2001. Building a knowl-
edge base to support a digital library. In MEDINFO
2001.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
ACL 2004 Workshop on QA in Restricted Domains.
David Sackett, Sharon Straus, W. Richardson, William
Rosenberg, and R. Haynes. 2000. Evidence-
Based Medicine: How to Practice and Teach EBM.
Churchill Livingstone, second edition.
E. Voorhees. 2005. Using question series to eval-
uate question answering system effectiveness. In
HLT/EMNLP 2005.
Y. Zhao and G. Karypis. 2002. Evaluation of hierar-
chical clustering algorithms for document datasets.
In CIKM 2002.
848
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 41?48, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Evaluating Summaries and Answers: Two Sides of the Same Coin?
Jimmy Lin1,3 and Dina Demner-Fushman2,3
1College of Information Studies
2Department of Computer Science
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
jimmylin@umd.edu, demner@cs.umd.edu
Abstract
This paper discusses the convergence
between question answering and multi-
document summarization, pointing out
implications and opportunities for knowl-
edge transfer in both directions. As a
case study in one direction, we discuss
the recent development of an automatic
method for evaluating definition questions
based on n-gram overlap, a commonly-
used technique in summarization evalua-
tion. In the other direction, the move to-
wards topic-oriented summaries requires
an understanding of relevance and topi-
cality, issues which have received atten-
tion in the question answering literature.
It is our opinion that question answering
and multi-document summarization repre-
sent two complementary approaches to the
same problem of satisfying complex user
information needs. Although this points
to many exciting opportunities for system-
building, here we primarily focus on im-
plications for system evaluation.
1 Introduction
Recent developments in question answering (QA)
and multi-document summarization point to many
interesting convergences that present exciting oppor-
tunities for collaboration and cross-fertilization be-
tween these largely independent communities. This
position paper attempts to draw connections be-
tween the task of answering complex natural lan-
guage questions and the task of summarizing mul-
tiple documents, the boundaries between which are
beginning to blur, as anticipated half a decade
ago (Carbonell et al, 2000).
Although the complementary co-evolution of
question answering and document summarization
presents new directions for system-building, this
paper primarily focuses on implications for evalu-
ation. Although assessment of answer and sum-
mary quality employs different methodologies, there
are many lessons that each community can learn
from the other. The summarization community has
extensive experience in intrinsic metrics based on
n-gram overlap for automatically scoring system
outputs against human-generated reference texts?
these techniques would help streamline aspects of
question answering evaluation. In the other direc-
tion, because question answering has its roots in
information retrieval, much work has focused on
extrinsic metrics based on relevance and topical-
ity, which may be valuable to summarization re-
searchers.
This paper is organized as follows: In Section 2,
we discuss the evolution of question answering re-
search and how recent trends point to the conver-
gence of question answering and multi-document
summarization. In Section 3, we present a case
study of automatically evaluating definition ques-
tions by employing metrics based on n-gram over-
lap, a general technique widely used in summariza-
tion and machine translation evaluations. Section 4
highlights some opportunities for knowledge trans-
fer in the other direction: how the notions of rele-
41
vance and topicality, well-studied in the information
retrieval literature, can guide the evaluation of topic-
oriented summaries. We conclude with thoughts
about the future in Section 5.
2 Convergence of QA and Summarization
Question answering was initially conceived as es-
sentially a fine-grained information retrieval task.
Much research has focused on so-called factoid
questions, which can typically be answered by
named entities such as people, organizations, loca-
tions, etc. As an example, a system might return
?Bee Gees? as the answer to the question ?What
band did the music for the 1970?s film ?Saturday
Night Fever???. For such well-specified information
needs, question answering systems represent an im-
provement over traditional document retrieval sys-
tems because they do not require a user to manu-
ally browse through a ranked list of ?hits?. Since
1999, the NIST-organized question answering tracks
at TREC (see, for example, Voorhees 2003a) have
served as a focal point of research in the field, pro-
viding an annual forum for evaluating systems de-
veloped by teams from all over the world. The
model has been duplicated and elaborated on by
CLEF in Europe and NTCIR in Asia, both of which
have also introduced cross-lingual elements.
Recently, research in question answering has
shifted away from factoid questions to more com-
plex information needs. This new direction can be
characterized as a move towards answers that can
only be arrived at through some form of reason-
ing and answers that require drawing information
from multiple sources. Indeed, there are many types
of questions that would require integration of both
capabilities: extracting raw information ?nuggets?
from potentially relevant documents, reasoning over
these basic facts to draw additional inferences, and
synthesizing an appropriate answer based on this
knowledge. ?What is the role of the Libyan gov-
ernment in the Lockerbie bombing?? is an example
of such a complex question.
Commonalities between the task of answering
complex questions and summarizing multiple doc-
uments are evident when one considers broader re-
search trends. Both tasks require the ability to
draw together elements from multiple sources and
cope with redundant, inconsistent, and contradic-
tory information. Both tasks require extracting finer-
grained (i.e., sub-document) segments, albeit based
on different criteria. These observations point to
the convergence of question answering and multi-
document summarization.
Complementary developments in the summariza-
tion community mirror the aforementioned shifts
in question answering research. Most notably, the
DUC 2005 task requires systems to generate an-
swers to natural language questions based on a col-
lection of known relevant documents: ?The system
task in 2005 will be to synthesize from a set of 25?
50 documents a brief, well-organized, fluent answer
to a need for information that cannot be met by just
stating a name, date, quantity, etc.? (DUC 2005
guidelines). These guidelines were modeled after
the information synthesis task suggested by Amigo?
et al (2004), which they characterize as ?the process
of (given a complex information need) extracting,
organizing, and inter-relating the pieces of informa-
tion contained in a set of relevant documents, in or-
der to obtain a comprehensive, non-redundant report
that satisfies the information need?. One of the ex-
amples they provide, ?I?m looking for information
concerning the history of text compression both be-
fore and with computers?, looks remarkably like a
user information need current question answering
systems aspire to satisfy. The idea of topic-oriented
multi-document summarization isn?t new (Goldstein
et al, 2000), but only recently have the connections
to question answering become explicit. Incidentally,
it appears that the current vision of question answer-
ing is more ambitious than the information synthesis
task because in the former, the set of relevant doc-
uments is not known in advance, but must first be
discovered within a larger corpus.
There is, however, an important difference be-
tween question answering and topic-focused multi-
document summarization: whereas summaries are
compressible in length, the same cannot be said of
answers.1 For question answering, it is difficult to
fix the length of a response a priori: there may be
cases where it is impossible to fit a coherent, com-
plete answer into an allotted space. On the other
1Wewould like to thank an anonymous reviewer for pointing
this out.
42
1 vital american composer
2 vital musical achievements ballets symphonies
3 vital born brooklyn ny 1900
4 okay son jewish immigrant
5 okay american communist
6 okay civil rights advocate
7 okay had senile dementia
8 vital established home for composers
9 okay won oscar for ?the Heiress?
10 okay homosexual
11 okay teacher tanglewood music center boston symphony
Table 1: The ?answer key? to the question ?Who is Aaron Copland??
hand, summaries are condensed representations of
content, and should theoretically be expandable and
compressible based on the level of detail desired.
What are the implications, for system evaluations,
of this convergence between question answering and
multi-document summarization? We believe that the
two fields have much to benefit from each other. In
one direction, the question answering community
currently lacks experience in automatically evalu-
ating unstructured answers, which has been the fo-
cus of much research in document summarization.
In the other direction, the question answering com-
munity, due to its roots in information retrieval, has
a good grasp on the notions of relevance and topi-
cality, which are critical to the assessment of topic-
oriented summaries. In the next section, we present
a case study in leveraging summarization evaluation
techniques to automatically evaluate definition ques-
tions. Following that, we discuss how lessons from
question answering (and more broadly, information
retrieval) can be applied to assist in evaluating sum-
marization systems.
3 Definition Questions: A Case Study
Definition questions represent complex information
needs that involve integrating facts from multiple
documents. A typical definition question is ?What
is the Cassini space probe??, to which a system
might respond with answers that include ?interplan-
etary probe to Saturn?, ?carries the Huygens probe
to study the atmosphere of Titan, Saturn?s largest
moon?, and ?a joint project between NASA, ESA,
and ASI?. The goal of the task is to return as
many interesting ?nuggets? of information as possi-
ble about the target entity being defined (the Cassini
space probe, in this case) while minimizing the
amount of irrelevant information retrieved. In the
two formal evaluations of definition questions that
have been conducted at TREC (in 2003 and 2004),
an information nugget is operationalized as a fact for
which an assessor could make a binary decision as to
whether a response contained that nugget (Voorhees,
2003b). Additionally, information nuggets are clas-
sified as either vital or okay. Vital nuggets rep-
resent facts central to the target entity, and should
be present in a ?good? definition. Okay nuggets
contribute worthwhile information about the target,
but are not essential. As an example, assessors?
nuggets for the question ?Who is Aaron Copland??
are shown in Table 1. The distinction between vi-
tal and okay nuggets is consequential for the score
calculation, which we will discuss below.
In the TREC setup, a system response to a defi-
nition question is comprised of an unordered set of
answer strings paired with the identifier of the doc-
ument from which it was extracted. Each of these
answer strings is presumed to have one or more in-
formation nuggets contained within it. Although
there is no explicit limit on the length of each answer
string and the number of answer strings a system is
allowed to return, verbosity is penalized against, as
we shall see below.
To evaluate system output, NIST gathers answer
strings from all participants, hides their association
43
[NYT19990708.0196] Once past a rather routine apprenticeship, which included three years of study
with Nadia Boulanger in Paris, Copland became one of the few American composers to make a living
from composition.
Nugget present: 1
[NYT20000107.0305] A passionate advocate of civil rights, Copland conducted a performance of the
?Lincoln Portrait? with Coretta Scott King as narrator.
Nuggets present: 6
[NYT19991117.0369] after four prior nominations, he won an Oscar in 1949 for his music for ?The
Heiress?
Nugget present: 9
Figure 1: Examples of judging actual system responses.
with the runs that produced them, and presents all
answer strings to a human assessor. Using these re-
sponses and research performed during the original
development of the question (with an off-the-shelf
document retrieval system), the assessor creates an
?answer key?; Table 1 shows the official answer key
for the question ?Who is Aaron Copland??.
After this answer key has been created, NIST as-
sessors then go back over each run and manually
judge whether or not each nugget is present in a par-
ticular system?s response. Figure 1 shows a few ex-
amples of real system output and the nuggets that
were found in them.
The final score of a particular answer is com-
puted as an F-measure, the harmonic mean between
nugget precision and recall. The ? parameter con-
trols the relative importance of precision and recall,
and is heavily biased towards the latter to model the
nature of the task. Nugget recall is calculated solely
as a function of the vital nuggets, which means that
a system receives no ?credit? (in terms of recall) for
returning okay nuggets. Nugget precision is approx-
imated by a length allowance based on the number
of vital and okay nuggets returned; a response longer
than the allowed length is subjected to a verbosity
penalty. Using answer length as a proxy to precision
appears to be a reasonable compromise because a
pilot study demonstrated that it was impossible for
humans to consistently enumerate the total number
of nuggets in a response, a necessary step in calcu-
lating nugget precision (Voorhees, 2003b).
The current TREC setup for evaluating definition
Let
r # of vital nuggets returned in a response
a # of okay nuggets returned in a response
R # of vital nuggets in the answer key
l # of non-whitespace characters in the entire
answer string
Then
recall (R) = r/R
allowance (?) = 100? (r + a)
precision (P) =
{
1 if l < ?
1? l??l otherwise
Finally, F (?) = (?
2 + 1)? P ?R
?2 ? P +R
? = 5 in TREC 2003, ? = 3 in TREC 2004.
Figure 2: Official definition of F-measure.
questions necessitates having a human ?in the loop?.
Even though answer keys are available for questions
from previous years, determining if a nugget was ac-
tually retrieved by a system currently requires hu-
man judgment. Without a fully-automated evalu-
ation method, it is difficult to consistently and re-
producibly assess the performance of a system out-
side the annual TREC cycle. Thus, researchers can-
not carry out controlled laboratory experiments to
rapidly explore the solution space. In many other
fields in computational linguistics, the ability to con-
duct evaluations with quick turnaround has lead to
rapid progress in the state of the art. Question an-
44
swering for definition questions appears to be miss-
ing this critical ingredient.
To address this evaluation gap, we have re-
cently developed POURPRE, a method for automat-
ically evaluating definition questions based on idf-
weighted unigram co-occurrences (Lin and Demner-
Fushman, 2005). This idea of employing n-gram
co-occurrence statistics to score the output of a com-
puter system against one or more desired reference
outputs has its roots in the BLEU metric for ma-
chine translation (Papineni et al, 2002) and the
ROUGE (Lin and Hovy, 2003) metric for summa-
rization. Note that metrics for automatically eval-
uating definitions should be, like metrics for eval-
uating summaries, biased towards recall. Fluency
(i.e., precision) is not usually of concern because
most systems employ extractive techniques to pro-
duce answers. Our study reports good correlation
between the automatically computed POURPRE met-
ric and official TREC system ranks. This measure
will hopefully spur progress in definition question
answering systems.
The development of automatic evaluation metrics
based on n-gram co-occurrence for question answer-
ing is an example of successful knowledge transfer
from summarization to question answering evalua-
tion. We believe that there exist many more op-
portunities for future exploration; as an example,
there are remarkable similarities between informa-
tion nuggets in definition question answering and
recently-proposed methods for assessing summaries
based on fine-grained semantic units (Teufel and van
Halteren, 2004; Nenkova and Passonneau, 2004).
Another promising direction of research in defini-
tion question answering involves applying the Pyra-
mid Method (Nenkova and Passonneau, 2004) to
better model the vital/okay nuggets distinction. As
it currently stands, the vital/okay dichotomy is trou-
blesome because there is no way to operationalize
such a classification scheme within a system; see
Hildebrandt et al (2004) for more discussion. Yet,
the effects on score are significant: a system that re-
turns, for example, all the okay nuggets but none of
the vital nuggets would receive a score of zero. In
truth, the vital/okay distinction is a poor attempt at
modeling the fact that some nuggets about a target
are more important than others?this is exactly what
the Pyramid Method is designed to capture. ?Build-
ing pyramids? for definition questions is an avenue
of research that we are currently pursuing.
In the next section, we discuss opportunities for
knowledge transfer in the other direction; i.e., how
summarization evaluation can benefit from work in
question answering evaluation.
4 Putting the Relevance in Summarization
The definition of a meaningful extrinsic evalua-
tion metric (e.g., a task-based measure) is an issue
that the summarization community has long grap-
pled with (Mani et al, 2002). This issue has been
one of the driving factors towards summaries that
are specifically responsive to complex information
needs. The evaluation of such summaries hinges on
the notions of relevance and topicality, two themes
that have received much research attention in the in-
formation retrieval community, from which question
answering evolved.
Debates about the nature of relevance are al-
most as old as the field of information retrieval it-
self (Cooper, 1971; Saracevic, 1975; Harter, 1992;
Barry and Schamber, 1998; Mizzaro, 1998; Spink
and Greisdorf, 2001). Theoretical discussions aside,
there is evidence suggesting that there exist sub-
stantial inter-assessor differences in document-level
relevance judgments (Voorhees, 2000; Voorhees,
2002); in the TREC ad hoc tracks, for example,
overlap between two humans can be less than 50%.
For factoid question answering, it has also been
shown that the notion of answer correctness is less
well-defined than one would expect (Voorhees and
Tice, 2000; Lin and Katz, 2005 in press). This
inescapable fact about the nature of information
needs represents a fundamental philosophical differ-
ence between research in information retrieval and
computational linguistics. Information retrieval re-
searchers accept the fact that the notion of ?ground
truth? is not particularly meaningful, and any pre-
scriptive attempt to dictate otherwise would result in
brittle and overtrained systems of limited value. A
retrieval system must be sensitive to the inevitable
variations in relevance exhibited by different users.
This philosophy represents a contrast from com-
putational linguistics research, where ground truth
does in fact exist. For example, there is a single cor-
rect parse of a natural language sentence (modulo
45
truly ambiguous sentences), there is the notion of a
correct word sense (modulo granularity issues), etc.
This view also pervades evaluation in machine trans-
lation and document summarization, and is implic-
itly codified in intrinsic metrics, except that there is
now the notion of multiple correct answers (i.e., the
reference texts).
Faced with the inevitability of variations in hu-
mans? notion of relevance, how can information
retrieval researchers confidently draw conclusions
about system performance and the effectiveness of
various techniques? Meta-evaluations have shown
that while some measures such as recall are rela-
tively meaningless in absolute terms (e.g., the to-
tal number of relevant documents cannot be known
without exhaustive assessment of the entire corpus,
which is impractical for current document collec-
tions), relative comparisons between systems are re-
markably stable. That is, if system A performs bet-
ter than system B (by a metric such as mean average
precision, for example), system A is highly likely
to out-perform system B with any alternative sets of
relevance judgments that represent different notions
of relevance (Voorhees, 2000; Voorhees, 2002).
Thus, it remains possible to determine the relative
effectiveness of different retrieval techniques, and
use evaluation results to guide system development.
We believe that this philosophical starting point
for conducting evaluations is an important point that
summarization researchers should take to heart, con-
sidering that notions such as relevance and topicality
are central to the evaluation of the information syn-
thesis task. What concrete implications of this view
are there? We outline some thoughts below:
First, we believe that summarization metrics
should embrace variations in human judgment as an
inescapable part of the evaluation process. Mea-
sures for automatically assessing the quality of a
system?s output such as ROUGE implicitly assume
that the ?best summary? is a statistical agglomera-
tion of the reference summaries, which is not likely
to be true. Until recently, ROUGE ?hard-coded? the
so-called ?jackknifing? procedure to estimate aver-
age human performance. Fortunately, it appears re-
searchers have realized that ?model averaging? may
not be the best way to capture the existence of many
?equally good? summaries. As an example, the
Pyramid Method (Nenkova and Passonneau, 2004),
represents a good first attempt at a realistic model of
human variations.
Second, the view that variations in judgment are
an inescapable part of extrinsic evaluations would
lead one to conclude that low inter-annotator agree-
ment isn?t necessarily bad. Computational linguis-
tics research generally attaches great value to high
kappa measures (Carletta, 1996), which indicate
high human agreement on a particular task. Low
agreement is seen as a barrier to conducting repro-
ducible research and to drawing generalizable con-
clusions. However, this is not necessarily true?low
agreement in information retrieval has not been a
handicap for advancing the state of the art. When
dealing with notions such as relevance, low kappa
values can most likely be attributed to the nature
of the task itself. Attempting to raise agreement
by, for example, developing rigid assessment guide-
lines, may do more harm than good. Prescriptive
attempts to define what a good answer or summary
should be will lead to systems that are not useful
in real-world settings. Instead, we should focus re-
search on adaptable, flexible systems.
Third, meta-evaluations are important. The infor-
mation retrieval literature has an established tradi-
tion of evaluating evaluations post hoc to insure the
reliability and fairness of the results. The aforemen-
tioned studies examining the impact of different rel-
evance judgments are examples of such work. Due
to the variability in human judgments, systems are
essentially aiming at a moving target, which neces-
sitates continual examination as to whether evalu-
ations are accurately answering the research ques-
tions and producing trustworthy results.
Fourth, a measure for assessing the quality of au-
tomatic scoring metrics should reflect the philosoph-
ical starting points that we have been discussing.
As a specific example, the correlation between an
automatically-calculated metric and actual human
preferences is better quantified by Kendall?s ? than
by the coefficient of determination R2. Since rela-
tive system comparisons are more meaningful than
absolute scores, we are generally less interested in
correlations among the scores than in the rankings of
systems produced by those scores. Kendall?s ? com-
putes the ?distance? between two rankings as the
minimum number of pairwise adjacent swaps neces-
sary to convert one ranking into the other. This value
46
is normalized by the number of items being ranked
such that two identical rankings produce a correla-
tion of 1.0; the correlation between a ranking and its
perfect inverse is ?1.0; and the expected correlation
of two rankings chosen at random is 0.0. Typically,
a value of greater than 0.8 is considered ?good?, al-
though 0.9 represents a threshold researchers gener-
ally aim for.
5 Conclusion
What?s in store for the ongoing co-evolution of sum-
marization and question answering? Currently, def-
inition questions exercise a system?s ability to inte-
grate information from multiple documents. In the
process, it needs to automatically recognize similar
information units to avoid redundant information,
much like in multi-document summarization. The
other research direction in advanced question an-
swering, integration of reasoning capabilities to gen-
erate answers that cannot be directly extracted from
text, remains more elusive for a variety of reasons.
Finer-grained linguistic analysis at a large scale and
sufficiently-rich domain ontologies to support po-
tentially long inference chains are necessary pre-
requisites?both of which represent open research
problems. Furthermore, it is unclear how exactly
one would operationalize the evaluation of such ca-
pabilities.
Nevertheless, we believe that advanced reasoning
capabilities based on detailed semantic analyses of
text will receive much attention in the future. The
recent flurry of work on semantic analysis, based
on resources such as FrameNet (Baker et al, 1998)
and PropBank (Kingsbury et al, 2002), provide the
substrate for reasoning engines. Developments in
the automatic construction, adaptation, and merg-
ing of ontologies will supply the knowledge nec-
essary to draw inferences. In order to jump-start
the knowledge acquisition process, we envision the
development of domain-specific question answering
systems, the lessons from which will be applied to
systems that operate on broader domains. In terms
of operationalizing evaluations for these advanced
capabilities, the field has already made important
first steps, e.g., the Pascal Recognising Textual En-
tailment Challenge.
What effect will these developments have on sum-
marization research? We believe that future sys-
tems will employ more detailed linguistic analysis.
As a simple example, the ability to reason about
people?s age based on their birthdates would un-
doubtedly be useful for answering particular types
of questions, but may also play a role in redundancy
detection, for example. In general, we anticipate a
move towards more abstractive techniques in multi-
document summarization. Fluent, cohesive, and top-
ical summaries cannot be generated solely using
an extractive approach?sentences are at the wrong
level of granularity, a source of problems ranging
from dangling anaphoric references to verbose sub-
ordinate clauses. Only through more detailed lin-
guistic analysis can information from multiple doc-
uments be truly synthesized. Already, there are
hybrid approaches to multi-document summariza-
tion that employ natural language generation tech-
niques (McKeown et al, 1999; Elson, 2004), and
researchers have experimented with sentential op-
erations to improve the discourse structure of sum-
maries (Otterbacher et al, 2002).
The primary purpose of this paper was to identify
similarities between multi-document summarization
and complex question answering, pointing out po-
tential synergistic opportunities in the area of system
evaluation. We hope that this is merely a small part
of a sustained dialogue between researchers from
these two largely independent communities. An-
swering complex questions and summarizing mul-
tiple documents are essentially opposite sides of the
same coin, as they represent different approaches to
the common problem of addressing complex user in-
formation needs.
6 Acknowledgements
We would like to thank Donna Harman and Ellen
Voorhees for many insights about the intricacies of
IR evaluation, Bonnie Dorr for introducing us to
DUC and bringing us into the summarization com-
munity, and Kiri for her kind support.
References
Enrique Amigo?, Julio Gonzalo, Victor Peinado, Anselmo
Pen?as, and Felisa Verdejo. 2004. An empirical study
of information synthesis task. In Proceedings of ACL
2004.
47
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of COLING/ACL 1998.
Carol Barry and Linda Schamber. 1998. Users? crite-
ria for relevance evaluation: A cross-situational com-
parison. Information Processing and Management,
34(2/3):219?236.
Jaime Carbonell, Donna Harman, Eduard Hovy, Steve
Maiorano, John Prange, and Karen Sparck-Jones.
2000. Vision statement to guide research in Question
& Answering (Q&A) and Text Summarization.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
William S. Cooper. 1971. A definition of relevance for
information retrieval. Information Storage and Re-
trieval, 7:19?37.
David K. Elson. 2004. Categorization of narrative se-
mantics for use in generative multidocument summa-
rization. In Proceedings of INLG 2004, pages 192?
197.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Jamie Callan. 2000. Creating and evaluating multi-
document sentence extract summaries. In Proceedings
of CIKM 2000.
Stephen P. Harter. 1992. Psychological relevance and
information science. Journal of the American Society
for Information Science, 43(9):602?615.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Answering definition questions with multiple knowl-
edge sources. In Proceedings of HLT/NAACL 2004.
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceeding of HLT 2002.
Jimmy Lin and Dina Demner-Fushman. 2005. Au-
tomatically evaluating answers to definition ques-
tions. Technical Report LAMP-TR-119/CS-TR-
4695/UMIACS-TR-2005-04, University of Maryland,
College Park.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of HLT/NAACL 2003.
Jimmy Lin and Boris Katz. 2005, in press. Building a
reusable test collection for question answering. Jour-
nal of the American Society for Information Science
and Technology.
Inderjeet Mani, Therese Firmin, David House, Gary
Klein, Beth Sundheim, and Lynette Hirschman. 2002.
The TIPSTER SUMMAC text summarization evalua-
tion. Natural Language Engineering, 8(1):43?68.
Kathleen R. McKeown, Judith L. Klavans, Vasileios
Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.
1999. Towards multidocument summarization by re-
formulation: Progress and prospects. In Proceedings
of AAAI-1999.
Stefano Mizzaro. 1998. How many relevances in in-
formation retrieval? Interacting With Computers,
10(3):305?322.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
Method. In Proceedings of HLT/NAACL 2004.
Jahna C. Otterbacher, Dragomir R. Radev, and Airong
Luo. 2002. Revisions that improve cohesion in multi-
document summaries: A preliminary study. In Pro-
ceedings of the ACL 2002 Workshop on Automatic
Summarization.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002.
Tefko Saracevic. 1975. Relevance: A review of and a
framework for thinking on the notion in information
science. Journal of the American Society for Informa-
tion Science, 26(6):321?343.
Amanda H. Spink and Howard Greisdorf. 2001. Regions
and levels: Mapping and measuring users relevance
judgments. Journal of the American Society for Infor-
mation Science and Technology, 52(2):161?173.
Simone Teufel and Hans van Halteren. 2004. Evaluating
information content by factoid analysis: Human anno-
tation and stability. In Proceedings of EMNLP 2004.
Ellen M. Voorhees and Dawn M. Tice. 2000. Building a
question answering test collection. In Proceedings of
SIGIR 2000.
Ellen M. Voorhees. 2000. Variations in relevance judg-
ments and the measurement of retrieval effectiveness.
Information Processing and Management, 36(5):697?
716.
Ellen M. Voorhees. 2002. The philosophy of information
retrieval evaluation. In Evaluation of Cross-Language
Information Retrieval Systems, Springer-Verlag LNCS
2406.
Ellen M. Voorhees. 2003a. Evaluating the evaluation: A
case study using the TREC 2002 question answering
track. In Proceedings of HLT/NAACL 2003.
Ellen M. Voorhees. 2003b. Overview of the TREC 2003
question answering track. In Proceedings of TREC
2003.
48
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 24?31,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Situated Question Answering in the Clinical Domain:
Selecting the Best Drug Treatment for Diseases
Dina Demner-Fushman1,3 and Jimmy Lin1,2,3
1Department of Computer Science
2College of Information Studies
3Institute for Advanced Computer Studies
University of Maryland
College Park, MD 20742, USA
demner@cs.umd.edu, jimmylin@umd.edu
Abstract
Unlike open-domain factoid questions,
clinical information needs arise within the
rich context of patient treatment. This en-
vironment establishes a number of con-
straints on the design of systems aimed
at physicians in real-world settings. In
this paper, we describe a clinical ques-
tion answering system that focuses on a
class of commonly-occurring questions:
?What is the best drug treatment for X??,
where X can be any disease. To evalu-
ate our system, we built a test collection
consisting of thirty randomly-selected dis-
eases from an existing secondary source.
Both an automatic and a manual evalua-
tion demonstrate that our system compares
favorably to PubMed, the search system
most commonly-used by physicians today.
1 Introduction
Over the past several years, question answering
(QA) has emerged as a general framework for ad-
dressing users? information needs. Instead of re-
turning ?hits?, as information retrieval systems do,
QA systems respond to natural language questions
with concise, targeted information. Recently, re-
search focus has shifted away from so-called fac-
toid questions such as ?What are pennies made
of?? and ?What country is Aswan High Dam lo-
cated in?? to more complex questions such as
?How have South American drug cartels been us-
ing banks in Liechtenstein to launder money?? and
?What was the Pentagon panel?s position with re-
spect to the dispute over the US Navy training
range on the island of Vieques???so-called ?re-
lationship? and ?opinion? questions, respectively.
These complex information needs differ from
factoid questions in many important ways. Un-
like factoids, they cannot be answered by named-
entities and other short noun phrases. They do not
occur in isolation, but are rather embedded within
a broader context, i.e., a ?scenario?. These com-
plex questions set forth parameters of the desired
knowledge, which may include additional facts
about the motivation of the information seeker,
her assumptions, her current state of knowledge,
etc. Presently, most systems that attempt to tackle
such complex questions are aimed at serving in-
telligence analysts, for activities such as counter-
terrorism and war-fighting.
Systems for addressing complex information
needs are interesting because they provide an op-
portunity to explore the role of semantic struc-
tures in question answering, e.g., (Narayanan and
Harabagiu, 2004). Opportunities include explicit
semantic representations for capturing the con-
tent of questions and documents, deep inferential
mechanisms (Moldovan et al, 2002), and attempts
to model task-specific influences in information-
seeking environments (Freund et al, 2005).
Our own interest in question answering falls
in line with these recent developments, but we
focus on a different type of user?the primary
care physician. The need to answer questions re-
lated to patient care at the point of service has
been well studied and documented (Gorman et
al., 1994; Ely et al, 1999; Ely et al, 2005).
However, research has shown that existing search
systems, e.g., PubMed, are often unable to sup-
ply clinically-relevant answers in a timely man-
ner (Gorman et al, 1994; Chambliss and Conley,
1996). Clinical question answering represents a
high-impact application that has the potential to
improve the quality of medical care.
24
From a research perspective, the clinical do-
main is attractive because substantial medical
knowledge has already been codified in the Uni-
fied Medical Language System (UMLS) (Lind-
berg et al, 1993). This large ontology en-
ables us to explore knowledge-rich techniques and
move beyond question answering methods primar-
ily driven by keyword matching. In this work, we
describe a paradigm of medical practice known as
evidence-based medicine and explain how it can
be computationally captured in a semantic domain
model. Two separate evaluations demonstrate that
semantic modeling yields gains in question an-
swering performance.
2 Considerations for Clinical QA
We begin our exploration of clinical question an-
swering by first discussing design constraints im-
posed by the domain and the information-seeking
environment. The practice of evidence-based
medicine (EBM) provides a well-defined process
model for situating our system. EBM is a widely-
accepted paradigm for medical practice that in-
volves the explicit use of current best evidence,
i.e., high-quality patient-centered clinical research
reported in the primary medical literature, to make
decisions about patient care. As shown by pre-
vious work (De Groote and Dorsch, 2003), cita-
tions from the MEDLINE database maintained by
the National Library of Medicine serve as a good
source of evidence.
Thus, we conceive of clinical question answer-
ing systems as fulfilling a decision-support role
by retrieving highly-relevant MEDLINE abstracts
in response to a clinical question. This repre-
sents a departure from previous systems, which fo-
cus on extracting short text segments from larger
sources. The implications of making potentially
life-altering decisions mean that all evidence must
be carefully examined in context. For example, the
efficacy of a drug in treating a disease is always
framed in the context of a specific study on a sam-
ple population, over a set duration, at some fixed
dosage, etc. The physician simply cannot recom-
mend a particular course of action without consid-
ering all these complex factors. Thus, an ?answer?
without adequate support is not useful. Given that
a MEDLINE abstract?on the order of 250 words,
equivalent to a long paragraph?generally encap-
sulates the context of a clinical study, it serves as a
logical answer unit and an entry point to the infor-
mation necessary to answer the physician?s ques-
tion (e.g., via drill-down to full text articles).
In order for a clinical QA system to be success-
ful, it must be suitably integrated into the daily ac-
tivities of a physician. Within a clinic or a hos-
pital setting, the traditional desktop application is
not the most ideal interface for a retrieval system.
In most cases, decisions about patient care must
be made by the bedside. Thus, a PDA is an ideal
vehicle for delivering question answering capabil-
ities (Hauser et al, 2004). However, the form fac-
tor and small screen size of such devices places
constraints on system design. In particular, since
the physician is unable to view large amounts of
text, precision is of utmost importance.
In summary, this section outlines considerations
for question answering in the clinical domain: the
necessity of contextualized answers, the rationale
for adopting MEDLINE abstract as the response
unit, and the importance of high precision.
3 EBM and Clinical QA
Evidence-based medicine not only supplies a pro-
cess model for situating question answering capa-
bilities, but also provides a framework for codify-
ing the knowledge involved in retrieving answers.
This section describes how the EBM paradigm
provides the basis of the semantic domain model
for our question answering system.
Evidence-based medicine offers three facets of
the clinical domain, that, when taken together,
describe a model for addressing complex clini-
cal information needs. The first facet, shown in
Table 1 (left column), describes the four main
tasks that physicians engage in. The second
facet pertains to the structure of a well-built clin-
ical question. Richardson et al (1995) identify
four key elements, as shown in Table 1 (middle
column). These four elements are often refer-
enced with a mnemonic PICO, which stands for
Patient/Problem, Intervention, Comparison, and
Outcome. Finally, the third facet serves as a tool
for appraising the strength of evidence, i.e., how
much confidence should a physician have in the
results? For this work, we adopted a system with
three levels of recommendations, as shown in Ta-
ble 1 (right column).
By integrating these three perspectives of
evidence-based medicine, we conceptualize clin-
ical question answering as ?semantic unifica-
tion? between information needs expressed in a
25
Clinical Tasks PICO Elements Strength of Evidence
Therapy: Selecting effective treat-
ments for patients, taking into account
other factors such as risk and cost.
Diagnosis: Selecting and interpret-
ing diagnostic tests, while considering
their precision, accuracy, acceptabil-
ity, cost, and safety.
Prognosis: Estimating the patient?s
likely course with time and anticipat-
ing likely complications.
Etiology: Identifying the causes for a
patient?s disease.
Patient/Problem: What is the pri-
mary problem or disease? What are
the characteristics of the patient (e.g.,
age, gender, co-existing conditions,
etc.)?
Intervention: What is the main inter-
vention (e.g., diagnostic test, medica-
tion, therapeutic procedure, etc.)?
Comparison: What is the main in-
tervention compared to (e.g., no inter-
vention, another drug, another thera-
peutic procedure, a placebo, etc.)?
Outcome: What is the effect of the
intervention (e.g., symptoms relieved
or eliminated, cost reduced, etc.)?
A-level evidence is based on con-
sistent, good quality patient-oriented
evidence presented in systematic re-
views, randomized controlled clini-
cal trials, cohort studies, and meta-
analyses.
B-level evidence is inconsistent, lim-
ited quality patient-oriented evidence
in the same types of studies.
C-level evidence is based on disease-
oriented evidence or studies less rigor-
ous than randomized controlled clin-
ical trials, cohort studies, systematic
reviews and meta-analyses.
Table 1: The three facets of evidence-based medicine.
PICO-based knowledge structure and correspond-
ing structures extracted fromMEDLINE abstracts.
Naturally, this matching process should be sensi-
tive to the clinical task and the strength of evidence
of the retrieved abstracts. As conceived, clini-
cal question answering is a knowledge-intensive
endeavor that requires automatic identification of
PICO elements from MEDLINE abstracts.
Ideally, a clinical question answering system
should be capable of directly performing this
semantic match on abstracts, but the size of
the MEDLINE database (over 16 million ci-
tations) makes this approach currently unfeasi-
ble. As an alternative, we rely on PubMed,1
a boolean search engine provided by the Na-
tional Library of Medicine, to retrieve an initial
set of results that we then postprocess in greater
detail?this is the standard two-stage architecture
commonly-employed by many question answer-
ing systems (Hirschman and Gaizauskas, 2001).
The complete architecture of our system is
shown in Figure 1. The query formulation mod-
ule converts the clinical question into a PubMed
search query, identifies the clinical task, and ex-
tracts the appropriate PICO elements. PubMed re-
turns an initial list of MEDLINE citations, which
is analyzed by the knowledge extractor to identify
clinically-relevant elements. These elements serve
as input to the semantic matcher, and are com-
pared to corresponding elements extracted from
the question. Citations are then scored and the top
ranking ones are returned as answers.
1http://www.ncbi.nih.gov/entrez/
Figure 1: Architecture of our clinical question an-
swering system.
Although we have outlined a general framework
for clinical question answering, the space of all
possible patient care questions is immense, and at-
tempts to develop a comprehensive system is be-
yond the scope of this paper. Instead, we focus on
a subset of therapy questions: specifically, ques-
tions of the form ?What is the best drug treatment
for X??, where X can be any disease. We have cho-
sen to tackle this class of questions because studies
of physicians? question-asking behavior in natural
settings have revealed that this question type oc-
curs frequently (Ely et al, 1999). By leveraging
the natural distribution of clinical questions, we
can make the greatest impact with the least amount
26
of development effort. For this class of questions,
we have implemented a working system with the
architecture described in Figure 1. The next three
sections detail each module.
4 Query Formulator
Since our system only handles one question type,
the query formulator is relatively simple: the task
is known in advance to be therapy and the Prob-
lem PICO element is the disease asked about in the
clinical question. In order to facilitate the semantic
matching process, we employMetaMap (Aronson,
2001) to identify the concept in the UMLS ontol-
ogy that corresponds to the disease; UMLS also
provides alternative names and other expansions.
The query formulator also generates a query
to PubMed, the National Library of Medicine?s
boolean search engine for MEDLINE. As an ex-
ample, the following query is issued to retrieve hits
for the disease ?meningitis?:
(Meningitis[mh:noexp]) AND drug therapy[sh]
AND hasabstract[text] AND Clinical Trial[pt]
AND English[Lang] AND humans[mh] AND
(1900[PDAT] : 2003/03[PDAT])
In order to get the best possible set of initial ci-
tations, we employ MeSH (Medical Subject Head-
ings) terms when available. MeSH terms are con-
trolled vocabulary concepts assigned manually by
trained medical librarians in the indexing process
(based on the full text of the article), and encode
a substantial amount of knowledge about the con-
tents of the citation. PubMed allows searches on
MeSH headings, which usually yield highly accu-
rate results. In addition, we limit retrieved cita-
tions to those that have the MeSH heading ?drug
therapy?and those that describe a clinical trial (an-
other metadata field). By default, PubMed orders
citations chronologically in reverse.
5 Knowledge Extractor
The knowledge extraction module provides the
basic frame elements used in the semantic
matching process, described in the next sec-
tion. We employ previously-implemented com-
ponents (Demner-Fushman and Lin, 2005) that
identify PICO elements within a MEDLINE cita-
tion using a combination of knowledge-based and
statistical machine-learning techniques. Of the
four PICO elements prescribed by evidence-based
medicine practitioners, only the Problem and Out-
come elements are relevant for this application
(there are no Interventions and Comparisons for
our question type). The Problem is the main dis-
ease under consideration in an abstract, and out-
comes are statements that assert clinical findings,
e.g., efficacy of a drug or a comparison between
two drugs. The ability to precisely identify these
clinically-relevant elements provides the founda-
tion for semantic question answering capabilities.
6 Semantic Matcher
Evidence-based medicine identifies three differ-
ent sets of factors that must be taken into account
when assessing citation relevance. These consid-
erations are computationally operationalized in the
semantic matcher, which takes as input elements
identified by the knowledge extractor and scores
the relevance of each PubMed citation with re-
spect to the question. After matching, the top-
scoring abstracts are presented to the physician as
answers. The individual score of a citation is com-
prised of three components:
SEBM = SPICO + SSoE + SMeSH (1)
By codifying the principles of evidence-based
medicine, our semantic matcher attempts to sat-
isfy information needs through conceptual analy-
sis, as opposed to simple keyword matching. In
the following subsections, we describe each of
these components in detail.
6.1 PICO Matching
The score of an abstract based on PICO elements,
SPICO, is broken up into two separate scores:
SPICO = Sproblem + Soutcome (2)
The first component in the above equation,
Sproblem, reflects a match between the primary prob-
lem in the query frame and the primary problem
identified in the abstract. A score of 1 is given if
the problems match exactly, based on their unique
UMLS concept id (as provided by MetaMap).
Matching based on concept ids addresses the issue
of terminological variation. Failing an exact match
of concept ids, a partial string match is given a
score of 0.5. If the primary problem in the query
has no overlap with the primary problem from the
abstract, a score of ?1 is given.
The outcome-based score Soutcome is the value as-
signed to the highest-scoring outcome sentence,
27
as determined by the knowledge extractor. Since
the desired outcome (i.e., improve the patient?s
condition) is implicit in the clinical question, our
system only considers the inherent quality of out-
come statements in the abstract. Given a match on
the primary problem, most clinical outcomes are
likely to be of interest to the physician.
For the drug treatment scenario, there is no in-
tervention or comparison, and so these elements
do not contribute to the semantic matching.
6.2 Strength of Evidence
The relevance score of a citation based on the
strength of evidence is calculated as follows:
SSoE = Sjournal + Sstudy + Sdate (3)
Citations published in core and high-impact
journals such as Journal of the American Medical
Association (JAMA) get a score of 0.6 for Sjournal,
and 0 otherwise. In terms of the study type, Sstudy,
clinical trials receive a score of 0.5; observational
studies, 0.3; all non-clinical publications, ?1.5;
and 0 otherwise. The study type is directly en-
coded as metadata in a MEDLINE citation.
Finally, recency factors into the strength of evi-
dence score according to the formula below:
Sdate = (yearpublication ? yearcurrent)/100 (4)
A mild penalty decreases the score of a citation
proportionally to the time difference between the
date of the search and the date of publication.
6.3 MeSH Matching
The final component of the EBM score reflects
task-specific considerations, and is computed from
MeSH terms associated with each citation:
SMeSH =
?
t?MeSH
?(t) (5)
The function ?(t) maps MeSH terms to positive
scores for positive indicators, negative scores for
negative indicators, or zero otherwise.
Negative indicators include MeSH headings as-
sociated with genomics, such as ?genetics? and
?cell physiology?. Positive indicators for therapy
were derived from the clinical query filters used in
PubMed searches (Haynes et al, 1994); examples
include ?drug administration routes? and any of its
children in the MeSH hierarchy. A score of ?1 is
given if theMeSH descriptor or qualifier is marked
as the main theme of the article (indicated via the
star notation by indexers), and ?0.5 otherwise.
7 Evaluation Methodology
Clinical Evidence (CE) is a periodic report cre-
ated by the British Medical Journal (BMJ) Pub-
lishing Group that summarizes the best treatments
for a few dozen diseases at the time of publica-
tion. We were able to mine the June 2004 edition
to create a test collection to evaluate our system.
Note that the existence of such secondary sources
does not obviate the need for clinical question an-
swering because they are perpetually falling out of
date due to rapid advances in medicine. Further-
more, such reports are currently created by highly-
experienced physicians, which is an expensive and
time-consuming process. From CE, we randomly
extracted thirty diseases, creating a development
set of five questions and a test set of twenty-five
questions. Some examples include: acute asthma,
chronic prostatitis, community acquired pneumo-
nia, and erectile dysfunction.
We conducted two evaluations?one auto-
matic and one manual?that compare the origi-
nal PubMed hits and the output of our semantic
matcher. The first evaluation is based on ROUGE,
a commonly-used summarization metric that com-
putes the unigram overlap between a particular
text and one or more reference texts.2 The treat-
ment overview for each disease in CE is accompa-
nied by a number of citations (used in writing the
overview itself)?the abstract texts of these cited
articles serve as our references. We adopt this ap-
proach because medical journals require abstracts
that provide factual information summarizing the
main points of the studies. We assume that the
closer an abstract is to these reference abstracts (as
measured by ROUGE-1 precision), the more rele-
vant it is. On average, each disease overview con-
tains 48.4 citations; however, we were only able
to gather abstracts of those that were contained in
MEDLINE (34.7 citations per disease, min 8, max
100). For evaluation purposes, we restricted ab-
stracts under consideration to those that were pub-
lished before our edition of CE. To quantify the
performance of our system, we computed the av-
erage ROUGE score over the top one, three, five,
and ten hits of our EBM and baseline systems.
To supplement our automatic evaluation, we
also conducted a double-blind manual evaluation
2We ran ROUGE-1.5.5 with DUC 2005 settings.
28
PubMed EBM PICO SoE MeSH
1 0.160 0.205 (+27.7%)M 0.186 (+16.1%)? 0.192 (+20.0%)? 0.166 (+3.6%)?
3 0.162 0.202 (+24.6%)N 0.192 (+18.0%)N 0.204 (+25.5%)N 0.172 (+6.1%)?
5 0.166 0.198 (+19.5%)N 0.196 (+18.0%)N 0.201 (+21.3%)N 0.168 (+1.2%)?
10 0.170 0.196 (+15.5%)N 0.191 (+12.5%)N 0.195 (+15.1%)N 0.174 (+2.8%)?
Table 2: Results of automatic evaluation: average ROUGE score using cited abstracts in CE as references.
The EBM column represents performance of our complete domain model. PICO, SoE, and MeSH rep-
resent performance of each component. (? denotes n.s., M denotes sig. at 0.95, N denotes sig. at 0.99)
PubMed results EBM-reranked results
Effect of vitamin A supplementation on childhood morbid-
ity and mortality.
Intrathecal chemotherapy in carcinomatous meningitis from
breast cancer.
Isolated leptomeningeal carcinomatosis (carcinomatous
meningitis) after taxane-induced major remission in patients
with advanced breast cancer.
A comparison of ceftriaxone and cefuroxime for the treat-
ment of bacterial meningitis in children.
Randomised comparison of chloramphenicol, ampicillin,
cefotaxime, and ceftriaxone for childhood bacterial menin-
gitis.
The beneficial effects of early dexamethasone administra-
tion in infants and children with bacterial meningitis.
Table 3: Titles of the top abstracts retrieved in response to the question ?What is the best treatment for
meningitis??, before and after applying our semantic reranking algorithm.
of the system. The top five citations from both
the original PubMed results and the output of our
semantic matcher were gathered, blinded, and ran-
domized (see Table 3 for an example of top results
obtained by PubMed and our system). The first
author of this paper, who is a medical doctor, man-
ually evaluated the abstracts. Since the sources of
the abstracts were hidden, judgments were guar-
anteed to be impartial. All abstracts were evalu-
ated on a four point scale: not relevant, marginally
relevant, relevant, and highly relevant, which cor-
responds to a score of zero to three.
8 Results
The results of our automatic evaluation are shown
in Table 2: the rows show average ROUGE scores
at one, three, five, and ten hits, respectively. In
addition to the PubMed baseline and our com-
plete EBM model, we conducted a component-
level analysis of our semantic matching algorithm.
Three separate ablation studies isolate the effects
of the PICO-based score, the strength of evi-
dence score, and the MeSH-based score (columns
?PICO?, ?SoE?, and ?MeSH?).
At all document cutoffs, the quality of the
EBM-reranked hits is higher than that of the origi-
nal PubMed hits, as measured by ROUGE. The dif-
ferences are statistically significant, according to
the Wilcoxon signed-rank test, the standard non-
parametric test employed in IR.
Based on the component analysis, we can see
that the strength of evidence score is responsi-
ble for the largest performance gain, although
the combination of all three components outper-
forms each one individually (for the most part).
All three components of our semantic model con-
tribute to the overall QA performance, which is
expected because clinical relevance is a multi-
faceted property that requires a multitude of con-
siderations. Evidence-based medicine provides a
theory of these factors, and we have shown that a
question answering algorithm which operational-
izes EBM yields good results.
The distribution of human judgments from our
manual evaluation is shown in Figure 2. For
the development set, the average human judg-
ment of the original PubMed hits is 1.52 (be-
tween ?marginally relevant? and ?relevant?); after
semantic matching, 2.32 (better than ?relevant?).
For the test set, the averages are 1.49 before rank-
ing and 2.10 after semantic matching. These re-
sults show that our system performs significantly
better than the PubMed baseline.
The performance improvement observed in our
experiments is encouraging, considering that we
were starting off with a strong state-of-the-art
29
Figure 2: Results of our manual evaluation: distribution of judgments, for development set (left) and test
set (right). (0=not relevant, 1=marginally relevant, 2=relevant, 3=highly relevant)
PubMed baseline that leverages MeSH terms. All
initial citations retrieved by PubMed were clinical
trials and ?about? the disease in question, as deter-
mined by human indexers. Our work demonstrates
that principles of evidence-based medicine can be
codified in an algorithm.
Since a number of abstracts were both auto-
matically evaluated with ROUGE and manually
assessed, it is possible to determine the degree
to which automatic metrics predict human judg-
ments. For the 125 human judgments gathered
on the test set, we computed a Pearson?s r score
of 0.544, which indicates moderate predictiveness.
Due to the structure of our PubMed query, the key-
word content of retrieved abstracts are relatively
homogeneous. Nevertheless, automatic evaluation
with ROUGE appears to be useful.
9 Discussion and Related Work
Recently, researchers have become interested
in restricted-domain question answering because
it provides an opportunity to explore the use
of knowledge-rich techniques without having
to tackle the commonsense reasoning problem.
Knowledge-based techniques dependent on rich
semantic representations contrast with TREC-
style factoid question answering, which is primar-
ily driven by keyword matching and named-entity
detection.
Our work represents a successful case study of
how semantic models can be employed to capture
domain knowledge (the practice of medicine, in
our case). The conception of question answer-
ing as the matching of knowledge frames provides
us with an opportunity to experiment with seman-
tic representations that capture the content of both
documents and information needs. In our case,
PICO-based scores were found to have a positive
impact on performance. The strength of evidence
and the MeSH-based scores represent attempts to
model user requirements by leveraging meta-level
information not directly present in either questions
or candidate answers. Both contribute positively
to performance. Overall, the construction of our
semantic model is enabled by the UMLS ontol-
ogy, which provides an enumeration of relevant
concepts (e.g., the names of diseases, drugs, etc.)
and semantic relations between those concepts.
Question answering in the clinical domain is an
emerging area of research that has only recently
begun to receive serious attention. As a result,
there exist relatively few points of comparison to
our own work, as the research space is sparsely
populated.
The idea that information systems should
be sensitive to the practice of evidence-based
medicine is not new. Many researchers have stud-
ied MeSH terms associated with basic clinical
tasks (Mendonc?a and Cimino, 2001; Haynes et al,
1994). Although originally developed as a tool to
assist in query formulation, Booth (2000) pointed
out that PICO frames can be employed to struc-
ture IR results for improving precision; PICO-
based querying is merely an instance of faceted
querying, which has been widely used by librari-
ans since the invention of automated retrieval sys-
tems. The feasibility of automatically identifying
outcome statements in secondary sources has been
demonstrated by Niu and Hirst (2004), but our
work differs in its focus on the primary medical lit-
erature. Approaching clinical needs from a differ-
ent perspective, the PERSIVAL system leverages
patient records to rerank search results (McKeown
et al, 2003). Since the primary focus is on person-
30
alization, this work can be viewed as complemen-
tary to our own.
The dearth of related work and the lack of a pre-
existing clinical test collection to a large extent ex-
plains the ad hoc nature of some aspects of our
semantic matching algorithm. All weights were
heuristically chosen to reflect our understanding
of the domain, and were not optimized in a prin-
cipled manner. Nevertheless, performance gains
observed in the development set carried over to
the blind held-out test collection, providing con-
fidence in the generality of our methods. Devel-
oping a more formal scoring model for evidence-
based medicine will be the subject of future work.
10 Conclusion
We see this work as having two separate contribu-
tions. From the viewpoint of computational lin-
guistics, we have demonstrated the effectiveness
of a knowledge-rich approach to QA based on
matching questions with answers at the semantic
level. From the viewpoint of medical informat-
ics, we have shown how principles of evidence-
based medicine can be operationalized in a sys-
tem to support physicians. We hope that this work
paves the way for future high-impact applications.
11 Acknowledgments
This work was supported in part by the National
Library of Medicine. The second author wishes to
thank Esther and Kiri for their loving support.
References
A. Aronson. 2001. Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the AMIA 2001.
A. Booth. 2000. Formulating the question. In
A. Booth and G. Walton, editors, Managing Knowl-
edge in Health Services. Facet Publishing.
M. Chambliss and J. Conley. 1996. Answering clinical
questions. The Journal of Family Practice, 43:140?
144.
S. De Groote and J. Dorsch. 2003. Measuring use
patterns of online journals and databases. Journal
of the Medical Library Association, 91(2):231?240,
April.
D. Demner-Fushman and J. Lin. 2005. Knowledge ex-
traction for clinical question answering: Preliminary
results. In Proceedings of the AAAI-05 Workshop on
Question Answering in Restricted Domains.
J. Ely, J. Osheroff, M. Ebell, G. Bergus, B. Levy,
M. Chambliss, and E. Evans. 1999. Analysis of
questions asked by family doctors regarding patient
care. BMJ, 319:358?361.
J. Ely, J. Osheroff, M. Chambliss, M. Ebell, and
M. Rosenbaum. 2005. Answering physicians? clin-
ical questions: Obstacles and potential solutions.
Journal of the American Medical Informatics Asso-
ciation, 12(2):217?224, March-April.
L. Freund, E. Toms, and C. Clarke. 2005. Modeling
task-genre relationships for IR in the Workplace. In
Proceedings of SIGIR 2005.
P. Gorman, J. Ash, and L. Wykoff. 1994. Can pri-
mary care physicians? questions be answered using
the medical journal literature? Bulletin of the Medi-
cal Library Association, 82(2):140?146, April.
S. Hauser, D. Demner-Fushman, G. Ford, and
G. Thoma. 2004. PubMed on Tap: Discovering
design principles for online information delivery to
handheld computers. In Proceedings of MEDINFO
2004.
R. Haynes, N. Wilczynski, K. McKibbon, C. Walker,
and J. Sinclair. 1994. Developing optimal search
strategies for detecting clinically sound studies in
MEDLINE. Journal of the American Medical In-
formatics Association, 1(6):447?458.
L. Hirschman and R. Gaizauskas. 2001. Natural
language question answering: The view from here.
Natural Language Engineering, 7(4):275?300.
D. Lindberg, B. Humphreys, and A. McCray. 1993.
The Unified Medical Language System. Methods of
Information in Medicine, 32(4):281?291, August.
K. McKeown, N. Elhadad, and V. Hatzivassiloglou.
2003. Leveraging a common representation for per-
sonalized search and summarization in a medical
digital library. In Proceedings JCDL 2003.
E. Mendonc?a and J. Cimino. 2001. Building a knowl-
edge base to support a digital library. In Proceedings
of MEDINFO 2001.
D. Moldovan, M. Pas?ca, S. Harabagiu, and M. Sur-
deanu. 2002. Performance issues and error analysis
in an open-domain question answering system. In
Proceedings of ACL 2002.
S. Narayanan and S. Harabagiu. 2004. Question an-
swering based on semantic structures. In Proceed-
ings of COLING 2004.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
Proceedings of the ACL 2004 Workshop on Question
Answering in Restricted Domains.
W. Richardson, M. Wilson, J. Nishikawa, and R. Hay-
ward. 1995. The well-built clinical question: A
key to evidence-based decisions. American Col-
lege of Physicians Journal Club, 123(3):A12?A13,
November-December.
31
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 65?72,
New York City, June 2006. c?2006 Association for Computational Linguistics
Generative Content Models for Structural Analysis of Medical Abstracts
Jimmy Lin1,2, Damianos Karakos3, Dina Demner-Fushman2, and Sanjeev Khudanpur3
1College of Information Studies 3Center for Language and
2Institute for Advanced Computer Studies Speech Processing
University of Maryland Johns Hopkins University
College Park, MD 20742, USA Baltimore, MD 21218, USA
jimmylin@umd.edu, demner@cs.umd.edu (damianos, khudanpur)@jhu.edu
Abstract
The ability to accurately model the con-
tent structure of text is important for
many natural language processing appli-
cations. This paper describes experi-
ments with generative models for analyz-
ing the discourse structure of medical ab-
stracts, which generally follow the pattern
of ?introduction?, ?methods?, ?results?,
and ?conclusions?. We demonstrate that
Hidden Markov Models are capable of ac-
curately capturing the structure of such
texts, and can achieve classification ac-
curacy comparable to that of discrimina-
tive techniques. In addition, generative
approaches provide advantages that may
make them preferable to discriminative
techniques such as Support Vector Ma-
chines under certain conditions. Our work
makes two contributions: at the applica-
tion level, we report good performance
on an interesting task in an important do-
main; more generally, our results con-
tribute to an ongoing discussion regarding
the tradeoffs between generative and dis-
criminative techniques.
1 Introduction
Certain types of text follow a predictable structure,
the knowledge of which would be useful in many
natural language processing applications. As an
example, scientific abstracts across many different
fields generally follow the pattern of ?introduction?,
?methods?, ?results?, and ?conclusions? (Salanger-
Meyer, 1990; Swales, 1990; Ora?san, 2001). The
ability to explicitly identify these sections in un-
structured text could play an important role in ap-
plications such as document summarization (Teufel
and Moens, 2000), information retrieval (Tbahriti
et al, 2005), information extraction (Mizuta et al,
2005), and question answering. Although there is
a trend towards analysis of full article texts, we
believe that abstracts still provide a tremendous
amount of information, and much value can still be
extracted from them. For example, Gay et al (2005)
experimented with abstracts and full article texts in
the task of automatically generating index term rec-
ommendations and discovered that using full article
texts yields at most a 7.4% improvement in F-score.
Demner-Fushman et al (2005) found a correlation
between the quality and strength of clinical conclu-
sions in the full article texts and abstracts.
This paper presents experiments with generative
content models for analyzing the discourse struc-
ture of medical abstracts, which has been con-
firmed to follow the four-section pattern discussed
above (Salanger-Meyer, 1990). For a variety of rea-
sons, medicine is an interesting domain of research.
The need for information systems to support physi-
cians at the point of care has been well studied (Cov-
ell et al, 1985; Gorman et al, 1994; Ely et al,
2005). Retrieval techniques can have a large im-
pact on how physicians access and leverage clini-
cal evidence. Information that satisfies physicians?
needs can be found in theMEDLINE database main-
tained by the U.S. National Library of Medicine
65
(NLM), which also serves as a readily available
corpus of abstracts for our experiments. Further-
more, the availability of rich ontological resources,
in the form of the Unified Medical Language Sys-
tem (UMLS) (Lindberg et al, 1993), and the avail-
ability of software that leverages this knowledge?
MetaMap (Aronson, 2001) for concept identification
and SemRep (Rindflesch and Fiszman, 2003) for re-
lation extraction?provide a foundation for studying
the role of semantics in various tasks.
McKnight and Srinivasan (2003) have previously
examined the task of categorizing sentences in med-
ical abstracts using supervised discriminative ma-
chine learning techniques. Building on the work of
Ruch et al (2003) in the same domain, we present a
generative approach that attempts to directly model
the discourse structure of MEDLINE abstracts us-
ing Hidden Markov Models (HMMs); cf. (Barzilay
and Lee, 2004). Although our results were not ob-
tained from the same exact collection as those used
by authors of these two previous studies, comparable
experiments suggest that our techniques are compet-
itive in terms of performance, and may offer addi-
tional advantages as well.
Discriminative approaches (especially SVMs)
have been shown to be very effective for many
supervised classification tasks; see, for exam-
ple, (Joachims, 1998; Ng and Jordan, 2001). How-
ever, their high computational complexity (quadratic
in the number of training samples) renders them pro-
hibitive for massive data processing. Under certain
conditions, generative approaches with linear com-
plexity are preferable, even if their performance is
lower than that which can be achieved through dis-
criminative training. Since HMMs are very well-
suited to modeling sequences, our discourse model-
ing task lends itself naturally to this particular gener-
ative approach. In fact, we demonstrate that HMMs
are competitive with SVMs, with the added advan-
tage of lower computational complexity. In addition,
generative models can be directly applied to tackle
certain classes of problems, such as sentence order-
ing, in ways that discriminative approaches cannot
readily. In the context of machine learning, we see
our work as contributing to the ongoing debate be-
tween generative and discriminative approaches?
we provide a case study in an interesting domain that
begins to explore some of these tradeoffs.
2 Methods
2.1 Corpus and Data Preparation
Our experiments involved MEDLINE, the biblio-
graphical database of biomedical articles maintained
by the U.S. National Library of Medicine (NLM).
We used the subset of MEDLINE that was extracted
for the TREC 2004 Genomics Track, consisting of
citations from 1994 to 2003. In total, 4,591,008
records (abstract text and associated metadata) were
extracted using the Date Completed (DCOM) field
for all references in the range of 19940101 to
20031231.
Viewing structural modeling of medical abstracts
as a sentence classification task, we leveraged the
existence of so-called structured abstracts (see Fig-
ure 1 for an example) in order to obtain the appro-
priate section label for each sentence. The use of
section headings is a device recommended by the
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature (1987) to help humans assess the
reliability and content of a publication and to facil-
itate the indexing and retrieval processes. Although
structured abstracts loosely adhere to the introduc-
tion, methods, results, and conclusions format, the
exact choice of section headings varies from ab-
stract to abstract and from journal to journal. In our
test collection, we observed a total of 2688 unique
section headings in structured abstracts?these were
manually mapped to the four broad classes of ?intro-
duction?, ?methods?, ?results?, and ?conclusions?.
All sentences falling under a section heading were
assigned the label of its appropriately-mapped head-
ing (naturally, the actual section headings were re-
moved in our test collection). As a concrete exam-
ple, in the abstract shown in Figure 1, the ?OBJEC-
TIVE? section would be mapped to ?introduction?,
the ?RESEARCH DESIGN AND METHODS? sec-
tion to ?methods?. The ?RESULTS? and ?CON-
CLUSIONS? sections map directly to our own la-
bels. In total, 308,055 structured abstracts were ex-
tracted and prepared in this manner, serving as the
complete dataset. In addition, we created a reduced
collection of 27,075 abstracts consisting of only
Randomized Controlled Trials (RCTs), which rep-
resent definitive sources of evidence highly-valued
in the clinical decision-making process.
Separately, we manually annotated 49 unstruc-
66
Integrating medical management with diabetes self-management training: a randomized control trial of the Diabetes
Outpatient Intensive Treatment program.
OBJECTIVE? This study evaluated the Diabetes Outpatient Intensive Treatment (DOIT) program, a multiday group educa-
tion and skills training experience combined with daily medical management, followed by case management over 6 months.
Using a randomized control design, the study explored how DOIT affected glycemic control and self-care behaviors over a
short term. The impact of two additional factors on clinical outcomes were also examined (frequency of case management
contacts and whether or not insulin was started during the program). RESEARCH DESIGN AND METHODS? Patients
with type 1 and type 2 diabetes in poor glycemic control (A1c ?8.5%) were randomly assigned to DOIT or a second con-
dition, entitled EDUPOST, which was standard diabetes care with the addition of quarterly educational mailings. A total
of 167 patients (78 EDUPOST, 89 DOIT) completed all baseline measures, including A1c and a questionnaire assessing
diabetes-related self-care behaviors. At 6 months, 117 patients (52 EDUPOST, 65 DOIT) returned to complete a follow-up
A1c and the identical self-care questionnaire. RESULTS? At follow-up, DOIT evidenced a significantly greater drop in A1c
than EDUPOST. DOIT patients also reported significantly more frequent blood glucose monitoring and greater attention to
carbohydrate and fat contents (ACFC) of food compared with EDUPOST patients. An increase in ACFC over the 6-month
period was associated with improved glycemic control among DOIT patients. Also, the frequency of nurse case manager
follow-up contacts was positively linked to better A1c outcomes. The addition of insulin did not appear to be a significant
contributor to glycemic change. CONCLUSIONS? DOIT appears to be effective in promoting better diabetes care and posi-
tively influencing glycemia and diabetes-related self-care behaviors. However, it demands significant time, commitment, and
careful coordination with many health care professionals. The role of the nurse case manager in providing ongoing follow-up
contact seems important.
Figure 1: Sample structured abstract from MEDLINE.
tured abstracts of randomized controlled trials re-
trieved to answer a question about the manage-
ment of elevated low-density lipoprotein cholesterol
(LDL-C). We submitted a PubMed query (?elevated
LDL-C?) and restricted results to English abstracts
of RCTs, gathering 49 unstructured abstracts from
26 journals. Each sentence was annotated with its
section label by the third author, who is a medical
doctor?this collection served as our blind held-out
testset. Note that the annotation process preceded
our experiments, which helped to guard against
annotator-introduced bias. Of 49 abstracts, 35 con-
tained all four sections (which we refer to as ?com-
plete?), while 14 abstracts were missing one or more
sections (which we refer to as ?partial?).
Two different types of experiments were con-
ducted: the first consisted of cross-validation on the
structured abstracts; the second consisted of train-
ing on the structured abstracts and testing on the
unstructured abstracts. We hypothesized that struc-
tured and unstructured abstracts share the same un-
derlying discourse patterns, and that content models
trained with one can be applied to the other.
2.2 Generative Models of Content
Following Ruch et al (2003) and Barzilay and
Lee (2004), we employed Hidden Markov Models
to model the discourse structure of MEDLINE ab-
stracts. The four states in our HMMs correspond
to the information that characterizes each section
(?introduction?, ?methods?, ?results?, and ?conclu-
sions?) and state transitions capture the discourse
flow from section to section.
Using the SRI language modeling toolkit, we
first computed bigram language models for each
of the four sections using Kneser-Ney discounting
and Katz backoff. All words in the training set
were downcased, all numbers were converted into
a generic symbol, and all singleton unigrams and bi-
grams were removed. Using these results, each sen-
tence was converted into a four dimensional vector,
where each component represents the log probabil-
ity, divided by the number of words, of the sentence
under each of the four language models.
We then built a four-state Hidden Markov Model
that outputs these four-dimensional vectors. The
transition probability matrix of the HMM was ini-
tialized with uniform probabilities over a fully
connected graph. The output probabilities were
modeled as four-dimensional Gaussians mixtures
with diagonal covariance matrices. Using the sec-
tion labels, the HMM was trained using the HTK
toolkit (Young et al, 2002), which efficiently per-
forms the forward-backward algorithm and Baum-
Welch estimation. For testing, we performed a
Viterbi (maximum likelihood) estimation of the la-
bel of each test sentence/vector (also using the HTK
toolkit).
67
In an attempt to further boost performance, we
employed Linear Discriminant Analysis (LDA) to
find a linear projection of the four-dimensional vec-
tors that maximizes the separation of the Gaussians
(corresponding to the HMM states). Venables and
Ripley (1994) describe an efficient algorithm (of lin-
ear complexity in the number of training sentences)
for computing the LDA transform matrix, which en-
tails computing the within- and between-covariance
matrices of the classes, and using Singular Value De-
composition (SVD) to compute the eigenvectors of
the new space. Each sentence/vector is then mul-
tiplied by this matrix, and new HMM models are
re-computed from the projected data.
An important aspect of our work is modeling con-
tent structure using generative techniques. To as-
sess the impact of taking discourse transitions into
account, we compare our fully trained model to
one that does not take advantage of the Markov
assumption?i.e., it assumes that the labels are in-
dependently and identically distributed.
To facilitate comparison with previous work, we
also experimented with binary classifiers specifi-
cally tuned to each section. This was done by creat-
ing a two-state HMM: one state corresponds to the
label we want to detect, and the other state corre-
sponds to all the other labels. We built four such
classifiers, one for each section, and trained them in
the same manner as above.
3 Results
We report results on three distinct sets of experi-
ments: (1) ten-fold cross-validation (90/10 split) on
all structured abstracts from the TREC 2004 MED-
LINE corpus, (2) ten-fold cross-validation (90/10
split) on the RCT subset of structured abstracts from
the TREC 2004 MEDLINE corpus, (3) training on
the RCT subset of the TREC 2004 MEDLINE cor-
pus and testing on the 49 hand-annotated held-out
testset.
The results of our first set of experiments are
shown in Tables 1(a) and 1(b). Table 1(a) reports
the classification error in assigning a unique label to
every sentence, drawn from the set {?introduction?,
?methods?, ?results?, ?conclusions?}. For this task,
we compare the performance of three separate mod-
els: one that does not make the Markov assumption,
Model Error
non-HMM .220
HMM .148
HMM + LDA .118
(a)
Section Acc Prec Rec F
Introduction .957 .930 .840 .885
Methods .921 .810 .875 .843
Results .921 .898 .898 .898
Conclusions .963 .898 .896 .897
(b)
Table 1: Ten-fold cross-validation results on all
structured abstracts from the TREC 2004 MED-
LINE corpus: multi-way classification on complete
abstract structure (a) and by-section binary classifi-
cation (b).
the basic four-state HMM, and the improved four-
state HMM with LDA. As expected, explicitly mod-
eling the discourse transitions significantly reduces
the error rate. Applying LDA further enhances clas-
sification performance. Table 1(b) reports accuracy,
precision, recall, and F-measure for four separate bi-
nary classifiers specifically trained for each of the
sections (one per row in the table). We only dis-
play results with our best model, namely HMM with
LDA.
The results of our second set of experiments (with
RCTs only) are shown in Tables 2(a) and 2(b).
Table 2(a) reports the multi-way classification er-
ror rate; once again, applying the Markov assump-
tion to model discourse transitions improves perfor-
mance, and using LDA further reduces error rate.
Table 2(b) reports accuracy, precision, recall, and F-
measure for four separate binary classifiers (HMM
with LDA) specifically trained for each of the sec-
tions (one per row in the table). The table also
presents the closest comparable experimental re-
sults reported by McKnight and Srinivasan (2003).1
McKnight and Srinivasan (henceforth, M&S) cre-
ated a test collection consisting of 37,151 RCTs
from approximately 12 million MEDLINE abstracts
dated between 1976 and 2001. This collection has
1After contacting the authors, we were unable to obtain the
same exact dataset that they used for their experiments.
68
Model Error
non-HMM .238
HMM .212
HMM + LDA .209
(a)
Present study McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F
Introduction .931 .898 .715 .807 .967 .920 .970 .945
Methods .904 .812 .847 .830 .895 .810 .830 .820
Results .902 .902 .831 .867 .860 .810 .830 .820
Conclusions .929 .772 .790 .781 .970 .880 .910 .820
(b)
Table 2: Ten-fold cross-validation results on the structured RCT subset of the TREC 2004 MEDLINE
corpus: multi-way classification (a) and binary classification (b). Table (b) also reproduces the results from
McKnight and Srinivasan (2003) for a comparable task on a different RCT-subset of structured abstracts.
Model Complete Partial
non-HMM .247 .371
HMM .226 .314
HMM + LDA .217 .279
(a)
Complete Partial McKnight and Srinivasan
Section Acc Prec Rec F Acc Prec Rec F Acc Prec Rec F
Introduction .923 .739 .723 .731 .867 .368 .636 .502 .896 .630 .450 .524
Methods .905 .841 .793 .817 .859 .958 .589 .774 .897 .880 .730 .799
Results .899 .913 .857 .885 .892 .942 .830 .886 .872 .840 .880 .861
Conclusions .911 .639 .847 .743 .884 .361 .995 .678 .941 .830 .750 .785
(b)
Table 3: Training on the structured RCT subset of the TREC 2004 MEDLINE corpus, testing on corpus of
hand-annotated abstracts: multi-way classification (a) and binary classification (b). Unstructured abstracts
with all four sections (complete), and with missing sections (partial) are shown. Table (b) again repro-
duces the results from McKnight and Srinivasan (2003) for a comparable task on a different subset of 206
unstructured abstracts.
69
significantly more training examples than our corpus
of 27,075 abstracts, which could be a source of per-
formance differences. Furthermore, details regard-
ing their procedure for mapping structured abstract
headings to one of the four general labels was not
discussed in their paper. Nevertheless, our HMM-
based approach is at least competitive with SVMs,
perhaps better in some cases.
The results of our third set of experiments (train-
ing on RCTs and testing on a held-out testset of
hand-annotated abstracts) is shown in Tables 3(a)
and 3(b). Mirroring the presentation format above,
Table 3(a) shows the classification error for the four-
way label assignment problem. We noticed that
some unstructured abstracts are qualitatively differ-
ent from structured abstracts in that some sections
are missing. For example, some unstructured ab-
stracts lack an introduction, and instead dive straight
into methods; other unstructured abstracts lack a
conclusion. As a result, classification error is higher
in this experiment than in the cross-validation ex-
periments. We report performance figures for 35 ab-
stracts that contained all four sections (?complete?)
and for 14 abstracts that had one or more miss-
ing sections (?partial?). Table 3(b) reports accu-
racy, precision, recall, and F-measure for four sep-
arate binary classifiers (HMM with LDA) specifi-
cally trained for each section (one per row in the
table). The table also presents the closest compa-
rable experimental results reported by M&S?over
206 hand-annotated unstructured abstracts. Interest-
ingly, M&S did not specifically note missing sec-
tions in their testset.
4 Discussion
An interesting aspect of our generative approach
is that we model HMM outputs as Gaussian vec-
tors (log probabilities of observing entire sentences
based on our language models), as opposed to se-
quences of terms, as done in (Barzilay and Lee,
2004). This technique provides two important ad-
vantages. First, Gaussian modeling adds an ex-
tra degree of freedom during training, by capturing
second-order statistics. This is not possible when
modeling word sequences, where only the probabil-
ity of a sentence is actually used in the HMM train-
ing. Second, using continuous distributions allows
us to leverage a variety of tools (e.g., LDA) that have
been shown to be successful in other fields, such as
speech recognition (Evermann et al, 2004).
Table 2(b) represents the closest head-to-head
comparison between our generative approach
(HMM with LDA) and state-of-the-art results
reported by M&S using SVMs. In some ways, the
results reported by M&S have an advantage because
they use significantly more training examples. Yet,
we can see that generative techniques for the model-
ing of content structure are at least competitive?we
even outperform SVMs on detecting ?methods?
and ?results?. Moreover, the fact that the training
and testing of HMMs have linear complexity (as
opposed to the quadratic complexity of SVMs)
makes our approach a very attractive alternative,
given the amount of training data that is available
for such experiments.
Although exploration of the tradeoffs between
generative and discriminative machine learning
techniques is one of the aims of this work, our ul-
timate goal, however, is to build clinical systems
that provide timely access to information essential
to the patient treatment process. In truth, our cross-
validation experiments do not correspond to any
meaningful naturally-occurring task?structured ab-
stracts are, after all, already appropriately labeled.
The true utility of content models is to struc-
ture abstracts that have no structure to begin with.
Thus, our exploratory experiments in applying con-
tent models trained with structured RCTs on un-
structured RCTs is a closer approximation of an
extrinsically-valid measure of performance. Such a
component would serve as the first stage of a clin-
ical question answering system (Demner-Fushman
and Lin, 2005) or summarization system (McKe-
own et al, 2003). We chose to focus on randomized
controlled trials because they represent the standard
benchmark by which all other clinical studies are
measured.
Table 3(b) shows the effectiveness of our trained
content models on abstracts that had no explicit
structure to begin with. We can see that although
classification accuracy is lower than that from our
cross-validation experiments, performance is quite
respectable. Thus, our hypothesis that unstructured
abstracts are not qualitatively different from struc-
tured abstracts appears to be mostly valid.
70
5 Related Work
Although not the first to employ a generative ap-
proach to directly model content, the seminal work
of Barzilay and Lee (2004) is a noteworthy point
of reference and comparison. However, our study
differs in several important respects. Barzilay and
Lee employed an unsupervised approach to building
topic sequence models for the newswire text genre
using clustering techniques. In contrast, because
the discourse structure of medical abstracts is well-
defined and training data is relatively easy to ob-
tain, we were able to apply a supervised approach.
Whereas Barzilay and Lee evaluated their work in
the context of document summarization, the four-
part structure of medical abstracts allows us to con-
duct meaningful intrinsic evaluations and focus on
the sentence classification task. Nevertheless, their
work bolsters our claims regarding the usefulness of
generative models in extrinsic tasks, which we do
not describe here.
Although this study falls under the general topic
of discourse modeling, our work differs from previ-
ous attempts to characterize text in terms of domain-
independent rhetorical elements (McKeown, 1985;
Marcu and Echihabi, 2002). Our task is closer to the
work of Teufel and Moens (2000), who looked at the
problem of intellectual attribution in scientific texts.
6 Conclusion
We believe that there are two contributions as a re-
sult of our work. From the perspective of machine
learning, the assignment of sequentially-occurring
labels represents an underexplored problem with re-
spect to the generative vs. discriminative debate?
previous work has mostly focused on stateless clas-
sification tasks. This paper demonstrates that Hid-
den Markov Models are capable of capturing dis-
course transitions from section to section, and are
at least competitive with Support Vector Machines
from a purely performance point of view.
The other contribution of our work is that it con-
tributes to building advanced clinical information
systems. From an application point of view, the abil-
ity to assign structure to otherwise unstructured text
represents a key capability that may assist in ques-
tion answering, document summarization, and other
natural language processing applications.
Much research in computational linguistics has
focused on corpora comprised of newswire articles.
We would like to point out that clinical texts provide
another attractive genre in which to conduct experi-
ments. Such texts are easy to acquire, and the avail-
ability of domain ontologies provides new opportu-
nities for knowledge-rich approaches to shine. Al-
though we have only experimented with lexical fea-
tures in this study, the door is wide open for follow-
on studies based on semantic features.
7 Acknowledgments
The first author would like to thank Esther and Kiri
for their loving support.
References
Ad Hoc Working Group for Critical Appraisal of the
Medical Literature. 1987. A proposal for more infor-
mative abstracts of clinical articles. Annals of Internal
Medicine, 106:595?604.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: The MetaMap
program. In Proceeding of the 2001 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2001), pages 17?21.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the 2004 Human Language Technology Confer-
ence and the North American Chapter of the Associ-
ation for Computational Linguistics Annual Meeting
(HLT/NAACL 2004).
David G. Covell, Gwen C. Uman, and Phil R. Manning.
1985. Information needs in office practice: Are they
being met? Annals of Internal Medicine, 103(4):596?
599, October.
Dina Demner-Fushman and Jimmy Lin. 2005. Knowl-
edge extraction for clinical question answering: Pre-
liminary results. In Proceedings of the AAAI-05 Work-
shop on Question Answering in Restricted Domains.
Dina Demner-Fushman, Susan E. Hauser, and George R.
Thoma. 2005. The role of title, metadata and ab-
stract in identifying clinically relevant journal arti-
cles. In Proceeding of the 2005 Annual Symposium of
the American Medical Informatics Association (AMIA
2005), pages 191?195.
John W. Ely, Jerome A. Osheroff, M. Lee Chambliss,
Mark H. Ebell, and Marcy E. Rosenbaum. 2005. An-
swering physicians? clinical questions: Obstacles and
71
potential solutions. Journal of the American Medical
Informatics Association, 12(2):217?224, March-April.
Gunnar Evermann, H. Y. Chan, Mark J. F. Gales, Thomas
Hain, Xunying Liu, David Mrva, Lan Wang, and Phil
Woodland. 2004. Development of the 2003 CU-HTK
Conversational Telephone Speech Transcription Sys-
tem. In Proceedings of the 2004 International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP04).
Clifford W. Gay, Mehmet Kayaalp, and Alan R. Aronson.
2005. Semi-automatic indexing of full text biomedi-
cal articles. In Proceeding of the 2005 Annual Sympo-
sium of the American Medical Informatics Association
(AMIA 2005), pages 271?275.
Paul N. Gorman, Joan S. Ash, and Leslie W. Wykoff.
1994. Can primary care physicians? questions be an-
swered using the medical journal literature? Bulletin
of the Medical Library Association, 82(2):140?146,
April.
Thorsten Joachims. 1998. Text categorization with Sup-
port Vector Machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning (ECML 1998).
Donald A. Lindberg, Betsy L. Humphreys, and Alexa T.
McCray. 1993. The Unified Medical Language Sys-
tem. Methods of Information in Medicine, 32(4):281?
291, August.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002).
Kathleen McKeown, Noemie Elhadad, and Vasileios
Hatzivassiloglou. 2003. Leveraging a common rep-
resentation for personalized search and summarization
in a medical digital library. In Proceedings of the
3rd ACM/IEEE Joint Conference on Digital Libraries
(JCDL 2003).
Kathleen R. McKeown. 1985. Text Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press, Cambridge, England.
Larry McKnight and Padmini Srinivasan. 2003. Catego-
rization of sentence types in medical abstracts. In Pro-
ceeding of the 2003 Annual Symposium of the Ameri-
can Medical Informatics Association (AMIA 2003).
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2005. Zone analysis in biology articles as a
basis for information extraction. International Journal
of Medical Informatics, in press.
Andrew Y. Ng and Michael Jordan. 2001. On discrim-
inative vs. generative classifiers: A comparison of lo-
gistic regression and naive Bayes. In Advances in Neu-
ral Information Processing Systems 14.
Constantin Ora?san. 2001. Patterns in scientific abstracts.
In Proceedings of the 2001 Corpus Linguistics Confer-
ence.
Thomas C. Rindflesch and Marcelo Fiszman. 2003. The
interaction of domain knowledge and linguistic struc-
ture in natural language processing: Interpreting hy-
pernymic propositions in biomedical text. Journal of
Biomedical Informatics, 36(6):462?477, December.
Patrick Ruch, Christine Chichester, Gilles Cohen, Gio-
vanni Coray, Fre?de?ric Ehrler, Hatem Ghorbel, Hen-
ning Mu?ller, and Vincenzo Pallotta. 2003. Report
on the TREC 2003 experiment: Genomic track. In
Proceedings of the Twelfth Text REtrieval Conference
(TREC 2003).
Franc?oise Salanger-Meyer. 1990. Discoursal movements
in medical English abstracts and their linguistic expo-
nents: A genre analysis study. INTERFACE: Journal
of Applied Linguistics, 4(2):107?124.
John M. Swales. 1990. Genre Analysis: English in Aca-
demic and Research Settings. Cambridge University
Press, Cambridge, England.
Imad Tbahriti, Christine Chichester, Fre?de?rique Lisacek,
and Patrick Ruch. 2005. Using argumentation to re-
trieve articles with similar citations: An inquiry into
improving related articles search in the MEDLINE
digital library. International Journal of Medical In-
formatics, in press.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000).
William N. Venables and Brian D. Ripley. 1994. Modern
Applied Statistics with S-Plus. Springer-Verlag.
Steve Young, Gunnar Evermann, Thomas Hain, Dan Ker-
shaw, Gareth Moore, Julian Odell, Dave Ollason, Dan
Povey, Valtcho Valtchev, and Phil Woodland. 2002.
The HTK Book. Cambridge University Press.
72
BioNLP 2007: Biological, translational, and clinical language processing, pages 105?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
From Indexing the Biomedical Literature to Coding Clinical Text: 
Experience with MTI and Machine Learning Approaches 
Alan R. Aronson1, Olivier Bodenreider1, Dina Demner-Fushman1, Kin Wah Fung1,  
Vivian K. Lee1,2, James G. Mork1, Aur?lie N?v?ol1, Lee Peters1, Willie J. Rogers1
1Lister Hill Center 
National Library of Medicine 
Bethesda, MD 20894 
{alan, olivier, demnerd, 
kwfung, mork, neveola,  
peters, wrogers} 
@nlm.nih.gov 
 
2Vanderbilt University 
Nashville, TN 37235 
vivian.lee@vanderbilt.edu 
 
 
Abstract 
This paper describes the application of an 
ensemble of indexing and classification 
systems, which have been shown to be suc-
cessful in information retrieval and classi-
fication of medical literature, to a new task 
of assigning ICD-9-CM codes to the clini-
cal history and impression sections of radi-
ology reports. The basic methods used are: 
a modification of the NLM Medical Text 
Indexer system, SVM, k-NN and a simple 
pattern-matching method. The basic meth-
ods are combined using a variant of stack-
ing. Evaluated in the context of a Medical 
NLP Challenge, fusion produced an F-
score of 0.85 on the Challenge test set, 
which is considerably above the mean 
Challenge F-score of 0.77 for 44 participat-
ing groups. 
1 Introduction 
Researchers at the National Library of Medicine 
(NLM) have developed the Medical Text Indexer 
(MTI) for the automatic indexing of the biomedical 
literature (Aronson et al, 2004). The unsupervised 
methods within MTI were later successfully com-
bined with machine learning techniques and ap-
plied to the classification tasks in the Genomics 
Track evaluations at the Text Retrieval Conference 
(TREC) (Aronson et al, 2005 and Demner-
Fushman et al, 2006). This fusion approach con-
sists of using several basic classification methods 
with complementary strengths, combining the re-
sults using a modified ensemble method based on 
stacking (Ting and Witten, 1997). 
While these methods have shown reasonable 
performance on indexing and retrieval tasks of 
biomedical articles, it remains to be determined 
how they would perform on a different biomedical 
corpus (e.g., clinical text) and on a different task 
(e.g., coding to a different controlled vocabulary). 
However, except for competitive evaluations such 
as TREC or BioCreAtIvE, corpora and gold stan-
dards for such tasks are generally not available, 
which is a limiting factor for such studies. For a 
survey of currently available corpora and devel-
opments in biomedical language processing, see 
Hunter and Cohen, 2006. 
The Medical NLP Challenge 1  sponsored by a 
number of groups including the Computational 
Medicine Center (CMC) at the Cincinnati Chil-
dren?s Hospital Medical Center gave us the oppor-
tunity to apply our fusion approach to a clinical 
corpus. The Challenge was to assign ICD-9-CM 
codes (International Classification of Diseases, 9th 
Revision, Clinical Modification) 2  to clinical text 
consisting of anonymized clinical history and im-
pression sections of radiology reports. 
The Medical NLP Challenge organizers distrib-
uted a training corpus of almost 1,000 of the ano-
nymized, abbreviated radiology reports along with 
                                                 
1 See www.computationalmedicine.org/challenge/.
2 See www.cdc.gov/nchs/icd9.htm.
105
gold standard ICD-9-CM assignments for each 
report obtained via a consensus of three independ-
ent sets of assignments. The primary measure for 
the Challenge was defined as the balanced F-score, 
with a secondary measure being cost-sensitive ac-
curacy. These measures were computed for sub-
missions to the Challenge based on a test corpus 
similar in size to the training corpus but distributed 
without gold standard code assignments. 
The main objective of this study is to determine 
what adaptation of the original methods is required 
to code clinical text with ICD-9-CM, in contrast to 
indexing and retrieving MEDLINE?. Note that an 
earlier study (Gay et al, 2005) showed that only 
minor adaptations were required in extending the 
original model to full-text biomedical articles. A 
secondary objective is to evaluate the performance 
of our methods in this new setting. 
 
2 Methods 
In early experimentation with the training corpus 
provided by the Challenge organizers, we discov-
ered that several of the training cases involved ne-
gated assertions in the text and that deleting these 
improved the performance of all basic methods 
being tested. For example, ?no pneumonia? occurs 
many times in the impression section of a report, 
sometimes with additional context. Section 2.1 
describes the process we used to remove these ne-
gated expressions; section 2.2 consists of descrip-
tions of the four basic methods used in this study; 
and section 2.3 defines the fusion of the basic 
methods to form a final result. 
2.1 Document Preparation 
The NegEx program (Chapman et al, 2001a and 
2001b, and Goldin and Chapman, 2003), which 
discovers negated expressions in text, was used to 
find negated expressions in the training and test 
corpora using a dictionary generated from concepts 
from the 2006AD version of the UMLS? Metathe-
saurus? (excluding the AMA vocabularies). A ta-
ble containing the concept unique identifier (CUI) 
and English string (STR with LAT=?ENG?) was 
extracted from the main concept table, MRCON, 
and was used as input to NegEx to generate a dic-
tionary that was later used as the universe of ex-
pressions which NegEx could find to be negated in 
the target corpora. (See the Appendix for examples 
of the input and output to this process.) 
The XML text of the training and test corpora 
was converted to a tree representation and then 
traversed, operating on one radiology report at a 
time. The clinical history and impression sections 
of each report were tokenized to allow whitespace 
to be separated from the punctuation, numbers and 
alphabetic text. The concepts from the UMLS were 
tokenized in the same way, to allow the concepts 
found by NegEx to be aligned with the text. The 
negation phrases discovered by NegEx were also 
tokenized to find the appropriate negation phrase 
preceding or trailing the target concept. Using the 
location information obtained by matching the set 
of one or more target concepts and the associated 
negation phrase, the overlapping concept spans 
were merged and the span for the negation phrase 
and the outermost negated concept was removed. 
Any intervening concepts associated with the same 
negation phrase were removed, too. The abbrevi-
ated tree representation was then re-serialized back 
into XML. 
As an example of our use of NegEx, consider 
the report with clinical history ?13-year 2-month - 
old female evaluate for cough.? and impression 
?No focal pneumonia.? After removal of negated 
text, the clinical history becomes ?13-year 2-month 
- old female?, and the discussion is empty. 
2.2 Basic Methods 
The four basic methods used for the Medical NLP 
Challenge are MTI (a modification of NLM?s 
Medical Text Indexer system), SVM (Support 
Vector Machines), k-NN (k Nearest Neighbors) 
and Pattern Matching (a simple, pattern-based clas-
sifier). Each of these methods is described here. 
Note that the MTI method uses a ?Restrict to ICD-
9-CM? algorithm that is described in the next sec-
tion. 
 
MTI. The original Medical Text Indexer (MTI) 
system, shown in Figure 1, consists of an infra-
structure for applying alternative methods of dis-
covering MeSH? headings for citation titles and 
abstracts and then combining them into an ordered 
list of recommended indexing terms. The top por-
tion of the diagram consists of two paths, or meth-
ods, for creating a list of recommended indexing 
terms: MetaMap Indexing and PubMed? Related 
Citations. The MetaMap Indexing path actually 
106
computes UMLS Metathesaurus concepts, which 
are passed to the Restrict to MeSH process 
(Bodenreider et al, 1998). The results from each 
path are weighted and combined using Post-
Processing, which also refines the results to con-
form to NLM indexing policy. The system is 
highly parameterized not only by path weights but 
also by several parameters specific to the Restrict 
to MeSH and Post-Processing processes. 
 
 
 
Figure 1: Medical Text Indexer (MTI) System 
 
For use in the Challenge, the Medical Text In-
dexer (MTI) program itself required few adapta-
tions.  Most of the changes involved the environ-
ment from which MTI obtains the data it uses 
without changing the normal parameter settings. 
We also added a further post-processing compo-
nent to filter our results. 
For the environment, we replaced MTI?s normal 
?Restrict to MeSH? algorithm with a ?Restrict to 
ICD-9-CM? algorithm, described below, in order 
to map UMLS concepts to ICD-9-CM codes in-
stead of MeSH headings. We also trained the Pub-
Med Related Citations component, TexTool (Ta-
nabe and Wilbur, 2002), on the Medical NLP Chal-
lenge training data instead of the entire MED-
LINE/PubMed database as is the case for normal 
MTI use at NLM.  For both of these methods, we 
used the actual ICD-9-CM codes to mimic UMLS 
CUIs used internally by MTI. 
To create the new training data for the TexTool 
(Related Citations), we reformatted the Medical 
NLP Challenge training data into a pseudo-
MEDLINE format using the ?doc id? component 
as the PMID, the ?CLINICAL_HISTORY? text 
component for the Title, the ?IMPRESSION? text 
component for the Abstract, and all of the 
?CMC_MAJORITY? codes as MeSH Headings 
(see Figure 2).  This provided us with direct ICD-
9-CM codes to work with instead of MeSH Head-
ings. 
 
<doc id="97663756" type="RADIOLOGY_REPORT"> 
  <codes> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">780.6</code> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY3" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">786.2</code> 
  </codes> 
  <texts> 
    <text origin="CCHMC_RADIOLOGY" 
type="CLINICAL_HISTORY">Cough and fever.</text> 
    <text origin="CCHMC_RADIOLOGY" 
type="IMPRESSION">Normal radiographic appear-
ance of the chest, no pneumonia.</text> 
  </texts> 
</doc> 
PMID- 97663756 
TI  - Cough and fever. 
AB  - Normal radiographic appearance of the 
chest, no pneumonia. 
MH  - Fever (780.6) 
MH  - Cough (786.2) 
 
Figure 2: XML Medical NLP Training Data modi-
fied to pseudo-ASCII MEDLINE format 
 
Within MTI we also utilized an experimental 
option for MetaMap (Composite Phrases), which 
provides a longer UMLS concept match than usual. 
We did not use the following: (1) UMLS concept-
specific checking and exclusion sections; and (2) 
the MeSH Subheading generation, checking, and 
removal elements, since they were not needed for 
this Challenge. We then had MTI use the new Re-
107
strict to ICD-9-CM file and the new TexTool to 
generate its results. 
 
Restrict to ICD-9-CM. The mapping of every 
UMLS concept to ICD-9-CM developed for the 
Medical NLP Challenge is an adaptation of the 
original mapping to MeSH, later generalized to any 
target vocabulary (Fung and Bodenreider, 2005). 
Based on the UMLS Metathesaurus, the mapping 
utilizes four increasingly aggressive techniques: 
synonymy, built-in mappings, hierarchical map-
pings and associative mappings. In order to comply 
with coding rules in ICD-9-CM, mappings to non-
leaf codes are later resolved into leaf codes. 
Mappings to ICD-9-CM are identified through 
synonymy when names from ICD-9-CM are in-
cluded in the UMLS concept identified by 
MetaMap. For example, the ICD-9-CM code 592.0 
Calculus of kidney is associated with the UMLS 
concept C0392525 Nephrolithiasis through synon-
ymy. 
Built-in mappings are mapping relations be-
tween UMLS concepts implied from mappings 
provided by source vocabularies in the UMLS. For 
example, the UMLS concept C0239937 Micro-
scopic hematuria is mapped to the concept 
C0018965 (which contains the ICD-9-CM code 
599.7 Hematuria) through a mapping provided by 
SNOMED CT. 
In the absence of a mapping through synonymy 
or built-in mapping, a hierarchical mapping is 
attempted. Starting from the concept identified by 
MetaMap, a graph of ancestors is built by first us-
ing its parent concepts and broader concepts, then 
adding the parent concepts and broader concepts of 
each concept, recursively. Semantic constraints 
(based on semantic types) are applied in order to 
prevent semantic drift. Ancestor concepts closest 
to the MetaMap source concept are selected from 
the graph. Only concepts that can be resolved into 
ICD-9-CM codes (through synonymy or built-in 
mapping) are selected. For example, starting from 
C0239574 Low grade pyrexia, a mapping is found 
to ICD-9-CM code 780.6 Fever, which is con-
tained in the concept C0015967, one of the ances-
tors of C0239574. 
The last attempt to find a mapping involves not 
only hierarchical, but also associative relations. 
Instead of starting from the concept identified by 
MetaMap, associative mappings explore the con-
cepts in associative relation to this concept. For 
example, the concept C1458136 Renal stone sub-
stance is mapped to ICD-9-CM code 592.0 Calcu-
lus of kidney. 
Finally, when the identified ICD-9-CM code 
was not a leaf code (e.g., 786.5 Chest pain), we 
remapped it to one of the corresponding leaf codes 
in the training set where possible (e.g., 786.50 Un-
specified chest pain). 
Of the 2,331 UMLS concepts identified by 
MetaMap in the test set after freezing the method, 
620 (27%) were mapped to ICD-9-CM. More spe-
cifically, 101 concepts were mapped to one of the 
45 target ICD-9-CM codes present in the training 
set. Of the 101 concepts, 40 were mapped through 
synonymy, 11 through built-in mappings, 40 
through hierarchical mapping and 10 through asso-
ciative mapping. 
 
After the main MTI processing was completed, 
we applied a post-processing filter, restricting our 
results to the list of 94 valid combinations of ICD-
9-CM codes provided in the training set (hence-
forth referred to as allowed combinations) and 
slightly emphasizing MetaMap results. Examples 
of the post-processing rules are: 
? If MTI recommended 079.99 (Unspecified 
viral infection in conditions?) via either 
MetaMap or Related Citations, use 079.99, 
493.90 (Asthma, unspecified type?), and 
780.6 (Fever) for indexing. This is the only 
valid combination for this code based on the 
training corpus. 
? Similarly, if MTI recommended ?Enlarge-
ment of lymph nodes? (785.6) via the 
MetaMap path with a score greater then 
zero, use 785.6 and 786.2 (Cough) for in-
dexing. 
The best F-score (F = 0.83) for the MTI method 
was obtained on the training set using the negation-
removed text.  This was a slight improvement over 
using the original text (F = 0.82). 
 
SVM. We utilized Yet Another Learning Envi-
ronment3 (YALE), an open source application de-
veloped for machine learning and data mining, to 
determine the data classification performance of 
support vector machine (SVM) learning on the 
                                                 
3 See http://rapid-i.com. 
108
training data. To prepare the Challenge data for 
analysis, we removed all stop words and created 
feature vectors for the free text extracted from the 
?CLINICAL_HISTORY? and ?IMPRESSION? 
fields of the records.  Since both the training and 
test Challenge data had a known finite number of 
individual ICD-9-CM labels (45) and distinct com-
binations of ICD-9-CM labels (94), the data was 
prepared both as feature vectors for 45 individual 
labels as well as a model with 94 combination la-
bels.  In addition, the feature vectors were created 
using both simple term frequency as well as in-
verse document frequency (IDF) weighting, where 
the weight is (1+log(term frequency))*(total 
documents/document frequency).  There were thus 
a total of four feature vector datasets: 1) 45 indi-
vidual ICD-9-CM labels and simple term fre-
quency, 2) 45 ICD-9-CM labels and IDF weight-
ing, 3) 94 ICD-9-CM combinations and simple 
term frequency, and 4) 94 ICD-9-CM combina-
tions and IDF weighting. 
The YALE tool encompasses a number of SVM 
learners and kernel types.  For the classification 
problem at hand, we chose the C-SVM learner and 
the radial basis function (rbf) kernel.  The C-SVM 
learner attempts to minimize the error function 
?
=
+
N
i
i
T Cww
1
,
2
1 ?  
Niandbxw iii
T
i ,,1,01))(( K=???+ ????
 
where w is the vector of coefficients, b is a con-
stant, ?  is the kernel function, x are the independ-
ent variables, and ?i are parameters for handling 
the inputs.  C > 0 is the penalty parameter of the 
error function.  The rbf kernel is defined as K(x, 
x?) = exp(?? |x ? x?|2), ? > 0 where ? is a kernel 
parameter that determines the rbf width. We ran 
cross-validation experiments using YALE on all 
training datasets and varying C (10, 100, 1000, 
10000) and ? (0.01, 0.001, 0.0001, 0.00001) to de-
termine the optimal C and ? combination.  The 
cross-validation experiments generated classifica-
tion models that were then applied to the complete 
training datasets to analyze the performance of the 
learner. The 94 ICD-9-CM combination and sim-
ple term frequency dataset with C = 10000 and ? = 
0.01 had the best F-score at 0.86.  The best F-score 
for the 94 ICD-9-CM combination and IDF weight 
dataset was 0.79, where C = 0.001 and ? = 10000.   
Further preprocessing the training dataset by 
removing negated expressions was found to im-
prove the best F-score from 0.86 to 0.87.  The C = 
10000 and ? = 0.01 combination was then applied 
to the test dataset, which was preprocessed to re-
move negation and stop words and transformed to 
a feature vector using 94 ICD-9-CM combinations 
and simple term weighting.  The predicted ICD-9-
CM classifications and confidence of the predic-
tions for each clinical free text report were output 
and later combined with other methods to optimize 
the accuracy and precision of our ICD-9-CM clas-
sifications. 
 
k-NN. The Challenge training set was used to 
build a k-NN classifier. The k-NN classification 
method works by identifying, within a labelled set, 
documents similar to the document being classi-
fied, and inferring a classification for it from the 
labels of the retrieved neighbors. 
The free text in the training data set was proc-
essed to obtain a vector-space representation of the 
patient reports.  
Several methods of obtaining this representation 
were tested: after stop words were removed, simple 
term frequency and inverse document frequency 
(IDF) weighting were applied alternatively. A 
higher weight was also given to words appearing in 
the history portion of the text (vs. impression). 
Eventually, the most efficient representation was 
obtained by using controlled vocabulary terms ex-
tracted from the free text with MetaMap.4 Further 
processing on this representation of the training 
data showed that removing negated portions of the 
free text improved the results, raising the F-score 
from 0.76 to 0.79.   
Other parameters were also assessed on the 
training data, such as the number of neighbors to 
use (2 was found to be the best vs. 5, 10 or 15) and 
the restriction of the ICD-9-CM predictions to the 
set of 94 allowed combinations. When the predic-
tion for a given document was not within the set of 
allowed 94 combinations, an allowed subset of the 
ICD-9-CM codes predicted was selected based on 
the individual scores obtained for each ICD-9-CM 
code.  
The best F-score (F = 0.79) obtained on the 
training set used the MetaMap-based representa-
                                                 
4 Note that this use of MetaMap is independent of its 
inclusion as a component of MTI. 
109
tion with simple frequency counts on the text with 
negated expressions removed. ICD-9-CM predic-
tions were obtained from the nearest neighbors and 
restricted to one of the 94 allowed combinations.   
 
Pattern Matching. We developed a pattern-
matching classifier as a baseline for our more so-
phisticated classification methods. A list of all 
UMLS string representations for each of 45 codes 
(including synonyms from source vocabularies 
other than ICD-9-CM) was created as described in 
the MTI section above. The strings were then con-
verted to lower case, punctuation was removed, 
and strings containing terms unlikely to be found 
in a clinical report were pruned. For example, Ab-
domen NOS pain and Abdominal pain (finding) 
were reduced to abdominal pain. For the same rea-
sons, some of the strings were relaxed into pat-
terns. For example, it is unlikely to see PAIN 
CHEST in a chart, but very likely to find pain in 
chest. The string, therefore, was relaxed to the fol-
lowing pattern: pain.*chest. The text of the clinical 
history and the impression fields of the radiology 
reports with negated expressions removed (see 
Section 2.2) was broken up into sentences. Each 
sentence was then searched for all available pat-
terns. A corresponding code was assigned to the 
document for each matched pattern. This pattern 
matching achieved F-score = 0.79 on the training 
set. To reduce the number of codes assigned to a 
document, a check for allowed combinations was 
added as a post-processing step. The combination 
of assigned codes was looked up in the table of 
allowed codes. If not present, the codes were re-
duced to the combination of assigned codes most 
frequently occurring in the training set. This 
brought the F-score up to 0.84 on the training data. 
As the performance of this classifier was compara-
ble to other methods, we decided to include these 
results when combining the predictions of the other 
classifiers.  
2.3 Fusion of  Basic Methods: Stacking 
Experience with ad hoc retrieval tasks in the TREC 
Genomics Track has shown that combining predic-
tions of several classifiers either significantly im-
proves classification results, or at least provides 
more consistent and stable results when the train-
ing data set is small (Aronson et al, 2005). We 
therefore experimented with stacking (Ting and 
Witten, 1997), using a simple majority vote and a 
union of all assigned codes as baselines. The pre-
dictions of base classifiers described in the previ-
ous section were combined using our re-
implementation of the stacked generalization pro-
posed by Ting and Witten.  
3 Results 
Table 1 shows the results obtained for the training 
set. The best stacking results were obtained using 
predictions of all four base classifiers on the text 
with deleted negated expressions and with check-
ing for allowed combinations. We retained all final 
predictions with probability of being a valid code 
greater than 0.3. Checking for the allowed combi-
nations for the ensemble classifiers degraded the F-
score significantly. 
 
Classifier F-score 
MTI 0.83 
SVM 0.87 (x-validation) 
k-NN 0.79 (x-validation) 
Pattern Matching 0.84 
Majority 0.82 
Stacking 0.89 
 
Table 1: Training results for each classifier, the ma-
jority and stacking 
 
Since stacking produced the best F-score on the 
training corpus and is known to be more robust 
than the individual classifiers, the corresponding 
results for the test corpus were submitted to the 
Challenge submission website. The stacking results 
for the test corpus achieved an F-score of 0.85 and 
a secondary, cost-sensitive accuracy score of 0.83. 
For comparison purposes, 44 Challenge submis-
sions had a mean F-score of 0.77 with a maximum 
of 0.89. Our F-score of 0.85 falls between the 70th 
and 75th percentiles. 
4 Discussion 
It is significant that it was fairly straightforward to 
port various methods developed for ad hoc MED-
LINE citation retrieval, indexing and classification 
to the assignment of codes to clinical text. The 
modifications to MTI consisted of replacing Re-
strict to MeSH with Restrict to ICD-9-CM, training 
the Related Citations method on clinical text and 
replacing MTI?s normal post-processing with a 
much simpler version. Preprocessing the text using 
110
NegEx to remove negated expressions was a fur-
ther modification of the overall approach. 
It is noteworthy that a simple pattern-matching 
method performed as well as much more sophisti-
cated methods in the effort to fuse results from 
several methods into a final outcome. This unex-
pected success might be explained by the follow-
ing limitations of the Challenge. 
Possible limitations on the extensibility of the 
current research arise from two observations: (1) 
the Challenge cases were limited to two relatively 
narrow topics, cough/fever/pneumonia and uri-
nary/kidney problems; and (2) the clinical text was 
almost error-free, a situation that would not be ex-
pected in the majority of clinical text. It is possible 
that these conditions contributed to the success of 
the pattern-matching method but also caused 
anomalous behavior, such as the fact that simple 
frequency counts provided a better representation 
than IDF for the SVM and k-NN methods. 
Finally, as a result of low confidence in the 
ICD-9-CM code assignment, no codes were as-
signed to 29 records in the test set. It is worthwhile 
to explore the causes for such null assignments. 
One of the reasons for low confidence could be the 
aggressive pruning of the text by the negation algo-
rithm. For example, after removal of negated text 
in the sample report given in section 2.1, the only 
remaining text is ?13-year 2-month - old female? 
from the clinical history field; this provided no 
evidence for code assignment. Secondly, in some 
cases the original text was not sufficient for confi-
dent code assignment. For example, for the docu-
ment with clinical history ?Bilateral grade 3.? and 
impression ?Interval growth of normal appearing 
Kidneys?, no code was assigned by the SVM, k-
NN, or pattern-matching classifiers. Code 593.70 
corresponding to the UMLS concept Vesicouret-
eral reflux with reflux nephropathy, unspecified or 
without reflux nephropathy was assigned by MTI 
with a very low confidence, which was not suffi-
cient for the final assignment of the code. The third 
reason for assigning no code to a document was 
the wide range of assignments provided by the 
base classifiers. For example, for the following 
document: ?CLINICAL_HISTORY: 3-year - old 
male with history of left ureteropelvic and uret-
erovesical obstruction. Status post left pyeloplasty 
and left ureteral reimplantation. IMPRESSION: 1. 
Stable appearance and degree of hydronephrosis 
involving the left kidney. Stable urothelial thicken-
ing. 2. Interval growth of kidneys, left greater than 
right. 3. Normal appearance of the right kidney 
with interval resolution of right urothelial thicken-
ing.? MTI assigned codes 593.89 Other specified 
disorders of kidney and ureter and 591 Hy-
dronephrosis. Codes 593.70 Vesicoureteral reflux 
with reflux nephropathy, unspecified or without 
reflux nephropathy and 753.3 Double kidney with 
double pelvis were assigned by the k-NN classifier. 
Pattern matching resulted in assignment of code 
591 with fairly low confidence. No code was as-
signed to this document by the SVM classifier. 
Despite failing to assign codes to these 29 records, 
the conservative approach (using threshold) re-
sulted in better performance, achieving F-score 
0.85 compared to F-score 0.80 when all 1,634 
codes assigned by the base classifiers were used. 
5 Conclusion 
We are left with two conclusions. First, this re-
search confirms that combining several comple-
mentary methods for accomplishing tasks, ranging 
from ad hoc retrieval to categorization, produces 
results that are better and more stable than the re-
sults for the contributing methods. Furthermore, 
we have shown that the basic methods employing 
domain knowledge and advanced statistical algo-
rithms are applicable to clinical text without sig-
nificant modification. Second, although there are 
some limitations of the current Challenge test col-
lection of clinical text, we appreciate the efforts of 
the Challenge organizers in the creation of a test 
collection of clinical text. This collection provides 
a unique opportunity to apply existing methods to a 
new and important domain. 
Acknowledgements 
This work was supported in part by the Intramural 
Research Program of the NIH, National Library of 
Medicine and by appointments of Aur?lie N?v?ol 
and Vivian Lee to the NLM Research Participation 
Program sponsored by the National Library of 
Medicine and administered by the Oak Ridge Insti-
tute for Science and Education. 
The authors gratefully acknowledge the many 
essential contributions to MTI, especially W. John 
Wilbur for the PubMed Related Citations indexing 
method, and Natalie Xie for adapting TexTool (an 
interface to Related Citations) for this paper. 
111
References 
Aronson AR, Demner-Fushman D, Humphrey SM, Lin 
J, Liu H, Ruch P, Ruiz ME, Smith LH, Tanabe LK, 
Wilbur WJ. Fusion of knowledge-intensive and sta-
tistical approaches for retrieving and annotating tex-
tual genomics documents. Proc TREC 2005, 36-45. 
Aronson AR, Mork JG, Gay CW, Humphrey SM and 
Rogers WJ. The NLM Indexing Initiative's Medical 
Text Indexer. Medinfo. 2004: 268-72. 
Bodenreider O, Nelson SJ, Hole WT and Chang HF. 
Beyond synonymy: exploiting the UMLS semantics 
in mapping vocabularies. Proc AMIA Symp 1998: 
815-9. 
Chapman WW, Bridewell W, Hanbury P, Cooper GF, 
Buchanan B. Evaluation of negation phrases in narra-
tive clinical reports. Proc AMIA Symp. 2001a:105-9.  
Chapman WW, Bridewell W, Hanbury P, Cooper GF 
and Buchanan BG. A simple algorithm for identify-
ing negated findings and diseases in discharge sum-
maries. J Biomed Inform. 2001b;34:301-10.  
Demner-Fushman D, Humphrey SM, Ide NC, Loane RF, 
Ruch P, Ruiz ME, Smith LH, Tanabe LK, Wilbur WJ 
and Aronson AR. Finding relevant passages in scien-
tific articles: fusion of automatic approaches vs. an 
interactive team effort. Proc TREC 2006, 569-76. 
Fung KW and Bodenreider O. Utilizing the UMLS for 
semantic mapping between terminologies. AMIA 
Annu Symp Proc 2005: 266-70. 
Gay CW, Kayaalp M and Aronson AR. Semi-automatic 
indexing of full text biomedical articles. AMIA Annu 
Symp Proc. 2005:271-5. 
Goldin I and Chapman WW. Learning to detect nega-
tion with ?not? in medical texts. Proc Workshop on 
Text Analysis and Search for Bioinformatics, ACM 
SIGIR, 2003. 
Hunter L and Cohen KB. Biomedical language process-
ing: what?s beyond PubMed? Mol Cell. 2006 Mar 
3;21(5):589-94. 
Tanabe L and Wilbur WJ. (2002) Tagging gene and 
protein names in biomedical text. Bioinformatics, 
Aug 2002; 18: 1124 ?32. 
Ting WK and Witten I. 1997. Stacking bagged and dag-
ged models. 367-375. Proc. of ICML'97. Morgan 
Kaufmann, San Francisco, CA.
Appendix  
A sample of the input to NegEx for dictionary generation:  
 
C0002390 pneumonitis, allergic interstitial 
C0002390 allergic interstitial pneumonitis, nos 
C0002390 extrinsic allergic bronchiolo alveolitis 
C0002390 extrinsic allergic bronchiolo alveolitis, nos 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 eaa  extrinsic allergic alveolitis 
C0002390 allergic extrinsic alveolitis nos (disorder) 
C0002390 extrinsic allergic alveolitis (disorder) 
C0002390 hypersensitivity pneumonitis nos (disorder) 
 
A sample of the dictionary generated by NegEx for later use in detecting negated expressions:  
 
C0002098 hypersensitivity granuloma (morphologic abnormality 
C0151726 hypersensitivity injection site 
C0020517 hypersensitivity nos 
C0429891 hypersensitivity observations 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 hypersensitivity pneumonitides 
C0005592 hypersensitivity pneumonitides, avian 
C0002390 hypersensitivity pneumonitis 
C0182792 hypersensitivity pneumonitis antibody determination re-
agents 
112
BioNLP 2007: Biological, translational, and clinical language processing, pages 137?144,
Prague, June 2007. c?2007 Association for Computational Linguistics
Interpreting Comparative Constructions in Biomedical Text 
Marcelo Fiszman,1   Dina Demner-Fushman,2    
Francois M. Lang,2  Philip Goetz,2 
Thomas C. Rindflesch2 
1University of Tennessee ? GSM, Knoxville, TN 37920 
mfiszman@utmck.edu 
2Lister Hill National Center for Biomedical Communications 
National Library of Medicine, Bethesda, MD 20894 
{ddemner|goetzp|flang|trindflesch}@mail.nih.gov 
 
Abstract 
We propose a methodology using 
underspecified semantic interpretation to 
process comparative constructions in 
MEDLINE citations, concentrating on two 
structures that are prevalent in the research 
literature reporting on clinical trials for 
drug therapies. The method exploits an 
existing semantic processor, SemRep, 
which constructs predications based on the 
Unified Medical Language System. Results 
of a preliminary evaluation were recall of 
70%, precision of 96%, and F-score of 
81%. We discuss the generalization of the 
methodology to other entities such as 
therapeutic and diagnostic procedures. The 
available structures in computable format 
are potentially useful for interpreting 
outcome statements in MEDLINE 
citations. 
1 Introduction 
As natural language processing (NLP) is 
increasingly able to support advanced information 
management techniques for research in medicine 
and biology, it is being incrementally improved to 
provide extended coverage and more accurate 
results. In this paper, we discuss the extension of 
an existing semantic interpretation system to 
address comparative structures. These structures 
provide a way of explicating the characteristics of 
one entity in terms of a second, thereby enhancing 
the description of the first. This phenomenon is 
important in clinical research literature reporting 
the results of clinical trials.  
In the abstracts of these reports, a treatment for 
some disease is typically discussed using two types 
of comparative structures. The first announces that 
the (primary) therapy focused on in the study will 
be compared to some other (secondary) therapy. A 
typical example is (1). 
(1) Lansoprazole compared with 
ranitidine for the treatment of 
nonerosive gastroesophageal reflux 
disease. 
An outcome statement (2) often appears near the 
end of the abstract, asserting results in terms of the 
relative merits of the primary therapy compared to 
the secondary. 
(2) Lansoprazole is more 
effective than ranitidine in 
patients with endoscopically 
confirmed non-erosive reflux 
esophagitis. 
The processing of comparative expressions such 
as (1) and (2) was incorporated into an existing 
system, SemRep [Rindflesch and Fiszman, 2003; 
Rindflesch et al, 2005], which constructs semantic 
predications by mapping assertions in biomedical 
text to the Unified Medical Language System? 
(UMLS)? [Humphreys et al, 1998].  
2 Background 
2.1 Comparative structures in English 
The range of comparative expressions in English is 
extensive and complex. Several linguistic studies 
have investigated their characteristics, with 
differing assumptions about syntax and semantics 
(for example [Ryan, 1981; Rayner and Banks, 
1990; Staab and Hahn, 1997; Huddleston and 
Pullum, 2002]). Our study concentrates on  
137
structures in which two drugs are compared with 
respect to a shared attribute (e.g. how well they 
treat some disease). An assessment of their relative 
merit in this regard is indicated by their positions 
on a scale. The compared terms are expressed as 
noun phrases, which can be considered to be 
conjoined. The shared characteristic focused on is 
expressed as a predicate outside the comparative 
structure. An adjective or noun is used to denote 
the scale, and words such as than, as, with, and to 
serve as cues to identify the compared terms, the 
scale, and the relative position of the terms on the 
scale.  
The first type of structure we address (called  
comp1 and illustrated in (3)) merely asserts that the 
primary and secondary terms (in bold) are being 
compared. A possible cue for identifying these 
structures is a form of compare. A further 
characteristic is that the compared terms are 
separated by a conjunction, or a preposition, as in 
(3). 
(3) To compare misoprostol with 
dinoprostone for cervical ripening 
and labor induction. 
As shown in (4), a scale may be  mentioned 
(efficacy); however, in this study, we only identify 
the compared terms in structures of this type.  
(4) To compare the efficacy of 
misoprostol with dinoprostone for 
cervical ripening and labor 
induction. 
In the more complex comparative expression we 
accommodate (called comp2), the relative ranking 
of two compared terms is indicated on a scale 
denoted by an adjective (e.g. effective in (5)). The 
relative position of the compared terms in scalar 
comparative structures of this type expresses either 
equality or inequality. Inequality is further divided 
into superiority, where the primary compared term 
is higher on the scale than the secondary, and 
inferiority, where the opposite is true. Cues 
associated with the adjective designating the scale 
signal these phenomena (e.g. as ADJ as in (5) for 
equality, ADJer than in (6) for superiority, and less 
ADJ than in (7) for inferiority).  
(5) Azithromycin is as effective 
as erythromycin estolate for the 
treatment of pertussis in children. 
(6) Naproxen is safer than 
aspirin in the treatment of the 
arthritis of rheumatic fever. 
(7) Sodium valproate was 
significantly less effective than 
prochlorperazine in reducing pain 
or nausea. 
In examples (3) through (7), the characteristic the 
compared drugs have in common is treatment of 
some disorder, for example treatment of pertussis 
in children in (5).  
Few studies describe an implemented automatic 
analysis of comparatives; however, Friedman 
[Friedman, 1989] is a notable exception. Jindal and 
Liu [Jindal and Liu, 2006] use machine learning to 
identify some comparative structures, but do not 
provide a semantic interpretation. We exploit 
SemRep machinery to interpret the aspects of 
comparative structures just described. 
2.2 SemRep 
SemRep [Rindflesch and Fiszman, 2003; 
Rindflesch et al, 2005] recovers underspecified 
semantic propositions in biomedical text based on 
a partial syntactic analysis and structured domain 
knowledge from the UMLS. Several systems that 
extract entities and relations are under 
development in both the clinical and molecular 
biology domains. Examples of systems for clinical 
text are described in [Friedman et al, 1994], 
[Johnson et al, 1993], [Hahn et al, 2002], and 
[Christensen et al, 2002]. In molecular biology, 
examples include [Yen et al, 2006], [Chun et al, 
2006], [Blaschke et al, 1999], [Leroy et al, 2003], 
[Rindflesch et al, 2005], [Friedman et al, 2001], 
and [Lussier et al, 2006].  
During SemRep processing, a partial syntactic 
parse is produced that depends on lexical look-up 
in the SPECIALIST lexicon [McCray et al, 1994] 
and a part-of-speech tagger [Smith et al, 2004]. 
MetaMap [Aronson, 2001] then matches noun 
phrases to concepts in the Metathesaurus? and 
determines the semantic type for each concept. For 
example, the structure in (9), produced for (8), 
allows both syntactic and semantic information to 
be used in further SemRep processing that 
interprets semantic predications.  
(8) Lansoprazole for the 
treatment of gastroesophageal 
reflux disease 
138
(9) [[head(noun(Lansoprazole),me
taconc(?lansoprazole?:[phsu]))],[p
rep(for),det(the),head(noun(treatm
ent))],[prep(of),mod(adj(gastroeso
phageal)),mod(noun(reflux)),head(n
oun(disease),metaconc(?Gastroesoph
ageal reflux disease?:[dsyn]))]] 
Predicates are derived from indicator rules that 
map syntactic phenomena (such as verbs and 
nominalizations) to relationships in the UMLS 
Semantic Network. Argument identification is 
guided by dependency grammar rules as well as 
constraints imposed by the Semantic Network. In 
processing (8), for example, an indicator rule links 
the nominalization treatment with the Semantic 
Network relation ?Pharmacologic Substance 
TREATS Disease or Syndrome.? Since the 
semantic types of the syntactic arguments 
identified for treatment in this sentence 
(?Pharmacologic Substance? for ?lansoprazole? and 
?Disease or Syndrome? for ?Gastroesophageal 
reflux disease?) match the corresponding semantic 
types in the relation from the Semantic Network, 
the predication in (10) is constructed, where 
subject and object are Metathesaurus concepts.  
(10) lansoprazole TREATS 
Gastroesophageal reflux disease 
3 Methods 
3.1 Linguistic patterns 
We extracted sentences for developing 
comparative processing from a set of  some 10,000 
MEDLINE citations reporting on the results of 
clinical trials, a rich source of comparative 
structures. In this sample, the most frequent 
patterns for comp1 (only announces that two terms 
are compared) and comp2 (includes a scale and 
positions on that scale) are given in (11) and (12). 
In the patterns, Term1 and Term2 refer to the 
primary and secondary compared terms, 
respectively. ?{BE}? means that some form of be 
is optional, and slash indicates disjunction. These 
patterns served as guides for enhancing SemRep 
argument identification machinery but were not 
implemented as such. That is, they indicate 
necessary components but do not preclude 
intervening modifiers and qualifiers.   
(11) comp1: Compared terms 
C1:   Term1 {BE} compare with/to Term2 
C2:   compare Term1 with/to Term2 
C3:   compare Term1 and/versus Term2 
C4a: Term1 comparison with/to Term2 
C4b: comparison of Term1 with/to Term2 
C4c: comparison of Term1 and/versus Term2 
C5   Term1 versus Term2 
(12) comp2: Scalar patterns 
S1:   Term1 BE as ADJ as {BE} Term2 
S2a: Term1 BE more ADJ than {BE} Term2 
S2b: Term1 BE ADJer than {BE}Term2  
S2c: Term1 BE less ADJ than {BE} Term2 
S4:   Term1 BE superior to Term2 
S5:   Term1 BE inferior to Term2 
As with SemRep in general, the interpretation of 
comparative structures exploits underspecified 
syntactic structure enhanced with Metathesaurus 
concepts and semantic types. Semantic groups 
[McCray et al, 2001] from the Semantic Network 
are also available. For this project, we exploit the 
group Chemicals & Drugs, which contains such 
semantic types as ?Pharmacologic Substance?, 
?Antibiotic?, and ?Immunologic Factor?. (The 
principles used here also apply to compared terms 
with semantic types from other semantic groups, 
such as ?Procedures?.) In the comp1 patterns, a 
form of compare acts as an indicator of a 
comparative predication. In comp2, the adjective 
serves that function. Other words appearing in the 
patterns cue the indicator word (in comp2) and 
help identify the compared terms (in both comp1 
and comp2). The conjunction versus  is special in 
that it cues the secondary compared term (Term2) 
in comp1, but may also indicate a comp1 structure 
in the absence of a form of compare (C5).  
3.2 Interpreting comp1 patterns  
When SemRep encounters a form of compare, it 
assumes a comp1 structure and looks to the right 
for the first noun phrase immediately preceded by 
with, to, and, or versus. If the head of this phrase is 
mapped to a concept having a semantic type in the 
group Chemicals & Drugs, it is marked as the 
secondary compared term. The algorithm then 
looks to the left of that term for a noun phrase 
having a semantic type also in the group Chemicals 
& Drugs, which becomes the primary compared 
term. When this processing is applied to (13), the 
semantic predication (14) is produced, in which the 
predicate is COMPARED_WITH; the first 
argument is the primary compared term and the 
139
other is the secondary. As noted earlier, although a 
scale is sometimes asserted in these structures (as 
in (13)), SemRep does not retrieve it. An assertion 
regarding position on the scale never appears in 
comp1 structures.  
(13) To compare the efficacy and 
tolerability of Hypericum 
perforatum with imipramine in 
patients with mild to moderate 
depression. 
(14) Hypericum perforatum 
COMPARED_WITH Imipramine 
SemRep considers noun phrases occurring 
immediately to the right and left of versus as being 
compared terms if their heads have been mapped to 
Metathesaurus concepts having semantic types 
belonging to the group Chemicals & Drugs. Such 
noun phrases are interpreted as part of a comp1 
structure, even if a form of compare has not 
occurred. The predication (16) is derived from 
(15).  
(15) Intravenous lorazepam versus 
dimenhydrinate for treatment of 
vertigo in the emergency 
department: a randomized clinical 
trial. 
(16) Lorazepam COMPARED_WITH 
Dimenhydrinate 
SemRep treats compared terms as being 
coordinated. For example, this identification 
allows both ?Lorazepam? and ?Dimenhydrinate? 
to function as arguments of TREATS in (15). 
Consequently, in addition to (16), the predications 
in (17) are returned as the semantic interpretation 
of (15). Such processing is done for all comp1 and 
comp2 structures (although these results are not 
given for (13) and are not further discussed in this 
paper). 
(17) Lorazepam TREATS Vertigo  
 Dimenhydrinate TREATS 
Vertigo 
3.3 Interpreting comp2 patterns  
In addition to identifying two compared terms 
when processing comp2 patterns, a scale must be 
named and the relative position of the terms on that 
scale indicated. The algorithm for finding 
compared terms in comp2 structures begins by 
locating one of the cues as, than, or to and then 
examines the next noun phrase to the right. If its 
head has been mapped to a concept with a 
semantic type in the group Chemicals & Drugs, it 
is marked as the secondary compared term. As in 
comp1, the algorithm then looks to the left for the 
first noun phrase having a head in the same 
semantic group, and that phrase is marked as the 
primary compared term.  
To find the scale name, SemRep examines the 
secondary compared term and then locates the first 
adjective to its left. The nominalization of that 
adjective (as found in the SPECIALIST Lexicon) 
is designated as the scale and serves as an 
argument of the predicate SCALE in the 
interpretation. For adjectives superior and inferior 
(patterns S4 and S5 in (12)) the scale name is 
?goodness.? 
In determining relative position on the scale, 
equality is contrasted with inequality. If the 
adjective of the construction is immediately 
preceded by as (pattern S1 in (12) above), the two 
compared terms have the same position on the 
scale (equality), and are construed as arguments of 
a predication with predicate SAME_AS. In all 
other comp2 constructions, the compared terms are 
in a relationship of inequality. The primary 
compared term is considered higher on the scale 
unless the adjective is inferior or is preceded by 
less, in which case the secondary term is higher. 
The predicates HIGHER_THAN and 
LOWER_THAN are used to construct predications 
with the compared terms to interpret position on 
the scale. The equality construction in (18) is 
expressed as the predications in (19).  
(18) Candesartan is as effective 
as lisinopril once daily in 
reducing blood pressure. 
(19) Candesartan COMPARED_WITH 
lisinopril 
 SCALE:Effectiveness  
 Candesartan SAME_AS 
lisinopril 
The superiority construction in (20) is expressed as 
the predications in (21).  
(20) Losartan was more effective 
than atenolol in reducing 
cardiovascular morbidity and 
mortality in patients with 
hypertension, diabetes, and LVH. 
(21) Losartan COMPARED_WITH 
Atenolol 
140
 SCALE:Effectiveness 
 Losartan HIGHER_THAN 
Atenolol 
The inferiority construction in (22) is expressed as 
the predications in (23).  
(22) Morphine-6-glucoronide was 
significantly less potent than 
morphine in producing pupil 
constriction. 
(23) morphine-6-glucoronide 
COMPARED_WITH Morphine 
 SCALE:Potency 
 morphine-6-glucoronide 
LOWER_THAN Morphine 
3.4 Accommodating negation  
Negation in comparative structures affects the 
position of the compared terms on the scale, and is 
accommodated differently for equality and for 
inequality. When a scalar comparison of equality 
(pattern S1, as ADJ as) is negated, the primary 
term is lower on the scale than the secondary 
(rather than being at least equal). For example, in 
interpreting the negated equality construction in 
(24), SemRep produces (25). 
(24) Amoxicillin-clavulanate was 
not as effective as ciprofloxacin 
for treating uncomplicated bladder 
infection in women. 
(25) Amoxicillin-clavulanate 
COMPARED_WITH Ciprofloxaci 
 SCALE:Effectiveness 
 Amoxicillin-clavulanate 
LOWER_THAN Ciprofloxacin 
For patterns of inequality, SemRep negates the 
predication indicating position on the scale. For 
example, the predications in (27) represent the 
negated superiority comparison in (26). Negation 
of inferiority comparatives (e.g. ?X is not less 
effective than Y?) is extremely rare in our sample.  
(26) These data show that 
celecoxib is not better than 
diclofenac (P = 0.414) in terms of 
ulcer complications. 
(27) celecoxib COMPARED_WITH 
diclofenac 
 SCALE:Goodness  
 celecoxib NEG_HIGHER_THAN 
diclofenac 
3.5 Evaluation 
To evaluate the effectiveness of the developed 
methods we created a test set of 300 sentences 
containing comparative structures. These were 
extracted by the second author (who did not 
participate in the development of the methodology) 
from 3000 MEDLINE citations published later in 
date than the  citations used to develop the 
methodology. The citations were retrieved with a 
PubMed query specifying randomized controlled 
studies and comparative studies on drug therapy.  
Sentences containing direct comparisons of the 
pharmacological actions of two drugs expressed in 
the target structures (comp1 and comp2) were 
extracted starting from the latest retrieved citation 
and continuing until 300 sentences with 
comparative structures had been examined. These 
were annotated with the PubMed ID of the citation, 
names of two drugs (COMPARED_WITH 
predication), the scale on which they are compared 
(SCALE), and the relative position of the primary 
drug with respect to the secondary (SAME_AS, 
HIGHER_THAN, or LOWER_THAN).  
The test sentences were processed using 
SemRep and evaluated against the annotated test 
set. We then computed recall and precision in 
several ways: overall for all comparative 
structures, for comp1 structures only, and for 
comp2 structures only. To understand how the 
overall identification of comparatives is influenced 
by the components of the construction, we also 
computed recall and precision separately for drug 
names, scale, and position on scale (SAME_AS, 
HIGHER_THAN and LOWER_THAN taken 
together). Recall measures the proportion of 
manually annotated categories that have been 
correctly identified automatically. Precision 
measures what proportion of the automatically 
annotated categories is correct.  
In addition, the overall identification of 
comparative structures was evaluated using the F-
measure [Rijsbergen, 1979], which combines recall 
and precision. The F-measure was computed using 
macro-averaging and micro-averaging. Macro-
averaging was computed over each category first 
and then averaged over the three categories (drug 
names, scale, and position on scale). This approach 
gives equal weight to each category. In micro-
averaging (which gives an equal weight to the 
performance on each sentence) recall and precision 
141
were obtained by summing over all individual 
sentences. Because it is impossible to enumerate 
all entities and relations which are not drugs, scale, 
or position we did not use the classification error 
rate and other metrics that require computing of 
true negative values. 
4 Results 
Upon inspection of the SemRep processing results 
we noticed that the test set contained nine 
duplicates.  In addition, four sentences were not 
processed for various technical reasons. We report 
the results for the remaining 287 sentences, which 
contain 288 comparative structures occurring in 
168 MEDLINE citations. Seventy four citations 
contain 85 comp2 structures. The remaining 203 
structures are comp1.  
Correct identification of comparative structures 
of both types depends on two factors: 1) 
recognition of both drugs being compared, and 2) 
recognition of the presence of a comparative 
structure itself. In addition, correct identification of 
the comp2 structures depends on recognition of the 
scale on which the drugs are compared and the 
relative position of the drugs on the scale. Table 1 
presents recall, precision, and F-score reflecting 
these factors. 
 
Table 1. SemRep performance 
Task Recall Precision F-score
Overall 0.70 0.96 0.81 
Drug extraction 0.69 0.96 0.81 
Comp1 0.74 0.98 0.84 
Comp2  0.62 0.92 0.74 
Scale  0.62 1.00 0.77 
Position on scale 0.62 0.98 0.76 
 
We considered drug identification to be correct 
only if both drugs participating in the relationship 
were identified correctly. The recall results 
indicate that approximately 30% of the drugs and 
comparative structures of comp1, as well as 40% 
of comp2 structures, remain unrecognized; 
however, all components are identified with high 
precision. Macro-averaging over compared drug 
names, scale, and position on scale categories we 
achieve an F-score = 0.78. The micro-average 
score for 287 comparative sentences is 0.5. 
5 Discussion 
In examining SemRep errors, we determined that 
more than 60% of the false negatives (for both 
comp1 and comp2) were due to ?empty heads? 
[Chodorow et al, 1985; Guthrie et al, 1990], in 
which the syntactic head of a noun phrase does not 
reflect semantic thrust. Such heads prevent 
SemRep from accurately determining the semantic 
type and group of the noun phrase. In our sample, 
expressions interpreted as empty heads include 
those referring to drug dosage and formulations, 
such as extended release (the latter often 
abbreviated as XR). Examples of missed 
interpretations are in sentences (28) and (29), 
where the empty heads are in bold. Ahlers et al 
[Ahlers et al, 2007] discuss enhancements to 
SemRep for accommodating empty heads. These 
mechanisms are being incorporated into the 
processing for comparative structures.  
(28) Oxybutynin 15 mg was more 
effective than propiverine 20 mg 
in reducing symptomatic and 
asymptomatic IDCs in ambulatory 
patients. 
(29) Intravesical atropine was as 
effective as oxybutynin immediate 
release for increasing bladder 
capacity and it was probably 
better with less antimuscarinic 
side effects 
False positives were due exclusively to word 
sense ambiguity. For example, in (30) bid (twice a 
day) was mapped to the concept ?BID protein?, 
which belongs to the semantic group Chemicals & 
Drugs. The most recent version of MetaMap, 
which will soon be called by comparative 
processing, exploits word sense disambiguation 
[Humphrey et al, 2006] and will likely resolve 
some of these errors.  
(30) Retapamulin ointment 1% (bid) 
for 5 days was as effective as 
oral cephalexin (bid) for 10 days 
in treatment of patients with SID, 
and was well tolerated. 
Although, in this paper, we tested the method on 
structures in which the compared terms belong to 
the semantic group Chemicals & Drugs, we can 
straightforwardly generalize the method by adding 
other semantic groups to the algorithm. For 
142
example, if SemRep recognized the noun phrases 
in bold in (31) and (32) as belonging to the group 
Procedures, comparative processing could proceed 
as for Chemicals & Drugs.  
(31) Comparison of multi-slice 
spiral CT and magnetic resonance 
imaging in evaluation of the un-
resectability of blood vessels in 
pancreatic tumor. 
(32) Dynamic multi-slice spiral 
CT is better than dynamic magnetic 
resonance to some extent in 
evaluating the un-resectability of 
peripancreatic blood vessels in 
pancreatic tumor. 
The semantic predications returned by SemRep 
to represent comparative expressions can be 
considered a type of executable knowledge that 
supports reasoning. Since the arguments in these 
predications have been mapped to the UMLS, a 
structured knowledge source, they can be 
manipulated using that knowledge. It is also 
possible to compute the transitive closure of all 
SemRep output for a collection of texts to 
determine which drug was asserted in that 
collection to be the best with respect to some 
characteristic. This ability could be very useful in 
supporting question-answering applications. 
As noted earlier, it is common in reporting on 
the results of randomized clinical trials and 
systematic reviews that a comp1 structure appears 
early in the discourse to announce the objectives of 
the study and that a comp2 structure often appears 
near the end to give the results. Another example 
of this phenomenon appears in (33) and (34) (from 
PMID 15943841).  
(33) To compare the efficacy of 
famotidine and omeprazole in 
Japanese patients with non-erosive 
gastro-oesophageal reflux disease 
by a prospective randomized 
multicentre trial. 
(34) Omeprazole is more effective 
than famotidine for the control of 
gastro-oesophageal reflux disease 
symptoms in H. pylori-negative 
patients. 
We suggest one example of an application that 
can benefit from the information provided by the 
knowledge inherent in the semantic interpretation 
of comparative structures, and that is the 
interpretation of outcome statements in MEDLINE 
citations, as a method for supporting automatic 
access to the latest results from clinical trials 
research. 
6 Conclusion 
We expanded a symbolic semantic interpreter to 
identify comparative constructions in biomedical 
text. The method relies on underspecified syntactic 
analysis and domain knowledge from the UMLS.  
We identify two compared terms and scalar 
comparative structures in MEDLINE citations. 
Although we restricted the method to comparisons 
of drug therapies, the method can be easily 
generalized to other entities such as diagnostic and 
therapeutic procedures. The availability of this 
information in computable format can support the 
identification of outcome sentences in MEDLINE, 
which in turn supports translation of biomedical 
research into improvements in quality of patient 
care. 
Acknowledgement This study was supported in 
part by the Intramural Research Programs of the 
National Institutes of Health, National Library of 
Medicine. 
References  
Ahlers C, Fiszman M, Demner-Fushman D, Lang F, 
Rindflesch TC. 2007. Extracting semantic 
predications from MEDLINE citations for 
pharmacogenomics. Pacific Symposium on 
Biocomputing  12:209-220. 
Aronson AR. 2001. Effective mapping of biomedical 
text to the UMLS Metathesaurus: The MetaMap 
program. Proc AMIA Symp, 17-21. 
Blaschke C, Andrade MA, Ouzounis C, and Valencia A. 
1999. Automatic extraction of biological information 
from scientific text: protein-protein interactions. 
Proceedings of the 7th International Conference on 
Intelligent Systems for Molecular Biology. Morgan 
Kaufman Publishers, San Francisco, CA. 
Christensen L, Haug PJ, and Fiszman M. 2002. 
MPLUS: A probabilistic medical language 
understanding system. Proceedings of the Workshop 
on Natural Language Processing in the Biomedical 
Domain, Association for Computational Linguistics, 
29-36. 
Chodorow MS, Byrd RI, and Heidom GE. 1985. 
Extracting Semantic Hierarchies from a Large On-
143
Line Dictionary. Proceedings of the 23rd Annual 
Meeting of the Association for Computational 
Linguistics, 299-304. 
Chun HW, Tsuruoka Y, Kim J-D, Shiba R, Nagata N, 
Hishiki T, and Tsujii J. 2006, Extraction of gene-
disease relations from Medline using domain 
dictionaries and machine learning. Pac Symp 
Biocomput, 4-15. 
Friedman C. 1989. A general computational treatment 
of the comparative. Proc 27th Annual Meeting Assoc 
Comp Linguistics, 161-168. 
Friedman C, Alderson PO, Austin JH, Cimino JJ, and 
Johnson SB. 1994.  A general natural-language text 
processor for clinical radiology. J Am Med Inform 
Assoc, 1(2):161-74. 
Friedman C, Kra P, Yu H, Krauthammer M, and 
Rzhetsky A. 2001.  GENIES: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles. Bioinformatics, 17 
Suppl 1:S74-S82. 
Guthrie L, Slater BM, Wilks Y, Bruce R. 1990. Is there 
content in empty heads? Proceedings of the 13th 
Conference on Computational Linguistics, v3:138 ? 
143.   
Hahn U, Romacker M, and Schulz S. 2002. 
MEDSYNDIKATE--a natural language system for 
the extraction of medical information from findings 
reports. Int J Med Inf, 67(1-3):63-74. 
Huddleston R, and Pullum GK. 2002. The Cambridge 
Grammar of the English Language. Cambridge 
University Press, Cambridge, UK. 
Humphrey SM, Rogers WJ, Kilicoglu H, Demner-
Fushman D, Rindflesch TC. 2006. Word sense 
disambiguation by selecting the best semantic type 
based on Journal Descriptor Indexing: Preliminary 
experiment. J Am Soc Inf SciTech 57(1):96-113. 
Humphreys BL, Lindberg DA, Schoolman HM, and 
Barnett OG. 1998. The Unified Medical Language 
System: An informatics research collaboration. J Am 
Med Inform Assoc, 5(1):1-11. 
Jindal, Nitin and Bing Liu. 2006. Identifying 
comparative sentences in text documents. 
Proceedings of the 29th Annual International ACM 
SIGIR Conference on Research & Development on 
Information Retrieval. 
Johnson SB, Aguirre A, Peng P, and Cimino J. 1993. 
Interpreting natural language queries using the 
UMLS. Proc Annu Symp Comput Appl Med Care, 
294-8. 
Leroy G, Chen H, and Martinez JD. 2003 A shallow 
parser based on closed-class words to capture 
relations in biomedical text. J Biomed Inform, 
36(3):145-158. 
Lussier YA, Borlawsky T, Rappaport D, Liu Y, and 
Friedman C. 2006 PhenoGO: assigning phenotypic 
context to Gene Ontology annotations with natural 
language processing. Pac Symp Biocomput, 64-75. 
McCray AT, Srinivasan S, and Browne AC. 1994. 
Lexical methods for managing variation in 
biomedical terminologies. Proc Annu Symp Comput 
Appl Med Care, 235-9. 
McCray AT, Burgun A, and Bodenreider O. 2001 
Aggregating UMLS semantic types for reducing 
conceptual complexity. Medinfo, 10(Pt 1): 216-20. 
Rayner M and Banks A. 1990. An implementable 
semantics for comparative constructions. 
Computational Linguistics, 16(2):86-112. 
Rindflesch TC. 1995. Integrating natural language 
processing and biomedical domain knowledge for 
increased information retrieval effectiveness. Proc 
5th  Annual Dual-use Technologies and Applications 
Conference, 260-5. 
Rindflesch TC and Fiszman M. 2003. The interaction of 
domain knowledge and linguistic structure in natural 
language processing: Interpreting hypernymic 
propositions in biomedical text. J Biomed Inform, 
36(6):462-77. 
Rindflesch TC, Marcelo Fiszman , and Bisharah Libbus.  
2005. Semantic interpretation for the biomedical 
research literature. Medical informatics: Knowledge 
management and data mining in biomedicine. 
Springer, New York, NY. 
Rijsbergen V. 1979.  Information Retrieval, 
Butterworth-Heinemann, Newton, MA. 
Ryan K. 1981. Corepresentational grammar and parsing 
English comparatives. Proc 19th Annual Meeting  
Assoc Comp Linguistics, 13-18. 
Smith L, Rindflesch T, and Wilbur WJ. 2004. MedPost: 
a part-of-speech tagger for biomedical text. 
Bioinformatics, 20(14):2320-1. 
Staab S and Hahn U. Comparatives in context. 1997. 
Proc 14th National Conference on Artificial 
Intelligence and 9th Innovative Applications of 
Artificial Intelligence Conference, 616-621. 
Yen YT, Chen B, Chiu HW, Lee YC, Li YC, and Hsu 
CY. 2006. Developing an NLP and IR-based 
algorithm for analyzing gene-disease relationships.
 
144
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 21?22,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adapting naturally occurring test suites
for evaluation of clinical question answering
Dina Demner-Fushman
Lister Hill National Center for Biomedical Communications,
National Library of Medicine, NIH, Bethesda, MD 20894, USA
ddemner@mail.nih.gov
Abstract
This paper describes the structure of a test
suite for evaluation of clinical question an-
swering systems; presents several manually
compiled resources found useful for test suite
generation; and describes the adaptation of
these resources for evaluation of a clinical
question answering system.
1 Introduction
The community-wide interest in rapid development
in many areas of natural language processing and in-
formation retrieval resulted in creation of reusable
test collections in large-scale evaluations such as the
Text REtrieval Conference (TREC)1. Researchers in
more specific areas, for which no TREC or other col-
lections are available, have to create or find suitable
test collections to evaluate their systems.
For example, Cramer et al (2006) recruited vol-
unteers and quickly gathered a sizeable corpus of
question-answer pairs for evaluation of German
open-domain question answering systems. This was
achieved through a Web-based tool that allowed
marking up ?interesting? passages in Wikipedia ar-
ticles and then asking questions about the content
of those passages. This appealing approach can not
easily be applied in the domain of clinical ques-
tion answering because the quality of the questions
and answers as well as the answer completeness
are paramount. A test suite for evaluation of clini-
cal question answering systems should contain a set
1http://trec.nist.gov/
of real-life questions asked by clinicians and high-
quality answers compiled by experts. The answers
should be presented in the form deemed useful by
clinicians.
One of the benefits of focusing on a specific do-
main, such as clinical question answering, is that the
user-needs and desirable results are well-studied and
their descriptions are readily-available. In the case
of clinical question answering, clinicians? desider-
ata are: to see a ?bottom-line advice? first, have
on-demand access to the context that was used in
generation of the advice, and finally have access
to the original sources of information (Ely et al,
2005). A fair number of high-quality manually cre-
ated collections present answers to clinical questions
in this form and could be obtained online. Three par-
tially freely-available sources: Family Practitioner
Inquiry Network (FPIN)2, Parkhurst Exchange Fo-
rum (PE)3, and BMJ Clinical Evidence (BMJ-CE)4
were used to design and develop the presented test
suites and evaluation methods.
Although there seems to be a distinction between
test collections and test suites (Co-
hen et al, 2004) (the former defined as ?pieces
of text? and associated with corpora, the latter, as
lists of specially constructed sentences, or sentence
sequences, or sentence fragments (Balkan et al,
1994)), evaluation of answers to clinical questions
crosses this boundary and requires the availability
of carefully generated sentence fragments as well as
suitable document collections.
2http://www.primeanswers.org/primeanswers/
3http://www.parkhurstexchange.com/qa/index.php
4http://www.clinicalevidence.com/ceweb/conditions/index.jsp
21
2 Test suite structure
The multi-tiered answer model of the FPIN and
BMJ-CE resources is adapted in this work. The top
tier contains the ?bottom-line advice?. FPIN pro-
vides the key-points of the advice in the form of a
short sentence sequence, whereas BMJ-CE provides
a list of sentence fragments (see Figure 1). Both
sources employ experts in question areas to care-
fully construct the answers. The second tier elab-
orates each of the key-points in 2-3 paragraph-long
summaries generated by the same experts. The third
tier provides references to the original sources used
in answer compilation.
Likely to be beneficial:
? Angiotensin converting enzyme inhibitors
? Aspirin
? ? Blockers . . .
Trade-off between benefits and harms:
? Nitrates (in the absence of thrombolysis)
Likely to be ineffective or harmful: . . .
Figure 1: The top tier of a multi-tiered answer to the clin-
ical question How to improve outcomes in acute myocar-
dial infarction? contains key-points generated by a panel
of cardiologists.
3 Using the test suite in an evaluation
The answer presented in Figure 1 can be used to
evaluate a system?s answer to this question by ex-
tracting the reference list from the FPIN or BMJ-CE
answer. Similarly, the second-tier summaries can be
used to evaluate the context for the key-points gener-
ated by a system. The references can be used to eval-
uate the quality of the original sources retrieved by a
system if the documents in both lists are represented
using their unique identifiers: DOI or a PubMed5
identifier. Availability of these test suites provides
for the following evaluation forms:
? diagnostic, in which developers could evaluate
how a tier is affected by changes in its own
module(s) or in the underlying tiers;
5http://www.ncbi.nlm.nih.gov/sites/entrez
? task-oriented, in which the system is evaluated
as a whole on its ability to answer clinical ques-
tions.
It is conceivable to evaluate a system as a whole
by evaluating its performance in each tier and then
combining the results. In a task-oriented evalua-
tion, it seems reasonable to evaluate the quality of
the first-tier answer and verify the adequacy of the
second-tier context.
3.1 Caveats
Even the simplest case of the top-tier evaluation,
checking the list of fragments generated by a sys-
tem against the reference list, ideally should be con-
ducted manually by a person with biomedical back-
ground. For example, Acetylsalicylic acid in a sys-
tem?s answer needs to be matched to Aspirin in the
reference list. Automation of this step is possible
through mapping of both lists to an ontology, e.g.,
UMLS6, but such evaluation will be significantly
less accurate and potentially biased (if a system uses
the same mapping algorithm to find the answer).
A manual evaluation based on 30 of 54 BMJ-CE
question-answer pairs in the presented test suite is
described in (Demner-Fushman and Lin, 2006). An-
other 50 question-answer pairs originated in FPIN
and PE.
References
Cramer I., Leidner J.L. and Klakow D. 2006. Building
an Evaluation Corpus for GermanQuestion Answering
by Harvesting Wikipedia. LREC-2006, Genoa, Italy.
Cohen K.B., Tanabe L., Kinoshita S., and Hunter L.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. HLT-NAACL 2004 Workshop: Biolink 2004,
Boston, Massachusetts
Balkan L., Netter K., Arnold D. and Meijer S. 1994.
TSNLP. Test Suites for Natural Language Processing.
Language Engineering Convention, Paris, France.
Ely J.W., Osheroff J.A., Chambliss M.L., Ebell M.H.
and Rosenbaum M.E. 2005. Answering Physicians?
Clinical Questions: Obstacles and Potential Solutions.
JAMIA, 12(2):217?224.
Demner-Fushman D. and Lin J. 2006. Answer Extrac-
tion, Semantic Clustering, and Extractive Summariza-
tion for Clinical Question Answering. ACL 2006, Syd-
ney, Australia
6http://www.nlm.nih.gov/research/umls/
22
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 737?744,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Using Non-lexical Features to Identify Effective Indexing Terms for
Biomedical Illustrations
Matthew Simpson, Dina Demner-Fushman, Charles Sneiderman,
Sameer K. Antani, George R. Thoma
Lister Hill National Center for Biomedical Communications
National Library of Medicine, NIH, Bethesda, MD, USA
{simpsonmatt, ddemner, csneiderman, santani, gthoma}@mail.nih.gov
Abstract
Automatic image annotation is an attrac-
tive approach for enabling convenient ac-
cess to images found in a variety of docu-
ments. Since image captions and relevant
discussions found in the text can be useful
for summarizing the content of images, it
is also possible that this text can be used to
generate salient indexing terms. Unfortu-
nately, this problem is generally domain-
specific because indexing terms that are
useful in one domain can be ineffective
in others. Thus, we present a supervised
machine learning approach to image an-
notation utilizing non-lexical features1 ex-
tracted from image-related text to select
useful terms. We apply this approach to
several subdomains of the biomedical sci-
ences and show that we are able to reduce
the number of ineffective indexing terms.
1 Introduction
Authors of biomedical publications often utilize
images and other illustrations to convey informa-
tion essential to the article and to support and re-
inforce textual content. These images are useful
in support of clinical decisions, in rich document
summaries, and for instructional purposes. The
task of delivering these images, and the publica-
tions in which they are contained, to biomedical
clinicians and researchers in an accessible way is
an information retrieval problem.
Current research in the biomedical domain (e.g.,
Antani et al, 2008; Florea et al, 2007), has in-
vestigated hybrid approaches to image retrieval,
combining elements of content-based image re-
trieval (CBIR) and annotation-based image re-
trieval (ABIR). ABIR, compared to the image-
1Non-lexical features describe attributes of image-related
text but not the text itself, e.g., unlike a bag-of-words model.
only approach of CBIR, offers a practical advan-
tage in that queries can be more naturally specified
by a human user (Inoue, 2004). However, manu-
ally annotating biomedical images is a laborious
and subjective task that often leads to noisy results.
Automatic image annotation is a more robust
approach to ABIR than manual annotation. Un-
fortunately, automatically selecting the most ap-
propriate indexing terms is an especially challeng-
ing problem for biomedical images because of
the domain-specific nature of these images and
the many vocabularies used in the biomedical sci-
ences. For example, the term ?sweat gland adeno-
carcinoma? could be a useful indexing term for an
image found in a dermatology publication, but it is
less likely to have much relevance in describing an
image from a cardiology publication. On the other
hand, the term ?mitral annular calcification? may
be of great relevance for cardiology images, but of
little relevance for dermatology ones.
Our problem may be summarized as follows:
Given an image, its caption, its discussion in the
article text (henceforth the image mention), and a
list of potential indexing terms, select the terms
that are most effective at describing the content of
the image. For example, assume the image shown
in Figure 1, obtained from the article ?Metastatic
Hidradenocarcinoma: Efficacy of Capecitabine?
by Thomas et al (2006) in Archives of Dermatol-
ogy, has the following potential indexing terms,
? Histopathology finding
? Reviewed
? Confirmation
? Diagnosis aspect
? Diagnosis
? Eccrine
? Sweat gland adenocarcinoma
? Lesion
which have been extracted from the image men-
tion. While most of these do not uniquely identify
737
Caption: Figure 1. On recurrence, histologic features
of porocarcinoma with an intraepidermal spread of
neoplastic clusters (hematoxylin-eosin, original magni-
fication x100).
Mention: Histopathologic findings were reviewed
and confirmed a diagnosis of eccrine hidradenocarci-
noma for all lesions excised (Figure 1).
Figure 1: Example Image. We index an image
with concepts generated from its caption and dis-
cussion in the document text (mention). This im-
age is from ?Metastatic Hidradenocarcinoma: Ef-
ficacy of Capecitabine? by Thomas et al (2006)
and is reprinted with permission from the authors.
the image, we would like to automatically select
?sweat gland adenocarcinoma? and ?eccrine? for
indexing because they clearly describe the content
and purpose of the image?supporting a diagno-
sis of hidradenocarinoma, an invasive cancer of
sweat glands. Note that effective indexing terms
need not be exact lexical matches of the text. Even
though ?diagnosis? is an exact match, its meaning
is too broad in this context to be a useful term.
In a machine learning approach to image anno-
tation, training data based on lexical features alone
is not sufficient for finding salient indexing terms.
Indeed, we must classify terms that are not en-
countered while training. Therefore, we hypoth-
esize that non-lexical features, which have been
successfully used for speech and genre classifica-
tion tasks, among others (see Section 5 for related
work), may be useful in classifying text associated
with images. While this approach is broad enough
to apply to any retrieval task, given the goals of our
ongoing research, we restrict ourselves to studying
its feasibility in the biomedical domain.
In order to achieve this, we make use of the
previously developed MetaMap (Aronson, 2001)
tool, which maps text to concepts contained in
the Unified Medical Language System R? (UMLS)
Metathesaurus R? (Lindberg et al, 1993). The
UMLS is a compendium of several controlled vo-
cabularies in the biomedical sciences that provides
a semantic mapping relating concepts from the
various vocabularies (Section 2). We then use a su-
pervised machine learning approach, described in
Section 3, to classify the UMLS concepts as useful
indexing terms based on their non-lexical features,
gleaned from the article text and MetaMap output.
Experimental results, presented in Section 4, in-
dicate that ineffective indexing terms can be re-
duced using this classification technique. We con-
clude that ABIR approaches to biomedical im-
age retrieval as well as hybrid CBIR/ABIR ap-
proaches, which rely on both image content and
annotations, can benefit from an automatic anno-
tation process utilizing non-lexical features to aid
in the selection of useful indexing terms.
2 Image Retrieval: Recent Work
Automatic image annotation is a broad topic, and
the automatic annotation of biomedical images,
specifically, has been a frequent component of
the ImageCLEF2 cross-language image retrieval
workshop. In this section, we describe previous
work in biomedical image retrieval that forms the
basis of our approach. Refer to Section 5 for work
related to our method in general.
Demner-Fushman et al (2007) developed a ma-
chine learning approach to identify images from
biomedical publications that are relevant to clin-
ical decision support. In this work, the authors
utilized both image and textual features to clas-
sify images based on their usefulness in evidence-
based medicine. In contrast, our work is focused
on selecting useful biomedical image indexing
terms; however, we utilize the methods developed
in their work to extract images and their related
captions and mentions.
Authors of biomedical publications often as-
semble multiple images into a single multi-panel
figure. Antani et al (2008) developed a unique
two-phase approach for detecting and segmenting
these figures. The authors rely on cues from cap-
tions to inform an image analysis algorithm that
determines panel edge information. We make use
of this approach to uniquely associate caption and
mention text with a single image.
2http://imageclef.org/
738
Our current work most directly stems from the
results of a term extraction and image annota-
tion evaluation performed by Demner-Fushman
et al (2008). In this study, the authors uti-
lized MetaMap to extract potential indexing terms
(UMLS concepts) from image captions and men-
tions. They then asked a group of five physicians
and one medical imaging specialist (four of whom
are trained in medical informatics) to manually
classify each concept as being ?useful for index-
ing? its associated images or ineffective for this
purpose. The reviewers also had the opportunity
to identify additional indexing terms that were not
automatically extracted by MetaMap.
In total, the reviewers evaluated 4006 concepts
(3,281 of which were unique), associated with
186 images from 109 different biomedical articles.
Each reviewer was given 50 randomly chosen im-
ages from the 2006?2007 issues of Archives of Fa-
cial Plastic Surgery3 and Cardiovascular Ultra-
sound4. Since MetaMap did not automatically ex-
tract all of the useful indexing terms, this selection
process exhibited high recall averaging 0.64 but
a low precision of 0.11. Indeed, assuming all the
extracted terms were selected for indexing, this re-
sults in an average F1-score of only 0.182 for the
classification problem. Our work is aimed at im-
proving this baseline classification by reducing the
number of ineffective terms selected for indexing.
3 Term Selection Method
A pictorial representation of our term extraction
and selection process is shown in Figure 2. We
rely on the previously described methods to ex-
tract images and their corresponding captions and
mentions, and the MetaMap tool to map this text
to UMLS concepts. These concepts are potential
indexing terms for the associated image.
We derive term features from various textual
items, such as the preferred name of the UMLS
concept, the MetaMap output for the concept, the
text that generated the concept, the article contain-
ing the image, and the document collection con-
taining the article. These are all described in more
detail in Section 3.2. Once the feature vectors are
built, we automatically classify the term as either
being useful for indexing the image or not.
To select useful indexing terms, we trained a
binary classifier, described in Section 3.3, in a
3http://archfaci.ama-assn.org/
4http://www.cardiovascularultrasound.com/
Figure 2: Term Extraction and Selection. We
gather features for the extracted terms and use
them to train a classifier that selects the terms that
are useful for indexing the associated images.
supervised learning scenario with data obtained
from the previous study by Demner-Fushman et al
(2008). We obtained our evaluation data from the
2006 Archives of Dermatology5 journal. Note that
our training and evaluation data represent distinct
subdomains of the biomedical sciences.
In order to reduce noise in the classification of
our evaluation data, we asked two of the review-
ers who participated in the initial study to man-
ually classify our extracted terms as they did for
our training data. In doing so, they each eval-
uated an identical set of 1539 potential indexing
terms relating to 50 randomly chosen images from
31 different articles. We measured the perfor-
mance of our classifier in terms of how well it per-
formed against this manual evaluation. These re-
sults, as well as a discussion pertaining to the inter-
annotator agreement of the two reviewers, are pre-
sented in Section 4.
Since our general approach is not specific to the
biomedical domain, it could equally be applied in
5http://archderm.ama-assn.org/
739
any domain with an existing ontology. For exam-
ple, the UMLS and MetaMap can be replaced by
the Art and Architecture Thesaurus6 and an equiv-
alent mapping tool to annotate images related to
art and art history (Klavans et al, 2008).
3.1 Terminology
To describe our features, we adopt the following
terminology.
? A collection contains all the articles from a
given publication for a specified number of
years. For example, the 2006?2007 issues of
Cardiovascular Ultrasound represent a sin-
gle collection.
? A document is a specific biomedical article
from a particular collection and contains im-
ages and their captions and mentions.
? A phrase is the portion of text that MetaMap
maps to UMLS concepts. For example, from
the caption in Figure 1, the noun phrase ?his-
tologic features? maps to four UMLS con-
cepts: ?Histologic,? ?Characteristics,? ?Pro-
tein Domain? and ?Array Feature.?
? A mapping is an assignment of a phrase to
a particular set of UMLS concepts. Each
phrase can have more than one mapping.
3.2 Features
Using this terminology, we define the following
features used to classify potential indexing terms.
We refer to these as non-lexical features because
they generally characterize UMLS concepts, go-
ing beyond the surface representation of words
and lexemes appearing in the article text.
F.1 CUI (nominal): The Concept Unique Iden-
tifier (CUI) assigned to the concept in the
UMLS Metathesaurus. We choose the con-
cept identifier as a feature because some fre-
quently mapped concepts are consistently
ineffective for indexing the images in our
training and evaluation data. For exam-
ple, the CUI for ?Original,? another term
mapped from the caption shown in Figure
1, is ?C0205313.? Our results indicate that
?C0205313,? which occurs 19 times in our
evaluation data, never identifies a useful in-
dexing term.
6http://www.getty.edu/research/conducting research/
vocabularies/aat/
F.2 Semantic Type (nominal): The concept?s se-
mantic categorization. There are currently
132 different semantic types7 in the UMLS
Metathesaurus. For example, The semantic
type of ?Original? is ?Idea or Concept.?
F.3 Presence in Caption (nominal): true if the
phrase that generated the concept is located
in the image caption; false if the phrase is
located in the image mention.
F.4 MeSH Ratio (real): The ratio of words ci in
the concept c that are also contained in the
Medical Subject Headings (MeSH terms)8
M assigned to the document to the total
number of words in the concept.
R(m) =
|{ci : ci ?M}|
|c|
(1)
MeSH is a controlled vocabulary created by
the US National Library of Medicine (NLM)
to index biomedical articles. For example,
?Adenoma, Sweat? is one MeSH term as-
signed to ?Metastatic Hidradenocarcinoma:
Efficacy of Capecitabine? (Thomas et al,
2006), the article containing the image from
Figure 1.
F.5 Abstract Ratio (real): The ratio of words
ci in the concept c that are also in the doc-
ument?s abstract A to the total number of
words in the concept.
R(a) =
|{ci : ci ? A}|
|c|
(2)
F.6 Title Ratio (real): The ratio of words ci in
the concept c that are also in the document?s
title T to the total number of words in the
concept.
R(t) =
|{ci : ci ? T }|
|c|
(3)
F.7 Parts-of-Speech Ratio (real): The ratio of
words pi in the phrase p that have been
tagged as having part of speech s to the total
number of words in the phrase.
R(s) =
|{pi : TAG(pi) = s}|
|p|
(4)
This feature is computed for noun, verb, ad-
jective and adverb part-of-speech tags. We
7http://www.nlm.nih.gov/research/umls/META3 current
semantic types.html
8http://www.nlm.nih.gov/mesh/
740
obtain tagging information from the output
of MetaMap.
F.8 Concept Ambiguity (real): The ratio of the
number of mappingsmi of phrase p that con-
tain concept c to the total number of map-
pings for the phrase:
A =
|{mpi : c ? m
p
i }|
|mp|
(5)
F.9 Tf-idf (real): The frequency of term ti (i.e.,
the phrase that generated the concept) times
its inverse document frequency:
tfidfi,j = tfi,j ? idfi (6)
The term frequency tfi,j of term ti in docu-
ment dj is given by
tfi,j =
ni,j
?|D|
k=1 nk,j
(7)
where ni,j is the number of occurrences of ti
in dj , and the denominator is the number of
occurrences of all terms in dj . The inverse
document frequency idfi of ti is given by
idfi = log
|D|
|{dj : ti ? dj}|
(8)
where |D| is the total number of documents
in the collection, and the denominator is the
total number of documents that contain ti
(see Salton and Buckley, 1988).
F.10 Document Location (real): The location in
the document of the phrase that generated
the concept. This feature is continuous on
[0, 1] with 0 representing the beginning of
the document and 1 representing the end.
F.11 Concept Length (real): The length of the
concept, measured in number of characters.
For the purpose of computing F.9 and F.10, we in-
dexed each collection with the Terrier9 informa-
tion retrieval platform. Terrier was configured to
use a block indexing scheme with a Tf-idf weight-
ing model. Computation of all other features is
straightforward.
3.3 Classifier
We explored these feature vectors using various
classification approaches available in the Rapid-
Miner10 tool. Unlike many similar text and image
9http://ir.dcs.gla.ac.uk/terrier/
10http://rapid-i.com/
classification problems, we were unable to achieve
results with a Support Vector Machine (SVM)
learner (libSVMLearner) using the Radial Base
Function (RBF). Common cost and width parame-
ters were used, yet the SVM classified all terms as
ineffective. Identical results were observed using
a Na??ve Bayes (NB) learner.
For these reasons, we chose to use the Aver-
aged One-Dependence Estimator (AODE) learner
(Webb et al, 2005) available in RapidMiner.
AODE is capable of achieving highly accurate
classification results with the quick training time
usually associated with NB. Because this learner
does not handle continuous attributes, we pre-
processed our features with equal frequency dis-
cretization. The AODE learner was trained in a
ten-fold cross validation of our training data.
4 Results
Results relating to specific aspects of our work
(annotation, features and classification) are pre-
sented below.
4.1 Inter-Annotator Agreement
Two independent reviewers manually classified
the extracted terms from our evaluation data as
useful for indexing their associated images or not.
The inter-annotator agreement between reviewers
A and B is shown in the first row of Table 1. Al-
though both reviewers are physicians trained in
medical informatics, their initial agreement is only
moderate, with ? = 0.519. This illustrates the
subjective nature of manual ABIR and, in general,
the difficultly in reliably classifying potential in-
dexing terms for biomedical images.
Annotator Pr(a) Pr(e) ?
A/B 0.847 0.682 0.519
A/Standard 0.975 0.601 0.938
B/Standard 0.872 0.690 0.586
Table 1: Inter-annotator Agreement. The prob-
ability of agreement Pr(a), expected probability of
chance agreement Pr(e), and the associated Co-
hen?s kappa coefficient ? are given for each re-
viewer combination.
After their initial classification, the two review-
ers were instructed to collaboratively reevaluate
the subset of extracted terms upon which they dis-
agreed (roughly 15% of the terms) and create a
741
Feature Gain ?2
F.1 CUI 0.003 13.331
F.2 Semantic Type 0.015 68.232
F.3 Presence in Caption 0.008 35.303
F.4 MeSH Ratio 0.043 285.701
F.5 Abstract Ratio 0.023 114.373
F.6 Title Ratio 0.021 132.651
F.7 Noun Ratio 0.053 287.494
Verb Ratio 0.009 26.723
Adjective Ratio 0.021 96.572
Adverb Ratio 0.002 5.271
F.8 Concept Ambiguity 0.008 33.824
F.9 Tf-idf 0.004 21.489
F.10 Document Location 0.002 12.245
F.11 Phrase Length 0.021 102.759
Table 2: Feature Comparison. The information
gain and chi-square statistic is shown for each fea-
ture. A higher score indicates greater influence on
term effectiveness.
gold standard evaluation. The second and third
rows of Table 1 suggest the resulting evaluation
strongly favors reviewer A?s initial classification
compared to that of reviewer B.
Since the reviewers of the training data each
classified terms from different sets of randomly
selected images, it is impossible to calculate their
inter-annotator agreement.
4.2 Effectiveness of Features
The effectiveness of individual features in describ-
ing the potential indexing terms is shown in Ta-
ble 2. We used two measures, both of which in-
dicate a similar trend, to calculate feature effec-
tiveness: Information gain (Kullback-Leibler di-
vergence) and the chi-square statistic.
Under both measures, the MeSH ratio (F.4) is
one of the most effective features. This makes
intuitive sense because MeSH terms are assigned
to articles by specially trained NLM profession-
als. Given the large size of the MeSH vocabu-
lary, it is not unreasonable to assume that an arti-
cle?s MeSH terms could be descriptive, at a coarse
granularity, of the images it contains. Also, the
subjectivity of the reviewers? initial data calls into
question the usefulness of our training data. It
may be that MeSH terms, consistently assigned
to all documents in a particular collection, are a
more reliable determiner of the usefulness of po-
tential indexing terms. Furthermore, the study by
Demner-Fushman et al (2008) found that, on aver-
age, roughly 25% of the additional (useful) terms
the reviewers added to the set of extracted terms
were also found in the MeSH terms assigned to
the document containing the particular image.
The abstract and title ratios (F.6 and F.5) also
had a significant effect on the classification out-
come. Similar to the argument for MeSH terms, as
these constructs are a coarse summary of the con-
tents of an article, it is not unreasonable to assume
they summarize the images contained therein.
Finally, the noun ratio (F.7) was a particularly
effective feature, and the length of the UMLS con-
cept (F.11) was moderately effective. Interest-
ingly, tf-idf and document location (F.9 and F.10),
both features computed using standard informa-
tion retrieval techniques, are among the least ef-
fective features.
4.3 Classification
While the AODE learner performed reasonably
well for this task, the difficulty encountered when
training the SVM learner may be explained as
follows. The initial inter-annotator agreement
of the evaluation data suggests that it is likely
that our training data contained contradictory or
mislabeled observations, preventing the construc-
tion of a maximal-margin hyperplane required by
the SVM. An SVM implementation utilizing soft
margins (Cortes and Vapnik, 1995) would likely
achieve better results on our data, although at the
expense of greater training time. The success of
the AODE learner in this case is probably due to
its resilience to mislabeled observations.
Annotator Precision Recall F1-score
A 0.258 0.442 0.326
B 0.200 0.225 0.212
Combined 0.326 0.224 0.266
Standard 0.453 0.229 0.304
Standarda 0.492 0.231 0.314
Training 0.502 0.332 0.400
Table 3: Classification Results. The classifier?s
precision and recall, as well as the corresponding
F1-score, are given for the responses of each re-
viewer.
aFor comparison, the classifier was also trained using the
subset of training data containing responses from reviewers
A and B only.
742
Classification results are shown in Table 3. The
precision and recall of the classification scheme is
shown for the manual classification by reviewers
A and B in the first and second rows. The third
row contains the results obtained from combining
the results of the two reviewers, and the fourth row
shows the classification results compared to the
gold standard obtained after discovering the initial
inter-annotator agreement.
We hypothesized that the training data labels
may have been highly sensitive to the subjectiv-
ity of the reviewers. Therefore, we retrained the
learner with only those observations made by re-
viewers A and B (of the five total reviewers) and
again compared the classification results with the
gold standard. Not surprisingly, the F1-score of
this classification (shown in the fifth row) is some-
what improved compared to that obtained when
utilizing the full training set.
The last row in Table 3 shows the results of clas-
sifying the training data. That is, it shows the re-
sults of classifying one tenth of the data after a ten-
fold cross validation and can be considered an up-
per bound for the performance of this classifier on
our evaluation data. Notice that the associated F1-
score for this experiment is only marginally bet-
ter than that of the unseen data. This implies that
it is possible to use training data from particular
subdomains of the biomedical sciences (cardiol-
ogy and plastic surgery) to classify potential in-
dexing terms in other subdomains (dermatology).
Overall, the classifier performed best when ver-
ified with reviewer A, with an F1-score of 0.326.
Although this is relatively low for a classification
task, these results improve upon the baseline clas-
sification scheme (all extracted terms are useful
for indexing) with an F1-score of 0.182 (Demner-
Fushman et al, 2008). Thus, non-lexical features
can be leveraged, albeit to a small degree with
our current features and classifier, in automatically
selecting useful image indexing terms. In future
work, we intend to explore additional features and
alternative tools for mapping text to the UMLS.
5 Related Work
Non-lexical features have been successful in many
contexts, particularly in the areas of genre classifi-
cation and text and speech summarization.
Genre classification, unlike text classification,
discriminates between document style instead of
topic. Dewdney et al (2001) show that non-lexical
features, such as parts of speech and line-spacing,
can be successfully used to classify genres, and
Ferizis and Bailey (2006) demonstrate that accu-
rate classification of Internet documents is possi-
ble even without the expensive part-of-speech tag-
ging of similar methods. Recall that the noun ratio
(F.7) was among the most effective of our features.
Finn and Kushmerick (2006) describe a study
in which they classified documents from various
domains as ?subjective? or ?objective.? They, too,
found that part-of-speech statistics as well as gen-
eral text statistics (e.g., average sentence length)
are more effective than the traditional bag-of-
words representation when classifying documents
from multiple domains. This supports the notion
that we can use non-lexical features to classify po-
tential indexing terms in one biomedical subdo-
main using training data from another.
Maskey and Hirschberg (2005) found that
prosodic features (see Ward, 2004) combined with
structural features are sufficient to summarize spo-
ken news broadcasts. Prosodic features relate to
intonational variation and are associated with par-
ticularly important items, whereas structural fea-
tures are associated with the organization of a typ-
ical broadcast: headlines, followed by a descrip-
tion of the stories, etc.
Finally, Schilder and Kondadadi (2008) de-
scribe non-lexical word-frequency features, sim-
ilar to our ratio features (F.4?F.7), which are
used with a regression SVM to efficiently gener-
ate query-based multi-document summaries.
6 Conclusion
Images convey essential information in biomedi-
cal publications. However, automatically extract-
ing and selecting useful indexing terms from the
article text is a difficult task given the domain-
specific nature of biomedical images and vocab-
ularies. In this work, we use the manual classifi-
cation results of a previous study to train a binary
classifier to automatically decide whether a poten-
tial indexing term is useful for this purpose or not.
We use non-lexical features generated for each
term with the most effective including whether the
term appears in the MeSH terms assigned to the
article and whether it is found in the article?s ti-
tle and caption. While our specific retrieval task
relates to the biomedical domain, our results in-
dicate that ABIR approaches to image retrieval in
any domain can benefit from an automatic annota-
743
tion process utilizing non-lexical features to aid in
the selection of indexing terms or the reduction of
ineffective terms from a set of potential ones.
References
Sameer Antani, Dina Demner-Fushman, Jiang Li,
Balaji V. Srinivasan, and George R. Thoma.
2008. Exploring use of images in clinical ar-
ticles for decision support in evidence-based
medicine. In Proc. of SPIE-IS&T Electronic
Imaging, pages 1?10.
Alan R. Aronson. 2001. Effective mapping of
biomedical text to the UMLS metathesaurus:
The MetaMap program. In Proc. of the Annual
Symp. of the American Medical Informatics As-
sociation (AMIA), pages 17?21.
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273?297.
Dina Demner-Fushman, Sameer Antani, Matthew
Simpson, and George Thoma. 2008. Combin-
ing medical domain ontological knowledge and
low-level image features for multimedia index-
ing. In Proc. of the Language Resources for
Content-Based Image Retrieval Workshop (On-
toImage), pages 18?23.
Dina Demner-Fushman, Sameer K. Antani, and
George R. Thoma. 2007. Automatically finding
images for clinical decision support. In Proc. of
the Intl. Workshop on Data Mining in Medicine
(DM-Med), pages 139?144.
Nigel Dewdney, Carol VanEss-Dykema, and
Richard MacMillan. 2001. The form is the sub-
stance: Classification of genres in text. In Proc.
of the Workshop on Human Language Technol-
ogy and Knowledge Management, pages 1?8.
George Ferizis and Peter Bailey. 2006. Towards
practical genre classification of web documents.
In Proc. of the Intl. Conference on the World
Wide Web (WWW), pages 1013?1014.
Aidan Finn and Nicholas Kushmerick. 2006.
Learning to classify documents according to
genre. Journal of the American Society for
Information Science and Technology (JASIST),
57(11):1506?1518.
F. Florea, V. Buzuloiu, A. Rogozan, A. Bensrhair,
and S. Darmoni. 2007. Automatic image an-
notation: Combining the content and context of
medical images. In Intl. Symp. on Signals, Cir-
cuits and Systems (ISSCS), pages 1?4.
Masashi Inoue. 2004. On the need for annotation-
based image retrieval. In Proc. of the Workshop
on Information Retrieval in Context (IRiX),
pages 44?46.
Judith Klavans, Carolyn Sheffield, Eileen Abels,
Joan Beaudoin, Laura Jenemann, Tom Lipin-
cott, Jimmy Lin, Rebecca Passonneau, Tandeep
Sidhu, Dagobert Soergel, and Tae Yano. 2008.
Computational linguistics for metadata build-
ing: Aggregating text processing technologies
for enhanced image access. In Proc. of the Lan-
guage Resources for Content-Based Image Re-
trieval Workshop (OntoImage), pages 42?47.
D.A. Lindberg, B.L. Humphreys, and A.T. Mc-
Cray. 1993. The unified medical language
system. Methods of Information in Medicine,
32(4):281?291.
Sameer Maskey and Julia Hirschberg. 2005.
Comparing lexical, acoustic/prosodic, struc-
tural and discourse features for speech sum-
marization. In Proc. of the European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH), pages 621?624.
Gerard Salton and Christopher Buckley. 1988.
Term-weighting approaches in automatic text
retrieval. Information Processing & Manage-
ment, 24(5):513?523.
Frank Schilder and Ravikumar Kondadadi. 2008.
FastSum: Fast and accurate query-based multi-
document summarization. In Proc. of the
Workshop on Human Language Technology and
Knowledge Management, pages 205?208.
Jouary Thomas, Kaiafa Anastasia, Lipinski
Philippe, Vergier Be?atrice, Lepreux Se?bastien,
Delaunay Miche`le, and Ta??ebAlain. 2006.
Metastatic hidradenocarcinoma: Efficacy of
capecitabine. Archives of Dermatology,
142(10):1366?1367.
Nigel Ward. 2004. Pragmatic functions of
prosodic features in non-lexical utterances.
In Proc. of the Intl. Conference on Speech
Prosody, pages 325?328.
Geoffrey I. Webb, Janice R. Boughton, and Zhihai
Wang. 2005. Not so na??ve bayes: Aggregating
one-dependence estimators. Machine Learning,
58(1):5?24.
744
Proceedings of NAACL HLT 2009: Short Papers, pages 41?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Automatic Image Region Annotation - Image Region Textual
Coreference Resolution
Emilia Apostolova
College of Computing and Digital Media
DePaul University
Chicago, IL 60604, USA
emilia.aposto@gmail.com
Dina Demner-Fushman
Communications Engineering Branch
National Library of Medicine
Bethesda, MD 20894, USA
ddemner@mail.nih.gov
Abstract
Detailed image annotation necessary for reli-
able image retrieval involves not only annotat-
ing the image as a single artifact, but also an-
notating specific objects or regions within the
image. Such detailed annotation is a costly en-
deavor and the available annotated image data
are quite limited. This paper explores the fea-
sibility of using image captions from scientific
journals for the purpose of automatically an-
notating image regions. Salient image clues,
such as an object location within the image or
an object color, together with the associated
explicit object mention, are extracted and clas-
sified using rule-based and SVM learners.
1 Introduction
The profusion of digitally available images has nat-
urally led to an interest in the field of automatic im-
age annotation and retrieval. A number of studies
attempt to associate image regions with the corre-
sponding concepts. In (Duygulu et al, 2002), for
example, the problem of annotation is treated as a
translation from a set of image segments (or blobs)
to a set of words. Modeling the association between
blobs and words for the purpose of automated an-
notation has also been proposed by (Barnard et al,
2003; Jeon et al, 2003).
A recurring hindrance that appears in studies aim-
ing at automatic image region annotation is the lack
of an appropriate dataset. All of the above studies
use the Corel image dataset that consists of 60,000
images annotated with 3 to 5 keywords. The need
for an image dataset with annotated image regions
has been recognized by many researchers. For ex-
ample, Russell et al(2008) have developed a tool
and a general purpose image database designed to
delineate and annotate objects within image scenes.
The need for an image dataset with annotated ob-
ject boundaries appears to be especially pertinent in
the biomedical field. Organizing and using for re-
search the available medical imaging data proved to
be a challenge and a goal of the ongoing research.
Rubin et al(2008), for example, propose an ontol-
ogy and annotation tool for semantic annotation of
image regions in radiology.
However, creating a dataset of image regions
manually annotated and delineated by domain ex-
perts, is a costly enterprise. Any attempts to auto-
mate or semi-automate the process would be of a
substantial value.
This work proposes an approach towards auto-
matic annotation of regions of interest in images
used in scientific publications. Publications abun-
dant in image data are an untapped source of an-
notated image data. Due to publication standards,
meaningful image captions are almost always pro-
vided within scientific articles. In addition, image
Regions of Interest (ROIs) are commonly referred to
within the image caption. Such ROIs are also com-
monly delineated with some kind of an overlay that
helps locating the ROI. This is especially true for
hard to interpret scientific images such as radiology
images. ROIs are also described in terms of location
within the image, or by the presence of a particular
color. Identifying ROI mentions within image cap-
tions and visual clues pinpointing the ROI within the
image would be the first step in building an object
41
1. Object Location - explicit ROI location, e.g. front row, back-
ground, top, bottom, left, right.
Shells of planktonic animals called formainifera record cli-
matic conditions as they are formed. This one, Globigeri-
noides ruber, lives year-round at the surface of the Sargasso Sea.
The form of the live animal is shown at right, and its shell, which
is actually about the size of a fine grain of sand, at left.
2. Object Color - presence of a distinct color that identifies a
ROI.
Anterior SSD image shows an elongated splenorenal varix (blue
area). The varix travels from the splenic hilar region inferiorly
along the left flank, down into the pelvis, and eventually back up to
the left renal vein via the left gonadal vein. The kidney is encoded
yellow, the portal system is encoded magenta, and the spleen is
encoded tan.
3. Overlay Marker - an overlay marker used to pinpoint the loca-
tion of the ROI, e.g. arrows, asterisks, bounding boxes, or circles.
Transverse sonograms obtained with a 7.5-MHz linear trans-
ducer in the subareolar region. The straight arrows
show a dilated tubular structure. The curved arrow indicates
an intraluminal solid mass.
4. Overlay Label - an overlay label used to pinpoint the location
of the ROI, e.g. numbers, letters, words, abbreviations.
Location of the calf veins. Transverse US image just
above ankle demonstrates the paired posterior tibial veins (V)
and posterior tibial artery (A) imaged from a posteromedial ap-
proach. Note there is inadequate venous flow velocity to visualize
with color Doppler without flow augmentation.
Table 1: Image Markers divided into four categories, followed by
a sample image caption1 in which Image Markers are marked in bold,
Image Marker Referents are underlined.
delineated and annotated image dataset.
2 Problem Definition
The goal of this research is to locate visually salient
image region characteristics in the text surrounding
scientific images that could be used to facilitate the
delineation of the image object boundaries. This
task could be broken down into two related subtasks
- 1) locating and classifying textual clues for visu-
ally salient ROI features (Image Markers), and 2) lo-
cating the corresponding ROI text mentions (Image
Marker Referents). Table 1 gives a classification of
Image Markers including examples of Image Mark-
ers and Image Marker Referents. Figure 1 shows the
frequency of Image Marker occurrences.
1The captions were extracted from Radiology and Ra-
diographics c? Radiological Society of North America and
Oceanus c?Woods Hole Oceanographic Institution.
3 Related Work
Cohen et al(2003) attempt to identify what they
refer to as ?image pointers? within captions in
biomedical publications. The image pointers of in-
terest are, for example, image panel labels, or letters
and abbreviations used as an overlay within the im-
age, similar to the Overlay Labels described in Table
1. They developed a set of hand-crafted rules, and a
learning method involving Boosted Wrapper Induc-
tion on a dataset consisting of biomedical articles
related to fluorescence microscope images.
Deschacht and Moens (2007) analyze text sur-
rounding images in news articles trying to identify
persons and objects in the text that appear in the
corresponding image. They start by extracting per-
sons? names and visual objects using Named Entity
Recognition (NER) tools. Next, they measure the
?salience? of the extracted named entities within the
text with the assumption that more salient named en-
tities in the text will also be present in the accompa-
nying image.
Davis et al(2003) develop a NER tool to iden-
tify references to a single art object (for example a
specific building within an image) in text related to
art images for the purpose of automatic cataloging
of images. They take a semi-supervised approach to
locating the named entities of interest by first provid-
ing an authoritative list of art objects of interest and
then seeking to match variants of the seed named en-
tities in related text.
4 Experimental Methods and Results
4.1 Dataset
Figure 1: Distribution of Image
Marker types across 400 annotated
image captions.
The chosen date-
set contains more
than 60,000 images
together with their as-
sociated captions from
three online life and
earth sciences jour-
nals1. 400 randomly
selected image cap-
tions were manually
annotated by a single
annotator with their
Image Markers and Image Marker Referents and
used for testing and for cross-validation respectively
42
in the two methods described below.
4.2 Rule Based Approach
First, we developed a two-stage rule-based, boot-
strapping algorithm for locating the image markers
and their coreferents from unannotated data. The al-
gorithm is based on the observation that textual im-
age markers commonly appear in parentheses and
are usually closely related semantic concepts. Thus
the seed for the algorithm consists of:
1. The predominant syntactic pattern - parenthe-
ses, as in ?hooking of the soft palate (arrow)?. This
pattern could easily be captured by a regular expres-
sion and doesn?t require sentence parsing.
2. A dozen seed phrases (e.g ?left?, ?circle?, ?as-
terisk?, ?blue?) identified by initially annotating a
small subset of the data (20 captions). Wordnet was
used to look up and prepare a list of their corre-
sponding inherited hypernyms. This hypernym list
contains concepts such as ?a spatially limited lo-
cation?, ?a two-dimensional shape?, ?a written or
printed symbol?, ?a visual attribute of things that
results from the light they emit or transmit or re-
flect?. Best results were achieved when inherited hy-
pernyms up to the third parent were used.
In the first stage of the algorithm, all image cap-
tions were searched for parenthesized expressions
that share the seed hypernyms. This step of the al-
gorithm will result in high precision, but a low re-
call since image markers do not necessarily appear
in parentheses. To increase recall, in stage 2 a full
text search was performed for the stemmed versions
of the expressions identified in stage 1.
A baseline measure was also computed for the
identification of the Image Marker Referents using a
simple heuristic - the coreferent of the Image Marker
is usually the closest Noun Phrase (NP). In the case
of parenthesized image markers, it is the closest NP
to the left of the image marker; in the case of non-
parenthesized image markers, the referent is usually
the complement of the verb; and in the case of pas-
sive voice, the NP preceding the verb phrase. The
Stanford parser was used to parse the sentences.
Table 2 summarizes the results validated against
the annotated dataset (excluding the 20 captions
used to identify the seed phrases). It appears that the
relatively low accuracy for Image Marker Referent
identification was mostly due to parsing errors since
Precision Recall F1-score
Image Marker 87.70 68.10 76.66
Image Marker Referent Accuracy 59.10
Table 2: Rule-based approach results for Image Marker and Im-
age Marker Referent identification. Image Marker Referent results are
reported as accuracy because the algorithm involves locating an Image
Marker Referent for each Image Marker. Referent identification accu-
racy was computed for all annotated Image Markers.
Kind k-5 . . . k0 . . . k+5
Orth o-5 . . . o0 . . . o+5
Stem s-5 . . . s0 . . . s+5
Hypernym h-5 . . . h0 . . . h+5
Dep Path d-5 . . . d0 . . . d+5
Category [c0]
Table 3: Features from a surrounding token window are used to
classify the current token into category [c0]. Best results were achieved
with a five-token window.
the syntactic structure of the image caption texts is
quite distinct from the Penn Treebank dataset used
for training the Stanford parser.
4.3 Support Vector Machines
Next we explored the possibility of improving the
rule-based method results by applying a machine
learning technique on the set of annotated data. Sup-
port Vector Machines (SVM) (Vapnik, 2000) was
the approach taken because it is a state-of-the-art
classification approach proven to perform well on
many NLP tasks.
In our approach, each sentence was tokenized,
and tokens were classified as Beginning, Inside, or
Outside an Image Marker type or Image Marker Ref-
erent. Image Marker Referents are not related to Im-
age Markers and creating a classifier trained on this
task is planned as future work. SVM classifiers were
trained for each of these categories, and combined
via ?one-vs-all? classification (the category of the
classifier with the largest output was selected). Fea-
tures of the surrounding context are used as shown
in Table 3 and Table 4.
Table 5 summarizes the results of a 10-fold cross-
validation. SVM performed well overall for iden-
tifying Image Markers, Location being the hardest
because of higher variability of expressing ROI posi-
tion. Image Marker Referents are harder to classify,
43
Token Kind The general type of the sentence to-
ken (Word, Number, Symbol, Punctuation,
White space).
Orthography Orthographic categorization of the token
(Upper initial, All capitals, Lower case,
Mixed case).
Stem The stem of the token, extracted with the
Porter stemmer.
Wordnet Super-
class
Wordnet hypernyms (nouns, verbs); the hy-
pernym of the derivationally related form
(adjectives); the superclass of the pertanym
(adverbs).
POS Category POS categories extracted using Brill?s tag-
ger.
Dependency
Path*
The smallest sentence parse subtree includ-
ing both the current token and the anno-
tated image marker(s), encoded as an undi-
rected path across POS categories.
Table 4: Orthographic, semantic, and grammatical classification
features computed for each token (*Dependency Path is used only for
classifying Image Marker Referents).
as deeper syntactic knowledge is necessary. Idiosyn-
cratic syntactic structures in image captions pose
a problem for the general-purpose trained Stanford
parser and performance is hindered by the accuracy
of computing Dependency Path feature.
5 Conclusion and Future Work
We explored the feasibility of determining the con-
tent of ROIs in images from scientific publications
using image captions. We developed a two-stage
rule-based approach that utilizes WordNet to find
ROI pointers (Image Markers) and their referents.
We also explored a supervised machine learning ap-
proach. Both approaches are promising. The rule-
based approach seeded with a small manually an-
notated set resulted in 78.7% precision and 68.1%
recall for Image Markers recognition. The SVM ap-
proach (which requires a greater annotation effort)
outperformed the rule based approach (p=93.6%,
r=87.7%). Future plans include training SVMs on
the results of the rule-based annotation. Further
work is also needed in improving Image Marker
Referent identification and co-reference resolution.
We also plan to involve two annotators in order
to collect a more robust dataset based on inter-
annotator agreement.
References
K. Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D.M.
Blei, and M.I. Jordan. 2003. Matching words and
Precision Recall F1-score
Location 60.93 45.15 51.86
Color 100.00 51.32 67.82
Overlay Marker 97.43 95.39 96.39
Overlay Label 85.74 87.69 86.70
Overall 93.64 87.69 90.56
Image Marker Referent Accuracy 61.15
Table 5: SVM classification results for the four types of Image
Markers, and for Image Marker Referents. LibSVM software was used
(3-degree polynomial kernel, cost parameter = 1, ? = 0.6 empirically
determined).
pictures. The Journal of Machine Learning Research,
3:1107?1135.
W.W. Cohen, R. Wang, and R.F. Murphy. 2003. Un-
derstanding captions in biomedical publications. In
Proceedings of the 9th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 499?504. ACM New York, NY, USA.
P.T. Davis, D.K. Elson, and J.L. Klavans. 2003. Methods
for precise named entity matching in digital collec-
tions. In Proceedings of the 3rd ACM/IEEE-CS joint
conference on Digital libraries, pages 125?127. IEEE
Computer Society Washington, DC, USA.
K. Deschacht and M. Moens. 2007. Text analysis for au-
tomatic image annotation. In Proceedings of the 45th
Annual ACL Meeting, pages 1000?1007. ACL.
P. Duygulu, K. Barnard, JFG de Freitas, and D.A.
Forsyth. 2002. Object Recognition as Machine Trans-
lation: Learning a Lexicon for a Fixed Image Vocab-
ulary. LECTURE NOTES IN COMPUTER SCIENCE,
pages 97?112.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Au-
tomatic image annotation and retrieval using cross-
media relevance models. In Proceedings of the 26th
annual international ACM SIGIR conference on Re-
search and development in informaion retrieval, pages
119?126. ACM New York, NY, USA.
D. Rubin, P. Mongkolwat, V. Kleper, K. Supekar, and
D. Channin. 2008. Medical imaging on the Semantic
Web: Annotation and image markup. In AAAI Spring
Symposium Series, Semantic Scientific Knowledge In-
tegration.
B.C. Russell, A. Torralba, K.P. Murphy, and W.T. Free-
man. 2008. LabelMe: A Database and Web-Based
Tool for Image Annotation. International Journal of
Computer Vision, 77(1):157?173.
V.N. Vapnik. 2000. The Nature of Statistical Learning
Theory. Springer.
44
Answering Clinical Questions with
Knowledge-Based and Statistical Techniques
Dina Demner-Fushman?
University of Maryland, College Park
Jimmy Lin?
University of Maryland, College Park
The combination of recent developments in question-answering research and the availability of
unparalleled resources developed specifically for automatic semantic processing of text in the
medical domain provides a unique opportunity to explore complex question answering in the
domain of clinical medicine. This article presents a system designed to satisfy the information
needs of physicians practicing evidence-based medicine. We have developed a series of knowl-
edge extractors, which employ a combination of knowledge-based and statistical techniques, for
automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted
elements serve as the input to an algorithm that scores the relevance of citations with respect to
structured representations of information needs, in accordance with the principles of evidence-
based medicine. Starting with an initial list of citations retrieved by PubMed, our system
can bring relevant abstracts into higher ranking positions, and from these abstracts generate
responses that directly answer physicians? questions. We describe three separate evaluations: one
focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking
task, and finally, an evaluation of answers by two physicians. Experiments on a collection
of real-world clinical questions show that our approach significantly outperforms the already
competitive PubMed baseline.
1. Introduction
Recently, the focus of question-answering research has shifted away from simple fact-
based questions that can be answered with relatively little linguistic knowledge to
?harder? questions that require fine-grained text analysis, reasoning capabilities, and
the ability to synthesize information from multiple sources. General purpose reasoning
on anything other than superficial lexical relations is exceedingly difficult because there
is a vast amount of world knowledge that must be encoded, either manually or auto-
matically, to overcome the brittleness often associated with long chains of evidence. This
situation poses a serious bottleneck to ?advanced? question-answering systems. How-
ever, the availability of existing knowledge sources and ontologies in certain domains
provides exciting opportunities to experiment with knowledge-rich approaches. How
might one go about leveraging these resources effectively? How might one integrate
? Department of Computer Science and Institute for Advanced Computer Studies. E-mail:
demner@umd.edu.
? College of Information Studies, Department of Computer Science, and Institute for Advanced Computer
Studies. E-mail: jimmylin@umd.edu.
Submission received: 4 July 2005; revised submission received: 7 January 2006; accepted for publication:
12 April 2006.
? 2007 Association for Computational Linguistics
Computational Linguistics Volume 33, Number 1
statistical techniques to overcome the brittleness often associated with knowledge-
based approaches?
We explore these interesting research questions in the domain of medicine, fo-
cusing on the information needs of physicians in clinical settings. This domain is
well-suited for exploring the posed research questions for several reasons. First,
substantial understanding of the domain has already been codified in the Unified
Medical Language System (UMLS) (Lindberg, Humphreys, and McCray 1993). Sec-
ond, software for utilizing this ontology already exists: MetaMap (Aronson 2001)
identifies concepts in free text, and SemRep (Rindflesch and Fiszman 2003) extracts
relations between the concepts. Both systems utilize and propagate semantic infor-
mation from UMLS knowledge sources: the Metathesaurus, the Semantic Network,
and the SPECIALIST lexicon. The 2004 version of the UMLS Metathesaurus (used
in this work) contains information about over 1 million biomedical concepts and
5 million concept names from more than 100 controlled vocabularies. The Seman-
tic Network provides a consistent categorization of all concepts represented in the
UMLS Metathesaurus. Third, the paradigm of evidence-based medicine (Sackett et al
2000) provides a task-based model of the clinical information-seeking process. The
PICO framework (Richardson et al 1995) for capturing well-formulated clinical queries
(described in Section 2) can serve as the basis of a knowledge representation that
bridges the needs of clinicians and analytical capabilities of a system. The conflu-
ence of these many factors makes clinical question answering a very exciting area of
research.
Furthermore, the need to answer questions related to patient care at the point of
service has been well studied and documented (Covell, Uman, and Manning 1985;
Gorman, Ash, and Wykoff 1994; Ely et al 1999, 2005). MEDLINE, the authoritative
repository of abstracts from the medical and biomedical primary literature maintained
by the National Library of Medicine, provides the clinically relevant sources for answer-
ing physicians? questions, and is commonly used in that capacity (Cogdill and Moore
1997; De Groote and Dorsch 2003). However, studies have shown that existing systems
for searching MEDLINE (such as PubMed, the search service provided by the National
Library of Medicine) are often inadequate and unable to supply clinically relevant
answers in a timely manner (Gorman, Ash, and Wykoff 1994; Chambliss and Conley
1996). Furthermore, it is clear that traditional document retrieval technology applied
to MEDLINE abstracts is insufficient for satisfactory information access; research and
experience point to the need for systems that automatically analyze text and return
only the relevant information, appropriately summarizing and fusing segments from
multiple texts. Not only is clinical question answering interesting from a research
perspective, it also represents a potentially high-impact, real-world application of lan-
guage processing and information retrieval technology?better information systems to
provide decision support for physicians have the potential to improve the quality of
health care.
Our question-answering system supports the practice of evidence-based medi-
cine (EBM), a widely accepted paradigm for medical practice that stresses the impor-
tance of evidence from patient-centered clinical research in the health care process.
EBM prescribes an approach to structuring clinical information needs and identi-
fies elements (for example, the problem at hand and the interventions under con-
sideration) that factor into the assessment of clinically relevant studies for medical
practice. The foundation of our question-answering strategy is built on knowledge
extractors that automatically identify these elements in MEDLINE abstracts. Using
these knowledge extractors, we have developed algorithms for scoring the relevance
64
Demner-Fushman and Lin Answering Clinical Questions
of MEDLINE citations in accordance with the principles of EBM. Our scorer is em-
ployed to rerank citations retrieved by the PubMed search engine, with the goal of
bringing as many topically relevant abstracts to higher ranking positions as possible.
From this reranked list of citations, our system is then able to generate textual re-
sponses that directly address physicians? information needs. We evaluated our system
with a collection of real-world clinical questions and demonstrate that our combined
knowledge-based and statistical approach delivers significantly better document re-
trieval and question-answering performance, compared to systems used by physicians
today.
This article is organized in the following manner: We start in the next section with
an overview of evidence-based medicine and its basic principles. Section 3 provides an
overview of MEDLINE, the bibliographic database used by our system, and PubMed,
the public gateway for accessing this database. Section 4 describes our system architec-
ture and outlines our conception of clinical question answering as ?semantic unifica-
tion? between query frames and knowledge frames derived from MEDLINE citations.
The knowledge extractors that underlie our approach are described in Section 5, along
with intrinsic evaluations of each component. In Section 6, we detail an algorithm for
scoring the relevance of MEDLINE citations with respect to structured query represen-
tations. This scoring algorithm captures the principles of EBM and uses the results of
the knowledge extractors as basic features. To evaluate the performance of this citation
scoring algorithm, we have gathered a corpus of real-world clinical questions. Section 7
presents results from a document reranking experiment where our EBM scores were
used to rerank citations retrieved by PubMed. Section 8 provides additional details on
attempts to optimize the performance of our EBM citation scoring algorithm. Answer
generation, based on reranked results, is described in Section 9. Answers from our
system were manually assessed by two physicians; results are presented in Section 10.
Related work is discussed in Section 11, followed by future work in Section 12. Finally,
we conclude in Section 13.
2. The Framework of Evidence-Based Medicine
Evidence-based medicine (EBM) is a widely accepted paradigm for medical practice
that involves the explicit use of current best evidence, that is, high-quality patient-
centered clinical research such as reports from randomized controlled trials, in mak-
ing decisions about patient care. Naturally, such evidence, as reported in the primary
medical literature, must be suitably integrated with the physician?s own expertise and
patient-specific factors. It is argued by many that practicing medicine in this manner
leads to better patient outcomes and higher quality health care. The goal of our work is
to develop question-answering techniques that complement this paradigm of medical
practice.
EBM offers three orthogonal facets that, when taken together, provide a framework
for codifying the knowledge involved in answering clinical questions. These three
complementary facets are outlined below.
The first facet describes the four main clinical tasks that physicians engage in
(arranged roughly in order of prevalence):
Therapy: Selecting treatments to offer a patient, taking into account effectiveness, risk,
cost, and other relevant factors (includes Prevention?selecting actions to reduce
the chance of a disease by identifying and modifying risk factors).
65
Computational Linguistics Volume 33, Number 1
Diagnosis: This encompasses two primary types:
Differential diagnosis: Identifying and ranking by likelihood potential diseases
based on findings observed in a patient.
Diagnostic test: Selecting and interpreting diagnostic tests for a patient, consid-
ering their precision, accuracy, acceptability, cost, and safety.
Etiology/Harm: Identifying factors that cause a disease or condition in a patient.
Prognosis: Estimating a patient?s likely course over time and anticipating likely
complications.
These activities represent what Ingwersen (1999) calls ?work tasks.? It is important
to note that they exist independently of information needs, namely, searching is not
necessarily implicated in any of these activities. We are, however, interested in situations
where questions arise during one of these clinical tasks?only then does the physician
engage in information-seeking behavior. These activities translate into natural ?search
tasks.? For therapy, the search task is usually therapy selection (for example, determining
which course of action is the best treatment for a disease) or prevention (for example,
selecting preemptive measures with respect to a particular disease). For diagnosis,
there are two different possibilities: in differential diagnosis, a physician is consider-
ing multiple hypotheses regarding what disease a patient has; in diagnostic methods
selection, the clinician is attempting to ascertain the relative utility of different tests.
For etiology, cause determination is the search task, and for prognosis, patient outcome
prediction.
Terms and the types of studies relevant to each of the four tasks have been exten-
sively studied by the Hedges Project at the McMaster University (Haynes et al 1994;
Wilczynski, McKibbon, and Haynes 2001). The results of this research are implemented
in the PubMed Clinical Queries tools, which can be used to retrieve task-specific cita-
tions (more about this in the next section).
The second facet is independent of the clinical task and pertains to the struc-
ture of a well-built clinical question. The following four components have been iden-
tified as the key elements of a question related to patient care (Richardson et al
1995):
 What is the primary problem or disease? What are the characteristics of
the patient (e.g., age, gender, or co-existing conditions)?
 What is the main intervention (e.g., a diagnostic test, medication, or
therapeutic procedure)?
 What is the main intervention compared to (e.g., no intervention, another
drug, another therapeutic procedure, or a placebo)?
 What is the desired effect of the intervention (e.g., cure a disease, relieve
or eliminate symptoms, reduce side effects, or lower cost)?
These four elements are often referenced with the mnemonic PICO, which stands
for Patient/Problem, Intervention, Comparison, and Outcome.
Finally, the third facet serves as a tool for appraising the strength of evidence
presented in the study, that is, how much confidence should a physician have in the
results? Several taxonomies for appraising the strength of evidence based on the type
and quality of the study have been developed. We chose the Strength of Recommenda-
tions Taxonomy (SORT) as the basis for determining the potential upper bound on the
66
Demner-Fushman and Lin Answering Clinical Questions
quality of evidence, due to its emphasis on the use of patient-oriented outcomes and its
attempt to unify other existing taxonomies (Ebell et al 2004). There are three levels of
recommendations according to SORT:
 A-level evidence is based on consistent, good-quality patient
outcome-oriented evidence presented in systematic reviews, randomized
controlled clinical trials, cohort studies, and meta-analyses.
 B-level evidence is inconsistent, limited-quality, patient-oriented evidence
in the same types of studies.
 C-level evidence is based on disease-oriented evidence or studies less
rigorous than randomized controlled clinical trials, cohort studies,
systematic reviews, and meta-analyses.
A question-answering system designed to support the practice of evidence-based
medicine must be sensitive to the multifaceted considerations that go into evaluating
an abstract?s relevance to a clinical information need. It is exactly these three comple-
mentary facets that we attempt to encode in a question-answering system for clinical
decision support.
3. MEDLINE and PubMed
MEDLINE is a large bibliographic database maintained by the U.S. National Library
of Medicine (NLM). This database is viewed by medical professionals, biomedical
researchers, and many other users as the authoritative source of clinical evidence, and
hence we have adopted it as the target corpus for our clinical question-answering sys-
tem. MEDLINE contains over 15 million references to articles from approximately 4,800
journals in 30 languages, dating back to the 1960s. In 2004, over 571,000 new citations
were added to the database, and it continues to grow at a steady pace. The subject
scope of MEDLINE is biomedicine and health, broadly defined to encompass those
areas of the life sciences, behavioral sciences, chemical sciences, and bioengineering
needed by health professionals and others engaged in basic research and clinical care,
public health, health policy development, or related educational activities. MEDLINE
also covers life sciences vital to biomedical practitioners, researchers, and educators,
including aspects of biology, environmental science, marine biology, plant and animal
science, as well as biophysics and chemistry.1
Each MEDLINE citation includes basic information such as the title of the article,
name of the authors, name of the publication, publication type, date of publication,
language, and so on. Of the entries added over the last decade or so, approximately
76% have English abstracts written by the authors of the articles?these texts provide
the source for answers extracted by our system.
Additional metadata are associated with each MEDLINE citation. The most impor-
tant of these is the controlled vocabulary terms assigned by human indexers. NLM?s
controlled vocabulary thesaurus, Medical Subject Headings (MeSH),2 contains approx-
imately 23,000 descriptors arranged in a hierarchical structure and more than 151,000
Supplementary Concept Records (additional chemical substance names) within a
1 http://www.nlm.nih.gov/pubs/factsheets/medline.html
2 Commonly referred to as MeSH terms or MeSH headings, although technically the latter is redundant.
67
Computational Linguistics Volume 33, Number 1
separate thesaurus. Indexing is performed by approximately 100 indexers with at least
bachelor?s degrees in life sciences and formal training in indexing provided by NLM.
Since mid-2002, the Library has been employing software that automatically suggests
MeSH headings based on content (Aronson et al 2004). Nevertheless, the indexing
process remains firmly human-centered.
As a concrete example, an abstract titled ?Antipyretic efficacy of ibuprofen vs.
acetaminophen? might have the following MeSH headings associated with it:
MH - Acetaminophen/*therapeutic use
MH - Child
MH - Comparative Study
MH - Fever/*drug therapy
MH - Ibuprofen/*therapeutic use
To represent different aspects of the topic described by a particular MeSH heading,
up to three subheadings may be assigned, as indicated by the slash notation. In this
example, a trained user could interpret from the MeSH terms that the article is about
drug therapy for fever and the therapeutic use of ibuprofen and acetaminophen. An
asterisk placed next to a MeSH heading indicates that the human indexer interprets the
term to be the main focus of the article. Multiple MeSH terms can be notated in this
manner.
MEDLINE is publicly accessible on the World Wide Web through PubMed, the Na-
tional Library of Medicine?s gateway, or through third-party organizations that license
MEDLINE from NLM. PubMed is a sophisticated boolean search engine that allows
users to query not only on abstract text, but also on metadata fields such as MeSH terms.
In addition, PubMed provides a number of pre-defined ?search templates? called Clin-
ical Queries (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001) that allow
users to narrow the scope of retrieved articles. These filters are implemented as fixed
boolean query fragments (containing restrictions on MeSH terms, for example) that are
appended to the original user query. Our experiments involve the use of PubMed to
retrieve an initial set of candidate citations for subsequent processing.
4. System Architecture
We view clinical question answering as ?semantic unification? between information
needs expressed in a PICO-based frame and corresponding structures automatically
extracted from MEDLINE citations. In accordance with the principles of EBM, this
matching process should be sensitive to the nature of the clinical task and the strength
of evidence of retrieved abstracts.
As a concrete example, consider the following clinical question:
In children with an acute febrile illness, what is the efficacy of single-medication
therapy with acetaminophen or ibuprofen in reducing fever?
The information need might be formally encoded in the following manner:
Search Task: therapy selection
Problem/Population: acute febrile illness/in children
Intervention: acetaminophen
Comparison: ibuprofen
Outcome: reducing fever
68
Demner-Fushman and Lin Answering Clinical Questions
This query representation explicitly encodes the search task and the PICO structure
of the clinical question. After processing MEDLINE citations, automatically extracting
PICO elements from the abstracts, and semantically matching these elements with the
query, a system might produce the following answer:
Ibuprofen provided greater temperature decrement and longer duration of antipyresis
than acetaminophen when the two drugs were administered in approximately equal
doses.
PMID: 1621668
Strength of Evidence: grade A
Physicians are usually most interested in outcome statements that assert a patient-
oriented clinical finding?for example, the relative efficacy of two drugs. Thus, out-
comes can serve as the basis for good answers and an entry point into the full text. The
system should automatically evaluate the strength of evidence of the citations supplying
the answer, but the decision to adopt the recommendations as suggested ultimately rests
with the physician.
What is the best input to a clinical question-answering system? Two possibilities
include a natural language question or a structured PICO query frame. We advocate
the latter. With a frame-based query interface, the physician shoulders the burden of
translating an information need into a frame-based representation, but this provides
several advantages. Most importantly, formal representations force physicians to ?think
through? their questions, ensuring that relevant elements are captured. Poorly formu-
lated queries have been identified by Ely et al (2005) as one of the obstacles to finding
answers to clinical questions. Because well-formed questions should have concretely
instantiated PICO slots, a frame representation clearly lets the physician see missing
elements. In addition, a structured query representation obviates the need for linguistic
analysis of a natural language question, where ambiguities may negatively impact
overall performance. We discuss alternative interfaces in Section 12.
Ideally, we would like to match structured representations derived from the ques-
tion with those derived from MEDLINE citations (taking into consideration other EBM-
relevant factors). However, we do not have access to the computational resources nec-
essary to apply knowledge extractors to the 15 million plus citations in the MEDLINE
database and directly index their results. As an alternative, we rely on PubMed to
retrieve an initial set of hits that we then postprocess in greater detail?this is the
standard pipeline architecture commonly employed in other question-answering sys-
tems (Voorhees and Tice 1999; Hirschman and Gaizauskas 2001).
The architecture of our system is shown in Figure 1. The query formulator is respon-
sible for converting a clinical question (in the form of a query frame) into a PubMed
search query. Presently, these queries are already encoded in our test collection (see
Section 6). PubMed returns an initial list of MEDLINE citations, which is then analyzed
by our knowledge extractors (see Section 5). The input to the semantic matcher, which
implements our EBM citation scoring algorithm, is the query frame and annotated
MEDLINE citations. The module outputs a ranked list of citations that have been scored
in accordance with the principles of EBM (see Section 6). Finally, the answer generator
takes these citations and extracts appropriate answers (see Section 9).
In summary, our conception of clinical question answering as semantic frame
matching suggests the need for a number of capabilities, which correspond to the
bold outlined boxes in Figure 1: knowledge extraction, semantic matching for scoring
69
Computational Linguistics Volume 33, Number 1
Figure 1
Architecture of our clinical question-answering system.
citations, and answer generation. We have realized all three capabilities in an imple-
mented clinical question-answering system and conducted three separate evaluations
to assess the effectiveness of our developed capabilities. We do not tackle the query
formulator, although see discussion in Section 12. Overall, results indicate that our
implemented system significantly outperforms the PubMed baseline.
5. Knowledge Extraction for Evidence-Based Medicine
The automatic extraction of PICO elements from MEDLINE citations represents a key
capability integral to clinical question answering. This section, which elaborates on
preliminary results reported in Demner-Fushman and Lin (2005), describes extraction
algorithms for population, problems, interventions, outcomes, and the strength of evi-
dence. For an example of a completely annotated abstract, see Figure 2. Each individual
PICO extractor takes as input the abstract text of a MEDLINE citation and identifies the
relevant elements: Outcomes are complete sentences, while population, problems, and
interventions are short noun phrases.
Our knowledge extractors rely extensively on MetaMap (Aronson 2001), a system
for identifying segments of text that correspond to concepts in the UMLS Metathe-
saurus. Many of our algorithms operate at the level of coarser-grained semantic
types called Semantic Groups (McCray, Burgun, and Bodenreider 2001), which capture
higher-level generalizations about entities (e.g., CHEMICALS & DRUGS). An additional
feature we take advantage of (when present) is explicit section markers present in some
abstracts. These so-called structured abstracts were recommended by the Ad Hoc Work-
ing Group for Critical Appraisal of the Medical Literature (1987) to help humans assess
the reliability and content of a publication and to facilitate the indexing and retrieval
processes. These abstracts loosely adhere to the introduction, methods, results, and
conclusions format common in scientific writing, and delineate a study using explicitly
marked sections with variations of the above headings. Although many core clinical
journals require structured abstracts, there is a great deal of variation in the actual
headings. Even when present, the headings are not organized in a manner focused on
patient care. In addition, abstracts of much high-quality work remain unstructured. For
these reasons, explicit section markers are not entirely reliable indicators for the various
semantic elements we seek to extract, but must be considered along with other sources
of evidence.
The extraction of each PICO element relies to a different extent on an annotated
corpus of MEDLINE abstracts, created through an effort led by the first author at
the National Library of Medicine (Demner-Fushman et al 2006). As will be described
herein, the population, problem, and the intervention extractors are based largely on
recognition of semantic types and a few manually constructed rules; the outcome extrac-
70
Demner-Fushman and Lin Answering Clinical Questions
tor, in contrast, is implemented as an ensemble of classifiers trained using supervised
machine learning techniques (Demner-Fushman et al 2006). These two very different
approaches can be attributed to differences in the nature of the frame elements: Whereas
problems and interventions can be directly mapped to UMLS concepts, and populations
easily mapped to patterns that include UMLS concepts, outcome statements follow no
predictable pattern. The initial goal of the annotation effort was to identify outcome
statements in abstract text. A physician, two registered nurses, and an engineering
researcher manually identified sentences that describe outcomes in 633 MEDLINE
abstracts; a post hoc analysis demonstrates good agreement (? = 0.77). The annotated
abstracts were retrieved using PubMed and attempted to model different user behaviors
ranging from naive to expert (where advanced search features were employed). With the
exception of 50 citations retrieved to answer a question about childhood immunization,
the rest of the results were retrieved by querying on a disease, for example, diabetes. Of
the 633 citations, 100 abstracts were also fully annotated with population, problems, and
interventions. These 100 abstracts were set aside as a held-out test set. Of the remaining
citations, 275 were used for training and rule derivation, as described in the following
sections.
After much exploration, Demner-Fushman et al (2006) discovered that it was not
practical to annotate PICO entities at the phrase level due to significant unresolvable
disagreement and interannotator reliability issues. Consider the following segment:
This double-blind, placebo-controlled, randomized, 3-period, complete block, 6-week
crossover study examined the efficacy of simvastatin in adult men and women (N =
151) with stable type 2 DM, low density lipoprotein-cholesterol 100 mg/dL, HDL-C <
40 mg/dL, and fasting triglyceride level > 150 and < 700 mg/dL.
All annotators agreed that the sentence contained the problem, population, and
intervention. However, they could not agree on the exact phrasal boundaries of each
element, and more importantly, general guidelines for ensuring consistent annotations.
For example, should the whole clause starting with adult men and women be marked as
population, or should type 2 Diabetes Mellitus (type 2 DM) be marked up only as the
problem? How should we indicate that the cholesterol levels description belongs to 151
subjects of the study, and so forth? This issue becomes important for evaluation because
there is a mismatch between annotated ground truth and the output of our knowledge
extractors, as we will discuss.
In what follows, we describe each of the individual PICO extractors and a series of
component evaluations that assess their accuracy. This section is organized such that the
description of each extractor and its evaluation are paired together. Results are reported
in terms of the percentage of correctly identified instances, percentage of instances for
which the extractor had no answer, and percentage of incorrectly identified instances.
The baselines and gold standards for each extractor vary, and will be described in-
dividually. The goal of these component evaluations is a general characterization of
performance, as we focused the majority of our efforts on the two other evaluations.
5.1 Population Extractor
The PICO framework makes no distinction between the population and the problem,
which is rooted in the concept of the population in clinical studies, as exemplified by
text such as POPULATION: Fifty-five postmenopausal women with a urodynamic diagnosis
of genuine urinary stress incontinence. Although this fragment simultaneously describes
71
Computational Linguistics Volume 33, Number 1
the population (of which a particular patient can be viewed as a sample therefrom) and
the problem, we chose to separate the extraction of the two elements because they are
not always specified together in abstracts (issues with respect to exact boundaries men-
tioned previously notwithstanding). Furthermore, many clinical questions ask about a
particular problem without specifying a population.
Population elements, which are typically noun phrases, are identified using a series
of manually crafted rules that codify the following assumptions:
 The concept describing the population belongs to the semantic type
GROUP or any of its children. In addition, certain nouns are often used to
describe study participants in medical texts; for example, an often
observed pattern is ?subjects? or ?cases? followed by a concept from the
semantic group DISORDER.
 The number of subjects that participated in the study often precedes or
follows a concept identified as a GROUP. In the latter case, the number is
sometimes given in parentheses using a common pattern n = number,
where ?n = ? is a shorthand for the number of subjects, and number
provides the actual number of study participants.
 The confidence that a clause with an identified number and GROUP
contains information about the population is inversely proportional to the
distance between the two entities.
 The confidence that a clause contains the population is influenced by the
position of the clause, with respect to headings in the case of structured
abstracts and with respect to the beginning of the abstract in the case of
unstructured abstracts.
Given these assumptions, the population extractor searches for the following
patterns:
 GROUP ([Nn]=[0?9]+)
for example, in 5?6-year-old French children (n = 234), Subjects (n = 54)
 number* GROUP
for example, forty-nine infants
 number* DISORDER* GROUP?
for example, 44 HIV-infected children
The confidence score assigned to a particular pattern match is a function of both its
position in the abstract and its position in the clause from which it was extracted. If a
number is followed by a measure, for example, year or percent, the number is discarded,
and pattern matching continues. After the entire abstract is processed in this manner,
the match with the highest confidence value is retained as the population description.
5.2 Evaluation of Population Extractor
Ninety of the 100 fully annotated abstracts in our collection were agreed upon by the
annotators as being clinical in nature, and were used as test data for our population
extractor. Because these abstracts were not examined in the process of developing the
extractor rules, they can be viewed as a blind held-out test set. The output of our popu-
72
Demner-Fushman and Lin Answering Clinical Questions
Table 1
Evaluation of the population extractor.
Correct (%) Unknown (%) Wrong (%)
Baseline 53 ? 47
Extractor 80 10 10
lation extractor was judged to be correct if it occurred in a sentence that was annotated
as containing the population in the gold standard. Note that this evaluation presents an
upper bound on the performance of the population extractor, whose outputs are noun
phrases. We adopted such a lenient evaluation setup because of the boundary issues
previously discussed, and also to forestall potential difficulties with scoring partially
overlapping string matches.
For comparison, our baseline simply returned the first three sentences of the ab-
stract. We considered the baseline correct if any one of the sentences were annotated
as containing the population in the gold standard (an even more lenient criterion).
This baseline was motivated by the observation that the aim and methods sections of
structured abstracts are likely to contain the population information?for structured ab-
stracts, explicit headings provide structural cues; for unstructured abstracts, positional
information serves as a surrogate.
The performance of the population extractor is shown in Table 1. A manual error
analysis revealed three sources of error: First, not all population descriptions contain
a number explicitly, for example, The medical charts of all patients who were treated with
etanercept for back or neck pain at a single private medical clinic in 2003. Second, not all study
populations are population groups, as for example in All primary care trusts in England.
Finally, tagging and chunking errors propagate to the semantic type assignment level
and affect the quality of MetaMap output. For example, consider the following sentence:
We have compared the LD and recombination patterns defined by single-nucleotide
polymorphisms in ENCODE region ENm010, chromosome 7p15 2, in Korean, Japanese,
and Chinese samples.
Both Korean and Japanese were mistagged as nouns, which lead to the following
erroneous chunking:
[We] [have] [compared] [the LD] [and] [recombination patterns] [defined] [by
single-nucleotide polymorphisms] [in] [ENCODE] [region ENm010,] [chromosome
7p15 2,] [in Korean,] [Japanese,] [and] [Chinese samples.]
This resulted in the tagging of Japanese as a population. Errors of this type affect
other extractors as well. For example, lead was mistagged as a noun in the phrase
Echocardiographic findings lead to the right diagnosis, which caused MetaMap to identify
the word as a PHARMACOLOGICAL SUBSTANCE (lead is sometimes used as a homeo-
pathic preparation).
5.3 Problem Extractor
The problem extractor relies on the recognition of concepts belonging to the UMLS
semantic group DISORDER. In short, it returns a ranked list of all such concepts within
a given span of text. We evaluate the performance of this simple heuristic on segments
73
Computational Linguistics Volume 33, Number 1
Table 2
Evaluation of the problem extractor.
Correct (%) Unknown (%) Wrong (%)
Abstract title 85 10 5
Title + 1st two sentences 90 5 5
Entire abstract 86 2 12
of the abstract varying in length: abstract title only, abstract title and first two sentences,
and entire abstract text. Concepts in the title, in the introduction section of structured
abstracts, or in the first two sentences in unstructured abstracts, are given higher confi-
dence values due to their discourse prominence. Finally, the highest-scoring problem
is designated as the primary problem in order to differentiate it from co-occurring
conditions identified in the abstract.
5.4 Evaluation of Problem Extractor
Although our problem extractor returns a list of clinical problems, we only evalu-
ate performance on identification of the primary problem. For some abstracts, MeSH
headings can be used as ground truth, because one of the human indexers? tasks in
assigning terms is to identify the main topic of the article (sometimes a disorder). For
this evaluation, we randomly selected 50 abstracts with disorders indexed as the main
topic from abstracts retrieved using PubMed on the five clinical questions described
in Sneiderman et al (2005).
We applied our problem extractor on different segments of the abstract: the title
only, the title and first two sentences, and the entire abstract. These results are shown in
Table 2. Here, a problem was considered correctly identified only if it shared the same
concept ID as the ground truth problem (from the MeSH heading). The performance of
our best variant (abstract title and first two sentences) approaches the upper bound on
MetaMap performance?which is limited by human agreement on the identification of
semantic concepts in medical texts, as established in Pratt and Yetisgen-Yildiz (2003).
Although problem extraction largely depends on disease coverage in UMLS and
MetaMap performance, the error rate could be further reduced by more sophisticated
recognition of implicitly stated problems. For example, with respect to a question about
immunization in children, an abstract about the measles-mumps-rubella vaccination
never mentioned the disease without the word vaccination; hence, no concept of the
type DISEASE OR SYNDROME was identified.
5.5 Intervention Extractor
The intervention extractor identifies both the intervention and comparison elements in
a PICO frame; processing of these two frame elements can be collapsed because they
belong to the same semantic group. In many abstracts, it is unclear which intervention is
the primary one and which are the comparisons, and hence our extractor simply returns
a list of all interventions under study.
For interventions, we are primarily interested in entities that may participate in the
UMLS Semantic Network relations associated with each clinical task. Restrictions on
the semantic types allowed in these relations prescribe the set of possible clinical in-
terventions. For therapy these relations include treats, prevents, and carries out; diagnoses
74
Demner-Fushman and Lin Answering Clinical Questions
Table 3
Evaluation of the intervention extractor.
Correct (%) Unknown (%) Wrong (%)
Baseline 60 ? 40
Extractor 80 ? 20
for diagnosis; causes and result of for etiology; and prevents for prognosis. At present, the
identification of nine semantic types, for example, DIAGNOSTIC PROCEDURE, CLINICAL
DRUG, and HEALTH CARE ACTIVITY, serves as the foundation for our intervention
extraction algorithm.
Candidate scores are further adjusted to reflect a few different factors. In structured
abstracts, concepts of the relevant semantic type are given additional weight if they
appear in the title, aims, and methods sections. In unstructured abstracts, concepts
towards the beginning of the abstract text are favored. Finally, the intervention extractor
takes into account the presence of certain cue phrases that describe the aim and/or
methods of the study, such as This study examines or This paper describes.
5.6 Evaluation of Intervention Extractor
The intervention extractor was evaluated in the same manner as the population extrac-
tor and compared to the same baseline. To iterate, 90 held-out clinical abstracts that
contained human-annotated interventions served as ground truth. The output of our
intervention extractor was judged to be correct if it occurred in a sentence that was
annotated as containing the intervention in the gold standard. As with the evaluation
of the population extractor, this represents an upper bound on performance. Results are
shown in Table 3.
Some of the errors were caused by ambiguity of terms. For example, in the clause
serum levels of anti-HBsAg and presence of autoantibodies (ANA, ENA) were evaluated,
serum is recognized as a TISSUE, levels as INTELLECTUAL PRODUCT, and autoantibodies
and ANA as IMMUNOLOGIC FACTORS. In this case, however, autoantibodies should
be considered a LABORATORY OR TEST RESULT.3 In other cases, extraction errors
were caused by summary sentences that were very similar to intervention statements,
for example, This study compared the effects of 52 weeks? treatment with pioglitazone, a
thiazolidinedione that reduces insulin resistance, and glibenclamide, on insulin sensitivity,
glycaemic control, and lipids in patients with Type 2 diabetes. For this particular abstract,
the correct interventions are contained in the sentence Patients with Type 2 diabetes
were randomized to receive either pioglitazone (initially 30 mg QD, n = 91) or micronized
glibenclamide (initially 1.75 mg QD, n = 109) as monotherapy.
5.7 Outcome Extractor
We approached outcome extraction as a classification problem at the sentence level, that
is, the outcome extractor assigns a probability of being an outcome to each sentence
in an abstract. Our preliminary work has led to a strategy based on an ensemble of
classifiers, which includes a rule-based classifier, a unigram ?bag of words? classifier,
3 MetaMap does provide alternative mappings, but the current extractor only considers the best candidate.
75
Computational Linguistics Volume 33, Number 1
an n-gram classifier, a position classifier, an abstract length classifier, and a semantic
classifier. With the exception of the rule-based classifier, all classifiers were trained on
the 275 citations from the annotated collection of abstracts described previously.
Knowledge for the rule-based classifier was hand-coded, prior to the annotation
effort, by a registered nurse with 20 years of clinical experience. This classifier estimates
the likelihood that a sentence states an outcome based on cue phrases such as signif-
icantly greater, well tolerated, and adverse events. The likelihood of a sentence being an
outcome as indicated by cue phrases is the ratio of the cumulative score for recognized
phrases to the maximum possible score. For example, the sentence The dropout rate due to
adverse events was 12.4% in the moxonidine and 9.8% in the nitrendipine group is segmented
into eight phrases by MetaMap, which sets the maximum score to 8. The two phrases
dropout rate and adverse events contribute one point each to the cumulative score, which
results in a likelihood estimate of 0.25 for this sentence.
The unigram ?bag of words? classifier is a naive Bayes classifier implemented with
the API provided by the MALLET toolkit.4 This classifier outputs the probability of a
class assignment.
The n-gram based classifier is also a naive Bayes classifier, but it operates on a differ-
ent set of features. We first identified the most informative unigrams and bigrams using
the information gain measure (Yang and Pedersen 1997), and then selected only the
positive outcome predictors using odds ratio (Mladenic and Grobelnik 1999). Disease-
specific terms, such as rheumatoid arthritis, were then manually removed. Finally, the
list of features was revised by the registered nurse who participated in the annotation
effort. This classifier also outputs the probability of a class assignment.
The position classifier returns the maximum likelihood estimate that a sentence is
an outcome based on its position in the abstract (for structured abstracts, with respect
to the results or conclusions sections; for unstructured abstracts, with respect to the end
of the abstract).
The abstract length classifier returns a smoothed (add one smoothing) probability
that an abstract of a given length (in the number of sentences) contains an outcome
statement. For example, the probability that an abstract four sentences long contains an
outcome statement is 0.25, and the probability of finding an outcome in a ten sentence?
long abstract is 0.92. This feature turns out to be useful because the average length of
abstracts with and without outcome statements differs: 11.7 sentences for the former,
7.95 sentences for the latter.
The semantic classifier assigns to a sentence an ad hoc score based on the presence
of UMLS concepts belonging to semantic groups highly associated with outcomes
such as THERAPEUTIC PROCEDURE or PHARMACOLOGICAL SUBSTANCE. The score is
given a boost if the concept has already been identified as the primary problem or an
intervention.
The outputs of our basic classifiers are combined using a simple weighted linear
interpolation scheme:
Soutcome = ?1Scues + ?2Sunigram + ?3Sn-gram + ?4Sposition + ?5Slength + ?6Ssemantic type (1)
We attempted two approaches for assigning these weights. The first method relied
on ad hoc weight selection based on intuition. The second involved a more principled
method using confidence values generated by the base classifiers and least squares lin-
4 http://mallet.cs.umass.edu/
76
Demner-Fushman and Lin Answering Clinical Questions
ear regression adapted for classification (Ting and Witten 1999), which can be described
by the following equation:
LR(x) =
N
?
k=1
?kPk(X) (2)
Pk is the probability that a sentence specifies an outcome, as determined by classifier
k (for classifiers that do not return actual probabilities, we normalized the scores and
treated them as such). To predict the class of a sentence, the probabilities generated
by n classifiers are combined using the coefficients (?0, ...,?n). These values are de-
termined in the training stage as follows: Probabilities predicted by base classifiers
for each sentence are represented in an N ? M matrix A, where M is the number of
sentences in the training set, and N is the number of classifiers. The gold standard
class assignments for each sentence is stored in a vector b, and weights are found by
computing the vector ? that minimizes ||A?? b||. The solution can be found using
singular value decomposition, as provided in the JAMA basic linear algebra package
released by NIST.5
5.8 Evaluation of Outcome Extractor
Because outcome statements were annotated in each of the 633 citations in our collec-
tion, it was possible to evaluate the outcome extractor on a broader set of abstracts. From
those not used in training the outcome classifiers, 153 citations pertaining to therapy
were selected. Of these, 143 contained outcome statements and were used as the blind
held-out test set. In addition, outcome statements in abstracts pertaining to diagnosis
(57), prognosis (111), and etiology (37) were also used.
The output of our outcome extractor is a ranked list of sentences sorted by con-
fidence. Based on the observation that human annotators typically mark two to three
sentences in each abstract as outcomes, we evaluated the performance of our extractor
at cutoffs of two and three sentences. These results are shown in Table 4: The columns
marked AH2 and AH3 show performance of the weighted linear interpolation approach
with ad hoc weight assignment at two- and three-sentence cutoffs, respectively; the
columns marked LR2 and LR3 show performance of the least squares linear regression
model at the same cutoffs. In the evaluation, our outcome extractor was considered
correct if the returned sentences intersected with sentences judged as outcomes by
our human annotators. Although this is a lenient criterion, it does roughly capture
the performance of our knowledge extractor. Because outcome statements are typically
found in the conclusion of a structured abstract (or near the end of the abstract in the
case of unstructured abstracts), we compared our answer extractor to the baseline of
returning either the final two or final three sentences in the abstract (B2 and B3 in
Table 4).
As can be seen, variants of our outcome extractor performed better than the baseline
at the two-sentence cutoff, for the most part. Bigger improvements, however, can be
seen at the three-sentence cutoff level. It is evident that the assignment of weights in
our ad hoc model is primarily geared towards therapy questions, perhaps overly so.
Better overall performance is obtained with the least squares linear regression model.
5 http://math.nist.gov/javanumerics/jama/
77
Computational Linguistics Volume 33, Number 1
Table 4
Evaluation of the outcome extractor. B = baseline, returns last sentences in abstract; AH = ad hoc
weight assignment; LR = least squares linear regression. Statistically significant improvement
over the baseline at the 1% level is indicated by  .
2-sentence cutoff (%) 3-sentence cutoff (%)
B2 AH2 LR2 B3 AH3 LR3
Therapy 74 75 77 75 95 93
Diagnosis 72 70 78 75 78 89
Prognosis 73 76 79 85 87 89
Etiology 64 68 74 78 83 88
Table 5
Examples of strength of evidence categories based on Publication Type and MeSH headings.
Strength of Evidence Publication Type/MeSH
Level A(1) Meta-analysis, randomized controlled trials, cohort study,
follow-up study
Level B(2) Case-control study, case series
Level C(3) Case report, in vitro, animal and animal testing,
alternatives studies
The majority of errors made by the outcome extractor were related to inaccurate
sentence boundary identification, chunking errors, and word sense ambiguity in the
Metathesaurus.
5.9 Determining the Strength of Evidence
The strength of evidence is a classification scheme that helps physicians assess the
quality of a particular citation for clinical purposes. Metadata associated with most
MEDLINE citations (MeSH terms) are extensively used to determine the strength of
evidence and in our EBM citation scoring algorithm (Section 6).
The potential highest level of the strength of evidence for a given citation can be
identified using the Publication Type (a metadata field) and MeSH terms pertaining
to the type of the clinical study. Table 5 shows our mapping from publication type
and MeSH headings to evidence grades based on principles defined in the Strength
of Recommendations Taxonomy (Ebell et al 2004).
5.10 Sample Output
A complete example of our knowledge extractors working in unison is shown in
Figure 2, which contains an abstract retrieved in response to the following question:
?In children with an acute febrile illness, what is the efficacy of single-medication
therapy with acetaminophen or ibuprofen in reducing fever?? (Kauffman, Sawyer, and
Scheinbaum 1992). Febrile illness is the only concept mapped to DISORDER, and hence
is identified as the primary problem. 37 otherwise healthy children aged 2 to 12 years is
correctly identified as the population. Acetaminophen, ibuprofen, and placebo are correctly
78
Demner-Fushman and Lin Answering Clinical Questions
Antipyretic efficacy of ibuprofen vs acetaminophen
Kauffman RE, Sawyer LA, Scheinbaum ML
Am J Dis Child. 1992 May;146(5):622-5
OBJECTIVE?To compare the antipyretic efficacy of ibuprofen, placebo, and
acetaminophen. DESIGN?Double-dummy, double-blind, randomized, placebo-
controlled trial. SETTING?Emergency department and inpatient units of a large,
metropolitan, university-based, children?s hospital in Michigan. PARTICIPANTS?
??
37
???????????
otherwise
????????
healthy
?????????
children
?????
aged
??
2
???
to
???
12
??????
yearsPopulation with
??????
acute,
?????????????
intercurrent,
??????
febrile
???????
illnessProblem. INTERVENTIONS?Each child was randomly assigned to receive
a single dose of
???????????????
acetaminophenIntervention (10 mg/kg),
??????????
ibuprofenIntervention (10 mg/kg) (7.5
or 10 mg/kg), or
????????
placeboIntervention (10 mg/kg). MEASUREMENTS/MAIN RESULTS?
Oral temperature was measured before dosing, 30 minutes after dosing, and
hourly thereafter for 8 hours after the dose. Patients were monitored for ad-
verse effects during the study and 24 hours after administration of the as-
signed drug.
???
All
?????
three
??????
active
???????????
treatments
??????????
produced
??????????
significant
???????????
antipyresis
???????????
compared
????
with
?????????
placebo.Outcome
??????????
Ibuprofen
??????????
provided
???????
greater
?????????????
temperature
??????????
decrement
?????
and
???????
longer
????????
duration
???
of
???????????
antipyresis
?????
than
???????????????
acetaminophen
??????
when
???
the
????
two
??????
drugs
?????
were
??????????????
administered
??
in
???????????????
approximately
??????
equal
??????
doses.Outcome No adverse effects were observed in any treat-
ment group. CONCLUSION?
?????????
Ibuprofen
???
is
??
a
????????
potent
???????????
antipyretic
???????
agent
?????
and
??
is
???
a
????
safe
???????????
alternative
????
for
????
the
?????????
selected
???????
febrile
??????
child
?????
who
?????
may
????????
benefit
?????
from
????????????
antipyretic
???????????
medication
????
but
?????
who
??????
either
???????
cannot
????
take
???
or
?????
does
????
not
????????
achieve
???????????
satisfactory
????????????
antipyresis
????
with
????????????????
acetaminophen.Outcome
Publication Type: Clinical Trial, Randomized Controlled Trial
PMID: 1621668
Strength of Evidence: grade A
Figure 2
Sample output from our PICO extractors.
extracted as the interventions under study. The three outcome sentences are correctly
classified; the short sentence concerning adverse effects was ranked lower than the
other three sentences and hence below the cutoff. The study design, from metadata
associated with the citation, allows our strength of evidence extractor to classify this
article as grade A.
6. Operationalizing Evidence-Based Medicine
In our view of clinical question answering, the knowledge extractors just described sup-
ply the features on which semantic matching occurs. This section describes an algorithm
that, when presented with a structured representation of an information need and a
MEDLINE citation, automatically computes a topical relevance score in accordance with
the principles of EBM.
In order to develop algorithms that operationalize the three facets of EBM, it is
necessary to possess a corpus of clinical questions on which to experiment. Because no
such test collection exists, we had to first manually create one. Fortunately, collections
of clinical questions (representing real-world information needs of physicians), are
79
Computational Linguistics Volume 33, Number 1
Table 6
Composition of our clinical questions collection.
Therapy Diagnosis Prognosis Etiology Total
Development 10 6 3 5 24
Test 12 6 3 5 26
available on-line. From two sources, the Journal of Family Practice6 and the Parkhurst
Exchange,7 we gathered 50 clinical questions, which capture a realistic sampling of the
scenarios that a clinical question-answering system would be confronted with. These
questions were minimally modified from their original form as downloaded from the
World Wide Web. In a few cases, a single question actually consisted of several smaller
questions; such clusters were simplified by removing questions more peripheral to the
central clinical problem. All questions were manually classified into one of the four
clinical tasks; the distribution of the questions roughly follows the prevalence of each
task type as observed in natural settings, noted by Ely et al (1999). The final step in
the preparation process was manual translation of the natural language questions into
PICO query frames.
Our collection was divided into a development set and a blind held-out test set for
verification purposes. The breakdown of these questions into the four clinical tasks and
the development/test split is shown in Table 6. An example of each question type from
our development set is presented here, along with its query frame:
Does quinine reduce leg cramps for young athletes? (Therapy)
search task: therapy selection
primary problem: leg cramps
co-occurring problems: muscle cramps, cramps
population: young adult
intervention: quinine
How often is coughing the presenting complaint in patients with gastroesophageal
reflux disease? (Diagnosis)
search task: differential diagnosis
primary problem: gastroesophageal reflux disease
co-occurring problems: cough
What?s the prognosis of lupoid sclerosis? (Prognosis)
search task: patient outcome prediction
primary problem: lupus erythematosus
co-occurring problems: multiple sclerosis
What are the causes of hypomagnesemia? (Etiology)
search task: cause determination
primary problem: hypomagnesemia
6 http://www.jfponline.com/
7 http://www.parkhurstexchange.com/qa/
80
Demner-Fushman and Lin Answering Clinical Questions
As discussed earlier, we do not believe that natural language text is the best input
for a question-answering system. Instead, a structured PICO-based representation cap-
tures physicians? information needs in a more perspicuous manner?primarily because
clinicians are trained to analyze clinical situations with this framework.
Mirroring the organization of our knowledge extractors, we broke up the P in PICO
into population, primary problem, and co-occurring problems in the query representa-
tion. The justification for this will become apparent when we present our algorithm for
scoring MEDLINE citations, as each of these three facets must be treated differently.
Note that many elements are specified only to the extent that they were explicit in
the original natural language question; for example, if the clinician does not specify
a population, that element will be empty. Finally, outcomes are not directly encoded in
the query representation because they are implicit most of the time; for example, in Does
quinine reduce leg cramps for young athletes?, the desired outcome, naturally, is to reduce
the occurrence and severity of leg cramps. Nevertheless, outcome identification is an
important component of the citation scoring algorithm, as we shall see later.
What is the relevance of an abstract with respect to a particular clinical question?
Evidence-based medicine outlines the need to consider three different facets (see Sec-
tion 2), which we operationalize in the following manner:
SEBM = SPICO + SSoE + Stask (3)
The relevance of a particular citation, with respect to a structured query, includes
contributions from matching PICO structures, the strength of evidence of the citation,
and factors specifically associated with the search tasks (and indirectly, the clinical
tasks). In what follows, we describe each of these contributions in detail.
Viewed as a whole, each score component is a heuristic reflection of the factors that
enter into consideration when a physician examines a MEDLINE citation. Although the
assignment of numeric scores is based on intuition and may seem ad hoc in many cases,
evaluation results in the next section demonstrate the effectiveness of our algorithm.
This issue will be taken up further in Section 8.
6.1 Scores Based on PICO Elements
The score of an abstract based on extracted PICO elements, SPICO, is broken into individ-
ual components according to the following formula:
SPICO = Sproblem + Spopulation + Sintervention + Soutcome (4)
The first component in the equation, Sproblem, reflects a match between the primary
problem in the query frame and the primary problem in the abstract (i.e., the highest-
scoring problem identified by the problem extractor). A score of 1 is given if the prob-
lems match exactly based on their unique UMLS concept ID as provided by MetaMap.
Matching based on concept IDs has the advantage that it abstracts away from termino-
logical variation; in essence, MetaMap performs terminological normalization. Failing
an exact match of concept IDs, a partial string match is given a score of 0.5. If the primary
problem in the query has no overlap with the primary problem from the abstract, a score
of ?1 is given. Finally, if our problem extractor could not identify a problem (but the
query frame does contain a problem), a score of ?0.5 is given.
Co-occurring problems must be taken into consideration in the differential diagnosis
and cause determination search tasks because knowledge of the problems is typically
81
Computational Linguistics Volume 33, Number 1
incomplete in these scenarios. Therefore, physicians would normally be interested in
any problems mentioned in the abstracts in addition to the primary problem specified
in the query frame. As an example, consider the question What is the differential diagnosis
of chronic diarrhea in immunocompetent patients? Although chronic diarrhea is the primary
problem, citations that discuss additional related disorders should be favored over those
that don?t. In terms of actual scoring, disorders mentioned in the title receive three
points, and disorders mentioned anywhere else receive one point (in addition to the
match score based on the primary problem, as discussed).
Scores based on population and intervention, Spopulation and Sintervention respectively, mea-
sure the overlap between query frame elements and corresponding elements extracted
from abstracts. A point is given to each matching intervention and matching population.
For example, finding the population group children from a query frame in the abstract
increments the match score; the remaining words in the abstract population are ignored.
Thus, if the query frame contains a population element and an intervention element, the
score for an abstract that contains the same UMLS concepts in the corresponding slots
is incremented by two.
The outcome-based score, Soutcome, is simply the value assigned to the highest-scoring
outcome sentence (we employed the outcome extractor based on the linear regression
model for our experiments). As outcomes are rarely explicitly specified in the original
question, we decided to omit them in the query representation. Our citation scoring
algorithm simply considers the inherent quality of the outcome statements in an ab-
stract, independent of the query. This is justified because, given a match on the primary
problem, all clinical outcomes are likely to be of interest to the physician.
6.2 Scores Based on Strength of Evidence
The relevance score component based on the strength of evidence is calculated in the
following manner:
SSoE = Sjournal + Sstudy + Sdate (5)
Citations published in core and high-impact journals such as Journal of the American
Medical Association (JAMA) get a score of 0.6 for Sjournal, and 0 otherwise. In terms of the
study type, Sstudy, clinical trials, such as randomized controlled trials, receive a score of
0.5; observational studies, for example, case reports, 0.3; all non-clinical publications,
?1.5; and 0 otherwise. The study type is directly encoded in the Publication Type field
of a MEDLINE citation.
Finally, recency factors into the strength of evidence score according to the formula:
Sdate = (yearpublication ? yearcurrent )/100 (6)
A mild penalty decreases the score of a citation proportionally to the time difference
between the date of the search and the date of publication.
6.3 Scores Based on Specific Tasks
The final component of our EBM score is based on task-specific considerations, as
reflected in manually assigned MeSH terms. For search tasks falling into each clinical
task, we gathered a list of terms that are positive and negative indicators of relevance.
82
Demner-Fushman and Lin Answering Clinical Questions
The task score, Stask, is given by:
Stask =
?
t?MeSH
?(t) (7)
The function ?(t) maps a MeSH term to a positive score if the term is a positive
indicator for that particular task type, or a negative score if the term is a negative indi-
cator for the clinical task. Note that although our current system uses MeSH headings
assigned by human indexers, manually assigned terms can be replaced with automatic
processing if needed (Aronson et al 2004).
Below, we enumerate the relevant indicator terms by clinical task. However, there
is a set of negative indicators common to all tasks; these were extracted from the set
of genomics articles provided for the secondary task in the TREC 2004 genomics track
evaluation (Hersh, Bhupatiraju, and Corley 2004); examples include genetics and cell
physiology. The positive and negative weights assigned to each term heuristically encode
the relative importance of different MeSH headings and are derived from the Clinical
Queries filters in PubMed, from the JAMA EBM tutorial series on critical appraisal of
medical literature, from MeSH scope notes, and based on a physician?s understanding
of the domain (the first author).
Indicators for Therapy Tasks. Positive indicators for therapy were derived from the
PubMed?s Clinical Queries filters; examples include drug administration routes and any
of its children in the MeSH hierarchy. A score of ?1 is given if the MeSH descriptor
or qualifier is marked as the main theme of the article (indicated via the star notation
by human indexers), and a score of ?0.5 otherwise. If the question pertains to the
search task of prevention, three additional headings are considered positive indicators:
prevention and control, prevention measures, and prophylaxis.
Indicators for Diagnosis Tasks. Positive indicators for therapy are also used as negative
indicators for diagnosis because the relevant studies are usually disjoint; it is highly
unlikely that the same clinical trial will study both diagnostic methods and treatment
methods. The MeSH term diagnosis and any of its children are considered positive
indicators. As with therapy questions, terms marked as the major theme get a score of
?1.0, and ?0.5 otherwise. This general assignment of indicator terms allows a system
to differentiate between questions such as Does a Short Symptom Checklist accurately
diagnose ADHD? and What is the most effective treatment for ADHD in children?, which
might retrieve very similar sets of citations.
Indicators for Prognosis Tasks. Positive indicators for prognosis include the following
MeSH terms: survival analysis, disease-free survival, treatment outcome, health status, preva-
lence, risk factors, disability evaluation, quality of life, and recovery of function. For terms
marked as the major theme, a score of +2 is given; +1 otherwise. There are no negative
indicators, other than those common to all tasks previously described.
Indicators for Etiology Tasks. Negative indicators for etiology include therapy-oriented
MeSH terms; these terms are given a score of ?0.3. Positive indicators for the diag-
nosis task are weak positive indicators for etiology, and receive a positive score of
+0.1. The following MeSH terms are considered highly indicative of citations rele-
vant to etiology: population at risk, risk factors, etiology, causality, and physiopathology. If
83
Computational Linguistics Volume 33, Number 1
one of these terms is marked as the major theme, a score of +2 is given; otherwise, a
score of +1 is given.
7. Evaluation of Citation Scoring
The previous section describes a relevance-scoring algorithm for MEDLINE citations
that attempts to capture the principles of EBM. In this section, we present an evaluation
of this algorithm.
Ideally, questions should be answered by directly comparing queries to knowl-
edge structures derived from MEDLINE citations. However, knowledge extraction on
such large scales is impractical given our computational resources, so we opted for
an IR-based pipeline approach. Under this strategy, an existing search engine would
be employed to generate a candidate list of citations to be rescored, according to our
algorithm. PubMed is a logical choice for gathering this initial list of citations because
it represents one of the most widely used tools employed by physicians and other
health professionals today. The system supports boolean operators and sorts results
chronologically, most recent citations first.
This two-stage retrieval process immediately suggests an evaluation methodology
for our citation scoring algorithm?as a document reranking task. Given an initial hit
list, can our algorithm automatically re-sort the results such that relevant documents
are brought to higher ranks? Not only is such a task intuitive to understand, this
conceptualization also lends itself to an evaluation based on widely accepted practices
in information retrieval.
For each question in our test collection, PubMed queries were manually crafted to
fetch an initial set of hits. These queries took advantage of existing advanced search
features to simulate the types of results that would be currently available to a knowl-
edgeable physician. Specifically, widely accepted tools for narrowing down PubMed
search results such as Clinical Queries were employed whenever appropriate.
As a concrete example, consider the following question: What is the best treatment for
analgesic rebound headaches? The search started with the initial terms ?analgesic rebound
headache? with a ?narrow therapy filter.? In PubMed, this query is:
((?headache disorders?[TIAB] NOT Medline[SB]) OR ?headache disorders?[MeSH
Terms] OR analgesic rebound headache[Text Word]) AND (randomized controlled
trial[Publication Type] OR (randomized[Title/Abstract] AND
controlled[Title/Abstract] AND trial[Title/Abstract])) AND hasabstract[text] AND
English[Lang] AND ?humans?[MeSH Terms]
Note that PubMed automatically identifies concepts and attempts matching both
in abstract text and MeSH headings. We always restrict searches to articles that have
abstracts, are published in English, and are assigned the MeSH term humans (as opposed
to say, experiments on animals)?these are all strategies commonly used by clinicians.
In this case, because none of the top 20 results were relevant, the query was ex-
panded with the term side effects to emphasize the aspect of the problem requiring an
intervention. The final query for the question became:
(((?analgesics?[TIAB] NOT Medline[SB]) OR ?analgesics?[MeSH Terms] OR
?analgesics?[Pharmacological Action] OR analgesic[Text Word]) AND
((?headache?[TIAB] NOT Medline[SB]) OR ?headache?[MeSH Terms] OR
headaches[Text Word]) AND (?adverse effects?[Subheading] OR side effects[Text
Word])) AND hasabstract[text] AND English[Lang] AND ?humans?[MeSH Terms]
84
Demner-Fushman and Lin Answering Clinical Questions
The first author, who is a medical doctor, performed the query formulation process
manually for every question in our collection; she verified that each hit list contained at
least some relevant documents and that the results were as good as could be reasonably
achieved. The process of generating queries averaged about 40 minutes per question.
The top 50 results for each query were retained for our experiments. In total, 2,309
citations were gathered because some queries returned fewer than 50 hits. The process
of generating a ?good? PubMed query is not a trivial task, which we have side-stepped
in this work by placing a human in the loop. We return to this issue in Section 12.
All abstracts gathered by this process were exhaustively examined for relevance by
the first author. It is important to note that relevance assessment in the clinical domain
requires significant medical knowledge (in short, a medical degree). After careful con-
sideration, we decided to assess only topical relevance, with the understanding that
the applicability of information from a specific citation in real-world settings depends
on a variety of other factors (see Section 10 for further discussion). Each citation was
assigned one of four labels:
 Contains answer: The citation directly contains information that answers
the question.
 Relevant: The citation does not directly answer the question, but provides
topically relevant information.
 Partially relevant: The citation provides information that is marginally
relevant.
 Not relevant: The citation does not provide any topically relevant
information.
Because all abstracts were judged, we did not have to worry about impartiality
issues when comparing different systems. In total, the relevance assessment process
took approximately 100 hours, or about an average of 2 hours per question.
Our reranking experiment compared four different systems:
 The baseline PubMed results.
 A term-based reranker that computes term overlap between the natural
language question and the citation (i.e., counted words shared between the
two strings). Each term match was weighted by the outcome score of the
sentence from which it came (see Section 5.7). This simple algorithm favors
term matches that occur in sentences recognized as outcome statements.
 A reranker based on the EBM scorer described in the previous section.
 A reranker that combines normalized scores from the term-based reranker
and the EBM-based reranker (weighted linear interpolation).
Questions in the development set were used to debug the EBM-based reranker as
we implemented the scoring algorithm. The development questions were also used to
tune the weight for combining scores from the term-based scorer and EBM-based scorer;
by simply trying all possible values, we settled on a ? of 0.8, that is, 80% weight to the
EBM score, and 20% weight to the term-based score. As we shall see later, it is unclear
if evidence combination in this simple manner helps at all; for one, it is debatable
which metric should be optimized. The test questions were hidden during the system
85
Computational Linguistics Volume 33, Number 1
development phase and served as a blind held-out test set for assessing the generality
of our algorithm.
In our experiment, we collected the following metrics, all computed automatically
using our relevance judgments:
 Precision at ten retrieved documents (P10) measures the fraction of
relevant documents in the top ten results.
 Mean Average Precision (MAP) is the average of precision values after
each relevant document is retrieved (Baeza-Yates and Ribeiro-Neto 1999).
It is the most widely accepted single-value metric in information retrieval,
and is seen to balance the need for both precision and recall.
 Mean Reciprocal Rank (MRR) is a measure of how far down a hit list the
user must browse before encountering the first relevant result. The score is
equal to the reciprocal of the rank, that is, a relevant document at rank 1
gets a score of 1, 1/2 at rank 2, 1/3 at rank 3, and so on. Note that this
measure only captures the appearance of the first relevant document.
Furthermore, due to its discretization, MRR values are noisy on small
collections.
 Total Document Reciprocal Rank (TDRR) is the sum of the reciprocal
ranks of all relevant documents. For example, if relevant documents were
found at ranks 2 and 5, the TDRR would be 1/2 + 1/5 = 0.7. TDRR
provides an advantage over MRR in that it captures the ranks of all
relevant documents?emphasizing their appearance at higher ranks. The
downside, however, is that TDRR does not have an intuitive interpretation.
For our reranking experiment, we applied the Wilcoxon signed-rank test to deter-
mine the statistical significance of the results. This test is commonly used in information
retrieval research because it makes minimal assumptions about the underlying distrib-
ution of differences. For each evaluation metric, significance at the 1% level is indicated
by either  or , depending on the direction of change; significance at the 5% level
is indicated by  or , depending on the direction of change. Differences that are not
statistically significant are marked with the symbol ?.
We report results under two different scoring criteria. Under the lenient condition,
documents marked ?contains answer? and ?relevant? were given credit; these results
are shown in Table 7 (for the development set) and Table 8 (for the blind held-out test
set). Across all questions, both the EBM-based reranker and combination reranker sig-
nificantly outperform the PubMed baseline on all metrics. In many cases, the differences
are particularly noteworthy?for example, our EBM citation scoring algorithm more
than doubles the baseline in terms of MAP and P10 on the test set. There are enough
therapy questions to achieve statistical significance in the task-specific results; however,
due to the smaller number of questions for the other clinical tasks, those results are
not statistically significant. Results also show that the simple term-based reranker out-
performs the PubMed baseline, demonstrating the importance of recognizing outcome
statements in MEDLINE abstracts.
Are the differences in performance between the term-based, EBM, and combination
rerankers statistically significant? Results of Wilcoxon signed-rank tests are shown in
Table 11. Both the EBM and combination rerankers significantly outperform the term-
based reranker (at the 1% level, on all metrics, on both development and test set), with
86
Demner-Fushman and Lin Answering Clinical Questions
Table 7
(Lenient, Development) Lenient results of reranking experiment on development questions
for the baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.300 0.367 0.400 0.533
Term 0.520 (+73%) 0.383 (+4.5%)? 0.433 (+8.3%)? 0.553 (+3.8%)?
EBM 0.730 (+143%) 0.800 (+118%) 0.633 (+58%)? 0.553 (+3.7%)?
Combo 0.750 (+150%) 0.783 (+114%) 0.633 (+58%)? 0.573 (+7.5%)?
Mean Average Precision (MAP)
PubMed 0.354 0.421 0.385 0.608
Term 0.622 (+76%) 0.438 (+4.0%)? 0.464 (+21%)? 0.720 (+18%)?
EBM 0.819 (+131%) 0.794 (+89%) 0.635 (+65%)? 0.649 (+6.7%)?
Combo 0.813 (+130%) 0.759 (+81%) 0.644 (+67%)? 0.686 (+13%)?
Mean Reciprocal Rank (MRR)
PubMed 0.428 0.792 0.733 0.900
Term 0.853 (+99%) 0.739 (?6.7%)? 0.833 (+14%)? 1.000 (+11%)?
EBM 0.933 (+118%) 0.917 (+16%)? 0.667 (?9.1%)? 1.000 (+11%)?
Combo 0.933 (+118%) 0.917 (+16%)? 1.000 (+36%)? 0.900 (+0.0%)?
Total Document Reciprocal Rank (TDRR)
PubMed 1.317 1.805 1.778 2.008
Term 2.305 (+75%) 1.887 (+4.6%)? 1.923 (+8.2%)? 2.291 (+14%)?
EBM 2.869 (+118%) 2.944 (+63%) 2.238 (+26%)? 2.104 (+4.8%)?
Combo 2.833 (+115%) 2.870 (+59%) 2.487 (+40%)? 2.108 (+5.0%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.378 0.428 0.656 1.640
Term 0.482 (+28%)? 0.577 (+35%) 0.853 (+30%)? 2.150 (+31%)
EBM 0.699 (+85%) 0.754 (+76%) 0.910 (+39%) 2.650 (+62%)
Combo 0.707 (+87%) 0.752 (+76%) 0.931 (+42%) 2.648 (+61%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
the exception of MRR on the development set. However, for all metrics, on both the
development set and test set, there is no significant difference between the EBM and
combination reranker (which combines both term-based and EBM-based evidence). In
the parameter tuning process, we could not find a weight where performance across all
measures was higher; in the end, we settled on what we felt was a reasonable weight
that improved P10 and MRR on the development set.
Under the strict condition, only documents marked ?contains answer? were given
credit; these results are shown in Table 9 (for the development set) and Table 10
(for the blind held-out test set). The same trend is observed?in fact, larger relative
gains were achieved under the strict scoring criteria for our EBM and combination
87
Computational Linguistics Volume 33, Number 1
Table 8
(Lenient, Test) Lenient results of reranking experiment on blind held-out test questions for
the baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.350 0.150 0.200 0.320
Term 0.575 (+64%) 0.383 (+156%)? 0.333 (+67%)? 0.460 (+43%)?
EBM 0.783 (+124%) 0.583 (+289%) 0.467 (+133%)? 0.660 (+106%)?
Combo 0.792 (+126%) 0.633 (+322%) 0.433 (+117%)? 0.660 (+106%)?
Mean Average Precision (MAP)
PubMed 0.421 0.279 0.235 0.364
Term 0.563 (+34%) 0.489 (+76%)? 0.415 (+77%)? 0.480 (+32%)?
EBM 0.765 (+82%) 0.637 (+129%) 0.722 (+207%)? 0.701 (+93%)?
Combo 0.770 (+83%) 0.653 (+134%) 0.690 (+194%)? 0.687 (+89%)?
Mean Reciprocal Rank (MRR)
PubMed 0.579 0.443 0.456 0.540
Term 0.660 (+14%)? 0.765 (+73%)? 0.611 (+34%)? 0.650 (+20%)?
EBM 0.917 (+58%) 0.889 (+101%)? 1.000 (+119%)? 1.000 (+85%)?
Combo 0.958 (+66%) 0.917 (+107%)? 1.000 (+119%)? 1.000 (+85%)?
Total Document Reciprocal Rank (TDRR)
PubMed 1.669 0.926 0.895 1.381
Term 2.204 (+32%) 1.880 (+103%)? 1.390 (+55%)? 1.736 (+26%)?
EBM 2.979 (+79%) 2.341 (+153%) 2.101 (+138%)? 2.671 (+93%)?
Combo 3.025 (+81%) 2.380 (+157%) 2.048 (+129%)? 2.593 (+88%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.281 0.356 0.526 1.353
Term 0.481 (+71%) 0.513 (+44%) 0.677 (+29%)? 1.945 (+44%)
EBM 0.677 (+141%) 0.718 (+102%) 0.936 (+78%) 2.671 (+98%)
Combo 0.688 (+145%) 0.718 (+102%) 0.962 (+83%) 2.680 (+98%)
(b) Performance across all clinical tasks
Significance at the 1% level, depending on direction of change.
Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
rerankers. Results of Wilcoxon signed-rank tests on the term-based, EBM, and com-
bination rerankers are also shown in Table 11 for the strict scoring condition. In most
cases, combining term scoring with EBM scoring does not help. In almost all cases,
the EBM and combination reranker perform significantly better than the term-based
reranker.
How does better ranking of citations impact end-to-end question answering perfor-
mance? We shall return to this issue in Sections 9 and 10, which describe and evaluate
the answer generation module, respectively. In the next section, we describe more
detailed experiments with our EBM citation scoring algorithm.
88
Demner-Fushman and Lin Answering Clinical Questions
Table 9
(Strict, Development) Strict results of reranking experiment on development questions for the
baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.130 0.133 0.100 0.253
Term 0.230 (+77%)? 0.217 (+63%)? 0.233 (+133%)? 0.293 (+16%)?
EBM 0.350 (+170%) 0.350 (+163%)? 0.267 (+167%)? 0.293 (+16%)?
Combo 0.350 (+170%) 0.367 (+175%)? 0.300 (+200%)? 0.313 (+24%)?
Mean Average Precision (MAP)
PubMed 0.088 0.108 0.058 0.164
Term 0.205 (+134%)? 0.142 (+32%)? 0.090 (+54%)? 0.246 (+50%)?
EBM 0.314 (+260%)? 0.259 (+140%)? 0.105 (+79%)? 0.265 (+62%)?
Combo 0.301 (+244%)? 0.248 (+130%)? 0.129 (+122%)? 0.273 (+67%)?
Mean Reciprocal Rank (MRR)
PubMed 0.350 0.453 0.394 0.367
Term 0.409 (+17%)? 0.581 (+28%)? 0.528 (+34%)? 0.700 (+91%)?
EBM 0.675 (+93%) 0.756 (+67%)? 0.444 (+13%)? 0.800 (+118%)?
Combo 0.569 (+63%) 0.676 (+49%)? 0.833 (+111%)? 0.700 (+91%)?
Total Document Reciprocal Rank (TDRR)
PubMed 0.610 0.711 0.568 0.721
Term 0.872 (+43%)? 1.022 (+44%)? 0.804 (+42%)? 1.224 (+70%)?
EBM 1.434 (+135%) 1.601 (+125%)? 0.824 (+45%)? 1.298 (+80%)?
Combo 1.282 (+110%) 1.502 (+111%)? 1.173 (+106%)? 1.241 (+72%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.153 0.105 0.385 0.653
Term 0.240 (+57%) 0.183 (+75%)? 0.527 (+37%) 0.974 (+49%)
EBM 0.328 (+115%) 0.264 (+152%) 0.693 (+80%) 1.371 (+110%)
Combo 0.340 (+123%) 0.260 (+148%) 0.656 (+71%) 1.315 (+101%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
8. Optimizing Citation Scoring
A potential, and certainly valid, criticism of our EBM citation scoring algorithm is its
ad hoc nature. Weights for various features were assigned based on intuition, reflecting
our understanding of the domain and our knowledge about the principles of evidence-
based medicine. Parameters were fine-tuned during the system implementation process
by actively working with the development set; however, this was not done in any
systematic fashion. Nevertheless, results on the blind held-out test set confirm the
generality of our citation scoring algorithm.
89
Computational Linguistics Volume 33, Number 1
Table 10
(Strict, Test) Strict results of reranking experiment on blind held-out test questions for the
baseline PubMed condition, term-based reranker, EBM-based reranker, and combination
reranker.
Therapy Diagnosis Prognosis Etiology
Precision at 10 (P10)
PubMed 0.108 0.017 0.000 0.080
Term 0.192 (+77%)? 0.133 (+700%)? 0.033 ? 0.140 (+75%)?
EBM 0.233 (+115%)? 0.167 (+900%)? 0.100 ? 0.200 (+150%)?
Combo 0.258 (+139%) 0.200 (+1100%)? 0.100 ? 0.220 (+175%)?
Mean Average Precision (MAP)
PubMed 0.061 0.024 0.015 0.050
Term 0.082 (+36%)? 0.118 (+386%)? 0.086 (+464%)? 0.086 (+74%)?
EBM 0.109 (+80%)? 0.091 (+276%)? 0.234 (+1442%)? 0.159 (+220%)?
Combo 0.120 (+99%)? 0.107 (+339%)? 0.224 (+1372%)? 0.165 (+232%)?
Mean Reciprocal Rank (MRR)
PubMed 0.282 0.073 0.031 0.207
Term 0.368 (+31%)? 0.429 (+488%)? 0.146 (+377%)? 0.314 (+52%)?
EBM 0.397 (+41%)? 0.431 (+490%)? 0.465 (+1422%)? 0.500 (+142%)?
Combo 0.556 (+97%) 0.422 (+479%)? 0.438 (+1331%)? 0.467 (+126%)?
Total Document Reciprocal Rank (TDRR)
PubMed 0.495 0.137 0.038 0.331
Term 0.700 (+41%)? 0.759 (+454%)? 0.171 (+355%)? 0.596 (+80%)?
EBM 0.807 (+63%)? 0.654 (+377%)? 0.513 (+1262%)? 0.946 (+186%)?
Combo 0.969 (+96%) 0.698 (+409%)? 0.479 (+1172%)? 0.975 (+195%)?
(a) Breakdown by clinical task
P10 MAP MRR TDRR
PubMed 0.069 0.045 0.190 0.328
Term 0.150 (+117%) 0.092 (+105%) 0.346 (+82%) 0.632 (+93%)
EBM 0.196 (+183%) 0.129 (+187%) 0.433 (+127%) 0.765 (+133%)
Combo 0.219 (+217%) 0.138 (+207%) 0.494 (+160%) 0.851 (+159%)
(b) Performance across all clinical tasks
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
In the development of various language technology applications, it is common for
the first materialization of a new capability to be rather ad hoc in its implementation.
This is a reflection of an initial attempt to understand both the problem and solution
spaces. Subsequent systems, with a better understanding of the possible technical ap-
proaches and their limitations, are then able to implement a more principled solution.
Because our clinical question-answering system is the first of its type that we are aware
of, in terms of both depth and scope, it is inevitable that our algorithms suffer from
some of these limitations. Similarly, our collection of clinical questions is the first test
collection of its type that we are aware of. Typically, construction of formal models is
only made possible by the existence of test collections. We hope that our work sheds new
insight on question answering in the clinical domain and paves the way for future work.
90
Demner-Fushman and Lin Answering Clinical Questions
Table 11
Performance differences between various rerankers.
P10 MAP MRR TDRR
Development Set
EBM vs. Term +45.0%  +30.8%  +6.7% ? +23.3% 
Combo vs. Term +46.7%  +30.4%  +9.1% ? +23.2% 
Combo vs. EBM +1.2% ? ?0.3% ? +2.3% ? ?0.1% ?
Test Set
EBM vs. Term +40.8  +40.1%  +38.3%  +37.3% 
Combo vs. Term +43.2  +40.0%  +42.1%  +37.8% 
Combo vs. EBM +1.7 ? ?0.1% ? +2.7% ? +0.3% ?
(a) Lenient Scoring
P10 MAP MRR TDRR
Development Set
EBM vs. Term +36.4%  +43.8%  +31.3% ? +40.7% 
Combo vs. Term +41.6%  +41.9%  +24.5% ? +35.0% 
Combo vs. EBM +3.8% ? ?1.3% ? ?5.2% ? ?4.1% ?
Test Set
EBM vs. Term +30.8 ? +40.4%  +24.9% ? +20.9% ?
Combo vs. Term +46.2  +50.0%  +42.8%  +34.6% 
Combo vs. EBM +11.8  +6.8% ? +14.3% ? +11.3% ?
(b) Strict Scoring
 Significance at the 1% level, depending on direction of change.
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
In addition, there are some theoretical obstacles for developing a more formal (say,
generative) model. Most methods for training such models require independently and
identically distributed samples from the underlying distribution?which is certainly not
the case with our test collection. Moreover, the event space of queries and documents
is extremely large or even infinite, depending on how it is defined. Our training data,
assumed to be samples from this underlying distribution, is extremely small compared
to the event space, and hence it is unlikely that popular methods (e.g., maximum
likelihood estimates) would yield an accurate characterization of the true distribution.
Furthermore, many techniques for automatically setting parameters make use
of maximum likelihood techniques?which do not maximize the correct objective
function. Maximizing the likelihood of generating the training data does not mean
that the evaluation metric under consideration (e.g., mean average precision) is also
maximized?this phenomenon is known as metric divergence.
Nevertheless, it is important to better understand the effects of parameter settings
in our system. This section describes a few experiments aimed at this goal.
The EBM score of a MEDLINE citation is the sum of three separate components,
each representing a facet of evidence-based medicine. This structure naturally suggests
a modification to Equation (3) that weights each score component differently:
SEBM = ?1SPICO + ?2SSoE + (1 ? ?1 ? ?2)Stask (8)
91
Computational Linguistics Volume 33, Number 1
Figure 3
The MAP performance surface for ?1 and ?2.
Table 12
Results of optimizing ?1 and ?2 on therapy questions.
P10 MAP MRR TDRR
Development Test
Baseline 0.730 0.819 0.933 2.869
Optimized 0.760 (+4.1%)? 0.822 (+0.4%)? 0.933 (+0.0%)? 2.878 (+0.3%)?
Test Test
Baseline 0.783 0.765 0.917 2.979
Optimized 0.783 (+0.0%)? 0.762 (?0.4%)? 0.917 (+0.0%)? 2.972 (?0.2%)?
?Difference not statistically significant.
The parameters ?1 and ?2 can be derived from our development set. For therapy
questions, we exhaustively searched through the entire parameter space, in increments
of hundredths, and determined the optimal settings to be ?1 = 0.38, ?2 = 0.34 (which
was found to slightly improve all metrics). The performance surface for mean average
precision is shown in Figure 3, which plots results for all possible parameter values
on the development set. Numeric results are shown in Table 12. It can be seen that
optimizing the parameters in this fashion does not lead to a statistically significant
increase in any of the metrics. Furthermore, these gains do not carry over to the blind
held-out test set. We also tried optimizing the ??s on all questions in the development
set. These results are shown in Table 13. Once again, differences are not statistically
significant.
Why does parameter optimization not help? We believe that there are two factors
at play here: On the one hand, parameter settings should be specific to the clinical
task. This explains why optimizing across all question types at the same time did
not improve performance. On the other hand, there are too few questions of any
particular type to represent an accurate sampling of all possible questions. This is why
parameter tuning on therapy questions did not significantly alter performance. These
experiments point to the need for larger test collections, which is an area for future
work.
92
Demner-Fushman and Lin Answering Clinical Questions
Table 13
Results of optimizing ?1 and ?2 on all questions.
P10 MAP MRR TDRR
Development Test
Baseline 0.699 0.754 0.910 2.650
Optimized 0.707 (+1.2%)? 0.755 (+0.1%)? 0.918 (+0.9%)? 2.660 (+0.4%)?
Test Test
Baseline 0.677 0.718 0.936 2.671
Optimized 0.669 (?1.1%)? 0.716 (?0.3%)? 0.936 (+0.0%)? 2.662 (?0.3%)?
?Difference not statistically significant.
Table 14
Results of assigning uniform weights to the EBM score component based on the clinical task.
P10 MAP MRR TDRR
Development Test
Baseline 0.699 0.754 0.910 2.650
?(t) = ?1 0.690 (?1.2%)? 0.738 (?2.1%)? 0.927 (+1.9%)? 2.646 (?0.2%)?
Test Test
Baseline 0.677 0.718 0.936 2.671
?(t) = ?1 0.627 (?7.4%) 0.681 (?5.2%)? 0.913 (?2.4%)? 2.519 (?5.7%)?
 Significance at the 5% level, depending on direction of change.
?Difference not statistically significant.
Another component of our EBM citation scoring algorithm that contains many
ad hoc weights is Stask, defined in Equation (7) and repeated here:
Stask =
?
t?MeSH
?(t) (9)
The function ?(t) maps a particular MeSH term to a weight that quantifies the
degree to which it is a positive or negative indicator for the particular clinical task.
Because these weights were heuristically assigned, it would be worthwhile to examine
the impact they have on performance. As a variant, we modified ?(t) so that all MeSH
terms were mapped to ?1; in other words, we did not encode granular levels of
?goodness.? These results are shown in Table 8. Although performance dropped across
all metrics, none of the differences were statistically significant except for P10 on the
test set.
The series of experiments described herein help us better understand the effects of
parameter settings on abstract reranking performance. As can be seen from the results,
our algorithm is relatively invariant with respect to the choice of parameters, con-
firming that our primary contribution is the EBM-based approach to clinical question
93
Computational Linguistics Volume 33, Number 1
answering, and that our performance gains cannot be simply attributed to a fortunate
choice of parameters.
9. From Scoring Citations to Answering Questions
The aim of question-answering technology is to move from the ?hit list? paradigm of
information retrieval, where users receive a list of potentially relevant documents that
they must then browse through, to a mode of interaction where users directly receive
responses that satisfy their information needs. In our current architecture, fetching a
higher-quality ranked list is a step towards generating responsive answers.
The most important characteristic of answers, as recommended by Ely et al (2005)
in their study of real-world physicians, is that they focus on bottom-line clinical
advice?information that physicians can directly act on. Ideally, answers should in-
tegrate information from multiple clinical studies, pointing out both similarities and
differences. The system should collate concurrences, that is, if multiple abstracts ar-
rive at the same conclusion?it need not be repeated unless the physician wishes to
?drill down?; the system should reconcile contradictions, for example, if two abstracts
disagree on a particular treatment because they studied different patient populations.
We have noted that many of these desiderata make complex question answering quite
similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these
features are also beyond the capabilities of current summarization systems.
It is clear that the type of answers desired by physicians require a level of semantic
analysis that is beyond the current state of the art, even with the aid of existing medical
ontologies. For example, even the seemingly straightforward task of identifying simi-
larities and differences in outcome statements is rendered exceedingly complex by the
tremendous amount of background medical knowledge that must be brought to bear
in interpreting clinical results and subtle differences in study design, objectives, and
results; the closest analogous task in computational linguistics?redundancy detection
for multi-document summarization?seems easy by comparison. Furthermore, it is
unclear if textual strings make ?good answers.? Perhaps a graphical rendering of the
semantic predicates present in relevant abstracts might more effectively convey the
desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Per-
haps some variation of multi-level bulleted lists, appropriately integrated with interface
elements for expanding and hiding items, might provide physicians a better overview
of the information landscape; see, for example, Demner-Fushman and Lin (2006).
Recognizing this complex set of issues, we decided to take a simple extractive
approach to answer generation. For each abstract in our reranked list of citations,
our system produces an answer by combining the title of the abstract and the top
three outcome sentences (in the order they appeared in the abstract). We employed the
outcome scores generated by the regression model. No attempt was made to synthesize
information from multiple citations. A formal evaluation of this simple approach to
answer generation is presented in the next section.
10. Evaluation of Clinical Answers
Evaluation of answers within a clinical setting involves a complex decision that must
not only take into account topical relevance (i.e., ?Does the answer address the infor-
mation need??), but also situational relevance (e.g., Saracevic 1975, Barry and Schamber
94
Demner-Fushman and Lin Answering Clinical Questions
1998). The latter factor includes many issues such as the strength of evidence, recency
of results, and reputation of the journal. Clinicians need to carefully consider all these
elements before acting on any information for the purposes of patient care. Within the
framework of evidence-based medicine, the physician is the final arbiter of how clinical
answers are integrated into the broader activities of medical care, but this complicates
any attempt to evaluate answers generated by our system.
In assessing answers produced by our system, we decided to focus only on the
evaluation of topical relevance?assessors were only presented with answer strings,
generated in the manner described in the previous section. Metadata that would con-
tribute to judgments about situational relevance, such as the strength of evidence,
names of the authors and the journal, and so on, were purposefully suppressed. Our
evaluation compared the top five answers generated from the original PubMed hit list
and the top five answers generated from our reranked list of citations. Answers were
prepared for all 24 questions in our development set.
We recruited two medical doctors (one family practitioner, one surgeon) from the
National Library of Medicine to evaluate the textual answers. Our instructions clearly
stated that only topical relevance was to be assessed. We asked the physicians to provide
three-valued judgments:
 A plus (+) indicates that the response directly answers the question.
Naturally, the physicians would need to follow up and examine the source
citation in more detail.
 A check (
?
) indicates that the response provides clinically relevant
information that may factor into decisions about patient treatment, and
that the source citation was worth examining in more detail.
 A minus (?) indicates that the response does not provide useful
information in answering the clinical question, and that the source citation
was not worth examining.
We purposely avoided short linguistic labels for the judgments so as to sidestep
the question of ?What exactly is an answer to a clinical question?? Informally, an-
swers marked with a plus can be considered ?actionable? clinical advice. Answers
marked with a check provide relevant information that may influence the physician?s
actions.
We adopted a double-blind study design for the actual assessment process: Answers
from both systems were presented in a randomized order without any indication of
which system the response came from (duplicates were suppressed). A paper printout,
containing each question followed by the blinded answers, was presented to each
assessor. We then coded the relevance judgments in a plain text file manually. During
this entire time, the key that maps answers to systems was kept in a separate file and
hidden from everyone, including the authors. All scores were computed automatically
without human intervention.
Answer precision was calculated for two separate conditions: Under the strict
condition (Table 15), only ?plus? judgments were considered good; under the lenient
condition (Table 16), both ?plus? and ?check? judgments were considered good. As can
be seen, our EBM algorithm significantly outperforms the baseline under both the strict
and lenient conditions, according to both assessors. On average, the length of answers
generated from the original PubMed list of citations was 90 words; answers generated
from the reranked list of citations averaged 87 words. Answers from both sources
95
Computational Linguistics Volume 33, Number 1
Table 15
Strict answer precision (considering only ?plus? judgments).
Therapy Diagnosis Prognosis Etiology All
Assessor 1
Baseline .160 .233 .333 .480 .267
EBM .260 (+63%) .367 (+58%) .333 (+0%) .600 (+25%) .367 (+37%)
Assessor 2
Baseline .040 .233 .200 .400 .183
EBM .200 (+400%) .300 (+29%) .266 (+33%) .560 (+40%) .308 (+68%)
Table 16
Lenient answer precision (considering both ?plus? and ?check? judgments).
Therapy Diagnosis Prognosis Etiology All
Assessor 1
Baseline .400 .300 .533 .520 .417
EBM .640 (+60%) .567 (+89%) .400 (?25%) .640 (+23%) .592 (+42%)
Assessor 2
Baseline .240 .267 .333 .440 .300
EBM .520 (+117%) .600 (+125%) .400 (+20%) .560 (+27%) .533 (+78%)
were significantly shorter than the abstracts from which they were extracted (250 word
average for original PubMed results and 270 word average for reranked results).
To give a feel for the types of responses that are generated by our system, consider
the following question:
What is the best treatment for analgesic rebound headaches?
The following is an example of a response that received a ?plus? judgment:
Medication overuse headache from antimigraine therapy: clinical features,
pathogenesis and management: Because of easy availability and low expense, the
greatest problem appears to be associated with barbiturate-containing combination
analgesics and over-the-counter caffeine-containing combination analgesics. The best
management advice is to raise awareness and strive for prevention. Reduction in
headache risk factors should include behavioural modification approaches to headache
control earlier in the natural history of migraine.
This answer was accepted by both physicians because it clearly states that specific
analgesics are most likely to cause the problem, and gives a direct guideline for preven-
tive treatment.
In contrast, the following response to the same question received a ?check?:
Does chronic daily headache arise de novo in association with regular use of
analgesics? Regular use of analgesics preceded the onset of daily headache in 5 patients
by a mean of 5.4 years (range, 2 to 10 years). In 1 patient, the onset of daily headache
preceded regular use of analgesics by almost 30 years. These findings suggest that
individuals with primary headache, specifically migraine, are predisposed to
developing chronic daily headache in association with regular use of analgesics.
96
Demner-Fushman and Lin Answering Clinical Questions
Although this answer provides information about the risks and causes of the
headaches, neither prevention nor treatment is explicitly mentioned. For these reasons
this response was marked as potentially leading to an answer, but not as containing one.
To summarize, we have presented a simple answer generation algorithm that
is capable of supplying clinically relevant responses to physicians. Compared to
PubMed, which does not take into account the principles of evidence-based medicine,
our question-answering system represents a leap forward in information access
capabilities.8
11. Related Work and Discussion
Clinical question answering is an emerging area of research that has only recently begun
to receive serious attention. As a result, there exist relatively few points of comparison to
our own work, as the research space is sparsely populated. In this section, however, we
will attempt to draw connections to other clinical information systems (although not
necessarily for question answering) and related domain-specific question-answering
systems. For an overview of systems designed to answer open-domain factoid ques-
tions, the TREC QA track overview papers are a good place to start (Voorhees and
Tice 1999). In addition, there has been much work on the application of linguistic and
semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for
a brief overview.
The idea that clinical information systems should be sensitive to the practice of
evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations,
Mendonc?a and Cimino (2001) have studied MeSH terms associated with the four basic
clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to auto-
matically classify citations for task-specific retrieval, similar in spirit to the Hedges
Project (Haynes et al 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and
Mendonc?a reported good performance for etiology, diagnosis, and in particular therapy,
but not prognosis. Although originally developed as a tool to assist in query formu-
lation, Booth (2000) pointed out that PICO frames can be employed to structure IR
results for improving precision. PICO-based querying in information retrieval is merely
an instance of faceted querying, which has been widely used by librarians since the
introduction of automated retrieval systems (e.g., Meadow et al 1989). The work of
Hearst (1996) demonstrates that faceted queries can be converted into simple filtering
constraints to boost precision.
The feasibility of automatically identifying outcome statements in secondary
sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates
the importance of semantic classes and relations. However, extraction of outcome state-
ments from secondary sources (meta-analyses, in this case) differs from extraction of
outcomes from MEDLINE citations because secondary sources represent knowledge
that has already been distilled by humans (which may limit its scope). Because sec-
ondary sources are often more consistently organized, it is possible to depend on
certain surface cues for reliable extraction (which is not possible for MEDLINE ab-
stracts in general). Our study tackles outcome identification in primary medical sources
and demonstrates that respectable performance is possible with a feature-combination
approach.
8 Although note that answer generation from the PubMed results also requires the use of the outcome
extractor.
97
Computational Linguistics Volume 33, Number 1
The literature also contains work on sentence-level classification of MEDLINE
abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) de-
scribe a machine learning approach to automatically label sentences as belonging to
introduction, methods, results, or conclusion using structured abstracts as training data
(see also Lin et al 2006). Tbahriti et al (2006) have demonstrated that differential
weighting of automatically labeled sections can lead to improved retrieval performance.
Note, however, that such labels are orthogonal to PICO frame elements, and hence
are not directly relevant to knowledge extraction for clinical question answering. In a
similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative
statements in MEDLINE abstracts, but once again, this work is not directly applicable
to clinical question answering.
In addition to question answering, multi-document summarization provides a com-
plementary approach to addressing clinical information needs. The PERSIVAL project,
the most comprehensive study of such techniques applied on medical texts to date,
leverages patient records to generate personalized summaries in response to physicians?
queries (McKeown, Elhadad, and Hatzivassiloglou 2003; Elhadad et al 2005). Although
the system incorporates both a user and a task model, it does not explicitly capture
the principles of evidence-based medicine. Patient information is no doubt important
to answering clinical questions, and our work could certainly benefit from experiences
gained in the PERSIVAL project.
The application of domain models and deep semantic knowledge to question
answering has been explored by a variety of researchers (e.g., Jacquemart and
Zweigenbaum 2003, Rinaldi et al 2004), and was also the focus of recent workshops
on question answering in restricted domains at ACL 2004 and AAAI 2005. Our work
contributes to this ongoing discourse by demonstrating a specific application in the
domain of clinical medicine.
Finally, the evaluation of answers to complex questions remains an open research
problem. Although it is clear that measures designed for open-domain factoid questions
are not appropriate, the community has not agreed on a methodology that will allow
meaningful comparisons of results from different systems. In Sections 9 and 10, we
have discussed many of these issues. Recently, there is a growing consensus that an
evaluation methodology based on the notion of ?information nuggets? may provide
an appropriate framework for assessing the quality of answers to complex questions.
Nugget F-score has been employed as a metric in the TREC question-answering track
since 2003, to evaluate so-called definition and ?other? questions (Voorhees 2003). A
number of studies (e.g., Hildebrandt, Katz, and Lin 2004) have pointed out shortcom-
ings of the original nugget scoring model, although a number of these issues have been
recently addressed (Lin and Demner-Fushman 2005a, 2006b). However, adaptation of
the nugget evaluation methodology to a domain as specific as clinical medicine is an
endeavor that has yet to be undertaken.
12. Future Work
The design and implementation of our current system leaves many open avenues for
future exploration, one of which concerns our assumptions about the query interface.
Previously, a user study (Lin et al 2003) has shown that people are reluctant to type
full natural language questions, even after being told that they were using a question-
answering system and that typing complete questions would result in better perform-
ance. We have argued that a query interface based on structured PICO frames will
yield better-formulated queries, although it is unclear whether physicians would invest
98
Demner-Fushman and Lin Answering Clinical Questions
the upfront effort necessary to accomplish this. Issuing extremely short queries appears
to be an ingrained habit of information seekers today, and the dominance of World Wide
Web searches reinforce this behavior. Given these trends, physicians may actually prefer
the rapid back-and-forth interaction style that comes with short queries. We believe
that if systems can produce noticeably better results with richer queries, users will
make more of an effort to formulate them. This, however, presents a chicken-and-egg
problem: One possible solution is to develop models that can automatically fill query
frames given a couple of keywords?this would serve to kick-start the query generation
process.
The astute reader will have noticed that the initial retrieval of abstracts in our
study was performed with high-quality manually crafted queries (that were part of
the test collection). Although this was intended to demonstrate the performance of our
EBM citation scoring algorithm with respect to a strong baseline, it also means that we
have omitted a component in the automatic question-answering process. Translating a
clinical question into a good PubMed query is not a trivial task?in our experiments,
it required an experienced searcher approximately 40 minutes on average per question.
However, it is important to note that query formulation in the clinical domain is not
a problem limited to question-answering systems, but one that users of all retrieval
systems must contend with.
Nevertheless, there are three potential solutions to this problem: First, although
there is an infinite variety of clinical questions, the number of query types is bounded
and far smaller in number; see Huang, Lin, and Demner-Fushman (2006) for an analysis.
In a query interface based on PICO frames, it is possible to identify a number of proto-
typical query frames. From these prototypes, one can generate query templates that ab-
stract over the actual slot fillers?this is the idea behind Clinical Queries. Although this
method will probably not retrieve citations as high in quality as custom-crafted queries,
there is reason to believe that as long as a reasonable set of citations is retrieved, our sys-
tem will be able to extract relevant answers (given the high accuracy of our knowledge
extractors and citation scoring algorithm). The second approach to tackling this problem
is to bypass PubMed altogether and index MEDLINE with another search engine.
Due to the rapidly changing nature of the entire MEDLINE database, experiments for
practical purposes would most likely be conducted on a static subset of the collection,
for example, the ten-year portion created for the TREC 2004 genomics track (Hersh,
Bhupatiraju, and Corley 2004). Recent results from TREC have demonstrated that high
performance ad hoc retrieval is possible in the genomics domain (Hersh et al 2005),
and it is not a stretch to imagine adopting these technologies for clinical tasks. Using
a separate search engine would provide other benefits as well: Greater control over
the document retrieval process would allow one to examine the effects of different
indexing schemes, different query operators, and techniques such as query expansion;
see, for example, Aronson, Rindflesch, and Browne (1994). Finally, yet another way to
solve the document retrieval problem is to eliminate that stage completely. Recall that
our two-stage architecture was a practical expediency, because we did not have access
to the computing resources necessary to pre-extract PICO elements from the entire
MEDLINE database and directly index the results. Given access to more resources,
a system could index identified PICO elements and directly match queries against a
knowledge store.
Finally, answer generation remains an area that awaits further exploration, although
we would have to first define what a good answer should be. We have empirically
verified that an extractive approach based on outcome sentences is actually quite
satisfactory, but our algorithm does not currently integrate evidence from multiple
99
Computational Linguistics Volume 33, Number 1
abstracts; although see Demner-Fushman and Lin (2006). Furthermore, the current an-
swer generator does not handle complex issues such as contradictory and inconsistent
statements. To address these very difficult challenges, finer-grained semantic analysis
of medical texts is required.
13. Conclusion
Our experiments in clinical question answering provide some answers to the broader
research question regarding the role of knowledge-based and statistical techniques in
advanced question answering. This work demonstrates that the two approaches are
complementary and can be seamlessly integrated into algorithms that draw from the
best of both worlds. Explicitly coded semantic knowledge, in the form of UMLS, and
software for leveraging this resource?for example, MetaMap?combine to simplify
many knowledge extraction tasks that would be far more difficult otherwise. The re-
spectable performance of our population, problem, and intervention extractors, all of
which use relatively simple rules, provides evidence that complex clinical problems
can be tackled by appropriate use of ontological knowledge. Explicitly coded semantic
knowledge is less helpful for outcome identification due to the large variety of possi-
ble ?outcomes;? nevertheless, knowledge-rich features can be combined with simple,
statistically derived features to build a good outcome classifier. Overall, this work
demonstrates that the application of a semantic domain model yields clinical question
answering capabilities that significantly outperform presently available technology,
especially when coupled with traditional statistical methods (classification, evidence
combination, etc.).
We have taken an important step in building a complete question-answering system
that assists physicians in the patient care process. Our work demonstrates that the
principles of evidence-based medicine can be computationally captured and imple-
mented in a system, and although we are still far from operational deployment, these
positive results are certainly encouraging. Information systems in support of the clinical
decision-making process have the potential to improve the quality of health care, which
is a worthy goal indeed.
Acknowledgments
We would like to thank Dr. Charles
Sneiderman and Dr. Kin Wah Fung for
the evaluation of the answers. For this
work, D. D-F. was supported by an
appointment to the National Library of
Medicine Research Participation Program
administered by the Oak Ridge Institute
for Science and Education through an
inter-agency agreement between the U.S.
Department of Energy and the National
Library of Medicine. For this work, J. L.
was supported in part by a grant from the
National Library of Medicine, where he
was a visiting researcher during the
summer of 2005. We would like to thank
the anonymous reviewers for their valuable
comments. J. L. would like to thank Kiri
and Esther for their kind support.
References
Ad Hoc Working Group for Critical
Appraisal of the Medical Literature. 1987.
A proposal for more informative abstracts
of clinical articles. Annals of Internal
Medicine, 106:595?604.
Aronson, Alan R. 2001. Effective mapping
of biomedical text to the UMLS
Metathesaurus: The MetaMap program.
In Proceeding of the 2001 Annual Symposium
of the American Medical Informatics
Association (AMIA 2001), pages 17?21,
Portland, OR.
Aronson, Alan R., James G. Mork,
Clifford W. Gay, Susanne M. Humphrey,
and Willie J. Rogers. 2004. The NLM
Indexing Initiative?s Medical Text
Indexer. In Proceedings of the 11th
World Congress on Medical Informatics
100
Demner-Fushman and Lin Answering Clinical Questions
(MEDINFO 2004), pages 268?272,
San Francisco, CA.
Aronson, Alan R., Thomas C. Rindflesch,
and Allen C. Browne. 1994. Exploiting a
large thesaurus for information retrieval.
In Proceedings of RIAO 1994: Intelligent
Multimedia Information Retrieval Systems
and Management, pages 197?216, New York.
Baeza-Yates, Ricardo and Berthier
Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press, New York.
Barry, Carol and Linda Schamber. 1998.
Users? criteria for relevance evaluation:
A cross-situational comparison.
Information Processing and Management,
34(2/3):219?236.
Booth, Andrew. 2000. Formulating the
question. In Andrew Booth and Graham
Walton, editors, Managing Knowledge in
Health Services. Library Association
Publishing, London, England.
Chambliss, M. Lee and Jennifer Conley. 1996.
Answering clinical questions. The Journal of
Family Practice, 43:140?144.
Cogdill, Keith W. and Margaret E. Moore.
1997. First-year medical students?
information needs and resource selection:
Responses to a clinical scenario. Bulletin of
the Medical Library Association, 85(1):51?54.
Covell, David G., Gwen C. Uman, and
Phil R. Manning. 1985. Information needs
in office practice: Are they being met?
Annals of Internal Medicine, 103(4):596?599.
De Groote, Sandra L. and Josephine L.
Dorsch. 2003. Measuring use patterns
of online journals and databases.
Journal of the Medical Library Association,
91(2):231?240.
Demner-Fushman, Dina, Barbara Few,
Susan E. Hauser, and George Thoma. 2006.
Automatically identifying health outcome
information in MEDLINE records. Journal
of the American Medical Informatics
Association, 13(1):52?60.
Demner-Fushman, Dina and Jimmy Lin.
2005. Knowledge extraction for clinical
question answering: Preliminary results.
In Proceedings of the AAAI-05 Workshop on
Question Answering in Restricted Domains,
pages 1?10, Pittsburgh, PA.
Demner-Fushman, Dina and Jimmy Lin.
2006. Answer extraction, semantic
clustering, and extractive summarization
for clinical question answering. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL
2006), pages 841?848, Sydney, Australia.
Ebell, Mark H., Jay Siwek, Barry D. Weiss,
Steven H. Woolf, Jeffrey Susman, Bernard
Ewigman, and Marjorie Bowman. 2004.
Strength of Recommendation Taxonomy
(SORT): A patient-centered approach to
grading evidence in the medical literature.
The Journal of the American Board of Family
Practice, 17(1):59?67.
Elhadad, Noemie, Min-Yen Kan, Judith
Klavans, and Kathleen McKeown. 2005.
Customization in a unified framework for
summarizing medical literature. Journal of
Artificial Intelligence in Medicine,
33(2):179?198.
Ely, John W., Jerome A. Osheroff, Mark H.
Ebell, George R. Bergus, Barcey T. Levy,
M. Lee Chambliss, and Eric R. Evans. 1999.
Analysis of questions asked by family
doctors regarding patient care. BMJ,
319:358?361.
Ely, John W., Jerome A. Osheroff, M. Lee
Chambliss, Mark H. Ebell, and Marcy E.
Rosenbaum. 2005. Answering physicians?
clinical questions: Obstacles and potential
solutions. Journal of the American Medical
Informatics Association, 12(2):217?224.
Fiszman, Marcelo, Thomas C. Rindflesch,
and Halil Kilicoglu. 2004. Abstraction
summarization for managing the
biomedical research literature. In
Proceedings of the HLT/NAACL 2004
Workshop on Computational Lexical
Semantics, pages 76?83, Boston, MA.
Gorman, Paul N., Joan S. Ash, and
Leslie W. Wykoff. 1994. Can primary care
physicians? questions be answered using
the medical journal literature? Bulletin of
the Medical Library Association,
82(2):140?146.
Haynes, R. Brian, Nancy Wilczynski, K. Ann
McKibbon, Cynthia J. Walker, and John C.
Sinclair. 1994. Developing optimal search
strategies for detecting clinically sound
studies in MEDLINE. Journal of the
American Medical Informatics Association,
1(6):447?458.
Hearst, Marti A. 1996. Improving full-text
precision on short queries using simple
constraints. In Proceedings of the Fifth
Annual Symposium on Document Analysis
and Information Retrieval (SDAIR 1996),
pages 217?232, Las Vegas, NV.
Hersh, William, Ravi Teja Bhupatiraju,
and Sarah Corley. 2004. Enhancing
access to the bibliome: The TREC
genomics track. In Proceedings of the
11th World Congress on Medical Informatics
(MEDINFO 2004), pages 773?777,
San Francisco, CA.
101
Computational Linguistics Volume 33, Number 1
Hersh, William, Aaron Cohen, Jianji Yang,
Ravi Teja Bhupatiraju1, Phoebe Roberts,
and Marti Hearst. 2005. TREC 2005
genomics track overview. In Proceedings
of the Fourteenth Text REtrieval Conference
(TREC 2005), Gaithersburg, MD.
Hildebrandt, Wesley, Boris Katz, and Jimmy
Lin. 2004. Answering definition questions
with multiple knowledge sources. In
Proceedings of the 2004 Human Language
Technology Conference and the North
American Chapter of the Association
for Computational Linguistics Annual
Meeting (HLT/NAACL 2004), pages 49?56,
Boston, MA.
Hirschman, Lynette and Robert Gaizauskas.
2001. Natural language question
answering: The view from here. Natural
Language Engineering, 7(4):275?300.
Huang, Xiaoli, Jimmy Lin, and Dina
Demner-Fushman. 2006. Evaluation of
PICO as a knowledge representation for
clinical questions. In Proceeding of the 2006
Annual Symposium of the American Medical
Informatics Association (AMIA 2006),
pages 359?363, Washington, D.C.
Ingwersen, Peter. 1999. Cognitive
information retrieval. Annual Review of
Information Science and Technology, 34:3?52.
Jacquemart, Pierre and Pierre Zweigenbaum.
2003. Towards a medical
question-answering system: A feasibility
study. In Robert Baud, Marius Fieschi,
Pierre Le Beux, and Patrick Ruch, editors,
The New Navigators: From Professionals to
Patients, volume 95 of Actes Medical
Informatics Europe, Studies in Health
Technology and Informatics. IOS Press,
Amsterdam, pages 463?468.
Kauffman, Ralph E., L. A. Sawyer, and
M. L. Scheinbaum. 1992. Antipyretic
efficacy of ibuprofen vs acetaminophen.
American Journal of Diseases of Children,
146(5):622?625.
Light, Marc, Xin Ying Qiu, and Padmini
Srinivasan. 2004. The language of
bioscience: Facts, speculations, and
statements in between. In Proceedings of the
BioLink 2004 Workshop at HLT/NAACL
2004, pages 17?24, Boston, MA.
Lin, Jimmy and Dina Demner-Fushman.
2005a. Automatically evaluating answers
to definition questions. In Proceedings of the
2005 Human Language Technology Conference
and Conference on Empirical Methods in
Natural Language Processing (HLT/EMNLP
2005), pages 931?938, Vancouver, Canada.
Lin, Jimmy and Dina Demner-Fushman.
2005b. Evaluating summaries and
answers: Two sides of the same coin? In
Proceedings of the ACL 2005 Workshop on
Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization, pages 41?48,
Ann Arbor, MI.
Lin, Jimmy and Dina Demner-Fushman.
2006a. The role of knowledge in
conceptual retrieval: A study in the
domain of clinical medicine. In Proceedings
of the 29th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2006),
pages 99?106, Seattle, WA.
Lin, Jimmy and Dina Demner-Fushman.
2006b. Will pyramids built of nuggets
topple over? In Proceedings of the 2006
Human Language Technology Conference
and North American Chapter of the
Association for Computational Linguistics
Annual Meeting (HLT/NAACL 2006),
pages 383?390, New York.
Lin, Jimmy, Damianos Karakos, Dina
Demner-Fushman, and Sanjeev
Khudanpur. 2006. Generative content
models for structural analysis of medical
abstracts. In Proceedings of the HLT/
NAACL 2006 Workshop on Biomedical
Natural Language Processing (BioNLP?06),
pages 65?72, New York.
Lin, Jimmy, Dennis Quan, Vineet Sinha,
Karun Bakshi, David Huynh, Boris Katz,
and David R. Karger. 2003. What makes a
good answer? The role of context in
question answering. In Proceedings of the
Ninth IFIP TC13 International Conference on
Human-Computer Interaction (INTERACT
2003), pages 25?32, Zu?rich, Switzerland.
Lindberg, Donald A., Betsy L. Humphreys,
and Alexa T. McCray. 1993. The Unified
Medical Language System. Methods of
Information in Medicine, 32(4):281?291.
McCray, Alexa T., Anita Burgun, and Olivier
Bodenreider. 2001. Aggregating UMLS
semantic types for reducing conceptual
complexity. In Proceedings of 10th World
Congress on Medical Informatics (MEDINFO
2001), pages 216?220, London, England.
McKeown, Kathleen, Noemie Elhadad,
and Vasileios Hatzivassiloglou. 2003.
Leveraging a common representation for
personalized search and summarization
in a medical digital library. In Proceedings
of the 3rd ACM/IEEE Joint Conference on
Digital Libraries (JCDL 2003), pages
159?170, Houston, TX.
McKnight, Larry and Padmini Srinivasan.
2003. Categorization of sentence types
in medical abstracts. In Proceeding of the
2003 Annual Symposium of the American
102
Demner-Fushman and Lin Answering Clinical Questions
Medical Informatics Association (AMIA
2003), pages 440?444, Washington, D.C.
Meadow, Charles T., Barbara A. Cerny,
Christine L. Borgman, and Donald O.
Case. 1989. Online access to knowledge:
System design. Journal of the American
Society for Information Science, 40(2):86?98.
Mendonc?a, Eneida A. and James J. Cimino.
2001. Building a knowledge base to
support a digital library. In Proceedings
of 10th World Congress on Medical
Informatics (MEDINFO 2001),
pages 222?225, London, England.
Mladenic, Dunja and Marko Grobelnik. 1999.
Feature selection for unbalanced class
distribution and Na??ve Bayes. In
Proceedings of the Sixteenth International
Conference on Machine Learning (ICML
1999), pages 258?267, Bled, Slovenia.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method. In
Proceedings of the 2004 Human Language
Technology Conference and the North
American Chapter of the Association for
Computational Linguistics Annual Meeting
(HLT/NAACL 2004), pages 145?152,
Boston, MA.
Niu, Yun and Graeme Hirst. 2004. Analysis
of semantic classes in medical text for
question answering. In Proceedings of the
ACL 2004 Workshop on Question Answering
in Restricted Domains, pages 54?61,
Barcelona, Spain.
Pratt, Wanda and Meliha Yetisgen-Yildiz.
2003. A study of biomedical concept
identification: MetaMap vs. people.
In Proceeding of the 2003 Annual
Symposium of the American Medical
Informatics Association (AMIA 2003),
pages 529?533, Washington, D.C.
Richardson, W. Scott, Mark C. Wilson, James
Nishikawa, and Robert S. Hayward. 1995.
The well-built clinical question: A key to
evidence-based decisions. American College
of Physicians Journal Club, 123(3):A12?A13.
Rinaldi, Fabio, James Dowdall, Gerold
Schneider, and Andreas Persidis. 2004.
Answering questions in the genomics
domain. In Proceedings of the ACL 2004
Workshop on Question Answering in
Restricted Domains, pages 46?53,
Barcelona, Spain.
Rindflesch, Thomas C. and Marcelo Fiszman.
2003. The interaction of domain
knowledge and linguistic structure in
natural language processing: Interpreting
hypernymic propositions in biomedical
text. Journal of Biomedical Informatics,
36(6):462?477.
Sackett, David L., Sharon E. Straus, W. Scott
Richardson, William Rosenberg, and
R. Brian Haynes. 2000. Evidence-Based
Medicine: How to Practice and Teach EBM,
second edition. Churchill Livingstone,
Edinburgh, Scotland.
Saracevic, Tefko. 1975. Relevance: A review
of and a framework for thinking on the
notion in information science. Journal of the
American Society for Information Science,
26(6):321?343.
Sneiderman, Charles, Dina
Demner-Fushman, Marcelo Fiszman, and
Thomas C. Rindflesch. 2005. Semantic
characteristics of MEDLINE citations
useful for therapeutic decision-making.
In Proceeding of the 2005 Annual Symposium
of the American Medical Informatics
Association (AMIA 2005), page 1117,
Washington, D.C.
Tbahriti, Imad, Christine Chichester,
Fre?de?rique Lisacek, and Patrick Ruch.
2006. Using argumentation to retrieve
articles with similar citations: An inquiry
into improving related articles search in
the MEDLINE digital library. International
Journal of Medical Informatics, 75(6):488?495.
Ting, Kai Ming and Ian H. Witten. 1999.
Issues in stacked generalization. Journal of
Artificial Intelligence Research, 10:271?289.
Voorhees, Ellen M. 2003. Overview of the
TREC 2003 question answering track. In
Proceedings of the Twelfth Text REtrieval
Conference (TREC 2003), pages 54?68,
Gaithersburg, MD.
Voorhees, Ellen M. and Dawn M. Tice.
1999. The TREC-8 question answering
track evaluation. In Proceedings of the
Eighth Text REtrieval Conference (TREC-8),
pages 83?106, Gaithersburg, MD.
Wilczynski, Nancy, K. Ann McKibbon, and
R. Brian Haynes. 2001. Enhancing retrieval
of best evidence for health care from
bibliographic databases: Calibration
of the hand search of the literature. In
Proceedings of 10th World Congress on
Medical Informatics (MEDINFO 2001),
pages 390?393, London, England.
Yang, Yiming and Jan O. Pedersen. 1997.
A comparative study on feature selection
in text categorization. In Proceedings
of the Fourteenth International Conference
on Machine Learning (ICML 1997),
pages 412?420, Nashville, TN.
103

Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 283?287,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation
and Speculation Scopes
Emilia Apostolova
DePaul University
Chicago, IL USA
emilia.aposto@gmail.com
Noriko Tomuro
DePaul University
Chicago, IL USA
tomuro@cs.depaul.edu
Dina Demner-Fushman
National Library of Medicine
Bethesda, MD USA
ddemner@mail.nih.gov
Abstract
Detecting the linguistic scope of negated and
speculated information in text is an impor-
tant Information Extraction task. This paper
presents ScopeFinder, a linguistically moti-
vated rule-based system for the detection of
negation and speculation scopes. The system
rule set consists of lexico-syntactic patterns
automatically extracted from a corpus anno-
tated with negation/speculation cues and their
scopes (the BioScope corpus). The system
performs on par with state-of-the-art machine
learning systems. Additionally, the intuitive
and linguistically motivated rules will allow
for manual adaptation of the rule set to new
domains and corpora.
1 Motivation
Information Extraction (IE) systems often face
the problem of distinguishing between affirmed,
negated, and speculative information in text. For
example, sentiment analysis systems need to detect
negation for accurate polarity classification. Simi-
larly, medical IE systems need to differentiate be-
tween affirmed, negated, and speculated (possible)
medical conditions.
The importance of the task of negation and spec-
ulation (a.k.a. hedge) detection is attested by a num-
ber of research initiatives. The creation of the Bio-
Scope corpus (Vincze et al, 2008) assisted in the de-
velopment and evaluation of several negation/hedge
scope detection systems. The corpus consists of
medical and biological texts annotated for negation,
speculation, and their linguistic scope. The 2010
i2b2 NLP Shared Task1 included a track for detec-
tion of the assertion status of medical problems (e.g.
affirmed, negated, hypothesized, etc.). The CoNLL-
2010 Shared Task (Farkas et al, 2010) focused on
detecting hedges and their scopes in Wikipedia arti-
cles and biomedical texts.
In this paper, we present a linguistically moti-
vated rule-based system for the detection of nega-
tion and speculation scopes that performs on par
with state-of-the-art machine learning systems. The
rules used by the ScopeFinder system are automat-
ically extracted from the BioScope corpus and en-
code lexico-syntactic patterns in a user-friendly for-
mat. While the system was developed and tested us-
ing a biomedical corpus, the rule extraction mech-
anism is not domain-specific. In addition, the lin-
guistically motivated rule encoding allows for man-
ual adaptation to new domains and corpora.
2 Task Definition
Negation/Speculation detection is typically broken
down into two sub-tasks - discovering a nega-
tion/speculation cue and establishing its scope. The
following example from the BioScope corpus shows
the annotated hedging cue (in bold) together with its
associated scope (surrounded by curly brackets):
Finally, we explored the {possible role of 5-
hydroxyeicosatetraenoic acid as a regulator of arachi-
donic acid liberation}.
Typically, systems first identify nega-
tion/speculation cues and subsequently try to
identify their associated cue scope. However,
the two tasks are interrelated and both require
1https://www.i2b2.org/NLP/Relations/
283
syntactic understanding. Consider the following
two sentences from the BioScope corpus:
1) By contrast, {D-mib appears to be uniformly ex-
pressed in imaginal discs }.
2) Differentiation assays using water soluble phor-
bol esters reveal that differentiation becomes irreversible
soon after AP-1 appears.
Both sentences contain the word form appears,
however in the first sentence the word marks a hedg-
ing cue, while in the second sentence the word does
not suggest speculation.
Unlike previous work, we do not attempt to iden-
tify negation/speculation cues independently of their
scopes. Instead, we concentrate on scope detection,
simultaneously detecting corresponding cues.
3 Dataset
We used the BioScope corpus (Vincze et al, 2008)
to develop our system and evaluate its performance.
To our knowledge, the BioScope corpus is the
only publicly available dataset annotated with nega-
tion/speculation cues and their scopes. It consists
of biomedical papers, abstracts, and clinical reports
(corpus statistics are shown in Tables 1 and 2).
Corpus Type Sentences Documents Mean Document Size
Clinical 7520 1954 3.85
Full Papers 3352 9 372.44
Paper Abstracts 14565 1273 11.44
Table 1: Statistics of the BioScope corpus. Document sizes
represent number of sentences.
Corpus Type Negation Cues Speculation Cues Negation Speculation
Clinical 872 1137 6.6% 13.4%
Full Papers 378 682 13.76% 22.29%
Paper Abstracts 1757 2694 13.45% 17.69%
Table 2: Statistics of the BioScope corpus. The 2nd and 3d
columns show the total number of cues within the datasets; the
4th and 5th columns show the percentage of negated and spec-
ulative sentences.
70% of the corpus documents (randomly selected)
were used to develop the ScopeFinder system (i.e.
extract lexico-syntactic rules) and the remaining
30% were used to evaluate system performance.
While the corpus focuses on the biomedical domain,
our rule extraction method is not domain specific
and in future work we are planning to apply our
method on different types of corpora.
4 Method
Intuitively, rules for detecting both speculation and
negation scopes could be concisely expressed as a
Figure 1: Parse tree of the sentence ?T cells {lack active NF-
kappa B } but express Sp1 as expected? generated by the Stan-
ford parser. Speculation scope words are shown in ellipsis. The
cue word is shown in grey. The nearest common ancestor of all
cue and scope leaf nodes is shown in a box.
combination of lexical and syntactic patterns. For
example, O?zgu?r and Radev (2009) examined sample
BioScope sentences and developed hedging scope
rules such as:
The scope of a modal verb cue (e.g. may, might, could)
is the verb phrase to which it is attached;
The scope of a verb cue (e.g. appears, seems) followed
by an infinitival clause extends to the whole sentence.
Similar lexico-syntactic rules have been also man-
ually compiled and used in a number of hedge scope
detection systems, e.g. (Kilicoglu and Bergler,
2008), (Rei and Briscoe, 2010), (Velldal et al,
2010), (Kilicoglu and Bergler, 2010), (Zhou et al,
2010).
However, manually creating a comprehensive set
of such lexico-syntactic scope rules is a laborious
and time-consuming process. In addition, such an
approach relies heavily on the availability of accu-
rately parsed sentences, which could be problem-
atic for domains such as biomedical texts (Clegg and
Shepherd, 2007; McClosky and Charniak, 2008).
Instead, we attempted to automatically extract
lexico-syntactic scope rules from the BioScope cor-
pus, relying only on consistent (but not necessarily
accurate) parse tree representations.
We first parsed each sentence in the training
dataset which contained a negation or speculation
cue using the Stanford parser (Klein and Manning,
2003; De Marneffe et al, 2006). Figure 1 shows the
parse tree of a sample sentence containing a nega-
tion cue and its scope.
Next, for each cue-scope instance within the sen-
tence, we identified the nearest common ancestor
284
Figure 2: Lexico-syntactic pattern extracted from the sentence
from Figure 1. The rule is equivalent to the following string
representation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*)
(NN *scope*))).
which encompassed the cue word(s) and all words in
the scope (shown in a box on Figure 1). The subtree
rooted by this ancestor is the basis for the resulting
lexico-syntactic rule. The leaf nodes of the resulting
subtree were converted to a generalized representa-
tion: scope words were converted to *scope*; non-
cue and non-scope words were converted to *; cue
words were converted to lower case. Figure 2 shows
the resulting rule.
This rule generation approach resulted in a large
number of very specific rule patterns - 1,681 nega-
tion scope rules and 3,043 speculation scope rules
were extracted from the training dataset.
To identify a more general set of rules (and in-
crease recall) we next performed a simple transfor-
mation of the derived rule set. If all children of a
rule tree node are of type *scope* or * (i.e. non-
cue words), the node label is replaced by *scope*
or * respectively, and the node?s children are pruned
from the rule tree; neighboring identical siblings of
type *scope* or * are replaced by a single node of
the corresponding type. Figure 3 shows an example
of this transformation.
(a) The children of nodes JJ/NN/NN are
pruned and their labels are replaced by
*scope*.
(b) The children
of node NP are
pruned and its la-
bel is replaced by
*scope*.
Figure 3: Transformation of the tree shown in Figure 2. The
final rule is equivalent to the following string representation:
(VP (VBP lack) *scope* )
The rule tree pruning described above reduced the
negation scope rule patterns to 439 and the specula-
tion rule patterns to 1,000.
In addition to generating a set of scope finding
rules, we also implemented a module that parses
string representations of the lexico-syntactic rules
and performs subtree matching. The ScopeFinder
module2 identifies negation and speculation scopes
in sentence parse trees using string-encoded lexico-
syntactic patterns. Candidate sentence parse sub-
trees are first identified by matching the path of cue
leaf nodes to the root of the rule subtree pattern. If an
identical path exists in the sentence, the root of the
candidate subtree is thus also identified. The candi-
date subtree is evaluated for a match by recursively
comparing all node children (starting from the root
of the subtree) to the rule pattern subtree. Nodes
of type *scope* and * match any number of nodes,
similar to the semantics of Regex Kleene star (*).
5 Results
As an informed baseline, we used a previously de-
veloped rule-based system for negation and spec-
ulation scope discovery (Apostolova and Tomuro,
2010). The system, inspired by the NegEx algorithm
(Chapman et al, 2001), uses a list of phrases split
into subsets (preceding vs. following their scope) to
identify cues using string matching. The cue scopes
extend from the cue to the beginning or end of the
sentence, depending on the cue type. Table 3 shows
the baseline results.
Correctly Predicted Cues All Predicted Cues
Negation P R F F
Clinical 94.12 97.61 95.18 85.66
Full Papers 54.45 80.12 64.01 51.78
Paper Abstracts 63.04 85.13 72.31 59.86
Speculation
Clinical 65.87 53.27 58.90 50.84
Full Papers 58.27 52.83 55.41 29.06
Paper Abstracts 73.12 64.50 68.54 38.21
Table 3: Baseline system performance. P (Precision), R (Re-
call), and F (F1-score) are computed based on the sentence to-
kens of correctly predicted cues. The last column shows the
F1-score for sentence tokens of all predicted cues (including er-
roneous ones).
We used only the scopes of predicted cues (cor-
rectly predicted cues vs. all predicted cues) to mea-
2The rule sets and source code are publicly available at
http://scopefinder.sourceforge.net/.
285
sure the baseline system performance. The base-
line system heuristics did not contain all phrase cues
present in the dataset. The scopes of cues that are
missing from the baseline system were not included
in the results. As the baseline system was not penal-
ized for missing cue phrases, the results represent
the upper bound of the system.
Table 4 shows the results from applying the full
extracted rule set (1,681 negation scope rules and
3,043 speculation scope rules) on the test data. As
expected, this rule set consisting of very specific
scope matching rules resulted in very high precision
and very low recall.
Negation P R F A
Clinical 99.47 34.30 51.01 17.58
Full Papers 95.23 25.89 40.72 28.00
Paper Abstracts 87.33 05.78 10.84 07.85
Speculation
Clinical 96.50 20.12 33.30 22.90
Full Papers 88.72 15.89 26.95 10.13
Paper Abstracts 77.50 11.89 20.62 10.00
Table 4: Results from applying the full extracted rule set on the
test data. Precision (P), Recall (R), and F1-score (F) are com-
puted based the number of correctly identified scope tokens in
each sentence. Accuracy (A) is computed for correctly identi-
fied full scopes (exact match).
Table 5 shows the results from applying the rule
set consisting of pruned pattern trees (439 negation
scope rules and 1,000 speculation scope rules) on the
test data. As shown, overall results improved signif-
icantly, both over the baseline and over the unpruned
set of rules. Comparable results are shown in bold
in Tables 3, 4, and 5.
Negation P R F A
Clinical 85.59 92.15 88.75 85.56
Full Papers 49.17 94.82 64.76 71.26
Paper Abstracts 61.48 92.64 73.91 80.63
Speculation
Clinical 67.25 86.24 75.57 71.35
Full Papers 65.96 98.43 78.99 52.63
Paper Abstracts 60.24 95.48 73.87 65.28
Table 5: Results from applying the pruned rule set on the test
data. Precision (P), Recall (R), and F1-score (F) are computed
based on the number of correctly identified scope tokens in each
sentence. Accuracy (A) is computed for correctly identified full
scopes (exact match).
6 Related Work
Interest in the task of identifying negation and spec-
ulation scopes has developed in recent years. Rele-
vant research was facilitated by the appearance of a
publicly available annotated corpus. All systems de-
scribed below were developed and evaluated against
the BioScope corpus (Vincze et al, 2008).
O?zgu?r and Radev (2009) have developed a super-
vised classifier for identifying speculation cues and
a manually compiled list of lexico-syntactic rules for
identifying their scopes. For the performance of the
rule based system on identifying speculation scopes,
they report 61.13 and 79.89 accuracy for BioScope
full papers and abstracts respectively.
Similarly, Morante and Daelemans (2009b) de-
veloped a machine learning system for identifying
hedging cues and their scopes. They modeled the
scope finding problem as a classification task that
determines if a sentence token is the first token in
a scope sequence, the last one, or neither. Results
of the scope finding system with predicted hedge
signals were reported as F1-scores of 38.16, 59.66,
78.54 and for clinical texts, full papers, and abstracts
respectively3. Accuracy (computed for correctly
identified scopes) was reported as 26.21, 35.92, and
65.55 for clinical texts, papers, and abstracts respec-
tively.
Morante and Daelemans have also developed a
metalearner for identifying the scope of negation
(2009a). Results of the negation scope finding sys-
tem with predicted cues are reported as F1-scores
(computed on scope tokens) of 84.20, 70.94, and
82.60 for clinical texts, papers, and abstracts respec-
tively. Accuracy (the percent of correctly identified
exact scopes) is reported as 70.75, 41.00, and 66.07
for clinical texts, papers, and abstracts respectively.
The top three best performers on the CoNLL-
2010 shared task on hedge scope detection (Farkas
et al, 2010) report an F1-score for correctly identi-
fied hedge cues and their scopes ranging from 55.3
to 57.3. The shared task evaluation metrics used
stricter matching criteria based on exact match of
both cues and their corresponding scopes4.
CoNLL-2010 shared task participants applied a
variety of rule-based and machine learning methods
3F1-scores are computed based on scope tokens. Unlike our
evaluation metric, scope token matches are computed for each
cue within a sentence, i.e. a token is evaluated multiple times if
it belongs to more than one cue scope.
4Our system does not focus on individual cue-scope pair de-
tection (we instead optimized scope detection) and as a result
performance metrics are not directly comparable.
286
on the task - Morante et al (2010) used a memory-
based classifier based on the k-nearest neighbor rule
to determine if a token is the first token in a scope se-
quence, the last, or neither; Rei and Briscoe (2010)
used a combination of manually compiled rules, a
CRF classifier, and a sequence of post-processing
steps on the same task; Velldal et al(2010) manu-
ally compiled a set of heuristics based on syntactic
information taken from dependency structures.
7 Discussion
We presented a method for automatic extraction
of lexico-syntactic rules for negation/speculation
scopes from an annotated corpus. The devel-
oped ScopeFinder system, based on the automati-
cally extracted rule sets, was compared to a base-
line rule-based system that does not use syntac-
tic information. The ScopeFinder system outper-
formed the baseline system in all cases and exhib-
ited results comparable to complex feature-based,
machine-learning systems.
In future work, we will explore the use of statisti-
cally based methods for the creation of an optimum
set of lexico-syntactic tree patterns and will evalu-
ate the system performance on texts from different
domains.
References
E. Apostolova and N. Tomuro. 2010. Exploring surface-
level heuristics for negation and speculation discovery
in clinical texts. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
81?82. Association for Computational Linguistics.
W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper,
and B.G. Buchanan. 2001. A simple algorithm
for identifying negated findings and diseases in dis-
charge summaries. Journal of biomedical informatics,
34(5):301?310.
A.B. Clegg and A.J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC bioinformatics,
8(1):24.
M.C. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006. Citeseer.
R. Farkas, V. Vincze, G. Mo?ra, J. Csirik, and G. Szarvas.
2010. The CoNLL-2010 Shared Task: Learning to
Detect Hedges and their Scope in Natural Language
Text. In Proceedings of the Fourteenth Conference on
Computational Natural Language Learning (CoNLL-
2010): Shared Task, pages 1?12.
H. Kilicoglu and S. Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC bioinformat-
ics, 9(Suppl 11):S10.
H. Kilicoglu and S. Bergler. 2010. A High-Precision
Approach to Detecting Hedges and Their Scopes.
CoNLL-2010: Shared Task, page 70.
D. Klein and C.D. Manning. 2003. Fast exact infer-
ence with a factored model for natural language pars-
ing. Advances in neural information processing sys-
tems, pages 3?10.
D. McClosky and E. Charniak. 2008. Self-training for
biomedical parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics on Human Language Technologies: Short Papers,
pages 101?104. Association for Computational Lin-
guistics.
R. Morante and W. Daelemans. 2009a. A metalearning
approach to processing the scope of negation. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning, pages 21?29. As-
sociation for Computational Linguistics.
R. Morante and W. Daelemans. 2009b. Learning the
scope of hedge cues in biomedical texts. In Proceed-
ings of the Workshop on BioNLP, pages 28?36. Asso-
ciation for Computational Linguistics.
R. Morante, V. Van Asch, and W. Daelemans. 2010.
Memory-based resolution of in-sentence scopes of
hedge cues. CoNLL-2010: Shared Task, page 40.
A. O?zgu?r and D.R. Radev. 2009. Detecting speculations
and their scopes in scientific text. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 3-Volume 3, pages
1398?1407. Association for Computational Linguis-
tics.
M. Rei and T. Briscoe. 2010. Combining manual rules
and supervised learning for hedge cue and scope detec-
tion. In Proceedings of the 14th Conference on Natu-
ral Language Learning, pages 56?63.
E. Velldal, L. ?vrelid, and S. Oepen. 2010. Re-
solving Speculation: MaxEnt Cue Classification and
Dependency-Based Scope Rules. CoNLL-2010:
Shared Task, page 48.
V. Vincze, G. Szarvas, R. Farkas, G. Mo?ra, and J. Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
bioinformatics, 9(Suppl 11):S9.
H. Zhou, X. Li, D. Huang, Z. Li, and Y. Yang. 2010.
Exploiting Multi-Features to Detect Hedges and Their
Scope in Biomedical Texts. CoNLL-2010: Shared
Task, page 106.
287
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 118?121,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Domain Adaptation of Coreference Resolution for Radiology Reports
Emilia Apostolova, Noriko Tomuro, Pattanasak Mongkolwat*, Dina Demner-Fushman?
College of Computing and Digital Media, DePaul University, Chicago, IL
*Department of Radiology, Northwestern University Medical School, Chicago, IL
?Communications Engineering Branch, National Library of Medicine, Bethesda, MD
emilia.aposto@gmail.com, tomuro@cs.depaul.edu,
p-mongkolwat@northwestern.edu, ddemner@mail.nih.gov
Abstract
In this paper we explore the applicability of
existing coreference resolution systems to a
biomedical genre: radiology reports. Analysis
revealed that, due to the idiosyncrasies of the
domain, both the formulation of the problem
of coreference resolution and its solution need
significant domain adaptation work. We refor-
mulated the task and developed an unsuper-
vised algorithm based on heuristics for coref-
erence resolution in radiology reports. The
algorithm is shown to perform well on a test
dataset of 150 manually annotated radiology
reports.
1 Introduction
Coreference resolution is the process of determin-
ing whether two expressions in natural language re-
fer to the same entity in the world. General purpose
coreference resolution systems typically cluster all
mentions (usually noun phrases) in a document into
coreference chains according to the underlying ref-
erence entity. A number of coreference resolution
algorithms have been developed for general texts. To
name a few, Soon et al (2001) employed machine
learning on the task and achieved an F-score of 62.6
and 60.4 on the MUC-6 (1995) and MUC-7 (1997)
coreference corpora respectively. Ng et al (2002)
improved this learning framework and achieved F-
scores of 70.4 and 63.4 respectively on the same
datasets.
There are also a number of freely available off-
the-shelf coreference resolution modules developed
for the general domain. For example, BART (Vers-
ley et al, 2008) is an open source coreference reso-
lution system which provides an implementation of
the Soon et al algorithm (2001). The Stanford De-
terministic Coreference Resolution System (Raghu-
nathan et al, 2010) uses an unsupervised sieve-like
approach to coreference resolution. Similarly, the
GATE Information Extraction system (Cunningham
et al, 2002) includes a rule-based coreference reso-
lution module consisting of orthography-based pat-
terns and a pronominal coreferencer (matching pro-
nouns to the most recent referent).
While coreference resolution is a universal dis-
course problem, both the scope of the problem and
its solution could vary significantly across domains
and text genres. Newswire coreference resolution
corpora (such as the MUC corpus) and general pur-
pose tools do not always fit the needs of specific do-
mains such as the biomedical domain well.
The importance and distinctive characteristics of
coreference resolution for biomedical articles has
been recognized, for example (Castano et al, 2002;
Gasperin, 2006; Gasperin et al, 2007; Su et al,
2008). Within the biomedical field, clinical texts
have been noted as a genre that needs specialized
coreference corpora and methodologies (Zheng et
al., 2011). The importance of the task for the clini-
cal domain has been attested by the 2011 i2b2 NLP
shared task (Informatics for Integrating Biology and
the Bedside1) which provided an evaluation plat-
form for coreference resolution for clinical texts.
However, even within the clinical domain, coref-
erence in different sub-genres could vary signifi-
1https://www.i2b2.org/NLP/Coreference/
118
cantly. In this paper we demonstrate the idiosyn-
crasies of the task of coreference resolution in a
clinical domain sub-genre, radiology reports, and
describe an unsupervised system developed for the
task.
2 Coreference Resolution for Radiology
Reports
Radiology reports have some unique characteristics
that preclude the use of coreference resolution mod-
ules or algorithms developed for the general biomed-
ical domain or even for other types of clinical texts.
The radiology report is a clinical text used to com-
municate medical image findings and observations
to referring physicians. Typically, radiology reports
are produced by radiologists after examining medi-
cal images and are used to describe the findings and
observations present in the accompanied images.
The radiology report accompanies an imaging
study and frequently refers to artifacts present in
the image. In radiology reports, artifacts present
in the image exhibit discourse salience, and as a
result are often introduced with definite pronouns
and articles. For example, consider the sentence
The pericardial space is clear. The definite noun
phrase the pericardial space does not represent an
anaphoric (or cataphoric) discourse entity and has
no antecedent. In contrast, coreference resolution
in general texts typically considers definite noun
phrases to be anaphoric discourse entities and at-
tempts to find their antecedents.
Another important distinction between general
purpose coreference resolution and the coreference
resolution module needed by an NLP system for
clinical texts is the scope of the task. General pur-
pose coreference resolution systems typically cluster
all mentions in a document into coreference chains.
Such comprehensive mention clustering is often not
necessary for the purposes of clinical text NLP sys-
tems. Biomedical Information Extraction systems
typically first identify named entities (medical con-
cepts) and map them to unambiguous biomedical
standard vocabularies (e.g. UMLS2 or RadLex3 in
the radiological domain). While multiple mentions
of the same named entity could exist in a document,
2http://www.nlm.nih.gov/research/umls/
3http://www.radlex.org/
in most cases these mentions were previously as-
signed to the same medical concept. For example,
multiple report mentions of ?the heart? or ?the lung?
will normally be mapped to the same medical con-
cept and clustering of these mentions into corefer-
ence chains is typically not needed.
3 Task Definition
Analysis revealed that the coreference resolution
task could be simplified and still meet the needs of
most Information Extraction tasks relevant to the ra-
diological domain. Due to their nature, texts de-
scribing medical image finding and observations do
not contain most pronominal references typically
targeted by coreference resolution systems. For ex-
ample, no occurrence of personal pronouns (e.g. he,
I), possessive pronouns (e.g. his, my), and indefi-
nite pronouns (e.g. anyone, nobody) was found in
the validation dataset. Demonstrative pronouns and
non-pleonastic ?it? mentions were the only pronom-
inal references observed in the dataset4. The fol-
lowing examples demonstrate the use of demonstra-
tive pronouns and the non-pleonastic ?it? pronoun
(shown in bold):
There is prominent soft tissue swelling involving
the premaxillary tissues. This measures approxi-
mately 15 mm in thickness and extends to the infe-
rior aspect of the nose.
There is a foreign object in the proximal left main-
stem bronchus on series 11 image 17 that was not
present on the prior study. It has a somewhat ovoid
to linear configuration.
Following these observations, the coreference res-
olution task has been simplified as follows. Corefer-
ence chains are assigned only for demonstrative pro-
nouns and ?it? noun phrases. The coreference reso-
lution task then involves selecting for each mention
a single best antecedent among previously annotated
named entities (medical concepts) or the NULL an-
tecedent.
4 Dataset
A total of 300 radiology reports were set aside for
validation and testing purposes. The dataset consists
4Pleonastic ?it? refers to its use as a ?dummy? pronoun, e.g.
It is raining, while non-pleonastic use of the pronoun refers to
a specific entity.
119
Figure 1: A sample DICOM image from an imaging
study described by the following radiology report snip-
pet: . . . FINDINGS: Targeted sonography of the upper in-
ner left breast was performed. At the site of palpable ab-
normality, at the 11 o?clock position 3 cm from the nipple,
there is an oval circumscribed, benign-appearing hypoe-
choic mass measuring 2.0 x 1.6 x 1.4 cm. There is mild
internal blood flow. It is surrounded by normal appearing
glandular breast tissue.. . .
of 100 Computed Tomography Chest reports, 100
Ultrasound Breast reports, and 100 Magnetic Res-
onance Brain reports, all randomly selected based
on their report types from a dataset of more than
100,000 de-identified reports spanning a period of
9 years5. These three types of reports represent
a diverse dataset covering representative imaging
modalities and body regions. Figure 1 shows a sam-
ple Breast Ultrasound DICOM6 image and its asso-
ciated radiology report.
The reports were previously tagged (using an au-
tomated system) with medical concepts and their
semantic types (e.g. anatomical entity, disorder,
imaging observation, etc.). Half of the dataset (150
reports) was manually annotated with coreference
chains using the simplified task definition described
above. The other half of the dataset was used for
validation of the system described next.
5 Method and Results
The coreference resolution task involves selecting
for each mention a single best antecedent among
previously annotated named entities or the NULL
antecedent. Mentions are demonstrative pronoun
phrases or definite noun phrases containing previ-
ously annotated named entities.
5The collection is a proprietary dataset belonging to North-
western University Medical School.
6Digital Imaging and Communications in Medicine, c? The
National Electrical Manufacturers Association.
We implemented an algorithm for the task de-
scribed above which was inspired by the work of
Haghighi and Klein (2009). The algorithm first iden-
tifies mentions within each report and orders them
linearly according to the position of the mention
head. Then it selects the antecedent (or the NULL
antecedent) for each mention as follows:
1. The possible antecedent candidates are first fil-
tered based on a distance constraint. Only mentions
of interest belonging to the preceding two sentences
are considered. The rationale for this filtering step is
that radiology reports are typically very concise and
less cohesive than general texts. Paragraphs often
describe multiple observations and anatomical enti-
ties sequentially and rarely refer to mentions more
distant than the preceding two sentences.
2. The remaining antecedent candidates are then
filtered based on a syntactic constraint: the co-
referent mentions must agree in number (singular or
plural based on the noun phrase head).
3. The remaining antecedent candidates are then
filtered based on a semantic constraint. If the two
mentions refer to named entities, the named entities
need to have the same semantic category7.
4. After filtering, the closest mention from the set
of remaining possible antecedents is selected. If the
set is empty, the NULL antecedent is selected.
Pairwise coreference decisions are considered
transitive and antecedent matches are propagated
transitively to all paired co-referents.
The algorithm was evaluated on the manually an-
notated test dataset. Results (Table 1) were com-
puted using the pairwise F1-score measure: preci-
sion, recall, and F1-score were computed over all
pairs of mentions in the same coreference cluster.
Precision Recall F1-score
74.90 48.22 58.66
Table 1: Pairwise coreference resolution results.
The system performance is within the range of
state-of-the-art supervised and unsupervised coref-
erence resolution systems8. F1-scores could range
7The same semantic type in the case of UMLS concepts or
the same parent in the case of RadLex concepts.
8Source code for the described system will be made avail-
able upon request.
120
between 39.8 and 67.3 for various methods and
test sets (Haghighi and Klein, 2009). The simpli-
fication of the coreference resolution problem de-
scribed above allowed us to focus only on corefer-
ence chains of interest to clinical text Information
Extraction tasks and positively influenced the out-
come. In addition, our goal was to focus on high
precision results as opposed to optimizing the over-
all F1-score. This guarantees that coreference reso-
lution errors will result in mostly omissions of coref-
erence pairs and will not introduce information ex-
traction inaccuracies.
6 Conclusion
In this paper, we presented some of the challenges
involved in the task of adapting coreference resolu-
tion for the domain of clinical radiology. We pre-
sented a domain-specific definition of the corefer-
ence resolution task. The task was reformulated and
simplified in a practical manner that ensures that the
needs of biomedical information extraction systems
are still met. We developed an unsupervised ap-
proach to the task of coreference resolution of radi-
ology reports and demonstrate state-of-the-art preci-
sion and reasonable recall results. The developed
system is made publicly available to the NLP re-
search community.
References
J. Castano, J. Zhang, and J. Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In International
Symposium on Reference Resolution. Citeseer.
D.H. Cunningham, D.D. Maynard, D.K. Bontcheva, and
M.V. Tablan. 2002. GATE: A Framework and Graph-
ical Development Environment for Robust NLP Tools
and Applications.
C. Gasperin, N. Karamanis, and R. Seal. 2007. Annota-
tion of anaphoric relations in biomedical full-text arti-
cles using a domain-relevant scheme. In Proceedings
of DAARC, volume 2007. Citeseer.
C. Gasperin. 2006. Semi-supervised anaphora resolution
in biomedical texts. In Proceedings of the Workshop
on Linking Natural Language Processing and Biology:
Towards Deeper Biological Literature Analysis, pages
96?103. Association for Computational Linguistics.
A. Haghighi and D. Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3-
Volume 3, pages 1152?1161. Association for Compu-
tational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 104?111.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010. A
multi-pass sieve for coreference resolution. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 492?501.
Association for Computational Linguistics.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
J. Su, X. Yang, H. Hong, Y. Tateisi, J. Tsujii, M. Ash-
burner, U. Leser, and D. Rebholz-Schuhmann. 2008.
Coreference resolution in biomedical texts: a machine
learning approach. Ontologies and Text Mining for
Life Sciences 08.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
Bart: A modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies: Demo Session, pages 9?12. As-
sociation for Computational Linguistics.
J. Zheng, W.W. Chapman, R.S. Crowley, and G.K.
Savova. 2011. Coreference resolution: A review of
general methodologies and applications in the clinical
domain. Journal of biomedical informatics.
121
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 54?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Interpreting Consumer Health Questions: The Role of Anaphora and 
Ellipsis 
 
 
Halil Kilicoglu, Marcelo Fiszman, Dina Demner-Fushman 
Lister Hill National Center for Biomedical Communications  
National Library of Medicine  
Bethesda, MD, USA 
{kilicogluh, fiszmanm, ddemner}@mail.nih.gov 
 
  
 
Abstract 
While interest in biomedical question answer-
ing has been growing, research in consumer 
health question answering remains relatively 
sparse. In this paper, we focus on the task of 
consumer health question understanding. We 
present a rule-based methodology that relies 
on lexical and syntactic information as well as 
anaphora/ellipsis resolution to construct struc-
tured representations of questions (frames). 
Our results indicate the viability of our ap-
proach and demonstrate the important role 
played by anaphora and ellipsis in interpreting 
consumer health questions. 
1 Introduction 
Question understanding is a major challenge in 
automatic question answering. An array of ap-
proaches has been developed for this task in the 
course of TREC Question Answering evaluations 
(see Prager (2006) for an overview). These col-
lectively developed approaches to question un-
derstanding were successfully applied and ex-
panded upon in IBM?s Watson system (Lally et 
al., 2012). Currently, Watson is being retargeted 
towards biomedical question answering, joining 
the ongoing research in domain-specific question 
answering (for a review, see Simpson and 
Demner-Fushman, 2012). 
Much research in automatic question answer-
ing has focused on answering well-formed fac-
toid questions. However, real-life questions that 
need to be handled by such systems are often 
posed by lay people and are not necessarily well-
formed or explicit. This is particularly evident in 
questions involving health issues. Zhang (2010), 
focusing on health-related questions submitted to 
Yahoo Answers, found that these questions pri-
marily described diseases and symptoms (ac-
companied by some demographic information), 
were fairly long, dense (incorporating more than 
one question), and contained many abbreviations 
and misspellings. For example, consider the fol-
lowing question posed by a consumer: 
(1) my question is this: I was born w/a esopha-
gus atresia w/dextrocardia. While the heart 
hasn't caused problems,the other has. I get 
food caught all the time. My question is...is 
there anything that can fix it cause I can't eat 
anything lately without getting it caught. I 
need help or will starve! 
It is clear that the person asking this question 
is mainly interested in learning about treatment 
options for his/her disease, in particular with re-
spect to his/her esophagus. Most of the textual 
content is not particularly relevant in understand-
ing the question (I need help or will starve! or I 
get food caught all the time). In addition, note 
the presence of anaphora (it referring to esopha-
gus atresia) and ellipsis (the other has [caused prob-
lems]), which should be resolved in order to auto-
matically interpret the question. Finally, note the 
informal fix instead of the more formal treat, and 
cause instead of because.  
The National Library of Medicine? (NLM?) 
receives questions from consumers on a variety 
of health-related topics. These questions are cur-
rently manually answered by customer support 
services. The overall goal of our work is to assist 
the customer support services by automatically 
interpreting these questions, using information 
retrieval techniques to find relevant documents 
and passages, and presenting the information in 
concise form for their assessment. 
In this paper, we specifically focus on ques-
tion understanding, rather than information re-
54
trieval aspects of our ongoing work. Our goal in 
question understanding is to capture the core as-
pects of the question in a structured representa-
tion (question frame), which can then be used to 
form a query for the search engine. In the current 
work, we primarily investigate and evaluate the 
role of anaphora and ellipsis resolution in under-
standing the questions. Our results confirm the 
viability of rule-based question understanding 
based on exploiting lexico-syntactic patterns and 
clearly demonstrate that anaphora and ellipsis 
resolution are beneficial for this task.  
2 Background 
Despite the growing interest to biomedical ques-
tion answering (Cairns et al, 2012; Ni et al, 
2012; Bauer and Berleant, 2012), consumer 
health question answering remains a fairly un-
derstudied area of research. The initial research 
has focused on the analysis of consumer lan-
guage (McCray et al, 1999) and the types of 
questions they asked. Spink et al (2004) found 
that health-related queries submitted to three web 
search engines in 2001 were often advice seeking 
and personalized, and fell into five major catego-
ries: general health, weight issues, reproductive 
health and puberty, pregnancy/obstetrics, and 
human relationships. Observing that health que-
ries constituted no more than 9.5% of all queries 
and declined over time, they concluded that the 
users turn more to the specialized resources for 
the answers to health-related questions. Similar 
to the findings of Zhang (2010), Beloborodov et 
al. (2013) found that diseases and symptoms 
were the most popular topics in a resource simi-
lar to Yahoo Answers, Otvety@Mail.Ru. They 
analyzed Otvety@Mail.Ru questions by mapping 
questions to body parts and organs, applying La-
tent Dirichlet Allocation method with Gibbs 
sampling to discover topics, and using a 
knowledge-based method to classify questions as 
evidence-directed or hypothesis-directed. 
First efforts in automated consumer health 
question processing were to classify the ques-
tions using machine learning techniques. In one 
study, frequently asked questions about diabetes 
were classified according to two somewhat or-
thogonal taxonomies: according to the ?medical 
type of the question? (Causes, Diagnostic, Pre-
vention, Symptoms, Treatment, etc.) and accord-
ing to the ?expected answer type? (Boolean, 
Causal, Definition, Factoid, Person, Place, etc.) 
(Cruchet et al, 2008). Support Vector Machine 
(SVM) classification achieved an F-score in low 
80s in classifying English questions to the ex-
pected answer type. The results for French and 
medical type classification in both languages 
were much lower. Liu et al (2011) found that 
SVM trained to distinguish questions asked by 
consumers from those posed by healthcare pro-
fessionals achieve F-scores in the high 80s - low 
90s. One of distinguishing characteristics of the 
consumer questions in Liu et al?s study was the 
significantly higher use of personal pronouns 
(compared to professional questions). This fea-
ture was found to be useful for machine learning; 
however, the abundance of pronouns in the long 
dense questions is also a potential source of fail-
ure in understanding the question.  
Vicedo and Ferr?ndez (2000) have shown that 
pronominal anaphora resolution improves several 
aspects of the QA systems? performance. This 
observation was supported by Harabagiu et al 
(2005) who have manually resolved coreference 
and ellipsis for 14 of the 25 scenarios in the 
TREC 2005 evaluation. Hickl et al (2006) have 
incorporated into their question answering sys-
tem a heuristic based question coreference mod-
ule that resolved referring expressions in the 
question series to antecedents mentioned in pre-
vious questions or in the target description. To 
our knowledge, coreference and ellipsis resolu-
tion has not been previously attempted in con-
sumer health question understanding. 
Another essential aspect in processing con-
sumer questions is defining a formal representa-
tion capable of capturing all important points 
needed for further processing in automatic query 
generation (in the systems that use document 
passage retrieval to find a set of potential an-
swers) and answer extraction and unification. 
Ontologies provide effective representation 
mechanisms for concepts, whereas relations are 
better captured in frame-like or event-related 
structures (Hunter and Cohen, 2006). Frame-
based representation of extracted knowledge has 
a long-standing tradition in the biomedical do-
main, for example, in MedLEE (Friedman et al, 
1994). Demner-Fushman et al (2011) showed 
that frame-based representation of clinical ques-
tions improve identification of patients eligible 
for cohort inclusion. Demner-Fushman and Ab-
hyankar (2012) extracted frames in four steps: 1) 
identification of domain concepts, 2) extraction 
of patient demographics (e.g., age, gender) and 
social history, 3) establishing dependencies be-
tween the concepts using the Stanford dependen-
cy parser (de Marneffe et al, 2006), and 4) add-
ing concepts not involved in the relations to the 
55
frame as a list of keywords.  Event-based repre-
sentations have also seen increasing use in recent 
years in biomedical text mining, with the availa-
bility of biological event corpora, including 
GENIA event (Kim et al, 2008) and GREC 
(Thompson et al, 2009), and shared task chal-
lenges (Kim et al, 2012). Most state-of-the-art 
systems address the event extraction task by 
adopting machine learning techniques, such as 
dual composition-based models (Riedel and 
McCallum, 2011), stacking-based model integra-
tion (McClosky et al, 2012), and domain adapta-
tion (Miwa et al, 2012). Good performance has 
also been reported with some rule-based systems 
(Kilicoglu and Bergler, 2012). Syntactic depend-
ency parsing has been a key component in all 
state-of-the-art event extraction systems, as well. 
The role of coreference resolution in event ex-
traction has recently been acknowledged (Kim et 
al., 2012), even though efforts in integrating co-
reference resolution into event extraction pipe-
lines have generally resulted in only modest im-
provements (Yoshikawa et al, 2011; Miwa et 
al., 2012; Kilicoglu and Bergler, 2012). 
Coreference resolution has also been tackled 
in open domain natural language processing. 
State-of-the-art systems often employ a combina-
tion of lexical, syntactic, shallow semantic and 
discourse information (e.g., speaker identifica-
tion) with deterministic rules (Lee et al, 2011). 
Interestingly, coreference resolution is one re-
search area, in which deterministic frameworks 
generally outperform machine learning models 
(Haghighi and Klein, 2009; Lee et al, 2011).  
In contrast to coreference resolution, ellipsis 
resolution remains an understudied NLP prob-
lem. One type of ellipsis that received some at-
tention is null instantiation (Fillmore and Baker, 
2001), whereby the goal is to recover the refer-
ents for an uninstantiated semantic role of a tar-
get predicate from the wider discourse context. A 
semantic evaluation challenge that focused on 
null instantiation was proposed, although partici-
pation was limited (Ruppenhofer et al, 2010). 
Gerber and Chai (2012) focused on implicit ar-
gumentation (i.e., null instantiation) for nominal 
predicates. They annotated a corpus of implicit 
arguments for a small number of nominal predi-
cates and trained a discriminative model based 
on syntactic, semantic and discourse features 
collected from various linguistic resources. Fo-
cusing on a different type of ellipsis, Bos and 
Spenader (2011) annotated a corpus of verb 
phrase ellipsis; however, so far there have been 
little work in verb phrase ellipsis resolution. We 
are also not aware of any work in ellipsis resolu-
tion in biomedical NLP.  
3 Methods   
We use a pipeline model for question analysis, 
which results in frame annotations that capture 
the content of the question. Our rule-based meth-
od begins with identifying terms (named enti-
ties/triggers) in question text. Next, we recognize 
anaphoric mentions and, if any, perform anapho-
ra resolution. The next step is to link frame trig-
gers with their theme and question cue by ex-
ploiting syntactic dependency relations. Finally, 
if frames with implicit arguments exist (that is, 
frames in which theme or question cue was not 
instantiated), we attempt to recover these argu-
ments by ellipsis resolution. In this section, we 
first describe our data selection. Then, we ex-
plain the steps in our pipeline, with particular 
emphasis on anaphora and ellipsis. The pipeline 
diagram is illustrated in Figure 1.  
 
 
Figure 1. The system pipeline diagram 
3.1 Data Selection and Annotation 
In this study, we focused on questions about ge-
netic diseases, due to their increasing prevalence. 
Since the majority of the consumers? questions 
submitted to NLM are about treatment and prog-
nosis, we selected mainly these types of ques-
tions for our training set. Note that while these 
questions mostly focused on treatment and prog-
nosis, some of them also include other types of 
questions, asking for general information or 
about diagnosis, etiology, and susceptibility 
(thus, confirming the finding of Zhang (2010)). 
The majority of selected questions were asked by 
real consumers in 2012. Due to our interest in 
genetics questions, we augmented this set with 
56
some frequently asked questions from the Genet-
ic and Rare Disease Information Center 
(GARD) 1 . Our selection yielded 32 treatment 
and 22 prognosis questions. An example treat-
ment question was provided earlier (1). The fol-
lowing is a training question on prognosis: 
(2) They have diagnosed my niece with Salla 
disease. I understand that this is a very rare 
disease and that its main origin is Finland. 
Can you please let me know what to expect? 
My niece is 7 years old. It has taken them 6 
years to finally come up with this diagnosis. 
We used training questions to gain linguistic 
insights into the problem, to develop and refine 
our methodology, and as the basis of a trig-
ger/question cue dictionary. 
After the system was developed, we selected 
29 previously unseen treatment-focused ques-
tions posed to GARD for testing. We annotated 
them with target frames (41 instances) using brat 
annotation tool (Stenetorp et al, 2012) and eval-
uated our system results against these frames. 29 
of the target frames were treatment frames. Addi-
tionally, there were 1 etiology, 6 general infor-
mation, 2 diagnosis, and 3 prognosis frames. 
3.2 Syntactic Dependency Parsing 
Our question analysis module uses typed de-
pendency relations as the basis of syntactic in-
formation. We extract syntactic dependencies 
using Stanford Parser (de Marneffe et al, 2006) 
and use its collapsed dependency format. We 
rely on Stanford Parser for tokenization, lemma-
tization, and part-of-speech tagging, as well. 
3.3 Named Entity/Trigger Detection 
We use simple dictionary lookup to map entity 
mentions in text to UMLS Metathesaurus con-
cepts (Lindberg, 1993). So far, we have focused 
on recognizing three mention categories: prob-
lems, interventions, and patients. Based on 
UMLS 2007AC release, we constructed a dic-
tionary of string/concept pairs. We limited the 
dictionary to concepts with predefined semantic 
types. For example, all problems in the diction-
ary have a semantic type that belongs to the Dis-
orders semantic group (McCray et al, 2001), 
such as Neoplastic Process and Congenital Ab-
normality. Currently our dictionary contains ap-
proximately 260K string/concept pairs. 
Dictionary lookup is also used to detect trig-
gers and question cues. We constructed a trigger 
                                                 
1 https://rarediseases.info.nih.gov/GARD/ 
and question cue dictionary based on training 
data and limited expansion. The dictionary cur-
rently contains 117 triggers and 14 question cues.  
3.4 Recognizing Anaphoric Mentions 
We focus on identifying two types of anaphoric 
phenomena: pronominal anaphora (including 
anaphora of personal and demonstrative pro-
nouns) and sortal anaphora. The following ex-
amples from the training questions illustrate 
these types. Anaphoric mentions are underlined 
and their antecedents are in bold. 
? Personal pronominal anaphora: My daughter 
has just been diagnosed with Meier-Gorlin 
syndrome. I would like to learn more about 
it ? 
? Demonstrative pronominal anaphora: We just 
found out that our grandson has 48,XXYY 
syndrome. ?  I was wondering if you could 
give us some information on what to expect 
and the prognosis for this and ..  
? Sortal anaphora: I have a 24-month-old niece 
who has the following symptoms of Cohen 
syndrome: ? I would like seek your help in 
learning more about this condition. 
To recognize mentions of personal pronominal 
and sortal anaphora, we mainly adapted the rule-
based techniques outlined in Kilicoglu and Ber-
gler (2012), itself based on the deterministic co-
reference resolution approach described in 
Haghighi and Klein (2009). While Kilicoglu and 
Bergler (2012) focused on anaphora involving 
gene/protein terms, our adaptation focuses on 
those involving problems and patients. In addi-
tion, we expanded their work by developing rules 
to recognize demonstrative pronominal anapho-
ra.  
3.4.1 Personal Pronouns 
Kilicoglu and Bergler (2012) focused on only 
resolving it and they, since, in scientific article 
genre, resolving other third person pronouns (he, 
she) was less relevant. We currently recognize 
these two pronouns, as well. For personal pro-
nouns, we merely tag the word as a pronominal 
anaphor if it is tagged as a pronoun and is in 
third person (i.e., she, he, it, they).  
3.4.2 Demonstrative Pronouns 
We rely on typed syntactic dependencies as well 
as part-of-speech tags to recognize demonstrative 
pronominal anaphora. A word is tagged as 
demonstrative pronominal anaphor if it is one of 
this, that, those, or these and if it is not the de-
57
pendent in a det (determiner) dependency (in 
other words, it is not a pronominal modifier). 
Furthermore, we ensure that the pronoun that 
does not act as a complementizer, requiring that 
it not be the dependent in a complm (complemen-
tizer) dependency. 
3.4.3 Sortal Anaphora 
In the current work, we limited sortal anaphora 
to problem terms. As in Kilicoglu and Bergler 
(2012), we require that the anaphoric noun 
phrases not include any named entity terms. 
Thus, we allow the syndrome as an anaphoric 
mention, while blocking the Stickler syndrome.  
To recognize sortal anaphora, we look for the 
presence of det dependency, where the depend-
ent is one of this, that, these, those, or the.  
Once the named entities, question cues, trig-
gers, and anaphoric mentions are identified in a 
sentence, we collapse the syntactic dependencies 
from the sentence to simplify further processing. 
This is illustrated in Table 1 for the sentence in 
(3). 
(3) My partner is a carrier for Simpson-Golabi-
Behmel syndrome and her son was diag-
nosed with this rare condition.  
 
Dependencies before Dependencies after 
amod (syndrome, simpson-golabi-behmel) 
prep_for(carrier, simpson-golabi-behmel syndrome) 
prep_for(carrier,syndrome) 
det(condition,this) 
prep_with (diagnosed, this rare condition) amod(condition, rare) 
prep_with(diagnosed, condition) 
Table 1: Syntactic dependency transformations 
3.5 Anaphora  Resolution 
Anaphora resolution is the task of finding the 
antecedent for an anaphoric mention in prior dis-
course. Our anaphora resolution method is again 
based on the work of Kilicoglu and Bergler 
(2012). However, we made simplifying assump-
tions based on our examination of the training 
questions. First observation is that each question 
is mainly about one salient topic (problem) and 
anaphoric mentions are highly likely to refer to 
this topic. Secondly, the salient topic often ap-
pears as the first named entity in the question.  
Based on these observations, we did not attempt 
to use the relatively complex, semantic graph-
based resolution strategies (e.g., graph distance) 
outlined in that work. Furthermore, we have not 
attempted to address set-instance anaphora or 
event anaphora in this work, since we did not see 
examples of these in the training data. 
Anaphora resolution begins with identifying 
the candidate antecedents (problems, patients) in 
prior discourse, which are then evaluated for syn-
tactic and semantic compatibility. For pronomi-
nal anaphora, compatibility involves person and 
number agreement between the anaphoric men-
tion and the antecedent. For sortal anaphora, 
number agreement as well as satisfying one of 
the following constraints is required: 
? Head word constraint: The head of the ana-
phoric NP and the antecedent NP match. 
This constraint allows Wolf-Hirschhorn Syn-
drome as an antecedent for this syndrome, 
matching on the word syndrome. 
? Hypernymy constraint: The head of the ana-
phoric NP is a problem hypernym and the 
antecedent is a problem term. Similar to 
gene/protein hypernym list in Kilicoglu and 
Bergler (2012), we used a small list of prob-
lem hypernym words, including disease, dis-
order, illness, syndrome, condition, and 
problem. This constraint allows Simpson-
Golabi-Behmel syndrome as an antecedent 
for this rare condition in example (3). 
We expanded number agreement test to in-
clude singular mass nouns, so that plural anapho-
ra (e.g., they) can refer to mass nouns such as 
family, group, population. In addition, we de-
fined lists of gendered nouns (e.g., son, father, 
nephew, etc. for male and wife, daughter, niece, 
etc. for female) and required gender agreement 
for pronominal anaphora. 
After the candidate antecedents are identified, 
we assign them salience scores based on the or-
der in which they appear in the question and their 
frequency in the question. The terms that appear 
earlier in the question and occur more frequently 
receive higher scores. The most salient anteced-
ent is then taken to be the coreferent. 
3.6 Frame Construction 
We adapted the frame extraction process based 
on lexico-syntactic information outlined in 
Demner-Fushman et al (2012) and somewhat 
58
modified the frames to accommodate consumer 
health questions. For each question posed, we 
aim to construct a frame which consists of the 
following elements: type, theme, and question 
cue: theme refers to the topic of the question 
(problem name, etc.), while type refers to the 
aspect of the theme that the question is about 
(treatment, prognosis, etc.) and question cue to 
the question words (what, how, are there, etc.). 
Theme element is semantically typed and is re-
stricted to the UMLS semantic group Disorders. 
From the question in (1), the following frame 
should be extracted: 
 
Treatment fix 
      Theme Esophageal atresia  
(Disease or Syndrome) 
      QCue Is there 
Table 2: Frame example 
 
We rely on syntactic dependencies to link frame 
indicators to their themes and question cues. We 
currently search for the following types of syn-
tactic dependencies between the indicator men-
tion and the argument mentions: dobj (direct ob-
ject), nsubjpass (passive nominal subject), nn 
(noun compound modifier), rcmod (relative 
clause modifier), xcomp (open clausal comple-
ment), acomp (adjectival complement), prep_of, 
prep_to, prep_for, prep_on, prep_from, 
prep_with, prep_regarding, prep_about (prepo-
sitional modifier cued by of, to, for, on, from, 
with, regarding, about, respectively). Two spe-
cial rules address the following cases: 
? If the dependency exists between a trigger of 
type T and another of type General Infor-
mation, the General Information trigger be-
comes a question cue for the frame type T. 
This handles cases such as ?Is there infor-
mation regarding prognosis..? where there is 
a prep_regarding dependency between the 
General Information trigger ?information? 
and the Prognosis trigger ?prognosis?. This 
results in ?information? becoming the ques-
tion cue for the Prognosis frame. 
? If a dependency exists between a trigger T 
and a patient term P and another between the 
patient term P and a potential theme argu-
ment A, the potential theme argument A is 
assigned as the theme of the frame indicated 
by T. This handles cases such as ?What is the 
life expectancy for a child with Dravet syn-
drome?? whereby Dravet syndrome is as-
signed the Theme role for the Prognosis 
frame indicated by life expectancy. 
3.6.1 Ellipsis Resolution 
The frame construction step may result in frames 
with uninstantiated themes or question cues. If a 
constructed frame includes a question cue but no 
theme, we attempt to recover the theme argument 
from prior discourse by ellipsis processing. Con-
sider the question in (4) and the frame in Table 3 
extracted from it in previous steps: 
(4) They have diagnosed my niece with Salla 
disease. ?Can you please let me know what 
to expect? ? 
Prognosis expect 
      Theme - 
      QCue what 
Table 3: Frame with uninstantiated Theme role 
 
In the context of consumer health questions, 
the main difficulty with resolving such cases is 
recognizing whether it is indeed a legitimate case 
of ellipsis. We use the following dependency-
based heuristics to determine the presence of el-
lipsis: 
? Check for the presence of a syntactic de-
pendency of one of the types listed in Sec-
tion 3.5, in which the frame trigger appears 
as an element. If such a dependency does not 
exist, consider it a case of ellipsis.  
? Otherwise, consider the other element of the 
dependency: 
o If the other element does not corre-
spond to a term, we cannot make a 
decision regarding ellipsis, since we 
do not know the semantics of this 
other element. 
o If it corresponds to an element that 
has already been used in creating the 
frame, the dependency is accounted 
for.  
? If all the dependencies involving the frame 
trigger are accounted for, consider it a case 
of ellipsis. 
In example (4), the trigger expect is found to 
be in an xcomp dependency with the question cue 
know, which has already been used in the frame. 
Therefore this dependency is accounted for, and 
we consider this a case of ellipsis.  On the other 
hand, consider the example:  
(5) My child has been diagnosed with pachgyria. 
What can I expect for my child?s future? 
As in the previous example, the Theme role of 
the Prognosis frame indicated by expect is unin-
stantiated. However, it is not considered an ellip-
59
tical case, since there is a prep_for dependency 
between expect and future, a word that is seman-
tically unresolved. 
Once the presence of ellipsis is ensured, we 
fill the Theme role of the frame with the most 
salient term in the question text, as in anaphora 
resolution. 
In rare cases, the frame may include a theme 
but not a question cue.  This may be due to a lack 
of explicit question expression (such as in the 
question ?treatment for Von Hippel-Lindau syn-
drome.?) or due to shortcomings in dependency-
based linking of frame triggers to question cues. 
If no fully instantiated frame was extracted from 
the question, as a last resort, we construct a 
frame without the question cue in an effort to 
increase recall.  
4 Results and Discussion 
We extracted frames from the test questions and 
compared the results with the annotated target 
frames. As evaluation metrics, we calculated 
precision, recall, and F-score. To assess the ef-
fect of various components of the system, we 
evaluated several scenarios: 
? Frame extraction without anaphora/ellipsis 
resolution (indicated as A in Table 4 below) 
? Frame extraction with anaphora/ellipsis reso-
lution (B) 
? Frame extraction without anaphora/ellipsis 
resolution but with gold triggers/named enti-
ties (C) 
? Frame extraction with anaphora/ellipsis reso-
lution and gold triggers/named entities (D) 
The evaluation results are provided in Table 4. In 
the second column, the numbers in parentheses 
correspond to the numbers of correctly identified 
frames. 
 
 # of frames Recall Precision F-score 
A 14 (13) 0.32 0.93 0.48 
B 26 (22) 0.54 0.85 0.66 
C 17 (16) 0.39 0.84 0.55 
D 35 (33) 0.80 0.94 0.86 
Table 4: Evaluation results 
 
The evaluation results show that the depend-
ency-based frame extraction method with dic-
tionary lookup is generally effective; it is precise 
in identifying frames, even though it misses 
many relevant frames, typical of most rule-based 
systems. On the other hand, anaphora/ellipsis 
resolution helps a great deal in recovering the 
relevant frames and only has a minor negative 
effect on precision of the frames, the overall ef-
fect being significantly positive. Note also that 
the increase in recall without gold triggers/named 
entities is about 40%, while that with gold trig-
gers/named entities is more than double, indicat-
ing that accurate term recognition contributes to 
better anaphora/ellipsis resolution and, in turn, to 
better question understanding. 
The dictionary-based named entity/trigger/ 
question cue detection is relatively simple, and 
while it yields good precision, the lack of terms 
in the corresponding dictionary causes recall er-
rors. An example is given in (6). The named enti-
ty Reed syndrome was not recognized due to its 
absence in the dictionary, causing two false 
negative errors. 
(6) A friend of mine was just told she has Reed 
syndrome? I was wondering if you could let 
me know where I can find more information 
on this topic. I am wondering what treat-
ments there are for this, ? 
Similarly, dependency-based frame construc-
tion is straightforward in that it mostly requires 
direct dependency relations between the trigger 
and the arguments.  While the two additional 
rules we implemented redress the shortcomings 
of this straightforward approach, there are cases 
in which dependency-based mechanism is still 
lacking. An example is given in (7). The lack of 
a direct dependency between treatments and this 
condition causes a recall error. A more sophisti-
cated mechanism based on dependency chains 
could recover such frames; however, such chains 
would also increase the likelihood of precision 
errors.  
(7) Are people with Lebers hereditary optic neu-
ropathy partially blind for a long period of 
time ?. ?Are there any surgical treatments 
available to alter this condition or is it per-
manent for life? 
Anaphora/ellipsis processing clearly benefited 
our question understanding system. However, we 
noted several errors due to shortcomings in this 
processing. For example, from the sentence in 
(8), the system constructed a General Infor-
mation frame with the trigger wonder and the 
Theme argument central core disease, which 
caused a false positive error.  
(8) After 34 years of living with central core dis-
ease, ?. My lower back doesn't seem to 
work, and I wonder if I will ever be able to 
walk up stairs or run.  
60
The system recognized that the trigger wonder 
had an uninstantiated theme argument, which it 
attempted to recover by ellipsis processing. 
However, this processing misidentified the case 
as legitimate ellipsis due to the dependency rela-
tions wonder is involved in. A more sophisticat-
ed approach would take into account specific 
selectional restrictions of predicates like wonder; 
however, the overall utility of such linguistic 
knowledge in the context of consumer health 
questions, which are often ungrammatical and 
not particularly well-written, remains uncertain. 
Our anaphora resolution method was unable to 
resolve some cases of anaphora. For example, 
consider the question in (6). The anaphoric men-
tion this topic corefers with Reed syndrome. 
However, we miss this anaphora since we did not 
consider topic as a problem hypernym in scenar-
io D, in which gold named entities are used. 
5 Conclusions and Future Work 
We presented a rule-based approach to consumer 
health question understanding which relies on 
lexico-syntactic information and anapho-
ra/ellipsis resolution. We showed that lexico-
syntactic information provides a good baseline in 
understanding such questions and that resolving 
anaphora and ellipsis has a significant impact on 
this task. 
With regard to question understanding, future 
work includes generalization of the system to 
questions on topics other than genetic disorders 
(e.g., drugs) and aspects (such as complications, 
prevention, ingredients, location information, 
etc.) and broader evaluation. We also plan to au-
tomate dictionary development to some extent 
and address misspellings and acronyms in ques-
tions. We have been extending our frames to in-
clude ancillary keywords (named entities ex-
tracted from the question) that are expected to 
assist the search engine in pinpointing the rele-
vant answer passages, similar to Demner-
Fushman and Abhyankar (2012). We will also 
continue to develop our anaphora/ellipsis pro-
cessing module, addressing the issues revealed 
by our evaluation as well as other anaphoric phe-
nomena, such as recognition of pleonastic it. 
Acknowledgments 
This research was supported by the Intramural 
Research Program of the NIH, National Library 
of Medicine. 
References  
Michael A. Bauer and Daniel Berleant. 2012. Usabil-
ity survey of biomedical question answering sys-
tems. Human Genomics, 6(1):17. 
Alexander Beloborodov, Artem Kuznetsov, Pavel 
Braslavski. 2013. Characterizing Health-Related 
Community Question Answering. In Advances in 
Information Retrieval, 680-683. 
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz, 
James H. Martin, Martha S. Palmer, Wayne H. 
Ward, Guergana K. Savova. 2011. The MiPACQ 
clinical question answering system. In AMIA An-
nual Symposium Proceedings, pages 171-180. 
Sarah Cruchet, Arnaud Gaudinat, C?lia Boyer. 2008. 
Supervised approach to recognize question type in 
a QA system for health. Studies in Health Technol-
ogy and Informatics, 136:407-412. 
Marie-Catherine de Marneffe, Bill MacCartney, 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation, pages 449-
454. 
Dina Demner-Fushman, Swapna Abhyankar, Antonio 
Jimeno-Yepes, Russell F. Loane, Bastien Rance, 
Fran?ois-Michel Lang, Nicholas C. Ide, Emilia 
Apostolova, Alan R. Aronson. 2011. A 
Knowledge-Based Approach to Medical Records 
Retrieval. In Proceedings of Text Retrieval Confer-
ence 2011.  
Dina Demner-Fushman and Swapna Abhyankar. 
2012. Syntactic-Semantic Frames for Clinical Co-
hort Identification Queries. Lecture Notes in Com-
puter Science, 7348:100-112. 
Charles J. Fillmore and Collin F. Baker. 2001. Frame 
semantics for text understanding. In Proceedings of 
the NAACL?01 Workshop on WordNet and Other 
Lexical Resources. 
Carol Friedman, Philip O. Alderson, John HM Austin, 
James J. Cimino, and Stephen B. Johnson. 1994. A 
general natural-language text processor for clinical 
radiology. Journal of the American Medical Infor-
matics Association, 1(2): 161-174. 
Matthew S. Gerber and Joyce Y. Chai. 2012. Seman-
tic Role Labeling of Implicit Arguments for Nomi-
nal Predicates. Computational Linguistics, 38(4): 
755-798. 
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic 
features. In Proceedings of EMNLP 2009, pages 
1152-1161. 
Sanda Harabagiu, Dan Moldovan, Christine Clark, 
Mitchell Bowden, Andrew Hickl, Patrick Wang. 
2005. Employing two question answering systems 
in TREC-2005. In Proceedings of Text Retrieval 
Conference  2005. 
61
Andrew Hickl, John Williams, Jeremy Bensley, Kirk 
Roberts, Ying Shi, Bryan Rink. 2006. Question 
Answering with LCC?s CHAUCER at TREC 2006. 
In Proceedings of Text Retrieval Conference  2006. 
Lawrence Hunter and Kevin B. Cohen. 2006. Bio-
medical language processing: what's beyond Pub-
Med? Molecular Cell. 21(5):589-94. 
Halil Kilicoglu and Sabine Bergler 2012. Biological 
Event Composition. BMC Bioinformatics, 13 
(Supplement 11):S7.  
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 
2008. Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics, 9:10. 
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi 
Tsujii, Toshihisa Takagi, Akinori Yonezawa. 2012. 
The Genia Event and Protein Coreference tasks of 
the BioNLP Shared Task 2011.  BMC Bioinformat-
ics, 13(Supplement 11):S1.  
Adam Lally, John M. Prager, Michael C. McCord, 
Branimir Boguraev, Siddharth Patwardhan, James 
Fan, Paul Fodor, Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How Watson reads a clue. IBM Jour-
nal of Research and Development, 56(3):2. 
Heeyoung Lee, Yves Peirsman, Angel Chang, Na-
thanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford's Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared 
Task. In Proceedings of the CoNLL-2011 Shared 
Task, pages 28-34. 
Donald A.B. Lindberg, Betsy L. Humphreys, Alexa T. 
McCray. 1993. The Unified Medical Language 
System. Methods of information in medicine, 
32(4): 281-291. 
Feifan Liu, Lamont D. Antieau, Hong Yu. 2011. To-
ward automated consumer question answering: au-
tomatically separating consumer questions from 
professional questions in the healthcare domain. 
Journal of Biomedical Informatics, 44(6): 1032-
1038. 
David McClosky, Sebastian Riedel, Mihai Surdeanu, 
Andrew McCallum, Christopher Manning. 2012. 
Combining joint models for biomedical event ex-
traction. BMC Bioinformatics, 13 (Supplement 11): 
S9. 
Alexa McCray, Russell Loane, Allen Browne, Anan-
tha Bangalore. 1999. Terminology issues in user 
access to Web-based medical information. In 
AMIA Annual Symposium Proceedings, pages 107?
111. 
Alexa McCray, Anita Burgun, Olivier Bodenreider. 
2001. Aggregating UMLS semantic types for re-
ducing conceptual complexity. In Proceedings of 
Medinfo, 10(Pt1): 216-220. 
Makoto Miwa, Paul Thompson, Sophia Ananiadou. 
2012. Boosting automatic event extraction from the 
literature using domain adaptation and coreference 
resolution. Bioinformatics, 28(13):1759-1765. 
Yuan Ni, Huijia Zhu, Peng Cai, Lei Zhang, Zhaoming 
Qui, Feng Cao. 2012. CliniQA: highly reliable 
clinical question answering system. Studies in 
Health Technology and Information, 180:215-219. 
John M. Prager. 2006. Open-domain question answer-
ing. Foundations and Trends in Information Re-
trieval, 1(2):91-231. 
Sebastian Riedel and Andrew McCallum. 2011. Fast 
and robust joint models for biomedical event ex-
traction. In Proceedings of EMNLP 2011, pages 1-
12. 
Josef Ruppenhofer, Caroline Sporleder, Roser Mo-
rante, Collin Baker, Martha Palmer. 2010. 
SemEval-2010 Task 10: Linking Events and Their 
Participants in Discourse. In Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
pages 45-50. 
Matthew S. Simpson and Dina Demner-Fushman. 
2012. Biomedical Text Mining: a Survey of Recent 
Progress. Mining Text Data 2012:465-517. 
Amanda Spink, Yin Yang, Jim Jansen, Pirrko 
Nykanen, Daniel P. Lorence, Seda Ozmutlu, H. 
Cenk Ozmutlu. 2004. A study of medical and 
health queries to web search engines. Health In-
formation & Libraries Journal. 21(1):44-51. 
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?, 
Tomoko Ohta, Sophia Ananiadou, Jun?ichi Tsujii. 
2012. Brat: a Web-based Tool for NLP-Assisted 
Text Annotation. In Proceedings of the Demon-
strations Sessions at EACL 2012, pages 102-107. 
Paul Thompson, Syed A. Iqbal, John McNaught, So-
phia Ananiadou. 2009. Construction of an annotat-
ed corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10:349. 
Jos? L. Vicedo and Antonio Ferr?ndez. 2000. Im-
portance of pronominal anaphora resolution in 
question answering systems. In Proceedings of the 
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 555-562. 
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu 
Hirao, Masayuki Asahara, Yuji Matsumoto. 2011. 
Coreference Based Event-Argument Relation Ex-
traction on Biomedical Text. Journal of Biomedi-
cal Semantics, 2 (Supplement 5):S6. 
Yan Zhang. 2010. Contextualizing Consumer Health 
Information Searching: an Analysis of Questions in 
a Social Q&A Community. In Proceedings of the 
1st ACM International Health Informatics Sympo-
sium (IHI?10), pages 210-219. 
62
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29?37,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Decomposing Consumer Health Questions
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
robertske@nih.gov, {kilicogluh,fiszmanm,ddemner}@mail.nih.gov
Abstract
This paper presents a method for decom-
posing long, complex consumer health
questions. Our approach largely decom-
poses questions using their syntactic struc-
ture, recognizing independent questions
embedded in clauses, as well as coordi-
nations and exemplifying phrases. Addi-
tionally, we identify elements specific to
disease-related consumer health questions,
such as the focus disease and background
information. To achieve this, our approach
combines rank-and-filter machine learning
methods with rule-based methods. Our
results demonstrate significant improve-
ments over the heuristic methods typically
employed for question decomposition that
rely only on the syntactic parse tree.
1 Introduction
Natural language questions provide an intuitive
method for consumers (non-experts) to query for
health-related content. The most intuitive way
for consumers to formulate written questions is
the same way they write to other humans: multi-
sentence, complex questions that contain back-
ground information and often more than one spe-
cific question. Consider the following:
? Will Fabry disease affect a transplanted kidney?
Previous to the transplant the disease was be-
ing managed with an enzyme supplement. Will
this need to be continued? What cautions or ad-
ditional treatments are required to manage the
disease with a transplanted kidney?
This complex question contains three question
sentences and one background sentence. The fo-
cus (Fabry disease) is stated in the first question
but is necessary for a full understanding of the
other questions as well. The background sentence
is necessary to understand the second question:
the anaphor this must be resolved to an enzyme
treatment, and the predicate continue?s implicit ar-
gument that must be re-constructed from the dis-
course (i.e., continue after a kidney transplant).
The final question sentence uses a coordination
to ask two separate questions (cautions and addi-
tional treatments). A decomposition of this com-
plex question would then result in four questions:
1. Will Fabry disease affect a transplanted kidney?
2. Will enzyme treatment for Fabry disease need to
be continued after a kidney transplant?
3. What cautions are required to manage Fabry
disease with a transplanted kidney?
4. What additional treatments are required to man-
age Fabry disease with a transplanted kidney?
Each question above could be independently an-
swered by a question answering (QA) system.
While previous work has discussed methods for
resolving co-reference and implicit arguments in
consumer health questions (Kilicoglu et al., 2013),
it does not address question decomposition.
In this work, we propose methods for auto-
matically recognizing six annotation types use-
ful for decomposing consumer health questions.
These annotations distinguish between sentences
that contain questions and background informa-
tion. They also identify when a question sentence
can be split in multiple independent questions, and
29
when they contain optional or coordinated infor-
mation embedded within a question.
For each of these decomposition annotations,
we propose a combination of machine learning
(ML) and rule based methods. The ML methods
largely take the form of a 3-step rank-and-filter
approach, where candidates are generated, ranked
by an ML classifier, then the top-ranked candidate
is passed through a separate ML filtering classi-
fier. We evaluate each of these methods on a set of
1,467 consumer health questions related to genetic
and rare diseases.
2 Background
QA in the biomedical domain has been well-
studied (Demner-Fushman and Lin, 2007; Cairns
et al., 2011; Cao et al., 2011) as a means for re-
trieving medical information. This work has typ-
ically focused, however, on questions posed by
medical professionals, and the methods proposed
for question analysis generally assume a single,
concise question. For example, Demner-Fushman
and Abhyankar (2012) propose a method for ex-
tracting frames from queries for the purpose of
cohort retrieval. Their method assumes syntactic
dependencies exist between the necessary frame
elements, and is thus not well-suited to handle
long, multi-sentence questions. Similarly, Ander-
sen et al. (2012) proposes a method for converting
a concise question into a structured query. How-
ever, many medical questions require background
information that is difficult to encode in a single
question sentence. Instead, it is often more natural
to ask multiple questions over several sentences,
providing background information to give context
to the questions. Yu and Cao (2008) use a ML
method to recognize question types in professional
health questions. Their method can identify more
than one type per complex question. Without de-
composing the full question into its sub-questions,
however, the type cannot be associated with its
specific span, or with other information specific to
the sub-question. This other information can in-
clude answer types, question focus, and other an-
swer constraints. By decomposing multi-sentence
questions, these question-specific attributes can be
extracted, and the discourse structure of the larger
question can be better understood.
Question decomposition has been utilized be-
fore in open-domain QA approaches, but rarely
evaluated on its own. Lacatusu et al. (2006)
demonstrates how question decomposition can im-
prove the performance of a multi-sentence sum-
marization system. They perform what we refer
to as syntactic question decomposition, where the
syntactic structure of the question is used to iden-
tify sub-questions that can be answered in isola-
tion. A second form of question decomposition is
semantic decomposition, which can semantically
break individual questions apart to answer them
in stages. For instance, the question ?When did
the third U.S. President die?? can be semantically
decomposed ?Who was the third U.S. President??
and ?When did X die??, where the answer to the
first question is substituted into the second. Katz
and Grau (2005) discusses this kind of decompo-
sition using the syntactic structure, though it is not
empirically validated. Hartrumpf (2008) proposes
a decomposition method using only the deep se-
mantic structure. Finally, Harabagiu et al. (2006)
proposes a different type of question decomposi-
tion based on a random walk over similar ques-
tions extracted from a corpus. In our work, we
focus on syntactic question decomposition. We
demonstrate the importance of empirical evalua-
tion of question decomposition, notably the pit-
falls of heuristic approaches that rely entirely on
the syntactic parse tree. Syntactic parsers trained
on Treebank are particularly poor at both analyz-
ing questions (Judge et al., 2006) and coordination
boundaries (Hogan, 2007). Robust question de-
composition methods, therefore, must be able to
overcome many of these difficulties.
3 Consumer Health Question
Decomposition
Our goal is to decompose multi-sentence, multi-
faceted consumer health questions into concise
questions coupled with important contextual in-
formation. To this end, we utilize a set of an-
notations that identify the decomposable elements
and important contextual elements. A more de-
tailed description of these annotations is provided
in Roberts et al. (2014). The annotations are pub-
licly available at our institution website
1
. Here, we
briefly describe each annotation:
(1) BACKGROUND - a sentence indicating useful
contextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-
cates an independent question.
1
http://lhncbc.nlm.nih.gov/project/consumer-health-
question-answering
30
Sentence Splitting 
Request 
Question 
Sentence 
Ignore 
Sentence 
Background 
Sentence 
Candidate Generation 
UMLS 
SVM Candidate Ranking 
Boundary Fixing 
Focus 
Focus Recognition 
Sentence Classification 
Background Classification 
SVM Comorbidity Classification 
SVM Diagnosis Classification 
SVM Family History Classification 
SVM ISF Classification 
SVM Lifestyle Classification 
SVM Symptom Classification 
SVM Test Classification 
Candidate Generation 
SVM Candidate Filter 
Question Recognition 
SVM Sentence Classification 
Question 
Candidate Generation 
SVM Candidate Ranking 
Exemplification Recognition 
Candidate Filter 
Candidate Generation 
SVM Candidate Ranking 
Coordination Recognition 
SVM Candidate Filter Coordination 
Exemplification 
Stanford 
Parser 
WordNet 
SVM Treatment Classification 
Figure 1: Question Decomposition Architecture. Modules with solid green lines indicate machine learn-
ing classifiers. Modules with dotted green lines indicate rule-based classifiers.
(3) COORDINATION - a phrase that spans a set of
decomposable items.
(4) EXEMPLIFICATION - a phrase that spans an
optional item.
(5) IGNORE - a sentence indicating nothing of
value is present.
(6) FOCUS - an NP indicating the theme of the
consumer health question.
Further explanations of each annotation are pro-
vided in Sections 4-9. To convert these annota-
tions into separate, decomposed questions, a sim-
ple set of recursive rules is used. The rules enu-
merate all ways of including one conjunct from
each COORDINATION as well as whether or not
to include the phrase within an EXEMPLIFICA-
TION. These rules must be applied recursively to
handle overlapping annotations (e.g., a COORDI-
NATION within an EXEMPLIFICATION). Our im-
plementation is straight-forward and not discussed
further in this paper. The BACKGROUND and FO-
CUS annotations do not play a direct role in this
process, though they provide important contextual
elements and are useful for co-reference, and are
thus still considered part of the overall decompo-
sition process.
It should also be noted that some questions are
syntactically decomposable, but doing so alters
their original meaning. Consider the following
two question sentences:
? Can this disease be cured or can we only treat
the symptoms?
? Are males or females worse affected?
While the first example contains two ?Can...?
questions and the second example contains the co-
ordination ?males or females?, both questions are
providing a choice between two alternatives and
decomposing them would alter the semantic na-
ture of the original question. In these cases, we do
not consider the questions to be decomposable.
Data We use a set of consumer health ques-
tions collected from the Genetic and Rare Dis-
eases Information Center (GARD), which main-
tains a website
2
with publicly available consumer-
submitted questions and professionally-authored
answers about genetic and rare diseases. We col-
lected 1,467 consumer health questions, consist-
ing of 4,115 sentences, 1,713 BACKGROUND sen-
tences, 37 IGNORE sentences, 2,465 QUESTIONs,
367 COORDINATIONs, 53 EXEMPLIFICATIONs,
and 1,513 FOCUS annotations. Questions with
more than one FOCUS are generally concerned
with the relation between diseases. Further infor-
mation about the corpus and the annotation pro-
cess can be found in Roberts et al. (2014).
System Architecture The architecture of our
question decomposition method is illustrated in
2
http://rarediseases.info.nih.gov/gard
31
Figure 1. To avoid confusion, in the rest of this
paper we refer to a complex consumer health ques-
tion simply as a request. Requests are sent to
the independent FOCUS recognition module (Sec-
tion 4), and then proceed through a pipeline that
includes the classification of sentences (Section 5),
the identification of separate QUESTIONs within
a question sentence (Section 6), the recognition
of COORDINATIONs (Section 7) and EXEMPLIFI-
CATIONs (Section 8), and the sub-classification of
BACKGROUND sentences (Section 9).
Experimental Setup The remainder of this pa-
per describes the individual modules in Figure 1.
For simplicity, we show results on the GARD data
for each task in its corresponding section. In all
cases, experiments are conducted using a 5-fold
cross-validation on the GARD data. The cross-
validation folds are organized at the request level
so that no two items from the same request will be
split between the training and testing data.
4 Identifying the Focal Disease
The FOCUS is the condition that disease-centered
questions are centered around. Many other dis-
eases may be mentioned, but the FOCUS is the dis-
ease of central concern. This is similar to the as-
sumption made about a central disease in Medline
abstracts (Demner-Fushman and Lin, 2007). Of-
ten the FOCUS is stated in the first sentence (typ-
ically a BACKGROUND) of the request while the
questions are near the end. The questions can-
not generally be answered outside the context of
the FOCUS, however, so its identification is a crit-
ical part of decomposition. As shown in Figure 1,
we use a 3-step process: (1) a high-recall method
identifies potential FOCUS diseases in the data, (2)
a support vector machine (SVM) ranks the FO-
CUS candidates, and (3) the highest-ranking can-
didate?s boundary is modified with a set of rules to
better match our annotation standard.
To identify candidates for the FOCUS, we use a
lexicon constructed from UMLS (Lindberg et al.,
1993). UMLS includes very generic terms, such as
disease and cancer, that are too general to exactly
match a FOCUS in our data. We allow these terms
to be candidates so as to not miss any FOCUS that
doesn?t exactly match an entry in UMLS. When
such a general term is selected as the top-ranked
FOCUS, the rules described below are capable of
expanding the term to the full disease name.
To rank candidates, we utilize an SVM (Fan et
E/R P R F
1
1st UMLS Disorder
E 19.6 19.0 19.3
R 28.2 27.4 27.8
SVM
E 56.4 54.7 55.6
R 89.2 86.5 87.9
SVM + Rules
E 74.8 72.5 73.6
R 89.5 86.8 88.1
Table 1: FOCUS recognition results. E = exact
match; R = relaxed match.
al., 2008) with a small number of feature types:
? Unigrams. Identifies generic words such as dis-
ease and syndrome that indicate good FOCUS
candidates, while also recognizing noisy UMLS
terms that are often false positives.
? UMLS semantic group (McCray et al., 2001).
? UMLS semantic type.
? Sentence Offset. The FOCUS is typically in the
first sentence, and is far more likely to be at the
beginning of the request than the end.
? Lexicon Offset. The FOCUS is typically the first
disease mentioned.
During training, the SVM considers any candidate
that overlaps the gold FOCUS to be correct. This
enables our approach to train on FOCUS examples
that do not perfectly align with a UMLS concept.
At test time, all candidates are classified, ranked
by the classifier?s confidence, and the top-ranked
candidate is considered the FOCUS.
As mentioned above, there are differences be-
tween how a FOCUS is annotated in our data and
how it is represented in the UMLS. We therefore
use a series of heuristics to alter the boundary to a
more usable FOCUS after it is chosen by the SVM.
The rules are applied iteratively to widen the FO-
CUS boundary until it cannot be expanded any fur-
ther. If a generic disease word is the only token
in the FOCUS, we add the token to the left. Con-
versely, if the token on the right is a generic dis-
ease word, it is added as well. If the word to the
left is capitalized, it is safe to assume it is part of
the disease?s name and so it is added as well. Fi-
nally, several rules recognize the various ways in
which a disease sub-type might be specified (e.g.,
Behcet?s syndrome vascular type, type 2 diabetes,
Charcot-Marie-Tooth disease type 2C).
We evaluate FOCUS recognition with both an
exact match, where the gold and automatic FOCUS
boundaries must line up perfectly, and a relaxed
match, which only requires a partial overlap. As a
baseline, we compare our results against a fully
rule-based system where the first UMLS Disor-
der term in the request is considered the FOCUS.
32
We also evaluate the effectiveness of our bound-
ary altering rules by measuring performance with-
out these rules. The results are shown in Table 1.
The baseline method shows significant problems
in precision and recall. It is not able to ignore
noisy UMLS terms (e.g., aim is both a gene and
a treatment). The SVM improves upon the rule-
based method by over 50 points in F
1
for relaxed
matching. Adding the boundary fixing rules has
little effect on relaxed matching, but greatly im-
proves exact matching: precision and recall are
improved by 18.4 and 17.8 points, respectively.
5 Classifying Sentences
Before precise question boundaries can be rec-
ognized, we first identify sentences that con-
tain QUESTIONs, as distinguished from BACK-
GROUND and IGNORE sentences. It should be
noted that many of the question sentences in our
data are not typical wh-word questions. About
20% of the questions in our data end in a period.
For instance:
? Please tell me more about this condition.
? I was wondering if you could let me know where
I can find more information on this topic.
? I would like to get in contact with other families
that have this illness.
We consider a sentence to be a question if it con-
tains any information request, explicit or implicit.
After sentence splitting, we identify sentences
using a multi-class SVM with three feature types:
? Unigrams with parts-of-speech (POS). Reduces
unigram ambiguities, such as what-WP (a pro-
noun, indicative of a question) versus what-
WDT (a determiner, not indicative).
? Bigrams.
? Parse tree tags. All Treebank tags from the syn-
tactic parse tree. Captures syntactic question
clues such as the phrase tags SQ (question sen-
tence) and WHNP (wh-word noun phrase).
The SVM classifier performs at 97.8%. For com-
parison, an SVM with only unigram features per-
forms at 97.2%. While the unigram model does a
good job classifying sentences, suggesting this is
a very easy task, the improved feature set reduces
the number of errors by 20%.
6 Identifying Questions
QUESTION recognition is the task of identifying
when a conjunction like and joins two independent
questions into a single sentence:
? [What causes the condition]
QUESTION
[and what
treatment is available?]
QUESTION
? [What is this disease]
QUESTION
[and what steps
can I take to protect my daughter?]
QUESTION
We consider the identification of separate QUES-
TIONs within a single sentence to be a differ-
ent task from COORDINATION recognition, which
finds phrases whose conjuncts can be treated in-
dependently. Linguistically, these tasks are quite
similar, but the distinction lies in whether the
right-conjunct syntactically depends on anything
to its left. For instance:
? I would like to learn [more about this condition
and what the prognosis is for a baby born with
it]
COORDINATION
.
Here, the right-conjunct starts with a question
stem (what), but is not a complete, grammatical
question on its own. Alternatively, this could be
re-formed into two separate QUESTIONs:
? [I would like to learn more about this
condition,]
QUESTION
[and what is the prognosis
is for a baby born with it.]
QUESTION
We make this distinction because the QUESTION
recognition task requires one fewer step since the
boundaries extend to the entire sentence, prevent-
ing error propagation from an input module. Fur-
ther, the features that differentiate our QUESTION
and COORDINATION annotations are different.
The two-step process for recognizing QUES-
TIONs includes: (1) a high-recall candidate gener-
ator, and (2) an SVM to eliminate candidates that
are not separate QUESTIONs. The candidates for
QUESTION recognition are simply all the ways a
sentence can be split by the conjunctions and, or,
as well as, and the forward slash (?/?). In our data,
this candidate generation process has a recall of
98.6, as a few examples were missed where candi-
dates were not separated by one of the above con-
junctions.
To filter candidates, we use an SVM with three
features types:
? The conjunction separating the QUESTIONs.
? Unigrams in the left-conjunct. Identifies when
the left-conjunct is not a QUESTION, or when a
question is part of a COORDINATION.
? The right-conjunct?s parse tree tag. Recog-
nizes when the right-conjunct is an independent
clause that may safely be split.
33
P R F
1
QUESTION split recognition
Baseline 24.7 82.4 38.0
SVM 67.7 64.7 66.2
Overall QUESTION recognition
Baseline 87.3 92.8 90.0
SVM 97.7 97.4 97.5
Table 2: QUESTION recognition results.
For evaluation, we measure both the F
1
score
for correct candidates, and the overall F
1
for all
QUESTION annotations (i.e., all QUESTION sen-
tences). We also evaluate a baseline method that
utilizes the parse tree to recognize separate QUES-
TIONs by splitting sentences where a conjunction
separates independent clauses. The results are
shown in Table 2. The baseline method has good
recall for recognizing where a sentence should be
split into multiple QUESTIONs, but it lacks preci-
sion. This is largely because it is unable to differ-
entiate clausal COORDINATIONs such as the above
example, as well as when the left-conjunct is not
actually a separate question. For instance:
? Our grandson was diagnosed recently with this
disease and I am wondering if you could send
me information on it.
The SVM-based method can overcome this prob-
lem by looking at the words in the left-conjunct.
Both methods, however, fail to recognize when
two independent question clauses are asking the
same question but providing alternative answers:
? Will this condition be with him throughout his
life, or is it possible that it will clear up?
While there are methods for handling this issue
for COORDINATION recognition, addressed be-
low, recognizing non-splittable QUESTIONs re-
quires far deeper semantic understanding which
we leave to future work.
7 Identifying Coordinations
COORDINATION recognition is the task of identi-
fying when a conjunction joins phrases within a
QUESTION that can in be separate questions:
? How can I learn more about [treatments and
clinical trials]
COORDINATION
?
? Are [muscle twitching, muscle cramps, and
muscle pain]
COORDINATION
effects of having sil-
icosis?
Unlike QUESTION recognition, the boundaries of
a COORDINATION need to be determined as well
as whether the conjuncts can semantically be split
into separate questions. We thus use a three-step
process for recognizing COORDINATIONs: (1) a
high-recall candidate generator, (2) an SVM to
rank all the candidates for a given conjunction, and
(3) an SVM to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid conjunctions within a QUESTION an-
notation. We use the same four conjunctions as in
QUESTION recognition: and, or, as well as, and
the forward slash. For each of these, all possi-
ble left and right boundaries are generated, so in
a QUESTION with 4 tokens on either side of the
conjunction, there would be 16 candidates. Addi-
tionally, two adjectives separated by a comma and
immediately followed by a noun are considered a
candidate (e.g., ?a [safe, permanent]
COORDINATION
treatment?). In our data, this candidate generation
process has a recall of 98.9, as a few instances ex-
ist in which a conjunction is not used, such as:
? I am looking for any information you have
about heavy metal toxicity, [treatment,
outcomes]
EXEMPLIFICATION+COORDINATION
.
To rank candidates, we use an SVM with the
following feature types:
? If the left-conjunct is congruent with the high-
est node in the syntactic parse tree whose right-
most leaf is also the right-most token in the left-
conjunct. Essentially, this is equivalent to say-
ing whether or not the syntactic parser agrees
with the left-conjunct?s boundary.
? The equivalent heuristic for the right-conjunct.
? If a noun is in both, just the left conjunct, just
the right conjunct, or neither conjunct.
? The Levenshtein distance between the POS tag
sequences for the left- and right-conjuncts.
The first two features encode the information a
rule-based method would use if it relied entirely
on the syntactic parse tree. The remaining features
help the classifier overcome cases where the parser
may be wrong.
At training time, all candidates for a given con-
junction are generated and only the candidate that
matches the gold COORDINATION is considered
a positive example. Additionally, we annotated
the boundaries for negative COORDINATIONs (i.e.,
syntactic coordinations that do not fit our annota-
tion standard). There were 203 such instances in
the GARD data. These are considered gold CO-
ORDINATIONs for boundary ranking only.
To filter the top-ranked candidates, we use an
SVM with several feature types:
34
E/R P R F
1
Baseline
E 28.1 36.5 31.8
R 62.9 75.8 68.7
Rank + Filter
E 38.2 34.8 36.4
R 78.5 69.0 73.5
Table 3: COORDINATION recognition results.
E = exact match; R = relaxed match.
? The conjunction.
? Unigrams in the left-conjunct.
? POS of the first word in both conjuncts. CO-
ORDINATIONs often have the same first POS in
both conjuncts.
? The word immediately before the candidate.
E.g., between is a good negative indicator.
? Unigrams in the question but not the candidate.
? If the candidate takes up almost the entire ques-
tion (all but 3 tokens). Typically, COORDINA-
TIONs are much smaller than the full question.
? If more than one conjunction is in the candidate.
? If a word in the left-conjunct has an antonym
in the right conjunct. Antonyms are recognized
via WordNet (Fellbaum, 1998).
At training time, the positive examples are drawn
from the annotated COORDINATIONs, while the
negative examples are drawn from the 203 non-
gold annotations mentioned above.
In addition to evaluating this method, we
evaluate a baseline method that relies entirely
on the syntactic parse to identify COORDINA-
TION boundaries without filtering. The results
are shown in Table 3. The rank-and-filter ap-
proach shows significant gains over the rule-based
method in precision and F
1
. As can be seen in
the difference between exact and relaxed match-
ing, most of the loss for both the baseline and ML
methods come in boundary detection. Most meth-
ods overly rely upon the syntactic parser, which
performs poorly both on questions and coordina-
tions. The ML method, though, is sometimes able
to overcome this problem.
8 Identifying Exemplifications
EXEMPLIFICATION recognition is the task of iden-
tifying when a phrase provides an optional, exem-
plifying example with a more specific type of in-
formation than that asked by the rest of the ques-
tion. For instance, the following contains both an
EXEMPLIFICATION and a COORDINATION:
? Is there anything out there that can help
him [such as [medications or alternative
therapies]
COORDINATION
]
EXEMPLIFICATION
?
We could consider this to denote 3 questions:
? Is there anything out there that can help him?
? Is there anything out there that can help him
such as medications?
? Is there anything out there that can help him
such as alternative therapies?
In the latter two questions, we consider the phrase
such as to now denote a mandatory constraint on
the answer to each question, whereas in the origi-
nal question it would be considered optional.
EXEMPLIFICATION recognition is similar to
COORDINATION recognition, and its three-step
process is thus similar as well: (1) a high-recall
candidate generator, (2) an SVM to rank all the
candidates for a given trigger phrase, and (3) a set
of rules to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid trigger words and phrases. These in-
clude: especially, including, particularly, specifi-
cally, and such as. For each of these, all possible
right boundaries are generated, thus EXEMPLIFI-
CATIONs have far fewer candidates than COORDI-
NATIONs. Additionally, all phrases within paren-
theses are added as EXEMPLIFICATIONs. In our
data, this candidate generation process has a recall
of 98.1, missing instances without a trigger (see
the example also missed by COORDINATION can-
didate generation in Section 7).
To rank candidates, we use an SVM with the
following feature types:
? If the right-conjunct is the highest parse node
as defined in the COORDINATION boundary fea-
ture.
? If a dependency relation crosses from the right-
conjunct to any word outside the candidate.
? POS of the word after the candidate.
As with COORDINATIONs, we annotated bound-
aries for negative EXEMPLIFICATIONs matching
the trigger words and used them as positive exam-
ples for boundary ranking.
To filter the top-ranked candidates, we use two
simple rules. First, EXEMPLIFICATIONs within
parentheses are filtered if they are acronyms or
acronym expansions. Second, cases such as the
below example are removed by looking at the
words before the candidate:
? I am particularly interested in learning more
about genetic testing for the syndrome.
In addition to evaluating this method, we eval-
uate a baseline method that relies entirely on the
35
E/R P R F
1
Baseline
E 28.9 62.3 39.5
R 39.5 84.9 53.9
Rank + Filter
E 60.8 58.5 59.6
R 80.4 77.4 78.8
Table 4: EXEMPLIFICATION recognition results.
E = exact match; R = relaxed match.
syntactic parser to identify EXEMPLIFICATION
boundaries and performs no filtering. The re-
sults are shown in Table 4. The rank-and-filter
approach shows significant gains over the rule-
based method in precision and F
1
, more than dou-
bling precision for both exact and relaxed match-
ing. There is still a drop in performance when go-
ing from relaxed to exact matching, again largely
due to the reliance on the syntactic parser.
9 Classifying Background Information
BACKGROUND sentences contain contextual in-
formation, such as whether or not a patient has
been diagnosed with the focal disease or what
symptoms they are experiencing. This informa-
tion was annotated at the sentence level, partly be-
cause of annotation convenience, but also because
phrase boundaries are not always clear for medical
concepts (Hahn et al., 2012; Forbush et al., 2013).
A difficult factor in this task, and especially on
the GARD dataset, is that consumers are not al-
ways asking about a disease for themselves. In-
stead, often they ask on behalf of another individ-
ual, often a family member. The BACKGROUND
types are thus annotated based on the person of
interest, who we refer to as the patient (in the lin-
guistic sense). For instance, if a mother has a dis-
ease but is asking about her son (e.g., asking about
the probability of her son inheriting the disease),
that sentence would be a FAMILY HISTORY, as
opposed to a DIAGNOSIS sentence.
The GARD corpus is annotated with eight
BACKGROUND types:
? COMORBIDITY
? DIAGNOSIS
? FAMILY HISTORY
? ISF (information
search failure)
? LIFESTYLE
? SYMPTOM
? TEST
? TREATMENT
ISF sentences indicate previous attempts to find
the requested information have failed, and are a
good signal to the QA system to enable more in-
depth search strategies. LIFESTYLE sentences de-
scribe the patient?s life habits (e.g., smoking, ex-
ercise). Currently, the automatic identification of
Type P R F
1
# Anns
COMORBIDITY 0.0 0.0 0.0 23
DIAGNOSIS 80.8 80.3 80.5 690
FAMILY HISTORY 67.4 38.4 48.9 151
ISF 75.0 65.9 70.1 41
LIFESTYLE 0.0 0.0 0.0 13
SYMPTOM 76.6 48.1 59.1 320
TEST 37.5 4.9 8.7 61
TREATMENT 87.3 35.0 50.0 137
Overall: Micro-F
1
: 67.3 Macro-F
1
: 39.7
Table 5: BACKGROUND results.
BACKGROUND types has not been a major focus
of our effort as no handling exists for it within our
QA system. We report a baseline method and re-
sults here to provide some insight into the diffi-
culty of the task.
BACKGROUND types are a multi-labeling prob-
lem, so we use eight binary classifiers, one for
each type. Each classifier utilizes only unigram
and bigram features. The results for the mod-
els are shown in Table 5. COMORBIDITY and
LIFESTYLE are too rare in the data (23 and 13
instances, respectively) for the classifier to iden-
tify. DIAGNOSIS questions are identified fairly
well because this is the most common type (690
instances) and because of the constrained vocabu-
lary for expressing a diagnosis. The performance
of the rest of the types is largely proportional to
the number of instances in the data, though ISF
performs quite well given only 41 instances.
10 Conclusion
We have presented a method for decomposing
consumer health questions by recognizing six an-
notation types. Some of these types are general
enough to use in open-domain question decom-
position (BACKGROUND, IGNORE, QUESTION,
COORDINATION, EXEMPLIFICATION), while oth-
ers are targeted specifically at consumer health
questions (FOCUS and the BACKGROUND sub-
types). We demonstrate that ML methods can
improve upon heuristic methods relying on the
syntactic parse tree, though parse errors are of-
ten difficult to overcome. Since significant im-
provements in performance would likely require
major advances in open-domain syntactic parsing,
we instead envision further integration of the key
tasks in consumer health question analysis: (1) in-
tegration of co-reference and implicit argument in-
formation, (2) improved identification of BACK-
GROUND types, and (3) identification of discourse
relations within questions to further leverage ques-
tion decomposition.
36
Acknowledgements
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health. We would
additionally like to thank Stephanie M. Morri-
son and Janine Lewis for their help accessing the
GARD data.
References
Ulrich Andersen, Anna Braasch, Lina Henriksen,
Csaba Huszka, Anders Johannsen, Lars Kayser,
Bente Maegaard, Ole Norgaard, Stefan Schulz, and
J?urgen Wedekind. 2012. Creation and use of Lan-
guage Resources in a Question-Answering eHealth
System. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 2536?2542.
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,
James H. Martin, Martha S. Palmer, Wayne H. Ward,
and Guergana K. Savova. 2011. The MiPACQ Clin-
ical Question Answering System. In Proceedings of
the AMIA Annual Symposium, pages 171?180.
YongGang Cao, Feifan Liu, Pippa Simpson, Lamont
Antieau, Andrew Bennett, James J. Cimino, John
Ely, and Hong Yu. 2011. AskHERMES: An on-
line question answering system for complex clini-
cal questions. Journal of Biomedical Informatics,
44:277?288.
Dina Demner-Fushman and Swapna Abhyankar. 2012.
Syntactic-Semantic Frames for Clinical Cohort
Identification Queries. In Data Integration in the
Life Sciences, volume 7348 of Lecture Notes in
Computer Science, pages 100?112.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge-Based
and Statistical Techniques. Computational Linguis-
tics, 33(1).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tyler B. Forbush, Adi V. Gundlapalli, Miland N.
Palmer, Shuying Shen, Brett R. South, Guy Divita,
Marjorie Carter, Andrew Redd, Jorie M. Butler, and
Matthew Samore. 2013. ?Sitting on Pins and Nee-
dles?: Characterization of Symptom Descriptions in
Clinical Notes. In AMIA Summit on Clinical Re-
search Informatics, pages 67?71.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik
Faessler, Jenny Traum?uller, Susann Schr?oder, and
Kerstin Hornbostel. 2012. Iterative Refinement
and Quality Checking of Annotation Guidelines ?
How to Deal Effectively with Semantically Sloppy
Named Entity Types, such as Pathological Phenom-
ena. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 3881?3885.
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answer Complex Questions with Random
Walk Models. In Proceedings of the 29th Annual
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 220?227.
Sven Hartrumpf. 2008. Semantic Decomposition
for Question Answering. In Proceedings on the
18th European Conference on Artificial Intelligence,
pages 313?317.
Dierdre Hogan. 2007. Coordinate Noun Phrase Dis-
ambiguation in a Generative Parsing Model. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680?687.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. QuestionBank: Creating a Corpus of Parse-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497?504.
Yarden Katz and Bernardo C. Grau. 2005. Repre-
senting Qualitative Spatial Information in OWL-DL.
Proceedings of OWL: Experiences and Directions.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting Consumer Health
Questions: The Role of Anaphora and Ellipsis. In
Proceedings of the 2013 BioNLP Workshop, pages
54?62.
Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.
2006. Impact of Question Decomposition on the
Quality of Answer Summaries. In Proceedings of
LREC, pages 1147?1152.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Alexa T McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS Semantic Types
for Reducing Conceptual Complexity. In Studies
in Health Technology and Informatics (MEDINFO),
volume 84(1), pages 216?220.
Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil
Kilicoglu, and Dina Demner-Fushman. 2014. An-
notating Question Decomposition on Complex Med-
ical Questions. In Proceedings of LREC.
Hong Yu and YongGang Cao. 2008. Automatically
Extracting Information Needs from Ad Hoc Clini-
cal Questions. In Proceedings of the AMIA Annual
Symposium.
37
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 45?53,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Coreference Resolution for Structured Drug Product Labels
Halil Kilicoglu and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD, 20894
{kilicogluh,ddemner}@mail.nih.gov
Abstract
FDA drug package inserts provide com-
prehensive and authoritative information
about drugs. DailyMed database is a
repository of structured product labels ex-
tracted from these package inserts. Most
salient information about drugs remains
in free text portions of these labels. Ex-
tracting information from these portions
can improve the safety and quality of drug
prescription. In this paper, we present a
study that focuses on resolution of coref-
erential information from drug labels con-
tained in DailyMed. We generalized and
expanded an existing rule-based coref-
erence resolution module for this pur-
pose. Enhancements include resolution of
set/instance anaphora, recognition of ap-
positive constructions and wider use of
UMLS semantic knowledge. We obtained
an improvement of 40% over the baseline
with unweighted average F
1
-measure us-
ing B-CUBED, MUC, and CEAF metrics.
The results underscore the importance of
set/instance anaphora and appositive con-
structions in this type of text and point out
the shortcomings in coreference annota-
tion in the dataset.
1 Introduction
Almost half of the US population uses at least one
prescription drug and over 75% of physician of-
fice visits involve drug therapy
1
. Knowing how
these drugs will affect the patient is very impor-
tant, particularly, to over 20% of the patients that
are on three or more prescription drugs
1
. FDA
drug package inserts (drug labels or Structured
1
Centers for Disease Control and Preven-
tion: FASTSTATS - Therapeutic Drug Use:
http://www.cdc.gov/nchs/fastats/drugs.htm
Product Labels (SPLs)) provide curated informa-
tion about the prescription drugs and many over-
the-counter drugs. The drug labels for most drugs
are publicly available in XML format through Dai-
lyMed
2
. Some information in these labels, such as
the drug identifiers and ingredients, could be eas-
ily extracted from the structured fields of the XML
documents. However, the salient content about in-
dications, side effects and drug-drug interactions,
among others, is buried in the free text of the
corresponding sections of the labels. Extracting
this information with natural language process-
ing techniques can facilitate automatic timely up-
dates to databases that support Electronic Health
Records in alerting physicians to potential drug in-
teractions, recommended doses, and contraindica-
tions.
Natural language processing methods are in-
creasingly used to support various clinical and
biomedical applications (Demner-Fushman et al.,
2009). Extraction of drug information is playing a
prominent role in these applications and research.
In addition to earlier research in extraction of med-
ications and relations involving medications from
clinical text and the biomedical literature (Rind-
flesch et al., 2000; Cimino et al., 2007), in the
third i2b2 shared task (Uzuner et al., 2010), 23
organizations have explored extraction of medica-
tions, their dosages, routes of administration, fre-
quencies, durations, and reasons for administra-
tion from clinical text. The best performing sys-
tems used rule-based and machine learning tech-
niques to achieve over 0.8 F-measure for extrac-
tion of medication names; however, the remain-
ing information was harder to extract. Researchers
have also tackled extraction of drug-drug interac-
tions (Herrero-Zazo et al., 2013), side effects (Xu
and Wang, 2014), and indications (Fung et al.,
2013) from various biomedical resources.
As for many other information extraction tasks,
2
DailyMed: http://dailymed.nlm.nih.gov/dailymed/about.cfm
45
extracting drug information is often made more
difficult by coreference. Coreference is defined as
the relation between linguistic expressions that are
referring to the same entity (Zheng et al., 2011).
Coreference resolution is a fundamental task in
NLP and can benefit many downstream applica-
tions, such as relation extraction, summarization,
and question answering. Difficulty of the task is
due to the fact that various levels of linguistic in-
formation (lexical, syntactic, semantic, and dis-
course contextual features) generally play a role.
Coreference occurs frequently in all types of
biomedical text, including the drug package in-
serts. Consider the example below:
(1) Since amiodarone is a substrate for
CYP3A and CYP2C8, drugs/substances
that inhibit these isoenzymes may decrease
the metabolism . . . .
In this example, the expression these isoenzymes
refer to CYP3A and CYP2C8. Resolving this
coreference instance would allow us to capture the
following drug interactions mentioned in the sen-
tence: inhibitors of CYP3A POTENTIATE amio-
darone and inhibitors of CYP2C8 POTENTIATE
amiodarone.
In this paper, we present a study that focuses on
identification of coreference links in drug labels,
with the view that these relations will facilitate
the downstream task of drug interaction recogni-
tion. The rule-based system presented is an exten-
sion of the previous work reported in Kilicoglu et
al. (2013). The main focus of the dataset, based
on SPLs, is drug interaction information. Coref-
erence is only annotated when it is relevant to ex-
tracting such information. In addition to evaluat-
ing the system against a baseline, we also manu-
ally assessed the system output for precision. Fur-
thermore, we also evaluated the system on a sim-
ilarly drug-focused corpus annotated for anaphora
(DrugNerAR) (Segura-Bedmar et al., 2010). Our
results demonstrate that set/instance anaphora res-
olution and appositive recognition can play a sig-
nificant role in this type of text and highlight some
of the major areas of difficulty and potential en-
hancements.
2 Related Work
We discuss two areas of research related to this
study in this section: processing of drug labels
and coreference resolution focusing on biomedi-
cal text. Drug labels, despite their availability and
the wealth of information contained within them,
remain underutilized. One of the reasons might be
the complexity of the text in the labels: in a review
of publicly available text sources that could be
used to augment a repository of drug indications
and adverse effects (ADEs), Smith et al. (2011)
concluded that many indication and adverse drug
event relationships in the drug labels are too com-
plex to be captured in the existing databases of in-
teractions and ADEs. Despite the complexity, the
labels were used to extract indications for drugs in
several studies. Elkin et al. (2011) automatically
extracted indications, mapped them to SNOMED-
CT and then automatically derived rules in the
form (?Drug? HasIndication ?SNOMED CT?).
Fung et al. (2013) used MetaMap (Aronson and
Lang, 2010) to extract indications and map them
to the UMLS (Lindberg et al., 1993), and then
manually validated the quality of the mappings.
Oprea et al. (2011) used information extracted
from the adverse reactions sections of 988 drugs
for computer-aided drug repurposing. Duke et
al. (2011) have developed a rule-based system that
extracted 534,125 ADEs from 5602 SPLs. Zhu
et al. (2013) extracted disease terms from five
SPL sections (indication, contraindication, ADE,
precaution, and warning) and combined the ex-
tracted terms with the drug and disease relation-
ships in NDF-RT to disambiguate the PharmGKB
drug and disease associations. A hybrid NLP sys-
tem, AutoMCExtractor, uses conditional random
fields and post-processing rules to extract medical
conditions from SPLs and build triplets in the form
of([drug name]-[medical condition]-[LOINC sec-
tion header]) (Li et al., 2013).
Coreference resolution in the biomedical do-
main was addressed in the 2011 i2b2/VA shared
task (Uzuner et al., 2012), and the 2011 BioNLP
Shared Task (Kim et al., 2012); however these
community-wide evaluations did not change much
the observation in the 2011 review by Zheng
et al. (2011) that only a handful of systems
were developed for handling anaphora and coref-
erence in clinical text and biomedical publica-
tions. Since this comprehensive article was pub-
lished, Yoshikawa et al. (2011) proposed two
coference resolution models based on support vec-
tor machine and joint Markov logic network to
aid the task of biological event extraction. Sim-
ilarly, Miwa et al. (2012) and Kilicoglu and
Bergler (2012) extended their biological event
46
extraction pipelines using rule-based corefer-
ence systems that rely on syntactic information
and predicate argument structures. Nguyen et
al. (2012) evaluated contribution of discourse pref-
erence, number agreement, and domain-specific
semantic information in capturing pronominal and
nominal anaphora referring to proteins. An ef-
fort similar to ours is that of Segura-Bedmar et
al. (2010), who resolve anaphora to support drug-
drug interaction extraction. They created a cor-
pus of 49 interactions sections extracted from the
DrugBank database, having on average 40 sen-
tences and 716 tokens. They then manually anno-
tated pronominal and nominal anaphora, and de-
veloped a rule-based approach that achieve 0.76
F
1
-measure in anaphora resolution.
3 Methods
3.1 The dataset
We used a dataset extracted from FDA drug pack-
age labels by our collaborators at FDA interested
in extracting interactions between cardiovascu-
lar drugs. The dataset consists of 159 drug la-
bels, with an average of 105 sentences and 1787
tokens per label. It is annotated for three en-
tity types (Drug, Drug Class, and Substance) and
four drug interaction types (Caution, Decrease, In-
crease, and Specific). 377 instances of corefer-
ence were annotated. Two annotators separately
annotated the labels and one of the authors per-
formed the adjudication. The relatively low num-
ber of coreference instances is due to the fact that
coreference was annotated only when it would be
relevant to drug interaction recognition task. This
parsimonious approach to annotation presents dif-
ficulty in automatically evaluating the system, and
to mitigate this, we present an assessment of the
precision of our end-to-end coreference system, as
well. We split the dataset into training and test sets
by random sampling. Training data consists of 79
documents and the test set has 80 documents. We
used the training data for analysis and as the basis
of our enhancements.
3.2 The system
The work described in this paper extends and
refines earlier work, described in Kilicoglu et
al. (2013), which focused on disease anaphora and
ellipsis in the context of consumer health ques-
tions. We briefly recap that system here. The sys-
tem begins by mapping named entities to UMLS
Metathesaurus concepts (CUIs). Next, it identifies
anaphoric expressions in text, which include per-
sonal (e.g., it, they) and demonstrative pronouns
(e.g., this, those), as well as sortal anaphora (def-
inite (e.g., with the) and demonstrative (e.g., with
that) noun phrases). The candidate antecedents
are then recognized using syntactic (person, gen-
der and number agreement, head word matching)
and semantic (hypernym and UMLS semantic type
matching) constraints. Finally, the co-referent is
then selected as the focus of the question, which is
taken as the first disease mention in the question.
The coreference resolution pipeline used in the
current work, while enhanced significantly, fol-
lows the same basic sequence. The relatively sim-
ple approach of earlier work is generally sufficient
for consumer health questions; however, we found
it insufficient when it comes to drug labels. Aside
from the obvious point that the approach was lim-
ited to diseases, there are other stylistic differences
that have an impact on coreference resolution. In
contrast to informal and casual style of consumer
health questions, drug labels are curated and pro-
vide complex indication and ADE information in
a formal style, more akin to biomedical literature.
Our analysis of the training data highlighted sev-
eral facts regarding coreference in drug labels: (1)
the set/instance anaphora (including those involv-
ing distributive anaphora such as both, each, ei-
ther) instances are prominent, (2) demonstrative
pronominal anaphora is non-existent in contrast
to consumer health questions, (3) the focus-based
salience scoring is simplistic for longer texts. We
describe the system enhancements below.
3.2.1 Generalizing from diseases to drugs
and beyond
We generalized from resolution of disease coref-
erence only to resolution of coreference involv-
ing other entity types. For this purpose, we para-
materized semantic groups and hypernym lists as-
sociated with each semantic group. We general-
ized the system in the sense that new semantic
types and hypernyms can be easily defined and
used by the system. In addition to Disorder se-
mantic group and Disorder hypernym list defined
in earlier work, we used Drug, Intervention, Pop-
ulation, Procedure, Anatomy, and Gene/Protein
semantic groups and hypernym lists. Semantic
group classification largely mimics coarse-grained
UMLS semantic groups (McCray et al., 2001).
For example, UMLS semantic types Pharmaco-
47
logic Substance and Clinical Drug are aggregated
into both Drug and Intervention semantic groups,
while Therapeutic or Preventive Procedure is as-
signed to Procedure group only. Drug hypernyms,
such as medication, drug, agent, were derived
from the training data.
3.2.2 Set/instance anaphora
Set/instance anaphora instances are prevalent in
drug labels. In our dataset, 19% of all anno-
tated anaphoric expressions indicate set/instance
anaphora (co-referring with 29% of antecedent
terms). An example was provided earlier (Ex-
ample 1). While recognizing anaphoric expres-
sions that indicate set/instance anaphora is not
necessarily difficult (i.e., recognizing these isoen-
zymes in the example), linking them to their an-
tecedents can be difficult, since it generally in-
volves correctly identifying syntactic coordina-
tion, a challenging syntactic parsing task (Ogren,
2010). Our identification of these structures re-
lies on collapsed Stanford dependency output (de
Marneffe et al., 2006) and uses syntactic and se-
mantic constraints. We examine all the depen-
dency relations extracted from a sentence and only
consider those with the type conj * (e.g., conj and,
conj or). For increased accuracy, we then check
the tokens involved in the dependency (conjuncts)
and ensure that there is a coordinating conjunc-
tion (e.g., and, or, , (comma), & (ampersand)) be-
tween them. Once such a conjunction is identified,
we then examine the semantic compatibility of the
conjuncts. In the case of entities, the compatibil-
ity involves that at the semantic group level. In the
current work, we also began recognizing distribu-
tive anaphora, such as either, each as anaphoric
expressions. When the recognized anaphoric ex-
pression is plural (as in they, these agents or either
drug), we allow the coordinated structures previ-
ously identified in this fashion as candidate an-
tecedents. The current work does not address a
more complex kind of set/instance anaphora, in
which the instances are not syntactically coordi-
nated, such as in Example (2), where such agents
refer to thiazide diuretics, in the preceding sen-
tence, as well as Potassium-sparing diuretics and
potassium supplements.
(2) . . . can attenuate potassium loss caused
by thiazide diuretics. Potassium-sparing
diuretics . . . or potassium supplements can
increase . . . . if concomitant use of
such agents is indicated . . .
3.2.3 Appositive constructions
Coreference involving appositive constructions
3
are annotated in some corpora, including the
BioNLP shared task coreference dataset (Kim
et al., 2012) and DrugNerAR corpus (Segura-
Bedmar et al., 2010). An example is given below,
in which the indefinite noun phrase a drug and the
drug lovastatin are appositives.
(3) PLETAL does not, however, appear to cause
increased blood levels of drugs metabolized
by CYP3A4, as it had no effect on lovastatin,
a drug with metabolism very sensitive to
CYP3A4 inhibition.
In our dataset, coreference involving apposi-
tive constructions were generally left unannotated.
However, it was consistently the case that when
one of the items in the construction is annotated
as the antecedent for an anaphoric expression,
the other item in the construction was also anno-
tated as such. Therefore, we identified appositive
constructions in text to aid the antecedent selec-
tion task. We used dependency relations for this
task, as well. Identifying appositives is relatively
straightforward using syntactic dependency rela-
tions. We adapted the following rule from Kil-
icoglu and Bergler (2012):
APPOS(Antecedent,Anaphor) ?
APPOS(Anaphor,Antecedent) ?
COREF(Anaphor,Antecedent)
where APPOS ? {appos, abbrev, prep including,
prep such as}. In our case, this rule becomes
(APPOS(Antecedent1,Antecedent2) ?
APPOS(Antecedent2,Antecedent1)) ?
COREF(Anaphor,Antecedent1) ?
COREF(Anaphor,Antecedent2)
which essentially states that a candidate is taken as
an antecedent, only if its appositive has been rec-
ognized as an antecedent. Additionally, semantic
compatibility between the items is required.
This allows us to identify their and Class Ia an-
tiarrhythmic drugs as co-referents in the following
example, due to the fact that the exemplification
indicated by the appositive construction between
Class Ia antiarrythmic drugs and disopyramide is
recognized, the latter previously identified as an
antecedent for their.
3
We use the term ?appositive? to cover exemplifications,
as well.
48
(4) Class Ia antiarrhythmic drugs, such as
disopyramide, quinidine and procainamide
and other Class III drugs (e.g., amiodarone)
are not recommended . . . because of their
potential to prolong refractoriness.
3.2.4 Relative pronouns
Similar to appositive constructions, relative pro-
nouns are annotated as anaphoric expressions in
some corpora (same as those for appositives), but
not in our dataset. In the example below, the rela-
tive pronoun which refers to potassium-containing
salt substitutes.
(5) . . . the concomitant use of potassium-sparing
diuretics, potassium supplements, and/or
potassium-containing salt substitutes, which
should be used cautiously. . .
Since we aim for generality and this type of
anaphora can be important for downstream ap-
plications, we implemented a rule, again taken
from Kilicoglu and Bergler (2012), which simply
states that the antecedent of a relative pronominal
anaphora is the noun phrase head it modifies.
rel(X,Anaphor) ? rcmod(Antecedent,X) ?
COREF(Anaphor,Antecedent)
where rel indicates a relative dependency, and rc-
mod a relative clause modifier dependency. We
extended this in the current work to include the
following rules:
(6) (a) LEFT(Antecedent,Anaphor) ?
NO INT WORD(Antecedent,Anaphor)
? COREF(Anaphor,Antecedent)
(b) LEFT(Antecedent,Anaphor) ? rc-
mod(Antecedent,X)? LEFT(Anaphor,X)
? COREF(Anaphor,Antecedent)
where LEFT indicates that the first argument is
to the left of the second and NO INT WORD in-
dicates that the arguments have no intervening
words between them.
3.3 Drug ingredient/brand name synonymy
A specific, non-anaphoric type of coreference,
between drug ingredient name and drug?s brand
name, is commonly annotated in our dataset. An
example is provided below, where COREG CR is
the brand name for carvedilol.
(7) The concomitant administration of amio-
darone or other CYP2C9 inhibitors such as
fluconazole with COREG CR may enhance
the -blocking properties of carvedilol . . . .
To identify this type of coreference, we use se-
mantic information from UMLS Metathesaurus.
We stipulate that, to qualify as co-referents, both
terms under consideration should map to the same
UMLS concept (i.e., that they are considered syn-
onyms). If the terms are within the same sentence,
we further require that they are appositive.
3.3.1 Demonstrative pronouns
Anaphoric expressions of demonstrative pronoun
type generally have discourse-deictic use; in other
words, they often refer to events, propositions de-
scribed in prior discourse or even to the full sen-
tences or paragraphs, rather than concrete objects
or entities (Webber, 1988). This fact was implic-
itly exploited in consumer health questions, since
the coreference resolution focused on diseases
only, which are essentially processes. However,
in drug labels, discourse-deictic use of demonstra-
tives is much more overt. Consider the sentence
below, where the demonstrative This refers to the
event of increasing the exposure to lovastatin.
(8) Co-administration of lovastatin and SAMSCA
increases the exposure to lovastatin and . . . .
This is not a clinically relevant change.
To handle such cases, we blocked entity an-
tecedents (such as drugs) for demonstrative pro-
nouns and only allowed predicates (verbs, nomi-
nalizations) as candidate antecedents.
3.3.2 Pleonastic it
We recognized pleonastic instances of the pronoun
it to disqualify them as anaphoric expressions (for
instance, it in It may be necessary to . . . ). Gen-
erally, lexical patterns involving sequence of to-
kens are used to recognize such instances (e.g.,
(Segura-Bedmar et al., 2010). We used a simple
dependency-based rule that mimics these patterns,
given below.
nsubj*(X,it) ? DEP(X,Y) ? PLEONASTIC(it)
where nsubj* refers to nsubj or nsubjpass depen-
dencies and DEP is any dependency, where DEP
/? {infmod, ccomp, xcomp}.
3.3.3 Discourse-based constraints
Previously, we did not impose limits on how far
the co-referents could be from each other, since
the entire discourse was generally short and the
salient antecedent (often the topic of the question)
appeared early in discourse. This is often not the
49
case in drug labels, especially because often intri-
cate interactions between the drug of interest and
other medications are discussed. Therefore, we
limit the discourse window from which candidate
antecedents are identified. Generally, the search
space for the antecedents is limited to the current
sentence as well as the two preceding sentences
(Segura-Bedmar et al., 2010; Nguyen et al., 2012).
In our dataset, we found that 98% of antecedents
occurred within this discourse window and, thus,
use the same search space. We make an exception
for the cases in which the anaphoric expression ap-
pear in the first sentence of a paragraph and no
compatible antecedent is found in the same sen-
tence. In this case, the search space is expanded to
the entire preceding paragraph.
We also extended the system to include different
types of salience scoring methods. For drug labels,
we use linear distance between the co-referents (in
terms of surface elements) as the salience score;
the lower this score, the better candidate the an-
tecedent is. Additionally, we implemented syn-
tactic tree distance between the co-referents as a
potential salience measure, even though this type
of salience scoring did not have an effect on our
results on drug labels.
Finally, we block candidate antecedents that
are in a direct syntactic dependency with the
anaphoric expression, except when the anaphor is
reflexive (e.g., itself ).
3.4 Evaluation
To evaluate our approach, we used a baseline simi-
lar to that reported in Segura-Bedmar et al. (2010),
which consists of selecting the closest preceding
nominal phrase for the anaphoric expressions an-
notated in their corpus. These expressions in-
clude pronominal (personal, relative, demonstra-
tive, etc.) and nominal (definite, possessive,
etc.) anaphora. We compared our system to
this baseline using the unweighted average of F
1
-
measure over B-CUBED (Bagga and Baldwin,
1998), MUC (Vilain et al., 1995), and CEAF (Luo,
2005) metrics, the standard evaluation metrics for
coreference resolution. We used the scripts pro-
vided by i2b2 shared task organizers for this pur-
pose. Since coreference annotation was parsimo-
nious in our dataset, we also manually examined a
subset of the coreference relations extracted by the
system for precision. Additionally, we tested our
system on DrugNerAR corpus (Segura-Bedmar et
al., 2010), which similarly focuses on drug inter-
actions. We compared our results to theirs, us-
ing as evaluation metrics precision, recall, and F
1
-
measure, the metrics that were used in their evalu-
ation.
4 Results and Discussion
With the drug label dataset, we obtained the best
results without relative pronominal anaphora reso-
lution and drug ingredient/brand name synonymy
strategies (OPTIMAL) and with linear distance
as the salience measure. In this setting, using
gold entity annotations, we recognized 318 coref-
erence chains, 54 of which were annotated in the
corpus. The baseline identified 1415 coreference
chains, only 10 of which were annotated. The im-
provement provided by the system over the base-
line is clear; however, the low precision/recall/F
1
-
measure, given in Table 1, should be taken with
caution due to the sparse coreference annotation
in the dataset. To get a better sense of how well
our system performs, we also performed end-to-
end coreference resolution and manually assessed
a subset of the system output (22 randomly se-
lected drug labels with 249 coreference instances).
Of these 249, 181 were deemed correct, yielding a
precision of 0.73. The baseline method extracted
1439 instances, 56 of which were deemed cor-
rect, yielding a precision of 0.04. The precision
of our method is more in line with what has been
reported in the literature (Segura-Bedmar et al.,
2010; Nguyen et al., 2012). For i2b2-style eval-
uation using the unweighted average F
1
measure
over B-CUBED, MUC, and CEAF metrics, we
considered both exact and partial mention overlap.
These results, provided in Table 1, also indicate
that the system provides a clear improvement over
the baseline.
Metric Baseline OPTIMAL
With gold entity annotations
Unweighted F
1
Partial 0.55 0.77
Unweighted F
1
Exact 0.66 0.78
Precision 0.01 0.17
Recall 0.04 0.26
F
1
-measure 0.01 0.21
End-to-end coreference resolution
Precision 0.04 0.73
Table 1: Evaluation results on drug labels
50
We also assessed the effect of various resolution
strategies on results. These results are presented in
Table 2.
Strategy F
1
-measure
OPTIMAL 0.21
OPTIMAL - SIA 0.21
OPTIMAL - APPOS 0.15
OPTIMAL + DIBS 0.16 (0.39 recall)
Table 2: Effect of coreference strategies
Disregarding set/instance anaphora resolution
(SIA) does not appear to affect the results by
much; however, this is mostly due to the fact that
the ?instance? mentions are generally exemplifica-
tions of a particular drug class which also appear
in text. In the absence of set/instance anaphora
resolution, the system often defaults to these drug
class mentions, which were annotated more often
than not, unlike the ?instance? mentions. Take the
following example:
(9) Use of ZESTRIL with potassium-sparing
diuretics (e.g., spironolactone, eplerenone,
triamterene or amiloride) . . . may lead to sig-
nificant increases . . . if concomitant use of
these agents . . .
Without set-instance anaphora resolution, the sys-
tem links these agents to potassium-sparing di-
uretics, an annotated relation. With set-instance
anaphora resolution, the same expression is linked
to individual drug names (spironolactone, etc.) as
well as the the drug class, creating a number of
false positives, which, in effect, offsets the im-
provement provided by this strategy.
On the other hand, recognizing appositive con-
structions (APPOS) appears to have a larger im-
pact; however, it should be noted that this is mostly
because it helps us expand the antecedent mention
list in the case of set/instance anaphora. For in-
stance, in Example (9), this strategy allows us to
establish the link between the anaphora and the
drug class (diuretics), since the drug class and in-
dividual drug name (spironolactone) are identified
earlier as appositive. We can conclude that, in gen-
eral, set/instance anaphora benefits from recogni-
tion of appositive constructions.
Recognizing drug ingredient/brand name syn-
onymy (DIBS) improved the recall and hurt the
precision significantly, the overall effect being
negative. Since this non-anaphoric type of coref-
erence is strictly semantic in nature and resources
from which this type of semantic information can
be derived already exist (UMLS, among others), it
is perhaps not of utmost importance that a coref-
erence resolution system recognizes such corefer-
ence.
We additionally processed the DrugNerAR cor-
pus with our system. The optimal setting for
this corpus was disregarding the drug ingredi-
ent/brand name synonymy but using relative pro-
noun anaphora resolution, based on the discus-
sion in Segura-Bedmar et al. (2010). Somewhat to
our surprise, our system did not fare well on this
corpus. We extracted 524 chains, 327 of which
(out of 669) were annotated in the corpus, yield-
ing a precision of 0.71, recall of 0.56, and F
1
-
measure of 0.63. This is about 20% lower than
their reported results. When we used their base-
line method (explained earlier), we obtained simi-
larly lower scores (precision of 0.18, recall of 0.45,
F
1
-measure of 0.26, about 40% lower than their
reported results). In light of this apparent discrep-
ancy, which clearly warrants further investigation,
it is perhaps more sensible to focus on ?improve-
ment over baseline? (reported as 73% in their pa-
per and is 140% in our case).
We analyzed some of the annotations more
closely to get a better sense of the shortcomings
of the system. The majority of errors were due to
using linear distance as the salience score. For in-
stance, in the following example, they is linked to
ACE inhibitors due to proximity, whereas the true
antecedent is these reactions (itself an anaphor and
is presumably linked to another antecedent). It
could be possible to recover this link using prin-
ciples of Centering Theory (Grosz et al., 1995),
which suggests that subjects are more central than
objects and adjuncts in an utterance. Following
this principle, the subject (these reactions) would
be preferred to ACE inhibitors as the antecedent.
(10) In the same patients, these reactions were
avoided when ACE inhibitors were temporar-
ily withheld, but they reappeared upon inad-
vertent rechallenge.
Semantic (but not syntactic) coordination some-
times leads to number disagreement between the
anaphora and a true antecedent, as shown in Ex-
ample (11), leading to false negatives. In this ex-
ample, such diuretics refers to both ALDACTONE
51
and a second diuretic; however, we are unable to
identify the link between them and the number dis-
agreement between the anaphora and either of the
antecedents blocks a potential coreference relation
between these items.
(11) If, after five days, an adequate diuretic re-
sponse to ALDACTONE has not occurred,
a second diuretic that acts more proximally
in the renal tubule may be added to the reg-
imen. Because of the additive effect of AL-
DACTONE when administered concurrently
with such diuretics . . .
5 Conclusion
We presented a coreference resolution system en-
hanced based on insights from a dataset of FDA
drug package inserts. Sparse coreference annota-
tion in the dataset presented difficulties in evaluat-
ing the results; however, based on various eval-
uation strategies, the performance improvement
due to the enhancements seems evident. Our re-
sults show that recognizing coordination and ap-
positive constructions are particularly useful and
that non-anaphoric cases of coreference can be
identified using synonymy in semantic resources,
such as UMLS. However, whether this is a task
for a coreference resolution system or a concept
normalization system is debatable. We exper-
imented with using hierarchical domain knowl-
edge in UMLS (for example, the knowledge that
lisinopril ISA angiotensin converting enzyme in-
hibitor) to resolve some cases of sortal anaphora.
Even though we did not see an improvement due
to using this type of information on our dataset,
further work is needed to assess its usefulness.
While the enhancements were evaluated on drug
labels only, they are not specific to this type of
text. Their portability to different text types is
limited only by the accuracy of underlying tools,
such as parsers, for the text type of interest and
the availability of domain knowledge in the form
of relevant semantic types, groups, hypernyms
for the entity types under consideration. The re-
sults also indicate that a more rigorous application
of syntactic constraints in the spirit of Centering
Theory (Grosz et al., 1995) could be beneficial.
Event (or clausal) anaphora and anaphora indicat-
ing discourse deixis, while rarely annotated in our
dataset, appear to occur fairly often in biomedical
text. These types of anaphora are known to be par-
ticularly challenging, and we plan to investigate
them in future research, as well.
Acknowledgments
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health.
References
Alan R. Aronson and Franc?ois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17(3):229?236.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
James J. Cimino, Tiffani J. Bright, and Jianhua Li.
2007. Medication reconciliation using natural lan-
guage processing and controlled terminologies. In
Klaus A. Kuhn, James R. Warren, and Tze-Yun
Leong, editors, MedInfo, volume 129 of Studies in
Health Technology and Informatics, pages 679?683.
IOS Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Dina Demner-Fushman, Wendy W. Chapman, and
Clem J. McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of Biomedical Informatics, 5(42):760?762.
Jon Duke, Jeff Friedlin, and Patrick Ryan. 2011. A
quantitative analysis of adverse events and ?over-
warning? in drug labeling. Archives of internal
medicine, 10(171):944?946.
Peter L. Elkin, John S. Carter, Manasi Nabar, Mark
Tuttle, Michael Lincoln, and Steven H. Brown.
2011. Drug knowledge expressed as computable se-
mantic triples. Studies in health technology and in-
formatics, (166):38?47.
Kin Wah Fung, Chiang S. Jao, and Dina Demner-
Fushman. 2013. Extracting drug indication infor-
mation from structured product labels using natural
language processing. JAMIA, 20(3):482?488.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
52
Mar??a Herrero-Zazo, Isabel Segura-Bedmar, Paloma
Mart??nez, and Thierry Declerck. 2013. The DDI
corpus: An annotated corpus with pharmacological
substances and drug-drug interactions. Journal of
Biomedical Informatics, 46(5):914?920.
Halil Kilicoglu and Sabine Bergler. 2012. Biolog-
ical Event Composition. BMC Bioinformatics, 13
(Suppl 11):S7.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting consumer health ques-
tions: The role of anaphora and ellipsis. In Proceed-
ings of the 2013 Workshop on Biomedical Natural
Language Processing, pages 54?62.
Jin-Dong Kim, Ngan Nguyen, YueWang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia Event and Protein Coreference tasks of
the BioNLP Shared Task 2011. BMC Bioinformat-
ics, 13(Suppl 11):S1.
Qi Li, Louise Deleger, Todd Lingren, Haijun Zhai,
Megan Kaiser, Laura Stoutenborough, Anil G.
Jegga, Kevin B. Cohen, and Imre Solti. 2013. Min-
ing FDA drug labels for medical conditions. BMC
medical informatics and decision making, 13(1):53.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32:281?291.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In In Proc. of HLT/EMNLP,
pages 25?32.
Alexa T. McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS semantic types
for reducing conceptual complexity. Proceedings of
Medinfo, 10(pt 1):216?20.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Ngan L. T. Nguyen, Jin-Dong Kim, Makoto Miwa,
Takuya Matsuzaki, and Junichi Tsujii. 2012. Im-
proving protein coreference resolution by simple se-
mantic classification. BMC Bioinformatics, 13:304.
Philip V. Ogren. 2010. Improving Syntactic Coor-
dination Resolution using Language Modeling. In
NAACL (Student Research Workshop), pages 1?6.
The Association for Computational Linguistics.
T.I. Oprea, S.K. Nielsen, O. Ursu, J.J. Yang,
O. Taboureau, S.L. Mathias, L. Kouskoumvekaki,
L.A. Sklar, and C.G. Bologa. 2011. Associat-
ing Drugs, Targets and Clinical Outcomes into an
Integrated Network Affords a New Platform for
Computer-Aided Drug Repurposing. Molecular in-
formatics, 2-3(30):100?111.
Thomas C. Rindflesch, Lorrie Tanabe, John N. We-
instein, and Lawrence Hunter. 2000. EDGAR:
Extraction of drugs, genes, and relations from the
biomedical literature. In Proceedings of Pacific
Symposium on Biocomputing, pages 514?525.
Isabel Segura-Bedmar, Mario Crespo, C?esar de Pablo-
S?anchez, and Paloma Mart??nez. 2010. Resolving
anaphoras for the extraction of drug-drug interac-
tions in pharmacological documents. BMC Bioin-
formatics, 11 (Suppl 2):S1.
J.C. Smith, J.C. Denny, Q. Chen, H. Nian, A. 3rd
Spickard, S.T. Rosenbloom, and R. A. Miller. 2011.
Lessons learned from developing a drug evidence
base to support pharmacovigilance. Applied clinical
informatics, 4(4):596?617.
?
Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010.
Extracting medication information from clinical
text. JAMIA, 17(5):514?518.
?
Ozlem Uzuner, Andrea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R. South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. JAMIA,
19(5):786?791.
Marc B. Vilain, John D. Burger, John S. Aberdeen,
Dennis Connolly, and Lynette Hirschman. 1995.
A model-theoretic coreference scoring scheme. In
MUC, pages 45?52.
Bonnie L. Webber. 1988. Discourse Deixis: Reference
to Discourse Segments. In ACL, pages 113?122.
Rong Xu and QuanQiu Wang. 2014. Large-scale com-
bining signals from both biomedical literature and
the FDA Adverse Event Reporting System (FAERS)
to improve post-marketing drug safety signal detec-
tion. BMC Bioinformatics, 15:17.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference Based Event-Argument Relation Ex-
traction on Biomedical Text. Journal of Biomedical
Semantics, 2 (Suppl 5):S6.
Jiaping Zheng, Wendy W. Chapman, Rebecca S. Crow-
ley, and Guergana K. Savova. 2011. Corefer-
ence resolution: A review of general methodologies
and applications in the clinical domain. Journal of
Biomedical Informatics, 44(6):1113?1122.
Qian Zhu, Robert R. Freimuth, Jyotishman Pathak,
Matthew J. Durski, and Christopher G. Chute. 2013.
Disambiguation of PharmGKB drug-disease rela-
tions with NDF-RT and SPL. Journal of Biomedical
Informatics, 46(4):690?696.
53
