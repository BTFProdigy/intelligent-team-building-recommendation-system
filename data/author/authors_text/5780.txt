Automatic Construction of Japanese KATAKANA Variant List
from Large Corpus
Takeshi Masuyama
Information Technology Center
University of Tokyo
7-3-1, Hongo, Bunkyo
Tokyo 113-0023
Japan
tak@r.dl.itc.u-tokyo.ac.jp
Satoshi Sekine
Computer Science Department
New York University
715 Broadway, 7th floor
New York NY 10003
USA
sekine@cs.nyu.edu
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
7-3-1, Hongo, Bunkyo
Tokyo 113-0023
Japan
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
This paper presents a method to construct
Japanese KATAKANA variant list from
large corpus. Our method is useful for
information retrieval, information extrac-
tion, question answering, and so on, because
KATAKANA words tend to be used as
?loan words? and the transliteration causes
several variations of spelling. Our method
consists of three steps. At step 1, our sys-
tem collects KATAKANA words from large
corpus. At step 2, our system collects can-
didate pairs of KATAKANA variants from
the collected KATAKANA words using a
spelling similarity which is based on the edit
distance. At step 3, our system selects vari-
ant pairs from the candidate pairs using a
semantic similarity which is calculated by
a vector space model of a context of each
KATAKANA word. We conducted exper-
iments using 38 years of Japanese news-
paper articles and constructed Japanese
KATAKANA variant list with the perfor-
mance of 97.4% recall and 89.1% precision.
Estimating from this precision, our system
can extract 178,569 variant pairs from the
corpus.
1 Introduction
?Loan words? in Japanese are usually writ-
ten by a phonogram type of Japanese charac-
ter set, KATAKANA. Because of loan words,
the transliteration causes several variations of
spelling. Therefore, Japanese KATAKANA
words sometimes have several different or-
thographies for each original word. For exam-
ple, we found at least six different spellings of
?spaghetti? in 38 years of Japanese newspaper
articles, such as ???????,? ??????
??,? ???????,? ??????,? ????
???,? and ??????.? The different ex-
pression causes problems when we use search
engines, question answering systems, and so on
(Yamamoto et al, 2003). For example, when we
input ???????? as a query for a search en-
gine or a query for a question answering system,
we may not be able to find the web pages or the
answers for which we are looking, if a different
orthography for ???????? is used.
We investigated how many documents were
retrieved by Google 1 when each Japanese
KATAKANA variant of ?spaghetti? was used
as a query. The result is shown as Table 1.
For example, when we inputted ??????
?? as a query of Google, 104,000 documents
were retrieved and the percentage was 34.6%,
calculated by 104,000 divided by 300,556. From
Table 1, we see that each of six variants appears
frequently and thus we may not be able to find
the web pages for which we are looking.
Although we can manually create Japanese
KATAKANA variant list, it is a labor-intensive
task. In order to solve the problem, we propose
an automatic method to construct Japanese
KATAKANA variant list from large corpus.
Variant # of retrieved documents
?????? 104,000 (34.6%)
??????? 25,400 (8.5%)
?????? 1,570 (0.5%)
????? 131,000 (43.6%)
?????? 37,700 (12.5%)
????? 886 (0.3%)
Total 300,556 (100%)
Table 1: Number of retrieved documents when
we inputted each Japanese KATAKANA vari-
ant of ?spaghetti? as a query of Google.
Our method consists of three steps. First,
we collect Japanese KATAKANA words from
large corpus. Then, we collect candidate pairs of
KATAKANA variants based on a spelling simi-
larity from the collected Japanese KATAKANA
words. Finally, we select variant pairs using
1
http://www.google.co.jp/
a semantic similarity based on a vector space
model of a context of each KATAKANA word.
This paper is organized as follows. Section
2 describes related work. Section 3 presents
our method to construct Japanese KATAKANA
variant list from large corpus. Section 4 shows
some experimental results using 38 years of
Japanese newspaper articles, which we call ?the
Corpus? from now on, followed by evaluation
and discussion. Section 5 describes future work.
Section 6 offers some concluding remarks.
2 Related Work
There are some related work for the problems
with Japanese spelling variations. In (Shishi-
bori and Aoe, 1993), they have proposed a
method for generating Japanese KATAKANA
variants by using replacement rules, such as
? (be) ? ?? (ve) and ? (chi) ? ??(tsi).
Here, ??? represents ?substitution.? For ex-
ample, when we apply these rules to ?????
(Venezia),? three different spellings are gener-
ated as variants, such as ??????,? ????
??,? and ???????.?
Kubota et al have extracted Japanese
KATAKANA variants by first transforming
KATAKANA words to directed graphs based on
rewrite rules and by then checking whether the
directed graphs contain the same labeled path
or not (Kubota et al, 1993). A part of their
rewrite rules is shown in Table 2. For exam-
ple, when applying these rules to ??????
(Kuwait),? ?? a?c,? ?? b?c,? ??? d?c? are
generated as variants.
KATAKANA String ? Symbol
?? (we), ? (e) ? a
?? (we), ?? (ue) ? b
?? (twu), ? (to), ? (tsu) ? c
? (macron) ? ?
? (small e), ? (e) ? d
Table 2: A part of rewrite rules.
In (Shishibori and Aoe, 1993) and (Kubota
et al, 1993), they only paid attention to apply-
ing their replacement or rewrite rules to words
themselves and didn?t pay attention to their
contexts. Therefore, they wrongly decide that
?????? is a variant of ????.? Here, ??
???? represents ?wave? and ????? repre-
sents ?web.? In our method, we will decide if ?
????? and ????? convey the same mean-
ing or not using a semantic similarity based on
their contexts.
3 Construct Japanese KATAKANA
Variant List from Large Corpus
Our method consists of the following three
steps.
1. Collect Japanese KATAKANA words from
large corpus.
2. Collect candidate pairs of KATAKANA
variants from the collected KATAKANA
words using a spelling similarity.
3. Select variant pairs from the candidate
pairs based on a semantic similarity.
3.1 Collect KATAKANA Words from
Large Corpus
At the first step, we collected Japanese
KATAKANA words which consist of a
KATAKANA character, ? (bullet), ?
(macron-1), and ? (macron-2), which are
commonly used as a part of KATAKANA
words, using pattern matching. For example,
our system collects three KATAKANA words ?
???????????? (Ludwig Erhard-1),?
?? (Soviet),? ??????????????
(Ludwig Erhard-2),? ???? (Germany)?
from the following sentences. Note that two
mentions of ?Ludwig Erhard? have different
orthographies.
? ????????????????????
???????????(Defunct Ludwig
Erhard-1 is called ?Father of The Mirac-
ulous Economic Revival.?)
? ???????????????????
????????????????????
???????????????????
???????????????????
????????(If Soviet and East Eu-
ropean countries give up their controlling
concepts and pursue the economic deregu-
lation which Ludwig Erhard-2 of West Ger-
many did in 1948, they may achieve the
miraculous revival like West Germany.)
3.2 Spelling Similarity
At the second step, our system collects candi-
date pairs of two KATAKANA words, which
are similar in spelling, from the collected
KATAKANA words described in Section 3.1.
We used ?string penalty? to collect candidate
pairs. String penalty is based on the edit dis-
tance (Hall and DOWLING, 1980) which is a
similarity measure between two strings. We
used the following three types of operations.
? Substitution
Replace a character with another charac-
ter.
? Deletion
Delete a character.
? Insertion
Insert a character.
We also added some scoring heuristics to the
operations based on a pronunciation similarity
between characters. The rules are tuned by
hand using randomly selected training data.
Some examples are shown in Table 3. Here,
??? represents ?substitution? and lines with-
out ? represent ?deletion? or ?insertion.? Note
that ?Penalty? represents a score of the string
penalty from now on.
For example, we give penalty 1 between ??
????? and ??????,? because the strings
become the same when we replace ??? with ?
?? and its penalty is 1 as shown in Table 3.
Rules Penalty
? (a) ? ? (small a) 1
? (zi) ? ? (di) 1
? (macron) 1
? (ha) ? ? (ba) 2
? (u) ? ? (vu) 2
? (a) ? ? (ya) 3
? (tsu) ? ? (small tsu) 3
Table 3: A part of our string penalty rules.
We analyzed hundreds of candidate pairs
of training data and figured out that most
KATAKANA variations occur when the string
penalties were less than a certain threshold. In
this paper, we set 4 for the threshold and regard
KATAKANA pairs as candidate pairs when the
string penalties are less than 4. The thresh-
old was tuned by hand using randomly selected
training data.
For example, from the collected KATAKANA
words described in Section 3.1, our system col-
lects the pair of ???????????? and
?????????????, since the string
penalty is 3.
3.3 Context Similarity
At the final step, our system selects variant
pairs from the candidate pairs described in Sec-
tion 3.2 based on a semantic similarity. We used
a vector space model as a semantic similarity.
In the vector space model, we treated 10 ran-
domly selected articles from the Corpus as a
context of each KATAKANA word.
We divided sentences of the articles into
words using JUMAN2 (Kurohashi and Nagao,
1999) which is the Japanese morphological an-
alyzer, and then extracted content words which
consist of nouns, verbs, adjectives, adverbs, and
unknown words except stopwords. Stopwords
are composed of Japanese HIRAGANA charac-
ters, punctuations, numerals, common words,
and so on.
We used a cosine measure to calculate a se-
mantic similarity of two KATAKANA words.
Suppose that one KATAKANA word makes a
context vector a and the other one makes b.
The semantic similarity between two vectors a
and b is calculated as follows.
sim(a,b) = cos? = a ? b
|a||b|
(1)
The cosine measure tends to overscore fre-
quently appeared words. Therefore, in order to
avoid the problem, we treated log(N + 1) as a
score of a word appeared in a context. Here, N
represents the frequency of a word in a context.
We set 0.05 for the threshold of the seman-
tic similarity, i.e. we regard candidate pairs as
variant pairs when the semantic similarities are
more than 0.05. The threshold was tuned by
hand using randomly selected training data.
In the case of ????????????? (Lud-
wig Erhard-1)? and ?????????????
? (Ludwig Erhard-2)?, the semantic similarity
becomes 0.17 as shown in Table 4. Therefore,
we regard them as a variant pair.
Note that in Table 4, a decimal number rep-
resents a score of a word appeared in a context
calculated by log(N+1). For example, the score
of ?? (miracle) in the first context is 0.7.
4 Experiments
4.1 Data Preprocessing and
Performance Measures
We conducted the experiments using the Cor-
pus. The number of documents in the Cor-
2
http://www.kc.t.u-tokyo.ac.jp/nl-
resource/juman.html
Word ????????????
?? (miracle):0.7
?? (economy):1.9
Context ? (father):0.7
?? (revival):0.7
? ? ?
Word ?????????????
?? (miracle):1.1
??? (liberalization):1.4
Context ?? (economy):2.4
?? (revival):1.1
? ? ?
Similarity 0.17
Table 4: Semantic similarity between ????
?????????? and ???????????
???.?
pus was 4,678,040 and the distinct number
of KATAKANA words in the Corpus was
1,102,108.
As for a test set, we collected candidate
pairs whose string penalties range from 1 to
12. The number of collected candidate pairs
was 2,590,240. In order to create sample correct
KATAKANA variant data, 500 out of 2,590,240
were randomly selected and we evaluated them
manually by checking their contexts. Through
the evaluation, we found that no correct vari-
ant pairs appeared from 10 to 12. Thus, we
think that treating candidate pairs whose string
penalties range from 1 to 12 can cover almost
all of correct variant pairs.
To evaluate our method, we used recall (Re),
precision (Pr), and F measure (F ). These per-
formance measures are calculated by the follow-
ing formulas:
Re = number of pairs found and correcttotal number of pairs correct ,
Pr = number of pairs found and correcttotal number of pairs found ,
F = 2RePrRe + Pr .
4.2 Experiment-1
We conducted the first experiment based on
two settings; one method uses only the spelling
similarity and the other method uses both the
spelling similarity and the semantic similarity.
Henceforth, we use ?Method
p
,?
?Method
p&s
,? ?Ext,? and ?Cor? as the
following meanings.
Method
p
: The method using only the spelling
similarity
Method
p&s
: The method using both the
spelling similarity and the semantic
similarity
Ext: The number of extracted candidate pairs
Cor: The number of correct variant pairs among
the extracted candidate pairs
Note that in Method
p&s
, we ignored candi-
date pairs whose string penalties ranged from
4 to 12, since we set 4 for the threshold of the
string penalty as described in Section 3.2.
The result is shown in Table 5. For example,
when the penalty was 2, 81 out of 117 were se-
lected as correct variant pairs in Method
p
and
the precision was 69.2%. Also, 80 out of 98 were
selected as correct variant pairs in Method
p&s
and the precision was 81.6%.
As for Penalty 1-12 of Method
p
, i.e. we fo-
cused on the string penalties between 1 and 12,
the recall was 100%, because we regarded 269
out of 500 as correct variant pairs and Method
p
extracted all of them. Also, the precision was
53.8%, calculated by 269 divided by 500. Com-
paring Method
p&s
to Method
p
, the recall and
the precision of Method
p&s
were well-balanced,
since the recall was 97.4% and the precision was
89.1%.
In the same way, for Penalty 1-3, i.e. the
string penalties between 1 and 3, the recall of
Method
p
was 98.1%, since five correct variant
pairs between 4 and 12 were ignored and the
remaining 264 out of 269 were found. The preci-
sion of Method
p
was 77.2%. It was 23.4% higher
than the one of Penalty 1-12. Thus, F measure
also improved 16.4%. This result indicates that
setting 4 for the threshold works well to improve
overall performance.
Now, comparing Method
p&s
to Method
p
when the string penalties ranged from 1 to 3, the
recall of Method
p&s
was 0.7% lower. This was
because Method
p&s
couldn?t select two correct
variant pairs when the penalties were 1 and 2.
However, the precision of Method
p&s
was 16.2%
higher. Thus, F measure of Method
p&s
im-
proved 6.7% compared to the one of Method
p
.
From this result, we think that taking the se-
mantic similarity into account is a better strat-
egy to construct Japanese KATAKANA variant
list.
Penalty Method
p
Method
p&s
Cor/Ext (%) Cor/Ext (%)
1 130/134 (97.0) 129/129 (100)
2 81/117 (69.2) 80/98 (81.6)
3 53/91 (58.2) 53/67 (79.1)
4 2/14 (14.3)
5 0/30 (0.0)
6 1/14 (7.1)
7 1/20 (5.0)
8 0/14 (0.0)
9 1/12 (8.3)
10 0/16 (0.0)
11 0/17 (0.0)
12 0/21 (0.0)
Re 264/269 (98.1) 262/269 (97.4)
1-3 Pr 264/342 (77.2) 262/294 (89.1)
F 86.4% 93.1%
Re 269/269 (100)
1-12 Pr 269/500 (53.8)
F 70.0%
Table 5: Comparison of Method
p
and
Method
p&s
.
4.3 Experiment-2
We investigated how many variant pairs were
extracted in the case of six different spellings
of ?spaghetti? described in Section 1. Table 6
shows the result of all combination pairs when
we applied Method
p&s
.
For example, when the penalty was 1,
Method
p&s
selected seven candidate pairs and
all of them were correct. Thus, the recall was
100%. From Table 6, we see that the string
penalties of all combination pairs ranged from
1 to 3 and our system selected all of them by
the semantic similarity.
Penalty Method
p&s
1 7/7 (100%)
2 6/6 (100%)
3 2/2 (100%)
Total 15/15 (100%)
Table 6: A result of six different spellings of
?spaghetti? described in Section 1.
4.4 Estimation of expected correct
variant pairs
We estimated how many correct variant pairs
could be selected from the Corpus based on the
precision of Method
p&s
as shown in Table 5.
The result is shown in Table 7. We find that
the number of candidate pairs in the Corpus
was 100,746 for the penalty of 1, and 56,569 for
the penalty of 2, and 40,004 for the penalty of
3.
For example, when the penalty was 2, we esti-
mate that 46,178 out of 56,569 could be selected
as correct variant pairs, since the precision was
81.6% as shown in Table 5. In total, we estimate
that 178,569 out of 197,319 could be selected as
correct variant pairs from the Corpus.
Penalty # of expected variant pairs
1 100,746/100,746 (100%)
2 46,178/56,569 (81.6%)
3 31,645/40,004 (79.1%)
Total 178,569/197,319 (90.5%)
Table 7: Estimation of expected correct variant
pairs.
4.5 Error Analysis-1
As shown in Table 5, our system couldn?t select
two correct variant pairs using semantic sim-
ilarity when the penalties were 1 and 2. We
investigated the reason from the training data.
The problem was caused because the contexts
of the pairs were diffrent. For example, in the
case of ????????? and ????????
?,? which represent the same building material
company ?Aroc Sanwa? of Fukui prefecture in
Japan, their contexts were completely different
because of the following reason.
? ???????, ????????
??????? (Aroc Sanwa): This
word appeared with the name of
an athlete who took part in the
national athletic meet held in Toyama
prefecture in Japan, and the company
sponsored the athlete.
???????? (Aroc?Sanwa): This
word was used to introduce the
company in the article.
Note that each context of these words was
composed of only one article.
4.6 Error Analysis-2
From Table 5, we see that the numbers of incor-
rect variant pairs selected by Method
p&s
were
18 and 14 for each penalty of 2 and 3. We in-
vestigated such cases in the training data. The
example of ???? (Cart, Kart)? and ????
(Card)? is shown as follows.
? ???, ???
??? (Cart, Kart): This word was
used as the abbreviation of ?Shopping
Cart,? ?Racing Kart,? or ?Sport
Kart.?
??? (Card): This word was used as
the abbreviation of ?Credit Card? or
?Cash Card? and was also used as the
meaning of ?Schedule of Games.?
Although these were not a variant pair, our
system regarded the pair as the variant pair,
because their contexts were similar. In both
contexts, ??? (utilization),? ??? (record),?
?? (guest),? ???? (aim),? ???? (team),?
??? (victory),? ??? (high, expensive),? ??
? (success),? ??? (entry),? and so on were
appeared frequently and therefore the semantic
similarity became high.
5 Future Work
In this paper, we have used newspaper articles
to construct Japanese KATAKANA variant list.
We are planning to apply our method on differ-
ent types of corpus, such as patent documents
and Web data. We think that more variations
can be found from Web data. In the case of
?spaghetti? described in Section 1, we found at
least seven more different spellings, such as ?
??????,? ????????,? ??????
?,? ????????? ??????,? ?????
??,? and ????????.?
Although we have manually tuned scoring
rules of the string penalty using training data,
we are planning to introduce an automatic
method for learning the rules.
We will also have to consider other charac-
ter types of Japanese, i.e. KANJI variations
and HIRAGANA variations, though we have fo-
cused on only KATAKANA variations in this
paper. For example, both ????? and ???
??? mean ?move? in Japanese.
6 Conclusion
We have described the method to construct
Japanese KATAKANA variant list from large
corpus. Unlike the previous work, we focused
not only on the similarity in spelling but also
on the semantic similarity.
From the experiments, we found that
Method
p&s
performs better than Method
p
,
since it constructed Japanese KATAKANA
variant list with high performance of 97.4% re-
call and 89.1% precision.
Estimating from the precision, we found that
178,569 out of 197,319 could be selected as cor-
rect variant pairs from the Corpus. The result
could be helpful to solve the variant problems
of information retrieval, information extraction,
question answering, and so on.
References
Patrick A. V. Hall and GEOFF R. DOWLING.
1980. Approximate string matching. Com-
puting Surveys, 12(4):381?402.
Jun?ichi Kubota, Yukie Shoda, Masahiro
Kawai, Hirofumi Tamagawa, and Ryoichi
Sugimura. 1993. A method of detecting
KATAKANA variants in a document. IPSJ
NL97-16, pages 111?117.
Sadao Kurohashi and Makoto Nagao. 1999.
Japanese morphological analysis system JU-
MAN version 3.61. Department of Informat-
ics, Kyoto University.
Masami Shishibori and Jun?ichi Aoe. 1993. A
method for generation and normalization of
katakana variant notations. IPSJ NL94-5,
pages 33?40.
Eiko Yamamoto, Yoshiyuki Takeda, and Kyoji
Umemura. 2003. An IR similarity measure
which is tolerant for morphological variation.
Natural Language Processing, 10(1):63?80.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 345?352
Manchester, August 2008
Modeling Chinese Documents 
 with Topical Word-Character Models  
Wei Hu1           Nobuyuki Shimizu2 
1Department of Computer Science 
Shanghai Jiao Tong University 
Shanghai, China 200240 
{no_bit,hysheng} 
@sjtu.edu.cn 
Hiroshi Nakagawa2           Huanye Sheng1 
2Information Technology Center 
The University of Tokyo 
Tokyo, Japan 113-0033 
{shimizu, nakagawa} 
@r.dl.itc.u-tokyo.ac.jp 
Abstract 
As Chinese text is written without word 
boundaries, effectively recognizing Chi-
nese words is like recognizing colloca-
tions in English, substituting characters 
for words and words for collocations. 
However, existing topical models that in-
volve collocations have a common limi-
tation. Instead of directly assigning a top-
ic to a collocation, they take the topic of 
a word within the collocation as the topic 
of the whole collocation. This is unsatis-
factory for topical modeling of Chinese 
documents. Thus, we propose a topical 
word-character model (TWC), which al-
lows two distinct types of topics: word 
topic and character topic. We evaluated 
TWC both qualitatively and 
quantitatively to show that it is a power-
ful and a promising topic model.  
1 Introduction 
Topic models (Blei et al, 2003; Griffiths & 
Steyvers 2004, 2007) are a class of statistical 
models in which documents are expressed as 
mixtures of topics, where a topic is a probability 
distribution over words. A topic model is a gen-
erative model for documents: it specifies a prob-
abilistic procedure for generating documents. To 
make a new document, we choose a distribution 
over topics. Then, for each word in this docu-
ment, we randomly select a topic from the distri-
bution, and draw a word from the topic. Once we 
have a topic model, we can invert the generating 
process, inferring the set of topics that was re-
sponsible  for  generating  a  collection  of  docu- 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
ments.  
Although most topic models treat a document 
as a bag-of-words, the assumption has obvious 
shortcomings. Suppose there are many docu-
ments about art and musicals in New York. Then, 
we find a topic represented by words such as 
?art?, ?musical?, and ?New?, instead of getting 
?New York?. The bag-of-words assumption 
makes the topic model split a collocation?a 
phrase with meaning beyond the individual 
words?into individual words that have a differ-
ent meaning. One example of a collocation is the 
phrase ?white house?. In politics, it carries a spe-
cial meaning beyond a house that is white, 
whereas ?yellow house? does not.  
While it is reasonable for English, the equiva-
lent bag-of-characters assumption is especially 
troublesome for modeling Chinese documents, 
where almost all basic vocabularies are the equi-
valents of English collocations. In Chinese, some 
of the most commonly used couple-of-thousand 
characters are combined to make up a word, and 
no word boundary is given in the text. 
Effectively, Chinese words are like collocations 
in English. The difficulty is that there are over-
whelmingly more of them, enough to render a 
bag-of-character assumption unreasonable for 
Chinese. Therefore, a topical model for Chinese 
should be capable of detecting the boundary be-
tween two words, as well as assigning a topic to 
each word. 
While topic models for Chinese documents 
bear some similarity to collocation models in 
English, existing topical collocation discovery 
models, such as the LDA (latent Dirichelt alloca-
tion) Collocation model (LDACOL) (Griffiths et 
al., 2007) and the topical N-gram model (TNG) 
(Wang et al, 2007), do not directly assign a topic 
to a collocation. These models find the bounda-
ries of phrases and assign a topic to each word. 
The problem is in the next step?the topic of the 
collocation is exactly the same as one of the 
words. This is like saying that the topic of ?white 
345
house? is the same as either that of ?white? or 
?house?. We propose a new topical model, the 
topical word-character model (TWC), which 
aims to overcome these limitations.  
We evaluated the model both quantitatively 
and qualitatively. For the quantitative analysis, 
we compared the performance of TWC and TNG 
using a standard measure perplexity. For the 
qualitative analysis, we evaluated TWC?s ability 
to discover Chinese words and assign topics in 
comparison with TNG. 
The rest of the paper is organized as follows. 
Section 2 reviews topic models that aim to in-
clude collocations explicitly in the model and 
analyzes their limitations. Section 3 presents our 
new model TWC. Section 4 gives details of our 
consideration on inference for TWC. Section 5 
presents our qualitative and quantitative experi-
ments. Section 6 concludes with a summary and 
briefly mentions future work.  
2 Topic Models for Collocation Discov-
ery 
Since Chinese word discovery is similar to 
English collocation discovery, we first review 
some related topic models for collocation 
discovery. 
Although collocation discovery has long been 
studied, most methods are based on frequency or 
variance. LDACOL is an attempt to model collo-
cations in a topical scheme. Starting from the 
LDA topic model, LDACOL introduces special 
random variables xG . Variable xi = 1 implies that 
the  corresponding  word  wi  and  previous  word 
wi-1 belong to the same phrase, while xi = 0 im-
plies otherwise. Thus, LDACOL can decide the 
length of a phrase dynamically.  
TNG is a powerful generalization of LDACOL. 
Its graphical model is shown in Figure 1. 
 
Figure 1: Topical n-gram model. 
The model is defined in terms of three sets of  
variables: a sequence of words w
G
, a sequence of 
topics z
G
, and a sequence of indicators x
G
. TNG 
assumes the following generative process for 
documents. 
1. For each document d, draw ?d ~ 
Dirichlet(?). 
2. For each topic z, draw ?z ~ Dirichlet(?). 
3. For each topic z and each word w, draw ?zw 
~ Dirichlet(?).  
4. For each topic z and each word w, draw ?zw 
~ Beta(?). 
5. For each word wd,,i in document d: 
(a) draw xd,,i ~ Bernoulli(
d ,i 1 d ,i 1z ,w
? ? ? ), 
(b) draw zd,,i ~ Discrete(?d), 
(c) draw wd,,i ~ Discrete(
d ,iz
? )  if xd,,i=0, 
 draw wd,,i ~ Discrete(
d ,i d ,i 1z ,w
? ? )  if xd,,i=1, 
where ?, ?, ? are Dirichlet priors and ? is a Beta 
prior, zd,i denotes the ith topic assignment in 
document d, wd,i denotes the ith word in document 
d, and xd,i denotes the indicator between wd,i-1 and 
wd,i. Note that the variable xd,i = 1 implies that 
word wd,i-1 and its neighbor wd,i belong to the 
same phrase, while xd,i = 0 implies otherwise. 
However, the topics assigned to them (zd,i-1 and 
zd,i) are not required to be identical to each other. 
To decide the topic of a phrase, we can simply 
take the first (or last) word?s topic or the most 
common topic in the phrase. The authors of TNG 
prefer to choose the last word?s topic as the 
phrase topic because the last noun in a colloca-
tion is usually the ?head noun? in English.  
However, this simple strategy may be ineffec-
tive when we apply TNG to Chinese documents. 
The topics of ???? (game) and ????? 
(tournament) should be represented by their last 
characters while those of ???? (farmer) and 
???? (agriculture) should be represented by 
their first characters. And occasionally, the topic 
of a Chinese word is not identical to any topic of 
its component characters. For example ???? 
(Bluetooth) is neither a color nor a tooth.  
To overcome the limitation of TNG, we must 
discard its underlying assumption: that the topic 
of a whole word is the same as the topic of at 
least one of its components.  
3 Modeling Word Topic and Character 
Topic 
This section describes our topical word-character 
model (TWC), which models two distinct types 
of topics: word topic and character topic. 
 
? ?? ? 
    
? ? 
?? 
    
??? 
??? 
??? 
???
???
???
zi+1 zi+2 zi zi-1 
xi+2 xi+3xi-1xi xi-1 
  wi+1 wi+2 wi wi-1 
346
3.1 Word topic and character topic 
To solve the problem associated with the ??
?? (Bluetooth) example, we need to distinguish 
between the topics of characters and words. 
Therefore, we introduce a new type of topic for 
words in addition to the topics assigned to char-
acters. When generating a Chinese character, we 
first draw a word topic and then choose a charac-
ter topic. A schematic description of this model 
is shown in Figure 2.  
 
Figure 2: Schematic description of modeling 
Chinese documents with character and word top-
ics.  
Here, we use random variables z
G
 and t
G
 to 
denote word and character topics, respectively. 
Note that the word topic and character topics 
have a hierarchical tree-like structure (upper 
layer in Figure 2), whereas character topics and 
characters form a hidden Markov model (HMM) 
(lower layer in Figure 2). 
3.2 Topical word-character model (TWC)  
There are some indicators in the upper-right 
corner of each character topic in Figure 2. They 
help us to tell whether the current character be-
longs to the same word as the previous one. Now 
the question left is how to probabilistically draw 
these indicators, i.e., how to determine the length 
of the Markov chain. 
There are two ways to set the values of the in-
dicators. One is similar to that applied in the hid-
den semi-Markov model (HSMM), which gener-
ates the duration of a segment from the state. Ac-
cordingly, we could first choose the length of a 
word from the distribution associated with the 
word topic and then assign 0 or 1 to each indica-
tor. The other method is to directly draw indica-
tors from the distribution associated with the 
previous character and topic, just as LDACOL 
and TNG do. The difference between these two 
methods is that the former determines the length 
of a word in advance while the latter increases 
the length dynamically.  
We  prefer the  second choice  because it takes 
into consideration a lot of context information. In 
fact, our experimental results indicate that it has 
better performance. 
The formal definition of our model with word 
and character topics is as follows.  
 
Figure 3: Topical word-character model.  
TWC has four sets of variables: a sequence of 
characters c
G
, a sequence of character topics t
G
, a 
sequence of word topics z
G
, and a sequence of 
indicators x
G
. A document is generated via the 
following procedure.  
1. For each document d, draw ?d ~ 
Dirichlet(?);  
2. For each word topic z, draw ?z ~ 
Dirichlet(?); 
3. For each word topic z and each character 
topic t, draw ?zt ~ Dirichlet(?);  
4. For each word topic z, each character topic t 
and each character c, draw ?ztc ~ Beta(?); 
5. For each character topic t, draw ?t ~ 
Dirichlet(?); 
6. For each character cd,,i in document d: 
(a) draw xd,,i ~ Bernoulli( ? ? ?d ,i 1 d ,i 1 d ,i 1z ,t ,c? ); 
(b) draw zd,,i ~ Discrete (?d)  if xd,,i=0; 
     zd,,i= zd,,i-1    if xd,,i=1; 
(c) draw td,,i ~ Discrete(
d ,iz
? ) if xd,,i=0; 
    draw td,,i ~ Discrete( ?d ,i d ,i 1z ,t? )  if xd,,i=1; 
(d) draw cd,,i ~ Discrete(
d ,it
? ). 
Here, ?, ?, ?, ? are Dirichlet priors and ? is a 
Beta prior, zd,i denotes the ith word topic assignment 
in document d, td,i denotes the ith character topic as-
signment in document d, cd,i denotes the ith character 
in document d, and xd,i denotes the indicator between 
cd,i-1 and cd,i. 
Note that compared with the schematic model 
in Figure 2, each character has its corresponding 
Character
 topic
Word
 topic
 
? 
 
   
z2 z1 
t13 t21 t12 t11 
c13 c21 c12 c11 
 
 
t22 
c22 
1 1 0 1 0 
? ?? ?
 
  
??
?? ? ? 
  
??? 
??? 
??? 
??? 
??? 
???
??? 
??? 
  
zi+1 zi+2 zizi-1
ti+1 ti+2 titi-1
ci+1 ci+2 cici-1
xi+2 xi+3 xi-1xixi-1
347
word topic in the TWC model. This is because 
we cannot decide how many words there will be 
in a document and how many characters there 
will be in a certain word in advance. In other 
words, the structure of the ideal model is not 
fixed. Therefore, we duplicate word topic vari-
ables for each character. 
4 Inference with TWC 
Many approximate inference techniques such as 
variational methods, expectation propagation, 
and Gibbs sampling can be applied to graphical 
models. We use Gibbs sampling to perform our 
Bayesian inference in TWC.  
Gibbs sampling is a simple and widely appli-
cable Markov chain Monte Carlo (MCMC) algo-
rithm. In a traditional procedure, variables are 
sequentially sampled from their distributions 
conditioned on all other variables in the model. 
An extension of the basic approach is to 
choose blocks of variables first and then sample 
jointly from the variables in each block in turn, 
conditioned on the remaining variables; this is 
called blocking Gibbs sampling.  
When sampling for TWC, we separate vari-
ables into three types of blocks in the following 
manner (as shown in Figure 4). 
1. character variables ti 
2. indicators xi, whose value is 1 after n itera-
tions 
3. word topics zi, zi+1, ?, zi+l-1 and indicator xi, 
satisfying xi=xi+l=1 and xj=0 (j from i to i+l-1) 
after n iterations 
 
Figure 4: Illustration of partition in a certain it-
eration.  
Note that variables , , ,? ? ? ?
G G GG
 and ?
G
are not 
sampled. This is because we can integrate them 
out according to their conjugate priors. We only 
need to sample variables z
G
, x
G
, and t
G
. 
Before discussing the inference of conditional 
probabilities, let us analyze our partition strategy 
in detail. We will explain the reasons for (1) 
sampling zi, zi+1, ?, zi+l-1 together and (2) sam-
pling z
G
 and xi together 
1. Why do we sample zi, zi+1, ?, zi+l-1 together? 
Assume that we draw zi, zi+1, ?, zi+l-1 one by 
one, and it is now time to sample zi+1 according 
to the conditional probability 
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ?+ ? += GG GG , 
where ( )i 1z? +
G
denotes a word topic except i 1z + . 
Recall step 6-b in the generative TWC model: it 
says ?If xd,,i=1, then zd,,i= zd,,i-1?, which implies  
( | , ) ( )i 1 i i 1 i 1 iP z z x 1 I z z+ + += = = . 
As this probability is a factorial of the target 
probability, it follows that   
( )( | , , , , , , , , )i 1 i 1P z j z x t c ? ? ? ? ? 0+ ? += =GG GG   
for all ij z? . In other words, zi+1 should be equal 
to zi and not change during sampling.  
It seems that step 6-b in the generative model 
causes the problem. But supposing that we do not 
set zi+1 to zi; it is still more reasonable to sample 
z
G
 together. According to our partition principle, 
xi, xi+1, ?, xi+l-1, xi+l is a continuous indicator 
sequence whose head and tail are both 0 and the 
rest are 1, which implies that character string ci, 
ci+1, ?, ci+l-1 forms a word and has the same 
word topic. Recall the schematic model in Figure 
2: the word topic and character topics have a 
tree-like structure and each word has only one 
word topic node. We add some auxiliary dupli-
cates just because the ideal model is not fixed. 
Therefore, it is natural to sample the word topic 
together with its duplicates.  
2. Why do we sample z
G
 and xi together? 
Let us consider the probability of converting xi 
from 0 to 1 in the current sampling iteration. As-
sume that the number of word topics is 3, zi-1=2, 
and 
( ... | ) / ( ) ,
( | ... ) / ( ) ,
i i l 1 i
i i i l 1
P z z j x 0 1 3 1 j 3
P x k z z 2 1 2 0 k 1
+ ?
+ ?
= = = = = ? ?
= = = = = ? ?
where other variables and priors are omitted. If 
we first sample z
G
 and next sample xi, then the 
probability of drawing 1 for xi is 1/6, according 
to the multiplication principle. If we sample z
G
 
and xi together, the probability of drawing 1 for xi 
is ( ... , )i i l 1 iP z z 2 x 1+ ?= = = = . 
Since 
( ... , )
( ... , )
( ... , ) ( )
1 3
i i l 1 i
k 0 j 1
3
i i l 1 i
j 1
i i l 1 i i 1
1 P z z j x k
P z z j x 0
z z 2 x 1 z 2
+ ?
= =
+ ?
=
+ ? ?
=
==
= = = = =
= = = = =
+ = = = = =
??
?  
and  
( ... , )i i l 1 iP z z 2 x 1+ ?= = = =  
  
1 0 1 1 0
    
???
???
???ti ti+2 ti-1 ti-2 
??? 
zi zi+1 zi-1 zi-2 zi+2 
ti+2 
??? 
???  
0 
 
348
( ... , )
( ... , ) ( )
i i l 1 i
i i l 1 i
P z z 2 x 0
P z z j x 0 1 j 3
+ ?
+ ?
= = = = =
= = = = = ? ? , 
we get  
( ... , ) / /i i l 1 iP z z 2 x 1 1 4 1 6+ ?= = = = = > . 
In conclusion, the model is more likely to form 
long words, if we sample z
G
and xi together. This 
is preferred because both TNG and TWC tend to 
produce shorter words than we would like.  
For each type of block, we need to work out 
the corresponding conditional probability. 
, ,
, ,
, , , ,
, ( : ) ,
( | , , , , , , , , )
( | , , , , , , , , )
( ,
| , , , , , , , , )
d i d i
d i d i
d i d i 1 d i l 1 d i
d i i l 1 d i
P t s z x t c ? ? ? ? ?
P x k z x t c ? ? ? ? ?
P z z z j x k
z x t c ? ? ? ? ?P
?
?
+ + ?
? + ? ?
=
=
= = = = =
GG GG
GG GG
"
GG GG
 
where ,d it ?
G
 denotes the character topic assign-
ments except td,i, ,d ix ?
G
 denotes the indicators ex-
cept xd,i, and , ( : )d i i l 1z ? + ?
G
denotes the word topic 
assignments except ,d jz
G
 (j from i to i+l-1). De-
tails of the derivation of these conditional prob-
abilities are provided in Appendix A.1. 
5 Experiments 
In this section, we discuss our evaluation of 
TWC in Chinese document modeling and Chi-
nese word and topic discovery.  
5.1 Modeling documents 
To evaluate the generalization performance of 
our model, we trained both TWC and TNG on a 
Chinese corpus and computed the perplexity of 
the held-out test set. Perplexity, which indicates 
the uncertainty in predicting a single character, is 
a standard measure of performance for statistical 
models of natural language. A lower perplexity 
score indicates better generalization performance. 
Formally, the perplexities for TWC and TNG 
are defined as follows. 
( )
? ? ? ??log ( | , , , , )
exp{ }
TWC test
D
d
d 1
D
d
d 1
perplexity D
p c ? ? ? ? ?
N
=
=
= ?
?
?
GG G G GG
, 
where Dtest is the testing data, D is the number of 
documents in Dtest, Nd is the number of characters 
in document d, ,d? z? is simply set to 1/Z (Z is 
number of word topics), and ? ? ??, , ,? ? ? ?
G G GG
are poste-
rior estimates provided by applying TWC to 
training data. Details of the parameter estimation 
for TWC are provided in Appendix A.2. 
( )TNG testperplexity D  
? ? ??log ( | , , , )
exp{ }
D
d
d 1
D
d
d 1
p c ? ? ? ?
N
=
=
= ?
?
?
GG G GG
, 
where Dtest, D, and Nd are the same as defined for 
the TWC perplexity, ,d? z? is simply set to 1/T (T is 
number of topics), and ? ??, ,? ? ?
G GG
 are posterior esti-
mates provided by applying TNG to training data.  
Now, the remaining question is how to work 
out the likelihood function in the definition of 
perplexity. The likelihood function can be ob-
tained by marginalizing latent variables, but the 
time complexity is exponential. Therefore, we 
propose an efficient method of computing the 
likelihood that is similar to the forward algo-
rithm for an HMM. Details of the forward ap-
proach to computing likelihood for TWC and 
TNG are provided in Appendix B.  
In our experiments, we used a subset of Chi-
nese corpus LDC2005T14. The dataset contains 
6000 documents with 4476 unique characters and 
2,454,616 characters. We evaluated both TWC 
and TNG using 10-fold cross validation. In each 
experiment, both models ran for 500 iterations on 
90% of the data and computed the complexity for 
the remaining 10% of the data. 
TWC used fixed Dirichlet (Beta) priors ?=1, 
?=1, ?=1, ?=0.1 and ?=0.01 while TNG used 
?=1, ?=0.01, ?=0.01, and ?=0.1. 
0
50
100
150
200
250
0 20 40 60 80 100
          No. of topics (TNG)
No. of charcter topics *  no. of word topics
Pe
rp
le
xi
ty
TNG TWC
Figure 5: Perplexity results with LDC2005T14 
corpora for TNG and TWC. 
The results of these computations are shown in 
Figure 5. Note that the abscissa for TWC is the 
number of word topics (Z) multiplied by the 
(TWC)
349
number of character topics (T), while the ab-
scissa for TNG is the number of topics (T). They 
both represent the number of partitions into 
which the model classifies characters. 
Chance performance results in a perplexity 
equal to the number of unique characters, which 
was 4476 in our experiments. Therefore, both 
TWC and TNG are competitive models. And the 
lower curve shows that TWC is much better than 
TNG. 
We also found that both perplexity curves in-
creased with the number of partitions. In other 
words, both models suffer from overfitting issues. 
This is because the documents in a test set are 
very likely to contain words that do not appear in 
any of the documents in the training set. Such 
words will have a very low probability, which is 
inversely proportional to the number of partitions. 
Therefore, the perplexity of TWC increased from 
7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), while 
that of TNG increased from 20.3789 (T=5) to 
193.6065 (T=100).  
5.2 Chinese word and topic discovery 
As shown in the previous subsection, TWC is 
a competitive method for topically modeling 
Chinese documents. Next, we show its ability to 
extract Chinese words and topics in comparison 
with TNG.  
In our qualitative experiments, the task of 
Chinese word and topic discovery was addressed 
as a supervised learning problem, where a set of 
words with their topical assignments was given 
as a seed set. Each seed can be viewed as some 
constraints imposed on the TWC and TNG mod-
els. For example, suppose that ???? (teacher) 
together with its assignment ?education? is a 
seed. This assumption implies that the indicator 
between characters ??? and ??? is 1 and that 
the (word) topic for each character is ?education?.  
We make use of such constraints in a simple 
but effective way. In each sampling iteration, we 
first sample all variables as usual and then reset 
observed variables according to the constraints. 
We used 8000 Chinese documents in the Chi-
nese Gigaword corpus (LDC2005T14) provided 
by the Linguistic Data Consortium for our ex-
periments. The dataset contains 4651 unique 
characters and 3,295,810 characters.  
The number of word topics in TWC, the num-
ber of character topics in TWC, and the number 
of topics in TNG were all set to 15. Furthermore, 
16 seeds scattered in 4 distinct topics were given, 
as listed in Table 1 column ?seed?. Dirichlet 
(Beta)   priors  were  set  to  the  same  values   as 
described in the previous subsection.  
Word and topic assignments were extracted af-
ter running 300 Gibbs sampling iterations on the 
training corpus together with the seed set. For the 
TNG model, we took the first character?s topic as 
the topic of the word. We omitted one-character 
words and ranked the extracted words using the 
following formula 
( )
( )
i
15
i
i 1
occ W
occ W
=
?
,  
where occi(W) represents how many words were 
assigned to (word) topic i. The top-50 extracted 
words are presented in Table 1.  
We find found that both TWC and TNG could 
assemble many common words used in corre-
sponding topics. And the TWC model had ad-
vantages over the TNG model in the following 
three respects. 
First, TNG drew more words related to the 
seeds. In Table 1, highly related words are 
marked in pink (underline) and partly related 
words are marked in blue (italic). It is clear that 
the TWC column is more colorful than the TNG 
column. 
Secondly, we found that many words extracted 
by TNG had the same prefix. For example, con-
sider the topic ?agriculture?: there are 14 words 
marked with superscript 1 in Table 1. They all 
have the prefix ???. This is because we took the 
first character?s topic as the topic of the word. 
Although this strategy is beneficial in some cases, 
such as for words with prefix ???, it is detri-
mental in other cases. For example, ??? ? 
(sugar cane) and ???? (Gansu) have the same 
prefix and topic assignment, but the latter is a 
name of a province in China and is not related to 
agriculture. Similarly, even though the character 
string ???? does not form a Chinese word, this 
string ???? and ???? (Iran) are classified in 
the same cluster. Compared with TNG, TWC can 
also extract words whose topics are identical to 
the topic of any character. For example, the top-
ics ????? (freestyle swimming), ????? 
(medley swimming), and ??? ? (butterfly 
stroke) depend on their suffixes.  
Thirdly, although TNG stands for ?topical n-
gram model?, it infrequently draws words con-
taining more than two characters. On the other 
hand, the TWC model extracts many n-character 
words, such as ???????? (president of 
United States, George Bush), ??????? 
(individual medley), and  ?????? (four  per- 
350
Seeds TNG TWC 
??(football) 
??(player) 
??(match) 
??
(championship) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,?? 
??,?? 2,??,???,???,??,??,??,?
?,??,??,??,??,??,??,???,??,?
?,??,???,??,??,??,?? 2,??,???,
??,???,??? 2,?????,??,??,??,?
?,??,??,??,??,??,???,???,??,?
?,??,??,??,??,??,???,?? 
?? (foodstuff) 
?? (country) 
?? (farmer) 
?? (paddies) 
??,?? 1,??,?? 1,??,?? 1,?? 1,??
1,??,?? 1,?? 1,??,?? 1,?? 1,??,?
?,?? 1,??,?? 1,??,?? 1,?? 1,??,?
? 1,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
?? 
??,??,??,??,??,??,??,??,??,?
?,??,???,??,??,??,??,??,??,??
?,??,?? 2,??,??? 2,??,????,?? 2,
??,??,??,??,??? 2,??,??,??,??,
??,??,??? 2,??,??,??,?? 2,?? 2,?
?,????,??,??,??,??,?? 
?? (school) 
?? (teacher) 
?? (student) 
?? (education) 
??,??,??,??,??,??,??,??,?
?,??,??,??,???,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,?? 
??,??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,????,??,???,??,??,??,??,?
?,??,??,???,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,??,??,?? 
?? (war) 
?? (solider) 
?? (general) 
?? (weapon) 
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,
??,??,??,??,??,??,??,??,?
?,??,??,???,??,??,??,??? 
??,??,??,??,???,??,??,??,??,?
?????,??,????,????? 2,??? 2,?
??,????,??,??,????,??,????,?
?,??,???,??,??,??,??,????,??,
???,??? 2,??,??,??,??,??,??,?
?,??,??,??,??,??,??,??,??,??,
??,?? 
Table 1: Top-50 words extracted by TWC and TNG.  
cent). This is partly due to our sampling strategy, 
discussed in subsection 4.1, which increases the 
probability of forming long words. 
We also found that some extracted character 
strings were very close to real Chinese words. 
For instance, ???? is a substring of ????? 
(tournament); ????? is a suffix of ????
?? (Chinese player), ?????? (American 
player), and ?????? (French player); and 
????? is a substring of  ?????? (100 
thousand kilograms) and ?????? (50 thou-
sand kilograms). (Such substrings are marked 
with superscript 2 in Table 1.) We believe that 
this result occurred because the training corpus 
was not large enough and that TWC will achieve 
better performance with a large dataset. 
6 Conclusion and Future Work 
In this paper, we presented a topical word-
character (TWC) model, which models two dis-
tinct types of topics: word topic and character 
topic. The experimental results show that TWC 
is a powerful approach to modeling Chinese 
documents according to the standard evaluation 
measure of perplexity. We also demonstrated 
TWC?s ability to detect words and assign topics. 
Since TWC is a straightforward improvement 
that removes the limitations of existing topical 
collocation models, we expect that its application 
to English collocation will also result in higher 
performance.  
Appendix A.1 Gibbs Sampling Derivation 
for TWC 
Symbols used here are defined as follows.  
C is the number of unique characters, T is the num-
ber of character topics, and Z is the number of word 
topics.  
Nd denotes the number of characters in a document 
d. 
( )I ?  is an indicator function, taking the value 1 
when its argument is true, and 0 otherwise. 
qd,z,0 represents how words are assigned to topic z in 
document d; pz,t,c,k represents how many times an indi-
cator is k given the previous character c, the previous 
character topic t, and the previous word topic z; nz,t,0 
represents how many times a character topic is t given 
a word topic z and the corresponding indicator 0; 
mz,v,t,1 represents how many times a character topic is t 
given a word topic z, the previous character topic v, 
and the corresponding indicator 1; and rt,c represents 
how many times character c is assigned to character 
topic t. 
'
'
', ' ', ' ', '
, ,
' ', ' ', ' ', ' '
' ' '
', ', ' ', ' , ,
' ' ' ' '
'
( | , , , , , , , , )
( | ) ( | , , )
( | ) ( | )
(
d
d
d i 1 d i 1 d i 1
d i d i
ND D
d d i d i d i 1 d
d 1 d 1 i 1
NZ T C D
z t c d i z t c
z 1 t 1 c 1 d 1 i 1
z
P t s z x t c ? ? ? ? ?
P ? ? P z x z ? d?
P ? ? P x ? d?
P ?
? ? ?
?
?
= = =
= = = = =
=
? ? ? ?
? ?
?
? ???
??? ???
GG GG
G G G
G G G
G '
', '', ' ', ' ', '
' ' ' ' '
| ) ( | ) ( | , ,
d
d i
NZ Z T D
z t d i d i z
z 1 z 1 t 1 d 1 i 1
? P ? ? P t x ?
= = = = =
? ?? ?? ???? GG
 
351
'', ' ', ' ', '
', ', ', '
', ', ' ', ', '
,
, ,, ,
, ' ', '
' ' '
' '
' ' ' ' '
) ( | ) ( | )
?( )
( ) ( )
?( )
?( )
?( )
d
d i d i 1 d i
z t c x
z t c z t c
d i 1
z s cd i d i
NT D
z t t d i t
t 1 d 1 i 1
Z T C 2 2
px ? 1 x
2
z 1 t 1 c 1 x 1 x 1
x
C
t
? d? d? P ? ? P c ? d?
2?
? ?
?
C?
? d?
?
?
+
= = =
?
= = = = =
? ? ? ? ?
? ? ? ?
? ?
? ???
??? ? ??
G G G GG G
G
', ' ,
' '
', ',
,', ', ',
, ,
' '
' ' '
' '
' '
' ' ' ' '
' '
', ' ', '
' ' ,
( ) ( )
?( ) ?( )
( ) ( )
?( ) ?( )
( ) ( )
t c d i
t t s
z t 0
d iz t v 1
d i d i 1
T C C
r cc ? 1 c
1 c 1 c 1
Z T T Z T
nt ? 1 t
z zT T
z 1 t 1 t 1 z 1 t 1
sT T
zmv ? 1 v
z t z t
v 1 v 1 z t
? ? ? d?
T? T?
? ?
? ?
?
? ?
? ?
?
= = =
?
= = = = =
?
= =
? ? ?
? ? ? ?
? ?
? ? ??
? ? ? ????
? ?
G
,
,, , , ,
, ,, ,
, ,
,
,
, ,
,
,*,, , , ,
, , ,, , ,* ,*
,
, ,*,
d i
d id i d i d i d i
d i d i 1d i d i
d i d i 1
d i
s
d i
z s 0
d i
z 0z s c x s c
z t s 1z s c s
d i
z t 1
x 0
d? d?
x 1
n ?
x 0
n T?p ? r ?
m ?p 2? r C?
x 1
m T?
?
?
? =? ? ?? =??
+? =? ++ + ?? ? ? ? ++ + ? =? +?
G G
  
Similarly, 
,
, , ,
, , ,
, ,
,
, , ,
, ,
, ,
, ,
, , ,
,*,
, , ,*
, ,
, ,
,*,
, , ,
, ,*,
( | , , , , , , , , )
( )
d i
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i d i
d i
d i d i 1 d i
d i d i 1
d i d i
d z 0
z t c k
d 0
z t c
d i d i 1
z t 0
z 0
z t t 1
z t
P x k z x t c ? ? ? ? ?
q ?
p ?k 0
q Z?
p 2?
I z z k 1
n ?
k 0
n T?
m ?
m
? ? ?
? ? ?
?
?
?
?
=
+? +=? +? ? ?? +? = =?
+
+?
=+
GG GG
1
k 1
T?
????? =? +?
 
, , ,
, , ,
, , ,
, , , , , ( : ) ,
, ,
, , ,
,*,
, , ,*
,
, , ,
( , | , , ,
, , , , , )
( )
d i 1 d i 1 d i 1
d i 1 d i 1 d i 1
d i u 2 d i u 2 d i u 1
d i d i 1 d i l 1 d i d i i l 1 d i
d j 0
z t c k
d 0
z t c
d i 1
j t c x
j
P z z z j x k z x t
c ? ? ? ? ?
q ?
k 0 p ?
q Z?
p 2?
I z j k
P
1
p ?
p
? ? ?
? ? ?
+ ? + ? + ?
+ + ? ? + ? ?
?
= = = = =
+? = +? +? ? ??
?
+? = =?
+
GGG"
G
, ,
, , ,
,
, ,
,
, , ,
, , ,* , ,*
, ,
,*,
, , ,
, ,*,
( )d i u 2 d i u 1
d i u 2 d i u 2 d i u 2
d i
d i 1 d i
d i 1
l 1 l
j t t 1
u 2 u 2t c j t 1
j t 0
j 0
j t t 1
j t 1
m ?
2? m T?
n ?
k 0
n T?
m ?
k 1
m T?
+ ? + ?
+ ? + ? + ?
?
?
+
= =
?
+? ?+ +
+? =? +?? +? =? +?
? ?
 
Appendix A.2 Parameter estimation for 
TWC 
After each Gibbs sampling iteration, we obtain pos-
terior estimates ? ? ??, , ,? ? ? ?
G G GG
 and r by 
, , , , ,
, , , ,
,*, , , ,*
, , , , ,
, , ,
, ,*, ,*,
,
,
,*
? ?
??
?
d z 0 z t c k
d z z t c k
d 0 z t c
z v t 1 z t 0
z v t z t
z v 1 z 0
t c
t c
t
q ? p ?
? ?
q Z? p 2?
m ? n ?
? ?
m T? n T?
r ?
?
r C?
+ += =+ +
+ += =+ +
+= +
, 
where the symbols are the same as those defined in 
Appendix A.1. These values correspond to the predic-
tive distribution over new word topics, new indicators, 
new character topics, and new characters. 
Appendix B. Likelihood Function Deriva-
tion for TWC and TNG 
To compute the likelihood function for TWC, a qua-
ternion function gi is defined as follows: (formula has 
a broken character) 
, , , , ,
, ,
( , , , ) ( , ,..., , , ,
? ? ? ??, | , , , , )
i d 1 d 2 d i d i d i
d i 1 d i
g r s u v P c c c z r x s
t u t v ? ? ? ? ??=====
= =
= ======

G G G GG . 
Then, it is clear that 
? ? ? ??( | , , , , ) ( , , , )
d
Z 1 T T
d N
r 1 s 0 u 1 v 1
P c ? ? ? ? ? g r s u v
= = = =
=????GG G G GG , 
where Z is the number of word topics and T is the 
number of character topics. The function gi can be 
rewritten in a recursive manner.  
,
,
,, ,
,
( , , , )
?( , , , )
? ?( , , , ) ( , , , )
? ?
?( )
d 1
d i 1
d i
1
cr v
1 d r v
Z 1 T
cs
i 1 i j u c v
j 1 k 0 l 1
vr
rd
v
r u
g r 1 u v 0
1
g r 0 u v ? ? ?
T
g r s u v g j k l u ? ?
s 0??
s 1?I r j
+
+
= = =
=
= ? ? ?
= ? ?
? =?? ?? ==??
===========
???  
Similarly we can define function hi to help compute 
the likelihood for TNG. (formula has a broken charac-
ter) 
,
,
, ,
,
, ,
,
,
? ? ??( , ) ( ,..., , , | , , , )
? ? ??( | , , , ) ( , )
( , )
? ?( , )
???( , ) ( , )
?
d
d 1
d i 1
d i d i 1
d i
i d 1 d i i i
Z 1
d N
r 1 s 0
1
cr
1 d r
c
Z 1
rs r
i 1 i j c d c
j 1 k 0 r c
h r s P c c z r x s ? ? ? ?
P c ? ? ? ? h r s
h r 1 0
h r 0 ? ?
? s 0
h r s h j k ? ?
s 1?
+
+
= =
+
= =
= =
=
=
= ?
? =?= ? ? ? ? =??
??
??
G G GG
GG G GG
 
References 
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent 
Dirichlet alocation. Journal of Machine Learning 
Research, 3:993?1022. 
Griffiths, T. L. and Steyvers, M. 2004. Finding Scien-
tific Topics. Proceedings of the National Academy 
of Sciences, 101 (suppl. 1), 5228-5235. 
Steyvers, M. and Griffiths, T. L. 2007. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. Laurence Erlbaum. 
Griffiths, T. L., Steyvers, M., and Tenenbaum, J. B. T. 
2007. Topics in Semantic Representation Psycho-
logical Review, 114(2), 211-244.  
Wang, X., McCallum, A., and Wei, X. 2007. Topical 
N-grams: Phrase and Topic Discovery, with an 
Application to Information Retrieval. Proceedings 
of the 7th IEEE International Conference on Data 
Mining (ICDM 2007).  
352
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 793?800
Manchester, August 2008
Metric Learning for Synonym Acquisition
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Masato Hagiwara
Graduate School of Information Science
Nagoya University
hagiwara@kl.i.is.nagoya-u.ac.jp
Yasuhiro Ogawa and Katsuhiko Toyama
Graduate School of Information Science
Nagoya University
{yasuhiro,toyama}@kl.i.is.nagoya-u.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
The distance or similarity metric plays an
important role in many natural language
processing (NLP) tasks. Previous stud-
ies have demonstrated the effectiveness of
a number of metrics such as the Jaccard
coefficient, especially in synonym acqui-
sition. While the existing metrics per-
form quite well, to further improve perfor-
mance, we propose the use of a supervised
machine learning algorithm that fine-tunes
them. Given the known instances of sim-
ilar or dissimilar words, we estimated the
parameters of the Mahalanobis distance.
We compared a number of metrics in our
experiments, and the results show that the
proposed metric has a higher mean average
precision than other metrics.
1 Introduction
Accurately estimating the semantic distance be-
tween words in context has applications for
machine translation, information retrieval (IR),
speech recognition, and text categorization (Bu-
danitsky and Hirst, 2006), and it is becoming
clear that a combination of corpus statistics can be
used with a dictionary, thesaurus, or other knowl-
edge source such as WordNet or Wikipedia, to in-
crease the accuracy of semantic distance estima-
tion (Mohammad and Hirst, 2006). Although com-
piling such resources is labor intensive and achiev-
ing wide coverage is difficult, these resources to
some extent explicitly capture semantic structures
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of concepts and words. In contrast, corpus statis-
tics achieve wide coverage, but the semantic struc-
ture of a concept is only implicitly represented in
the context. Assuming that two words are semanti-
cally closer if they occur in similar contexts, statis-
tics on the contexts of words can be gathered and
compared for similarity, by using a metric such as
the Jaccard coefficient.
Our proposal is to extend and fine-tune the latter
approach with the training data obtained from the
former. We apply metric learning to this task. Al-
though still in their infancy, distance metric learn-
ing methods have undergone rapid development in
the field of machine learning. In a setting simi-
lar to semi-supervised clustering, where known in-
stances of similar or dissimilar objects are given,
a metric such as the Mahalanobis distance can be
learned from a few data points and tailored to fit a
particular purpose. Although classification meth-
ods such as logistic regression now play impor-
tant roles in natural language processing, the use
of metric learning has yet to be explored.
Since popular current methods for synonym ac-
quisition require no statistical learning, it seems
that supervised machine learning should easily
outperform them. Unfortunately, there are obsta-
cles to overcome. Since metric learning algorithms
usually learn the parameters of a Mahalanobis dis-
tance, the number of parameters is quadratic to the
number of features. They learn how two features
should interact to produce the final metric. While
traditional metrics forgo examining of the interac-
tions entirely, in applying metrics such as Jaccard
coefficient, it is not uncommon nowadays to use
more than 10,000 features, a number that a typical
metric learner is incapable of processing. Thus we
have two options: one is to find the most impor-
tant features and model the interactions between
793
them, and the other is simply to use a large number
of features. We experimentally examined the two
options and found that metric learning is useful in
synonym acquisition, despite it utilizing fewer fea-
tures than traditional methods.
The remainder of this paper is organized as fol-
lows: in section 2, we review prior work on syn-
onym acquisition and metric learning. In section
3, we introduce the Mahalanobis distance metric
and a learning algorithm based on this metric. In
section 4 and 5, we explain the experimental set-
tings and propose the use of normalization to make
the Mahalanobis distances work in practice, and
then in section 6, we discuss issues we encountered
when applying this metric to synonym acquisition.
We conclude in section 7.
2 Prior Work
As this paper is based on two different lines of re-
search, we first review the work in synonym acqui-
sition, and then review the work in generic metric
learning. To the best of the authors? knowledge,
none of the metric learning algorithms have been
applied to automatic synonym acquisition.
Synonym relation is important lexical knowl-
edge for many natural language processing
tasks including automatic thesaurus construction
(Croach and Yang, 1992; Grefenstette, 1994) and
IR (Jing and Croft, 1994). Various methods (Hin-
dle, 1990; Lin, 1998) of automatically acquiring
synonyms have been proposed. They are usu-
ally based on the distributional hypothesis (Har-
ris, 1985), which states that semantically simi-
lar words share similar contexts, and they can be
roughly viewed as the combinations of two steps:
context extraction and similarity calculation. The
former extracts useful features from the contexts of
words, such as surrounding words or dependency
structure. The latter calculates how semantically
similar two given words are based on similarity or
distance metrics.
Many studies (Lee, 1999; Curran and Moens,
2002; Weeds et al, 2004) have investigated
similarity calculation, and a variety of dis-
tance/similarity measures have already been com-
pared and discussed. Weeds et al?s work is espe-
cially useful because it investigated the character-
istics of metrics based on a few criteria such as
the relative frequency of acquired synonyms and
clarified the correlation between word frequency,
distributional generality, and semantic generality.
However, all of the existing research conducted
only a posteriori comparison, and as Weeds et al
pointed out, there is no one best measure for all ap-
plications. Therefore, the metrics must be tailored
to applications, even to corpora and other settings.
We next review the prior work in generic metric
learning. Most previous metric learning methods
learn the parameters of the Mahalanobis distance.
Although the algorithms proposed in earlier work
(Xing et al, 2002; Weinberger et al, 2005; Glober-
son and Roweis, 2005) were shown to yield excel-
lent classification performance, these algorithms
all have worse than cubic computational complex-
ity in the dimensionality of the data. Because of
the high dimensionality of our objects, we opted
for information-theoretic metric learning proposed
by (Davis et al, 2007). This algorithm only uses
an operation quadratic in the dimensionality of the
data.
Other work on learning Mahalanobis metrics in-
cludes online metric learning (Shalev-Shwartz et
al., 2004), locally-adaptive discriminative methods
(Hastie and Tibshirani, 1996), and learning from
relative comparisons (Schutz and Joahims, 2003).
Non-Mahalanobis-based metric learning methods
have also been proposed, though they seem to suf-
fer from suboptimal performance, non-convexity,
or computational complexity. Examples include
neighborhood component analysis (Goldberger et
al., 2004).
3 Metric Learning
3.1 Problem Formulation
To set the context for metric learning, we first de-
scribe the objects whose distances from one an-
other we would like to know. As noted above re-
garding the distributional hypothesis, our object is
the context of a target word. To represent the con-
text, we use a sparse vector in Rd. Each dimension
of an input vector represents a feature of the con-
text, and its value corresponds to the strength of
the association. The vectors of two target words
represent their contexts as points in multidimen-
sional feature-space. A suitable metric (for exam-
ple, Euclidean) defines the distance between the
two points, thereby estimating the semantic dis-
tance between the target words.
Given points x
i
, x
j
? R
d
, the (squared) Ma-
halanobis distance between them is parameter-
ized by a positive definite matrix A as follows
d
A
(x
i
, x
j
) = (x
i
? x
j
)
?
A(x
i
? x
j
). The Ma-
794
halanobis distance is a straightforward extension
of the standard Euclidean distance. If we let A
be the identity matrix, the Mahalanobis distance
reduces to the Euclidean distance. Our objective
is to obtain the positive definite matrix A that pa-
rameterizes the Mahalanobis distance, so that the
distance between the vectors of two synonymous
words is small, and the distance between the vec-
tors of two dissimilar words is large. Stated more
formally, the Mahalanobis distance between two
similar points must be smaller than a given upper
bound, i.e., d
A
(x
i
, x
j
) ? u for a relatively small
value of u. Similarly, two points are dissimilar if
d
A
(x
i
, x
j
) ? l for sufficiently large l.
As we discuss below, we were able to use
the Euclidean distance to acquire synonyms quite
well. Therefore, we would like the positive definite
matrix A of the Mahalanobis distance to be close to
the identity matrix I . This keeps the Mahalanobis
distance similar to the Euclidean distance, which
would help to prevent overfitting the data. To op-
timize the matrix, we follow the information theo-
retic metric learning approach described in (Davis
et al, 2007). We summarize the problem formula-
tion advocated by this approach in this section and
the learning algorithm in the next section.
To define the closeness between A and I , we
use a simple bijection (up to a scaling function)
from the set of Mahalanobis distances to the set
of equal mean multivariate Gaussian distributions.
Without loss of generalization, let the equal mean
be ?. Then given a Mahalanobis distance pa-
rameterized by A, the corresponding Gaussian is
p(x;A) =
1
Z
exp(?
1
2
d
A
(x, ?)) where Z is the
normalizing factor. This enables us to measure
the distance between two Mahalanobis distances
with the Kullback-Leibler (KL) divergence of two
Gaussians:
KL(p(x; I)||p(x;A)) =
?
p(x, I) log
(
p(x; I)
p(x;A)
)
dx.
Given pairs of similar points S and pairs of dis-
similar points D, the optimization problem is:
min
A
KL(p(x; I)||p(x;A))
subject to d
A
(x
i
, x
j
) ? u (i, j) ? S
d
A
(x
i
, x
j
) ? l (i, j) ? D
3.2 Learning Algorithm
(Davis and Dhillon, 2006) has shown that the
KL divergence between two multivariate Gaus-
sians can be expressed as the convex combination
of a Mahalanobis distance between mean vectors
and the LogDet divergence between the covariance
matrices. The LogDet divergence equals
D
ld
(A,A
0
) = tr(AA
?1
0
)? log det(AA
?1
0
)? n
for n by n matrices A,A
0
. If we assume the means
of the Gaussians to be the same, we have
KL(p(x;A
0
||p(x,A)) =
1
2
D
ld
(A,A
0
)
The optimization problem can be restated as
min
A0
D
ld
(A, I)
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? u (i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? l (i, j) ? D
We then incorporate slack variables into the for-
mulation to guarantee the existence of a feasible
solution for A. The optimization problem be-
comes:
min
A0
D
ld
(A, I) + ?D
ld
(diag(?), diag(?
0
))
s.t. tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? S
tr(A(x
i
? x
j
)(x
i
? x
j
)
?
) ? ?
c(i,j)
(i, j) ? D
where c(i, j) is the index of the (i, j)-th constraint
and ? is a vector of slack variables whose compo-
nents are initialized to u for similarity constraints
and l for dissimilarity constraints. The tradeoff
between satisfying the constraints and minimiz-
ing D
ld
(A, I) is controlled by the parameter ?.
To solve this optimization problem, the algorithm
shown in Algorithm 3.1 repeatedly projects the
current solution onto a single constraint.
This completes the summary of (Davis et al,
2007).
4 Experimental Settings
In this section, we describe the experimental set-
tings including the preprocessing of data and fea-
tures, creation of the query word sets, and settings
of the cross validation.
4.1 Features
We used a dependency structure as the context for
words because it is the most widely used and one
of the best performing contextual information in
the past studies (Ruge, 1997; Lin, 1998). As the
extraction of an accurate and comprehensive de-
pendency structure is in itself a complicated task,
the sophisticated parser RASP Toolkit 2 (Briscoe
et al, 2006) was utilized to extract this kind of
word relation.
Let N(w, c) be the raw cooccurrence count of
word w and context c, the grammatical relation
795
Algorithm
3.1: INFORMATION THEORETIC METRIC LEARNING
Input :
X(d by n matrix), I(identity matrix)
S(set of similar pairs),D(set of dissimilar pairs)
?(slack parameter), c(constraint index function)
u, l(distance thresholds)
Output :
A(Mahalanobis matrix)
A := I
?
ij
:= 0
?
c(i,j)
:= u for (i, j) ? S; otherwise, ?
c(i,j)
:= l
repeat
Pick a constraint (i, j) ? S or (i, j) ? D
p := (x
i
? x
j
)
?
A(x
i
? x
j
)
? := 1 if (i, j) ? S,?1 otherwise.
? := min(?
ij
,
?
2
(
1
p
?
?
?
c(i,j)
))
? := ??/(1? ???
c(i,j)
)
?
c(i,j)
:= ??
c(i,j)
/(? + ???
c(i,j)
)
?
ij
:= ?
ij
? ?
A := A + ?A(x
i
? x
j
)(x
i
? x
j
)
?
A
until convergence
return (A)
in which w occurs. These raw counts were ob-
tained from New York Times articles (July 1994)
extracted from English Gigaword 1. The section
consists of 7,593 documents and approx. 5 million
words. As discussed below, we limited the vocab-
ulary to the nouns in the Longman Defining Vo-
cabulary (LDV) 2. The features were constructed
by weighting them using pointwise mutual infor-
mation: wgt(w, c) = PMI(w, c) = log P (w,c)
P (w)P (c)
.
Co-occurrence data constructed this way can
yield more than 10,000 context types, rendering
metric learning impractical. As the applications
of feature selection reduce the performance of the
baseline metrics, we tested them in two different
settings: with and without feature selection. To
mitigate this problem, we applied a feature selec-
tion technique to reduce the feature dimensional-
ity. We selected features using two approaches.
The first approach is a simple frequency cutoff, ap-
plied as a pre-processing to filter out words and
contexts with low frequency and to reduce com-
putational cost. Specifically, all words w such
that
?
c
N(w, c) < ?
f
and contexts c such that
?
w
N(w, c) < ?
f
, with ?
f
= 5, are removed from
the co-occurrence data.
The second approach is feature selection by con-
1http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2003T05
2http://www.cs.utexas.edu/users/kbarker/working notes/
ldoce-vocab.html
text importance (Hagiwara et al, 2008). First, the
context importance score for each context type is
calculated, and then the least important context
types are eliminated, until a desired numbers of
them remains. To measure the context importance
score, we used the number of unique words the
context co-occurs with: df(c) = |{w|N(w, c) >
0}|. We adopted this context selection criterion
on the assumption that the contexts shared by
many words should be informative, and the syn-
onym acquisition performance based on normal
distributional similarity calculation retains its orig-
inal level of performance until up to almost 90%
of context types are eliminated (Hagiwara et al,
2008). In our experiment, we selected features
rather aggressively, finally using only 10% of the
original contexts. These feature reduction oper-
ations reduced the dimensionality to a figure as
small as 1,281, while keeping the performance loss
at a minimum.
4.2 Similarity and Distance Functions
We compared seven similarity/distance functions
in our experiments: cosine similarity, Euclidean
distance, Manhattan distance, Jaccard coeffi-
cient, vector-based Jaccard coefficient (Jaccardv),
Jensen-Shannon Divergence (JS) and skew diver-
gence (SD99). We first define some notations. Let
C(w) be the set of context types that co-occur with
word w, i.e., C(w) = {c|N(w, c) > 0}, and w
i
be
the feature vector corresponding to word w, i.e.,
w
i
= [wgt(w
i
, c
1
) ... wgt(w
i
, c
M
)]
?
. The first
three, the cosine, Euclidean and Manhattan dis-
tance, are vector-based metrics.
cosine similarity
w
1
?w
2
||w
1
|| ? ||w
2
||
Euclidean distance
?
?
c?C(w
1
)?C(w
2
)
(wgt(w
1
, c)? wgt(w
2
, c))
2
Manhattan distance
?
c?C(w
1
)?C(w
2
)
|wgt(w
1
, c)? wgt(w
2
, c)|
Jaccard coefficient
?
c?C(w
1
)?C(w
2
)
min(wgt(w
1
, c),wgt(w
2
, c))
?
c?C(w
1
)?C(w
2
)
max(wgt(w
1
, c),wgt(w
2
, c))
,
796
vector-based Jaccard coefficient (Jaccardv)
w
i
?w
j
||w
i
|| + ||w
j
|| ?w
i
?w
j
.
Jensen-Shannon divergence (JS)
1
2
{KL(p
1
||m) + KL(p
2
||m)}, m = p
1
+ p
2
.
JS and SD99 are based on the KL divergence, so
the vectors must be normalized to form a probabil-
ity distribution. For notational convenience, we let
p
i
be the probability distribution representation of
feature vector w
i
, i.e., p
i
(c) = N(w
i
, c)/N(w
i
).
While the KL divergence suffers from the so-called
zero-frequency problem, a symmetric version of
the KL divergence called the Jensen-Shannon di-
vergence naturally avoids it.
skew divergence (SD99)
KL(p
1
||?p
2
+ (1? ?)p
1
).
As proposed by (Lee, 2001), the skew diver-
gence also avoids the zero-frequency problem by
mixing the original distribution with the target dis-
tribution. Parameter ? is set to 0.99.
4.3 Query Word Set and Cross Validation
To formalize the experiments, we must prepare a
set of query words for which synonyms are known
in advance. We chose the Longman Defining
Vocabulary (LDV) as the candidate set of query
words. For each word in the LDV, we consulted
three existing thesauri: Roget?s Thesaurus (Ro-
get, 1995), Collins COBUILD Thesaurus (Collins,
2002), and WordNet (Fellbaum, 1998). Each LDV
word was looked up as a noun to obtain the union
of synonyms. After removing words marked ?id-
iom?, ?informal? or ?slang? and phrases com-
prised of two or more words, this union was used
as the reference set of query words. LDV words for
which no noun synonyms were found in any of the
reference thesauri were omitted. From the remain-
ing 771 LDV words, there were 231 words that had
five or more synonyms in the combined thesaurus.
We selected these 231 words to be the query words
and distributed them into five partitions so as to
conduct five-fold cross validation. Four partitions
were used in training, and the remaining partition
was used in testing. For each fold, we created
the training set from four partitions as follows; for
each query word in the partitions, we randomly se-
lected five synonymous words and added the pairs
of query words and synonymous words to S, the
set of similar pairs. Similarly, five pairs of query
words and dissimilar words were randomly added
to D, the set of dissimilar pairs. The training set
for each fold consisted of S and D. Since a learner
trained on an imbalanced dataset may not learn
to discriminate enough between classes, we sam-
pled dissimilar pairs to create an evenly distributed
training dataset.
To make the evaluation realistic, we used a dif-
ferent method to create the test set: we paired each
query word with each of the 771 remaining words
to form the test set. Thus, in each fold, the training
set had an equal number of positive and negative
pairs, while in the test set, negative pairs outnum-
bered the positive pairs. While this is not a typical
setting for cross validation, it renders the evalua-
tion more realistic since an automatic synonym ac-
quisition system in operation must be able to pick
a few synonyms from a large number of dissimilar
words.
The meta-parameters of the metric learning
model were simply set u = 1, l = 2 and ? = 1.
Each training set consisted of 1,850 pairs, and the
test set consisted of 34,684 pairs. Since we con-
ducted five-fold cross validation, the reported per-
formance in this paper is actually a summary over
different folds.
4.4 Evaluation Measures
We used an evaluation program for KDD Cup
2004 (Caruana et al, 2004) called Perf to measure
the effectiveness of the metrics in acquiring syn-
onyms. To use the program, we used the following
formula to convert each distance metric to a simi-
larity metric. s(x
i
, x
j
) = 1/(1 + exp(d(x
i
, x
j
))).
Below, we summarize the three measures we
used: Mean Average Precision, TOP1, and Aver-
age Rank of Last Synonym.
Mean Average Precision (APR)
Perf implements a definition of average preci-
sion sometimes called ?expected precision?. Perf
calculates the precision at every recall where it is
defined. For each of these recall values, Perf finds
the threshold that produces the maximum preci-
sion, and takes the average over all of the recall
values greater than 0. Average precision is mea-
sured on each query, and then the mean of each
query?s average precision is used as the final met-
ric. A mean average precision of 1.0 indicates per-
fect prediction. The lowest possible mean average
797
precision is 0.0.
Average Rank of Last Synonym (RKL)
As in other evaluation measures, synonym can-
didates are sorted by predicted similarity, and this
metric measures how far down the sorted cases we
must go to find the last true synonym. A rank of
1 indicates that the last synonym is placed in the
top position. Given a query word, the highest ob-
tainable rank is N if there are N synonyms in the
corpus. The lower this measure is the better. Aver-
age ranks near 771 indicate poor performance.
TOP1
In each query, synonym candidates are sorted by
predicted similarity. If the word that ranks at the
top (highest similarity to the query word) is a true
synonym of the query word, Perf scores a 1 for
that query, and 0 otherwise. If there are ties, Perf
scores 0 unless all of the tied cases are synonyms.
TOP1 score ranges from 1.0 to 0.0. To achieve 1.0,
perfect TOP1 prediction, a similarity metric must
place a true synonym at the top of the sorted list
in every query. In the next section, we report the
mean of each query?s TOP1.
5 Results
The evaluations of the metrics are listed in Table
1. The figure on the left side of ? represents the
performance with 1,281 features, and that on the
right side with 12,812 features. Of all the met-
rics in Table 1, only the Mahalanobis L2 is trained
with the previously presented metric learning al-
gorithm. Thus, the values for the Mahalanobis
L2 are produced by the five-fold cross validation,
while the rest are given by the straight application
of the metrics discussed in Section 4.2 to the same
dataset. Strictly speaking, this is not a fair com-
parison, since we ought to compare a supervised
learning with a supervised learning. However, our
baseline is not the simple Euclidean distance; it
is the Jaccard coefficient and cosine similarity, a
handcrafted, best performing metric for synonym
acquisition, with 10 times as many features.
The computational resources required to obtain
the Mahalanobis L2 results were as follows: in the
training phase, each fold of cross validation took
about 80 iterations (less than one week) to con-
verge on a Xeon 5160 3.0GHz. The time required
to use the learned distance was a few hours at most.
At first, we were unable to perform competi-
tively with the Euclidean distance. As seen in Ta-
ble 1, the TOP1 measure of the Euclidean distance
is only 1.732%. This indicates that the likelihood
of finding the first item on the ranked list to be a
true synonym is 1.732%. The vector-based Jac-
card coefficient performs much better than the Eu-
clidean distance, placing a true synonym at the top
of the list 30.736% of the time.
Table 2 shows the Top 10 Words for Query
?branch?. The results for the Euclidean distance
rank ?hut? and other dissimilar words highly. This
is because the norm of such vectors is small, and in
a high dimensional space, the sparse vectors near
the origin are relatively close to many other sparse
vectors. To overcome this problem, we normal-
ized the input vectors by the L2 norm x? = x/||x||
This normalization enables the Euclidean distance
to perform very much like the cosine similarity,
since the Euclidean distance between points on a
sphere acts like the angle between the vectors. Sur-
prisingly, normalization by L2 did not affect other
metrics all that much; while the performances of
some metrics improved slightly, the L2 normaliza-
tion lowered that of the Jaccardv metric.
Once we learned the normalization trick, the
learned Mahalanobis distance consistently outper-
formed all other metrics, including the ones with
10 times more features, in all three evaluation
measures, achieving an APR of 18.66%, RKL of
545.09 and TOP1 of 45.455%.
6 Discussion
Examining the learned Mahalanobis matrix re-
vealed interesting features. The matrix essentially
shows the covariance between features. While it
was not as heavily weighted as the diagonal ele-
ments, we found that its positive non-diagonal el-
ements were quite interesting. They indicate that
some of the useful features for finding synonyms
are correlated and somewhat interchangeable. The
example includes a pair of features, (dobj begin
*) and (dobj end *). It was a pleasant surprise to
see that one implies the other. Among the diag-
onal elements of the matrix, one of the heaviest
features was being the direct object of ?by?. This
indicates that being the object of the preposition
?by? is a good indicator that two words are simi-
lar. A closer inspection of the NYT corpus showed
that this preposition overwhelmingly takes a per-
son or organization as its object, indicating that
words with this feature belong to the same class
of a person or organization. Similarly, the class
798
Metric APR RKL TOP1
Cosine 0.1184 ? 0.1324 580.27 ? 579.00 0.2987 ? 0.3160
Euclidean 0.0229 ? 0.0173 662.74 ? 695.71 0.0173 ? 0.0000
Euclidean L2 0.1182 ? 0.1324 580.30 ? 578.99 0.2943 ? 0.3160
Jaccard 0.1120 ? 0.1264 580.76 ? 579.51 0.2684 ? 0.2943
Jaccard L2 0.1113 ? 0.1324 580.29 ? 570.88 0.2640 ? 0.2987
Jaccardv 0.1189 ? 0.1318 580.50 ? 580.19 0.3073 ? 0.3030
Jaccardv L2 0.1184 ? 0.1254 580.27 ? 570.00 0.2987 ? 0.3160
JS 0.0199 ? 0.0170 681.97 ? 700.53 0.0129 ? 0.0000
JS L2 0.0229 ? 0.0173 679.21 ? 699.00 0.0303 ? 0.0086
Manhattan 0.0181 ? 0.0168 687.73 ? 701.47 0.0043 ? 0.0000
Manhattan L2 0.0185 ? 0.0170 686.56 ? 701.11 0.0043 ? 0.0086
SD99 0.0324 ? 0.1039 640.71 ? 588.16 0.0173 ? 0.2640
SD99 L2 0.0334 ? 0.1117 633.32 ? 586.78 0.0216 ? 0.2900
Mahalanobis L2 0.1866 545.09 0.4545
Table 1: Evaluation of Various Metrics, as Number of Features Increase from 1,281 to 12,812
Cosine Euclidean Euclidean L2 Jaccard Jaccardv Mahalanobis L2
1 (*) office hut (*) office (*) office (*) office (*) division
2 area wild area border area group
3 (*) division polish (*) division area (*) division (*) office
4 border thirst border plant border line
5 group hollow group (*) division group period
6 organization shout organization mouth organization organization
7 store fold store store store (*) department
8 mouth dear mouth circle mouth charge
9 plant hate plant stop plant world
10 home wake home track home body
(*) = a true synonym
Table 2: Top 10 Words for Query ?branch?
of words that ?to? and ?within?, take as an objects
were clear from the corpus: ?to? takes a person
or place, ?within? takes duration of time 3. Other
heavy features includes being the object of ?write?
or ?about?. While not obvious, we postulate that
having these words as a part of the context indi-
cates that a word is an event of some type.
7 Conclusion
We applied metric learning to automatic synonym
acquisition for the first time, and our experiments
showed that the learned metric significantly out-
performs existing similarity metrics. This outcome
indicates that while we must resort to feature se-
lection to apply metric learning, the performance
gain from the supervised learning is enough to off-
set the disadvantage and justify its usage in some
applications. This leads us to think that a com-
bination of the learned metric with unsupervised
metrics with even more features may produces the
best results. We also discussed interesting features
found in the learned Mahalanobis matrix. Since
3Interestingly, we note that not all prepositions were as
heavy: ?beyond? and ?without? were relatively light among
the diagonal elements. In the NYT corpus, the class of words
they take was not as clear as, for example, ?by?.
metric learning is known to boost clustering per-
formance in a semi-supervised clustering setting,
we believe these automatically identified features
would be helpful in assigning a target word to a
word class.
References
T. Briscoe, J. Carroll and R. Watson. 2006. The Sec-
ond Release of the RASP System. Proc. of the COL-
ING/ACL 2006 Interactive Presentation Sessions,
77?80.
T. Briscoe, J. Carroll, J. Graham and A. Copestake,
2002. Relational evaluation schemes. Proc. of the
Beyond PARSEVAL Workshop at the Third Interna-
tional Conference on Language Resources and Eval-
uation, 4?8.
A. Budanitsky and G. Hirst. 2006. Evaluat-
ing WordNet-based measures of semantic distance.
Computational Linguistics, 32(1):13?47.
R. Caruana, T. Jachims and L. Backstrom. 2004. KDD-
Cup 2004: results and analysis ACM SIGKDD Ex-
plorations Newslatter, 6(2):95?108.
C. J. Croach and B. Yang. 1992. Experiments in au-
tomatic statistical thesaurus construction. the 15th
799
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
77?88.
J. R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Workshop on Un-
supervised Lexical Acquisition. Proc. of the ACL
SIGLEX, 231?238.
J. V. Davis and I. S. Dhillon. 2006. Differential En-
tropic Clustering of Multivariate Gaussians. Ad-
vances in Neural Information Processing Systems
(NIPS).
J. V. Davis, B. Kulis, P. Jain, S. Sra and I. S. Dhillon.
2007. Information Theoretic Metric Learning. Proc.
of the International Conference on Machine Learn-
ing (ICML).
A. Globerson and S. Roweis. 2005. Metric Learning by
Collapsing Classes. Advances in Neural Information
Processing Systems (NIPS).
J. Goldberger, S. Roweis, G. Hinton and R. Salakhut-
dinov. 2004. Neighbourhood Component Analysis.
Advances in Neural Information Processing Systems
(NIPS).
G. Grefenstette. 1994. Explorations in Automatic The-
suarus Discovery. Kluwer Academic Publisher.
M. Hagiwara, Y. Ogawa, and K. Toyama. 2008. Con-
text Feature Selection for Distributional Similarity.
Proc. of IJCNLP-08, 553?560.
Z. Harris. 1985. Distributional Structure. Jerrold
J. Katz (ed.) The Philosophy of Linguistics. Oxford
University Press. 26?47.
T. Hastie and R. Tibshirani. 1996. Discriminant adap-
tive nearest neighbor classification. Pattern Analysis
and Machine Intelligence, 18, 607?616.
D. Hindle. 1990. Noun classification from predicate-
argument structures. Proc. of the ACL, 268?275.
J. J. Jiang and D. W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxon-
omy. Proceedings of International Conference on
Research on Computational Linguistics (ROCLING
X), Taiwan.
Y. Jing and B. Croft. 1994. An Association The-
saurus for Information Retrieval. Proc. of Recherche
d?Informations Assiste?e par Ordinateur (RIAO),
146?160.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. Proc. of COLING/ACL 1998, 786?
774.
L. Lee. 1999. Measures of distributional similarity.
Proc. of the ACL, 23?32
L. Lee. 2001. On the Effectiveness of the Skew Diver-
gence for Statistical Language Analysis. Artificial
Intelligence and Statistics 2001, 65?72.
S. Mohammad and G. Hirst. 2006. Distributional mea-
sures of concept-distance: A task-oriented evalua-
tion. Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sydney, Australia.
P. Resnik. 1995. Using information content to evaluate
semantic similarity. Proceedings of the 14th Inter-
national Joint Conference on Artificial Intelligence
(IJCAI-95), 448?453, Montreal, Canada.
G. Ruge. 1997. Automatic detection of thesaurus re-
lations for information retrieval applications. Foun-
dations of Computer Science: Potential - Theory -
Cognition, LNCS, Volume 1337, 499?506, Springer
Verlag, Berlin, Germany.
S. Shalev-Shwartz, Y. Singer and A. Y. Ng. 2004. On-
line and Batch Learning of Pseudo-Metrics. Proc. of
the International Conference on Machine Learning
(ICML).
M. Schutz and T. Joachims. 2003. Learning a Dis-
tance Metric from Relative Comparisons. Advances
in Neural Information Processing Systems (NIPS)..
J. Weeds, D. Weir and D. McCarthy. 2004. Character-
ising Measures of Lexical Distributional Similarity.
Proc. of COLING 2004, 1015?1021.
K. Q. Weinberger, J. Blitzer and L. K. Saul. 2005.
Distance Metric Learning for Large Margin Nearest
Neighbor Classification. Advances in Neural Infor-
mation Processing Systems (NIPS).
E. P. Xing, A. Y. Ng, M. Jordan and S. Russell 2002.
Distance metric learning with application to cluster-
ing with sideinformation. Advances in Neural Infor-
mation Processing Systems (NIPS).
Y. Yang and J. O. Pedersen. 1997. A Comparative
Study on Feature Selection in Text Categorization.
Proc. of the International Conference on Machine
Learning (ICML), 412?420.
800
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 421?429, Prague, June 2007. c?2007 Association for Computational Linguistics
Bayesian Document Generative Model with Explicit Multiple Topics
Issei Sato
Graduate School of Information Science
and Technology,
The University of Tokyo
sato@r.dl.itc.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center,
The University of Tokyo
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
In this paper, we proposed a novel prob-
abilistic generative model to deal with ex-
plicit multiple-topic documents: Parametric
Dirichlet Mixture Model(PDMM). PDMM
is an expansion of an existing probabilis-
tic generative model: Parametric Mixture
Model(PMM) by hierarchical Bayes model.
PMM models multiple-topic documents by
mixing model parameters of each single
topic with an equal mixture ratio. PDMM
models multiple-topic documents by mix-
ing model parameters of each single topic
with mixture ratio following Dirichlet dis-
tribution. We evaluate PDMM and PMM
by comparing F-measures using MEDLINE
corpus. The evaluation showed that PDMM
is more effective than PMM.
1 Introduction
Documents, such as those seen on Wikipedia and
Folksonomy, have tended to be assigned with ex-
plicit multiple topics. In this situation, it is impor-
tant to analyze a linguistic relationship between doc-
uments and the assigned multiple topics . We at-
tempt to model this relationship with a probabilistic
generative model. A probabilistic generative model
for documents with multiple topics is a probability
model of the process of generating documents with
multiple topics. By focusing on modeling the gener-
ation process of documents and the assigned multi-
ple topics, we can extract specific properties of doc-
uments and the assigned multiple topics. The model
can also be applied to a wide range of applications
such as automatic categorization for multiple topics,
keyword extraction and measuring document simi-
larity, for example.
A probabilistic generative model for documents
with multiple topics is categorized into the following
two models. One model assumes a topic as a latent
topic. We call this model the latent-topic model. The
other model assumes a topic as an explicit topic. We
call this model the explicit-topic model.
In a latent-topic model, a latent topic indicates
not a concrete topic but an underlying implicit
topic of documents. Obviously this model uses
an unsupervised learning algorithm. Representa-
tive examples of this kind of model are Latent
Dirichlet Allocation(LDA)(D.M.Blei et al, 2001;
D.M.Blei et al, 2003) and Hierarchical Dirichlet
Process(HDP)(Y.W.Teh et al, 2003).
In an explicit-topic model, an explicit topic indi-
cates a concrete topic such as economy or sports, for
example. A learning algorithm for this model is a
supervised learning algorithm. That is, an explicit
topic model learns model parameter using a training
data set of tuples such as (documents, topics). Rep-
resentative examples of this model are Parametric
Mixture Models(PMM1 and PMM2)(Ueda, N. and
Saito, K., 2002a; Ueda, N. and Saito, K., 2002b). In
the remainder of this paper, PMM indicates PMM1
because PMM1 is more effective than PMM2.
In this paper, we focus on the explicit topic model.
In particular, we propose a novel model that is based
on PMM but fundamentally improved.
The remaining part of this paper is organized as
follows. Sections 2 explains terminology used in the
421
following sections. Section 3 explains PMM that is
most directly related to our work. Section 4 points
out the problem of PMM and introduces our new
model. Section 5 evaluates our new model. Section
6 summarizes our work.
2 Terminology
This section explains terminology used in this paper.
K is the number of explicit topics. V is the number
of words in the vocabulary. V = {1, 2, ? ? ? , V } is
a set of vocabulary index. Y = {1, 2, ? ? ? ,K} is a
set of topic index. N is the number of words in a
document. w = (w1, w2, ? ? ? , wN ) is a sequence of
N words where wn denotes the nth word in the se-
quence. w is a document itself and is called words
vector. x = (x1, x2, ? ? ? , xV ) is a word-frequency
vector, that is, BOW(Bag Of Words) representation
where xv denotes the frequency of word v. wvn
takes a value of 1(0) when wn is v ? V (is not
v ? V ). y = (y1, y2, ? ? ? , yK) is a topic vector
into which a document w is categorized, where yi
takes a value of 1(0) when the ith topic is (not) as-
signed with a document w. Iy ? Y is a set of topic
index i, where yi takes a value of 1 in y.
?
i?Iy
and ?i?Iy denote the sum and product for all i in
Iy, respectively. ?(x) is the Gamma function and
? is the Psi function(Minka, 2002). A probabilistic
generative model for documents with multiple top-
ics models a probability of generating a documentw
in multiple topics y using model parameter ?, i.e.,
models P (w|y,?). A multiple categorization prob-
lem is to estimate multiple topics y? of a document
w? whose topics are unknown. The model parame-
ters are learned by documents D = {(wd, yd)}Md=1,
where M is the number of documents.
3 Parametric Mixture Model
In this section, we briefly explain Parametric Mix-
ture Model(PMM)(Ueda, N. and Saito, K., 2002a;
Ueda, N. and Saito, K., 2002b).
3.1 Overview
PMM models multiple-topic documents by mixing
model parameters of each single topic with an equal
mixture ratio, where the model parameter ?iv is the
probability that word v is generated from topic i.
This is because it is impractical to use model param-
eter corresponding to multiple topics whose num-
ber is 2K ? 1(all combination of K topics). PMM
achieved more useful results than machine learn-
ing methods such as Naive Bayes, SVM, K-NN and
Neural Networks (Ueda, N. and Saito, K., 2002a;
Ueda, N. and Saito, K., 2002b).
3.2 Formulation
PMM employs a BOW representation and is formu-
lated as follows.
P (w|y, ?) = ?Vv=1(?(v,y, ?))
xv (1)
? is a K ? V matrix whose element is ?iv =
P (v|yi = 1). ?(v,y, ?) is the probability that word
v is generated from multiple topics y and is de-
fined as the linear sum of hi(y) and ?iv as follows:
?(v,y, ?) =
?K
i=1 hi(y)?iv
hi(y) is a mixture ratio corresponding to topic i
and is formulated as follows:
hi(y) =
yi?K
j=1 yj
,
?K
i=1 hi(y) = 1.
(if yi = 0, then hi(y) = 0)
3.3 Learning Algorithm of Model Parameter
The learning algorithm of model parameter ? in
PMM is an iteration method similar to the EM al-
gorithm. Model parameter ? is estimated by max-
imizing ?Md=1P (wd|yd, ?) in training documents
D = {(wd, yd)}Md=1. Function g corresponding to
a document d is introduced as follows:
gdiv(?) =
h(yd)?iv
?K
j=1 hj(yd)?jv
(2)
The parameters are updated along with the following
formula.
?(t+1)iv =
1
C
(
M?
d
xdvg
d
iv(?
(t)) + ? ? 1) (3)
xdv is the frequency of word v in document d. C
is the normalization term for
?V
v=1 ?iv = 1. ? is
a smoothing parameter that is Laplace smoothing
when ? is set to two. In this paper, ? is set to two
as the original paper.
4 Proposed Model
In this section, firstly, we mention the problem re-
lated to PMM. Then, we explain our solution of the
problem by proposing a new model.
422
4.1 Overview
PMM estimates model parameter ? assuming that
all of mixture ratios of single topic are equal. It is
our intuition that each document can sometimes be
more weighted to some topics than to the rest of the
assigned topics. If the topic weightings are averaged
over all biases in the whole document set, they could
be canceled. Therefore, model parameter ? learned
by PMM can be reasonable over the whole of docu-
ments.
However, if we compute the probability of gener-
ating an individual document, a document-specific
topic weight bias on mixture ratio is to be consid-
ered. The proposed model takes into account this
document-specific bias by assuming that mixture ra-
tio vector pi follows Dirichlet distribution. This is
because we assume the sum of the element in vec-
tor pi is one and each element pii is nonnegative.
Namely, the proposed model assumes model param-
eter of multiple topics as a mixture of model pa-
rameter on each single topic with mixture ratio fol-
lowing Dirichlet distribution. Concretely, given a
document w and multiple topics y , it estimates
a posterior probability distribution P (pi|x, y) by
Bayesian inference. For convenience, the proposed
model is called PDMM(Parametric Dirichlet Mix-
ture Model).
In Figure 1, the mixture ratio(bias) pi =
(pi1, pi2, pi3),
?3
i=1 pii = 1, pii > 0 of three topics is
expressed in 3-dimensional real spaceR3. The mix-
ture ratio(bias) pi constructs 2D-simplex inR3. One
point on the simplex indicates one mixture ratio pi of
the three topics. That is, the point indicates multiple
topics with the mixture ratio. PMM generates doc-
uments assuming that each mixture ratio is equal.
That is, PMM generates only documents with mul-
tiple topics that indicates the center point of the 2D-
simplex in Figure 1. On the contrary, PDMM gen-
erates documents assuming that mixture ratio pi fol-
lows Dirichlet distribution. That is, PDMM can gen-
erate documents with multiple topics whose weights
can be generated by Dirichlet distribution.
4.2 Formulation
PDMM is formulated as follows:
P (w|y, ?, ?)
=
?
P (pi|?, y)?Vv=1(?(v,y, ?, pi))
xvdpi (4)
Figure 1: Topic Simplex for Three Topics
pi is a vector whose element is pii(i ? Iy). pii is a
mixture ratio(bias) of model parameter correspond-
ing to single topic i where pii > 0,
?
i?Iy pii = 1.
pii can be considered as a probability of topic i , i.e.,
pii = P (yi = 1|pi). P (pi|?, y) is a prior distri-
bution of pi whose index i is an element of Iy, i.e.,
i ? Iy. We use Dirichlet distribution as the prior. ?
is a parameter vector of Dirichlet distribution corre-
sponding to pii(i ? Iy). Namely, the formulation is
as follows.
P (pi|?, y) =
?(
?
i?Iy ?i)
?i?Iy?(?i)
?i?Iypi
?i?1
i (5)
?(v,y, ?, pi) is the probability that word v is gener-
ated frommultiple topics y and is denoted as a linear
sum of pii(i ? Iy) and ?iv(i ? Iy) as follows.
?(v,y, ?, pi) =
?
i?Iy
pii?iv (6)
=
?
i?Iy
P (yi = 1|pi)P (v|yi = 1, ?) (7)
4.3 Variational Bayes Method for Estimating
Mixture Ratio
This section explains a method to estimate the
posterior probability distribution P (pi|w, y, ?, ?)
of a document-specific mixture ratio. Basically,
P (pi|w, y, ?, ?) is obtained by Bayes theorem us-
ing Eq.(4). However, that is computationally im-
practical because a complicated integral computa-
tion is needed. Therefore we estimate an approx-
imate distribution of P (pi|w, y, ?, ?) using Varia-
tional Bayes Method(H.Attias, 1999). The concrete
explanation is as follows
423
Use Eqs.(4)(7).
P (w, pi|y, ?, ?) =
P (pi|?, y)?Vv=1(
?
i?Iy
P (yi = 1|pi)P (v|yi = 1, ?))xv
Transform document expression of above equa-
tion into words vector w = (w1, w2, ? ? ? , wN ).
P (w, pi|y, ?, ?) =
P (pi|?, y)?Nn=1
?
in?Iy
P (yin = 1|pi)P (wn|yin = 1, ?)
By changing the order of
?
and ?, we have
P (w, pi|y, ?, ?) =
P (pi|?, y)
?
i?INy
?Nn=1P (yin = 1|pi)P (wn|yin = 1, ?)
(
?
i?INy
?
?
i1?Iy
?
i2?Iy
? ? ?
?
iN?Iy
)
Express yin = 1 as zn = i.
P (w|y, ?, ?) =
? ?
z?INy
P (pi|?, y)?Nn=1P (zn|pi)P (wn|zn, ?)dpi
(
?
z?INy
?
?
z1?Iy
?
z2?Iy
? ? ?
?
zN?Iy
) (8)
Eq.(8) is regarded as Eq.(4) rewritten by introducing
a new latent variable z = (z1, z2, ? ? ? , zN ).
P (w|y, ?, ?) =
? ?
z?INy
P (pi, z, w|y, ?, ?)dpi (9)
Use Eqs.(8)(9)
P (pi, z, w|y, ?, ?)
= P (pi|?, y)?Nn=1P (zn|pi)P (wn|zn, ?) (10)
Hereafter, we explain Variational Bayes Method
for estimating an approximate distribution of
P (pi, z|w, y, ?, ?) using Eq.(10). This approach is
the same as LDA(D.M.Blei et al, 2001; D.M.Blei et
al., 2003). The approximate distribution is assumed
to be Q(pi, z|?, ?). The following assumptions are
introduced.
Q(pi, z|?, ?) = Q(pi|?)Q(z|?) (11)
Q(pi|?) =
?(
?
i?Iy ?i)
?i?Iy?(?i)
?i?Iypi
?i?1
i (12)
Q(z|?) = ?Nn=1Q(zn|?) (13)
Q(zn|?) = ?Ki=1(?ni)
zin (14)
Q(pi|?) is Dirichlet distribution where ? is its pa-
rameter. Q(zn|?) is Multinomial distribution where
?ni is its parameter and indicates the probability
that the nth word of a document is topic i, i.e.
P (yin = 1). z
i
n is a value of 1(0) when zn is (not)
i. According to Eq.(11), Q(pi|?) is regarded as an
approximate distribution of P (pi|w, y, ?, ?)
The log likelihood of P (w|y, ?, ?) is derived as
follows.
logP (w|y, ?, ?)
=
? ?
z?INy
Q(pi, z|?, ?)dpi logP (w|y, ?, ?)
=
? ?
z?INy
Q(pi, z|?, ?) log
P (pi, z, w|y, ?, ?)
Q(pi, z|?, ?)
dpi
+
? ?
z?INy
Q(pi, z|?, ?) log
Q(pi, z|?, ?)
P (pi, z|w, y, ?, ?)
dpi
logP (w|y, ?, ?) = F [Q] + KL(Q,P ) (15)
F [Q] =
? ?
z?INy
Q(pi,z|?,?) log P (pi,z,w|y,?,?)Q(pi,z|?,?) dpi
KL(Q,P ) =
? ?
z?INy
Q(pi,z|?,?) log Q(pi,z|?,?)P (pi,z|w,y,?,?)dpi
KL(Q,P ) is the Kullback-Leibler Divergence
that is often employed as a distance between
probability distributions. Namely, KL(Q,P )
indicates a distance between Q(pi, z|?, ?) and
P (pi, z|w, y, ?, ?). logP (w|y, ?, ?) is not
relevant to Q(pi, z|?, ?). Therefore, Q(pi, z|?, ?)
that maximizes F [Q] minimizes KL(Q,P ),
and gives a good approximate distribution of
P (pi, z|w, y, ?, ?).
We estimate Q(pi, z|?, ?), concretely its param-
eter ? and ?, by maximizingF [Q] as follows.
Using Eqs.(10)(11).
F [Q] =
?
Q(pi|?) logP (pi|?, y)d? (16)
+
? ?
z?INy
Q(pi|?)Q(z|?) log?Nn=1P (zn|pi)d? (17)
+
?
z?INy
Q(z|?) log?Nn=1P (wn|zn, ?) (18)
?
?
Q(pi|?) logQ(pi|?)d? (19)
?
?
z?INy
Q(z|?) logQ(z|?) (20)
424
= log ?(
?
i?Iy ?j)?
?
i?Iy log ?(?i)
+
?
i?Iy(?i ? 1)(?(?i)??(
?
j?Iy ?j))(21)
+
N?
n=1
?
i?Iy
?ni(?(?i)??(
?
j?Iy
?j)) (22)
+
N?
n=1
?
i?Iy
V?
j=1
?niw
j
n log ?ij (23)
? log ?(
?
j?Iy
?j) +
?
i?Iy
log ?(
?
j?Iy
?j)
?
?
i?Iy
(?i ? 1)(?(?i)??(
?
j?Iy
?j)) (24)
?
N?
n=1
?
i?Iy
?ni log ?ni (25)
F [Q] is known to be a function of ?i and ?ni from
Eqs.(21) through (25). Then we only need to re-
solve the maximization problem of nonlinear func-
tion F [Q] with respect to ?i and ?ni. In this case,
the maximization problem can be resolved by La-
grange multiplier method.
First, regard F [Q] as a function of ?i, which
is denoted as F [?i]. Then ,?i does not have con-
straints. Therefore we only need to find the follow-
ing ?i, where
?F [?i]
??i
= 0. The resultant ?i is ex-
pressed as follows.
?i = ?i +
N?
n=1
?ni (i ? Iy) (26)
Second, regard F [Q] as a function of ?ni, which is
denoted asF [?ni]. Then, considering the constraint
that
?
i?Iy ?ni = 1, Lagrange function L[?ni] is ex-
pressed as follows:
L[?ni] = F [?ni] + ?(
?
i?Iy
?ni ? 1) (27)
? is a so-called Lagrange multiplier.
We find the following ?ni where
?L[?ni]
??ni = 0.
?ni =
?iwn
C
exp(?(?i)??(
?
j?Iy
?j)) (i ? Iy)) (28)
C is a normalization term. By Eqs.(26)(28), we ob-
tain the following updating formulas of ?i and ?ni.
?(t+1)i = ?i +
N?
n=1
?(t)ni (i ? Iy) (29)
?(t+1)ni =
?iwn
C
exp(?(?(t+1)i )??(
?
j?Iy
?(t+1)j )) (30)
Using the above updating formulas , we can es-
timate parameters ? and ?, which are specific to a
document w and topics y. Last of all , we show a
pseudo code :vb(w, y) which estimates ? and ?. In
addition , we regard ? , which is a parameter of a
prior distribution of pi, as a vector whose elements
are all one. That is because Dirichlet distribution
where each parameter is one becomes Uniform dis-
tribution.
? Variational Bayes Method for PDMM????
function vb(w, y):
1. Initialize ?i? 1 ?i ? Iy
2. Compute ?(t+1), ?(t+1) using Eq.(29)(30)
3. if ? ?(t+1) ? ?(t) ?< 
& ? ?(t+1) ? ?(t) ?< 
4. then return (?(t+1), ?(t+1)) and halt
5. else t? t + 1 and goto step (2)
????????????????????
4.4 Computing Probability of Generating
Document
PMM computes a probability of generating a docu-
ment w on topics y and a set of model parameter ?
as follows:
P (w|y,?) = ?Vv=1(?(v,y, ?))
xv (31)
?(v,y, ?) is the probability of generating a word
v on topics y that is a mixture of model parame-
ter ?iv(i ? Iy) with an equal mixture ratio. On the
other hand, PDMM computes the probability of gen-
erating a word v on topics y using ?iv(i ? Iy) and
an approximate posterior distribution Q(pi|?) as fol-
lows:
425
?(v,y, ?, ?)
=
?
(
?
i?Iy
pii?iv)Q(pi|?)dpi (32)
=
?
i?Iy
?
piiQ(pi|?)dpi?iv (33)
=
?
i?Iy
p?ii?iv (34)
p?ii =
?
piiQ(pi|?)dpi =
?i?
j?Iy
?j
(C.M.Bishop,
2006)
The above equation regards the mixture ratio of
topics y of a document w as the expectation p?ii(i ?
Iy) of Q(pi|?). Therefore, a probability of gener-
ating w P (w|y,?) is computed with ?(v,y, ?, ?)
estimated in the following manner:
P (w|y,?) = ?Vv=1(?(v,y, ?, ?)))
xv (35)
4.5 Algorithm for Estimating Multiple Topics
of Document
PDMM estimates multiple topics y? maximizing
a probability of generating a document w?, i.e.,
Eq.(35). This is the 0-1 integer problem(i.e., NP-
hard problem), so PDMM uses the same approxi-
mate estimation algorithm as PMM does. But it is
different from PMM?s estimation algorithm in that
it estimates the mixture ratios of topics y by Varia-
tional Bayes Method as shown by vb(w,y) at step 6
in the following pseudo code of the estimation algo-
rithm:
? Topics Estimation Algorithm????????
function prediction(w):
1. Initialize S ? {1, 2, ? ? ? }, yi ? 0 for
i(1, 2 ? ? ? ,K)
2. vmax ? ??
3. while S is not empty do
4. foreach i ? S do
5. yi ? 1, yj?S\i ? 0
6. Compute ? by vb(w, y)
7. v(i)? P (w|y)
8. end foreach
9. i? ? argmax v(i)
10. if v(i?) > vmax
11. yi? ? 1, S ? S\i?, vmax ? v(i?)
12. else
13. return y and halt
????????????????????
5 Evaluation
We evaluate the proposed model by using F-measure
of multiple topics categorization problem.
5.1 Dataset
We use MEDLINE1 as a dataset. In this experiment,
we use five thousand abstracts written in English.
MEDLINE has a metadata set called MeSH Term.
For example, each abstract has MeSH Terms such as
RNAMessenger and DNA-Binding Proteins. MeSH
Terms are regarded as multiple topics of an abstract.
In this regard, however, we use MeSH Terms whose
frequency are medium(100-999). We did that be-
cause the result of experiment can be overly affected
by such high frequency terms that appear in almost
every abstract and such low frequency terms that ap-
pear in very few abstracts. In consequence, the num-
ber of topics is 88. The size of vocabulary is 46,075.
The proportion of documents with multiple topics on
the whole dataset is 69.8%, i.e., that of documents
with single topic is 30.2%. The average of the num-
ber of topics of a document is 3.4. Using TreeTag-
ger2, we lemmatize every word. We eliminate stop
words such as articles and be-verbs.
5.2 Result of Experiment
We compare F-measure of PDMM with that of
PMM and other models.
F-measure(F) is as follows:
F = 2PRP+R , P =
|Nr?Ne|
|Ne|
, R = |Nr?Ne||Nr| .
Nr is a set of relevant topics . Ne is a set of esti-
mated topics. A higher F-measure indicates a better
ability to discriminate topics. In our experiment, we
compute F-measure in each document and average
the F-measures throughout the whole document set.
We consider some models that are distinct in
learning model parameter ?. PDMM learns model
parameter ? by the same learning algorithm as
PMM. NBM learns model parameter ? by Naive
Bayes learning algorithm. The parameters are up-
dated according to the following formula: ?iv =
Miv+1
C . Miv is the number of training documents
where a word v appears in topic i. C is normaliza-
tion term for
?V
v=1 ?iv = 1.
1http://www.nlm.nih.gov/pubs/factsheets/medline.html
2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
426
The comparison of these models with respect to
F-measure is shown in Figure 2. The horizontal axis
is the proportion of test data of dataset(5,000 ab-
stracts). For example, 2% indicates that the number
of documents for learning model is 4,900 and the
number of documents for the test is 100. The vertical
axis is F-measure. In each proportion, F-measure is
an average value computed from five pairs of train-
ing documents and test documents randomly gener-
ated from dataset.
F-measure of PDMM is higher than that of other
methods on any proportion, as shown in Figure
2. Therefore, PDMM is more effective than other
methods on multiple topics categorization.
Figure 3 shows the comparison of models with
respect to F-measure, changing proportion of mul-
tiple topic document for the whole dataset. The pro-
portion of document for learning and test are 40%
and 60%, respectively. The horizontal axis is the
proportion of multiple topic document on the whole
dataset. For example, 30% indicates that the pro-
portion of multiple topic document is 30% on the
whole dataset and the remaining documents are sin-
gle topic , that is, this dataset is almost single topic
document. In 30%. there is little difference of F-
measure among models. As the proportion of mul-
tiple topic and single topic document approaches
90%, that is, multiple topic document, the differ-
ences of F-measure among models become appar-
ent. This result shows that PDMM is effective in
modeling multiple topic document.
Figure 2: F-measure Results
5.3 Discussion
In the results of experiment described in section
5.2, PDMM is more effective than other models in
Figure 3: F-measure Results changing Proportion of
Multiple Topic Document for Dataset
multiple-topic categorization. If the topic weight-
ings are averaged over all biases in the whole of
training documents, they could be canceled. This
cancellation can lead to the result that model pa-
rameter ? learned by PMM is reasonable over the
whole of documents. Moreover, PDMM computes
the probability of generating a document using a
mixture of model parameter, estimating the mixture
ratio of topics. This estimation of the mixture ra-
tios, we think, is the key factor to achieve the re-
sults better than other models. In addition, the es-
timation of a mixture ratio of topics can be effec-
tive from the perspective of extracting features of
a document with multiple topics. A mixture ratio
of topics assigned to a document is specific to the
document. Therefore, the estimation of the mixture
ratio of topics is regarded as a projection from a
word-frequency space of QV where Q is a set of
integer number to a mixture ratio space of topics
[0, 1]K in a document. Since the size of vocabu-
lary is much more than that of topics, the estima-
tion of the mixture ratio of topics is regarded as a
dimension reduction and an extraction of features in
a document. This can lead to analysis of similarity
among documents with multiple topics. For exam-
ple, the estimated mixture ratio of topics [Compara-
tive Study]C[Apoptosis] and [Models,Biological] in
one MEDLINE abstract is 0.656C0.176 and 0.168,
respectively. This ratio can be a feature of this doc-
ument.
Moreover, we can obtain another interesting re-
sults as follows. The estimation of mixture ratios of
topics uses parameter ? in section 4.3. We obtain
interesting results from another parameter ? that
needs to estimate ?. ?ni is specific to a document. A
427
Table 1: Word List of Document X whose Topics are
[Female], [Male] and [Biological Markers]
Ranking Top10 Ranking Bottom10
1(37) biomarkers 67(69) indicate
2(19) Fusarium 68(57) problem
3(20) non-Gaussian 69(45) use
4(21) Stachybotrys 70(75) %
5(7) chrysogenum 71(59) correlate
6(22) Cladosporium 72(17) population
7(3) mould 73(15) healthy
8(35) Aspergillus 7433) response
9(23) dampness 75(56) man
10(24) 1SD 76(64) woman
?ni indicates the probability that a word wn belongs
to topic i in a document. Therefore we can compute
the entropy on wn as follows:
entropy(wn) =
?K
i=1 ?ni log(?ni)
We rank words in a document by this entropy. For
example, a list of words in ascending order of the
entropy in document X is shown in Table 1. A value
in parentheses is a ranking of words in decending or-
der of TF-IDF(= tf ? log(M/df),where tf is term
frequency in a test document, df is document fre-
quency andM is the number of documents in the set
of doucuments for learning model parameters) (Y.
Yang and J. Pederson, 1997) . The actually assigned
topics are [Female] , [Male] and [Biological Mark-
ers], where each estimated mixture ratio is 0.499 ,
0.460 and 0.041, respectively.
The top 10 words seem to be more technical than
the bottom 10 words in Table 1. When the entropy of
a word is lower, the word is more topic-specific ori-
ented, i.e., more technical . In addition, this ranking
of words depends on topics assigned to a document.
When we assign randomly chosen topics to the same
document, generic terms might be ranked higher.
For example, when we rondomly assign the topics
[Rats], [Child] and [Incidence], generic terms such
as ?use? and ?relate? are ranked higher as shown
in Table 2. The estimated mixture ratio of [Rats],
[Child] and [Incidence] is 0.411, 0.352 and 0.237,
respectively.
For another example, a list of words in ascending
order of the entropy in document Y is shown in Ta-
ble 3. The actually assigned topics are Female, An-
imals, Pregnancy and Glucose.. The estimated mix-
ture ratio of [Female], [Animals] ,[Pregnancy] and
Table 2: Word List of Document X whose Topics are
[Rats], [Child] and [Incidence]
Ranking Top10 Ranking Bottom10
1(69) indicate 67(56) man
2(63) relate 68(47) blot
3(53) antigen 69(6) exposure
4(45) use 70(54) distribution
5(3) mould 71(68) evaluate
6(4) versicolor 72(67) examine
7(35) Aspergillus 73(59) correlate
8(7) chrysogenum 74(58) positive
9(8) chartarum 75(1) IgG
10(9) herbarum 76(60) adult
[Glucose] is 0.442, 0.437, 0.066 and 0.055, respec-
tively In this case, we consider assigning sub topics
of actual topics to the same document Y.
Table 4 shows a list of words in document Y as-
signed with the sub topics [Female] and [Animals].
The estimated mixture ratio of [Female] and [An-
imals] is 0.495 and 0.505, respectively. Estimated
mixture ratio of topics is chaged. It is interesting
that [Female] has higher mixture ratio than [Ani-
mals] in actual topics but [Female] has lower mix-
ture ratio than [Animals] in sub topics [Female] and
[Animals]. According to these different mixture ra-
tios, the ranking of words in docment Y is changed.
Table 5 shows a list of words in document Y as-
signed with the sub topics [Pregnancy] and [Glu-
cose]. The estimated mixture ratio of [Pregnancy]
and [Glucose] is 0.502 and 0.498, respectively. It
is interesting that in actual topics, the ranking of
gglucose-insulinh and ?IVGTT? is high in document
Y but in the two subset of actual topics, gglucose-
insulinh and ?IVGTT? cannot be find in Top 10
words.
The important observation known from these ex-
amples is that this ranking method of words in a doc-
ument can be assosiated with topics assigned to the
document. ? depends on ? seeing Eq.(28). This is
because the ranking of words depends on assigned
topics, concretely, mixture ratios of assigned topics.
TF-IDF computed from the whole documents can-
not have this property. Combined with existing the
extraction method of keywords, our model has the
potential to extract document-specific keywords us-
ing information of assigned topics.
428
Table 3: Word List of Document Y whose Ac-
tual Topics are [Femaile],[Animals],[Pregnancy]
and [Glucose]
Ranking Top 10 Ranking Bottom 10
1(2) glucose-insulin 94(93) assess
2(17) IVGTT 95(94) indicate
3(11) undernutrition 96(74) CT
4(12) NR 97(28) %
5(13) NRL 98(27) muscle
6(14) GLUT4 99(85) receive
7(56) pregnant 100(80) status
8(20) offspring 101(100) protein
9(31) pasture 102(41) age
10(32) singleton 103(103) conclusion
Table 4: Word List of Document Y whose Topics are
[Femaile]and [Animals]
Ranking Top 10 Ranking Bottom 10
1(31) pasture 94(65) insulin
2(32) singleton 95(76) reduced
3(33) insulin-signaling 96(27) muscle
4(34) CS 97(74) CT
5(35) euthanasia 98(68) feed
6(36) humane 99(100) protein
7(37) NRE 100(80) status
8(38) 110-term 101(85) receive
9(50) insert 102(41) age
10(11) undernutrition 103(103) conclusion
6 Concluding Remarks
We proposed and evaluated a novel probabilistic
generative models, PDMM, to deal with multiple-
topic documents. We evaluated PDMM and other
models by comparing F-measure using MEDLINE
corpus. The results showed that PDMM is more ef-
fective than PMM. Moreover, we indicate the poten-
tial of the proposed model that extracts document-
specific keywords using information of assigned
topics.
Acknowledgement This research was funded in
part by MEXT Grant-in-Aid for Scientific Research
on Priority Areas ?i-explosion? in Japan.
References
H.Attias 1999. Learning parameters and structure of la-
tent variable models by variational Bayes. in Proc of
Uncertainty in Artificial Intelligence.
C.M.Bishop 2006. Pattern Recognition And Machine
Table 5: Word List of Document Y whose Topics are
[Pregnancy]and [Glucose]
Ranking Top 10 Ranking Bottom 10
1(84) mass 94(18) IVGTT
2(74) CT 95(72) metabolism
3(26) requirement 96(73) metabolic
4(45) intermediary 97(57) pregnant
5(50) insert 98(58) prenatal
6(53) feeding 99(59) fetal
7(55) nutrition 100(3) gestation
8(61) nutrient 101(20) offspring
9(31) pasture 102(65) insulin
10(32) singleton 103(16) glucose
Learning (Information Science and Statistics), p.687.
Springer-Verlag.
D.M. Blei, Andrew Y. Ng, and M.I. Jordan. 2001. Latent
Dirichlet Allocation. Neural Information Processing
Systems 14.
D.M. Blei, Andrew Y. Ng, and M.I. Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Research, vol.3, pp.993-1022.
Minka 2002. Estimating a Dirichlet distribution. Techni-
cal Report.
Y.W.Teh, M.I.Jordan, M.J.Beal, and D.M.Blei. 2003.
Hierarchical dirichlet processes. Technical Report
653, Department Of Statistics, UC Berkeley.
Ueda, N. and Saito, K. 2002. Parametric mixture models
for multi-topic text. Neural Information Processing
Systems 15.
Ueda, N. and Saito, K. 2002. Singleshot detection of
multi-category text using parametric mixture models.
ACM SIG Knowledge Discovery and Data Mining.
Y. Yang and J. Pederson 1997. A comparative study on
feature selection in text categorization. Proc. Interna-
tional Conference on Machine Learning.
429
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1166?1169,
Prague, June 2007. c?2007 Association for Computational Linguistics
Structural Correspondence Learning for Dependency Parsing
Nobuyuki Shimizu
Information Technology Center
University of Tokyo
Tokyo, Japan
shimizu@r.dl.itc.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
Tokyo, Japan
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Following (Blitzer et al, 2006), we present
an application of structural correspondence
learning to non-projective dependency pars-
ing (McDonald et al, 2005). To induce the
correspondences among dependency edges
from different domains, we looked at ev-
ery two tokens in a sentence and examined
whether or not there is a preposition, a de-
terminer or a helping verb between them.
Three binary linear classifiers were trained
to predict the existence of a preposition,
etc, on unlabeled data and we used singu-
lar value decomposition to induce new fea-
tures. During the training, the parser was
trained with these additional features in ad-
dition to these described in (McDonald et
al., 2005). We discriminatively trained our
parser in an on-line fashion using a vari-
ant of the voted perceptron (Collins, 2002;
Collins and Roark, 2004; Crammer and
Singer, 2003).
1 Introduction
We have recently seen growing popularity of depen-
dency parsing. It is no longer rare to see dependency
relations used as features, in tasks such as machine
translation (Ding and Palmer, 2005) and relation ex-
traction (Bunescu and Mooney, 2005). However,
there is one factor that prevents the use of depen-
dency parsing: sparseness of annotated corpora out-
side Wall Street Journal. In many situations we need
to parse sentences from a target domain with no la-
beled data, which is a different distribution from a
source domain where plentiful labeled training data
is available.
In this paper, we investigate the effectiveness of
structural correspondence learning (SCL) (Blitzer
et al, 2006) in the domain adaptation task given by
the CoNLL 2007. They hypothesize that a model
trained in the source domain using this common fea-
ture representation will generalize better to the tar-
get domain, and focus on using unlabeled data from
both the source and target domains to learn a com-
mon feature representation that is meaningful across
both domains.
The paper is structured as follows: in section
2, we review the decoding and learning aspects of
(McDonald et al, 2005), in section 3, structural cor-
respondence learning applied to dependency pars-
ing, and in section 4, we describe the experiments
and the features needed for the CoNLL 2006 shared
task.
2 Non-Projective Dependency Parsing
2.1 Dependency Structure
Let us define x to be a generic sequence of input to-
kens together with their POS tags and other morpho-
logical features, and y to be a generic dependency
structure, that is, a set of edges for x.
A labeled edge is a tuple ?DEPREL, i ? j? where
i is the start point of the edge, j is the end point, and
DEPREL is the label of the edge. The token at i is
the head of the token at j.
Table 1 shows our formulation of a structured pre-
diction problem. Given x, the input tokens and their
features (column 2 and 3, Table 1), the task is to pre-
1166
Index Token POS Labeled Edge
1 John NN ?SUBJ, 2 ? 1?
2 saw VBD ?PRED, 0 ? 2?
3 a DT ?DET, 4 ? 3?
4 dog NN ?OBJ, 2 ? 4?
5 yesterday RB ?ADJU, 2 ? 5?
6 which WDT ?MODWH, 7 ? 6?
7 was VBD ?MODPRED, 4 ? 7?
8 a DT ?DET, 10 ? 8?
9 Yorkshire NN ?MODN, 10 ? 9?
10 Terrier NN ?OBJ, 7 ? 10?
11 . . ?., 10 ? 11?
Table 1: Example Edges
dict y, the set of labeled edges (column 4, Table 1).
In this paper we use the common method of fac-
toring the score of the dependency structure as the
sum of the scores of all the labeled edges. A de-
pendency structure is characterized by its labeled
edges, and for each labeled edge, we have features
and corresponding weights. The score of a depen-
dency structure is the sum of these weights.
For example, let us say we would like to find the
score of the labeled edge ?OBJ, 2 ? 4?. This is the
edge going to the 4th token ?dog? in Table 1. The
features for this edge could be:
? There is an edge starting at saw, with the POS tag VBD,
and the distance between the head and the child is 2. (
head = wordj , headPOS = posj , dist(i, j) = |i? j| )
? There is an edge ending at dog, with the POS tag NN,
and the distance between the head and the child is 2. (
child = wordi, childPOS = posi, dist(i, j) = |i? j| )
In the upcoming section, we explain a decoding
algorithm for the dependency structures, and later
we give a method for learning the weight vector used
in the decoding.
2.2 Maximum Spanning Tree Algorithm
As in (McDonald et al, 2005), we use Chu-Liu-
Edmonds (CLE) algorithm (Chu and Liu, 1965; Ed-
monds, 1967) for decoding. CLE finds the Maxi-
mum Spanning Tree in a directed graph. The follow-
ing is a summary given in (McDonald et al, 2005).
Informally, the algorithm has each vertex in the
graph greedily select the incoming edge with high-
est weight.
Note that the edge is coming from the parent to
the child. That is, given a child node wordj , we are
finding the parent, or the head wordi such that the
edge (i, j) has the highest weight among all i, i 6= j.
If a tree results, then this must be the maximum
spanning tree. If not, there must be a cycle. The
procedure identifies a cycle and contracts it into a
single vertex and recalculates edge weights going
into and out of the cycle. It can be shown that a
maximum spanning tree on the contracted graph is
equivalent to a maximum spanning tree in the orig-
inal graph (Leonidas, 2003). Hence the algorithm
can recursively call itself on the new graph.
2.3 Online Learning
Again following (McDonald et al, 2005), we have
used the single best MIRA (Crammer and Singer,
2003), which is a ?margin aware? variant of percep-
tron (Collins, 2002; Collins and Roark, 2004) for
structured prediction. In short, the update is exe-
cuted when the decoder fails to predict the correct
parse, and we compare the correct parse yt and the
incorrect parse y? suggested by the decoding algo-
rithm. The weights of the features in y? will be low-
ered, and the weights of the features in yt will be
increased accordingly.
3 Domain Adaptation
Following (Blitzer et al, 2006), we present an appli-
cation of structural correspondence learning (SCL)
to non-projective dependency parsing (McDonald
et al, 2005). SCL is a method for adapting a clas-
sifier learned in a source domain to a target domain.
We assume that both domains have unlabeled data,
but only the source domain has labeled training data.
SCL works as follows: 1. Define a set of pivot
features on the unlabeled data from both domains. 2.
Use these pivot features to learn a mapping from the
original feature spaces of both domains to a shared,
low-dimensional real-valued feature space. A high
inner product in this new space indicates a high de-
gree of correspondence. 3. Use both the transformed
and original features from the source domain. 4.
Again using both the transformed and original fea-
tures, test the samples from the target domain. If we
learned a good mapping, then the effectiveness of
the classifier in the source domain should transfer to
the target domain.
To induce the correspondences among depen-
dency edges in the source domain and the target
domain, we looked at every two tokens in a sen-
tence and examined whether or not there is a prepo-
sition, a determiner or a helping verb between them.
Although no edge is present in unlabeled data, the
1167
presence of a preposition indicates that this edge be-
tween the tokens, if existed, will not be a noun mod-
ifier (in English corpus, this label is NMOD). Thus,
this induced feature should correlate with the label
of an edge candidate. We postulate that the label of
an edge candidate, if known, may allow the super-
vised learner to choose the correct edge among the
edge candidates in the target domain.
In the first step, we chose the presence of a prepo-
sition, a determiner or a helping verb between tokens
as pivot features. Then three binary linear classifiers
were trained to predict the existence of a preposi-
tion (prep), determiner (det) and helping verb (hv)
on unlabeled data and obtained a weight vector for
each classifier.
classifierprep(e) = sign(wprep?(e))
classifierdet(e) = sign(wdet?(e))
classifierhv(e) = sign(whv?(e))
The input to the above classifiers is an edge e in-
stead of a whole sentence x. ? is a mapping from
an edge to a feature vector. Since POS tags were
not available in unlabeled data, for pivot predictors,
we took the subset of the features given by an edge.
The features for pivot predictors are listed in Table 2.
The reminder of the features are the same as ones
used in (McDonald et al, 2005).
Using each weight vector as a column, we created
a weight matrix. W = [wprep|wdet|whv ]. And run a
singular value decomposition to induce a lower di-
mensional feature space. W = U?V . We then took
the transpose of the resulting unitary matrix, U?
which maps the original data to the space spanned
by the principal components, and applied it to the
feature vector of every potential edge. The origi-
nal feature vector is
(
fsubset
freminder
)
. We argument the
feature vector with the additional feature induced by
U?. The augmented feature vectors
( fsubset
freminder
U?fsubset
)
were used throughout the training and testing of the
dependency parser.
4 Experiments
Our experiments were conducted on CoNLL-2007
shared task domain adaptation track (Nivre et al,
2007) using treebanks (Marcus et al, 1993; Johans-
son and Nugues, 2007; Kulick et al, 2004).
Given an edge ?DEPREL, i, j?
head?1 = wordi?1
head = wordi
head+1 = wordi+1
child?1 = wordj?1
child = wordj
child+1 = wordj+1
Table 2: Binary Features for Pivot Predictors
4.1 Dependency Relation
The CLE algorithm works on a directed graph with
unlabeled edges. Since the CoNLL shared task
requires the labeling of edges, as a preprocessing
stage, we created a directed complete graph. Then
we labeled each edge with the highest scoring de-
pendency relation. This complete graph was given
to the CLE algorithm and the edge labels were never
altered in the course of finding the maximum span-
ning tree.
4.2 Features
The features we used for pivot predictors to classify
each edge ?DEPREL, i, j? are shown in Table 2. The
index i is the position of the parent and j is that of
the child.
wordj = the word token at the position j.
posj = the coarse part-of-speech at j.
No other features were used beyond the combina-
tions of the word token in Table 2.
The hardware used was an Intel CPU at 3.0 Ghz
with 32 GB of memory, and the software was writ-
ten in C++. While more iterations should help, due
to the time constraints, we were unable to complete
more training. The parser required a few days to
train.
5 Results
Unfortunately, we have discovered a bug in our
codes after submitting our results for the blind tests,
and the reported results in (Nivre et al, 2007) were
not representative of our approach. The current re-
sults (closed class) are shown in Table 3.
For the explanations of Labeled Attachment
Score, Unlabeled Attachment Score and Label Ac-
curacy, the readers are suggested to refer to the
shared task introductory paper (Nivre et al, 2007).
WSJ represents the application of the parser without
SCL to the source domain test set, and WSJ-SCL
the parser with SCL to the same test set. Similarily
1168
Domain LAS UAS Label Accuracy
WSJ 83.01% ? 83.43% 86.43% ? 86.81% 88.77% ? 88.99%
WSJ-SCL 83.43% ? 83.59% 86.87% ? 86.93% 88.75% ? 89.01%
Chem 74.75% ? 75.18% 80.74% ? 81.24% 82.34% ? 82.70%
Chem-SCL 75.04% ? 74.91% 81.02% ? 80.82% 82.18% ? 82.18%
Table 3: Labeled Attachment Score, Unlabeled Attachment Score and Label Accuracy
Chem and Chem-SCL represents the application of
the parser without SCL and with SCL to the source
domain test set respectively. We did batch learn-
ing by running the online algorithm 4 times. An
arrow ? indicates how the results after 2nd itera-
tion changed at the end of 4th iteration. Contrary
to our expectations, we seem to see SCL overfitting
to the source domain WSJ in this experiment. Due
to the lack of POS tags in unlabeled data, our fea-
ture set for pivot predictors uses tokens extensively
unlike that for the dependency parser. Since tokens
are not as abstract as POS tags, we suspect induced
features may have caused overfitting.
6 Conclusion
We presented an application of structural correspon-
dence learning to non-projective dependency pars-
ing. Effectiveness of SCL for domain adaptation is
mixed in this experiment perhaps due to the mis-
match between feature sets. Future work includes
use of more sophisticated features such as POS and
other morphological features, possibly a joint do-
main adaptation of POS tagging and dependency
parsing for unlabeled data as well as re-examination
of pivot features.
References
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of Empirical Methods in Natural Language Pro-
cessing (EMNLP).
R. Bunescu and R. Mooney. 2005. A shortest path de-
pendency kernel for relation extraction. In Proc. of
the Joint Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. In Science Sinica, page
14:13961400.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of the 42rd Annual
Meeting of the ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. In JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. of the 43rd Annual Meeting of the ACL.
J. Edmonds. 1967. Optimum branchings. In Journal of
Research of the National Bureau of Standards, page
71B:233240.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
G. Leonidas. 2003. Arborescence optimization problems
solvable by edmonds algorithm. In Theoretical Com-
puter Science, page 301:427 437.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Joint Conf. on Hu-
man Language Technology and Empirical Methods in
Natural Language Processing (HLT/EMNLP).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
1169
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 603?611,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deterministic shift-reduce parsing for unification-based grammars by 
using default unification 
 
 
Takashi Ninomiya 
Information Technology Center 
University of Tokyo, Japan 
ninomi@r.dl.itc.u-tokyo.ac.jp 
Takuya Matsuzaki 
Department of Computer Science 
University of Tokyo, Japan 
matuzaki@is.s.u-tokyo.ac.jp 
  
Nobuyuki Shimizu 
Information Technology Center 
University of Tokyo, Japan 
shimizu@r.dl.itc.u-tokyo.ac.jp 
Hiroshi Nakagawa 
Information Technology Center 
University of Tokyo, Japan 
nakagawa@dl.itc.u-tokyo.ac.jp
 
 
Abstract 
Many parsing techniques including pa-
rameter estimation assume the use of a 
packed parse forest for efficient and ac-
curate parsing.  However, they have sev-
eral inherent problems deriving from the 
restriction of locality in the packed parse 
forest.  Deterministic parsing is one of 
solutions that can achieve simple and fast 
parsing without the mechanisms of the 
packed parse forest by accurately choos-
ing search paths.  We propose (i) deter-
ministic shift-reduce parsing for unifica-
tion-based grammars, and (ii) best-first 
shift-reduce parsing with beam threshold-
ing for unification-based grammars.  De-
terministic parsing cannot simply be ap-
plied to unification-based grammar pars-
ing, which often fails because of its hard 
constraints.  Therefore, it is developed by 
using default unification, which almost 
always succeeds in unification by over-
writing inconsistent constraints in gram-
mars. 
1 Introduction 
Over the last few decades, probabilistic unifica-
tion-based grammar parsing has been investi-
gated intensively.  Previous studies (Abney, 
1997; Johnson et al, 1999; Kaplan et al, 2004; 
Malouf and van Noord, 2004; Miyao and Tsujii, 
2005; Riezler et al, 2000) defined a probabilistic 
model of unification-based grammars, including 
head-driven phrase structure grammar (HPSG), 
lexical functional grammar (LFG) and combina-
tory categorial grammar (CCG), as a maximum 
entropy model (Berger et al, 1996).  Geman and 
Johnson (Geman and Johnson, 2002) and Miyao 
and Tsujii (Miyao and Tsujii, 2002) proposed a 
feature forest, which is a dynamic programming 
algorithm for estimating the probabilities of all 
possible parse candidates.  A feature forest can 
estimate the model parameters without unpack-
ing the parse forest, i.e., the chart and its edges.  
Feature forests have been used successfully 
for probabilistic HPSG and CCG (Clark and Cur-
ran, 2004b; Miyao and Tsujii, 2005), and its 
parsing is empirically known to be fast and accu-
rate, especially with supertagging (Clark and 
Curran, 2004a; Ninomiya et al, 2007; Ninomiya 
et al, 2006).  Both estimation and parsing with 
the packed parse forest, however, have several 
inherent problems deriving from the restriction 
of locality.  First, feature functions can be de-
fined only for local structures, which limit the 
parser?s performance.  This is because parsers 
segment parse trees into constituents and factor 
equivalent constituents into a single constituent 
(edge) in a chart to avoid the same calculation.  
This also means that the semantic structures must 
be segmented.  This is a crucial problem when 
we think of designing semantic structures other 
than predicate argument structures, e.g., syn-
chronous grammars for machine translation.  The 
size of the constituents will be exponential if the 
semantic structures are not segmented.  Lastly, 
we need delayed evaluation for evaluating fea-
ture functions.  The application of feature func-
tions must be delayed until all the values in the 
603
segmented constituents are instantiated.  This is 
because values in parse trees can propagate any-
where throughout the parse tree by unification.  
For example, values may propagate from the root 
node to terminal nodes, and the final form of the 
terminal nodes is unknown until the parser fi-
nishes constructing the whole parse tree.  Conse-
quently, the design of grammars, semantic struc-
tures, and feature functions becomes complex.  
To solve the problem of locality, several ap-
proaches, such as reranking (Charniak and John-
son, 2005), shift-reduce parsing (Yamada and 
Matsumoto, 2003), search optimization learning 
(Daum? and Marcu, 2005) and sampling me-
thods (Malouf and van Noord, 2004; Nakagawa, 
2007), were studied. 
In this paper, we investigate shift-reduce pars-
ing approach for unification-based grammars 
without the mechanisms of the packed parse for-
est.  Shift-reduce parsing for CFG and dependen-
cy parsing have recently been studied (Nivre and 
Scholz, 2004; Ratnaparkhi, 1997; Sagae and La-
vie, 2005, 2006; Yamada and Matsumoto, 2003), 
through approaches based essentially on deter-
ministic parsing.  These techniques, however, 
cannot simply be applied to unification-based 
grammar parsing because it can fail as a result of 
its hard constraints in the grammar.  Therefore, 
in this study, we propose deterministic parsing 
for unification-based grammars by using default 
unification, which almost always succeeds in 
unification by overwriting inconsistent con-
straints in the grammars.  We further pursue 
best-first shift-reduce parsing for unification-
based grammars. 
Sections 2 and 3 explain unification-based 
grammars and default unification, respectively.  
Shift-reduce parsing for unification-based gram-
mars is presented in Section 4.  Section 5 dis-
cusses our experiments, and Section 6 concludes 
the paper. 
2 Unification-based grammars 
A unification-based grammar is defined as a pair 
consisting of a set of lexical entries and a set of 
phrase-structure rules.  The lexical entries ex-
press word-specific characteristics, while the 
phrase-structure rules describe constructions of 
constituents in parse trees.  Both the phrase-
structure rules and the lexical entries are 
represented by feature structures (Carpenter, 
1992), and constraints in the grammar are forced 
by unification.  Among the phrase-structure rules, 
a binary rule is a partial function: ? ? ? ? ? , 
where ? is the set of all possible feature struc-
tures.  The binary rule takes two partial parse 
trees as daughters and returns a larger partial 
parse tree that consists of the daughters and their 
mother.  A unary rule is a partial function: 
? ? ?, which corresponds to a unary branch. 
In the experiments, we used an HPSG (Pollard 
and Sag, 1994), which is one of the sophisticated 
unification-based grammars in linguistics.  Gen-
erally, an HPSG has a small number of phrase-
structure rules and a large number of lexical en-
tries.  Figure 1 shows an example of HPSG pars-
ing of the sentence, ?Spring has come.?  The up-
per part of the figure shows a partial parse tree 
for ?has come,? which is obtained by unifying 
each of the lexical entries for ?has? and ?come? 
with a daughter feature structure of the head-
complement rule.  Larger partial parse trees are 
obtained by repeatedly applying phrase-structure 
rules to lexical/phrasal partial parse trees.  Final-
ly, the parse result is output as a parse tree that 
dominates the sentence. 
3 Default unification 
Default unification was originally investigated in 
a series of studies of lexical semantics, in order 
to deal with default inheritance in a lexicon.  It is 
also desirable, however, for robust processing, 
because (i) it almost always succeeds and (ii) a 
feature structure is relaxed such that the amount 
of information is maximized (Ninomiya et al, 
2002).  In our experiments, we tested a simpli-
fied version of Copestake?s default unification.  
Before explaining it, we first explain Carpenter?s 
 
Figure 1: Example of HPSG parsing. 
 
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
HEAD  noun
SUBJ  <      SUBJ  <>     >
COMPS <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
head-comp
Spring has come
1
1 12
2
HEAD  verb
SUBJ  <>
COMPS <>
HEAD  noun
SUBJ  <>
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <>
HEAD  verb
SUBJ  <   >
COMPS <   >
HEAD  verb
SUBJ  <   >
COMPS <>
subject-head
head-comp
Spring has come
1
1 11 2
2
604
two definitions of default unification (Carpenter, 
1993). 
 
(Credulous Default Unification) 
? ?? ? ? =  ?? ? ????
? ? ? is maximal such
that ? ? ? ?is defined ? 
 
(Skeptical Default Unification) 
? ?? ? ? =  ?(? ?
?
? ?) 
 
?  is called a strict feature structure, whose in-
formation must not be lost, and ? is called a de-
fault feature structure, whose information can be 
lost but as little as possible so that ? and ? can 
be unified. 
Credulous default unification is greedy, in that 
it tries to maximize the amount of information 
from the default feature structure, but it results in 
a set of feature structures.  Skeptical default un-
ification simply generalizes the set of feature 
structures resulting from credulous default unifi-
cation.  Skeptical default unification thus leads to 
a unique result so that the default information 
that can be found in every result of credulous 
default unification remains.  The following is an 
example of skeptical default unification: 
 
[F: ?]  ?? ? ?
F: 1 ?
G: 1
H: ?
? =  ???
F: ?
G: ?
H: ?
? , ?
F: 1 ?
G: 1
H: ?
?? = ?
F: ?
G: ?
H: ?
?. 
 
Copestake mentioned that the problem with 
Carpenter?s default unification is its time com-
plexity (Copestake, 1993).  Carpenter?s default 
unification takes exponential time to find the op-
timal answer, because it requires checking the 
unifiability of the power set of constraints in a 
default feature structure.  Copestake thus pro-
posed another definition of default unification, as 
follows. Let ??(?) be a function that returns a 
set of path values in ?, and let ??(?) be a func-
tion that returns a set of path equations, i.e., in-
formation about structure sharing in ?. 
 
(Copestake?s default unification) 
? ?? ? ? =  ? ? ? ???
? ? ??(?)and there is no ?? ? ??(?)
such that ? ? ??is defined and
? ? ? ? ??is not defined
?, 
where ? = ? ? ???(?). 
 
Copestake?s default unification works effi-
ciently because all path equations in the default 
feature structure are unified with the strict fea-
ture structures, and because the unifiability of 
path values is checked one by one for each node 
in the result of unifying the path equations.  The 
implementation is almost the same as that of 
normal unification, but each node of a feature 
structure has a set of values marked as ?strict? or 
?default.?  When types are involved, however, it 
is not easy to find unifiable path values in the 
default feature structure.  Therefore, we imple-
mented a more simply typed version of Corpes-
take?s default unification. 
Figure 2 shows the algorithm by which we 
implemented the simply typed version.  First, 
each node is marked as ?strict? if it belongs to a 
strict feature structure and as ?default? otherwise. 
The marked strict and default feature structures 
procedure forced_unification(p, q) 
   queue := {?p, q?}; 
   while( queue is not empty ) 
      ?p, q? := shift(queue); 
      p := deref(p); q := deref(q); 
      if p ? q 
         ?(p) ?  ?(p) ? ?(q); 
         ?(q) ? ptr(p); 
         forall f ? feat(p)? feat(q) 
            if f ? feat(p) ? f ? feat(q) 
               queue := queue ? ??(f, p), ?(f, q)?; 
            if f ? feat(p) ? f ? feat(q) 
               ?(f, p) ?  ?(f, q); 
procedure mark(p, m) 
   p := deref(p); 
   if p has not been visited 
      ?(p) := {??(p),m?}; 
      forall f ? feat(p) 
         mark(?(f, p), m); 
procedure collapse_defaults(p) 
   p := deref(p); 
   if p has not been visited 
      ts := ?; td := ?; 
      forall ?t, ??????? ? ?(p) 
         ts := ts ? t; 
      forall ?t, ???????? ? ?(p) 
         td := td ? t; 
      if ts is not defined 
         return false; 
      if ts ? td is defined 
         ?(p) := ts ? td; 
      else 
         ?(p) := ts; 
      forall f ? feat(p) 
         collapse_defaults(?(f, p)); 
procedure default_unification(p, q) 
   mark(p, ??????); 
   mark(q, ???????); 
   forced_unification(p, q); 
   collapse_defaults(p); 
 
?(p) is (i) a single type, (ii) a pointer, or (iii) a set of pairs of 
types and markers in the feature structure node p. 
A marker indicates that the types in a feature structure node 
originally belong to the strict feature structures or the default 
feature structures. 
A pointer indicates that the node has been unified with other 
nodes and it points the unified node.  A function deref tra-
verses pointer nodes until it reaches to non-pointer node. 
?(f, p) returns a feature structure node which is reached by 
following a feature f from p. 
 
Figure 2: Algorithm for the simply typed ver-
sion of Corpestake?s default unification. 
605
are unified, whereas the types in the feature 
structure nodes are not unified but merged as a 
set of types.  Then, all types marked as ?strict? 
are unified into one type for each node.  If this 
fails, the default unification also returns unifica-
tion failure as its result.  Finally, each node is 
assigned a single type, which is the result of type 
unification for all types marked as both ?default? 
and ?strict? if it succeeds or all types marked 
only as ?strict? otherwise. 
4 Shift-reduce parsing for unification-
based grammars 
Non-deterministic shift-reduce parsing for unifi-
cation-based grammars has been studied by Bris-
coe and Carroll (Briscoe and Carroll, 1993).  
Their algorithm works non-deterministically with 
the mechanism of the packed parse forest, and 
hence it has the problem of locality in the packed 
parse forest.  This section explains our shift-
reduce parsing algorithms, which are based on 
deterministic shift-reduce CFG parsing (Sagae 
and Lavie, 2005) and best-first shift-reduce CFG 
parsing (Sagae and Lavie, 2006).  Sagae?s parser 
selects the most probable shift/reduce actions and 
non-terminal symbols without assuming explicit 
CFG rules.  Therefore, his parser can proceed 
deterministically without failure.  However, in 
the case of unification-based grammars, a deter-
ministic parser can fail as a result of its hard con-
straints in the grammar.  We propose two new 
shift-reduce parsing approaches for unification-
based grammars: deterministic shift-reduce pars-
ing and shift-reduce parsing by backtracking and 
beam search.  The major difference between our 
algorithm and Sagae?s algorithm is that we use 
default unification.  First, we explain the deter-
ministic shift-reduce parsing algorithm, and then 
we explain the shift-reduce parsing with back-
tracking and beam search. 
4.1 Deterministic shift-reduce parsing for 
unification-based grammars 
The deterministic shift-reduce parsing algorithm 
for unification-based grammars mainly compris-
es two data structures: a stack S, and a queue W.  
Items in S are partial parse trees, including a lex-
ical entry and a parse tree that dominates the 
whole input sentence.  Items in W are words and 
POSs in the input sentence.  The algorithm de-
fines two types of parser actions, shift and reduce, 
as follows. 
? Shift: A shift action removes the first item 
(a word and a POS) from W.  Then, one 
lexical entry is selected from among the 
candidate lexical entries for the item.  Fi-
nally, the selected lexical entry is put on 
the top of the stack. 
Common features: Sw(i), Sp(i), Shw(i), Shp(i), Snw(i), Snp(i), 
Ssy(i), Shsy(i), Snsy(i), wi-1, wi,wi+1, pi-2, pi-1, pi, pi+1, 
pi+2, pi+3 
Binary reduce features: d, c, spl, syl, hwl, hpl, hll, spr, syr, 
hwr, hpr, hlr 
Unary reduce features: sy, hw, hp, hl 
 
Sw(i) ? head word of i-th item from the top of the stack 
Sp(i) ? head POS of i-th item from the top of the stack 
Shw(i) ? head word of the head daughter of i-th item from the 
top of the stack 
Shp(i) ? head POS of the head daughter of i-th item from the 
top of the stack 
Snw(i) ? head word of the non-head daughter of i-th item 
from the top of the stack 
Snp(i) ? head POS of the non-head daughter of i-th item from 
the top of the stack 
Ssy(i) ? symbol of phrase category of the i-th item from the 
top of the stack 
Shsy(i) ? symbol of phrase category of the head daughter of 
the i-th item from the top of the stack 
Snsy(i) ? symbol of phrase category of the non-head daughter 
of the i-th item from the top of the stack 
d ? distance between head words of daughters 
c ? whether a comma exists between daughters and/or inside 
daughter phrases 
sp ? the number of words dominated by the phrase 
sy ? symbol of phrase category 
hw ? head word 
hp ? head POS 
hl ? head lexical entry 
 
Figure 3: Feature templates. 
Shift Features 
  [Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [wi-1, wi] [wi, wi+1] [pi-1, 
wi] [pi, wi] [pi+1, wi] [pi, pi+1, pi+2, pi+3] [pi-2, pi-1, pi] 
[pi-1, pi, pi+1] [pi, pi+1, pi+2] [pi-2, pi-1] [pi-1, pi] [pi, 
pi+1] [pi+1, pi+2] 
 
Binary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [d,c,hw,hp,hl] [d,c,hw,hp] [d, 
c, hw, hl] [d, c, sy, hw] [c, sp, hw, hp, hl] [c, sp, hw, hp] [c, 
sp, hw,hl] [c, sp, sy, hw] [d, c, hp, hl] [d, c, hp] [d, c, hl] [d, 
c, sy] [c, sp, hp, hl] [c, sp, hp] [c, sp, hl] [c, sp, sy] 
 
Unary Reduce Features 
[Sw(0)] [Sw(1)] [Sw(2)] [Sw(3)] [Sp(0)] [Sp(1)] [Sp(2)] 
[Sp(3)] [Shw(0)] [Shw(1)] [Shp(0)] [Shp(1)] [Snw(0)] 
[Snw(1)] [Snp(0)] [Snp(1)] [Ssy(0)] [Ssy(1)] [Shsy(0)] 
[Shsy(1)] [Snsy(0)] [Snsy(1)] [d] [wi-1] [wi] [wi+1] [pi-2] 
[pi-1] [pi] [pi+1] [pi+2] [pi+3] [hw, hp, hl] [hw, hp] [hw, hl] 
[sy, hw] [hp, hl] [hp] [hl] [sy]
 
Figure 4: Combinations of feature templates. 
606
? Binary Reduce: A binary reduce action 
removes two items from the top of the 
stack.  Then, partial parse trees are derived 
by applying binary rules to the first re-
moved item and the second removed item 
as a right daughter and left daughter, re-
spectively.  Among the candidate partial 
parse trees, one is selected and put on the 
top of the stack. 
? Unary Reduce: A unary reduce action re-
moves one item from the top of the stack.  
Then, partial parse trees are derived by 
applying unary rules to the removed item.  
Among the candidate partial parse trees, 
one is selected and put on the top of the 
stack. 
Parsing fails if there is no candidate for selec-
tion (i.e., a dead end).  Parsing is considered suc-
cessfully finished when W is empty and S has 
only one item which satisfies the sentential con-
dition: the category is verb and the subcategori-
zation frame is empty.  Parsing is considered a 
non-sentential success when W is empty and S 
has only one item but it does not satisfy the sen-
tential condition. 
In our experiments, we used a maximum en-
tropy classifier to choose the parser?s action.  
Figure 3 lists the feature templates for the clas-
sifier, and Figure 4 lists the combinations of fea-
ture templates.  Many of these features were tak-
en from those listed in (Ninomiya et al, 2007), 
(Miyao and Tsujii, 2005) and (Sagae and Lavie, 
2005), including global features defined over the 
information in the stack, which cannot be used in 
parsing with the packed parse forest.  The fea-
tures for selecting shift actions are the same as 
the features used in the supertagger (Ninomiya et 
al., 2007).  Our shift-reduce parsers can be re-
garded as an extension of the supertagger. 
The deterministic parsing can fail because of 
its grammar?s hard constraints.  So, we use de-
fault unification, which almost always succeeds 
in unification.  We assume that a head daughter 
(or, an important daughter) is determined for 
each binary rule in the unification-based gram-
mar.   Default unification is used in the binary 
rule application in the same way as used in Ni-
nomiya?s offline robust parsing (Ninomiya et al, 
2002), in which a binary rule unified with the 
head daughter is the strict feature structure and 
the non-head daughter is the default feature 
structure, i.e.,  (? ? ?) ?? ??, where R is a bi-
nary rule, H is a head daughter and NH is a non-
head daughter.  In the experiments, we used the 
simply typed version of Copestake?s default un-
ification in the binary rule application1.  Note 
that default unification was always used instead 
of normal unification in both training and evalua-
tion in the case of the parsers using default unifi-
cation.  Although Copestake?s default unification 
almost always succeeds, the binary rule applica-
tion can fail if the binary rule cannot be unified 
with the head daughter, or inconsistency is 
caused by path equations in the default feature 
structures.  If the rule application fails for all the 
binary rules, backtracking or beam search can be 
used for its recovery as explained in Section 4.2.  
In the experiments, we had no failure in the bi-
nary rule application with default unification. 
4.2 Shift-reduce parsing by backtracking 
and beam-search 
Another approach for recovering from the pars-
ing failure is backtracking.  When parsing fails 
or ends with non-sentential success, the parser?s 
state goes back to some old state (backtracking), 
and it chooses the second best action and tries 
parsing again.  The old state is selected so as to 
minimize the difference in the probabilities for 
selecting the best candidate and the second best 
candidate.  We define a maximum number of 
backtracking steps while parsing a sentence.  
Backtracking repeats until parsing finishes with 
sentential success or reaches the maximum num-
ber of backtracking steps.  If parsing fails to find 
a parse tree, the best continuous partial parse 
trees are output for evaluation. 
From the viewpoint of search algorithms, pars-
ing with backtracking is a sort of depth-first 
search algorithms.  Another possibility is to use 
the best-first search algorithm.  The best-first 
parser has a state priority queue, and each state 
consists of a tree stack and a word queue, which 
are the same stack and queue explained in the 
shift-reduce parsing algorithm.  Parsing proceeds 
by applying shift-reduce actions to the best state 
in the state queue.  First, the best state is re-
                                                 
1 We also implemented Ninomiya?s default unification, 
which can weaken path equation constraints.  In the prelim-
inary experiments, we tested binary rule application given 
as (? ? ?) ?? ?? with Copestake?s default unification, 
(? ? ?) ?? ?? with Ninomiya?s default unification, and 
(? ? ??) ?? ? with Ninomiya?s default unification.  How-
ever, there was no significant difference of F-score among 
these three methods.  So, in the main experiments, we only 
tested (? ? ?) ?? ?? with Copestake?s default unification 
because this method is simple and stable. 
607
moved from the state queue, and then shift-
reduce actions are applied to the state.  The new-
ly generated states as results of the shift-reduce 
actions are put on the queue.  This process re-
peats until it generates a state satisfying the sen-
tential condition.  We define the probability of a 
parsing state as the product of the probabilities of 
selecting actions that have been taken to reach 
the state.  We regard the state probability as the 
objective function in the best-first search algo-
rithm, i.e., the state with the highest probabilities 
is always chosen in the algorithm.  However, the 
best-first algorithm with this objective function 
searches like the breadth-first search, and hence, 
parsing is very slow or cannot be processed in a 
reasonable time.  So, we introduce beam thre-
sholding to the best-first algorithm.  The search 
space is pruned by only adding a new state to the 
state queue if its probability is greater than 1/b of 
the probability of the best state in the states that 
has had the same number of shift-reduce actions.  
In what follows, we call this algorithm beam 
search parsing. 
In the experiments, we tested both backtrack-
ing and beam search with/without default unifi-
cation.  Note that, the beam search parsing for 
unification-based grammars is very slow com-
pared to the shift-reduce CFG parsing with beam 
search.  This is because we have to copy parse 
trees, which consist of a large feature structures, 
in every step of searching to keep many states on 
the state queue.  In the case of backtracking, co-
pying is not necessary. 
5 Experiments 
We evaluated the speed and accuracy of parsing 
with Enju 2.3?, an HPSG for English (Miyao and 
Tsujii, 2005).  The lexicon for the grammar was 
extracted from Sections 02-21 of the Penn Tree-
bank (39,832 sentences).  The grammar consisted 
of 2,302 lexical entries for 11,187 words.  Two 
probabilistic classifiers for selecting shift-reduce 
actions were trained using the same portion of 
the treebank.  One is trained using normal unifi-
cation, and the other is trained using default un-
ification. 
We measured the accuracy of the predicate ar-
gument relation output of the parser.  A predi-
cate-argument relation is defined as a tuple 
??, ??, ?, ???, where ? is the predicate type (e.g., 
  Section 23 (Gold POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non- 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 87.26 86.50 86.88 604 - - - - - 
(Ninomiya et al, 2007) 89.78 89.28 89.53 234 - - - - - 
Ours 
det 76.45 82.00 79.13 122 0 - 867 35 1514 
det+du 87.78 87.45 87.61 256 0 - 0 117 2299 
back40 81.93 85.31 83.59 519 18986 - 386 23 2007 
back10 + du 87.79 87.46 87.62 267 574 - 0 45 2371 
beam(7.4) 86.17 87.77 86.96 510 - 226 369 30 2017 
beam(20.1)+du 88.67 88.79 88.48 457 - 205 0 16 2400 
beam(403.4) 89.98 89.92 89.95 10246 - 2822 71 14 2331 
           
  Section 23 (Auto POS) 
  LP 
(%) 
LR 
(%) 
LF 
(%) 
Avg. 
Time 
(ms) 
# of 
backtrack
Avg. #
of 
states 
# of 
dead 
end 
# of non 
sentential 
success 
# of 
sentential
success 
Previous 
studies 
(Miyao and Tsujii, 2005) 84.96 84.25 84.60 674 - - - - - 
(Ninomiya et al, 2007) 87.28 87.05 87.17 260 - - - - - 
(Matsuzaki et al, 2007)  86.93 86.47 86.70 30 - - - - - 
(Sagae et al, 2007)  88.50 88.00 88.20 - - - - - - 
Ours 
det 74.13 80.02 76.96 127 0 - 909 31 1476 
det+du 85.93 85.72 85.82 252 0 - 0 124 2292 
back40 78.71 82.86 80.73 568 21068 - 438 27 1951 
back10 + du 85.96 85.75 85.85 270 589 - 0 46 2370 
beam(7.4) 83.84 85.82 84.82 544 - 234 421 33 1962 
beam(20.1)+du 86.59 86.36 86.48 550 - 222 0 21 2395 
beam(403.4) 87.70 87.86 87.78 16822 - 4553 89 16 2311 
 
Table 1: Experimental results for Section 23. 
608
adjective, intransitive verb), ?? is the head word 
of the predicate, ? is the argument label (MOD-
ARG, ARG1, ?, ARG4), and ??  is the head 
word of the argument.  The labeled precision 
(LP) / labeled recall (LR) is the ratio of tuples 
correctly identified by the parser, and the labeled 
F-score (LF) is the harmonic mean of the LP and 
LR. This evaluation scheme was the same one 
used in previous evaluations of lexicalized 
grammars (Clark and Curran, 2004b; Hocken-
maier, 2003; Miyao and Tsujii, 2005).  The expe-
riments were conducted on an Intel Xeon 5160 
server with 3.0-GHz CPUs. Section 22 of the 
Penn Treebank was used as the development set, 
and the performance was evaluated using sen-
tences of ? 100 words in Section 23.  The LP, 
LR, and LF were evaluated for Section 23. 
Table 1 lists the results of parsing for Section 
23.  In the table, ?Avg. time? is the average pars-
ing time for the tested sentences.  ?# of backtrack? 
is the total number of backtracking steps that oc-
curred during parsing.  ?Avg. # of states? is the 
average number of states for the tested sentences.  
?# of dead end? is the number of sentences for 
which parsing failed.  ?# of non-sentential suc-
cess? is the number of sentences for which pars-
ing succeeded but did not generate a parse tree 
satisfying the sentential condition.  ?det? means 
the deterministic shift-reduce parsing proposed 
in this paper.  ?back?? means shift-reduce pars-
ing with backtracking at most ? times for each 
sentence.  ?du? indicates that default unification 
was used.  ?beam?? means best-first shift-reduce 
parsing with beam threshold ?.  The upper half 
of the table gives the results obtained using gold 
POSs, while the lower half gives the results ob-
tained using an automatic POS tagger.  The max-
imum number of backtracking steps and the 
beam threshold were determined by observing 
the performance for the development set (Section 
22) such that the LF was maximized with a pars-
ing time of less than 500 ms/sentence (except 
?beam(403.4)?). The performance of 
?beam(403.4)? was evaluated to see the limit of 
the performance of the beam-search parsing. 
Deterministic parsing without default unifica-
tion achieved accuracy with an LF of around 
79.1% (Section 23, gold POS).  With backtrack-
ing, the LF increased to 83.6%.  Figure 5 shows 
the relation between LF and parsing time for the 
development set (Section 22, gold POS).  As 
seen in the figure, the LF increased as the parsing 
time increased.  The increase in LF for determi-
nistic parsing without default unification, how-
ever, seems to have saturated around 83.3%.  
Table 1 also shows that deterministic parsing 
with default unification achieved higher accuracy, 
with an LF of around 87.6% (Section 23, gold 
POS), without backtracking.  Default unification 
is effective: it ran faster and achieved higher ac-
curacy than deterministic parsing with normal 
unification.  The beam-search parsing without 
default unification achieved high accuracy, with 
an LF of around 87.0%, but is still worse than 
deterministic parsing with default unification.  
However, with default unification, it achieved 
the best performance, with an LF of around 
88.5%, in the settings of parsing time less than 
500ms/sentence for Section 22. 
For comparison with previous studies using 
the packed parse forest, the performances of 
Miyao?s parser, Ninomiya?s parser, Matsuzaki?s 
parser and Sagae?s parser are also listed in Table 
1.  Miyao?s parser is based on a probabilistic 
model estimated only by a feature forest.  Nino-
miya?s parser is a mixture of the feature forest 
 
Figure 5: The relation between LF and the average parsing time (Section 22, Gold POS). 
 
82.00%
83.00%
84.00%
85.00%
86.00%
87.00%
88.00%
89.00%
90.00%
0 1 2 3 4 5 6 7 8
LF
Avg. parsing time (s/sentence)
back
back+du
beam
beam+du
609
and an HPSG supertagger.  Matsuzaki?s parser 
uses an HPSG supertagger and CFG filtering.  
Sagae?s parser is a hybrid parser with a shallow 
dependency parser.  Though parsing without the 
packed parse forest is disadvantageous to the 
parsing with the packed parse forest in terms of 
search space complexity, our model achieved 
higher accuracy than Miyao?s parser. 
?beam(403.4)? in Table 1 and ?beam? in Fig-
ure 5 show possibilities of beam-search parsing.  
?beam(403.4)? was very slow, but the accuracy 
was higher than any other parsers except Sagae?s 
parser. 
Table 2 shows the behaviors of default unifi-
cation for ?det+du.?  The table shows the 20 
most frequent path values that were overwritten 
by default unification in Section 22.  In most of 
the cases, the overwritten path values were in the 
selection features, i.e., subcategorization frames 
(COMPS:, SUBJ:, SPR:, CONJ:) and modifiee 
specification (MOD:).  The column of ?Default 
type? indicates the default types which were 
overwritten by the strict types in the column of 
?Strict type,? and the last column is the frequency 
of overwriting.  ?cons? means a non-empty list, 
and ?nil? means an empty list.  In most of the 
cases, modifiee and subcategorization frames 
were changed from empty to non-empty and vice 
versa.  From the table, overwriting of head in-
formation was also observed, e.g., ?noun? was 
changed to ?verb.? 
6 Conclusion and Future Work 
We have presented shift-reduce parsing approach 
for unification-based grammars, based on deter-
ministic shift-reduce parsing.  First, we presented 
deterministic parsing for unification-based 
grammars.  Deterministic parsing was difficult in 
the framework of unification-based grammar 
parsing, which often fails because of its hard 
constraints.  We introduced default unification to 
avoid the parsing failure.  Our experimental re-
sults have demonstrated the effectiveness of de-
terministic parsing with default unification.  The 
experiments revealed that deterministic parsing 
with default unification achieved high accuracy, 
with a labeled F-score (LF) of 87.6% for Section 
23 of the Penn Treebank with gold POSs.  
Second, we also presented the best-first parsing 
with beam search for unification-based gram-
mars.  The best-first parsing with beam search 
achieved the best accuracy, with an LF of 87.0%, 
in the settings without default unification.  De-
fault unification further increased LF from 
87.0% to 88.5%.  By widening the beam width, 
the best-first parsing achieved an LF of 90.0%. 
References 
Abney, Steven P. 1997. Stochastic Attribute-Value 
Grammars. Computational Linguistics, 23(4), 597-
618. 
Path Strict 
type 
Default 
type 
Freq
SYNSEM:LOCAL:CAT:HEAD:MOD: cons nil 434
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD:MOD: cons nil 237
SYNSEM:LOCAL:CAT:VAL:SUBJ: nil cons 231
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: nil cons 125
SYNSEM:LOCAL:CAT:HEAD: verb noun 110
SYNSEM:LOCAL:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:hd:LOCAL:CAT: 
HEAD:MOD: 
cons nil 101
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SPR:hd:LOCAL:CAT:VAL:SPEC:
hd:LOCAL:CAT:HEAD:MOD: 
cons nil 96
SYNSEM:LOCAL:CAT:HEAD:MOD: nil cons 92
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: verb noun 91
SYNSEM:LOCAL:CAT:VAL:SUBJ: cons nil 79
SYNSEM:LOCAL:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:HEAD: noun verbal 77
SYNSEM:LOCAL:CAT:HEAD: nominal verb 75
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:HEAD:MOD: cons nil 74
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:HEAD:MOD: cons nil 69
SYNSEM:LOCAL:CAT:VAL:CONJ:tl:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:CONJ:hd:LOCAL:CAT:VAL:SUBJ: nil cons 64
SYNSEM:LOCAL:CAT:VAL:COMPS:hd:LOCAL:CAT:HEAD: nominal verb 63
SYNSEM:LOCAL:CAT:HEAD:MOD:hd:CAT:VAL:SUBJ: cons nil 63
? ? ? ?
Total   10,598
 
Table 2: Path values overwritten by default unification in Section 22. 
610
Berger, Adam, Stephen Della Pietra, and Vincent Del-
la Pietra. 1996. A Maximum Entropy Approach to 
Natural Language Processing. Computational Lin-
guistics, 22(1), 39-71. 
Briscoe, Ted and John Carroll. 1993. Generalized 
probabilistic LR-Parsing of natural language (cor-
pora) with unification-based grammars. Computa-
tional Linguistics, 19(1), 25-59. 
Carpenter, Bob. 1992. The Logic of Typed Feature 
Structures: Cambridge University Press. 
Carpenter, Bob. 1993. Skeptical and Credulous De-
fault Unification with Applications to Templates 
and Inheritance. In Inheritance, Defaults, and the 
Lexicon. Cambridge: Cambridge University Press. 
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative 
Reranking. In proc. of ACL'05, pp. 173-180. 
Clark, Stephen and James R. Curran. 2004a. The im-
portance of supertagging for wide-coverage CCG 
parsing. In proc. of COLING-04, pp. 282-288. 
Clark, Stephen and James R. Curran. 2004b. Parsing 
the WSJ using CCG and log-linear models. In proc. 
of ACL'04, pp. 104-111. 
Copestake, Ann. 1993. Defaults in Lexical Represen-
tation. In Inheritance, Defaults, and the Lexicon. 
Cambridge: Cambridge University Press. 
Daum?, Hal III and Daniel Marcu. 2005. Learning as 
Search Optimization: Approximate Large Margin 
Methods for Structured Prediction. In proc. of 
ICML 2005. 
Geman, Stuart and Mark Johnson. 2002. Dynamic 
programming for parsing and estimation of sto-
chastic unification-based grammars. In proc. of 
ACL'02, pp. 279-286. 
Hockenmaier, Julia. 2003. Parsing with Generative 
Models of Predicate-Argument Structure. In proc. 
of ACL'03, pp. 359-366. 
Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi 
Chi, and Stefan Riezler. 1999. Estimators for Sto-
chastic ``Unification-Based'' Grammars. In proc. of 
ACL '99, pp. 535-541. 
Kaplan, R. M., S. Riezler, T. H. King, J. T. Maxwell 
III, and A. Vasserman. 2004. Speed and accuracy 
in shallow and deep stochastic parsing. In proc. of 
HLT/NAACL'04. 
Malouf, Robert and Gertjan van Noord. 2004. Wide 
Coverage Parsing with Stochastic Attribute Value 
Grammars. In proc. of IJCNLP-04 Workshop 
``Beyond Shallow Analyses''. 
Matsuzaki, Takuya, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2007. Efficient HPSG Parsing with Supertag-
ging and CFG-filtering. In proc. of IJCAI 2007, pp. 
1671-1676. 
Miyao, Yusuke and Jun'ichi Tsujii. 2002. Maximum 
Entropy Estimation for Feature Forests. In proc. of 
HLT 2002, pp. 292-297. 
Miyao, Yusuke and Jun'ichi Tsujii. 2005. Probabilistic 
disambiguation models for wide-coverage HPSG 
parsing. In proc. of ACL'05, pp. 83-90. 
Nakagawa, Tetsuji. 2007. Multilingual dependency 
parsing using global features. In proc. of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007, pp. 915-932. 
Ninomiya, Takashi, Takuya Matsuzaki, Yusuke 
Miyao, and Jun'ichi Tsujii. 2007. A log-linear 
model with an n-gram reference distribution for ac-
curate HPSG parsing. In proc. of IWPT 2007, pp. 
60-68. 
Ninomiya, Takashi, Takuya Matsuzaki, Yoshimasa 
Tsuruoka, Yusuke Miyao, and Jun'ichi Tsujii. 2006. 
Extremely Lexicalized Models for Accurate and 
Fast HPSG Parsing. In proc. of EMNLP 2006, pp. 
155-163. 
Ninomiya, Takashi, Yusuke Miyao, and Jun'ichi Tsu-
jii. 2002. Lenient Default Unification for Robust 
Processing within Unification Based Grammar 
Formalisms. In proc. of COLING 2002, pp. 744-
750. 
Nivre, Joakim and Mario Scholz. 2004. Deterministic 
dependency parsing of English text. In proc. of 
COLING 2004, pp. 64-70. 
Pollard, Carl and Ivan A. Sag. 1994. Head-Driven 
Phrase Structure Grammar: University of Chicago 
Press. 
Ratnaparkhi, Adwait. 1997. A linear observed time 
statistical parser based on maximum entropy mod-
els. In proc. of EMNLP'97. 
Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and 
Mark Johnson. 2000. Lexicalized Stochastic Mod-
eling of Constraint-Based Grammars using Log-
Linear Measures and EM Training. In proc. of 
ACL'00, pp. 480-487. 
Sagae, Kenji and Alon Lavie. 2005. A classifier-based 
parser with linear run-time complexity. In proc. of 
IWPT 2005. 
Sagae, Kenji and Alon Lavie. 2006. A best-first prob-
abilistic shift-reduce parser. In proc. of COL-
ING/ACL on Main conference poster sessions, pp. 
691-698. 
Sagae, Kenji, Yusuke Miyao, and Jun'ichi Tsujii. 
2007. HPSG parsing with shallow dependency 
constraints. In proc. of ACL 2007, pp. 624-631. 
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis with Support Vector 
Machines. In proc. of IWPT-2003. 
 
611
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 18?19,
Vancouver, October 2005.
WebExperimenter for multiple-choice question generation
Ayako Hoshino
Interfaculty Initiative in Information Studies
University of Tokyo
7-3-1 Hongo, Bunkyo, Tokyo,
113-0033, JAPAN
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
7-3-1 Hongo, Bunkyo, Tokyo,
113-0033, JAPAN
 hoshino,nakagawa@dl.itc.u-tokyo.ac.jp
1 Aim
Automatic generation of multiple-choice questions
is an emerging topic in application of natural lan-
guage processing. Particularly, applying it to lan-
guage testing has been proved to be useful (Sumita
et al, 2005).
This demo presents an novel approach of question
generation using machine learning we have intro-
duced in (Hoshino and Nakagawa, 2005). Our study
aims to generate TOEIC-like 1 multiple choice, fill-
in-the-blank questions from given text using a clas-
sifier trained on a set of human-made questions. The
system comprises of a question pool, which is a
database of questions, an instance converter which
does feature extraction, etc. for machine learning
and a question generator. Each step of learning
and generation is conducted through a web-browser.
Figure 1: A system diagram
The demo serves for the following three purposes;
To facilitates repeating the experiment with different
1TOEIC: Test of English for International Communication
parameters, to demonstrate our method of question
generation by showing the result of each steps, and
to collect the data (training data and the students?
answers) from multiple users in possibly different
places.
2 Processes
An experiment is performed in a sequence of pro-
cesses in each of which the system allows the user to
change input/parameters and shows the result. The
demo follows the processes described in the follow-
ing.
Input Questions
The questions in the question pool are listed on the
browser. The user can modify those questions or add
new ones.
Convert to Instances
Each question in the question pool is automatically
converted into instances each of which represents a
possible blank position.
A sentence is [ ] to instances.
1.convert 2. converted 3. converts 4. conversion
Above question sentence is converted into the fol-
lowing instances, then, features such as POS 2,
lemma, POS of the previous word, POS of the next
word, position-in-sentence, sentence length are as-
signed to each instance in a totally automatic fash-
ion.
We decide a blank position for a question by clas-
sifying an instance into true or false. Temporally,
2Part-of-speech tags are tagged by a modified version of the
Tree Tagger by the University of Stuttgart.
18
the original blank positions are labeled true, and the
shifted ones are labeled as false.
false [ ] sentence is converted to multiple instances.
false A [ ] is converted to multiple instances.
false A sentence [ ] converted to multiple instances.
true A sentence is [ ] to multiple instances.
false A sentence is converted [ ] multiple instances.
false A sentence is converted to [ ] instances.
false A sentence is converted to multiple [ ] .
false A sentence is converted to multiple instances [ ]
First Training
The instances are fed to a classifier selected among
ones of Naive Bayes, K-Nearest Neighbors, Logistic
Regression.
Test on Train
A semi-supervised learning is conducted here for the
purpose of discovering falsely labeled true instances
(which correspond with blank positions shifted from
the original ones, but has the same properties with
true instances) and the labels of those instances are
changed. The classifier is re-trained on the data
with new labels. This process can be iterated sev-
eral times.
Figure 2: A screenshot of a result of test on train
The instances classified as true are shown along
with its temporal label and its certainty value (cer-
tainty for an instance to belong to a class true) given
by the classifier.
Supply Test Data
The user supplies a source text for question genera-
tion from a text area. The test data is converted into
instances in the same way as the training data.
Classify Test
The test instances are classified by the classifier
which has been trained through semi-supervised
learning. True instances which represents blank po-
sition are shown. Instances with a label true are
passed to the next step of deciding distractors, where
instances with false are discarded.
Generate Questions
A set of wrong answers (called distractors) are de-
cided. The user can choose a method of deciding
distractors among WordNet, Edit Distance, Mutual
Information and Random. The resulting four-choice
questions are shown.
Question Session
An interface to collect the students? answers to gen-
erated questions is scheduled. The students? perfor-
mance is used to evaluate the questions.
3 Related Studies
The application of NLP techniques to generation of
multiple-choice questions does not have a long his-
tory. Few attempts had been made before (Mitkov
and Ha, 2003), in which a semi-automatic ques-
tion generation on student?s knowledge of linguis-
tic terms are evaluated. Sumita et al used auto-
matically generated questions to measure test taker?s
proficiency in English (2005). We are proposing
a machine learning approach which depends on a
training on a collection of manually made questions
(Hoshino and Nakagawa, 2005).
References
Ayako Hoshino and Hiroshi Nakagawa. 2005. A real-
time multiple-choice question generation for language
testing: A preliminary study. In Proceedings of the
ACL 2005 The Second Workshop on Building Educa-
tional Applications Using Natural Language Process-
ing, to appear.
Ruslan Mitkov and Le An Ha. 2003. Computer-aided
generation of multiple-choice tests. In Proceedings of
the HLT-NAACL 2003 Workshop on Building Educa-
tional Applications Using Natural Language Process-
ing, pages 17 ? 22, Edmonton, Canada, May.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speaker?s pro-
ficiency of english by using a test with automatically-
generated fill-in-the-blank questions. In Proceedings
of the ACL 2005 The Second Workshop on Build-
ing Educational Applications Using Natural Language
Processing, to appear.
19
 	 
 A Simple but Powerful Automatic Term Extraction Method 
 
Hiroshi Nakagawa 
Information Technology Center,  
The University of Tokyo 
7-3-1, Bunkyo, Hongo 
Tokyo, Japan, 113-0033 
nakagawa@r.dl.itc.u-tokyo.ac.jp 
Tatsunori Mori 
Yokohama National University 
79-5, Tokiwadai,Hodogaya 
Yokohama, Japan,240-0085 
mori@forest.dnj.ynu.ac.jp 
 
Abstract  
In this paper, we propose a new idea for 
the automatic recognition of domain 
specific terms. Our idea is based on the 
statistics between a compound noun and 
its component single-nouns. More 
precisely, we focus basically on how 
many nouns adjoin the noun in question 
to form compound nouns. We propose 
several scoring methods based on this 
idea and experimentally evaluate them on 
the NTCIR1 TMREC test collection. The 
results are very promising especially in 
the low recall area. 
Introduction 
Automatic term recognition, ATR in short, 
aims at extracting domain specific terms 
from a corpus of a certain academic or 
technical domain. The majority of domain 
specific terms are compound nouns, in 
other words, uninterrupted collocations.  
85% of domain specific terms are said to 
be compound nouns. They include 
single-nouns of the remaining 15% very 
frequently as their components, where 
?single-noun? means a noun which could 
not be further divided into several 
shorter and more basic nouns. In other 
words, the majority of compound nouns 
consist of the much smaller number of 
the remaining 15% single-noun terms 
and other single-nouns. In this situation, 
it is natural to pay attention to the 
relation among single-nouns and 
compound nouns, especially how 
single-noun terms contribute to make up 
compound noun terms.  
Another important feature of domain 
specific terms is termhood proposed in 
(Kageura & Umino 96) where ?termhood? 
refers to the degree that a linguistic unit 
is related to a domain-specific concept.  
Thus, what we really have to pursue is an 
ATR method which directly uses the 
notion of termhood.  
Considering these factors, the way of 
making up compound nouns must be 
heavily related to the termhood of the 
compound nouns. The first reason is that 
termhood is usually calculated based on 
term frequency and bias of term 
frequency like inverse document 
frequency. Even though these 
calculations give a good approximation of 
termhood, still they are not directly 
related to termhood because these 
calculations are based on superficial 
statistics. That means that they are not 
necessarily meanings in a writer's mind 
but meanings in actual use. Apparently, 
termhood is intended to reflect this type 
of meaning. The second reason is that if a 
certain single-noun, say N, expresses the 
key concept of a domain that the 
document treats, the writer of the 
document must be using N not only 
frequently but also in various ways. For 
instance, he/she composes quite a few 
compound nouns using N and uses these 
compound nouns in documents he/she 
writes. Thus, we focus on the relation 
among single-nouns and compound nouns 
in pursuing new ATR methods. 
The first attempt to make use of this 
relation has been done by (Nakagawa & 
Mori 98) through the number of distinct 
single-nouns that come to the left or right 
of a single-noun term when used in 
compound noun terms. Using this type of 
number associated with a single-noun 
term, Nakagawa and Mori proposed a 
scoring function for term candidates. 
Their term extraction method however is 
just one example of employing the 
relation among single-nouns and 
compound nouns.  Note that this 
relation is essentially based on a noun 
bigram. In this paper, we expand the 
relation based on noun bigrams that 
might be the components of longer 
compound nouns. Then we 
experimentally evaluate the power of 
several variations of scoring functions 
based on the noun bigram relation using 
the NTCIR1 TMREC test collection. By 
this experimental clarification, we could 
conclude that the single-noun term?s 
power of generating compound noun 
terms is useful and essential in ATR. 
In this paper, section 1 gives the 
background of ATR methods. Section 2 
describes the proposed method of the 
noun bigram based scoring function for 
term extraction. Section 3 describes the 
experimental results and discusses them.  
1 Background 
1.1 Candidates Extraction 
The first thing to do in ATR is to extract 
term candidates from the given text 
corpus. Here we only focus on nouns, 
more precisely a single-noun and a 
compound noun, which are exactly the 
targets of the NTCIR1 TMREC 
task(Kageura et al1999). To extract 
compound nouns which are promising 
term candidates and at the same time to 
exclude undesirable strings such as ?is a? 
or ?of the?, the frequently used method is 
to filter out the words that are members 
of a stop-word-list. More complex 
structures like noun phrases, collocations 
and so on, become focused on (Frantzi 
and Ananiadou 1996). All of these are 
good term candidates in a corpus of a 
specific domain because all of them have 
a strong unithood (Kageura&Umino96) 
which refers to the degree of strength or 
stability of syntagmatic combinations or 
collocations. We assume the following 
about compound nouns or collocations: 
 
Assumption   Terms having complex 
structure a e t  be made of xisting 
simple terms 
r o e
 
The structure of complex terms is 
another important factor for automatic 
term candidates extraction. It is 
expressed syntactically or semantically. 
As a syntactic structure, dependency 
structures that are the results of parsing 
are focused on in many works.  Since we 
focus on these complex structures, the 
first task in extracting term candidates is 
a morphological analysis including part 
of speech (POS) tagging. For Japanese, 
which is an agglutinative language, a 
morphological analysis was carried out 
which segmented words from a sentence 
and did POS tagging (Matsumoto et al 
1996). 
After POS tagging, the complex 
structures mentioned above are extracted 
as term candidates. Previous studies 
have proposed many promising ways for 
this purpose, Hisamitsu(2000) and 
Nakagawa (1998) concentrated their 
efforts on compound nouns. Frantzi and 
Ananiadou (1996) tried to treat more 
general structures like collocations. 
1.2 Scoring 
The next thing to do is to assign a score to 
each term candidate in order to rank 
them in descending order of termhood. 
Many researchers have sought the 
definition of the term candidate?s score 
which approximates termhood. In fact, 
many of those proposals make use of 
surface statistics like tf?idf. Ananiadou et 
al. proposed C-value (Frantzi and 
Ananiadou 1996) and NC-value (Frantzi 
and Ananiadou 1999) which count how 
independently the given compound noun 
is used in the given corpus. Hisamitsu 
(2000) propose a way to measure 
termhood that counts how far the given 
term is different from the distribution of 
non-domain-specific terms. All of them 
tried to capture how important and 
independent a writer regards and uses 
individual terms in a corpus 
2 Single-Noun Bigrams as Components of 
Compound Nouns 
2.1 Single-Noun Bigrams 
The relation between a single-noun and 
complex nouns that include this 
single-noun is very important. 
Nevertheless, to our knowledge, this 
relation has not been paid enough 
attention so far. Nakagawa and Mori 
(1998) proposed a term scoring method 
that utilizes this type of relation. In this 
paper, we extend our idea 
comprehensively. Here we focus on 
compound nouns among the various types 
of complex terms. In technical documents, 
the majority of domain-specific terms are 
noun phrases or compound nouns 
consisting of a small number of single 
nouns. Considering this observation, we 
propose a new scoring method that 
measures the importance of each 
single-noun. In a nutshell, this scoring 
method for a single-noun measures how 
many distinct compound nouns contain a 
particular single-noun as their part in a 
given document or corpus. Here, think 
about the situtation where single-noun N 
occurs with other single-nouns which 
might be a part of many compound nouns 
shown in Figure 1 where [N M] means 
bigram of noun N and M. 
 
[LN1  N] (#L1)         [N  RN1](#R1) 
:                              : 
[LNn  N](LN)         [N  RNm](#Rm) 
Figure 1. Noun Bigram and their Frequency 
In Figure 1, [LNi  N] (i=1,..,n) and [N  
RNj] (j=1,...,m) are single-noun bigrams 
which constitute (parts of) compound 
nouns. #Li and #Rj (i=1,..,n and j=1,..,m) 
mean the frequency of the bigram [LNi 
N] and [N RNj] respectively. Note that 
since we depict only bigrams, compound 
nouns like [LNi N RNj]  which contains 
[LNi  N] and/or [N RNj] as their parts 
might actually occur in a corpus. Again 
this noun trigram might be a part of 
longer compound nouns.  
Let us show an example of a noun bigram. 
Suppose that we extract compound nouns 
including ?trigram? as candidate terms 
from a corpus shown in the following 
example. 
Example 1. 
trigram statistics, word trigram, class 
trigram, word trigram, trigram 
acquisition, word trigram statistics, 
character trigram 
 
Then, noun bigrams consisting of a 
single-noun ?trigram? are shown in the 
following where the number bewteen 
( and ) shows the frequency. 
 
word  trigram (3)  trigram statistics (2) 
class trigram (1)  trigram acquisition (1) 
character trigram(1)                               
Figure 2. An example of noun bigram 
 
We just focus on and utilize single-noun 
bigrams to define the function on which 
scoring is based. Note that we are 
concerned only with single-noun bigrams 
and not with a single-noun per se. The 
reason is that we try to sharply focus on 
the fact that the majority of domain 
specific terms are compound nouns. 
Compound nouns are well analyzed as 
noun bigram. 
2.2 Scoring Function 
2.2.1 The direct score of a noun bigram 
Since a scoring function based on [LNi N] 
or [N RNj] could have an infinite number 
of variations, we here consider the 
following simple but representative 
scoring functions.  
 
#LDN(N) and #RDN(N) : These are the 
number of distinct single-nouns which 
directly precede or succeed N. These are 
exactly ?n? and ?m? in Figure 1. For 
instance, in an example shown in Figure 
2, #LDN(trigram)=3, #RDN(trigram)=2 
 
LN(N,k) and RN(N,k): The general 
functions that take into account the 
number of occurrences of  each noun 
bigram like [LNi N] and [N RNj] are 
defined as follows. 
For instance, if we use LN(N,1) and 
RN(N,1) in example 1, GM(trigram,1) = 
)15()13( +?+  = 4.90. In (3), GM does 
not depend on the length of a compound 
noun that is the number of single-nouns 
within the compound noun. This is 
because we have not yet had any idea 
about the relation between the 
importance of a compound noun and a 
length of the compound noun. It is fair to 
treat all compound nouns, including 
single-nouns, equally no matter how long 
or short each compound noun is. 
?
=
=
LDN(N)#
1i
k     Li)(#k)LN(N,           (1) 
?
=
=
RDN(N)#
1j
k    Rj)(#k)RN(N,           (2) 
We can find various functions by varying 
parameter k of (1) and (2). For instance, 
#LDN(N) and #RDN(N)  can be defined 
as  LN(N,0) and RN(N,0). LN(N,1) and 
RN(N,1) are the frequencies of nouns that 
directly precede or succeed N.  In the 
example shown in Figure 2, for example, 
LN(trigram,1)=5, and RN(trigram,1)=3. 
Now we think about the nature of (1) and 
(2) with various value of the parameter k. 
The larger k is, the more we take into 
account the frequencies of each noun 
bigram. One extreme is the case k=0, 
namely LN(N,0) and RN(N,0), where we 
do not take into account the frequency of 
each noun bigram at all. LN(N,0) and 
RN(N,0) describe how linguistically and 
domain dependently productive the noun 
N is in a given corpus. That means that 
noun N presents a key and/or basic 
concept of the domain treated by the 
corpus. Other extreme cases are large k, 
like k=2 , 4, etc. In these cases, we rather 
focus on frequency of each noun bigram. 
In other words, statistically biased use of 
noun N is the main concern. In the 
example shown in Figure 2, for example, 
LN(trigram,2)=11, and RN(trigram,2)=5. 
If k<0, we discount the frequency of each 
noun bigram. However, this case does not 
show good results of in our ATR 
experiment. 
2.2.3 Combining Compound Noun Frequency 
Information we did not use in the bigram 
based methods described in 2.2.1 and 
2.2.2 is the frequency of single-nouns and 
compound-nouns that occur 
independently, namely left and right 
adjacent words not being nouns. For 
instance,  ?word patterns? occurs 
independently in ?? use the word 
patterns occurring in ? .? Since the 
scoring functions proposed in 2.2.1 are 
noun bigram statistics,  the number of 
this kind of independent occurrences of 
nouns themselves are not used. If we take 
this information into account, a new type 
of information is used and  better results 
are expected.  
In this paper, we employ a very simple 
method for this. We observe that if a 
single-noun or a compound noun occurs 
independently, the score of the noun is 
multiplied by the number of its 
independent occurrences. Then 
GM(CN,k) of the formula (3) is  revised. 
We call this new GM FGM(CN,k)  and 
define it as follows. 
2.2.2 Score of compound nouns 
The next thing to do is to extend the 
scoring functions of a single-noun to the 
scoring functions of a compound noun. We 
adopt a very simple method, namely a 
geometric mean. Now  think about a 
compound noun : CN = N1 N2?N L. Then  
a geometric mean: GM of CN is defined as 
follows. 
 
if N occurs independently  
then f(CN)k)GM(CN,k)FGM(CN, ?=  
where f(CN) means the number of 
independent occurrences of noun CN 
)3(1)k),1)(RN(Nk),(LN(N
k)GM(CN,
L2
1
L
1i
ii ????
?
???
? ++= ?
=
        
(33 +
                     
For instance, in example 1, if we find 
independent ?trigram? three times in the corpus, 
FGM(trigram,1)= 1)(51) +?? =14.70 
---  (4)                
2.2.4 Modified C-value 
We compare our methods with the 
C-value based method(Frantzi and 
Ananiadou 1996) because  1) their 
method is very powerful to extract and 
properly score compound nouns., and 2) 
their method is basically based on 
unithood. On the contrary, our scoring 
functions proposed in 2.2.1 try to capture 
termhood. However the original 
definition of C-value can not score a 
single-noun because the important part 
of the definition C-value is: 
)
c(a)
t(a)-1)(n(a)-length(a)(value(a)-C =   
--- (5) 
where a is compound noun, length(a) is 
the number of single-nouns which make 
up a, n(a) is  the total frequency of 
occurrence of a on the corpus, t(a) is the 
frequency of occurrence of a in longer 
candidate terms, and c(a) is the number 
of those candidate terms. 
As known from (5), all single-noun?s 
C-value come to be 0. The reason why the 
first term of right hand side is 
(length(a)-1) is that C-value originally 
seemed to capture how much 
computational effort is to be made in 
order to recognize the important part of 
the term. Thus, if the length(a)  is 1, we 
do not need any effort to recognize its 
part because the term a is a  single-word 
and does not have its part. But we intend 
to capture how important the term is for 
the writer or reader, namely its termhood. 
In order to make the C-value capture 
termhood, we modify (5) as follows. 
)
c(a)
t(a)-n(a)length(a)(value(a)-MC =  (6) 
Where ?MC-value? means ?Modified 
C-value.? 
3 Experimental Evaluation  
3.1 Experiment 
In our experiment, we use the NTCIR1 
TMREC test collection (Kageura et al
1999). As an activity of TMREC, they 
have provided us with a Japanese test 
collection of a term recognition task. The 
goal of this task is to automatically 
recognize and extract terms from a text 
corpus which contains  1,870 abstracts 
gathered from the computer science and 
communication engineering domain 
corpora of the NACSIS Academic 
Conference Database, and 8,834 
manually collected correct terms. The 
TMREC text corpus is morphologically 
analyzed and POS tagged by hand. From 
this POS tagged text, we extract 
uninterrupted noun sequences as term 
candidates. Actually 16,708 term 
candidates are extracted and several 
scoring methods are applied to them. All 
the extracted term candidates CNs are 
ranked according to their  GM(CN,k), 
FGM(CN,k) and MC-value(CN) in 
descending order. As for parameter k of 
(1) and (2), we choose k=1 because its 
performance is the best among various 
values of k in the range from 0 to 4. Thus, 
henceforth, we omit k from GM and FGM, 
like GM(CN) and FGM(CN). We use 
GM(CN) as the baseline. 
In evaluation, we conduct experiments 
where we pick up the highest ranked 
term candidate down to the PNth highest 
ranked term candidate by these three 
scoring methods, and evaluate the set of 
selected terms with the number of correct 
terms, we call it CT, within it. In the 
following figures, we only show CT 
because recall is CT/8834, where 8834 is 
the number of all correct terms, precision 
is CT/PN.  
Another measure NTCIR1 provides us 
with is the terms which include the 
correct term as its part. We call it ?longer 
term? or LT. They are sometimes valued 
terms and also indicate in what context 
the correct terms are used. Then we also 
use the number of longer terms in our 
evaluation.  
3.2 Results 
In Figure 3 through 5, PN of X-axis 
means PN. 
0
1000
2000
3000
4000
5000
0
50
0
10
00
15
00
20
00
25
00
30
00
PN
Ex
tra
ct
ed
 c
or
re
ct
te
rm
s
GM GM - longer term
 
Figure 3.  CT and LT of GM(CN) for each PN  
-50
0
50
100
150
200
250
300
350
0
40
0
80
0
12
00
16
00
20
00
24
00
28
00
PN
D
iff
re
nc
e 
of
 c
or
re
nt
 te
rm
s
FGM-GM MCvalue-GM
 
Figure 4.  CT of FGM(CN) minus CT of 
GM(CN), and CT of MC-value(CN) minus CT 
of GM(CN) for each PN 
 
-600
-400
-200
0
0
50
0
10
00
15
00
20
00
25
00
30
00
PN
D
iff
re
nc
e 
of
 lo
ng
er
te
rm
s
MCvalue-GM FGM-GM
 
Figure 5.  LT of GM(CN) minus LT of 
FGM(CN) , and LT of GM(CN) minus LT of  
MC-value(CN) for each PN 
In Figure 3, the Y-axis represents CT in 
other words the number of correct terms 
picked up by GM(CN) and the number of 
longer terms picked up by GM(CN) for 
each PN. They are our baseline. The 
Figure 4 shows the difference between CT 
of FGM(CN) and CT of GM(CN) and the 
difference between CT of MC-value(CN) 
and CT of GM(CN)  for each PN. Figure 
5 shows the difference between LT of 
GM(CN) and LT of FGM(CN) or LT of 
MC-value(CN) for each PN. As known 
from Figure 4, FGM based method 
outperforms MC-value up to 1,400 
highest ranked terms. Since in the 
domains of TMREC task that are 
computer science and communication 
engineering, 1,400 technical terms are 
important core terms, FGM method we 
propose is very promising to extract and 
recognize domain specific terms. We also 
show CT of each method for larger PN, 
say, from 3000 up to 15000 in Table 1 and 
2. 
Table 1.  CT of each ranking method for PN 
larger than 3000 
PN GM FGM MC- 
value 
3000 1784 1970 2111 
6000 3286 3456 3671 
9000 4744 4866 4930 
12000 6009 6090 6046 
15000 7042 7081 7068 
Table 2. LT of each ranking method for PN 
larger than 3000 
PN GM FGM MC- 
Value 
3000 2893 2840 2531 
6000 5644 5576 5011 
9000 8218 8152 7578 
12000 10523 10488 9852 
15000 12174 12186 12070 
 
As seen in these figures and tables, if we 
want more terms about these domains, 
MC-value is more powerful, but when PN 
is larger than 12,000, again FGM 
outperforms. As for recognizing longer 
terms, GM(CN), which is the baseline, 
performs best for every PN. MC-value is 
the worst. From this observation we come 
to know that MC-value tends to assign 
higher score to shorter terms than GM or 
FGM. We are also interested in what kind 
of term is favored by each method. For 
this, we show the average length of the 
highest PN ranked terms of each method 
in Figure 6 where length of CN means 
the number of single-words CN consists 
of. Clearly, GM prefers longer terms. So 
does FGM. On the contrary, MC-value 
prefers shorter terms. However, as shown 
in Figure 6, the average length of the 
MC-value is more fluctuating. That 
means GM and FGM have more 
consistent tendency in ranking compound 
nouns. Finally we compare our results 
with NTCIR1 results (Kageura et al
1999). Unfortunately since (Kageura et al
1999) only provides the number of the all 
extracted terms and also the number of 
the all extracted correct terms, we could 
not directly compare our results with 
other NTCIR1 participants. Then, what 
is important is the fact that we extracted 
7,082 correct terms from top 15,000 term 
candidates with the FGM methods. This 
fact is indicating that our methods show 
the highest performance among all other 
participants of NTCIR1 TMREC task 
because 1) the highest number of terms 
within the top 16,000 term candidates is 
6,536 among all the participants of 
NTCIR1 TMREC task, and 2) the highest 
number or terms in all the participants of 
NTCIR1 TMREC task is 7,944, but they 
are extracted from top 23,270 term 
candidates, which means extremely low 
precision.  
0
1
2
3
4
5
0 1000 2000 3000 4000
PN
te
rm
 le
ng
th
GM FGM MC-value
  
Figure 6.  The average length of extracted 
terms by GM(CN), FGM(CN) and 
MC-value(CN) for each PN 
References  
Frantzi, T.K. and Ananiadou, S. 1996. 
?Extracting nested collocations?. In 
Proceedings of 16th International 
Conference on Compu ational 
Linguistics, 41-46. 
t
e
t
e
z
e
Frantzi, T.K. and Ananiadou, S. 1999. 
?The c-value/nc-value method for atr?. 
Journal of Natural Language 
Processing 6(3), 145-179. 
Hisamitsu, T, 2000. ?A Method of 
Measuring Term Representativeness?. 
In Proce dings of 18th International 
Conference on Compu ational 
Linguistics,  320-326. 
Kageura, K. and Umino, B. 1996. 
?Methods of automatic term 
recognition: a review?. Terminology  
3(2), 259-289. 
Conclusion Kageura, K. et al 1999. ?TMREC Task: 
Overview and Evaluation?. In 
Proceedings of the First NTCIR 
Workshop on Research in Japanese 
T xt Retrieval and Term Recognition, 
411-440. 
In this paper, we introduce a new 
ngle-noun bigram based statistical 
methods for ATR, which capture how 
many nouns adjoin the single-noun in 
question to form compound nouns. 
Through experimental evaluation using 
the NTCIR1 TMREC test collection, the 
FGM method we proposed showed the 
best performance in selecting up to 1,400 
domain specific terms.  
Matsumoto, Y., Kurohashi, S., Yamaji, O., 
Taeki, H. and Nagao, M. 1996. 
Instruction Manual of Japanese 
Morphological Analy er JUMAN3.1. 
Nagao Lab. at Kyoto University. 
Nakagawa, H. and Mori, T. 1998. ?Nested 
Collocation and Compound Noun for 
Term Recognition?. In Proceedings of 
the First Workshop on Computational 
T rminology COMPTERM?98, 64-70. 
Acknowledgements 
This research is funded by the Ministry of 
Education Science and Academic, Japan. 
Chinese Term Extraction from Web Pages Based on Compound word 
Productivity 
Hiroshi Nakagawa 
Information Technology Center, The 
University of Tokyo  
7-3-1 Hongou, Bunkyo 
Tokyo, JAPAN, 113-0033 
nakagawa@dl.itc.u-tokyo.ac.jp 
Hiroyuki Kojima*, Akira Maeda** 
Faculty of Economics, The University 
of Tokyo  
7-3-1 Hongou, Bunkyo 
Tokyo, JAPAN, 113-0033 
* kojima@e.u-tokyo.ac.jp,  
**maeda@lib.u-tokyo.ac.jp  
 
Abstract 
In this paper, we propose an automatic term 
recognition system for Chinese. Our idea is 
based on the relation between a compound 
word and its constituents that are simple words 
or individual Chinese character. More 
precisely, we basically focus on how many 
words/characters adjoin the word/character in 
question to form compound words. We also 
take into account the frequency of term. We 
evaluated word based method and character 
based method with several Chinese Web pages, 
resulting in precision of 75% for top ten 
candidate terms. 
1 Introduction 
Automatic term recognition, ATR in short, aims 
at extracting domain specific terms from a corpus 
or Web pages. Domain specific terms are terms 
that express the concept specifically defined in the 
given domain. They are required to have a unique 
meaning in order for efficient communication 
about the topic of the domain. It is, however, 
difficult to decide automatically whether they are 
unique. So we put this issue aside. In terms of 
feasibility, their grammatical status is important, 
for instance part of speeches. Although they are 
not necessarily confined to simple words where 
?simple word? means a word which could not be 
further divided into shorter and more basic words, 
the majority of them are actually compound words,. 
Thus, we here focus on both of simple and 
compound words.  
In terms of text length, even one Web page 
which is not long gives us a number of domain 
specific vocabulary like ?national library?, ?library 
policy? if the Web page is about libraries. If we 
expand domain specific terms to this extent, the 
big portion of domain specific terms are compound 
words. Obviously, the majority of compound 
words consist of relatively small number of distinct 
simple words. In this situation, it is natural to pay 
attention to the relation among compound words 
and their constituent simple words.  
(Kageura and Umino 1996) proposed an 
important feature of domain specific terms called 
termhood which refers to the degree that a 
linguistic unit is related to a domain-specific 
concept. Presumably, it is necessary to develop an 
ATR method that calculates termhood of each term 
candidate extracted from a domain corpus that 
usually consists of a number of documents. Many 
works of ATR use statistics of term candidate 
distribution in a corpus such as term frequency to 
calculate the termhood of every term candidate.  
This frequency based methods, however, heavily 
depend on the size of corpus. Thus we do not 
expect a good result if we extract domain specific 
terms from one or a few Web pages. If we shift our 
focus from a corpus based statistics like frequency 
to term space that consists of all term candidates, 
we expect better result of extracted terms even 
from one Web page because of the following 
reason: A set of term candidates has its own 
structure like relations between compound words 
and their constituent simple words as stated before. 
The statistical information about these relations 
comes from more microscopic structure than term 
frequency. Thus, if we utilize more information 
from term space, it is reasonable to expect better 
performance in extracting from a small text like 
one Web page. Without this kind of information, 
we will be suffering from the shortage of 
information for ATR. 
Now look at frequency based information and 
information inherent with term space more closely. 
Even though several kinds of statistics about actual 
use in a corpus such as term frequency give a good 
approximation of termhood. They are not 
necessarily meanings in a writer's mind. On the 
contrary, the statistics of term space can reflect the 
meaning in a writer?s mind because it is up to a 
writer?s decision how to make a compound word 
term to express a complicated concept using 
simple word terms as its components. More 
precisely, if a certain simple word, say N, 
expresses the basic concept of a domain that the 
document treats, the writer of the document, we 
expect, uses N not only many times but in various 
ways. One of typical way of this kind is that he/she 
composes quite a few compound words using N 
and uses these compound words in documents 
he/she writes. For this reason, we have to focus on 
the relation among simple words and compound 
words when pursuing new ATR methods. 
One of the attempts to make use of this relation 
has been done by Nakagawa and Mori (2003). 
Their method is based on the number of distinct 
simple words that come left or right of a simple 
word term to make up compound word terms. In 
this paper, we apply their method to deal with Web 
pages written in Chinese. 
In this paper, section 2 gives the background of 
ATR methods. In section 3 we introduce ATR 
method developed by Nakagawa and Mori(2003). 
Section 4, 5 and 6 are for how to apply their 
method to Chinese language and evaluation of two 
proposed method: 1) Word based method using 
Chinese morphological analyzer ICTCLAS(Zhang, 
Yu, Xiong and Liu. 2003), 2) Stop character based 
method.  
2 Background 
2.1 Typical Procedures of Automatic Term 
Recognition 
An ATR procedure consists of two procedures 
in general. The first one is a procedure of 
extracting term candidates from a corpus. The 
second procedure is to assign each term candidate 
a score that indicates how likely the term candidate 
is a term to be recognized. Then all candidates are 
ranked according to their scores. In the remaining 
part of this section, we describe the background of 
a candidate extraction procedure and a scoring 
procedure respectively. 
2.2 Candidates Extraction 
In term candidates extraction from the given text 
corpus, we mainly focus on compound words as 
well as simple words. To extract compound words 
which are promising term candidates and at the 
same time to exclude undesirable strings such as 
?is a? or ?of the?, the frequently used method is to 
filter out the words that are the member of a stop-
word-list.  
The structure of complex term is another 
important factor for automatic term candidate 
extraction. A syntactic structure that is the result of 
parsing is focused on in many works. Since we 
focus on these complex structures, the first task in 
extracting term candidates is a morphological 
analysis including part of speech (POS) tagging. 
There are no explicit word boundary marker in 
Chinese, we first have to do morphological 
analysis which segments out words from a 
sentence and does POS tagging at the same time. 
After POS tagging, the complex structures 
mentioned above are extracted as term candidates. 
Previous studies have proposed many promising 
ways for this purpose, for instance, Smadja and 
McKeown (1990), and Frantzi and Ananiadou 
(1996) tried to treat more general structures like 
collocations. 
2.3 Scoring 
The next step of ATR is to assign each term 
candidate its score in order to rank them in 
descending order of termhood. Many researchers 
have sought the definition of term candidate?s 
score which approximates termhood. In fact, many 
of those proposals make use of statistics of actual 
use in a corpus such as term frequency which is so 
powerful and simple that many researchers directly 
or indirectly have used it. The combination of term 
frequency and inverse document frequency is also 
well studied i.e. (Uchimoto et al2000), (Fukushige 
and Noguchi 2000). On the other hand, several 
scoring methods that are neither directly nor 
heavily based on frequency of term candidates 
have been proposed. Among those, Ananiadou et 
al. proposed C-value (Frantzi and Ananiadou 
1996) which counts how independently the given 
compound word is used in the given corpus. 
Hisamitsu (2000) proposes a way to measure 
termhood which estimates how far the document 
containing given term is different from the 
distribution of documents not containing the given 
term. However, the method proposed by Nakagawa 
and Mori (2003) outperforms these methods in 
terms of NTCIR1 TMREC task(Kageura, et al 
1999). 
2.4 Chinese Term Extraction 
As for Chinese language NLP, very many works 
about word segmentation were published i.e. (Ma 
and Xia 2003). Nevertheless the term ?Term 
extraction? has not yet been used for Chinese NLP, 
key words extraction have been a target for a long 
time. For instance, key words extraction from news 
articles (Li. et al 2003) is the recent result which 
uses frequency and length of character string for 
scoring. Max-duplicated string based method 
(Yang and Li. 2002) is also promising. In spite of 
previous research efforts, there have been no 
attempt so far to apply the relation between simple 
and compound words to Chinese term extraction, 
and that is exactly what we propose in this paper. 
 
3 Scoring methods with Simple word 
Bigrams 
3.1 Simple word Bigrams 
The relation between a simple word and 
complex words that include the simple word is 
very important in terms of term space structure. 
Nevertheless, to my knowledge, this relation has 
not been paid enough attention so far except for the 
method proposed by Nakagawa and Mori (2003). 
In this paper, taking over their works, we focus on 
compound words among the various types of 
complex terms. In technical documents, the 
majority of domain-specific terms are noun phrases 
or compound words consisting of small size 
vocabulary of simple words. This observation 
leads to a new scoring methods that measures how 
many distinct compound words contain the simple 
word in question as their part in a given document 
or corpus. Here, suppose the situation where 
simple word: N occurs with other simple words as 
a part of many compound words shown in Figure 1 
where [N M] means bigram of noun N and M. 
 
 
[LN1  N] (#L1)           [N  RN1](#R1) 
[LN2  N] (#L2)           [N  RN2](#R2) 
:                              : 
[LNn  N](#Ln)           [N  RNm](#Rm) 
 
Figure 1. Noun Bigram and their Frequency 
 
In Figure 1, [LNi  N] (i=1,..,n) and [N  RNj] 
(j=1,...,m) are simple word bigrams which make (a 
part of) compound words. #Li and #Rj (i=1,..,n and 
j=1,..,m) mean the frequency of the bigram [LNi 
N] and [N RNj] in the corpus respectively. Note 
that since we depict only bigrams, compound 
words like [LNi N RNj]  which contains [LNi  N] 
and/or [N RNj] as their parts might actually occur 
in a corpus. Note that this noun trigram might be a 
part of longer compound words. We show an 
example of a set of noun bigrams. Suppose that we 
extract compound words including ?trigram? as 
term candidates from a corpus as shown in the 
following example. 
 
Example 1. 
trigram statistics, word trigram, class trigram, word 
trigram, trigram acquisition, word trigram statistics, 
character trigram 
 
Then, noun bigrams consisting of a simple word 
?trigram? are shown in Figure 2 where the number 
between ( and ) shows the frequency in the corpus. 
 
 
 
word  trigram (3)      trigram statistics (2) 
class trigram (1)      trigram acquisition (1) 
character trigram(1) 
 
Figure 2. An example of noun bigram 
 
Now we focus on and utilize simple word 
bigrams to define the scoring function. Note that 
we are only concerned with simple word bigrams 
and not with a simple word per se because, as 
stated before, we are concerned with the relation 
between a compound word and its component 
simple words.   
 
3.2 Scoring Function 
3.2.1 Score of simple word 
Since there are infinite number of scoring 
functions based on [LNi N] or [N RNj], we here 
consider the following simple but representative 
scoring functions.  
 
#LDN(N) and #RDN(N) : These are the number 
of distinct simple words which directly precede or 
succeed N. These coincide with ?n? and ?m? in 
Figure 1 respectively. For instance, in an example 
shown in Figure 2, #LDN(trigram)=3, 
#RDN(trigram)=2. 
Using #LDN and #RDN we define LN(N) and 
RN(N): These are based on the number of 
occurrence of each noun bigram, and defined for 
[LNi N] and [N RNj] as follows respectively. 
?
=
=
LDN(N)#
1i
    Li)(#LN(N)                             (1) 
?
=
=
RDN(N)#
1j
   Rj)(#RN(N)                             (2) 
LN(N) and RN(N) are the frequencies of nouns 
that directly precede or succeed N. For instance, in 
an example shown in Figure 2, LN(trigram)=5, and 
RN(trigram)=3.  
Let?s think about the background of these 
scoring functions. #LDN(N) and #RDN(N), where 
we do not take into account the frequency of each 
noun bigram but take into account the number of 
distinct nouns that adjoin to N to make compound 
words. That indicates how linguistically and 
domain dependently productive the noun:N is in a 
given corpus. That means that if N presents a key 
and/or basic concept of the domain treated by the 
corpus, writers in that domain work out many 
distinct compound words with N to express more 
complicated concepts. On the other hand, as for 
LN(N) and RN(N), we also focus on frequency of 
each noun bigram as well. In other words, statistic 
bias in actual use of noun:N is, this time, one of 
our main concern. For example, in Figure 2, 
LN(trigram,2)=11, and RN(trigram,2)=5. In 
conclusion, since LN(N) and RN(N) convey more 
information than #LDN(N) and #RDN(N), we 
adopt LN(N) and RN(N) in this research.  
3.2.2 Score of compound words  
The next thing to do is expanding those scoring 
functions for simple word to the scoring functions 
for compound words. We adopt a geometric mean 
for this purpose. Now think of a compound word : 
CN = N1 N2?N L, where Ni (i= 1,.., L) is a simple 
word. Then a geometric mean: LR of CN is 
defined as follows. 
L2
1
L
1i
ii 1))1)(RN(N)(LN(N(CN)LR
?
?
?
?
?
?
?
?
++= ?
=
           
                                 (3) 
For instance, if we use LN(N) and RN(N) in 
example 1, LR(trigram) = )15()13( +?+  = 4.90. 
LR does not depend on the length of CN where 
?length? means the number of simple words that 
consist of CN. This is because since we have not 
yet had any idea about the relation between the 
importance of a compound word and a length of 
the compound word, it is fair to treat all compound 
words, including simple words, equally no matter 
how long or short each compound word is. 
3.2.3 Combining LR and Frequency of Nouns 
We still have not fully utilized the information 
about statistics of actual use in a corpus in the 
bigram based methods described in 3.2.1 and 3.2.2. 
Among various kinds of information about actual 
use, the important and basic one is the frequency of 
single-and compound words that occur 
independently. The term ?independently? means 
that the left and right adjacent words are not nouns. 
For instance, ?word patterns? occurs independently 
in ?we use the word patterns which occur in this 
sentence.? Since the scoring functions proposed in 
3.2.1 is noun bigram statistics, the number of this 
kind of independent occurrences of nouns 
themselves have not been used so far. If we take 
this information into account, the better results are 
expected. Thus, if a simple word or a compound 
word occurs independently, the score of the noun 
is simply multiplied by the number of its 
independent occurrences. We call this new scoring 
function as FLR(CN) which is defined as follows. 
 
if N occurs independently  
then f(CN)(CN)LR(CN)LRF ?=  
where f(CN) means the number of independent 
occurrences of noun CN                                (4) 
4 Term Extraction for Chinese based on 
Morphological Analysis 
If we try to apply the scoring method proposed 
in section 3 directly to a Chinese text, every word 
should be POS tagged because we extract multi-
word unit of several types of POS tag sequences as 
candidates of domain specific terms. For this we 
need a Chinese morphological analyzer because 
Chinese is an agglutinative language. Actually, we 
use Chinese morphological analyzer: 
ICTCLAS(Zhang and Liu 2004). As term 
candidates, we extract compound word: MWU 
having the following POS tag sequence expressed 
in (5). A multi-word-unit: MWU is defined by the 
following CFG rules where the right hand sides are 
expressed as a regular expression. 
 
MWU <-- [ag a]* [ng  n  nr  ns  nt  nz  nx  vn  
an  i  j]+ 
MWU <-- MWU?b [ng  n  nr  ns  nt  nz  nx  vn  
an  i  j]+ 
MWU <-- [ag a]+ [u k] MWU 
MWU <-- MWU (u|k|he-2|yu-3) MWU 
                     (5) 
where ?ag?, ?a?, ?n??, ?u? are all tags used in 
ICTCLAS. 
Roughly speaking (5) means an adjective 
followed by the repetition of [adjective noun 
particle] followed by a noun. The problem is the 
ambiguity of POS tagging because the same word 
is very often used verb as well as noun. In addition, 
unknown words like newly appeared proper names 
also impairs the accuracy. Due to this problem 
caused by morphological analyzer, the accuracy is 
degraded. 
Once we segment out word sequences 
conforming the above POS tag sequences, we 
calculate LN and RN of each component word. In 
calculation of LN and RN, a word whose POS is c, 
u or k is omitted. In other words, if a word 
sequence ?w1 w2 w3? where POS of w2 is c u or k, 
then we calculate RN of w1 and LN of w3 by 
regarding the word sequence as ?w1 w3.? 
Then we combine LN and RN of each word to 
calculate FLR by definition of (3) and (4) to sort 
all extracted candidates in descending order of 
FLR. 
We apply the proposed methods to 30 Web 
pages from People?s Daily news. The areas are 
social, international and IT related news. The 
average length is 592.6 characters. Firstly, we 
extract relevant terms by hand from each news 
article and use it as the gold standard. The average 
number of gold standard terms per one news 
particle is 15.9 words. Secondly, we extract terms 
from each news article and sort them in descending 
order by the proposed method and evaluate them 
by a precision of top N terms defined as follows. 
 
CT(K)= 1  if Kth term is one of the gold 
standard terms. 
0   otherwise 
K
iCT
Kprecision
K
i? =
=
1
)(
)(                             (6) 
where N is the number of the gold standard 
terms, and in our experiment, N=20. Precision(K), 
where K=1,..,20, are shown in Figure 3 as ?Strict.?  
We also use another precision rate precision? 
which is not strict and defined as follows. 
CTpart(K)= 1 if one of gold standard terms. 
is a part of Kth term 
                    0  otherwise 
K
iCTpart
Kprecision
K
i? =
=
1
)(
)('                    (7) 
These are also shown in Figure 3 as ?Partly.? 
 
 
Figure 3. Strict and partly precision of word based 
extraction method. 
 
From Figure 3, we see that If we pick up the ten 
highest ranked terms, about 75% of them meet the 
gold standard. The case we loosen the definition of 
precision shows better than the strict case of (6) 
but the difference is not so large. That means that 
the proposed word based ranking method works 
very well to extract important Chinese terms from 
news articles.  
5 Character based Term Extraction 
There are several reasons why we would like to 
develop a term extraction system without 
morphological analyzer.  
The first reason is that the accuracy of 
morphological analyzer is, in spite of the great 
advancement of these years, still around 95% 
(GuoDong and Jian 2003).  
The second reason is that there possibly exist 
terminologies with unexpected POS sequences. If 
we deal only with academic papers or technical 
documents, we expect POS sequences of 
terminologies with high accuracy. However, if we 
consider terminology extraction from Web pages, 
the possibility of unexpected POS sequence may 
rise. 
The third reason is language independency. 
Currently proposed and/or used morphological 
analyzers heavily depend either upon the 
sophisticated linguistic knowledge about the target 
language or upon a big size corpus of the target 
language if machine learning is employed. These 
linguistic resources, however, are not always 
available.  
Due to these reasons, we also developed term 
candidate extraction system which does not use a 
morphological analyzer. Instead of morphological 
analyzer, we try to employ a stop word list. In 
Chinese, as stop words, we find many character 
unigrams and bigrams because one Chinese 
character conveys larger amount of information 
than a character of Latin alphabet. They are partly 
shown in Appendix A. 
As term candidates, we simply extract character 
strings between two stop words that are nearest 
each other within a sentence. Obviously, the 
character strings thus extracted are not necessarily 
meaningful compound words. Therefore we cannot 
directly use these strings as words to calculate LN 
and RN function. Back to the idea that Chinese 
characters are ideograms, we come up to the idea 
that we calculate LN and RN of each character 
appearing within every character strings extracted 
as candidates. An example is shown in Figure 4. 
 
 
LN(zuo-4)=3                  RN(zuo-4)=2 
Figure 4.  LN and RN of Chinese character zuo-4 
 
In calculation of LN and RN, we neglect 
characters whose POS are c ,u or k as same as we 
did in morphological analyzer based method. 
Once we calculate LN and RN of each character, 
FLR of every character string is calculated as 
defined by (3) and (4) to sort them in descending 
order of FLR. 
Actually this idea is very similar with left and 
right entropy used to extract two character Chinese 
words from a corpus (Luo and Sun. 2003). 
However what we would like to extract is a set of 
longer compound words or even phrases used in a 
Web page. Moreover we only use the Web page 
and do not use any other language resources such 
as a big corpus at all due to the reason described 
above in this section. 
We evaluate the proposed character based 
extraction method against the same Web pages 
from People?s Daily news used in Morphological 
Analysis based method described in Section 4. We 
also use the same gold standard terms described in 
Section 4 for evaluation. The strict and partly 
precision defined by (6) and (7) are used. The 
result is shown in Figure 5.  
 
Figure 5. Strict and partly precision of character based 
extraction method. 
 
Comparing Figure 3 with Figure 5, apparently 
the result of extracted terms of word based method 
is better than that of character based method. 
However, it does not necessarily mean that the 
character based term extraction is inferior. 
If you take a glance at the stop word list of 
Appendix A, it seems that several of the stop 
words are selected mainly from words in auxiliary 
verbs, pronouns, adverbs, particles, prepositions, 
conjunctions, exclamations, onomatopoeic words 
and punctuation marks. However, in reality, our 
selection is based rather on meaning, usage and 
generally frequency of use than parts of speech. 
Thus some of them are not function words but 
content words in order to exclude non-domain-
specific words. Actually, the stop words are not 
only character unigram but character bigram. 
Because Chinese character is ideograph and each 
character may have plural meanings, it is difficult 
only to use character unigram as a stop word in 
Chinese.  
Our method based on these viewpoints resulted 
in getting an interesting consequence. We show an 
example of news article and extracted terms from it 
by this method in Appendix B and Appendix C. 
This news article is entitled ?The Culture of 
Tibetan Web Site is formally created.? Let?s take a 
look at an underlined sentence in this short article 
and underlined terms extracted from there. This 
sentence says: According to the introduction, The 
Culture of Tibetan Web Site is a site of special 
pure culture for the purpose of ?investigating the 
essence of Tibetan culture, showing the scale of 
Tibetan culture and raising the spirit of Tibetan 
culture?. In the case of method based on stop word 
list, we can extract compound term of 
?investigating the essence of Tibetan culture 
?, ?showing the scale of 
Tibetan culture ( )?, ?raising the 
spirit of Tibetan culture ( )? and 
so on from this sentence. On the contrary, by the 
term extraction method based on morphological 
analysis, gerund , for example, ?showing( )? 
and ?raising ( ), can not be extracted.  
 We said that the majority of domain specific 
terms are noun phrases or compound words 
consisting of small size vocabulary of simple 
words as stated in section 3. So we especially 
have paid attention to relation among nouns. 
However most of Chinese nouns can also be 
used as verbs. Moreover inflection of Chinese 
verbs can hardly be recognized visually. It is 
difficult to distinguish verb from noun by 
morphological analysis. Certainly ICTCLAS 
classifies the character that has meaning of both 
verb and noun into the category of vn (verb and 
noun). But gerunds and verbal noun infinitives are 
not contained in vn. For instance, ? ? means not 
only ?write a letter? but ?writing letter.? Thus we 
have to pay attention to verbs in Chinese too. Only 
by tuning up stop word list, we can take gerunds 
and verbal noun infinitives into account to some 
extent. Appendix C shows one of the evidence of 
this observation.  
6 Conclusion 
In this paper, we apply automatic term 
recognition system based on FLR proposed by 
Nakagawa and Mori (2003) to Chinese Web pages 
because the term extraction from small text like 
one Web page is the future oriented topic. We 
proposed two methods: word based and character 
based extraction and ranking using the compound 
word productivity of simple words. Since the 
accuracies of term recognition are around 60% for 
top 1,000 term candidates in NTCIR TMREC 
task(Kageura et al1999), the result of 75% 
accuracy of top ten candidates is a good start. 
References  
Ananiadou, S. 1994. A Methodology for Automatic 
Term Recognition. In Proceedings of 14th 
International Conference on Computational 
Linguistics :1034 - 1038.  
GuoDong Zhou and Jian Su. 2003. A Chinese Efficient 
Analyser Integrating Word Segmentation, Part-Of-
Speech Tagging, Partial Parsing and Full Parsing, In 
Proceedings of The Second SIGHAN Workshop, on 
Chinese Language Processing .ACL2003 :78-83 
Frantzi, T.K. and Ananiadou, S. 1996. Extracting nested 
collocations. In Proceedings of 16th International 
Conference on Computational Linguistics :41-46. 
Fukushige, Y. and N. Nogichi. 2000. Statistical and 
linguistic approaches to automatic term recognition 
NTCIR experiments at Matsushita. Terminology 
6(2) :257-286 
Hua-PingZhang, Hong-KuYu, De-Yi Xiong and Qun 
Liu. 2003. HHMM-based Chinese Lexical Analyzer 
ICTCLAS. In Proceedings of The Second SIGHAN 
Workshop, on Chinese Language 
Processing .ACL2003 :184-187 
Hisamitsu, T, 2000. A Method of Measuring Term 
Representativeness. In Proceedings of 18th 
International Conference on Computational 
Linguistics :320-326. 
Kageura, K. and Umino, B. 1996. Methods of automatic 
term recognition: a review. Terminology  3(2) :259-
289. 
Kageura, K. et al 1999. TMREC Task: Overview and 
Evaluation. In Proceedings of the First NTCIR 
Workshop on Research in Japanese Text Retrieval 
and Term Recognition :411-440. 
Shengfen Luo and Maosong Sun. 2003. Two-Character 
Chinese Word Extraction Based on Hybrid of 
Internal and Contextual Measures. In Proceedings of 
The Second SIGHAN Workshop, on Chinese 
Language Processing .ACL2003 :24-30 
Qing Ma and Fei Xia. 2003. Proceedings of The Second 
SIGHAN Workshop, on Chinese Language 
Processing .ACL2003, Sappro 
Sujian Li,Houfeng Wang,Shiwen Yu and Chengsheng 
Xin. 2003. News-Oriented Automatic Chinese 
Keyword Indexing. In Proceedings of The Second 
SIGHAN Workshop, on Chinese Language 
Processing .ACL2003 :92-97 
Nakagawa, H. and Tatsunori Mori. 2003. Automaic 
Term Recognition based on Statistics of Compound 
words and their Components, Terminology 9(2) :201-
219 
Smadja, F.A. and Mckeown, K.R. 1990. Automatically 
extracting and representing collocations for language 
generation. In Proceedings of the 28th Annual 
Meetings of the Association for Computational 
Linguistics :252-259. 
Uchimoto, K., S.Sekine, M. Murata, H.Ozaku and H. 
Isahara. 2000. Term recognition using corpora from 
different fields. Terminology 6(2) :233-256 
Wenfeng Yang and Xing Li. 2002. Chinese keyword 
extraction based on max-duplicated strings of the 
documents. In Proceedings of the 25th annual 
international ACM SIGIR conference on Research 
and develop-ment in information retrieval, pp. 439-
440.  
Kevin Zhang and Qun Liu. 2004. ICTCLAS. 
http://www.nlplab.cn/zhangle/morphix-nlp /manual 
/node12.html 
Appendix A: A part of stop word list 
 
Appendix B: An example of news article 
 
 
Appendix C: Terms extracted from Appendix 
B. 
Word Based (Top 10 terms with score of 
equation (4) ) 
 
 
Character Based(Top 11 terms with score 
equation (4) ) 
 
 
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 17?20, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
A real-time multiple-choice question generation for language testing
? a preliminary study?
Ayako Hoshino
Interfaculty Initiative in Information Studies
University of Tokyo
7-3-1 Hongo, Bunkyo, Tokyo,
113-0033, JAPAN
qq36126@iii.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center
University of Tokyo
7-3-1 Hongo, Bunkyo, Tokyo,
113-0033, JAPAN
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
An automatic generation of multiple-
choice questions is one of the promising
examples of educational applications of
NLP techniques. A machine learning ap-
proach seems to be useful for this pur-
pose because some of the processes can
be done by classification. Using basic ma-
chine learning algorithms as Naive Bayes
and K-Nearest Neighbors, we have devel-
oped a real-time system which generates
questions on English grammar and vocab-
ulary from on-line news articles. This pa-
per describes the current version of our
system and discusses some of the issues
on constructing this kind of system.
1 Introduction
Multiple-choice question exams are widely used and
are effective to assess students? knowledge, how-
ever it is costly to manually produce those questions.
Naturally, this kind of task should be done with a
help of computer.
Nevertheless, there have been very few attempts
to generate multiple-choice questions automatically.
Mitkov et al(2003) generated questions for a lin-
guistics exam in a semi-automatic way and evalu-
ated that it exceeds manually made ones in cost and
is at least equivalent in quality. There are some
other researches that involve generating questions
with multiple alternatives (Dicheva and Dimitrova,
1998). But to the best of our knowledge, no attempt
has been made to generate this kind of questions in
a totally automatic way.
This paper presents a novel approach to generate
multiple-choice questions using machine learning
techniques. The questions generated are those of fill-
in-the-blank type, so it does not involve transform-
ing declarative sentences into question sentences as
in Mitkov?s work. This simplicity makes the method
to be language independent.
Although this application can be very versatile, in
that it can be used to test any kind of knowledge as in
history exams, as a purpose of this research we limit
ourselves to testing student?s proficiency in a foreign
language. One of the purposes of this research is to
automatically extract important words or phrases in
a text for a learner of the language.
2 System Design
The system we have implemented works in a sim-
ple pipelined manner; it takes an HTML file and
turns it into the one of quiz session. The process
of converting the input to multiple-choice questions
includes extracting features, deciding the blank po-
sitions, and choosing the wrong alternatives (which
are called distractors), which are all done in a mo-
ment when the user feeds the input. When the user
submits their answer, it shows the text with the cor-
rect answers as well as an overall feed back.
3 Methodology
The process of deciding blank positions in a given
text follows a standard machine learning framework,
which is first training a classifier on a training data
17
Table 1: the full list of test instances classified as true in test-on-train
certainty a test instance (sentence with a blank) the answer
0.808 Joseph is preparing for tomorrow?s big [ ] to the president. presentation
0.751 Ms. Singh listened [ ] to the president?s announcement. carefully
0.744 The PR person is the one in charge of [ ] meetings and finding accommoda-
tions for our associates.
scheduling
0.73 Ms. Havlat received a memo from the CEO [ ] the employees? conduct. regarding
0.718 The amount of money in the budget decreased [ ] over the past year. significantly
0.692 Mr. Gomez is [ ] quickly; however it will be a log time before he gets used to
the job.
learning
0.689 The boss can never get around to [ ] off his desk. cleaning
0.629 The interest rate has been increasingly [ ] higher. getting
0.628 Employees are [ ] to comply with the rules in the handbook. asked
0.62 The lawyer [ ] his case before the court. presented
0.59 The secretary was [ ] to correspond with the client immediately. supposed
0.576 The maintenance worker checked the machine before [ ] it on. turning
0.523 The [ ] manager?s office is across the corridor. assistant
(i.e. TOEIC questions), then applying it on an un-
seen test data, (i.e. the input text). In the current sys-
tem, the mechanism of choosing distractors is imple-
mented with the simplest algorithm, and its investi-
gation is left to future work.
3.1 Preparing the Training Data
The training data is a collection of fill-in-the-blank
questions from a TOEIC preparation book (Matsuno
et al, 2000). As shown in the box below, a ques-
tion consists of a sentence with a missing word (or
words) and four alternatives one of among which
best fits into the blank.
Many people showed up early to [ ] for the posi-
tion that was open.
1. apply 2. appliance 3. applies 4. application
The training instances are obtained from 100
questions by shifting the blank position. The orig-
inal position is labeled as true, while sentences with
a blank in a shifted position are at first labeled as
false. The instance shown above therefore yields in-
stances [ ] people showed up early to apply for the
position that was open., Many [ ] showed up early
to apply for the position that was open., and so on,
all of which are labeled as false except the original
blank position. 1962 (100 true and 1862 false) in-
stances were obtained.
The label true here is supposed to indicate that
it is possible to make a question with the sentence
with a blank in the specified position, while many
of the shifted positions which are labeled false can
also be good blanks. A semi-supervised learning
(Chakrabarti, 2003) 1 is conducted in the following
manner to retrieve the instances that are potentially
true among the ones initially classified as false.
We retrieved the 13 instances (shown in Table 1.)
which had initially been labeled as false and classi-
fied as true in a test-on-train result with a certainty 2
of more than 0.5 with a Naive Bayes classifier 3. The
labels of those instances were changed to true before
re-training the classifier. In this way, a training set
with 113 true instances was obtained.
3.2 Deciding Blank Positions
For the current system we use news articles from
BBC.com 4, which consist approximately 200-500
words. The test text goes through tagging and fea-
ture extraction in the same manner as the training
1Semi-supervised learning is a method to identify the class
of unclassified instances in the dataset where only some of the
instances are classified.
2The result of a classification of a instance is obtained along
with a certainty value between 0.0 to 1.0 for each class, which
indicates how certain it is that an instance belongs to the class.
3Seven features which are word, POS, POS of the previous
word, POS of the next word, position in the sentence, sentence
length, word length and were used.
4http://news.bbc.co.uk/
18
data, and the instances are classified into true or
false. The positions of the blanks are decided ac-
cording to the certainty of the classification so the
blanks (i.e. questions) are generated as many as the
user has specified.
3.3 Choosing Distractors
In the current version of the system, the distractors
are chosen randomly from the same article exclud-
ing punctuations and the same word as the other al-
ternatives.
4 Current system
The real-time system we are presenting is imple-
mented as a Java servlet, whose one of the main
screens is shown below. The tagger used here is the
Tree tagger (Schmid, 1994), which uses the Penn-
Treebank tagset.
Figure 1: a screen shot of the question session page
with an enlarged answer selector.
The current version of the system is avail-
able at http://www.iii.u-tokyo.ac.jp/
?qq36126/mcwa1/. The interface of the system
consists of three sequenced web pages, namely 1)the
parameter selection page, 2)the quiz session page
and 3)the result page.
The parameter selection page shows the list of the
articles which are linked from the top page of the
BBC website, along with the option selectors for
number of blanks (5-30) and the classifier (Naive
Bayes or Nearest Neighbors).
The question session page is shown in Figure 1. It
displays the headline and the image from the chosen
article under the title and a brief instruction. The
alternatives are shown on option selectors, which are
placed in the article text.
The result page shows the text with the right an-
swers shown in green when the user?s choice is cor-
rect, red when it is wrong.
5 Evaluation
To examine the quality of the questions generated
by the current system, we have evaluated the blank
positions determined by a Naive Bayes classifier and
a KNN classifier (K=3) with a certainty of more than
50 percent in 10 articles.
Among 3138 words in total, 361 blanks were
made and they were manually evaluated according
to their possibility of being a multiple-choice ques-
tion, with an assumption of having alternatives of
the same part of speech. The blank positions were
categorized into three groups, which are E (possible
to make a question), and D (difficult, but possible to
make a question), NG (not possible or not suitable
e.g. on a punctuation). The guideline for deciding
E or D was if a question is on a grammar rule, or it
requires more semantic understanding, for instance,
a background knowledge 5.
Table 2. shows the comparison of the number of
blank positions decided by the two classifiers, each
with a breakdown for each evaluation. The num-
ber in braces shows the proportion of the blanks
with a certain evaluation over the total number of
blanks made by the classifier. The rightmost column
I shows the number of the same blank positions se-
lected by both classifiers.
The KNN classifier tends to be more accurate and
seems to be more robust, although given the fact that
it produces less blanks. The fact that an instance-
based algorithm exceeds Naive Bayes, whose deci-
sion depends on the whole data, can be ascribed to
a mixed nature of the training data. For example,
blanks for grammar questions might have different
features from ones for vocabulary questions.
The result we sampled has exhibited another
problem of Naive Bayes algorithm. In two articles
among the data, it has shown the tendency to make a
blank on be-verbs. Naive Bayes tends to choose the
5A blank on a verbs or a part of idioms (as [according] to)
was evaluated as E, most of the blanks on an adverbs, and (as
[now]) were D and a blank on a punctuation or a quotation mark
was NG.
19
Table 2: The evaluation on the blank positions decided by a Naive Bayes (NB) and a KNN classifier.
NB KNN I
blanks E(%) D(%) NG(%) blanks E(%) D(%) NG(%) blanks
Article1 69 44(63.8) 21(30.4) 4(5.8) 33 20(60.6) 11(33.3) 2(6.1) 18
Article2 22 5(22.7) 3(13.6) 14(63.6) 8 5(62.5) 3(37.5) 0(0.0) 0
Article3 38 21(55.3) 15(39.5) 2(5.3) 18 12(66.7) 5(27.8) 1(5.6) 8
Article4 19 10(52.6) 9(47.4) 0(0.0) 9 7(77.8) 2(22.2) 0(0.0) 3
Article5 28 18(64.3) 10(35.7) 0(0.0) 14 10(71.4) 4(28.6) 0(0.0) 6
Article6 26 17(65.4) 8(30.8) 1(3.8) 11 6(54.5) 5(45.5) 0(0.0) 4
Article7 18 9(50.0) 5(27.8) 4(22.2) 6 3(50.0) 3(50.0) 0(0.0) 3
Article8 24 14(58.3) 9(37.5) 1(4.2) 5 3(60.0) 2(40.0) 0(0.0) 5
Article9 20 16(80.0) 4(20.0) 0(0.0) 6 2(33.3) 4(66.7) 0(0.0) 4
Article10 30 18(60.0) 12(40.0) 0(0.0) 14 11(78.6) 3(21.4) 0(0.0) 6
294 172(58.5) 96(32.7) 26(8.8) 124 79(63.7) 42(33.9) 3(2.4) 57
same word as a blank position, therefore generates
many questions on the same word in one article.
Another general problem of these methods would
be that the blank positions are decided without con-
sideration of one another; the question will be some-
times too difficult when another blank is next to or
in the vicinity of the blank.
6 Discussion and Future work
From the problems of the current system, we can
conclude that the feature set we have used is not suf-
ficient. It is necessary that we use larger number
of features, possibly including semantic ones, so a
blank position would not depend on its superficial
aspects. Also, the training data should be examined
in more detail.
As it was thought to be a criteria of evaluating
generated questions, if a question requires simply a
grammatical knowledge or a farther knowledge (i.e.
background knowledge) can be a critical property of
a generated question. We should differentiate the
features from the ones which are used to generate,
for example, history questions, which require rather
background knowledge. Selecting suitable distrac-
tors, which is left to future work, would be a more
important process in generating a question. A se-
mantic distance between an alternative and the right
answer are suggested (Mitkov and Ha, 2003), to be
a good measure to evaluate an alternative. We are
investigating on a method of measuring those dis-
tances and a mechanism to retrieve best alternatives
automatically.
7 Conclusion
We have presented a novel application of automat-
ically generating fill-in-the-blank, multiple-choice
questions using machine learning techniques, as
well as a real-time system implemented. Although
it is required to explore more feature settings for the
process of determining blank positions, and the pro-
cess of choosing distractors needs more elaboration,
the system has proved to be feasible.
References
Soumen Chakrabarti. 2003. Mining the Web. Morgan
Kaufmann Publishers.
Darina Dicheva and Vania Dimitrova. 1998. An ap-
proach to representation and extraction of terminolog-
ical knowledge in icall. In Journal of Computing and
Information Technology, pages 39 ? 52.
Shuhou Matsuno, Tomoko Miyahara, and Yoshi Aoki.
2000. STEP-UP Bunpo mondai TOEIC TEST. Kiri-
hara Publisher.
Ruslan Mitkov and Le An Ha. 2003. Computer-aided
generation of multiple-choice tests. In Proceedings of
the HLT-NAACL 2003 Workshop on Building Educa-
tional Applications Using Natural Language Process-
ing, pages 17 ? 22, Edmonton, Canada, May.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, Manchester, UK, September.
20
Automatic Term Extraction Based on Perplexity
of Compound Words
Minoru Yoshida1,2 and Hiroshi Nakagawa1,2
1 Information Technology Center, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033
2 JST CREST, Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012
mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jp
Abstract. Many methods of term extraction have been discussed in
terms of their accuracy on huge corpora. However, when we try to apply
various methods that derive from frequency to a small corpus, we may
not be able to achieve sufficient accuracy because of the shortage of
statistical information on frequency. This paper reports a new way of
extracting terms that is tuned for a very small corpus. It focuses on the
structure of compound terms and calculates perplexity on the term unit?s
left-side and right-side. The results of our experiments revealed that the
accuracy with the proposed method was not that advantageous. However,
experimentation with the method combining perplexity and frequency
information obtained the highest average-precision in comparison with
other methods.
1 Introduction
Term extraction, which is the task of extracting terminology (or technical terms)
from a set of documents, is one of major topics in natural language processing. It
has a wide variety of applications including book indexing, dictionary generation,
and keyword extraction for information retrieval systems.
Most automatic term extraction systems make a sorted list of candidate terms
extracted from a given corpus according to the ?importance? scores of the terms,
so they require scores of ?importance? for the terms. Existing scores include
TF-IDF, C-Value [1], and FLR [9]. In this paper, we propose a new method
that involves revising the definition of the FLR method in a more sophisticated
way. One of the advantages of the FLR method is its size-robustness, i.e, it can
be applied to small corpus with less significant drop in performance than other
standard methods like TF and IDF, because it is defined using more fine-grained
features called term units. Our new method, called FPP, inherit this property
while exhibiting better performance than FLR.
At the same time, we also propose a new scheme for evaluating term ex-
traction systems. Our idea is to use summaries1 of articles as a gold standard.
This strategy is based on the assumption that summaries of documents can
1 In more detail, an article revised for display on mobile phones.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 269?279, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
270 M. Yoshida and H. Nakagawa
serve as collections of important terms because, in writing summaries, peo-
ple may make an original document shorter by dropping unnecessary parts
of original documents, while retaining essential fragments. Thus, we regard a
term in an original document to be important if it also appears in the
summary.
2 Term Extraction
Term extraction is the task of extracting important terms from a given corpus.
Typically, term extraction systems first extract term candidates, which are usu-
ally the noun phrases detected by handcrafted POS sequence patterns, from the
corpus. After that, term candidates are sorted according to some importance
score. Important terms, (i.e., terms that appear in the summary, in our problem
setting,) are desired to be ranked higher than others. In this paper we focus
on the second step, i.e., term candidate sorting by importance scores. We pro-
pose a new score of term importance by modifying an existing one in a more
sophisticated manner.
In the remainder of this paper, a term candidate is represented by W = w1w2
? ? ? wn where wi represents a term unit contained in W , and n is the number of term
units contained in W . Here, a term unit is the basic element comprising term can-
didates that is not further decomporsable without destruction of meaning. Term
units are used to calculate of the LR score that is explained in the next section.
3 Related Work
Many methods of term scoring have been proposed in the literature [7] [3] [4].
Methods that use corpus statistics have especially emerged over the past decade
due to the increasing number of machine-readable documents such as news arti-
cles and WWW documents. These methods can be mainly categorized into the
following three types according to what types of features are used to calculate
the scores.
? Measurement by frequencies
? Measurement by internal structures of term candidates
? Combination of the above
3.1 Score by Frequency: TF
Frequency is one of the most basic features of term extraction. Usually, a term
that appears frequently is assumed to be important. We introduce a score of this
type: tf(W ).
tf(W ) represents the TF(Term Frequency) of W . It is defined as the number
of occurrences of W in all documents. Note that tf(W ) is the result of the
brute force counting of W occurrences. This method, for example, counts the
Automatic Term Extraction Based on Perplexity of Compound Words 271
term natural even if it is merely part of another phrase such as natural language
processing.2
3.2 Score by Internal Structures in Term Candidates: LR
An LR method [9] is based on the intuition that some words are used as term
units more frequently than others, and a phrase that contains such ?good? term
units is likely to be important. The left score l(wi) of each term unit wi of a target
term is defined as the number (or the number of types) of term units connected
to the left of wi (i.e., appearing just in the left of wi in term candidates), and the
right score r(wi) is defined in the same manner.3 An LR score lr(wi) is defined
as the geometric mean of left and right scores:
lr(wi) =
?
l(wi)r(wi)
The total LR score of W is defined as a geometric mean of the scores of term
units as:
LR(W ) = (lr(w1)lr(w2) ? ? ? lr(wn))
1
n .
An example of LR score calculation is given in the next section.
3.3 Mixed Measures
C-Value. C-Value[1] is defined by the following two expressions:
t(W ): frequency of terms that contain W ,
c(W ): number of types of terms that contain W .
Note that t(W ) does not count W itself. Intuitively, t(W ) is the degree of being
part of another term, and c(W ) is the degree of being part of various types of
terms.
C-Value is defined by using these two expressions in the following way.
c-val(W ) = (n ? 1) ?
(
tf(W ) ? t(W )
c(W )
)
Note that the value is zero where n = 1. MC-Value [9] is a modified version
of C-Value adapted for use in term collections that include the term of length 1
(i.e., n = 1).
MC-val(W ) = n ?
(
tf(W ) ? t(W )
c(W )
)
We used MC-Value in the experiments because our task was to extract terms
regardless of whether each term is one-word term or not.
2 We can also use another frequency score F(Frequency), or f(W ), that is defined as
the number of independent occurrences of W in all documents. (Independent means
that W is not included in any larger term candidate.) However, we observed that
f(W ) (or the combination of f(W ) and another score) had no advantage over tf(W )
(or the combination of tf(W ) and another score) in the experiments,so in this paper
we omit scores that are the combination of f(W ) and other scores.
3 In addition, we apply the adding-one smoothing to both of them to avoid the score
being zero when wi has no connected terms.
272 M. Yoshida and H. Nakagawa
FLR. The LR method reflects the number of appearances of term units, but does
not reflect that of a whole term itself. For example, even if ?natural language? is
more frequent than ?language natural? and the former should be given a higher
score than the latter, LR cannot be used to do this.
An FLR method [9] was proposed to overcome this shortcoming of LR. It
reflects both the frequencies and inner structures of terms. FLR(W ) is defined
as the product of LR(W ) and tf(W ) as:
FLR(W ) = tf(W )LR(W ).
4 Our Method: Combining Types and Frequencies via
Entropy
4.1 Preliminaries: Token-LR and Type-LR
Figure 1 outlines example statistics for term unit connections. For example, the
term disaster information appeared three times in the corpus.
Disaster
3 times
Information
Ethics 2 times
System 1 times
Security 3 times












Fig. 1. An example of statistics for term unit connections
LR scores have two versions: Token-LR and Type-LR. Token-LR (and Type-
LR) are calculated by simply counting the frequency (and the types) of terms
connected to each term unit, respectively. In this case, a Type-LR score for the
term unit ?information? is
l(information) = 1 + 14, r(information) = 3 + 1, LR(information) =
?
8,
and a Token-LR score is
l(information) = 3 + 1, r(information) = 6 + 1, LR(information) =
?
28.
4 Note that the adding-one smoothing is applied.
Automatic Term Extraction Based on Perplexity of Compound Words 273
Type-LR cannot reflect frequencies which suggest whether there are spe-
cially important connecting terms or not. However, Token-LR cannot reflect the
number of types that suggest the variety of connections. To solve these short-
comings with LR measures, we propose a new kind that combines these two
through perplexity.
4.2 Term Extraction by Perplexity
Our method is based on the idea of perplexity [8]. The score of a term is defined
by the left perplexity and right perplexity of its term units. In this subsection we
first give a standard definition of the perplexity of language, from which our left
and right perplexity measures are derived. After that, we describe how to score
terms by using these perplexities.
Perplexity of language. Assume that language L is information source that
produces word lists of length n and each word list is produced independently
with probability P (wn1 ). Then, the entropy of language L is calculated as:
H0(L) = ?
?
wn1
P (wn1 ) log P (w
n
1 ).
The entropy per word is then calculated as:
H(L) = ? 1
n
?
wn1
P (wn1 ) log P (w
n
1 ).
This value indicates the number of bits needed to express each word generated
from L. Perplexity of language L is defined using H(L) as:
Perplexity = 2H(L).
Perplexity can be seen as the average number of types of words that follow each
preceding word. The larger the perplexity of L, the less predictable the word
connection in L.
Left and right perplexity. Assume that k types of unit words can connect to
the right of wi (see Figure 2).
Also assume that Ri is a random variable assigned to the i-th term unit which
represents its right connections and takes its value from the set {r1, r2, ? ? ? , rk}.
Then, entropy H(Ri) is calculated as:
H(Ri) = ?
k
?
j=1
P (rj) log2 P (rj)
Note that we define 0 log 0 = 0, according to the fact that x log x ? 0 where
x ? 0.
274 M. Yoshida and H. Nakagawa
ffifl
fi
wi
ffifl
fi
r1
ffifl
fi
r2
...
ffifl
fi
rk






















Fig. 2. Example of term unit and term units connected to its right
This entropy value can be thought of as a variety of terms that connect to
the right of wi, or, more precisely, the number of bits needed to describe words
that connect to the right of wi.
Then right perplexity ppr(wi) of term unit wi is defined as
ppr(wi) = 2H(R
i).
This value can be seen as the number of branches, in the sense of information
theory, of right-connection from wi. It naturally reflects both the frequency and
number of types of each connection between term units.
Random variable Li for the left connections is defined in the same manner.
The perplexity for left connections is thus defined as:
ppl(wi) = 2H(L
i).
Term Score by Perplexity. We define our measure by substituting l and r
in the definition of LR with ppl and ppr. First, a combination of left and right
perplexities is defined as the geometric mean of both:
pp(wi) = (ppl(wi) ? ppr(wi))
1
2 .
After that, perplexity score PP (W ) for W is defined as the geometric mean of
all pp(wi)s:
PP (W ) =
[
n
?
i=1
pp(wi)
]
1
n
.
Automatic Term Extraction Based on Perplexity of Compound Words 275
We used log PP (W ) instead of PP (W ) to make implementation easier. Notice
that log x is a monotonic (increasing) function of x.
PP (W ) =
[
n
?
i=1
{ppl(wi) ? ppr(wi)}
1
2
]
1
n
? log2 PP (W ) =
1
n
log2
(
n
?
i=1
{ppl(wi) ? ppr(wi)}
1
2
)
? log2 PP (W ) =
1
2n
n
?
i=1
(log2 ppl(wi) + log2 ppr(wi))
Using ppr(wi) = 2H(R
i) and ppl(wi) = 2H(l
i), we obtain
log2 PP (W ) =
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
.
The right side means the sum of the left and right entropies of all term units.
4.3 Term Extraction by Perplexity and TF
Perplexity itself serves as a good score for terms, but combining it with TF,
which is a measure from another point of view, can provide a still better score
that reflects both the inner structures of term candidates and their frequencies
which are regarded as global information about the whole corpus.
Our new score, FPP (W ), which is a combination of PP and TF, is defined
as their product:
FPP (W ) = tf(W )PP (W )
? log2 FPP (W ) = log2 tf(W ) + log2 PP (W )
? log2 FPP (W ) = log2 tf(W ) +
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
We avoided the problem of log2 tf(W ) being undefined with tf(W ) = 0
5
by applying the adding-one smoothing to tf(W ). Therefore, the above defi-
nition of log FPP (W ) changed as follows:
log2 FPP
?(W ) = log2(tf(W ) + 1) +
1
2n
n
?
i=1
(
H(Ri) + H(Li)
)
.
We used this log2 FPP
?(W ) measure for evaluation.
5 This situation occurs when we want to score a new term candidate from outside of
corpus.
276 M. Yoshida and H. Nakagawa
5 Experiments
5.1 Test Collection
We collected news articles and their summaries from the Mainichi Web News
from April, 2001 to March, 2002. The articles were categorized into four genres:
Economy, Society, World, and Politics. A shorter version of each article was
provided for browsing on mobile phones. Articles for mobile phones were written
manually from the original ones, which were shorter versions of the original
articles adapted to small displays. We regard them as summaries of the original
articles and used them to evaluate whether the extracted terms were correct
or not. If a term in the original article was also in the summary, the term was
correct, and incorrect if otherwise. Each article had a size of about 300 letters
and each summary had a size of about 50.
Table 1 lists the number of articles in each category.
Table 1. Number of articles in test collection
Economy Society World Politics
# of articles 4,177 5,952 6,153 4,428
5.2 Experimental Setup
We used test data on the various numbers of articles to investigate how the
performance of each measure changed according to corpus size. A corpus of each
size was generated by singly adding an article randomly selected from the corpus
of each genre. We generated test data consisting of 50 different sizes (from 1 to
50) for each genre. The average number of letters in the size 50 corpus was about
19,000, and the average number of term candidates was about 1,300. We used
five different seed numbers to randomly select articles. The performance of each
method was evaluated in terms of recall and precision, which were averaged over
the five trials.
5.3 Preprocessing: Term Candidate Extraction
Each article was preprocessed with a morphological analyzer, the Chasen 2.3.3.[2]
The output of Chasen was further modified according to heuristic rules as follows.
? Nouns and undefined words were extracted for further processes and other
words were discarded.
? Suffixes and prefixes were concatenated to their following and preceding
words, respectively.
The result was a set of term candidates to be evaluated with the term importance
scores described in the previous sections.
We applied the following methods to the term candidates: F, TF, DF
(Document Frequency) [8], LR, MC-Value, FLR, TF-IDF [8], PP, and FPP?.
Automatic Term Extraction Based on Perplexity of Compound Words 277
5.4 Evaluation Method
We used average precision [8] for the evaluation. Let D be a set of all the term
candidates and Dq ? D be a set of the correct ones among them. The extracted
term was correct if it appeared in the summary. Then, the average precision can
be calculated in the following manner.
Average-Precision =
1
|Dq|
?
1?k?|D|
?
?
?
rk ?
?
?
1
k
?
1?i?k
ri
?
?
?
?
?
where ri = 1 if the i-th term is correct, and ri = 0 if otherwise.
Note that the total number of correct answers was |Dq|. The next section
presents the experimental results obtained by average precision.
Table 2. Average precision on corpus of 1, 10, and 50 articles. Each cell contains
results for the Economy/World/Society/Politics genres.
Measure SIZE=1 SIZE=10 SIZE=50
F 0.275/0.274/0.246/0.406 0.337/0.350/0.325/0.378 0.401/0.415/0.393/0.425
TF 0.305/0.388/0.281/0.430 0.386/0.406/0.376/0.435 0.454/0.462/0.436/0.477
DF 0.150/0.173/0.076/0.256 0.237/0.253/0.234/0.294 0.337/0.357/0.332/0.378
LR 0.192/0.370/0.194/0.378 0.255/0.280/0.254/0.317 0.303/0.302/0.273/0.320
MC-Val 0.218/0.296/0.240/0.388 0.317/0.334/0.307/0.365 0.399/0.400/0.369/0.420
FLR 0.305/0.410/0.298/0.469 0.361/0.397/0.364/0.429 0.423/0.435/0.404/0.455
TF-IDF 0.150/0.173/0.076/0.256 0.388/0.407/0.376/0.437 0.457/0.465/0.438/0.479
PP 0.223/0.327/0.285/0.514 0.285/0.299/0.282/0.331 0.329/0.317/0.279/0.331
FPP? 0.320/0.457/0.380/0.561 0.407/0.444/0.409/0.471 0.487/0.480/0.448/0.493
6 Results and Discussion
Table 2 shows the results on the corpus of 1, 10, and 50 articles in all the gen-
res. Figure 3 plots the average precision for each corpus size (from 1 to 50) in
the economy category.6 In some cases, results on one article were better than
those on 10 and 50 articles. This was mainly caused by the fact that the av-
erage precision is tend to be high on articles of short length, and the average
length for one article was much shorter than that of ten articles in some genres.
PP outperformed LR in most cases. We think the reason was that PP could
provide more precious information about connections among term units. We ob-
served that PP depended less on the size of the corpus than frequency-based
methods like TF and MC-Val. FPP? had the best performance of all methods in
all genres.
6 We only show a graph in the economy genre, but the results in other genres were
similar to this.
278 M. Yoshida and H. Nakagawa
0.1
0.2
0.3
0.4
0.5
1 11 21 31 41
Corpus size (# of articles)
A
v
e
r
a
g
e
 
p
r
e
c
i
s
i
o
n
F TF DF
LR MC-Val FLR
TF-IDF PP FPP'
Fig. 3. Results in economy genre
0.3
0.4
0.5
0.6
50 150 250 350 450 550 650 750 850 950
Corpus size (# of articles)
A
v
e
r
a
g
e
 
p
r
e
c
i
s
i
o
n
F TF DF
LR MC-Val FLR
TF-IDF PP FPP'
Fig. 4. Results on 50 ? 1000 articles
Automatic Term Extraction Based on Perplexity of Compound Words 279
Figure 4 plots the results in the economy genre when the corpus size was
increased to 1,000 in increments of 50 articles. We observed that the perfor-
mance of PP and LR got close with the increase in corpus size, especially with
200 articles and more. FPP? once again outperformed all the other methods in
this experiment. The FPP? method exhibited the best performance regardless of
corpus size.
7 Conclusion and Future Work
We proposed a new method for extracting terms. It involved the combination of
two LR methods: Token-LR and Type-LR. We showed that these two could be
combined by using the idea of perplexity, and gave a definition for the combined
method. This new method was then combined with TF and experimental results
on the test corpus consisting of news articles and their summaries revealed that
the new method (FPP?) outperformed existing methods including TF, TF-IDF,
MC-Value, and FLR.
In future work, we would like to improve the performance of the method by,
for example, adding preprocessing rules, such as the appropriate treatment of
numerical characters, and developing more sophisticated methods for combin-
ing TF and PP. We also plan to extend our experiments to include other test
collections like TMREC [6].
References
1. Ananiadou, S.: A methodology for automatic term recognition. In Proceedings of
the 15th InternationalConference on Computational Linguistcs (COLING) (1994),
pp. 1034?1038.
2. Asahara, M., Matsumoto, Y.: Extended Models and Tools for High-performance
Part-of-Speech Tagger. Proceedings of COLING 2000. (2000).
3. COMPUTERM?98 First Workshop on Computational Terminology. (1998).
4. COMPUTERM?02 Second Workshop on Computational Terminology. (2002).
5. Frantzi, K. and Ananiadou, S.: The C-value/NC-value method for ATR. Journal of
NLP, Vol. 6, No. 3, (1999). pp.145?179.
6. Kageura, K.: TMREC Task: Overview and Evaluation. Proc. of the First NTCIR
Workshop on Research in Japanese Text Retrieval and Term Recognition, (1999).
pp. 411?440.
7. Kageura, K and Umino, B.: Methods of automatic term recognition: A review.
Terminology, Vol. 3, No. 2, (1996). pp. 259?289.
8. Manning, C.D., and Schutze, H..: Foundations of Statistical Natural Language Pro-
cessing. (1999). The MIT Press.
9. Nakagawa, H. and Mori, T.: Automatic Term Recognition based on Statistics of
Compound Nouns and their Components. Terminology, Vol. 9, No. 2, (2003). pp.
201?219.
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 121?124, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Reformatting Web Documents via Header Trees
Minoru Yoshida and Hiroshi Nakagawa
Information Technology Center, University of Tokyo
7-3-1, Hongo, Bunkyo-ku, Tokyo 113-0033, Japan
CREST, JST
mino@r.dl.itc.u-tokyo.ac.jp, nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
We propose a new method for reformat-
ting web documents by extracting seman-
tic structures from web pages. Our ap-
proach is to extract trees that describe hier-
archical relations in documents. We devel-
oped an algorithm for this task by employ-
ing the EM algorithm and clustering tech-
niques. Preliminary experiments showed
that our approach was more effective than
baseline methods.
1 Introduction
This paper proposes a novel method for reformat-
ting (i.e., changing visual representations,) of web
documents. Our final goal is to implement the sys-
tem that appropriately reformats layouts of web doc-
uments by separating semantic aspects (like XML)
from layout aspects (like CSS) of web documents,
and changing the layout aspects while retaining the
semantic aspects.
We propose a header tree, which is a reasonable
choice as a semantic representation of web docu-
ments for this goal. Header trees can be seen as vari-
ants of XML trees where each internal node is not an
XML tag, but a header which is a part of document
that can be regarded as tags annotated to other parts
of the document. Titles, headlines, and attributes are
examples of headers. The left part of Figure 1 shows
an example web document. In this document, the
headers are About Me, which is a title, and NAME
and AGE, which are attributes. (For example, NAME
can be seen as a tag annotated to John Smith.)
Figure 2 shows a header tree for the example docu-
ment. It should be noted that each node is labeled
with parts of HTML pages, not abstract categories
such as XML tags.
About Me
* NAME *
John Smith
* AGE *
25
Back to Home Page
...<h1>About Me</h1><center><br><br>
* NAME *<br>...
[ About,  Me,  NAME,  John,  Smith, ? ]
</h1><center><br><br>*
Web Page
HTML Source
List of Blocks
Separator
Figure 1: An Example Web Document and Conver-
sion from HTML Documents to Block Lists.
Therefore, the required task is to extract header
trees from given web documents. Web documents
can be reformatted by converting their header trees
into various forms including Powerpoint-like in-
dented lists, HTML tables1, and Tree-class objects
of Java. We implemented the system that produces
these representations by extracting header trees from
given web documents.
One application of such reformatting is a web
browser on small devices that shows extracted
header trees regardless of original HTML visual ren-
dering. Trees can be used as compact representa-
tions of web documents because they show internal
structures of web documents concisely, and they can
be further augmented with open/close operations on
each node for the purpose of closing unnecessary
nodes, or sentence summarization on leaf nodes con-
taining long sentences. Another application is a lay-
out changer, which change a layout (i.e., HTML tag
usage) of one web page to another, by aligning ex-
tracted header trees of two web documents. Other
applications include HTML to XML transformation
and audio-browsable web content (Mukherjee et al,
2003).
1For example, the first column represents the root, the sec-
ond column represents its children, etc.
121
About Me
NAME
John Smith
AGE
25
Back to Home Page
Figure 2: A Header Tree for the Example Web Doc-
ument
1.1 Related Work
Several studies have addressed the problem of ex-
tracting logical structures from general HTML doc-
uments without labeled training examples. One
of these studies used domain-specific knowledge to
extract information used to organize logical struc-
tures (Chung et al, 2002). However, their ap-
proach cannot be applied to domains for which
any knowledge is not provided. Another type of
study employed algorithms to detect repeated pat-
terns in a list of HTML tags and texts (Yang and
Zhang, 2001; Nanno et al, 2003), or more struc-
tured forms (Mukherjee et al, 2003; Crescenzi et
al., 2001; Chang and Lui, 2001) such as DOM
trees. This approach might be useful for certain
types of web documents, particularly those with
highly regular formats such as www.yahoo.com
and www.amazon.com. However, in many cases,
HTML tag usage does not have so much regularity,
and, there are even the case where headers do not
repeat at all. Therefore, this type of algorithm may
be inadequate for the task of header extraction from
arbitrary web documents.
The remainder of this paper is organized as fol-
lows. Section 2 defines the terms used in this paper.
Section 3 provides the details of our algorithm. Sec-
tion 4 lists the experimental results and Section 5
concludes this paper.
2 Definitions
2.1 Definition of Terms
Our system decomposes an HTML document into a
list of blocks. A block is defined as the part of a
web document that is separated by a separator. A
separator is a sequence of HTML tags and symbols.
Symbols are defined as characters in texts that are
neither numbers nor letters. Figure 1 shows an ex-
ample of the conversion of an HTML document to a
list of blocks.
[ [About Me, [NAME, John Smith], [AGE, 25] ], Back to Home
Page] ]
Figure 3: A List Representation of the Example Web
Document
A header is defined as a block that modifies sub-
sequent blocks. In other words, a block that can be
a tag annotated to subsequent blocks is defined as a
header. Some examples of headers are Titles (e.g.,
?About Me?), Headlines (e.g., ?Here is my pro-
file:?), Attributes (e.g., ?Name?, ?Age?, etc.), and
Dates.
2.2 Definition of the Task
The system produces header trees for given web
documents. A header tree can be seen as an indented
list of blocks where the level of each node?s indent
is equal to the depth of the node, as shown in Figure
2. Therefore, the main part of our task is to give a
depth to each block in a given web document. After
that, some heuristic rules are employed to construct
header trees from a list of depths. In the next sec-
tion, we discuss the task of assigning a depth to each
block. Therefore, an input to the system is a list of
blocks and the output is a list of depths.
The system also produces nested-list representa-
tion of header trees for the purpose of evaluation. In
nested-list representation, each node that has chil-
dren is represented by the list whose first element
represents the parent and remaining elements repre-
sent the children. Figure 3 shows list representation
of the tree in Figure 2.
3 Header Extraction Algorithm
In this section, we describe our algorithm that re-
ceives a list of blocks and returns a list of depths.
3.1 Basic Concepts
The algorithm proceeds in two steps: separator cat-
egorization and block clustering. The first step
estimates local block relations (i.e., relations be-
tween neighboring blocks) via probabilistic models
for characters and tags that appear around separa-
tors. The second step supplements the first by ex-
tracting the undetermined relations between blocks
by focusing on global features, i.e., regularities in
HTML tag sequences. We employed a clustering
framework to implement a flexible regularity detec-
tion system that is robust to noise.
3.2 STEP 1: Separator Categorization
The algorithm classifies each block relation into one
of three classes: NON-BOUNDARY, RELATING,
122
[ About,  Me,  NAME,  John,  Smith, AGE, ? ]
List of Blocks
NON-BOUNDARY RELATING UNRELATING
NON-BOUNDARYRELATING
Figure 4: An Example of Separator Categorization.
and UNRELATING. Both RELATING and UNRE-
LATING can be considered to be boundaries; how-
ever, blocks that sandwich RELATING separators
are regarded to consist of a header and its modified
block. Figure 4 shows an example of separator cate-
gorization for the list of blocks in Figure 1.
The left block of a RELATING separator must be
in the smaller depth than the right block. Figure 2
shows an example. In this tree, NAME is in a smaller
depth than John. On the other hand, both the left
and right blocks in a NON-BOUNDARY separator
must be in the same depth in a tree representation,
for example, John and Smith in Figure 2.
3.2.1 Local Model
We use a probabilistic model that assumes the lo-
cality of relations among separators and blocks. In
this model, each separator   and the strings around
it,  and , are modeled by means of the hidden vari-
able , which indicates the class in which   is cate-
gorized. We use the character zerogram, unigram, or
bigram (changed according to the number of appear-
ances2) for  and  to avoid data sparseness prob-
lems.
For example, let us consider the following part of
the example document:
NAME: John Smith.
In this case, : is a separator, ME is the left string and
Jo is the right string.
Assuming the locality of separator appearances,
the model for all separators in a given document set
is defined as       
 
      where   is a
vector of left strings,  is a vector of separators, and
 is a vector of right strings.
The joint probability of obtaining ,  , and  is
                    
assuming that  and  depend only on : a class of
relation between the blocks around  .34
2This generalization is performed by a heuristic algorithm.
The main idea is to use a bigram if its number of appearances is
over a threshold, and unigrams or zerograms otherwise.
3If the frequency for     is over a threthold,       is
used instead of        .
4If the frequency for  is under a threthold,  is replaced by
its longest prefix whose frequency is over the threthold.
Based on this model, each class of separators is
determined as follows:
  
 
             
The hidden parameters     ,    , and
   , are estimated by the EM algorithm (Demp-
ster et al, 1977). Starting with arbitrary initial pa-
rameters, the EM algorithm iterates E-STEPs and
M-STEPs in order to increase the (log-)likelihood
function 	
      

	
      .
To characterize each class of separators, we use a
set of typical symbols and HTML tags, called rep-
resentatives from each class. This constraint con-
tributes to give a structure to the parameter space.
3.3 STEP 2: Block Clustering
The purpose of block clustering is to take advantage
of the regularity in visual representations. For exam-
ple, we can observe regularity between NAME and
AGE in Figure 1 because both are sandwiched by the
character * and preceded by a null line. This visual
representation is described in the HTML source as,
for example,
... <br><br>* NAME *<br> ...
... <br><br>* AGE *<br> ...
Our idea is to define the similarities between (con-
text of) blocks based on the similarities between
their surrounding separators. Each separator is rep-
resented by the vector that consist of symbols and
HTML tags included in it, and the similarity be-
tween separators are calculated as cosine values.
The algorithm proceeds in a bottom-up manner by
examining a given block list from tail to head, find-
ing the block that is the most similar to the current
block, and collecting them into the same cluster. Af-
ter that, all blocks in the same cluster is assigned the
same depth.
4 Preliminary Experiments
We used a training data that consists of 1,418 web
documents5 of moderate file size6 that did not have
?src? or ?script? tags7. The former criteria is based
on the observation that too small or too large doc-
uments are hard to use for measuring performance
of algorithms, and the latter criteria is caused by the
fact our system currently has no module to handle
image files as blocks.
We randomly selected 20 documents as test doc-
uments. Each test document was bracketed by hand
5They are collected by retrieving all user pages on one server
of a Japanese ISP.
6from 1,000 to 10,000 bytes
7Src tags indicate inclusion of image files, java codes, etc
123
Algorithm Recall Precision F-measure
OUR ALGORITHM 0.477 0.266 0.329
NO-CL 0.178 0.119 0.139
NO-EM 0.389 0.211 0.265
PREV 0.144 0.615 0.202
Table 1: Macro-Averaged Recall, Precision, and F-
measure on Test Documents
to evaluate machine-made bracketings. The per-
formance of web-page structuring algorithms can
be evaluated via the nested-list form of tree by
bracketed recall and bracketed precision (Goodman,
1996). Recall is the rate that bracketing given by
hand are also given by machine, and precision is the
rate that bracketing given by machine are also given
by hand. F-measure is a harmonic mean of recall and
precision that is used as a combined measure. Recall
and precision were evaluated for each test document
and they were averaged across all test documents.
These averaged values are called macro-average re-
call, precision, and f-measure (Yang, 1999).
We implemented our algorithm and the following
three ones as baselines.
NO-CL does not perform block clustering.
NO-EM does not perform the EM-parameter-
estimation. Every boundary but representatives
is defined to be categorized as ?UNRELAT-
ING?.
PREV performs neither the EM-learning nor the
block clustering. Every boundary but represen-
tatives is defined to be categorized as ?NON-
BOUNDARY?8. It uses the heuristics that ?ev-
ery block depends on its previous block.?
Table 1 shows the result. We observed that use of
both the EM-learning and block clustering resulted
in the best performance. NO-EM performs the best
among the three baselines. It suggests that only rely-
ing on HTML tag information is not a so bad strat-
egy when the EM-training is not available because
of, for example, the lack of a sufficient number of
training examples.
Results on the documents that were rich in HTML
tags with highly coherent layouts were better than
those on the others like the documents with poor
separators such as only one space character or one
line feed. Some of the current results on the doc-
uments with such poor visual cues seemed difficult
for use in practical systems, which indicates our sys-
tem still leaves room for improvement.
8This strategy is based on the fact that it maximized the per-
formance in a preliminary investigation.
5 Conclusions and Future Work
This paper proposed a method for reformatting web
documents by extracting header trees that give hi-
erarchical structures of web documents. Prelimi-
nary experiments showed that the proposed algo-
rithm was effective compared with some baseline
methods. However, the performance of the algo-
rithm on some of the test documents was not suf-
ficient for practical use. We plan to improve the
performance by, for example, using larger amount
of training examples. Finding other reformatting
strategies in addition to the ones proposed in this pa-
per is also important future work.
References
Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: In-
formation extraction based on pattern discovery. In
Proceedings of WWW2001, pages 681?688.
Christina Yip Chung, Michael Gertz, and Neel Sundare-
san. 2002. Reverse engineering for web data: From
visual to semantic structures. In ICDE.
Valter Crescenzi, Giansalvatore Mecca, and Paolo Meri-
aldo. 2001. ROADRUNNER: Towards automatic data
extraction from large web sites. In Proceedings of
VLDB ?01, pages 109?118.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM al-
gorithm. Journal of Royal Statistical Society: Series
B, 39:1?38.
Joshua Goodman. 1996. Parsing algorithms and metrics.
In Proceedings of ACL96, pages 177?183.
Saikat Mukherjee, Guizhen Yang, Wenfang Tan, and I.V.
Ramakrishnan. 2003. Automatic discovery of seman-
tic structures in HTML documents. In Proceedings of
ICDAR 2003.
Tomoyuki Nanno, Suguru Saito, and Manabu Okumura.
2003. Structuring web pages based on repetition of
elements. In Proceedings of WDA2003.
Yudong Yang and Hongjiang Zhang. 2001. HTML page
analysis based on visual cues. In Proceedings of IC-
DAR01.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. INRT, 1:69?90.
124
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 736?746,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatically Determining a Proper Length for Multi-document
Summarization: A Bayesian Nonparametric Approach
Tengfei Ma and Hiroshi Nakagawa
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
{matf@r., nakagawa@}dl.itc.u-tokyo.ac.jp
Abstract
Document summarization is an important task
in the area of natural language processing,
which aims to extract the most important in-
formation from a single document or a clus-
ter of documents. In various summarization
tasks, the summary length is manually de-
fined. However, how to find the proper sum-
mary length is quite a problem; and keeping
all summaries restricted to the same length
is not always a good choice. It is obvi-
ously improper to generate summaries with
the same length for two clusters of docu-
ments which contain quite different quantity
of information. In this paper, we propose
a Bayesian nonparametric model for multi-
document summarization in order to automat-
ically determine the proper lengths of sum-
maries. Assuming that an original document
can be reconstructed from its summary, we
describe the ?reconstruction? by a Bayesian
framework which selects sentences to form
a good summary. Experimental results on
DUC2004 data sets and some expanded data
demonstrate the good quality of our sum-
maries and the rationality of the length deter-
mination.
1 Introduction
Text summarization is the process of generating a
short version of a given text to indicate its main top-
ics. As the number of documents on the web expo-
nentially increases, text summarization has attracted
increasing attention, because it can help people get
the most important information within a short time.
In most of the existing summarization systems,
people need to first define a constant length to re-
strict all the output summaries. However, in many
cases it is improper to require all summaries are of
the same length. Take the multi-document summa-
rization as an example, generating the summaries
of the same length for a 5-document cluster and a
50-document cluster is intuitively improper. More
specifically, consider two different clusters of doc-
uments: one cluster contains very similar articles
which all focus on the same event at the same time;
the other contains different steps of the event but
each step has its own topics. The former cluster may
need only one or two sentences to explain its infor-
mation, while the latter needs to include more.
Research on summary length dates back in the
late 90s. Goldstein et al (1999) studied the char-
acteristics of a good summary (single-document
summarization for news) and showed an empiri-
cal distribution of summary length over document
size. However, the length problem has been grad-
ually ignored later, since researchers need to fix
the length so as to estimate different summarization
models conveniently. A typical instance is the Doc-
ument Understanding Conferences (DUC)1, which
provide authoritative evaluation for summarization
systems. The DUC conferences collect news arit-
cles as the input data and define various summariza-
tion tasks, such as generic multi-document summa-
rization, query-focused summarization and update
summarization. In all the DUC tasks, the output is
restricted within a length. Then human-generated
1After 2007, the DUC tasks are incorporated into the Text
Analysis Conference (TAC).
736
summaries are provided to evaluate the results of dif-
ferent summarization systems. Limiting the length
of summaries contributed a lot to the development
of summarization techniques, but as we discussed
before, in many cases keeping the summaries of the
same size is not a good choice.
Moreover, even in constant-length summariza-
tion, how to define a proper size of summaries for
the summarization tasks is quite a problem. Why
does DUC2007 main task require 250 words while
Update task require 100 words? Is it reasonable?
A short summary may sacrifice the coverage, while
a long summary may cause redundance. Automati-
cally determining the best size of summaries accord-
ing to the input documents is valuable, and it may
deepen our understanding of summarization.
In this work, we aim to find the proper length
for document summarization automatically and gen-
erate varying-length summaries based on the doc-
ument itself. The varying-length summarization is
more robust for unbalanced clusters. It can also
provide a recommended size as the predefined sum-
mary length for general constant-length summariza-
tion systems. We advance a Bayesian nonparametric
model of extractive multi-document summarization
to achieve this goal. As far as we are concerned, it is
the first model that can learn appropriate lengths of
summaries.
Bayesian nonparametric (BNP) methods are pow-
erful tools to determine the size of latent vari-
ables (Gershman and Blei, 2011). They let the data
?speak for itself? and allow the dimension of la-
tent variables to grow with the data. In order to
integrate the BNP methods into document summa-
rization, we follow the assumption that the original
documents should be recovered from the reconstruc-
tion of summaries (Ma and Wan, 2010; He et al,
2012). We use the Beta process as a prior to gen-
erate binary vectors for selecting active sentences
that reconstruct the original documents. Then we
construct a Bayesian framework for summarization
and use the variational approximation for inference.
Experimental results on DUC2004 dataset demon-
strate the effectiveness of our model. Besides, we
reorganize the original documents to generate some
new datasets, and examine how the summary length
changes on the new data. The results prove that our
summary length determination is rational and neces-
sary on unbalanced data.
2 Related Work
2.1 Research on Summary Length
Summary length is an important aspect for gener-
ating and evaluating summaries. Early research on
summary length (Goldstein et al, 1999) focused on
discovering the properties of human-generated sum-
maries and analyzing the effect of compression ratio.
It demonstrated that an evaluation of summarization
systems must take into account both the compres-
sion ratios and the characteristics of the documents.
Radev and Fan (2000) compared the readability and
speedup in reading time of 10% summaries and 20%
summaries2 for topic sets with different number of
documents. Sweeney et al (2008) developed an in-
cremental summary containing additional sentences
that provide context. Kaisser et al (2008) studied
the impact of query types on summary length of
search results. Other than the content of original
documents, there are also some other factors affect-
ing summary length especially in specific applica-
tions. For example, Sweeney and Crestani (2006)
studied the relation between screen size and sum-
mary length on mobile platforms. The conclusion of
their work is the optimal summary size always falls
into the shorter one regardless of the screen size.
In sum, the previous works on summary length
mostly put their attention on the empirical study of
the phenomenon, factors and impacts of summary
length. None of them automatically find the best
length, which is our main task in this paper. Nev-
ertheless, they demonstrated the importance of sum-
mary length in summarization and the reasonability
of determining summary length based on content of
news documents (Goldstein et al, 1999) or search
results (Kaisser et al, 2008). As our model is mainly
applied for generic summarization of news articles,
we do not consider the factor of screen size in mo-
bile applications.
2.2 BNP Methods in Document Summarization
Bayesian nonparametric methods provide a
Bayesian framework for model selection and
adaptation using nonparametric models (Gershman
210% and 20% are the compression rates, and the documents
are from search results in information retrieval systems.
737
and Blei, 2011). A BNP model uses an infinite-
dimensional parameter space, but invokes only a
finite subset of the available parameters on any
given finite data set. This subset generally grows
with the data set. Thus BNP models address the
problem of choosing the number of mixture compo-
nents or latent factors. For example, the hierarchical
Dirichlet process (HDP) can be used to infer the
number of topics in topic models or the number of
states in the infinite Hidden Markov model (Teh et
al., 2006).
Recently, some BNP models are also involved in
document summarization approaches (Celikyilmaz
and Hakkani-Tu?r, 2010; Chang et al, 2011; Darling
and Song, 2011). BNP priors such as the nested Chi-
nese restaurant process (nCRP) are associated with
topic analysis in these models. Then the topic dis-
tributions are used to get the sentence scores and
rank sentences. BNP here only impacts the number
and the structure of the latent topics, but the sum-
marization framework is still constant-length. Our
BNP summarization model differs from the previous
models. Besides using the HDP for topic analysis,
our approach further integrates the beta process into
sentence selection. The BNP method in our model
are directly used to determine the number of sum-
mary sentences but not latent topics.
3 BNP Summarization
In this section, we first introduce the BNP priors
which will be used in our model. Then we propose
our model called BNP summarization.
3.1 The Beta Process and the Bernoulli process
The beta process(BP) (Thibaux and Jordan, 2007;
Paisley and Carin, 2009) and the related Indian buf-
fet process(IBP) (Griffiths and Ghahramani, 2005)
are widely applied to factor/feature analysis. By
defining the infinite dimensional priors, these factor
analysis models need not to specify the number of
latent factors but automatically determine it.
Definition of BP (Paisley et al, 2010): Let B
0
be
a continuous measure on a space ? and B
0
(?) = ?.
If Bk is defined as follows,
Bk =
N
?
k=1
?k??
k
,
?k ? Beta(
??
N
,?(1?
?
N
))
?k ?
1
?
B
0
(1)
(where ??
k
is the atom at the location ?k; and ? is a
positive scalar), then as N ? ?, Bk ? B and B is
a beta process: B ? BP (?B
0
).
Finite Approximation: The beta process is de-
fined on an infinite parameter space, but sometimes
we can also use its finite approximation by sim-
ply setting N to a large number (Paisley and Carin,
2009).
Bernoulli Process: The beta process is conju-
gate to a class of Bernoulli processes, denoted by
X ? Bep(B). If B is discrete, of the form in
(1), then X =
?
k bk??k where the bk are indepen-
dent Bernoulli variables with the probability p(bk =
1) = ?k. Due to the conjugation between the
beta process priors and Bernoulli process, the pos-
terior of B given M samples X
1
, X
2
, ...XM where
Xi ? Bep(B)fori = 1, , ,M. is also a beta process
which has updated parameters:
B|X
1
, X
2
, ..., XM
? BP (?+M, ??+MB0 +
1
c+M
?
iXi) (2)
Application of BP: Furthermore, marginalizing
over the beta process measure B and taking ? =
1, provides a predictive distribution on indicators
known as the Indian buffet process (IBP) (Thibaux
and Jordan, 2007). The beta process or the IBP is
often used in a feature analysis model to generate
infinite vectors of binary indicator variables(Paisley
and Carin, 2009), which indicates whether a feature
is used to represent a sample. In this paper, we use
the beta process as the prior to select sentences.
3.2 Framework of BNP Summarization
Most existing approaches for generic extractive
summarization are based on sentence ranking. How-
ever, these methods suffer from a severe problem
that they cannot make a good trade-off between
the coverage and minimum redundancy (He et al,
738
2012). Some global optimization algorithms are de-
veloped, instead of greedy search, to select the best
overall summaries (Nenkova and McKeown, 2012).
One approach to global optimization of summariza-
tion is to regard the summarization as a reconstruc-
tion process (Ma and Wan, 2010; He et al, 2012)
. Considering a good summary must catch most of
the important information in original documents, the
original documents are assumed able to be recov-
ered from summaries with some information loss.
Then the summarization problem is turned into find-
ing the sentences that cause the least reconstruction
error (or information loss). In this paper, we fol-
low the assumption and formulate summarization as
a Bayesian framework.
First we review the models of (Ma and Wan,
2010) and (He et al, 2012). Given a cluster of
M documents x
1
, x
2
, ..., xM and the sentence set
contained in the documents as S = [s
1
, s
2
, ..., sN ],
we denote all corresponding summary sentences as
V = [v
1
, ..., vn], where n is the number of summary
sentences and N is the number of all sentences in
the cluster. A document xi and a sentence vi or si
here are all represented by weighted term frequency
vectors in the space Rd, where d is the number of
total terms (words).
Following the reconstruction assumption, a can-
didate sentence vi can be approximated by the
linear combination of summary sentences: si 
?n
j=1 w
?
jvj , where w
?
j is the weight for summary
sentence vj . Thus the document can also be ap-
proximately represented by a linear combination of
summary sentences (because it is the sum of the sen-
tences).
xi 
n
?
j=1
wjvj . (3)
Then the work in (He et al, 2012) aims to find
the summary sentence set that can minimize the re-
construction error
?N
i=1 ||si ?
?n
j=1 w
?
jvj ||
2; while
the work in (Ma and Wan, 2010) defines the prob-
lem as finding the sentences that minimize the dis-
tortion between documents and its reconstruction
dis(xi,
?n
j=1 wjvj) where this distortion function
can also be a squared error function.
Now we consider the reconstruction for each doc-
ument, if we see the document xi as the dependent
variable, and the summary sentence set S as the
independent variable, the problem to minimize the
reconstruction error can be seen as a linear regres-
sion model. The model can be easily changed to a
Bayesian regression model by adding a zero-mean
Gaussian noise  (Bishop, 2006), as follows.
xi =
n
?
j=1
wjvj + i (4)
where the weights wj are also assigned a Gaussian
prior.
The next step is sentence selection. As our sys-
tem is an extractive summarization model, all the
summary sentences are from the original document
cluster. So we can use a binary vector zi =<
zi1, ..., ziN >
T to choose the active sentences V
(i.e. summary sentences) from the original sen-
tence set S. The Equation (4) is turned into xi =
?N
j=1 ?ij ?zijsj+i. Using a beta process as a prior
for the binary vector zi, we can automatically infer
the number of active component associated with zi.
As to the weights of the sentences, we use a random
vector ?i which has the multivariate normal distri-
bution because of the conjugacy. ?i ? RN is an
extension to the weights {w
1
, ...wn} in (4).
Integrating the linear reconstruction (4) and the
beta process3 (1), we get the complete process of
summary sentence selection as follows.
xi = S(?i ? zi) + i
S = [s
1
, s
2
, ..., sN ]
zij ? Bernoulli(?j)
?j ? Beta(
??
N
,?(1?
?
N
))
?i ? N (0, ?
2
?I)
i ? N (0, ?
2
 I) (5)
where N is the number of sentences in the whole
document cluster. The symbol ? represents the ele-
mentwise multiplication of two vectors.
One problem of the reconstruction model is that
the word vector representation of the sentences are
sparse, which dramatically increase the reconstruc-
tion error. So we bring in topic models to reduce the
3We use the finite approximation because the number of sen-
tences is large but finite
739
dimension of the data. We use a HDP-LDA (Teh et
al., 2006) to get topic distributions for each sentence,
and we represent the sentences and documents as
the topic weight vectors instead of word weight vec-
tors. Finally xi is a K-dimensional vector and S is
a K ?N matrix, where K is the number of topics in
topic models.
4 Variational Inference
In this section, we derive a variational Bayesian al-
gorithm for fast inference of our sentence selec-
tion model. Variational inference (Bishop, 2006)
is a framework for approximating the true posterior
with the best from a set of distributions Q : q? =
argminq?QKL(q(Z)|p(Z|X)). Suppose q(Z) can
be partitioned into disjoint groups denoted by Zj ,
and the q distribution factorizes with respect to these
groups: q(Z) =
?M
j=1 q(Zj). We can obtain a gen-
eral expression for the optimal solution q?j (Zj) given
by
ln q?j (Zj) = Ei =j [ln p(X,Z)] + const. (6)
where Ei =j [ln p(X,Z)] is the expectation of the log-
arithm of the joint probability of the data and latent
variables, taken over all variables not in the parti-
tion. We will therefore seek a consistent solution
by first initializing all of the factors qj(Zj) appro-
priately and then cycling through the factors and re-
placing each in turn with a revised estimate given by
(6) evaluated using the current estimates for all of
the other factors.
Update for Z
p(zij |?j , xi, S, ?i) ? p(xi|zij , sj , ?i)p(zij |?j)
We use q(zij) to approximate the posterior:
q(zij)
? exp{E[ln(p(xi|zij , z
?j
i , S, ?i)) + ln(p(zij |?))]}
? exp{E[ln(?j)]}?
exp{E[?
1
2?2
(
x
?j
i ? sjzij?ij
)T (
x
?j
i ? sjzij?ij
)
]}
? exp{ln(?j)}?
exp{?
(
?2ij ? z
2
ij ? s
T
j sj ? 2?ij ? zij ? sj
T
? x
?j
i
)
2?2
}
(7)
where x?ji = xi ? S
?j(?
?j
i ? z
?j
i ), and the symbol
? indicates the expectation value. The ?2ij can be
extended to this form:
?2ij = ?ij
2
+?
j
i (8)
where ?ji means the j
th diagonal element of ?i
which is defined by Equation 13.
As zi is a binary vector, we only calculate the
probability of zij = 1 and zij = 0.
q(zij = 1) ? exp{ln(?j)} ?
exp{?
1
2?2
(
?2ij ? s
T
j sj ? 2?ij ? sj
T
? x
?j
i
)
}
q(zij = 0) ? exp{ln(1? ?j)} (9)
The expectations can be calculated as
ln(?j) = ?(
??
N
+ nj)? ?(?+M) (10)
ln(1? ?j) = ?(?(1?
?
N
)+M ?nj)??(?+M)
(11)
where nj =
?M
i=1 zij .
Update for ?
p(?j |Z) ? p(?j |?, ?,N)p(Z|?j)
Because of the conjugacy of the beta to Bernoulli
distribution, the posterior of ? is still a beta distribu-
tion:
?j ? Beta(
??
N
+ nj , ?(1?
?
N
) +M ? nj) (12)
Update for ?
p(?i|xi, Z, S) ? p(xi|?i, Z, S)p(?i|?
2
?)
The posterior is also a normal distribution with mean
?i and covariance ?i.
?i =
(
1
?2
S?i
T
S?i +
1
?2?
I
)
?1
(13)
?i = ?i
(
1
?2
S?i
T
xi
)
(14)
Here S?i ? S ? z?i and z?i ? [zi, ..., zi]T is a K ? N
matrix with the vector zi repeated K(the number of
the latent topics) times.
S?i = S ? z?i (15)
740
S?i
T
S?i = (S
TS) ? (zi ? zi
T +Bcovi) (16)
Bcovi = diag[zi1(1? zi1), ..., ziN (1? ziN )] (17)
Update for ?2
p(?2 |?, X, Z, S) ? p(X|?, Z, S, ?
2
 )p(?
2
 )
By using a conjugate prior, inverse gamma prior
InvGamma(u, v), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
u? = u+MK/2
v? = v +
1
2
M
?
i=1
(||xi ? S(zi ? ?i)||+ ?i)
(18)
where
?i =
?N
j=1(z
2
ij ? ?
2
ij ? s
T
j sj ? zij
2
? ?ij
2
? sTj sj)
+
?
j =l zij ? zil ??i,jl ? s
T
j sl
Update for ?2?
p(?2?|?) ? p(?|?
2
?)p(?
2
?)
By using a conjugate prior, inverse gamma prior
InvGamma(e, f), the posterior can be calculated
as a new inverse gamma distribution with parame-
ters
e? = e+MN/2
f ? = f +
1
2
M
?
i=1
(
(?)T?+ trace(??i)
)
(19)
5 Experiments
To test the capability of our BNP summarization sys-
tems, we design a series of experiments. The aim of
the experiments mainly includes three aspects:
1. To demonstrate the summaries extracted by our
model have good qualities and the summary
length determined by the model is reasonable.
2. To give examples where varying summary
length is necessary.
3. To observe the distribution of summary length.
We evaluate the performance on the dataset of
DUC2004 task2. The data contains 50 document
clusters, with 10 news articles in each cluster. Be-
sides, we construct three new datasets from the
DUC2004 dataset to further prove the advantage of
variable-length summarization. We separate each
cluster in the original dataset into two parts where
each has 5 documents, hence getting the Separate
Dataset; Then we randomly combine two origi-
nal clusters in the DUC2004 dataset, and get two
datasets called Combined1 and Combined2. Thus
each of the clusters in the combined datasets include
20 documents with two different themes.
5.1 Evaluation of Summary Qualities
First, we implement our BNP summarization model
on the DUC2004 dataset, with summary length not
limited. At the topic analysis step, we use the HDP
model and follow the inference in (Teh et al, 2006).
For the sentence selection step, we use the varia-
tional inference described in Section 4, where the
parameters in the beta process (5) are set as ? =
1, ? = 1. The summaries that we finally generate
have an average length of 164 words. We design sev-
eral popular unsupervised summarization systems
and compare them with our model.
? The Random model selects sentences randomly
for each document cluster.
? The MMR (Carbonell and Goldstein, 1998)
strives to reduce redundancy while maintaining
relevance. For generic summarization, we re-
place the query relevance with the relevance to
documents.
? The Lexrank model (Erkan and Radev, 2004) is
a graph-based method which choose sentences
based on the concept of eigenvector centrality.
? The Linear Representation model (Ma and
Wan, 2010) has the same assumption as ours
and it can be seen as an approximation of the
constant-length version of our model.
741











	


	



    
	


Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1374?1384,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Formalizing Word Sampling for Vocabulary Prediction
as Graph-based Active Learning
Yo Ehara
?
National Institute of
Information and Communications
Technology
ehara@nict.go.jp
Yusuke Miyao
National Institute of
Informatics
yusuke@nii.ac.jp
Hidekazu Oiwa
Issei Sato
Hiroshi Nakagawa
The University of Tokyo
{oiwa,sato}@r.dl.itc.u-tokyo.ac.jp
nakagawa@dl.itc.u-tokyo.ac.jp
Abstract
Predicting vocabulary of second language
learners is essential to support their lan-
guage learning; however, because of the
large size of language vocabularies, we
cannot collect information on the entire
vocabulary. For practical measurements,
we need to sample a small portion of
words from the entire vocabulary and pre-
dict the rest of the words. In this study, we
propose a novel framework for this sam-
pling method. Current methods rely on
simple heuristic techniques involving in-
flexible manual tuning by educational ex-
perts. We formalize these heuristic tech-
niques as a graph-based non-interactive
active learning method as applied to a spe-
cial graph. We show that by extending the
graph, we can support additional function-
ality such as incorporating domain speci-
ficity and sampling from multiple corpora.
In our experiments, we show that our ex-
tended methods outperform other methods
in terms of vocabulary prediction accuracy
when the number of samples is small.
1 Introduction
Predicting the vocabulary of second language
learners is essential to support them when they are
reading. Educational experts have been continu-
ously studying methods for measuring the size of
a learner?s vocabulary, i.e., the number of words
?
The main body of this work was done when the first
author was a Ph.D. candidate in the University of Tokyo and
the paper was later greatly revised when the first author was
a JSPS (Japan Society for the Promotion of Science) research
fellow (PD) at National Institute of Informatics. See http:
//yoehara.com/ for details.
the learner knows, over the decades (Meara and
Buxton, 1987; Laufer and Nation, 1999). Ehara
et al. (2012) formalized a more fine-grained mea-
surement task called vocabulary prediction. The
goal of this task is to predict whether a learner
knows a given word based on only a relatively
small portion of his/her vocabulary. This vocabu-
lary prediction task can be further used for predict-
ing the readability of texts. By predicting vocab-
ulary unknown to readers and showing the mean-
ing of those specific words to readers, Ehara et al.
(2013) showed that the number of documents that
learners can read increases.
Word sampling is essential for vocabulary pre-
diction. Because of the large size of language vo-
cabularies, we usually cannot collect information
on the entire vocabulary. For practical measure-
ments, we inevitably need to sample a small por-
tion of words from the entire vocabulary and then
predict the rest. We refer to this sampling tech-
nique as word sampling.
Word sampling can greatly affect the perfor-
mance of vocabulary prediction. For example, if
we consider only short everyday general domain
words such as ?cat? and ?dog? as samples, the rest
of the vocabulary is difficult to predict since learn-
ers likely know most of these words. To more ac-
curately measure a learner?s vocabulary, we ide-
ally must sample words that are representative of
the entire set of words. More specifically, we wish
to sample words such that if a learner knows these
words, he/she is likely to know the rest of the
words in the given vocabulary, and vice versa.
To our knowledge, however, all current studies
have relied on a simple heuristic method. In this
heuristic method, educational experts first some-
how create groups of words with the aim that the
words in a group are of similar difficulty for learn-
1374
ers. To create groups of words, the experts typi-
cally make use of word frequencies and sometimes
manually reclassify words based on experience.
Next, a fixed number of words are randomly sam-
pled from each group via a uniform distribution.
We call this approach heuristic word sampling.
In this study, we propose a novel framework
that formalizes word sampling as non-interactive
graph-based active learning based on weighted
graphs. In our approach, nodes of a graph corre-
spond to words, whereas the edge weights show
how similar the difficulty levels of a word pair
are. Unlike interactive active learning algorithms
used in the NLP community, which use expert an-
notators? human labels for sampling nodes, non-
interactive active learning algorithms exclude ex-
pert annotators? human labels from the protocol
(Ji and Han, 2012; Gu and Han, 2012). Given
a weighted graph and using only its structure,
without human labels, these algorithms sample
nodes that are important for classification with al-
gorithms called label propagation. Excluding an-
notators? human labels from the protocol is bene-
ficial for educational purposes since learners can
share the same set of sampled words via, for ex-
ample, printed handouts.
Formalizing the current methods as non-
interactive graph-based active learning enables us
to extend the sampling methods with additional
functionality that current methods cannot han-
dle without applying burdensome manual heuris-
tics because we can flexibly design the weighted
graphs fed to the active learning algorithms. In our
framework, this extension is achieved by extend-
ing the graph, namely, our framework can handle
domain specificity and multiple corpora.
Domains are important when one wants to mea-
sure the vocabulary of learners. For example, con-
sider measuring non-native English speakers tak-
ing computer science graduate courses. We may
want to measure their English vocabulary with an
emphasis on computer science rather than their
general English vocabulary. However, such an
extension is impossible via current methods, and
thus it is desirable to sample algorithms to be able
to handle domain specificity. Our framework can
incorporate domain specificity between words in
the form of edges between such words.
Handling multiple corpora is important when
we cannot single out which corpus we should rely
on. The current technique used by educational
experts to handle multiple corpora is to heuristi-
cally integrate multiple frequency lists from mul-
tiple corpora into a single list of words; however,
such manual integration is burdensome. Thus, au-
tomatic integration is desirable. Our framework
converts multiple corpora into graphs, merges
these graphs together, and then samples from the
merged graph.
Our contributions as presented in this paper are
summarized as follows:
1. We formalize word sampling for vocabulary
prediction as graph-based active learning.
2. Based on this formalization, we can perform
more flexible word sampling that can handle
domain specificity and multiple corpora.
The remaining parts of this paper are orga-
nized as follows. In ?2, we explain the problem
setting in detail. We first explain how existing
heuristic word sampling works and how it relies
on the cluster assumption from the viewpoint of
graphs. Then, we introduce existing graph-based
non-interactive active learning methods. In ?3,
we show that the existing heuristic word sampling
is merely a special case of a non-interactive ac-
tive learning method (Gu and Han, 2012). Pre-
cisely, the existing sampling is identical to the case
where a special graph called a ?multi-complete
graph? is fed to a non-interactive active learning
method. Since this method can take any weighted
graphs other than this special graph, this imme-
diately leads to a way of devising new sampling
methods by modifying graphs. ?4 explains exactly
how we can modify graphs for improving active
learning. ?5 evaluates the proposed method both
quantitatively and qualitatively, and ?6 concludes
our paper.
2 Problem Setting
2.1 Heuristic Word Sampling
A simple vocabulary estimation technique intro-
duced by educational experts is to use the fre-
quency rank of words in a corpus based on the
assumption that learners using words with similar
frequency ranks have a similar vocabulary (Laufer
and Nation, 1999). In accordance with this as-
sumption, they first group words by frequency
ranks in a corpus and then assume that words in
each group have a similar vocabulary status. For
example, they sampled words as follows:
1375
1. Rank words by frequency in a corpus.
2. Group words with frequency ranks from 1 to
1, 000 as Level 1000, words with frequency
ranks from 1, 001 to 2, 000 as Level 2000,
and so on.
3. Take 18 samples from Level 1000, another 18
samples from Level 2000, and so on.
The rationale behind this method is to treat
high-ranked and low-ranked words separately
rather than sample words from the entire vocabu-
lary. After sampling words, this sampling method
can be used for various measurements; for exam-
ple, Laufer and Nation (1999) used this method
to estimate the size of the learners? vocabulary
by simply adding 1, 000 ?
Correctly answered words
18
for
each level.
2.2 Cluster Assumption
In the previous subsection, we noted that existing
word sampling methods rely on the assumption
that words with similar frequency ranks are known
to learners whose familiar words are similar each
other. This assumption is known as the cluster as-
sumption in the field of graph studies (Zhou et al.,
2004).
To further describe the cluster assumption, we
first define graphs. A graph G = (V, E) consists
of a set of nodes (vertices) V and a set of edges E .
Here, each node has a label, and each edge has a
weight. A label denotes the category of its corre-
sponding node. For example, in binary classifica-
tion, a label is taken from {+1,?1}. A weight is
a real value; when the weight of an edge is large,
we describe the edge as being heavy.
The cluster assumption is an assumption that
heavily connected nodes in a graph should have
similar labels. In other words, the cluster as-
sumption states that weights of edges and labels
of nodes should be consistent.
We explain how the cluster assumption relates
to our task. In our application, each node corre-
sponds to a word. Labels of the nodes in a graph
denote the vocabulary of a learner. If he/she knows
a word, the label of the node corresponding to the
word is +1; if not, the label is ?1. The cluster
assumption in our application is that the heavier
the edge, the higher the similarity between users
familiar with the two words.
In this manner, existing word sampling meth-
ods implicitly assume cluster assumption. This
is therefore the underlying approach for reducing
the word sampling problem into graph-based ac-
tive learning. Since graphs allow for more flexible
modeling by changing the weights of edges, we
expect that more flexible word sampling will be
enabled by graph-based active learning.
2.3 Label Propagation
Since the graph-based active learning algorithms
are based on label propagation algorithms, we will
explain them first. Basically, given a weighted
graph, label propagation algorithms classify their
nodes in a weakly supervised manner. While the
graph-based active learning algorithm that we are
trying to use (Gu and Han, 2012) does not use la-
bel propagation algorithms? outputs directly, it is
tuned to be used with a state-of-the-art label prop-
agation method called Learning with Local and
Global Consistency (LLGC) (Zhou et al., 2004).
Label propagation algorithms predict the labels
of nodes from a few manually supervised labels
and graph weights. To this end, label propaga-
tion algorithms follow the following steps. First,
humans label a small subset of the nodes in the
graph. This subset of nodes is called the set of la-
beled nodes, and the remaining nodes are called
unlabeled nodes. Second, label propagation al-
gorithms propagate labels to the unlabeled nodes
based on edge weights. The rationale behind la-
bel propagation algorithms lies in cluster assump-
tion; as label propagation algorithms assume that
two nodes connected by a heavily weighted edge
should have similar labels, more heavily weighted
edges should propagate more labels.
We formalize Learning with Local and Global
Consistency (LLGC) (Zhou et al., 2004), one
of the state-of-the-art label propagation methods.
Here, for simplicity, suppose that we want to per-
form binary classification of nodes. Let N be the
total number of nodes in a graph. Then, we de-
note labels of each node by y
def
= (y
1
, . . . , y
N
)
>
.
For unlabeled nodes, y
i
is set to 0. For labeled
nodes, y
i
is set to +1 if the learner knows a word,
?1 if not. We also introduce a label propagation
(LP) score vector f = (f
1
, . . . , f
N
)
>
. This LP
score vector is the output of label propagation and
is real-valued. To obtain the classification result
from this real-valued LP score vector for an un-
labeled node (word) i, the learner is predicted to
know the word i if f
i
> 0, and he/she is predicted
to be unfamiliar with the word if f
i
? 0.
1376
Next, we formally define a normalized graph-
Laplacian matrix, which is used for penalization
based on the cluster assumption. Let an N ? N
-sized square matrix W be a weighted adjacency
matrix of G. W is symmetric and non-negative
definite; its diagonal elements W
i,i
= 0 and
all other elements are non-negative
1
. The graph
Laplacian of a normalized graph, known as a nor-
malized graph Laplacian matrix, is defined as
L
norm
W
def
= I?D
?
1
2
W
WD
?
1
2
W
. Here, D
W
is defined
as a diagonal matrix whose diagonal element is
(D
W
)
i,i
def
=
?
|V|
j=1
W
i,j
, and I denotes the iden-
tity matrix of the appropriate size. Note that a
normalized graph Laplacian L
norm
W
depends on the
weighted adjacency matrix W.
Then, LLGC can be formalized as a simple op-
timization problem as shown in Equation 1.
min
f
?f ? y?
2
2
+ ?f
>
L
norm
W
f (1)
Equation 1 consists of two terms. Intuitively,
the first term tries to make the LP score vector, the
final output f , as close as possible to the given la-
bels y. The second term is designed to meet the
cluster assumption: it penalizes the case where
two nodes with heavy edges have very different
LP scores. ? > 0 is the only hyper-parameter of
LLGC: it determines how strong the penalization
based on the cluster assumption should be. Thus,
in total, Equation 1 outputs an LP score vector f
considering both the labeled input y and the clus-
ter assumption of the given graph W: the heav-
ier an edge, the closer the scores of the two nodes
connected by the edge becomes.
2.4 Graph-based active learning algorithms
An important categorization of graph-based active
learning for applications is whether it is interactive
or non-interactive. Here, interactive approaches
use human labels during the learning process; they
present a node for humans to label, and based on
this label, the algorithms compute the next node to
be presented to the humans. Thus, in interactive
algorithms, human labeling and computations of
the next node must run concurrently.
Non-interactive algorithms do not use human
labels during the learning process. Given the
entire graph, these algorithms sample important
1
While all elements of a non-negative definite matrix are
not necessarily non-negative, we define all elements of W
as non-negative here, following the definition of Zhou et al.
(2004).
nodes for label propagation algorithms. Here, im-
portant nodes are the ones that minimize estimated
classification error of label propagation when the
nodes are labeled. Note that, unlike active learning
used in the NLP community, non-interactive active
learning algorithms exclude expert annotators? hu-
man labels from the protocol. While they exclude
expert annotators, they are still regarded as active
learning methods in the machine learning commu-
nity since they try to choose such nodes that are
beneficial for classification (Ji and Han, 2012; Gu
and Han, 2012).
For educational purposes, non-interactive algo-
rithms are preferred over interactive algorithms.
The main drawback of interactive algorithms is
that they must run concurrently with the hu-
man labeling. For our applications, this means
that the vocabulary tests for vocabulary prediction
must always be computerized. In contrast, non-
interactive algorithms allow us to have vocabulary
tests printed in the form of handouts, so we focus
on non-interactive algorithms throughout this pa-
per.
Compared with interactive algorithm studies,
such as Zhu et al. (2003), graph-based non-
interactive active learning algorithms have been
introduced in recent years. There has been a sem-
inal paper on non-interactive algorithms (Ji and
Han, 2012). We used Gu and Han?s algorithm be-
cause it reports higher accuracy for many tasks
with competitive computation times over Ji and
Han?s algorithm (Gu and Han, 2012).
These active learning methods share two basic
rules although their objective functions are dif-
ferent. First, these methods tend to select glob-
ally important nodes, also known as hubs. A no-
table example of global importance is the num-
ber of edges. Second, these methods tend to
avoid sampling nodes that are heavily connected
to previously sampled nodes. This is due to clus-
ter assumption, the assumption that similar nodes
should have similar labels, which suggests that it is
redundant to select nodes close to previously sam-
pled nodes; the labels of such nodes should be reli-
ably predicted from the previously sampled nodes.
Gu and Han?s algorithm, which is the algorithm
we used, also follows these rules. In this algo-
rithm, when considering the k-th sample, for every
node i in the current set of not-yet-chosen nodes, a
score score(k, i) is calculated, and the node with
the highest score is chosen. First, the score is de-
1377
signed to be large if the i-th node is globally im-
portant. In the algorithm, the global importance
of a node is measured by an eigenvalue decompo-
sition of the normalized graph-Laplacian, L
norm
.
Transformed from the graph?s adjacency matrix,
this matrix stores the graph?s global information.
Second, the score is designed to be smaller if the
i-th node is close to one of the previously sampled
nodes.
Score score (k, i) is defined as follows. We
perform eigenvalue decomposition beforehand.
L
norm
W
= U?U
>
, u
i
is the transpose of the i-th
row of U, and ?
i
is its corresponding eigenvalue.
score (k, i)
def
=
(
H
?1
k
u
i
)
>
?
?1
(
H
?1
k
u
i
)
1 + u
>
i
H
?1
k
u
i
(2)
In Equation 2, H
k
preserves information of the
previous k ? 1 samples. First, H
0
is a diag-
onal matrix whose i-th diagonal element is de-
fined as
1
(??
i
+1)
2
?1
where ? is a hyper-parameter.
H
0
weighs the score of globally important nodes
through the eigenvalue decomposition. Second,
H
k
is updated such that the scores of the nodes
distant from the previously taken samples are
higher. The precise update formula of H
k
follows.
i
k+1
is the index of the node sampled at k + 1-th
round. For the derivation of this formula, see Gu
and Han (2012).
H
?1
k+1
= H
?1
k
?
(
H
?1
k
u
i
k+1
) (
H
?1
k
u
i
k+1
)
>
1 + u
>
i
k+1
H
?1
k
u
i
k+1
(3)
Hyper-parameter ? determines how strong the
cluster assumption should be; the larger the value,
the more strongly the algorithm avoids selecting
nodes near previously selected samples over the
graph. Note that ? is inherited from the LLGC
2
algorithm (Zhou et al., 2004), i.e., the label prop-
agation algorithm that Gu and Han?s algorithm is
based on. From the optimization viewpoint, ? de-
termines the degree of penalization.
Remember that the score has nothing to do with
the LP scores described in ?2.3. score is used
to choose nodes used for training in the graph-
based non-interactive active learning. LP scores
are later used for classification by label propaga-
tion algorithms that use the chosen training nodes.
Throughout this paper, when we mean LP scores,
we explicitly write ?LP scores?. All the other
scores mean score.
2
Learning with Local and Global Consistency.
Figure 1: Converting frequency list into multiple-
complete graph.
3 Formalizing heuristic word sampling
as graph-based active learning
Figure 1 shows how to formalize a word frequency
list into a multiple complete graph. The word fre-
quency list is split into clusters, and each cluster
forms a complete graph. Each node in a graph cor-
responds to a word. By gathering all the complete
graphs, a multiple complete graph can be formed.
Multiple complete graph G
T,n
is defined as a
graph of T complete graphs, each of which con-
sists of n nodes fully connected within the n
nodes. An example of a multiple complete graph
can be seen in Figure 2. We can define the
Tn ? Tn adjacency matrix for multiple com-
plete graphs. W
complete
all
is defined as follows:
W
complete
all
def
=
?
?
?
?
?
?
W
complete
0 ? ? ? 0
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0 ? ? ? 0 W
complete
?
?
?
?
?
?
(4)
W
complete
def
=
?
?
?
?
?
?
?
?
0 1 ? ? ? 1 1
1 0 1 ? ? ? 1
.
.
. 1
.
.
.
.
.
.
1
.
.
.
.
.
.
1
1 1 ? ? ? 1 0
?
?
?
?
?
?
?
?
(5)
We can see that W
complete
all
is a block-diagonal
matrix where each block is a n ? n matrix,
W
complete
.
Heuristic word sampling can be rewritten into
non-interactive active learning on graphs. Suppose
there are T groups, each of which has n words,
and we want to sample n
0
words from each. In
1378
Figure 2: Example of multi-complete graph, where Theorem 3.1 holds true. Here, T = 4, n = 5, and
k = 10; 10 light blue (light) nodes have already been sampled, and 10 blue (dark) nodes remain; the
11-th node is sampled uniformly randomly from the nodes within the red rectangles.
heuristic word sampling, for each group from T
groups, n
0
words are sampled from the n words
in the group uniformly randomly. Thus, there are
Tn
0
words in total.
Since heuristic word sampling takes a node
from each of the T groups, T concurrent sampling
processes are involved. For simplicity, we further
express the same sampling using only one sam-
pling process from the entire graph as follows:
? For every round, we sample words uniformly
randomly from the remaining words of the
groups where the number of samples selected
in previous rounds is least.
Figure 2 shows an example of this sampling
process. Here, the second and third groups from
the left are the groups in which the number of pre-
viously selected nodes is the least. This is because
they have only two previously selected nodes,
while the others have three. Thus, in the figure, the
remaining words of the groups are the nodes with
red rectangles. Randomly sampling one node from
the nodes with red rectangles means sampling a
node from the second or third group. We call the
set of nodes in a graph from which samples will be
taken in the next round a seed pool. Thus, in Fig-
ure 2, the set of nodes with red rectangles is the
seed pool. Nodes that have already been sampled
are taken out of the current seed pool.
Next, we more formally explain the seed pool
concept. We start sampling nodes from a multiple
complete graph via the algorithm presented by Gu
and Han. The initial seed pool is set to all nodes
in the graph, i.e., V . We sample one node in each
round; thus, k ? |V| nodes are selected by the k-
th round. Let t ? T be the index of the complete
graph in the multiple complete graph. Then, the
following theorem holds with  being a small pos-
itive value that substitutes the 0 eigenvalues in the
eigen decomposition.
Theorem 3.1 Let 0 <  < 1 and n ?
{2, 3, 4, . . .}. Then, among T complete graphs,
k mod T complete graphs have b
k
T
c + 1 sam-
ples, and the remaining graphs have b
k
T
c sam-
ples
3
. Moreover, the (k + 1)-th sample is taken
uniformly randomly from the remaining complete
graphs.
In Theorem 3.1,  > 0 is a substitute for the
0 eigenvalue of L
W
4
. Since  is a substitute for
the 0 eigenvalue, it is rational to assume 1 > .
Also, remember that n is the number of nodes in
one complete graph. The algorithm stops when
k = Tn
0
+ 1, i.e., at the Tn
0
+ 1-th round when
there are no remaining nodes to sample. Figure 2
shows an example of Theorem 3.1.
A proof of this theorem is presented in the sup-
plementary material. Briefly, in a multiple com-
plete graph, the score of a node depends only on
the complete graph or the cluster that the node
belongs to. Thus, we only have to consider one
complete graph in which k is the number of nodes
that have been already chosen. Then, mathe-
matical induction proves that, within one com-
plete graph, all the not-yet-chosen nodes have the
same score(k, i). Second, we have to show that
the score always decreases by taking a sample,
i.e., score(k, i) > score(k + 1, i). By a long
but straightforward calculation, we can express
score(k, i) by using only ?, , n, and k. Then, by
substituting the formula to score(k, i), we obtain
score(k, i)? score(k + 1, i) > 0.
4 Extending Graphs
In the previous section, we explained how to for-
malize heuristic word sampling as active learn-
ing on multiple complete graphs. This formaliza-
3
Here, both k and T are non-negative integers. Thus,
k%T denotes the remainder of the division of k by T , and
b
k
T
c is the quotient of the division.
4
In Gu and Han?s algorithm, they substitute the 0 eigen-
value with a small positive value , and they set  = 10
?6
.
1379
Figure 3: Example of merging two graphs.
tion can lead to better active learning by extend-
ing these graphs. In this section, we describe such
graph extensions.
We extend graphs by merging graphs. Figure 3
shows how to merge graphs. We define ?merging?
two weighted graphs as creating a weighted graph
whose adjacency matrix is the sum of the two ad-
jacency matrices of the two weighted graphs. This
suggests that an edge of the merged graph is sim-
ply the sum of the corresponding edges of the two
weighted graphs.
The merged graph is expected to inherit the
characteristics of its original graphs. Thus, ap-
plying graph-based active learning to the merged
graph is expected to sample nodes in accordance
with the characteristics of its original graphs.
For example, if we merge a graph representing
domain-specific relations and a multiple complete
graph representing difficulty grouping of words,
active learning from the resulting merged graph
is expected to sample words considering both do-
main specificity and difficulty grouping of words.
For another example, suppose we merge two
multiple complete graphs created from frequency
lists from two different corpora. Then, active
learning from the resulting merged graph is ex-
pected to sample words taking into account fre-
quency lists from both corpora.
5 Evaluation
We evaluate our proposed method both quantita-
tively and qualitatively. In the quantitative eval-
uation, we measure the prediction accuracy of
graphs. Note that the heuristic word sampling
method is identical to using Gu and Han?s algo-
rithm with a multiple complete graph; however,
our proposed graphs have enriched relations be-
tween words. In the qualitative evaluation, we ex-
plain in detail what words are appropriate as train-
ing examples for vocabulary prediction by pre-
senting sampled examples.
5.1 Quantitative evaluation
To evaluate the accuracy of vocabulary prediction,
we used the dataset that Ehara et al. (2010) and
Ehara et al. (2012) used. This dataset was gleaned
from questionnaires answered by 15 English as a
second language (ESL) learners. Every learner
was asked to answer how well he/she knew 11,999
English words. The data was collected in January
2009. One learner was unpaid, whereas the other
15 learners were paid. We used the data from the
15 paid learners since the data from the unpaid
learner was noisy. Most of the learners were na-
tive Japanese speakers and graduate students. Be-
cause most of the learners in this dataset were na-
tive Japanese speakers, words from SVL 12,000
(SPACE ALC Inc., 1998) were used for the learn-
ers in this dataset. Note that SVL 12,000 is a col-
lection of 12,000 words that are deemed important
for Japanese learners of English, as judged by na-
tive English teachers.
Next, we required frequency lists for the words
that appeared in the dataset. To create frequency
lists, lemmatization is important because the num-
ber of word types depends on the method used
to lemmatize the words. Note that in the field of
vocabulary measurement, lemmatization is mainly
performed by ignoring conjugation (Nation and
Beglar, 2007). Lemmatizing the dataset resulted
in a word list of 8,463 words. We adjusted the size
of the word list to a round 8,000 by removing 463
randomly chosen words. Note that all constituent
words were labeled by the 15 ESL learners.
We created the following four graphs by span-
ning edges among the 8, 000 words.
BNC multi-complete This graph corresponds to
heuristic word sampling and served as our
baseline. It is a multiple complete graph
comprising eight complete graphs, each of
which consisted of 1,000 words based on the
sorted frequency list from the British Na-
tional Corpus (BNC). We chose the BNC be-
cause the method presented by Nation and
Beglar was based on it (Nation and Beglar,
2007). Note that all edge weights are set to 1.
BNC+domain To form this graph, edges rep-
resenting domain specificity are added to
the ?BNC multi-complete? graph. For do-
main specificity, we used domain information
1380
from WordNet 3.0.
5
First, we extracted 102
domain-specific words under the ?computer?
domain among the 8,000 words and created
a complete graph consisting of these domain-
specific words. The edge weights of the com-
plete graph were set to 1. Next, we simply
merged
6
the complete graph consisting of the
domain-specific words with the ?BNC multi-
complete? graph.
BNC+COCA In addition to the ?BNC multi-
complete? graph, edges based on another cor-
pus, the Corpus of Contemporary American
English (COCA), were introduced. We first
created the COCA multi-complete graph, a
multiple complete graph consisting of eight
complete graphs, each of which consisted of
1,000 words based on the sorted frequency
list using COCA. The edge weights of the
COCA multi-complete graph were set to 1.
Next, we merged the BNC multi-complete
and COCA multi-complete graphs to form
the ?BNC + COCA graph?.
BNC+domain+COCA This graph is the graph
produced by merging the ?BNC + domain?
and ?BNC + COCA? graphs.
Note that our experiment setting differed from
the usual label propagation setting used for semi-
supervised learning because the purpose of our
task differed. In the usual label propagation set-
ting, the ?test? nodes (data) are prepared sepa-
rately from the training nodes to determine how
accurately the algorithm can classify forthcoming
or unseen nodes. However, in our setting, there
were no such forthcoming words. Of course, there
will always be words that do not emerge, even in a
large corpus; however, such rare words are too dif-
ficult for language learners to identify, and many
are proper nouns, which are not helpful for mea-
suring the vocabulary of second language learners.
Therefore, our focus here is to measure how
well the learners know a fixed set of words, that
is, the given 8,000 words. Even if an algorithm
can achieve high accuracy for words outside this
fixed set, we have no way of evaluating it using
the pooled annotations. Here, we want to measure,
from a fixed number of samples (e.g., 50), how ac-
curately an algorithm can predict a learner?s vo-
5
We used the NLTK toolkit http://nltk.org/ to extract the
domain information.
6
Definition of how to merge two graphs is in ?4.
Figure 4: Results of our quantitative experiments.
Vertical axis denotes accuracy, and horizontal axis
shows number of samples, i.e., training words.
cabulary for the entire 8,000 words. Thus, we
define accuracy to be the number of words that
each algorithm finds correctly divided by the vo-
cabulary size. We set hyper-parameter ? to 0.01
as Gu and Han (2012) did. Note that this hyper-
parameter is reportedly not sensitive to accuracy
(Zhou et al., 2011).
Figure 4 and Table 1 show the results of the
experiment over the different datasets. The ver-
tical axis in the figure denotes accuracy, whereas
the horizontal axis denotes the number of samples,
i.e., training words. Note that the accuracy is av-
eraged over 15 learners and that LLGC is used for
classification unless otherwise specified. For ex-
ample, ?BNC multi-complete? indicates that sam-
ples taken from the BNC multi-complete graph are
used for training, and LLGC is used for classifica-
tion. Note that ?BNC + domain + COCA (SVM)?
uses a support vector machine (SVM) for classifi-
cation, and ?BNC + domain + COCA (LR)? uses
logistic regression (LR) for classification. Among
many supervised machine learning methods, we
chose SVM and LR because SVM is widely used
in the NLP community, and LR was used for the-
oretical reasons (Ehara et al., 2012; Ehara et al.,
2013).
SVM and LR require features of a word
for classification while LLGC requires a
weighted graph of words. Since the graph
?BNC+domain+COCA? is made from three
features, namely the word frequencies of BNC
1381
Table 1: Results of our quantitative experiments. LLGC is used for classification unless otherwise spec-
ified. Bold letters indicate top accuracy. Asterisks (*) indicate that values are statistically significant
against baseline, heuristic sampling, i.e., ?BNC multi-complete? (using sign test p < 0.01).
10 15 20 30 40 50
BNC multi-complete 64.15 (%) 67.54 73.73 73.66 74.92 74.82
BNC+domain 65.27 71.88 72.88 75.02 76.03 * 75.95
BNC+COCA 73.45 74.10 74.57 74.90 74.96 75.29
BNC+domain+COCA 75.23 * 75.71 * 75.18 * 75.35 * 75.47 76.44 *
BNC+domain+COCA (SVM) 58.99 57.74 60.44 70.79 69.29 74.46
BNC+domain+COCA (LR) 60.29 61.74 59.27 69.17 70.63 73.42
and COCA corpora and whether a word is in the
computer domain, we used these features for the
features of SVM and LR in this experiment for a
fair comparison. When using word frequencies for
features, we used the logarithm of raw frequencies
since it is reported to work well (Ehara et al.,
2013). SVM and LR are also known to heavily
depend on a hyper-parameter called C, which
determines the strength of regularization. We
tried C = 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 50.0,
and 100.0 for each of SVM and LR where the size
of training data is 50 and chose the C value that
performs best. As a result, we set C = 5.0 for
SVM and C = 50.0 for LR. Note that this setting
is advantageous for SVM and LR compared to
LLGC because the hyper-parameters of SVM
and LR are tuned while LLGC?s hyper-parameter
remains untuned. For the implementation of SVM
and LR, we used the ?scikit-learn? package in
Python
7
.
We first observed that our proposed methods
constantly outperform the baseline, heuristic word
sampling, i.e., ?BNC multi-complete? in Table 1.
This indicates that we successfully obtained bet-
ter accuracy by formalizing heuristic word sam-
pling as active learning and extending graphs. In
Table 1, the accuracy of the top-ranked methods
(shown using bold letters) is statistically signif-
icantly better than the accuracy of ?BNC multi-
complete? (using the sign test p < 0.01).
We then observed that ?BNC multi-complete?
and ?BNC + domain? show competitive accuracy
with sample sizes from 10 to 20; furthermore,
?BNC + domain? is slightly better than ?BNC
multi-complete? with sample sizes ranging from
30 to 50 (statistically significant p < 0.01 using
sign test). Next, we note that there is a trade-off
between domain and word frequency when choos-
7
http://scikit-learn.org/stable/
ing samples. More specifically, if we select too
many words from the domain, the measurement of
the general English ability of learners can be in-
accurate; conversely, if we select too many words
from the corpus-based word frequency list, while
the general English ability of learners is accu-
rately measured, we may obtain no information
on the learner?s vocabulary for the targeted do-
main. The competitive or slightly better accuracy
of ?BNC + domain? over ?BNC multi-complete?
shows that ?BNC + domain? could successfully
integrate domain information into the frequency-
based groups without deteriorating measurements
of general English ability.
We also observe that ?BNC + COCA? greatly
outperforms ?BNC multi-complete? when the
number of samples is 10. This shows that the inte-
gration of the two corpora, BNC and COCA (i.e.,
?BNC + COCA?), successfully increases the accu-
racy when there are only a small number of sam-
ples.
?BNC + domain + COCA? achieves the best ac-
curacy of all the graphs except when the number
of samples is 40. This indicates that the domain
information and the information from the COCA
corpus helped one another to improve the accuracy
because ?BNC + domain? and ?BNC + COCA? in-
troduce different types of domain information into
?BNC multi-complete.?
Finally, we observe that ?BNC + domain +
COCA (SVM)? and ?BNC + domain + COCA
(LR)? perform worse than LLGC over the same
dataset for all sample sizes, particularly when the
size of the training data is small. Since LLGC is
a semi-supervised classifier while SVM and LR
are not, SVM and LR perform poorly for small
amounts of training data. This result shows that
LLGC is appropriate for this task compared to
SVM because, in this task, an increase in the size
1382
Table 2: Computer-related samples in top 30 sam-
ples.
Name Num. of
Samples
Examples
BNC multi-
complete
0 -
BNC+domain 5 input, client, field,
background, regis-
ter
BNC+COCA 0 -
BNC+domain
+COCA
3 drive, client, com-
mand
of training data directly leads to an increased bur-
den on the human learners.
5.2 Qualitative evaluation
In this subsection, we qualitatively evaluate our
results to determine the types of nodes that are
sampled when domain specificity is introduced.
Specifically, we evaluate what words are selected
as samples in the ?BNC + domain? graph.
As noted above, in the ?BNC + domain? graph,
the computer science domain is introduced into
?BNC multi-complete? to measure learners? vo-
cabulary with a specific emphasis on the computer
science domain. Thus, it is desirable that some
words in the computer science domain are sam-
pled from the ?BNC + domain? graph; otherwise,
we need to predict the learners? vocabulary for
the computer science domain from general words
rather than those in the computer science domain,
which is extremely difficult.
Table 2 shows the number of words in the com-
puter science domain sampled in the first 30 sam-
ples. Note that only ?BNC + domain? and ?BNC
+ domain + COCA? select samples from the com-
puter science domain. This indicates that in the
other two methods, to measure vocabulary with
an emphasis on the computer science domain, we
need to predict learners? vocabulary from the gen-
eral words, which is almost impossible with only
30 samples. Furthermore, it is interesting to note
that ?BNC + domain? and ?BNC + domain +
COCA? select different samples from the com-
puter science domain, except for the word ?client,?
although originally the same computer science do-
main wordlist was introduced to both graphs.
Since ?BNC + domain? achieves competitive
or slightly better accuracy than ?BNC multi-
complete? in the quantitative analysis and the
qualitative analysis, we conclude that our method
can successfully introduce domain specificity into
the sampling methodology without reducing accu-
racy.
6 Conclusion
In this study, we propose a novel sampling frame-
work that measures the vocabulary of second lan-
guage learners. We call existing sampling meth-
ods heuristic sampling. This approach to sampling
ranks words from a single corpus by frequency and
creates groups of 1,000 words. Next, tens of words
are sampled from each group. This method as-
sumes that the relative difficulty of all 1,000 words
is the same.
In this paper, we introduce a novel sampling
method by showing that the existing heuristic sam-
pling approach is simply a special case of a graph-
based active learning algorithm by Gu and Han
(2012) applied to a special graph. We also pro-
pose a method to extend this graph to enable us to
handle domain specificity of words and multiple
corpora, which are difficult or impossible to han-
dle using current methods.
We evaluate our method both quantitatively and
qualitatively. In our quantitative evaluation, the
proposed method achieves higher prediction accu-
racy compared with the current approach to vo-
cabulary prediction. This suggests that our pro-
posed method can successfully make use of do-
main specificity and multiple corpora for pre-
dicting vocabulary. In our qualitative evaluation,
we examine the words sampled by our proposed
method and observe that targeted domain-specific
words are successfully sampled.
For our future work, because the graph used
in this paper was constructed manually, we plan
to automatically create a graph suitable for active
learning and classification. There are several algo-
rithms that create graphs from feature-based rep-
resentations of words, but these have never been
used for active learning of this task.
Acknowledgments
This work was supported by the Grant-in-Aid for
JSPS Fellows (JSPS KAKENHI Grant Number
12J09575).
1383
References
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2010. Personalized reading
support for second-language web documents by col-
lective intelligence. In Proceedings of the 15th in-
ternational conference on Intelligent user interfaces
(IUI 2010), pages 51?60, Hong Kong, China. ACM.
Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-
agawa. 2012. Mining words in the minds of second
language learners: learner-specific word difficulty.
In Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 2012),
Mumbai, India, December.
Yo Ehara, Nobuyuki Shimizu, Takashi Ninomiya, and
Hiroshi Nakagawa. 2013. Personalized reading
support for second-language web documents. ACM
Transactions on Intelligent Systems and Technology,
4(2).
Quanquan Gu and Jiawei Han. 2012. Towards active
learning on graphs: An error bound minimization
approach. In Proceedings of the IEEE International
Conference on Data Mining (ICDM) 2012.
Ming Ji and Jiawei Han. 2012. A variance minimiza-
tion criterion to active learning on graphs. In Pro-
ceedings of the 15th international conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Batia Laufer and Paul Nation. 1999. A vocabulary-
size test of controlled productive ability. Language
testing, 16(1):33?51.
Paul Meara and Barbara Buxton. 1987. An alterna-
tive to multiple choice vocabulary tests. Language
Testing, 4(2):142?154.
Paul Nation and David Beglar. 2007. A vocabulary
size test. The Language Teacher, 31(7):9?13.
SPACE ALC Inc. 1998. Standard vocabulary list
12,000.
Dengyong Zhou, Oliver Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Sch?olkopf. 2004.
Learning with local and global consistency. In Pro-
ceedings in 18th Annual Conference on Neural In-
formation Processing Systems (NIPS), pages 321?
328.
Xueyuan Zhou, Mikhail Belkin, and Nathan Srebro.
2011. An iterated graph laplacian approach for rank-
ing on manifolds. In Proceedings of 17th ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining (KDD), pages 877?885.
Xiaojin Zhu, John Lafferty, and Zoubin Ghahra-
mani. 2003. Combining active learning and semi-
supervised learning using gaussian fields and har-
monic functions. In Proceedings of ICML 2003
workshop on The Continuum from Labeled to Unla-
beled Data in Machine Learning and Data Mining.
1384
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 721?729,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Reducing Wrong Labels in Distant Supervision for Relation Extraction
Shingo Takamatsu
System Technologies Laboratories
Sony Corporation
5-1-12 Kitashinagawa, Shinagawa-ku, Tokyo
Shingo.Takamatsu@jp.sony.com
Issei Sato and Hiroshi Nakagawa
Information Technology Center
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo
{sato@r., n3@}dl.itc.u-tokyo.ac.jp
Abstract
In relation extraction, distant supervision
seeks to extract relations between entities
from text by using a knowledge base, such as
Freebase, as a source of supervision. When
a sentence and a knowledge base refer to the
same entity pair, this approach heuristically la-
bels the sentence with the corresponding re-
lation in the knowledge base. However, this
heuristic can fail with the result that some sen-
tences are labeled wrongly. This noisy labeled
data causes poor extraction performance. In
this paper, we propose a method to reduce
the number of wrong labels. We present a
novel generative model that directly models
the heuristic labeling process of distant super-
vision. The model predicts whether assigned
labels are correct or wrong via its hidden vari-
ables. Our experimental results show that this
model detected wrong labels with higher per-
formance than baseline methods. In the ex-
periment, we also found that our wrong label
reduction boosted the performance of relation
extraction.
1 Introduction
Machine learning approaches have been developed
to address relation extraction, which is the task of
extracting semantic relations between entities ex-
pressed in text. Supervised approaches are limited in
scalability because labeled data is expensive to pro-
duce. A particularly attractive approach, called dis-
tant supervision (DS), creates labeled data by heuris-
tically aligning entities in text with those in a knowl-
edge base, such as Freebase (Mintz et al, 2009).
   	  

     
  
    
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 138?143,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Features for Detecting Hedge Cues
Nobuyuki Shimizu
Information Technology Center
The University of Tokyo
shimizu@r.dl.itc.u-tokyo.ac.jp
Hiroshi Nakagawa
Information Technology Center
The University of Tokyo
n3@dl.itc.u-tokyo.ac.jp
Abstract
We present a sequential labeling approach
to hedge cue detection submitted to the bi-
ological portion of task 1 for the CoNLL-
2010 shared task. Our main approach is
as follows. We make use of partial syntac-
tic information together with features ob-
tained from the unlabeled corpus, and con-
vert the task into one of sequential BIO-
tagging. If a cue is found, a sentence is
classified as uncertain and certain other-
wise. To examine a large number of fea-
ture combinations, we employ a genetic al-
gorithm. While some features obtained by
this method are difficult to interpret, they
were shown to improve the performance of
the final system.
1 Introduction
Research on automatically extracting factual in-
formation from biomedical texts has become pop-
ular in recent years. Since these texts are abundant
with hypotheses postulated by researchers, one
hurdle that an information extraction system must
overcome is to be able to determine whether or not
the information is part of a hypothesis or a factual
statement. Thus, detecting hedge cues that indi-
cate the uncertainty of the statement is an impor-
tant subtask of information extraction (IE). Hedge
cues include words such as ?may?, ?might?, ?ap-
pear?, ?suggest?, ?putative? and ?or?. They also
includes phrases such as ?. . .raising an intriguing
question that. . .? As these expressions are sparsely
scattered throughout the texts, it is not easy to gen-
eralize results of machine learning from a training
set to a test set. Furthermore, simply finding the
expressions listed above does not guarantee that
a sentence contains a hedge. Their function as a
hedge cue depends on the surrounding context.
The primary objective of the CoNLL-2010
shared task (Farkas et al, 2010) is to detect hedge
cues and their scopes as are present in biomedi-
cal texts. In this paper, we focus on the biological
portion of task 1, and present a sequential labeling
approach to hedge cue detection. The following
summarizes the steps we took to achieve this goal.
Similarly to previous work in hedge cue detec-
tion (Morante and Daelemans, 2009), we first con-
vert the task into a sequential labeling task based
on the BIO scheme, where each word in a hedge
cue is labeled as B-CUE, I-CUE, or O, indicating
respectively the labeled word is at the beginning
of a cue, inside of a cue, or outside of a hedge
cue; this is similar to the tagging scheme from
the CoNLL-2001 shared task. We then prepared
features, and fed the training data to a sequential
labeling system, a discriminative Markov model
much like Conditional Random Fields (CRF), with
the difference being that the model parameters are
tuned using Bayes Point Machines (BPM), and
then compared our model against an equivalent
CRF model. To convert the result of sequential
labeling to sentence classification, we simply used
the presence of a hedge cue, i.e. if a cue is found, a
sentence is classified as uncertain and certain oth-
erwise.
To prepare features, we ran the GENIA tag-
ger to add partial syntactic parse and named en-
tity information. We also applied Porter?s stem-
mer (Jones and Willet, 1997) to each word in the
corpus. For each stem, we acquired the distribu-
tion of surrounding words from the unlabeled cor-
pus, and calculated the similarity between these
distributions and the distribution of hedge cues in
the training corpus. Given a stem and its similari-
ties to different hedge cues, we took the maximum
similarity and discretized it. All these features are
passed on to a sequential labeling system. Using
these base features, we then evaluated the effects
of feature combinations by repeatedly training the
system and selecting feature combinations that in-
creased the performance on a heldout set. To au-
138
tomate this process, we employed a genetic algo-
rithm.
The contribution of this paper is two-fold. First,
we describe our system, outlined above, that we
submitted to the CoNLL-2010 shared task in more
detail. Second, we analyze the effects of partic-
ular choices we made when building our system,
especially the feature combinations and learning
methods.
The rest of this paper is organized as follows.
In Section 2, we detail how the task of sequential
labeling is formalized in terms of linear classifi-
cation, and explain the Viterbi algorithm required
for prediction. We next present several algorithms
for optimizing the weight vector in a linear classi-
fier in Section 3. We then detail the complete list
of feature templates we used for the task of hedge
cue detection in Section 4. In order to evaluate the
effects of feature templates, in Section 5, we re-
move each feature template and find that several
feature templates overfit the training set. We fi-
nally conclude with Section 6.
2 Sequential Labeling
We discriminatively train a Markov model us-
ing Bayes Point Machines (BPM). We will first
explain linear classification, and then apply a
Markov assumption to the classification formal-
ism. Then we will move on to BPM. Note that
we assume all features are binary in this and up-
coming sections as it is sufficient for the task at
hand.
In the setting of sequential labeling, given the
input sequence x = (x1, x2, x3, ...xn), a system
is asked to produce the output sequence y =
(y1, y2, y3, ...yn). Considering that y is a class,
sequential labeling is simply a classification with
a very large number of classes. Assuming that the
problem is one of linear classification, we may cre-
ate a binary feature vector ?(x) for an input x and
have a weight vector wy of the same dimension
for each class y. We choose a class y that has the
highest dot product between the input vector and
the weight vector for the class y. For binary classi-
fication, this process is very simple: compare two
dot product values. Learning is therefore reduced
to specifying the weight vectors.
To follow the standard notations in sequential
labeling, let weight vectors wy be stacked into
one large vector w, and let ?(x,y) be a binary
feature vector such that w>?(x,y) is equal to
w>y ?(x). Classification is to choose y such that
y = argmaxy?(w>?(x,y?)).
Unfortunately, a large number of classes created
out of sequences makes the problem intractable,
so the Markov assumption factorizes y into a se-
quence of labels, such that a label yi is affected
only by the label before and after it (yi?1 and yi+1
respectively) in the sequence. Each structure, or
label y is now associated with a set of the parts
parts(y) such that y can be recomposed from the
parts. In the case of sequential labeling, parts con-
sist of states yi and transitions yi ? yi+1 between
neighboring labels. We assume that the feature
vector for an entire structure y decomposes into
a sum over feature vectors for individual parts as
follows: ?(x,y) =?r?parts(y) ?(x, r). Note that
we have overloaded the symbol ? to apply to either
a structure y or its parts r.
The Markov assumption for factoring labels lets
us use the Viterbi algorithm (much like a Hidden
Markov Model) in order to find
y = argmaxy? (w>?(x,y?))
= argmaxy? (
?n
j=1w
>?(x, y?j)
+
?n?1
j=1 w
>?(x, y?j ? y?j+1)).
3 Optimization
We now turn to the optimization of the weight pa-
rameter w. We compare three approaches ? Per-
ceptron, Bayes Point Machines and Conditional
Random Fields, using our c++ library for struc-
tured output prediction 1.
Perceptron is an online update scheme that
leaves the weights unchanged when the predicted
output matches the target, and changes them when
it does not. The update is:
wk := wk ? ?(xi,y) + ?(xi,yi).
Despite its seemingly simple update scheme, per-
ceptron is known for its effectiveness and perfor-
mance (Collins, 2002).
Conditional Random Fields (CRF) is a condi-
tional model
P (y|x) = 1Zx exp(w
>?(x,y))
where w is the weight for each feature and Zx is a
normalization constant for each x.
Zx =
?
y
exp(w>?(x,y))
1Available at http://soplib.sourceforge.net/
139
for structured output prediction. To fit the weight
vector w using the training set {(xi,yi)}ni=1, we
use a standard gradient-descent method to find the
weight vector that maximizes the log likelihood?n
i logP (yi|xi) (Sha and Pereira, 2003). To
avoid overfitting, the log likelihood is often pe-
nalized with a spherical Gaussian weight prior:?n
i logP (yi|xi) ? C||w||2 . We also evaluated thispenalized version, varying the trade-off parameter
C.
Bayes Point Machines (BPM) for structured
prediction (Corston-Oliver et al, 2006) is an en-
semble learning algorithm that attempts to set the
weight w to be the Bayes Point which approxi-
mates to Bayesian inference for linear classifiers.
Assuming a uniform prior distribution over w, we
revise our belief of w after observing the training
data and produce a posterior distribution. We cre-
ate the final wbpm for classification using a poste-
rior distribution as follows:
wbpm = Ep(w|D)[w] =
|V (D)|?
i=1
p(wi|D)wi
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the ex-
pectation taken with respect to this distribution.
V (D) is the version space, which is the set of
weightswi that classify the training data correctly,
and |V (D)| is the size of the version space. In
practice, to explore the version space of weights
consistent with the training data, BPM trains a few
different perceptrons (Collins, 2002) by shuffling
the samples. The approximation of Bayes Point
wbpm is the average of these perceptron weights:
wbpm = Ep(w|D)[w] ?
K?
k=1
1
Kwk.
The pseudocode of the algorithm is shown in Al-
gorithm 3.1. We see that the inner loop is simply
a perceptron algorithm.
4 Features
4.1 Base Features
For each sentence x, we have state features, rep-
resented by a binary vector ?(x, y?j) and transition
features, again a binary vector ?(x, y?j ? y?j+1).
For transition features, we do not utilize lexical-
ized features. Thus, each dimension of ?(x, y?j ?
Algorithm
3.1: BPM(K,T, {(xi,yi)}ni=1)
wbpm := 0;
for k := 1 to K
Randomly shuffle the sequential order of
samples {(xi,yi)}ni=1
wk := 0;
for t := 1 to T # Perceptron iterations
for i := 1 to n # Iterate shuffled samples
y := argmaxy?(w>k ?(xi,y?))if (y 6= yi)
wk := wk ? ?(xi,y) + ?(xi,yi);
wbpm := wbpm + 1Kwk;return (wbpm)
y?j+1) is an indicator function that tests a com-
bination of labels, for example, O?B-CUE, B-
CUE?I-CUE or I-CUE?O.
For state features ?(x, y?j), the indicator func-
tion for each dimension tests a combination of
y?j and lexical features obtained from x =
(x1, x2, x3, ...xn). We now list the base lexical
features that were considered for this experiment.
F 0 a token, which is usually a word. As a part of
preprocessing, words in each input sentence
are tokenized using the GENIA tagger 2. This
tokenization coincides with Penn Treebank
style tokenization 3.
We add a subscript to indicate the position. F 0j is
exactly the input token xj . From xj , we also create
other lexical features such as F 1j , F 2j , F 3j , and so
on.
F 1 the token in lower case, with digits replaced
by the symbol #.
F 2 1 if the letters in the token are all capitalized,
0 otherwise.
F 3 1 if the token contains a digit, 0 otherwise.
F 4 1 if the token contains an uppercase letter, 0
otherwise.
F 5 1 if the token contains a hyphen, 0 otherwise.
2Available at: http:// www-tsujii.is.s.u-tokyo.ac.jp/ GE-
NIA/ tagger/
3A tokenizer is available at: http:// www.cis.upenn.edu/
treebank/ tokenization.html
140
F 6 first letter in the token.
F 7 first two letters in the token.
F 8 first three letters in the token.
F 9 last letter in the token.
F 10 last two letters in the token.
F 11 last three letters in the token.
The features F 0 to F 11 are known to be useful
for POS tagging. We postulated that since most
frequent hedge cues tend not to be nouns, these
features might help identify them.
The following three features are obtained by
running the GENIA tagger.
F 12 a part of speech.
F 13 a CoNLL-2000 style shallow parse. For ex-
ample, B-NP or I-NP indicates that the token
is a part of a base noun phrase, B-VP or I-VP
indicates that it is part of a verb phrase.
F 14 named entity, especially a protein name.
F 15 a word stem by Porter?s stemmer 4. Porter?s
stemmer removes common morphological
and inflectional endings from words in En-
glish. It is often used as part of an informa-
tion retrieval system.
Upon later inspection, it seems that Porter?s
stemmer may be too aggressive in stemming
words. The word putative, for example, after be-
ing processed by the stemmer, becomes simply put
(which is clearly erroneous).
The last nine types of features utilize the unla-
beled corpus for the biological portion of shared
task 1, provided by the shared task organizers.
For each stem, we acquire a histogram of sur-
rounding words, with a window size of 3, from
the unlabeled corpus. Each histogram is repre-
sented as a vector; the similarity between his-
tograms was then computed. The similarity met-
ric we used is called the Tanimoto coefficient, also
called extended/vector-based Jaccard coefficient.
vi ? vj
||vi|| + ||vj || ? vi ? vj
It is based on the dot product of two vectors and
reduces to Jaccard coefficient for binary features.
4Available at: http://tartarus.org/ martin/PorterStemmer/
This metric is known to perform quite well for
near-synonym discovery (Hagiwara et al, 2008).
Given a stem and its similarities to different hedge
cues, we took the maximum similarity and dis-
cretized it.
F 16 1 if similarity is bigger than 0.9, 0 otherwise.
...
F 19 1 if similarity is bigger than 0.6, 0 otherwise.
...
F 24 1 if similarity is bigger than 0.1, 0 otherwise.
This concludes the base features we considered.
4.2 Combinations of Base Features
In order to discover combinations of base features,
we implemented a genetic algorithm (Goldberg,
1989). It is an adaptive heuristic search algorithm
based on the evolutionary ideas of natural selec-
tion and genetics. After splitting the training set
into three partitions, given the first partition as the
training set, the fitness is measured by the score
of predicting the second partition. We removed
the feature sets that did not score high, and intro-
duced mutations ? new feature sets ? as replace-
ments. After several generations, surviving fea-
ture sets performed quite well. To avoid over fit-
ting, occasionally feature sets were evaluated on
the third partition, and we finally chose the feature
set according to this partition.
The features of the submitted system are listed
in Table 1. Note that Table 1 shows the dimensions
of the feature vector that evaluate to 1 given x and
y?j . The actual feature vector is created by instan-
tiating all the combinations in the table using the
training set.
Surprisingly, our genetic algorithm removed
features F 10 and F 11, the last two/three let-
ters in a token. It also removed the POS in-
formation F 12, but kept the sequence of POS
tags F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3. The reason for
longer sequences is due to our heuristics for muta-
tions. Occasionally, we allowed the genetic algo-
rithm to insert a longer sequence of feature com-
binations at once. One other notable observation
is that shallow parses and NEs are removed. Be-
tween the various thresholds from F 16 to F 24,
it only kept F 19, discovering 0.6 as a similarity
threshold.
141
State ?(x, y?j)
y?j
y?j , F 0j?2
y?j , F 0j?1
y?j , F 0j
y?j , F 0j , F 19j
y?j , F 0j?1, F 0j , F 0j+1, F 0j+2, F 0j+3, F 0j+4 ?(1)
y?j , F 0j+1
y?j , F 0j+2
y?j , F 1j
y?j , F 2j ?(2)
y?j , F 3j
y?j , F 4j
y?j , F 4j?2, F 4j?1, F 4j , F 4j+1, F 4j+2
y?j , F 5j
y?j , F 5j , F 7j?1
y?j , F 6j
y?j , F 7j
y?j , F 8j
y?j , F 9j?1, F 9j , F 9j+1, F 9j+2, F 9j+3
y?j , F 12j?1, F 12j , F 12j+1, F 12j+2, F 12j+3
y?j , F 15j , F 15j+1, F 15j+2, F 15j+3
y?j , F 19j?2, F 19j?1, F 19j , F 19j+1, F 19j+2
Table 1: Features for Sequential Labeling
5 Experiments
In order to examine the effects of learning parame-
ters, we conducted experiments on the test data af-
ter it was released to the participants of the shared
task.
While BPM has two parameters, K and T , we
fixed T = 5 and varied K, the number of percep-
trons. As increasing the number of perceptrons re-
sults in more thorough exploration of the version
space V (D), we expect that the performance of
the classifier would improve as K increases. Ta-
ble 2 shows how the number of perceptrons affects
the performance.
TP stands for True Positive, FP for False Pos-
itive, and FN for False Negative. The evaluation
metrics were precision P (the number of true pos-
K TP FP FN P (%) R (%) F1 (%)
10 641 80 149 88.90 81.14 84.84
20 644 79 146 89.07 81.52 85.13
30 644 80 146 88.95 81.52 85.07
40 645 81 145 88.84 81.65 85.09
50 645 80 145 88.97 81.65 85.15
Table 2: Effects of K in Bayes Point Machines
itives divided by the total number of elements la-
beled as belonging to the positive class) recall R
(the number of true positives divided by the to-
tal number of elements that actually belong to the
positive class) and their harmonic mean, the F1
score (F1 = 2PR/(P + R)). All figures in this
paper measure hedge cue detection performance at
the sentence classification level, not word/phrase
classification level. From the results, once the
number of perceptrons hits 20, the performance
stabilizes and does not seem to show any improve-
ment.
Next, in order to examine whether or not we
have overfitted to the training/heldout set, we re-
moved each row of Table 1 and reevaluated the
performance of the system. Reevaluation was
conducted on the labeled test set released by the
shared task organizers after our system?s output
had been initially evaluated. Thus, these figures
are comparable to the sentence classification re-
sults reported in Farkas et al (2010).
TP FP FN P (%) R (%) F1 (%)
1 647 79 143 89.12 81.90 85.36
2 647 80 143 89.00 81.90 85.30
1,2 647 81 143 88.87 81.90 85.24
Table 3: Effects of removing features (1) or (2), or
both
Table 3 shows the effect of removing (1), (2),
or both (1) and (2), showing that they overfit the
training data. Removing any other rows in Ta-
ble 1 resulted in decreased classification perfor-
mance. While there are other large combination
features such as ones involving F 4, F 9, F 12, F 15
and F 19, we find that they do help improving the
performance of the classifier. Since these fea-
tures seem unintuitive to the authors, it is likely
that they would not have been found without the
genetic algorithm we employed. Error analysis
shows that inclusion of features involving F 9 af-
fects prediction of ?believe?, ?possible?, ?puta-
tive?, ?assumed?, ?seemed?, ?if?, ?presumably?,
?perhaps?, ?suggestion?, ?suppose? and ?intrigu-
ing?. However, as this feature template is unfolded
into a large number of features, we were unable to
obtain further linguistic insights.
In the following experiments, we used the cur-
rently best performing features, that is, all fea-
tures except (1) in Table 1, and trained the classi-
fiers using the formalism of Perceptron and Con-
ditional Random Fields besides Bayes Point Ma-
142
chines as we have been using. The results in Table
4 shows that BPM performs better than Percep-
tron or Conditional Random Fields. As the train-
ing time for BPM is better than CRF, our choice
of BPM helped us to run the genetic algorithm re-
peatedly as well. After several runs of empirical
tuning and tweaking, the hyper-parameters of the
algorithms were set as follows. Perceptron was
stopped at 40 iterations (T = 40). For BPM, we
fixed T = 5 and K = 20. For Conditional Ran-
dom Fields, we compared the penalized version
with C = 1 and the unpenalized version (C = 0).
The results in Table 4 is that of the unpenalized
version, as it performed better than the penalized
version.
Perceptron
TP FP FN P (%) R (%) F1 (%)
671 128 119 83.98 84.94 84.46
Conditional Random Fields
TP FP FN P (%) R (%) F1 (%)
643 78 147 89.18 81.39 85.11
Bayes Point Machines
TP FP FN P (%) R (%) F1 (%)
647 79 143 89.12 81.90 85.36
Table 4: Performance of different optimization
strategies
6 Conclusion
To tackle the hedge cue detection problem posed
by the CoNLL-2010 shared task, we utilized a
classifier for sequential labeling following previ-
ous work (Morante and Daelemans, 2009). An
essential part of this task is to discover the fea-
tures that allow us to predict unseen hedge expres-
sions. As hedge cue detection is semantic rather
than syntactic in nature, useful features such as
word stems tend to be specific to each word and
hard to generalize. However, by using a genetic al-
gorithm to examine a large number of feature com-
binations, we were able to find many features with
a wide context window of up to 5 words. While
some features are found to overfit, our analysis
shows that a number of these features are success-
fully applied to the test data yielding good general-
ized performance. Furthermore, we compared dif-
ferent optimization schemes for structured output
prediction using our c++ library, freely available
for download and use. We find that Bayes Point
Machines have a good trade-off between perfor-
mance and training speed, justifying our repeated
usage of BPM in the genetic algorithm for feature
selection.
Acknowledgments
The authors would like to thank the reviewers for
their comments. This research was supported by
the Information Technology Center through their
grant to the first author. We would also like to
thank Mr. Ono, Mr. Yonetsuji and Mr. Yamada
for their contributions to the library.
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of Empirical Methods in Natural Language Process-
ing (EMNLP).
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using bayes point machines. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 160?167, June.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden. ACL.
David E. Goldberg. 1989. Genetic Algorithms
in Search, Optimization, and Machine Learning.
Addison-Wesley Professional.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2008. Context feature selection for distri-
butional similarity. In Proceedings of IJCNLP-08.
Karen Spa?rk Jones and Peter Willet. 1997. Readings
in Information Retrieval. Morgan Kaufmann.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 28?36.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceed-
ings of the Human Language Technology Confer-
ence (HLT).
143
Proceedings of the TextGraphs-8 Workshop, pages 44?52,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Understanding seed selection in bootstrapping
Yo Ehara?? Issei Sato?
? Graduate School of Information Science and Technology ? Information Technology Center
The University of Tokyo / 7-3-1 Hongo, Bunkyo-ku, Tokyo, Japan
? JSPS Research Fellow
Kojimachi Business Center Building, 5-3-1 Kojimachi, Chiyoda-ku, Tokyo, Japan
{ehara@r., sato@r., oiwa@r., nakagawa@}dl.itc.u-tokyo.ac.jp
Hidekazu Oiwa?? Hiroshi Nakagawa?
Abstract
Bootstrapping has recently become the focus
of much attention in natural language process-
ing to reduce labeling cost. In bootstrapping,
unlabeled instances can be harvested from the
initial labeled ?seed? set. The selected seed set
affects accuracy, but how to select a good seed
set is not yet clear. Thus, an ?iterative seed-
ing? framework is proposed for bootstrapping
to reduce its labeling cost. Our framework
iteratively selects the unlabeled instance that
has the best ?goodness of seed? and labels the
unlabeled instance in the seed set. Our frame-
work deepens understanding of this seeding
process in bootstrapping by deriving the dual
problem. We propose a method called ex-
pected model rotation (EMR) that works well
on not well-separated data which frequently
occur as realistic data. Experimental results
show that EMR can select seed sets that pro-
vide significantly higher mean reciprocal rank
on realistic data than existing naive selection
methods or random seed sets.
1 Introduction
Bootstrapping has recently drawn a great deal of
attention in natural language processing (NLP) re-
search. We define bootstrapping as a method for
harvesting ?instances? similar to given ?seeds? by
recursively harvesting ?instances? and ?patterns? by
turns over corpora using the distributional hypothe-
sis (Harris, 1954). This definition follows the def-
initions of bootstrapping in existing NLP papers
(Komachi et al, 2008; Talukdar and Pereira, 2010;
Kozareva et al, 2011). Bootstrapping can greatly
reduce the cost of labeling instances, which is espe-
cially needed for tasks with high labeling costs.
The performance of bootstrapping algorithms,
however, depends on the selection of seeds. Al-
though various bootstrapping algorithms have been
proposed, randomly chosen seeds are usually used
instead. Kozareva and Hovy (2010) recently reports
that the performance of bootstrapping algorithms
depends on the selection of seeds, which sheds light
on the importance of selecting a good seed set. Es-
pecially a method to select a seed set considering
the characteristics of the dataset remains largely un-
addressed. To this end, we propose an ?iterative
seeding? framework, where the algorithm iteratively
ranks the goodness of seeds in response to current
human labeling and the characteristics of the dataset.
For iterative seeding, we added the following two
properties to the bootstrapping;
? criteria that support iterative updates of good-
ness of seeds for seed candidate unlabeled in-
stances.
? iterative update of similarity ?score? to the
seeds.
To invent a ?criterion? that captures the character-
istics of a dataset, we need to measure the influence
of the unlabeled instances to the model. This model,
however, is not explicit in usual bootstrapping algo-
rithms? notations. Thus, we need to reveal the model
parameters of bootstrapping algorithms for explicit
model notations.
To this end, we first reduced bootstrapping al-
gorithms to label propagation using Komachi et al
44
(2008)?s theorization. Komachi et al (2008) shows
that simple bootstrapping algorithms can be inter-
preted as label propagation on graphs (Komachi
et al, 2008). This accords with the fact that
many papers such as (Talukdar and Pereira, 2010;
Kozareva et al, 2011) suggest that graph-based
semi-supervised learning, or label propagation, is
another effective method for this harvesting task.
Their theorization starts from a simple bootstrap-
ping scheme that can model many bootstrapping al-
gorithms so far proposed, including the ?Espresso?
algorithm (Pantel and Pennacchiotti, 2006), which
was the most cited among the Association for Com-
putational Linguistics (ACL) 2006 papers.
After reducing bootstrapping algorithms to label
propagation, next, we will reveal the model param-
eters of a bootstrapping algorithm by taking the
dual problem of bootstrapping formalization of (Ko-
machi et al, 2008). By revealing the model param-
eters, we can obtain an interpretation of selecting
seeds which helps us to create criteria for the iter-
ative seeding framework. Namely, we propose ex-
pected model rotation (EMR) criterion that works
well on realistic, and not well-separated data.
The contributions of this paper are summarized as
follows.
? The iterative seeding framework, where seeds
are selected by certain criteria and labeled iter-
atively.
? To measure the influence of the unlabeled in-
stances to the model, we revealed the model
parameters through the dual problem of boot-
strapping.
? The revealed model parameters provides an in-
terpretation of selecting seeds focusing on how
well the dataset is separated.
? ?EMR? criterion that works well on not well-
separated data which frequently occur as real-
istic data. .
2 Related Work
Kozareva and Hovy (2010) recently shed light on
the problem of improving the seed set for bootstrap-
ping. They defined several goodness of seeds and
proposed a method to predict these measures using
support vector regression (SVR) for their doubly an-
chored pattern (DAP) system. However, Kozareva
and Hovy (2010) does not show how effective the
seed set selected by the goodness of seeds that they
defined was for the bootstrapping process while they
show how accurately they could predict the good-
ness of seeds.
Early work on bootstrapping includes that of
(Hearst, 1992) and that of (Yarowsky, 1995). Abney
(2004) extended self-training algorithms including
that of (Yarowsky, 1995), forming a theory different
from that of (Komachi et al, 2008). We chose to ex-
tend the theory of (Komachi et al, 2008) because it
can actually explain recent graph-based algorithms
including that of (Pantel and Pennacchiotti, 2006).
The theory of Komachi et al (2008) is also newer
and simpler than that of (Abney, 2004).
The iterative seeding framework can be regarded
as an example of active learning on graph-based
semi-supervised learning. Selecting seed sets cor-
responds to sampling a data point in active learn-
ing. In active learning on supervised learning, the
active learning survey (Settles, 2012) includes a
method called expected model change, after which
this paper?s expected model rotation (EMR) is
named. They share a basic concept: the data
point that surprises the classifier the most is selected
next. Expected model change mentioned by (Settles,
2012), however, is for supervised setting, not semi-
supervised setting, with which this paper deals. It
also does not aim to provide intuitive understanding
of the dataset. Note that our method is for semi-
supervised learning and we also made the calcula-
tion of EMR practical.
Another idea relevant to our EMR is an ?an-
gle diversity? method for support vector machines
(Brinker, 2003). Unlike our method, the angle diver-
sity method interprets each data point as data ?lines?
in a version space. The weight vector is expressed
as a point in a version space. Then, it samples a data
?line? whose angle formed with existing data lines
is large. Again, our method builds upon different
settings in that this method is only for supervised
learning, while ours is for semi-supervised learning.
45
3 Theorization of Bootstrapping
This section introduces a theorization of bootstrap-
ping by (Komachi et al, 2008).
3.1 Simple bootstrapping
Let D = {(y1,x1), . . . , (yl,xl),xl+1, . . . ,xl+u}
be a dataset. The first l data are labeled, and the
following u data are unlabeled. We let n = l + u
for simplicity. Each xi ? Rm is an m-dimensional
input feature vector, and yi ? C is its corresponding
label where C is the set of semantic classes. To han-
dle |C| classes, for k ? C, we call an n-sized 0-1
vector yk = (y1k, . . . , ynk)? a ?seed vector?, where
yik = 1 if the i-th instance is labeled and its label is
k, otherwise yik = 0.
Note that this multi-class formalization includes
typical ranking settings for harvesting tasks as its
special case. For example, if the task is to har-
vest animal names from all given instances, such
as ?elephant? and ?zebra?, C is set to be binary as
C = {animal, not animal}. The ranking is obtained
by the score vector resulting from the seed vector
yanimal ? ynot animal due to the linearity.
By stacking row vectors xi, we denote X =
(x1, . . . ,xn)?. Let X be an instance-pattern (fea-
ture) matrix where (X)ij stores the value of the
jth feature in the ith datum. Note that we can al-
most always assume the matrix X to be sparse for
bootstrapping purposes due to the language sparsity.
This sparsity enables the fast computation.
The simple bootstrapping (Komachi et al, 2008)
is a simple model of bootstrapping using matrix rep-
resentation. The algorithm starts from f0
def= y and
repeats the following steps until f c converges.
1. ac+1 = X?f c. Then, normalize ac+1?
2. f c+1 = Xac+1. Then, normalize f c+1.
The score vector after c iterations of the simple
bootstrapping is obtained by the following equation.
f =
(
1
m
1
n
XX?
)c
y (1)
?Simplified Espresso? is a special version of the
simple bootstrapping where Xij = pmi(i,j)max pmi and we
normalize score vectors uniformly: f c ? f c/n,
ac ? ac/m. Here, pmi(i, j)
def= log p(i,j)p(i)p(j) .
Komachi et al (2008) pointed out that, although
the scores f c are normalized during the iterations in
the simple bootstrapping, when c ? ?, f c con-
verges to a score vector that does not depend on
the seed vector y as the principal eigenvector of
( 1
m
1
nXX
?) becomes dominant. For bootstrapping
purposes, however, it is appropriate for the resulting
score vector f c to depend on the seed vector y.
3.2 Laplacian label propagation
To make f seed dependent, Komachi et al (2008)
noted that we should use a power series of a ma-
trix rather than a simple power of a matrix. As
the following equation incorporates the score vec-
tors ((?L)cy) with both low and high c values, it
provides a seed dependent score vector with taking
higher c into account.
?
?
c=0
?c ((?L)cy) = (I + ?L)?1 y (2)
Instead of using
( 1
m
1
nXX
?), Komachi et al
(2008) used L def= I ? D?1/2XX?D?1/2, a nor-
malized graph Laplacian for graph theoretical rea-
sons. D is a diagonal matrix defined as Dii
def=
?
j(XX?)ij . This infinite summation of the ma-
trix can be expressed by inverting the matrix under
the condition that 0 < ? < 1?(L) , where ?(L) be the
spectral radius of L.
Komachi et al (2008)?s Laplacian label propaga-
tion is simply expressed as (3). Given y, it outputs
the score vector f to rank unlabeled instances. They
reports that the resulting score vector f constantly
achieves better results than those by Espresso (Pan-
tel and Pennacchiotti, 2006).
f = (I + ?L)?1 y. (3)
4 Proposal: criteria for iterative seeding
This section describes our iterative seeding frame-
work. The entire framework is shown in Algo-
rithm 1.
Let gi be the goodness of seed for an unlabeled
instance i. We want to select the instance with the
highest goodness of seed as the next seed added in
the next iteration.
i? = argmax
i
gi (4)
46
Algorithm 1 Iterative seeding framework
Require: y, X , the set of unlabeled instances U ,
the set of classes C.
Initialize gk,i? ; ?k ? C, ?i? ? U
repeat
Select instance i? by (4).
Label i?. Let k? be i??s class.
U ? U\{?i}
for all i? ? U do
Recalculate gk?,i?
end for
until A sufficient number of seeds are collected.
Each seed selection criterion defines each good-
ness of seed gi. To measure the goodness of seeds,
we want to measure how an unlabeled instance will
affect the model underlying Eq. (3). That is, we
want to choose the unlabeled instance that would
most influence the model. However, as the model
parameters are not explicitly shown in Eq. (3), we
first need to reveal them before measuring the influ-
ence of the unlabeled instances.
4.1 Scores as margins
This section reveals the model parameters through
the dual problem of bootstrapping. We show that
the score obtained by Eq. (3) can be regarded as
the ?margin? between each unlabeled data point and
the hyperplane obtained by ridge regression; specif-
ically, we can show that the i-th element of the re-
sulting score vector obtained using Eq. (3) can be
written as fi = ? (yi ? ?w?, ? (xi)?), where w? is
the optimal model parameter that we need to reveal
(Figure 1). ? is a feature function mapping xi to a
feature space and is set to make this relation hold.
Note that, for unlabeled instances, yi = 0 holds, and
thus fi is simply fi = ?? ?w?, ? (xi)?. Therefore,
|fi| ? ? ?w?, ? (xi)? ? denotes the ?margin? between
each unlabeled data point and the underlying hyper-
plane.
Let ? be defined as ? def= (? (x1) , . . . , ? (xn))?.
The score vector f can be written using ? as in (6).
If we set ? as Eq. (6), Eq. (5) is equivalent to Eq.
(3).
f =
(
I + ????
)?1
y (5)
Figure 1: Scores as margins. The absolute values of the
scores of the unlabeled instances are shown as the mar-
gin between the unlabeled instances and the underlying
hyperplane in the feature space.
??? = L = I ?D?
1
2XXTD?
1
2 (6)
By taking the diagonal of ??? in Eq. (6), it is
easy to see that ?? (xi) ?2 = ?? (xi) , ? (xi)? ? 1.
Thus, the data points mapped into the feature space
are within a unit circle in the feature space shown
as the dashed circles in Figure 1-3. The weight vec-
tor is then represented by the classifying hyperplane
that goes through the origin in the feature space.
The classifying hyperplane views all the points posi-
tioned left of this hyperplane as the green class, and
all the points positioned right of this hyperplane as
the blue gray-stroked class. Note that all the points
shown in Figure 1 are unlabeled, and thus the clas-
sifying hyperplane does not know the true classes of
the data points. Due to the lack of space, the proof
is shown in the appendix.
4.2 Margin criterion
Section 4.1 uncovered the latent weight vector for
the bootstrapping model Eq. (3). A weight vector
specifies a hyperplane that classifies instances into
semantic classes. Thus, weight vector interpretation
easily leads to an iterative seeding criterion: an unla-
beled instance closer to the classifying hyperplane is
more uncertain, and therefore obtains higher good-
ness of seed. We call this criterion the ?margin cri-
terion? (Figure 2).
First, we define gk,i?
def= |(fk)i? |/sk as the good-
ness of an instance i? to be labeled as k. sk is the
number of seeds labeled as class k in the current
seed set. In the margin criterion, the goodness of the
seed i? is then obtained by the difference between
47
Figure 2: Margin criterion in binary setting. The instance
closest to the underlying hyperplane, the red-and-black-
stroked point, is selected. The part within the large gray
dotted circle is not well separated. Margin criterion con-
tinues to select seeds from this part only in this example,
and fails to sample from the left-bottom blue gray-stroked
points. Note that all the points are unlabeled and thus the
true classes of data points cannot be seen by the underly-
ing hyperplane in this figure.
the largest and second largest gk,i? among all classes
as follows:
gMargini
def= ?
(
max
k
gMargink,i? ? 2
ndlargestkg
Margin
k,i?
)
.
(7)
The shortcoming of Margin criterion is that it can
be ?stuck?, or jammed, or trapped, when the data are
not well separated and the underlying hyperplanes
goes right through the not well-separated part. In
Figure 2, the part within the large gray dotted cir-
cle is not well separated. Margin criterion continues
to select seeds from this part only in this example,
and fails to sample from the left-bottom blue gray-
stroked points.
4.3 Expected Model Rotation
To avoid Margin criterion from being stuck in the
part where the data are not well separated, we pro-
pose another more promising criterion: the ?Ex-
pected Model Rotation (EMR)?. EMR measures the
expected rotation of the classifying hyperplane (Fig-
ure 3) and selects the data point that rotates the un-
Figure 3: EMR criterion in binary setting. The instance
that would rotate the underlying hyperplane the most is
selected. The amount denoted by the purple brace ?{? is
the goodness of seeds in the EMR criterion. This criterion
successfully samples from the left bottom blue points.
derlying hyperplane ?the most? is selected. This se-
lection method prevents EMR from being stuck in
the area where the data points are not well sepa-
rated. Another way of viewing EMR is that it selects
the data point that surprises the current classifier the
most. This makes the data points influential to the
classification selected in early iteration in the itera-
tive seeding framework. A simple rationale of EMR
is that important information must be made available
earlier.
To obtain the ?expected? model rotation, in EMR,
we define the goodness of seeds for an instance i?,
gi? as the sum of each per-class goodness of seeds
gk,i? weighted by the probability that i? is labeled
as k. Intuitively, gk,i? measures how the classifying
hyperplane would rotate if the instance i? were la-
beled as k. Then, gk,i? is weighted by the probability
that i? is labeled as k and summed. The probability
for i? to be labeled as k can be obtained from the
i?-th element of the current normalized score vector
pi? (k)
def= |(fk)i?/sk|?
k?C|(fk)i?/sk|
, where sk is the number
of seeds labeled as class k in the current seed set.
gEMRi?
def=
?
k?C
pi? (k) gEMRk,i? (8)
The per-class goodness of seeds gk,i? can be cal-
culated as follows:
gEMRk,i?
def= 1?
?
?
?
?
w?k
||wk||
wk,+i?
||wk,+i? ||
?
?
?
?
. (9)
48
From Eq. (17) in the proof, w = ??f . Here, ei?
is a unit vector whose i?-th element is 1 and all other
elements are 0.
wk = ??fk = ?? (I + ?L)?1 yk (10)
wk,+i? = ??fk,+i? = ?? (I + ?L)?1 (yk + ei?) (11)
Although Eqs. (10) and (11) use ?, we do not
need to directly calculate ?. Instead, we can use Eq.
(6) to calculate these weight vectors as follows:
w?k wk,+i? = f?k
(
I ?D?
1
2XXTD?
1
2
)
fk,+i? (12)
||w|| =
?
f?
(
I ?D?
1
2XXTD?
1
2
)
f . (13)
For more efficient computation, we cached
(I + ?L) ei? to boost the calculation in Eqs. (10)
and (11) by exploiting the fact that yk can be writ-
ten as the sum of ei for all the instances in class k.
5 Evaluation
We evaluated our method for two bootstrapping
tasks with high labeling costs. Due to the nature
of bootstrapping, previous papers have commonly
evaluated each method by using running search en-
gines. While this is useful and practical, it also re-
duces the reproducibility of the evaluation. We in-
stead used openly available resources for our evalu-
ation.
First, we want to focus on the separatedness of the
dataset. To this end, we prepared two datasets: one
is ?Freebase 1?, a not well-separated dataset, and
another is ?sb-8-1?, a well-separated dataset. We
fixed ? = 0.01 as Zhou et al (2011) reports that
? = 0.01 generally provides good performance on
various datasets and the performance is not keen to ?
except extreme settings such as 0 or 1. In all exper-
iments, each class initially has 1 seed and the seeds
are selected and increased iteratively according to
each criterion. The meaning of each curve is shared
by all experiments and is explained in the caption of
Figure 4.
?Freebase 1? is an experiment for information ex-
traction, a common application target of bootstrap-
ping methods. Based on (Talukdar and Pereira,
2010), the experiment setting is basically the same
as that of the experiment Section 3.1 in their paper1.
1Freebase-1 with Pantel Classes, http://www.
talukdar.net/datasets/class_inst/
As 39 instances have multiple correct labels, how-
ever, we removed these instances from the exper-
iment to perform the experiment under multi-class
setting. Eventually, we had 31, 143 instances with
1, 529 features in 23 classes. The task of ?Freebase
1? is bootstrapping instances of a certain semantic
class. For example, to harvest the names of stars,
given {Vega, Altair} as a seed set, the bootstrap-
ping ranks Sirius high among other instances (proper
nouns) in the dataset. Following the experiment set-
ting of (Talukdar and Pereira, 2010), we used mean
reciprocal rank (MRR) throughout our evaluation 2.
?sb-8-1? is manually designed to be well-
separated and taken from 20 Newsgroup subsets3.
It has 4, 000 instances with 16, 282 features in 8
classes.
Figure 4 and Figure 5 shows the results. We can
easily see that ?EMR? wins in ?Freebase 1?, a not
well-separated dataset, and ?Margin? wins in ?sb-8-
1?, a well-separated dataset. This result can be re-
garded as showing that ?EMR? successfully avoids
being ?stuck? in the area where the data are not
well separated. In fact, in Figure 4, ?Random? wins
?Margin?. This implies that the not well-separated
part of this dataset causes the classifying hyperplane
in ?Margin? criterion to be stuck and make it lose
against even simple ?Random? criterion.
In contrast, in the ?sb-8-1?, a well-separated bal-
anced dataset, ?Margin? beats the other remaining
two. This implies the following: When the dataset
is well separated, uncertainty of a data point is the
next important factor to select a seed set. As ?Mar-
gin? exactly takes the data point that is the most un-
certain to the current hyperplane, ?Margin? works
quite well in this example.
Note that all figures in all the experiments show
the average of 30 random trials and win-and-lose re-
lationships mentioned are statistically tested using
Mann-Whitney test.
While ?sb-8-1? is a balanced dataset, realistic data
like ?freebase 1? is not only not-well-separated, but
also imbalanced . Therefore, we performed ex-
periments ?sb-8-1?, an imbalanced well-separated
dataset, and ?ol-8-1?, an imbalanced not-well sepa-
2MRR is defined as MRR def= 1|Q|
?
i?Q
1
ri
, where Q is
the test set, i ? Q denotes an instance in the test set Q, and ri
is the rank of the correct class among all |C| classes.
3http://mlg.ucd.ie/datasets/20ng.html
49
Figure 4: Freebase 1, a NOT well-separated dataset. Av-
erage of 30 random trials. ?Random? and ?Margin? are
baselines. ?Random? is the case that the seeds are se-
lected randomly. ?Margin? is the case that the seeds
are selected using the margin criterion described in ?4.2.
?EMR? is proposed and is the case that the seeds are se-
lected using the EMR criterion described in ?4.3. At the
rightmost point, all the curves meet because all the in-
stances in the seed pool were labeled and used as seeds
by this point. The MRR achieved by this point is shown
as the line ?All used?. If a curve of each method crosses
?All used?, this can be intepretted as that iterative seeding
of the curve?s criterion can reduce the cost of labeling all
the instances to the crossing point of the x-axis. ?EMR?
significantly beats ?Random? and ?Margin? where x-axis
is 46 and 460 with p-value < 0.01.
rated dataset under the same experiment setting used
for ?sb-8-1?. ?sl-8-1? have 2, 586 instances with
10, 764 features. ?ol-8-1? have 2, 388 instances with
9, 971 features. Both ?sl-8-1? and ?ol-8-1? have 8
classes.
Results are shown in Figure 6 and Figure 7. In
Figure 6, ?EMR? beats the other remaining two even
though this is a well-separated data set. This im-
plies that ?EMR? can also be robust to the imbal-
ancedness as well. In Figure 7, although the MRR
of ?Margin? eventually is the highest, the MRR of
?EMR? rises far earlier than that of ?Margin?. This
result can be explained as follows: ?Margin? gets
?stuck? in early iterations as this dataset is not well
separated though ?Margin? achieves best once it gets
out of being stuck. In contrast, as ?EMR? can avoid
being stuck, it rises early achieving high perfor-
mance with small number of seeds, or labeling. This
result suggests that ?EMR? is pereferable for reduc-
Figure 5: sb-8-1. A dataset manually designed to be well
separated. Average of 30 random trials. Legends are the
same as those in Figure 4. ?Margin? beats ?Random? and
?EMR? where x-axis is 500 with p-value < 0.01.
Figure 6: sl-8-1. An imbalanced well separated dataset.
Average of 30 random trials. Legends are the same as
those in Figure 4. ?EMR? significantly beats ?Random?
and ?Margin? where x-axis is 100 with p-value < 0.01.
ing labeling cost while ?Margin? can sometimes be
preferable for higher performance.
6 Conclusion
Little is known about how best to select seed sets
in bootstrapping. We thus introduced the itera-
tive seeding framework, which provides criteria for
selecting seeds. To introduce the iterative seed-
ing framework, we deepened the understanding of
the seeding process in bootstrapping through the
dual problem by further extending the interpretation
of bootstrapping as graph-based semi-supervised
learning (Komachi et al, 2008), which generalizes
50
Figure 7: ol-8-1. An imbalanced NOT well separated
dataset. Average of 30 random trials. Legends are the
same as those in Figure 4. ?EMR? significantly beats
?Random? and ?Margin? where x-axis is 100 with p-
value < 0.01. ?Margin? significantly beats ?EMR? and
?Random? where x-axis is 1, 000 with p-value < 0.01.
and improves Espresso-like algorithms.
Our method shows that existing simple ?Margin?
criterion can be ?stuck? at the area when the data
points are not well separated. Note that many real-
istic data are not well separated. To deal with this
problem, we proposed ?EMR? criterion that is not
stuck in the area where the data points are not well
separated.
We also contributed to make the calculation of
?EMR? practical. In particular, we reduced the num-
ber of matrix inversions for calculating the goodness
of seeds for ?EMR?.We also showed that the param-
eters for bootstrapping also affect the convergence
speed of each matrix inversion and that the typical
parameters used in other work are fairly efficient and
practical.
Through experiments, we showed that the pro-
posed ?EMR? significantly beats ?Margin? and
?Random? baselines where the dataset are not well
separated. We also showed that the iterative seed-
ing framework with the proposed measures for the
goodness of seeds can reduce labeling cost.
Appendix: Proof Consider a simple ridge regres-
sion of the following form where 0 < ? < 1 is a
positive constant.
minw
?
2
n
?
i=1
?yi ? ?w, ? (xi)??2 + ?w?2 . (14)
We define ?i = yi ? ?w, ? (xi)?. By using ?i, we
can rewrite Eq. (14) into an optimization problem
with equality constraints as follows:
minw
?
2
n
?
i=1
?2i + ?w?
2 (15)
s.t.?i ? {1, . . . , n} ; yi = w?? (xi) + ?i. (16)
Because of the equality constraints of Eq. (16),
we obtain the following Lagrange function h. Here,
each bootstrapping score fi occurs as Lagrange mul-
tipliers: h (w, ?, f) def= 12 ?w?
2 + ?2
?n
i=1 ?2i ?
?n
i=1 (?w, ? (xi)?+ ?i ? yi) fi.
By taking derivatives of h, we can derive w? by
expressing it with the sum of each fi and ? (xi).
?h
?w = 0? w? =
n
?
i=1
fi? (xi) (17)
?h
??i
= 0? fi = ? (?i = ?yi ? ?w?, ? (xi)?) (18)
Substituting the relations derived in Eqs. (17) and
(18) to the equation ?h?fi = 0 results in Eq. (19).
?h
?fi
= 0?
n
?
j=1
fj? (xi)? ? (xj)+
1
?
fi = yi (19)
Equation (19) can be written as a matrix equation
using ? defined as ? def= (? (x1) , . . . , ? (xn))?.
From Eq. (20), we can easily derive the form of Eq.
(3) as
(
??? + 1? I
)?1
y ?
(
I + ????
)?1 y.
(
??? + 1
?
I
)
f = y (20)
2
References
Steven Abney. 2004. Understanding the yarowsky algo-
rithm. Computational Linguistics, 30(3):365?395.
Klaus Brinker. 2003. Incorporating diversity in active
learning with support vector machines. In Proc. of
ICML, pages 59?66, Washington D.C.
Zelling S. Harris. 1954. Distributional structure. Word.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of COLING,
pages 539?545.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proc. of EMNLP, pages 1011?1020, Honolulu,
Hawaii.
51
Zornitsa Kozareva and Eduard Hovy. 2010. Not all seeds
are equal: Measuring the quality of text mining seeds.
In Proc. of NAACL-HLT, pages 618?626, Los Angeles,
California.
Zornitsa Kozareva, Konstantin Voevodski, and Shanghua
Teng. 2011. Class label enhancement via related in-
stances. In Proc. of EMNLP, pages 118?128, Edin-
burgh, Scotland, UK.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proc. of ACL-COLING,
pages 113?120, Sydney, Australia.
Burr Settles. 2012. Active Learning. Synthesis Lectures
on Artificial Intelligence and Machine Learning. Mor-
gan & Claypool Publishers.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proc. of
ACL, pages 1473?1481, Uppsala, Sweden.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proc. of
ACL, pages 189?196, Cambridge, Massachusetts.
Xueyuan Zhou, Mikhail Belkin, and Nathan Srebro.
2011. An iterated graph laplacian approach for rank-
ing on manifolds. In Proc. of KDD, pages 877?885.
52
