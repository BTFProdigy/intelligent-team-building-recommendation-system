Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 109?113,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Joint Syntactic and Semantic Dependency Parsing System based on
Maximum Entropy Models
Buzhou Tang1 Lu Li2 Xinxin Li1 Xuan Wang2 Xiaolong Wang2
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen,518055, China
1{tangbuzhou,lixxin2}@gmail.com
2{lli,wangxuan,wangxl}@insun.hit.edu.cn
Abstract
A joint syntactic and semantic dependency
parsing system submitted to the CoNLL-2009
shared task is presented in this paper. The
system is composed of three components: a
syntactic dependency parser, a predicate clas-
sifier and a semantic parser. The first-order
MSTParser is used as our syntactic depen-
dency pasrser. Projective and non-projective
MSTParsers are compared with each other on
seven languages. Predicate classification and
semantic parsing are both recognized as clas-
sification problem, and the Maximum Entropy
Models are used for them in our system. For
semantic parsing and predicate classifying, we
focus on finding optimized features on multi-
ple languages. The average Macro F1 Score
of our system is 73.97 for joint task in closed
challenge.
1 Introduction
The task for CoNLL-2009 is an extension of the
CoNLL-2008 shared task to multiple languages: En-
glish (Surdeanu et al, 2008), Catalan plus Span-
ish (Mariona Taule? et al, 2008), Chinese (Martha
Palmer et al, 2009), Czech (Jan Hajic? et al,
2006), German (Aljoscha Burchardt et al, 2006) and
Japanese (Daisuke Kawahara et al, 2002). Com-
pared to the CoNLL-2008 shared task, the predi-
cates are given for us in semantic dependencies task.
Therefore, we have only need to label the semantic
roles of nouns and verbs, and the frames of predi-
cates.
In this paper, a joint syntactic and semantic de-
pendency parsing system submitted to the CoNLL-
2009 shared task is presented. The system is com-
posed of three components: a syntactic dependency
parser, a predicate classifier and a semantic parser.
The first-order MSTParser is used as our syntactic
dependency parser. Projective and non-projective
MSTParsers are compared with each other on seven
languages. The predicate classifier labeling the
frames of predicates and the semantic parser label-
ing the semantic roles of nouns and verbs for each
predicate are both recognized as classification prob-
lem, and the Maximum Entropy Models (MEs) are
used for them in our system. Among three com-
ponents, we mainly focus on the predicate classifier
and the semantic parser.
For semantic parsing and predicate classifying,
features of different types are selected to our sys-
tem. The effect of them on multiple languages will
be described in the following sections in detail.
2 System Description
Generally Speaking, a syntactic and semantic de-
pendency parsing system is usually divided into four
separate subtasks: syntactic parsing, predicate iden-
tification, predicate classification, and semantic role
labeling. In the CoNLL-2009 shared task, the pred-
icate identification is not required, since the pred-
icates are given for us. Therefore, the system we
present is only composed of three components: a
syntactic dependency parser, a predicate classifier
and a semantic parser. The syntactic dependencies
are processed with the MSTParser 0.4.3b. The pred-
icates identification and semantic role label are pro-
cessed with MEs-based classifier respectively. Un-
like conventional systems, the predicates identifica-
109
tion and the semantic parser are independent with
each other. Figure 1 is the architecture of our sys-
tem.
Figure 1: System Architecture
In our system, we firstly select an appropriate
mode (projective or non-projective) of Graph-based
Parser (MSTParser) for each language, then con-
struct the MEs-based predicates classification and
the MEs-based semantic parser with syntactic de-
pendency relationships and predicate classification
respectively.
2.1 Syntactic Dependency Parsing
MSTParser (McDonald, 2008) is used as our syn-
tactic dependency parser. It is a state-of-the-art de-
pendency parser that searches for maximum span-
ning trees (MST) over directed graph. Both of pro-
jective and non-projective are supported by MST-
Parser. Our system employs the first-order frame-
work with projective and non-projective modes on
seven given languages.
2.2 Predicate Classification
In this phase, we label the sense of each predicate
and the MEs are adopted for classification. Features
of different types are extracted for each predicate,
and an optimized combination of them is adopted in
our final system. Table 1 lists all features. 1-20 are
the features used in Li?s system (Lu Li et al, 2008),
No Features No Features
1 w0 20 Lemma
2 p0 21 DEPREL
3 p?1 22 CHD POS
4 p1 23 CHD POS U
5 p?1p0 24 CHD REL
6 p0p1 25 CHD REL U
7 p?2p0 26 SIB REL
8 p0p2 27 SIB REL U
9 p?3p0 28 SIB POS
10 p0p3 29 SIB POS U
11 p?1p0p1 30 VERB V
12 w0p0 31 4+11
13 w0p?1p0 32 Indegree
14 w0p0p1 33 Outdegree
15 w0p?2p0 34 Degree
16 w0p0p2 35 ARG IN
17 w0p?3p0 36 ARG OUT
18 w0p0p3 37 ARG Degree
19 w0p?1p0p1 38 Span
Table 1: Features for Predicate Classification.
and 21-31 are a part of the optimized features pre-
sented in Che?s system (Wanxiang Che et al, 2008)
In Table 1, ?w? denotes the word and ?p? de-
notes POS of the words. Features in the form of
part1 part2 denote the part2 of the part1, while fea-
tures in the form of part1+part2 denote the combi-
nation of the part1 and part2. ?CHD? and ?SIB? de-
note a sequence of the child and the sibling words
respectively, ?REL? denotes the type of relations,
?U? denotes the result after reducing the adjacent
duplicate tags to one, ?V? denotes whether the part
is a voice, ?In? and ?OUT? denote the in degree and
out degree, which denotes how many dependency
relations coming into this word and going away from
this word,and ?ARG? denotes the semantic roles of
the predicate. The ?Span? denotes the maximum
length between the predicate and its arguments. The
final optimized feature combination is :1-31 and 33-
37.
2.3 Semantic Role Labeling
The semantic role labeling usually contains two sub-
tasks: argument identification and argument classi-
fication. In our system, we perform them in a single
110
stage through one classifier, which specifies a par-
ticular role label to the argument candidates directly
and assigns ?NONE? label to the argument candi-
dates with no role. MEs are also adopted for classifi-
cation. For each word in a sentence, MEs gives each
candidate label (including semantic role labels and
none label) a probability for the predicate. The fea-
tures except for the feature (lemma plus sense num-
ber of the predicate in (Lu Li et al, 2008)) and the
features 32-38 in Table 1 are selected in our system.
3 Experiments and Results
We train the first-order MSTParser 1 with projective
and non-projective modes in terms of default param-
eters respectively. Our maximum entropy classifiers
are implemented with the Maximum Entropy Mod-
eling Toolkit 2 . The default classifier parameters are
used in our system except for iterations. All mod-
els are trained using all training data, and tested on
the whole development data and test data, with 64-
bit 3.00GHz Intel(R) Pentium(R) D CPU and 4.0G
memory.
3.1 Syntactic Dependency Parsing
Table 2 is a performance comparison between pro-
jective parser and non-projective parser on the devel-
opment data of seven languages. In Table 2, ?LAS?,
?ULAS? and ?LCS? denote as Labeled attachment
score, Unlabeled attachment score and Label accu-
racy score respectively.
The experiments show that Catalan, Chinese and
Spanish have projective property and others have
non-projective property.
3.2 Predicate Classification
To get the optimized system, three group features are
used for comparison.
? group 1: features 1-20 in Table 1.
? group 2: features 1-31 in Table 1.
? group 3: all features in Table 1.
The performance of predicate classification on the
development data of the six languages, which con-
tain this subtask, are given in Table 3. The results
1http://sourceforge.net/projects/mstparser.
2http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
LAS(%) ULAS(%) LCS(%)
Catalan 84.18 88.18 91.76
83.69 87.74 91.59
Chinese 72.58 77.06 82.07
62.85 69.47 73.00
Czech 72.79 81.40 80.93
73.18 81.86 81.30
English 86.89 90.29 91.50
86.88 90.34 91.58
German 83.43 86.89 90.24
84.00 87.40 90.61
Japanese 92.23 93.16 98.38
92.23 93.14 98.45
Spanish 83.88 87.93 91.36
83.46 87.46 91.37
Table 2: Performance of Syntactic Dependency
Parsing with different modes. The above line is the
performance of projective mode, while the below
one is the performance of non-projective mode for
each language.
group 1 group 2 group 3
Catalan 75.51 80.90 82.23
Chinese 93.79 94.99 94.75
Czech 91.83 91.77 91.86
English 92.12 92.48 93.20
German 74.49 74.14 75.85
Spanish 74.01 76.22 76.53
Table 3: Performance of predicate classification (F1
scores) for different group features on the develop-
ment data of the six languages.
show that Che?s features and the degrees of the pred-
icate and its arguments are useful for all languages,
the former improves the labeled F1 measure by 0.3%
to 5.4%, and the latter by 0.3% to 1.7%.
3.3 Semantic Role Labeling
In this phase, feature selection and performance lose
caused by P-columns are studied. Firstly, we com-
pare the following two group features:
? group 1: The features except for the lemma
plus sense number of the predicate in (Lu Li
et al, 2008).
111
LF1 ULF1 PF1
Catalan 73.25 92.69 38.41
72.71 91.93 35.22
83.23 100.00 61.88
Chinese 69.60 82.15 28.35
71.49 81.71 29.41
85.44 95.21 58.20
Czech 80.62 92.49 70.04
79.10 91.44 68.34
85.42 96.93 77.78
English 73.91 87.26 33.16
76.10 88.58 36.28
79.35 91.74 43.32
German 64.85 88.05 27.21
65.36 88.63 26.70
72.78 94.54 41.50
Japanese 69.43 82.79 29.27
69.87 83.31 29.69
72.80 87.13 34.96
Spanish 73.49 93.15 39.64
78.18 91.68 33.57
81.96 99.98 59.20
Table 4: Performance of Semantic Role Labeling
(F1 score) with different features.
? group 2: group1+the degrees of the predicate
and its arguments presented in the last section.
Secondly, features extracted from golden-columns
and P-columns are both used for testing.
The performance of them are given in Table 4,
where ?LF1?, ?ULF1? and ?PF1? denote as Labeled
F1 score, Unlabeled F1 score and Proposition F1
score respectively. The above line is the F1 scores of
Semantic Role Labeling with different features. The
uppermost line is the result of group1 features, the
middle line is the result of group2 features extracted
from P-columns, and the downmost one is the result
of group2 features extracted from golden-columns
for each language.
The results show that the features of degree also
improves the labeled F1 measure by 3.4% to 15.8%,
the different labeled F1 between golden-columns
and P-columns is about 2.9%?13.9%.
LAS LF1 M LF1
Catalan 84.18 72.71 81.46
75.68 66.95 71.32
Chinese 72.58 71.49 72.20
63.95 67.06 65.53
Czech 73.18 79.10 76.37
72.60 79.08 75.85
Czech-ood 69.81 79.80 74.81
English 86.88 76.10 82.89
86.61 77.17 81.92
English-ood 80.09 67.21 73.69
German 84.00 65.36 83.06
79.85 61.98 70.93
German-ood 71.86 61.83 66.86
Japanese 92.23 69.87 83.77
91.26 69.58 80.49
Spanish 83.88 71.18 80.74
77.21 66.23 71.72
Table 5: Overall performance of our final joint sys-
tem.
3.4 Overall Performance
In the final system, we select the optimized feature
subset discussed in the former sections. The overall
performance of the system on the development data ,
test data and Out-of-domain data are shown in Table
5 (all features are extracted from P-columns). The
average Macro F1 Scores of our system are 73.97
on test data and 71.79 on Out-of-domain data.
In Table 5, ?LAS?, ?LF1? and ?M LF1? denote
as Labeled accuracy score for Syntactic Dependency
Parsing, Labeled F1 score for Semantic Role Label-
ing, and Overall Macro Labeled F1 score respec-
tively. The topmost line is the result on the devel-
opment data, the middle one is the result on the test
data for each language and the downmost one is the
result on the Out-of-domain data if the data exist.
4 Conclusion and Discussion
We present a joint syntactic and semantic depen-
dency parsing system for CoNLL2009 Shared Task,
which composed of three components: a syntac-
tic dependency parser, a predicate classifier and a
semantic parser. All of them are built with some
state-of-the-art methods. For the predicate classifier
and the semantic parser, a new kind of features?
112
degrees, which reflect the activeness of the words
in a sentence improves their performance. In order
to improve the performance further, we will study
new machine learning methods for semantic depen-
dency parsing, especially the joint learning methods,
which can avoid the information loss problem of our
system.
Acknowledgments
We would like to thank McDonald for providing
the MSTParser program, to Zhang Le for provid-
ing the Maxent program. This research has been
partially supported by the National Natural Science
Foundation of China(No.60703015) and the Na-
tional 863 Program of China (No.2006AA01Z197,
No.2007AA01Z194).
References
Jan Hajic? and Massimiliano Ciaramita and Richard Jo-
hansson and Daisuke Kawahara and Maria Anto`nia
Mart?? and Llu??s Ma`rquez and Adam Meyers and
Joakim Nivre and Sebastian Pado? and Jan S?te?pa?nek
and Pavel Stran?a?k and Miahi Surdeanu and Nianwen
Xue and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of the 13th Conference on
Computational Natural Language Learning (CoNLL-
2009), June 4-5. Boulder, Colorado, USA.
Mariona Taule? and Maria Anto`nia Mart?? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Cor-
pora for Catalan and Spanish. Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008). Marrakesh, Morroco.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1),pages 143?172.
Jan Hajic? and Jarmila Panevova? and Eva Hajic?ova? and
Petr Sgall and Petr Pajas and Jan S?te?pa?nek and Jir???
Havelka and Marie Mikulova? and Zdene?k Z?abokrtsky?.
2006. Prague Dependency Treebank 2.0. CD-ROM,
Cat. No. LDC2006T01, ISBN 1-58563-370-4. Lin-
guistic Data Consortium, Philadelphia, Pennsylvania,
USA. URL: http://ldc.upenn.edu.
Surdeanu, Mihai and Johansson, Richard and Meyers,
Adam and Ma`rquez, Llu??s and Nivre, Joakim. 2008.
The CoNLL-2008 Shared Task on Joint Parsing of
Syntactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning(CoNLL-2008).
Aljoscha Burchardt and Katrin Erk and Anette Frank and
Andrea Kowalski and Sebastian Pado? and Manfred
Pinkal. 2006. The SALSA corpus: a German corpus
resource for lexical semantics. Proceedings of the 5rd
International Conference on Language Resources and
Evaluation (LREC-2006), pages 2008?2013. Genoa,
Italy.
Daisuke Kawahara and Sadao Kurohashi and Ko?iti
Hasida. 2002. Construction of a Japanese Relevance-
tagged Corpus. Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008?2013. Las Palmas, Canary
Islands.
McDonald and Ryan. 2006. Discriminative Learning
and Spanning Tree Algorithms for Dependency Pars-
ing, Ph.D. thesis. University of Pennsylvania.
Lu Li, Shixi Fan, Xuan Wang, XiaolongWang. 2008.
Discriminative Learning of Syntactic and Semantic
Dependencies. CoNLL 2008: Proceedings of the
12th Conference on Computational Natural Language
Learning, pages 218?222. Manchester.
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li,
Bing Qin, Ting Liu, Sheng Li. 2008. A Cascaded
Syntactic and Semantic Dependency Parsing System.
CoNLL 2008: Proceedings of the 12th Conference
on Computational Natural Language Learning, pages
238?242. Manchester.
113
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 430?434,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Zhijun Wu: Chinese Semantic Dependency Parsing with Third-Order 
Features 
 
 
Zhijun Wu Xuan Wang Xinxin Li 
Computer Application Research Center 
School of Computer Science and Technology 
Harbin Institute of Technology Shenzhen Graduate School 
Shenzhen 518055, P.R.China 
mattwu305@gmail.com wangxuan@insun.hit.edu.cn  lixin2@gmail.com 
 
 
 
 
 
 
Abstract 
This paper presents our system participated on 
SemEval-2012 task: Chinese Semantic De-
pendency Parsing. Our system extends the 
second-order MST model by adding two 
third-order features. The two third-order fea-
tures are grand-sibling and tri-sibling. In the 
decoding phase, we keep the k best results for 
each span. After using the selected third-order 
features, our system presently achieves LAS 
of 61.58% ignoring punctuation tokens which 
is 0.15% higher than the result of purely 
second-order model on the test dataset. 
1 Introduction 
Recently, semantic role labeling (SRL) has been a 
hot research topic. CoNLL shared tasks for joint 
parsing for syntactic and semantic dependencies 
both in the year 2008 and 2009, cf. (Surdeanu et al, 
2008; Haji? et al, 2009; Bohnet, 2009). Same 
shared tasks in SemEval-2007 (Sameer S., 2007). 
The SRL is traditionally implemented as two sub-
tasks, argument identification and classification. 
However, there are some problems for the seman-
tic representation method used by the semantic role 
labeling. For example, the SRL only considers the 
predicate-argument relations and ignores the rela-
tions between a noun and its modifier, the meaning 
of semantic roles is related with special predicates. 
In order to overcome those problems, semantic 
dependency parsing (SDP) is introduced. Semantic 
dependencies express semantic links between pre-
dicates and arguments and represent relations be-
tween entities and events in text. The SDP is a kind 
of dependency parsing, and its task is to build a 
dependency structure for an input sentence and to 
label the semantic relation between a word and its 
head. However, semantic relations are different 
from syntactic relations, such as position indepen-
dent. Table 1 shows the position independent of 
semantic relations for the sentence XiaoMing hit 
XiaoBai with a book today.  
Today, XiaoMing hit XiaoBai with a book. 
XiaoBai was hit by XiaoMing today with a book. 
With a book, XiaoMing hit XiaoBai today. 
XiaoMing hit XiaoBai with a book today. 
Table 1: An example position dependency 
   Although semantic relations are different from 
syntactic relations, yet they are identical in the de-
pendency tree. That means the methods used in 
syntactic dependency parsing can also be applied 
in SDP. 
    Two main approaches to syntactic dependency 
paring are Maximum Spanning Tree (MST) based 
dependency parsing and Transition based depen-
dency parsing (Eisner, 1996; Nivre et al, 2004; 
McDonald and Pereira, 2006). The main idea of 
430
MSTParser is to take dependency parsing as a 
problem of searching a maximum spanning tree 
(MST) in a directed graph (Dependency Tree). We 
see MSTParser a better chance to improve the 
parsing speed and MSTParser provides the state-
of-the-art performance for both projective and non-
projective tree banks. For the reasons above, we 
choose MSTParser as our SemEval-2012 shared 
task participating system basic framework. 
2 System Architecture  
Our parser is based on the projective MSTParser 
using all the features described by (McDonald et 
al., 2006) as well as some third-order features de-
scribed in the following sections. Semantic depen-
dency paring is introduced in Section 3. We 
explain the reasons why we choose projective 
MSTParser in Section 4 which also contains the 
experiment result analysis in various conditions. 
Section 5 gives our conclusion and future work. 
3 Semantic Dependency parsers 
3.1 First-Order Model 
Dependency tree parsing as the search for the max-
imum spanning tree in a directed graph was pro-
posed by McDonald et al (2005c). This 
formulation leads to efficient parsing algorithms 
for both projective and non-projective dependency 
trees with the Eisner algorithm (Eisner, 1996) and 
the Chu-Liu-Edmonds algorithm (Chu and Liu, 
1965; Edmonds, 1967) respectively. The formula-
tion works by defining in McDonald et al(2005a). 
The score of a dependency tree y for sentence x is 
( )
( , )
, ( , ) ( , )
i j y
s x y s i j w f i j
?
= = ?? ?
 
f(i, j) is a multidimensional feature vector repre-
sentation of the edge from node i to node j. We set 
the value of f(i, j) as 1 if there an edge from node i 
to node j. w is the corresponding weight vector 
between the two nodes that will be learned during 
training. Hence, finding a dependency tree with 
highest score is equivalent to finding a maximum 
spanning tree. Obviously, the scores are restricted 
to a single edge in the dependency tree, thus we 
call this first-order dependency parsing. This is a 
standard linear classifier. The features used in the 
first-order dependency parser are based on those 
listed in (Johansson, 2008). Table 2 shows the fea-
tures we choose in the first-order parsing. We use 
some shorthand notations in order to simplify the 
feature representations: h is the abbreviation for 
head, d for dependent, s for nearby nodes (may not 
be siblings), f for form, le for the lemmas, pos for 
part-of-speech tags, dir for direction, dis for dis-
tance, ?+1? and ?-1? for right and left position re-
spectively. Additional features are built by adding 
the direction and the distance plus the direction. 
The direction is left if the dependent is left to its 
head otherwise right. The distance is the number of 
words minus one between the head and the depen-
dent in a certain sentence, if ? 5, 5 if > 5, 10 if > 
10. ? means  that previous part is built once and 
the additional part follow ? together with the pre-
vious part is built again.  
Head and Dependent 
h-f, h-l, d-pos ?dir(h, d) ?dis(h, d) 
h-l, h-pos, d-f ?dir(h, d) ?dis(h, d) 
h-pos, h-f, d-l ?dir(h, d) ?dis(h, d) 
h-f, d-l, d-pos ?dir(h, d)  ?dis(h, d) 
h-f, d-f, d-l  ?dir(h, d) ?dis(h, d) 
h-f, h-l, d-f, d-l  ?dir(h, d) ?dis(h, d) 
h-f, h-l, d-f, d-pos ?dir(h, d) ?dis(h, d) 
h-f, h-pos, d-f, d-pos ?dir(h, d) ?dis(h, d) 
h-l, h-pos, d-l, d-pos ?dir(h, d) ?dis(h, d) 
Dependent and Nearby 
d-pos-1, d-pos, s-pos ?dir(d, s) ?dis(d, s) 
d-pos-1, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos, s-pos, s-pos+1 ?dir(d, s) ?dis(d, s) 
d-pos, d-pos+1, s-pos-1 ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos-1 ?dir(d, s) ?dis(d, s) 
d-pos, d-pos+1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos, s-pos-1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos+1, s-pos-1, s-pos ?dir(d, s) ?dis(d, s) 
d-pos-1, d-pos, s-pos-1, s-pos ? dir(d, s) ?
dis(d, s) 
d-pos, d-pos+1, s-pos-1, s-pos ?dir(d, s) ?
dis(d, s) 
d-pos-1, d-pos, s-pos, s-pos+1 ?dir(d, s) ?
dis(d, s) 
Table 2: Selected features in first order parsing 
431
3.2 Second-Order Model 
A second order model proposed by McDonald 
(McDonald and Pereira, 2006) alleviates some of 
the first order factorization limitations. Because the 
first order parsing restricts scores to a single edge 
in a dependency tree, the procedure is sufficient. 
However, in the second order parsing scenario 
where more than one edge are considered by the 
parsing algorithm, combinations of two edges 
might be more accurate which will be described in 
the Section 4. The second-order parsing can be 
defined as below: 
( )
( , )
, ( , , )
i j y
s x y s i k j
?
= ?  
where k and j are adjacent,  same-side children of i 
in the tree y. The shortcoming of this definition is 
that it restricts i on the same side of its sibling. In 
our system, we extend this restriction by adding 
the feature that as long as i is another child of k or j. 
In that case, i may be the child or grandchild of k 
or j which is shown in Figure 1. 
k  i  j ? k  i j
 
Figure 1: Sibling and grand-child relations. 
Siblings 
c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
c1-f, c2-f?dir(c1, c2) 
c1-f, c2-pos?dir(c1, c2) 
c1-pos, c2-f?dir(c1, c2) 
Parent and Two Children 
p-pos, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
p-f, c1-pos, c2-pos?dir(c1, c2)?dis(c1, c2) 
p-f, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2) 
p-f, c1-f, c2-f ?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-f, c2-f?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-f, c2-pos?dir(c1, c2) ?dis(c1, c2) 
p-pos, c1-pos, c2-f?dir(c1, c2) ?dis(c1, c2) 
Table 3: Selected features in second-order parsing 
   Shorthand notations are almost the same with the 
Section 3.1 except for that we use c1 and c2 to 
represent the two children and p for parent. In 
second-order parsing? the features selected are 
shown in Table 3. We divide the dependency dis-
tance into six parts which are 1 if > 1, 2 if > 2, ? , 
5 if  > 5, 10 if > 10. 
3.3 Third-Order Features 
The order of parsing is defined according to the 
number of dependencies it contains (Koo and Col-
lins, 2010). Collins classifies the third-order as two 
models, Model 1 is all grand-siblings, and Model 2 
is grand-siblings and tri-siblings. A grand-sibling 
is a 4-tuple of indices (g, h, m, s) where g is grand-
father. (h, m, s) is a sibling part and (g, h, m) is a 
grandchild part as well as (g, h, s). A tri-sibling 
part is also a 4-tuple of indices (h, m, s, t). Both (h, 
m, s) and (h, s, t) are siblings. Figure 2 clearly 
shows these relations. 
g h  s  m ?h t  s m
 
Figure 2: Grand-siblings and tri-siblings dependency. 
   Collins and Koo implement an efficient third-
order dependency parsing algorithm, but still time 
consuming compared with the second-order 
(McDonald, 2006). For that reason, we only add 
third-order relation features into our system instead 
of implementing the third-order dependency pars-
ing model. These features shown in Table 4 are 
grand-sibling and tri-sibling described above. 
Shorthand notations are almost the same with the 
Section 3.1 and 3.2 except that we use c3 for the 
third sibling and g represent the grandfather. We 
attempt to add features of words form and parts-of-
speech as well as directions into our system, which 
is used both in first-order and second-order as fea-
tures, but result shows that these decrease the sys-
tem performance. 
Tri-Sibling 
c1-pos, c2-pos, c3-pos?dir(c1, c2) 
Grandfather and Two Children 
g-pos, c1-pos, c2-pos?dir(c1, c2) 
g-pos, p-pos, c1-pos, c2-pos?dir(c1, c2) 
Table 4: Third-order features. 
432
4 Experiment result analysis 
As we all know that projective dependency parsing 
using edge based factorization can be processed by 
the Einster algorithm (Einster, 1996). The corpus 
given by SemEval-2012 is consists of 10000 sen-
tences converting into dependency structures from 
Chinese Penn Treebank randomly. We find that 
none of non-projective sentence existing by testing 
the 8301 sentences in training data. For this reason, 
we set the MSTParser into projective parsing mode. 
    We perform a number of experiments where we 
compare the first-order, second-order and second-
order by adding third-order features proposed in 
the previous sections. We train the model on the 
full training set which contains 8301 sentences to-
tally. We use 10 training iterations and projective 
decoding in the experiments. Experimental results 
show that 10 training iterations are better than oth-
ers. After adjusting the features of third-order, our 
best result reaches the labeled attachment score of 
62.48% on the developing dataset which ignores 
punctuation. We submitted our currently best result 
to SemEval-2012 which is 61.58% on the test data-
set. The results in Table 5 show that by adding 
third-order features to second-order model, we im-
prove the dependency parsing accuracies by 1.21% 
comparing to first-order model and 0.15% compar-
ing to second-order model. 
Models LAS UAS 
First-Order 61.26 80.18 
Second-Order 62.33 81.40 
Second-Order+ 62.48 81.43 
Table 5: Experimental results. Second-Order+ means 
second-order model by adding third-order features. 
Results are tested under the developping dataset which 
contains the heads and semantic relations given by 
organizer. 
5 Conclusion and Future Work  
In this paper, we have presented the semantic de-
pendency parsing and shown it works on the first-
order model, second-order model and second-order 
model by adding third-order features. Our experi-
mental results show more significant improve-
ments than the conventional approaches of third-
order model. 
In the future, we firstly plan to implement the 
third-order model by adding higher-order features, 
such as forth-order features. We have found that 
both in the first-order and second-order model of 
MSTParser, words form and lemmas are recog-
nized as two different features. These features are 
essential in languages that have different grid, 
however, which are the same in Chinese in the giv-
en dataset. Things are the same in POS (part-of-
speech tags) and CPOS (fine-grid POS) which are 
viewed as different features. For the applications of 
syntactic and semantic parsing, the parsing time 
and memory footprint are very important. There-
fore, secondly, we decide to remove these repeated 
features in order to reduce to training time as well 
as the space if it does not lower the system perfor-
mance.  
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. 
References  
Mihai Surdeanu, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and JoakimNivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic andse-
mantic dependencies. In Proceedings of the 12th
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?is 
M?rquez, Adam Meyers, Joakim Nivre, SebastianPa-
do, Jan ?tep?nek, Pavel Stran?k, Miahi Surdeanu, 
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of the 
13th CoNLL-2009, June 4-5, Boulder, Colorado, 
USA. 
 
CoNLL-2008. 
Bohnet, Bernd. 2009. Efficient parsing of syntactic and 
semantic dependency structures. In Proceedings of 
CoNLL-09. 
Ryan McDonald. 2006. Discriminative Learning and       
Spanning Tree Algorithms for Dependency Parsing.     
Ph.D. thesis, University of Pennsylvania. 
Ryan McDonald and Fernando Pereira. 2006. Online 
     Learning of Approximate Dependency Parsing Al-
grithms. In In Proc. of EACL, pages 81?88. 
Ryan. McDonald, K. Crammer, and F. Pereira. 2005a.         
Online large-margin training of dependency parsers. 
In Proc. of the 43rd Annual Meeting of the ACL. 
Ryan. McDonald, F. Pereira, K. Ribarov, and J. Haji?c. 
2005c. Non-projective dependency parsing using 
spanning tree algorithms. In Proc. HLT-EMNLP. 
Richard Johansson. 2008. Dependency-based Semantic 
Analysis of Natural-language Text. Ph.D. thesis, 
Lund University. 
433
Jason Eisner. 1996. Three new probabilistic models for   
dependency parsing: An exploration. In Proceedings 
of the 16th International Conference on Computa-
tional Linguistics (COLING-96), pages 340?345, 
Copenhaen. 
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. 
Memory-Based Dependency Parsing. In Proceedings 
of the 8th CoNLL, pages 49?56, Boston, Massachu-
setts. 
Terry Koo, Michael Collins. 2010. Efficient Third-order 
Dependency Parsers. Proceedings of the 48th Annual 
Meeting of the Association for Computational Lin-
guistics, pages 1?11. 
 
434
Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 1?8
Manchester, August 2008
Semantic Chunk Annotation for complex questions using Conditional 
Random Field 
Shixi Fan 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
fanshixi@hit.edu.cn 
Yaoyun Zhang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
Xiaoni5122@gmail.com 
Wing W. Y. Ng 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wing@hitsz.edu.cn 
Xuan Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxuan@insun.hit.edu.cn 
Xiaolong Wang 
Department of computer science  
Harbin Institute of Technology 
Shenzhen Graduate School, 
Shenzhen,518055, china 
wangxl@insun.hit.edu.cn 
 
 
Abstract 
This paper presents a CRF (Conditional 
Random Field) model for Semantic 
Chunk Annotation in a Chinese Question 
and Answering System (SCACQA). The 
model was derived from a corpus of real 
world questions, which are collected 
from some discussion groups on the 
Internet. The questions are supposed to 
be answered by other people, so some of 
the questions are very complex. Mutual 
information was adopted for feature se-
lection.  The training data collection con-
sists of 14000 sentences and the testing 
data collection consists of 4000 sentences. 
The result shows an F-score of 93.07%. 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1 Introduction 
1.1 Introduction of Q&A System 
Automated question answering has been a hot 
topic of research and development since the ear-
liest AI applications (A.M. Turing, 1950). Since 
then there has been a continual interest in proc-
essing knowledge and retrieving it efficiently to 
users automatically. The end of the 1980s saw a 
boost in information retrieval technologies and 
applications, with an unprecedented growth in 
the amount of digital information available, an 
explosion of growth in the use of computers for 
communications, and the increasing number of 
users that have access to all this information 
(Diego Moll?and Jose?Luis Vicedo, 2007).  
Search engines such as Google, Yahoo, Baidu 
and etc have made a great success for people?s 
information need. 
Anyhow, search engines are keywords-based 
which can only return links of relevant web 
pages, failing to provide a friendly user-interface 
with queries expressed in natural language sen-
tences or questions, or to return precise answers 
to users. Especially from the end of the 1990s, as 
1
information retrieval technologies and method-
ologies became mature and grew more slowly in 
pace, automated question answering(Q&A) sys-
tems which accept questions in free natural lan-
guage formations and return exactly the answer 
or a short paragraph containing relevant informa-
tion has become an urgent necessity. Major in-
ternational evaluations such as TREC, CLEF and 
NTCIR have attracted the participation of many 
powerful systems.  
The architecture of a Q&A system generally in-
cludes three modules: question processing, can-
didate answer/document retrieval, and answer 
extraction and re-ranking.      
1.2 Introduction of Question Analyzing      
Question Analyzing, as the premise and founda-
tion of the latter two modules, is of paramount 
importance to the integrated performance of a 
Q&A system. The reason is quite intuitive: a 
question contains all the information to retrieve 
the corresponding answer. Misinterpretation or 
too much loss of information during the process-
ing will inevitably lead to poor precision of the 
system. 
The early research efforts and evaluations in 
Q&A were focused mainly on factoid questions 
asking for named entities, such as time, numbers, 
and locations and so on. The questions in the test 
corpus of TREC and other organizations are also 
in short and simple form. Complex hierarchy in 
question types (Dragomir Radev et al 2001), 
question templates (Min-Yuh Day et al 2005), 
question parsing (Ulf Hermjakob, 2001) and 
various machine learning methods (Dell Zhang 
and Wee Sun Lee, 2003)are used for factoid 
question analysis, aiming to find what named 
entity is asked in the question. There are some 
questions which are very complicated or even 
need domain restricted knowledge and reasoning 
technique. Automatic Q&A system can not deal 
with such questions with current technique.    
In china, there is a new kind of web based Q&A 
system which is a special kind of discussion 
group. Unlike common discussion group, in the 
web based Q&A system one user posts a ques-
tion, other users can give answers to it. It is 
found that at least 50% percent questions 
(Valentin Jijkoun and Maarten de Rijke, 
2005)posted by users are non-factoid and surely 
more complicated both in question pattern and 
information need than those questions in the test 
set of TREC and other FAQ.  An example is as 
follows: 
 
This kind of Q&A system can complement the 
search engines effectively.  As the best search 
engines in china, Baidu open the Baidu Knowl-
edge2 Q&A system from 2003, and now it has 
more than 29 million question-answer pairs. 
There are also many other systems of this kind 
such as Google Groups, Yahoo Answers and 
Sina Knowledge3. This kind of system is a big 
question-answer pair database which can be 
treated as a FAQ database. How to search from 
the database and how to analyze the questions in 
the database needs new methods and techniques.   
More deeper and precise capture of the semantics 
in those complex questions is required. This phe-
nomenon has also been noticed by some re-
searchers and organizations. The spotlight gradu-
ally shifted to the processing and semantic un-
derstanding of complex questions. From 2006, 
TREC launched a new annually evaluation 
CIQ&A (complex, interactive Question Answer-
ing), aiming to promote the development of in-
teractive systems capable of addressing complex 
information needs. The targets of national pro-
grams AQUAINT and QUETAL are all at new 
interface and new enhancements to current state-
of-the-art Q&A systems to handle more complex 
inputs and situations. 
A few researchers and institutions serve as pio-
neers in complex questions study. Different tech-
nologies, such as definitions of different sets of 
question types, templates and sentence patterns 
(Noriko Tomuro, 2003) (Hyo-Jung Oh et al 
2005) machine learning methods (Radu Soricut 
and Eric Brill, 2004), language translation model 
(Jiwoon Jeon, W et al 2005), composition of 
information needs of the complex question 
(Sanda Harabagiu et al 2006) and so on, have 
been experimented on the processing of complex 
question, gearing the acquired information to the 
facility of other Q&A modules.  
Several major problems faced now by researcher 
of complex questions are stated as follow:  
First: Unlike factoid questions, it is very dif-
ficult to define a comprehensive type hierarchy 
for complex questions. Different domains under 
research may require definitions of different sets 
of question types, as shown in (Hyo-Jung Oh et 
al, 2005). Especially, the types of certain ques-
                                                 
2 http://zhidao.baidu.com/ 
3 http://iask.sina.com.cn/ 
2
tions are ambiguous and hard to identify. For 
example: 
 
This question type can be treated as definition, 
procedure or entity. 
Second: Lack of recognition of different seman-
tic chunks and the relations between them. 
FAQFinder (Radu Soricut and Eric Brill, 2004) 
also used semantic measure to credit the similar-
ity between different questions. Nevertheless, the 
question similarity is only a simple summation of 
the semantic similarity between words from the 
two question sentences. Question pattern are very 
useful and easy to implement, as justified by pre-
vious work. However, just like the problem with 
question types, question patterns have limitation 
on the coverage of all the variations of complex 
question formation. Currently, after the question 
processing step in most systems, the semantic 
meaning of large part of complex questions still 
remain vague. Besides, confining user?s input 
only within the selection of provided pattern may 
lead to unfriendly and unwelcome user interface. 
(Ingrid Zukerman and Eric Horvitz, 2001) used 
decision tree to model and recognize the infor-
mation need, question and answer coverage, 
topic, focus and restrictions of a question. Al-
though features employed in the experiments 
were described in detail, no selection process of 
those feature, or comparison between them was 
mentioned. 
This paper presents a general method for Chinese 
question analyzing. Our goal is to annotate the 
semantic chunks for the question automatically.  
2 Semantic Chunk Annotation 
Chinese language differs a lot from English in 
many aspects. Mature methodologies and fea-
tures well-justified in English Q&A systems are 
valuable sources of reference, but no direct copy 
is possible.  
The Ask-Answer system 4  is a Chinese online 
Q&A system where people can ask and answer 
questions like other web based Q&A system. The 
characteristic of this system is that it can give the 
answer automatically by searching from the 
asked question database when a new question is 
presented by people. The architecture of the 
automatically answer system is shown in figure 1.  
The system contains a list of question-answer 
pairs on particular subject. When users input a 
                                                 
 
 
 
4 http://haitianyuan.com/qa 
question from the web pages, the question is 
submitted to the system and then question-
answer pair is returned by searching from the 
questions asked before. The system includes four 
main parts: question pre-processing, question 
analyzing, searching and answer getting.  
The question pre-processing part will segment 
the input questions into words, label POS tags 
for every word.  Sometimes people ask two or 
more questions at one time, the questions should 
be made into simple forms by conjunctive struc-
ture detection. The question analyzing program 
will find out the question type, topic, focus and 
etc. The answer getting part will get the answer 
by computing the similarity between the input 
question and the questions asked before. The 
question analyzing part annotates the semantic 
chunks for the question. So that the question can 
be mapped into semantic space and the question 
similarity can be computed semantically. The 
Semantic chunk annotation is the most important 
part of the system. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Question Pre- processing 
 
Segmentation and  pos 
tagging 
Detect conjunctive structure  
Question Analyzing 
Semantic chunk annotationGet and extend key words
Question pattern and knowledge base 
Search reference question-answer pairs form database 
Answer getting 
Score the constituent 
answers 
Out put the top 
five answers 
 
Figure 1 the architecture of the automatically 
answer system 
Currently, no work has been reported yet on the 
question semantic chunk annotation in Chinese. 
The prosperity of major on-line discussion 
groups provides an abundant ready corpus for 
question answering research. Using questions 
collected from on-line discussion groups; we 
make a deep research on semantic meanings and 
build a question semantic chunk annotation 
model based on Conditional Random Field. 
Five types of semantic chunks were defined: 
Topic, Focus, Restriction, Rubbish information 
and Interrogative information. The topic of a 
3
question which is the topic or subject asked is the 
most important semantic chunk. The focus of a 
question is the asking point of the question. The 
restriction information can restrict the question?s 
information need and the answers. The rubbish 
information is those words in the question that 
has no semantic meanings for the question. Inter-
rogative information is a semantic tag set which 
corresponds to the question type. The interroga-
tive information includes interrogative words, 
some special verbs and nouns words and all these 
words together determine the question type. The 
semantic chunk information is shown in table 1.  
 
Semantic 
chunk   tag 
Abbreviation Meaning 
Topic T The question subject 
Focus F The additional information 
of topic 
Restrict 
 
Re Such as Time restriction and 
location restriction 
Rubbish 
information 
Ru Words no meaning for the 
question 
Other O other information without 
semantic meaning 
The following is interrogative information 
Quantity Wqua  
Description Wdes The answer need description
Yes/No Wyes The answer should be yes or 
no 
List Wlis The answer should be a list 
of entity 
Definition Wdef The answer is the definition 
of topic 
Location Wloc The answer is location 
Reason Wrea The answer can explain the 
question 
Contrast Wcon The answer is the compari-
son of the items proposed in 
the question 
People Wwho The answer is about the 
people?s information 
Choice Wcho The answer is one of the 
choice proposed in the ques-
tion 
Time Wtim The answer is the data or 
time length about the event 
in the question 
Entity Went The answer is the attribute 
of the topic. 
Table 1: Semantic chunks  
An annotation example question is as follows: 
 
This question can be annotated as follows: 
 
This kind of annotation is not convenient for CRF 
model, so the tags were transfer into the B I O 
form. (Shown as follows) 
 
Then the Semantic chunk annotation can be 
treated as a sequence tag problem.  
3 Semantic Chunk Annotation model 
3.1 Overview of the CRF model 
The conditional random field (CRF) is a dis-
criminative probabilistic model proposed by John 
Lafferty, et al(2001) to overcome the long-range 
dependencies problems associated with genera-
tive models. CRF was originally designed to la-
bel and segment sequences of observations, but 
can be used more generally. Let X, Y be random 
variables over observed data sequences and cor-
responding label sequences, respectively. For 
simplicity of descriptions, we assume that the 
random variable sequences X and Y have the 
same length, and use [ ]mxxxx ......, 21=   
and [ ]myyyy ......, 21=  to represent instances of 
X and Y, respectively. CRF defines the condi-
tional probability distribution P(Y |X) of label 
sequences given observation sequences as fol-
lows 
)),(exp(
)(
1
)|(
1
?
=
=
n
i
ii YXfXZ
XYP ?
?
?    (1) 
Where  is the normalizing factor that 
ensures equation 2. 
)(XZ?
 ? =y xyP 1)|(?                   (2) 
In equation 2 the i? is a model parameter and 
 is a feature function (often binary-
valued) that becomes positive (one for binary-
valued feature function) when X contains a cer-
tain feature in a certain position and Y takes a 
certain label, and becomes zero otherwise. 
Unlike Maximum Entropy model which use sin-
gle normalization constant to yield a joint distri-
bution, CRFs use the observation-dependent 
normalization  for conditional distribu-
tions. So CRFs can avoid the label biased prob-
lem. Given a set of training data 
),( YXfi
)(XZ?
}....2,1),,{( nkyxT kk ==  
 With an empirical distribution , CRF ),(
~
YXP
4
determines the model parameters }{ i?? =  by 
maximizing the log-likelihood of the training set 
)|(log),(
)|(log)(
,
~
1
xyPyxP
xyPP
yx
N
k
kk
?
??
?
?
?
=?
=                       (3) 
3.2 Features for the model 
The following features, which are used for train-
ing the CRF model, are selected according to the 
empirical observation and some semantic mean-
ings. These features are listed in the following 
table. 
 
Feature type in-
dex 
Feature type name 
1 Current word 
2 Current POS tag 
3 Pre-1 word POS tag 
4 Pre-2 word POS tag 
5 Post -1 word POS tag 
6 Post -2 word POS tag 
7 Question pattern 
8 Question type 
9 Is pattern key word 
10 Pattern tag 
Table 2: the Features for the model 
Current word: 
The current word should be considered when 
adding semantic tag for it. But there are too 
many words in Chinese language and only part 
of them will contribute to the performance, a set 
of words was selected. The word set includes 
segment note and some key words such as time 
key word and rubbish key word. When the cur-
rent word is in the word set the current word fea-
ture is the current word itself, and null on the 
other hand. 
Current POS tag: 
Current POS tag is the part of speech tag for the 
current word. 
Pre-1 word POS tag: 
Pre- 1 word POS tag is the POS tag of the first 
word before the labeling word in the sentence. If 
the Pre-1 word does not exit (the current is the 
first word in the sentence), the Pre- 1 word POS 
tag is set to null. 
Pre-2 word POS tag: 
Pre- 2 word POS tag is the POS tag of the second 
word before the labeling word in the sentence. If 
the Pre-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Post -1 word POS tag: 
Post - 1 word POS tag is the POS tag of the first 
word after the labeling word in the sentence. If 
the Post -1 word does not exit (the current is the 
first word in the sentence), the Post - 1 word POS 
tag is set to null. 
Post -2 word POS tag: 
Post - 2 word POS tag is the POS tag of the sec-
ond word after the labeling word in the sentence. 
If the Post-2 word does not exit, the Pre- 2 word 
POS tag is set to null. 
Question pattern: 
Question pattern which is associated with ques-
tion type, can locate question topic, question fo-
cus by surface string matching. For example, 
(where is <topic>). The patterns are extracted 
from the training data automatically. When a pat-
tern is matched, it is treated as a feature. There 
are 1083 question patterns collected manually.  
Question type: 
Question type is an important feature for ques-
tion analyzing. The question patterns have the 
ability of deciding the question type. If there is 
no question pattern matching the question, the 
question type is defined by a decision tree algo-
rithm. 
Is pattern key word: 
For each question pattern, there are some key 
words. When the current word belongs to the 
pattern key word this feature is set to ?yes?, else 
it is set to ?no?. 
Pattern tag: 
When a pattern is matched, the topic, focus and 
restriction can be identified by the pattern. We 
can give out the tags for the question and the tags 
are treated as features. If there is no pattern is 
matched, the feature is set to null.   
4 Feature Selection experiment 
Feature selection is important in classifying sys-
tems such as neural networks (NNs), Maximum 
Entropy, Conditional Random Field and etc. The 
problem of feature selection has been tackled by 
many researchers. Principal component analysis 
(PCA) method and Rough Set Method are often 
used for feature selection. Recent years, mutual 
information has received more attention for fea-
ture selection problem.  
According to the information theory, the uncer-
tainty of a random variable X can be measured 
by its entropy . For a classifying problem, 
there are class label set represented by C and fea-
ture set represented by F. The conditional en-
tropy  measures the uncertainty about 
)(XH
)|( FCH
5
C when F is known, and the Mutual information 
I(C, F) is defined as:  
 F)|(C -(C));( HHFCI =                   (4) 
The feature set is known; so that the objective of 
training the model is to minimize the conditional 
entropy   equally maximize the mutual 
information . In the feature set F, some 
features are irrelevant or redundant. So that the 
goal of a feature selection problem is to find a 
feature S ( ), which achieve the higher 
values of . The set S is a subset of F and 
its size should be as small as possible. There are 
some algorithms for feature selection problem. 
The ideal greedy selection algorithm using mu-
tual information is realized as follows (Nojun 
Kwak and Chong-Ho Choi, 2002): 
)|( FCH
);( FCI
FS ?
);( FCI
 Input:   S- an empty set 
             F- The selected feature set 
Output:  a small reduced feature set S which is 
equivalent to F 
Step 1: calculate the MI with the Class 
set C , , compute  Ffi ?? );( ifCI
Step 2: select the feature that maximizes , 
set  
);( ifCI
}{},{\ ii fSfFF ??
Step 3: repeat until desired number of features 
are selected. 
1) Calculate the MI with the Class set C and S, 
Ffi ?? , compute  ),;( ifSCI
2) Select the feature that maximizes , 
set 
),;( ifSCI
}{},{\ ii fSfFF ??  
Step 4: Output the set S  that contains the se-
lected features 
To calculate MI the PDFs (Probability Distribu-
tion Functions) are required. When features and 
classing types are dispersing, the probability can 
be calculated statistically.  In our system, the 
PDFs are got from the training corpus statistically. 
The training corpus contains 14000 sentences. 
The training corpus was divided into 10 parts, 
with each part 1400 sentences.  And each part is 
divided into working set and checking set. The 
working set, which contains 90% percent data, 
was used to select feature by MI algorithm. The 
checking set, which contains 10% percent data, 
was used to test the performance of the selected 
feature sequence. When the feature sequence was 
selected by the MI algorithm, a sequence of CRF 
models was trained by adding one feature at each 
time. The checking data was used to test the per-
formance of these models.  
 The open test result 
Selected feature 
sequence 
1 2 3 4 5 6 7 8 9 10 
7, 10, 3, 1, 5, 2, 
4, 6, 8?9 
0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018 
7, 10, 1, 3, 5, 2, 
4?6?8?9 
0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949 
7, 10, 1, 3, 5, 2, 
4, 6?9?8 
0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007 
7, 10, 3, 1, 5, 2, 
4?6?8?9 
0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011 
7, 10, 1, 3, 5, 2, 
4, 6, 8, 9 
0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023 
7, 10, 1, 3, 5, 2, 
4, 6?8?9 
0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986 
7, 10, 1, 3, 5, 2, 
4, 6, 8?9 
0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067 
Table 3: the feature selection result and the test result 
In table 3, each row contains data corresponding 
to one part of the training corpus so there are ten 
rows with data in the table. The third row corre-
sponds to the first part and the last row corre-
sponds to the tenth part. There are eleven col-
umns in the table, the first columns is the fea-
tures sequence selected by the mutual informa-
tion algorithm for each part. The second column 
is the open test result with the first feature in the 
feature sequence. The third column is the open 
test result with the first two features in the fea-
ture sequence and so on. From the table, it is 
6
clear that the feature 7(Question pattern) and 
10(Pattern tag) are very important, while the fea-
ture 8(Question type) and 9(Is pattern key word) 
are not necessary. The explanation about this 
phenomenon is that the ?pattern key word? and 
?Question type? information can be covered by 
the Question patterns. So feature 8 and 9 are not 
used in the Conditional Random Field model. 
5 Semantic Chunk Annotation Experi-
ment 
The test and training data used in our system are 
collected from the website (Baidu knowledge 
and the Ask-Answer system), where people pro-
posed questions and answers. The training data 
consists of 14000 and the test data consists of 
4000 sentences. The data set consists of word 
tokens, POS and semantic chunk tags. The POS 
and semantic tags are assigned to each word to-
kens.  
The performance is measured with three rates: 
precision (Pre), recall (Rec) and F-score (F1). 
Pre = Match/Model                     (5) 
Rec=Match/Manual                    (6) 
F1=2*Pre*Rec/(Pre+Rec)              (7) 
Match is the count of the tags that was predicted 
right. Model is the count of the tags that was pre-
dicted by the model. Manual is the count of the 
tags that was labeled manually. 
Table 4 shows the performance of annotation of 
different semantic chunk types. The first column 
is the semantic chunk tag. The last three columns 
are precision, recall and F1 value of the semantic 
chunk performance, respectively.   
 
Label Manual Model Match Pre.() Rec.() F1 
B-T?I-T 17061?78462 16327?80488 14825?76461 90.80?95.00 86.89?97.45 88.80?96.21 
B-F?I-F 5072?13029 5079?13583 4657?12259 91.69?90.25 91.82?94.09 91.75?92.13 
B-Ru?I-Ru 775?30 11?0 2?0 18.18?0.00 0.26?0.00 0.51?0.00 
O 8354 8459 6676 78.92 79.91 79.41 
B-Wqua?I-Wqua 1363?934 1327?1028 1298?881 97.81?85.70 95.23?94.33 96.51?89.81 
B-Wyes?I-Wyes 5669?1162 5702?1098 5550?1083 97.33?98.63 97.90?93.20 97.62?95.84 
B-Wdes?I-Wdes 2907?278 2855?185 2779?184 97.34?99.46 95.60?66.19 96.46?79.48 
B-Wlis?I-Wlis 603?257 563?248 560?248 99.47?100 92.87?96.50 96.05?98.22 
B-Wdef?I-Wdef 1420?1813 1430?1878 1280?1695 89.51?90.26 90.14?93.49 89.82?91.85 
B-Wloc?I-Wloc 683?431 665?395 661?392 99.40?99.24 96.78?90.95 98.07?94.92 
B-Wrea?I-Wrea 902?159 873?83 843?82 96.56?98.80 93.46?51.57 94.99?67.77 
B-Wcon?I-Wcon 552?317 515?344 503?291 97.67?84.59 91.12?91.80 94.28?88.05 
B-Wwho?I-Wwho 420?364 357?350 348?336 97.48?96.00 82.86?92.31 89.58?94.12 
B-Wcho?I-Wcho 857?85 738?0 686?0 92.95?0.00 80.05?0.00 86.02?0.00 
B-Wtim?I-Wtim 408?427 401?419 355?380 88.53?90.69 87.01?88.99 87.76?89.83 
B-Went?I-Went 284?150 95?81 93?80 97.89?98.77 32.75?53.33 49.08?69.26 
Avg 145577 145577 135488 93.07 93.07 93.07 
Table 4: the performance of different semantic chunk
 
The semantic chunk type of ?Topic? and ?Focus? 
can be annotated well. Topic and focus semantic 
chunks have a large percentage in all the seman-
tic chunks and they are important for question 
analyzing. So the result is really good for the 
whole Q&A system. 
As for ?Rubbish? semantic chunk, it only has 
0.51 and 0.0 F1 measure for B-Ru and I-Ru. One 
reason is lacking enough training examples, for 
there are only 1031 occurrences in the training 
data. Another reason is sometimes restriction is 
complex. 
6 Conclusion and future work 
This paper present a new method for Chinese 
question analyzing based on CRF. The features 
are selected by using mutual information algo-
rithm. The selected features work effectively for 
the CRF model. The experiments on the test data 
set achieve 93.07% in F1 measure. In the future, 
new features should be discovered and new 
methods will be used.  
Acknowledgment  
This work is supported by Major Program of Na-
tional Natural Science Foundation of China 
(No.60435020 and No. 90612005) and the High 
Technology Research and Development Program 
of China (2006AA01Z197). 
 
References 
A.M. Turing. 1950. Computing Machinery and 
Intelligence. Mind, 236 (59): 433~460. 
Diego Moll?, Jose?Luis Vicedo. 2007. Question 
Answering in Restricted Domains: An Overview. 
Computational Linguistics, 33(1),  
7
Dragomir Radev, WeiGuo Fan, Leila Kosseim. 2001. 
The QUANTUM Question Answering System. 
TREC. 
Min-Yuh Day, Cheng-Wei Lee, Shih-Hung WU, 
Chormg-Shyong Ong,  Wen-Lian Hsu. 2005. An 
Integrated Knowledge-based and Machine 
Learning Approach for Chinese Question 
Classification. Proceedings of the IEEE 
International Conference on Natural Language 
Processing and Knowledge Engineering, Wuhan, 
China,:620~625. 
Ulf Hermjakob. 2001. Parsing and Question 
Classification for Question Answering.  
Proceedings of the ACL Workshop on Open-
Domain Question Answering, Toulouse,:19~25. 
Dell Zhang, Wee Sun Lee. 2003. Question 
classification using support vector machines. 
Proceedings of the 26th Annual International ACM 
Conference on Research and Development in 
Information Retrieval(SIGIR), Toronto, Canada,26 
~ 32. 
Valentin Jijkoun, Maarten de Rijke.2005. Retrieving 
Answers from Frequently Asked Questions Pages 
on the Web. CIKM?05, Bermen, Germany. 
Noriko Tomuro. 2003. Interrogative Reformulation 
Patterns and Acquisition of Question Paraphrases. 
Proceeding of the Second International Workshop 
on Paraphrasing, :33~40. 
Hyo-Jung Oh, Chung-Hee Lee, Hyeon-Jin Kim, 
Myung-Gil Jang. 2005. Descriptive Question 
Answering in Encyclopedia. Proceedings of the 
ACL Interactive Poster and Demonstration Sessions, 
pages 21?24, Ann Arbor. 
Radu Soricut, Eric Brill. 2004, Automatic Question 
Answering: Beyond the Factoid.  Proceedings of 
HLT-NAACL ,:57~64. 
Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. 2005. 
Finding Similar Questions in Large Question and 
Answer Archives. CIKM?05, Bremen, Germany. 
Sanda Harabagiu, Finley Lacatusu and Andrew Hickl. 
2006 . Answering Complex Questions with Random 
Walk Models. SIGIR?06, Seattle, Washington, 
USA.pp220-227. 
Ingrid Zukerman, Eric Horvitz. 2001. Using Machine 
Learning Techniques to Interpret WH-questions. 
ACL. 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: probabilistic 
Models for Segmenting and Labeling Sequence 
Data. Proceedings of the Eighteenth International 
Conference on Machine Learning, p.282-289. 
Nojun Kwak and Chong-Ho Choi. 2002. Input 
feature selection for classification problems. 
IEEE Trans on Neural Networks,,13(1):143-
159 
 
8
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 218?222
Manchester, August 2008
Discriminative Learning of Syntactic and Semantic Dependencies
Lu Li
1
, Shixi Fan
2
, Xuan Wang
1
, Xiaolong Wang
1
Shenzhen Graduate School, Harbin Institute of Technology,
Xili, Shenzhen 518055, China
1
{lli,wangxuan,wangxl}@insun.hit.edu.cn
2
fanshixi@hit.edu.cn
Abstract
A Maximum Entropy Model based system
for discriminative learning of syntactic and
semantic dependencies submitted to the
CoNLL-2008 shared task (Surdeanu, et al,
2008) is presented in this paper. The sys-
tem converts the dependency learning task
to classification issues and reconstructs the
dependent relations based on classification
results. Finally F1 scores of 86.69, 69.95
and 78.35 are obtained for syntactic depen-
dencies, semantic dependencies and the
whole system respectively in closed chal-
lenge. For open challenge the correspond-
ing F1 scores are 86.69, 68.99 and 77.84.
1 Introduction
Given sentences and corresponding part-of-speech
of each word, the learning of syntactic and seman-
tic dependency contains two separable goals: (1)
building a dependency tree that defines the syn-
tactic dependency relationships between separated
words; (2) specifying predicates (no matter verbs
or nouns) of the sentences and labeling the seman-
tic dependents for each predicate.
In this paper a discriminative parser is pro-
posed to implement maximum entropy (ME) mod-
els (Berger, et al, 1996) to address the learning
task. The system is divided into two main subsys-
tems: syntactic dependency parsing and semantic
dependency labeling. The former is used to find a
well-formed syntactic dependency tree that occu-
pies all the words in the sentence. If edges are
added between any two words, a full-connected
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
graph is constructed and the dependency tree could
be found using a maximum spanning tree (MST)
algorithm (McDonald, et al, 2005). The latter fo-
cuses on separable predicates whose semantic de-
pendents could be determined using classification
tools, such as ME models
1
etc..
We participated in both closed and open chal-
lenge of the CoNLL-2008 shared task (Surdeanu,
et al, 2008). Results are reported on both develop-
ment and test sets in this paper.
2 System Description
2.1 Syntactic Parsing
The goal of syntactic parsing is to create a la-
beled syntactic dependency parse y for input sen-
tence x including words and their parts of speech
(POS). Inspired by the parsing model that imple-
ments maximum spanning tree (MST) algorithm
to induce the dependency parsing tree (McDonald,
et al, 2005), the system employs the same frame-
work. The incorporated features are defined over
parts of speech of words occurring between and
around a possible head-dependent relation.
Suppose G = (V, E) is a directed graph, where
V is the set of vertices denoting the words in sen-
tence x and E is the set of directed edges between
any two vertices with some scores. The MST al-
gorithm is to find the most probable subgraph of G
that satisfies tree constraints over all vertices. The
score function of the parsing tree y is defined as
s(y) =
?
(i,j)?y
s(i, j) (1)
where (i, j) ? y indicates an edge in y from word
i to word j and s(i, j) denotes its score. Suppose Y
1
http://homepages.inf.ed.ac.uk/s0450736/maxent.html
218
wi
w
j
p
i
p
j
(w
i
, p
i
) (w
j
, p
j
)
(w
i
, w
j
) (p
i
, p
j
)
(w
i
, p
j
) (w
j
, p
i
)
(w
i
, w
j
, p
i
) (w
i
, w
j
, p
j
)
(p
i
, p
j
, w
i
) (p
i
, p
j
, w
j
)
(w
i
, w
j
, p
i
, p
j
) (p
i
, p
k
, p
j
), i < k < j
(p
i
, p
i+1
, p
j?1
, p
j
) (p
i?1
, p
i
, p
j?1
, p
j
)
(p
i
, p
i+1
, p
j
, p
j+1
) (p
i?1
, p
i
, p
j
, p
j+1
)
Table 1: Features for syntactic parsing.
is the set of syntactic dependency labels, the score
function of edges is defined as
s(i, j) = max
l?Y
Pr(l|x, i, j) (2)
ME models are used to calculate the value of
Pr(l|x, i, j), where the features are extracted from
input sentence x. Given i and j as the subscripts
of words in the sentence and word i is the parent
of word j, the features can be illustrated in table
1. w
i
and p
i
are denoted as the ith word and the
ith part of speech respectively in the sentence. The
tuples define integrated features, such as (w
i
, p
i
)
indicates the feature combining the ith word and
ith part of speech. Besides these features, the dis-
tant between word i and word j in sentence x is
considered as a single feature. The distant is also
combined with features in table 1 to produce com-
plex features.
2.2 Semantic Dependency Labeling
Semantic dependencies are always concerning
with specific predicates. Unlike syntactic depen-
dencies, semantic dependency relationships usu-
ally can not be represented as a tree. Thus, the
method used for semantic dependency labeling
is somewhat different from syntactic dependency
parsing. The work of semantic labeling can be di-
vided into two stages: predicate tagging and de-
pendents recognizing.
2.2.1 Predicate Tagging
According to PropBank (Palmer, et al, 2005)
and NomBank (Meyers, et al, 2004), predicates
usually have several rolesets corresponding to dif-
ferent meanings. For example, the verb abandon
has three rolesets marked as ordinal numbers 01,
02 and 03 as described below.
w
i
p
i
p
i?1
p
i+1
(p
i?1
, p
i
) (p
i
, p
i+1
)
(p
i?2
, p
i
) (p
i
, p
i+2
)
(p
i?3
, p
i
) (p
i
, p
i+3
)
(p
i?1
, p
i
, p
i+1
) (w
i
, p
i
)
(w
i
, p
i?1
, p
i
) (w
i
, p
i
, p
i+1
)
(w
i
, p
i?2
, p
i
) (w
i
, p
i
, p
i+2
)
(w
i
, p
i?3
, p
i
) (w
i
, p
i
, p
i+3
)
(w
i
, p
i?1
, p
i
, p
i+1
)
Table 2: Features used for predicate tagging.
<frameset>
<predicate lemma=?abandon?>
<roleset id=?abandon.01? name=?leave
behind? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.02?
name=?exchange? vncls=?51.2?>
. . .
</roleset>
<roleset id=?abandon.03?
name=?surrender, give over? vncls=?-
?>
. . .
</roleset>
</predicate>
</frameset>
The goal of this part is to identify the predicates
in the sentences and to determine the roleset for
each of them. It should be cleared that the ordi-
nal numbers are only used to distinguish different
meanings of a predicate. However, if these num-
bers are treated as tags for predicates, some statisti-
cal properties will be obtained as illustrated in Fig-
ure 1. As can be seen, the distribution of the train
data would be quite informative for representing
the distribution of other three data sets. Based on
this idea, a classification framework is introduced
for predicate tagging.
Suppose the tag set is chosen to be T =
{01, 02, ..., 22} according to the horizontal axis of
Figure 1 and 00 is added to indicate that the ex-
amining word is not a predicate. Suppose t
i
is a
variable indicating the tag of word at position i in
sentences x. ME models are implemented to tag
the predicates.
t
i
= argmax
t? T
Pr(t|x, i) (3)
219
0 5 10 15 200
2
4
6
8
10
12
Ordinal Numbers of Predicates
Log
arith
mic
al N
umb
er o
f Oc
curr
enc
e
traindevelbrownws j
Figure 1: Distribution of the ordinal numbers of
predicates on different data sets. 01 - 21 are at-
tached with the predicates in the corpus and 22
stands for ?SU?.
The features for predicate tagging are listed in ta-
ble 2, where the symbols share the same mean-
ing as in table 1. Experiments show that this pure
statistic processing method is effective for predi-
cate tagging.
2.2.2 Dependents Recognizing
This subtask depends deeply on the results of
syntactic parsing and predicate tagging described
earlier in the system. Predicate tagging identifies
central words and syntactic parsing provides syn-
tactic features for its dependents identification and
classification.
Generally speaking, given a specific predicate in
a sentence, only a few of words are associated as its
semantic dependents. By statistical analysis a list
of part of speech tuples that are appearing to be se-
mantic dependency are collected. All other tuples
are filtered out to improve system performance.
Suppose (p, d) is a couple of predicate and one
of its possible dependents, T is the dependency
tree generated by syntactic parsing, L is the set of
semantic dependency labels. The dependents can
be recognized by using a classification model, ME
models are chosen as before.
l
(p,d)
= argmax
l?L
Pr(l|p, d, T ) (4)
Besides the semantic dependency labels, null is in-
cluded as a special tag to indicate that there is no
semantic dependency between p and d. As a result,
dependents identification (binary classification)
and dependents tagging (multi-classification) can
be solved together within one multi-classification
framework.
The selected features are listed below.
1. Predicate Features
? Lemma and POS of predicate, pred-
icate?s parent in syntactic dependency
tree.
? Voice active or passive.
? Syntactic dependency label of edge be-
tween predicate and its parent.
? POS framework POS list of predicate?s
siblings, POS list of predicate?s children.
? Syntactic dependency framework syn-
tactic dependency label list of the edges
between predicate?s parent and its sib-
lings.
? Parent framework syntactic depen-
dency label list of edges connecting to
predicate?s parent.
2. Dependent Features
? Lemma and POS of dependent, depen-
dent?s parent.
? POS framework POS list of depen-
dent?s siblings.
? Number of children of dependent?s par-
ent.
3. In Between Features
? Position of dependent according to
predicate: before or after.
? POS pair of predicate and dependent.
? Family relation between predicate and
dependent: ancestor or descendant.
? Path length between predicate and de-
pendent.
? Path POS POS list of all words appear-
ing on the path from predicate to depen-
dent.
? Path syntactic dependency label list of
dependency label of edges of path be-
tween predicate and dependent.
3 Experiment results
The classification models were trained using all the
training data. The detailed information are shown
in table 3. All experiments ran on 32-bit Intel(R)
Pentium(R) D CPU 3.00GHz processors with 2.0G
memory.
220
Feature Number Training Time
Syn. 7,488,533 30h
Prd. 1,484,398 8h
Sem. 3,588,514 12h
Table 3: Details of ME models. Syn. is for syntac-
tic parsing, Prd. is for predicate tagging and Sem.
is for semantic dependents recognizing.
Syntactic Semantic Overall
devel 85.29 69.60 77.49
brown 80.80 59.17 70.01
wsj 87.42 71.27 79.38
brown+wsj 86.69 69.95 78.35
(a) Closed Challenge
Syntactic Semantic Overall
devel 85.29 68.45 76.87
brown 80.80 58.22 69.51
wsj 87.42 70.32 78.87
brown+wsj 86.69 68.99 77.84
(b) Open Challenge
Table 4: Scores for joint learning of syntactic and
semantic dependencies.
3.1 Closed Challenge
The system for closed challenge is designed as a
two-stage parser: syntactic parsing and semantic
dependency labeling as described previously. Ta-
ble 4(a) shows the results on different corpus. As
shown in table 4(a), the scores of semantic depen-
dency labeling are quite low, that are influencing
the overall scores. The reason could be inferred
from the description in section 2.2.2 since seman-
tic dependent labeling inherits the errors from the
output of syntactic parsing and predicate tagging.
Following evaluates each part independently.
Besides the multiple classification model de-
scribed in table 3, a binary classification model
was built based on ME for predicate tagging. The
binary model can?t distinguish different rolesets of
predicate, but can identify which words are predi-
cates in sentences. The precision and recall for bi-
nary model are 90.80 and 88.87 respectively, while
for multiple model, the values are 84.60 and 85.60.
For semantic dependent labeling, experiments
were performed under conditions that the gold syn-
tactic dependency tree and predicates list were
given as input. The semantic scores became 80.09,
77.08 and 82.25 for devel, brown and wsj respec-
tively. This implies that the error of syntactic pars-
ing and predicate tagging could be probably aug-
mented in semantic dependent labeling. In order to
improve the performance of the whole system, the
deep dependence between the two stages should be
broken up in future research.
3.2 Open Challenge
In open challenge, the same models are used for
syntactic parsing and predicate tagging as in closed
challenge and two other models are trained for se-
mantic dependent labeling. Suppose M
mst
, M
malt
and M
chunk
are denoted as these three semantic
models, where M
mst
is the model used in closed
challenge, M
malt
is trained on the syntactic de-
pendency tree provided by the open corpus with
the same feature set as M
mst
, and M
chunk
is
trained using features extracted from name entity
and wordnet super senses results provided by the
open corpus.
Considering a possible dependent given a spe-
cific predicate, the feature set used for M
chunk
contains only six elements:
? Whether the dependent is in name entity
chunk: True or False.
? Name entity label of the dependent.
? Whether the dependent is in BBN name entity
chunk: True or False.
? BBN name entity label of the dependent.
? Whether the dependent is in wordnet super
sense chunk: True or False.
? Wordnet super sense label of the dependent.
After implementing these three models on se-
mantic dependents recognizing, the results were
merged to generate the scores described in table
4(b).
The merging strategy is quite simple. Given a
couple of predicate and dependent (p, d), the sys-
tem produces three semantic dependency labels
denoting as l
mst
, l
malt
and l
chunk
, the result la-
bel is chosen to be most frequent semantic label
among the three.
Comparing the scores of open challenge and
closed challenge, it can be found that the score of
the former is less than the latter, which is quite
strange since more resources were used in open
challenge. To examine the influences of differ-
ent semantic dependents recognizing models, each
221
Mmst
M
malt
M
chunk
devel 69.60 64.48 41.72
brown 59.17 56.52 34.04
wsj 71.27 66.40 41.83
Table 5: Semantic scores of different models.
model was implemented in the closed challenge
and the results are shown in table 5. Specially,
model M
chunk
generated too low scores and gave a
heavy negative influence on the final results. Find-
ing a good way to combine several results requires
further research.
4 Conclusions
This paper have presented a simple discriminative
system submitted to the CoNLL-2008 shared task
to address the learning task of syntactic and se-
mantic dependencies. The system was divided into
syntactic parsing and semantic dependents label-
ing. Maximum spanning tree was used to find
a syntactic dependency tree in the full-connected
graph constructed over the words of a sentence.
Maximum entropy models were implemented to
classify syntactic dependency edges, predicates
and their semantic dependents. A brief analysis
has also been given on the results of both closed
challenge and open challenge.
Acknowledgement
This research has been partially supported by the
National Natural Science Foundation of China
(No. 60435020 and No. 90612005), the Goal-
oriented Lessons from the National 863 Program
of China (No.2006AA01Z197) and Project of Mi-
crosoft Research Asia. We would like to thank
Zhixin Hao, Xiao Xin, Languang He and Tao Qian
for their wise suggestion and great help. Thanks
also to Muhammad Waqas Anwar for English im-
provement.
References
Adam Berger, Stephen Della Pietra, Vincent Della
Pietra 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39-71.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young
and Ralph Grishman 2004. The NomBank Project:
An Interim Report HLT-NAACL 2004 Workshop:
Frontiers in Corpus Annotation, 24-31.
Martha Palmer, Daniel Gildea, Paul Kingsbury 2005.
The Proposition Bank: An Annotated Corpus of Se-
mantic Roles Computational Linguistics, 31(1):71-
106.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s M`arquez and Joakim Nivre 2008. The
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008)
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c 2005. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP, 523-530.
222
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 13?17,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Cascade Method for Detecting Hedges and their Scope in Natural
Language Text
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, Shixi Fan
Key Laboratory of Network Oriented Intelligent Computation
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, Guangdong, China
{tangbuzhou,yuanbo.hitsz}@gmail.com
{wangxl,wangxuan,fanshixi}@insun.hit.edu.cn
Abstract
Detecting hedges and their scope in nat-
ural language text is very important for
information inference. In this paper,
we present a system based on a cascade
method for the CoNLL-2010 shared task.
The system composes of two components:
one for detecting hedges and another one
for detecting their scope. For detecting
hedges, we build a cascade subsystem.
Firstly, a conditional random field (CRF)
model and a large margin-based model are
trained respectively. Then, we train an-
other CRF model using the result of the
first phase. For detecting the scope of
hedges, a CRF model is trained according
to the result of the first subtask. The ex-
periments show that our system achieves
86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia cor-
pus for hedge detection, and 49.95% F-
measure on biological corpus for hedge
scope detection. Among them, 86.36%
is the best result on biological corpus for
hedge detection.
1 Introduction
Hedge cues are very common in natural language
text. Vincze et al (2008) report that 17.70% of
the sentences in the abstract section and 19.94% of
sentences in the full paper section contain hedges
on BioScope corpus. As Vincze et al (2008)
suggest that information that falls in the scope
of hedges can not be presented as factual in-
formation. Detecting hedges and their scope in
natural language text is very important for in-
formation inference. Recently, relative research
has received considerable interest in the biomed-
ical NLP community, including detecting hedges
and their in-sentence scope in biomedical texts
(Morante and Daelemans, 2009). The CoNLL-
2010 has launched a shared task for exploiting the
hedge scope annotated in the BioScope (Vincze et
al., 2008) and publicly available Wikipedia (Gan-
ter and Strube, 2009) weasel annotations. The
shared task contains two subtasks (Farkas et al,
2010): 1. learning to detect hedges in sentences on
BioScope and Wikipedia; 2. learning to detect the
in-sentence scope of these hedges on BioScope.
In this paper, we present a system based on a
cascade method for the CoNLL-2010 shared task.
The system composes of two components: one
for detecting hedges and another one for detect-
ing their scope. For detecting hedges, we build
a cascade subsystem. Firstly, conditional ran-
dom field (CRF) model and a large margin-based
model are trained respectively. Then, we train
another CRF model using the result of the first
phase. For detecting the scope of hedges, a CRF
model is trained according to the result of the first
subtask. The experiments show that our system
achieves 86.36% F-measure on biological corpus
and 55.05% F-measure on Wikipedia corpus for
hedge detection, and 49.95% F-measure on bio-
logical corpus for hedge scope detection. Among
them, 86.36% is the best result on biological cor-
pus for hedge detection.
2 System Description
As there are two subtasks, we present a system
based on a cascade supervised machine learning
methods for the CoNLL-2010 shared task. The ar-
chitecture of our system is shown in Figure 1.
The system composes of two subsystems for
two subtasks respectively, and the first subsystem
is a two-layer cascaded classifier.
2.1 Hedge Detection
The hedges are represented by indicating whether
a token is in a hedge and its position in the
CoNLL-2010 shared task. Three tags are used for
13
Figure 1: System architecture
this scheme, where O cue indicates a token out-
side of a hedge, B cue indicates a token at the
beginning of a hedge and I cue indicates a to-
ken inside of a hedge. In this subsystem, we do
preprocessing by GENIA Tagger (version 3.0.1)1
at first, which does lemma extraction, part-of-
speech (POS), chunking and named entity recog-
nition (NER) for feature extraction. For the out-
put of GENIA Tagger, we convert the first char
of a lemma into lower case and BIO chunk tag
into BIOS chunk tag, where S indicates a token
is a chunk, B indicates a token at the beginning
of a chunk, I indicates a token inside of a chunk,
and O indicates a token outside of a chunk. Then
a two-layer cascaded classifier is built for pre-
diction. There are a CRF classifier and a large
margin-based classifier in the first layer and a CRF
classifier in the second layer.
In the first layer, the following features are used
in our system:
? Word andWord Shape of the lemma: we used
the similar scheme as shown in (Tsai et al,
2005).
1http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/
? Prefix and Suffix with length 3-5.
? Context of the lemma, POS and the chunk in
the window [-2,2].
? Combined features including L0C0, LiP0
and LiC0, where ?1 ? i ? 1 L denotes the
lemma of a word, P denotes a POS and C
denotes a chunk tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? Whether a token is a part of the pairs ?neither
... nor? and ?either ... or? as both tokens of a
pair are always labeled with the same tag.
? Whether a token can possibly be classified
into B cue, I cue or O cue; its lemma, POS
and chunk tag for each possible case: these
features are extracted according to a dictio-
nary extracted from training corpus, which
lists all possible hedge tag for each word in
the training corpus.
In the second layer, we used some features
about the result of the last layer besides those men-
tioned above. They are listed as follow:
? The lemma and POS sequences of the hedge
predicted by each classifier.
? The times of a token classified into B cue,
I cue and O cue by the first two classifiers.
? Whether a token is the last token of the hedge
predicted by each classifier.
2.2 Hedge Scope Detection
We follow the way of Morante and Daelemans
(2009) to represent the scope of a hedge, where
F scope indicates a token at the beginning of a
scope sequence, L scope indicates a token at the
last of a scope sequence, and NONE indicates
others. In this phase, we do preprocessing by
GDep Tagger (version beta1)2 at first, which does
lemma extraction, part-of-speech (POS), chunk-
ing, named entity recognition (NER) and depen-
dency parse for feature extraction. For the out-
put of GDep Tagger, we deal with the lemma and
chunk tag using the same way mentioned in the
last section. Then, a CRF classifier is built for pre-
diction, which uses the following features:
2http://www.cs.cmu.edu/ sagae/parser/gdep
14
? Word.
? Context of the lemma, POS, the chunk, the
hedge and the dependency relation in the
window [-2,2].
? Combined features including L0C0,
L0H0, L0D0, LiP0, PiC0,PiH0, CiH0,
PiD0,CiD0, where ?1 ? i ? 1 L denotes
the lemma of a word, P denotes a POS, C
denotes a chunk tag, H denotes a hedge tag
and D denotes a dependency relation tag.
? The type of a chunk; the lemma and POS se-
quences of it.
? The type of a hedge; the lemma, POS and
chunk sequences of it.
? The lemma, POS, chunk, hedge and depen-
dency relation sequences of 1st and 2nd de-
pendency relation edges; the lemma, POS,
chunk, hedge and dependency relation se-
quences of the path from a token to the root.
? Whether there are hedges in the 1st, 2nd de-
pendency relation edges or path from a token
to the root.
? The location of a token relative to the nega-
tion signal: previous the first hedge, in the
first hedge, between two hedge cues, in the
last hedge, post the last hedge.
At last, we provided a postprocessing system
for the output of the classifier to build the com-
plete sequence of tokens that constitute the scope.
We applied the following postprocessing:
? If a hedge is bracketed by a F scope and a
L scope, its scope is formed by the tokens be-
tween them.
? If a hedge is only bracketed by a F scope, and
there is no L scope in the sentence, we search
the first possible word from the end of the
sentence according to a dictionary, which ex-
tracted from the training corpus, and assign it
as L scope. The scope of the hedge is formed
by the tokens between them.
? If a hedge is only bracketed by a F scope, and
there are at least one L scope in the sentence,
we think the last L scope is the L scope of the
hedge, and its scope is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there is no F scope in the sentence, we
search the first possible word from the begin-
ning of the sentence to the hedge according to
the dictionary, and assign it as F scope. The
scope of the hedge is formed by the tokens
between them.
? If a hedge is only bracketed by a L scope,
and there are at least one F scope in the sen-
tence, we search the first possible word from
the hedge to the beginning of the sentence ac-
cording to the dictionary, and think it as the
F scope of the hedge. The scope of the hedge
is formed by the tokens between them.
? If a hedge is bracketed by neither of them, we
remove it.
3 Experiments and Results
Two annotated corpus: BioScope and Wikipedia
are supplied for the CoNLL-2010 shared task. The
BioScope corpus consists of two parts: biological
paper abstracts and biological full papers, and it
is used for two subtasks. The Wikipedia corpus is
only used for hedge detection. The detailed infor-
mation of these two corpora is shown in Table 1
and Table 2, respectively.
Abstracts Papers Test
#Documents 1273 9 15
#Sentences 11871 2670 5003
%Hedge sent. 17.70 19.44 15.75
#Hedges 2694 682 1043
#AvL. of sent. 30.43 27.95 31.30
#AvL. of scopes 17.27 14.17 17.51
Table 1: The detailed information of BioScope
corpus. ?AvL.? stands for average length.
Train Test
#Documents 2186 2737
#Sentences 11111 9634
%Hedge sentences 22.36 23.19
#Hedges 3133 3143
#AvL. of sentences 23.07 20.82
Table 2: The detail information of Wikipedia cor-
pus. ?AvL.? stands for average length.
In our experiments, CRF++-0.533 implemen-
3http://crfpp.sourceforge.net/
15
tation is employed to CRF, and svm hmm 3.104
implementation is employed to the large margin
method. All parameters are default except C
(the trade-off between training error and margin,
C=8000, for selecting C, the training corpus is par-
titioned into three parts, two of them are used for
training and the left one is used as a development
dataset) in svm hmm. Both of them are state-of-
the-art toolkits for the sequence labeling problem.
3.1 Hedge Detection
We first compare the performance of each single
classifier with the cascaded system on two corpora
in domain, respectively. Each model is trained by
whole corpus, and the performance of them was
evaluated by the official tool of the CoNLL-2010
shared task. There were two kinds of measure:
one for sentence-level performance and another
one for cue-match performance. Here, we only
focused on the first one, and the results shown in
Table 3.
Corpus System Prec. Recall F1
CRF 87.12 86.46 86.79
BioScope LM 85.24 87.72 86.46
CAS 85.03 87.72 86.36
CRF 86.10 35.77 50.54
Wikipedia LM 82.28 41.36 55.05
CAS 82.28 41.36 55.05
Table 3: In-sentence performance of the hedge
detection subsystem for in-domain test. ?Prec.?
stands for precision, ?LM? stands for large mar-
gin, and ?CAS? stands for cascaded system.
From Table 3, we can see that the cascaded sys-
tem is not better than other two single classifiers
and the single CRF classifier achieves the best per-
formance with F-measure 86.79%. The reason for
selecting this cascaded system for our final sub-
mission is that the cascaded system achieved the
best performance on the two training corpus when
we partition each one into three parts: two of them
are used for training and the left one is used for
testing.
For cross-domain test, we train a cascaded clas-
sifier using BioScope+Wikipedia cropus. Table 4
shows the results.
As shown in Table 5, the performance of cross-
domain test is worse than that of in-domain test.
4http://www.cs.cornell.edu/People/tj/svm light/svm-
hmm.html
Corpus Precision Recall F1
BioScope 89.91 73.29 80.75
Wikipedia 81.56 40.20 53.85
Table 4: Results of the hedge detection for cross-
domain test. ?LM? stands for large margin, and
?CAS? stands for cascaded system.
3.2 Hedge Scope Detection
For test the affect of postprocessing for hedge
scope detection, we test our system using two eval-
uation tools: one for scope tag and the other one
for sentence-level scope (the official tool). In or-
der to evaluate our system comprehensively, four
results are used for comparison. The ?gold? is the
performance using golden hedge tags for test, the
?CRF? is the performance using the hedge tags
prediction of single CRF for test, the ?LM? is the
performance using the hedge tag prediction of sin-
gle large margin for test, and ?CAS? is the per-
formance of using the hedge tag prediction of cas-
caded subsystem for test. The results of scope tag
and scope sentence-level are listed in Table 5 and
Table 6, respectively. Here, we should notice that
the result listed here is different with that submit-
ted to the CoNLL-2010 shared task because some
errors for feature extraction in the previous system
are revised here.
HD tag Precision Recall F1
F scope 92.06 78.83 84.94
gold L scope 80.56 68.67 74.14
NONE 99.68 99.86 99.77
F scope 78.83 66.89 72.37
CRF L scope 72.52 60.50 65.97
NONE 99.56 99.75 99.65
F scope 77.25 67.57 72.09
LM L scope 72.33 61.41 66.42
NONE 99.56 99.73 99.31
F scope 77.32 67.86 72.29
CAS L scope 72.00 61.29 66.22
NONE 99.57 99.73 99.65
Table 5: Results of the hedge scope tag. ?HD?
stands for hedge detection subsystem we used,
?LM? stands for large margin, and ?CAS? stands
for cascaded system.
As shown in Table 5, the performance of
L scope is much lower than that of F scope.
Therefore, the first problem we should solve is
16
HD subsystem Precision Recall F1
gold 57.92 55.95 56.92
CRF 52.36 48.40 50.30
LM 51.06 48.89 49.95
CAS 50.96 48.98 49.95
Table 6: Results of the hedge scope in-sentence.
?HD? stands for hedge detection subsystem we
used, ?LM? stands for large margin, and ?CAS?
stands for cascaded system.
how to improve the prediction performance of
L scope. Moreover, compared the performance
shown in Table 5 and 6, about 15% (F1 of L scope
in Table 5 - F1 in Table 6) scope labels are mis-
matched. An efficient postprocessing is needed to
do F-L scope pair match.
As ?CRF? hedge detection subsystem is bet-
ter than the other two subsystems, our system
achieves the best performance with F-measure
50.30% when using the ?CRF? subsystem.
4 Conclusions
This paper presents a cascaded system for the
CoNLL-2010 shared task, which contains two
subsystems: one for detecting hedges and an-
other one for detecting their scope. Although
the best performance of hedge detection subsys-
tem achieves F-measure 86.79%, the best per-
formance of the whole system only achieves F-
measure 50.30%. How to improve it, we think
some complex features such as context free gram-
mar may be effective for detecting hedge scope.
In addition, the postprocessing can be further im-
proved.
Acknowledgments
We wish to thank the organizers of the CoNLL-
2010 shared task for preparing the datasets
and organizing the challenge shared tasks.
We also wish to thank all authors supply-
ing the toolkits used in this paper. This
research has been partially supported by the
National Natural Science Foundation of China
(No.60435020 and No.90612005), National 863
Program of China (No.2007AA01Z194) and the
Goal-oriented Lessons from the National 863 Pro-
gram of China (No.2006AA01Z197).
References
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, Gyo?rgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their
Scope in Natural Language Text. In Proceedings of
the Fourteenth Conference on Computational Nat-
ural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 173?176, Suntec, Singapore,
August. Association for Computational Linguistics.
Roser Morante andWalter Daelemans. 2009. Learning
the scope of hedge cues in biomedical texts. In Pro-
ceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Com-
putational Linguistics.
Tzong-Han Tsai, Chia-Wei Wu, and Wen-Lian Hsu.
2005. Using Maximum Entropy to Extract Biomed-
ical Named Entities without Dictionaries. In Sec-
ond International Joint Conference on Natural Lan-
guage Processing, pages 268?273.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9.
17
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 78?83,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Exploiting Rich Features for Detecting Hedges and Their Scope 
Xinxin Li, Jianping Shen, Xiang Gao, Xuan Wang 
Harbin Institute of Technology Shenzhen Graduate School 
Shenzhen, Guangdong, China 
{lixxin2, jpshen2008}@gmail.com,  
sky0306201@163.com, wangxuan@insun.hit.edu.cn 
 
Abstract 
This paper describes our system about 
detecting hedges and their scope in natural 
language texts for our participation in CoNLL-
2010 shared tasks. We formalize these two 
tasks as sequence labeling problems, and 
implement them using conditional random 
fields (CRFs) model. In the first task, we use a 
greedy forward procedure to select features for 
the classifier. These features include part-of-
speech tag, word form, lemma, chunk tag of 
tokens in the sentence. In the second task, our 
system exploits rich syntactic features about 
dependency structures and phrase structures, 
which achieves a better performance than only 
using the flat sequence features. Our system 
achieves the third score in biological data set 
for the first task, and achieves 0.5265 F1 score 
for the second task. 
1 Introduction 
In recent years, a fair amount of approaches have 
been developed on detecting speculative and 
negative information from biomedical and 
natural language texts, for its benefit to the 
applications like information extraction. These 
approaches evolve from hand-crafted rule-based 
approaches, which use regular expressions to 
match the sentences or its grammatical parsing, 
such as NegEx (Chapman et al, 2001), 
Negfinder (Mutalik et al, 2001), and 
NegExpander (Aronow et al, 1999), to machine 
learning approaches, including semi-supervised 
methods (Medlock and Briscoe, 2007; Szarvas, 
2008), and supervised methods (Morante and 
Daelemans, 2009).  
In this paper, we describe the machine 
learning system submitted to CoNLL-2010 
Shared task (Farkas et al, 2010). Our system 
formalizes these two tasks as consecutive 
sequence labeling problems, and learns the 
classifiers using conditional random fields 
approach. In the first task, a model is trained to 
identify the hedge cues in sentences, and in the 
second task, another model is used to find the 
corresponding scope for each hedge cue 
generated in the first task. Our system follows 
the study of Morante and Daelemans (2009), but 
applies more refined feature selection. In the first 
task, we use a greedy forward procedure to select 
features for the classifier. In the second task, we 
exploit rich syntactic information to improve the 
performance of the model, from dependency 
structures and phrase structures. A rule-based 
post processing procedure is used to eliminate 
the errors brought by the classifier for each task. 
The remainder of the paper is organized as 
follows. In section 2, we briefly describe the task 
and the details of our system, including how to 
select features for the hedge cue detection 
system, and how to find the corresponding scope 
for each hedge cue. The experimental results are 
discussed in section 3. In section 4 we put 
forward some conclusion. 
2 System Description  
We model these two tasks for identifying the 
hedge cues and finding their scope as two 
consecutive sequence labeling problems, such as 
chunking, segmentation and named entity 
recognition, and train the classifiers using 
conditional random fields approach (Lafferty et 
al., 2001). For each task, a post-processing 
procedure is used to refine the results from the 
classifier. 
In the first task, we detect the hedge cue by 
classifying the tokens of a sentence as being at 
the beginning of, inside or outside of the hedge 
signal. In the second task, we find the scope of a 
hedge cue by classifying the tokens of a sentence 
as being the first one of, the last one or neither of 
the scope.  
A sentence from biological full articles data 
set omitting the id number is shown below in 
Figure 1. In this sentence, there is only one 
hedge cue, the phrase ?raises an interesting 
question?, and its corresponding scope is the 
sequence from token ?raises? to token ?acid?. 
78
<sentence>This <xcope><cue>raises an 
interesting question</cue>: "Is there a 23rd 
amino acid</xcope>?".</sentence> 
 
Figure 1: A sentence with hedge cue and scope 
annotation in biological full articles data set 
2.1 Hedge detection 
Since hedge cues usually consist of one or more 
tokens, we predict the tokens in BIO 
representation, whether the token is the first 
token of a hedge cue (B-cue), inside a hedge cue 
(I-cue), or outside of the hedge cue (O-cue). For 
the sentence in Figure 1, token ?raises? is 
denoted as B-cue, tokens ?an interesting 
question? all as I-cue, and the other tokens in the 
sentence as O-cue. 
The classifier is trained using conditional 
random fields (Lafferty et al, 2001), which 
combines the benefits of conditional models with 
the global normalization of random field models, 
and avoid the label bias problem that exists in 
maximum entropy Markov models (MEMMs). 
The CRF model we use is implemented as 
CRF++ 0.51 1 . The parameters of the CRF 
classifier are set as defaults. 
We use a greedy forward procedure to select a 
better feature sets for the classifier according to 
the evaluation results in the development set. We 
first start from a basic feature set, and then add 
each feature outside the basic set and remove 
each feature inside the basic set one by one to 
check the effectiveness of each feature by the 
performance change in the development set. This 
procedure is repeated until no feature is added or 
removed or the performance is not improved. 
The selected features are listed below: 
? Cn (n=-2,-1, 0, 1, 2) 
? CnCn+1 (n=-1,0) 
? Cn-1CnCn+1 (n=-1,0,1) 
? Cn-2Cn-1CnCn+1  (n=0,1) 
Where C denote features of each token, 
including FORM, LEMMA, and POS (in Table 
1), C0 represents the feature of current token and 
Cn(C-n) represents the feature of the token n 
positions to the right (left) of current token. 
CnCn+1 denote the combination of Cn and Cn+1. So 
are Cn-1CnCn+1 and Cn-2Cn-1CnCn+1. 
 
 
                                                 
1
 http://crfpp.sourceforge.net/ 
Feature 
Name 
Description 
FORM Word form or punctuation symbol. 
LEMMA Lemma or stem of word form. 
POS Part-of-speech tag of the token. 
CHUNK Chunk tag of the token, e.g. B_NP, 
B_ SBAR, and I_NP. 
TCHUNK Chunk type of the token, e.g. NP. 
 
Table 1: Description of features of each token 
 
Although our system is based on token, chunk 
features are also important. Analyzing the 
training data set, it is shown that if one token in a 
chunk is in the hedge cue, the other tokens in the 
chunk are usually in the same hedge cue. The 
chunk feature can provide more information for 
the multiword hedge cues. The LEMMA, POS, 
and CHUNK of each token used in our system 
are determined using GENIA tagger (Tsuruoka et 
al., 2005).  
The selected CHUNK features in our system 
are listed as follows: 
? Cn (n=-3, -2,-1, 0, 1, 2, 3 ) 
? CnCn+1 (n=-3, -2,-1, 0, 1, 2, 3 ) 
? Cn-1CnCn+1  (n=-2,-1,0,1,-2) 
? Cn-2Cn-1CnCn+1 (n=-1,0,1,2) 
We can obtain the preliminary results using 
the CRF model-based classifier, but there are 
some missed or incorrectly classified hedge cues 
which can be recognized by rule-based patterns. 
Through statistical analysis on the training and 
development data sets, we obtain some effective 
rules for post processing, including: 
 
? If the first token of a NP chunk tag is 
annotated as I-cue, the whole NP chunk is 
in the hedge cues. 
? If the B-VP chunk tag of a token is 
followed by a B-SBAR chunk tag, the 
token is annotated as B-cue. 
? If token ?that? follows token ?indicate? 
and the POS of token ?that? is IN, the 
chunk tag of token ?that? is B-SBAR, then 
the ?indicate? will be annotated with B-
cue and ?that? will be annotated with I-
cue. 
? If token ?indicate? is followed by token 
?an? or token ?a?, then the token 
?indicate? is annotated as B-cue. 
79
2.2 Scope finding 
In this task, we train a classifier to predict 
whether each token in the sentence is in the 
scope by classifying them as the first one (F-
scope), the last one (L-scope), or neither 
(NONE) of the scope, which is the same as 
Morante and Daelemans (2009). For the sentence 
in Figure 1, token ?raises? is denoted as F-scope, 
token ?acid? as L-scope, and the other tokens in 
the sentence as NONE.  
After the classification, a post processing 
procedure is used to match the scope to each 
hedge, guaranteeing that each hedge has only one 
corresponding scope sequence, and must be 
inside its scope sequence. There is no cross 
between different scope sequences, but inclusion 
is allowed. The hedges are selected from the first 
task. 
The classifier is also implemented using 
conditional random fields model, and the 
parameters of the CRF classifier are set as 
defaults. We first build a set of baseline sequence 
features for the classifier, some borrowed from 
Morante and Daelemans (2009). The selected 
baseline sequence features are: 
? Of the token in focus: FORM, POS, 
LEMMA, CHUNK, TCHUNK, 
combination of FORM and POS; POS, 
LEMMA, CHUNK, TCHUNK of two 
tokens to the left and three tokens to the 
right; first word, last word, chain of 
FORM, POS of two chunks to the left and 
two chunks to the right; All combination 
of POS in the window of length less than 3; 
All combination of CHUNK in the 
window of length 2. 
? Of the left closest hedge: chain of the 
FORM, POS, LEMMA, CHUNK, and 
TCHUNK; All combination of POS and 
FORM in the window of length 2. 
? Of the right closest hedge: chain of the 
FORM, POS, LEMMA, CHUNK, and 
TCHUNK; All combination of POS and 
FORM in the window of length 2. 
? Of the tokens between the left closest 
hedge and the token in focus: chain of 
FORM, POS, LEMMA, CHUNK and 
TCHUNK; the number. 
? Of the tokens between the right closest 
hedge and the token in focus: chain of 
FORM, POS, LEMMA, CHUNK and 
TCHUNK; the number. 
? Others: the number of hedge cues in the 
sentence; the sequence relation between 
the token in focus and hedge cues (LEFT, 
RIGHT, MIDDLE, IN, NULL) 
Besides the sequence features listed above, 
syntactic features between the token in focus and 
hedge cues are explored in our classifier. Huang 
and Low (2007) notes that structure information 
stored in parse trees helps identifying the scope 
of negative hedge cues, and Szarvas (2008) 
points out that the scope of a keyword can be 
determined on the basic of syntax. Thus we 
believe that a highly accurate extraction of 
syntactic structure would be beneficial for this 
task.  
For sentences in the dataset, their dependency 
structures are extracted using GENIA 
Dependency parser (Sagae and Tsujii, 2007), and 
phrase structure using Brown self-trained 
biomedical parser (McClosky, 2009). Figure 2 
shows the corresponding dependency tree and 
Figure 3 shows the corresponding phrase 
structure tree for the sentence in Figure 1. In the 
following part in the section, we will illustrate 
these syntactic features and give examples for 
their value. We take the token ?acid? as the token 
in focus, to determine whether it is classified as 
F-scope, L-scope or NONE. 
 
 
 
Figure 2: Dependency tree of the sentence in 
Figure 1 
 
For the token ?acid? in the dependency trees 
in Figure 2, its father node is the token ?there?, 
and the dependency relation between these two 
token is ?NMOD?. 
Dependency features between the token in 
focus and the left closest hedge cue are: 
? Dependency relation of the token in 
focus to its father, left closest hedge to its 
80
father and the dependency relation pair: 
NOMD, ROOT, ROOT+NMOD. 
? Chain of POS: ->VBZ<-VBZ<-EX<-NN 
? Chain of POS without consecutive 
redundant POS: ->VBZ <-EX<-NN 
? POS of their nearest co-father: VBZ 
? Whether it is a linear relation (self, up , 
down, no): up 
? Kinship (grandfather, grandson, father, 
son, brother, self, no): no. 
? The number of tokens in the chain: 4 
Similar features are extracted for dependency 
relation between the token in focus and its right 
closest hedge cue. There is no right hedge cue for 
token ?acid?. Thus these features are set as 
?NULL?. 
 
This raises an interesting question :  " Is there a 23rd amino acid ? " .
DT VBZ DT JJ NN : NN VBZ RB DT NN NN NN . RB .
NP NP NP ADVP NP
VP
S
NP
ADVP
NP
VP
S
S
 
 
Figure 3: Phrase structure tree of the sentence in 
Figure 1 
 
Phrase structure features between the token in 
focus and its left closest hedge cue are: 
? Chain of syntactic categories: VBZ-
>VP<- NP <-NP <-S<-VP <-NP<-NN 
? syntactic categories without consecutive 
redundant ones: VBZ->VP<-NP<-S<-
VP<- NP<-NN 
? Syntactic category of their nearest co-
father: VP 
? The number of syntactic categories in the 
chain: 8 
The phrase structure features between the 
token in focus and the nearest right hedge cue are 
similar, setting as ?NULL?. 
Scope finding requires each hedge cue has 
only one corresponding scope. A hedge-scope 
pair is true positive only if the hedge cue and its 
corresponding scope are correctly identified. We 
perform the post processing procedure in 
sequence: 
? For each hedge cue from the beginning 
to the end of the sentence, find its left 
closest F-scope which has not been 
identified by other hedge cues, and 
identify it as its F-scope. 
? For each hedge cue from the end to the 
beginning of the sentence, find its right 
closest L-scope which has not been 
identified by other hedge cues, and 
identify it as its L-scope. 
? For each hedge:  
 If both its F-scope and L-scope is 
identified, then done;  
 If only its F-scope is identified, then 
its L-scope is set as L-scope of the 
last hedge cue in the sentence if it 
exists or according to the dictionary 
which we build with training data 
set; 
 If only its L-scope is identified, then 
its F-scope is set as its first token; 
 If none of its F-scope and L-scope is 
identified, then discard the hedge 
cue. 
3 Overall Results 
In this section we will present our experimental 
results for these two tasks. In the first task, the 
chief evaluation is carried on sentence level: 
whether a sentence contains hedge/weasel cue or 
not. Our system compares the performance of 
different machine learning algorithm, CRF and 
SVM-HMM on hedge cue detection. A post 
processing procedure is used to increase the 
recall measure for our system. 
In the second task, three experiments are 
performed. The first experiment is used to 
validate the benefit of dependency features and 
phrase structure features for scope finding. The 
second experiment is designed to evaluate the 
effect of abstract dataset on full article dataset. 
These two experiments are all performed using 
gold hedge cues. The performance of our scope 
finding system with predicted hedge cues is 
presented in the third experiment. 
81
3.1 Hedge detection 
The first experiment is used to compare two 
machine learning algorithms, SVM-HMM and 
CRF. We train the classifiers on abstract and full 
articles data sets. The results of the classifier on 
evaluation data set are shown in Table 2. 
 
Model Precision Recall F1 
SVM-HMM 88.71 81.52 84.96 
CRF 90.4 81.01 85.45 
 
Table 2: Results of hedge cues detection using 
CRF and SVM-HMM 
 
From Table 1, it is shown that CRF model 
outperforms SVM-HMM model in both 
precision and recall measure. The results are 
obtained without post processing. The 
experimental result with post processing is 
shown in Table 3. 
 
Feature Precision Recall F1 
Without Post 
processing 
90.4 81.01 85.45 
Post  
processing 
90.1 82.05 85.89 
 
Table 3: Result of biological evaluation data set 
without/with post processing 
 
By post processing, some mislabeled or 
incorrectly classified hedge cues can be 
recognized, especially the recall of the I-cue 
improved largely, from 55.26% to 68.51%. 
Though the precision is a little lower, the F1 
measure increases 0.44%. 
3.2 Scope finding 
To measure the benefit of syntactic features on 
scope finding task, we perform the experiment 
with different features on abstract data set, of 
which we split two-thirds as training data, and 
the other one third as testing data. The results are 
presented in Table 4. 
We take the classifier with sequence features 
as baseline classifier. From Table 4, it is shown 
that adding dependency features achieves a 
slightly better performance than the baseline 
classifier, and adding phrase structure features 
improve much better, about 1.2% F1-score. The 
classifier with all syntactic features achieves the 
best F1-score, 2.19% higher than baseline 
classifier. However, in later experiment on 
evaluation dataset after the shared task, we 
observed that dependency features actually 
harmed the performance for full articles dataset. 
 
Feature set Precision Recall F1 
Sequence  
(Baseline) 
82.20 81.61 81.90 
Sequence + 
Dependency 
82.28 82.09 82.19 
Sequence  
+ Phrase structure 
83.14 83.04 83.09 
All 84.19 83.99 84.09 
 
Table 4: Results of scope finding system with 
different feature sets on abstract data set 
 
Three experiments are designed to evaluate 
the benefit of abstract dataset for full articles 
dataset. The first one is performed on full articles 
data set, of which we split two-thirds for training, 
and the other one third for testing. The second 
experiment is trained on abstract data set, and 
evaluated on full articles data set. In the third 
experiment, we take abstract data set and one 
third of full articles as training data, and evaluate 
on the remaining full articles data set. The results 
are shown below in Table 5. 
 
Training 
data 
Testing 
data 
Prec. Recall F1 
Part Art. Part Art. 53.14 51.80 52.46 
Abs. Full Art. 54.32 54.64 54.48 
Mix Part Art. 59.59 59.74 59.66 
 
Table 5: Results of scope finding system with 
gold-standard hedge cues 
 
Results in Table 5 reveal that more abstract 
and full article dataset are added to the classifier 
as training data, better performance the system 
achieve. Thus we use the combination of abstract 
and full articles as training data for the final 
evaluation.  
Table 6 presents the results of our scope 
finding system with or without dependency 
features, using both gold-standard hedge cues 
and predicated hedge cues generated by our 
hedge cue finding system. 
Comparing the results in Table 4, 5, and 6, we 
observe that the performance of scope finding 
classifier on full article dataset is much lower 
than on abstract dataset, and dependency features 
are beneficial for the abstract dataset, but useless 
for full article dataset. We ascribe this 
phenomenon to the lack of enough full articles 
training data and the different properties of 
82
abstract and full articles data sets. Deep research 
is expected to continue. 
 
Hedge 
cues 
Dep. 
features 
Prec. Recall F1 
with 57.42 47.92 52.
24 
Predicted 
without 58.13 48.11 52.
65 
with 59.43 58.28 58.
85 
Gold 
standard 
without 60.20 58.86 59.
52 
 
Table 6: Results of scope finding system 
with/without dependency features using both 
gold-standard and predicated hedge cues 
4 Conclusion 
In this paper, we describe a machine learning 
system for detecting hedges and their scope in 
natural language texts. These two tasks are 
formalized as sequence labeling problems, and 
implemented using conditional random fields 
approach. We use a greedy forward procedure to 
select features for the classifier, and exploit rich 
syntactic features to achieve a better performance. 
In the in-domain evaluation, our system achieves 
the third score in biological data set for the first 
task, and achieves 0.5265 F1 score for the second 
task. 
 
Acknowledgments 
The authors would like to thank Buzhou Tang for 
useful discussions of the paper. This work is 
supported by the National High-tech R&D 
Program of China (863 Program, No. 
2007AA01Z194). 
References  
David B. Aronow, Fangfang Feng, and W. Bruce 
Croft. 1999. Ad Hoc Classification of Radiology 
Reports. Journal of the American Medical 
Informatics Association, 6(5):393?411. 
Wendy W. Chapman, Will Bridewell, Paul Hanbury, 
Gregory F. Cooper, and Bruce G. Buchanan. 2001. 
A Simple Algorithm for Identifying Negated 
Findings and Diseases in Discharge Summaries. 
Journal of Biomedical Informatics, 34:301?310. 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of the Fourteenth Conference 
on Computational Natural Language Learning 
(CoNLL-2010): Shared Task, pages 1?12.  
Yang Huang, and Henry J. Lowe. 2007. A novel 
hybrid approach to automated negation detection in 
clinical radiology reports. Journal of the 
American Medical Informatics Association, 
14(3):304?311. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data. In Proceedings of the Eighteenth 
International Conference on Machine 
Learning, pages 282?289. 
David McClosky. 2009. Any Domain Parsing: 
Automatic Domain Adaptation for Natural 
Language Parsing. Ph.D. thesis, Department of 
Computer Science, Brown University. 
Ben Medlock, and Ted Briscoe. 2007. Weakly 
supervised learning for hedge classification in 
scientific literature. In Proc. of ACL 2007, pages 
992?999. 
Roser Morante, and Walter Daelemans. 2009. 
Learning the scope of hedge cues in biomedical 
texts. In Proceedings of the Workshop on 
BioNLP, pages 28?36.  
Pradeep G. Mutalik, Aniruddha Deshpande, and 
Prakash M. Nadkarni. 2001. Use of general-
purpose negation detection to augment concept 
indexing of medical documents: a quantitative 
study using the UMLS. Journal of the American 
Medical Informatics Association, 8(6):598?609. 
Kenji Sagae, and Jun?ichi Tsujii. 2007. Dependency 
parsing and domain adaptation with LR models 
and parser ensembles. In Proceedings of the 
CoNLL-2007 Shared Task, pages 82?94 
Gy?rgy Szarvas. 2008. Hedge classification in 
biomedical texts with a weakly supervised 
selection of keywords. In Proc. of ACL 2008, 
pages 281?289, Columbus, Ohio, USA. ACL. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas, 
and J?nos Csirik. 2008. The BioScope corpus: 
annotation for negation, uncertainty and their scope 
in biomedical texts. In Proc. of BioNLP 2008, 
pages 38?45, Columbus, Ohio. ACL. 
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim, 
Tomoko Ohta, John McNaught, Sophia Ananiadou, 
and Jun'ichi Tsujii. 2005. Developing a Robust 
Part-of-Speech Tagger for Biomedical Text. 
Advances in Informatics - 10th Panhellenic 
Conference on Informatics, LNCS 3746, pages 
382?392. 
83
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 107?111,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Coreference Resolution with Loose Transitivity Constraints
Xinxin Li, Xuan Wang, Shuhan Qi
Shenzhen Graduate School
Harbin Institute of Technology, ShenZhen, China
lixxin2@gmail.com, wangxuan@insun.hit.edu.cn
shuhan qi@qq.com
Abstract
Our system treats coreference resolution as
an integer linear programming (ILP) problem.
Extending Denis and Baldridge (2007) and
Finkel andManning (2008)?s work, we exploit
loose transitivity constraints on coreference
pairs. Instead of enforcing transitivity closure
constraints, which brings O(n3) complexity,
we employ a strategy to reduce the number
of constraints without large performance de-
crease, i.e., eliminating coreference pairs with
probability below a threshold . Experimental
results show that it achieves a better perfor-
mance than pairwise classifiers.
1 Introduction
This paper describes our coreference resolution sys-
tem participating in the close track of CoNLL 2011
shared task (Pradhan et al, 2011). The task aims to
identify all mentions of entities and events and clus-
ter them into equivalence classes in OntoNotes Cor-
pus (Pradhan et al, 2007a). During the last decade,
several machine learning methods for coreference
resolution have been developed, from local pair-
wise classifiers (Soon et al, 2001) to global learn-
ing methods (Luo et al, 2004; Ng, 2005; Denis
and Baldridge, 2007), from simple morphological,
grammatical features to more liguistically rich fea-
tures on syntactic structures and semantic relations
(Pradhan et al, 2007b; Haghighi and Klein, 2009).
Our system supports both local classifiers and
global learning. Maximum entropy model is used
for anaphoricity and coreference, because it assigns
probability mass to mentions and coreference pairs
directly. In global phase, instead of determining
each coreference pair independently in a greedy
fashion, we employ an integer linear programming
(ILP) formulation for this problem. Extending (De-
nis and Baldridge, 2007) and (Finkel and Manning,
2008)?s work, we introduce a loose selection strat-
egy for transitivity constraints, attempting to over-
come huge computation complexity brought by tran-
sitivity closure constraints. Details are described in
section 2.3.
2 System Description
2.1 Mention Detection
Mention detection is a method that identifies the
anaphoricity and non-anaphoricity mentions before
coreference resolution. The non-anaphoric men-
tions usually influence the performance of corefer-
ence resolution as noises. Coreference resolution
can benefit from accurate mention detection since
it might eliminate the non-anaphoric mentions. We
take mention detection as the first step, and then
combine coreference classifier into one system.
Total 70 candidate features are used for mention
detection, including lexical, syntactic, semantic fea-
tures (Ng and Cardie, 2002). Features are selected
according to the information gain ratio (Han and
Kamber, 2006)
GainRation(A) =
Gain(A)
SplitInfo(A)
The top 10 features with highest gain ratio are:
string match, head word match, all uppercase, pro-
noun, starting with article, number, following prepo-
sition, nesting in verb phrase, nesting in preposition,
107
and starting with definite article. Many string fea-
tures that cannot be calculated by gain ratio method
are also added.
2.2 Coreference Determination
For coreference determination, we first build sev-
eral baseline systems with different training in-
stance generation methods and clustering algo-
rithms. These strategies are shown below. Detailed
description can be found in Ng (2005).
 training instance generation methods: Mc-
Carthy and Lehnerts method, Soon et al?s
method, Ng and Cardie?s method.
 clustering algorithms: closest-first clustering,
best-first clustering, and aggressive merge clus-
tering.
Overall 65 features are considered in our system.
Features are extracted from various linguistic infor-
mation, including:
 distance: sentence distance, minimum edit dis-
tance (Strube et al, 2002)
 lexical: string match, partial match, head word
match (Daume? III and Marcu, 2005)
 grammar: gender agreement, number agree-
ment(Soon et al, 2001)
 syntactic: same head, path (Yang et al, 2006)
 semantic: semantic class agreement, predicate
(Ponzetto and Strube, 2006; Ng, 2007)
Combining different training instance generation
methods and clustering algorithms, we get total 9
baseline systems. For each system, we use a greedy
forward approach to select features. Starting from
a base feature set (Soon et al, 2001), each feature
out of the base set is added one by one according to
the performance change on development data. Fi-
nally, the procedure is ended until the performance
is not improved. The baseline system with best per-
formance is selected for further improvement.
2.3 ILP with Loose Transitivity Constraints
Previous systems usually take coreference resolu-
tion as binary classification problem, and build the
coreference chain by determining each coreference
pair indepedently. The binary classifier is easily
implemented, but may cause inconsistency between
coreference pairs. Several work have been devel-
oped to overcome the problem, e.g., Bell trees (Luo
et al, 2004), conditional random fields (McCallum
and Wellner, 2004) and reranker (Ng, 2005).
Denis and Baldridge (2007) proposed an ILP for-
mulation to find the optimal solution for the prob-
lem. It utilizes the output of other local classifiers
and performs global learning. The objective func-
tion for their conference-only model takes the form:
min
X
hi;ji2M
2
c
hi;ji
 x
hi;ji
+ c
hi;ji
 (1  x
hi;ji
)
where c
hi;ji
=   log(P
C
), c
hi;ji
=   log(1   P
C
).
M is the candidate mention set for each document.
P
C
refers to the probability of coreference link be-
tween two mentions produced by our maximum en-
tropy model, and x
hi;ji
is a binary variable that is set
to 1 if two mentions are coreferent, 0 otherwise.
However, as Finkel and Manning showed, D&B?s
coreference-only model without transitivity con-
straints is not really necessary, because they only se-
lect the coreference links with probability P
C
> 0:5.
Klenner (2007) and Finkel and Manning (2008)?s
work extended the ILP framework to support tran-
sitivity constraints. The transitivity constraints are
formulated as
8i; j; k 2 M(i < j < k)
x
hi;ji
 x
hj;ki
+ x
hi;ki
  1
x
hj;ki
 x
hi;ji
+ x
hi;ki
  1
x
hi;ki
 x
hi;ji
+ x
hj;ki
  1
These constraints ensure that when any two core-
frent links (e.g., x
hi;ji
, x
hi;ki
) among three men-
tions exist, the third one x
hj;ki
must also be a link.
However, these constraints also bring huge time and
space complexity with n3 constraints (n is number of
candidate mention set M, which is larger than 700
in some documents), and cannot be solved in a re-
stricted time and memory environment. We intro-
duce a loose method to eliminate conference links
108
Ratio Recall Precision F-value
0.4 84.03 43.75 57.54
0.6 70.6 70.85 70.72
0.8 64.24 74.35 68.93
1.0 58.63 76.13 66.25
Table 1: Results of mention dection
below a probability threshold . The constraints are
transformed as
x
hi;ki
+ x
hj;ki
 1 (1)
x
hi;ji
= 0 (2)
when P
C
(i; j) < . The threshold  is tuned on de-
velopment data for faster computation without large
performance decrease.
3 Experiments and Analysis
In the paper we mainly take noun phrases (NPs) and
pronouns as candidate mentions, and ignore other
phrases since more than 91% of the mentions are
NPs and pronouns.
3.1 Mention Detection
We observe that the ratio of positive examples and
negative examples is about 1:3 in training data. To
balance the bias, we propose a ratio control method
which sets a ratio to limit the number of negative
examples. Our system will select all positive exam-
ples, and part of negative examples according to the
ratio. By tuning the ratio, we can control the propor-
tion of positive and negative examples. With differ-
ent ratios for negative feature selection, the results
on development data are shown in table 1.
From table 1, we can see that as the ratio in-
creases, recall becomes smaller and precision be-
comes larger. Small threshold means less negative
examples are generated in training procedure, and
the classifier tends to determine a mention as posi-
tive. Finally, we choose the ratio 0.6 for our model
because it gets the best F-value on the development
data.
3.2 Coreference Resolution
Our system participates in the close track with
auto mention and gold boundary annotation. The
TIGM Soon Soon Soon Ng
CA A B C B
MUC 44.29 46.18 46.18 45.33
B
3 59.76 61.39 60.03 60.93
CEAF(M) 42.77 44.43 43.01 44.41
CEAF(E) 35.77 36.37 36.08 36.54
BLANC 60.22 63.94 59.9 63.96
Official 46.6 47.98 46.76 47.6
Table 2: Results of baseline systems
the performance is evaluated on MUC, B-CUBED,
CEAF(M), CEAF(E), BLANC metrics. The official
metric is calculated as (MUC+B
3
+CEAF )
=
3
.
Table 2 summarizes the performance of top 4 of
9 baseline systems with different training instance
generation methods and clustering algorithms on de-
velopment data. In the table, TIGM means training
instance generation method, and CA denotes clus-
tering algorithm, which includes C as closest-first,
B as best-first, and A as aggressive-merge clustering
algorithm. The results in Table 2 show that the sys-
tem with Soon?s training instance generation method
and best-first clustering algorithm achieves the best
performance. We take it as baseline for further im-
provement.
In ILP model, we perform experiments on docu-
ments with less than 150 candidate mentions to find
the suitable probability threshold  for loose tran-
sitivity constraints. There are totol 181 documents
meeting the condition in development data. We take
two strategies to loose transitivity constraints: (I)
formula 1 and 2, and (II) formula 2 only. Glpk pack-
age is used to solve our ILP optimization problems.1
Table 3 shows that as threshold  increases, the
running time reduces dramatically with a small per-
formance decrease from 49.06 to 48.88. Strategy I
has no benefit for the performance. Finally strategy
II and  = 0:06 are used in our system.
We also combine mentions identified in first phase
into coreference resolution. Two strategies are used:
feature model and cascaded model. For feature
model, we add two features which indicate whether
the two candidate mentions of a coreference pair are
mentions identified in first phase or not. For cas-
caded model, we take mentions identified in first
phase as inputs for coreference resolution. For ILP
1http://www.gnu.org/software/glpk/
109
 0 0.02 0.02 0.04 0.04 0.06 0.06 0.08 0.08 0.1 0.1
Strategy I II I II I II I II I II
MUC 40.95 40.64 40.92 40.64 40.83 40.64 40.8 40.64 40.75 40.64 40.68
B
3 65.6 65.47 65.59 65.47 65.58 65.47 65.57 65.47 65.5 65.47 65.49
CEAF(M) 48.62 48.39 48.59 48.39 48.56 48.39 48.54 48.39 48.42 48.39 48.39
CEAF(E) 40.62 40.47 40.62 40.47 40.63 40.47 40.61 40.47 40.5 40.47 40.47
BLANC 61.87 61.76 61.85 61.76 61.84 61.76 61.83 61.76 61.79 61.76 61.78
Official 49.06 48.88 49.04 48.88 49.01 48.88 48.99 48.88 48.92 48.88 48.88
Time(s) 1726 1047 913 571 451 361 264 253 166 153 109
Table 3: Results on different probability thresholds and strategies
Model Feature Cascade ILP
MUC 41.08 47.41 45.89
B
3 59.74 57.67 61.85
CEAF(M) 41.9 42.04 44.52
CEAF(E) 34.72 32.33 36.85
BLANC 61.1 62.99 63.92
Official 45.18 45.81 48.19
Table 4: Results of coreference resolution systems.
model, we perform experiments on coreference-only
system with our loose transitivity constraints. The
results on development data are shown in Table 4.
In Core Quad 2.40G CPU and 2G memory ma-
chine, our ILP model can optimize one document
per minute on average. From table 4, we can see that
the ILP model achieves the best F-value, implying
the benefit of our algorithm. It also shows that tra-
ditional coreference resolution methods combining
mention detection decrease the performance. For
restricted time deadline, other constraints strategies
(Klenner, 2007) and joint anaphoricity-coreference
ILP model are not used in our system. It would be
in our future work.
3.3 Test
Table 5 shows the performance of our system for
both development and test data, with auto mention
and gold boundary annotation.
The results in table 5 show that in auto mention
annotation, the performance on test data is a little
bit better than development data. The reason might
be that the system on test data uses more data to
train, including development data. A phenomenon
surprises us is that the performance on test data with
gold annotation is less than on development data,
Data Dev Dev Test Test
Mention Auto Gold Auto Gold
MUC 45.89 46.75 46.62 44.00
B
3 61.85 61.48 61.93 57.42
CEAF(M) 44.52 45.17 44.75 42.36
CEAF(E) 36.85 37.19 36.83 34.22
BLANC 63.92 63.83 64.27 62.96
Official 48.19 48.47 48.46 45.21
Table 5: Results for development and test data
even than auto annotation. It turns out that the mis-
take is made because we confuse the the definition
of gold bourdaries as gold mentions, which are ?all?
and ?only? mentions in coreference chains.
4 Conclusion
In this paper, we present a coreference resolution
system which employs an ILP formulation for global
optimization. To reduce computation complexity,
our system employs loose transitivity constraints to
the ILP model. Experimental results show that it
achieves a better performance than pairwise classi-
fiers.
References
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
110
ing integer programming. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
236?243, Rochester, New York, April. Association for
Computational Linguistics.
Jenny Rose Finkel and Christopher D. Manning. 2008.
Enforcing transitivity in coreference resolution. In
Proceedings of ACL-08: HLT, Short Papers, pages 45?
48, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161, Singapore, August. Association for Com-
putational Linguistics.
J. Han and M. Kamber. 2006. Data mining: concepts
and techniques. Morgan Kaufmann.
Manfred Klenner. 2007. Enforcing consistency on coref-
erence sets. In Recent Advances in Natural Language
Processing (RANLP), pages 323?328.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 135?142, Barcelona,
Spain, July.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In NIPS 2004.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of the 19th in-
ternational conference on Computational linguistics -
Volume 1, COLING ?02, pages 1?7, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
157?164, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Vincent Ng. 2007. Shallow semantics for coreference
resolution. In Proceedings of IJCAI, pages 1689?
1694.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 192?199, New York City,
USA, June. Association for Computational Linguis-
tics.
Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha
Palmer, Lance Ramshaw, and Ralph Weischedel.
2007a. Ontonotes: A unified relational semantic rep-
resentation. International Journal of Semantic Com-
puting (IJSC), 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007b. Un-
restricted coreference: Identifying entities and events
in ontonotes. In in Proceedings of the IEEE Inter-
national Conference on Semantic Computing (ICSC),
September 17-19.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27:521?544, December.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on
reference resolution. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 312?319,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 41?48, Sydney, Australia,
July. Association for Computational Linguistics.
111
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 83?87,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Simple Maximum Entropy Models for Multilingual Coreference Resolution
Xinxin Li, Xuan Wang, Xingwei Liao
Computer Application Research Center
Harbin Institute of Technology Shenzhen Graduate School
Shenzhen, China
lixxin2@gmail.com
Abstract
This paper describes our system participat-
ing in the CoNLL-2012 shared task: Mod-
eling Multilingual Unrestricted Coreference
in Ontonotes. Maximum entropy models are
used for our system as classifiers to deter-
mine the coreference relationship between ev-
ery two mentions (usually noun phrases and
pronouns) in each document. We exploit rich
lexical, syntactic and semantic features for the
system, and the final features are selected us-
ing a greedy forward and backward strategy
from an initial feature set. Our system partici-
pated in the closed track for both English and
Chinese languages.
1 Introduction
In this paper, we present our system for the CoNLL-
2012 shared task which aims to model coreference
resolution for multiple languages. The task of coref-
erence resolution is to group different mentions in a
document into coreference equivalent classes (Prad-
han et al, 2012). Plenty of machine learning al-
gorithms such as Decision tree (Ng and Cardie,
2002), maximum entropy model, logistic regres-
sion (Bjo?rkelund and Nugues, 2011), Support Vec-
tor Machines, have been used to solve this problem.
Meanwhile, the CoNLL-2011 shared task on En-
glish language show that a well-designed rule-based
approach can achieve a comparable performance as
a statistical one (Pradhan et al, 2011).
Our system treats coreference resolution problem
as classification problem by determining whether
every two mentions in a document has a corefer-
ence relationship or not. We use maximum entropy
(ME) models to train the classifiers. Previous work
reveal that features play an important role on coref-
erence resolution problem, and many different kinds
of features has been exploited. In this paper, we use
many different lexical, syntactic and semantic fea-
tures as candidate features, and use a greedy forward
and backward approach for feature selection for ME
models.
2 System Description
The framework of our system is shown in figure 1. It
includes four components: candidate mention selec-
tion, training example generation, model generation,
and decoding algorithm for test data. The details of
each component as described below.
2.1 Candidate Mention Selection
In both training and test sets, our system only con-
sider all noun phrases (NP) and pronouns (PRP,
PRP$) as candidate mentions for both English and
Chinese. The mentions in each sentence are ob-
tained from given syntactic tree by their syntactic
label. Other phrases in the syntactic tree are omit-
ted due to their small proportion. For example, in
the English training dataset, our candidate mentions
includes about 91% of golden mentions.
2.2 Training Example Generation
There are many different training example gen-
eration algorithms, e.g., McCarthy and Lehnert?s
method, Soon et als method, Ng and Cardies
method (Ng, 2005). For our baseline system, we
choose Soon et al?s method because it is easily un-
derstandable, implemented and popularly used. It
83
Figure 1: The framework of our coreference resolution
system
selects pairs of two coreferent mentions as positive
examples, and pairs between mentions among the
two mentions and the last mention as negative ex-
amples.
2.3 Feature Selection
Rich and meaningful features are important for
coreference resolution. Our system starts with
Soon?s 12 features as baseline features (Soon et al,
2001), and exploits many lexical, syntactic, and se-
mantic features as candidate features. Totally 71 fea-
tures are considered in our system, and summarized
below:
 Distance features: sentence distance, distance
in phrases, whether it?s a first mention (Strube
et al, 2002)
 Lexical features: string match, partial match,
apposition, proper name match, head word
match, partial head word match, minimum edit
distance (Daume? III and Marcu, 2005)
 Grammatical features: pronoun, demonstrative
noun phrase, embedded noun, gender agree-
ment, number agreement (Soon et al, 2001)
 Syntactic features: same head, maximal NP,
syntactic path (Yang et al, 2006)
 Semantic features: semantic class agreement,
governing verb and its grammatical role, predi-
cate (Ponzetto and Strube, 2006)
For English, the number agreement and gender
agreement features can be obtained through the gen-
der corpus provided. However, there is no corpus
for Chinese. Our system obtains this information
by collecting dictionaries for number and gender in-
formation from training dataset. For example, the
Algorithm 1 Greedy forward and backward feature
selection
Initialization: all candidate features in set C
Choose initial feature set 
Compute F1 with features c
while forward jj backward:
while forward:
for each feature f in C-c
Compute F1 with features c+f
if best(F1) increases:
backward = true, c=c+f, continue forward
else forward = false
while backward:
for each feature f in in c
Compute F1 with features c-f
if best(F1) increases:
forward = true, c=c-f continue backward
else backward = false
pronoun ??? (he) denotes a male mention, and the
noun phrase ?s?? (girlfriend) represents a female
mention. Similarly for number information, e.g., the
mentions containing ??? (and), ??? (group) are
plural. We use these words to build number and
gender dictionaries, and determine the number and
gender information of a new mention by checking
whether one of the words in the dictionaries is in the
mention.
For semantic class agreement feature in English,
the relation between two mentions is extracted from
WordNet 3.0 (Ng, 2007),(Miller, 1995). There is no
corresponding dictionary for Chinese, so we keep
it blank. The head word for each mention is se-
lected by its dependency head, which can be ex-
tracted throught the conversion head rules ( English
1 and Chinese 2).
Maximum Entropy modeling is used to train the
classifier for our system 3. We employ a greedy for-
ward and backward procedure for feature selection.
The procedure is shown in Algorithm 1.
The algorithm will iterate forward and backward
procedures until the performance does not improve.
We use two initial feature sets: a blank set and
Soon?s baseline feature set. Both feature sets start
1http://w3.msi.vxu.se/ nivre/research/headrules.txt
2http://w3.msi.vxu.se/ nivre/research/chn headrules.txt
3http://homepages.inf.ed.ac.uk/lzhang10/maxent.html
84
with a forward procedure.
2.4 Decoding
For every candidate mention pair, to determine their
coreference relationship is simple because the prob-
ability whether they are coreferent can be obtained
by our maximum entropy model. We can just set a
threshold  = 0:5 and select the pairs with probabil-
ity larger than . But usually it is hard for multiple
mentions. Suppose there are three mentions A, B, C
where the probability between A and B, A and C is
larger than , but B and C is small. Thus choosing
an appropriate decoding algorithm is necessary.
We use best-first clustering method for our system
which for each candidate mention in a document,
chooses the mention before it with best probability
larger than threshold . The difference between En-
glish and Chinese is that we consider the coreference
relationship of two mentions nested in Chinese, but
not in English.
3 Experiments
3.1 Setting
Our system participates in the English and Chinese
closed tracks with auto mentions. For both the En-
glish and Chinese datasets, we use gold annotated
training data for training, and a portion of auto an-
notated development data for feature selection. Only
part of development data is chosen because the eval-
uation procedure takes lot of time. To simplify, We
only select one or two file in each directory as our
development data.
The performance of the system is evaluated on
MUC, B-CUBED, CEAF(M), CEAF(E), BLANC
metrics. The official metric is calculated as
(MUC+B
3
+CEAF )
=
3
.
3.2 Development set
Figures 2 and 3 show the performance on the En-
glish and Chinese development datasets using fea-
ture selection starting from a empty feature set and
Soon?s baseline feature set. The x-axis means the
number of iterations with either forward or back-
ward selection. The performance on Soon?s baseline
feature set for both languages are shown on 1st itera-
tion. The performance from empty feature set starts
on 2nd iteration. From these figures, we can see that
Figure 2: Performance of English development data with
Feature selection
Figure 3: Performance of Chinese development data with
Feature selection
using feature selection in both initial feature sets, the
performance improves.
However the performance of our system is im-
proved only on a few iteration. The best system for
English stops at the 4th iteration with total 10 fea-
tures left, which starts from Soon?s baseline feature
set. Similarly, the system for Chinese achieves its
best performance at the 4th iteration with only 8 fea-
tures. The phenomenon reveals that most of the fea-
tures left for our system are still from Soon?s base-
line features, and our newly exploited lexical, syn-
tactic, and semantic features are not well utilized.
Then we evaluate our model on the entire devel-
opment data. The results are shown on Table 1.
Comparing Figures 2, 3 and Table 1, we can observe
that the performance on entire development data is
lower than part one, about 1% decrease.
3.3 Test
For test data, we retrain our model on both gold
training data and development data using the se-
lected features. The final results for English and
Chinese are shown in Table 2.
85
Model English Chinese
MUC 49.28 48.31
B
3 62.79 67.97
CEAF(M) 46.77 49.49
CEAF(E) 38.19 38.9
BLANC 66.31 68.91
Average 50.09 51.73
Table 1: Results on entire development data
Model English Chinese
MUC 48.27 48.09
B
3 61.37 68.31
CEAF(M) 44.83 49.92
CEAF(E) 36.68 38.89
BLANC 65.42 71.44
Official 48.77 51.76
Table 2: Results on test data
Comparing tables 2 and 1, we can observe that
the performance for the Chinese test data is similar
as the development data. The result seems reason-
able because the model for testing use additional de-
velopment data which is much smaller than training
data. However, the result on English test data seem a
little odd. The performance is about 1.4% less than
that on the development data. The result needs fur-
ther analysis.
4 Conclusion
In this paper, we presented our coreference resolu-
tion system which uses maximum entropy model to
determine the coreference relationship between two
mentions. Our system exploits many lexical, syn-
tactic and semantic features. However, using greedy
forward and backward feature selection strategy for
ME model, these rich features are not well utilized.
In future work we will analyze the reason for this
phenomenon and extend these features to other ma-
chine learning algorithms.
References
Anders Bjo?rkelund and Pierre Nugues. 2011. Explor-
ing lexicalized features for coreference resolution. In
Proceedings of the Fifteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 45?50, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
Hal Daume? III and Daniel Marcu. 2005. A large-scale
exploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 97?104, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Communications of the ACM, 38:39?41,
November.
Vincent Ng and Claire Cardie. 2002. Improvingmachine
learning approaches to coreference resolution. In Pro-
ceedings of the ACL, pages 104?111.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
157?164, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 536?543, Prague, Czech Republic, June.
Association for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 192?199, New York City,
USA, June. Association for Computational Linguis-
tics.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to coref-
erence resolution of noun phrases. Comput. Linguist.,
27:521?544, December.
Michael Strube, Stefan Rapp, and Christoph Mu?ller.
2002. The influence of minimum edit distance on
86
reference resolution. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing - Volume 10, EMNLP ?02, pages 312?319,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured syn-
tactic knowledge. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 41?48, Sydney, Australia,
July. Association for Computational Linguistics.
87
