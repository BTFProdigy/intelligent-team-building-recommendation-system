Proceedings of NAACL-HLT 2013, pages 497?501,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Negative Deceptive Opinion Spam
Myle Ott Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{myleott,cardie}@cs.cornell.edu
Jeffrey T. Hancock
Department of Communication
Cornell University
Ithaca, NY 14853
jeff.hancock@cornell.edu
Abstract
The rising influence of user-generated online
reviews (Cone, 2011) has led to growing in-
centive for businesses to solicit and manufac-
ture DECEPTIVE OPINION SPAM?fictitious
reviews that have been deliberately written to
sound authentic and deceive the reader. Re-
cently, Ott et al (2011) have introduced an
opinion spam dataset containing gold standard
deceptive positive hotel reviews. However, the
complementary problem of negative deceptive
opinion spam, intended to slander competitive
offerings, remains largely unstudied. Follow-
ing an approach similar to Ott et al (2011), in
this work we create and study the first dataset
of deceptive opinion spam with negative sen-
timent reviews. Based on this dataset, we find
that standard n-gram text categorization tech-
niques can detect negative deceptive opinion
spam with performance far surpassing that of
human judges. Finally, in conjunction with
the aforementioned positive review dataset,
we consider the possible interactions between
sentiment and deception, and present initial
results that encourage further exploration of
this relationship.
1 Introduction
Consumer?s purchase decisions are increasingly in-
fluenced by user-generated online reviews of prod-
ucts and services (Cone, 2011). Accordingly,
there is a growing incentive for businesses to so-
licit and manufacture DECEPTIVE OPINION SPAM?
fictitious reviews that have been deliberately writ-
ten to sound authentic and deceive the reader (Ott et
al., 2011). For example, Ott et al (2012) has esti-
mated that between 1% and 6% of positive hotel re-
views appear to be deceptive, suggesting that some
hotels may be posting fake positive reviews in order
to hype their own offerings.
In this work we distinguish between two kinds of
deceptive opinion spam, depending on the sentiment
expressed in the review. In particular, reviews in-
tended to promote or hype an offering, and which
therefore express a positive sentiment towards the
offering, are called positive deceptive opinion spam.
In contrast, reviews intended to disparage or slander
competitive offerings, and which therefore express a
negative sentiment towards the offering, are called
negative deceptive opinion spam. While previous
related work (Ott et al, 2011; Ott et al, 2012) has
explored characteristics of positive deceptive opin-
ion spam, the complementary problem of negative
deceptive opinion spam remains largely unstudied.
Following the framework of Ott et al (2011), we
use Amazon?s Mechanical Turk service to produce
the first publicly available1 dataset of negative de-
ceptive opinion spam, containing 400 gold standard
deceptive negative reviews of 20 popular Chicago
hotels. To validate the credibility of our decep-
tive reviews, we show that human deception detec-
tion performance on the negative reviews is low, in
agreement with decades of traditional deception de-
tection research (Bond and DePaulo, 2006). We then
show that standard n-gram text categorization tech-
niques can be used to detect negative deceptive opin-
ion spam with approximately 86% accuracy ? far
1Dataset available at: http://www.cs.cornell.
edu/?myleott/op_spam.
497
surpassing that of the human judges.
In conjunction with Ott et al (2011)?s positive de-
ceptive opinion spam dataset, we then explore the
interaction between sentiment and deception with
respect to three types of language features: (1)
changes in first-person singular use, often attributed
to psychological distancing (Newman et al, 2003),
(2) decreased spatial awareness and more narrative
form, consistent with theories of reality monitor-
ing (Johnson and Raye, 1981) and imaginative writ-
ing (Biber et al, 1999; Rayson et al, 2001), and (3)
increased negative emotion terms, often attributed to
leakage cues (Ekman and Friesen, 1969), but per-
haps better explained in our case as an exaggeration
of the underlying review sentiment.
2 Dataset
One of the biggest challenges facing studies of de-
ception is obtaining labeled data. Recently, Ott et
al. (2011) have proposed an approach for generat-
ing positive deceptive opinion spam using Amazon?s
popular Mechanical Turk crowdsourcing service. In
this section we discuss our efforts to extend Ott et
al. (2011)?s dataset to additionally include negative
deceptive opinion spam.
2.1 Deceptive Reviews from Mechanical Turk
Deceptive negative reviews are gathered from Me-
chanical Turk using the same procedure as Ott et
al. (2011). In particular, we create and divide 400
HITs evenly across the 20 most popular hotels in
Chicago, such that we obtain 20 reviews for each
hotel. We allow workers to complete only a single
HIT each, so that each review is written by a unique
worker.2 We further require workers to be located
in the United States and to have an average past ap-
proval rating of at least 90%. We allow a maximum
of 30 minutes to complete the HIT, and reward ac-
cepted submissions with one US dollar ($1).
Each HIT instructs a worker to imagine that they
work for the marketing department of a hotel, and
that their manager has asked them to write a fake
negative review of a competitor?s hotel to be posted
online. Accompanying each HIT is the name and
2While Mechanical Turk does not provide a convenient
mechanism for ensuring the uniqueness of workers, this con-
straint can be enforced with Javascript. The script is available
at: http://uniqueturker.myleott.com.
URL of the hotel for which the fake negative re-
view is to be written, and instructions that: (1) work-
ers should not complete more than one similar HIT,
(2) submissions must be of sufficient quality, i.e.,
written for the correct hotel, legible, reasonable in
length,3 and not plagiarized,4 and, (3) the HIT is for
academic research purposes.
Submissions are manually inspected to ensure
that they are written for the correct hotel and to
ensure that they convey a generally negative senti-
ment.5 The average accepted review length was 178
words, higher than for the positive reviews gathered
by Ott et al (2011), who report an average review
length of 116 words.
2.2 Truthful Reviews from the Web
Negative (1- or 2-star) truthful reviews are mined
from six popular online review communities: Expe-
dia, Hotels.com, Orbitz, Priceline, TripAdvisor, and
Yelp. While reviews mined from these communities
cannot be considered gold standard truthful, recent
work (Mayzlin et al, 2012; Ott et al, 2012) suggests
that deception rates among travel review portals is
reasonably small.
Following Ott et al (2011), we sample a subset
of the available truthful reviews so that we retain an
equal number of truthful and deceptive reviews (20
each) for each hotel. However, because the truthful
reviews are on average longer than our deceptive re-
views, we sample the truthful reviews according to
a log-normal distribution fit to the lengths of our de-
ceptive reviews, similarly to Ott et al (2011).6
3 Deception Detection Performance
In this section we report the deception detection per-
formance of three human judges (Section 3.1) and
supervised n-gram Support Vector Machine (SVM)
classifiers (Section 3.2).
3We define ?reasonable length? to be ? 150 characters.
4We use http://plagiarisma.net to determine
whether or not a review is plagiarized.
5We discarded and replaced approximately 2% of the sub-
missions, where it was clear that the worker had misread the
instructions and instead written a deceptive positive review.
6We use the R package GAMLSS (Rigby and Stasinopou-
los, 2005) to fit a log-normal distribution (left truncated at 150
characters) to the lengths of the deceptive reviews.
498
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN
JUDGE 1 65.0% 65.0 65.0 65.0 65.0 65.0 65.0
JUDGE 2 61.9% 63.0 57.5 60.1 60.9 66.3 63.5
JUDGE 3 57.5% 57.3 58.8 58.0 57.7 56.3 57.0
META
MAJORITY 69.4% 70.1 67.5 68.8 68.7 71.3 69.9
SKEPTIC 58.1% 78.3 22.5 35.0 54.7 93.8 69.1
Table 1: Deception detection performance, incl. (P)recision, (R)ecall, and (F)1-score, for three human judges and two
meta-judges on a set of 160 negative reviews. The largest value in each column is indicated with boldface.
3.1 Human Performance
Recent large-scale meta-analyses have shown hu-
man deception detection performance is low, with
accuracies often not much better than chance (Bond
and DePaulo, 2006). Indeed, Ott et al (2011) found
that two out of three human judges were unable to
perform statistically significantly better than chance
(at the p < 0.05 level) at detecting positive decep-
tive opinion spam. Nevertheless, it is important to
subject our reviews to human judgments to validate
their convincingness. In particular, if human detec-
tion performance is found to be very high, then it
would cast doubt on the usefulness of the Mechan-
ical Turk approach for soliciting gold standard de-
ceptive opinion spam.
Following Ott et al (2011), we asked three vol-
unteer undergraduate university students to read and
make assessments on a subset of the negative review
dataset described in Section 2. Specifically, we ran-
domized all 40 deceptive and truthful reviews from
each of four hotels (160 reviews total). We then
asked the volunteers to read each review and mark
whether they believed it to be truthful or deceptive.
Performance for the three human judges appears
in Table 1. We additionally show the deception de-
tection performance of two meta-judges that aggre-
gate the assessments of the individual human judges:
(1) the MAJORITY meta-judge predicts deceptive
when at least two out of three human judges predict
deceptive (and truthful otherwise), and (2) the SKEP-
TIC meta-judge predicts deceptive when at least one
out of three human judges predicts deceptive (and
truthful otherwise).
A two-tailed binomial test suggests that JUDGE 1
and JUDGE 2 both perform better than chance (p =
0.0002, 0.003, respectively), while JUDGE 3 fails to
reject the null hypothesis of performing at-chance
(p = 0.07). However, while the best human judge
is accurate 65% of the time, inter-annotator agree-
ment computed using Fleiss? kappa is only slight
at 0.07 (Landis and Koch, 1977). Furthermore,
based on Cohen?s kappa, the highest pairwise inter-
annotator agreement is only 0.26, between JUDGE
1 and JUDGE 2. These low agreements suggest
that while the judges may perform statistically better
than chance, they are identifying different reviews
as deceptive, i.e., few reviews are consistently iden-
tified as deceptive.
3.2 Automated Classifier Performance
Standard n-gram?based text categorization tech-
niques have been shown to be effective at detect-
ing deception in text (Jindal and Liu, 2008; Mihal-
cea and Strapparava, 2009; Ott et al, 2011; Feng et
al., 2012). Following Ott et al (2011), we evaluate
the performance of linear Support Vector Machine
(SVM) classifiers trained with unigram and bigram
term-frequency features on our novel negative de-
ceptive opinion spam dataset. We employ the same
5-fold stratified cross-validation (CV) procedure as
Ott et al (2011), whereby for each cross-validation
iteration we train our model on all reviews for 16
hotels, and test our model on all reviews for the re-
maining 4 hotels. The SVM cost parameter, C, is
tuned by nested cross-validation on the training data.
Results appear in Table 2. Each row lists the sen-
timent of the train and test reviews, where ?Cross
Val.? corresponds to the cross-validation procedure
described above, and ?Held Out? corresponds to
classifiers trained on reviews of one sentiment and
tested on the other. The results suggest that n-gram?
based SVM classifiers can detect negative decep-
tive opinion spam in a balanced dataset with perfor-
mance far surpassing that of untrained human judges
(see Section 3.1). Furthermore, our results show that
499
TRUTHFUL DECEPTIVE
Train Sentiment Test Sentiment Accuracy P R F P R F
POSITIVE POSITIVE (800 reviews, Cross Val.) 89.3% 89.6 88.8 89.2 88.9 89.8 89.3
(800 reviews) NEGATIVE (800 reviews, Held Out) 75.1% 69.0 91.3 78.6 87.1 59.0 70.3
NEGATIVE POSITIVE (800 reviews, Held Out) 81.4% 76.3 91.0 83.0 88.9 71.8 79.4
(800 reviews) NEGATIVE (800 reviews, Cross Val.) 86.0% 86.4 85.5 85.9 85.6 86.5 86.1
COMBINED POSITIVE (800 reviews, Cross Val.) 88.4% 87.7 89.3 88.5 89.1 87.5 88.3
(1600 reviews) NEGATIVE (800 reviews, Cross Val.) 86.0% 85.3 87.0 86.1 86.7 85.0 85.9
Table 2: Automated classifier performance for different train and test sets, incl. (P)recision, (R)ecall and (F)1-score.
classifiers trained and tested on reviews of differ-
ent sentiments perform worse, despite having more
training data,7 than classifiers trained and tested on
reviews of the same sentiment. This suggests that
cues to deception differ depending on the sentiment
of the text (see Section 4).
Interestingly, we find that training on the com-
bined sentiment dataset results in performance that
is comparable to that of the ?same sentiment? classi-
fiers (88.4% vs. 89.3% accuracy for positive reviews
and 86.0% vs. 86.0% accuracy for negative reviews).
This is explainable in part by the increased training
set size (1,280 vs. 640 reviews per 4 training folds).
4 Interaction of Sentiment and Deception
An important question is how language features op-
erate in our fake negative reviews compared with the
fake positive reviews of Ott et al (2011). For exam-
ple, fake positive reviews included less spatial lan-
guage (e.g., floor, small, location, etc.) because in-
dividuals who had not actually experienced the ho-
tel simply had less spatial detail available for their
review (Johnson and Raye, 1981). This was also the
case for our negative reviews, with less spatial lan-
guage observed for fake negative reviews relative to
truthful. Likewise, our fake negative reviews had
more verbs relative to nouns than truthful, suggest-
ing a more narrative style that is indicative of imag-
inative writing (Biber et al, 1999; Rayson et al,
2001), a pattern also observed by Ott et al (2011).
There were, however, several important differ-
ences in the deceptive language of fake negative rel-
ative to fake positive reviews. First, as might be
expected, negative emotion terms were more fre-
7?Cross Val.? classifiers are effectively trained on 80% of
the data and tested on the remaining 20%, whereas ?Held Out?
classifiers are trained and tested on 100% of each data.
quent, according to LIWC (Pennebaker et al, 2007),
in our fake negative reviews than in the fake posi-
tive reviews. But, importantly, the fake negative re-
viewers over-produced negative emotion terms (e.g.,
terrible, disappointed) relative to the truthful re-
views in the same way that fake positive reviewers
over-produced positive emotion terms (e.g., elegant,
luxurious). Combined, these data suggest that the
more frequent negative emotion terms in the present
dataset are not the result of ?leakage cues? that re-
veal the emotional distress of lying (Ekman and
Friesen, 1969). Instead, the differences suggest that
fake hotel reviewers exaggerate the sentiment they
are trying to convey relative to similarly-valenced
truthful reviews.
Second, the effect of deception on the pattern of
pronoun frequency was not the same across posi-
tive and negative reviews. In particular, while first
person singular pronouns were produced more fre-
quently in fake reviews than truthful, consistent with
the case for positive reviews, the increase was di-
minished in the negative reviews examined here. In
the positive reviews reported by Ott et al (2011),
the rate of first person singular in fake reviews
(M=4.36%, SD=2.96%) was twice the rate observed
in truthful reviews (M=2.18%, SD=2.04%). In con-
trast, the rate of first person singular in the deceptive
negative reviews (M=4.47%, SD=2.83%) was only
57% greater than for truthful reviews (M=2.85%,
SD=2.23%). These results suggest that the empha-
sis on the self, perhaps as a strategy of convinc-
ing the reader that the author had actually been to
the hotel, is not as evident in the fake negative re-
views, perhaps because the negative tone of the re-
views caused the reviewers to psychologically dis-
tance themselves from their negative statements, a
phenomenon observed in several other deception
studies, e.g., Hancock et al (2008).
500
5 Conclusion
We have created the first publicly-available corpus
of gold standard negative deceptive opinion spam,
containing 400 reviews of 20 Chicago hotels, which
we have used to compare the deception detection ca-
pabilities of untrained human judges and standard
n-gram?based Support Vector Machine classifiers.
Our results demonstrate that while human deception
detection performance is greater for negative rather
than positive deceptive opinion spam, the best detec-
tion performance is still achieved through automated
classifiers, with approximately 86% accuracy.
We have additionally explored, albeit briefly, the
relationship between sentiment and deception by
utilizing Ott et al (2011)?s positive deceptive opin-
ion spam dataset in conjunction with our own. In
particular, we have identified several features of lan-
guage that seem to remain consistent across senti-
ment, such as decreased awareness of spatial details
and exaggerated language. We have also identified
other features that vary with the sentiment, such as
first person singular use, although further work is re-
quired to determine if these differences may be ex-
ploited to improve deception detection performance.
Indeed, future work may wish to jointly model sen-
timent and deception in order to better determine the
effect each has on language use.
Acknowledgments
This work was supported in part by NSF Grant BCS-
0904822, a DARPA Deft grant, the Jack Kent Cooke
Foundation, and by a gift from Google. We also thank
the three Cornell undergraduate volunteer judges, as well
as the NAACL reviewers for their insightful comments,
suggestions and advice on various aspects of this work.
References
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
Cone. 2011. 2011 Online Influence Trend Tracker. On-
line: http://www.coneinc.com/negative-
reviews-online-reverse-purchase-
decisions, August.
P. Ekman and W.V. Friesen. 1969. Nonverbal leakage
and clues to deception. Psychiatry, 32(1):88.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syn-
tactic stylometry for deception detection. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics: Short Papers-Volume
2, pages 171?175. Association for Computational Lin-
guistics.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1?23.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230. ACM.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67?85.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
Dina Mayzlin, Yaniv Dover, and Judith A Chevalier.
2012. Promotional reviews: An empirical investiga-
tion of online review manipulation. Technical report,
National Bureau of Economic Research.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309?312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of the
imagination. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies-Volume 1, pages 309?
319. Association for Computational Linguistics.
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Es-
timating the prevalence of deception in online review
communities. In Proceedings of the 21st international
conference on World Wide Web, pages 201?210. ACM.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX: LIWC
(www.liwc.net).
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295?
306.
R. A. Rigby and D. M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape,(with dis-
cussion). Applied Statistics, 54:507?554.
501
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 309?319,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Finding Deceptive Opinion Spam by Any Stretch of the Imagination
Myle Ott Yejin Choi Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{myleott,ychoi,cardie}@cs.cornell.edu
Jeffrey T. Hancock
Department of Communication
Cornell University
Ithaca, NY 14853
jth34@cornell.edu
Abstract
Consumers increasingly rate, review and re-
search products online (Jansen, 2010; Litvin
et al, 2008). Consequently, websites con-
taining consumer reviews are becoming tar-
gets of opinion spam. While recent work
has focused primarily on manually identifi-
able instances of opinion spam, in this work
we study deceptive opinion spam?fictitious
opinions that have been deliberately written to
sound authentic. Integrating work from psy-
chology and computational linguistics, we de-
velop and compare three approaches to detect-
ing deceptive opinion spam, and ultimately
develop a classifier that is nearly 90% accurate
on our gold-standard opinion spam dataset.
Based on feature analysis of our learned mod-
els, we additionally make several theoretical
contributions, including revealing a relation-
ship between deceptive opinions and imagina-
tive writing.
1 Introduction
With the ever-increasing popularity of review web-
sites that feature user-generated opinions (e.g.,
TripAdvisor1 and Yelp2), there comes an increasing
potential for monetary gain through opinion spam?
inappropriate or fraudulent reviews. Opinion spam
can range from annoying self-promotion of an un-
related website or blog to deliberate review fraud,
as in the recent case3 of a Belkin employee who
1http://tripadvisor.com
2http://yelp.com
3http://news.cnet.com/8301-1001_
3-10145399-92.html
hired people to write positive reviews for an other-
wise poorly reviewed product.4
While other kinds of spam have received consid-
erable computational attention, regrettably there has
been little work to date (see Section 2) on opinion
spam detection. Furthermore, most previous work in
the area has focused on the detection of DISRUPTIVE
OPINION SPAM?uncontroversial instances of spam
that are easily identified by a human reader, e.g., ad-
vertisements, questions, and other irrelevant or non-
opinion text (Jindal and Liu, 2008). And while the
presence of disruptive opinion spam is certainly a
nuisance, the risk it poses to the user is minimal,
since the user can always choose to ignore it.
We focus here on a potentially more insidi-
ous type of opinion spam: DECEPTIVE OPINION
SPAM?fictitious opinions that have been deliber-
ately written to sound authentic, in order to deceive
the reader. For example, one of the following two
hotel reviews is truthful and the other is deceptive
opinion spam:
1. I have stayed at many hotels traveling for both business
and pleasure and I can honestly stay that The James is
tops. The service at the hotel is first class. The rooms
are modern and very comfortable. The location is per-
fect within walking distance to all of the great sights and
restaurants. Highly recommend to both business trav-
ellers and couples.
2. My husband and I stayed at the James Chicago Hotel
for our anniversary. This place is fantastic! We knew
as soon as we arrived we made the right choice! The
rooms are BEAUTIFUL and the staff very attentive and
wonderful!! The area of the hotel is great, since I love
to shop I couldn?t ask for more!! We will definatly be
4It is also possible for opinion spam to be negative, poten-
tially in order to sully the reputation of a competitor.
309
back to Chicago and we will for sure be back to the James
Chicago.
Typically, these deceptive opinions are neither
easily ignored nor even identifiable by a human
reader;5 consequently, there are few good sources
of labeled data for this research. Indeed, in the ab-
sence of gold-standard data, related studies (see Sec-
tion 2) have been forced to utilize ad hoc procedures
for evaluation. In contrast, one contribution of the
work presented here is the creation of the first large-
scale, publicly available6 dataset for deceptive opin-
ion spam research, containing 400 truthful and 400
gold-standard deceptive reviews.
To obtain a deeper understanding of the nature of
deceptive opinion spam, we explore the relative util-
ity of three potentially complementary framings of
our problem. Specifically, we view the task as: (a)
a standard text categorization task, in which we use
n-gram?based classifiers to label opinions as either
deceptive or truthful (Joachims, 1998; Sebastiani,
2002); (b) an instance of psycholinguistic decep-
tion detection, in which we expect deceptive state-
ments to exemplify the psychological effects of ly-
ing, such as increased negative emotion and psycho-
logical distancing (Hancock et al, 2008; Newman et
al., 2003); and, (c) a problem of genre identification,
in which we view deceptive and truthful writing as
sub-genres of imaginative and informative writing,
respectively (Biber et al, 1999; Rayson et al, 2001).
We compare the performance of each approach
on our novel dataset. Particularly, we find that ma-
chine learning classifiers trained on features tradi-
tionally employed in (a) psychological studies of
deception and (b) genre identification are both out-
performed at statistically significant levels by n-
gram?based text categorization techniques. Notably,
a combined classifier with both n-gram and psy-
chological deception features achieves nearly 90%
cross-validated accuracy on this task. In contrast,
we find deceptive opinion spam detection to be well
beyond the capabilities of most human judges, who
perform roughly at-chance?a finding that is consis-
tent with decades of traditional deception detection
research (Bond and DePaulo, 2006).
5The second example review is deceptive opinion spam.
6Available by request at: http://www.cs.cornell.
edu/?myleott/op_spam
Additionally, we make several theoretical con-
tributions based on an examination of the feature
weights learned by our machine learning classifiers.
Specifically, we shed light on an ongoing debate in
the deception literature regarding the importance of
considering the context and motivation of a decep-
tion, rather than simply identifying a universal set
of deception cues. We also present findings that are
consistent with recent work highlighting the difficul-
ties that liars have encoding spatial information (Vrij
et al, 2009). Lastly, our study of deceptive opinion
spam detection as a genre identification problem re-
veals relationships between deceptive opinions and
imaginative writing, and between truthful opinions
and informative writing.
The rest of this paper is organized as follows: in
Section 2, we summarize related work; in Section 3,
we explain our methodology for gathering data and
evaluate human performance; in Section 4, we de-
scribe the features and classifiers employed by our
three automated detection approaches; in Section 5,
we present and discuss experimental results; finally,
conclusions and directions for future work are given
in Section 6.
2 Related Work
Spam has historically been studied in the contexts of
e-mail (Drucker et al, 2002), and the Web (Gyo?ngyi
et al, 2004; Ntoulas et al, 2006). Recently, re-
searchers have began to look at opinion spam as
well (Jindal and Liu, 2008; Wu et al, 2010; Yoo
and Gretzel, 2009).
Jindal and Liu (2008) find that opinion spam is
both widespread and different in nature from either
e-mail or Web spam. Using product review data,
and in the absence of gold-standard deceptive opin-
ions, they train models using features based on the
review text, reviewer, and product, to distinguish
between duplicate opinions7 (considered deceptive
spam) and non-duplicate opinions (considered truth-
ful). Wu et al (2010) propose an alternative strategy
for detecting deceptive opinion spam in the absence
7Duplicate (or near-duplicate) opinions are opinions that ap-
pear more than once in the corpus with the same (or similar)
text. While these opinions are likely to be deceptive, they are
unlikely to be representative of deceptive opinion spam in gen-
eral. Moreover, they are potentially detectable via off-the-shelf
plagiarism detection software.
310
of gold-standard data, based on the distortion of pop-
ularity rankings. Both of these heuristic evaluation
approaches are unnecessary in our work, since we
compare gold-standard deceptive and truthful opin-
ions.
Yoo and Gretzel (2009) gather 40 truthful and 42
deceptive hotel reviews and, using a standard statis-
tical test, manually compare the psychologically rel-
evant linguistic differences between them. In con-
trast, we create a much larger dataset of 800 opin-
ions that we use to develop and evaluate automated
deception classifiers.
Research has also been conducted on the re-
lated task of psycholinguistic deception detection.
Newman et al (2003), and later Mihalcea and
Strapparava (2009), ask participants to give both
their true and untrue views on personal issues
(e.g., their stance on the death penalty). Zhou et
al. (2004; 2008) consider computer-mediated decep-
tion in role-playing games designed to be played
over instant messaging and e-mail. However, while
these studies compare n-gram?based deception clas-
sifiers to a random guess baseline of 50%, we addi-
tionally evaluate and compare two other computa-
tional approaches (described in Section 4), as well
as the performance of human judges (described in
Section 3.3).
Lastly, automatic approaches to determining re-
view quality have been studied?directly (Weimer
et al, 2007), and in the contexts of helpful-
ness (Danescu-Niculescu-Mizil et al, 2009; Kim et
al., 2006; O?Mahony and Smyth, 2009) and credibil-
ity (Weerkamp and De Rijke, 2008). Unfortunately,
most measures of quality employed in those works
are based exclusively on human judgments, which
we find in Section 3 to be poorly calibrated to de-
tecting deceptive opinion spam.
3 Dataset Construction and Human
Performance
While truthful opinions are ubiquitous online, de-
ceptive opinions are difficult to obtain without re-
sorting to heuristic methods (Jindal and Liu, 2008;
Wu et al, 2010). In this section, we report our ef-
forts to gather (and validate with human judgments)
the first publicly available opinion spam dataset with
gold-standard deceptive opinions.
Following the work of Yoo and Gretzel (2009), we
compare truthful and deceptive positive reviews for
hotels found on TripAdvisor. Specifically, we mine
all 5-star truthful reviews from the 20 most popular
hotels on TripAdvisor8 in the Chicago area.9 De-
ceptive opinions are gathered for those same 20 ho-
tels using Amazon Mechanical Turk10 (AMT). Be-
low, we provide details of the collection methodolo-
gies for deceptive (Section 3.1) and truthful opinions
(Section 3.2). Ultimately, we collect 20 truthful and
20 deceptive opinions for each of the 20 chosen ho-
tels (800 opinions total).
3.1 Deceptive opinions via Mechanical Turk
Crowdsourcing services such as AMT have made
large-scale data annotation and collection efforts fi-
nancially affordable by granting anyone with ba-
sic programming skills access to a marketplace of
anonymous online workers (known as Turkers) will-
ing to complete small tasks.
To solicit gold-standard deceptive opinion spam
using AMT, we create a pool of 400 Human-
Intelligence Tasks (HITs) and allocate them evenly
across our 20 chosen hotels. To ensure that opin-
ions are written by unique authors, we allow only a
single submission per Turker. We also restrict our
task to Turkers who are located in the United States,
and who maintain an approval rating of at least 90%.
Turkers are allowed a maximum of 30 minutes to
work on the HIT, and are paid one US dollar for an
accepted submission.
Each HIT presents the Turker with the name and
website of a hotel. The HIT instructions ask the
Turker to assume that they work for the hotel?s mar-
keting department, and to pretend that their boss
wants them to write a fake review (as if they were
a customer) to be posted on a travel review website;
additionally, the review needs to sound realistic and
portray the hotel in a positive light. A disclaimer
8TripAdvisor utilizes a proprietary ranking system to assess
hotel popularity. We chose the 20 hotels with the greatest num-
ber of reviews, irrespective of the TripAdvisor ranking.
9It has been hypothesized that popular offerings are less
likely to become targets of deceptive opinion spam, since the
relative impact of the spam in such cases is small (Jindal and
Liu, 2008; Lim et al, 2010). By considering only the most
popular hotels, we hope to minimize the risk of mining opinion
spam and labeling it as truthful.
10http://mturk.com
311
Time spent t (minutes)
All submissions
count: 400
tmin: 0.08, tmax: 29.78
t?: 8.06, s: 6.32
Length ` (words)
All submissions
`min: 25, `max: 425
?`: 115.75, s: 61.30
Time spent t < 1
count: 47
`min: 39, `max: 407
?`: 113.94, s: 66.24
Time spent t ? 1
count: 353
`min: 25, `max: 425
?`: 115.99, s: 60.71
Table 1: Descriptive statistics for 400 deceptive opinion
spam submissions gathered using AMT. s corresponds to
the sample standard deviation.
indicates that any submission found to be of insuffi-
cient quality (e.g., written for the wrong hotel, unin-
telligible, unreasonably short,11 plagiarized,12 etc.)
will be rejected.
It took approximately 14 days to collect 400 sat-
isfactory deceptive opinions. Descriptive statistics
appear in Table 1. Submissions vary quite dramati-
cally both in length, and time spent on the task. Par-
ticularly, nearly 12% of the submissions were com-
pleted in under one minute. Surprisingly, an inde-
pendent two-tailed t-test between the mean length of
these submissions (?`t<1) and the other submissions
(?`t?1) reveals no significant difference (p = 0.83).
We suspect that these ?quick? users may have started
working prior to having formally accepted the HIT,
presumably to circumvent the imposed time limit.
Indeed, the quickest submission took just 5 seconds
and contained 114 words.
3.2 Truthful opinions from TripAdvisor
For truthful opinions, we mine all 6,977 reviews
from the 20 most popular Chicago hotels on
TripAdvisor. From these we eliminate:
? 3,130 non-5-star reviews;
? 41 non-English reviews;13
? 75 reviews with fewer than 150 characters
since, by construction, deceptive opinions are
11A submission is considered unreasonably short if it con-
tains fewer than 150 characters.
12Submissions are individually checked for plagiarism at
http://plagiarisma.net.
13Language is determined using http://tagthe.net.
at least 150 characters long (see footnote 11 in
Section 3.1);
? 1,607 reviews written by first-time authors?
new users who have not previously posted an
opinion on TripAdvisor?since these opinions
are more likely to contain opinion spam, which
would reduce the integrity of our truthful re-
view data (Wu et al, 2010).
Finally, we balance the number of truthful and
deceptive opinions by selecting 400 of the remain-
ing 2,124 truthful reviews, such that the document
lengths of the selected truthful reviews are similarly
distributed to those of the deceptive reviews. Work
by Serrano et al (2009) suggests that a log-normal
distribution is appropriate for modeling document
lengths. Thus, for each of the 20 chosen hotels, we
select 20 truthful reviews from a log-normal (left-
truncated at 150 characters) distribution fit to the
lengths of the deceptive reviews.14 Combined with
the 400 deceptive reviews gathered in Section 3.1
this yields our final dataset of 800 reviews.
3.3 Human performance
Assessing human deception detection performance
is important for several reasons. First, there are few
other baselines for our classification task; indeed, re-
lated studies (Jindal and Liu, 2008; Mihalcea and
Strapparava, 2009) have only considered a random
guess baseline. Second, assessing human perfor-
mance is necessary to validate the deceptive opin-
ions gathered in Section 3.1. If human performance
is low, then our deceptive opinions are convincing,
and therefore, deserving of further attention.
Our initial approach to assessing human perfor-
mance on this task was with Mechanical Turk. Un-
fortunately, we found that some Turkers selected
among the choices seemingly at random, presum-
ably to maximize their hourly earnings by obviating
the need to read the review. While a similar effect
has been observed previously (Akkaya et al, 2010),
there remains no universal solution.
Instead, we solicit the help of three volunteer un-
dergraduate university students to make judgments
on a subset of our data. This balanced subset, cor-
responding to the first fold of our cross-validation
14We use the R package GAMLSS (Rigby and Stasinopoulos,
2005) to fit the left-truncated log-normal distribution.
312
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6
META
MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the
first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
experiments described in Section 5, contains all 40
reviews from each of four randomly chosen hotels.
Unlike the Turkers, our student volunteers are not
offered a monetary reward. Consequently, we con-
sider their judgements to be more honest than those
obtained via AMT.
Additionally, to test the extent to which the in-
dividual human judges are biased, we evaluate the
performance of two virtual meta-judges. Specifi-
cally, the MAJORITY meta-judge predicts ?decep-
tive? when at least two out of three human judges
believe the review to be deceptive, and the SKEP-
TIC meta-judge predicts ?deceptive? when any hu-
man judge believes the review to be deceptive.
Human and meta-judge performance is given in
Table 2. It is clear from the results that human
judges are not particularly effective at this task. In-
deed, a two-tailed binomial test fails to reject the
null hypothesis that JUDGE 2 and JUDGE 3 per-
form at-chance (p = 0.003, 0.10, 0.48 for the three
judges, respectively). Furthermore, all three judges
suffer from truth-bias (Vrij, 2008), a common find-
ing in deception detection research in which hu-
man judges are more likely to classify an opinion
as truthful than deceptive. In fact, JUDGE 2 clas-
sified fewer than 12% of the opinions as decep-
tive! Interestingly, this bias is effectively smoothed
by the SKEPTIC meta-judge, which produces nearly
perfectly class-balanced predictions. A subsequent
reevaluation of human performance on this task sug-
gests that the truth-bias can be reduced if judges
are given the class-proportions in advance, although
such prior knowledge is unrealistic; and ultimately,
performance remains similar to that of Table 2.
Inter-annotator agreement among the three
judges, computed using Fleiss? kappa, is 0.11.
While there is no precise rule for interpreting
kappa scores, Landis and Koch (1977) suggest
that scores in the range (0.00, 0.20] correspond
to ?slight agreement? between annotators. The
largest pairwise Cohen?s kappa is 0.12, between
JUDGE 2 and JUDGE 3?a value far below generally
accepted pairwise agreement levels. We suspect
that agreement among our human judges is so
low precisely because humans are poor judges of
deception (Vrij, 2008), and therefore they perform
nearly at-chance respective to one another.
4 Automated Approaches to Deceptive
Opinion Spam Detection
We consider three automated approaches to detect-
ing deceptive opinion spam, each of which utilizes
classifiers (described in Section 4.4) trained on the
dataset of Section 3. The features employed by each
strategy are outlined here.
4.1 Genre identification
Work in computational linguistics has shown that
the frequency distribution of part-of-speech (POS)
tags in a text is often dependent on the genre of the
text (Biber et al, 1999; Rayson et al, 2001). In our
genre identification approach to deceptive opinion
spam detection, we test if such a relationship exists
for truthful and deceptive reviews by constructing,
for each review, features based on the frequencies of
each POS tag.15 These features are also intended to
provide a good baseline with which to compare our
other automated approaches.
4.2 Psycholinguistic deception detection
The Linguistic Inquiry and Word Count (LIWC)
software (Pennebaker et al, 2007) is a popular au-
tomated text analysis tool used widely in the so-
cial sciences. It has been used to detect personality
15We use the Stanford Parser (Klein and Manning, 2003) to
obtain the relative POS frequencies.
313
traits (Mairesse et al, 2007), to study tutoring dy-
namics (Cade et al, 2010), and, most relevantly, to
analyze deception (Hancock et al, 2008; Mihalcea
and Strapparava, 2009; Vrij et al, 2007).
While LIWC does not include a text classifier, we
can create one with features derived from the LIWC
output. In particular, LIWC counts and groups
the number of instances of nearly 4,500 keywords
into 80 psychologically meaningful dimensions. We
construct one feature for each of the 80 LIWC di-
mensions, which can be summarized broadly under
the following four categories:
1. Linguistic processes: Functional aspects of text
(e.g., the average number of words per sen-
tence, the rate of misspelling, swearing, etc.)
2. Psychological processes: Includes all social,
emotional, cognitive, perceptual and biological
processes, as well as anything related to time or
space.
3. Personal concerns: Any references to work,
leisure, money, religion, etc.
4. Spoken categories: Primarily filler and agree-
ment words.
While other features have been considered in past
deception detection work, notably those of Zhou et
al. (2004), early experiments found LIWC features
to perform best. Indeed, the LIWC2007 software
used in our experiments subsumes most of the fea-
tures introduced in other work. Thus, we focus our
psycholinguistic approach to deception detection on
LIWC-based features.
4.3 Text categorization
In contrast to the other strategies just discussed,
our text categorization approach to deception de-
tection allows us to model both content and con-
text with n-gram features. Specifically, we consider
the following three n-gram feature sets, with the
corresponding features lowercased and unstemmed:
UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the
superscript + indicates that the feature set subsumes
the preceding feature set.
4.4 Classifiers
Features from the three approaches just introduced
are used to train Na??ve Bayes and Support Vector
Machine classifiers, both of which have performed
well in related work (Jindal and Liu, 2008; Mihalcea
and Strapparava, 2009; Zhou et al, 2008).
For a document ~x, with label y, the Na??ve Bayes
(NB) classifier gives us the following decision rule:
y? = arg max
c
Pr(y = c) ? Pr(~x | y = c) (1)
When the class prior is uniform, for example
when the classes are balanced (as in our case), (1)
can be simplified to the maximum likelihood classi-
fier (Peng and Schuurmans, 2003):
y? = arg max
c
Pr(~x | y = c) (2)
Under (2), both the NB classifier used by Mihal-
cea and Strapparava (2009) and the language model
classifier used by Zhou et al (2008) are equivalent.
Thus, following Zhou et al (2008), we use the SRI
Language Modeling Toolkit (Stolcke, 2002) to esti-
mate individual language models, Pr(~x | y = c),
for truthful and deceptive opinions. We consider
all three n-gram feature sets, namely UNIGRAMS,
BIGRAMS+, and TRIGRAMS+, with corresponding
language models smoothed using the interpolated
Kneser-Ney method (Chen and Goodman, 1996).
We also train Support Vector Machine (SVM)
classifiers, which find a high-dimensional separating
hyperplane between two groups of data. To simplify
feature analysis in Section 5, we restrict our evalu-
ation to linear SVMs, which learn a weight vector
~w and bias term b, such that a document ~x can be
classified by:
y? = sign(~w ? ~x + b) (3)
We use SVMlight (Joachims, 1999) to train our
linear SVM models on all three approaches and
feature sets described above, namely POS, LIWC,
UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also
evaluate every combination of these features, but
for brevity include only LIWC+BIGRAMS+, which
performs best. Following standard practice, doc-
ument vectors are normalized to unit-length. For
LIWC+BIGRAMS+, we unit-length normalize LIWC
and BIGRAMS+ features individually before com-
bining them.
314
TRUTHFUL DECEPTIVE
Approach Features Accuracy P R F P R F
GENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2
PSYCHOLINGUISTIC
LIWCSVM 76.8% 77.2 76.0 76.6 76.4 77.5 76.9
DECEPTION DETECTION
TEXT CATEGORIZATION
UNIGRAMSSVM 88.4% 89.9 86.5 88.2 87.0 90.3 88.6
BIGRAMS+SVM 89.6% 90.1 89.0 89.6 89.1 90.3 89.7
LIWC+BIGRAMS+SVM 89.8% 89.8 89.8 89.8 89.8 89.8 89.8
TRIGRAMS+SVM 89.0% 89.0 89.0 89.0 89.0 89.0 89.0
UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9
BIGRAMS+NB 88.9% 89.8 87.8 88.7 88.0 90.0 89.0
TRIGRAMS+NB 87.6% 87.7 87.5 87.6 87.5 87.8 87.6
HUMAN / META
JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.
Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false
positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for
JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion
subset on which they are assessed only corresponds to the first cross-validation fold.
5 Results and Discussion
The deception detection strategies described in Sec-
tion 4 are evaluated using a 5-fold nested cross-
validation (CV) procedure (Quadrianto et al, 2009),
where model parameters are selected for each test
fold based on standard CV experiments on the train-
ing folds. Folds are selected so that each contains all
reviews from four hotels; thus, learned models are
always evaluated on reviews from unseen hotels.
Results appear in Table 3. We observe that auto-
mated classifiers outperform human judges for every
metric, except truthful recall where JUDGE 2 per-
forms best.16 However, this is expected given that
untrained humans often focus on unreliable cues to
deception (Vrij, 2008). For example, one study ex-
amining deception in online dating found that hu-
mans perform at-chance detecting deceptive pro-
files because they rely on text-based cues that are
unrelated to deception, such as second-person pro-
nouns (Toma and Hancock, In Press).
Among the automated classifiers, baseline per-
formance is given by the simple genre identifica-
tion approach (POSSVM) proposed in Section 4.1.
Surprisingly, we find that even this simple auto-
16As mentioned in Section 3.3, JUDGE 2 classified fewer than
12% of opinions as deceptive. While achieving 95% truthful re-
call, this judge?s corresponding precision was not significantly
better than chance (two-tailed binomial p = 0.4).
mated classifier outperforms most human judges
(one-tailed sign test p = 0.06, 0.01, 0.001 for the
three judges, respectively, on the first fold). This
result is best explained by theories of reality mon-
itoring (Johnson and Raye, 1981), which suggest
that truthful and deceptive opinions might be clas-
sified into informative and imaginative genres, re-
spectively. Work by Rayson et al (2001) has found
strong distributional differences between informa-
tive and imaginative writing, namely that the former
typically consists of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunctions,
while the latter consists of more verbs,17 adverbs,18
pronouns, and pre-determiners. Indeed, we find that
the weights learned by POSSVM (found in Table 4)
are largely in agreement with these findings, no-
tably except for adjective and adverb superlatives,
the latter of which was found to be an exception by
Rayson et al (2001). However, that deceptive opin-
ions contain more superlatives is not unexpected,
since deceptive writing (but not necessarily imagi-
native writing in general) often contains exaggerated
language (Buller and Burgoon, 1996; Hancock et al,
2008).
Both remaining automated approaches to detect-
ing deceptive opinion spam outperform the simple
17Past participle verbs were an exception.
18Superlative adverbs were an exception.
315
TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE
Category Variant Weight Category Variant Weight
NOUNS
Singular 0.008
VERBS
Base -0.057
Plural 0.002 Past tense 0.041
Proper, singular -0.041 Present participle -0.089
Proper, plural 0.091 Singular, present -0.031
ADJECTIVES
General 0.002 Third person
0.026
Comparative 0.058 singular, present
Superlative -0.164 Modal -0.063
PREPOSITIONS General 0.064
ADVERBS
General 0.001
DETERMINERS General 0.009 Comparative -0.035
COORD. CONJ. General 0.094
PRONOUNS
Personal -0.098
VERBS Past participle 0.053 Possessive -0.303
ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017
Table 4: Average feature weights learned by POSSVM. Based on work by Rayson et al (2001), we expect weights on
the left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptive
opinions). Boldface entries are at odds with these expectations. We report average feature weights of unit-normalized
weight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds.
genre identification baseline just discussed. Specifi-
cally, the psycholinguistic approach (LIWCSVM) pro-
posed in Section 4.2 performs 3.8% more accurately
(one-tailed sign test p = 0.02), and the standard text
categorization approach proposed in Section 4.3 per-
forms between 14.6% and 16.6% more accurately.
However, best performance overall is achieved by
combining features from these two approaches. Par-
ticularly, the combined model LIWC+BIGRAMS+SVM
is 89.8% accurate at detecting deceptive opinion
spam.19
Surprisingly, models trained only on
UNIGRAMS?the simplest n-gram feature set?
outperform all non?text-categorization approaches,
and models trained on BIGRAMS+ perform even
better (one-tailed sign test p = 0.07). This suggests
that a universal set of keyword-based deception
cues (e.g., LIWC) is not the best approach to de-
tecting deception, and a context-sensitive approach
(e.g., BIGRAMS+) might be necessary to achieve
state-of-the-art deception detection performance.
To better understand the models learned by these
automated approaches, we report in Table 5 the top
15 highest weighted features for each class (truthful
and deceptive) as learned by LIWC+BIGRAMS+SVM
and LIWCSVM. In agreement with theories of reality
monitoring (Johnson and Raye, 1981), we observe
that truthful opinions tend to include more sensorial
and concrete language than deceptive opinions; in
19The result is not significantly better than BIGRAMS+SVM.
LIWC+BIGRAMS+SVM LIWCSVM
TRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE
- chicago hear i
... my number family
on hotel allpunct perspron
location , and negemo see
) luxury dash pronoun
allpunctLIWC experience exclusive leisure
floor hilton we exclampunct
( business sexual sixletters
the hotel vacation period posemo
bathroom i otherpunct comma
small spa space cause
helpful looking human auxverb
$ while past future
hotel . husband inhibition perceptual
other my husband assent feel
Table 5: Top 15 highest weighted truthful and deceptive
features learned by LIWC+BIGRAMS+SVM and LIWCSVM.
Ambiguous features are subscripted to indicate the source
of the feature. LIWC features correspond to groups
of keywords as explained in Section 4.2; more details
about LIWC and the LIWC categories are available at
http://liwc.net.
particular, truthful opinions are more specific about
spatial configurations (e.g., small, bathroom, on, lo-
cation). This finding is also supported by recent
work by Vrij et al (2009) suggesting that liars have
considerable difficultly encoding spatial information
into their lies. Accordingly, we observe an increased
focus in deceptive opinions on aspects external to
the hotel being reviewed (e.g., husband, business,
316
vacation).
We also acknowledge several findings that, on the
surface, are in contrast to previous psycholinguistic
studies of deception (Hancock et al, 2008; Newman
et al, 2003). For instance, while deception is often
associated with negative emotion terms, our decep-
tive reviews have more positive and fewer negative
emotion terms. This pattern makes sense when one
considers the goal of our deceivers, namely to create
a positive review (Buller and Burgoon, 1996).
Deception has also previously been associated
with decreased usage of first person singular, an ef-
fect attributed to psychological distancing (Newman
et al, 2003). In contrast, we find increased first
person singular to be among the largest indicators
of deception, which we speculate is due to our de-
ceivers attempting to enhance the credibility of their
reviews by emphasizing their own presence in the
review. Additional work is required, but these find-
ings further suggest the importance of moving be-
yond a universal set of deceptive language features
(e.g., LIWC) by considering both the contextual (e.g.,
BIGRAMS+) and motivational parameters underly-
ing a deception as well.
6 Conclusion and Future Work
In this work we have developed the first large-scale
dataset containing gold-standard deceptive opinion
spam. With it, we have shown that the detection
of deceptive opinion spam is well beyond the ca-
pabilities of human judges, most of whom perform
roughly at-chance. Accordingly, we have introduced
three automated approaches to deceptive opinion
spam detection, based on insights coming from re-
search in computational linguistics and psychology.
We find that while standard n-gram?based text cate-
gorization is the best individual detection approach,
a combination approach using psycholinguistically-
motivated features and n-gram features can perform
slightly better.
Finally, we have made several theoretical con-
tributions. Specifically, our findings suggest the
importance of considering both the context (e.g.,
BIGRAMS+) and motivations underlying a decep-
tion, rather than strictly adhering to a universal set
of deception cues (e.g., LIWC). We have also pre-
sented results based on the feature weights learned
by our classifiers that illustrate the difficulties faced
by liars in encoding spatial information. Lastly, we
have discovered a plausible relationship between de-
ceptive opinion spam and imaginative writing, based
on POS distributional similarities.
Possible directions for future work include an ex-
tended evaluation of the methods proposed in this
work to both negative opinions, as well as opinions
coming from other domains. Many additional ap-
proaches to detecting deceptive opinion spam are
also possible, and a focus on approaches with high
deceptive precision might be useful for production
environments.
Acknowledgments
This work was supported in part by National
Science Foundation Grants BCS-0624277, BCS-
0904822, HSD-0624267, IIS-0968450, and NSCC-
0904822, as well as a gift from Google, and the
Jack Kent Cooke Foundation. We also thank, al-
phabetically, Rachel Boochever, Cristian Danescu-
Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel,
Danielle Kirshenblat, Lillian Lee, Bin Lu, Jack
Newton, Melissa Sackler, Mark Thomas, and Angie
Yoo, as well as members of the Cornell NLP sem-
inar group and the ACL reviewers for their insight-
ful comments, suggestions and advice on various as-
pects of this work.
References
C. Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea. 2010.
Amazon mechanical turk for subjectivity word sense
disambiguation. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazons Mechanical Turk, Los Angeles,
pages 195?203.
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
D.B. Buller and J.K. Burgoon. 1996. Interpersonal
deception theory. Communication Theory, 6(3):203?
242.
W.L. Cade, B.A. Lehman, and A. Olney. 2010. An ex-
ploration of off topic conversation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
317
the North American Chapter of the Association for
Computational Linguistics, pages 669?672. Associa-
tion for Computational Linguistics.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 310?318. Asso-
ciation for Computational Linguistics.
C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg,
and L. Lee. 2009. How opinions are received by on-
line communities: a case study on amazon.com help-
fulness votes. In Proceedings of the 18th international
conference on World wide web, pages 141?150. ACM.
H. Drucker, D. Wu, and V.N. Vapnik. 2002. Support
vector machines for spam categorization. Neural Net-
works, IEEE Transactions on, 10(5):1048?1054.
G. Forman and M. Scholz. 2009. Apples-to-Apples in
Cross-Validation Studies: Pitfalls in Classifier Perfor-
mance Measurement. ACM SIGKDD Explorations,
12(1):49?57.
Z. Gyo?ngyi, H. Garcia-Molina, and J. Pedersen. 2004.
Combating web spam with trustrank. In Proceedings
of the Thirtieth international conference on Very large
data bases-Volume 30, pages 576?587. VLDB Endow-
ment.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1?23.
J. Jansen. 2010. Online product research. Pew Internet
& American Life Project Report.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230. ACM.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
Machine Learning: ECML-98, pages 137?142.
T. Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in kernel
methods, page 184. MIT Press.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67?85.
S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.
2006. Automatically assessing review helpfulness.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 423?
430. Association for Computational Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers us-
ing rating behaviors. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 939?948. ACM.
S.W. Litvin, R.E. Goldsmith, and B. Pan. 2008. Elec-
tronic word-of-mouth in hospitality and tourism man-
agement. Tourism management, 29(3):458?468.
F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore.
2007. Using linguistic cues for the automatic recogni-
tion of personality in conversation and text. Journal of
Artificial Intelligence Research, 30(1):457?500.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309?312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
2006. Detecting spam web pages through content
analysis. In Proceedings of the 15th international con-
ference on World Wide Web, pages 83?92. ACM.
M.P. O?Mahony and B. Smyth. 2009. Learning to rec-
ommend helpful hotel reviews. In Proceedings of
the third ACM conference on Recommender systems,
pages 305?308. ACM.
F. Peng and D. Schuurmans. 2003. Combining naive
Bayes and n-gram language models for text classifica-
tion. Advances in Information Retrieval, pages 547?
547.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX, LIWC.
Net.
N. Quadrianto, A.J. Smola, T.S. Caetano, and Q.V.
Le. 2009. Estimating labels from label proportions.
The Journal of Machine Learning Research, 10:2349?
2374.
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295?
306.
R.A. Rigby and D.M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape. Jour-
nal of the Royal Statistical Society: Series C (Applied
Statistics), 54(3):507?554.
318
F. Sebastiani. 2002. Machine learning in automated
text categorization. ACM computing surveys (CSUR),
34(1):1?47.
M.A?. Serrano, A. Flammini, and F. Menczer. 2009.
Modeling statistical properties of written text. PloS
one, 4(4):5372.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing, volume 3, pages 901?
904. Citeseer.
C. Toma and J.T. Hancock. In Press. What Lies Beneath:
The Linguistic Traces of Deception in Online Dating
Profiles. Journal of Communication.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues
to deception and ability to detect lies as a function
of police interview styles. Law and human behavior,
31(5):499?518.
A. Vrij, S. Leal, P.A. Granhag, S. Mann, R.P. Fisher,
J. Hillman, and K. Sperry. 2009. Outsmarting the
liars: The benefit of asking unanticipated questions.
Law and human behavior, 33(2):159?166.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
W. Weerkamp and M. De Rijke. 2008. Credibility im-
proves topical blog post retrieval. ACL-08: HLT,
pages 923?931.
M. Weimer, I. Gurevych, and M. Mu?hlha?user. 2007. Au-
tomatically assessing the post quality in online discus-
sions on software. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 125?128. Association
for Computational Linguistics.
G. Wu, D. Greene, B. Smyth, and P. Cunningham. 2010.
Distortion as a validation criterion in the identification
of suspicious reviews. Technical report, UCD-CSI-
2010-04, University College Dublin.
K.H. Yoo and U. Gretzel. 2009. Comparison of De-
ceptive and Truthful Travel Reviews. Information and
Communication Technologies in Tourism 2009, pages
37?47.
L. Zhou, J.K. Burgoon, D.P. Twitchell, T. Qin, and J.F.
Nunamaker Jr. 2004. A comparison of classifica-
tion methods for predicting deception in computer-
mediated communication. Journal of Management In-
formation Systems, 20(4):139?166.
L. Zhou, Y. Shi, and D. Zhang. 2008. A Statistical Lan-
guage Modeling Approach to Online Deception De-
tection. IEEE Transactions on Knowledge and Data
Engineering, 20(8):1077?1081.
319
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 23?30,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
In Search of a Gold Standard in Studies of Deception
Stephanie Gokhman1, Jeff Hancock1,3, Poornima Prabhu2, Myle Ott2, Claire Cardie2,3
Departments of Communication1, Computer Science2, and Information Science3
Cornell University, Ithaca, NY 14853
{sbg94,jth34,pmp67,mao37,ctc9}@cornell.edu
Abstract
In this study, we explore several popular
techniques for obtaining corpora for decep-
tion research. Through a survey of tra-
ditional as well as non-gold standard cre-
ation approaches, we identify advantages
and limitations of these techniques for web-
based deception detection and offer crowd-
sourcing as a novel avenue toward achiev-
ing a gold standard corpus. Through an in-
depth case study of online hotel reviews,
we demonstrate the implementation of this
crowdsourcing technique and illustrate its
applicability to a broad array of online re-
views.
1 Introduction
Leading deception researchers have recently ar-
gued that verbal cues are the most promising indi-
cators for detecting deception (Vrij, 2008) while
lamenting the fact that the majority of previous
research has focused on nonverbal cues. At the
same time, increasing amounts of language are
being digitized and stored on computers and the
Internet ? from email, Twitter and online dating
profiles to legal testimony and corporate commu-
nication. With the recent advances in natural lan-
guage processing that have enhanced our ability
to analyze language, researchers now have an op-
portunity to similarly advance our understanding
of deception.
One of the crucial components of this enter-
prise, as recognized by the call for papers for the
present workshop, is the need to develop corpora
for developing and testing models of deception.
To date there has not been any systematic ap-
proach for corpus creation within the deception
field. In the present study, we first provide an
overview of traditional approaches for this task
(Section 2) and discuss recent deception detec-
tion methods that rely on non-gold standard cor-
pora (Section 3). Section 4 introduces novel ap-
proaches for corpus creation that employ crowd-
sourcing and argues that these have several ad-
vantages over traditional and non-gold standard
approaches. Finally, we describe an in-depth
case study of how these techniques can be im-
plemented to study deceptive online hotel reviews
(Section 5).
2 Traditional Approaches
The deception literature involves a number of
widely used traditional methods for gathering
deceptive and truthful statements. We classify
these according to whether they are sanctioned,
in which the experimenter supplies instructions to
individuals to lie or not lie, or unsanctioned ap-
proaches, in which the participant lies of his or
her own accord.
2.1 Sanctioned Deception
The vast majority of studies examining deception
employ some form of the sanctioned lie method.
A common example is recruiting participants for a
study on deception and randomly assigning them
to a lie or truth condition. A classic example of
this kind of procedure is the original study by Ek-
man and Friesen (1969), in which nurses were
required to watch pleasant or highly disturbing
movie clips. The nurses were instructed to indi-
cate that they were watching a pleasing movie,
which required the nurses watching the disturbing
clips to lie about their current emotional state.
In another example, Newman et. al. (2003) ask
23
participants about their beliefs concerning a given
topic, such as abortion, and then instruct partici-
pants to convince a partner that they hold the op-
posite belief.
Another form of sanctioned deception is to in-
struct participants to engage in some form of
mock crime and then ask them to lie about it. For
example, in one study (Porter and Yuille, 1996),
participants were asked to take an item, such as
a wallet, from a room and then lie about it after-
wards. The mock crime approach improves the
ecological validity of the deception, and makes it
the case that the person actually did in fact act a
certain way that they then must deny.
2.1.1 Advantages and Limitations
The advantages are obvious for these sanc-
tioned lie approaches. The researcher has large
degrees of experimental control over what the par-
ticipant lies about and when, which allows for
careful comparison across the deceptive and non-
deceptive accounts. Another advantage is the rel-
ative ease of instructing participants to lie vs. try-
ing to identify actual (but unknown) lies in a dia-
logue.
The limitations for this approach, however, are
also obvious. In asking participants to lie, the
researcher is essentially giving permission to the
person to lie. This should affect the partici-
pant?s behavior as the lie is being conducted at
the behest of a power figure, essentially acting
out their deception. Indeed, a number of schol-
ars have pointed out this problem (Frank and Ek-
man, 1997), and have suggested that unless high
stakes are employed the paradigm produces data
that does not replicate any typical lying situation.
High stakes refers to the potential for punishment
if the lie is detected or reward if the lie goes unde-
tected. Perhaps because of the difficulty in creat-
ing high-stakes deception scenarios, to date there
are few corpora involving high-stakes lies.
2.2 Unsanctioned Deception
Unsanctioned lies are those that are told without
any explicit instruction or permission from the re-
searcher. These kinds of lies have been collected
in a number of ways.
2.2.1 Diary studies and surveys
Two related methods for collecting information
about unsanctioned lies are diary studies and sur-
vey studies. In diary studies participants are asked
on an ongoing basis (e.g., every night) to recall
lies that they told over a given period (e.g., a day,
a week) (DePaulo et al, 1996; Hancock et al,
2004). Similarly, recent studies have asked par-
ticipants in national surveys how often they have
lied in the last 24 hours (Serota et al, 2010).
One important feature of these approaches is
that the lies have already taken place, and thus
they do not share the same limitations as sanc-
tioned lies. There are several drawbacks, how-
ever, especially given the current goal to collect
deception corpora. First, both diary studies and
survey approaches require self-reported recall of
deception. Several biases are likely to affect the
results, including under-reporting of deception in
order to reduce embarrassment and difficult-to-
remember deceptions that have occurred over the
time period. More importantly, this kind of ap-
proach does not lend itself to collecting the actual
language of the lie, for incorporation into a cor-
pus: people have a poor memory for conversation
recall (Stafford and Sharkey, 1987).
2.2.2 Retrospective Identification
One method for getting around the memory
limitations for natural discourse is to record the
discourse and ask participants to later identify any
deceptions in their discourse. For instance, one
study (Feldman and Happ, 2002) asked partici-
pants to meet another individual and talk for ten
minutes. After the discussion, participants were
asked to examine the videotape of the discussion
and indicated any times in which they were de-
ceptive. More recently, others have used the ret-
rospective identification technique on mediated
communication, such as SMS, which produces
an automatic record of the conversation that can
be reviewed for deception (Hancock, 2009). Be-
cause this approach preserves a record that the
participant can use to identify the deception, this
technique can generate data for linguistic analy-
sis. However, an important limitation, as with the
diary and survey data, is that the researcher must
assume that the participant is being truthful about
their deception reporting.
2.2.3 Cheating Procedures
The last form of unsanctioned lying involves
incentivizing participants to first cheat on a task
and to then lie when asked about the cheating be-
havior. Levine et al (2010) have recently used
24
this approach, which involved students perform-
ing a trivia quiz. During the quiz, an opportunity
to cheat arises where some of the students will
take the opportunity. At this point, they have not
yet lied, but, after the quiz is over, all students
are asked whether they cheated by an interviewer
who does not know if they cheated or not. While
most of the cheaters admit to cheating, a small
fraction of the cheaters deny cheating. This sub-
set of cheating denials represents real deception.
The advantages to this approach are three-
fold: (1) the deception is unsanctioned, (2) it
does not involve self-report, and (3) the decep-
tions have objective ground-truth. Unfortunately,
these kinds of experiments are extremely effort-
intensive given the number of deceptions pro-
duced. Only a tiny fraction of the participants
typically end up cheating and subsequently lying
about the cheating.
2.2.4 Limitations
While these techniques have been useful in
many psychology experiments, in which assess-
ing deception detection has been the priority
rather than corpus creation, they are not very
feasible when considering obtaining corpora for
large-scale settings, e.g., the web. Furthermore,
the techniques are limited in the kinds of con-
texts that can be created. For instance, in many
cases, e.g., deliberate posting of fake online re-
views, subjects can be both highly incentivized
to lie and highly concerned with getting caught.
One could imagine surveying hotel owners as to
whether they have ever posted a fake review?but
it would seem unlikely that any owner would ever
admit to having done so.
3 Non-gold Standard Approaches
Recently, alternative approaches have emerged to
study deception in the absence of gold standard
deceptive data. These approaches can typically
be broken up into three distinct types. In Sec-
tion 3.1, we discuss approaches to deception cor-
pus creation that rely on the manual annotation of
deceptive instances in the data. In Section 3.2, we
discuss approaches that rely on heuristic methods
for deriving approximate, but non-gold standard
deception labels. In Section 3.3, we discuss a re-
cent approach that uses assumptions about the ef-
fects of deception to identify examples of decep-
tion in the data. We will refer to the latter as the
unlabeled approach to deception corpus creation.
3.1 Manual Annotations of Deception
In Section 2.2, we discussed diary and self-report
methods of obtaining gold standard labels of de-
ception. Recently, work studying deceptive (fake)
online reviews has suggested using manual anno-
tations of deception, given by third-party human
judges.
Lim et al (2010) study deceptive product re-
views found on Amazon.com. They develop a
sophisticated software interface for manually la-
beling reviews as deceptive or truthful. The inter-
face allows annotators to view all of each user?s
reviews, ranked according to dimensions poten-
tially of importance to identifying deception, e.g.,
whether the review is duplicated, whether the re-
viewer has authored many reviews in a single day
with identical high or low ratings, etc.
Wu et al (2010a) also study deceptive online
reviews of TripAdvisor hotels, manually labeling
a set of reviews according to ?suspiciousness.?
This manually labeled dataset is then used to val-
idate eight proposed characteristics of deceptive
hotels. The proposed characteristics include fea-
tures based on the number of reviews written, e.g.,
by first-time reviewers, as well as the review rat-
ings, especially as they compare to other ratings
of the same hotel.
Li et al (2011) study deceptive product reviews
found on Epinions.com. Based on user-provided
helpfulness ratings, they first draw a subsample of
reviews such that the majority are considered to
be unhelpful. They then manually label this sub-
sample according to whether or not each review
seems to be fake.
3.1.1 Limitations
Manual annotation of deception is problematic
for a number of reasons. First, many of the same
challenges that face manual annotation efforts in
other domains also applies to annotations of de-
ception. For example, manual annotations can be
expensive to obtain, especially in large-scale set-
tings, e.g., the web.
Most seriously however, is that human abil-
ity to detect deception is notoriously poor (Bond
and DePaulo, 2006). Indeed, recent studies have
confirmed that human agreement and deception
detection performance is often no better than
chance (Ott et al, 2011); this is especially the
25
case when considering the overtrusting nature of
most human judges, a phenomenon referred to in
the psychological deception literature as a truth
bias (Vrij, 2008).
3.2 Heuristically Labeled
Work by Jindal and Liu (2008) studying the char-
acteristics of untruthful (deceptive) Amazon.com
reviews, has instead developed an approach for
heuristically assigning approximate labels of de-
ceptiveness, based on a set of assumptions spe-
cific to their domain. In particular, after re-
moving certain types of irrelevant ?reviews,? e.g.,
questions, advertisements, etc., they determine
whether each review has been duplicated, i.e.,
whether the review?s text heavily overlaps with
the text of other reviews in the same corpus. Then,
they simply label all discovered duplicate reviews
as untruthful.
Heuristic labeling approaches do not produce a
true gold-standard corpus, but for some domains
may offer an acceptable approximation. How-
ever, as with other non-gold standard approaches,
certain behaviors might have other causes, e.g.,
duplication could be accidental, and just because
something is duplicated does not make the origi-
nal (first) post deceptive. Indeed, in cases where
the original review is truthful, its duplication is
not a good example of deceptive reviews written
from scratch.
3.3 Unlabeled
Rather than develop heuristic labeling ap-
proaches, Wu et al (2010b) propose a novel strat-
egy for evaluating hypotheses about deceptive ho-
tel reviews found on TripAdvisor.com, based on
distortions of popularity rankings. Specifically,
they test the Proportion of Positive Singletons and
Concentration of Positive Singletons hypotheses
of Wu et al (2010a) (Section 3.1), but instead of
using manually-derived labels they evaluate their
hypotheses by the corresponding (distortion) ef-
fect they have on the hotel rankings.
Unlabeled approaches rely on assumptions
about the effects of the deception. For example,
the approach utilized by Wu et al (2010b) observ-
ing distortion effects on hotel rankings, relies on
the assumption that the goal of deceivers in the
online hotel review setting is to increase a hotel?s
ranking. And while this may be true for positive
hotel reviews, it is likely to be very untrue for fake
negative reviews intended to defame a competitor.
Indeed, great care must be taken in making such
assumptions in unlabeled approaches to studies of
deception.
4 Crowdsourcing Approaches
As with traditional sanctioned deception ap-
proaches (see Section 2.1), one way of obtain-
ing gold standard labels is to simply create gold
standard deceptive content. Crowdsourcing plat-
forms are a particularly compelling space to pro-
duce such deceptive content: they connect people
who request the completion of small tasks with
workers who will carry out the tasks. Crowd-
sourcing platforms that solicit small copywriting
tasks include Clickworker, Amazon?s Mechanical
Turk, Fiverr, and Worth1000. Craigslist, while not
a crowdsourcing platform, also promotes similar
solicitations for writing. In the case of fake online
reviews (see Section 5), and by leveraging plat-
forms such as Mechanical Turk, we can often gen-
erate gold standard deceptive content in contexts
very similar to those observed in practice.
Mihalcea and Strapparava (2009) were among
the first to use Mechanical Turk to collect decep-
tive and truthful opinions ? personal stances on
issues such as abortion and the death penalty. In
particular, for a given topic, they solicited one
truthful and one deceptive stance from each Me-
chanical Turk participant.
Ott et al (2011) have also used Mechanical
Turk to produce gold standard deceptive content.
In particular, they use Mechanical Turk to gener-
ate a dataset of 400 positive (5-star), gold stan-
dard deceptive hotel reviews. These were com-
bined with 400 (positive) truthful reviews cov-
ering the same set of hotels and used to train a
learning-based classifier that could distinguish de-
ceptive vs. truthful positive reviews at 90% accu-
racy levels. The truthful reviews were mined di-
rectly from a well-known hotel review site. The
Ott et al (2011) approach for collecting the gold
standard deceptive reviews is the subject of the
case study below.
5 Case Study: Crowdsourcing Deceptive
Reviews
To illustrate in more detail how crowdsourcing
techniques can be implemented to create gold
standard data sets for the study of deception, we
26
draw from the Ott et al (2011) approach that
crowdsources the collection of deceptive positive
hotel reviews using Mechanical Turk. The key
assumptions of the approach are as follows:
? We desire a balanced data set, i.e., equal
numbers of truthful and deceptive reviews.
This is so that statistical analyses of the data
set won?t be biased towards either type of re-
view.
? The truthful and deceptive reviews should
cover the same set of entities. If the two
sets of reviews cover different entities (e.g.,
different hotels), then the language that dis-
tinguishes truthful from deceptive reviews
might be attributed to the differing entities
under discussion rather than to the legiti-
macy of the review.
? The resulting data set should be of a rea-
sonable size. Ott et al (2011) found that
a dataset of 800 total reviews (400 truthful,
400 deceptive) was adequate for their goal
of training a learning-based classifier.
? The truthful and deceptive reviews should
exhibit the same valence, i.e., sentiment.
If the truthful reviews gathered from the on-
line site are positive reviews, the deceptive
reviews should be positive as well.
? More generally, the deceptive reviews
should be generated under the same ba-
sic guidelines as governs the generation
of truthful reviews. E.g., they should have
the same length constraints, the same quality
constraints, etc.
Step 1: Identify the set of entities to be cov-
ered in the truthful reviews. In order to de-
fine a set of desirable reviews, a master database,
provided by the review site itself, is mined to
identify the most commented (most popular) en-
tities. These are a good source of truthful re-
views. In particular, previous work has hypoth-
esized that popular offerings are less likely to
be targeted by spam (Jindal and Liu, 2008), and
therefore reviews for those entities are less likely
to be deceptive?enabling those reviews to later
comprise the truthful review corpus. The review
site database typically divides the entity set into
subcategories that differ across contexts: in the
case of hotel reviews the subcategories might re-
fer to cities, or in the case of doctor reviews
subcategories might refer to specialties. To en-
sure that enough reviews of the entity can be col-
lected, it may be important to select subcategories
that themselves are popular. The study of Ott et
al. (2011), for example, focused on reviews of ho-
tels in Chicago, IL, gathering positive (i.e., 5-star)
reviews for the 20 most popular hotels.
Step 2: Develop the crowdsourcing prompt.
Once a set of entities has been identified for the
deceptive reviews (Step 1), the prompt for Me-
chanical Turk is developed. This begins with a
survey of other solicitations for reviews within the
same subcategory through searching Mechanical
Turk, Craigslist, and other online resources. Us-
ing those solicitations as reference, a scenario can
then be developed that will be used in the prompt
to achieve the appropriate (in our case, positive)
valence. The result is a prompt that mimics the
vocabulary and tone that ?Turkers? (i.e., the work-
ers on Mechanical Turk) may find familiar and de-
sirable.
For example, the prompt of Ott et al (2011)
read: Imagine you work for the marketing depart-
ment of a hotel. Your boss asks you to write a fake
review for the hotel (as if you were a customer) to
be posted on a travel review website. The review
needs to sound realistic and portray the hotel in
a positive light. Look at their website if you are
not familiar with the hotel. (A link to the website
was provided.)
Step 3: Attach appropriate warnings to the
crowdsource solicitation. It is important that
warnings are attached to the solicitation to avoid
gathering (and paying for) reviews that would
invalidate the review set for the research. For
example, because each review should be written
by a different person, the warning might disallow
coders from performing multiple reviews; forbid
any form of plagiarism; require that reviews be
?on topic,? coherent, etc. Finally, the prompt
may inform the Turker that this exercise is for
academic purposes only and will not be posted
online, however, if such a notice is presented
before the review is written and submitted, the
resulting lie may be overly sanctioned.
27
Step 4: Incorporate into the solicitation a
means for gathering additional data. Append
to the end of the solicitation some mechanism
(e.g., Mechanical Turk allows for a series of ra-
dio buttons) to input basic information about age,
gender, or education of the coder. This allows for
post-hoc understanding of the demographic of the
participating Turkers. Ott et al (2011) also sup-
ply a space for comments by the workers, with an
added incentive of a potential bonus for particu-
larly helpful comments. Ott et al (2011) found
this last step critical to the iterative process for
providing insights from coders on inconsistencies,
technical difficulties, and other unforeseen prob-
lems that arise in the piloting phase.
Step 5: Gather the deceptive reviews in
batches. The solicitation is then published in a
small pilot test batch. In Ott et al (2011), each pi-
lot requested ten (10) reviews from unique work-
ers. Once the pilot run is complete, the results
are evaluated, with particular attention to the com-
ments, and is then iterated upon in small batches
of 10 until there are no technical complaints and
the results are of desired experiment quality.
Once this quality is achieved, the solicitation is
then published as a full run, generating 400 re-
views by unique workers. The results are man-
ually evaluated and cleaned to ensure all reviews
are valid, then filtered for plagiarism. The result-
ing set of gold standard online deceptive spam is
then used to train the algorithm for deceptive pos-
itive reviews.
5.1 Handling Plagiarism
One of the main challenges facing crowdsourced
deceptive content is identifying plagiarism. For
example, when a worker on Mechanical Turk
is asked to write a deceptive hotel review, that
worker may copy an available review from var-
ious sources on the Internet (e.g., TripAdvisor).
These plagiarized reviews lead to flaws in our
gold standard. Hence there arises a need to detect
such reviews and separate them from the entire
review set.
One way to address this challenge is to do
a manual check of the reviews, one-by-one, us-
ing online plagiarism detection web services, e.g.,
plagiarisma.net or searchenginereports.net. The
manual process is taxing, especially when there
are reviews in large numbers (as large as 400) to
be processed. This illustrates a need to have a
tool which automates the detection of plagiarized
content in Turker submissions. There are several
plagiarism detection softwares which are widely
available in the market. Most of them maintain
a database of content against which to check for
plagiarism. The input content is checked against
these databases and the content is stored in the
same database at the end of the process. Such
tools are an appropriate fit for detecting plagia-
rized content in term papers, course assignments,
journals etc. However, online reviews define a
separate need which checks for plagiarism against
the content available on the web. Hence the avail-
able software offerings are not adequate.
We implemented a command line tool using the
Yahoo! BOSS API, which is used to query sen-
tences on the web. Each of the review files is
parsed to read as individual sentences. Each sen-
tence is passed as a query input to the API. We
introduce the parameters, n and m, defined as:
1. Any sentence which is greater than n words
is considered to be a ?long sentence? in the
application usage. If the sentence is a ?long
sentence? and the Yahoo! BOSS API returns
no result, we query again using the first n
words of the sentence. Here n is a config-
urable parameter, and in our experiments we
configured n = 10.
2. A sentence that is commonly used on the
web can return many matches, even if it was
not plagiarized. Thus, we introduce another
parameter, m, such that if the number of
search results returned by the Yahoo! BOSS
API is greater than m, then the sentence is
considered common and is ignored. Our ob-
servations indicate that such frequently used
sentences are likely to be short. For exam-
ple: ?We are tired,? ?No room,? etc. For our
usage we configured m = 30.
We consider a sentence to be plagiarized if the
total number of results returned by the Yahoo!
BOSS API is less than m. Hence each sentence
is assigned a score as follows:
? If the total number of results is greater than
m: assign a score of 0
? If the total number of results is less than or
equal to m: assign a score of 1
28
We then divide the sum of the sentence scores in a
review by the total number of sentences to obtain
the ratio of the number of matches to total num-
ber of sentences. We use this ratio to determine
whether or not a review was plagiarized.
6 Discussion and Conclusion
We have discussed several techniques for creating
and labeling deceptive content, including tradi-
tional, non-gold standard, and crowdsourced ap-
proaches. We have also given an illustrative in-
depth look at how one might use crowdsourcing
services such as Mechanical Turk to solicit decep-
tive hotel reviews.
While we argue that the crowdsourcing ap-
proach to creating deceptive statements has
tremendous potential, there remain a number of
important limitations, some shared by the pre-
vious traditional methods laid out above. First,
workers are given ?permission? to lie, so these
lies are sanctioned and have the same concerns
as the traditional sanctioned methods, including
the concern that the workers are just play-acting
rather than lying. Other unique limitations in-
clude the current state of knowledge about work-
ers. In a laboratory setting we can fairly tightly
measure and control for gender, race, and even
socioeconomic status, but this is not the case for
the Amazon Turkers, who potentially make up a
much more diverse population.
Despite these issues we believe that the ap-
proach has much to offer. First, and perhaps most
importantly, the deceptions are being solicited in
exactly the manner real-world deceptions are ini-
tiated. This is important in that the deception task,
though sanctioned, is precisely the same task that
a real-world deceiver might use, e.g., to collect
fake hotel reviews for themselves. Second, this
approach is extremely cost effective in terms of
the time and finances required to create custom
deception settings that fit a specific context. Here
we looked at creating fake hotel reviews, but we
can easily apply this approach to other types of
reviews, including reviews of medical profession-
als, restaurants, and products.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant NSCC-0904913, and the
Jack Kent Cooke Foundation. We also thank the
EACL reviewers for their insightful comments,
suggestions and advice on various aspects of this
work.
References
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychol-
ogy Review, 10(3):214.
B.M. DePaulo, D.A. Kashy, S.E. Kirkendol, M.M.
Wyer, and J.A. Epstein. 1996. Lying in everyday
life. Journal of personality and social psychology,
70(5):979.
P. Ekman and W. V. Friesen. 1969. Nonverbal Leak-
age And Clues To Deception, volume 32.
Forrest J. A. Feldman, R. S. and B. R. Happ. 2002.
Self-presentation and verbal deception: Do self-
presenters lie more? Basic and Applied Social Psy-
chology, 24:163?170.
M.G. Frank and P. Ekman. 1997. The Ability To De-
tect Deceit Generalizes Across Different Types of
High-Stake Lies. Journal of Personality and Social
Psychology, 72:1429?1439.
J.T. Hancock, J. Thom-Santelli, and T. Ritchie. 2004.
Deception and design: The impact of communi-
cation technology on lying behavior. In Proceed-
ings of the SIGCHI conference on Human factors in
computing systems, pages 129?134. ACM.
J.T. Hancock. 2009. Digital Deception: The Practice
of Lying in the Digital Age. Deception: Methods,
Contexts and Consequences, pages 109?120.
N. Jindal and B. Liu. 2008. Opinion spam and analy-
sis. In Proceedings of the international conference
on Web search and web data mining, pages 219?
230. ACM.
Kim R. K. Levine, T. R. and J. P. Blair. 2010.
(In)accuracy at detecting true and false confessions
and denials: An initial test of a projected motive
model of veracity judgments. Human Communica-
tion Research, 36:81?101.
F. Li, M. Huang, Y. Yang, and X. Zhu. 2011. Learning
to identify review spam. In Twenty-Second Interna-
tional Joint Conference on Artificial Intelligence.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers
using rating behaviors. In Proceedings of the 19th
ACM international conference on Information and
knowledge management, pages 939?948. ACM.
R. Mihalcea and C. Strapparava. 2009. The lie de-
tector: Explorations in the automatic recognition of
deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309?
312. Association for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting decep-
tion from linguistic styles. Personality and Social
Psychology Bulletin, 29(5):665.
29
M. Ott, Y. Choi, C. Cardie, and J.T. Hancock. 2011.
Finding deceptive opinion spam by any stretch of
the imagination. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies-Volume 1,
pages 309?319. Association for Computational Lin-
guistics.
S. Porter and J.C. Yuille. 1996. The language of de-
ceit: An investigation of the verbal clues to decep-
tion in the interrogation context. Law and Human
Behavior, 20:443?458.
K.B. Serota, T.R. Levine, and F.J. Boster. 2010.
The prevalence of lying in america: Three studies
of self-reported lies. Human Communication Re-
search, 36(1):2?25.
Burggraf C. S. Stafford, L. and W.F. Sharkey. 1987.
Conversational Memory The Effects of Time, Re-
call, Mode, and Memory Expectancies on Remem-
brances of Natural Conversations. Human Commu-
nication Research, 14:203?229.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
G. Wu, D. Greene, B. Smyth, and P. Cunningham.
2010a. Distortion as a validation criterion in the
identification of suspicious reviews. In Proceedings
of the First Workshop on Social Media Analytics,
pages 10?13. ACM.
G. Wu, D. Greene, B. Smyth, and P. Cunningham.
2010b. Distortion as a validation criterion in
the identification of suspicious reviews. Techni-
cal report, UCD-CSI-2010-04, University College
Dublin.
30
