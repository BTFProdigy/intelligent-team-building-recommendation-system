Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 530?538,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semantic-based Estimation of Term Informativeness
Kirill Kireyev
University of Colorado ? Boulder
kireyev@colorado.edu
Abstract
The idea that some words carry more semantic 
content  than  others,  has  led  to  the  notion  of 
term specificity,  or informativeness. Computa-
tional  estimation of  this  quantity  is  important 
for various applications such as information re-
trieval. We propose a new method of comput-
ing term specificity, based on modeling the rate 
of learning of word meaning in Latent Semantic 
Analysis  (LSA).  We analyze  the performance 
of this method both qualitatively and quantitat-
ively and  demonstrate  that  it  shows excellent 
performance compared to existing methods on 
a  broad  range  of  tests.  We  also  demonstrate 
how it can be used to improve existing applica-
tions  in  information  retrieval  and  summariza-
tion.
1 Introduction
The idea that some words carry more semantic 
content than others has been occurring in various 
literature in linguistics, psychology and computer 
science for some time. The intuitive notion of spe-
cificity has long existed before it was formalized; 
consider, for example, the distinction between the 
more general word ?beverage? and more specific 
terms   ?tea?, ?coffee? and ?cocoa? made by  Sp?r-
ck-Jones (1973). Another informal mention of spe-
cificity is mentioned by Gorman (1961):
A word may be ?abstract? and either general  
or specific, or ?concrete? and either general or  
specific.
where it  is contrasted with another psycholinguistic 
property of concreteness, which is generally defined 
as ?the  extent to which the word's referent can be 
touched or felt? (Reilly et al, 2007). 
The field of information retrieval has attracted 
greater  attention  to  the  computational  estimation 
and  applications  of  term specificity.  It  has  been 
noted that words with higher specificity, or inform-
ation content, deserve to be weighted more heavily 
when  matching  documents  with  queries,  since 
these words play a greater importance in character-
izing what a query or a document is about. By con-
trast,  stopwords,  words  that  contribute  the  least 
amount  of  semantic  content,  are  often  down-
weighted  or  removed  altogether  (see  (Lo  et  al., 
2005), for example).
In addition to IR, term specificity, or informat-
iveness,  has been shown useful  in other applica-
tions,  such as  Named  Entity Tagging (Rennie  et 
al.,  2005),  creating  back-of-the-book  glossaries 
(Csomai  et al,  2007),  and extractive summariza-
tion (Kireyev, 2008).
A related notion of  communication density  has 
been introduced by Gorman et al (2003) in team 
communication analysis, to measure the extent to 
which  a  team conveys  information  in  a  concise 
manner, or, in other words, the rate of meaningful 
discourse, defined by the ratio of  meaningfulness 
to number  of  words spoken.  The  meaningfulness 
described here should not  be confused with psy-
cholinguistic  quality  of  meaningfulness  as  de-
scribed by Toglia and Battig (1978), which is the 
degree to  which a  word is  associated with other 
words.
In this paper we consider the terms  specificity, 
informativeness and  information content of words 
to mean the same thing. A precise formulation or 
analysis of important qualitative characteristics of 
these concepts has not been performed in previous 
literature; we hope to make some progress in that 
direction in this paper. 
Our main goal is to introduce a new method of 
computing word specificity based on the rate and 
strength of semantic associations between words, 
as modeled by Latent Semantic Analysis (LSA).
2 Previous Approaches
To date, most of the known approaches to estim-
ating  term  informativeness  have  relied  on  fre-
quency-based methods. 
530
A very basic, yet surprisingly effective approach 
to measuring term informativeness is its frequency 
of occurrence in a large representative corpus of 
language.  Sp?rck Jones (1973) defines  IDF or  in-
verse document frequency, which is determined by 
the  probability  of  occurrence  of  documents  con-
taining a particular word:
IDF ?w ?=?log2?df w /D?
where  D is the total number of documents in the 
corpus.   The assumption behind it is that low fre-
quency words tend to be rich in content, and vice 
versa.
Church and Gale (1995) correctly note that this 
measure is fundamentally different from collection 
frequency fw,  (the total number of times the word 
type occurs in the corpus) or  its transformations, 
despite  the  fact  that  the  two  measures  appear 
highly correlated. In fact, what is particularly of in-
terest are the words for which these two quantities 
deviate the most. This happens most dramatically 
for  most  informative,  or  content  words,  such  as 
?boycott? (Church, 1995a). These words happen to 
exhibit ?bursty? behavior, where they tend to ap-
pear multiple times but in fewer documents,  thus 
having  fw  > dfw. In  contrast,  less  content-loaded 
words like ?somewhat? tend to occur on average 
once in documents,  and thus have similar  values 
for collection and document frequencies ( fw ? dfw ).  
As a result, more informative words can be less ac-
curately  estimated  by  the  Poisson  distribution, 
which is  based on the simplistic assumption that 
the expected number of occurrences of word in a 
document can be estimated by its total number of 
occurrences in the corpus.
Most prominent statistical measures of term in-
formativeness  rely  on  quantifying  this  notion  of 
deviation  from  the  Poisson  distribution.  If  the 
mean expected word rate is:
?tw= f wD
then the variance metric can be defined as:
variance ?w?= 1D?1?d=1
D
? tdw??t w?2
where  tdw is  the actual  number  of  occurrences of 
term  w in document  d.  The idea is that  a higher 
variance would indicate greater deviation from ex-
pected  frequency  of  occurrence  in  a  document, 
which  is  assumed  to  be  higher  for  informative 
words. 
Another  measure,  suggested by  Church  and 
Gale (1995a) is burstiness which attempts to com-
pare collection frequency and document frequency 
directly:
burstiness ?w?= ?twdf w /D=
f w
df w
Church and Gale also noted that nearly all words 
have IDF scores that are larger than what one 
would expect according to an independence-based 
model such as the Poisson. They note that interest-
ing or informative words tend to have the largest 
deviations from what would be expected. They 
thus introduce the notion of residual IDF which 
measures exactly this deviation:
residualIDF ?w?= IDF ?w ??log2 ?1?e??t ?
Papineni (2001) introduces the notion of gain:
gain ?w?= df wD ? df wD ?1?log ? df wD ??
This  measure  tends  to  give low weights  to  very 
high- and very low- frequency words.
Most closely related to our work is the notion of 
meaningfulness in (Gorman et al2003), computed 
as the LSA vector length. We will discuss it further 
in the subsequent sections, and show that a small 
but crucial modification to this quantity gives the 
best results.
3 Using Latent Semantic Analysis for Ap-
proximating Term Informativeness
3.1 Latent Semantic Analysis
Latent Semantic Analysis  (LSA) is a language 
model  that  represents  semantic  word meaning as 
vectors  in  high-dimensional  space.  Word  vectors 
are positioned in such a way that semantically-re-
lated words vectors point in similar directions or 
have a smaller angle / higher cosine between them. 
The representation is  derived in  an unsupervised 
manner, by observing occurrence patterns of words 
in a large corpus of natural language documents. 
Singular  Value  Decomposition  on  the  matrix  of 
word/document occurrence counts is used to derive 
the optimal set of dimensions of the space in which 
531
all of the words can be represented as vectors. The 
number of dimensions is then artificially reduced 
to a smaller number (typically around 300) of most 
important  dimensions,  which  has  the  effect  of 
smoothing  out  incidental  relationships  and  pre-
serving significant ones between words. 
The  resulting  geometric  space  allows  for 
straightforward  representation  of  meaning  of 
words  and/or  documents;  the  latter  are  simply  a 
weighted  geometric  composition  of  constituent 
word vectors. Similarity in meaning between a pair 
of words or documents can be obtained by comput-
ing the cosine between their corresponding vectors. 
For  details  of  LSA,  please  see  (Landauer  et  al., 
2007), and others
3.2 LSA Term Vector Length
Most of the LSA applications focus on compar-
ing semantic similarity between words and/or text, 
using the cosine measure of the angle between the 
corresponding vectors. There is, however, another 
significant characteristic of LSA word vectors be-
sides  their  direction  in  space;  it  is  their  vector 
length. The vector length for words differs signi-
ficantly, as is shown in Table 1.
Word dfw Vector Length
dog 1365 1.3144
green 2067 0.7125
run 2721 0.4788
puppy 127 0.2648
electron 264 0.9009
the 44474 0.0098
Table 1: LSA vector length for some of the words in 
TASA corpus.
The vector length plays a very important role in 
many LSA calculations,  in particular  ? in giving 
relative weights to the word vectors that constitute 
a particular text passage. 
What causes differences in vector lengths? They 
are based roughly on how much information LSA 
learns about a word based on its patterns of occur-
rence in the corpus. Kintsch (2001) writes:
Intuitively, the vector length tells us how much in-
formation LSA has about this vector. [...] Words that 
LSA  knows  a  lot  about  (because  they  appear  fre-
quently in the training corpus[...]) have greater vector 
lengths than words LSA does not know well. Func-
tion words that are used frequently in many different 
contexts have low vector lengths -- LSA knows noth-
ing about them and cannot tell them apart since they 
appear in all contexts.
Essentially, there are two factors that affect vec-
tor length: (1) number of occurrences and (2) the 
consistency of contexts in which the word occurs.
3.3 Deriving Specificity from Vector Length
Based on the observations above we propose a 
new metric of term informativeness, or specificity, 
which we call  LSAspec, which is simply the ratio 
of LSA vector length to the number of documents 
in the LSA training corpus that contain a particular 
word:
LSAspec ?w?=??w?/df w
The value can be interpreted as the rate of vector 
length growth. We argue that more specific, or in-
formative, words have the greatest rate of  vector 
length  growth;  LSA  learns  about  their  meaning 
faster,  with  relatively  fewer  exposures.  To  illus-
trate this concept, let's look at a few examples, that 
were obtained by controlling the number of occur-
rences of a particular word in the LSA training cor-
pus.  The  base  corpus  was  obtained  using  the 
44000-passage  TASA  corpus  with  all  passages 
containing  the  three  words  below  initially  re-
moved.  Each data point on the graph reflects the 
vector  length  of  a  particular  word,  after  training 
LSA on the base corpus plus the specified number 
of  passages  containing  a  particular  word  added 
back. Highly specific words like ?cellulose? gain 
vector length quite quickly compared to a low-spe-
cificity word like ?dismay?.
4 Comparison of Specificity Metrics
Past attempts to examine the merits of various 
existing term informativeness estimation methods 
in the literature thus far has largely involved em-
Illustration 1: Vector lengths for some words vs the 
number of documents containing those words.
10 20 30 40 50
0.00
0.01
0.02
0.03
0.04
0.05
Vector Length
cellulose
classmate
dismay
# Occurences
532
pirical summative evaluations as part of informa-
tion  retrieval  or  named  entity  tagging  systems 
(Rennie  et  al.,  2005).  Here,  we  provide  some 
measures which hopefully provide more illuminat-
ing insights into the various methods.
In all of the tests below we derived the metrics 
(including the LSA space for  LSAspec) from the 
same  corpus  ?  MetaMetrics  2002  corpus,  com-
posed of ~188k passages mostly used in education-
al texts.  No stemming or stopword removal of any 
kind was performed. All word types were conver-
ted  to  lowercase.  We  computed  the  specificity 
score for each of the 174,374 most frequent words 
in the corpus using each of the metrics described 
above:  LSAspec,  IDF,  residualIDF,  burstiness,  
gain and variance. 
4.1 Correlation with Number of Senses
Intuitively,  one  would  expect  more  specific 
words  to have more  precise  meaning,  and there-
fore, generally fewer senses. For example, ?elec-
tron? is a specific physics term that has only one 
sense, whereas ?run? has a very general meaning, 
and thus has over 50 senses in the WordNet data-
base (Miller et al, 1990). There are many excep-
tions to this, of course, but overall, one would ex-
pect a negative correlation between specificity and 
number of senses. 
In this test, we measure the correlation between 
the specificity score of a word by various methods 
and its number of senses in WordNet version 3.0. 
A total of 75,978 words were considered. We use 
Spearman  correlation  coefficient,  since  the  rela-
tionships are likely to be non-linear. 
Metric Corr Metric Corr
LSAspec -0.46 burstiness -0.02
IDF -0.44 variance 0.40
residualIDF -0.03 gain 0.44
Table 2: Correlation of specificity metrics with number  
of senses in WordNet
LSAspec gives the highest  negative correlation 
with number of WordNet senses.
4.2 Correlation with Hypernymy
WordNet organizes concepts into a hypernymy 
tree, where each parent node is a hypernym of the 
child node below it. For example:
substance
element
metal
nickel copper
In general one would expect that for each pair of 
child-parent pairs in the hypernym tree, the child 
will  have greater specificity than the parent1.  We 
examined of  a total  of  14451 of  such hypernym 
word pairs and computed how often the child's in-
formativeness  score,  according  to  each  of  the 
measures,  is  greater  than  its  parent's  (its  hyper-
nym's) score.
Metric Percent Metric Percent
IDF 88.8% burstiness 47.2%
LSAspec 87.7% variance 13.4%
residualIDF 48.8% gain 11.1%
Table 3: Percentage of the time specificity of child ex-
ceeds that of its hypernym in WordNet
4.3 Writing Styles and Levels
One may expect that the specificity of words on 
average would change with texts that are known to 
be of different writing styles and difficulty level. 
To test this hypotheses we extracted texts from the 
TASA  collection  of  educational  materials.  The 
texts are annotated with genre (?Science?, ?Social 
Studies? or ?Language Arts?), and difficulty level 
on the DRP readability scale (Koslin et al, 1987). 
Intuitively,  one would expect to see two patterns 
among these texts:
(1) The specificity of words would generally in-
crease with increasing level of difficulty of texts.
(2)  Informative  (Science)  texts  should  have 
more specific terms than narrative (Language Arts) 
texts;  with Social  Studies somewhere in between 
(McCarthy et al, 2006).
We extracted 100 text  passages for  each com-
bination  of  style  (?Science?,  ?Social  Studies?, 
?Language Arts?) and DRP difficulty level (50, 55, 
60,  65,  70)2,  thus resulting in 15 batches of  100 
passages. For each passage we computed the medi-
an specificity measure of each unique word type in 
1 In practice this is more difficult to determine, since some Word-
Net entries are actually phrases, rather than words (e.g. ?tulip? ? 
?liliaceous plant?  ? ...  ? ?plant?). In such cases we search up 
the tree until we stumble upon a node where the entry (or one of 
the entries) is a single word.
2 DRP level of 50 roughly corresponds to the beginning of 6th grade 
in US schools, 70 corresponds to end of 10th grade. 
533
the passage,  and averaged these values over 100 
passages of each batch. Table 4 shows the results.
LSAspec
LSA
Vector
Length
IDF
residualIDF
burstiness
variance
gain
Table 4: Average median specificity scores for texts of  
different genres and DRP levels.
Note  that  the  absolute  values  for  a  particular 
batch of texts are not important in this case; it's  the 
relative  differences  between  batches  of  different 
styles and difficulty levels that are of interest. Of 
all the measures, only  LSAspec appears to exhibit 
the two characteristics described above (increasing 
with text difficulty, and separating the three genres 
in the expected way). The metrics residualIDF and 
burstiness also appear to separate the genres as ex-
pected,  but  they do  not  increase  with  text  diffi-
culty. 
It is also evident that  LSA Vector Length alone 
does not serve as a good measure of informative-
ness, contrary to its use as such in (Gorman et al, 
2003). In fact, it shows the most dramatic and reli-
able  inverse relationship with text difficulty. This 
is likely due to the fact that texts of lower diffi-
culty use common (easier) words more often; these 
words tend to have longer LSA vector lengths.
4.4 Back-of-the-Book Glossary
Educational textbooks typically have a glossary 
(index) at the end which lists important terms or 
concepts mentioned in the book. One would expect 
these terms to have greater informativeness com-
pared to other words in the textbook. This was a 
crucial assumption used by Csomai and Mihalcea 
(2007), who used informativeness (as measured by 
IDF and other metrics) as one of the main features 
used  to  automatically  generate   glossaries  from 
textbooks.
We can use existing textbooks and their glossar-
ies to validate this assumptions, by observing the 
extent  to  which  the  words  in  the  glossary  are 
ranked higher by different specificity metrics com-
pared to other words. Note that the goal here is not 
to actually achieve optimal  performance  in  auto-
matically finding glossary words;  for  this  reason 
we do not use recall/precision- based evaluation or 
rely on  additional features such as term frequency 
(or the popular tfw?idfw measure). Rather the goal is 
to simply see how much the glossary words exhibit 
the property (informativeness) that we are trying to 
compute with various methods. 
We obtained a  collection of textbook chapters 
(middle-school  level  material  from Prentice  Hall 
Publishing) and their corresponding glossaries, in 
two  different  genres:  8  on  World  Studies  (e.g. 
?Africa?,  ?Medieval  Times?)  and  13  on  Science 
(e.g. ?Structure of Animals?, ?Electricity?).  Each 
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
50 55 60 65 70
SocialStudies
LanguageArts
Science
534
chapter was converted into text and a list of unique 
words was extracted.
For each of the specificity metrics, we compute 
how well it predicts glossary words:
1. Compute the specificity of each word in a 
chapter, according to the metric.
2. Order all the words in decreasing order of 
specificity.
3. Compute the median percentile rank (posi-
tion)  in  the  list  above  of  all  single-word 
entries in the glossary (top word has  the 
rank of 0; bottom has a rank of 100).
If  a  specificity  metric  predicts  the  glossary 
words well, we would expect the average rank to 
be low; i.e. glossary words would be near the top 
of the specificity-ordered list.
Metric Word Studies
(~9000 total wds / ch
~260 gloss wds / ch)
Science
(~1000 total wds / ch
~20 gloss wds / ch)
LSAspec 0.21 0.10
residualIDF 0.21 0.11
burstiness 0.21 0.12
IDF 0.29 0.16
variance 0.49 0.64
gain 0.51 0.68
Table 5: Average median rank of glossary words among 
all other words in textbook by specificity.
LSAspec shows the lowest median percentile for 
both genres of books.
4.5 Qualitative Analysis
It is useful to inspect the significant differences 
between the word rankings by different methods, 
to see if some notable patterns emerge.  We can 
find words  on  which the  methods  disagree  most 
dramatically by observing which of them have the 
most significant differences of position (0-100) in 
the word lists ranked by different specificity met-
rics. To avoid dealing with overly-rare words, we 
restrict  our attention to the 23,000 most  frequent 
words in the corpus.
Let's  first  compare  LSAspec and  residualIDF. 
From the list of 100 words with the most extreme 
disagreements, we select some examples that have 
high rank for  LSAspec (and low for  residuaIDF) 
and vice-versa.  From Table 6 we can see that  re-
sidualIDF misses some term words (such as ?chro-
matin?) which  LSAspec correctly rates as highly-
specific  words.  Conversely,  residualIDF,  incor-
rectly ranks common words like ?her? and ?water? 
as highly-specific. The reason for this behavior is 
that words like ?chromatin? happen to occur only 
once per document in the texts they are mentioned 
(e.g.  dfcromatin =  tfchromatin =  7),  whereas  ?her?  and 
?school? tend to occur frequently per document. In 
real applications ?her? will probably be discarded 
using  stopword  lists,  but  ?school?  will  probably 
not.
Word LSAspec residualIDF
oviducts 0.5 98.8
cuspids 0.6 98.8
chromatin 0.7 98.7
disassembly 0.7 98.7
her 99.9 1.5
my 99.9 3.5
water 97.5 5.1
school 97.8 10.3
Table 6: Words ranked most differently by LSAspec and 
residualIDF
 Comparing  LSAspec  and  burstiness we see al-
most  the  same  pattern,  which  is  not  surprising, 
since  burstiness and  residualIDF work  from the 
same assumptions that content words tend to occur 
multiple times but in fewer documents.
The table below lists examples of most notable 
differences between LSAspec and IDF.
Word LSAspec IDF
billy 10.3 93.5
jack 15.0 95.9
melody 4.1 83.8
cells 10.8 86.3
inducing 34.0 9.8
vagueness 32.5 9.6
initiating 31.5 8.7
apathetic 32.3 9.8
Table 7: Words ranked most differently by LSAspec and 
IDF and their percentiles
There is a large disagreement between rankings 
of  common proper  names  (e.g.  ?jack?).  It  is  not 
clear what the correct answer for these should be, 
although Rennie & Jaakkola (2005) use informat-
iveness for named entity detection, assuming that 
proper names should have high specificity.  Com-
mon  but  important  words  like  ?melody?  and 
?cells? are considered low-specificity by  IDF. By 
535
contrast, rare but vague words like ?inducing? or 
?vagueness?  are  improperly   given  a  high   spe-
cificity ranking.
5 Applications in LSA
Having demonstrated that  our word specificity 
metric performs well with regards to some natural 
linguistic phenomena, we can now show that it can 
be used successfully as part of existing NLP tech-
nologies. Here we will  focus particularly on  ap-
plications within  Latent Semantic Analysis (LSA), 
although it is highly likely that this specificity met-
ric can be used successfully in other places as well. 
We will demonstrate that  LSAspec shows better 
results  that  the  conventional  term  weighting 
scheme in LSA. It is also important to note that al-
though LSAspec is derived using LSA, it is in fact 
logically  independent  from  the  term  weighting 
mechanism used by LSA; other metrics  (such as 
the ones described above) could also be potentially 
used for LSA term weighting.
In order to represent the meaning of text in LSA, 
one typically computes the document vector of the 
text by geometric addition of word vectors for each 
of the constituent words:
?V d=?
w?d
aw?log?1?tdw???vw
where aw is the log-entropy weight of the word 
w, typically set to tfw?idfw (or some variation there-
of) , tdw is the number of occurrences of the word w 
in the document, and vw is the vector of the word. 
Implicit in  vw  is its geometric length, which tends 
to be much greater for frequently-used words (un-
less  they  are  extremely  vague).  It  is  tempered 
somewhat by  aw which is higher for content words, 
but  perhaps  not  effectively  enough,  as  the  sub-
sequent  tests  will  show. McNamara et  al.  (2007) 
experimented  with  changing  the  weighting 
scheme,  mainly  focusing  on  prioritizing  rare  vs. 
frequent words, and has shown significant differ-
ences in short-sentence comparison results. 
In the sections below we compare the original 
LSA  weighting  scheme  with  our  new  scheme 
based on LSAspec:
?V d=?
w?d
LSAspec ?w??log?1?tdw?? ?vw??vw?
In other words, we replace the weight parameter aw  
and the implicit weight contained in the length of 
each word vector (by normalizing it) with the spe-
cificity value of LSAspec.
We  show  that  the  resulting  term  weighting 
scheme improves  performance  in three important 
applications:  information  retrieval,  gisting  and 
short-sentence comparison.
5.1 Information Retrieval
LSA was first introduced as Latent Semantic In-
dexing (Deerwester et al 1990), designed for the 
goal of more effective information retrieval by rep-
resenting both documents and queries as vectors in 
a common latent semantic space.
In this  IR context,  the type  of term weighting 
used to compose document and query vectors plays 
an  important  role.  We  show  that  using  our 
LSAspec-based term weighting gives superior per-
formance to the traditional weighting scheme de-
scribed in the previous section.
We used the SMART Time3 dataset, a collection 
of 425 documents and 83 queries related to Time 
magazine news articles. For this task only, we used 
a LSA space that was built using the AQUAINT-2 
corpus4, a large collection (~440,000) of news art-
icles from prominent newspapers such as the New 
York Times.  The variable parameter in the LSA 
IR models was the cosine threshold between the 
document and the query, which was varied 
between 0 and 1
Figure 1 shows the performance of the original 
LSA and LSA with LSAspec5 term weighting 
method, in terms of the F-measure, which is the 
harmonic mean of precision and recall; a higher 
value means better performance. The abscissa in 
3   ftp://ftp.cs.cornell.edu/pub/smart/time/
4 TREC conference: http://trec.nist.gov/
5 LSAspec measure was the same as before, derived from LSA built 
on MetaMetrics corpus.
Figure 2: The performance of default LSA and 
LSA+LSASpec on SMART IR dataset.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
0.00
0.10
0.20
0.30
0.40
F-measure
Default
LSAspec
536
the graph is the value of the threshold cosine para-
meter. The LSAspec term weighting outperforms 
the original term weighting.
5.2 Sentence Similarity
Here we analyze performance of the two LSA 
term  weighting  methods  on  automated  sentence 
similarity comparisons. Although LSA works best 
on units of text of paragraph-size or larger, it can 
work reasonably well on sentence-length units.
We  use  the  dataset  reported  by  McNamara 
(2007),  where the authors collected a set  of sen-
tence pairs from several books. A total of 96 sen-
tence pairs was provided, consisting of a combina-
tion of subsequent sentences in the book (16), non-
adjacent  sentences  in  the  same  book  (16),  sen-
tences  from  two  different  books  (48),  and  sen-
tences where one is a manually-created paraphrase 
of one another (16). The standard of reference for 
this task is human similarity ratings of these sen-
tences within each pair, reported on a Likert scale 
between 6 (most  similar)  and 1 (completely dis-
similar). Here we report correlations between hu-
man rating and LSA similarity with the two term 
weighting metrics.
Original LSA: 0.66 LSA + LSAspec: 0.85
Using LSAspec term weighting gives better per-
formance  compared  to  the  original  LSA  term 
weighting scheme.
5.3 Gisting (Very Short Summarization)
The ability to represent documents and words in 
a common geometric space allows LSA to easily 
compute  the  gist  of  a  document  by  finding  the 
word (or sentence) whose vector is most similar by 
cosine metric to the document  vector.  This word 
can be interpreted as the most representative of the 
cumulative meaning of the document; it can also 
be thought  as a one-word summary of the docu-
ment.  Gisting  is  discussed  from a  psychological 
perspective by Kintsch (2002).
Once again, the choice of term weighting mech-
anism can make a significant difference in how the 
overall  document vector is constructed. Here, we 
compare  the  original  weighting  scheme  and 
LSAspec in the performance on gisting. To perform 
this evaluation, we selected 46 well-written Wiki-
pedia6 articles in various categories: Sports, Anim-
als, Countries, Sciences, Religions, Diseases.  The 
6 http://en.wikipedia.org  , circa May 2008.
original single-word Wikipedia title of each of the 
articles  can  be  thought  as  the  optimal  one-word 
gist of the article, thus serving as a reference an-
swer in evaluation. A perfect gisting performance 
by the model would always select the original title 
as the closest  word to the  meaning of the docu-
ment. We also measure the position of the original 
title in the list of all words in the article ranked by 
their  similarity to  the  document  vector,  and ran-
ging from 0 (original title picked as top word) and 
1. Table 10 shows a few examples of both the top 
word and rank of the title, as well  as the overall 
mean rank of all 46 articles.
Title Orig LSA LSA + LSAspectop word rank top word rank
Skiing skiing 0.0000 skiing 0.0000
Thailand buddhism 0.0189 thailand 0.0000
Sociology sociologists 0.0012 sociology 0.0000
Pneumonia infections 0.0064 infections 0.0092
Mean rank (all 46 articles) 0.0191 0.0061
St. dev. of rank 0.0847 0.0133
Table 8: Examples of gisting (picking most representat-
ive word for text) in with and without LSASpec in LSA
Using  LSAspec noticeably  improves  gisting  per-
formance,  compared  to  the  original  LSA  term 
weighting method, as is evidenced by much lower 
mean rank of the original title.
6 Conclusion
We have introduced a new method of measuring 
word informativeness. The method gives good res-
ults modeling some real linguistic phenomena, and 
improves LSA applications.
We attempted to look more deeply at the relev-
ant characteristics of word specificity (such as cor-
relation  with  number  of  senses).  Our  method 
seems to correspond with intuition on emulating a 
wide range of these characteristics. It also avoids a 
lot  of  pitfalls  of  existing methods  that  are based 
purely on frequency statistics, such as unduly pri-
oritizing rare but vague words.
Further research should examine the stability of 
this method (compared to others) with regards to 
variation/size of the training corpus. It should also 
analyze application of the method in other natural 
language tasks. Lastly, it should be correlated with 
human judgments, similar to other psycholinguistic 
properties.
537
References
Kenneth W. Church and  William A. Gale. 1995. Pois-
son mixtures.  Journal  of  Natural  Language Engin-
eering,  1995
Kenneth W. Church and  William A. Gale. 1995a. In-
verse document frequency (IDF): A measure of devi-
ation  from  Poisson.  In  Proceedings  of  the  Third 
Workshop  on  Very  Large  Corpora,  pp  121?130, 
1995.
Andr?s  Csomai  and  Rada  Mihalcea.  2007.  Investiga-
tions in Unsupervised Back-of-the-Book Indexing. In 
Proceedings of the Florida Artificial Intelligence Re-
search Society, Key West.
Scott Deerwester, Susan T. Dumais, George W. Furnas 
and Thomas K. Landauer. 1990. Indexing by Latent 
Semantic Analysis.  Journal of the American Society  
for Information Science, 41.
Aloysia  M.  Gorman.  1961.  Recognition  Memory  for 
Nouns as a Function of Abstractness and Frequency. 
Journal of Experimental Psychology. Vol. 61, No. 1.
Jamie  C.  Gorman,  Peter  W.  Foltz,  Preston  A. 
Kiekel and Melanie J. Martin. 2003. Evaluation of 
Latent Semantic Analysis-based  Measures of Team 
Communication Content. Proceedings of the Human 
Factors and Ergonomics Society, 47th Annual Meet-
ing, pp 424-428.
Walter Kintsch. 2002. On the notions of theme and top-
ic in psychological  process models of text compre-
hension.  In  M.  Louwerse  &  W.  van  Peer  (Eds.) 
Thematics  :  Interdisciplinary  Studies,  Amsterdam, 
Benjamins, pp 157-170.
Walter Kintsch. 2001. Predication. Journal of Cognitive 
Science, 25.
Kirill  Kireyev.  2008. Using Latent  Semantic Analysis 
for  Extractive  Summarization.  Proceedings  of  Text  
Analysis Conference, 2008.
B. L. Koslin, S. Zeno, and S. Koslin. 1987. The DRP: 
An Effective Measure in Reading. New York College 
Entrance Examination Board.
Thomas K Landauer and Susan Dumais. 1997. A solu-
tion to Plato's problem: The Latent Semantic nalysis 
theory of the   acquisition, induction, and representa-
tion of knowledge.   Psychological  Review, 104, pp 
211-240.
Thomas  K  Landauer,  Danielle  S.  McNamara,  Simon 
Dennis, and Walter Kintsch. 2007. Handbook of Lat-
ent Semantic Analysis Lawrence Erlbaum.
Rachel  TszWai  Lo,  Ben  He,  and  Iadh  Ounis.  2005. 
Automatically Building a Stopword List  for  an In-
formation Retrieval  System.  5th Dutch-Belgium In-
formation Retrieval Workshop (DIR). 2005.
Philip  M.  McCarthy,  Arthur  C.  Graesser,  Danielle  S. 
McNamara. 2006. Distinguishing Genre Using Coh-
Metrix Indices of Cohesion. 16th Annual Meeting of  
the  Society  for  Text  and  Discourse, Minneapolis,  
MN, 2006.
Danielle  S.  McNamara,  Zhiqiang  Cai,  and  MaxM. 
Louwerse. 2007. Optimizing LSA Measures of Cohe-
sion.  Handbook of Latent Semantic Analysis . Mah-
wah, NJ: Erlbaum. ch 19, pp 379-399.
George  A.  Miller,  Richard  Beckwith,  Christiane  Fell-
baum,  Derek  Gross  and  Katherine  Miller.   1990. 
WordNet: An on-line lexical database.  International  
Journal of Lexicography, 3 (4), 1990.
Kishore  Papineni.  2001.  Why  inverse  document  fre-
quency. In Proceedings of the NAACL, 2001
Jamie Reilly and Jacob Kean. 2007. Formal Distinctive-
ness Of High- and Low- Imageability Nouns: Ana-
lyses  and  Theoretical  Implications.  Cognitive  Sci-
ence, 31.
Jason D. M. Rennie and Tommi Jaakkola. 2005. Using 
Term Informativeness  for  Named Entity  Detection. 
Proceedings of ACM SIGIR 2005.
Karen Sp?rck-Jones. 1973. "A Statistical Interpretation 
of Term Specificity and its Application in Retrieval," 
Journal of Documentation, 28:1.
Michael P. Toglia and William R. Battig. 1978.  Hand-
book  of  semantic  word  norms.  Hillsdale,  NJ: 
Lawrence Erlbaum Associates.
538
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 27?33,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A New Yardstick and Tool for Personalized Vocabulary Building
Thomas K Landauer Kirill Kireyev
Pearson Education,
 Knowledge Technologies
Charles Panaccione
{tom.landauer,kirill.kireyev,charles.panaccione}@pearson.com
Abstract 
The goal of this research is to increase the 
value of each individual student's vocabulary 
by finding words that the student doesn?t 
know, needs to, and is ready to learn. To help 
identify such words, a better model of how 
well any given word is expected to be known 
was created. This is accomplished by using a 
semantic language model, LSA, to track how 
every word changes with the addition of more 
and more text from an appropriate corpus. We 
define the ?maturity? of a word as the degree 
to which it has become similar to that after 
training on the entire corpus. 
An individual student?s average vocabu-
lary level can then be placed on the word-
maturity scale by an adaptive test. Finally, the 
words that the student did or did not know on 
the test can be used to predict what other 
words the same student knows by using mul-
tiple maturity models trained on random sam-
ples of typical educational readings. This 
detailed information can be used to generate 
highly customized vocabulary teaching and 
testing exercises, such as Cloze tests.
1 Introduction
1.1 Why ?Vocabulary First?
There are many arguments for the importance 
of more effective teaching of vocabulary. Here are 
some examples: 
(1) Baker, Simmons, & Kame'enui (1997) 
found that children who enter school with limited 
vocabulary knowledge grow much more discrepant 
over time from their peers who have rich vocabu-
lary knowledge.
(2.) Anderson & Freebody (1981) found that 
the number of words in student?s meaning vocabu-
laries was the best predictor of how well they 
comprehend text. 
(3) An unpublished 1966 study of the correla-
tion between entering scores of Stanford Students 
on the SAT found the vocabulary component to be 
the best predictor of grades in every subject, in-
cluding science.    
(4) The number of words students learn varies 
greatly, from 0.2 to 8 words per day and from 50 to 
over 3,000 per year. (Anderson & Freebody,1981)
(5) Printed materials in grades 3 to 9 on average 
contain almost 90,000, distinct word families and 
nearly 500,000 word forms (including proper 
names.) (Nagy & Anderson, 1984). 
(6) Nagy and Anderson (1984) found that on 
average not knowing more than one word in a sen-
tence prevented its tested understanding, and that 
the probability of learning the meaning of a new 
word by one encounter on average was less than 
one in ten.
(7) John B. Carroll?s (1993) meta-analysis of 
factor analyses of measured cognitive ability found 
the best predictor to be tests of vocabulary.
(8) Hart and Risley?s large randomized obser-
vational study of the language used in households 
with young children found that the number of 
words spoken within hearing of a child was associ-
ated with a three-fold difference in vocabulary by 
school entry.
1.2 The Challenge
Several published sources and inspection of the 
number of words taught in recent literacy text-
books and online tools suggest that less than 400 
words per year are directly tutored in American 
schools. Thus, the vast majority of vocabulary 
must be acquired from language exposure, espe-
cially from print because the oral vocabulary of 
daily living is usually estimated to be about 20,000 
27
words, of which most are known by early school 
years. But it would obviously be of great value to 
find a way to make the explicit teaching of vocabu-
lary more effective, and to make it multiply the 
effects of reading. These are the goals of the new 
methodologies reported here.
It is also clear that words are not learned in iso-
lation: learning the meaning of a new word re-
quires prior knowledge of many other words, and 
by most estimates it takes a (widely variable) aver-
age of ten encounters in different and separated 
contexts. (This, by the way, is what is required to 
match human adult competence in the computa-
tional language model used here. Given a text cor-
pus highly similar to that experienced by a 
language learner, the model learns at very close to 
the same rate as an average child, and it learns new 
words as much as four times faster the more old 
words it knows (Landauer & Dumais, 1997).)
An important aside here concerns a widely cir-
culated inference from the Nagy and Anderson
(1984) result that teaching words by presenting 
them in context doesn?t produce enough vocabu-
lary growth to be the answer. The problem is that 
the experiments actually show only that the in-
serted target word itself is usually not learned well 
enough to pass a test. But in the simulations, words 
are learned a little at a time; exposure to a sentence 
increases the knowledge of many other words, both 
ones in the sentence and not. Every encounter with 
any word in context percolates meaning through 
the whole current and future vocabulary. Indeed, in 
the simulator, indirect learning is three to five 
times as much as direct, and is what accounts for 
its ability to match human vocabulary growth and 
passage similarity. Put differently, the helpful thing 
that happens on encountering an unknown word is 
not guessing its meaning but its contribution to 
underlying understanding of language. 
However, a vicious negative feedback loop 
lurks in this process. Learning from reading re-
quires vocabulary knowledge. So the vocabulary-
rich get richer and the vocabulary-poor get rela-
tively poorer. Fortunately, however, in absolute 
terms there is a positive feedback loop: the more 
words you know, the faster you can learn new 
ones, generating exponential positive growth. Thus 
the problem and solution may boil down to in-
creasing the growth parameter for a given student 
enough to make natural reading do its magic better. 
Nonetheless, importantly, it is patently obvious 
that it matters greatly what words are taught how, 
when and to which students. 
The hypothesis, then, is that a set of tools that 
could determine what particular words an individ-
ual student knows and doesn?t, and which ones 
learned (and sentences understood) would most 
help other words to be learned by that student 
might have a large multiplying effect. It is such a 
toolbox that we are endeavoring to create by using 
a computational language model with demon-
strated ability to simulate human vocabulary 
growth to a reasonably close approximation. The 
principal foci are better selection and ?personaliza-
tion? of what is taught and teaching more quickly 
and with more permanence by application of opti-
mal spacing of tests and practice?into which we 
will not go here. 
1.3 Measuring vocabulary knowledge 
Currently there are three main methods for 
measuring learner vocabulary, all of which are in-
adequate for the goal. They are:
1. Corpus Frequency. Collect a large sample 
of words used in the domain of interest, for exam-
ple a collection of textbooks and readers used in 
classrooms, text from popular newspapers, a large 
dictionary or the Internet. Rank the words by fre-
quency of occurrence. Test students on a random 
subset of, say, the 1,000, 2,000 and 5,000 most 
frequent words, compute the proportion known at 
each ?level? and interpolate and extrapolate. This 
is a reasonable method, because frequently en-
countered words are the ones most frequently 
needed to be understood. 
2. Educational Materials. Sample vocabulary 
lessons and readings over classrooms at different 
school grades.
3. Expert Judgments. Obtain informed expert 
opinions about what words are important to know 
by what age for what purposes.
Some estimates combine two or more of these 
approaches, and they vary in psychometric sophis-
tication. For example, one of the most sophisti-
cated, the Lexile Framework, uses Rasch scaling
(Rasch, 1980) of a large sample of student vocabu-
lary test scores (probability right on a test, holding 
student ability constant) to create a difficulty 
measure for sentences and then infers the difficulty 
of words, in essence, from the average difficulty of 
the sentences in which they appear.
28
The problem addressed in the present project 
goal is that all of these methods measure only the 
proportion of tested words known at one or more 
frequency ranges, in chosen school grades or for
particular subsets of vocabulary (e.g. ?academic? 
words), and for a very small subset?those tested -
some of the words that the majority of a class 
knows.  What they don?t measure is exactly which 
words in the whole corpus a given student knows 
and to what extent, or which words would be most 
important for that student to learn.
A lovely analog of the problem comes from 
Ernst Rothkopf?s (1970) metaphor that everyone 
passes through highly different ?word swarms? 
each day on their way to their (still highly differen-
tiated) adult literacy. 
2 A new metric: Word Maturity
The new metric first applies Latent Semantic 
Analysis (LSA) to model how representation of
individual words changes and grows toward their 
adult meaning as more and more language is en-
countered. Once the simulation has been created, 
an adaptive testing method can be applied to place 
individual words on separate growth curves - char-
acteristic functions in psychometric terminology. 
Finally, correlations between growth curves at 
given levels can be used to estimate the achieved 
growth of other words.
2.1 How it works in more detail: LSA.
A short review of how LSA works will be use-
ful here because it is often misunderstood and a 
correct interpretation is important in what follows. 
LSA models how words combine into meaningful 
passages, the aspect of verbal meaning we take to 
be most critical to the role of words in literacy. It 
does this by assuming that the ?meaning? (please 
bear with the nickname) of a meaningful passage is 
the sum of the meanings of its words:
Meaning of passage = 
{meaning of first wd} + 
{meaning of second word} + ?  + 
{meaning of last word}
A very large and representative corpus of the 
language to be modeled is first collected and repre-
sented as a term-by-document matrix. A powerful 
matrix algebra method called Singular Value De-
composition is then used to make every paragraph 
in the corpus conform to the above objective func-
tion?word representations sum to passage repre-
sentations - up to a best least-squares 
approximation. A dimensionality-reduction step is 
performed, resulting in each word and passage 
meanings represented as a (typically) 300 element 
real number vector. Note that the property of a vec-
tor standing for a word form in this representation 
is the effect that it has on the vector standing for 
the passage. (In particular, it is only indirectly a 
reflection of how similar two words are to each 
other or how frequently they have occurred in the 
same passages.) In the result, the vector for a word 
is the average of the vectors for all the passages in 
which it occurs, and the vector for a passage is, of 
course, the average all of its words.
In many previous applications to education, in-
cluding automatic scoring of essays, the model?s
similarity to human judgments (e.g. by mutual in-
formation measures) has been found to be 80 to 
90% as high as that between two expert humans, 
and, as mentioned earlier, the rate at which it 
learns the meaning of words as assessed by various 
standardized and textbook-based tests has been 
found to closely match that of students. For more 
details, evaluations and previous educational appli-
cations, see (Landauer et al, 2007).
2.2 How it works in more detail: Word Ma-
turity.
Taking LSA to be a sufficiently good approxi-
mation of human learning of the meanings con-
veyed by printed word forms, we can use it to track 
their gradual acquisition as a function of increasing 
exposure to text representative in size and content 
of that which students at successive grade levels
read. 
Thus, to model the growth of meaning of indi-
vidual words, a series of sequentially accumulated 
LSA ?semantic spaces? (the collection of vectors 
for all of the words and passages) are created. Cu-
mulative portions of the corpus thus emulate the 
growing total amount of text that has been read by 
a student. At each step, a new LSA semantic space 
is created from a cumulatively larger subset of the 
full adult corpus. 
Several different ways of choosing the succes-
sive sets of passages to be added to the training set 
have been tried, ranging from ones based on read-
ability metrics (such as Lexiles or DRPs) to en-
29
tirely randomly selected subsets. Here, the steps 
are based on Lexiles to emulate their order of en-
counter in typical school reading. 
This process results in a separate LSA model of 
word meanings corresponding to each stage of lan-
guage learning. To determine how well a word or 
passage is known at a given stage of learning?a 
given number or proportion of passages from the 
corpus?its vector in the LSA model correspond-
ing to a particular stage is compared with the vec-
tor of the full adult model (one that has been 
trained on a corpus corresponding to a typical 
adult?s amount of language exposure). This is done 
using a linear transformation technique known as 
Procrustes Alignment to align the two spaces?
those after a given step to those based on the full 
corpus, which we call its ?adult? meaning.
Word maturity is defined as the similarity of a 
word?s vector at a given stage of training and that 
at its adult stage as measured by cosine. It is scaled 
as values ranging between 0 (least mature) and 1 
(most mature).
Figure 1 shows growth curves for an illustrative 
set of words. In this example, 17 successive cumu-
lative steps were created, each containing ~5000 
additional passages. 
Word Meaning Maturity
0.0
0.2
0.4
0.6
0.8
1.0
1 3 5 7 9 11 13 15 17
Model Level
Si
m
ila
rit
y
dog
electoral
primate
productivity
turkey
Figure 1. An illustration of meaning maturity growth of sev-
eral words as a function of language exposure.
Some words (e.g. ?dog?) are almost at their 
adult meaning very early. Others hardly get started 
until later. Some grow quickly, some slowly. Some 
grow smoothly, some in spurts. Some, like ?tur-
key,? grow rapidly, plateau, then resume growing 
again, presumably due to multiple senses 
(?Thanksgiving bird? vs. ?country?) learned at dif-
ferent periods (in LSA, multiple ?senses? are com-
bined in a word representation approximately in 
proportion to their frequency.) 
The maturity metric has several conceptual ad-
vantages over existing measures of the status of 
a word?s meaning, and in particular should be kept 
conceptually distinct from the ambiguous and often 
poorly defined term ?difficulty? and from whether 
or not students in general or at some developmen-
tal stage can properly use, define or understand its 
meaning. It is a mathematical property of a word 
that may or may not be related to what particular 
people can do with it. 
What it does is provide a detailed view of the 
course of development of a word?s changing repre-
sentation?its ?meaning?, reciprocally defined as 
its effect on the ?meaning? of passages in which it 
occurs,?as a function of the amount and nature of 
the attestedly meaningful passages in which it has 
been encountered. Its relation to ?difficulty? as 
commonly used would depend, among other 
things, on whether a human could use it for some 
purpose at some stage of development of the word. 
Thus, its relation to a student?s use of a word re-
quires a second step of aligning the student?s word 
knowledge with the metric scaling. This is analo-
gous to describing a runner?s ?performance? by 
aligning it with well-defined metrics for time and 
distance.
It is nevertheless worth noting that the word 
maturity metric is not based directly on corpus fre-
quency as some other measures of word status are 
(although its average level over all maturities is 
moderately highly correlated with total corpus fre-
quency as it should be) or on other heuristics, such 
as grade of first use or expert opinions of suitabil-
ity.
What is especially apparent in the graph above 
is that after a given amount of language exposure, 
analogous to age or school grade, there are large 
differences in the maturity of different words. In 
fact the correlation between frequency of occur-
rence in a particular one of the 17 intermediate cor-
pora and word maturity is only 0.1, measured over 
20,000 random words. According to the model--
and surely common sense--words of the same fre-
quency of encounter (or occurrence in a corpus) 
are far from equally well known. Thus, all methods 
for ?leveling? text and vocabulary instruction 
based on word frequency must hide a great range 
of differences.
To illustrate this in more detail, Table 1, shows 
computed word maturities for a set of words that 
have nearly the same frequency in the full corpus 
30
(column four) when they have been added only 
50?5 times (column two). The differences are so 
large as to suggest the choice of words to teach 
students in a given school grade would profit much 
from being based on something more discrimina-
tive than either average word frequency or word 
frequency as found in the texts being read or in the 
small sample that can be humanly judged. Even 
better, it would appear, should be to base what is 
taught to a given student on what that student does 
and doesn?t know but needs to locally and would 
most profit from generally.
Word Occurrences 
in intermedi-
ate corpus
(level 5)
Occurrences 
in adult 
corpus
Word 
maturity 
(at level 
5)
marble 54 485 0.21
sunshine 49 508 0.31
drugs 53 532 0.42
carpet 48 539 0.59
twin 48 458 0.61
earn 53 489 0.70
beam 47 452 0.76
Table 1 A sample of words with roughly the same number of 
occurrences in both intermediate (~50) and adult (~500) cor-
pus
The word maturity metric appears to perform 
well when validated by some external methods. 
For example, it reliably discriminates between 
words that were assigned to be taught in different 
school grades by (Biemiller, 2008), based on a 
combination of expert judgments and comprehen-
sion tests (p < 0.03), as shown in Table 2.
grade 2,
known 
by > 80%
grade 2,
known by 
40-80%
grade 6,
known by 
40-80%
grade 6,
known 
by < 40%
n=1034 n=606 n=1125 n=1411
4.4 6.5 8.8 9.5
Table 2 Average level for each word to reach a 0.5 maturity 
threshold, for words that are known at different levels by stu-
dents of different grades (Biemiller, 2008).
Median word maturity also tracks the differ-
ences (p < 0.01) between essays written by stu-
dents in different grades as shown in Figure 2.
Percent of "adult" words in essay
0%
1%
2%
3%
4%
5%
4 6 8 10 12
Student grade level
Figure 2 Percentage of ?adult? words used in essays written 
by students of different grade levels. ?Adult? words are de-
fined as words that reach a 0.5 word maturity threshold at or 
later than the point where half of the words in the language 
have reached 0.5 threshold.
2.3 Finding words to teach individual stu-
dents
Using the computed word maturity values, a 
sigmoid characteristic curve is generated to ap-
proximate the growth curve of every word in the 
corpus. A model similar to one used in item re-
sponse theory (Rasch, 1980) can be constructed 
from the growth curve due to its similarity in shape 
and function to an IRT characteristic curve; both 
curves represent the ability of a student.  The char-
acteristic curve for the IRT is needed to properly 
administer adaptive testing, which greatly in-
creases the precision and generalizeability of the 
exam. Words to be tested are chosen from the cor-
pus beginning at the average maturity of words at 
the approximate grade level of the student. Thirty 
to fifty word tests are used to home in on the stu-
dent?s average word maturity level. In initial trials, 
a combination of yes/no and Cloze tests are being 
used. Because our model does not treat all words 
of a given frequency as equivalent, this alone sup-
ports a more precise and personalized measure of a 
student?s vocabulary. In plan, the student level will 
be updated by the results of additional tests admin-
istered in school or by Internet delivery.
The final step is to generalize from the assessed 
knowledge of words a particular student (let?s call 
her Alice) is tested on to other words in the corpus. 
This is accomplished by first generating a large 
number of simulated students (and their word ma-
turity curves) using the method described above. 
Each simulated student is trained on one of many ~ 
12 million word corpora, size and content ap-
proximating the lifelong reading of a typical col-
lege student, that have been randomly sampled 
from a representative corpus of more than half a 
31
billion words. Some of these simulated students? 
knowledge of the words being tested will be more 
similar to Alice than others. We can then estimate 
Alice?s knowledge of any other word w in the cor-
pus by averaging the levels of knowledge of w by 
simulated students whose patterns of tested word 
knowledge are most similar hers. The method rests 
on the assumption that there are sufficiently strong 
correlations between the words that a given student 
has learned at a given stage (e.g. resulting from 
Rothkopf?s personal ?swarms?.) While simulations 
are promising, empirical evidence as to the power 
of the approach with non-simulated students is yet 
to be determined. 
3 Applying the method
On the assumption that learning words by their 
effects on passage meanings as LSA does is good, 
initial applications use Cloze items to simultane-
ously test and teach word meanings by presenting 
them in a natural linguistic context. Using the 
simulator, the context words in an item are pre-
dicted to be ones that the individual student already 
knows at a chosen level. The target words, where 
the wider pedagogy permits, are ones that are re-
lated and important to the meaning of the sentence 
or passage, as measured by LSA cosine similarity 
metric, and, ipso facto, the context tends to contex-
tually teach their meaning. They can also be cho-
sen to be those that are computationally estimated 
to be the most important for a student to know in 
order to comprehend assigned or student-chosen 
readings?because their lack has the most effect on 
passage meanings?and/or in the language in gen-
eral. Using a set of natural language processing 
algorithms (such as n-gram models, POS-tagging, 
WordNet relations and LSA) the distracter items 
for each Cloze are chosen in such a way that they 
are appropriate grammatically, but not semanti-
cally, as illustrated in the example below.
In summary, Cloze-test generation involves the 
following steps:
1. Determine the student?s overall knowledge 
level and individual word knowledge predictions 
based on previous interactions.
2. Find important words in a reading that are 
appropriate for a particular student (using metrics 
that include word maturity).
3. For each word, find a sentence in a large 
collection of natural text, such that the rest of the 
sentence semantically implies (is related to) the 
target word and is appropriate for student?s knowl-
edge level.
4.Find distracter words that are (a) level-
appropriate, (b) are sufficiently related and (c) fit 
grammatically, but (d) not semantically, into the 
sentence.
All the living and nonliving things around an ___ 
is its environment.
A. organism   B. oxygen   C. algae
Freshwater habitats can be classified according to 
the characteristic species of fish found in them, 
indicating the strong ecological relationship be-
tween an ___ and its environment.
A. adaptation   B. energy   C. organism
Table 3 Examples of auto-generated Cloze tests for the same 
word (organism) and two students of lower and higher ability, 
respectively.
4 Summary and present status
A method based on computational model-
ing of language, in particular one that makes the 
representation of the meaning of a word its effect 
on the meaning of a passage its objective, LSA, 
has been developed and used to simulate the 
growth of meaning of individual word representa-
tions towards those of literate adults. Based 
thereon, a new metric for word meaning growth 
called ?Word Maturity? is proposed. The measure 
is then applied to adaptively measuring the average 
level of an individual student?s vocabulary, pre-
sumably with greater breadth and precision than 
offered by other methods, especially those based 
on knowledge of words at different corpus fre-
quency. There are many other things the metric 
may support, for example better personalized 
measurement of text comprehensibility.
However, it must be emphasized that the 
method is very new and essentially untried except 
in simulation. And it is worth noting that while the 
proposed method is based on LSA, many or all of 
its functionalities could be obtained with some 
other computational language models, for example 
the Topics model. Comparisons with other meth-
ods will be of interest, and more and more rigorous 
evaluations are needed, as are trials with more 
various applications to assure robustness. 
32
5 References
Richard C. Anderson, Peter Freebody. 1981. Vo-
cabulary Knowledge. In J. T. Guthrie (Ed.), 
Comprehension and teaching: Research reviews
(pp. 77-117). International Reading Association, 
Newark DE.
Scott K. Baker,  Deborah C. Simmons, Edward J. 
Kameenui. 1997. Vocabulary acquisition: Re-
search bases. In Simmons, D. C. & Kameenui, 
E. J. (Eds.), What reading research tells us 
about children with diverse learning needs: 
Bases and basics. Erlbaum, Mahwah, NJ. 
Andrew Biemiller (2008).  Words Worth Teaching.  
Co-lumbus, OH:  SRA/McGraw-Hill.
John B Carroll. 1993. Cognitive Abilities: A survey 
of factor-analytic studies. Cambridge: Cam-
bridge University Press, 1993. 
Betty Hart, Todd R. Risley. 1995. Meaningful dif-
ferences in the everyday experience of young 
American children. Brookes Publishing, 1995.
Melanie R. Kuhn, Steven A. Stahl. 1998. Teaching 
children to learn word meanings from context: 
A synthesis and some questions. Journal of Lit-
eracy Research, 30(1) 119-138.
Thomas K Landauer, Susan Dumais. 1997. A solu-
tion to Plato's problem: The Latent Semantic 
Analysis theory of the Acquisition, Induction, 
and Representation of Knowledge.  Psychologi-
cal Review, 104, pp 211-240.
Thomas K Landauer, Danielle S. McNamara, 
Simon Dennis, and Walter Kintsch. 2007. Hand-
book of Latent Semantic Analysis. Lawrence 
Erlbaum.
Cleborne D. Maddux (1999). Peabody Picture Vo-
cabulary Test III (PPVT-III). Diagnostique, v24 
n1-4, p221-28, 1998-1999
William E. Nagy, Richard C. Anderson. 1984. 
How many words are there in printed school 
English? Reading Research Quarterly, 19, 304-
330.
Ernst Z. Rothkopf, Ronald D. Thurner. 1970. Ef-
fects of written instructional material on the sta-
tistical structure of test essays. Journal of 
Educacational Psychology, 61, 83-89.
George Rasch. (1980). Probabilistic models for 
some intelligence and attainment tests. (Copen-
hagen, Danish Institute for Educational Re-
search), expanded edition (1980) with foreword 
and afterword by B.D. Wright. Chicago: The 
University of Chicago Press.
33
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 299?308,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Word Maturity: Computational Modeling of Word Knowledge 
Kirill Kireyev   Thomas K Landauer  Pearson Education, Knowledge Technologies Boulder, CO  {kirill.kireyev, tom.landauer}@pearson.com
 
 
Abstract 
While computational estimation of difficulty of words in the lexicon is useful in many edu-cational and assessment applications, the concept of scalar word difficulty and current corpus-based methods for its estimation are inadequate. We propose a new paradigm called word meaning maturity which tracks the degree of knowledge of each word at dif-ferent stages of language learning. We pre-sent a computational algorithm for estimating word maturity, based on modeling language acquisition with Latent Semantic Analysis. We demonstrate that the resulting metric not only correlates well with external indicators, but captures deeper semantic effects in lan-guage. 1 Motivation It is no surprise that through stages of language learning, different words are learned at different times and are known to different extents. For ex-ample, a common word like ?dog? is familiar to even a first-grader, whereas a more advanced word like ?focal? does not usually enter learners? vocabulary until much later. Although individual rates of learning words may vary between high- and low-performing students, it has been observed that ?children [?] acquire word meanings in roughly the same sequence? (Biemiller, 2008). The aim of this work is to model the degree of knowledge of words at different learning stages. Such a metric would have extremely useful appli-cations in personalized educational technologies, for the purposes of accurate assessment and per-sonalized vocabulary instruction. 
2 Rethinking Word Difficulty Previously, related work in education and psy-chometrics has been concerned with measuring word difficulty or classifying words into different difficulty categories.  Examples of such approaches include creation of word lists for targeted vocabulary instruction at various grade levels that were compiled by educa-tional experts, such as Nation (1993) or Biemiller (2008). Such word difficulty assignments are also implicitly present in some readability formulas that estimate difficulty of texts, such as Lexiles (Stenner, 1996), which include a lexical difficulty component based on the frequency of occurrence of words in a representative corpus, on the as-sumption that word difficulty is inversely correlat-ed to corpus frequency. Additionally, research in psycholinguistics has attempted to outline and measure psycholinguistic dimensions of words such as age-of-acquisition and familiarity, which aim to track when certain words become known and how familiar they appear to an average per-son. Importantly, all such word difficulty measures can be thought of as functions that assign a single scalar value to each word w: ? ?????? ? ? ? ? ? (1) There are several important limitations to such metrics, regardless of whether they are derived from corpus frequency, expert judgments or other measures. First, learning each word is a continual process, one that is interdependent with the rest of the vo-cabulary. Wolter (2001) writes:  
299
[?] Knowing a word is quite often not an either-or situation; some words are known well, some not at all, and some are known to varying degrees. [?] How well a particular word is known may condition the connections made between that particular word and the other words in the mental lexicon.  Thus, instead of modeling when a particular word will become fully known, it makes more sense to model the degree to which a word is known at different levels of language exposure.  Second, word difficulty is inherently perspec-tival: the degree of word understanding depends not only on the word itself, but also on the sophis-tication of a given learner. Consider again the dif-ference between ?dog? and ?focal?: a typical first-grader will have much more difficulty understand-ing the latter word compared to the former, where-as a well-educated adult will be able to use these words with equal ease. Therefore, the degree, or maturity, of word knowledge is inherently a func-tion of two parameters -- word w and learner level l: ??????? ? ? ?, ? ? ? (2) As the level l increases (i.e. for more advanced learners), we would expect the degree of under-standing of word w to approach its full value cor-responding to perfect knowledge; this will happen at different rates for different words.  Ideally, we would obtain maturity values by testing word knowledge of learners across differ-ent levels (ages or school grades) for all the words in the lexicon. Such a procedure, however, is pro-hibitively expensive; so instead we would like to estimate word maturity by using computational models.  To summarize: our aim is to model the devel-opment of meaning of words as a function of in-creasing exposure to language, and ultimately - the degree to which the meaning of words at each stage of exposure resemble their ?adult? meaning. We therefore define word meaning maturity to be the degree to which the understanding of the word (expected for the average learner of a particular level) resembles that of an ideal mature learner. 
3 Modeling Word Meaning Acquisition with Latent Semantic Analysis  3.1 Latent Semantic Analysis (LSA) An appealing choice for quantitatively modeling word meanings and their growth over time is La-tent Semantic Analysis (LSA), an unsupervised method for representing word and document meaning in a multi-dimensional vector space.  The LSA vector representation is derived in an unsupervised manner, based on occurrence pat-terns of words in a large corpus of natural lan-guage documents. A Singular Value Decomposition on the high-dimensional matrix of word/document occurrence counts (A) in the cor-pus, followed by zeroing all but the largest r ele-ments1 of the diagonal matrix S, yields a lower-rank word vector matrix (U).  The dimensionality reduction has the effect of smoothing out inci-dental co-occurrences and preserving significant semantic relationships between words. The result-ing word vectors2 in U are positioned in such a way that semantically related words vectors point in similar directions or, equivalently, have higher cosine values between them. For more details, please refer to Landauer et al (2007) and others.  
 Figure 1. The SVD process in LSA illustrated. The original high-dimensional word-by-document matrix A is decomposed into word (U) and document (V) matrices of lower dimen-sionality. In addition to merely measuring semantic relat-edness, LSA has been shown to emulate the learn-ing of word meanings from natural language (as can be evidenced by a broad range of applications from synonym tests to automated essay grading), at rates that resemble those of human learners (Laundauer et al 1997). Landauer and Dumais (1997) have demonstrated empirically that LSA can emulate not only the rate of human language acquisition, but also more subtle phenomena, such as the effects of learning certain words on mean-ing of other words. LSA can model meaning with                                                             1 Typically the first approx. 300 dimensions are retained 2 U? is used to project word vectors into V-space 
SVD x x
Document	 ?VectorsWord	 ?VectorsOriginal	 ?Matrix
word	 ?1word	 ?2
word	 ?n
doc	 ?1 doc	 ?2 doc	 ?m. 	 ?.	 ?.
.	 ?.	 ?. r
r
r
r A U S V ? 
300
high accuracy, as attested, for example, by 90% correlation with human judgments on assessing the quality of student essay content (Landauer, 2002). 3.2 Using LSA to Compute Word Maturity In this work, the general procedure behind computationally estimating word maturity of a learner at a particular intermediate level (i.e. age or school grade level) is as follows: 1. Create an intermediate corpus for the given level. This corpus approximates the amount and sophistication of language encountered by a learner at the given level. 2. Build an LSA space on that corpus. The re-sulting LSA word vectors model the mean-ing of each word to the particular intermediate-level learner. 3. Compare the meaning representation of each word (its LSA vector) to the corresponding one in a reference model. The reference model is trained on a much larger corpus and approximates the word meanings by a mature adult learner.  We can repeat this process for each of a num-ber of levels. These levels may directly correspond to school grades, learner ages or any other arbi-trary gradations.  In summary, we estimate word maturity of a given word at a given learner level by comparing the word vector from an intermediate LSA model (trained on a corpus of size and sophistication comparable to that which a typical real student at the given level encounters) to the corresponding vector from a reference adult LSA model (trained on a larger corpus corresponding to a mature lan-guage learner). A high discrepancy between the vectors would suggest that an intermediate mod-el?s meaning of a particular word is quite different from the reference meaning, and thus the word maturity at the corresponding level is relatively low. 3.3 Procrustes Alignment (PA) Comparing vectors across different LSA spaces is less straightforward, since the individual dimen-sions in LSA do not have a meaningful interpreta-tion, and are an artifact of the content and ordering of the training corpus used. Therefore, direct com-
parisons across two different spaces, even of the same dimensionality, are meaningless, due to a mismatch in their coordinate systems.  Fortunately, we can employ a multivariate al-gebra technique known as Procrustes Alignment (or Procrustes Analysis) (PA) typically used to align two multivariate configurations of a corre-sponding set of points in two different geometric spaces.  PA has been used in conjunction with LSA, for example, in cross-language information retrieval (Littman, 1998). The basic idea behind PA is to derive a rotation matrix that allows one space to be rotated into the other. The rotation matrix is computed in such a way as to minimize the differences (namely: sum of squared distances) between corresponding points, which in the case of LSA can be common words or documents in the training set. For more details, the reader is advised to con-sult chapter 5 of (Krzanowski, 2000) or similar literature on multivariate analysis. In summary, given two matrices containing coordinates of n corresponding points X and Y (and assuming mean-centering and equal number of dimensions, as is the case in this work), we would like to min-imize the sum of squared distances between the points: 
?? = ? ? ? ?? ?????
?
???  We try to find an orthogonal rotation matrix Q, which minimizes M2  by rotating Y relative to X. That matrix can be obtained by solving the equa-tion: ?? = ???? (??? + ??? ? 2?????) It turns out that the solution to Q is given by VU?, where U?V? is the singular value decomposition of the matrix X?Y.  In our situation, where there are two spaces, adult and intermediate, the alignment points are the corresponding document vectors correspond-ing to the documents that the training corpora of the two models have in common (recall that the adult corpus is a superset of each of the intermedi-ate corpora).  The result of the Procrustes Align-ment of the two spaces is effectively a joint LSA space containing two distinct word vectors for each word (e.g. ?dog1?, ?dog2?), corresponding to the vectors from each of the original spaces. After 
301
merging using Procrustes Alignment, the compari-son of word meanings becomes a simple problem of comparing word vectors in the joint space using the standard cosine metric. 4 Implementation Details In our experiments we used passages from the MetaMetrics Inc. 2002 corpus3, largely consisting of educational and literary content representative of the reading material used in American schools at different grade levels. The average length of each passage is approximately 135 words. The first-level intermediate corpus was com-posed of 6,000 text passages, intended for school grade 1 or below. The grade level is approximated using the Coleman-Liau readability formula (Coleman, 1975), which estimates the US grade level necessary to comprehend a given text, based on its average sentence and word length statistics: ?? = 0.0588? ? 0.296? ? 15.8 (4) where L is the average number of letters per 100 words and S is the average number of sentences per 100 words.   Each subsequent intermediate corpus contains additional 6,000 new passages of the next grade level, in addition to the previous corpus. In this way, we create 14 levels. The adult corpus is twice as large, and of same grade level range (0-14) as the largest intermediate corpus. In summary, the following describes the size and makeup of the corpora used:  Corpus Size (passages) Approx. Grade Level (Coleman-Liau Index) Intermediate 1 6,000 0.0 - 1.0 Intermediate 2 12,000 0.0 - 2.0 Intermediate 3 18,000 0.0 - 3.0 Intermediate 4 24,000 0.0 - 4.0 ?   Intermediate 14 84,000 0.0 - 14.0 Adult 168,000 0.0 - 14.0 Table 1. Size and makeup of corpora. used for LSA models. The particular choice of the Coleman-Liau readability formula (CLI) is not essential; our ex-periments show that other well-known readability formulas (such as Lexiles) work equally well. All that is needed is some approximate ordering of                                                             3 We would like to acknowledge Jack Stenner and MetaMet-rics for the use of their corpus. 
passages by difficulty, in order to mimic the way typical human learners encounter progressively more difficult materials at successive school grades.  After creating the corpora, we: 1. Build LSA spaces on the adult and each of the intermediate corpora 2. Merge the intermediate space for level l with the adult space, using Procrustes Alignment. This results in a joint space with two sets of vec-tors: the versions from the intermediate space {vlw}, and adult space{vaw}. 3. Compute the cosine in the joint space be-tween the two word vectors for the given word w ? ?, ? = ??  ?( ? ? , ? ?) (5) In the cases where a word w has not been encoun-tered in a given intermediate space, or in the rare cases where the cosine value falls below 0, the word maturity value is set to 0. Hence, the range for the word maturity function falls in the closed interval [0.0, 1.0]. A higher cosine value means greater similarity in meaning between the refer-ence and intermediate spaces, which implies a more mature meaning of word w at the level l, i.e. higher word meaning maturity. The scores be-tween discrete levels are interpolated, resulting in a continuous word maturity curve for each word. Figure 1 below illustrates resulting word ma-turity curves for some of the words. 
!"
!#$"
!#%"
!#&"
!#'"
("
!" (" $" )" %" *" &" +" '" ," (!" ((" ($" ()" (%"-."
!"
#$%&
'()
#*(+
%
,-.-/%
/01"234567"846/9204":0;9<"
Figure 2. Word maturity curves for selected words. Consistent with intuition, simple words like ?dog? approach their adult meaning rather quickly, while ?focal? takes much longer to become known to any degree.  An interesting example is ?turkey?, which has a noticeable plateau in the middle. This can be explained by the fact that this word has two dis-tinct senses. Closer analysis of the corpus and the semantic near-neighbor word vectors at each in-
302
termediate space, shows that earlier meaning deal almost exclusively with the first sense (bird), while later readings with the other (country). Therefore, even though the word ?turkey? is quite prevalent in earlier readings, its full meaning is not learned until later levels. This demonstrates that our method takes into account the meaning, and not merely the frequency of occurrence. 5 Evaluation 5.1 Time-to-maturity Evaluation of the word maturity metric against external data is not always straightforward be-cause, to the best of our knowledge, data that con-tains word knowledge statistics at different learner levels does not exist. Instead, we often have to evaluate against external data consisting of scalar difficulty values (see Section 2 for discussion) for each word, such as age-of-acquisition norms de-scribed in the following subsection.  There are two ways to make such comparisons possible. One is to compute the word maturity at a particular level, obtaining a single number for each word. Another is by computing time-to-maturity: the minimum level (the value on the x-axis of the word maturity graph) at which the word maturity reaches4 a particular threshold ?:  ?? ? = min ? ?. ?. ? ?, ? > ?       (6) Intuitively, this measure corresponds to the age in a learner?s development when a given word be-comes sufficiently understood. The parameter ? can be estimated empirically (in practice ?=0.45 gives good correlations with external measures). Since the values of word maturity are interpolated, the ttm(w) can take on fractional values. It should be emphasized that such a collapsing of word maturity into a scalar value inherently results in loss of information; we only perform it in order to allow evaluation against external data sources. As a baseline for these experiments we include word frequency, namely the document frequency of words in the adult corpus. 
                                                            4 Values between discrete levels are obtained using piecewise linear interpolation 
5.2 Age-of-Acquisition Norms Age-of-Acquisition (AoA) is a psycholinguistic property of words originally reported by Carol & White (1973). Age of Acquisition approximates the age at which a word is first learned and has been proposed as a significant contributor to lan-guage and memory processes. With some excep-tions, AoA norms are collected by subjective measures, typically by asking each of a large number of participants to estimate in years the age when they have learned the word. AoA estimates have been shown to be reliable and provide a valid estimate for the objective age at which a word is acquired; see (Davis, in press) for references and discussion. In this experiment we compute Spearman cor-relations between time-to-maturity and two avail-able collections of AoA norms: Gilhooly et al, (1980) norms5, and Bristol norms6 (Stadthagen-Gonzalez et al, 2010).  Measure Gilhooly (n=1643)  Bristol (n=1402) (-) Frequency 0.59 0.59 Time-to-Maturity (?=0.45) 0.72 0.64 Table 2. Correlations with Age of Acquisition norms. 5.3 Instruction Word Lists In this experiment, we examine leveled lists of words, as created by Biemiller (2008) in the book entitled ?Words Worth Teaching: Closing the Vo-cabulary Gap?. Based on results of multiple-choice word comprehension tests administered to students of different grades as well as expert judgments, the author derives several word diffi-culty lists for vocabulary instruction in schools, including: o Words known by most children in grade 2 o Words known by 40-80% of children in grade 2 o Words known by 40-80% of children in grade 6 o Words known by fewer than 40% of chil-dren in grade 6 One would expect the words in these four groups to increase in difficulty, in the order they are pre-sented above.  
                                                            5 http://www.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htm 6 http://language.psy.bris.ac.uk/bristol_norms.html 
303
To verify how these word groups correspond to the word maturity metric, we assign each of the words in the four groups a difficulty rating 1-4 respectively, and measure the correlation with time-to-maturity.    Measure Correlation (-) Frequency 0.43 Time-to-maturity (?=0.45) 0.49 Table 3. Correlations with instruction word lists (n=4176). The word maturity metric shows higher correla-tion with instruction word list norms than word frequency. 5.4 Text Complexity Another way in which our metric can be evaluated is by examining the word maturity in texts that have been leveled, i.e. have been assigned ratings of difficulty. On average, we would expect more difficult texts to contain more difficult words. Thus, the correlation between text difficulty and our word maturity metric can serve as another val-idation of the metric. For this purpose, we obtained a collection of readings that are used as reading comprehension tests by different state websites in the US7. The collection consists of 1,220 readings, each anno-tated with a US school grade level (in the range between 3-12) for which the reading is intended. The average length each passage was approxi-mately 489 words. In this experiment we computed the correlation of the grade level with time-to-maturity, and two other measures, namely: ? Time-to-maturity: average time-to-maturity of unique words in text (excluding stopwords) with ?=0.45. ? Coleman-Liau. The Coleman-Liau reada-bility index (Equation 4). ? Frequency. Average of corpus log-frequency for unique words in the text, ex-cluding stopwords.                                                                 7 The collection was created as part of the ?Aspects of Text Complexity? project funded by the Bill and Melinda Gates Foundation, 2010. 
Measure Correlation Frequency (avg. of unique words) 0.60 Coleman-Liau 0.64 Time-to-maturity (?=0.45) (avg. of unique non-stopwords) 0.70 Table 4. Correlations of grade levels with different metrics. 6 Emphasis on Meaning In this section, we would like to highlight certain properties of the LSA-based word maturity metric, particularly aiming to illustrate the fact that the metric tracks acquisition of meaning from expo-sure to language and not merely more shallow ef-fects, such as word frequency in the training corpus.  6.1 Maturity based on Frequency For a baseline that does not take meaning into ac-count, let us construct a set of maturity-like curves based on frequency statistics alone. More specifi-cally, we define the frequency-maturity for a par-ticular word at a given level as the ratio of the number of occurrences at the intermediate corpus  for that level (l) to the number of occurrences in the reference corpus (a):  
? ?, ? = ?? _ ???? ?(?)?? _ ???? ?(?) Similarly to the original LSA-based word maturity metric, this ratio increases from 0 to 1 for each word as the amount of cumulative language expo-sure increases. The corpora used at each interme-diate level are identical to the original word maturity model, but instead of creating LSA spac-es we simply use the corpora to compute word frequency. The following figure shows the Spearman cor-relations between the external measures used for experiments in Section 5, and time-to-maturity computed based on the two maturity metrics: the new frequency-based maturity and the original LSA-based word maturity.  
304
 Figure 3. Correlations of word maturity computed using fre-quency (as well as the original) against external metrics de-scribed in Section 5.  The results indicate that the original LSA-based word maturity correlates better with real-world data than a maturity metric simply based on fre-quency.  6.2 Homographs Another insight into the fact that the LSA-based word maturity metric tracks word meaning rather than mere frequency may be gained from analysis of words that are homographs: words that contain two or more unrelated meanings in the same writ-ten form, such as the word ?turkey? illustrated in Section 4. (This is related to but distinct from the merely polysemous words that have several related meanings),  Because of the conflation of several unrelated meanings into the same orthographic form, homo-graphs implicitly contain more semantic content in a single word. Therefore, one would expect the meaning of homographs to mature more slowly than would be predicted by frequency alone: all things being equal, a learner has to learn the mean-ings for all of the senses of a homograph word before the word can be considered fully known. More specifically, one would expect the time-to-maturity of homographs to have greater values than words of similar frequency. To test this hy-pothesis, we obtained8 a list 174 common English homographs. For each of them, we compared their time-to-maturity to the average time-to-maturity of words that have the same (+/- 1%) corpus fre-quency.                                                             8 http://en.wikipedia.org/wiki/List_of_English_homographs 
The results of a paired t-test confirms the hy-pothesis that the time-to-maturity of homographs is greater than other words of the same frequency, with the p-value = 5.9e-6. This is consistent with the observation that homographs will take longer to learn and serves as evidence that LSA-based word maturity approximates effects related to meaning. 6.3 Size of the Reference Corpus Another area of investigation is the repercus-sions of the choice of the corpus for the reference (adult) model. The size (and content) of the corpus used to train the reference model is potentially important, since it affects the word maturity calcu-lations, which are comparisons of the intermediate LSA spaces to the reference LSA space built on this corpus.  It is interesting to investigate how the word maturity model would be affected if the adult cor-pus were made significantly more sophisticated. If the word maturity metric were simply based on word frequency (including the frequency-based maturity baseline described in Section 6.1), one would expect the word maturity of the words at each level to decrease significantly if the reference model is made significantly larger, since each in-termediate level will have encountered fewer words by comparison. Intuition about language learning, however, tells us that with enough lan-guage exposure a learner learns virtually all there is to know about any particular word; after the word reaches its adult maturity, subsequent en-counters of natural readings do little to further change the knowledge of that word. Therefore, if word maturity were tracking something similar to real word knowledge, one would expect the word maturity for most words to plateau over time, and subsequently not change significantly, no matter how sophisticated the reference model becomes.  To evaluate this inquiry we created a reference corpus that is twice as large as before (four times as large and of the same difficulty range as the corpus for the last intermediate level), containing roughly 329,000 passages. We computed the word maturity model using this larger reference corpus, while keeping all the original intermediate corpora of the same size and content.  The results show that the average word maturi-ty of words at the last intermediate level (14) de-
0.66	 ?
0.56	 ?
0.42	 ?
0.68	 ?
0.72	 ?
0.64	 ?
0.49	 ?
0.71	 ?
0.0	 ?
0.2	 ?
0.4	 ?
0.6	 ?
0.8	 ?
1.0	 ?
AoA	 ?
(Gilhooly)	 ?
AoA	 ?
(Bristol)	 ?
Word	 ?Lists	 ? Readings	 ?
freq-??WM	 ?(?=0.15)	 ?
LSA-??WM	 ?(?=0.45)	 ?
305
creases by less than 14% as a result of doubling the adult corpus. Furthermore, this number is as low as 6%, if one only considers more common words that occur 50 times or more in the corpus. This relatively small difference, in spite of a two-fold increase of the adult corpus, is consistent with the idea that word knowledge should approach a plateau, after which further exposure to language does little to change most word meanings. 6.4 Integration into Lexicon Another important consideration with respect to word learning mentioned in Wotler (2001), is the ?connections made between [a] particular word and the other words in the mental lexicon.? One implication of that is that measuring word maturity must take into account the way words in the lan-guage are integrated with other words. One way to test this effect is to introduce read-ings where a large part of the important vocabu-lary is not well known to learners at a given level. One would expect learning to be impeded when the learning materials are inappropriate for the learner level. This can be simulated in the word maturity model by rearranging the order of some of the training passages, by introducing certain advanced passages at a very early level. If the results of the word maturity metric were merely based on fre-quency, such a reordering would have no effect on the maturity of important words (measured after all the passages containing these words have been encountered), since the total number of relevant word encounters does not change as a result of this reshuffling. If, however, the metric reflected at least some degree of semantics, we would expect word maturities for important words in these read-ings to be lower as a result of such rearranging, due to the fact that they are being introduced in contexts consisting of words that are not well known at the early levels. To test this effect, we first collected all passag-es in the training corpus of intermediate models containing some advanced words from different topics, namely: ?chromosome?, ?neutron? and ?filibuster? together with their plural variants. We changed the order of inclusion of these 89 passag-es into the intermediate models in each of the two following ways: 
1. All the passages were introduced at the first level (l=1) intermediate corpus 2. All the passages were introduced at the last level (l=14) intermediate corpus. This resulted in two new variants of word ma-turity models, which were computed in all the same ways as before, except that all of these 89 advanced passages were introduced either at the very first level or at the very last level. We then computed the word maturity at the levels they were introduced. The hypothesis consistent with a meaning-based maturity method would be that less learning (i.e. lower word maturity) of the relevant words will occur when passages are introduced prematurely (at level 1). Table 5 shows the word maturities measured for each of those cases, at the level (1 or 14) when all of the passages have been introduced. Word Introduced at l=1 (WM at l=1) Introduced at l=14 (WM at l=14) chromosome 0.51 0.73 neutron 0.51 0.72 filibuster 0.58 0.85 Table 5. Word maturity of words resulting when all the rele-vant passages are introduced early vs late. Indeed, the results show lower word maturity val-ues when advanced passages are introduced too early, and higher ones when the passages are in-troduced at a later stage, when the rest of the sup-porting vocabulary is known. 7 Conclusion We have introduced a new metric for estimating the degree of knowledge of words by learners at different levels. We have also proposed and evalu-ated an implementation of this metric using Latent Semantic Analysis. The implementation is based on unsupervised word meaning acquisition from natural text, from corpora that resemble in volume and complexity the reading materials a typical human learner might encounter.  The metric correlates better than word frequen-cy to a range of external measures, including vo-cabulary word lists, psycholinguistic norms and leveled texts. Furthermore, we have shown that the metric is based on word meaning (to the extent that it can be approximated with LSA), and not merely on shallow measures like word frequency. 
306
Many interesting research questions still re-main pertaining to the best way to select and parti-tion the training corpora, align adult and intermediate LSA models, correlate the results with real school grade levels, as well as other free parameters in the model. Nevertheless, we have shown that LSA can be employed to usefully mimic model word knowledge. The models are currently used (at Pearson Education) to create state-of-the-art personalized vocabulary instruc-tion and assessment tools.   
307
 References Andrew Biemiller (2008). Words Worth Teaching. Co-lumbus, OH:  SRA/McGraw-Hill. John B. Carrol and M. N. White (1973). Age of acquisi-tion norms for 220 picturable nouns. Journal of Ver-bal Learning & Verbal Behavior, 12, 563-576. Meri Coleman and T.L. Liau (1975). A computer read-ability formula designed for machine scoring, Jour-nal of Applied Psychology, Vol. 60, pp. 283-284. Ken J. Gilhooly and R. H. Logie (1980). Age of acqui-sition, imagery, concreteness, familiarity and ambi-guity measures for 1944 words. Behaviour Research Methods & Instrumentation, 12, 395-427. Wojtek J. Krzanowski (2000) Principles of Multivari-ate Analysis: A User?s Perspective (Oxford Statisti-cal Science Series). Oxford University Press, USA. Thomas K Landauer and Susan Dumais (1997). A solu-tion to Plato's problem: The Latent Semantic Analy-sis Theory of the Acquisition, Induction, and Representation of Knowledge. Psychological Re-view, 104, pp 211-240. Thomas K Landauer (2002). On the Computation Basis of Learning and Cognition: Arguments from LSA. In N. Ross (Ed.), The Psychology of Learning and Mo-tivation, 41, 43-84. Thomas K Landauer, Danielle S. McNamara, Simon Dennis, and Walter Kintsch (2007). Handbook of Latent Semantic Analysis. Lawrence Erlbaum. Paul Nation (1993). Measuring readiness for simplified material: a test of the first 1,000 words of English. In Simplification: Theory and Application M. L. Tickoo (ed.), RELC Anthology Series 31: 193-203. Hans Stadthagen-Gonzalez and C. J. Davis (2006). The Bristol Norms for Age of Acquisition, Imageability and Familiarity. Behavior Research Methods, 38, 598-605.  A. Jackson Stenner (1996). Measuring Reading Com-prehension with the Lexile Framework. Forth North American Conference on Adolescent/Adult Literacy. Brent Wolter (2001). Comparing the L1 and L2 Mental Lexicon. Studies in Second Language Acquisition. Cambridge University Press.       
308
