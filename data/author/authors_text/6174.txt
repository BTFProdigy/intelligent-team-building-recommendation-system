Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Coling 2010: Poster Volume, pages 9?17,
Beijing, August 2010
Document Expansion Based on WordNet
for Robust IR
Eneko Agirre
IXA NLP Group
Univ. of the Basque Country
e.agirre@ehu.es
Xabier Arregi
IXA NLP Group
Univ. of the Basque Country
xabier.arregi@ehu.es
Arantxa Otegi
IXA NLP Group
Univ. of the Basque Country
arantza.otegi@ehu.es
Abstract
The use of semantic information to im-
prove IR is a long-standing goal. This pa-
per presents a novel Document Expansion
method based on a WordNet-based system
to find related concepts and words. Ex-
pansion words are indexed separately, and
when combined with the regular index,
they improve the results in three datasets
over a state-of-the-art IR engine. Consid-
ering that many IR systems are not robust
in the sense that they need careful fine-
tuning and optimization of their parame-
ters, we explored some parameter settings.
The results show that our method is spe-
cially effective for realistic, non-optimal
settings, adding robustness to the IR en-
gine. We also explored the effect of doc-
ument length, and show that our method
is specially successful with shorter docu-
ments.
1 Introduction
Since the earliest days of IR, researchers noted
the potential pitfalls of keyword retrieval, such
as synonymy, polysemy, hyponymy or anaphora.
Although in principle these linguistic phenom-
ena should be taken into account in order to ob-
tain high retrieval relevance, the lack of algo-
rithmic models prohibited any systematic study
of the effect of this phenomena in retrieval. In-
stead, researchers resorted to distributional se-
mantic models to try to improve retrieval rele-
vance, and overcome the brittleness of keyword
matches. Most research concentrated on Query
Expansion (QE) methods, which typically ana-
lyze term co-occurrence statistics in the corpus
and in the highest scored documents for the orig-
inal query in order to select terms for expanding
the query terms (Manning et al, 2009). Docu-
ment expansion (DE) is a natural alternative to
QE, but surprisingly it was not investigated un-
til very recently. Several researchers have used
distributional methods from similar documents in
the collection in order to expand the documents
with related terms that do not actually occur in the
document (Liu and Croft, 2004; Kurland and Lee,
2004; Tao et al, 2006; Mei et al, 2008; Huang
et al, 2009). The work presented here is com-
plementary, in that we also explore DE, but use
WordNet instead of distributional methods.
Lexical semantic resources such as WordNet
(Fellbaum, 1998) might provide a principled and
explicit remedy for the brittleness of keyword
matches. WordNet has been used with success
in psycholinguistic datasets of word similarity and
relatedness, where it often surpasses distributional
methods based on keyword matches (Agirre et al,
2009b). WordNet has been applied to IR before.
Some authors extended the query with related
terms (Voorhees, 1994; Liu et al, 2005), while
others have explicitly represented and indexed
word senses after performing word sense disam-
biguation (WSD) (Gonzalo et al, 1998; Stokoe
et al, 2003; Kim et al, 2004). More recently,
a CLEF task was organized (Agirre et al, 2008;
Agirre et al, 2009a) where queries and docu-
ments were semantically disambiguated, and par-
ticipants reported mixed results.
This paper proposes to use WordNet for docu-
ment expansion, proposing a new method: given
9
a full document, a random walk algorithm over
the WordNet graph ranks concepts closely related
to the words in the document. This is in con-
trast to previous WordNet-based work which fo-
cused on WSD to replace or supplement words
with their senses. Our method discovers impor-
tant concepts, even if they are not explicitly men-
tioned in the document. For instance, given a doc-
ument mentioning virus, software and DSL, our
method suggests related concepts and associated
words such us digital subscriber line, phone com-
pany and computer. Those expansion words are
indexed separately, and when combined with the
regular index, we show that they improve the re-
sults in three datasets over a state-of-the-art IR en-
gine (Boldi and Vigna, 2005). The three datasets
used in this study are ResPubliQA (Pen?as et al,
2009), Yahoo! Answers (Surdeanu et al, 2008)
and CLEF-Robust (Agirre et al, 2009a).
Considering that many IR systems are not ro-
bust in the sense that they need careful fine-tuning
and optimization of their parameters, we decided
to study the robustness of our method, explor-
ing some alternative settings, including default pa-
rameters, parameters optimized in development
data, and parameters optimized in other datasets.
The study reveals that the additional semantic ex-
pansion terms provide robustness in most cases.
We also hypothesized that semantic document
expansion could be most profitable when docu-
ments are shorter, and our algorithm would be
most effective for collections of short documents.
We artificially trimmed documents in the Robust
dataset. The results, together with the analysis of
document lengths of the three datasets, show that
document expansion is specially effective for very
short documents, but other factors could also play
a role.
The paper is structured as follows. We first in-
troduce the document expansion technique. Sec-
tion 3 introduces the method to include the expan-
sions in a retrieval system. Section 4 presents the
experimental setup. Section 5 shows our main re-
sults. Sections 6 and 7 analyze the robustness and
relation to document length. Section 8 compares
to related work. Finally, the conclusions and fu-
ture work are mentioned.
2 Document Expansion Using WordNet
Our key insight is to expand the document with
related words according to the background infor-
mation in WordNet (Fellbaum, 1998), which pro-
vides generic information about general vocabu-
lary terms. WordNet groups nouns, verbs, adjec-
tives and adverbs into sets of synonyms (synsets),
each expressing a distinct concept. Synsets are in-
terlinked with conceptual-semantic and lexical re-
lations, including hypernymy, meronymy, causal-
ity, etc.
In contrast with previous work, we select those
concepts that are most closely related to the doc-
ument as a whole. For that, we use a technique
based on random walks over the graph represen-
tation of WordNet concepts and relations.
We represent WordNet as a graph as fol-
lows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets asso-
ciated to them by directed edges. We used ver-
sion 3.0, with all relations provided, including the
gloss relations. This was the setting obtaining the
best results in a word similarity dataset as reported
by Agirre et al (2009b).
Given a document and the graph-based repre-
sentation of WordNet, we obtain a ranked list of
WordNet concepts as follows:
1. We first pre-process the document to obtain
the lemmas and parts of speech of the open
category words.
2. We then assign a uniform probability distri-
bution to the terms found in the document.
The rest of nodes are initialized to zero.
3. We compute personalized PageR-
ank (Haveliwala, 2002) over the graph,
using the previous distribution as the reset
distribution, and producing a probability
distribution over WordNet concepts The
higher the probability for a concept, the
more related it is to the given document.
Basically, personalized PageRank is computed
by modifying the random jump distribution vec-
tor in the traditional PageRank equation. In our
case, we concentrate all probability mass in the
concepts corresponding to the words in the docu-
10
ment.
Let G be a graph with N vertices v1, . . . , vN
and di be the outdegree of node i; let M be a N ?
N transition probability matrix, where Mji = 1diif a link from i to j exists, and zero otherwise.
Then, the calculation of the PageRank vector Pr
over G is equivalent to resolving Equation (1).
Pr = cMPr + (1? c)v (1)
In the equation, v is a N ? 1 vector and c is the
so called damping factor, a scalar value between
0 and 1. The first term of the sum on the equa-
tion models the voting scheme described in the
beginning of the section. The second term repre-
sents, loosely speaking, the probability of a surfer
randomly jumping to any node, e.g. without fol-
lowing any paths on the graph. The damping fac-
tor, usually set in the [0.85..0.95] range, models
the way in which these two terms are combined at
each step.
The second term on Eq. (1) can also be seen as a
smoothing factor that makes any graph fulfill the
property of being aperiodic and irreducible, and
thus guarantees that PageRank calculation con-
verges to a unique stationary distribution.
In the traditional PageRank formulation the
vector v is a stochastic normalized vector whose
element values are all 1N , thus assigning equalprobabilities to all nodes in the graph in case of
random jumps. In the case of personalized PageR-
ank as used here, v is initialized with uniform
probabilities for the terms in the document, and
0 for the rest of terms.
PageRank is actually calculated by applying an
iterative algorithm which computes Eq. (1) suc-
cessively until a fixed number of iterations are
executed. In our case, we used a publicly avail-
able implementation1, with default values for the
damping value (0.85) and the number of iterations
(30). In order to select the expansion terms, we
chose the 100 highest scoring concepts, and get
all the words that lexicalize the given concept.
Figure 1 exemplifies the expansion. Given the
short document from Yahoo! Answers (cf. Sec-
tion 4) shown in the top, our algorithm produces
the set of related concepts and words shown in the
1http://ixa2.si.ehu.es/ukb/
bottom. Note that the expansion produces syn-
onyms, but also other words related to concepts
that are not mentioned in the document.
3 Including Expansions in a Retrieval
System
Once we have the list of words for document ex-
pansion, we create one index for the words in the
original documents and another index with the ex-
pansion terms. This way, we are able to use the
original words only, or to also include the expan-
sion words during the retrieval.
The retrieval system was implemented using
MG4J (Boldi and Vigna, 2005), as it provides
state-of-the-art results and allows to combine sev-
eral indices over the same document collection.
We conducted different runs, by using only the in-
dex made of original words (baseline) and also by
using the index with the expansion terms of the
related concepts.
BM25 was the scoring function of choice. It is
one of the most relevant and robust scoring func-
tions available (Robertson and Zaragoza, 2009).
wBM25Dt := (2)
tfDt
k1
(
(1? b) + b dlDavdlD
)
+ tfDt
idft
where tfDt is the term frequency of term t in doc-
ument D, dlD is the document length, idft is the
inverted document frequency (or more specifically
the RSJ weight, (Robertson and Zaragoza, 2009)),
and k1 and b are free parameters.
The two indices were combined linearly, as fol-
lows (Robertson and Zaragoza, 2009):
score(d, e, q) := (3)
?
t?q?d
wBM25Dt + ?
?
t?q?e
wBM25Et
where D and E are the original and expanded in-
dices, d, e and q are the original document, the
expansion of the document and the query respec-
tively, t is a term, and ? is a free parameter for the
relative weight of the expanded index.
11
You should only need to turn off virus and anti-spy not uninstall. And that?s
done within each of the softwares themselves. Then turn them back on later after
installing any DSL softwares.
06566077-n? computer software, package, software, software package, software program, software system
03196990-n? digital subscriber line, dsl
01569566-v? instal, install, put in, set up
04402057-n? line, phone line, suscriber line, telephone circuit, telephone line
08186221-n? phone company, phone service, telco, telephone company, telephone service
03082979-n? computer, computing device, computing machine, data processor, electronic computer
Figure 1: Example of a document expansion, with original document on top, and some of the relevant
WordNet concepts identified by our algorithm, together with the words that lexicalize them. Words in
the original document are shown in bold, synonyms in italics, and other related words underlined.
4 Experimental Setup
We chose three data collections. The first is based
on a traditional news collection. DE could be
specially interesting for datasets with short docu-
ments, which lead our choice of the other datasets:
the second was chosen because it contains shorter
documents, and the third is a passage retrieval task
which works on even shorter paragraphs. Table 1
shows some statistics about the datasets.
One of the collections is the English dataset
of the Robust task at CLEF 2009 (Agirre et al,
2009a). The documents are news collections from
LA Times 94 and Glasgow Herald 95. The top-
ics are statements representing information needs,
consisting of three parts: a brief title statement; a
one-sentence description; a more complex narra-
tive describing the relevance assessment criteria.
We use only the title and the description parts of
the topics in our experiments.
The Yahoo! Answers corpus is a subset of a
dump of the Yahoo! Answers web site2 (Surdeanu
et al, 2008), where people post questions and
answers, all of which are public to any web user
willing to browse them. The dataset is a small
subset of the questions, selected for their linguis-
tic properties (for example they all start with ?how
{to?do?did?does?can?would?could?should}?).
Additionally, questions and answers of obvious
low quality were removed. The document set was
created with the best answer of each question
(only one for each question).
2Yahoo! Webscope dataset ?ydata-yanswers-manner-
questions-v1 0? http://webscope.sandbox.yahoo.com/
docs length q. train q. test
Robust 166,754 532 150 160
Yahoo! 89610 104 1000 88610
ResPubliQA 1,379,011 20 100 500
Table 1: Number of documents, average docu-
ment length, number of queries for train and test
in each collection.
The other collection is the English dataset of
ResPubliQA exercise at the Multilingual Ques-
tion Answering Track at CLEF 2009 (Pen?as et al,
2009). The exercise is aimed at retrieving para-
graphs that contain answers to a set of 500 natu-
ral language questions. The document collection
is a subset of the JRC-Acquis Multilingual Paral-
lel Corpus, and consists of 21,426 documents for
English which are aligned to a similar number of
documents in other languages3. For evaluation,
we used the gold standard released by the orga-
nizers, which contains a single correct passage for
each query. As the retrieval unit is the passage,
we split the document collection into paragraphs.
We applied the expansion strategy only to pas-
sages which had more than 10 words (half of the
passages), for two reasons: the first one was that
most of these passages were found not to contain
relevant information for the task (e.g. ?Article 2?
or ?Having regard to the proposal from the Com-
mission?), and the second was that we thus saved
some computation time.
In order to evaluate the quality of our expansion
in practical retrieval settings, the next Section re-
3Note that Table 1 shows the number of paragraphs,
which conform the units we indexed.
12
base. expa. ?
Robust MAP .3781 .3835*** 1.43%
Yahoo! MRR .2900 .2950*** 1.72%P@1 .2142 .2183*** 1.91%
ResPubliQA MRR .3931 .4077*** 3.72%P@1 .2860 .3000** 4.90%
Table 2: Results using default parameters.
port results with respect to several parameter set-
tings. Parameter optimization is often neglected
in retrieval with linguistic features, but we think it
is crucial since it can have a large effect on rele-
vance performance and therefore invalidate claims
of improvements over the baseline. In each setting
we assign different values to the free parameters in
the previous section, k1, b and ?.
5 Results
The main evaluation measure for Robust is mean
Average Precision (MAP), as customary. In two of
the datasets (Yahoo! and ResPubliQA) there is a
single correct answer per query, and therefore we
use Mean Reciprocal Rank (MRR) and Mean Pre-
cision at rank 1 (P@1) for evaluation. Note that in
this setting MAP is identical to MRR. Statistical
significance was computed using Paired Random-
ization Test (Smucker et al, 2007). In the tables
throughout the paper, we use * to indicate statis-
tical significance at 90% confidence level, ** for
95% and *** for 99%. Unless noted otherwise,
base. refers to MG4J with the standard index, and
expa. refers to MG4J using both indices. Best
results per row are in bold when significant. ? re-
ports relative improvement respect to the baseline.
5.1 Default Parameter Setting
The values for k1 and b are the default values as
provided in the wBM25 implementation of MG4J,
1.2 and 0.5 respectively. We could not think of a
straightforward value for ?. A value of 1 would
mean that we are assigning equal importance to
original and expanded terms, which seemed an
overestimation, so we used 0.1. Table 2 shows
the results when using the default setting of pa-
rameters. The use of expansion is beneficial in all
datasets, with relative improvements ranging from
1.43% to 4.90%.
base. expa. ?
Robust MAP .3740 .3823** 2.20%
Yahoo! MRR .3070 .3100*** 0.98%P@1 .2293 .2317* 1.05%
ResPubliQA MRR .4970 .4942 -0.56%P@1 .3980 .3940 -1.01%
Table 3: Results using optimized parameters.
Setting System k1 b ?
Default base. 1.20 0.50 -expa. 1.20 0.50 0.100
Robust base. 1.80 0.64 -expa. 1.66 0.55 0.075
Yahoo! basel. 0.99 0.82 -expa. 0.84 0.87 0.146
ResPubliQA base. 0.09 0.56 -expa. 0.13 0.65 0.090
Table 4: Parameters as in the default setting or as
optimized in each dataset. The ? parameter is not
used in the baseline systems.
5.2 Optimized Parameter Setting
We next optimized all three parameters using the
train part of each collection. The optimization of
the parameters followed a greedy method called
?promising directions? (Robertson and Zaragoza,
2009). The comparison between the baseline and
expansion systems in Table 3 shows that expan-
sion helps in Yahoo! and Robust, with statistical
significance. The differences in ResPubliQA are
not significant, and indicate that expansion terms
were not helpful in this setting.
Note that the optimization of the parameters
yields interesting effects in the baseline for each
of the datasets. If we compare the results of the
baseline with default settings (Table 2) and with
optimized setting (Table 3), the baseline improves
MRR dramatically in ResPubliQA (26% relative
improvement), significantly in Yahoo! (5.8%) and
decreases MAP in Robust (-0.01%). This dis-
parity of effects could be explained by the fact
that the default values are often approximated us-
ing TREC-style news collections, which is exactly
the genre of the Robust documents, while Yahoo
uses shorter documents, and ResPubliQA has the
shortest documents.
Table 4 summarizes the values of the parame-
ters in both default and optimized settings. For k1,
the optimization yields very different values. In
Robust the value is similar to the default value, but
13
base. expa. ? ?
Rob MAP .3781 .3881*** 2.64% 0.18
Y! MRR .2900 .2980*** 2.76% 0.27P@1 .2142 .2212*** 3.27%
ResP. MRR .3931 .4221*** 7.39% 0.61P@1 .2860 .3180** 11.19%
Table 5: Results obtained using the ? optimized
setting, including actual values of ?.
in ResPubliQA the optimization pushes it down
below the typical values cited in the literature
(Robertson and Zaragoza, 2009), which might ex-
plain the boost in performance for the baseline in
the case of ResPubliQA. When all three param-
eters are optimized together, the values ? in the
table range from 0.075 to 0.146. The values of the
optimized ? can be seem as an indication of the
usefulness of the expanded terms, so we explored
this farther.
5.3 Exploring ?
As an additional analysis experiment, we wanted
to know the effect of varying ? keeping k1 and b
constant at their default values. Table 5 shows the
best values in each dataset, which that the weight
of the expanded terms and the relative improve-
ment are highly correlated.
5.4 Exploring Number of Expansion
Concepts
One of the free parameters of our system is the
number of concepts to be included in the docu-
ment expansion. We have performed a limited
study with the default parameter setting on the
Robust setting, using 100, 500 and 750 concepts,
but the variations were not statistically significant.
Note that with 100 concepts we were actually ex-
panding with 268 words, with 500 concepts we
add 1247 words and with 750 concepts we add
1831 words.
6 Robustness
The results in the previous section indicate that
optimization is very important, but unfortunately
real applications usually lack training data. In this
Section we wanted to study whether the param-
eters can be carried over from one dataset to the
other, and if not, whether the extra terms found by
train base. expa. ?
Rob.
def. MAP .3781 .3835*** 1.43%
Rob. MAP .3740 .3823** 2.20%
Y! MAP .3786 .3759 -0.72%
Res. MAP .3146 .3346*** 6.35%
Y!
def. MRR .2900 .2950*** 1.72%
Rob. MRR .2920 .2920 0.0%
Y! MRR .3070 .3100** 0.98%
Res. MRR .2600 .2750*** 5.77%
ResP.
def. MRR .3931 .4077*** 3.72%
Rob. MRR .3066 .3655*** 19.22%
Y! MRR .3010 .3459*** 14.93%
Res. MRR .4970 .4942 -0.56%
Table 6: Results optimizing parameters with train-
ing from other datasets. We also include default
and optimization on the same dataset for compar-
ison. Only MRR and MAP results are given.
DE would make the system more robust to those
sub-optimal parameters.
Table 6 includes a range of parameter set-
tings, including defaults, and optimized parame-
ters coming from the same and different datasets.
The values of the parameters are those in Table
4. The results show that when the parameters are
optimized in other datasets, DE provides improve-
ment with statistical significance in all cases, ex-
cept for the Robust dataset when using parameters
optimized from Yahoo! and vice-versa.
Overall, the table shows that our DE method ei-
ther improves the results significantly or does not
affect performance, and that it provides robustness
across different parameter settings, even with sub-
optimal values.
7 Exploring Document Length
The results in Table 6 show that the perfor-
mance improvements are best in the collection
with shortest documents (ResPubliQA). But the
results for Robust and Yahoo! do not show any re-
lation to document length. We thus decided to do
an additional experiment artificially shrinking the
document in Robust to a certain percentage of its
original length. We create new pseudo-collection
with the shrinkage factors of 2.5%, 10%, 20% and
50%, keeping the first N% words in the document
and discarding the rest. In all cases we used the
same parameters, as optimized for Robust.
Table 7 shows the results (MAP), with some
clear indication that the best improvements are ob-
14
tained for the shortest documents.
length base. expa. ?
2.5% 13 .0794 .0851 7.18%
10% 53 .1757 .1833 4.33%
20% 107 .2292 .2329 1.61%
50% 266 .3063 .3098 1.14%
100% 531 .3740 .3823 2.22%
Table 7: Results (MAP) on Robust when arti-
ficially shrinking documents to a percentage of
their length. In addition to the shrinking rate we
show the average lengths of documents.
8 Related Work
Given the brittleness of keyword matches, most
research has concentrated on Query Expansion
(QE) methods. These methods analyze the user
query terms and select automatically new related
query terms. Most QE methods use statistical
(or distributional) techniques to select terms for
expansion. They do this by analyzing term co-
occurrence statistics in the corpus and in the high-
est scored documents of the original query (Man-
ning et al, 2009). These methods seemed to im-
prove slightly retrieval relevance on average, but
at the cost of greatly decreasing the relevance of
difficult queries. But more recent studies seem
to overcome some of these problems (Collins-
Thompson, 2009).
An alternative to QE is to perform the expan-
sion in the document. Document Expansion (DE)
was first proposed in the speech retrieval commu-
nity (Singhal and Pereira, 1999), where the task
is to retrieve speech transcriptions which are quite
noisy. Singhal and Pereira propose to enhance the
representation of a noisy document by adding to
the document vector a linearly weighted mixture
of related documents. In order to determine re-
lated documents, the original document is used as
a query into the collection, and the ten most rele-
vant documents are selected.
Two related papers (Liu and Croft, 2004; Kur-
land and Lee, 2004) followed a similar approach
on the TREC ad-hoc document retrieval task.
They use document clustering to determine simi-
lar documents, and document expansion is carried
out with respect to these. Both papers report sig-
nificant improvements over non-expanded base-
lines. Instead of clustering, more recent work (Tao
et al, 2006; Mei et al, 2008; Huang et al, 2009)
use language models and graph representations of
the similarity between documents in the collec-
tion to smooth language models with some suc-
cess. The work presented here is complementary,
in that we also explore DE, but use WordNet in-
stead of distributional methods. They use a tighter
integration of their expansion model (compared to
our simple two-index model), which coupled with
our expansion method could help improve results
further. We plan to explore this in the future.
An alternative to statistical expansion methods
is to use lexical semantic knowledge bases such as
WordNet. Most of the work has focused on query
expansion and the use of synonyms from Word-
Net after performing word sense disambiguation
(WSD) with some success (Voorhees, 1994; Liu
et al, 2005). The short context available in
the query when performing WSD is an impor-
tant problems of these techniques. In contrast,
we use full document context, and related words
beyond synonyms. Another strand of WordNet
based work has explicitly represented and indexed
word senses after performing WSD (Gonzalo et
al., 1998; Stokoe et al, 2003; Kim et al, 2004).
The word senses conform a different space for
document representation, but contrary to us, these
works incorporate concepts for all words in the
documents, and are not able to incorporate con-
cepts that are not explicitly mentioned in the doc-
ument. More recently, a CLEF task was orga-
nized (Agirre et al, 2009a) where terms were se-
mantically disambiguated to see the improvement
that this would have on retrieval; the conclusions
were mixed, with some participants slightly im-
proving results with information from WordNet.
To the best of our knowledge our paper is the first
on the topic of document expansion using lexical-
semantic resources.
We would like to also compare our performance
to those of other systems as tested on the same
datasets. The systems which performed best in
the Robust evaluation campaign (Agirre et al,
2009a) report 0.4509 MAP, but note that they de-
ployed a complex system combining probabilis-
tic and monolingual translation-based models. In
ResPubliQA (Pen?as et al, 2009), the official eval-
15
uation included manual assessment, and we can-
not therefore reproduce those results. Fortunately,
the organizers released all runs, but only the first
ranked document for each query was included, so
we could only compute P@1. The P@1 of best
run was 0.40. Finally (Surdeanu et al, 2008) re-
port MRR figure around 0.68, but they evaluate
only in the questions where the correct answer
is retrieved by answer retrieval in the top 50 an-
swers, and is thus not comparable to our setting.
Regarding the WordNet expansion technique
we use here, it is implemented on top of publicly
available software4, which has been successfully
used in word similarity (Agirre et al, 2009b) and
word sense disambiguation (Agirre and Soroa,
2009). In the first work, a single word was in-
put to the random walk algorithm, obtaining the
probability distribution over all WordNet synsets.
The similarity of two words was computed as the
similarity of the distribution of each word, obtain-
ing the best results for WordNet-based systems on
the word similarity dataset, and comparable to the
results of a distributional similarity method which
used a crawl of the entire web. Agirre et al (2009)
used the context of occurrence of a target word to
start the random walk, and obtained very good re-
sults for WordNet WSD methods.
9 Conclusions and Future Work
This paper presents a novel Document Expan-
sion method based on a WordNet-based system
to find related concepts and words. The docu-
ments in three datasets were thus expanded with
related words, which were fed into a separate in-
dex. When combined with the regular index we
report improvements over MG4J usingwBM25 for
those three datasets across several parameter set-
tings, including default values, optimized param-
eters and parameters optimized in other datasets.
In most of the cases the improvements are sta-
tistically significant, indicating that the informa-
tion in the document expansion is useful. Similar
to other expansion methods, parameter optimiza-
tion has a stronger effect than our expansion strat-
egy. The problem with parameter optimization is
that in most real cases there is no tuning dataset
4http://ixa2.si.ehu.es/ukb
available. Our analysis shows that our expansion
method is more effective for sub-optimal param-
eter settings, which is the case for most real-live
IR applications. A comparison across the three
datasets and using artificially trimmed documents
indicates that our method is particularly effective
for short documents.
As document expansion is done at indexing
time, it avoids any overhead at query time. It
also has the advantage of leveraging full document
context, in contrast to query expansion methods,
which use the scarce information present in the
much shorter queries. Compared to WSD-based
methods, our method has the advantage of not
having to disambiguate all words in the document.
Besides, our algorithm picks the most relevant
concepts, and thus is able to expand to concepts
which are not explicitly mentioned in the docu-
ment. The successful use of background informa-
tion such as the one in WordNet could help close
the gap between semantic web technologies and
IR, and opens the possibility to include other re-
sources like Wikipedia or domain ontologies like
those in the Unified Medical Language System.
Our method to integrate expanded terms using
an additional index is simple and straightforward,
and there is still ample room for improvement.
A tighter integration of the document expansion
technique in the retrieval model should yield bet-
ter results, and the smoothed language models of
(Mei et al, 2008; Huang et al, 2009) seem a
natural choice. We would also like to compare
with other existing query and document expan-
sion techniques and study whether our technique
is complementary to query expansion approaches.
Acknowledgments
This work has been supported by KNOW2
(TIN2009-14715-C04-01) and KYOTO (ICT-
2007-211423) projects. Arantxa Otegi?s work is
funded by a PhD grant from the Basque Govern-
ment. Part of this work was done while Arantxa
Otegi was visiting Yahoo! Research Barcelona.
References
Agirre, E. and A. Soroa. 2009. Personalizing PageR-
ank for Word Sense Disambiguation. In Proc. of
16
EACL 2009, Athens, Greece.
Agirre, E., G. M. Di Nunzio, N. Ferro, T. Mandl,
and C. Peters. 2008. CLEF 2008: Ad-Hoc Track
Overview. In Working Notes of the Cross-Lingual
Evaluation Forum.
Agirre, E., G. M. Di Nunzio, T. Mandl, and A. Otegi.
2009a. CLEF 2009 Ad Hoc Track Overview: Ro-
bust - WSD Task. In Working Notes of the Cross-
Lingual Evaluation Forum.
Agirre, E., A. Soroa, E. Alfonseca, K. Hall, J. Kraval-
ova, and M. Pasca. 2009b. A Study on Similarity
and Relatedness Using Distributional and WordNet-
based Approaches. In Proc. of NAACL, Boulder,
USA.
Boldi, P. and S. Vigna. 2005. MG4J at TREC 2005.
In The Fourteenth Text REtrieval Conference (TREC
2005) Proceedings, number SP 500-266 in Special
Publications. NIST.
Collins-Thompson, Kevyn. 2009. Reducing the risk
of query expansion via robust constrained optimiza-
tion. In Proceedings of CIKM ?09, pages 837?846.
Fellbaum, C., editor. 1998. WordNet: An Elec-
tronic Lexical Database and Some of its Applica-
tions. MIT Press, Cambridge, Mass.
Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings ACL/COLING Work-
shop on Usage of WordNet for Natural Language
Processing.
Haveliwala, T. H. 2002. Topic-sensitive PageRank. In
Proceedings of WWW ?02, pages 517?526.
Huang, Yunping, Le Sun, and Jian-Yun Nie. 2009.
Smoothing document language model with local
word graph. In Proceedings of CIKM ?09, pages
1943?1946.
Kim, S. B., H. C. Seo, and H. C. Rim. 2004. Informa-
tion retrieval using word senses: root sense tagging
approach. In Proceedings of SIGIR ?04, pages 258?
265.
Kurland, O. and L. Lee. 2004. Corpus structure, lan-
guage models, and ad hoc information retrieval. In
Proceedings of SIGIR ?04, pages 194?201.
Liu, X. and W. B. Croft. 2004. Cluster-based retrieval
using language models. In Proceedings of SIGIR
?04, pages 186?193.
Liu, S., C. Yu, and W. Meng. 2005. Word sense dis-
ambiguation in queries. In Proceedings of CIKM
?05, pages 525?532.
Manning, C. D., P. Raghavan, and H. Schu?tze. 2009.
An introduction to information retrieval. Cam-
bridge University Press, UK.
Mei, Qiaozhu, Duo Zhang, and ChengXiang Zhai.
2008. A general optimization framework for
smoothing language models on graph structures. In
Proceedings of SIGIR ?08, pages 611?618.
Pen?as, A., P. Forner, R. Sutcliffe, A. Rodrigo,
C. Fora?scu, I. Alegria, D. Giampiccolo, N. Moreau,
and P. Osenova. 2009. Overview of ResPubliQA
2009: Question Answering Evaluation over Euro-
pean Legislation. In Working Notes of the Cross-
Lingual Evaluation Forum.
Robertson, S. and H. Zaragoza. 2009. The Proba-
bilistic Relevance Framework: BM25 and Beyond.
Foundations and Trends in Information Retrieval,
3(4):333?389.
Singhal, A. and F. Pereira. 1999. Document expansion
for speech retrieval. In Proceedings of SIGIR ?99,
pages 34?41, New York, NY, USA. ACM.
Smucker, M. D., J. Allan, and B. Carterette. 2007. A
comparison of statistical significance tests for infor-
mation retrieval evaluation. In Proc. of CIKM 2007,
Lisboa, Portugal.
Stokoe, C., M. P. Oakes, and J. Tait. 2003. Word sense
disambiguation in information retrieval revisited. In
Proceedings of SIGIR ?03, page 166.
Surdeanu, M., M. Ciaramita, and H. Zaragoza. 2008.
Learning to Rank Answers on Large Online QA
Collections. In Proceedings of ACL 2008.
Tao, T., X. Wang, Q. Mei, and C. Zhai. 2006. Lan-
guage model information retrieval with document
expansion. In Proceedings of HLT/NAACL, pages
407?414, June.
Voorhees, E. M. 1994. Query expansion using lexical-
semantic relations. In Proceedings of SIGIR ?94,
page 69.
17
