Proceedings of the 12th Conference of the European Chapter of the ACL, pages 24?32,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Contextual Phrase-Level Polarity Analysis using Lexical Affect Scoring
and Syntactic N-grams
Apoorv Agarwal
Department of Computer Science
Columbia University
New York, USA
aa2644@columbia.edu
Fadi Biadsy
Department of Computer Science
Columbia University
New York, USA
fadi@cs.columbia.edu
Kathleen R. Mckeown
Department of Computer Science
Columbia University
New York, USA
kathy@cs.columbia.edu
Abstract
We present a classifier to predict con-
textual polarity of subjective phrases in
a sentence. Our approach features lexi-
cal scoring derived from the Dictionary of
Affect in Language (DAL) and extended
through WordNet, allowing us to automat-
ically score the vast majority of words in
our input avoiding the need for manual la-
beling. We augment lexical scoring with
n-gram analysis to capture the effect of
context. We combine DAL scores with
syntactic constituents and then extract n-
grams of constituents from all sentences.
We also use the polarity of all syntactic
constituents within the sentence as fea-
tures. Our results show significant im-
provement over a majority class baseline
as well as a more difficult baseline consist-
ing of lexical n-grams.
1 Introduction
Sentiment analysis is a much-researched area that
deals with identification of positive, negative and
neutral opinions in text. The task has evolved from
document level analysis to sentence and phrasal
level analysis. Whereas the former is suitable for
classifying news (e.g., editorials vs. reports) into
positive and negative, the latter is essential for
question-answering and recommendation systems.
A recommendation system, for example, must be
able to recommend restaurants (or movies, books,
etc.) based on a variety of features such as food,
service or ambience. Any single review sentence
may contain both positive and negative opinions,
evaluating different features of a restaurant. Con-
sider the following sentence (1) where the writer
expresses opposing sentiments towards food and
service of a restaurant. In tasks such as this, there-
fore, it is important that sentiment analysis be done
at the phrase level.
(1) The Taj has great food but I found their ser-
vice to be lacking.
Subjective phrases in a sentence are carriers of
sentiments in which an experiencer expresses an
attitude, often towards a target. These subjective
phrases may express neutral or polar attitudes de-
pending on the context of the sentence in which
they appear. Context is mainly determined by con-
tent and structure of the sentence. For example, in
the following sentence (2), the underlined subjec-
tive phrase seems to be negative, but in the larger
context of the sentence, it is positive.1
(2) The robber entered the store but his efforts
were crushed when the police arrived on time.
Our task is to predict contextual polarity of sub-
jective phrases in a sentence. A traditional ap-
proach to this problem is to use a prior polarity
lexicon of words to first set priors on target phrases
and then make use of the syntactic and semantic
information in and around the sentence to make
the final prediction. As in earlier approaches, we
also use a lexicon to set priors, but we explore
new uses of a Dictionary of Affect in Language
(DAL) (Whissel, 1989) extended using WordNet
(Fellbaum, 1998). We augment this approach with
n-gram analysis to capture the effect of context.
We present a system for classification of neutral
versus positive versus negative and positive versus
negative polarity (as is also done by (Wilson et al,
2005)). Our approach is novel in the use of fol-
lowing features:
? Lexical scores derived from DAL and ex-
tended through WordNet: The Dictionary
of Affect has been widely used to aid in in-
terpretation of emotion in speech (Hirschberg
1We assign polarity to phrases based on Wiebe (Wiebe et
al., 2005); the polarity of all examples shown here is drawn
from annnotations in the MPQA corpus. Clearly the assign-
ment of polarity chosen in this corpus depends on general
cultural norms.
24
et al, 2005). It contains numeric scores as-
signed along axes of pleasantness, activeness
and concreteness. We introduce a method for
setting numerical priors on words using these
three axes, which we refer to as a ?scoring
scheme? throughout the paper. This scheme
has high coverage of the phrases for classi-
fication and requires no manual intervention
when tagging words with prior polarities.
? N-gram Analysis: exploiting automatically
derived polarity of syntactic constituents
We compute polarity for each syntactic con-
stituent in the input phrase using lexical af-
fect scores for its words and extract n-grams
over these constituents. N-grams of syntactic
constituents tagged with polarity provide pat-
terns that improve prediction of polarity for
the subjective phrase.
? Polarity of Surrounding Constituents: We
use the computed polarity of syntactic con-
stituents surrounding the phrase we want to
classify. These features help to capture the
effect of context on the polarity of the sub-
jective phrase.
We show that classification of subjective
phrases using our approach yields better accuracy
than two baselines, a majority class baseline and a
more difficult baseline of lexical n-gram features.
We also provide an analysis of how the differ-
ent component DAL scores contribute to our re-
sults through the introduction of a ?norm? that
combines the component scores, separating polar
words that are less subjective (e.g., Christmas ,
murder) from neutral words that are more subjec-
tive (e.g., most, lack).
Section 2 presents an overview of previous
work, focusing on phrasal level sentiment analy-
sis. Section 3 describes the corpus and the gold
standard we used for our experiments. In sec-
tion 4, we give a brief description of DAL, dis-
cussing its utility and previous uses for emotion
and for sentiment analysis. Section 5 presents, in
detail, our polarity classification framework. Here
we describe our scoring scheme and the features
we extract from sentences for classification tasks.
Experimental set-up and results are presented in
Section 6. We conclude with Section 7 where we
also look at future directions for this research.
2 Literature Survey
The task of sentiment analysis has evolved from
document level analysis (e.g., (Turney., 2002);
(Pang and Lee, 2004)) to sentence level analy-
sis (e.g., (Hu and Liu., 2004); (Kim and Hovy.,
2004); (Yu and Hatzivassiloglou, 2003)). These
researchers first set priors on words using a prior
polarity lexicon. When classifying sentiment at
the sentence level, other types of clues are also
used, including averaging of word polarities or
models for learning sentence sentiment.
Research on contextual phrasal level sentiment
analysis was pioneered by Nasukawa and Yi
(2003), who used manually developed patterns to
identify sentiment. Their approach had high preci-
sion, but low recall. Wilson et al, (2005) also ex-
plore contextual phrasal level sentiment analysis,
using a machine learning approach that is closer to
the one we present. Both of these researchers also
follow the traditional approach and first set priors
on words using a prior polarity lexicon. Wilson
et al (2005) use a lexicon of over 8000 subjec-
tivity clues, gathered from three sources ((Riloff
and Wiebe, 2003); (Hatzivassiloglou and McKe-
own, 1997) and The General Inquirer2). Words
that were not tagged as positive or negative were
manually labeled. Yi et al (2003) acquired words
from GI, DAL and WordNet. From DAL, only
words whose pleasantness score is one standard
deviation away from the mean were used. Na-
sukawa as well as other researchers (Kamps and
Marx, 2002)) also manually tag words with prior
polarities. All of these researchers use categorical
tags for prior lexical polarity; in contrast, we use
quantitative scores, making it possible to use them
in computation of scores for the full phrase.
While Wilson et al (2005) aim at phrasal level
analysis, their system actually only gives ?each
clue instance its own label? [p. 350]. Their gold
standard is also at the clue level and assigns a
value based on the clue?s appearance in different
expressions (e.g., if a clue appears in a mixture of
negative and neutral expressions, its class is neg-
ative). They note that they do not determine sub-
jective expression boundaries and for this reason,
they classify at the word level. This approach is
quite different from ours, as we compute the po-
larity of the full phrase. The average length of
the subjective phrases in the corpus was 2.7 words,
with a standard deviation of 2.3. Like Wilson et al
2http://www.wjh.harvard.edu/ inquirer
25
(2005) we do not attempt to determine the bound-
ary of subjective expressions; we use the labeled
boundaries in the corpus.
3 Corpus
We used the Multi-Perspective Question-
Answering (MPQA version 1.2) Opinion corpus
(Wiebe et al, 2005) for our experiments. We
extracted a total of 17,243 subjective phrases
annotated for contextual polarity from the corpus
of 535 documents (11,114 sentences). These
subjective phrases are either ?direct subjective?
or ?expressive subjective?. ?Direct subjective?
expressions are explicit mentions of a private state
(Quirk et al, 1985) and are much easier to clas-
sify. ?Expressive subjective? phrases are indirect
or implicit mentions of private states and therefore
are harder to classify. Approximately one third of
the phrases we extracted were direct subjective
with non-neutral expressive intensity whereas the
rest of the phrases were expressive subjective. In
terms of polarity, there were 2779 positive, 6471
negative and 7993 neutral expressions. Our Gold
Standard is the manual annotation tag given to
phrases in the corpus.
4 DAL
DAL is an English language dictionary built to
measure emotional meaning of texts. The samples
employed to build the dictionary were gathered
from different sources such as interviews, adoles-
cents? descriptions of their emotions and univer-
sity students? essays. Thus, the 8742 word dictio-
nary is broad and avoids bias from any one par-
ticular source. Each word is given three kinds of
scores (pleasantness ? also called evaluation, ee,
activeness, aa and imagery, ii) on a scale of 1 (low)
to 3 (high). Pleasantness is a measure of polarity.
For example, in Table 1, affection is given a pleas-
antness score of 2.77 which is closer to 3.0 and
is thus a highly positive word. Likewise, active-
ness is a measure of the activation or arousal level
of a word, which is apparent from the activeness
scores of slug and energetic in the table. The third
score, imagery, is a measure of the ease with which
a word forms a mental picture. For example, af-
fect cannot be imagined easily and therefore has a
score closer to 1, as opposed to flower which is a
very concrete and therefore has an imagery score
of 3.
A notable feature of the dictionary is that it has
different scores for various inflectional forms of a
word ( affect and affection) and thus, morphologi-
cal parsing, and the possibility of resulting errors,
is avoided. Moreover, Cowie et al, (2001) showed
that the three scores are uncorrelated; this implies
that each of the three scores provide complemen-
tary information.
Word ee aa ii
Affect 1.75 1.85 1.60
Affection 2.77 2.25 2.00
Slug 1.00 1.18 2.40
Energetic 2.25 3.00 3.00
Flower 2.75 1.07 3.00
Table 1: DAL scores for words
The dictionary has previously been used for de-
tecting deceptive speech (Hirschberg et al, 2005)
and recognizing emotion in speech (Athanaselis et
al., 2006).
5 The Polarity Classification Framework
In this section, we present our polarity classifi-
cation framework. The system takes a sentence
marked with a subjective phrase and identifies the
most likely contextual polarity of this phrase. We
use a logistic regression classifier, implemented
in Weka, to perform two types of classification:
Three way (positive, negative, vs. neutral) and
binary (positive vs. negative). The features we
use for classification can be broadly divided into
three categories: I. Prior polarity features com-
puted from DAL and augmented using WordNet
(Section 5.1). II. lexical features including POS
and word n-gram features (Section 5.3), and III.
the combination of DAL scores and syntactic fea-
tures to allow both n-gram analysis and polarity
features of neighbors (Section 5.4).
5.1 Scoring based on DAL and WordNet
DAL is used to assign three prior polarity scores
to each word in a sentence. If a word is found in
DAL, scores of pleasantness (ee), activeness (aa),
and imagery (ii) are assigned to it. Otherwise, a
list of the word?s synonyms and antonyms is cre-
ated using WordNet. This list is sequentially tra-
versed until a match is found in DAL or the list
ends, in which case no scores are assigned. For
example, astounded, a word absent in DAL, was
scored by using its synonym amazed. Similarly,
in-humane was scored using the reverse polarity of
26
its antonym humane, present in DAL. These scores
are Z-Normalized using the mean and standard de-
viation measures given in the dictionary?s manual
(Whissel, 1989). It should be noted that in our cur-
rent implementation all function words are given
zero scores since they typically do not demonstrate
any polarity. The next step is to boost these nor-
malized scores depending on how far they lie from
the mean. The reason for doing this is to be able
to differentiate between phrases like ?fairly decent
advice? and ?excellent advice?. Without boosting,
the pleasantness scores of both phrases are almost
the same. To boost the score, we multiply it by
the number of standard deviations it lies from the
mean.
After the assignment of scores to individual
words, we handle local negations in a sentence by
using a simple finite state machine with two states:
RETAIN and INVERT. In the INVERT state, the
sign of the pleasantness score of the current word
is inverted, while in the RETAIN state the sign of
the score stays the same. Initially, the first word in
a given sentence is fed to the RETAIN state. When
a negation (e.g., not, no, never, cannot, didn?t)
is encountered, the state changes to the INVERT
state. While in the INVERT state, if ?but? is en-
countered, it switches back to the RETAIN state.
In this machine we also take care of ?not only?
which serves as an intensifier rather than nega-
tion (Wilson et al, 2005). To handle phrases like
?no better than evil? and ?could not be clearer?,
we also switch states from INVERT to RETAIN
when a comparative degree adjective is found after
?not?. For example, the words in phrase in Table
(2) are given positive pleasantness scores labeled
with positive prior polarity.
Phrase has no greater desire
POS VBZ DT JJR NN
(ee) 0 0 3.37 0.68
State RETAIN INVERT RETAIN RETAIN
Table 2: Example of scoring scheme using DAL
We observed that roughly 74% of the content
words in the corpus were directly found in DAL.
Synonyms of around 22% of the words in the cor-
pus were found to exist in DAL. Antonyms of
only 1% of the words in the corpus were found in
DAL. Our system failed to find prior semantic ori-
entations of roughly 3% of the total words in the
corpus. These were rarely occurring words like
apartheid, apocalyptic and ulterior. We assigned
zero scores for these words.
In our system, we assign three DAL scores, us-
ing the above scheme, for the subjective phrase
in a given sentence. The features are (1) ?ee, the
mean of the pleasantness scores of the words in the
phrase, (2) ?aa, the mean of the activeness scores
of the words in the phrase, and similarly (3) ?ii,
the mean of the imagery scores.
5.2 Norm
We gave each phrase another score, which we call
the norm, that is a combination of the three scores
from DAL. Cowie et al (2001) suggest a mecha-
nism of mapping emotional states to a 2-D contin-
uous space using an Activation-Evaluation space
(AE) representation. This representation makes
use of the pleasantness and activeness scores from
DAL and divides the space into four quadrants:
?delightful?, ?angry?, ?serene?, and ?depressed?.
Whissel (2008), observes that tragedies, which
are easily imaginable in general, have higher im-
agery scores than comedies. Drawing on these ap-
proaches and our intuition that neutral expressions
tend to be more subjective, we define the norm in
the following equation (1).
norm =
?
ee2 + aa2
ii
(1)
Words of interest to us may fall into the follow-
ing four broad categories:
1. High AE score and high imagery: These
are words that are highly polar and less sub-
jective (e.g., angel and lively).
2. Low AE score and low imagery: These are
highly subjective neutral words (e.g., gener-
ally and ordinary).
3. High AE score and low imagery: These are
words that are both highly polar and subjec-
tive (e.g., succeed and good).
4. Low AE score and high imagery: These are
words that are neutral and easily imaginable
(e.g., car and door).
It is important to differentiate between these
categories of words, because highly subjective
words may change orientation depending on con-
text; less subjective words tend to retain their prior
orientation. For instance, in the example sentence
from Wilson et al(2005)., the underlined phrase
27
seems negative, but in the context it is positive.
Since a subjective word like succeed depends on
?what? one succeeds in, it may change its polar-
ity accordingly. In contrast, less subjective words,
like angel, do not depend on the context in which
they are used; they evoke the same connotation as
their prior polarity.
(3) They haven?t succeeded and will never succeed
in breaking the will of this valiant people.
As another example, AE space scores of good-
ies and good turn out to be the same. What differ-
entiates one from the another is the imagery score,
which is higher for the former. Therefore, value of
the norm is lower for goodies than for good. Un-
surprisingly, this feature always appears in the top
10 features when the classification task contains
neutral expressions as one of the classes.
5.3 Lexical Features
We extract two types of lexical features, part of
speech (POS) tags and n-gram word features. We
count the number of occurrences of each POS in
the subjective phrase and represent each POS as
an integer in our feature vector.3 For each subjec-
tive phrase, we also extract a subset of unigram,
bigrams, and trigrams of words (selected automat-
ically, see Section 6). We represent each n-gram
feature as a binary feature. These types of features
were used to approximate standard n-gram lan-
guage modeling (LM). In fact, we did experiment
with a standard trigram LM, but found that it did
not improve performance. In particular, we trained
two LMs, one on the polar subjective phrases and
another on the neutral subjective phrases. Given a
sentence, we computed two perplexities of the two
LMs on the subjective phrase in the sentence and
added them as features in our feature vectors. This
procedure provided us with significant improve-
ment over a chance baseline but did not outper-
form our current system. We speculate that this
was caused by the split of training data into two
parts, one for training the LMs and another for
training the classifier. The resulting small quantity
of training data may be the reason for bad perfor-
mance. Therefore, we decided to back off to only
binary n-gram features as part of our feature vec-
tor.
3We use the Stanford Tagger to assign parts of speech tags
to sentences. (Toutanova and Manning, 2000)
5.4 Syntactic Features
In this section, we show how we can combine the
DAL scores with syntactic constituents. This pro-
cess involves two steps. First, we chunk each
sentence to its syntactic constituents (NP, VP,
PP, JJP, and Other) using a CRF Chunker.4 If
the marked-up subjective phrase does not contain
complete chunks (i.e., it partially overlaps with
other chunks), we expand the subjective phrase to
include the chunks that it overlaps with. We term
this expanded phrase as the target phrase, see Fig-
ure 1.
Second, each chunk in a sentence is then as-
signed a 2-D AE space score as defined by Cowie
et al, (2001) by adding the individual AE space
scores of all the words in the chunk and then nor-
malizing it by the number of words. At this point,
we are only concerned with the polarity of the
chunk (i.e., whether it is positive or negative or
neutral) and imagery will not help in this task; the
AE space score is determined from pleasantness
and activeness alone. A threshold, determined
empirically by analyzing the distributions of posi-
tive (pos), negative (neg) and neutral (neu) expres-
sions, is used to define ranges for these classes of
expressions. This enables us to assign each chunk
a prior semantic polarity. Having the semantic ori-
entation (positive, negative, neutral) and phrasal
tags, the sentence is then converted to a sequence
of encodings [Phrasal ? Tag]polarity. We mark
each phrase that we want to classify as a ?target? to
differentiate it from the other chunks and attach its
encoding. As mentioned, if the target phrase par-
tially overlaps with chunks, it is simply expanded
to subsume the chunks. This encoding is illus-
trated in Figure 1.
After these two steps, we extract a set of fea-
tures that are used in classifying the target phrase.
These include n-grams of chunks from the all
sentences, minimum and maximum pleasantness
scores from the chunks in the target phrase itself,
and the syntactic categories that occur in the con-
text of the target phrase. In the remainder of this
section, we describe how these features are ex-
tracted.
We extract unigrams, bigrams and trigrams of
chunks from all the sentences. For example, we
may extract a bigram from Figure 1 of [V P ]neu
followed by [PP ]targetneg . Similar to the lexical
4Xuan-Hieu Phan, ?CRFChunker: CRF English Phrase
Chunker?, http://crfchunker.sourceforge.net/, 2006.
28
?? ??????? ?? ?? ?? ?? ??
?????????
????????????????????? ?????????????????????
? ? ? ? ? ?
Figure 1: Converting a sentence with a subjective phrase to a sequence of chunks with their types and polarities
n-grams, for the sentence containing the target
phrase, we add binary values in our feature vec-
tor such that the value is 1 if the sentence contains
that chunk n-gram.
We also include two features related to the tar-
get phrase. The target phrase often consists of
many chunks. To detect if a chunk of the target
phrase is highly polar, minimum and maximum
pleasantness scores over all the chunks in the tar-
get phrase are noted.
In addition, we add features which attempt to
capture contextual information using the prior se-
mantic polarity assigned to each chunk both within
the target phrase itself and within the context of the
target phrase. In cases where the target phrase is
in the beginning of the sentence or at the end, we
simply assign zero scores. Then we compute the
frequency of each syntactic type (i.e., NP, VP, PP,
JJP) and polarity (i.e., positive, negative, neutral)
to the left of the target, to the right of the target
and for the target. This additional set of contextual
features yields 36 features in total: three polari-
ties: {positive, negative, neutral} * three contexts:
{left, target, right} * four chunk syntactic types:
{NP, VP, PP, JJP}.
The full set of features captures different types
of information. N-grams look for certain patterns
that may be specific to either polar or neutral senti-
ments. Minimum and maximum scores capture in-
formation about the target phrase standalone. The
last set of features incorporate information about
the neighbors of the target phrase. We performed
feature selection on this full set of n-gram related
features and thus, a small subset of these n-gram
related features, selected automatically (see sec-
tion 6) were used in the experiments.
6 Experiments and Results
Subjective phrases from the MPQA corpus were
used in 10-fold cross-validation experiments. The
MPQA corpus includes gold standard tags for each
Feature Types Accuracy Pos.* Neg.* Neu.*
Chance baseline 33.33% - - -
N-gram baseline 59.05% 0.602 0.578 0.592
DAL scores only 59.66% 0.635 0.635 0.539
+ POS 60.55% 0.621 0.542 0.655
+ Chunks 64.72% 0.681 0.665 0.596
+ N-gram (all) 67.51% 0.703 0.688 0.632
All (unbalanced) 70.76% 0.582 0.716 0.739
Table 3: Results of 3 way classification (Positive, Negative,
and Neutral). In the unbalanced case, majority class baseline
is 46.3% (*F-Measure).
Feature Types Accuracy Pos.* Neg.*
Chance baseline 50% - -
N-gram baseline 73.21% 0.736 0.728
DAL scores only 77.02% 0.763 0.728
+ POS 79.02% 0.788 0.792
+ Chunks 80.72% 0.807 0.807
+ N-gram (all) 82.32% 0.802 0.823
All (unbalanced) 84.08% 0.716 0.889
Table 4: Positive vs. Negative classification results. Baseline
is the majority class. In the unbalanced case, majority class
baseline is 69.74%. (* F-Measure)
phrase. A logistic classifier was used for two po-
larity classification tasks, positive versus negative
versus neutral and positive versus negative. We
report accuracy, and F-measure for both balanced
and unbalanced data.
6.1 Positive versus Negative versus Neutral
Table 3 shows results for a 3-way classifier. For
the balanced data-set, each class has 2799 in-
stances and hence the chance baseline is 33%. For
the unbalanced data-set, there are 2799 instances
of positive, 6471 instances of negative and 7993
instances of neutral phrases and thus the baseline
is about 46%. Results show that the accuracy in-
creases as more features are added. It may be
seen from the table that prior polarity scores do
not do well alone, but when used in conjunction
with other features they play an important role
in achieving an accuracy much higher than both
baselines (chance and lexical n-grams). To re-
29
Figure 2: (a) An example sentence with three annotated subjective phrases in the same sentence. (b) Part of the sentence with
the target phrase (B) and their chunks with prior polarities.
confirm if prior polarity scores add value, we ex-
perimented by using all features except the prior
polarity scores and noticed a drop in accuracy by
about 4%. This was found to be true for the
other classification task as well. The table shows
that parts of speech and lexical n-grams are good
features. A significant improvement in accuracy
(over 4%, p-value = 4.2e-15) is observed when
chunk features (i.e., n-grams of constituents and
polarity of neighboring constituents) are used in
conjunction with prior polarity scores and part of
speech features.5 This improvement may be ex-
plained by the following observation. The bi-
gram ?[Other]targetneu [NP ]neu? was selected as a
top feature by the Chi-square feature selector. So
were unigrams, [Other]targetneu and [Other]
target
neg .
We thus learned n-gram patterns that are char-
acteristic of neutral expressions (the just men-
tioned bigram and the first of the unigrams) as
well as a pattern found mostly in negative ex-
pressions (the latter unigram). It was surpris-
ing to find another top chunk feature, the bigram
?[Other]targetneu [NP ]neg? (i.e., a neutral chunk of
syntactic type ?Other? preceding a negative noun
phrase), present in neutral expressions six times
more than in polar expressions. An instance where
these chunk features could have been responsi-
ble for the correct prediction of a target phrase is
shown in Figure 2. Figure 2(a) shows an exam-
ple sentence from the MPQA corpus, which has
three annotated subjective phrases. The manually
labeled polarity of phrases (A) and (C) is negative
and that of (B) is neutral. Figure 2(b) shows the
5We use the binomial test procedure to test statistical sig-
nificance throughout the paper.
relevant chunk bigram which is used to predict the
contextual polarity of the target phrase (B).
It was interesting to see that the top 10 features
consisted of all categories (i.e., prior DAL scores,
lexical n-grams and POS, and syntactic) of fea-
tures. In this and the other experiment, pleasant-
ness, activation and the norm were among the top
5 features. We ran a significance test to show the
importance of the norm feature in our classifica-
tion task and observed that it exerted a significant
increase in accuracy (2.26%, p-value = 1.45e-5).
6.2 Positive versus Negative
Table 4 shows results for positive versus negative
classification. We show results for both balanced
and unbalanced data-sets. For balanced, there are
2779 instances of each class. For the unbalanced
data-set, there are 2779 instances of positive and
6471 instances of neutral, thus our chance base-
line is around 70%. As in the earlier classification,
accuracy and F-measure increase as we add fea-
tures. While the increase of adding the chunk fea-
tures, for example, is not as great as in the previous
classification, it is nonetheless significant (p-value
= 0.0018) in this classification task. The smaller
increase lends support to our hypothesis that po-
lar expressions tend to be less subjective and thus
are less likely to be affected by contextual polar-
ity. Another thing that supports our hypothesis that
neutral expressions are more subjective is the fact
that the rank of imagery (ii), dropped significantly
in this classification task as compared to the previ-
ous classification task. This implies that imagery
has a much lesser role to play when we are dealing
with non-neutral expressions.
30
7 Conclusion and Future Work
We present new features (DAL scores, norm
scores computed using DAL, n-gram over chunks
with polarity) for phrasal level sentiment analysis.
They work well and help in achieving high accu-
racy in a three-way classification of positive, neg-
ative and neutral expressions. We do not require
any manual intervention during feature selection,
and thus our system is fully automated. We also
introduced a 3-D representation that maps differ-
ent classes to spatial coordinates.
It may seem to be a limitation of our system that
it requires accurate expression boundaries. How-
ever, this is not true for the following two reasons:
first, Wiebe et al, (2005) declare that while mark-
ing the span of subjective expressions and hand
annotating the MPQA corpus, the annotators were
not trained to mark accurate expression bound-
aries. The only constraint was that the subjective
expression should be within the mark-ups for all
annotators. Second, we expanded the marked sub-
jective phrase to subsume neighboring phrases at
the time of chunking.
A limitation of our scoring scheme is that it
does not handle polysemy, since words in DAL
are not provided with their parts of speech. Statis-
tics show, however, that most words occurred with
primarily one part of speech only. For example,
?will? occurred as modal 1272 times in the corpus,
whereas it appeared 34 times as a noun. The case
is similar for ?like? and ?just?, which mostly occur
as a preposition and an adverb, respectively. Also,
in our state machine, we haven?t accounted for the
impact of connectives such as ?but? or ?although?;
we propose drawing on work in argumentative ori-
entation to do so ((Anscombre and Ducrot, 1983);
(Elhadad and McKeown, 1990)).
For future work, it would be interesting to do
subjectivity and intensity classification using the
same scheme and features. Particularly, for the
task of subjectivity analysis, we speculate that the
imagery score might be useful for tagging chunks
with ?subjective? and ?objective? instead of posi-
tive, negative, and neutral.
Acknowledgments
This work was supported by the National Science
Foundation under the KDD program. Any opin-
ions, ndings, and conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reect the views of the National
Science Foundation. score.
We would like to thank Julia Hirschberg for use-
ful discussion. We would also like to acknowledge
Narayanan Venkiteswaran for implementing parts
of the system and Amal El Masri, Ashleigh White
and Oliver Elliot for their useful comments.
References
J.C. Anscombre and O. Ducrot. 1983. Philosophie et
langage. l?argumentation clans la langue. Bruxelles:
Pierre Mardaga.
T. Athanaselis, S. Bakamidis, , and L. Dologlou. 2006.
Automatic recognition of emotionally coloured
speech. In Proceedings of World Academy of Sci-
ence, Engineering and Technology, volume 12, ISSN
1307-6884.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Vot-
sis, S. Kollias, and W. Fellenz et al 2001. Emo-
tion recognition in human-computer interaction. In
IEEE Signal Processing Magazine, 1, 32-80.
M. Elhadad and K. R. McKeown. 1990. Generating
connectives. In Proceedings of the 13th conference
on Computational linguistics, pages 97?101, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. In MIT press.
V. Hatzivassiloglou and K. McKeown. 1997. Predict-
ing the semantic orientation of adjectives. In Pro-
ceedings of ACL.
J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, and
S. Friedman. 2005. Distinguishing deceptive from
non-deceptive speech. In Proceedings of Inter-
speech, 1833-1836.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD.
J. Kamps and M. Marx. 2002. Words with attitude. In
1st International WordNet Conference.
S. M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In In Coling.
T. Nasukawa and J. Yi. 2003. Sentiment analysis:
Capturing favorability using natural language pro-
cessing. In Proceedings of K-CAP.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity analysis using
subjectivity summarization based on minimum cuts.
In Proceedings of ACL.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik.
1985. A comprehensive grammar of the english lan-
guage. Longman, New York.
31
E. Riloff and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
EMNLP.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC-2000), pp. 63-70.
P. Turney. 2002. Thumbs up or thumbs down? seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of ACL.
C. M. Whissel. 1989. The dictionary of affect in lan-
guage. In R. Plutchik and H. Kellerman, editors,
Emotion: theory research and experience, volume 4,
Acad. Press., London.
C. M. Whissell. 2008. A psychological investiga-
tion of the use of shakespeare=s emotional language:
The case of his roman tragedies. In Edwin Mellen
Press., Lewiston, NY.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
In Language Resources and Evaluation, volume 39,
issue 2-3, pp. 165-210.
T. Wilson, J. Wiebe, and P. Hoffman. 2005. Recog-
nizing contextual polarity in phrase level sentiment
analysis. In Proceedings of ACL.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extracting sentiments about a
given topic using natural language processing tech-
niques. In Proceedings of IEEE ICDM.
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP.
32
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 397?405,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving the Arabic Pronunciation Dictionary for Phone and Word
Recognition with Linguistically-Based Pronunciation Rules
Fadi Biadsy? and Nizar Habash? and Julia Hirschberg?
?Department of Computer Science, Columbia University, New York, USA
{fadi,julia}@cs.columbia.edu
?Center for Computational Learning Systems, Columbia University, New York, USA
habash@ccls.columbia.edu
Abstract
In this paper, we show that linguistically mo-
tivated pronunciation rules can improve phone
and word recognition results for Modern Stan-
dard Arabic (MSA). Using these rules and
the MADA morphological analysis and dis-
ambiguation tool, multiple pronunciations per
word are automatically generated to build two
pronunciation dictionaries; one for training
and another for decoding. We demonstrate
that the use of these rules can significantly
improve both MSA phone recognition and
MSA word recognition accuracies over a base-
line system using pronunciation rules typi-
cally employed in previous work on MSA Au-
tomatic Speech Recognition (ASR). We ob-
tain a significant improvement in absolute ac-
curacy in phone recognition of 3.77%?7.29%
and a significant improvement of 4.1% in ab-
solute accuracy in ASR.
1 Introduction
The correspondence between orthography and pro-
nunciation in Modern Standard Arabic (MSA) falls
somewhere between that of languages such as Span-
ish and Finnish, which have an almost one-to-one
mapping between letters and sounds, and languages
such as English and French, which exhibit a more
complex letter-to-sound mapping (El-Imam, 2004).
The more complex this mapping is, the more diffi-
cult the language is for Automatic Speech Recogni-
tion (ASR).
An essential component of an ASR system is its
pronunciation dictionary (lexicon), which maps the
orthographic representation of words to their pho-
netic or phonemic pronunciation variants. For lan-
guages with complex letter-to-sound mappings, such
dictionaries are typically written by hand. However,
for morphologically rich languages, such as MSA,1
pronunciation dictionaries are difficult to create by
hand, because of the large number of word forms,
each of which has a large number of possible pro-
nunciations. Fortunately, the relationship between
orthography and pronunciation is relatively regu-
lar and well understood for MSA. Moreover, re-
cent automatic techniques for morphological anal-
ysis and disambiguation (MADA) can also be useful
in automating part of the dictionary creation process
(Habash and Rambow, 2005; Habash and Rambow,
2007) Nonetheless, most documented Arabic ASR
systems appear to handle only a subset of Arabic
phonetic phenomena; very few use morphological
disambiguation tools.
In Section 2, we briefly describe related work, in-
cluding the baseline system we use. In Section 3, we
outline the linguistic phenomena we believe are crit-
ical to improving MSA pronunciation dictionaries.
In Section 4, we describe the pronunciation rules we
have developed based upon these linguistic phenom-
ena. In Section 5, we describe how these rules are
used, together with MADA, to build our pronuncia-
tion dictionaries for training and decoding automat-
ically. In Section 6, we present results of our eval-
uations of our phone- and word-recognition systems
(XPR and XWR) on MSA comparing these systems
to two baseline systems, BASEPR and BASEWR.
1MSA words have fourteen features: part-of-speech, person,
number, gender, voice, aspect, determiner proclitic, conjunctive
proclitic, particle proclitic, pronominal enclitic, nominal case,
nunation, idafa (possessed), and mood. MSA features are real-
ized using both concatenative (affixes and stems) and templatic
(root and patterns) morphology with a variety of morphological
and phonological adjustments that appear in word orthography
and interact with orthographic variations.
397
We conclude in Section 7 and identify directions for
future research.
2 Related Work
Most recent work on ASR for MSA uses a sin-
gle pronunciation dictionary constructed by map-
ping every undiacritized word in the training cor-
pus to all of the diacritized Buckwalter analyses and
the diacritized versions of this word in the Arabic
Treebank (Maamouri et al, 2003; Afify et al, 2005;
Messaoudi et al, 2006; Soltau et al, 2007). In these
papers, each diacritized word is converted to a sin-
gle pronunciation with a one-to-one mapping using
?very few? unspecified rules. None of these systems
use morphological disambiguation to determine the
most likely pronunciation of the word given its con-
text. Vergyri et al (2008) do use morphological in-
formation to predict word pronunciation. They se-
lect the top choice from the MADA (Morphological
Analysis and Disambiguation for Arabic) system for
each word to train their acoustic models. For the test
lexicon they used the undiacritized orthography, as
well as all diacritizations found for each word in the
training data as possible pronunciation variants. We
use this system as our baseline for comparison.
3 Arabic Orthography and Pronunciation
MSA is written in a morpho-phonemic orthographic
representation using the Arabic script, an alphabet
accented with optional diacritical marks.2 MSA has
34 phonemes (28 consonants, 3 long vowels and 3
short vowels). The Arabic script has 36 basic let-
ters (ignoring ligatures) and 9 diacritics. Most Ara-
bic letters have a one-to-one mapping to an MSA
phoneme; however, there are a small number of
common exceptions (Habash et al, 2007; El-Imam,
2004) which we summarize next.
3.1 Optional Diacritics
Arabic script commonly uses nine optional diacrit-
ics: (a) three short-vowel diacritics representing the
vowels /a/, /u/ and /i/; (b) one long-vowel diacritic
(Dagger Alif ?) representing the long vowel /A/ in a
2We provide Arabic script orthographic transliteration in
the Buckwalter transliteration scheme (Buckwalter, 2004). For
Modern Standard Arabic phonological transcription, we use a
variant of the Buckwalter transliteration with the following ex-
ceptions: glottal stops are represented as /G/ and long vowels as
/A/, /U/ and /I/. All Arabic script diacritics are phonologically
spelled out.
small number of words; (c) three nunation diacrit-
ics (F /an/, N /un/, K /in/) representing a combina-
tion of a short vowel and the nominal indefiniteness
marker /n/ in MSA; (d) one consonant lengthening
diacritic (called Shadda ?) which repeats/elongates
the previous consonant (e.g., kat?ab is pronounced
/kattab/); and (e) one diacritic for marking when
there is no diacritic (called Sukun o).
Arabic diacritics can only appear after a let-
ter. Word-initial diacritics (in practice, only short
vowels) are handled by adding an extra Alif A A
(also called Hamzat-Wasl) at the beginning of the
word. Sentence/utterance initial Hamzat-Wasl is
pronounced like a glottal stop preceding the short
vowel; however, the sentence medial Hamzat-Wasl
is silent except for the short vowel. For exam-
ple, Ainkataba kitAbN is /Ginkataba kitAbun/ but
kitAbN Ainkataba is /kitAbun inkataba/. A ?real?
Hamza (glottal stop) is always pronounced as a glot-
tal stop. The Hamzat-Wasl appears most commonly
as the Alif of the definite article Al. It also appears
in specific words and word classes such as relative
pronouns (e.g., Aly ?who? and verbs in pattern VII
(Ain1a2a3).
Arabic short vowel diacritics are used together
with the glide consonant letters w and y to denote
the long vowels /U/ (as uw) and /I/ (iy). This makes
these two letters ambiguous.
Diacritics are largely restricted to religious texts
and Arabic language school textbooks. In other
texts, fewer than 1.5% of words contain a diacritic.
Some diacritics are lexical (where word meaning
varies) and others are inflectional (where nominal
case or verbal mood varies). Inflectional diacritics
are typically word final. Since nominal case, verbal
mood and nunation have all disappeared in spoken
dialectal Arabic, Arabic speakers do not always pro-
duce these inflections correctly or at all.
Much work has been done on automatic Arabic
diacritization (Vergyri and Kirchhoff, 2004; Anan-
thakrishnan et al, 2005; Zitouni et al, 2006; Habash
and Rambow, 2007). In this paper, we use the
MADA (Morphological Analysis and Disambigua-
tion for Arabic) system to diacritize Arabic (Habash
and Rambow, 2005; Habash and Rambow, 2007).
MADA, which uses the Buckwalter Arabic mor-
phological Analyzer databases (Buckwalter, 2004),
provides the necessary information to determine
Hamzat-Wasl through morphologically tagging the
definite article; in most other cases it outputs the spe-
cial symbol ?{? for Hamzat-Wasl.
398
3.2 Hamza Spelling
The consonant Hamza (glottal stop /G/) has multi-
ple forms in Arabic script:  ?,  >,  <,  &, ?
}, Proceedings of ACL-08: HLT, pages 807?815,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Unsupervised Approach to Biography Production using Wikipedia
Fadi Biadsy,? Julia Hirschberg? and Elena Filatova*
?Department of Computer Science
Columbia University, New York, NY 10027, USA
{fadi,julia}@cs.columbia.edu
*InforSense LLC
Cambridge, MA 02141, USA
efilatova@inforsense.com
Abstract
We describe an unsupervised approach to
multi-document sentence-extraction based
summarization for the task of producing
biographies. We utilize Wikipedia to auto-
matically construct a corpus of biographical
sentences and TDT4 to construct a corpus
of non-biographical sentences. We build a
biographical-sentence classifier from these
corpora and an SVM regression model for
sentence ordering from the Wikipedia corpus.
We evaluate our work on the DUC2004
evaluation data and with human judges.
Overall, our system significantly outperforms
all systems that participated in DUC2004,
according to the ROUGE-L metric, and is
preferred by human subjects.
1 Introduction
Producing biographies by hand is a labor-intensive
task, generally done only for famous individuals.
The process is particularly difficult when persons of
interest are not well known and when information
must be gathered from a wide variety of sources. We
present an automatic, unsupervised, multi-document
summarization (MDS) approach based on extractive
techniques to producing biographies, answering the
question ?Who is X??
There is growing interest in automatic MDS in
general due in part to the explosion of multilingual
and multimedia data available online. The goal of
MDS is to automatically produce a concise, well-
organized, and fluent summary of a set of docu-
ments on the same topic. MDS strategies have been
employed to produce both generic summaries and
query-focused summaries. Due to the complexity
of text generation, most summarization systems em-
ploy sentence-extraction techniques, in which the
most relevant sentences from one or more docu-
ments are selected to represent the summary. This
approach is guaranteed to produce grammatical sen-
tences, although they must subsequently be ordered
appropriately to produce a coherent summary.
In this paper we describe a sentence-extraction
based MDS procedure to produce biographies from
online resources automatically. We make use of
Wikipedia, the largest free multilingual encyclope-
dia on the internet, to build a biographical-sentence
classifier and a component for ordering sentences in
the output summary. Section 2 presents an overview
of our system. In Section 3 we describe our cor-
pus and in Section 4 we discuss the components of
our system in more detail. In Section 5, we present
an evaluation of our work on the Document Under-
standing Conference of 2004 (DUC2004), the biog-
raphy task (task 5) test set. In Section 6 we com-
pare our research with previous work on biography
generation. We conclude in Section 7 and identify
directions for future research.
2 System Overview
In this section, we present an overview of our biog-
raphy extraction system. We assume as input a set of
documents retrieved by an information retrieval en-
gine from a query consisting of the name of the per-
son for whom the biography is desired. We further
assume that these documents have been tagged with
Named Entities (NE)s with coreferences resolved
807
using a system such as NYU?s 2005 ACE system
(Grishman et al, 2005), which we used for our ex-
periments. Our task is to produce a concise biogra-
phy from these documents.
First, we need to select the most ?important? bio-
graphical sentences for the target person. To do so,
we first extract from the input documents all sen-
tences that contain some reference to the target per-
son according to the coreference assignment algo-
rithm; this reference may be the target?s name or
a coreferential full NP or pronominal referring ex-
pression, such as the President or he. We call these
sentences hypothesis sentences. We hypothesize that
most ?biographical? sentences will contain a refer-
ence to the target. However, some of these sentences
may be irrelevant to a biography; therefore, we filter
them using a binary classifier that retains only ?bio-
graphical? sentences. These biographical sentences
may also include redundant information; therefore,
we cluster them and choose one sentence from each
cluster to represent the information in that cluster.
Since some of these sentences have more salient bi-
ographical information than others and since manu-
ally produced biographies tend to include informa-
tion in a certain order, we reorder our summary sen-
tences using an SVM regression model trained on
biographies. Finally, the first reference to the tar-
get person in the initial sentence in the reordering
is rewritten using the longest coreference in our hy-
pothesis sentences which contains the target?s full
name. We then trim the output to a threshold to pro-
duce a biography of a certain length for evaluation
against the DUC2004 systems.
3 Training Data
One of the difficulties inherent in automatic biog-
raphy generation is the lack of training data. One
might collect training data by manually annotating
a suitable corpus containing biographical and non-
biographical data about a person, as in (Zhou et al,
2004). However, such annotation is labor intensive.
To avoid this problem, we adopt an unsupervised ap-
proach. We use Wikipedia biographies as our corpus
of ?biographical? sentences. We collect our ?non-
biographical? sentences from the English newswire
documents in the TDT4 corpus.1 While each corpus
1http://projects.ldc.upenn.edu/TDT4
may contain positive and negative examples, we as-
sume that most sentences in Wikipedia biographies
are biographical and that the majority of TDT4 sen-
tences are non-biographical.
3.1 Constructing the Biographical Corpus
To automatically collect our biographical sentences,
we first download the xml version of Wikipedia
and extract only the documents whose authors used
the Wikipedia biography template when creating
their biography. There are 16,906 biographies in
Wikipedia that used this template. We next apply
simple text processing techniques to clean the text.
We select at most the first 150 sentences from each
page, to avoid sentences that are not critically impor-
tant to the biography. For each of these sentences we
perform the following steps:
1. We identify the biography?s subject from its ti-
tle, terming this name the ?target person.?
2. We run NYU?s 2005 ACE system (Grish-
man et al, 2005) to tag NEs and do coref-
erence resolution. There are 43 unique NE
tags in our corpora, including PER Individual,
ORG Educational, and so on, and TIMEX tags
for all dates.
3. For each sentence, we replace each NE by its
tag name and type ([name-type subtype]) as as-
signed by the NYU tagger. This modified sen-
tence we term a class-based/lexical sentence.
4. Each non-pronominal referring expression
(e.g., George W. Bush, the US president) that
is tagged as coreferential with the target per-
son is replaced by our own [TARGET PER] tag
and every pronoun P that refers to the target
person is replaced by [TARGET P], where P is
the pronoun itself. This allows us to general-
ize our sentences while retaining a) the essen-
tial distinction between this NE (and its role in
the sentence) and all other NEs in the sentence,
and b) the form of referring expressions.
5. Sentences containing no reference to the tar-
get person are assumed to be irrelevant and re-
moved from the corpus, as are sentences with
808
fewer than 4 tokens; short sentences are un-
likely to contain useful information beyond the
target reference.
For example, given sentences from the Wikipedia
biography of Martin Luther King, Jr. we produce
class-based/lexical sentences as follows:
Martin Luther King, Jr., was born on January 15, 1929, in Atlanta,
Georgia. He was the son of Reverend Martin Luther King, Sr. and
Alberta Williams King. He had an older sister, Willie Christine
(September 11, 1927) and a younger brother, Albert Daniel.
[TARGET PER], was born on [TIMEX], in [GPE PopulationCenter].
[TARGET HE] was the son of [PER Individual] and [PER Individual].
[TARGET HE] had an older sister, [PER Individual] ([TIMEX]) and a
younger brother, [PER Individual].
3.2 Constructing the Non-Biographical Corpus
We use the TDT4 corpus to identify non-
biographical sentences. Again, we run NYU?s 2005
ACE system to tag NEs and do coreference resolu-
tion on each news story in TDT4. Since we have
no target name for these stories, we select an NE
tagged as PER Individual at random from all NEs in
the story to represent the target person. We exclude
any sentence with no reference to this target person
and produce class-based/lexical sentences as above.
4 Our Biography Extraction System
4.1 Classifying Biographical Sentences
Using the biographical and non-biographical cor-
pora described in Section 3, we train a binary classi-
fier to determine whether a new sentence should be
included in a biography or not. For our experiments
we extracted 30,002 sentences from Wikipedia bi-
ographies and held out 2,108 sentences for test-
ing. Similarly. we extracted 23,424 sentences from
TDT4, and held out 2,108 sentences for testing.
For each sentence, we then extract the frequency of
three class-based/lexical features ? unigram, bia-
gram, and trigram ? and two POS features ? the
frequency of unigram and bigram POS. To reduce
the dimensionality of our feature space, we first sort
the features in decreasing order of Chi-square statis-
tics computed from the contingency tables of the ob-
served frequencies from the training data. We then
take the highest 30-80% features, where the num-
ber of features used is determined empirically for
Classifier Accuracy F-Measure
SVM 87.6% 0.87
M. na??ve Bayes 84.1% 0.84
C4.5 81.8% 0.82
Table 1: Binary classification results: Wikipedia bi-
ography class-based/lexical sentences vs. TDT4 class-
based/lexical sentences
each feature type. This process identifies features
that significantly contribute to the classification task.
We extract 3K class-based/lexical unigrams, 5.5K
bigrams, 3K trigrams, 20 POS unigrams, and 166
POS bigrams.
Using the training data described above, we ex-
perimented with three different classification algo-
rithms using the Weka machine learning toolkit
(Witten et al, 1999): multinomial na??ve Bayes,
SVM with linear kernel, and C4.5. Weka also pro-
vides a classification confidence score that repre-
sents how confident the classifier is on each classi-
fied sample, which we will make use of as well.
Table 1 presents the classification results on our
4,216 held-out test-set sentences. These results are
quite promising. However, we should note that they
may not necessarily represent the successful clas-
sification of biographical vs. non-biographical sen-
tences but rather the classification of Wikipedia sen-
tences vs. TDT4 sentences. We will validate these
results for our full systems in Section 5.
4.2 Removing Redundant Sentences
Typically, redundancy removal is a standard com-
ponent in MDS systems. In sentence-extraction
based summarizers, redundant sentences are defined
as those which include the same information with-
out introducing new information and identified by
some form of lexically-based clustering. We use
an implementation of a single-link nearest neighbor
clustering technique based on stem-overlap (Blair-
Goldensohn et al, 2004b) to cluster the sentences
classified as biographical by our classifier, and then
select the sentence from each cluster that maximizes
the confidence score returned by the classifier as the
representative for that cluster.
4.3 Sentence Reordering
It is essential for MDS systems in the extraction
framework to choose the order in which sentences
809
should be presented in the final summary. Present-
ing more important information earlier in a sum-
mary is a general strategy for most domains, al-
though importance may be difficult to determine re-
liably. Similar to (Barzilay and Lee, 2004), we au-
tomatically learn how to order our biographical sen-
tences by observing the typical order of presentation
of information in a particular domain. We observe
that our Wikipedia biographies tend to follow a gen-
eral presentation template, in which birth informa-
tion is mentioned before death information, infor-
mation about current professional position and af-
filiations usually appear early in the biography, and
nuclear family members are typically mentioned be-
fore more distant relations. Learning how to order
information from these biographies however would
require that we learn to identify particular types of
biographical information in sentences.
We directly use the position of each sentence in
each Wikipedia biography as a way of determin-
ing where sentences containing similar information
about different target individuals should appear in
their biographies. We represent the absolute posi-
tion of each sentence in its biography as an inte-
ger and train an SVM regression model with RBF
kernel, from the class/lexical features of the sen-
tence to its position. We represent each sentence by
a feature vector whose elements correspond to the
frequency of unigrams and bigrams of class-based
items (e.g., GPE, PER) (cf. Section 3) and lexical
items; for example, the unigrams born, became, and
[GPE State-or-Province], and the bigrams was born,
[TARGET PER] died and [TARGET PER] joined
would be good candidates for such features.
To minimize the dimensionality of our regres-
sion space, we constrained our feature choice to
those features that are important to distinguish bi-
ographical sentences, which we term biographical
terms. Since we want these biographical terms to
impact the regression function, we define these to
be phrases that consist of at least one lexical item
that occurs in many biographies but rarely more than
once in any given biography. We compute the bio-
graphical term score as in the following equation:
bio score(t)= | Dt || D | ?
?
d?Dt(1?
n(t)d
maxt(n(t)d) )
| D | (1)
where D is the set of 16,906 Wikipedia biographies,
n(t)d is the number of occurrences of term t in doc-
ument d, and Dt = {d ? D : t ? d}. The left factor
represents the document frequency of term t, and the
right factor calculates how infrequent the term is in
each biography that contains t at least once.2 We or-
der the unigrams and bigrams in the biographies by
their biographical term scores and select the high-
est 1K unigrams and 500 bigrams; these thresholds
were determined empirically.
4.4 Reference Rewriting
We observe that news articles typically mention bio-
graphical information that occurs early in Wikipedia
biographies when they mention individuals for the
first time in a story (e.g. Stephen Hawking, the Cam-
bridge University physicist). We take advantage of
the fact that the coreference resolution system we
use tags full noun phrases including appositives as
part of NEs. Therefore, we initially search for the
sentence that contains the longest identified NE (of
type PER) that includes the target person?s full name
and is coreferential with the target according to the
reference resolution system; we denote this NE NE-
NP. If this sentence has already been classified as
a biographical sentence by our classifier, we simply
boost its rank in the summary to first. Otherwise,
when we order our sentences, we replace the refer-
ence to the target person in the first sentence by NE-
NP. For example, if the first sentence in the biogra-
phy we have produced for Jimmy Carter is He was
born in 1947 and a sentence not chosen for inclusion
in our biography Jimmy Carter, former U.S. Presi-
dent, visited the University of California last year.
contains the NE-NP, and Jimmy Carter and He are
coreferential, then the first sentence in our biography
will be rewritten as Jimmy Carter, former U.S. Presi-
dent, was born in 1947. Note that, in the evaluations
presented in Section 5, sentence order was modified
by this process in only eight summaries.
5 Evaluation
To evaluate our biography generation system, we use
the document sets created for the biography evalua-
2We considered various approaches to feature selection here,
such as comparing term frequency between our biographical
and non-biographical corpora. However, terms such as killed
and died, which are useful biographical terms, also occur fre-
quently in our non-biographical corpus.
810
ROUGE-L Average_F
0.25
0.275
0.3
0.325
0.35
0 1 2 3 4 5 6 7 8 9 10 11 12
SVM 
reg. onlytop-DUC2004 C4.5 SVM SVM  + SVM reg. MNB +SVM reg
 
MNBC4.5  +  SVM reg. SVM + baseline orderC4.5 +   baseline order MNB +   baseline order
Figure 1: Comparing our approaches against the top performing system in DUC2004 according to ROUGE-L (dia-
mond).
tion (task 5) of DUC2004.3 The task for systems
participating in this evalution was ? Given each doc-
ument cluster and a question of the form ?Who is
X??, where X is the name of a person or group of
people, create a short summary (no longer than 665
bytes) of the cluster that responds to the question.?
NIST assessors chose 50 clusters of TREC docu-
ments such that all the documents in a given cluster
provide at least part of the answer to this question.
Each cluster contained on average 10 documents.
NIST had 4 human summaries written for each clus-
ter. A baseline summary was also created for each
cluster by extracting the first 665 bytes of the most
recent document in the cluster. 22 systems partici-
pated in the competition, producing a total of 22 au-
tomatic summaries (restricted to 665 bytes) for each
cluster. We evaluate our system against the top per-
forming of these 22 systems, according to ROUGE-
L, which we denote top-DUC2004.4
5.1 Automatic Evaluation Using ROUGE
As noted in Section 4.1, we experimented with a
number of learning algorithms when building our
biographical-sentence classifier. For each machine
learning algorithm tested, we build a system that ini-
tially classifies the input list of sentences into bio-
graphical and non-biographical sentences and then
3http://duc.nist.gov/duc2004
4Note that this system out-performed 19 of the 22 systems
on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2
(p < .05) (Blair-Goldensohn et al, 2004a). No ROUGE metric
produced scores where this system scored significantly worse
than any other system. See Figure 2 below for a comparison
of all DUC2004 systems with our top system where all systems
are evaluated using ROUGE-L-1.5.5.
removes redundant sentences. Next, we produce
three versions of each system: one which imple-
ments a baseline ordering procedure, in which sen-
tences from the clusters are ordered by their ap-
pearance in their source document (e.g. any sen-
tence which occurred first in its original document
is placed first in the summary, with ties ordered ran-
domly within the set), a second which orders the
biographical sentences by the confidence score ob-
tained from the classifier, and a third which uses the
SVM regression as the reordering component. Fi-
nally, we run our reference rewriting component on
each and trim the output to 665 bytes.
We evaluate first using the ROUGE-L metric (Lin
and Hovy, 2003) with a 95% (ROUGE computed)
confidence interval for all systems and compared
these to the ROUGE-L score of the best-performing
DUC2004 system.5 The higher the ROUGE score,
the closer the summary is to the DUC2004 human
reference summaries. As shown in Figure 1, our
best performing system is the multinomial na??ve
Bayes classifier (MNB) using the classifier confi-
dence scores to order the sentences in the biography.
This system significantly outperforms the top ranked
DUC2004 system (top-DUC2004).6 The success of
this particularly learning algorithm on our task may
be due to: (1) the nature of our feature space ? n-
gram frequencies are modeled properly by a multi-
nomial distribution; (2) the simplicity of this classi-
fier particularly given our large feature dimensional-
5We used the same version (1.5.5) of the ROUGE metric to
compute scores for the DUC systems and baseline also.
6Significance for each pair of systems was determined by
paired t-test and calculated at the .05 significance level.
811
ity; and (3) the robustness of na??ve Bayes with re-
spect to noisy data: Not all sentences in Wikipedia
biographies are biographical sentences and some
sentences in TDT4 are biographical.
While the SVM regression reordering component
has a slight negative impact on the performance
of the MNB system, the difference between the
two versions is not significant. Note however, that
both the C4.5 and the SVM versions of our system
are improved by the SVM regression sentence re-
ordering. While neither performs better than top-
DUC2004 without this component, the C4.5 system
with SVM reordering is significantly better than top-
DUC2004 and the performance of the SVM sys-
tem with SVM regression is comparable to top-
DUC2004. In fact, when we use only the SVM
regression model to rank the hypothesis sentences,
without employing any classifier, then remove re-
dundant sentences, rewrite and trim the results, we
find that, interestingly, this approach also outper-
forms top-DUC2004, although the difference is not
statistically significant. However, we believe that
this is an area worth pursuing in future, with more
sophisticated features.
The following biography of Brian Jones was pro-
duced by our MNB system and then the sentences
were ordered using the SVM regression model:
Born in Bristol in 1947, Brian Jones, the co-pilot on the
Breitling mission, learned to fly at 16, dropping out of
school a year later to join the Royal Air Force. After earn-
ing his commercial balloon flying license, Jones became
a ballooning instructor in 1989 and was certified as an ex-
aminer for balloon flight licenses by the British Civil Avi-
ation Authority. He helped organize Breitling?s most re-
cent around-the-world attempts, in 1997 and 1998. Jones,
52, replaces fellow British flight engineer Tony Brown.
Jones, who is to turn 52 next week, is actually the team?s
third co-pilot. After 13 years of service, he joined a cater-
ing business and, in the 1980s,...
Figure 2 illustrates the performance of our MNB
system with classifier confidence score sentence or-
dering when compared to mean ROUGE-L-1.5.5
scores of DUC2004 human-generated summaries
and the 22 DUC2004 systems? summaries across all
summary tasks. Human summaries are labeled A-
H, DUC2004 systems 1-22, and our MNB system
is marked by the rectangle. Results are sorted by
mean ROUGE-L score. Note that our system perfor-
mance is actually comparable in ROUGE-L score to
one of the human summary generators and is signif-
icantly better that all DUC2004 systems, including
top-DUC2004, which is System 1 in the figure.
5.2 Manual Evaluation
ROUGE evaluation is based on n-gram overlap be-
tween the automatically produced summary and the
human reference summaries. Thus, it is not able to
measure how fluent or coherent a summary is. Sen-
tence ordering is one factor in determining fluency
and coherence. So, we conducted two experiments
to measure these qualities, one comparing our top-
performing system according to ROUGE-L score
(MNB) vs. the top-performing DUC2004 system
(top-DUC2004) and another comparing our top sys-
tem with two different ordering methods, classifier-
based and SVM regression.7 In each experiment,
summaries were trimmed to 665 bytes.
In the first experiment, three native American En-
glish speakers were presented with the 50 questions
(Who is X?). For each question they were given a
pair of summaries (presented in random order): one
was the output of our MNB system and the other
was the summary produced by the top-DUC2004
system. Subjects were asked to decide which sum-
mary was more responsive in form and content to the
question or whether both were equally responsive.
85.3% (128/150) of subject judgments preferred one
summary over the other. 100/128 (78.1%) of these
judgments preferred the summaries produced by our
MNB system over those produced by top-DUC2004.
If we compute the majority vote, there were 42/50
summaries in which at least two subjects made the
same choice. 37/42 (88.1%) of these majority judg-
ments preferred our system?s summary (using bino-
mial test, p = 4.4e?7). We used the weighted kappa
statistic with quadratic weighting (Cohen, 1968)
to determine the inter-rater agreement, obtaining a
mean pairwise ? of 0.441.
Recall from Section 5.1 that our SVM regression
reordering component slightly decreases the aver-
age ROUGE score (although not significantly) for
our MNB system. For our human evaluations, we
decided to evaluate the quality of the presentation
of our summaries with and without this compo-
7Note that top-DUC2004 was ranked sixth in the DUC 2004
manual evaluation, with no system performing significantly
better for coverage and only 1 system performing significantly
better for responsiveness.
812
ROUGE-L Average_F
0.2
0.25
0.3
0.35
0.4
0.45
B E H G F A D C * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BL 18 19 20 21 22
Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004
competing systems (1-22 anonymized), with the baseline system labeled BL.
nent to see if this reordering component affected hu-
man judgments even if it did not improve ROUGE
scores. For each question, we produced two sum-
maries from the sentences classified as biographi-
cal by the MNB classifier, one ordered by the con-
fidence score obtained by the MNB, in decreasing
order, and the other ordered by the SVM regression
values, in increasing order. Note that, in three cases,
the summary sentences were ordered identically by
both procedures, so we used only 47 summaries
for this evaluation. Three (different) native Amer-
ican English speakers were presented with the 47
questions for which sentence ordering differed. For
each question they were given the two summaries
(presented in random order) and asked to determine
which biography they preferred.
We found inter-rater agreement for these judg-
ments using Fleiss? kappa (Fleiss, 1971) to be only
moderate (?=0.362). However, when we computed
the majority vote for each question, we found that
61.7% (29/47) preferred the SVM regression order-
ing over the MNB classifier confidence score order-
ing. Although this difference is not statistically sig-
nificant, again we find the SVM regression ordering
results encouraging enough to motivate our further
research on improving such ordering procedures.
6 Related Work
The DUC2004 system achieving the highest over-
all ROUGE score, our top-DUC2004 in Section 5,
was Blair-Goldensohn et al (2004a)?s DefScriber,
which treats ?Who is X?? as a definition question
and targets definitional themes (e.g. genus-species)
found in the input document collections which in-
clude references to the target person. Extracted sen-
tences are then rewritten using a reference rewriting
system (Nenkova and McKeown, 2003) which at-
tempts to shorten subsequent references to the tar-
get. Sentences are ordered in the summary based
on a weighted combination of topic centrality, lex-
ical cohesion, and topic coverage scores. A simi-
lar approach is explored in Biryukov et al (2005),
which uses Topic Signatures (Lin and Hovy, 2000)
constructed around the target individual?s name to
identify sentences to be included in the biography.
Zhou et al (2004)?s biography generation system,
like ours, trains biographical and non-biographical
sentence classifiers to select sentences to be included
in the biography. Their system is trained on a hand-
annotated corpus of 130 biographies of 12 people,
tagged with 9 biographical elements (e.g., bio, ed-
ucation, nationality) and uses binary unigram and
bigram lexical and unigram part-of-speech features
for classification. Duboue et al (2003) also ad-
dress the problem of learning content selection rules
for biography. They learn rules from two corpora,
a semi-structured corpus with lists of biographical
facts about show business celebrities and a corpus
of free-text biographies about the same celebrities.
Filatova et al (2005) learn text features typical
of biographical descriptions by deducing biograph-
ical and occupation-related activities automatically
by compariing descriptions of people with differ-
ent occupations. Weischedel et al (2004) models
kernel-fact features typical for biographies using lin-
guistic and semantic processing. Linguistic features
813
are derived from predicate-argument structures de-
duced from parse trees, and semantic features are the
set of biography-related relations and events defined
in the ACE guidelines (Doddington et al, 2004).
Sentences containing kernel facts are ranked using
probabilities estimated from a corpus of manually
created biographies, including Wikipedia, to esti-
mate the conditional distribution of relevant material
given a kernel fact and a background corpus.
The problem of ordering sentences and preserv-
ing coherence in MDS is addressed by Barzi-
lay et al (2001), who combine chronological order-
ing of events with cohesion metrics. SVM regres-
sion has recently been used by (Li et al, 2007) for
sentence ranking for general MDS. The authors cal-
culated a similarity score for each sentence to the
human summaries and then regress numeric features
(e.g., the centroid) from each sentence to this score.
Barzilay and Lee (2004) use HMMs to capture topic
shift within a particular domain; sequence of topic
shifts then guides the subsequent ordering of sen-
tences within the summary.
7 Discussion and Future Work
In this paper, we describe a MDS system for produc-
ing biographies, given a target name. We present an
unsupervised approach using Wikipedia biography
pages and a general news corpus (TDT4) to automat-
ically construct training data for our system. We em-
ploy a NE tagger and a coreference resolution sys-
tem to extract class-based and lexical features from
each sentence which we use to train a binary classi-
fier to identify biographical sentences. We also train
an SVM regression model to reorder the sentences
and then employ a rewriting heuristic to create the
final summary.
We compare versions of our system based upon
three machine learning algorithms and two sentence
reordering strategies plus a baseline. Our best per-
forming system uses the multinomial na??ve Bayes
(MNB) classifier with classifier confidence score re-
ordering. However, our SVM regression reorder-
ing improves summaries produced by the other two
classifiers and is preferred by human judges. We
compare our MNB system on the DUC2004 bi-
ography task (task 5) to other DUC2004 systems
and to human-generated summaries. Our system
out-performs all DUC2004 systems significantly,
according to ROUGE-L-1.5.5. When presented
with summaries produced by our system and sum-
maries produced by the best-performing (according
to ROUGE scores) of the DUC2004 systems, human
judges (majority vote of 3) prefer our system?s bi-
ographies in 88.1% of cases.
In addition to its high performance, our approach
has the following advantages: It employs no manual
annotation but relies upon identifying appropriately
different corpora to represent our training corpus.
It employs class-based as well as lexical features
where the classes are obtained automatically from
an ACE NE tagger. It utilizes automatic corefer-
ence resolution to identify sentences containing ref-
erences to the target person. Our sentence reorder-
ing approaches make use of either classifier confi-
dence scores or ordering learned automatically from
the actual ordering of sentences in Wikipedia biogra-
phies to determine the order of presentation of sen-
tences in our summaries.
Since our task is to produce concise summaries,
one focus of our future research will be to simplify
the sentences we extract before classifying them
as biographical or non-biographical. This proce-
dure should also help to remove irrelevant informa-
tion from sentences. Recall that our SVM regres-
sion model for sentence ordering was trained using
only biographical class-based/lexical items. In fu-
ture, we would also like to experiment with more
linguistically-informed features. While Wikipedia
does not enforce any particular ordering of infor-
mation in biographies, and while different biogra-
phies may emphasize different types of information,
it would appear that the success of our automatically
derived ordering procedures may capture some un-
derlying shared view of how biographies are written.
The same underlying views may also apply to do-
mains such as organization descriptions or types of
historical events. In future we plan to explore such a
generalization of our procedures to such domains.
Acknowledgments
We thank Kathy McKeown, Andrew Rosenberg, Wisam Dakka, and the
Speech and NLP groups at Columbia for useful discussions. This mate-
rial is based upon work supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR001106C0023 (ap-
proved for public release, distribution unlimited). Any opinions, find-
ings and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of DARPA.
814
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proceedings of the First Human Lan-
guage Technology Conference, San Diego, California.
Maria Biryukov, Roxana Angheluta, and Marie-Francine
Moens. 2005. Multidocument question answering
text summarization using topic signatures. In Pro-
ceedings of the 5th Dutch-Belgium Information Re-
trieval Workshop, Utrecht, the Netherlands.
Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi-
vassiloglou, Kathleen McKeown, Ani Nenkova, Re-
becca Passonneau, Barry Schiffman, Andrew Schlaik-
jer, Advaith Siddharthan, and Sergey Siegelman.
2004a. Columbia University at DUC 2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence, Boston, Massachusetts, USA.
Sasha Blair-Goldensohn, Kathy McKeown, and Andrew
Schlaikjer. 2004b. Answering definitional questions:
A hybrid approach. In Mark Maybury, editor, New
Directions In Question Answering, chapter 4. AAAI
Press.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. volume 70, pages 213?220.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
program - tasks, data, and evaluation. In Proceedings
of the LREC Conference, Canary Islands, Spain, July.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Proceedings of the Conference
on Empirical Methods for Natural Language Process-
ing, pages 121?128, Sapporo, Japan, July.
Elena Filatova and John Prager. 2005. Tell me what
you do and I?ll tell you what you are: Learning
occupation-related activities for biographies. In Pro-
ceedings of the Joint Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 113?120, Van-
couver, Canada, October.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. volume 76, No. 5, pages 378?382.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyu?s english ace 2005 system description. In
ACE 05 Evaluation Workshop, Gaithersburg, MD.
Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007.
Multi-document summarization using support vector
regression. In http://duc.nist.gov/pubs/2007papers.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495?501,
Saarbru?cken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Language Technology
Conference, Edmonton, Canada.
Ani Nenkova and Kathleen McKeown. 2003. References
to named entities: A corpus study. In Proceedings
of the Joint Human Language Technology Conference
and North American chapter of the Association for
Computational Linguistics Annual Meeting, Edmon-
ton, Canada, May.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions.
In Mark Maybury, editor, New Directions In Question
Answering, chapter 5. AAAI Press.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In International Workshop: Emerging Knowl-
edge Engineering and Connectionist-Based Informa-
tion Systems, pages 192?196.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004.
Multi-document biography summarization. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 434?441,
Barcelona, Spain.
815
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 53?61,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Spoken Arabic Dialect Identification Using Phonotactic Modeling
Fadi Biadsy and Julia Hirschberg
Department of Computer Science
Columbia University, New York, USA
{fadi,julia}@cs.columbia.edu
Nizar Habash
Center for Computational Learning Systems
Columbia University, New York, USA
habash@ccls.columbia.edu
Abstract
The Arabic language is a collection of
multiple variants, among which Modern
Standard Arabic (MSA) has a special sta-
tus as the formal written standard language
of the media, culture and education across
the Arab world. The other variants are in-
formal spoken dialects that are the media
of communication for daily life. Arabic di-
alects differ substantially from MSA and
each other in terms of phonology, mor-
phology, lexical choice and syntax. In this
paper, we describe a system that automat-
ically identifies the Arabic dialect (Gulf,
Iraqi, Levantine, Egyptian and MSA) of a
speaker given a sample of his/her speech.
The phonotactic approach we use proves
to be effective in identifying these di-
alects with considerable overall accuracy
? 81.60% using 30s test utterances.
1 Introduction
For the past three decades, there has been a great
deal of work on the automatic identification (ID)
of languages from the speech signal alone. Re-
cently, accent and dialect identification have be-
gun to receive attention from the speech science
and technology communities. The task of dialect
identification is the recognition of a speaker?s re-
gional dialect, within a predetermined language,
given a sample of his/her speech. The dialect-
identification problem has been viewed as more
challenging than that of language ID due to the
greater similarity between dialects of the same lan-
guage. Our goal in this paper is to analyze the ef-
fectiveness of a phonotactic approach, i.e. making
use primarily of the rules that govern phonemes
and their sequences in a language ? a techniques
which has often been employed by the language
ID community ? for the identification of Arabic
dialects.
The Arabic language has multiple variants, in-
cluding Modern Standard Arabic (MSA), the for-
mal written standard language of the media, cul-
ture and education, and the informal spoken di-
alects that are the preferred method of communi-
cation in daily life. While there are commercially
available Automatic Speech Recognition (ASR)
systems for recognizing MSA with low error rates
(typically trained on Broadcast News), these rec-
ognizers fail when a native Arabic speaker speaks
in his/her regional dialect. Even in news broad-
casts, speakers often code switch between MSA
and dialect, especially in conversational speech,
such as that found in interviews and talk shows.
Being able to identify dialect vs. MSA as well as to
identify which dialect is spoken during the recog-
nition process will enable ASR engines to adapt
their acoustic, pronunciation, morphological, and
language models appropriately and thus improve
recognition accuracy.
Identifying the regional dialect of a speaker will
also provide important benefits for speech tech-
nology beyond improving speech recognition. It
will allow us to infer the speaker?s regional origin
and ethnicity and to adapt features used in speaker
identification to regional original. It should also
prove useful in adapting the output of text-to-
speech synthesis to produce regional speech as
well as MSA ? important for spoken dialogue sys-
tems? development.
In Section 2, we describe related work. In Sec-
tion 3, we discuss some linguistic aspects of Ara-
bic dialects which are important to dialect iden-
tification. In Section 4, we describe the Arabic
dialect corpora employed in our experiments. In
Section 5, we explain our approach to the identifi-
cation of Arabic dialects. We present our experi-
mental results in Section 6. Finally, we conclude
in Section 7 and identify directions for future re-
search.
2 Related Work
A variety of cues by which humans and machines
distinguish one language from another have been
explored in previous research on language identi-
53
fication. Examples of such cues include phone in-
ventory and phonotactics, prosody, lexicon, mor-
phology, and syntax. Some of the most suc-
cessful approaches to language ID have made
use of phonotactic variation. For example, the
Phone Recognition followed by Language Model-
ing (PRLM) approach uses phonotactic informa-
tion to identify languages from the acoustic sig-
nal alone (Zissman, 1996). In this approach, a
phone recognizer (not necessarily trained on a re-
lated language) is used to tokenize training data for
each language to be classified. Phonotactic lan-
guage models generated from this tokenized train-
ing speech are used during testing to compute lan-
guage ID likelihoods for unknown utterances.
Similar cues have successfully been used for
the identification of regional dialects. Zisssman
et al (1996) show that the PRLM approach yields
good results classifying Cuban and Peruvian di-
alects of Spanish, using an English phone recog-
nizer trained on TIMIT (Garofolo et al, 1993).
The recognition accuracy of this system on these
two dialects is 84%, using up to 3 minutes of test
utterances. Torres-Carrasquillo et al (2004) devel-
oped an alternate system that identifies these two
Spanish dialects using Gaussian Mixture Models
(GMM) with shifted-delta-cepstral features. This
system performs less accurately (accuracy of 70%)
than that of (Zissman et al, 1996). Alorfi (2008)
uses an ergodic HMM to model phonetic dif-
ferences between two Arabic dialects (Gulf and
Egyptian Arabic) employing standard MFCC (Mel
Frequency Cepstral Coefficients) and delta fea-
tures. With the best parameter settings, this system
achieves high accuracy of 96.67% on these two
dialects. Ma et al (2006) use multi-dimensional
pitch flux features and MFCC features to distin-
guish three Chinese dialects. In this system the
pitch flux features reduce the error rate by more
than 30% when added to a GMM based MFCC
system. Given 15s of test-utterances, the system
achieves an accuracy of 90% on the three dialects.
Intonational cues have been shown to be good
indicators to human subjects identifying regional
dialects. Peters et al (2002) show that human sub-
jects rely on intonational cues to identify two Ger-
man dialects (Hamburg urban dialects vs. North-
ern Standard German). Similarly, Barakat et
al. (1999) show that subjects distinguish between
Western vs. Eastern Arabic dialects significantly
above chance based on intonation alone.
Hamdi et al (2004) show that rhythmic dif-
ferences exist between Western and Eastern Ara-
bic. The analysis of these differences is done by
comparing percentages of vocalic intervals (%V)
and the standard deviation of intervocalic inter-
vals (?C) across the two groups. These features
have been shown to capture the complexity of the
syllabic structure of a language/dialect in addition
to the existence of vowel reduction. The com-
plexity of syllabic structure of a language/dialect
and the existence of vowel reduction in a language
are good correlates with the rhythmic structure of
the language/dialect, hence the importance of such
a cue for language/dialect identification (Ramus,
2002).
As far as we could determine, there is no
previous work that analyzes the effectiveness of
a phonotactic approach, particularly the parallel
PRLM, for identifying Arabic dialects. In this pa-
per, we build a system based on this approach and
evaluate its performance on five Arabic dialects
(four regional dialects and MSA). In addition, we
experiment with six phone recognizers trained on
six languages as well as three MSA phone recog-
nizers and analyze their contribution to this classi-
fication task. Moreover, we make use of a discrim-
inative classifier that takes all the perplexities of
the language models on the phone sequences and
outputs the hypothesized dialect. This classifier
turns out to be an important component, although
it has not been a standard component in previous
work.
3 Linguistic Aspects of Arabic Dialects
3.1 Arabic and its Dialects
MSA is the official language of the Arab world.
It is the primary language of the media and cul-
ture. MSA is syntactically, morphologically and
phonologically based on Classical Arabic, the lan-
guage of the Qur?an (Islam?s Holy Book). Lexi-
cally, however, it is much more modern. It is not
a native language of any Arabs but is the language
of education across the Arab world. MSA is pri-
marily written not spoken.
The Arabic dialects, in contrast, are the true na-
tive language forms. They are generally restricted
in use to informal daily communication. They
are not taught in schools or even standardized, al-
though there is a rich popular dialect culture of
folktales, songs, movies, and TV shows. Dialects
are primarily spoken, not written. However, this
is changing as more Arabs gain access to elec-
54
tronic media such as emails and newsgroups. Ara-
bic dialects are loosely related to Classical Ara-
bic. They are the result of the interaction between
different ancient dialects of Classical Arabic and
other languages that existed in, neighbored and/or
colonized what is today the Arab world. For ex-
ample, Algerian Arabic has many influences from
Berber as well as French.
Arabic dialects vary on many dimensions ?
primarily, geography and social class. Geo-
linguistically, the Arab world can be divided in
many different ways. The following is only one
of many that covers the main Arabic dialects:
? Gulf Arabic (GLF) includes the dialects of
Kuwait, Saudi Arabia, Bahrain, Qatar, United
Arab Emirates, and Oman.
? Iraqi Arabic (IRQ) is the dialect of Iraq. In
some dialect classifications, Iraqi Arabic is
considered a sub-dialect of Gulf Arabic.
? Levantine Arabic (LEV) includes the di-
alects of Lebanon, Syria, Jordan, Palestine
and Israel.
? Egyptian Arabic (EGY) covers the dialects
of the Nile valley: Egypt and Sudan.
? Maghrebi Arabic covers the dialects of
Morocco, Algeria, Tunisia and Mauritania.
Libya is sometimes included.
Yemenite Arabic is often considered its own
class. Maltese Arabic is not always consid-
ered an Arabic dialect. It is the only Arabic
variant that is considered a separate language
and is written with Latin script.
Socially, it is common to distinguish three sub-
dialects within each dialect region: city dwellers,
peasants/farmers and Bedouins. The three degrees
are often associated with a class hierarchy from
rich, settled city-dwellers down to Bedouins. Dif-
ferent social associations exist as is common in
many other languages around the world.
The relationship between MSA and the dialect
in a specific region is complex. Arabs do not think
of these two as separate languages. This particular
perception leads to a special kind of coexistence
between the two forms of language that serve dif-
ferent purposes. This kind of situation is what lin-
guists term diglossia. Although the two variants
have clear domains of prevalence: formal written
(MSA) versus informal spoken (dialect), there is
a large gray area in between and it is often filled
with a mixing of the two forms.
In this paper, we focus on classifying the di-
alect of audio recordings into one of five varieties:
MSA, GLF, IRQ, LEV, and EGY. We do not ad-
dress other dialects or diglossia.
3.2 Phonological Variations among Arabic
Dialects
Although Arabic dialects and MSA vary on many
different levels ? phonology, orthography, mor-
phology, lexical choice and syntax ? we will
focus on phonological difference in this paper.1
MSA?s phonological profile includes 28 conso-
nants, three short vowels, three long vowels and
two diphthongs (/ay/ and /aw/). Arabic dialects
vary phonologically from standard Arabic and
each other. Some of the common variations in-
clude the following (Holes, 2004; Habash, 2006):
The MSA consonant (/q/) is realized as a glot-
tal stop /?/ in EGY and LEV and as /g/ in GLF and
IRQ. For example, the MSA word /t
?
ari:q/ ?road?
appears as /t
?
ari:?/ (EGY and LEV) and /t
?
ari:g/ (GLF
and IRQ). Other variants also are found in sub di-
alects such as /k/ in rural Palestinian (LEV) and
/dj/ in some GLF dialects. These changes do not
apply to modern and religious borrowings from
MSA. For instance, the word for ?Qur?an? is never
pronounced as anything but /qur?a:n/.
The MSA alveolar affricate (/dj/) is realized as
/g/ in EGY, as /j/ in LEV and as /y/ in GLF. IRQ
preserves the MSA pronunciation. For example,
the word for ?handsome? is /djami:l/ (MSA, IRQ),
/gami:l/ (EGY), /jami:l/ (LEV) and /yami:l/ (GLF).
The MSA consonant (/k/) is generally realized
as /k/ in Arabic dialects with the exception of GLF,
IRQ and the Palestinian rural sub-dialect of LEV,
which allow a /c?/ pronunciation in certain con-
texts. For example, the word for ?fish? is /samak/
in MSA, EGY and most of LEV but /simac?/ in IRQ
and GLF.
The MSA consonant /?/ is pronounced as /t/ in
LEV and EGY (or /s/ in more recent borrowings
from MSA), e.g., the MSA word /?ala:?a/ ?three?
is pronounced /tala:ta/ in EGY and /tla:te/ in LEV.
IRQ and GLF generally preserve the MSA pronun-
ciation.
1It is important to point out that since Arabic dialects are
not standardized, their orthography may not always be con-
sistent. However, this is not a relevant point to this paper
since we are interested in dialect identification using audio
recordings and without using the dialectal transcripts at all.
55
The MSA consonant /?/ is pronounced as /d/
in LEV and EGY (or /z/ in more recent borrow-
ings from MSA), e.g., the word for ?this? is pro-
nounced /ha:?a/ in MSA versus /ha:da/ (LEV) and
/da/ EGY. IRQ and GLF generally preserve the
MSA pronunciation.
The MSA consonants /d
?
/ (emphatic/velarized
d) and /?
?
/ (emphatic /?/) are both normalized to
/d
?
/ in EGY and LEV and to /?
?
/ in GLF and IRQ.
For example, the MSA sentence /?
?
alla yad
?
rubu/
?he continued to hit? is pronounced /d
?
all yud
?
rub/
(LEV) and /?
?
all yu?
?
rub/ (GLF). In modern bor-
rowings from MSA, /?
?
/ is pronounced as /z
?
/ (em-
phatic z) in EGY and LEV. For instance, the word
for ?police officer? is /?
?
a:bit
?
/ in MSA but /z
?
a:bit
?
/
in EGY and LEV.
In some dialects, a loss of the emphatic feature
of some MSA consonants occurs, e.g., the MSA
word /lat
?
i:f/ ?pleasant? is pronounced as /lati:f/ in
the Lebanese city sub-dialect of LEV. Empha-
sis typically spreads to neighboring vowels: if a
vowel is preceded or succeeded directly by an em-
phatic consonant (/d
?
/, /s
?
/, /t
?
/, /?
?
/) then the vowel
becomes an emphatic vowel. As a result, the loss
of the emphatic feature does not affect the conso-
nants only, but also their neighboring vowels.
Other vocalic differences among MSA and the
dialects include the following: First, short vow-
els change or are completely dropped, e.g., the
MSA word /yaktubu/ ?he writes? is pronounced
/yiktib/ (EGY and IRQ) or /yoktob/ (LEV). Sec-
ond, final and unstressed long vowels are short-
ened, e.g., the word /mat
?
a:ra:t/ ?airports? in MSA
becomes /mat
?
ara:t/ in many dialects. Third, the
MSA diphthongs /aw/ and /ay/ have mostly be-
come /o:/ and /e:/, respectively. These vocalic
changes, particularly vowel drop lead to different
syllabic structures. MSA syllables are primarily
light (CV, CV:, CVC) but can also be (CV:C and
CVCC) in utterance-final positions. EGY sylla-
bles are the same as MSA?s although without the
utterance-final restriction. LEV, IRQ and GLF al-
low heavier syllables including word initial clus-
ters such as CCV:C and CCVCC.
4 Corpora
When training a system intended to classify lan-
guages or dialects, it is of course important to use
training and testing corpora recorded under simi-
lar acoustic conditions. We are able to obtain cor-
pora from the Linguistic Data Consortium (LDC)
with similar recording conditions for four Arabic
dialects: Gulf Arabic, Iraqi Arabic, Egyptian Ara-
bic, and Levantine Arabic. These are corpora of
spontaneous telephone conversations produced by
native speakers of the dialects, speaking with fam-
ily members, friends, and unrelated individuals,
sometimes about predetermined topics. Although,
the data have been annotated phonetically and/or
orthographically by LDC, in this paper, we do not
make use of any of annotations.
We use the speech files of 965 speakers (about
41.02 hours of speech) from the Gulf Arabic
conversational telephone Speech database for our
Gulf Arabic data (Appen Pty Ltd, 2006a).2 From
these speakers we hold out 150 speakers for test-
ing (about 6.06 hours of speech).3 We use the Iraqi
Arabic Conversational Telephone Speech database
(Appen Pty Ltd, 2006b) for the Iraqi dialect, se-
lecting 475 Iraqi Arabic speakers with a total du-
ration of about 25.73 hours of speech. From
these speakers we hold out 150 speakers4 for test-
ing (about 7.33 hours of speech). Our Levan-
tine data consists of 1258 speakers from the Ara-
bic CTS Levantine Fisher Training Data Set 1-3
(Maamouri, 2006). This set contains about 78.79
hours of speech in total. We hold out 150 speakers
for testing (about 10 hours of speech) from Set 1.5
For our Egyptian data, we use CallHome Egyp-
tian and its Supplement (Canavan et al, 1997)
and CallFriend Egyptian (Canavan and Zipperlen,
1996). We use 398 speakers from these corpora
(75.7 hours of speech), holding out 150 speakers
for testing.6 (about 28.7 hours of speech.)
Unfortunately, as far as we can determine, there
is no data with similar recording conditions for
MSA. Therefore, we obtain our MSA training data
from TDT4 Arabic broadcast news. We use about
47.6 hours of speech. The acoustic signal was pro-
cessed using forced-alignment with the transcript
to remove non-speech data, such as music. For
testing we again use 150 speakers, this time iden-
tified automatically from the GALE Year 2 Dis-
tillation evaluation corpus (about 12.06 hours of
speech). Non-speech data (e.g., music) in the test
2We excluded very short speech files from the corpora.
3The 24 speakers in devtest folder and the last 63 files,
after sorting by file name, in train2c folder (126 speakers).
The sorting is done to make our experiments reproducible by
other researchers.
4Similar to the Gulf corpus, the 24 speakers in devtest
folder and the last 63 files (after sorting by filename) in
train2c folder (126 speakers)
5We use the last 75 files in Set 1, after sorting by name.
6The test speakers were from evaltest and devtest folders
in CallHome and CallFriend.
56
corpus was removed manually. It should be noted
that the data includes read speech by anchors and
reporters as well as spontaneous speech spoken in
interviews in studios and though the phone.
5 Our Dialect ID Approach
Since, as described in Section 3, Arabic dialects
differ in many respects, such as phonology, lex-
icon, and morphology, it is highly likely that
they differ in terms of phone-sequence distribu-
tion and phonotactic constraints. Thus, we adopt
the phonotactic approach to distinguishing among
Arabic dialects.
5.1 PRLM for dialect ID
As mentioned in Section 2, the PRLM approach to
language identification (Zissman, 1996) has had
considerable success. Recall that, in the PRLM
approach, the phones of the training utterances of
a dialect are first identified using a single phone
recognizer.7 Then an n-gram language model is
trained on the resulting phone sequences for this
dialect. This process results in an n-gram lan-
guage model for each dialect to model the dialect
distribution of phone sequence occurrences. Dur-
ing recognition, given a test speech segment, we
run the phone recognizer to obtain the phone se-
quence for this segment and then compute the per-
plexity of each dialect n-gram model on the se-
quence. The dialect with the n-gram model that
minimizes the perplexity is hypothesized to be the
dialect from which the segment comes.
Parallel PRLM is an extension to the PRLM ap-
proach, in which multiple (k) parallel phone rec-
ognizers, each trained on a different language, are
used instead of a single phone recognizer (Ziss-
man, 1996). For training, we run all phone recog-
nizers in parallel on the set of training utterances
of each dialect. An n-gram model on the outputs of
each phone recognizer is trained for each dialect.
Thus if we have m dialects, k x m n-gram models
are trained. During testing, given a test utterance,
we run all phone recognizers on this utterance and
compute the perplexity of each n-gram model on
the corresponding output phone sequence. Finally,
the perplexities are fed to a combiner to determine
the hypothesized dialect. In our implementation,
7The phone recognizer is typically trained on one of the
languages being identified. Nonetheless, a phone recognize
trained on any language might be a good approximation,
since languages typically share many phones in their phonetic
inventory.
we employ a logistic regression classifier as our
back-end combiner. We have experimented with
different classifiers such as SVM, and neural net-
works, but logistic regression classifier was supe-
rior. The system is illustrated in Figure 1.
We hypothesize that using multiple phone rec-
ognizers as opposed to only one allows the system
to capture subtle phonetic differences that might
be crucial to distinguish dialects. Particularly,
since the phone recognizers are trained on differ-
ent languages, they may be able to model different
vocalic and consonantal systems, hence a different
phonetic inventory. For example, an MSA phone
recognizer typically does not model the phoneme
/g/; however, an English phone recognizer does.
As described in Section 3, this phoneme is an
important cue to distinguishing Egyptian Arabic
from other Arabic dialects. Moreover, phone rec-
ognizers are prone to many errors; relying upon
multiple phone streams rather than one may lead
to a more robust model overall.
5.2 Phone Recognizers
In our experiments, we have used phone recogniz-
ers for English, German, Japanese, Hindi, Man-
darin, and Spanish, from a toolkit developed by
Brno University of Technology.8 These phone rec-
ognizers were trained on the OGI multilanguage
database (Muthusamy et al, 1992) using a hybrid
approach based on Neural Networks and Viterbi
decoding without language models (open-loop)
(Matejka et al, 2005).
Since Arabic dialect identification is our goal,
we hypothesize that an Arabic phone recognizer
would also be useful, particularly since other
phone recognizers do not cover all Arabic con-
sonants, such as pharyngeals and emphatic alveo-
lars. Therefore, we have built our own MSA phone
recognizer using the HMM toolkit (HTK) (Young
et al, 2006). The monophone acoustic models
are built using 3-state continuous HMMs without
state-skipping, with a mixture of 12 Gaussians per
state. We extract standard Mel Frequency Cepstral
Coefficients (MFCC) features from 25 ms frames,
with a frame shift of 10 ms. Each feature vec-
tor is 39D: 13 features (12 cepstral features plus
energy), 13 deltas, and 13 double-deltas. The fea-
tures are normalized using cepstral mean normal-
ization. We use the Broadcast News TDT4 corpus
(Arabic Set 1; 47.61 hours of speech; downsam-
pled to 8Khz) to train our acoustic models. The
8www.fit.vutbr.cz/research/groups/speech/sw/phnrec
57
???????????
????????
?????????? ?
???????????????
??????????????
?????????
??????????????
?????????????
????????????
??????????????
? ??????????
???????????????
????????????
??????? ?
?????? ?
?????????? ?
??????????? ?
????? ?
?????????
?????? ?
?????????? ?
??????????? ?
????? ?
????????????????
?????????
?????? ?
?????????? ?
??????????? ?
????? ?
?????????
????????????
Figure 1: Parallel Phone Recognition Followed by Language Modeling (PRLM) for Arabic Dialect Identification.
pronunciation dictionary is generated as described
in (Biadsy et al, 2009). Using these settings we
build three MSA phone recognizers: (1) an open-
loop phone recognizer which does not distinguish
emphatic vowels from non-emphatic (ArbO), (2)
an open-loop with emphatic vowels (ArbOE), and
(3) a phone recognizer with emphatic vowels and
with a bi-gram phone language model (ArbLME).
We add a new pronunciation rule to the set of
rules described in (Biadsy et al, 2009) to distin-
guish emphatic vowels from non-emphatic ones
(see Section 3) when generating our pronunciation
dictionary for training the acoustic models for the
the phone recognizers. In total we build 9 (Arabic
and non-Arabic) phone recognizers.
6 Experiments and Results
In this section, we evaluate the effectiveness of the
parallel PRLM approach on distinguishing Ara-
bic dialects. We first run the nine phone recog-
nizers described in Section 5 on the training data
described in Section 4, for each dialect. This pro-
cess produces nine sets of phone sequences for
each dialect. In our implementation, we train a
tri-gram language model on each phone set using
the SRILM toolkit (Stolcke, 2002). Thus, in total,
we have 9 x (number of dialects) tri-grams.
In all our experiments, the 150 test speakers of
each dialect are first decoded using the phone rec-
ognizers. Then the perplexities of the correspond-
ing tri-gram models on these sequences are com-
puted, and are given to the logistic regression clas-
sifier. Instead of splitting our held-out data into
test and training sets, we report our results with
10-fold cross validation.
We have conducted three experiments to eval-
uate our system. The first is to compare the per-
formance of our system to Alorfi?s (2008) on the
same two dialects (Gulf and Egyptian Arabic).
The second is to attempt to classify four collo-
quial Arabic dialects. In the third experiment, we
include MSA as well in a five-way classification
task.
6.1 Gulf vs. Egyptian Dialect ID
To our knowledge, Alorfi?s (2008) work is the
only work dealing with the automatic identifica-
tion of Arabic dialects. In this work, an Ergodic
HMM is used to model phonetic differences be-
tween Gulf and Egyptian Arabic using MFCC and
delta features. The test and training data used in
this work was collected from TV soap operas con-
taining both the Egyptian and Gulf dialects and
from twenty speakers from CallHome Egyptian
database. The best accuracy reported by Alorfi
(2008) on identifying the dialect of 40 utterances
of duration of 30 seconds each of 40 male speakers
(20 Egyptians and 20 Gulf speakers) is 96.67%.
Since we do not have access to the test collec-
tion used in (Alorfi, 2008), we test a version of our
system which identifies these two dialects only on
our 150 Gulf and 150 Egyptian speakers, as de-
scribed in Section 4. Our best result is 97.00%
(Egyptian and Gulf F-Measure = 0.97) when us-
ing only the features from the ArbOE, English,
Japanese, and Mandarin phone recognizers. While
our accuracy might not be significantly higher than
that of Alorfi?s, we note a few advantages of our
experiments. First, the test sets of both dialects
are from telephone conversations, with the same
recording conditions, as opposed to a mix of dif-
ferent genres. Second, in our system we test 300
speakers as oppose to 40, so our results may be
more reliable. Third, our test data includes female
58
4 dialectsseconds accuracy Gulf Iraqi Levantine Egyptian5 60.833 49.2 52.7 58.1 8315 72.83 60.8 61.2 77.6 91.930 78.5 68.7 67.3 84 9445 81.5 72.6 72.4 86.9 93.760 83.33 75.1 75.7 87.9 94.6120 84 75.1 75.4 89.5 96
??
??
??
??
??
??
??
??
??
??
??
??
? ?? ?? ?? ?? ??
?????
????????
????????
??????????
?????????
?
?????????????????
Figure 2: The accuracies and F-Measures of the four-way
classification task with different test-utterance durations
speakers as well as male, so our results are more
general.
6.2 Four Colloquial Arabic Dialect ID
In our second experiment, we test our system on
four colloquial Arabic dialects (Gulf, Iraqi, Levan-
tine, and Egyptian). As mentioned above, we use
the phone recognizers to decode the training data
to train the 9 tri-gram models per dialect (9x4=36
tri-gram models). We report our 10-fold cross val-
idation results on the test data in Figure 2. To
analyze how dependent our system is on the du-
ration of the test utterance, we report the system
accuracy and the F-measure of each class for dif-
ferent durations (5s ? 2m). The longer the ut-
terance, the better we expect the system to per-
form. We can observe from these results that re-
gardless of the test-utterance duration, the best dis-
tinguished dialect among the four dialects is Egyp-
tian (F-Measure of 94% with 30s test utterances),
followed by Levantine (F-Measure of 84% with
30s), and the most confusable dialects, according
to the classification confusion matrix, are those of
the Gulf and Iraqi Arabic (F-Measure of 68.7%,
67.3%, respectively with 30s). This confusion is
consistent with dialect classifications that consider
Iraqi a sub-dialect of Gulf Arabic, as mentioned in
Section 3.
We were also interested in testing which phone
recognizers contribute the most to the classifica-
tion task. We observe that employing a subset of
the phone recognizers as opposed to all of them
provides us with better results. Table 1 shows
which phone recognizers are selected empirically,
for each test-utterance duration condition.9
9Starting from all phone recognizers, we remove one rec-
ognizer at a time; if the cross-validation accuracy decreases,
Dur. Acc. (%) Phone Recognizers
5s 60.83 ArbOE+ArbLME+G+H+M+S
15s 72.83 ArbOE+ArbLME+G+H+M
30s 78.50 ArbO+H+S
45s 81.5 ArbE+ArbLME+H+G+S
60s 83.33 ArbOE+ArbLME+E+G+H+M
120s 84.00 ArbOE+ArbLME+G+M
Table 1: Accuracy of the four-way classification (four col-
loquial Arabic dialects) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
M=Mandarin, S=Spanish, ArbO=open-loop MSA without
emphatic vowels, ArbOE=open-loop MSA with emphatic
vowels, ArbLME=MSA with emphatic vowels and bi-gram
phone LM
We observe that the MSA phone recognizers are
the most important phone recognizers for this task,
usually when emphatic vowels are modeled. In all
scenarios, removing all MSA phone recognizers
leads to a significant drop in accuracy. German,
Mandarin, Hindi, and Spanish typically contribute
to the classification task, but English, and Japanese
phone recognizers are less helpful. It is possible
that the more useful recognizers are able to cap-
ture more of the distinctions among the Arabic di-
alects; however, it might also be that the overall
quality of the recognizers also varies.
6.3 Dialect ID with MSA
Considering MSA as a dialectal variant of Ara-
bic, we are also interested in analyzing the perfor-
mance of our system when including it in our clas-
sification task. In this experiment, we add MSA as
the fifth dialect. We perform the same steps de-
scribed above for training, using the MSA corpus
described in Section 4. For testing, we use also
our 150 hypothesized MSA speakers as our test
set. Interestingly, in this five-way classification,
we observe that the F-Measure for the MSA class
in the cross-validation task is always above 98%
regardless of the test-utterance duration, as shown
in Figure 3.
It would seem that MSA is rarely confused with
any of the colloquial dialects: it appears to have a
distinct phonotactic distribution. This explanation
is supported by linguists, who note that MSA dif-
fers from Arabic dialects in terms of its phonology,
lexicon, syntax and morphology, which appears to
lead to a profound impact on its phonotactic distri-
bution. Similar to the four-way classification task,
we add it back. We have experimented with an automatic
feature selection methods, but with the empirical (?greedy?)
selection we typically obtain higher accuracy.
59
4 dialectsseconds accuracy Gulf Iraqi Levantine Egyptian5 68.6667 54.5 50.7 60 77.915 76.6667 57.3 62.6 73.8 90.730 81.6 68.3 71.7 79.4 90.245 84.8 69.9 73.6 86.2 94.960 86.933 76.8 76.5 85.4 96.3120 87.86 79.1 77.4 90.1 93.6
??
??
??
??
??
??
??
??
??
??
??
??
? ?? ?? ?? ?? ??
?????
????????
????????
??????????
?????????
???????
?
?????????????????
Figure 3: The accuracies and F-Measures of the five-way
classification task with different test-utterance durations
Dur. Acc. (%) Phone Recognizers
5s 68.67 ArbO+ArbLME+H+M
15s 76.67 ArbLME+G+H+J+M
30s 81.60 ArbO+ArbOE+E+G+H+J+M+S
45s 84.80 ArbOE+ArbLME+E+G+H+J+M+S
60s 86.93 ArbOE+ArbLME+G+J+M+S
120s 87.86 ArbO+ArbLME+E+S
Table 2: Accuracy of the five-way classification (4 colloquial
Arabic dialects + MSA) and the best combination of phone
recognizers used per test-utterances duration; The phone
recognizers used are: E=English, G=German, H=Hindi,
J=Japanese, M=Mandarin, S=Spanish, ArbO=open-loop
MSA without emphatic vowels, ArbOE=open-loop MSA
with emphatic vowels, ArbLME=MSA with emphatic vow-
els and bi-gram phone LM
Egyptian was the most easily distinguished dialect
(F-Measure=90.2%, with 30s test utterance) fol-
lowed by Levantine (79.4%), and then Iraqi and
Gulf (71.7% and 68.3%, respectively). Due to the
high MSA F-Measure, the five-way classifier can
also be used as a binary classifier to distinguish
MSA from colloquial Arabic (Gulf, Iraqi, Levan-
tine, and Egyption) reliably.
It should be noted that our classification results
for MSA might be inflated for several reasons: (1)
The MSA test data were collected from Broad-
cast News, which includes read (anchor and re-
porter) speech, as well as telephone speech (for in-
terviews). (2) The identities of the test speakers in
the MSA corpus were determined automatically,
and so might not be as accurate.
As a result of the high identification rate of
MSA, the overall accuracy in the five-way clas-
sification task is higher than that of the four-way
classification. Table 2 presents the phone recog-
nizers selected the accuracy for each test utterance
duration. We observe here that the most impor-
tant phone recognizers are those trained on MSA
(ArbO, ArbOE, and/or ArbLME). Removing them
completely leads to a significant drop in accu-
racy. In this classification task, we observe that all
phone recognizers play a role in the classification
task in some of the conditions.
7 Conclusions and Future Work
In this paper, we have shown that four Arabic
colloquial dialects (Gulf, Iraqi, Levantine, and
Egyptian) plus MSA can be distinguished using
a phonotactic approach with good accuracy. The
parallel PRLM approach we employ thus appears
to be effective not only for language identification
but also for Arabic dialect ID.
We have found that the most distinguishable
dialect among the five variants we consider here
is MSA, independent of the duration of the test-
utterance (F-Measure is always above 98.00%).
Egyptian Arabic is second (F-Measure of 90.2%
with 30s test-utterances), followed by Levantine
(F-Measure of 79.4%, with 30s test). The most
confusable dialects are Iraqi and Gulf (F-Measure
of 71.7% and 68.3%, respectively, with 30s test-
utterances). This high degree of Iraqi-Gulf confu-
sion is consistent with some classifications of Iraqi
Arabic as a sub-dialect of Gulf Arabic. We have
obtained a total accuracy of 81.60% in this five-
way classification task when given 30s-duration
utterances. We have also observed that the most
useful phone streams for classification are those
of our Arabic phone recognizers ? typically those
with emphatic vowels.
As mentioned above, the high F-measure for
MSA may be due to the MSA corpora we have
used, which differs in genre from the dialect cor-
pora. Therefore, one focus of our future research
will be to collect MSA data with similar record-
ing conditions to the other dialects to validate
our results. We are also interested in including
prosodic features, such as intonational, durational,
and rhythmic features in our classification. A more
long-term and general goal is to use our results to
improve ASR for cases in which code-switching
occurs between MSA and other dialects.
Acknowledgments
We thank Dan Ellis, Michael Mandel, and Andrew Rosenberg
for useful discussions. This material is based upon work sup-
ported by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023 (approved
for public release, distribution unlimited). Any opinions,
findings and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reflect the views of DARPA.
60
References
F. S. Alorfi. 2008. PhD Dissertation: Automatic Identifica-
tion Of Arabic Dialects Using Hidden Markov Models. In
University of Pittsburgh.
Appen Pty Ltd. 2006a. Gulf Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
Appen Pty Ltd. 2006b. Iraqi Arabic Conversational Tele-
phone Speech Linguistic Data Consortium, Philadelphia.
M. Barkat, J. Ohala, and F. Pellegrino. 1999. Prosody as a
Distinctive Feature for the Discrimination of Arabic Di-
alects. In Proceedings of Eurospeech?99.
F. Biadsy, N. Habash, and J. Hirschberg. 2009. Improv-
ing the Arabic Pronunciation Dictionary for Phone and
Word Recognition with Linguistically-Based Pronuncia-
tion Rules. In Proceedings of NAACL/HLT 2009, Col-
orado, USA.
A. Canavan and G. Zipperlen. 1996. CALLFRIEND Egyp-
tian Arabic Speech Linguistic Data Consortium, Philadel-
phia.
A. Canavan, G. Zipperlen, and D. Graff. 1997. CALL-
HOME Egyptian Arabic Speech Linguistic Data Consor-
tium, Philadelphia.
J. S. Garofolo et al 1993. TIMIT Acoustic-Phonetic
Continuous Speech Corpus Linguistic Data Consortium,
Philadelphia.
N. Habash. 2006. On Arabic and its Dialects. Multilingual
Magazine, 17(81).
R. Hamdi, M. Barkat-Defradas, E. Ferragne, and F. Pelle-
grino. 2004. Speech Timing and Rhythmic Structure in
Arabic Dialects: A Comparison of Two Approaches. In
Proceedings of Interspeech?04.
C. Holes. 2004. Modern Arabic: Structures, Functions, and
Varieties. Georgetown University Press. Revised Edition.
B. Ma, D. Zhu, and R. Tong. 2006. Chinese Dialect Iden-
tification Using Tone Features Based On Pitch Flux. In
Proceedings of ICASP?06.
M. Maamouri. 2006. Levantine Arabic QT Training Data
Set 5, Speech Linguistic Data Consortium, Philadelphia.
P. Matejka, P. Schwarz, J. Cernocky, and P. Chytil. 2005.
Phonotactic Language Identification using High Quality
Phoneme Recognition. In Proceedings of Eurospeech?05.
Y. K. Muthusamy, R.A. Cole, and B.T. Oshika. 1992. The
OGI Multi-Language Telephone Speech Corpus. In Pro-
ceedings of ICSLP?92.
J. Peters, P. Gilles, P. Auer, and M. Selting. 2002. Iden-
tification of Regional Varieties by Intonational Cues. An
Experimental Study on Hamburg and Berlin German.
45(2):115?139.
F. Ramus. 2002. Acoustic Correlates of Linguistic Rhythm:
Perspectives. In Speech Prosody.
A. Stolcke. 2002. SRILM - an Extensible Language Model-
ing Toolkit. In ICASP?02, pages 901?904.
P. Torres-Carrasquillo, T. P. Gleason, and D. A. Reynolds.
2004. Dialect identification using Gaussian Mixture Mod-
els. In Proceedings of the Speaker and Language Recog-
nition Workshop, Spain.
S. Young, G. Evermann, M. Gales, D. Kershaw, G. Moore,
J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Wood-
land. 2006. The HTK Book, version 3.4.
M. A. Zissman, T. Gleason, D. Rekart, and B. Losiewicz.
1996. Automatic Dialect Identification of Extempora-
neous Conversational, Latin American Spanish Speech.
In Proceedings of the IEEE International Conference on
Acoustics, Speech, and Signal Processing, Atlanta, USA.
M. A. Zissman. 1996. Comparison of Four Approaches to
Automatic Language Identification of Telephone Speech.
IEEE Transactions of Speech and Audio Processing, 4(1).
61
