Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 22?30,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Two methods to incorporate local morphosyntactic features in Hindi de-
pendency parsing 
 
Bharat Ram Ambati, Samar Husain, Sambhav Jain, Dipti Misra Sharma 
and Rajeev Sangal 
Language Technologies Research Centre, IIIT-Hyderabad, India - 500032. 
{ambati,samar}@research.iiit.ac.in, sambhav-
jain@students.iiit.ac.in,{dipti,sangal}@mail.iiit.ac.in 
 
 
Abstract 
In this paper we explore two strategies to in-
corporate local morphosyntactic features in 
Hindi dependency parsing. These features are 
obtained using a shallow parser. We first ex-
plore which information provided by the shal-
low parser is most beneficial and show that 
local morphosyntactic features in the form of 
chunk type, head/non-head information, 
chunk boundary information, distance to the 
end of the chunk and suffix concatenation are 
very crucial in Hindi dependency parsing. We 
then investigate the best way to incorporate 
this information during dependency parsing. 
Further, we compare the results of various ex-
periments based on various criterions and do 
some error analysis. All the experiments were 
done with two data-driven parsers, MaltParser 
and MSTParser, on a part of multi-layered and 
multi-representational Hindi Treebank which 
is under development. This paper is also the 
first attempt at complete sentence level pars-
ing for Hindi. 
1 Introduction 
The dependency parsing community has since a 
few years shown considerable interest in parsing 
morphologically rich languages with flexible word 
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages, 
but it is also motivated by the observation that the 
performance obtained for these languages have not 
been very high (Nivre et al, 2007a). Attempts at 
handling various non-configurational aspects in 
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty 
and Sima'an, 2008; Eryigit et al, 2008; Seddah et 
al., 2009; Husain et al, 2009; Gadde et al, 2010). 
Among other things, it has been pointed out that 
the use of language specific features may play a 
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways, 
and it has been hypothesized that the integration of 
morphological and syntactic information could be 
a key to better accuracy. However, it has also been 
noted that incorporating these language specific 
features in parsing is not always straightforward 
and many intuitive features do not always work in 
expected ways. 
In this paper we explore various strategies to in-
corporate local morphosyntactic features in Hindi 
dependency parsing. These features are obtained 
using a shallow parser. We conducted experiments 
with two data-driven parsers, MaltParser (Nivre et 
al., 2007b) and MSTParser (McDonald et al, 
2006). We first explore which information pro-
vided by the shallow parser is most beneficial and 
show that local morphosyntactic features in the 
form of chunk type, head/non-head information, 
chunk boundary information, distance to the end of 
the chunk and suffix concatenation are very crucial 
in Hindi dependency parsing. We then investigate 
the best way to incorporate this information during 
dependency parsing. All the experiments were 
done on a part of multi-layered and multi-
representational Hindi Treebank (Bhatt et al, 
2009)1.  
The shallow parser performs three tasks, (a) it 
gives the POS tags for each lexical item, (b) pro-
vides morphological features for each lexical item, 
and (c) performs chunking. A chunk is a minimal 
(non-recursive) phrase consisting of correlated, 
inseparable words/entities, such that the intra-
chunk dependencies are not distorted (Bharati et 
                                                          
1 This Treebank is still under development. There are currently 
27k tokens with complete sentence level annotation.  
22
al., 2006). Together, a group of lexical items with 
some POS tag and morphological features within a 
chunk can be utilized to automatically compute 
local morphosyntactic information. For example, 
such information can represent the postposi-
tion/case-marking in the case of noun chunks, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. In 
the experiments conducted for this paper such local 
information is automatically computed and incor-
porated as a feature to the head of a chunk. In gen-
eral, local morphosyntactic features correspond to 
all the parsing relevant local linguistic features that 
can be utilized using the notion of chunk. Previous-
ly, there have been some attempts at using chunk 
information in dependency parsing. Attardi and 
Dell?Orletta (2008) used chunking information in 
parsing English. They got an increase of 0.35% in 
labeled attachment accuracy and 0.47% in unla-
beled attachment accuracy over the state-of-the-art 
dependency parser. 
Among the three components (a-c, above), the 
parsing accuracy obtained using the POS feature is 
taken as baseline. We follow this by experiments 
where we explore how each of morph and chunk 
features help in improving dependency parsing 
accuracy. In particular, we find that local morpho-
syntactic features are the most crucial. These expe-
riments are discussed in section 2. In section 3 we 
will then see an alternative way to incorporate the 
best features obtained in section 2. In all the pars-
ing experiments discussed in section 2 and 3, at 
each step we explore all possible features and ex-
tract the best set of features. Best features of one 
experiment are used when we go to the next set of 
experiments. For example, when we explore the 
effect of chunk information, all the relevant morph 
information from previous set of experiments is 
taken into account.  
This paper is also the first attempt at complete 
sentence level parsing for Hindi. Due to the availa-
bility of dependency treebank for Hindi (Begum et 
al., 2008), there have been some previous attempts 
at Hindi data-driven dependency parsing (Bharati 
et al, 2008; Mannem et al, 2009; Husain et al, 
2009). Recently in ICON-09 NLP Tools Contest 
(Husain, 2009; and the references therein), rule-
based, constraint based, statistical and hybrid ap-
proaches were explored for dependency parsing. 
Previously, constraint based approaches to Indian 
language (IL) dependency parsing have also been 
explored (Bharati et al, 1993, 1995, 2009b, 
2009c). All these attempts, however, were finding 
inter-chunk dependency relations, given gold-
standard POS and chunk tags. Unlike these pre-
vious parsers, the dependencies in this work are 
between lexical items, i.e. the dependency tree is 
complete.  
The paper is arranged as follows, in section 2 
and 3, we discuss the parsing experiments. In sec-
tion 4, we describe the data and parser settings. 
Section 5 gives the results and discusses some re-
lated issues. General discussion and possible future 
work is mentioned in section 6. We conclude the 
paper in section 7. 
2 Getting the best linguistic features  
As mentioned earlier, a shallow parser consists of 
three main components, (a) POS tagger, (b) mor-
phological analyzer and (c) chunker. In this section 
we systematically explore what is the effect of 
each of these components. We?ll see in section 2.3 
that the best features of a-c can be used to compute 
local morphosyntactic features that, as the results 
show, are extremely useful. 
2.1 Using POS as feature (PaF): 
In this experiment we only use the POS tag infor-
mation of individual words during dependency 
parsing. First a raw sentence is POS-tagged. This 
POS-tagged sentence is then given to a parser to 
predict the dependency relations. Figure 1, shows 
the steps involved in this approach for (1). 
 
(1)  raama   ne         eka     seba        khaayaa  
  ?Ram?   ERG    ?one?  ?apple?      ?ate? 
       ?Ram ate an apple? 
 
Figure 1: Dependency parsing using only POS informa-
tion from a shallow parser. 
23
 
In (1) above, ?NN?, ?PSP?, ?QC?, ?NN? and ?VM? 
are the POS tags2 for raama, ne, eka, seba and 
khaayaa respectively. This information is provided 
as a feature to the parser. The result of this experi-
ment forms our baseline accuracy. 
2.2 Using Morph as feature (MaF): 
In addition to POS information, in this experiment 
we also use the morph information for each token. 
This morphological information is provided as a 
feature to the parser. Morph has the following in-
formation 
 
? Root: Root form of the word 
? Category: Course grained POS 
? Gender: Masculine/Feminine/Neuter 
? Number: Singular/Plural 
? Person: First/Second/Third person 
? Case: Oblique/Direct case 
? Suffix: Suffix of the word 
 
Take raama in (1), its morph information com-
prises of root = ?raama?, category = ?noun? gender 
= ?masculine?, number = ?singular?, person = 
?third?, case = ?direct?, suffix = ?0?. Similarly, 
khaayaa (?ate?) has the following morph informa-
tion. root = ?khaa?, category = ?verb? gender = 
?masculine?, numer = ?singular?, person = ?third?, 
case = ?direct?, suffix = ?yaa?. 
Through a series of experiments, the most cru-
cial morph features were selected. Root, case and 
suffix turn out to be the most important features. 
Results are discussed in section 5. 
2.3 Using local morphosyntax as feature 
(LMSaF) 
Along with POS and the most useful morph fea-
tures (root, case and suffix), in this experiment we 
also use local morphosyntactic features that reflect 
various chunk level information. These features 
are: 
? Type of the chunk 
? Head/non-head of the chunk 
                                                          
2 NN: Common noun, PSP: Post position, QC: Cardinal, VM: 
Verb. A list of complete POS tags can be found here: 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/POS-Tag-
List.pdf. The POS/chunk tag scheme followed in the Treebank 
is described in Bharati et al (2006). 
? Chunk boundary information 
? Distance to the end of the chunk 
? Suffix concatenation 
 
In example 1 (see section 2.1), there are two 
noun chunks and one verb chunk. raama and seba 
are the heads of the noun chunks. khaayaa is the 
head of the verb chunk. We follow standard IOB3 
notation for chunk boundary. raama,  eka and 
khaayaa are at the beginning (B) of their respective 
chunks. ne and seba are inside (I) their respective 
chunks. raama is at distance 1 from the end of the 
chunk and ne is at a distance 0 from the end of the 
chunk. 
Once we have a chunk and morph feature like 
suffix, we can perform suffix concatenation auto-
matically. A group of lexical items with some POS 
tags and suffix information within a chunk can be 
utilized to automatically compute this feature. This 
feature can, for example, represent the postposi-
tion/case-marking in the case of noun chunk, or it 
may represent the tense, aspect and modality 
(TAM) information in the case of verb chunks. 
Note that, this feature becomes part of the lexical 
item that is the head of a chunk. Take (2) as a case 
in point: 
 
(2) [NP raama/NNP   ne/PSP]     [NP seba/NN]        
              ?Ram?           ERG                ?apple?   
      [VGF khaa/VM     liyaa/VAUX] 
                 ?eat?           ?PRFT? 
      ?Ram ate an apple? 
 
The suffix concatenation feature for khaa, which 
is the head of the VGF chunk, will be ?0+yaa? and 
is formed by concatenating the suffix of the main 
verb with that of its auxiliary. Similarly, the suffix 
concatenation feature for raama, which is head of 
the NP chunk, will be ?0+ne?. This feature turns 
out to be very important. This is because in Hindi 
(and many other Indian languages) there is a direct 
correlation between the TAM markers and the case 
that appears on some nominals (Bharati et al, 
1995). In (2), for example, khaa liyaa together 
gives the past perfective aspect for the verb khaa-
naa ?to eat?. Since, Hindi is split ergative, the sub-
ject of the transitive verb takes an ergative case 
marker when the verb is past perfective. Similar 
                                                          
3 Inside, Outside, Beginning of the chunk. 
24
correlation between the case markers and TAM 
exist in many other cases. 
3 An alternative approach to use best fea-
tures: A 2-stage setup (2stage) 
So far we have been using various information 
such as POS, chunk, etc. as features. Rather than 
using them as features and doing parsing at one go, 
we can alternatively follow a 2-stage setup. In par-
ticular, we divide the task of parsing into:  
 
? Intra-chunk dependency parsing 
? Inter-chunk dependency parsing 
 
We still use POS, best morphological features 
(case, suffix, root) information as regular features 
during parsing. But unlike LMSaF mentioned in 
section 2.3, where we gave local morphosyntactic 
information as a feature, we divided the task of 
parsing into sub-tasks. A similar approach was also 
proposed by Bharati et al (2009c). During intra-
chunk dependency parsing, we try to find the de-
pendency relations of the words within a chunk. 
Following which, chunk heads of each chunk with-
in a sentence are extracted. On these chunk heads 
we run an inter-chunk dependency parser. For each 
chunk head, in addition to POS tag, useful morpho-
logical features, any useful intra-chunk information 
in the form of lexical item, suffix concatenation, 
dependency relation are also given as a feature. 
 
Figure 2: Dependency parsing using chunk information: 
2-stage approach. 
Figure 2 shows the steps involved in this ap-
proach for (1). There are two noun chunks and one 
verb chunk in this sentence. raama and seba are 
the heads of the noun chunks. khaaya is the head 
of the verb chunk. The intra-chunk parser attaches 
ne to raama and eka to seba with dependency la-
bels ?lwg__psp? and ?nmod__adj?4 respectively. 
Heads of each chunk along with its POS, morpho-
logical features, local morphosyntactic features and 
intra-chunk features are extracted and given to in-
ter-chunk parser. Using this information the inter-
chunk dependency parser marks the dependency 
relations between chunk heads. khaaya becomes 
the root of the dependency tree. raama and seba 
are attached to khaaya with dependency labels ?k1? 
and ?k2?5 respectively. 
4 Experimental Setup 
In this section we describe the data and the parser 
settings used for our experiments.  
4.1 Data 
For our experiments we took 1228 dependency 
annotated sentences (27k tokens), which have 
complete sentence level annotation from the new 
multi-layered and multi-representational Hindi 
Treebank (Bhatt et al, 2009). This treebank is still 
under development. Average length of these sen-
tences is 22 tokens/sentence and 10 
chunks/sentence. We divided the data into two 
sets, 1000 sentences for training and 228 sentences 
for testing.  
4.2 Parsers and settings 
All experiments were performed using two data-
driven parsers, MaltParser6 (Nivre et al, 2007b), 
and MSTParser7 (McDonald et al, 2006).
                                                          
4 nmod__adj is an intra-chunk label for quantifier-noun mod-
ification. lwg__psp is the label for post-position marker. De-
tails of the labels can be seen in the intra-chunk guidelines 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/IntraChunk-
Dependency-Annotation-Guidelines.pdf 
5 k1 (karta) and k2 (karma) are syntactico-semantic labels 
which have some properties of both grammatical roles and 
thematic roles. k1 behaves similar to subject and agent. k2 
behaves similar to object and patient (Bharati et al, 1995; 
Vaidya et al, 2009). For complete tagset, see (Bharati et al, 
2009). 
6 Malt Version 1.3.1 
7 MST Version 0.4b 
25
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 89.4 78.2 80.5 90.4 80.1 82.4 86.3 75.1 77.9 87.9 77.0 79.3 
MaF 89.6 80.5 83.1 90.4 81.7 84.1 89.1 79.2 82.5 90.0 80.9 83.9 
LMSaF 91.5 82.7 84.7 91.8 84.0 86.2 90.8 79.8 82.0 92.0 81.8 83.8 
2stage 91.8 83.3 85.3 92.4 84.4 86.3 92.1 82.2 84.3 92.7 84.0 86.2 
Table 1: Results of all the four approaches using gold-standard shallow parser information. 
 
Malt is a classifier based shift/reduce parser. It 
provides option for six parsing algorithms, namely, 
arc-eager, arc-standard, convington projective, co-
vington non-projective, stack projective, stack ea-
ger and stack lazy. The parser also provides option 
for libsvm and liblinear learning model. It uses 
graph transformation to handle non-projective trees 
(Nivre and Nilsson, 2005). MST uses Chu-Liu-
Edmonds (Chu and Liu, 1965; Edmonds, 1967) 
Maximum Spanning Tree algorithm for non-
projective parsing and Eisner's algorithm for pro-
jective parsing (Eisner, 1996). It uses online large 
margin learning as the learning algorithm (McDo-
nald et al, 2005). In this paper, we use MST only 
for unlabeled dependency tree and use a separate 
maximum entropy model8 (MaxEnt) for labeling. 
Various combination of features such as node, its 
parent, siblings and children were tried out before 
arriving at the best results. 
As the training data size is small we did 5-fold 
cross validation on the training data for tuning the 
parameters of the parsers and for feature selection. 
Best settings obtained using cross-validated data 
are applied on test set. We present the results both 
on cross validated data and on test data.  
For the Malt Parser, arc-eager algorithm gave 
better performance over others in all the approach-
es. Libsvm consistently gave better performance 
over liblinear in all the experiments. For SVM set-
tings, we tried out different combinations of best 
SVM settings of the same parser on different lan-
guages in CoNLL-2007 shared task (Hall et al, 
2007) and applied the best settings. For feature 
model, apart from trying best feature settings of the 
same parser on different languages in CoNLL-
2007 shared task (Hall et al, 2007), we also tried 
out different combinations of linguistically intui-
tive features and applied the best feature model. 
The best feature model is same as the feature mod-
el used in Ambati et al (2009a), which is the best 
                                                          
8 http://maxent.sourceforge.net/ 
performing system in the ICON-2009 NLP Tools 
Contest (Husain, 2009). 
For the MSTParser, non-projective algorithm, 
order=2 and training-k=5 gave best results in all 
the approaches. For the MaxEnt, apart from some 
general useful features, we experimented consider-
ing different combinations of features of node, par-
ent, siblings, and children of the node.  
5 Results and Analysis 
All the experiments discussed in section 2 and 3 
were performed considering both gold-standard 
shallow parser information and automatic shallow 
parser9 information. Automatic shallow parser uses 
a rule based system for morph analysis, a 
CRF+TBL based POS-tagger and chunker. The 
tagger and chunker are 93% and 87% accurate re-
spectively. These accuracies are obtained after us-
ing the approach of PVS and Gali, (2007) on larger 
training data. In addition, while using automatic 
shallow parser information to get the results, we 
also explored using both gold-standard and auto-
matic information during training. As expected, 
using automatic shallow parser information for 
training gave better performance than using gold 
while training.  
Table 1 and Table 2 shows the results of the four 
experiments using gold-standard and automatic 
shallow parser information respectively. We eva-
luated our experiments based on unlabeled attach-
ment score (UAS), labeled attachment score (LAS) 
and labeled score (LS) (Nivre et al, 2007a). Best 
LAS on test data is 84.4% (with 2stage) and 75.4% 
(with LMSaF) using gold and automatic shallow 
parser information respectively. These results are 
obtained using MaltParser. In the following sub-
section we discuss the results based on different 
criterion.
                                                          
9 http://ltrc.iiit.ac.in/analyzer/hindi/ 
26
 Malt MST+MaxEnt 
Cross-validation Test-set Cross-validation Test-set 
UAS LAS LS UAS LAS LS UAS LAS LS UAS LAS LS 
PaF 82.2 69.3  73.4  84.6  72.9  76.5  79.4  66.5  70.7  81.6  69.4  73.1  
MaF 82.5 71.6  76.1  84.0  73.6  77.6  82.3  70.4  75.4  83.4  72.7  77.3  
LMSaF 83.2 73.0  77.0  85.5  75.4  78.9  82.6  71.3  76.1  85.0  73.4  77.3  
2stage 79.0 69.5 75.6 79.6 71.1 76.8 78.8  66.6  72.6 80.1  69.7  75.4  
Table 2: Results of all the four experiments using automatic shallow parser information. 
 
POS tags provide very basic linguistic informa-
tion in the form of broad grained categories. The 
best LAS for PaF while using gold and automatic 
tagger were 80.1% and 72.9% respectively. The 
morph information in the form of case, suffix and 
root information proved to be the most important 
features. But surprisingly, gender, number and per-
son features didn?t help. Agreement patterns in 
Hindi are not straightforward. For example, the 
verb agrees with k2 if the k1 has a post-position; it 
may also sometimes take the default features. In a 
passive sentence, the verb agrees only with k2. The 
agreement problem worsens when there is coordi-
nation or when there is a complex verb. It is un-
derstandable then that the parser is unable to learn 
the selective agreement pattern which needs to be 
followed.  
LMSaF on the other hand encode richer infor-
mation and capture some local linguistic patterns. 
The first four features in LMSaF (chunk type, 
chunk boundary, head/non-head of chunk and dis-
tance to the end of chunk) were found to be useful 
consistently. The fifth feature, in the form of suffix 
concatenation, gave us the biggest jump, and cap-
tures the correlation between the TAM markers of 
the verbs and the case markers on the nominals. 
5.1 Feature comparison: PaF, MaF vs. 
LMSaF 
Dependency labels can be classified as two types 
based on their nature, namely, inter-chunk depen-
dency labels and intra-chunk labels. Inter-chunk 
dependency labels are syntacto-semantic in nature. 
Whereas intra-chunk dependency labels are purely 
syntactic in nature.  
Figure 3, shows the f-measure for top six inter-
chunk and intra-chunk dependency labels for PaF, 
MaF, and LMSaF using Maltparser on test data 
using automatic shallow parser information. The 
first six labels (k1, k2, pof, r6, ccof, and k7p) are 
the top six inter-chunk labels and the next six la-
bels (lwg__psp, lwg__aux, lwg__cont, rsym, 
nmod__adj, and pof__cn) are the top six intra-
chunk labels. First six labels (inter-chunk) corres-
pond to 28.41% and next six labels (intra-chunk) 
correspond to 48.81% of the total labels in the test 
data. The figure shows that with POS information 
alone, f-measure for top four intra-chunk labels 
reached more than 90% accuracy. The accuracy 
increases marginally with the addition of morph 
and local morphosytactic features. The results cor-
roborates with our intuition that intra-chunk de-
pendencies are mostly syntactic.  For example, 
consider an intra-chunk label ?lwg__psp?. This is 
the label for postposition marker. A post-position 
marker succeeding a noun is attached to that noun 
with the label ?lwg__psp?. POS tag for post-
position marker is PSP. So, if a NN (common 
noun) or a NNP (proper noun) is followed by a 
PSP (post-position marker), then the PSP will be 
attached to the preceding NN/NNP with the de-
pendency label ?lwg_psp?. As a result, providing 
POS information itself gave an f-measure of 98.3% 
for ?lwg_psp?.  With morph and local morphosy-
tactic features, this got increased to 98.4%. How-
ever, f-measure for some labels like ?nmod__adj? 
is around 80% only. ?nmod__adj? is the label for 
adjective-noun, quantifier-noun modifications. 
Low accuracy for these labels is mainly due to two 
reasons. One is POS tag errors. And the other is 
attachment errors due to genuine ambiguities such 
as compounding. 
For inter-chunk labels (first six columns in the 
figure 3), there is considerable improvement in the 
f-measure using morph and local morphosytactic 
features. As mentioned, local morphosyntactic fea-
tures provide local linguistic information. For ex-
ample, consider the case of verbs. At POS level, 
there are only two tags ?VM? and ?VAUX? for 
main verbs and auxiliary verbs respectively (Bha-
rati et al, 2006). Information about finite/non-
finiteness is not present in the POS tag. But, at 
chunk level there are four different chunk tags for
27
30
40
50
60
70
80
90
100
k1 k2 pof r6 ccof k7p lwg__psp lwg__vaux lwg__cont rsym nmod__adj pof__cn
PaF
MaF
LMaF
Figure 3: F-measure of top 6, inter-chunk and intra-chunk labels for PaF, MaF and LMSaF approaches using Malt-
parser on test data using automatic shallow parser information. 
 
verbs, namely VGF, VGNF, VGINF and VGNN. 
They are respectively, finite, non-finite, infinitival 
and gerundial chunk tags. The difference in the 
verbal chunk tag is a good cue for helping the 
parser in identifying different syntactic behavior of 
these verbs. Moreover, a finite verb can become 
the root of the sentence, whereas a non-finite or 
infinitival verb can?t. Thus, providing chunk in-
formation also helped in improving the correct 
identification of the root of the sentence. 
Similar to Prague Treebank (Hajicova, 1998), 
coordinating conjuncts are heads in the treebank 
that we use. The relation between a conjunct and 
its children is shown using ?ccof? label. A coordi-
nating conjuct takes children of similar type only. 
For example, a coordinating conjuct can have two 
finite verbs or two non-finite verbs as its children, 
but not a finite verb and a non-finite verb. Such 
instances are also handled more effectively if 
chunk information is incorporated. The largest in-
crease in performance, however, was due to the 
?suffix concatenation? feature. Significant im-
provement in the core inter-chunk dependency la-
bels (such as k1, k2, k4, etc.) due to this feature is 
the main reason for the overall improvement in the 
parsing accuracy. As mentioned earlier, this is be-
cause this feature captures the correlation between 
the TAM markers of the verbs and the case mark-
ers on the nominals. 
5.2 Approach comparison: LMSaF vs. 2stage 
Both LMSaF and 2stage use chunk information. In 
LMSaF, chunk information is given as a feature 
whereas in 2stage, sentence parsing is divided into 
intra-chunk and inter-chunk parsing. Both the ap-
proaches have their pros and cons. In LMSaF as 
everything is done in a single stage there is much 
richer context to learn from. In 2stage, we can pro-
vide features specific to each stage which can?t be 
done in a single stage approach (McDonald et al, 
2006). But in 2stage, as we are dividing the task, 
accuracy of the division and the error propagation 
might pose a problem. This is reflected in the re-
sults where the 2-stage performs better than the 
single stage while using gold standard information, 
but lags behind considerably when the features are 
automatically computed.  
During intra-chunk parsing in the 2stage setup, 
we tried out using both a rule-based approach and 
a statistical approach (using MaltParser). The rule 
based system performed slightly better (0.1% 
LAS) than statistical when gold chunks are consi-
dered. But, with automatic chunks, the statistical 
approach outperformed rule-based system with a 
difference of 7% in LAS. This is not surprising 
because, the rules used are very robust and mostly 
based on POS and chunk information. Due to er-
rors induced by the automatic POS tagger and 
chunker, the rule-based system couldn?t perform 
well. Consider a small example chunk given be-
low. 
 ((    NP 
 meraa ?my?   PRP  
 bhaaii ?brother? NN 
)) 
As per the Hindi chunking guidelines (Bharati et 
al., 2006), meraa and bhaaii should be in two sepa-
rate chunks. And as per Hindi dependency annota-
tion guidelines (Bharati et al, 2009), meraa is 
attached to bhaaii with a dependency label ?r6?10. 
When the chunker wrongly chunks them in a single 
                                                          
10?r6? is the dependency label for genitive relation. 
28
chunk, intra-chunk parser will assign the depen-
dency relation for meraa. Rule based system can 
never assign ?r6? relation to meraa as it is an inter-
chunk label and the rules used cannot handle such 
cases. But in a statistical system, if we train the 
parser using automatic chunks instead of gold 
chunks, the system can potentially assign ?r6? la-
bel.  
5.3 Parser comparison: MST vs. Malt 
In all the experiments, results of MaltParser are 
consistently better than MST+MaxEnt. We know 
that Maltparser is good at short distance labeling 
and MST is good at long distance labeling (McDo-
nald and Nivre, 2007). The root of the sentence is 
better identified by MSTParser than MaltParser. 
Our results also confirm this. MST+MaxEnt and 
Malt could identify the root of the sentence with an 
f-measure of 89.7% and 72.3% respectively. Pres-
ence of more short distance labels helped Malt to 
outperform MST. Figure 5, shows the f-measure 
relative to dependency length for both the parsers 
on test data using automatic shallow parser infor-
mation for LMSaF.  
30
40
50
60
70
80
90
100
0 5 10 15+
Dependency Length
f-
m
ea
su
re
Malt
MST+MaxEnt
 
Figure 5: Dependency arc f-measure relative to depen-
dency length. 
6 Discussion and Future Work 
We systematically explored the effect of various 
linguistic features in Hindi dependency parsing. 
Results show that POS, case, suffix, root, along 
with local morphosyntactic features help depen-
dency parsing. We then described 2 methods to 
incorporate such features during the parsing 
process. These methods can be thought as different 
paradigms of modularity. For practical reasons (i.e. 
given the POS tagger/chunker accuracies), it is 
wiser to use this information as features rather than 
dividing the task into two stages.  
As mentioned earlier, this is the first attempt at 
complete sentence level parsing for Hindi. So, we 
cannot compare our results with previous attempts 
at Hindi dependency parsing, due to, (a) The data 
used here is different and (b) we produce complete 
sentence parses rather than chunk level parses. 
As mentioned in section 5.1, accuracies of intra-
chunk dependencies are very high compared to 
inter-chunk dependencies. Inter-chunk dependen-
cies are syntacto-semantic in nature. The parser 
depends on surface syntactic cues to identify such 
relations. But syntactic information alone is always 
not sufficient, either due to unavailability or due to 
ambiguity. In such cases, providing some semantic 
information can help in improving the inter-chunk 
dependency accuracy. There have been attempts at 
using minimal semantic information in dependency 
parsing for Hindi (Bharati et al, 2008). Recently, 
Ambati et al (2009b) used six semantic features 
namely, human, non-human, in-animate, time, 
place, and abstract for Hindi dependency parsing. 
Using gold-standard semantic features, they 
showed considerable improvement in the core in-
ter-chunk dependency accuracy. Some attempts at 
using clause information in dependency parsing for 
Hindi (Gadde et al, 2010) have also been made. 
These attempts were at inter-chunk dependency 
parsing using gold-standard POS tags and chunks. 
We plan to see their effect in complete sentence 
parsing using automatic shallow parser information 
also.  
7 Conclusion 
In this paper we explored two strategies to incorpo-
rate local morphosyntactic features in Hindi de-
pendency parsing. These features were obtained 
using a shallow parser. We first explored which 
information provided by the shallow parser is use-
ful  and showed that local morphosyntactic fea-
tures in the form of chunk type, head/non-head 
info, chunk boundary info, distance to the end of 
the chunk and suffix concatenation are very crucial 
for Hindi dependency parsing. We then investi-
gated the best way to incorporate this information 
during dependency parsing. Further, we compared 
the results of various experiments based on various 
criterions and did some error analysis. This paper 
was also the first attempt at complete sentence lev-
el parsing for Hindi. 
29
References  
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing. In 
Proc of the ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, pp 32-37.  
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. 
Effect of Minimal Semantics on Dependency Pars-
ing. In Proc of RANLP09 student paper workshop. 
G. Attardi and F. Dell?Orletta. 2008. Chunking and De-
pendency Parsing. In Proc of LREC Workshop on 
Partial Parsing: Between Chunking and Deep Pars-
ing. Marrakech, Morocco. 
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and 
R. Sangal. 2008. Dependency annotation scheme for 
Indian languages. In Proc of IJCNLP-2008. 
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural 
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi. 
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, 
and R. Sangal. 2008. Two semantic features make all 
the difference in parsing accuracy. In Proc of ICON. 
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. 
AnnCorra: Annotating Corpora Guidelines for POS 
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad. 
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam 
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi 
TreeBank. 
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf 
A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 
2009b. Two stage constraint based hybrid approach 
to free word order language dependency parsing. In 
Proc. of IWPT. 
A. Bharati, S. Husain, M. Vijay, K. Deepak, D. M. 
Sharma and R. Sangal. 2009c. Constraint Based Hy-
brid Approach to Parsing Indian Languages. In Proc 
of PACLIC 23. Hong Kong. 2009. 
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. 
M. Sharma and F. Xia. 2009. Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu. In 
Proc. of the Third LAW at 47th ACL and 4th IJCNLP. 
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400. 
J. Edmonds. 1967. Optimum branchings. Journal of 
Research of the National Bureau of Standards, 
71B:233?240. 
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc of 
COLING-96, pp. 340?345. 
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency 
Parsing of Turkish. Computational Linguistics 34(3), 
357-389. 
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. 
Sangal. 2010. Improving Data Driven Dependency 
Parsing using Clausal Information. In Proc of 
NAACL-HLT 2010, Los Angeles, CA. 
E. Hajicova. 1998. Prague Dependency Treebank: From 
Analytic to Tectogrammatical Annotation. In Proc of 
TSD?98. 
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. 
Nilsson and M. Saers. 2007. Single Malt or Blended? 
A Study in Multilingual Parser Optimization. In Proc 
of the CoNLL Shared Task Session of EMNLP-
CoNLL 2007, 933?939. 
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. In Proc of ICON09 NLP Tools Contest: In-
dian Language Dependency Parsing. Hyderabad, 
India. 
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. 
Sangal. 2009. A modular cascaded approach to com-
plete parsing. In Proc. of the COLIPS IALP. 
P. Mannem, A. Abhilash and A. Bharati. 2009. LTAG-
spinal Treebank and Parser for Hindi. In Proc of In-
ternational Conference on NLP, Hyderabad. 2009. 
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In 
Proc of ACL. pp. 91?98. 
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. In Proc of the Tenth (CoNLL-X), pp. 
216?220. 
R. McDonald and J. Nivre. 2007. Characterizing the 
errors of data-driven dependency parsing models. In 
Proc. of EMNLP-CoNLL. 
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson,  S. 
Riedel and D. Yuret. 2007a. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proc of 
EMNLP/CoNLL-2007. 
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. 
K?bler, S. Marinov and E Marsi. 2007b. MaltParser: 
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering, 
13(2), 95-135. 
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL-2005, pp. 99?106. 
Avinesh PVS and K. Gali. 2007. Part-Of-Speech Tag-
ging and Chunking Using Conditional Random 
Fields and Transformation Based Learning. In Proc 
of the SPSAL workshop during IJCAI '07. 
D. Seddah, M. Candito and B. Crabb?. 2009. Cross 
parser evaluation: a French Treebanks study. In Proc. 
of IWPT, 150-161. 
R. Tsarfaty and K. Sima'an. 2008. Relational-
Realizational Parsing. In Proc. of CoLing, 889-896. 
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 
2009. A karaka-based dependency annotation scheme 
for English. In Proc. of CICLing, 41-52. 
30
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 159?167,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Animacy Annotation in the Hindi Treebank
Itisree Jena, Riyaz Ahmad Bhat, Sambhav Jain and Dipti Misra Sharma
Language Technologies Research Centre, IIIT-Hyderabad, India
{itisree|riyaz.bhat|sambhav.jain}@research.iiit.ac.in, dipti@iiit.ac.in
Abstract
In this paper, we discuss our efforts to anno-
tate nominals in the Hindi Treebank with the
semantic property of animacy. Although the
treebank already encodes lexical information
at a number of levels such as morph and part
of speech, the addition of animacy informa-
tion seems promising given its relevance to
varied linguistic phenomena. The suggestion
is based on the theoretical and computational
analysis of the property of animacy in the con-
text of anaphora resolution, syntactic parsing,
verb classification and argument differentia-
tion.
1 Introduction
Animacy can either be viewed as a biological prop-
erty or a grammatical category of nouns. In a
strictly biological sense, all living entities are ani-
mate, while all other entities are seen as inanimate.
However, in its linguistic sense, the term is syn-
onymous with a referent?s ability to act or instigate
events volitionally (Kittila? et al, 2011). Although
seemingly different, linguistic animacy can be im-
plied from biological animacy. In linguistics, the
manifestation of animacy and its relevance to lin-
guistic phenomena have been studied quite exten-
sively. Animacy has been shown, cross linguisti-
cally, to control a number of linguistic phenomena.
Case marking, argument realization, topicality or
discourse salience are some phenomena, highly cor-
related with the property of animacy (Aissen, 2003).
In linguistic theory, however, animacy is not seen
as a dichotomous variable, rather a range capturing
finer distinctions of linguistic relevance. Animacy
hierarchy proposed in Silverstein?s influential arti-
cle on ?animacy hierarchy? (Silverstein, 1986) ranks
nominals on a scale of the following gradience: 1st
pers> 2nd pers> 3rd anim> 3rd inanim. Several such
hierarchies of animacy have been proposed follow-
ing (Silverstein, 1986), one basic scale taken from
(Aissen, 2003) makes a three-way distinction as hu-
mans > animates > inanimates. These hierarchies can
be said to be based on the likelihood of a referent of
a nominal to act as an agent in an event (Kittila? et
al., 2011). Thus higher a nominal on these hierar-
chies higher the degree of agency/control it has over
an action. In morphologically rich languages, the
degree of control/agency is expressed by case mark-
ing. Case markers capture the degree of control a
nominal has in a given context (Hopper and Thomp-
son, 1980; Butt, 2006). They rank nominals on the
continuum of control as shown in (1)1. Nominals
marked with Ergative case have highest control and
the ones marked with Locative have lowest.
Erg > Gen > Inst > Dat > Acc > Loc (1)
Of late the systematic correspondences between
animacy and linguistic phenomena have been ex-
plored for various NLP applications. It has been
noted that animacy provides important informa-
tion, to mention a few, for anaphora resolution
(Evans and Orasan, 2000), argument disambiguation
(Dell?Orletta et al, 2005), syntactic parsing (?vre-
lid and Nivre, 2007; Bharati et al, 2008; Ambati et
al., 2009) and verb classification (Merlo and Steven-
1Ergative, Genitive, Instrumental, Dative, Accusative and
Locative in the given order.
159
son, 2001). Despite the fact that animacy could play
an important role in NLP applications, its annota-
tion, however, is not usually featured in a treebank
or any other annotated corpora used for developing
these applications. There are a very few annotation
projects that have included animacy in their anno-
tation manual, following its strong theoretical and
computational implications. One such work, mo-
tivated by the theoretical significance of the prop-
erty of animacy, is (Zaenen et al, 2004). They
make use of a coding scheme drafted for a para-
phrase project (Bresnan et al, 2002) and present
an explicit annotation scheme for animacy in En-
glish. The annotation scheme assumes a three-way
distinction, distinguishing Human, Other animates
and Inanimates. Among the latter two categories
?Other animates? is further sub-categorized into
Organizations and Animals, while the category of
?Inanimates? further distinguishes between con-
crete and non-concrete, and time and place nomi-
nals. As per the annotation scheme, nominals are
annotated according to the animacy of their referents
in a given context. Another annotation work that
includes animacy for nominals is (Teleman, 1974),
however, the distinction made is binary between hu-
man and non-human referents of a nominal in a
given context. In a recent work on animacy annota-
tion, Thuilier et al (2012) have annotated a multi-
source French corpora with animacy and verb se-
mantics, on the lines of (Zaenen et al, 2004). Apart
from the manual annotation for animacy, lexical re-
sources like wordnets are an important source of this
information, if available. These resources usually
cover animacy, though indirectly (Fellbaum, 2010;
Narayan et al, 2002). Although a wordnet is an
easily accessible resource for animacy information,
there are some limitations on its use, as discussed
below:
1. Coverage: Hindi wordnet only treats common
nouns while proper nouns are excluded (except
famous names) see Table 1. The problem is se-
vere where the domain of text includes more
proper than common nouns, which is the case
with the Hindi Treebank as it is annotated on
newspaper articles.
2. Ambiguity: Since words can be ambiguous, the
animacy listed in wordnet can only be used in
presence of a high performance word sense dis-
ambiguation system. As shown in Table 2, only
38.02% of nouns have a single sense as listed in
Hindi Wordnet.
3. Metonymy or Complex Types: Domains like
newspaper articles are filled with metonymic
expressions like courts, institute names, coun-
try names etc, that can refer to a building, a ge-
ographical place or a group of people depend-
ing on the context of use. These words are not
ambiguous per se but show different aspects of
their semantics in different contexts (logically
polysemous). Hindi wordnet treats these types
of nouns as inanimate.
Nominals in HTB Hindi WordNet Coverage
78,136 65,064 83.27%
Table 1: Coverage of Hindi WordNet on HTB Nominals.
HTB Nominals Single Unique Sense
with WN Semantics in Hindi WordNet
65,064 24,741 (38.02%)
Table 2: Nominals in HTB with multiple senses
Given these drawbacks, we have included ani-
macy information manually in the annotation of the
Hindi Treebank, as discussed in this work. In the
rest, we will discuss the annotation of nominal ex-
pressions with animacy and the motivation for the
same, the discussion will follow as: Section 2 gives
a brief overview of the Hindi Treebank with all its
layers. Section 3 motivates the annotation of nom-
inals with animacy, followed by the annotation ef-
forts and issues encountered in Section 4. Section
5 concludes the paper with a discussion on possible
future directions.
2 Description of the Hindi Treebank
In the following, we give an overview of the Hindi
Treebank (HTB), focusing mainly on its dependency
layer. The Hindi-Urdu Treebank (Palmer et al,
2009; Bhatt et al, 2009) is a multi-layered and
multi-representational treebank. It includes three
levels of annotation, namely two syntactic levels and
one lexical-semantic level. One syntactic level is a
dependency layer which follows the CPG (Begum
160
et al, 2008), inspired by the Pa?n. inian grammati-
cal theory of Sanskrit. The other level is annotated
with phrase structure inspired by the Chomskyan ap-
proach to syntax (Chomsky, 1981) and follows a bi-
nary branching representation. The third layer of an-
notation, a purely lexical semantic one, encodes the
semantic relations following the English PropBank
(Palmer et al, 2005).
In the dependency annotation, relations are
mainly verb-centric. The relation that holds between
a verb and its arguments is called a kar.aka relation.
Besides kar.aka relations, dependency relations also
exist between nouns (genitives), between nouns and
their modifiers (adjectival modification, relativiza-
tion), between verbs and their modifiers (adver-
bial modification including subordination). CPG
provides an essentially syntactico-semantic depen-
dency annotation, incorporating kar.aka (e.g., agent,
theme, etc.), non-kar.aka (e.g. possession, purpose)
and other (part of) relations. A complete tag set of
dependency relations based on CPG can be found in
(Bharati et al, 2009), the ones starting with ?k? are
largely Pa?n. inian kar.aka relations, and are assigned
to the arguments of a verb. Figure 1 encodes the de-
pendency structure of (5), the preterminal node is a
part of speech of a lexical item (e.g. NN,VM, PSP).
The lexical items with their part of speech tags are
further grouped into constituents called chunks (e.g.
NP, VGF) as part of the sentence analysis. The de-
pendencies are attached at the chunk level, marked
with ?drel? in the SSF format. k1 is the agent of
an action (KAyA ?eat?), whereas k2 is the object or
patient.
(5) s\@yA n
Sandhya-Erg
sb
apple-Nom
KAyA
eat-Perf
?
?Sandhya ate an apple.?
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?>
1.1 s\@yA NNP<fs af=?s\@yA,n,f,sg,3,o,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k2:VGF?>
2.1 sb NN <fs af=?sb,n,m,sg,3,d,0,0?>
))
3 (( VGF<fs name=?VGF?>
3.1 KAyA VM <fs af=?KA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 1: Annotation of an Example Sentence in SSF.
Despite the fact that the Hindi Treebank already
features a number of layers as discussed above, there
have been different proposals to enrich it further.
Hautli et al (2012) proposed an additional layer to
the treebank, for the deep analysis of the language,
by incorporating the functional structure (or
f-structure) of Lexical Functional Grammar which
encodes traditional syntactic notions such as sub-
ject, object, complement and adjunct. Dakwale et
al. (2012) have also extended the treebank with
anaphoric relations, with a motive to develop a data
driven anaphora resolution system for Hindi. Given
this scenario, our effort is to enrich the treebank
with the animacy annotation. In the following
sections, we will discuss in detail, the annotation of
the animacy property of nominals in the treebank
and the motive for the same.
3 Motivation: In the Context of
Dependency Parsing
Hindi is a morphologically rich language, gram-
matical relations are depicted by its morphology
via case clitics. Hindi has a morphologically
split-ergative case marking system (Mahajan, 1990;
Dixon, 1994). Case marking is dependent on the
aspect of a verb (progressive/perfective), transitivity
(transitive/intransitive) and the type of a nominal
(definite/indefinite, animate/inanimate). Given
this peculiar behavior of case marking in Hindi,
arguments of a verb (e.g. transitive) have a number
of possible configurations with respect to the case
marking as shown in the statistics drawn from
the Hindi Treebank released for MTPIL Hindi
Dependency parsing shared task (Sharma et al,
2012) in Table 3. Almost in 15% of the transitive
clauses, there is no morphological case marker on
any of the arguments of a verb which, in the context
of data driven parsing, means lack of an explicit
cue for machine learning. Although, in other cases
there is a case marker, at least on one argument of a
verb, the ambiguity in case markers (one-to-many
mapping between case markers and grammatical
functions as presented in Table 4) further worsens
the situation (however, see Ambati et al (2010) and
Bhat et al (2012) for the impact of case markers on
parsing Hindi/Urdu). Consider the examples from
161
(6a-e), the instrumental se is extremely ambiguous.
It can mark the instrumental adjuncts as in (6a),
source expressions as in (6b), material as in (6c),
comitatives as in (6d), and causes as in (6e).
K2-Unmarked K2-Marked
K1-Unmarked 1276 741
K1-Marked 5373 966
Table 3: Co-occurrence of Marked and Unmarked verb argu-
ments (core) in HTB.
n/ne ko/ko s/se m\/meN pr/par kA/kaa
(Ergative) (Dative) (Instrumental) (Locative) (Locative) (Genitive)
k1(agent) 7222 575 21 11 3 612
k2(patient) 0 3448 451 8 24 39
k3(instrument) 0 0 347 0 0 1
k4(recipient) 0 1851 351 0 1 4
k4a(experiencer) 0 420 8 0 0 2
k5(source) 0 2 1176 12 1 0
k7(location) 0 1140 308 8707 3116 19
r6(possession) 0 3 1 0 0 2251
Table 4 : Distribution of case markers across case function.
(6a) mohn n
Mohan-Erg
cAbF s
key-Inst
taAlA
lock-Nom
KolA
open
?
?Mohan opened the lock with a key.?
(6b) gFtaA n
Geeta-Erg
Ed?F s
Delhi-Inst
sAmAn
luggage-Nom
m\gvAyA
procure
?
?Geeta procured the luggage from Delhi.?
(6c) m EtakAr n
sculptor-Erg
p(Tr s
stone-Inst
m Eta
idol-Nom
bnAyF
make
?
?The sculptor made an idol out of stone.?
(6d) rAm kF
Ram-Gen
[yAm s
Shyaam-Inst
bAta
talk-Nom
h  I
happen
?
?Ram spoke to Shyaam.?
(6e) bAErf s
rain-Inst
kI Psl\
many crops-Nom
tabAh
destroy
ho gyF\
happen-Perf
?
?Many crops were destroyed due to the rain.?
(7) EcEwyA
bird-Nom
dAnA
grain-Nom
c  g rhF h{
devour-Prog
?
?A bird is devouring grain.?
A conventional parser has no cue for the disam-
biguation of instrumental case marker se in exam-
ples (6a-e) and similarly, in example (7), it?s hard
for the parser to know whether ?bird? or ?grain? is
the agent of the action ?devour?. Traditionally, syn-
tactic parsing has largely been limited to the use
of only a few lexical features. Features like POS-
tags are way too coarser to provide deep informa-
tion valuable for syntactic parsing while on the other
hand lexical items often suffer from lexical ambi-
guity or out of vocabulary problem. So in oder to
assist the parser for better judgments, we need to
complement the morphology somehow. A careful
observation easily states that a simple world knowl-
edge about the nature (e.g. living-nonliving, arti-
fact, place) of the participants is enough to disam-
biguate. For Swedish, ?vrelid and Nivre (2007) and
?vrelid (2009) have shown improvement, with an-
imacy information, in differentiation of core argu-
ments of a verb in dependency parsing. Similarly
for Hindi, Bharati et al (2008) and Ambati et al
(2009) have shown that even when the training data
is small simple animacy information can boost de-
pendency parsing accuracies, particularly handling
the differentiation of core arguments. In Table 5,
we show the distribution of animacy with respect to
case markers and dependency relations in the anno-
tated portion of the Hindi Treebank. The high rate
of co-occurrence between animacy and dependency
relations makes a clear statement about the role an-
imacy can play in parsing. Nominals marked with
dependency relations as k1 ?agent?, k4 ?recipient?,
k4a ?experiencer? are largely annotated as human
while k3 ?instrument? is marked as inanimate,
which confirms our conjecture that with animacy
information a parser can reliably predict linguistic
patterns. Apart from parsing, animacy has been re-
ported to be beneficial for a number of natural lan-
guage applications (Evans and Orasan, 2000; Merlo
and Stevenson, 2001). Following these computa-
tional implications of animacy, we started encoded
this property of nominals explicitly in our treebank.
In the next section, we will present these efforts fol-
162
lowed by the inter-annotator agreement studies.
Human Other-Animates Inanimate
k1
n/ne (Erg) 2321 630 108
ko/ko (Dat/Acc) 172 8 135
s/se (Inst) 6 0 14
m\/me (Loc) 0 0 7
pr/par (Loc) 0 0 1
kA/kaa (Gen) 135 2 99
? (Nom) 1052 5 3072
k2
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 625 200 226
s/se (Inst) 67 0 88
m\/me (Loc) 2 0 6
pr/par (Loc) 5 0 37
kA/kaa (Gen) 15 0 14
? (Nom) 107 61 2998
k3
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 2 0 199
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 20
k4
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 597 0 13
s/se (Inst) 53 0 56
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 7 0 8
k4a
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 132 0 8
s/se (Inst) 4 0 2
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 1 0 0
? (Nom) 56 0 1
k5
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 7 0 460
m\/me (Loc) 0 0 1
pr/par (Loc) 0 0 0
kA/kaa (Gen) 0 0 0
? (Nom) 0 0 2
k7
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 4 0 0
s/se (Inst) 3 0 129
m\/me (Loc) 0 1977 1563
pr/par (Loc) 66 0 1083
kA/kaa (Gen) 0 0 8
? (Nom) 5 0 1775
r6
n/ne (Erg) 0 0 0
ko/ko (Dat/Acc) 0 0 0
s/se (Inst) 1 0 0
m\/me (Loc) 0 0 0
pr/par (Loc) 0 0 0
kA/kaa (Gen) 156 80 605
? (Nom) 13 3 25
Table 5: Distribution of semantic features with respect
to case markers and dependency relations a.
ak1 ?agent?, k2 ?patient?, k3 ?instrument?, k4 ?recipient?,
k4a ?experiencer?, k5 ?source?, k7 ?location?, r6 ?possession?
4 Animacy Annotation
Following Zaenen et al (2004), we make a three-
way distinction, distinguishing between Human,
Other Animate and In-animate referents of a
nominal in a given context. The animacy of a ref-
erent is decided based on its sentience and/or con-
trol/volitionality in a particular context. Since, pro-
totypically, agents tend to be animate and patients
tend to be inanimate (Comrie, 1989), higher ani-
mates such as humans, dogs etc. are annotated as
such in all contexts since they frequently tend to be
seen in contexts of high control. However, lower
animates such as insects, plants etc. are anno-
tated as ?In-animate? because they are ascribed
less or no control in human languages like inan-
imates (Kittila? et al, 2011). Non-sentient refer-
ents, except intelligent machines and vehicles, are
annotated as ?In-animate? in all contexts. Intel-
ligent machines like robots and vehicles, although,
lack any sentience, they possess an animal like be-
havior which separates them from inanimate nouns
with no animal resemblance, reflected in human lan-
guage as control/volitionality. These nouns unlike
humans and other higher animates are annotated as
per the context they are used in. They are anno-
tated as ?Other animate? only in their agentive
roles. Nominals that vary in sentience in varying
contexts are annotated based on their reference in a
given context as discussed in Subsection 4.2. These
nominals include country names referring to geo-
graphical places, teams playing for the country, gov-
ernments or their inhabitants; and organizations in-
cluding courts, colleges, schools, banks etc. Un-
like Zaenen et al (2004) we don?t further categorize
?Other Animate? and ?In-animate? classes. We
163
don?t distinguish between Organizations and Ani-
mals in ?Other Animate? and Time and Place in
?In-animates?.
The process of animacy annotation in the Hindi
Treebank is straight forward. For every chunk in a
sentence, the animacy of its head word is captured
in an ?attribute-value? pair in SSF format, as
shown in Figure 3. Hitherto, around 6485 sentence,
of the Hindi Treebank, have been annotated with
the animacy information.
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k1:VGF?
semprop=?human?>
1.1 mohn NNP <fs af=?mohn,n,m,sg,3,d,0,0?>
1.2 n PSP <fs af=?n,psp,,,,,,? name=?n?>
))
2 (( NP <fs name=?NP2? drel=?k4:VGF?
semprop=?other-animate?>
2.1 Eb?F NN <fs af=?Eb?F,n,f,sg,3,d,0,0?>
2.2 ko PSP <fs af=?ko,psp,,,,,,? name=?ko?>
))
3 (( NP <fs name=?NP3? drel=?k3:VGF?
semprop=?inanimate?>
3.1 botal NN <fs af=?botal ,n,f,sg,3,d,0,0?>
3.2 s PSP <fs af=?s,psp,,,,,,?>
))
4 (( NP <fs name=?NP4? drel=?k2:VGF?
semprop=?inanimate?>
4.1 d D NN <fs af=?d D,n,m,sg,3,d,0,0?>
))
5 (( VGF <fs name=?VGF?>
5.1 EplAyA VM <fs af=?EplA,v,m,sg,any,,yA,yA?>
))
</Sentence>
Figure 3: Semantic Annotation in SSF.
(8) mohn n
Mohan-Erg
Eb?F ko
cat-Dat
botal s
bottle-Inst
d D
milk-Nom
EplAyA
drink-Perf
?
?Mohan fed milk to the cat with a bottle.?
In the following, we discuss some of the interest-
ing cross linguistic phenomena which added some
challenge to the annotation.
4.1 Personification
Personification is a type of meaning extension
whereby an entity (usually non-human) is given
human qualities. Personified expressions are an-
notated, in our annotation procedure, as Human,
since it is the sense they carry in such contexts.
However, to retain their literal sense, two attributes
are added. One for their context bound sense
(metaphorical) and the other for context free sense
(literal). In example (9), waves is annotated with
literal animacy as In-animante and metaphoric
animacy as Human, as shown in Figure 4 (offset
2).
<Sentence id = ?1?>
Offset Token Tag Feature structure
1 (( NP <fs name=?NP? drel=?k7p:VGF? >
1.1 sAgr NNC <fs af=?sAgr,n,m,sg,3,d,0,0?>
1.2 taV NN <fs af=?taV,n,m,sg,3,d,0,0?>
1.3 pr PSP <fs af=?pr,psp,,,,,,?>
))
2 (( NP <fs name=?NP2? drel=?k1:VGF?
semprop=?inanimate?
metaphoric=?human?>
2.1 lhr\ NN <fs af=?lhr\,n,f,pl,3,d,0,0?>
))
3 (( VGF <fs name=?VGF?>
3.1 nAc VM <fs af=?nAc,v,any,any,any,,0,0?>
3.2 rhF VAUX <fs af=?rhF,v,f,sg,any,,ya,ya?>
3.3 h{\ AUX <sf AF=?h{\,v,any,pl,1,,he,he?>
))
</Sentence>
Figure 4: Semantic Annotation in SSF.
(9) sAgr taV pr
sea coast-Loc
lhr\
waves-Nom
nAc rhF h{\
dance-Prog
?
?Waves are dancing on the sea shore.?
4.2 Complex Types
The Hindi Treebank in largely built on newspa-
per corpus. Logically polysemous expressions
(metonymies) such as government, court,
newspaper etc. are very frequent in news re-
porting. These polysemous nominals can exhibit
contradictory semantics in different contexts. In
example (10a), court refers to a person (judge) or
a group of persons (jury) while in (10b) it is a
building (see Pustejovsky (1996) for the semantics
of complex types). In our annotation procedure,
such expressions are annotated as per the sense or
reference they carry in a given context. So, in case
of (10a) court will be annotated as Human while
in (10b) it will be annotated as In-animante.
(10a) adAlta n
court-Erg
m  kdm kA
case-Gen
P{\slA
decision-Nom
s  nAyA
declare-Perf
?
?The court declared its decision on the case.?
164
(10b) m{\
I-Nom
adAlta m\
court-Loc
h ?
be-Prs
?I am in the court.?
4.3 Inter-Annotator Agreement
We measured the inter-annotator agreement on a
set of 358 nominals (?50 sentences) using Cohen?s
kappa. We had three annotators annotating the same
data set separately. The nominals were annotated
in context i.e., the annotation was carried consider-
ing the role and reference of a nominal in a partic-
ular sentence. The kappa statistics, as presented in
Table 6, show a significant understanding of anno-
tators of the property of animacy. In Table 7, we
report the confusion between the annotators on the
three animacy categories. The confusion is high for
?Inanimate? class. Annotators don?t agree on this
category because of its fuzziness. As discussed ear-
lier, although ?Inanimate? class enlists biologically
inanimate entities, some entities may behave like an-
imates in some contexts. They may be sentient and
have high linguistic control in some contexts. The
difficulty in deciphering the exact nature of the ref-
erence of these nominals, as observed, is the reason
behind the confusion. The confusion is observed for
nouns like organization names, lower animates and
vehicles. Apart from the linguistically and contextu-
ally defined animacy, there was no confusion, as ex-
pected, in the understanding of biological animacy.
Annotators ?
ann1-ann2 0.78
ann1-ann3 0.82
ann2-ann3 0.83
Average ? 0.811
Table 6: Kappa Statistics
Human Other-animate Inanimate
Human 71 0 14
Other-animate 0 9 5
Inanimate 8 10 241
Table 7: Confusion Matrix
5 Conclusion and Future Work
In this work, we have presented our efforts to enrich
the nominals in the Hindi Treebank with animacy
information. The annotation was followed by the
inter-annotator agreement study for evaluating the
confusion over the categories chosen for annotation.
The annotators have a significant understanding of
the property of animacy as shown by the higher val-
ues of Kappa (?). In future, we plan to continue the
animacy annotation for the whole Hindi Treebank.
We also plan to utilize the annotated data to build
a data driven automatic animacy classifier (?vrelid,
2006). From a linguistic perspective, an annotation
of the type, as discussed in this paper, will also be of
great interest for studying information dynamics and
see how semantics interacts with syntax in Hindi.
6 Acknowledgments
The work reported in this paper is supported by the
NSF grant (Award Number: CNS 0751202; CFDA
Number: 47.070). 2
References
Judith Aissen. 2003. Differential object marking:
Iconicity vs. economy. Natural Language & Linguis-
tic Theory, 21(3):435?483.
B.R. Ambati, P. Gade, S. Husain, and GSK Chaitanya.
2009. Effect of minimal semantics on dependency
parsing. In Proceedings of the Student Research Work-
shop.
B.R. Ambati, S. Husain, J. Nivre, and R. Sangal. 2010.
On the role of morphosyntactic features in Hindi de-
pendency parsing. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 94?102. As-
sociation for Computational Linguistics.
R. Begum, S. Husain, A. Dhwaj, D.M. Sharma, L. Bai,
and R. Sangal. 2008. Dependency annotation scheme
for Indian languages. In Proceedings of IJCNLP. Cite-
seer.
Akshar Bharati, Samar Husain, Bharat Ambati, Sambhav
Jain, Dipti Sharma, and Rajeev Sangal. 2008. Two se-
mantic features make all the difference in parsing ac-
curacy. Proceedings of ICON, 8.
2Any opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the author(s) and do
not necessarily reflect the views of the National Science Foun-
dation.
165
A. Bharati, D.M. Sharma, S. Husain, L. Bai, R. Begum,
and R. Sangal. 2009. AnnCorra: TreeBanks for Indian
Languages Guidelines for Annotating Hindi TreeBank
(version?2.0).
R.A. Bhat, S. Jain, and D.M. Sharma. 2012. Experi-
ments on Dependency Parsing of Urdu. In Proceed-
ings of TLT11 2012 Lisbon Portugal, pages 31?36.
Edic?es Colibri.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for hindi/urdu. In Pro-
ceedings of the Third Linguistic Annotation Workshop,
pages 186?189. Association for Computational Lin-
guistics.
Joan Bresnan, Jean Carletta, Richard Crouch, Malvina
Nissim, Mark Steedman, Tom Wasow, and Annie Za-
enen. 2002. Paraphrase analysis for improved genera-
tion, link project.
Miriam Butt. 2006. The dative-ergative connection. Em-
pirical issues in syntax and semantics, 6:69?92.
N. Chomsky. 1981. Lectures on Government and Bind-
ing. Dordrecht: Foris.
Bernard Comrie. 1989. Language universals and lin-
guistic typology: Syntax and morphology. University
of Chicago press.
Praveen Dakwale, Himanshu Sharma, and Dipti M
Sharma. 2012. Anaphora Annotation in Hindi Depen-
dency TreeBank. In Proceedings of the 26th Pacific
Asia Conference on Language, Information, and Com-
putation, pages 391?400, Bali,Indonesia, November.
Faculty of Computer Science, Universitas Indonesia.
Felice Dell?Orletta, Alessandro Lenci, Simonetta Mon-
temagni, and Vito Pirrelli. 2005. Climbing the
path to grammar: A maximum entropy model of sub-
ject/object learning. In Proceedings of the Workshop
on Psychocomputational Models of Human Language
Acquisition, pages 72?81. Association for Computa-
tional Linguistics.
R.M.W. Dixon. 1994. Ergativity. Number 69. Cam-
bridge University Press.
Richard Evans and Constantin Orasan. 2000. Improv-
ing anaphora resolution by identifying animate entities
in texts. In Proceedings of the Discourse Anaphora
and Reference Resolution Conference (DAARC2000),
pages 154?162.
Christiane Fellbaum. 2010. WordNet. Springer.
A. Hautli, S. Sulger, and M. Butt. 2012. Adding an an-
notation layer to the Hindi/Urdu treebank. Linguistic
Issues in Language Technology, 7(1).
Paul J Hopper and Sandra A Thompson. 1980. Tran-
sitivity in grammar and discourse. Language, pages
251?299.
Seppo Kittila?, Katja Va?sti, and Jussi Ylikoski. 2011.
Case, Animacy and Semantic Roles, volume 99. John
Benjamins Publishing.
A.K. Mahajan. 1990. The A/A-bar distinction and move-
ment theory. Ph.D. thesis, Massachusetts Institute of
Technology.
Paola Merlo and Suzanne Stevenson. 2001. Auto-
matic verb classification based on statistical distribu-
tions of argument structure. Computational Linguis-
tics, 27(3):373?408.
Dipak Narayan, Debasri Chakrabarty, Prabhakar Pande,
and Pushpak Bhattacharyya. 2002. An experience in
building the indo wordnet-a wordnet for hindi. In First
International Conference on Global WordNet, Mysore,
India.
Lilja ?vrelid and Joakim Nivre. 2007. When word or-
der and part-of-speech tags are not enough ? Swedish
dependency parsing with rich linguistic features. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
pages 447?451.
Lilja ?vrelid. 2006. Towards robust animacy classifica-
tion using morphosyntactic distributional features. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics: Student Research Workshop, pages 47?
54. Association for Computational Linguistics.
Lilja ?vrelid. 2009. Empirical evaluations of animacy
annotation. In Proceedings of the 12th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL).
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. volume 31, pages 71?106. MIT Press.
M. Palmer, R. Bhatt, B. Narasimhan, O. Rambow, D.M.
Sharma, and F. Xia. 2009. Hindi Syntax: Annotat-
ing Dependency, Lexical Predicate-Argument Struc-
ture, and Phrase Structure. In The 7th International
Conference on Natural Language Processing, pages
14?17.
J. Pustejovsky. 1996. The Semantics of Complex Types.
Lingua.
Dipti Misra Sharma, Prashanth Mannem, Joseph van-
Genabith, Sobha Lalitha Devi, Radhika Mamidi, and
Ranjani Parthasarathi, editors. 2012. Proceedings of
the Workshop on Machine Translation and Parsing in
Indian Languages. The COLING 2012 Organizing
Committee, Mumbai, India, December.
Michael Silverstein. 1986. Hierarchy of features and
ergativity. Features and projections, pages 163?232.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
166
Juliette Thuilier, Laurence Danlos, et al 2012. Seman-
tic annotation of French corpora: animacy and verb
semantic classes. In LREC 2012-The eighth interna-
tional conference on Language Resources and Evalu-
ation.
Annie Zaenen, Jean Carletta, Gregory Garretson, Joan
Bresnan, Andrew Koontz-Garboden, Tatiana Nikitina,
M Catherine O?Connor, and Tom Wasow. 2004. Ani-
macy Encoding in English: why and how. In Proceed-
ings of the 2004 ACL Workshop on Discourse Anno-
tation, pages 118?125. Association for Computational
Linguistics.
167
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 15?21,
Dublin, Ireland, August 23, 2014.
A Two-Stage Approach for Computing Associative Responses to a Set of
Stimulus Words
Urmi Ghosh, Sambhav Jain and Soma Paul
Language Technologies Research Center
IIIT-Hyderabad, India
{urmi.ghosh, sambhav.jain}@research.iiit.ac.in,
soma@iiit.ac.in
Abstract
This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task
on multiword association. The task involves generating a ranked list of responses to a set of
stimulus words. The two-stage approach combines the strength of neural network based word
embeddings and frequency based association measures. The system achieves an accuracy of
34.9% over the test set.
1 Introduction
Research in psychology gives evidence that word associations reveal the respondents? perception, learn-
ing and verbal memories and thus determine language production. Hence, it is possible to simulate
human derived word associations by analyzing the statistical distribution of words in a corpus. Church
and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by
utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that
corpus-based computations of word associations are similar to association norms collected from human
subjects.
The CogALex-2014 shared task on multi-word association involves generating a ranked list of re-
sponse words for a given set of stimulus words. For example, the stimulus word bank can invoke as-
sociative responses such as river, loan, finance and money. Priming
1
bank with bed and bridge, results
in strengthening association with the word river and it emerges as the best response amongst the afore-
mentioned response choices. This task is motivated by the tip-of-the-tongue problem, where associated
concepts from the memory can help recall the target word. Other practical applications include query ex-
pansion for information retrieval and natural language generation where missing words can be predicted
from their context.
The participating systems are distinguished into two categories - Unrestricted systems that allows
usage of any kind of data and Restricted systems that can only make use of the ukWaC (Baroni et al.,
2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since
we only used ukWaC for extracting information on word associations. It follows a two-staged approach:
Candidate Response Generation, which involves selection of words that are semantically similar to the
primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted
Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived
from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%.
When ignoring the inflectional variations of the response word, an accuracy of 39.55% was achieved.
2 Observations on Training Data
The training set consists of 2000 sets of five words (multiword stimuli or primes) and the word that is
most closely associated to all of them (associative response). For example, a set of primes such as wheel,
driver, bus, drive and lorry are given along with the expected associative response - car.
In this section, our initial observations on the given training data are enlisted.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The phenomenon of providing multiple stimulus words is called priming.
15
2.1 Relation between the Associative Response and the Prime Words
It is observed that a response largely exhibits two kind of relations with a priming word.
Primes Associative Response
presents, Christmas, birthday, shops, present gifts
butterfly, light, ball, fly, insect moth
mouse, cat, catcher, race, tail rat
Table 1: Some examples of primes and their associative responses from the training set
Type A relation depicts a synonymous/antonymous behavior or ?of the same kind? nature. Word pairs
with paradigmatic relation are highly semantically related and belong to the same part of speech. And
hence, they tend to show a substitutive nature amongst themselves without affecting the grammar of the
sentence. From Table - 1, we observe that present/presents , butterfly/insect and mouse/cat can be substi-
tuted in place of gifts, moth and rat respectively. Type B relation depicts contextual co-occurrence, where
the words tend to occur together or form a collocation. This kind of relationship can be demonstrated by
taking examples from Table - 1, such as Christmas gifts, gift shops, birthday gifts, moth ball, rat catcher,
rat race and rat tail. In theory, the above have been formally categorized as paradigmatic (Type A) and
syntagmatic (Type B ) relations by De Saussure et al. (1916) and we will be referring to them accordingly
in rest of the paper.
Type C relation, depicting associations based on the phonological component of the words was also
observed. According to McCarthy (1990), responses can be affected by phonological shapes and or-
thographic patterns especially when instantaneous paradigmatic or syntagmatic association is difficult.
Examples from the training data set include ajar-Ajax, hypothalamus-hippopotamus and cravat-caravan.
Such examples were very few and hence, have not been dealt with in this paper.
2.2 Context Window Size
Words exhibiting syntagmatic associations often occur in close proximity in the corpus. We tested this
phenomenon on 500 randomly chosen sets of primes by calculating the distance of each prime from the
associative responses in the corpus. Figure - 1 testifies that a majority of primes occur within a context
window size of ?2 from the associative response.
1 2 3 4 5 6 7 8 9 10
0
500
1,000
1,500
2,000
d
f
Figure 1: Co-occurrence frequency f of an association at distance d from the response, averaged over the
2500 stimulus word and response word pairs from randomly chosen 500 training datasets
Next, a mechanism to interpret the above associations in a quantitative manner is required.
16
3 Word Representation
In order to have a quantitative comparison of association, first we need a representation for words in
a context. Traditionally co-occurrence vectors serve as a simple mechanism for such a representation.
However, such vectors are unable to effectively capture deeper semantics of words and also tend to suffer
from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been
made to represent word vectors in a lower dimensional space. Largely, these can be categorized into:
1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive
a vector based representation for each cluster, where semantically similar clusters are closer in
distance.
2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics.
Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls
in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation
of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors
over the probabilistic version of LSA (Hofmann, 1999).
3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector
corresponding to a word which effectively signifies its position in the semantic space. There has
been different suggestions on the nature of the neural-net and how the context needs to be fed to
the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008),
Turian et al. (2010) and Mikolov et al. (2013a).
4 Methodology
Our system follows a two-staged approach, where we first generate response candidates which are seman-
tically similar to prime words, followed by a re-ranking step where we give weightage to the responses
likely to occur in proximity.
4.1 Candidate Response Generation
The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word
embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by
the fact that this approach models semantic similarity and outperforms other approaches in terms of
accuracy as well as computational efficiency(Mikolov et al., 2013a; Mikolov et al., 2013c).
The word2vec
2
utility is used to learn this model and thereby create 300-dimensional word embed-
dings. word2vec implements two classification networks - the Skip-gram architecture and the Continuous
Bag-of-words (CBOW) architecture. We applied CBOW architecture as it works better on large corpora
and is significantly faster than Skip-gram(Mikolov et al., 2013b). The CBOW architecture predicts the
current word based on its context. The architecture employs a feed forward neural network, which con-
sists of:
1. An input layer, where the context words are fed to the network.
2. A projection layer, which projects words onto continuous space and reduces number of parameters
that are needed to be estimated.
3. An output layer.
This log-linear classifier learns to predict words based on its neighbors in a window of ?5. We also
applied a minimum word count of 25 so that infrequent words are filtered out.
With the vector representation available, a response r to a set of primes S, is searched in the vocabulary
by measuring its cosine similarity with each prime x
i
in S. The overall similarity of the response r, with
the prime word set S, is defined as the average of these similarities.
2
word2vec : https://code.google.com/p/word2vec/
17
sim(r, S) =
1
|S|
?
|S|
?
i=1
x
i
.r
|x
i
|.|r|
Using the best similarity score as the selection criterion for response, the approach resulted in an
accuracy of 20.8% over the test set. Error analysis revealed that the above approach is biased towards
finding a paradigmatic candidate. However, it is further observed that much of the correct answers
(> 80%) exist in a k-best(k=500) list but with a relatively lower similarity score. This confirmed that our
broader selection is correct but a better re-ranking approach is required.
4.2 Re-ranking by Association Measures
To give due weightage to responses with high syntagmatic associativity, we utilize word co-occurrences
from the corpus. Since we are dealing with semantically related candidates, applying even a basic lexical
association measure like Pointwise Mutual Information (PMI) (Church and Hanks, 1990) tend to improve
the results.
PMI
For each prime word, we calculate co-occurrence frequency information for its neighbors within a win-
dow of ?2 as mentioned in Section 2. Also, a threshold of 3 is set to the observed frequency measures
as PMI tends to give very high association score to infrequent words.
For each candidate response r, we calculate its PMI
i
with each of the primes (x
i
) in the set S. The
total association score Score
PMI
for a candidate is defined as the average of the individual measures.
PMI
i
=
p(x
i
r)
p(x
i
)p(r)
Score
PMI
=
1
|S|
?
?
i?S
PMI
i
Ranking the candidates based on PMI improved the results to 30.45%
Weighted PMI
It should be duly noted that only some primes exhibit a syntagmatic relation with the response, while
the rest exhibit a paradigmatic relation. For example, the expected response for primes Avenue, column,
dimension, sixth, fourth is fifth. The first three words share a syntagmatic relation with the response
while the last two words share a paradigmatic relation with the response. As PMI deals with word
co-occurrences, ideally, only primes exhibiting syntagmatic associations should be considered for re-
ranking. However, a clear distinction between the two categories of primes is a difficult task as the target
response is unknown.
In order to take effective contribution of each prime, we propose a weighed extension of PMI which
gives more weightage to syntagmatic primes as to the paradigmatic ones. Since, primes sharing a
paradigmatic relation with the response word are highly semantically related, they are expected to be
closer in the semantic space too. On the other hand, the primes showcasing syntagmatic relations are
expected to be distant.
Using the vector representation described in Section 4.1, we calculate an average vector of the five
primes, p
avg
, and compute its cosine distance from individual primes. The cosine distance thus obtained
is used as the weight w for the PMI associativity of a prime. In a nutshell, larger the distance of a
prime from p
avg
, the greater is its contribution in the PMI based re-ranking score. This ranking schema
assumes that the prime set consists of at least two words demonstrating paradigmatic relation with the
target response. Table - 2 displays the primes along with their distance from p
avg
.
Score
wPMI
=
1
|S|
?
?
i?S
w
i
PMI
i
Next, a ranked list of candidate responses for each set is generated by sorting the previously ranked
list according to the new score. The new ranking scheme based on weighted PMI (wPMI) improves the
results to 34.9%. Table -3 displays some sets which show improvement upon implementing the wPMI
18
Primes Cosine Distance
Avenue 0.612
column 0.422
dimension 0.390
sixth 0.270
fourth 0.212
Table 2: An example demonstrating Cosine Distance between the primes and the p
avg
of the prime set
ranking scheme. Taking a case from Table - 3, we observe that the correct response skeleton is generated
for primes cupboard, body, skull, bone and bones when ranked according to the wPMI scheme. This is
due to larger weights being assigned to primes cupboard and body which have a closer proximity to the
word skeleton than the word vertebral which is generated by the simple PMI ranking scheme.
cupboard 0.615 pit 0.553 boat 0.499
Primes(with weights) body 0.410 band 0.549 sailing 0.476
skull 0.248 hand 0.426 drab 0.338
bone 0.244 limb 0.0.340 dark 0.318
bones 0.172 leg 0.270 dull 0.307
PMI vertebral amputated drizzly
wPMI skeleton arm dingy
Expected Response skeleton arm dingy
Table 3: Comparison between results from PMI and wPMI re-ranking approaches
5 Results and Evaluation
The system was evaluated on the test set derived from the Edinburgh Associative Thesaurus (EAT) which
lists the associations to thousands of English stimulus words as collected from native speakers. For
example, for the stimulus word visual the top associations are aid, eyes, aids, see, eye, seen and sight.
For the shared task, top five associations for 2000 randomly selected stimulus words were provided as
prime sets and the system was evaluated based on its ability to predict the corresponding stimulus word
for each set. Table - 4 displays the top ten responses generated by our system for some prime sets and
their corresponding stimulus word.
Primes
knight, plate, soldier,
protection, sword
ants, flies, fly,
bees, bite
babies, baby, rash,
wet, washing
butterfly, moth, caterpillar,
cocoon, insect
Top 10
Responses
armor
armour
helmet
shield
guard
bulletproof
guards
warrior
enemy
gallant
mosquitoes
wasps
beetles
insects
spiders
sting
moths
butterflies
arachnids
bedbugs
nappy
shaving
nappies
clothes
skin
bathing
dry
eczema
bedding
dirty
larva
larvae
pupa
species
pests
beetle
silkworm
wings
pupate
pollinated
Target armour insects nappies chrysalis
Table 4: Top ten responses for some prime sets and their corresponding target response
19
As we have considered exact string match(ignoring capitalization), the evaluation does not account
for spelling variations. For example, the response output armor instead of the expected response armour
results in counting it as incorrect.
We achieved an accuracy of 34.9% by considering the top response for each list of ranked responses.
However, it was observed that the correct response was present within the top ten responses in 59.8%
of the cases. For example, the primes ants, flies, fly, bees, bite generate the response output mosquitoes.
The expected output insects ranks 4
th
in our list of responses.
For primes babies, baby, rash, wet, washing, our system outputs nappy while the expected response is
nappies. Such inflected forms of the responses are challenging to predict and hence, another evaluation
is presented which ignores the inflectional variation of the response word. Under this evaluation, we
achieved an accuracy of 39.55% for the best response and 63.15% if the expected response occurs in the
top ten responses. Table - 5 displays accuracy of our system when the target response lies within the
top-n responses for both evaluation methods.
Exact Match Ignoring Inflections
n=1 34.9 39.55
n=3 48.15 49.65
n=5 53.2 55.45
n=10 59.8 63.15
Table 5: Evaluation results in %
6 Conclusion
There exist some word associations that are asymmetric in nature. Rapp (2013) observed that the primary
response of a given stimulus word may have stronger association with another word and need not gen-
erate the stimulus word back. For example, the strongest association to bitter is sweet but the strongest
association to sweet is sour. Therefore, the EAT data set chosen for evaluation, may not be the best judge
for certain cases. Taking a case from our test data, for primes butterfly, moth, caterpillar, cocoon, insect,
our system outputs larva instead of the original stimulus word chrysalis which does not feature even in
the top ten responses (Refer Table - 4).
In this work, we proposed a system to generate a ranked list of responses for multiple stimulus words.
Candidate responses were generated by computing its semantic similarity with the stimulus words and
then re-ranked using a lexical association measure, PMI. This system scored 34.9% when the top ranked
response was considered and 59.8% when the top ten responses were taken into account. When ignoring
inflectional variations, the accuracy improved to 39.55% and 63.15% for the two evaluation methods
respectively.
In future, a more sophisticated re-ranking approach in place of PMI measure can be used such as
product-of-rank algorithm (Rapp, 2008). Since, the re-ranking methodologies discussed by far, take
into account word co-occurrences, it is biased towards syntagmatic responses. A better trade-off can be
worked out to give due weightage to paradigmatic responses too.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209?
226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
20
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22?29, March.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pages 160?167. ACM.
Ferdinand De Saussure, Charles Bally, Albert Sechehaye, and Albert Riedlinger. 1916. Cours de linguistique
g?en?erale: Publi?e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire
Payot & Cie.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCI-
ENCE, 41(6):391?407.
Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence, pages 289?296. Morgan Kaufmann Publishers Inc.
George R. Kiss, Christine A. Armstrong, and Robert Milroy. 1972. An associative thesaurus of English. Medical
Research Council, Speech and Communication Unit, University of Edinburgh, Scotland.
Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211?240.
Michael McCarthy. 1990. Vocabulary. Oxford University Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word
representations. Proceedings of NAACL-HLT, pages 746?751.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages
1081?1088.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the
workshop on Cognitive Aspects of the Lexicon, pages 102?109. Association for Computational Linguistics.
Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive
Science, page 78.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Manfred Wettler and Reinhard Rapp. 1989. A connectionist system to simulate lexical decisions in information
retrieval. Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.), Connectionism in perspective. Amsterdam:
Elsevier, 463:469.
Manfred Wettler and Reinhard Rapp. 1993. Computation of word associations based on the co-occurrences of
words in large corpora.
21
