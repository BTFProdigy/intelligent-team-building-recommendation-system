An Automatic Evaluation Method for Localization Oriented 
Lexicalised EBMT System 
 
Jianmin Yao+, Ming Zhou++, Tiejun Zhao+, Hao Yu+, Sheng Li+ 
  +School of Computer Science and Technology
Harbin Institute of Technology,  
Harbin, China, 150001 
{james, tjzhao, yu, shengli}@mtlab.hit.edu.cn
++Natural Language Computing Group 
Microsoft Research Asia 
Beijing, China, 100080 
Mingzhou@microsoft.com 
 
Abstract  
To help developing a localization oriented 
EBMT system, an automatic machine 
translation evaluation method is 
implemented which adopts edit distance, 
cosine correlation and Dice coefficient as 
criteria. Experiment shows that the 
evaluation method distinguishes well 
between ?good? translations and ?bad? ones. 
To prove that the method is consistent with 
human evaluation, 6 MT systems are scored 
and compared. Theoretical analysis is made 
to validate the experimental results. 
Correlation coefficient and significance tests 
at 0.01 level are made to ensure the 
reliability of the results. Linear regression 
equations are calculated to map the 
automatic scoring results to human scorings. 
Introduction 
Machine translation evaluation has always been 
a key and open problem. Various evaluation 
methods exist to answer either of the two 
questions (Bohan 2000): (1) How can you tell if 
a machine translation system is ?good?? And (2) 
How can you tell which of two machine 
translation systems is ?better?? Since manual 
evaluation is time consuming and inconsistent, 
automatic methods are broadly studied and 
implemented using different heuristics. Jones 
(2000) utilises linguistic information such as 
balance of parse trees, N-grams, semantic 
co-occurrence and so on as indicators of 
translation quality. Brew C (1994) compares 
human rankings and automatic measures to 
decide the translation quality, whose criteria 
involve word frequency, POS tagging 
distribution and other text features. Another type 
of evaluation method involves comparison of the 
translation result with human translations. 
Yokoyama (2001) proposed a two-way MT 
based evaluation method, which compares 
output Japanese sentences with the original 
Japanese sentence for the word identification, 
the correctness of the modification, the syntactic 
dependency and the parataxis. Yasuda (2001) 
evaluates the translation output by measuring the 
similarity between the translation output and 
translation answer candidates from a parallel 
corpus. Akiba (2001) uses multiple edit 
distances to automatically rank machine 
translation output by translation examples. 
Another path of machine translation evaluation 
is based on test suites. Yu (1993) designs a test 
suite consisting of sentences with various test 
points. Guessoum (2001) proposes a 
semi-automatic evaluation method of the 
grammatical coverage machine translation 
systems via a database of unfolded grammatical 
structures. Koh (2001) describes their test suite 
constructed on the basis of fine-grained 
classification of linguistic phenomena. 
There are many other valuable reports on 
automatic evaluation. All the evaluation 
methods show the wisdom of authors in their 
utilisation of available tools and resources for 
automatic evaluation tasks. For our 
localization-oriented lexicalised EBMT system 
an automatic evaluation module is implemented. 
Some string similarity criteria are taken as 
heuristics. Experimental results show that this 
method is useful in quality feedback in 
development of the EBMT system. Six machine 
translation systems are utilised to test the 
consistency between the automatic method and 
human evaluation. To avoid stochastic errors, 
significance test and linear correlation are 
calculated. Compared with previous works, ours 
is special in the following ways: 1) It is 
developed for localisation-oriented EBMT, 
which demands higher translation quality. 2) 
Statistical measures are introduced to verify the 
significance of the experiments. Linear 
regression provides a bridge over human and 
automatic scoring for systems. 
The paper is organised as follows: First the 
localization-oriented lexicalised EBMT system 
is introduced as the background of evaluation 
task. Second the automatic evaluation method is 
further described. Both theoretical and 
implementation of the evaluation method are 
fully discussed. Then six systems are evaluated 
both manually and with our automatic method. 
Consistency between the two methods is 
analysed. At last before the conclusion, linear 
correlation and significance test validate the 
result and exclude the possibility of random 
consistency. 
1 EBMT Evaluation Solution 
1.1 EBMT System Setup 
From Figure 1 you can get a general overview of 
our EBMT system. 
 
Input sentence 
Transfer 
<Phrase Alignment>
Translation result 
Resources 
(Bilingual and 
monolingual) 
Example Base 
(Software 
manual) Match 
Recombine 
Figure 1. Flowchart of the EBMT System 
The EBMT system is developed for 
localization purpose, which demands the 
translation to be restricted in style and 
expression. This makes it rational to take string 
similarity as criterion for translation quality 
evaluation. The solution is useful because in 
localization, an example based machine 
translation system helps only if it outputs the 
very high quality translation results. 
1.2 Evaluation Criteria 
The criteria we utilise for evaluation include edit 
distance, dice coefficient and cosine correlation 
between (the vectors or word bag sets of) the 
machine translation and the gold standard 
translation. Followed is a detailed description of 
the three criteria. 
The edit distance between two strings s1 
and s2, is defined as the minimum number of 
operations to become the same 
(Levenshtein1965). It gives an indication of how 
`close' or ?similar? two strings are. Denote the 
length of a sentence s as |s|. A two-dimensional 
matrix, m[0...|s1|,0...|s2|] is used to hold the edit 
distance values. The algorithm is as follows 
(Wagner 1974): 
Step 1 Initialization: 
For i=0 to |s1| 
m[i, 0] = i//initializing the columns  
For j=1 to |s2| 
m[0, j] = j //initializing the rows 
Step 2 Iteration: 
For i=1 to |s1| 
For j=1 to |s2| 
 if(s1[i] = s2[j]) 
 { 
    d=m[i-1,j-1] 
 }//equality 
 else 
 { 
d=m[i-1,j-1]+1 
 }//substring 
 m[i, j]=min(m[i-1,j]+1,m[i,j-1]+1,d) 
End For 
End For 
Step 3: Result: 
Return m[i,j] 
Figure 2. Algorithm for Edit Distance 
The time complexity of this algorithm is 
O(|s1|*|s2|). If s1 and s2 have a `similar' length, 
about `n' say, this complexity is O(n2). 
Taking into account the lengths of 
translations, the edit distance is normalised as 
21
)2,1(d2
tDistancenormal_edi  
ss
ss
+
?=
     (1) 
Cosine correlation between the vectors of 
two sentences is often used to compute the 
similarity in information retrieval between a 
document and a query (Manning 1999). In our 
task, it is a similarity criterion defined as 
follows: 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
    (2) 
Where 
w1i = weight of ith term in vector of sentence 
s1, 
w2i = weight of ith term in vector for sentence 
s2, 
n = number of words in sum vector of s1 and s2. 
The cosine correlation reaches maximum value 
of 1 when the two strings s1 and s2 are the same, 
while if none of the elements co-occurs in both 
vectors, the cosine value will reach its minimum 
of 0. 
Another criterion we utilised is the Dice 
coefficient of element sets of strings s1 and s2, 
21
21
2)2,1(
ss
ss
ssDice +?=
I
   (3) 
The Dice coefficient demonstrates the 
intuitive that good translation tends to have 
more common words with standard than bad 
ones. This is especially true for example based 
machine translation for localization purpose. 
1.3 Relationship Among Similarity Criteria 
In this section we analyse the relationship 
between the criteria so that we have a better 
understanding of the experiment results. 
If weight of all words are 1, i.e. each word has 
the uniform importance to translation quality, 
the cosine value becomes very similar to the 
Dice coefficient criterion. if we assume 
??
?=
                                                    else        0 
rsboth vectoin  occurs ith word  theiff     1 
bi
 
??
?=
                                                    else        0 
s1 ofin vector  occurs ith word  theiff     1 
1ib
 
??
?=
                                                    else        0 
s2 ofin vector  occurs ith word  theiff     1 
2ib
 
then 
?= ?=
?
?=
?
= n
1i
n
1i
2w2i2w1i
n
1i
2i)(w1i
s2)cos(s1,
w
 
?= ?=
?
?== n
i
n
i
ibib
n
i
bi
1 1
2221
1
21
21
1 1
21
1
ss
ss
n
i
n
i
ibib
n
i
bi
?=?= ?=
?
?== I
  
Similar to (3), this is also a calculation of the 
number of words in common The Dice 
coefficient and cosine function have common 
characteristics. Especially when two strings are 
of the same length, we have 
)2,1(
21
21
2
1
21
11
21
21
21
1 1
21
1)2,1cos(
                 ssDice
ss
ss
s
ss
ss
ss
ss
ss
n
i
n
i
ibib
n
i
bi
ss
=+?==
?
=
?
=
?= ?=
?
?==
II
II
 
The above equation holds if and only if |s1| 
== |s2|. The experimental results will clearly 
demonstrate the correspondence between cosine 
correlation and Dice coefficient. The two values 
become more similar as the lengths of the two 
strings draw nearer. They become the same 
when the two sentences are of the same length. 
The (normalized) edit distance evaluation 
has a somewhat different variance from the other 
two values. Edit distance cares not only how 
many words there are in common, but also takes 
into account the factor of word order adjustment. 
For example, take two strings of s1 and s2 
composed of words, 
s1 = w1 w2 w3 w4 
s2 = w1 w3 w2 w4 
Then, 
1
44
4221
21
2)2,1( =+?=+?= ss
ss
ssDice
I
 
1
44
4
1 1
2221
1)2,1cos( =
?
=
?
=
?
=
?
?
==
n
i
n
i
ibib
n
i
bi
ss
 
5.0
44
22
21
)2,1(d2 tDistancenormal_edi
2s2)ce(s1,editDistan
=+
?=+
?=
=
ss
ss
 
Edit distance and the other two criteria have 
their respective good aspects and shortcomings. 
So they can complement each other in the 
evaluation work.  
In the EBMT development, we sort the 
translations by a combination of the three factors, 
i.e. first by Dice coefficient in descending order, 
then by cosine correlation in descending order, 
last by normalized edit distance in ascending 
order. This method makes a simple combination 
of the three factors, while no more complexity 
arises from this combination. 
2 Experiments and Results 
2.1 Experimental Setup 
Our evaluation method is designed to help in 
developing the EBMT system. It is supposed to 
sort the translations by quality. Experiments 
show that it works well sorting the sentences by 
order of it?s being good or bad translations. In 
order to justify the effectiveness of the 
evaluation method, we also design experiments 
to compare the automatic evaluation with human 
evaluation. The result shows good compatibility 
between the automatic and human evaluation 
results. Followed are details of the experimental 
setup and results. 
In order to evaluate the performance of our 
EBMT system, a sample from a bilingual corpus 
of Microsoft Software Manual is taken as the 
standard test set. Denote the source sentences in 
the test set as set S, and the target T. Sentences 
in S are fed into the EBMT system. We denote 
the output translation set as R. Every sentence ti 
in T is compared with the corresponding 
sentence ri in R. Evaluation results are got via 
the functions cosine(ti, ri), Dice(ti, ri), and 
normalized edit distance normal_editDistance(ti, 
ri). As discussed in the previous section, good 
translations tend to have higher values of cosine 
correlation, Dice coefficient and lower edit 
distance. After sorting the translations by these 
values, we will see clearly which sentences are 
translated with high quality and which are not. 
Knowledge engineers can obtain much help 
finding the weakness of the EBMT system. 
Some sample sentences and evaluation 
results are attached in the Appendix. In our 
experience, with Dice as example, the 
translations scored above 0.7 are fairly good 
translations with only some minor faults; those 
between 0.5 and 0.7 are faulty ones with some 
good points; while those scored under 0.4 are 
usually very bad translations. From these 
examples, we can see that the three criteria 
really help sorting the good translation from 
those bad ones. This greatly aids the developers 
to find out the key faults in sentence types and 
grammar points. 
2.2 Comparison with Human Evaluation 
In the above descriptions, we have presented our 
theoretical analysis and experimental results of 
our string similarity based evaluation method. 
The evaluation has gained the following 
achievements: 1) It helps distinguishing ?good? 
translations from ?bad? ones in developing the 
EBMT system; 2) The scores give us a clear 
view of the quality of the translations in 
localization based EBMT. In this section we will 
make a direct comparison between human 
evaluation and our automatic machine 
evaluation to test the effectiveness of the string 
similarity evaluation method. To tackle this 
problem, we carry out another experiment, in 
which human scoring of systems are compared 
with the machine scoring. 
The human scoring is carried out with a test 
suite of High School English. Six undergraduate 
students are asked to score the translations 
independent from each other. The average of 
their scoring is taken as human scoring result. 
The method is similar to ALPAC scoring system. 
We score the translations with a 6-point scale 
system. The best translations are scored 1. If it?s 
not so perfect, with small errors, the translation 
gets a score of 0.8. If a fatal error occurs in the 
translation but it?s still understandable, a point 
of 0.6 is scored. The worst translation gets 0 
Table 1. Human Evaluation of 6 Machine Translation Systems 
System# #1 #2 #3 #4 #5 #6 
Error5 5 5% 1 1% 2 2% 4 4% 9 9% 7 7% 
Error4 4 4% 6 6% 4 4% 7 7% 18 18% 21 21%
Error3 7 7% 14 14% 21 21% 23 23% 23 23% 26 26%
Error2 14 14% 15 14% 21 21% 19 19% 18 18% 17 17%
Error1 15 14% 17 17% 33 32% 16 16% 15 15% 8 8% 
Perfect 57 56% 49 48% 21 21% 33 32% 19 19% 23 23%
Good% 70% 65% 43% 48% 34% 31% 
Score 81 78 69 68 55 54 
point of score. Table 1 shows the manual 
evaluation results for 6 general-purpose machine 
translation systems available to us. In table 1, 
Error5 means the worst translation. Error4 to 
Error1 are better when the numbering becomes 
smaller. A translation is labelled ?Perfect? when 
it?s a translation without any fault in it. 
?Good%? is the sum of percent of ?Error1? and 
?Perfect?. Because ?Error1? translations refer to 
those have small imperfections. ?Score? is the 
weighted sum of scores of the 6 kinds of 
translations. E.g. for machine translation system 
MTS1, the score is calculated as follows: 
811578.0156.014                         
4.072.0405)1(
=?+?+?
+?+?+?=MTSscore
 
In table 2, the human scorings and automatic 
scorings of the 6 machine translation systems are 
listed. The translations of system #1 are taken as 
standard for automatic evaluations, i.e. all 
scorings are made on the basis of the result of 
system #1. In principle this will introduce some 
errors, but we suppose it not so great as to 
invalidate the automatic evaluation result. This 
is also why the scorings of system #1 are 100. 
The last row labele AutoAver is the average of 
automatic evaluations. 
Table 2. Scoring of 6 MT Systems 
System# #1 #2 #3 #4 #5 #6 
Human 100 78 69 68 55 54 
Dice 100 70 57 65 48 56 
Cosine 100 75 64 72 55 63 
Edistance 100 78 69 75 63 68 
AutoAver 100 74 63 71 55 62 
Figure 3 presents the scorings of Dice 
coefficient, cosine correlation, edit distance and 
the average of the three automatic criterions in a 
chart, we can clearly see the consistency among 
these parameters. 
4 0
5 0
6 0
7 0
8 0
9 0
1 0 0
1 2 3 4 5 6
D i c e C o s i n e
E d i t D A u t o A v e r
 
Figure 3. Automatic Scoring of 6 MT Systems 
In Figure 3, the numbers on X-axis are the 
numbering of machine translation systems, 
while the Y-axis denotes the evaluation scores. 
40
50
60
70
80
90
100
1 2 3 4 5 6
Human Automatic
 
Figure 4. Scoring of 6 MT Systems 
The human and automatic average scoring 
is shown in Figure 4. The Automatic data refers 
to the average of Dice, cosine correlation and 
edit distance scorings. On the whole, human and 
automatic evaluations tend to present similar 
scores for a specific system, e.g. 78/74 for 
system #2, while 69/63 for system #3. 
3 Result Analysis 
The experimental results and the charts have 
shown some intuitionistic relationship among 
the automatic criteria of Dice coefficient, cosine 
value, edit distance and the human evaluation 
result. A more solid analysis is made in this 
section to verify this relationship. Statistical 
analysis is a useful tool to 1) find the 
relationship between data sets and 2) decide 
whether the relationship is significant enough or 
just for random errors.  
The measure of linear correlation is a way 
of assessing the degree to which a linear 
relationship between two variables is implied by 
observed data. The correlation coefficient 
between variable X and Y is defined as 
YXss
YXCOVYXr ),(),( =
   (7) 
where 
COV(X,Y) is the covariance defined by 
? ???= ))((11),( YYXXnYXCOV ii  (8) 
The symbol meanings are as follows: 
sX: sample standard deviation of variable X 
sY: sample standard deviation of variable Y 
n: sample size 
Xi (Yi) : the ith component of variable X (Y) 
X (Y ): the sample mean of variable X (Y) 
From its definition, we know that the correlation 
coefficient is scale-independent and 11 ??? r . 
After we get the correlation coefficient r, a 
significance test at the level 01.0=?  is made 
to verify whether the correlation is real or just 
due to random errors. Linear regression is used 
to construct a model that specifies the linear 
relationship between the variables X and Y. A 
scatter diagram and regression line will be 
presented for an intuitionistic view of the 
relationship. The results are presented in the 
graphs below. In the graphs, the human 
evaluation results are placed on the X axis, while 
the automatic results are on the Y axis. 
Correlation coefficient and the linear regression 
equation are shown below the graphs. Taking 
into the sample size and the correlation 
coefficient, the significance level is also 
calculated for the statistical analysis. 
 
Figure 5. Human (X) and AutoAver (Y) 
Y=8.0+0.89X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 6. Human (X) and Dice (Y) 
Y=6.9+1.03X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 7. Human (X) and Cosine (Y) 
Y=9.3+0.88X, P < 0.01 
r = 0.96, P < 0.01 
 
Figure 8. Human (X) and Edistance (Y) 
Y=23.3+0.74X, P < 0.01 
r = 0.95, P < 0.01 
It is a property of r that it has a value 
domain of [-1,+1]. A positive r implies that the 
X and Y tend to increase/decrease together. A 
minus r implies a tendency for Y to decrease as 
X increases and vice versa. When there is no 
particular relation between X and Y, r tends to 
have a value close to zero. From the above 
analysis, we can see that the Dice coefficient, 
cosine, and average of the automatic values are 
highly correlated with the human evaluation 
results with r=0.96. P < 0.01 shows the two 
variables are strongly correlated with a 
significance level beyond the 99%. While P < 
0.01 for the linear regression equation has the 
same meaning. 
Conclusion 
Our evaluation method is designed for the 
localization oriented EBMT system. This is why 
we take string similarity criteria as basis of the 
evaluation. In our approach, we take edit 
distance, dice coefficient and cosine correlation 
between the machine translation results and the 
standard translation as evaluation criteria. A 
theoretical analysis is first made so that we can 
know clearly the goodness and shortcomings of 
the three factors. The evaluation has been used 
in our development to distinguish bad 
translations from good ones. Significance test at 
0.01 level is made to ensure the reliability of the 
results. Linear regression and correlation 
coefficient are calculated to map the automatic 
scoring results to human scorings. 
Acknowledgements 
This work was done while the author visited 
Microsoft Research Asia. Our thanks go to Wei 
Wang, Jinxia Huang, and Professor Changning 
Huang at Microsoft Research Asia and Jing 
Zhang, Wujiu Huang at Harbin Institute of 
Technology. Their help has contributed much to 
this paper. 
References  
A. Guessoum, R. Zantout, Semi-Automatic 
Evaluation of the Grammatical Coverage of 
Machine Translation Systems, MT Summit? 
conference, Santiago de Compostela, 2001 
Brew C, Thompson H.S, Automatic Evaluation of 
Computer Generated Text: A Progress Report on 
the TextEval Project, Proceedings of the Human 
Language Technology Workshop, 108-113, 1994. 
Christopher D. Manning, Hinrich Schutze, 
Foundations of Statistical Natural Language 
Processing, the MIT Press, 1999, 530-572 
Douglas A. Jones, Gregory M. Rusk, 2000, Toward a 
Scoring Function for Quality-Driven Machine 
Translation, Proceedings of COLING-2000. 
Keiji Yasuda, Fumiaki Sugaya, etc, An Automatic 
Evaluation Method of Translation Quality Using 
Translation Answer Candidates Queried from a 
Parallel Corpus, MT Summit? conference, Santiago 
de Compostela, 2001 
Language and Machines. Computers in Translation 
and Linguistics, (ALPAC report, 1966). National 
Academy of Sciences, 1966 
Niamh Bohan, Elisabeth Breidt, Martin Volk, 2000, 
Evaluating Translation Quality as Input to Product 
Development, 2nd International Conference on 
Language Resources and Evaluation, Athens, 2000. 
Shoichi Yokoyama, Hideki Kashioka, etc., An 
Automatic Evaluation Method for Machine 
Translation using Two-way MT, 8th MT Summit 
conference, Santiago de Compostela, 2001 
Sungryong Koh, Jinee Maeng, etc, A Test Suite for 
Evaluation of English-to-Korean Machine 
Translation Systems, MT Summit? conference, 
Santiago de Compostela, 2001 
Shiwen Yu, Automatic Evaluation of Quality for 
Machine Translation Systems, Machine Translation, 
8: 117-126, 1993, Kluwer Academic Publishers, 
printed in the Netherlands. 
Wagner A.R.  and Fischer M., The string-to-stirng 
correction problem, Journal of the ACM, Vol. 21, 
No. 1, 168-173 
V.I. Levenshtein, Binary codes capable of correcting 
deletions, insertions and reversals. Doklady 
Akademii Nauk SSSR 163(4) 845-848, 1965 
Yasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita, 
Using Multiple Edit Distances to Automatically 
Rank Machine Translation Output, MT Summit? 
conference, Santiago de Compostela, 2001 
Appendix: Automatic Evaluation Results 
cosine      Dice  edistance* `  standard translation&EBMT translation 
0.27273     0.27273      44/6=7     ???????MAPI?? 
                   ????extendedmapi? 
0.43301     0.42857     28/6=4     ???????? 
                             ??mail?? 
0.53452     0.53333 30/7=4      ???????? 
                               ??role??? 
0.62994     0.625     32/4=8      ????????? 
                               ??????? 
0.7     0.7      80/16=5      ???????????????????? 
                               ???????????????????? 
0.72058     0.72      50/11=4      ???????????? 
                               ????????????? 
0.78335     0.78261 46/3=15      ???????????? 
                         ??????????? 
0.81786     0.81633 98/20=4      ?????????????????????????? 
                             ??????????????????????? 
0.8528     0.84211 76/12=6      ?????????????????????? 
                               ???????????????? 
0.86772     0.86486 37/2=18      ?????????? 
                               ????????: 
0.875      0.875  32/1=32   ???????? 
                               ???????? 
0.90889     0.90476 42/2=21      ??????????... 
                               ????????... 
 
*Notes: The data presented in ?edistance? is the reciprocal of the normalized edit distance: the numerator is |s1 + s2| in bytes ; the 
denominator is the edit distance in Chinese characters or English words. 
Statistics Based Hybrid Approach to Chinese Base Phrase Identification 
Tie-jun ZHAO, Mu-yun YANG~ Fang LIU, Jian-min YAO, Hao YU 
Department of Computer Science and Engineering, Harbin Institute of Technology 
{tjzaho, )may, flu.fang, james, yu} @mtlab.hit.edu.en 
ABSTRACT 
This paper extends the base noun 
phrase(BNP) identification i to a research 
on Chinese base phrase identification. ARer 
briefly introducing some basic concepts on 
Chinese base phrase, this paper presents a
statistics based hybrid model for identifying 
7 types of Chinese base phrases in view. 
Experiments how the efficiency of the 
proposed method in simplifying sentence 
structure. Significance of the research es in 
it provides a solid foundation for the 
Chinese parser. 
Keywords: Chinese base phrase 
identification, parsing, statistical model 
1 Introduction 
Decomposing syntactic analysis into 
several phases o as to decrease its difficulty 
is a new stream in NIP research. The 
successful POS tagging has encouraged 
researchers to explore further possibility for 
resolving sub-problems in parsing(Zhou, et 
al, 1999). The typical examples are the 
recognition of BaseNP in English and 
Chinese. 
In English BNP (base noun phrase) is 
defined as simple and non-nesting noun 
phrases, i.e. noun phrases that do not contain 
other noun phrase descendants (Church, 
1988). After that researches on BNP 
identification reports promising results for 
such task in English. Observing that the 
Chinese BNP is different form English, 
(Zhao & Huang, 1999) puts forward the 
definition of Chinese BNP in terms of 
combination of determinative modifier and 
head noun. According to them a BNP in 
Chinese can be recursively defined as: 
BaseNP ::= Determinative modifier + 
Noun I Nominalized verb(NIO 
Determinative modifier ::= Adjective I
Differentiable Adjective(DA) I Verb I Noun I 
Location I String l Numeral + Classifier 
Inspired by these researches, we extend 
the concept of BNP to Base Phrase in 
Chinese. It is based on such knowledge that 
there are many structures, not only NP, in 
which the trivial components closely attach 
to their central words and constitute a basic 
phrase in a Chinese sentence. Obviously, 
resolving all these base phrases will greatly 
benefit Chinese parser by reliving it from 
some pre-processing (though non-trivial) 
and enable it focus on the most subtle 
syntactic structures. 
Since the whole system of Chinese base 
phrase is still under discussing, this paper 
just presents some tentative research 
achievements on statistics based hybrid 
model to Chinese base phrase identification. 
For the 7 types we considered at present, our 
algorithm turns out promising results and 
smoothes the way for a better Chinese 
parser. 
2 Statistics Based Hybrid Approach to 
Chinese Base Phrase Identification 
2.1 Concepts and Defmitions 
In addition to BNP, constituents of 
many local structure in Chinese centers 
around a core word with certain fixed POS 
sequences. Therefore their identification is
slightly different from parsing in that it 
bears relatively simple phenomenon. Like 
BNP identification, identification of these 
phenomena before parsing will provide a 
simpler sequence for parser, and thus 
deserves a separate r search. 
CutTenfly, we are considering 7 Chinese 
base phrases in our research, namely base 
adjective phrase(BADJP), base adverbial 
phrase (BADVP), base noun phrase (BNP), 
73 
base temporal phrase (BTN), base location 
phrase (BNS), base verb phrase (BVP) and 
base quantity phrase (BMP) Though 
theoretically definitions for these base 
phrases are still unavailable, Appendix I lists 
the preliminary illustrations for them in 
BNF format (necessary account for POS 
annotation can also be found).. 
To frame the identification of Chinese 
base phrases, we fm'ther develop the 
following concepts: 
Definition 1: Chinese based phrases are 
recognized as atomic parts of a sentence 
beyond words that posses certain functions 
and meanings. A base phrase may consist of 
words or other base phrases, but its 
constituents, in turn, should not contain any 
base phrases. 
Definition 2: Base phrase tag is the 
token representing the syntactic function of 
the phrase. At present, base tag either falls in 
one of the 7 Chinese base phrases we are 
considering or not: 
Phrase-Tag ::= BADJP I BADVP I BNP I 
Br  r l Bm I BrP I BMP I lVULL 
Definition 3: Boundary tag denotes the 
possible relative position of a word to a base 
phrase. A boundary tag for a gfven word is 
either L( left boundary of a base phrase), 
R( right boundary of a ), I(inside a base 
phrase) or O(outside the base phrase). 
2.2 Duple Based HMM Parser 
Based on above definitions, we could, 
in view of Wojciech's proposal \[Wojeieeh and 
Thorsten, 1998\], interpret the parsing of 
Chinese base phrases as the following: 
Suppose the input as a sequence of POS 
annotations T= (to, ....... t , , ) .  The task is to 
find RC, a most possible sequence of duples 
formed by base phrase tags and boundary 
tags, among the POS sequence T.
RC = (<ro, co > ........ <rn, Cn>), 
in whil~h ri ( l  <i< =n )indicates the boundary 
tags, ci represents he base phrase tags. 
To go along with the POS tagger 
developed previously by us, we first think of 
preserving HMM (hidden Markov Model) 
for parsing Chinese base phrases. Thus the 
following formula is usually?at hand: 
RC = arg max p(RC I T) 
= arg max p(RC)*  p (T IRC)  
p(T) 
For a given sequence of T, this formula 
can be transformed into: 
RC = arg max p(RC IT) 
= arg max p(RC)*p(T  \ [RC)  
Essentially this model could be 
established through bigram or tri-gram 
statistical training by a annotated corpus. In 
practice, we just build our model from 
l O, O00 manual annotated sentences with 
common bi-gram training: 
p(RC p(RC , IRC  ,_,) 
i=1 
p(T  I RC ) = 1FI p (T i  I RC i) 
i= l  
In realization, a Viterbi algorithm is 
adopted to search the best path. An open test 
on additional 1000 sentences i  performed to 
check its accuracy. Results are shown in 
Tablel(note precision is calculated by 
word'k 
Precision 
for R 
Close 85.7% Test 
Open 82.4% 
Test 
Precision Precision for Both for .C RandC 
87.5% 79.0% 
85.1% 74.7% 
Table 1. Results for Duple Based HMM 
2.3 Triple Based MM Exploiting 
Linguistic Information 
Although results shown in Table 1 i s  
encouraging enough for research purposes, 
it is still lies a long way for practical 
Chinese parser we are aiming at. Reasons 
for errors may be account by too 
coarse-grained information provided by RC. 
Observing the fact that the Chinese base 
phrase occurs more frequently with some 
fixed patterns, i.e. some frozen POS chains, 
we decide to improved our previous model 
by emphasizing the contribution given by 
POS information. 
Adding t denoting POS in the duple (r, 
74 
c), we develop a triple in the form of (t,r,e) 
for the calculation of a node. Naturally, the 
new model is changed into a MM (Markov 
model) as: 
TRC = arg max p(TRC ) 
= arg max I~  p(TRC i I TRC i - 1) 
To train this model, we still using a 
bi-gram model. Applying the same corpus 
and tests described above, we got the 
performance of triple based MM identifier 
for Chinese base phrases (see Table 2). 
Precision Precision Precision 
for R ~rC 
89.2% 91 .5% 84.6% 
88.4% 89.9% 83% 
Close 
Open 
for Both 
R and C 
Table 2. Result for Triple Based MM 
2.4 Further  Improvement Through TBED 
Learning 
Like other statistical models, the above 
model, whether duple based or triple based, 
both seem to reach an accuracy ceiling after 
enlarging training set to 12, 000 or so. To 
cover the remaining accuracy, we apply the 
transformation-based error driven (TBED) 
learning strategy described in \[Brill, 1992\] 
to acquired esired rules. 
In our module, some initial rules are 
first designed as compensation of statistical 
model. Applying these rules will cause new 
mistakes as well as make correct 
identifications. Then the module will 
compare the processed texts with training 
sentences, generate new rules according to 
pre-defmed actions and update its rule bank 
after evaluation (see Fig 1.). 
I I Compare and Rules Passing 
Generate New Rules - - - - !~ Evaluation I 
Tt 
TextTraining \] TextPr?eessed \]
Identifier 
'T 
Input Text 
Figure 1. TBED Learning Module 
The dotted line in fig 2. will stop 
functioning if pre-set accuracy is reached by 
the identifier for the Chinese base phrase. 
Evaluation of new rules is based on an 
greedy algorithm: only rule with max 
contribution (max correction and rain error) 
will be added. Design of rule generation 
(pre-defined actions) is similar to those 
described in \[Brill, 1992\]. 
Table 3 shows a significant 
improvement after applying rules obtained 
through TBED learner. It is also the final 
performance of the proposed Chinese base 
phrase identification model. 
Precision Precision Precision for Both for R for C Rand C 
91.2% 92.8% 89% 
90.4% 91.1% 87.1% 
Close 
open 
Table 3. Results after TBED Module 
3 Conclusions and Discussions 
We have accomplished preliminary 
expedments on identification of various 
types of base phrases defined in this paper. 
The data shown in last seetion prove that our 
method generates atisfactory results for 
75 
Chinese base phrase identification. The 
overall process of our method is outlined the 
following figure. 
Input Chinese Sentences 
after Sengmentation and 
POS tagging 
~ Converted into Nodes 
to Be Parsed 
Triple Based Bi-gram 
MM with Viterbi 
Algorithm 
TBED Based 
Correction 
T" ' Output 
\ 
Fig 2. Processing ofChinese Based Phrase Identification 
However, the 7 types Chinese base 
phrases we have proposed are far l~om 
perfection. Even what we have proposed for 
the 7 phrases is still under test. Further 
improvement will focus on two aspects: one 
is to discuss and add new base phrase for a 
broader coverage; the other is to define, 
theoretically or empirically, the Chinese 
base phrases with more strict constraints. Of 
course, new techniques to improved the 
accuracy of statistical model are the constant 
aim of our research. 
To sum up, Chinese base phrase 
identification will reduce complexity of a 
Chinese parser. The successful idemifieation 
of the 7 base phrases clearly simplifies the 
structure of the sentence. We expect hat the 
research described in this paper will lay a 
solid foundation for a high-accuracy 
Chinese parser. 
22(2): pp141-146 
\[Zhou, et al 1999\] Zhou Qiang, Sun 
Mao-Song, Huang Chang-Ning, Chunk 
parsing scheme for Chinese sentences, 
Chinese J. Computer, 22(11): pp1159-1165 
Reference 
\[Church, 1988\] K. Church, A stochastic 
parts program and noun phrase parser for 
unrestricted text, In: Proc. of Second 
Conference on Applied Natural Language 
Processing, 1988 
\[Wojciech and Thorsten, 1998\] Wojciech Skut and 
Thorsten Brants, Chunk Tagger, Statistical 
Recongnition of Noun Phrases, In ESSLLI-98 
Workshop on Automated Acquisition of Syntax 
and Parsing, Saarbrvcken, 1998. 
\[Zhao & Huang, 1999\] Zhao Jun and Huang 
Chang-Ning, The model for Chinese baseNP 
structure analysis, Chinese J. Computer, 
76 
Appendix Illustration of 7 Chinese Base 
Phrases in BNF 
The patterns listed here are far from 
complete (even for the 7 phrases 
themselves). Theoretical definition is 
beyond this paper and what we provide here 
is actually stage results of expert 
observation and linguistic abstraction. 
BADJP ::= d++a \[ d+BADJP \] a + I a+BADJP 
\[ BADVP+a I BADVP+BADJP 
BADVP ::= a+usdi(:~) I d+usdi I vg+usdi I 
BADJP+usdi IBADVP+usdi IBMP+usdi 
BMP ::= m + \[ m*+q* \[ m+q+m \[ d+m+q \] 
f+m+q \[ r+m+q I BMP ? 
BNP ::= a+n I a+usde(~)+n I a+usde+BNP I 
a+BNP \]b+n \] b+usde+n I b+usde+BNP I 
b+BNP I d+usde+n I f+n I f+usde+n I f+BNP 
1 m+n I m+BNP I n+ I n+usde+n I
n+usde+BNP I n+usde+BMP I n+BNP I q+n 
I q+BNP I r+a+n I r+m+n I r+n I r+usde+n I 
r+usde+BNP \[ r+BNP I s+n I s+usde+n \[
s+usde+BNP I t+nl t+usde+n \[ t+usde+BNP 
I vg +usde+n I vg+usde+BNP I BADJP+n 
BADJP+usde+n \] BADJP+usde+BNP 
BADJP+BNP \[ BMP+n \[ BMP+usde+n 
BMP+usde+BNP \[ BMP+BNP \[ BNP+n 
BNP+usde+n \[ BNP+usde+BNP 
BNP+usde+BMP \[ BNP+BNP 
BNS+usde+n \[ BNS+usde+BNP 
BNS+BNP I BTN+usde+n 
BTN+usde+BNP \[ BVP+usde+n 
BVP+usde+BNP 
BNS ::= a+nd I m+nd I n+s I r+nd I 
n+usde+f I n+usde+nd I n+usde+s I 
n+usde+BNS I nd + I r+usde+nd \[ r+usde+s I 
s+usde+nd I s+usde+BNS I BNP BNS I 
BNS + 
BTN ::= a+t I m+t I r+t I t+ I t+usd~t  I 
BMP+t I BTN+t I BNP+usde+t 
BVP ::= a+vg I d+vg I vg+d+a I vg+d+vq I 
vg+d+vb I vg+usdf(~)+a I vg+usdf+d I 
vg+usdf+vq \[ vg+usdf+u I vg+usdf+BADJP I 
vg+ut I vg+vb I vg+ut+vq I vq+vg I vq+BVP 
\] vz+vg I vz+BVP I BADJP+vg I 
BADVP+vg \[ BADVP+BVP I BVP+ut I 
BVP+vq I BVP+BVP 
Symbol 
a 
d 
Part-Of-Speech 
Adjective 
Adverb 
TemporaYspacial 
position word 
Examples 
~(beaut i fu l ) ,  ~( romant ic )  
~(very), ~(s t i l l )  
~(in), _k(on), ~N(between) 
m numeral --(one), ~(two), -~(three) 
n noun ~ ~ (people), ~ ~I~  (tomato), 
"bl-~JL(computer) 
nd Name of place ~(Be i j ing) ,  I I~(Harb in ) ,  
~.\]t~(New York) 
q classifier \]\]~(flock), +(NULL) 
r pronoun '~'~(you), ~(I, me), ~(he, him) 
s location oun I~.(around), ~:gb(outside) 
t time noun ~;~(yesterday), --L~ (July) 
ut tense auxiliary ~,T,~c_(NULL) 
vb Complemental verb ~,~_t(NULL) 
vg common verb ~ll~(know), ~( long  for) 
vq directional verb ~,T  ~i~(NULL) 
vz modal verb ~I ~(can), )~(shou ld )  
Table for POS symbols used in Appendix 
77 
Automatic Information Transfer Between English And Chinese 
Jianmin Yao, Hao Yu, Tiejun Zhao  
School of Computer Science and Technology 
Harbin Institute of Technology 
Harbin, China, 150001 
james@mtlab.hit.edu.cn 
Xiaohong Li 
Department of Foreign Studies 
Harbin Institute of Technology 
Harbin, China, 150001 
goodtreeyale@yahoo.com.cn 
 
Abstract  
The translation choice and transfer modules 
in an English Chinese machine translation 
system are introduced. The translation 
choice is realized on basis of a grammar tree 
and takes the context as a word bag, with the 
lexicon and POS tag information as context 
features. The Bayes minimal error 
probability is taken as the evaluation 
function of the candidate translation. The 
rule-based transfer and generation module 
takes the parsing tree as the input and 
operates on the information of POS tag, 
semantics or even the lexicon. 
Introduction 
Machine translation is urgently needed to get 
away with the language barrier between 
different nations. The task of machine 
translation is to realize mapping from one 
language to another. At present there are three 
main methods for machine translation systems 
[Zhao 2000]: 1) pattern/rule based systems: 
production rules compose the main body of the 
knowledge base. The rules or patterns are often 
manually written or automatically acquired from 
training corpus; 2) example based method. The 
knowledge base is a bilingual corpus of source 
slices S? and their translations T? Given a source 
slice of input S, match S with the source slices 
and choose the most similar as the translation or 
get the translation from it. 3) Statistics based 
method: it is a method based on monolingual 
language model and bilingual language model. 
The probabilities are acquired from large-scale 
(bilingual) corpora.  
Machine translation is more than a 
manipulation of one natural language (e.g. 
Chinese). Not only the grammatical and 
semantic characteristics of the source language 
must be considered, but also those of the target 
language. To sum up, the characteristics of 
bilingual translation is the essence of a machine 
translation system.  
A machine translation system usually 
includes 3 sub-systems [Zhao 1999] ? (1) 
Analysis: to analyse the source language 
sentence and generate a syntactic tree with 
syntactic functional tags; (2) Transfer: map a 
source parsing tree into a target language parsing 
tree; (3) Generation: generate the target 
language sentence according to the target 
language syntactic tree.  
The MTS2000 system developed in Harbin 
Institute of Technology is a bi-directional 
machine translation system based on a 
combination of stochastic and rule-based 
methods. Figure 1 shows the flow of the system.  
 Input English Sentence  
 Morphology Analysis  
 
Syntactic Analysis  
 
Word Translation Choice  
 
Transfer and Generation  
 
 
 
Output Chinese Sentence 
Figure 1 Flowchart of MTS2000 System  
Analysis and transfer are separated in the 
architecture of the MTS2000 system. This 
modularisation is helpful to the integration of 
stochastic method and the rule based method. 
New techniques are easier to be integrated into 
the modularised system. Two modules 
implement the transfer step and the generation 
step after analysis of the source sentence. The 
specific task of transfer and generation is to 
produce a target language sentence given the 
source language syntactic tree. In details, given 
an English syntactic tree (e.g. S[PP[ In/IN 
BNP[our/PRP$ workshop/NN]] BNP[ there/EX] 
VP[ is/VBZ NP[ no/DT NP[ NN[ machine/NN 
tool/NN] SBAR[ but/CC VP[ is/VBZ 
made/VBN PP[ in/IN BNP[ China/NNP ]]]]]]]]), 
using knowledge sources such as grammatical 
features, simple semantic features, construct a 
Chinese syntactic tree, whose terminal nodes 
compromise in sequence the Chinese translation.  
The input sentence are analysed using the 
morphology analyser, part-of-speech tagger, and 
syntactic analyser. After these steps, a syntactic 
parsing tree is obtained which has multiple 
levels with functional tags [Meng 2000]. 
Followed is the parser flow: 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Parser based on Hybrid Methods 
At present, our English parser is able to 
generate syntactic tree ble 
way. The English parsin
information about relatio
in the source sentence
information of the nodes
of transfer and generatio
the nodes is the starting
generation. After syntact
transfer and generation in
choice of ambiguous 
adjustment and inserti
functional words. Trans
implemented using two
word translation choice, 
transfer and translation m
1 Parsing Based Tran
First we will give a f
translation choice in 
[Manning 1999]: Suppose the source sentence to 
be translated to be ES. In the sentence the 
ambiguous word EW has M target translations 
CW1, CW2, ... CWM. And the translations 
occurs in a specific context C with probabilities 
P(CW1 | C)?P(CW2 | C), ... P(CWM|C)?From 
the Bayes minimum error probability formula, 
we get: 
CW = argmax[P(CWk|C)] 
= argmax[logP(CWk) + logP(C|CWk) ] (1) 
Generally when the condition fulfills 
P(CW1|C)>P(CW2|C)>...>P(CWM|C), we may 
choose CW1 as the translation for EW. From the 
Na?ve Bayes formula? 
P(C|CWk) = P({vj | vj in C}|CWk) 
 = ?Vj in C P(vj|sk)              (2) 
So formula (1) can be rewritten as: 
CW = argmax[P(CWk|C)] 
Input Sentence Statistics Knowledge = argmax[logP(CWk)+?Vj in ClogP(vj|CWk)] (3) 
Where P(CWk) denotes the probability that 
CWk occurs in the corpus; P (vj| CWk) denotes 
the probability that the context feature vj 
co-occurs with translation CWk? 
A general algorithm of supervised word 
sense disambiguation is as follows: 
1. comment: Training 
2. for all senses sk of w do 
3.    for all words vj in the vocabulary do 
4.       P(vj|sk) = C(vj, sk)/C(vj) 
5.    end 
6. end 
7. for all senses sk of w do 
8.    P(sk) = C(sk)/C(w) 
9. end 
POS Tagger Manual Rule 
Base 
PPA Resolution 
Layered Parsing eein comparative usa
g 
nsh
, a
, is
n. 
 p
ic 
clu
w
on
fer 
 m
the
od
sla
orm
mParsing Trtree, with the basic 
ip among the nodes 
lso with semantic 
 input to the module 
The information of 
oint of transfer and 
parsing, the task of 
des word translation 
ords, word order 
/deletion of some 
and generation are 
odules: one is for 
 other for structure 
ification. 
tion Choice 
al description for 
achine translation 
10. comment: Disambiguation 
11. for all sense sk of w do 
12.    Score(sk) = logP(sk) 
13.    for all words vj in the context window c do
14.       score(sk) = score(sk) + logP(vj|sk) 
15.    end 
16. end 
17. choose s? = argmaxskscore(sk) 
Figure 5. Bayesian disambiguation 
From the above formal description we can 
see that the key to the stochastic word 
translation is to select proper context and context 
features Vj. Present methods often define a word 
window of some size, i.e. to suppose only words 
within the window contributes to the translation 
choice of the ambiguous word. For example, 
[Huang 1997] uses a word window of length 6 
words for word sense disambiguation; [Xun 
1998] define a moveable window of length 4 
words; [Ng 1997] uses a word window with 
offset ?2. But two problems exist for this 
method: (1) some words that are informative to 
sense disambiguation may not be covered by the 
window; (2) some words that are covered by the 
word window really contribute nothing to the 
sense choice, but only bring noise information. 
After a broad investigation for large-scale 
ambiguous words, we choose the context 
according to the correlation of the context words 
with the ambiguous word, but not only the 
distance from the word. 
From the above analysis, we choose the 
translation choice method based on syntactic 
analysis. Place the module of translation choice 
between the parser and the generator; acquire a 
context set for the ambiguous word. When 
choosing the translation, we may take the 
context set as a word bag, i.e. the grammatical 
context as word bag. No single word is 
considered but only that lexical and 
part-of-speech information are taken as context 
features. Bayes minimum error probability is 
taken as evaluation function for word translation 
choice. 
In this paper, grammatical context is 
considered for word translation choice. The 
structure related features of the ambiguous 
words are taken into account for fully use of the 
parsing result. It has the characteristics below: (1) 
The window size is not defined by human but on 
basis of the grammatical structure of the 
sentence, so we can acquire more efficiently the 
useful context features; (2) The unrelated 
context features in sentence structure are filtered 
out for translation choice; (3) The features are 
based on the structure relationship, but not 100% 
right parsing result. From the above 
characteristics, we can see the method is really 
practical. 
2 Rule Based Transfer & Generation 
For MTS2000, structural transfer is to start from 
the syntactic parsing tree and construct the 
Chinese syntactic tree. While the generation of 
Chinese is to generate a word link from the 
Chinese tree and build the translation sentence 
[Yao 2001]. This module has adopted the 
rule-based knowledge representation method. 
The design of the rule system is highly related to 
the performance of the machine translation 
system. 
The rule description language of the 
machine translation system is in the form of 
production rules, i.e. a rule composed of a 
conditional part and an operational part. The 
conditional part is a scan window of variable 
length, which uses the context constraint 
conditions such as phrases or some linguistic 
features. The operational part generates the 
corresponding translation or some corresponding 
generation features in the operational part. If the 
conditions are met, the operations will be 
performed. The representation of the rule system 
has shown a characteristic of the system, that is 
the integration of transfer and generation. The 
rule description language is similar to natural 
language and consistent with human habits. 
Multiple description methods are implemented. 
The conditional part of the rules is 
composed of node numbers and ?+? symbols 
that is used to link the nodes. The operation part 
consists of corresponding conditional parts and 
translations and also, if necessary, some action 
functions. 
For example, the rule to combine an 
adjective and a noun to generate a noun phrase is 
as follows:  
0:Cate=A + 1:Cate=N 
->0:* + 1:* + _NodeUf(N?0?1) 
in which, ?*? stands for corresponding 
translation of the nodes, _NodeUf() is a function 
that combines the nodes to generate a new node. 
The new translation is generated at the same 
time with the combination of nodes. 
In general, the English Chinese machine 
translation system has the following features in 
the transfer and generation phase: 
1) The grammatical and semantic features are 
described by a string composed of frame 
name and values linked with ?=?; 
2) The conditions may be operated by ?and?, 
?or? and ?not?; 
3) Nodes in the same level of the sentence may 
be scanned and tested arbitrarily; 
4) The action functions and test functions can 
generate corresponding features for feature 
transmission and test. 
The rules are organized into various levels. 
All the rules are put in the knowledge base with 
part-of-speech as the entry feature. The rules 
have different priorities, which decide their 
sequence in rule matching. In general, the more 
specific the rule, the higher is its priority. The 
more general the rule, the lower is its priority. 
The levels of the rules help resolve rule 
collision. 
Conclusion 
The system prototype has been implemented and 
large-scale development and refinement are 
under progress. From our knowledge of the 
system, knowledge acquisition and rule base 
organization is the bottleneck for MTS2000 
system and similar natural language processing 
systems. The knowledge acquisition for word 
translation choice needs large-scale word 
aligned bilingual corpus. We are making 
research on new word translation methods on 
basis of our 60,000-sentence aligned bilingual 
corpus. The transfer and generation knowledge 
base are facing much knowledge collision and 
redundancy problem. The organization 
technique of knowledge base is also an 
important issue in the project. 
References  
Tie-Jun Zhao, En-Dong Xun, Bin Chen, Xiao-Hu 
Liu,Sheng Li, Research on Word Sense 
Disambiguation based on Target Language 
Statistics, Applied Fundamental and Engineering 
Journal, 1999?7?1??101-110 
Meng Yao, Zhao Tiejun, Yu Hao, Li Sheng, A 
Decision Tree Based Corpus Approach to English 
Base Noun Phrase Identification, Proceedings 
International conference on East-Asian Language 
Processing and Internet Information Technology, 
Shenyang, 2000: 5-10 
Christopher D. Manning, Hinrich Sch ? tze, 
Foundation of Statistical Natural Language 
Processing. The MIT Press. pp229-262. 1999. 
Chang-Ning Huang, Juan-Zi Li, A language model 
for word sense disambiguation, 10th anniversary 
for Chinese Linguistic Society, October, 1997, 
Fuzhou  
En-Dong Xun, Sheng Li, Tie-Jun Zhao, Bi-gram 
co-occurrence based stochastic method for word 
sense disambiguation, High Technologies, 1998, 
10(8): 21-25  
Hwee Tou Ng. Exemplar-Based Word Sense 
Disambiguation: Some Recent Improvements. In 
Proceedings of the Second Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), August 1997  
Tie-Jun Zhao etc, Principle of Machine Translation, 
Press of Harbin Institute of Technology, 2000. 
Jian-Min Yao, Jing Zhang, Hao Yu, Tie-Jun 
Zhao,Sheng Li, Transfer from an English parsing 
tree to a Chinese syntactic tree, Joint Conference of 
the Society of Computational Linguistics, 2001, 
Taiyuan.-138.  
  
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 457?464,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 An Equivalent Pseudoword Solution to Chinese  
Word Sense Disambiguation 
 
Zhimao Lu+    Haifeng Wang++    Jianmin Yao+++    Ting Liu+    Sheng Li+
+ Information Retrieval Laboratory, School of Computer Science and Technology,  
Harbin Institute of Technology, Harbin, 150001, China 
{lzm, tliu, lisheng}@ir-lab.org 
++ Toshiba (China) Research and Development Center 
5/F., Tower W2, Oriental Plaza, No. 1, East Chang An Ave., Beijing, 100738, China 
wanghaifeng@rdc.toshiba.com.cn 
+++ School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China 
jyao@suda.edu.cn 
 
  
 
Abstract 
This paper presents a new approach 
based on Equivalent Pseudowords (EPs) 
to tackle Word Sense Disambiguation 
(WSD) in Chinese language. EPs are par-
ticular artificial ambiguous words, which 
can be used to realize unsupervised WSD. 
A Bayesian classifier is implemented to 
test the efficacy of the EP solution on 
Senseval-3 Chinese test set. The per-
formance is better than state-of-the-art 
results with an average F-measure of 0.80. 
The experiment verifies the value of EP 
for unsupervised WSD. 
1 Introduction 
Word sense disambiguation (WSD) has been a 
hot topic in natural language processing, which is 
to determine the sense of an ambiguous word in 
a specific context. It is an important technique 
for applications such as information retrieval, 
text mining, machine translation, text classifica-
tion, automatic text summarization, and so on. 
Statistical solutions to WSD acquire linguistic 
knowledge from the training corpus using ma-
chine learning technologies, and apply the 
knowledge to disambiguation. The first statistical 
model of WSD was built by Brown et al (1991). 
Since then, most machine learning methods have 
been applied to WSD, including decision tree, 
Bayesian model, neural network, SVM, maxi-
mum entropy, genetic algorithms, and so on. For 
different learning methods, supervised methods 
usually achieve good performance at a cost of 
human tagging of training corpus. The precision 
improves with larger size of training corpus. 
Compared with supervised methods, unsuper-
vised methods do not require tagged corpus, but 
the precision is usually lower than that of the 
supervised methods. Thus, knowledge acquisi-
tion is critical to WSD methods.  
This paper proposes an unsupervised method 
based on equivalent pseudowords, which ac-
quires WSD knowledge from raw corpus. This 
method first determines equivalent pseudowords 
for each ambiguous word, and then uses the 
equivalent pseudowords to replace the ambigu-
ous word in the corpus. The advantage of this 
method is that it does not need parallel corpus or 
seed corpus for training. Thus, it can use a large-
scale monolingual corpus for training to solve 
the data-sparseness problem. Experimental re-
sults show that our unsupervised method per-
forms better than the supervised method. 
The remainder of the paper is organized as fol-
lows. Section 2 summarizes the related work. 
Section 3 describes the conception of Equivalent 
Pseudoword. Section 4 describes EP-based Un-
supervised WSD Method and the evaluation re-
sult. The last section concludes our approach. 
2 Related Work 
For supervised WSD methods,  a knowledge ac-
quisition bottleneck is to prepare the manually 
457
tagged corpus. Unsupervised method is an alter-
native, which often involves automatic genera-
tion of tagged corpus, bilingual corpus alignment, 
etc. The value of unsupervised methods lies in 
the knowledge acquisition solutions they adopt. 
2.1 Automatic Generation of Training Corpus 
Automatic corpus tagging is a solution to WSD, 
which generates large-scale corpus from a small 
seed corpus. This is a weakly supervised learning 
or semi-supervised learning method. This rein-
forcement algorithm dates back to Gale et al 
(1992a). Their investigation was based on a 6-
word test set with 2 senses for each word. 
Yarowsky (1994 and 1995), Mihalcea and 
Moldovan (2000), and Mihalcea (2002) have 
made further research to obtain large corpus of 
higher quality from an initial seed corpus. A 
semi-supervised method proposed by Niu et al 
(2005) clustered untagged instances with tagged 
ones starting from a small seed corpus, which 
assumes that similar instances should have simi-
lar tags. Clustering was used instead of boot-
strapping and was proved more efficient.  
2.2 Method Based on Parallel Corpus 
Parallel corpus is a solution to the bottleneck of 
knowledge acquisition. Ide et al (2001 and 
2002), Ng et al (2003), and Diab (2003, 2004a, 
and 2004b) made research on the use of align-
ment for WSD.  
Diab and Resnik (2002) investigated the feasi-
bility of automatically annotating large amounts 
of data in parallel corpora using an unsupervised 
algorithm, making use of two languages simulta-
neously, only one of which has an available 
sense inventory. The results showed that word-
level translation correspondences are a valuable 
source of information for sense disambiguation. 
The method by Li and Li (2002) does not re-
quire parallel corpus. It avoids the alignment 
work and takes advantage of bilingual corpus. 
In short, technology of automatic corpus tag-
ging is based on the manually labeled corpus. 
That is to say, it still need human intervention 
and is not a completely unsupervised method. 
Large-scale parallel corpus; especially word-
aligned corpus is highly unobtainable, which has 
limited the WSD methods based on parallel cor-
pus.  
3 Equivalent Pseudoword 
This section describes how to obtain equivalent 
pseudowords without a seed corpus. 
Monosemous words are unambiguous priori 
knowledge. According to our statistics, they ac-
count for 86%~89% of the instances in a diction-
ary and 50% of the items in running corpus, they 
are potential knowledge source for WSD.  
A monosemous word is usually synonymous 
to some polysemous words. For example the 
words "?? , ?? , ?? ?? ?? ??, , , , 
??" has similar meaning as one of the senses 
of the ambiguous word "??", while "??, ?
?, ?? ?? ??, , ?? ?? ?? ??, , , , , 
?? ?? ?? ??, , , " are the same for "??". 
This is quite common in Chinese, which can be 
used as a knowledge source for WSD. 
3.1 Definition of Equivalent Pseudoword 
If the ambiguous words in the corpus are re-
placed with its synonymous monosemous word, 
then is it convenient to acquire knowledge from 
raw corpus? For example in table 1, the ambigu-
ous word "??" has three senses, whose syn-
onymous monosemous words are listed on the 
right column. These synonyms contain some in-
formation for disambiguation task. 
An artificial ambiguous word can be coined 
with the monosemous words in table 1. This 
process is similar to the use of general pseu-
dowords (Gale et al, 1992b; Gaustad, 2001; Na-
kov and Hearst, 2003), but has some essential 
differences. This artificial ambiguous word need 
to simulate the function of the real ambiguous 
word, and to acquire semantic knowledge as the 
real ambiguous word does. Thus, we call it an 
equivalent pseudoword (EP) for its equivalence 
with the real ambiguous word. It's apparent that 
the equivalent pseudoword has provided a new 
way to unsupervised WSD. 
S1 ??/??? 
S2 ??/??/??/??/????(ba3 wo4)
S3 ??/??/??/??/??
Table 1. Synonymous Monosemous Words for 
the Ambiguous Word "??" 
The equivalence of the EP with the real am-
biguous word is a kind of semantic synonym or 
similarity, which demands a maximum similarity 
between the two words. An ambiguous word has 
the same number of EPs as of senses. Each EP's 
sense maps to a sense of ambiguous word. 
The semantic equivalence demands further 
equivalence at each sense level. Every corre-
458
sponding sense should have the maximum simi-
larity, which is the strictest limit to the construc-
tion of an EP. 
The starting point of unsupervised WSD based 
on EP is that EP can substitute the original word 
for knowledge acquisition in model training. 
Every instance of each morpheme of the EP can 
be viewed as an instance of the ambiguous word, 
thus the training set can be enlarged easily. EP is 
a solution to data sparseness for lack of human 
tagging in WSD. 
3.2 Basic Assumption for EP-based WSD 
It is based on the following assumptions that EPs 
can substitute the original ambiguous word for 
knowledge acquisition in WSD model training. 
Assumption 1: Words of the same meaning 
play the same role in a language. The sense is an 
important attribute of a word. This plays as the 
basic assumption in this paper. 
Assumption 2: Words of the same meaning 
occur in similar context. This assumption is 
widely used in semantic analysis and plays as a 
basis for much related research. For example, 
some researchers cluster the contexts of ambigu-
ous words for WSD, which shows good perform-
ance (Schutze, 1998). 
Because an EP has a higher similarity with the 
ambiguous word in syntax and semantics, it is a 
useful knowledge source for WSD. 
3.3 Design and Construction of EPs 
Because of the special characteristics of EPs, it's 
more difficult to construct an EP than a general 
pseudo word. To ensure the maximum similarity 
between the EP and the original ambiguous word, 
the following principles should be followed. 
1) Every EP should map to one and only one 
original ambiguous word. 
2) The morphemes of an EP should map one 
by one to those of the original ambiguous word. 
3) The sense of the EP should be the same as 
the corresponding ambiguous word, or has the 
maximum similarity with the word. 
4) The morpheme of a pseudoword stands for 
a sense, while the sense should consist of one or 
more morphemes.  
5) The morpheme should be a monosemous 
word. 
The fourth principle above is the biggest dif-
ference between the EP and a general pseudo 
word. The sense of an EP is composed of one or 
several morphemes. This is a remarkable feature 
of the EP, which originates from its equivalent 
linguistic function with the original word. To 
construct the EP, it must be ensured that the 
sense of the EP maps to that of the original word. 
Usually, a candidate monosemous word for a 
morpheme stands for part of the linguistic func-
tion of the ambiguous word, thus we need to 
choose several morphemes to stand for one sense.  
The relatedness of the senses refers to the 
similarity of the contexts of the original ambigu-
ous word and its EP. The similarity between the 
words means that they serve as synonyms for 
each other. This principle demands that both se-
mantic and pragmatic information should be 
taken into account in choosing a morpheme word. 
3.4 Implementation of the EP-based Solution 
An appropriate machine-readable dictionary is 
needed for construction of the EPs. A Chinese 
thesaurus is adopted and revised to meet this de-
mand. 
Extended Version of TongYiCiCiLin 
To extend the TongYiCiCiLin (Cilin) to hold 
more words, several linguistic resources are 
adopted for manually adding new words. An ex-
tended version of the Cilin is achieved, which 
includes 77,343 items. 
A hierarchy of three levels is organized in the 
extended Cilin for all items. Each node in the 
lowest level, called a minor class, contains sev-
eral words of the same class. The words in one 
minor class are divided into several groups ac-
cording to their sense similarity and relatedness, 
and each group is further divided into several 
lines, which can be viewed as the fifth level of 
the thesaurus. The 5-level hierarchy of the ex-
tended Cilin is shown in figure 1. The lower the 
level is, the more specific the sense is. The fifth 
level often contains a few words or only one 
word, which is called an atom word group, an 
atom class or an atom node. The words in the 
same atom node hold the smallest semantic dis-
tance. 
From the root node to the leaf node, the sense 
is described more and more detailed, and the 
words in the same node are more and more re-
lated. Words in the same fifth level node have 
the same sense and linguistic function, which 
ensures that they can substitute for each other 
without leading to any change in the meaning of 
a sentence. 
 
 
459
 ?  ? 
?
?? ?? 
? ? ? ?
? ? ?
 
? ? ? ?
Level 1 
Level 2 
Level 3 
Level 4 
Level 5 
?  ? 
Figure 1. Organization of Cilin (extended) 
 
The extended version of extended Cilin is 
freely downloadable from the Internet and has 
been used by over 20 organizations in the world1. 
Construction of EPs 
According to the position of the ambiguous word, 
a proper word is selected as the morpheme of the 
EP. Almost every ambiguous word has its corre-
sponding EP constructed in this way. 
The first step is to decide the position of the 
ambiguous word starting from the leaf node of 
the tree structure. Words in the same leaf node 
are identical or similar in the linguistic function 
and word sense. Other words in the leaf node of 
the ambiguous word are called brother words of 
it. If there is a monosemous brother word, it can 
be taken as a candidate morpheme for the EP. If 
there does not exist such a brother word, trace to 
the fourth level. If there is still no monosemous 
brother word in the fourth level, trace to the third 
level. Because every node in the third level con-
tains many words, candidate morpheme for the 
ambiguous can usually be found. 
In most cases, candidate morphemes can be 
found at the fifth level. It is not often necessary 
to search to the fourth level, less to the third. Ac-
cording to our statistics, the extended Cilin con-
tains about monosemous words for 93% of the 
ambiguous words in the fifth level, and 97% in 
the fourth level. There are only 112 ambiguous 
words left, which account for the other 3% and 
mainly are functional words. Some of the 3% 
words are rarely used, which cannot be found in 
even a large corpus. And words that lead to se-
mantic misunderstanding are usually content 
words. In WSD research for English, only nouns, 
verbs, adjectives and adverbs are considered. 
                                                 
1 It is located at http://www.ir-lab.org/. 
From this aspect, the extended version of Cilin 
meets our demand for the construction of EPs. 
If many monosemous brother words are found 
in the fourth or third level, there are many candi-
date morphemes to choose from. A further selec-
tion is made based on calculation of sense simi-
larity. More similar brother words are chosen. 
Computing of EPs 
Generally, several morpheme words are needed 
for better construction of an EP. We assume that 
every morpheme word stands for a specific sense 
and does not influence each other. It is more 
complex to construct an EP than a common 
pseudo word, and the formulation and statistical 
information are also different. 
An EP is described as follows:  
 
iikiiii
k
k
WWWWS
WWWWS
WWWWS
L
MMMMMM
L
L
,,,:
,,,:
,,,:
321
22322212
11312111
2
1
 
WEP?????????? 
Where WEP is the EP word, Si is a sense of the 
ambiguous word, and Wik is a morpheme word of 
the EP. 
The statistical information of the EP is calcu-
lated as follows: 
1? stands for the frequency of the S)( iSC i : 
?=
k
iki WCSC )()(  
2? stands for the co-occurrence fre-
quency of S
),( fi WSC
i and the contextual word Wf : 
?=
k
fikfi WWCWSC ),(),(  
460
 Ambiguous word citation (Qin and Wang, 2005) Ours Ambiguous word 
citation (Qin and 
Wang, 2005) Ours 
??(ba3 wo4) 0.56 0.87 ??(mei2 you3) 0.75 0.68 
?(bao1) 0.59 0.75 ??(qi3 lai2) 0.82 0.54 
??(cai2 liao4) 0.67 0.79 ?(qian2) 0.75 0.62 
??(chong1 ji1) 0.62 0.69 ??(ri4 zi3) 0.75 0.68 
?(chuan1) 0.80 0.57 ?(shao3) 0.69 0.56 
??(di4 fang1) 0.65 0.65 ??(tu1 chu1) 0.82 0.86 
??(fen1 zi3) 0.91 0.81 ??(yan2 jiu1) 0.69 0.63 
??(yun4 dong4) 0.61 0.82 ??(huo2 dong4) 0.79 0.88 
?(lao3) 0.59 0.50 ?(zou3) 0.72 0.60 
?(lu4) 0.74 0.64 ?(zuo4) 0.90 0.73 
Average 0.72 0.69 Note: Average of the 20 words 
Table 2. The F-measure for the Supervised WSD 
 
4 EP-based Unsupervised WSD Method 
EP is a solution to the semantic knowledge ac-
quisition problem, and it does not limit the 
choice of statistical learning methods. All of the 
mathematical modeling methods can be applied 
to EP-based WSD methods. This section focuses 
on the application of the EP concept to WSD, 
and chooses Bayesian method for the classifier 
construction. 
4.1 A Sense Classifier Based on the Bayes-
ian Model 
Because the model acquires knowledge from the 
EPs but not from the original ambiguous word, 
the method introduced here does not need human 
tagging of training corpus. 
In the training stage for WSD, statistics of EPs 
and context words are obtained and stored in a 
database. Senseval-3 data set plus unsupervised 
learning method are adopted to investigate into 
the value of EP in WSD. To ensure the compara-
bility of experiment results, a Bayesian classifier 
is used in the experiments. 
Bayesian Classifier 
Although the Bayesian classifier is simple, it is 
quite efficient, and it shows good performance 
on WSD. 
The Bayesian classifier used in this paper is 
described in (1) 
???
?
???
?
+= ?
? ij
k
cv
kjkSi SvPSPwS )|(log)(logmaxarg)( (1)
Where wi is the ambiguous word,  is the 
occurrence probability of the sense S
)( kSP
k,  
is the conditional probability of the context word 
v
)|( kj SvP
j, and ci is the set of the context words. 
To simplify the experiment process, the Naive 
Bayesian modeling is adopted for the sense clas-
sifier. Feature selection and ensemble classifica-
tion are not applied, which is both to simplify the 
calculation and to prove the effect of EPs in 
WSD. 
Experiment Setup and Results  
The Senseval-3 Chinese ambiguous words are 
taken as the testing set, which includes 20 words, 
each with 2-8 senses. The data for the ambiguous 
words are divided into a training set and a testing 
set by a ratio of 2:1. There are 15-20 training 
instances for each sense of the words, and occurs 
by the same frequency in the training and test set. 
Supervised WSD is first implemented using 
the Bayesian model on the Senseval-3 data set. 
With a context window of (-10, +10), the open 
test results are shown in table 2. 
The F-measure in table 2 is defined in (2). 
RP
RP
F +
??= 2  (2) 
461
Where P and R refer to the precision and recall 
of the sense tagging respectively, which are cal-
culated as shown in (3) and (4) 
)tagged(
)correct(
C
C
P =  (3) 
)all(
)correct(
C
C
R =  (4) 
Where C(tagged) is the number of tagged in-
stances of senses, C(correct) is the number of 
correct tags, and C(all) is the number of tags in 
the gold standard set. Every sense of the am-
biguous word has a P value, a R value and a F 
value. The F value in table 2 is a weighted aver-
age of all the senses. 
In the EP-based unsupervised WSD experi-
ment, a 100M corpus (People's Daily for year 
1998) is used for the EP training instances. The 
Senseval-3 data is used for the test. In our ex-
periments, a context window of (-10, +10) is 
taken. The detailed results are shown in table 3. 
4.2 Experiment Analysis and Discussion 
Experiment Evaluation Method 
Two evaluation criteria are used in the experi-
ments, which are the F-measure and precision. 
Precision is a usual criterion in WSD perform-
ance analysis. Only in recent years, the precision, 
recall, and F-measure are all taken to evaluate 
the WSD performance. 
In this paper, we will only show the f-measure 
score because it is a combined score of precision 
and recall. 
Result Analysis on Bayesian Supervised WSD 
Experiment 
The experiment results in table 2 reveals that the 
results of supervised WSD and those of (Qin and 
Wang, 2005) are different. Although they are all 
based on the Bayesian model, Qin and Wang 
(2005) used an ensemble classifier. However, the 
difference of the average value is not remarkable. 
As introduced above, in the supervised WSD 
experiment, the various senses of the instances 
are evenly distributed. The lower bound as Gale 
et al (1992c) suggested should be very low and 
it is more difficult to disambiguate if there are 
more senses. The experiment verifies this reason-
ing, because the highest F-measure is less than 
90%, and the lowest is less than 60%, averaging 
about 70%. 
With the same number of senses and the same 
scale of training data, there is a big difference 
between the WSD results. This shows that other 
factors exist which influence the performance 
other than the number of senses and training data 
size. For example, the discriminability among the 
senses is an important factor. The WSD task be-
comes more difficult if the senses of the ambigu-
ous word are more similar to each other. 
Experiment Analysis of the EP-based WSD 
The EP-based unsupervised method takes the 
same open test set as the supervised method. The 
unsupervised method shows a better performance, 
with the highest F-measure score at 100%, low-
est at 59% and average at 80%. The results 
shows that EP is useful in unsupervised WSD. 
 
Sequence 
Number Ambiguous word F-measure
Sequence 
Number Ambiguous word 
F-measure 
(%) 
1 ??(ba3 wo4) 0.93 11 ??(mei2 you3) 1.00 
2 ?(bao1) 0.74 12 ??(qi3 lai2) 0.59 
3 ?(cai2 liao4) 0.80 13 ?(qian2) 0.71 
4 ??(chong1 ji1) 0.85 14 ??(ri4 zi3) 0.62 
5 ?(chuan1) 0.79 15 ?(shao3) 0.82 
6 ??(di4 fang1) 0.78 16 ??(tu1 chu1) 0.93 
7 ??(fen1 zi3) 0.94 17 ??(yan2 jiu1) 0.71 
8 ??(yun4 
dong4) 
0.94 18 ??(huo2 dong4) 0.89 
9 ?(lao3) 0.85 19 ?(zou3) 0.68 
10 ?(lu4) 0.81 20 ?(zuo4) 0.67 
Average 0.80 Note: Average of the 20 words 
Table 3. The Results for Unsupervised WSD based on EPs 
462
 
From the results in table 2 and table 3, it can 
be seen that 16 among the 20 ambiguous words 
show better WSD performance in unsupervised 
SWD than in supervised WSD, while only 2 of 
them shows similar results and 2 performs worse . 
The average F-measure of the unsupervised 
method is higher by more than 10%. The reason 
lies in the following aspects: 
1) Because there are several morpheme words 
for every sense of the word in construction of the 
EP, rich semantic information can be acquired in 
the training step and is an advantage for sense 
disambiguation. 
2) Senseval-3 has provided a small-scale train-
ing set, with 15-20 training instances for each 
sense, which is not enough for the WSD model-
ing. The lack of training information leads to a 
low performance of the supervised methods. 
3) With a large-scale training corpus, the un-
supervised WSD method has got plenty of train-
ing instances for a high performance in disam-
biguation. 
4) The discriminability of some ambiguous 
word may be low, but the corresponding EPs 
could be easier to disambiguate. For example, 
the ambiguous word "?" has two senses which 
are difficult to distinguish from each other, but 
its Eps' senses of "??/??/??" and "?/?/
?/?"can be easily disambiguated. It is the same 
for the word "??", whose Eps' senses are "?
?/?? /??" and "??/??". EP-based 
knowledge acquisition of these ambiguous words 
for WSD has helped a lot to achieve high per-
formance. 
5 Conclusion 
As discussed above, the supervised WSD method 
shows a low performance because of its depend-
ency on the size of the training data. This reveals 
its weakness in knowledge acquisition bottleneck. 
EP-based unsupervised method has overcame 
this weakness. It requires no manually tagged 
corpus to achieve a satisfactory performance on 
WSD. Experimental results show that EP-based 
method is a promising solution to the large-scale 
WSD task. In future work, we will examine the 
effectiveness of EP-based method in other WSD 
techniques. 
References 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1991. Word-
Sense Disambiguation Using Statistical Methods. 
In Proc. of the 29th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-1991), 
pages 264-270. 
Mona Talat Diab. 2003. Word Sense Disambiguation 
Within a Multilingual Framework. PhD thesis, 
University of Maryland College Park. 
Mona Diab. 2004a. Relieving the Data Acquisition 
Bottleneck in Word Sense Disambiguation. In Proc. 
of the 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL-2004), pages 303-
310. 
Mona T. Diab. 2004b. An Unsupervised Approach for 
Bootstrapping Arabic Sense Tagging. In Proc. of 
Arabic Script Based Languages Workshop at COL-
ING 2004, pages 43-50. 
Mona Diab and Philip Resnik. 2002. An Unsuper-
vised Method for Word Sense Tagging Using Par-
allel Corpora. In Proc. of the 40th Annual Meeting 
of the Association for Computational Linguistics 
(ACL-2002), pages 255-262. 
William Gale, Kenneth Church, and David Yarowsky. 
1992a. Using Bilingual Materials to Develop Word 
Sense Disambiguation Methods. In Proc. of the 4th 
International Conference on Theoretical and Meth-
odolgical Issues in Machine Translation(TMI-92), 
pages 101-112. 
William Gale, Kenneth Church, and David Yarowsky. 
1992b. Work on Statistical Methods for Word 
Sense Disambiguation. In Proc. of AAAI Fall Sym-
posium on Probabilistic Approaches to Natural 
Language, pages 54-60. 
William Gale, Kenneth Ward Church, and David 
Yarowsky. 1992c. Estimating Upper and Lower 
Bounds on the Performance of Word Sense Disam-
biguation Programs. In Proc. of the 30th Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-1992), pages 249-256. 
Tanja Gaustad. 2001. Statistical Corpus-Based Word 
Sense Disambiguation: Pseudowords vs. Real Am-
biguous Words. In Proc. of the 39th ACL/EACL, 
Student Research Workshop, pages 61-66. 
Nancy Ide, Tomaz Erjavec, and Dan Tufi?. 2001. 
Automatic Sense Tagging Using Parallel Corpora. 
In Proc. of the Sixth Natural Language Processing 
Pacific Rim Symposium, pages 83-89. 
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. 
Sense Discrimination with Parallel Corpora. In 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions, pages 54-60. 
Cong Li and Hang Li. 2002. Word Translation Dis-
ambiguation Using Bilingual Bootstrapping. In 
Proc. of the 40th Annual Meeting of the Association 
463
for Computational Linguistics (ACL-2002), pages 
343-351. 
Rada Mihalcea and Dan Moldovan. 2000. An Iterative 
Approach to Word Sense Disambiguation. In Proc. 
of Florida Artificial Intelligence Research Society 
Conference (FLAIRS 2000), pages 219-223. 
Rada F. Mihalcea. 2002. Bootstrapping Large Sense 
Tagged Corpora. In Proc. of the 3rd International 
Conference on Languages Resources and Evalua-
tions (LREC 2002), pages 1407-1411. 
Preslav I. Nakov and Marti A. Hearst. 2003. Cate-
gory-based Pseudowords. In Companion Volume to 
the Proceedings of HLT-NAACL 2003, Short Pa-
pers, pages 67-69. 
Hwee Tou. Ng, Bin Wang, and Yee Seng Chan. 2003. 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In Proc. of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 455-462. 
Zheng-Yu Niu, Dong-Hong Ji, and Chew-Lim Tan. 
2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. In 
Proc. of the 43th Annual Meeting of the Association 
for Computational Linguistics (ACL-2005), pages 
395-402. 
Ying Qin and Xiaojie Wang. 2005. A Track-based 
Method on Chinese WSD. In Proc. of Joint Sympo-
sium of Computational Linguistics of China (JSCL-
2005), pages 127-133. 
Hinrich. Schutze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1): 97-
123. 
David Yarowsky. 1994. Decision Lists for Lexical 
Ambiguity Resolution: Application to Accent Res-
toration in Spanish and French. In Proc. of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics(ACL-1994), pages 88-95. 
David Yarowsky. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. In 
Proc. of the 33rd Annual Meeting of the Association 
for Computational Linguistics (ACL-1995), pages 
189-196. 
 
464
Coling 2010: Poster Volume, pages 294?302,
Beijing, August 2010
A Novel Method for Bilingual Web Page Acquisition from 
Search Engine Web Records 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin Yao, Qiaoming Zhu 
School of Computer Science & Technology, Soochow University 
{20094227002, hongy, 20074227065071, jyao, qmzhu}@suda.edu.cn 
Abstract
A new approach has been developed 
for acquiring bilingual web pages from 
the result pages of search engines, 
which is composed of two challenging 
tasks. The first task is to detect web 
records embedded in the result pages 
automatically via a clustering method 
of a sample page. Identifying these 
useful records through the clustering 
method allows the generation of highly 
effective features for the next task 
which is high-quality bilingual web 
page acquisition. The task of 
high-quality bilingual web page 
acquisition is a classification problem. 
One advantage of our approach is that it 
is search engine and domain 
independent. The test is based on 2516 
records extracted from six search 
engines automatically and annotated 
manually, which gets a high precision 
of 81.3% and a recall of 94.93%. The 
experimental results indicate that our 
approach is very effective. 
1 Introduction 
There have been extensive studies on parallel 
resource extraction from parallel monolingual 
web pages of some bilingual web sites (Chen 
and Nie, 2000; Resnik and Smith, 2003; Zhang 
et al, 2006; Shi et al, 2006). Candidate parallel 
web pages are acquired by making use of URL 
strings or HTML tags, then the translation 
equivalence of the candidate pairs are verified 
via content-based features.  
  However, we observe that bilingual 
resources may exist not only in two parallel 
monolingual web pages, but also in single 
bilingual web pages. For example, many news 
web pages and English learning pages are 
bilingual. Based on this observation, 
researchers have proposed methods to improve 
parallel sentences extraction within a bilingual 
web page. Jiang (2009) uses an adaptive 
pattern-based method to mine interesting 
bilingual data based on the observation that 
bilingual data usually appears collectively 
following similar patterns. Because the World 
Wide Web is composed of billions of pages, it 
is a challenging task to locate valuable 
bilingual pages. 
  To acquire bilingual web pages 
automatically, a novel and effective method is 
proposed in this paper by making use of search 
engines, such as Baidu (http://www.baidu.com). 
By submitting parallel sentence pairs to the 
given search engine, lots of result pages with 
web records are returned, most of which are 
linked to bilingual web pages. We first identify 
and extract all result records automatically by 
selecting and analyzing a sample page with a 
clustering method, and then select high-quality 
bilingual web pages from candidates with 
classification algorithms. 
Our method has the following advantages: 
  1. Former researchers extract parallel corpus 
from specific bilingual web sites. Since search 
engines index amounts of web pages, and we 
aim to acquire bilingual pages based on them, 
our method expands the corpus source greatly. 
  2. For one search engine, only one sample 
result page is used to generate the record 
wrapper. Then the wrapper is used to identify 
web records from other result pages of the same 
search engine. Compared with existing data 
record extraction technologies, such as MDR 
(Liu et al, 2003; Zhai and Liu, 2006), our 
method is more effective and efficient. 
294
  3. We model the issue of verification 
bilingual pages as a binary-class classification 
problem. The records acquired automatically 
and annotated manually are utilized to train and 
test the classifier. This work is domain and 
search engine independent. That is to say, the 
records acquired from any search engine in any 
domain are used indiscriminately as training 
and testing dataset. 
  The rest of the paper is organized as follows. 
Related works are introduced in section 2. 
Section 3 provides an overview of our solution. 
The work about bilingual page acquisition and 
verification is introduced in section 4 and 5. 
Section 6 presents the experiments and results. 
Finally section 7 concludes the paper. 
2 Related Work 
As far as we know, there is no publication 
available on acquiring bilingual web pages. 
Most existing studies, such as Nie (1999), 
Resnik and Smith (2003) and Shi (2006), mine 
parallel web documents within bilingual web 
sites first and then extract bilingual sentences 
from mined parallel documents using sentence 
alignment method. 
  In this paper, the candidate bilingual web 
pages are acquired by analyzing web records 
embedded in the search engines? result pages. 
Therefore, record extraction from result pages 
is a critical technique in our method. Many 
researches, such as Laender (2002), have been 
developed various solutions in web information 
extraction from kinds of perspectives. 
  Earlier web information extraction systems 
(Baumgartner et al, 2001; Liu et al, 2000; Zhai 
and Liu, 2005) require users to provide labeled 
data so that the extraction rules could be learned. 
Yet such semi-automatic methods are not 
scalable enough to the whole Web which 
changes at any time. That?s why more and more 
researchers focus on fully or nearly fully 
automatic solutions.  
  Structured data objects are normally database 
records retrieved from underlying web 
databases and displayed on the web pages with 
some fixed templates, so automatic extraction 
methods try to find such patterns and use them 
to extract more data. Several approaches have 
succeeded to address the problem automatically 
without human assistance. IEPAD (Chang and 
Lui, 2001) identifies sub-strings that appear 
many times in a document. By traversing the 
DOM tree of the Web page, MDR extracts the 
data-rich sub-tree indirectly by detecting the 
existence of multiple similar generalized-nodes. 
The key limitation is its greedy manner of 
identifying a data region. DEPTA (Zhai and Liu, 
2005) uses visual information (locations on the 
screen at which the tags are rendered) to infer 
the structural relationship among tags and to 
construct a tag tree. NET (Liu and Zhai, 2005) 
extracts flat or nested data records by 
post-order or pre-order traversal of the tag tree. 
ViNTs (Zhao et al, 2005) considers the web 
page as a tag tree, and utilizes both visual 
content features as well as tag tree structures. It 
assumes that data records are located in a 
minimum data-rich sub-tree and separated by 
separators of tag forests. Zhao (2006) explicitly 
aims at extracting all dynamic sections from 
web pages, and extracting records in each 
section, whereas ViNTs focuses on record 
extraction from a single section. Miao (2009) 
figures out how tag paths format the whole page. 
Compared with the previous method, it 
compares pairs of tag path occurrence patterns 
to estimate how likely these tag paths represent 
the same list of objects instead of comparing 
one pair of individual sub-trees in the record. It 
brings some noise. We follow this method and 
make appropriate improvement for our task. 
3 Basic Concepts and Overview 
3.1 Basic Concepts 
Some basic concepts are introduced below. 
Figure 1. An example of search engine return
295
Tag Path: The path of a tag consists of all 
nodes from the tree root <html> to itself. We 
use tag path to specify the location of the tag. 
The tag paths are classified into two types: text 
tag paths and non-text tag paths. 
  Data Record: When a page is considered as 
strings of tokens, data records are enwrapped 
by one or more tag paths, which compose the 
visually repeating pattern in a page. This paper 
aims to extract such structured data records that 
are produced by computer programs following 
some fixed templates, while whose contents are 
usually retrieved from backend databases. For 
example, there are four records in Figure 1. 
3.2 Method Overview 
We can get much more bilingual web pages by 
submitting parallel sentence pairs to the search 
engine than submitting monolingual queries. 
Based on this observation, our work is as 
shown in Figure 2. The algorithm consists of 
two steps: 1) Record wrapper generation. By 
submitting parallel sentence pairs to search 
engines, result pages containing lots of web 
records are returned. In order to generate record 
wrappers, we select and analyze a sample page 
and then apply clustering method to tag paths 
with similar patterns. We apply these wrappers 
to extract more records, which are linked to 
candidate bilingual web pages. 2) High-quality 
bilingual page acquisition. In order to acquire 
high-quality bilingual pages from candidates, a 
binary classifier is constructed to decide 
whether the candidate pages are bilingual or not. 
In order to improve the classifier, some useful 
resources are used, such as a dictionary and 
translation equivalents. 
However, a result page often contains some 
information irrelevant to the query, such as 
information related to the hosting site of the 
search engine, which increases the difficulty of 
record extraction. Besides, there are also many 
irrelevant records irrelevant to the query. So 
our focus is to acquire plenty of features to 
filter out the irrelevant pages from the 
candidates.
In this paper, the first result page is chosen as 
the sample page and Affinity Propagation (AP) 
clustering is used. The reason lies in Frey and 
Dueck (2007), which proves that to produce the 
groups of tag paths; the AP algorithm does not 
require the three restrictions: 1) the samples 
must be of a specific kind, 2) the similarity 
values must be in a specific range, and 3) the 
similarity matrix must be symmetric. In order 
to decide the type of a page, the Support 
Vector Machines (SVM) (Cortes and Vapnik, 
1995) classifier on Fuzzy C-means is 
constructed combining with word-overlap, 
length and frequency measures. SVM is 
well-fitted to treat such classification problems 
that involve interrelated features likes ours, 
while most probabilistic classifiers, such as 
Na?ve Bayes classifier, strongly assume feature 
independence (DuVerle and Prendinger, 2009). 
Figure 2. Overview of the method 
4 Bilingual Page Acquisition 
4.1 Result Page Extraction 
The result pages of a search engine consist of a 
ranked list of document summaries linked to the 
actual documents or web pages. A web 
document summary typically contains the title 
and URL of the web page, links to live and 
cached versions of the page and, most 
importantly, a short text summary, or a snippet, 
to convey the contents of the page. Such 
snippets embedded in result pages of search 
engines are query-dependent summaries. White 
(2001) finds the result pages are sensitive to the 
content and language of the query. If the query 
is monolingual, the returned search results are 
mostly monolingual, while the result pages are 
bilingual if the query is bilingual. In order to 
acquire more bilingual web pages, we submit 
parallel translation pairs. Figure 1 gives an 
example result page from Baidu, in which the 
snapshot consists of four records related to the 
query, which consists of ?I see.? and its 
translation ???????. The results have 
296
more effective advantages than submitting the 
query ?I see.? or ??????? respectively. 
4.2 Clustering With Path Similarity 
Given a web page, we get the occurrence 
positions of each tag path the same as the 
sequence in the preorder traversal of the page?s 
DOM tree. Certainly, there are many tag paths 
which appear several times in the whole page. 
So an inverted mapping from HTML tag paths 
to their positions is built easily. For example, 
there are 599 tag paths formatting the sample 
page in Figure 1, and after the inverted mapping, 
we acquire 86 unique tag paths in all. Only tick 
off one part of the results as shown in Table 1, 
where Pi represents the ith unique tag path, and 
the vector Si is defined to store the occurrence 
positions of Pi in the third column.  
  As introduced above, detecting visually 
repeating tag paths is a clustering problem. 
Above all, a factor in determining the clustering 
performance is the choice of similarity 
functions, which captures the likelihood that 
two data samples belong to the same cluster. In 
our case, the similarity scores between two tag 
paths aim to capture how their positions are 
close to each other and how they interleave 
each other. 
  With the purpose of characterizing how close 
two tag paths appear, we only acquire the 
distance between paths? average positions, 
which is easy to obtain by the acquired 
occurrence vectors. For example, the average 
position of P11 and P15 in Table 1 is 227 and 
215, so the distance between them is 12. 
L UniqueTag  Path (Pi)
Occurrences (Si) of Pi
1 \html 1 
3 \html\head\#text 3,4,7,8,9 
9 \html\body\table 
84,93,115,146,180, 
217,258,292,335,372,
406,437 
11 \html\body\table\tr
15,85,94,116,147,181,
218,259,293,336,373,
407,438 
14
\html\body\table\tr
\td\#text 
18,21,24,27,55,79,87,
91,97,111,113 
15
\html\body\table\tr
\td\a 
19,88,118,149,183, 
220,261,295,338,375,
409,440 
Table 1. Unique tag paths of the sample page 
  However, the most difficult problem is how 
to capture the interleaving characteristic 
between two tag paths. Before doing that, 
another vector Oi is produced. Oi(k) indicates 
whether the tag path Pi occurs in the position k 
or not by its value. In addition, the value is 
binary that 0 or 1, and 0 shows Pi doesn?t occur 
in the position k, while 1 shows the opposite. Of 
particular note, the length of each Oi is equal to 
the total number of HTML tags that formatting 
the whole web page. Take the tag path P3
(?\html\head\#text?) in Table 1 as an example, 
whose position vector O3 is (0, 0, 1, 1, 0, 0, 1, 1, 
1, 0? 0), and the vector?s length is 599, 
because there are totally 599 tag paths 
formatting the sample page in Figure 1. 
  Based on the position vectors, we capture 
how tag path Pi and Pj interleave each other by 
a segment 
ji
OOD /  of Oi divided by Oj. We 
aim to find such tag paths that divide each other 
in average. In other words if the variance of 
counts in the segment 
ji
OOD /  is stable, they 
are likely to be grouped in the same cluster. So, 
we define the interleaving measureP in terms 
of the variances of 
ji
OOD /  and ij OOD /  as: 
)}( ),(   max{),( // ijji OOOOji DVarDVarOO  P (1)
where
ji
OOD /  is acquired by Oj as follows: if 
value of Oj(k) is 1, Oi(k) is a separator to
segment itself into several regions. The value of 
every element in the segment is the count of Pi
that occurs in every region, which is the 
number of 1 in the region. 
Figure 3. An Example of tag paths 
  In addition, there may be many consecutive 
separators in Oi, and we integrate them into one. 
Besides, the segment is a non-empty set. So if 
there is no occurrence of Pi in one region, we 
297
will ignore this special region. Figure 3 shows 
three tag paths. P1 and P2 are likely to belong 
to the same cluster because of their regular 
occurrences, whereas the occurrences of P3 are 
comparatively irregular. By our method, 
31 / OO
D  = {1, 1, 1} and 
13 /OO
D = {1, 2, 1}. We 
integrate separators once and ignore an empty 
region in the process of getting
31 / OO
D .
  Both the score of the closeness measure and 
the interleaving measure for any two tag paths 
are non-negative real numbers. And a smaller 
value of either measure indicates a high 
frequency that the two tag paths appear 
regularly. The measure ),( ji PPV  defined 
below is inversely proportional of these two 
measures. 
HP
HV
u
 
),(),(
),(
jiji
ji OOSSc
PP (2)
where H  is a non-negative term that avoids 
dividing by 0 and normalizes the similarity 
value so that it falls into the range (0, 1]. In our 
experiment, we choose H = 10. By Equation 2, 
we calculate the similarity value of any pair of 
tag paths. As expected, the pairwise similarity 
matrix is fed into the AP clustering algorithm 
directly, and each cluster acquired from AP 
clustering contains n tag paths, which indicates 
that those n paths appear repeatedly together 
with high frequency, and the tag paths that have 
no remarkable relation are spilt into different 
clusters. For the given sample page in Figure 1, 
the number of identified clusters is 16.  
  We observe that HTML code of most data 
records contain more than three HTML tags, so 
we only examine the clusters containing four or 
more visual signals. In the clustering result of 
sample page in Figure 1, there are three 
clusters? sizes less than four. Meanwhile, we 
also note that: 
 1. The feature page of a common search 
engine usually contains 10 or more web records 
with similar layout pattern. So we define a 
threshold T=3. If an ancestor tag path doesn?t 
occur more than T times, we believe these tag 
path dose not lead a record.  
 2. Usually the content of the result pages 
returned by search engines is completely related 
to the queries, which means the data records 
that we are interested in are distributed in the 
whole page as main component. So the 
occurrence position of valuable tag paths must 
be global optimization. In this paper, the scope 
between beginning and ending occurrence must 
be wider than three quarters of the length of the 
web page. 
  Thus, we get essential clusters fit with above 
observations, which is denoted by C= {C1,
C2?CM}. Once we have the essential clusters, 
we apply them in new web page of the same 
search engine to identify data records. 
4.3 Data Record Extraction  
Based on the essential clusters, we extract the 
exact data records from the real content of text 
tag path that follow the ancestor tag path.  
  In order to describe the extraction process in 
details, we firstly define DaI as the child tag 
paths of an ancestor tag path Pa, and suppose 
that (Pos1? Posi? Posm) is the occurrence 
vector of Pa, which means at each position Posi
the tag path Pi occurs. Da(i) is such a tag path set 
that the position Pos of every path in it is Posi
<Pos<Posi+1. In the meantime, such path strings 
must begin with the same prefix of Pa. Such as 
in Table 2, Da(i) contains tag paths from Posi to 
Posi+1-1, and we obtain the ith records 
embedded in the result pages by acquiring the 
real content of all text tag paths in Da(i).
Occurrence 
of Pa
DaI of 
Pa
Child tag path 
Pos1 Pa:\html\body\table\tr 
Pos1+1 Pt:\html\body\table\tr\?
?? ?? 
Pos2-1 
Da(1)
Pk: ?? 
? ? ?
Posi Pa:\html\body\table\tr 
Posi+1 Pt:\html\body\table\tr\?
  ?? ?? 
Posi+1-1 
Da(i)
Pn: ?? 
? ? ?
Posm Pa:\html\body\table\tr 
  ?? 
Da(m)
 ?? 
Table 2. Collection of child tag paths for 
ancestor tag path
298
5 Bilingual Web Page Verification 
Based on the previous work, we capture a list of 
records based on a holistic analysis of a result 
page, and each record contains snippets and 
URLs related to the query. In this section, we 
aim to decide whether the candidate pages that 
returned records are linked to are bilingual or 
not by putting some statistical features 
(collected from snippets) into an effective SVM 
classification. 
  To the acquired snippets, some necessary 
preprocessing is made before we acquire 
useful features. We remove most of the noise 
that affect the precision and robustness of the 
entire system by such methods as recovery of 
abbreviation words, deletion of noisy words, 
amendment for half or full punctuations and 
simplified or traditional characters, and so on. 
  The snippet is described with more regular 
contents after preprocessing. We cut the 
snippet into several segments by its language. 
Each segment of the snippet is just represented 
in one language, which is either English or 
Chinese in this paper and different from its 
adjacent segments. So the source snippets are 
transferred into such language strings that 
consist of C and E, where C stands for Chinese 
and E stands for English. It is unlikely that 
continuous C or E exists in the same language 
string. We store the real text Tc (Te) that each C 
(E) stands for. We take the snippet ?I see. ??
???I quit! ?????? as example, its 
language string is ?ECEC? and real text string 
is TeTcTeTc, where the two Te stand for ?I see? 
and ?I quit?, the two Tc stand for ??????
and ??????.
  Note that different feature functions for the 
classifier will lead to different results, it is 
important to choose feature functions that will 
help to discriminate different classes. In this 
paper, the SVM classifier involves 
word-overlap, length and frequency features. 
We define these three features based on the 
snippet itself as follows: 
(1) Word-Overlap measure  
  Word overlap judges the similarity of 
Chinese term and English term. In this paper, 
we acquire the word-overlap score between any 
two adjacent language segments. The similarity 
Score(c_res,e_res) of Chinese term and English 
term is based on word-overlap as following: 
1 1
( ( , ))
( _ , _ )
p
i j
i j q
Max Sim c e
Score c res e res
I
 d d 
?
 (3) 
where the denominator is normalization factor, 
and in our experiment we select p+q as its value, 
where p stands for the length of Chinese term 
and q stands for the length of English term. In 
addition, ci stands for the ith word of Chinese 
term and ej stands for the jth word of English 
term. Sim(ci,ej) in Liu (2003) and Deng (2004) 
stands for the similarity of Chinese word ci and
English word ej.
  In our experiment, the Chinese and English 
sub-snippets are equivalent to Chinese and 
English sentences of the bilingual pages. In the 
segmented snippet, with regard to each 
sub-snippet T, which is at even position in the 
language string, we separately evaluate the 
intermediate score for snippet T with its left 
and right neighbors by Equation 3. Especially 
when T doesn?t have right or left neighbor, the 
score for T with its null neighbor is 0. So for 
every sub-snippet that needs to be scored the 
word-overlap score, there are two candidate 
scores with its adjacent neighbors. Then we 
choose the higher value as one item of an 
intermediate result vector. Either the length of 
the language string is 2 u n or 2 u n+1, the 
length of intermediate vector is n, and the final 
score is computed as follows: 
mn
InV
sScore
n
k
k
u
 
?
 1)( (4)
where Score(s) stands for the final score of 
snippet s on the word-overlap measure, and 
vector InV is the intermediate result vector as 
mentioned before. The length of the vector InV
is n, and m is the number of its items that is not 
equal to zero. m/n is used as a useful measure 
of length, because it indicates how many 
parallel pairs are there in the same snippet. 
(2) Length-Based measure  
  We acquire three scores about length 
measure. Take the language string ?ECECEC? 
as example, we use ?E1C1E2C2E3C3? to replace 
it for simple description. We acquire one score 
of the length measure as follows: 
)(
))()((
)( 1
sLen
eLencLen
sScore
m
i
?
 

 (5)
299
where s and m stand for the same as in Equation 
4. In addition, c and e stands for such 
sub-snippet that Score(c,e) contributes to 
?
 
n
k
kInV
1
. The function Len(s) is to compute the 
number of words in the sentence. 
We acquire the length of language string. If 
the length is too long or too short, the 
associated web page is unlikely to be a 
bilingual page. At the same time, we are not 
interested in some language strings although 
the lengths of them are appropriate. So we also 
store the variances of lengths about each 
sub-snippet. 
(3) Frequency-Based measure 
  According to the result pages, queries often 
occur in the title, snippet, or advertisements. 
They are highlighted to make them easier to 
identify. Hence we aim to acquire the 
frequency of the query in one whole snippet as 
a feature. 
  Based on the three measures above, a 
number of records (containing snippets and 
URLs) for training and testing can be converted 
them into a 6-dimensional feature space. In our 
experiments, nonlinear SVM with Gaussian 
Radial Basis Function (RBF) kernel is used. 
The performance of the SVM classifier 
indicates that it is a reliable way to verify 
whether the page is bilingual or not by the 
content of snippet. 
6 Experiments and Results 
6.1 The Data Set 
To acquire enough experimental data, we 
collect from Google, Baidu, Yahoo, Youdao, 
Bing and Tecent Soso, and the effectiveness of 
our algorithm is evaluated based on the data set 
from these six search engines. 
  Result records of search engines are 
collected by program and by human beings 
with submitting different queries respectively. 
They are used for checking the performance of 
record extraction. When evaluating the method 
of verification bilingual web pages, 2300 
records (60% are positive instances) are 
chosen for training the SVM classifier, and 
other 230 are selected randomly as test records 
from the whole record set. 
  The training data is annotated by human in 
two methods. The first method is motivated by 
the content of each source snippet. The 
annotators assign the type of web pages by 
scanning the text of every snippet. If the snippet 
contains many parallel term pairs, we annotate 
the page as bilingual or monolingual if not 
parallel. We also use another annotation 
method, which is to reach the URL by the 
Internet Explorer. By checking the content of 
the real web page, annotators decide the type of 
the candidate pages. And the biggest difference 
between the two public hand-classified dataset 
appears when some snippets of candidate 
pages have no clues in their content to predict 
classifications. 
6.2 Evaluation On Bilingual Page 
Acquisition
The entire system is evaluated by measuring 
the performance of the binary SVM classifier. 
And how the classifier performance changes 
with three features is shown in Table 3, where 
W, L and F separately stand for the 
word-overlap, length and frequency measures. 
  In order to improve the performance of 
word-overlap measure, we use not only the 
bilingual dictionary but also translation 
equivalents, which are extracted from parallel 
corpora. Because the bilingual dictionary 
doesn?t contain all necessary entries, the 
classifier with only word-overlap measure 
accepts many wrong pairs.  
Feature W W +L W +L+F
Precision 70.2% 81.02% 85.10% 
Table 3. SVM Classifier Performance changes 
with more features added to the classifier 
Table 3 shows that the length feature and the 
frequency feature have a significant effect on 
bilingual web page verification because of the 
natural relationship among queries, snippets 
and true web pages.  
#1 #2 
N
P(%) R(%) P(%) R(%)
1 85.1 92.3 75% 84.8 
2 80.7 95.1 72.8 85.7 
3 78.1 97.4 71.0 93.0 
aver 81.3 94.93 72.93 87.83
Table 4. Performance versus training data types 
300
  Three experiments of verification bilingual 
web pages based on two different training 
datasets are conducted whose results are 
shown in Table 4. #1 stands for the data set 
annotated by snippets, and #2 stands for the 
training data annotated by URLs. Precision and 
recall are used to evaluate our method. The 
average precision based on training dataset #2 
is 73%, which is lower than the precision of 
81.3% resulting from the dataset #1, because in 
many cases, some snippets are weakly related 
with real text in the real pages introduced by 
search engine summarization algorithm. From 
the table, we also see that the recalls in dataset 
#1 and #2 are both relatively high, which 
means our classifier can select high-quality 
bilingual pages with high accuracy. 
6.3 Evaluation On Web Record 
Extraction
Record extraction has significant effect on 
bilingual web page collection. A useful 
intermediate evaluation of the whole scheme is 
conducted by measuring the performance of 
record extraction.  
  We built a prototype system to test the 
algorithm of record extraction based on the 
clustering of similar records. On a laptop with a 
Pentium M 1.7G processor, the process of 
constructing records wrapper for a given search 
engine is done in 10 to 30 seconds. Once the 
wrapper is built, the record extraction from a 
new result page is done in a small fraction of a 
second.
  In order to test the robustness of the 
generated wrapper, we compare the records 
extracted by our method with the test records 
acquired manually. The precision and recall 
measures are used to evaluate the result. 98% 
of all the records are extracted by program, 
with a precision of 99%. The precision 
indicates that the generated wrappers in our 
experiment are quite robust to acquire records. 
The recall is lower than the precision, which 
indicates that it sometimes misses a few records. 
The reason for this is that in the extraction step, 
the records different from more common ones 
are eliminated.  
  We compare our performance with the work 
in Zhao (2006), which addresses the issue of 
differentiating dynamic sections and records 
based on the sample result pages. It generates 
section wrappers by identifying section 
boundary markers in nine steps. It is more 
complicated in computation than ours because 
it renders each result page and extracts its 
content lines by a traversal of the DOM tree, 
while we use tag structure of a page. The 
accordance is making full use of the sample 
pages for given search engines. The method 
also gets a high precision of 98.8% and a recall 
of 98.7%.  
7 Conclusion
The paper presents a novel method to acquire 
bilingual web pages automatically via search 
engines. In order to improve the efficiency and 
effectiveness, the snippets of search engines 
rather than the contents of the massive pages 
are analyzed to locate bilingual pages. 
Bilingual web page verification is modeled as 
a classification problem with word-overlap, 
length and frequency measures. Based on the 
similarity of HTML structures, AP clustering 
is used to extract web records from result 
pages of search engines. Experiments show 
that our algorithm has good performance in 
precision and recall. 
  As a valuable resource for up-to-date 
bilingual terms and sentences, bilingual web 
pages are counterpart to parallel monolingual 
web pages. Our method brings an efficient and 
effective solution to bilingual language 
engineering. 
References 
Adelberg B., NoDoSE. 1998. A tool for semi- 
  Automatically extracting structured and sem- 
  istructured data from text documents. In:
  Proc.ACM SIGMOD Conference on  man- 
  agement of Data, Seattle, WA (1998).
Baumgartner R., S. Flesca and G. Gottlob.2001. 
  Visual Web Information Extraction with 
  Lixto. Proceedings of the 27th International  
  Conference on Very Large Data Bases,
   pp.119-128, September 11-14, 2001  
Chang C., S. Lui. 2001. Information Extraction  
  based on Pattern Discovery. In Proceedings
  of the 10th international conference on 
  World Wide Web. pp.681-688, May 01-05, 
  2001, Hong Kong. 
Chen Jiang and Jian-Yun Nie. 2000. Web 
301
  Parallel text mining for Chinese-English 
  cross-language information retrieval. Proce-
  edings of RIAO2000 Content-Based Multi- 
  media Information Access, CID, Paris
Cortes, C. and V. Vapnik. 1995. Support-vector 
  network. Machine Learning 20, pp.273-297. 
Deng Dan. 2004. Research on Chinese-English 
  word alignment. Institute of Computing 
  Technology Chinese Academy of Sciences,
  Master Thesis. (in Chinese). 
DuVerle David, Helmut Prendinger. 2009. A  
  Novel Discourse Parser Based on Support 
  Vector Machine Classification. The 47th  
  Annual Meeting of the Association for 
  Computational Linguistics. pp. 665-673 
Frey B. J. and D. Dueck. 2007. Clustering by 
  passing messages between data points. 
Science, 315(5814):972-976. 
Laender A, B. Ribeiro-Neto, A. da Silva, J.  
  Teixeira. 2002. A Brief Survey of Web Data 
  Extraction Tools. ACM SIGMOD Record.
Volume 31, Number 2.
Liu B. and Y. Zhai. 2005. System for extracting 
  Web data from flat and nested data records.  
  In Proceedings of the Conference on Web  
  Information Systems Engineering,
  pp.487-495. 
Liu B., R. Grossman and Y. Zhai. 2003. Mining 
  Data Records in Web Pages. In Proceedings
  of the ninth ACM SIGKDD international  
  conference on Knowledge Discovery and  
  Data mining, Washington, D.C, pp.601-606. 
Liu Feifan, Jun Zhao, Bo Xu. 2003. Building 
  Large-Scale Domain Independent Chinese-  
  English Bilingual Corpus and the Researches 
  on Sentence Alignment. Joint Symposium on 
  Computational Linguistics.
Liu L., C. Pu and W. Han. 2000. An XML- 
  Enabled Wrapper Construction System for 
  Web Information Sources. Proceedings of  
  the 16th International Conference on Data  
  Engineering, pp.611. 
Long Jiang, Shiquan Yang, Ming Zhou, Xiao- 
  hua Liu and Qingsheng Zhou. 2009. Mining 
  Bilingual Data from the Web with Adaptive- 
  ly Learnt Patterns. The 47th Annual Meeting 
  of the Association for Computational Lingui- 
  stics. pp. 870-878 (2009) 
Miao Gengxin, Junichi Tatemura, Wang-Pin 
  Hsiung, Arsany Sawires, Louise E. Moser.  
  2009. Extracting data records from the web  
  using tag path clustering. In Proceedings of  
  the 18th International Conference on World 
  Wide Web, Spain, Madrid. 
Nie Jian-Yun, Michel Simard, Pierre Isabelle, 
Richard Durand 1999. Cross-Language 
Information Retrieval based on Parallel 
Texts and Automatic Mining of Parallel 
Texts in the Web. SIGIR-1999; 74-81. 
Resnik Philip and Noah A. Smith. 2003. The  
  web as a Parallel Corpus. Computational  
  Linguistics.
Shi Lei, Cheng Niu, Ming Zhou, and Jianfeng 
  Gao. 2006. A DOM Tree Alignment Model  
  for Mining Parallel Data from the Web. In  
Joint Proceedings of the Association for  
  Computational Linguistics and the Internati- 
  onal Conference on Computational Linguist- 
  ics, Sydney, Australia. 
White, R., Jose, J. & Ruthven, R. 2001.Query- 
  biased web page summarisation: a task- 
  oriented evaluation. In Proceedings of the 
  24th ACM SIGIR Conference on Research  
  and Development of Information Retrieval.
  New Orleans, Louisiana, United States, pp.  
  412-413. 
Zhai Y., B. Liu. 2005. Extracting Web Data  
  Using Instance-Based Learning. Web Infor- 
  mation Systems Engineering.
Zhai Y., B. Liu. 2005. Web Data Extraction 
  Based on Partial Tree Alignment. In 
Proceedings of the 14th international  
  conference on World Wide Web. May 10-14, 
  2005, Chiba, Japan.
Zhang Ying, Ke Wu, Jianfeng Gao, Phil Vines. 
  2006. Automatic Acquisition of Chinese- 
  English Parallel Corpus from the web. In 
Proceedings of 28th European Conference  
  on Information Retrieval.
Zhao H., W. Meng, Z. Wu, V. Raghavan, C.  
  Yu. 2006. Automatic Extraction of Dynamic 
  Record Sections From Search Engine Result 
  Pages. In Proceedings of the 32nd Internatio- 
nal conference on Very large databases.
302
Coling 2010: Poster Volume, pages 436?444,
Beijing, August 2010
 
ABSTRACT 
Re-ranking for Information Retrieval 
aims to elevate relevant feedbacks and 
depress negative ones in initial retrieval 
result list. Compared to relevance feed-
back-based re-ranking method widely 
adopted in the literature, this paper pro-
poses a new method to well use three 
features in known negative feedbacks to 
identify and depress unknown negative 
feedbacks. The features include: 1) the 
minor (lower-weighted) terms in negative 
feedbacks; 2) hierarchical distance (HD) 
among feedbacks in a hierarchical clus-
tering tree; 3) obstinateness strength of 
negative feedbacks. We evaluate the 
method on the TDT4 corpus, which is 
made up of news topics and their relevant 
stories. And experimental results show 
that our new scheme substantially out-
performs its counterparts. 
1. INTRODUCTION 
When we start out an information retrieval jour-
ney on a search engine, the first step is to enter a 
query in the search box. The query seems to be 
the most direct reflection of our information 
needs. However, it is short and often out of stan-
dardized syntax and terminology, resulting in a 
large number of negative feedbacks. Some re-
searches focus on exploring long-term query logs 
to acquire query intent. This may be helpful for 
obtaining information relevant to specific inter-
ests but not to daily real-time query intents. Es-
pecially it is extremely difficult to determine 
whether the interests and which of them should 
be involved into certain queries. Therefore, given 
a query, it is important to ?locally? ascertain its 
intent by using the real-time feedbacks. 
Intuitively it is feasible to expand the query 
using the most relevant feedbacks (Chum et al, 
2007). Unfortunately search engines just offer 
?farraginous? feedbacks (viz. pseudo-feedback) 
which may involve a great number of negative 
feedbacks. And these negative feedbacks never 
honestly lag behind relevant ones in the retrieval 
results, sometimes far ahead because of their 
great literal similarity to query. These noisy 
feedbacks often mislead the process of learning 
query intent.  
For so long, there had no effective approaches 
to confirm the relevance of feedbacks until the 
usage of the web click-through data (Joachims et 
al., 2003). Although the data are sometimes in-
credible due to different backgrounds and habits 
of searchers, they are still the most effective way 
to specify relevant feedbacks. This arouses re-
cent researches about learning to rank based on 
supervised or semi-supervised machine learning 
methods, where the click-through data, as the 
direct reflection of query intent, offer reliable 
training data to learning the ranking functions. 
Although the learning methods achieve sub-
stantial improvements in ranking, it can be found 
that lots of ?obstinate? negative feedbacks still 
permeate retrieval results. Thus an interesting 
question is why the relevant feedbacks are able 
to describe what we really need, but weakly repel 
what we do not need. This may attribute to the 
inherent characteristics of pseudo-feedback, i.e. 
their high literal similarity to queries. Thus no 
matter whether query expansion or learning to 
rank, they may fall in the predicament that ?fa-
voring? relevant feedbacks may result in ?favor-
ing? negative ones, and that ?hurting? negative 
feedbacks may result in ?hurting? relevant ones. 
However, there are indeed some subtle differ-
ences between relevant and negative feedbacks, 
e.g. the minor terms (viz. low-weighted terms in 
texts). Although these terms are often ignored in 
Negative Feedback: The Forsaken Nature Available for Re-ranking
Yu Hong, Qing-qing Cai, Song Hua, Jian-min Yao, Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
jyao@suda.edu.cn 
436
relevance measurement because their little effect 
on mining relevant feedbacks that have the same 
topic or kernel, they are useful in distinguishing 
relevant feedbacks from negative ones. As a re-
sult, these minor terms provides an opportunity 
to differentiate the true query intent from its 
counterpart intents (called ?opposite intents? 
thereafter in this paper). And the ?opposite in-
tents? are adopted to depress negative feedbacks 
without ?hurting? the ranks of relevant feedbacks. 
In addition, hierarchical clustering tree is helpful 
to establish the natural similarity correlation 
among information. So this paper adopts the hi-
erarchical distance among feedbacks in the tree 
to enhance the ?opposite intents? based division 
of relevant and negative feedbacks. Finally, an 
obstinateness factor is also computed to deal 
with some obstinate negative feedbacks in the 
top list of retrieval result list. In fact, Teevan 
(Teevan et al, 2008) observed that most search-
ers tend to browse only a few feedbacks in the 
first one or two result pages. So our method fo-
cuses on improving the precision of highly 
ranked retrieval results.  
The rest of the paper is organized as follows. 
Section 2 reviews the related work. Section 3 
describes our new irrelevance feedback-based 
re-ranking scheme and the HD measure. Section 
4 introduces the experimental settings while Sec-
tion 5 reports experimental results. Finally, Sec-
tion 6 draws the conclusion and indicates future 
work. 
2. RELATED WORK 
Our work is motivated by information search 
behaviors, such as eye-tracking and click through 
(Joachims, 2003). Thereinto, the click-through 
behavior is most widely used for acquiring query 
intent. Up to  present, several interesting fea-
tures, such as click frequency and hit time on 
click graph (Craswell et al, 2007), have been 
extracted from click-through data to improve 
search results. However, although effective on 
query learning, they fail to avoid the thorny 
problem that even when the typed query and the 
click-through data are the same, their intents may 
not be the same for different searchers.  
A considerable number of studies have ex-
plored pseudo-feedback to learn query intent, 
thus refining page ranking. However, most of 
them focus on the relevant feedbacks. It is until 
recently that negative ones begin to receive some 
attention. Zhang (Zhang et al, 2009) utilize the 
irrelevance distribution to estimate the true rele-
vance model. Their work gives the evidence that 
negative feedbacks are useful in the ranking 
process. However, their work focuses on gener-
ating a better description of query intent to attract 
relevant information, but ignoring that negative 
feedbacks have the independent effect on repel-
ling their own kind. That is, if we have a king, 
we will not refuse a queen. In contrast, Wang 
(Wang et al, 2008) benefit from the independent 
effect from the negative feedbacks. Their method 
represents the opposite of query intent by using 
negative feedbacks and adopts that to discount 
the relevance of each pseudo-feedback to a query. 
However, their work just gives a hybrid repre-
sentation of opposite intent which may overlap 
much with the relevance model. Although an-
other work (Wang et al, 2007) of them filters 
query terms from the opposite intent, such filter-
ing makes little effect because of the sparsity of 
the query terms in pseudo-feedback. 
Other related work includes query expansion, 
term extraction and text clustering. In fact, query 
expansion techniques are often the chief benefi-
ciary of click-through data (Chum et al, 2007). 
However, the query expansion techniques via 
clicked feedbacks fail to effectively repel nega-
tive ones. This impels us to focus on un-clicked 
feedbacks. Cao (Cao et al, 2008) report the ef-
fectiveness of selecting good expansion terms for 
pseudo-feedback. Their work gives us a hint 
about the shortcomings of the one-sided usage of 
high-weighted terms. Lee (Lee et al, 2008) adopt 
a cluster-based re-sampling method to emphasize 
the core topic of a query. Their repeatedly feed-
ing process reveals the hierarchical relevance of 
pseudo-feedback. 
3. RE-RANKING SCHEME 
3.1 Re-ranking Scheme 
The re-ranking scheme, as shown in Figure 1, 
consists of three components: acquiring negative 
feedbacks, measuring irrelevance feedbacks and 
re-ranking pseudo-feedback. 
Given a query and its search engine results, we 
start off the re-ranking process after a trigger 
point. The point may occur at the time when 
searchers click on ?next page? or any hyperlink. 
437
All feedbacks before the point are assumed to 
have been seen by searchers. Thus the un-clicked 
feedbacks before the point will be treated as the 
known negative feedbacks because they attract 
no attention of searchers. This may be questioned 
because searchers often skip some hyperlinks 
that have the same contents as before, even if the 
links are relevant to their interests. However, 
such skip normally reflects the true searching 
intent because novel relevant feedbacks always 
have more attractions after all. 
 
Figure 1. Re-ranking scheme 
Another crucial step after the trigger point is to 
generate the opposite intent by using the known 
negative feedbacks. But now we temporarily 
leave the issue to Section 3.2 and assume that we 
have obtained a good representation of the oppo-
site intent, and meanwhile that of query intent 
has been composed of the highly weighted terms 
in the known relevant feedbacks and query terms. 
Thus, given an unseen pseudo-feedback, we can 
calculate its overall ranking score predisposed to 
the opposite intent as follows: 
          scoreIscoreOscoreR ___ ??= ?        (1) 
where the O_score is the relevance score to the 
opposite intent, I_score is that to the query intent 
and ?  is a weighting factor. On the basis, we 
re-rank the unseen feedbacks in ascending order. 
That is, the feedback with the largest score ap-
pears at the bottom of the ranked list. 
It is worthwhile to emphasize that although the 
overall ranking score, i.e. R_score, looks similar 
to Wang (Wang et al, 2008) who adopts the in-
versely discounted value (i.e. the relevance score 
is calculated as -scoreI _ scoreO _?? ) to re-rank 
feedbacks in descending order, they are actually 
quite different because our overall ranking score 
as shown in Equation (1) is designed to depress 
negative feedbacks, thereby achieving the similar 
effect to filtering. 
3.2 Representing Opposite Intent 
It is necessary for the representation of opposite 
intent to obey two basic rules: 1) the opposite 
intent should be much different from the query 
intent; and 2) it should reflect the independent 
effect of negative feedbacks. 
Given a query, it seems easy to represent its 
opposite intent by using a vector of 
high-weighted terms of negative feedbacks. 
However, the vector is actually a ?close relative? 
of query intent because the terms often have 
much overlap with that of relevant feedbacks. 
And the overlapping terms are exactly the source 
of the highly ranked negative feedbacks. Thus 
we should throw off the overlapping terms and 
focus on the rest instead.  
In this paper, we propose two simple facilities 
in representing opposite intent. One is a vector of 
the weighted terms (except query terms) occur-
ring in the known negative feedbacks, named as 
)( qO ? , while another further filters out the 
high-weighted terms occurring in the known 
relevant feedbacks, named as . Although )( rqO ??
)( qO ?  filters out query terms, the terms are so 
sparse that they contribute little to opposite intent 
learning. Thus, we will not explore  fur-
ther in this paper (Our preliminary experiments 
confirm our reasoning). In contrast,  not 
only differs from the representation of query in-
tent due to its exclusion of query terms but also 
emphasize the low-weighted terms occurring in 
negative feedbacks due to exclusion of 
high-weighted terms occurring in the known 
relevant feedbacks. 
)( qO ?
)( rqO ??
3.3 Employing Opposite Intent 
Another key issue in our re-ranking scheme is 
how to measure the relevance of all the feed-
backs to the opposite intent, i.e. O_score, thereby 
the ranking score R_score. For simplicity, we 
only consider Boolean measures in employing 
opposite intent to calculate the ranking score 
R_score. 
Assume that given a query, there are  
known relevant feedbacks and 
N
N  known nega-
tive ones. First, we adopt query expansion to ac-
quire the representation of query intent. This is 
done by pouring all terms of the  relevant 
feedbacks and query terms into a bag of words, 
where all the occurring weights of each term are 
N
438
accumulated, and extracting n top-weighted 
terms to represent the query intent as )( rqI ++ . 
Then, we use the N  negative feedbacks to rep-
resent the n-dimensional opposite intents 
. For any unseen pseudo-feedback u, we 
also represent it using an n-dimensional vector 
 which contains its n top-weighted terms. In 
all the representation processes, the TFIDF 
weighting is adopted. 
)( rqO ??
)(uV
Thus, for an unseen pseudo-feedback u, the 
relevance scores to the query intent and the op-
posite intent can be measured as: 
                   (2) 
}  )(  ),(  {)(_
}  )(  ),(  {)(_
rqOuVBuscoreO
rqIuVBuscoreI
??=
++=
where  indicates Boolean calculation: },{ ??B
                       (3) 
??
?
?
?=
?=?
Yxif
Yxif
Yxb
XxYxbYXB
i
i
i
ii
        ,0
         ,1
},{
  },,{},{
In particular, we simply set the factor ? , as 
mentioned in Equation (1), to 1 so as to balance 
the effect of query intent and its opposite intent 
on the overall ranking score. The intuition is that 
if an unseen pseudo-feedback has more overlap-
ping terms with )( rqO ??  than , it will 
has higher probability of being depressed as an 
negative feedback. 
)( rqI ++
Two alternatives to the above Boolean meas-
ure are to employ the widely-adopted VSM co-
sine measure and Kullback-Liebler (KL) diver-
gence (Thollard et al, 2000). However, such 
term-weighting alternatives will seriously elimi-
nate the effect of low-weighted terms, which is 
core of our negative feedback-based re-ranking 
scheme.  
3.4 Hierarchical Distance (HD) Measure  
The proposed method in Section 3.3 ignores 
two key issues. First, given a query, although 
search engine has thrown away most opposite 
intents, it is unavoidable that the 
pseudo-feedback still involves more than one 
opposite intent. However, the representation 
 has the difficulty in highlighting all the 
opposite intents because the feature fusion of the 
representation smoothes the independent charac-
teristics of each opposite intent. Second, given 
several opposite intents, they have different lev-
els of effects on the negative score . 
And the effects cannot be measured by the uni-
lateral score.  
)( rqO ??
)(_ uscoreO
 
Figure 2. Weighted distance calculation 
To solve the issues, we propose a hierarchical 
distance based negative measure, abbr. HD, 
which measures the distances among feedbacks 
in a hierarchical clustering tree, and involves 
them into hierarchical division of relevance score. 
Given two random leaves u and v in the tree, 
their HD score is calculated as: 
             
),(
),(
),(_
vuW
vurel
vuscoreHD =           (4) 
where ),( ??rel  indicates textual similarity, ),( ??W  
indicates the weighted distance in the tree, which 
is calculated as: 
                ?
?
=
mi
i vuwvuW ),(),(              (5) 
where m is the total number of the edges between 
two leaves,  indicates the weight of the 
i-th edge. In this paper, we adopt CLUTO to 
generate the hierarchical binary tree, and simply 
let each  equal 1. Thus the 
),( ??iw
),( ??iw ),( ??W  be-
comes to be the number of edges m, for example, 
the  equals 5 in Figure 2. ),( kjW
On the basis, given an unseen feedback u, we 
can acquire its modified re-ranking score 
scoreR _ ?  by following steps. First, we regard 
each known negative feedback as an opposite 
intent, following the two generative rules (men-
tioned in section 3.2) to generate its 
n-dimensional representation . Addition-
ally we represent both the known relevant feed-
backs and the unseen feedback u as 
n-dimensional term vectors. Second, we cluster 
these feedbacks to generate a hierarchical binary 
tree and calculate the HD score for each pair of 
)( rqO ??
),( ?u , where ?  denotes a leaf in the tree except u. 
Thus the modified ranking score is calculated as: 
? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ (6) 
where iv  indicates the i-th known negative 
feedback in the leaves, N  is the total number of 
439
v , j  indicates the j-th known relevant feed-
back,  is the total number of 
v
N v . Besides, we 
still adopt Boolean value to measure the textual 
similarity  in both clustering process and 
ranking score calculation, thus the HD score in 
the formula (6) can be calculated as follows: 
),( ??rel
     
),(
)(_
),(_                   
),(
}  )(  ),(  {
),(_                
vuW
uscoreO
vuscoreHD
vuW
vVuVB
vuscoreHD
=
=
       (7) 
3.5 Obstinateness Factor 
Additionally we involve an interesting feature, 
i.e. the obstinate degree, into our re-ranking 
scheme. The degree is represented by the rank of 
negative feedbacks in the original retrieval re-
sults. That is, the more ?topping the list? an 
negative feedback is, the more obstinate it is.  
Therefore we propose a hypothesis that if a 
feedback is close to the obstinate feedback, it 
should be obstinate too. Thus given an unseen 
feedback u, its relevance to an opposite intent in 
HD can be modified as: 
          )(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?        (8) 
where  indicates the rank of the opposite 
intent in original retrieval results (Note: in HD, 
every known negative feedback is an opposite 
intent), 
rnk
?  is a smoothing factor. Because as-
cending order is used in our re-ranking process, 
by the weighting coefficient, i.e. )/1( rnk?+ , the 
feedback close to the obstinate opposite intents 
will be further depressed. But the coefficient is 
not commonly used. In HD, we firstly ascertain 
the feedback closest to u, and if the feedback is 
known to be negative, set to maxv , we will use 
the Equation (8) to punish the pair of (u, maxv ) 
alone, otherwise without any punishment. 
4. EXPERIMENTAL SETTING 
4.1 Data Set 
We evaluate our methods with two TDT collec-
tions: TDT 2002 and TDT 2003. There are 3,085 
stories in the TDT 2002 collection are manually 
labeled as relevant to 40 news topics, 30,736 
ones irrelevant to any of the topics. And 3,083 
news stories in the TDT 2003 collection are la-
beled as relevant to another 40 news topics, 
15833 ones irrelevant to them. In our evaluation, 
we adopt TDT 2002 as training set, and TDT 
2003 as test set. Besides, only English stories are 
used, both Mandarin and Arabic ones are re-
placed by their machine-translated versions (i.e. 
mttkn2 released by LDC). 
Corpus good fair poor 
TDT 2002 26 7 7 
TDT 2003 22 10 8 
Table 1. Number of queries referring to different 
types of feedbacks (Search engine: Lucene 2.3.2) 
In our experiments, we realize a simple search 
engine based on Lucene 2.3.2 which applies 
document length to relevance measure on the 
basis of traditional literal term matching. To 
emulate the real retrieval process, we extract the 
title from the interpretation of news topic and 
regard it as a query, and then we run the search 
engine on the TDT sets and acquire the first 1000 
pseudo-feedback for each query. All feedbacks 
will be used as the input of our re-ranking proc-
ess, where the hand-crafted relevant stories de-
fault to the clicked feedbacks. By the search en-
gine, we mainly obtain three types of 
pseudo-feedback: ?good?, ?fair? and ?poor?, 
where ?good? denotes that more than 5 clicked 
(viz. relevant) feedbacks are in the top 10, ?fair? 
denotes more than 2 but less than 5, ?poor? de-
notes less than 2. Table 1 shows the number of 
queries referring to different types of feedbacks. 
4.2 Evaluation Measure 
We use three evaluation measures in experiments, 
P@n, NDCG@n and MAP. Thereinto, P@n de-
notes the precision of top n feedbacks. On the 
basis, NDCG takes into account the influence of 
position to precision. NDCG at position n is cal-
culated as: 
      
n
n
i
ur
n Z
iNDCG
Z
nNDCG
i?= +?=?= 1
)(
)1log(
12
@
1
@    (9) 
where i is the position in the result list, Zn is a 
normalizing factor and chosen so that for the 
perfect list DCG at each position equals one, and 
r(ui) equals 1 when ui is relevant feedback, else 0. 
While MAP additionally takes into account recall, 
calculated as:  
        ? ?= = ?= mi kj ijii jpurRmMAP 1 1 ))@()((11    (10) 
where m is the total number of queries, so MAP 
gives the average measure of precision and recall 
440
for multiple queries, Ri is the total number of 
feedbacks relevant to query i, and k is the num-
ber of pseudo-feedback to the query. Here k is 
indicated to be 1000, thus Map can give the av-
erage measure for all positions of result list. 
4.3 Systems 
We conduct experiments using four main sys-
tems, in which the search engine based on Lu-
cene 2.3.2, regarded as the basic retrieval system, 
provides the pseudo-feedback for the following 
three re-ranking systems. 
Exp-sys: Query is expanded by the first N known 
relevant feedbacks and represented by an 
n-dimensional vector which consists of n distinct 
terms. The standard TFIDF-weighted cosine 
metric is used to measure the relevance of the 
unseen pseudo-feedback to query. And the rele-
vance-based descending order is in use. 
Wng-sys: A system realizes the work of Wang 
(Wang et al, 2008), where the known relevant 
feedbacks are used to represent query intent, and 
the negative feedbacks are used to generate op-
posite intent. Thus, the relevance score of a feed-
back is calculated as I_scorewng- O_score?w? wng, 
and the relevance-based descending order is used 
in re-ranking. 
Our-sys: A system is approximately similar to 
Wng-sys except that the relevance is measured by 
O_scoreour- ?? I_scoreour and the pseudo-feedback 
is re-ranked in ascending order.  
Additionally both Wng-sys and Our-sys have 
three versions. We show them in Table 2, where 
?I? corresponds to the generation rule of query 
intent, ?O? to that of opposite intent, Rel. means 
relevance measure, u is an unseen feedback, v is 
a known relevant feedback, v  is a known nega-
tive feedback. 
5. RESULTS 
5.1 Main Training Result 
We evaluate the systems mainly in two circum-
stances: when both  and N N  equal 1 and 
when they equal 5. In the first case, we assume 
that retrieval capability is measured under given 
few known feedbacks; in the second, we emulate 
the first page turning after several feedbacks 
have been clicked by searchers. Besides, the ap-
proximately optimal value of n for the Exp-sys, 
which is trained to be 50, is adopted as the global 
value for all other systems. The training results 
are shown in Figure 3, where the Exp-sys never 
gains much performance improvement when n is 
greater than 50. In fairness to effects of ?I? and 
?O? on relevance measure, we also make n  
equal 50. In addition, all the discount factors 
(viz.? , ? w2 and ? w3) initially equal 1, and the 
smoothing factor ?  is trained to be 0.5. 
Table 2. All versions of both Wngs and Ours 
 
Figure 3. Parameter training of Exp-sys 
For each query we re-rank all the 
pseudo-feedback, including that defined as 
known, so P@20 and NDCG@20 are in use to 
avoid over-fitting (such as P@10 and 
NDCG@10 given both  and N N  equal 5 ). 
We show the main training results in Table 3, 
where our methods achieve much better per-
formances than the re-ranking methods based on 
relevant feedback learning when N= N =5. 
Thereinto, our basic system, i.e. Our-sys1, at 
least achieves approximate 5% improvement on 
P@20, 3% on NDCG@20 and 1% on MAP than 
the optimal wng-sys (viz. wng-sys1). And obvi-
?I? n-dimensional vector for each v, Number of v in use is N
?O? None 
Wng-sys1
Rel. NvuscoreR
N
i
w /)),cos((_
1
1 ?==  
?I?
Number of v in use is N, all v combine into a n-dimensional 
bag of words bw2
?O?
Number of v  in use is N , all v combine into a 
n-dimensional words bag 2wb  
Wng-sys2
Rel. ),cos(),cos(_ 2222 wwww bubuscoreR ??= ?  
?I?
?O?
Similar generation rules to Wng-sys2 except that query 
terms are removed from bag of words  and 3wb 3wb  Wng-sys3
Rel. ),cos(),cos(_ 3333 wwww bubuscoreR ??= ?  
?I? )( rqI ++  in section 3.3 
?O? )( rqO ??  in section 3.2 Our-sys1
Rel. scoreIscoreOscoreR ___ ??= ?  
?I?
?O?
The same generation rules to Our-sys1 
Our-sys2
Rel.
HD algorithm: ? ?
? ?
?=?
Ni Nj
ji vuscoreHDIvuscoreHDIscoreR ),(_),(__ 
?I?
?O?
The same generation rules to Our-sys1 
Our-sys3
Rel.
HD algorithm + obstinateness factor: 
)(_)1()(_ uscoreO
rnk
uscoreO ?+=? ?  
441
ously the most substantial improvements are 
contributed by the HD measure which even in-
creases the P@20 of Our-sys1 by 8.5%, 
NDCG@20 by 13% and MAP by 9%. But it is 
slightly disappointing that the obstinateness fac-
tor only has little effectiveness on performance 
improvement, although Our-sys3 nearly wins 
the best retrieval results. This may stem from 
?soft? punishment on obstinateness, that is, for 
an unseen feedback, only the obstinate com-
panion closest to the feedback is punished in 
relevance measure. 
Table 3. Main training results 
It is undeniable that all the re-ranking systems 
work worse than the basic search engine when 
the known feedbacks are rare, such as =N N =1. 
This motivates an additional test on the higher 
values of both  and N N ( =N N =9), as shown 
in Table 4. Thus it can be found that most of the 
re-ranking systems achieve much better per-
formance than the basic search engine. An im-
portant reason for this is that more key terms can 
be involved into representations of both query 
intent and its opposite intent. So it seems that 
more manual intervention is always reliable. 
However in practice, seldom searchers are will-
ing to use an unresponsive search engine that can 
only offer relatively satisfactory feedbacks after 
lots of click-through and page turning. And in 
fact at least two pages (if one page includes 10 
pseudo-feedback) need to be turned in the train-
ing corpus when both  and N N  equal 9. So 
we just regard the improvements benefiting from 
high click-through rate as an ideal status, and 
still adopt the practical numerical value of  
and 
N
N , i.e. =N N =5, to run following test. 
5.2 Constraint from Query 
A surprising result is that Exp-sys always 
achieves the worst MAP value, even worse than 
the basic search engine even if high value of N is 
in use, such as the performance when N equal 9 
in Table 4. It seems to be difficult to question the 
reasonability of the system because it always 
selects the most key terms to represent query in-
tent by query expansion. But an obvious differ-
ence between Exp-sys and other re-ranking sys-
tems could explain the result. That is the query 
terms consistently involved in query representa-
tion by Exp-sys. 
Table 4. Effects of  and N N  on re-ranking 
performance (when =N N =9, n= n =50) 
In fact, Wng-sys1 never overly favor the query 
terms because they are not always the main body 
of an independent feedback, and our systems 
even remove the query terms from the opposite 
intent directly. Conversely Exp-sys continuously 
enhances the weights of query terms which result 
in over-fitting and bias. The visible evidence for 
this is shown in Figure 4, where Exp-sys 
achieves better Precision and NDCG than the 
basic search engine at the top of result list but 
worse at the subsequent parts. The results illus-
trate that too much emphasis placed on query 
terms in query expansion is only of benefit to 
elevating the originally high-ranked relevant 
feedback but powerless to pull the straggler out 
of the bottom of result list.  
 
Figure 4. MAP comparison (basic vs Exp) 
5.3 Positive Discount Loss 
Obviously Wang (Wang et al, 2008) has noticed 
the negative effects of query terms on re-ranking. 
Therefore his work (reproduced by Wng-sys1, 2, 
3 in this paper) avoids arbitrarily enhancing the 
terms in query representation, even removes 
them as Wng-sys3. This indeed contributes to the 
- Our-sys1 Our-sys2 Exp-sys Wng-sys1 Basic 
P@20 0.6603 0.8141 0.63125 0.7051 0.6588
NDCG@20 0.7614 0.8587 0.8080 0.7797 0.6944
MAP 0.6583 0.7928 0.5955 0.7010 0.6440
systems N = N P@20 NDCG@20 MAP Factor 
Basic - 0.6588 0.6944 0.6440 - 
1 0.4388 0.4887 0.3683 - Exp-sys 
5 0.5613 0.6365 0.5259 - 
1 0.5653 0.6184 0.5253 - Wng-sys1
5 0.6564 0.7361 0.6506 - 
1 0.5436 0.6473 0.4970 2w? =1Wng-sys2
5 0.5910 0.7214 0.5642 2w? =1
1 0.5436 0.6162 0.4970 3w? =1Wng-sys3
5 0.5910 0.6720 0.5642 3w? =1
1 0.5628 0.6358 0.4812 ? =1 Our-sys1
5 0.7031 0.7640 0.6603 ? =1 
1 0.6474 0.6761 0.5967 ? =1 Our-sys2
5 0.7885 0.8381 0.7499 ? =1 
1 0.6026 0.6749 0.5272 ? =0.5Our-sys3
5 0.7897 0.8388 0.7464 ? =0.5
442
improvement of the re-ranking system, such as 
the better performances of Wng-sys1, 2, 3 shown 
in Table 3, although Wng-sys3 has no further 
improvement than Wng-sys2 because of the spar-
sity of query terms. On the basis, the work re-
gards the terms in negative feedbacks as noises 
and reduces their effects on relevance measure as 
much as possible. This should be a reasonable 
scheme, but interestingly it does not work well in 
our experiments. For example, although 
Wng-sys2 and Wng-sys3 eliminate the relevance 
score calculated by using the terms in negative 
feedbacks, they perform worse than Wng-sys1 
which never make any discount. 
systems ?? =0.5 ?? =1 ?? =2 
Our-sys1 0.4751 0.6603 0.6901 
Wng-sys2 0.6030 0.5642 0.4739 
Wng-sys3 0.6084 0.5642 0.4739 
Table 5. Effects on MAP  
 Additionally when we increase the discount 
factor 2w?  and 3w? , as shown in Table 5, the 
performances (MAP) of Wng-sys2 and Wng-sys3 
further decrease. This illustrates that the 
high-weighted terms of high-ranked negative 
feedbacks are actually not noises. Otherwise why 
do the feedbacks have high textual similarity to 
query and even to their neighbor relevant feed-
backs? Thus it actually hurts real relevance to 
discount the effect of the terms. 
Conversely Our-sys1 can achieve further im-
provement when the discount factor ?  in-
creases, as shown in Table 5. It is because the 
discount contributes to highlighting minor terms 
of negative feedbacks, and these terms always 
have little overlap with the kernel of relevant 
feedbacks. Additionally the minor terms are used 
to generate the main body of opposite intent in 
our systems, thus the discount can effectively 
separate opposite intent from positive query rep-
resentation. Thereby we can use relatively pure 
representation of opposite intent to detect and 
repel subsequent negative feedbacks. 
5.4 Availability of Minor Terms 
Intuitively we can involve more terms into query 
representation to alleviate the positive discount 
loss. But it does not work in practice. For exam-
ple, Wng-sys2 shown in Figure 5 has no obvious 
improvement no matter how many terms are in-
cluded in query representation. Conversely 
Our-sys1 can achieve much more improvement 
when it involves more terms into the opposite 
intent. For example, when the number of terms 
increases to 150, Our-sys1 has approximately 5% 
better MAP than Wng-sys2, shown in Figure 5. 
 
Figure 5. Effects on MAP in modifying the di-
mensionality n (when N= N =5, ? =1) 
 This result illustrates that minor terms are 
available for repelling negative feedbacks, but 
too weak to recall relevant feedbacks. In fact, the 
minor terms are just the low-weighted terms in 
text. Current text representation techniques often 
ignore them because of their marginality. How-
ever minor terms can reflect fine distinctions 
among feedbacks, even if they have the same 
topic. And the distinctions are of great impor-
tance when we determine why searchers say 
?Yes? to some feedbacks but ?No? to others. 
Table 6. Main test results 
5.5 Test Result 
We run all systems on test corpus, i.e. TDT2003, 
but only report four main systems: Wng-sys1, 
Our-sys1, Our-sys2 and Our-sys3. Other systems 
are omitted because of their poor performances. 
The test results are shown in Table 6 which in-
cludes not only global performances for all test 
queries but also local ones on three distinct types 
of queries, i.e. ?good?, ?fair? and ?poor?. There-
into, Our-sys2 achieves the best performance 
around all types of queries. So it is believable 
systems metric good fair poor global Factor
P@20 0.7682 0.5450 0.2643 0.6205
NDCG@20 0.8260 0.6437 0.4073 0.7041Wng-sys1
MAP 0.6634 0.4541 0.9549 0.6620
- 
P@20 0.8273 0.5700 0.2643 0.6603
NDCG@20 0.8679 0.6620 0.4017 0.7314Our-sys1
MAP 0.6740 0.4573 0.9184 0.6623
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8199 0.4180 0.7894Our-sys2
MAP 0.7148 0.6313 0.9897 0.7427
? =2,
? =0.5
P@20 0.8523 0.7600 0.2714 0.7244
NDCG@20 0.8937 0.8200 0.4180 0.7894Our-sys3
MAP 0.7145 0.6292 0.9897 0.7420
? =2,
? =0.5
443
that hierarchical distance of clustering tree al-
ways plays an active role in distinguishing nega-
tive feedbacks from relevant ones. But it is sur-
prising that Our-sys3 achieves little worse per-
formance than Our-sys2. This illustrates poor 
robustness of obstinateness factor. 
Interestingly, the four systems all achieve very 
high MAP scores but low P@20 and NDCG@20 
for ?poor? queries. This is because the queries 
have inherently sparse relevant feedbacks: less 
than 6? averagely. Thus the highest p@20 is 
only approximate 0.3, i.e. 6/20. And the low 
NDCG@20 is in the same way. Besides, all 
MAP scores for ?fair? queries are the worst. We 
find that this type of query involves more mac-
roscopic features which results in more kernels 
of negative feedbacks. Although we can solve 
the issue by increasing the dimensionality of op-
posite intent, it undoubtedly impairs the effi-
ciency of re-ranking.  
6. CONCLUSION 
This paper proposes a new re-ranking scheme 
to well explore the opposite intent. In particular, 
a hierarchical distance-based (HD) measure is 
proposed to differentiate the opposite intent from 
the true query intent so as to repel negative 
feedbacks. Experiments show substantial out-
performance of our methods. 
Although our scheme has been proven effec-
tive in most cases, it fails on macroscopic queries. 
In fact, the key difficulty of this issue lies in how 
to ascertain the focal query intent given various 
kernels in pseudo-feedback. Fortunately, 
click-through data provide some useful informa-
tion for learning real query intent. Although it 
seems feasible to generate focal intent represen-
tation by using overlapping terms in clicked 
feedbacks, such representation is just a reproduc-
tion of macroscopic query since the overlapping 
terms can only reflect common topic instead of 
focal intent. Therefore, it is important to segment 
clicked feedbacks into different blocks, and as-
certain the block of greatest interest to searchers.  
References 
Allan, J., Lavrenko, V., and Nallapati, R. 2002. 
UMass at TDT 2002, Topic Detection and 
Tracking: Workshop. 
Craswell, N., and Szummer, M. Random walks on 
the click graph. 2007. In Proceedings of the 
Conference on Research and Development in 
Information Retrieval. SIGIR '30. ACM Press, 
New York, NY, 239-246. 
Cao, G. H., Nie, J. Y., and Gao, J. F. 2008. Stephen 
Robertson. Selecting Good Expansion Terms for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 243-250. 
Chum, O., Philbin, J., Sivic, J., and Zisserman, A. 
2007. Automatic query expansion with a genera-
tive feature model for object retrieval. In Pro-
ceedings of the 11th International Conference on 
Computer Vision, Rio de Janeiro, Brazil, 1?8. 
Joachims, T., Granka, L., and Pan, B. 2003. Accu-
rately Interpreting Clickthrough Data as Implicit 
Feedback. In Proceedings of the Conference on 
Research and Development in Information Re-
trieval. SIGIR '28. New York, NY, 154-161. 
Lee, K. S., Croft, W. B., and Allan, J. 2008 A Clus-
ter-Based Resampling Method for 
Pseudo-Relevance Feedback. In Proceedings of 
the Conference on Research and Development in 
Information Retrieval. SIGIR '31. ACM Press, 
New York, NY, 235-242. 
Thollard, F., Dupont, P., and Higuera, L.2000. 
Probabilistic DFA Inference Using Kull-
back-Leibler Divergence and Minimality. In 
Proceedings of the 17th Int'l Conf on Machine 
Learning. San Francisco: Morgan Kaufmann, 
975-982. 
Teevan, J. T., Dumais, S. T., and Liebling, D. J. 
2008. To Personalize or Not to Personalize: 
Modeling Queries with Variation in User Intent. 
In Proceedings of the Conference on Research 
and Development in Information Retrieval. 
SIGIR '31. New York, NY, 163-170. 
Wang, X. H., Fang, H., and Zhai, C. X. 2008. A 
Study of Methods for Negative Relevance Feed-
back. In Proceedings of the Conference on Re-
search and Development in Information Re-
trieval. SIGIR '31. ACM Press, New York, NY, 
219-226. 
Wang, X. H., Fang, H., and Zhai, C. X. 2007. Im-
prove retrieval accuracy for difficult queries us-
ing negative feedback. In Proceedings of the 
sixteenth ACM conference on Conference on 
information and knowledge management. ACM 
press, New York, NY, USA, 991-994. 
Zhang, P., Hou, Y. X., and Song, D. 2009. Ap-
proximating True Relevance Distribution from a 
Mixture Model based on Irrelevance Data. In 
Proceedings of the Conference on Research and 
Development in Information Retrieval. SIGIR 
'31. ACM Press, New York, NY, 107-114. 
444
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1216?1224,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
An Iterative Link-based Method for Parallel Web Page Mining 
Le Liu1, Yu Hong1, Jun Lu2, Jun Lang2, Heng Ji3, Jianmin Yao1 
1School of Computer Science & Technology, Soochow University, Suzhou, 215006, China 
2Institute for Infocomm Research, Singapore, 138632 
3Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA 
giden@sina.cn,{tianxianer,lujun59,billlangjun}@gmail.com 
jih@rpi.edu,jyao@suda.edu.cn 
 
Abstracts 
Identifying parallel web pages from bi-
lingual web sites is a crucial step of bi-
lingual resource construction for cross-
lingual information processing. In this 
paper, we propose a link-based approach 
to distinguish parallel web pages from bi-
lingual web sites. Compared with the ex-
isting methods, which only employ the 
internal translation similarity (such as 
content-based similarity and page struc-
tural similarity), we hypothesize that the 
external translation similarity is an effec-
tive feature to identify parallel web pages. 
Within a bilingual web site, web pages 
are interconnected by hyperlinks. The 
basic idea of our method is that the trans-
lation similarity of two pages can be in-
ferred from their neighbor pages, which 
can be adopted as an important source of 
external similarity. Thus, the translation 
similarity of page pairs will influence 
each other. An iterative algorithm is de-
veloped to estimate the external transla-
tion similarity and the final translation 
similarity. Both internal and external 
similarity measures are combined in the 
iterative algorithm. Experiments on six 
bilingual websites demonstrate that our 
method is effective and obtains signifi-
cant improvement (6.2% F-Score) over 
the baseline which only utilizes internal 
translation similarity. 
1 Introduction 
Parallel corpora have played an important role in 
multilingual Natural Language Processing, espe-
cially in Machine Translation (MT) and Cross-
lingual Information Retrieval(CLIR). However, 
it?s time-consuming to build parallel corpora 
manually. Some existing parallel corpora are 
subject to subscription or license fee and thus not 
freely available, while others are domain-specific. 
Therefore, a lot of previous research has focused 
on automatically mining parallel corpora from 
the web. 
In the past decade, there have been extensive 
studies on parallel resource extraction from the 
web (e.g., Chen and Nie, 2000; Resnik 2003; 
Jiang et al., 2009) and many effective Web min-
ing systems have been developed such as 
STRAND, PTMiner, BITS and WPDE. For most 
of these mining systems, there is a typical paral-
lel resource mining strategy which involves three 
steps: (1) locate the bilingual websites (2) identi-
fy  parallel web pages from these bilingual web-
sites and (3) extract bilingual resources from the 
parallel web pages.  
In this paper, we focus on the step (2) which is 
regarded as the core of the mining system 
(Chunyu, 2007). Estimating the translation simi-
larity of two pages is the most basic and key 
problem in this step. Previous approaches have 
tried to tackle this problem by using the infor-
mation within the pages. For example, in the 
STRAND and PTMiner system, a structural fil-
tering process that relies on the analysis of the 
underlying HTML structure of pages is used to 
determine a set of pair-specific structural values, 
and then the values are used to decide whether 
the pages are translations of one another. The 
BITS system filters out bad pairs by using a large 
bilingual dictionary to compute a content-based 
similarity score and comparing the score with a 
threshold. The WPDE system combines URL 
similarity, structure similarity with content-based 
similarity to discover and verify candidate paral-
lel page pairs. Some other features or rules such 
as page size ratio, predefined hypertexts which 
link to different language versions of a web page 
are also used in most of these systems. Here, all 
of the mining systems are simply using the in-
formation within the page in the process of find-
1216
ing parallel web pages. In this paper, we attempt 
to explore other information to identify parallel 
web pages. 
On the Internet, most web pages are linked by 
hyperlinks. We argue that the translation similar-
ity of two pages depends on not only their inter-
nal information but also their neighbors. The 
neighbors of a web page are a set of pages, 
which link to the page. We find that the similari-
ty of neighbors can provide more reliable evi-
dence in estimating the translation similarity of 
two pages.  
The main issues are discussed in this paper as 
follows:  
? Can the neighbors of candidate page pairs 
really contribute to estimating the translation 
similarity?  
? How to estimate the translation similarity of 
candidate page pairs by using their neighbors? 
Our method has the following advantages: 
High performance 
The external and internal information is com-
bined to verify parallel page pairs in our method, 
while in previous mining systems, only internal 
information was used. Experimental results show 
that compared with existing parallel page pair 
identification technologies, our method obtains 
both higher precision and recall (6.2% and 6.3% 
improvement than the baseline, respectively). In 
addition, the external information used in our 
method is a more effective feature than internal 
features alone such as structural similarity and 
content-based similarity. 
Language independent 
In principle, our method is language inde-
pendent and can be easily ported to new lan-
guage pairs, except for the language-specific bi-
lingual lexicons. Our method takes full ad-
vantage of the link information that is language-
independent. For the bilingual lexicons in our 
experiments, compared to previous methods, our 
method does not need a big bilingual lexicon, 
which is good news to less-resource language 
pairs. 
Unsupervised and fewer parameters 
In previous work, some parameters need to be 
optimized. Due to the diversity of web page 
styles, it is not trivial to obtain the best parame-
ters. Some previous researches(Resnik, 2003; 
Zhang et al., 2006) attempt to optimize parame-
ters by employing machine learning method. In 
contrast, in our method, only two parameters 
need to be estimated. One parameter remains 
stable for different style websites. Another pa-
rameter can be easily adjusted to achieve the best 
performance. Therefore, our method can be used 
in other websites with different styles, without 
much effort to optimize these parameters.  
2 Related Work 
A large amount of literature has been published 
on parallel resource mining from the web. Ac-
cording to the existing form of the parallel re-
source on the Internet, related work can be cate-
gorized as follows: 
Mining from bilingual websites 
Most existing web mining systems aimed at 
mining bilingual resource from the bilingual 
websites, such as PTMiner (Nie et al., 1999), 
STRAND (Resnik and Smith, 2003), BITS (Ma 
and Liberman, 1999), PTI (Chen et al., 2004). 
PTMiner uses search engines to pinpoint the 
candidate sites that are likely to contain parallel 
pages, and then uses the collected URLs as seeds 
to further crawl each web site for more URLs. 
Web page pairs are extracted based on manually 
defined URL pattern matching, and further fil-
tered according to several criteria. STRAND us-
es a search engine to search for multilingual 
websites and generated candidate page pairs 
based on manually created substitution rules. 
Then, it filters some candidate pairs by analyzing 
the HTML pages. PTI crawls the web to fetch 
(potentially parallel) candidate multilingual web 
documents by using a web spider. To determine 
the parallelism between potential document pairs, 
a filename comparison module is used to check 
filename resemblance, and a content analysis 
module is used to measure the semantic similari-
ty. BITS was the first to obtain bilingual web-
sites by employing a language identification 
module, and then for each bilingual website, it 
extracts parallel pages based on their content.  
Mining from bilingual web pages 
Parallel/bilingual resources may exist not only 
in two parallel monolingual web pages, but also 
in single bilingual web pages. Jiang et al. (2009) 
used an adaptive pattern-based method to mine 
interesting bilingual data based on the observa-
tion that bilingual data usually appears collec-
tively following similar patterns. They found that 
bilingual web pages are a promising source of 
up-to-date bilingual terms/sentences which cover 
many domains and application scenarios. In ad-
dition, Feng et al. (2010) proposed a new method 
1217
to automatically acquire bilingual web pages 
from the result pages of a search engine.  
Mining from comparable corpus 
Several attempts have been made to extract 
parallel resources from comparable corpora. 
Zhao et al. (2002) proposed a robust, adaptive 
approach for mining parallel sentences from a 
bilingual comparable news collection. In their 
method, sentence length models and lexicon-
based models were combined under a maximum 
likelihood criterion. Smith et al. (2010) found 
that Wikipedia contains a lot of comparable doc-
uments, and adopted a ranking model to select 
parallel sentence pairs from comparable docu-
ments. Bharadwaj et al. (2011) used a SVM clas-
sifier with some new features to identify parallel 
sentences from Wikipedia.  
3 Iterative Link-based Parallel Web 
Pages Mining 
As mentioned, the basic idea of our method is 
that the similarity of two pages can be inferred 
from their neighbors. This idea is illustrated in 
Figure 1.  
A D
E
C
B
A?
D?
E?
C?
B?
?
 
Figure 1 Illustration of the link-based method 
In Figure 1, A, B, C, D and E are some pages 
in the same language; while A?, B?, C?, D? and E? 
are some pages in another language. The solid 
black arrows indicate the links between these 
pages. For example, page A points to C, page B? 
points to C? and so on. Then the page set {A, B, 
D, E} is called the neighbors of page C. Similar-
ly, the page set {A?, B?, D?, E?} contains the 
neighbors of page C?. If the page pairs : <A, A?>, 
<B, B?>, <D, D?> and <E, E?> have high transla-
tion similarities, then it can be inferred that page 
C and C? have a high probability to be a pair of 
parallel pages. Every page has its own neighbors. 
For each web page, our method views link-in and 
link-out hyperlinks as the same. Thus, the linked 
pages will influence each other in estimating the 
translation similarity. For example, the similari-
ties of two pairs <A, A?> and <C, C?> will influ-
ence each other. It is an iterative process. We 
will elaborate the process in the following sec-
tions.  
Since our goal is to find parallel pages in a 
specific website, the key task is to evaluate the 
translation similarity of two pages (which are in 
different languages) as accurately as possible. 
The final similarity of two pages should depend 
both on their internal similarity and external sim-
ilarity. The internal similarity means the similari-
ty estimated by using the information in the page 
itself, such as the structure similarity and the 
content-based similarity of the two pages. On the 
other hand, the external similarity of two pages is 
the similarity depending on their neighbors. The 
final translation similarity is called the En-
hanced Translation Similarity (ETS). The ETS 
of two pages can be calculated as follows:  
   (   )        (   )  (   )  
                                      (   )   [   ]              (1) 
Where,    (   ) is the internal translation simi-
larity of two pages: e and c;     (   ) represents 
the external translation similarity of pages e and 
c.    (   ) indicates the final similarity of two 
pages, which combines the internal with external 
translation similarity. 
In this paper, we conduct the experiments on 
English-Chinese parallel page pair mining. How-
ever, our method is language-independent. Thus, 
it can be applied to other language pairs by only 
replacing a bilingual lexicon. The symbol e and c 
always indicate an English page and a Chinese 
page respectively in this paper. In the following 
sections, we will describe how to calculate the 
   (   ) and     (   ) step by step. 
3.1 Preprocessing 
The input of our method is a bilingual website. 
This paper aims to find English/Chinese parallel 
pages. So a 3-gram language model is used to 
identify (or classify) the language of a certain 
document. The performance of the language 
identification module achieves 99.5% accuracy 
through in-house testing. As a result, a set of 
English pages and a set of Chinese pages are ob-
tained. In order to get the neighbors of a page, 
for each bilingual website, two networks are con-
structed based on the hyperlinks, one for English 
pages and another for Chinese pages. 
3.2 The Internal Translation Similarity 
Following Resnik and Smith (2003), three fea-
tures are used to evaluate the internal translation 
similarity of two pages: 
1218
The size ratio of two pages 
The length ratio of two documents is the sim-
plest criterion for determining whether two doc-
uments are parallel or not. Parallel documents 
tend to be similar in length. And it is reasonable 
to assume that for text E in one language and text 
F in another language, length(E) ? C?length(F), 
where C is a constant that depends on the lan-
guage pair. Here, the content length of a web 
page is regarded as its length. 
The structure similarity of two pages 
The HTML tags describe and control a web 
page?s structure. Therefore, the structure similar-
ity of two pages can be calculated by their 
HTML tags. Here, the HTML tags of each page 
are extracted (except the visual tags such as ?B?, 
?FONT?.) as a linear sequence. Then the struc-
ture similarity of two pages is computed by com-
paring their linearized sequences. In this paper, 
the LCS algorithm (Dan, 1997) is adopted to find 
the longest common sequences of the two HTML 
tag sequences. The ratio of LCS length and the 
average length of two HTML tag sequences are 
used as the structure similarity of the two pages.  
The content-based translation similarity of 
two pages 
The basic idea is that if two documents are 
parallel, they will contain word pairs that are mu-
tual translations (Ma, 1999). So the percentage of 
translation word pairs in the two pages can be 
considered as the content-based similarity. The 
translation words of two documents can be ex-
tracted by using a bilingual lexicon. Here, for 
each word in English document, we will try to 
find a corresponding word in Chinese document.  
Finally, the internal translation similarity of 
two pages is calculated as follows: 
   (   )       (   )  (   )  
                                          (   )   [   ]        (2) 
Where,     (   )  and        (   )  are the con-
tent-based and structural similarity of page   and 
  respectively. In addition, the size ratio of two 
pages is used to filter invalid page pairs.  
3.3 The External and Enhanced Transla-
tion Similarity 
As described above, the external translation 
similarity of two pages depends on their neigh-
bors:  
    (   )     (  ( )   ( )) (3) 
Where, PG(x), a set of pages, is the neighbors of 
page x. Obviously, the similarity of two sets re-
lies on the similarity of the elements in the two 
sets. Here, the elements are namely web pages. 
So,     (   ) equals to    (  ( )   ( )), and 
   (  ( )   ( ))  depends on    (     ) 
(       belongs to    ( )   ( ) , respectively) 
and    (   ) . According to Equation (1), 
   (   )  depends on    (   )  and     (   ) . 
Therefore, it is a process of iteration.    (   ) 
will converge after a certain number of iterations. 
Thus,     (   )  is defined as the enhanced 
similarity of page   and   after the i-th iteration, 
and the same is for     
 (   ) and     (  ( ) 
  ( )) .     (  ( )   ( ))  is computed by 
the following algorithm: 
Algorithm 1: Estimating the external transla-
tion similarity 
Input:      ( )   ( ) 
Output:     
 (   ) 
Procedure:  
   ? 0 
     ?   ( ) 
      ?   ( ) 
While        and       are both not empty: 
             
?                          (   
   (   ))  
    ?     +        (   ) 
Remove   from        
Remove   from       
    
 (   )       ( ( )  ( )) 
                             (   ( )     ( ) ) 
Algorithm 2 Estimating the enhanced transla-
tion similarity 
Input:      , (the English and Chinese page set) 
Output:    (   )           
Initialization: Set ETS(e, c) random value or 
small value 
Procedure:  
LOOP: 
For each   in    : 
For each   in   : 
     (   )         
 (   ) 
                                          (   )     (   ) 
Parameters normalization 
UNTIL    (   ) is stable  
Algorithm 1 tries to find the real parallel pairs 
from   ( ) and   ( ). The similarity of   ( ) 
and   ( ) is calculated based on the similarity 
1219
values of these pairs. Finally,    (   ) is calcu-
lated by the following algorithm 2. 
In Algorithm 2, the input    and    are English 
and Chinese page sets in a certain bilingual web-
site. We use algorithm 2 to estimate the en-
hanced translation similarity. 
3.4 Find the Parallel Page Pairs 
At last, the enhanced translation similarity of 
every pair is obtained, and the parallel page pairs 
can be extracted in terms of these similarities: 
Algorithm 3 Finding parallel page pairs 
Input:       
    (   )              
       (or       ) 
Output:  Parallel Page Pairs List :     
Procedure:  
LOOP: 
                       (   (   )) 
Add       to     
Remove   from     
Remove   from     
UNTIL size of     >       (or    (   )  < 
       ) 
This algorithm is similar to Algorithm 1 in 
each bilingual website. The input      is an 
integer threshold which means that only top 
      page pairs will be extracted in a certain 
website. It needs to be noted that      is al-
ways less than      and     . While the input 
        is another kind of threshold that is 
used for extracting page pairs with high transla-
tion similarity.  
4 Experiments and Analysis 
4.1 Experimental setup 
Our experiments focus on six bilingual websites. 
Most of them are selected from HK government 
websites. All the web pages were retrieved by 
using a web site download tool: HTTrack1. We 
notice that a small amount of pages doesn?t al-
ways contain valuable contents. So, we put a 
threshold (100 bytes in our experiment) on the 
web pages' content to filter meaningless pages. In 
order to evaluate our method, the bilingual page 
pairs of each website are annotated by a human 
annotator. Finally, we got 23109 pages and 
11684 bilingual page pairs in total for testing. 
                                                 
1 http://www.httrack.com/ 
The basic information of these websites is listed 
in Table 1. 
It?s time-consuming to annotate whether two 
pages is parallel or not. Note that if a website 
contains N English pages and M Chinese pages, 
an annotator has to label N*M page pairs. To the 
best of our knowledge, there is no large scale and 
public parallel page pair dataset with human an-
notation. So we try to build a reliable and large-
scale dataset. 
In our experiments, URL similarity is used to 
reduce the workload for annotation. For a certain 
website, firstly, we obtain its URL pattern be-
tween English and Chinese pages manually. For 
example, in the website ?www.gov.hk?, the URL 
pairs like: 
http://www.gov.hk/en/about/govdirectory/   (English) 
http://www.gov.hk/sc/about/govdirectory/   (Chinese) 
The URL pairs always point to a pair of paral-
lel pages. So <?/en/?,?/sc/?> is considered as a 
URL pattern that was used to find parallel pages. 
For the other URLs that can?t match the pattern, 
we have to label them by hand. The column ?No 
pattern pairs? in Table 1 shows that the number 
of parallel page pairs which mismatch any pat-
terns. 
Table 1 Number of pages and bilingual page pairs of 
each websites 
Site ID En/Ch pages 
Total 
pairs 
No pat-
tern pairs 
URL 
S1 1101/1098 1092 20 www.gov.hk 
S2 501/497 487 7 www.customs.gov.hk 
S3 995/775 768 12 www.sbc.edu.sg 
S4 4085/3838 3648 4 www.swd.gov.hk 
S5 660/637 637 0 www.landsd.gov.hk 
S6 4733/4626 4615 8 www.td.gov.hk 
 total 12075/11471 11684 51  
Each website listed in Table 1 has a URL pat-
tern for most parallel web pages. Some previous 
researches used the URL similarity or patterns to 
find parallel page pairs. However, due to the di-
versity of web page styles and website mainte-
nance mechanisms, bilingual websites adopt var-
ied naming schemes for parallel documents (Shi, 
et al, 2006). The effect of URL pattern-based 
mining always depends on the style of website. 
In order to build a large dataset, the URL pattern 
is not used in our method. Our method is able to 
handle bilingual websites without URL pattern 
rules. 
In addition, an English-Chinese dictionary 
with 64K words pairs is used in our experiments. 
Algorithm 3 needs a threshold       or 
1220
       . It is very hard to tune the        
because it varies a lot in different websites and 
language pairs. However, Table 1 shows that the 
number of parallel pages is smaller than that of 
English and Chinese pages. Here, for each web-
site, the      is set to the number of Chinese 
pages (which is always smaller than that of Eng-
lish pages). In this way, the precision will never 
reach 100%, but it is more practical in a real ap-
plication. As a result, in some experiments, we 
only report the F-score, and the precision and 
recall can be calculated as follows:  
          
       (            )
      
                 (4) 
       
       (            )
        
                      (5) 
Where,        for each website is listed in the 
?Total  pairs? column of Table 1. 
4.2 Results and Analysis 
Performance of the Baseline 
Let?s start by presenting the performance of a 
baseline method as follows. The baseline only 
employs the internal translation similarity for 
parallel web pages mining. Algorithm 3 is also 
used to get the page pairs in baseline system. 
Here, the input    (   )  is replaced by 
   (   ) . The parameter   in Equation 2 is a 
discount factor. For different   values, the per-
formance of baseline system on six websites is 
shown in Figure 2. In the Figure 2, it shows that 
when   is set to 0.6, the baseline system achieves 
the best performance. The precision, recall and 
F-score are 85.84%, 87.55% and 86.69% respec-
tively. So in the following experiments, we al-
ways set ? to 0.6.  
 
Figure 2 Performances of baseline system with differ-
ent   value 
Performance of Our Method 
As described in Section 3, our method com-
bines the internal with external translation simi-
larity in estimating the final translation similarity 
(i.e., ETS) of two pages. So, the discount factor 
  in Equation (1) is important in our method. 
Besides, as shown in Algorithm 2, the iterative 
algorithm is used to calculate the similarity. Then, 
one question is that how many iterations are re-
quired in our algorithm. Figure 3 shows the per-
formance of our method on each website. Its hor-
izontal axis represents the number of iterations 
and the vertical axis represents the F-score. And 
for each website, the F-scores with different   
(range from 0.2 to 0.8) are also reported in this 
figure. From Figure 3, it is very easy to find that 
the best iteration number is 3. For almost all the 
websites, the performance of our method 
achieves the maximal values and converges after 
the third iteration. In addition, Figure 3 also indi-
cates that our method is robust for different web-
sites. In the following experiments, the iteration 
number is set to 3. 
Next, let?s turn to the discount factor  . Figure 
4 reports the experimental results on the whole 
dataset. Here, the horizontal axis represents the 
discount factor   and the vertical axis represents 
the F-score.     means that only the internal 
similarity is used in the algorithm, so the F-score 
equals to that in Figure 2 when      . On the 
contrary,     means that only the external 
similarity is used in the method, and the F-score 
is 80.20%. The performance is lower than the 
baseline system when only the external link in-
formation is used, but it is much better than the 
performance of the content-based method and 
structure-based method whose F-scores are 64.82% 
and 64.0% respectively. Besides, it is shown 
from Figure 4, the performance is improved sig-
nificantly when the internal and external similari-
ty measures are combined together. Furthermore, 
it is somewhat surprising that the discount factor 
  is not important as we previously expected. In 
fact, if we discard the cases that   equals to 0 or 
1, the difference between the maximum and min-
imum F-score will be 0.76% which is very small. 
This finding indicates that the internal and exter-
nal similarity can easily be combined and we 
don?t need to make many efforts to tune this pa-
rameter when our method is applied to other 
websites. The reason of this phenomenon is that, 
no matter how much weight (i.e., 1-  ) was as-
signed  to the internal similarity, the internal sim-
ilarity always provides a relatively good initial 
60
65
70
75
80
85
90
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
P
er
fo
rm
a
n
ce
(%
) 
? 
F-score Precision Recall
1221
 
Figure 3 Experiment results of our method on each website
iterative direction. In the following experiments, 
the parameter ? is set to 0.6. 
 
Figure 4 The F-scores of our method with different 
the value of ? 
The weight of pages 
The weight of the neighbor pages should also 
be considered. For example, in the most websites, 
it is very common that most of the web pages 
contain a hyperlink which points to the homep-
age of the website. While in most of the Eng-
lish/Chinese websites, almost every English page 
will link to the English homepage and each Chi-
nese page will point to Chinese homepage. The 
English and Chinese homepages are probably 
parallel, but they will be helpless to find parallel 
web pages, because they are neighbors of almost 
every page in the site. On the contrary, some-
times the parallel homepages have negative ef-
fects on finding parallel pages They will increase 
the translation similarity of two pages which are 
not indeed mutual translations. So it is necessary 
to amend the Algorithm 1.  
The weight of each page is calculated accord-
ing to its popularity: 
 ( )     
    
    ( )   
  (6) 
where ( ) indicates the weight of page  ,   is 
the number of all pages,     ( ) is the number 
of pages pointing to page   and   is a constant 
for smoothing.  
In this paper, the weights of pages are used in 
two ways: 
Weight 1: The 9th line of Algorithm 1 is 
amended by the page weight as follows: 
     ?           (   )  ( ( )   ( ))    
Weight 2: The pages with low weight are re-
moved from the input of Algorithm 1. 
The experiment results are shown in Table 2.  
Table 2 The effect of page weight 
Type No Weight Weight 1 Weight 2 
F-score (%) 92.91 92.78 92.75 
Surprisingly, no big differences are found after 
the introduction of the page weight. The side ef-
fect of popular pages is not so large in our meth-
od. In the neighbor pages of a certain page, the 
popular pages are the minority. Besides, the iter-
ative process makes our method more stable and 
robust. 
The impact of the size of bilingual lexicon 
The baseline system mainly combines the con-
tent-based similarity with structure similarity. 
86.69  
92.15  
92.42  
92.67  
92.78  
92.83  
92.91  
92.83  
92.61  
92.40  
80.20  
79
81
83
85
87
89
91
93
95
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
F
-s
co
re
(%
) 
? 
1222
And two kinds of similarity measures are also 
used in our method. As Ma and Liberman (1999) 
pointed out, not all translators create translated 
pages that look like the original page which 
means that the structure similarity does not al-
ways work well. Compared to the structure simi-
larity, the content-based is more reliable and has 
wider applicability. Furthermore, the bilingual 
lexicon is the only information that relates to the 
language pairs, and other features (such as struc-
ture and link information) are all language inde-
pendent. So, it?s important to investigate the ef-
fect of lexicon size in our method. We test the 
performance of our method with different size of 
the bilingual dictionary. The experiment results 
are shown in Figure 5. In this figure, the horizon-
tal axis represents the bilingual lexicon size and 
the vertical axis represents the F-score. With the 
decline of the lexicon size, the performances of 
both the baseline method and our method are 
decreased. However, we can find that the descent 
rate of our method is smaller than that of the 
baseline. It indicates that our method does not 
need a big bilingual lexicon which is good news 
for the low-resource language pairs. 
 
Figure 5 The impact of the size of bilingual lexicon 
Error analysis  
Errors occur when the two pages are similar in 
terms of structure, content and their neighbors. 
For example, Figure 6 illustrates a typical web 
page structure. There are 5 parts in the web page: 
 ,  ,  ,   and  . Part   always contains the 
main content of this page. While part  ,  ,   and 
  always contain some hyperlinks such as ?home? 
in part   and ?About us? in part  . Links in   
and   sometimes relate to the content of the page. 
For such a kind of non-parallel page pairs, let?s 
assume that the two pages have the same struc-
ture (as shown in Figure 6). In addition, their 
content part   is very short and contains the 
same or related topics. As a result, the links in 
other 4 parts are likely to be similar. In this case, 
our method is likely to regard the two pages as 
parallel.  
M
U
B
L R
 
Figure 6 A typical web page structure 
There are about 920 errors when our system 
obtains its best performance. By carefully inves-
tigating the error page pairs, we find that more 
than 90% errors fall into the category discussed 
above. The websites used in our experiments 
mainly come from Hong Kong government web-
sites. Some government departments regularly 
publish quarterly or monthly work reports on one 
issue through their websites. These reports look 
very similar except the publish date and some 
data in them. The other 10% errors happen be-
cause of the particularity of the web pages, e.g. 
very short pages, broken pages and so on. 
5 Conclusions and Future Work 
Parallel corpora are valuable resources for a lot 
of NLP research problems and applications, such 
as MT and CLIR. This paper introduces an effi-
cient and effective solution to bilingual language 
processing. We first explore how to extract paral-
lel page pairs in bilingual websites with link in-
formation between web pages. Firstly, we hy-
pothesize that the translation similarity of pages 
should be based on both internal and external 
translation similarity. Secondly, a novel iterative 
method is proposed to verify parallel page pairs. 
Experimental results show that our method is 
much more effective than the baseline system 
with 6.2% improvement on F-Score. Further-
more, our method has some significant contribu-
tions. For example, compared to previous work, 
our method does not depend on bilingual lexi-
cons, and the parameters in our method have lit-
tle effect on the final performance. These fea-
tures improve the applicability of our method. 
In the future work, we will study some method 
on extracting parallel resource from existing par-
allel page pairs, which are challenging tasks due 
to the diversity of page structures and styles. Be-
sides, we will evaluate the effectiveness of our 
mined data on MT or other applications. 
78
80
82
84
86
88
90
92
94
64K 32K 16K 8K 4K 2K 1K
F
-s
co
re
 (
%
) 
Lexicon Size 
Baseline Our Method
1223
Acknowledgments 
This research work has been sponsored by Na-
tional Natural Science Foundation of China 
(Grants No.61373097 and No.61272259), one 
National Natural Science Foundation of Jiangsu 
Province (Grants No.BK2011282), one Major 
Project of College Natural Science Foundation of 
Jiangsu Province (Grants No.11KJA520003) and 
one National Science Foundation of Suzhou City 
(Grants No.SH201212).  
The corresponding author of this paper, ac-
cording to the meaning given to this role by 
School of computer science and technology at 
Soochow University, is Yu Hong 
Reference 
Chen, Jiang and Jianyun Nie. 2000. Automatic con-
struction of parallel English-Chinese corpus for 
cross-language information retrieval. Proceedings 
of the sixth conference on Applied Natural Lan-
guage Processing, 21?28. 
Resnik, Philip and Noah A. Smith. 2003. The Web as 
a Parallel Corpus. Meeting of the Association for 
Computational Linguistics 29(3). 349?380. 
Kit, Chunyu and Jessica Yee Ha Ng. 2007. An Intelli-
gent Web Agent to Mine Bilingual Parallel Pages 
via Automatic Discovery of URL Pairing Patterns. 
Web Intelligence and Intelligent Agent Technology 
Workshops, 526?529. 
Zhang, Ying, Ke Wu, Jianfeng Gao and Phil Vines. 
2006. Automatic Acquisition of Chinese-English 
Parallel Corpus from the Web. Joint Proceedings of 
the Association for Computational Linguistics and 
the International Conference on Computational 
Linguistics, 420?431. 
Nie, Jianyun, Michel Simard, Pierre Isabelle and 
Richard Durand. 1999. Cross-language information 
retrieval based on parallel texts and automatic min-
ing of parallel texts from the Web. Proceedings of 
the 22nd annual international ACM SIGIR confer-
ence on Research and development in information 
retrieval, 74?81. 
Ma, Xiaoyi and Mark Y. Liberman. 1999. BITS: A 
Method for Bilingual Text Search over the Web. 
Machine Translation Summit VII. 
Chen, Jisong, Rowena Chau and Chung-Hsing Yeh. 
2004. Discovering Parallel Text from the World 
Wide Web. The Australasian Workshop on Data 
Mining and Web Intelligence, vol. 32, 157?161. 
Dunedin, New Zealand. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. 
Proceedings of the Joint Conference of the 47th 
Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, vol. 2, 870?878. 
Yanhui Feng, Yu Hong, Zhenxiang Yan, Jianmin  
Yao and Qiaoming Zhu. 2010. A novel method for 
bilingual web page acquisition from search engine 
web records. Proceedings of the 23rd International 
Conference on Computational Linguistics: Posters, 
294?302.  
Zhao, Bing and Stephan Vogel. 2002. Adaptive Paral-
lel Sentences Mining from Web Bilingual News 
Collection. IEEE International Conference on Data 
Mining, 745?748. 
Smith, Jason R., Chris Quirk and Kristina Toutanova. 
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. Hu-
man Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 403?
411. 
Bharadwaj, Rohit G. and Vasudeva Varma. 2011. 
Language independent identification of parallel 
sentences using wikipedia. Proceedings of the 20th 
International Conference Companion on World 
Wide Web, 11?12. Hyderabad, India. 
Gusfield, Dan. 1997. Algorithms on Strings, Trees 
and Sequences: Computerss Science and Computa-
tional Biology. Cambridge University Press  
Shi, Lei, Cheng Niu, Ming Zhou and Jianfeng Gao. 
2006. A DOM Tree Alignment Model for Mining 
Parallel Data from the Web. Proceedings of the 
21st International Conference on Computational 
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, 489?496. 
 
 
1224
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1127?1136,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Cross-Entity Inference to Improve Event Extraction 
Yu Hong     Jianfeng Zhang     Bin Ma     Jianmin Yao     Guodong Zhou     Qiaoming Zhu 
School of Computer Science and Technology, Soochow University, Suzhou City, China 
{hongy, jfzhang, bma, jyao, gdzhou, qmzhu}@suda.edu.cn 
 
 
Abstract 
Event extraction is the task of detecting certain 
specified types of events that are mentioned in 
the source language data. The state-of-the-art 
research on the task is transductive inference 
(e.g. cross-event inference). In this paper, we 
propose a new method of event extraction by 
well using cross-entity inference. In contrast to 
previous inference methods, we regard entity-
type consistency as key feature to predict event 
mentions. We adopt this inference method to 
improve the traditional sentence-level event ex-
traction system. Experiments show that we can 
get 8.6% gain in trigger (event) identification, 
and more than 11.8% gain for argument (role) 
classification in ACE event extraction. 
1 Introduction 
The event extraction task in ACE (Automatic Con-
tent Extraction) evaluation involves three challeng-
ing issues: distinguishing events of different types, 
finding the participants of an event and determin-
ing the roles of the participants. 
The recent researches on the task show the 
availability of transductive inference, such as that 
of the following methods: cross-document, cross-
sentence and cross-event inferences. Transductive 
inference is a process to use the known instances to 
predict the attributes of unknown instances. As an 
example, given a target event, the cross-event in-
ference can predict its type by well using the re-
lated events co-occurred with it within the same 
document. From the sentence: 
(1)He left the company. 
it is hard to tell whether it is a Transport event in 
ACE, which means that he left the place; or an 
End-Position event, which means that he retired 
from the company. But cross-event inference can 
use a related event ?Then he went shopping? within 
the same document to identify it as a Transport 
event correctly. 
As the above example might suggest, the avail-
ability of transductive inference for event extrac-
tion relies heavily on the known evidences of an 
event occurrence in specific condition. However, 
the evidence supporting the inference is normally 
unclear or absent. For instance, the relation among 
events is the key clue for cross-event inference to 
predict a target event type, as shown in the infer-
ence process of the sentence (1). But event relation 
extraction itself is a hard task in Information Ex-
traction. So cross-event inference often suffers 
from some false evidence (viz., misleading by un-
related events) or lack of valid evidence (viz., un-
successfully extracting related events). 
In this paper, we propose a new method of 
transductive inference, named cross-entity infer-
ence, for event extraction by well using the rela-
tions among entities. This method is firstly 
motivated by the inherent ability of entity types in 
revealing event types. From the sentences: 
(2)He left the bathroom. 
(3)He left Microsoft. 
it is easy to identify the sentence (2) as a Transport 
event in ACE, which means that he left the place, 
because nobody would retire (End-Position type) 
from a bathroom. And compared to the entities in 
sentence (1) and (2), the entity ?Microsoft? in (3) 
would give us more confidence to tag the ?left? 
event as an End-Position type, because people are 
used to giving the full name of the place where 
they retired. 
The cross-entity inference is also motivated by 
the phenomenon that the entities of the same type 
often attend similar events. That gives us a way to 
predict event type based on entity-type consistency. 
From the sentence: 
(4)Obama beats McCain. 
it is hard to identify it as an Elect event in ACE, 
which means Obama wins the Presidential Election, 
1127
or an Attack event, which means Obama roughs 
somebody up. But if we have the priori knowledge 
that the sentence ?Bush beats McCain? is an Elect 
event, and ?Obama? was a presidential contender 
just like ?Bush? (strict type consistency), we have 
ample evidence to predict that the sentence (4) is 
also an Elect event. 
Indeed above cross-entity inference for event-
type identification is not the only use of entity-type 
consistency. As we shall describe below, we can 
make use of it at all issues of event extraction: 
y For event type: the entities of the same type 
are most likely to attend similar events. And the 
events often use consistent or synonymous trigger. 
y For event argument (participant): the enti-
ties of the same type normally co-occur with simi-
lar participants in the events of the same type. 
y For argument role: the arguments of the 
same type, for the most part, play the same roles in 
similar events. 
With the help of above characteristics of entity, 
we can perform a step-by-step inference in this 
order:  
y Step 1: predicting event type and labeling 
trigger given the entities of the same type. 
y Step 2: identifying arguments in certain event 
given priori entity type, event type and trigger that 
obtained by step 1. 
y Step 3: determining argument roles in certain 
event given entity type, event type, trigger and ar-
guments that obtained by step 1 and step 2. 
On the basis, we give a blind cross-entity infer-
ence method for event extraction in this paper. In 
the method, we first regard entities as queries to 
retrieve their related documents from large-scale 
language resources, and use the global evidences 
of the documents to generate entity-type descrip-
tions. Second we determine the type consistency of 
entities by measuring the similarity of the type de-
scriptions. Finally, given the priori attributes of 
events in the training data, with the help of the en-
tities of the same type, we perform the step-by-step 
cross-entity inference on the attributes of test 
events (candidate sentences). 
In contrast to other transductive inference meth-
ods on event extraction, the cross-entity inference 
makes every effort to strengthen effects of entities 
in predicting event occurrences. Thus the inferen-
tial process can benefit from following aspects: 1) 
less false evidence, viz. less false entity-type con-
sistency (the key clue of cross-entity inference), 
because the consistency can be more precisely de-
termined with the help of fully entity-type descrip-
tion that obtained based on the related information 
from Web; 2) more valid evidence, viz. more enti-
ties of the same type (the key references for the 
inference), because any entity never lack its con-
geners. 
2 Task Description 
The event extraction task we addressing is that of 
the Automatic Content Extraction (ACE) evalua-
tions, where an event is defined as a specific occur-
rence involving participants. And event extraction 
task requires that certain specified types of events 
that are mentioned in the source language data be 
detected. We first introduce some ACE terminol-
ogy to understand this task more easily: 
y Entity: an object or a set of objects in one of 
the semantic categories of interest, referred to in 
the document by one or more (co-referential) entity 
mentions. 
y Entity mention: a reference to an entity (typi-
cally, a noun phrase). 
y Event trigger: the main word that most clear-
ly expresses an event occurrence (An ACE event 
trigger is generally a verb or a noun). 
y Event arguments: the entity mentions that 
are involved in an event (viz., participants). 
y Argument roles: the relation of arguments to 
the event where they participate. 
y Event mention: a phrase or sentence within 
which an event is described, including trigger and 
arguments. 
The 2005 ACE evaluation had 8 types of events, 
with 33 subtypes; for the purpose of this paper, we 
will treat these simply as 33 separate event types 
and do not consider the hierarchical structure 
among them. Besides, the ACE evaluation plan 
defines the following standards to determine the 
correctness of an event extraction: 
y A trigger is correctly labeled if its event type 
and offset (viz., the position of the trigger word in 
text) match a reference trigger. 
y An argument is correctly identified if its event 
type and offsets match any of the reference argu-
ment mentions, in other word, correctly recogniz-
ing participants in an event. 
y An argument is correctly classified if its role 
matches any of the reference argument mentions. 
Consider the sentence: 
1128
(5) It has refused in the last five years to revoke 
the license of a single doctor for committing medi-
cal errors.1
The event extractor should detect an End-
Position event mention, along with the trigger 
word ?revoke?, the position ?doctor?, the person 
whose license should be revoked, and the time dur-
ing which the event happened: 
 Event type End-Position 
Trigger revoke 
a single doctor Role=Person 
doctor Role=Position Arguments 
the last five years Role=Time-within 
Table 1: Event extraction example 
It is noteworthy that event extraction depends on 
previous phases like name identification, entity 
mention co-reference and classification. Thereinto, 
the name identification is another hard task in ACE 
evaluation and not the focus in this paper. So we 
skip the phase and instead directly use the entity 
labels provided by ACE. 
3 Related Work 
Almost all the current ACE event extraction sys-
tems focus on processing one sentence at a time 
(Grishman et al, 2005; Ahn, 2006; Hardyet al 
2006). However, there have been several studies 
using high-level information from a wider scope:  
Maslennikov and Chua (2007) use discourse 
trees and local syntactic dependencies in a pattern-
based framework to incorporate wider context to 
refine the performance of relation extraction. They 
claimed that discourse information could filter noi-
sy dependency paths as well as increasing the reli-
ability of dependency path extraction. 
Finkel et al (2005) used Gibbs sampling, a sim-
ple Monte Carlo method used to perform approxi-
mate inference in factored probabilistic models. By 
using simulated annealing in place of Viterbi de-
coding in sequence models such as HMMs, CMMs, 
and CRFs, it is possible to incorporate non-local 
structure while preserving tractable inference. 
They used this technique to augment an informa-
tion extraction system with long-distance depend-
ency models, enforcing label consistency and 
extraction template consistency constraints. 
Ji and Grishman (2008) were inspired from the 
hypothesis of ?One Sense Per Discourse? (Ya-
                                                          
1 Selected from the file ?CNN_CF_20030304.1900.02? in 
ACE-2005 corpus. 
rowsky, 1995); they extended the scope from a 
single document to a cluster of topic-related docu-
ments and employed a rule-based approach to 
propagate consistent trigger classification and 
event arguments across sentences and documents. 
Combining global evidence from related docu-
ments with local decisions, they obtained an appre-
ciable improvement in both event and event 
argument identification. 
Patwardhan and Riloff (2009) proposed an event 
extraction model which consists of two compo-
nents: a model for sentential event recognition, 
which offers a probabilistic assessment of whether 
a sentence is discussing a domain-relevant event; 
and a model for recognizing plausible role fillers, 
which identifies phrases as role fillers based upon 
the assumption that the surrounding context is dis-
cussing a relevant event. This unified probabilistic 
model allows the two components to jointly make 
decisions based upon both the local evidence sur-
rounding each phrase and the ?peripheral vision?. 
Gupta and Ji (2009) used cross-event informa-
tion within ACE extraction, but only for recovering 
implicit time information for events. 
Liao and Grishman (2010) propose document 
level cross-event inference to improve event ex-
traction. In contrast to Gupta?s work, Liao do not 
limit themselves to time information for events, but 
rather use related events and event-type consis-
tency to make predictions or resolve ambiguities 
regarding a given event. 
4 Motivation 
In event extraction, current transductive inference 
methods focus on the issue that many events are 
missing or spuriously tagged because the local in-
formation is not sufficient to make a confident de-
cision. The solution is to mine credible evidences 
of event occurrences from global information and 
regard that as priori knowledge to predict unknown 
event attributes, such as that of cross-document 
and cross-event inference methods.  
However, by analyzing the sentence-level base-
line event extraction, we found that the entities 
within a sentence, as the most important local in-
formation, actually contain sufficient clues for 
event detection. It is only based on the premise that 
we know the backgrounds of the entities before-
hand. For instance, if we knew the entity ?vesu-
vius? is an active volcano, we could easily identify 
1129
the word ?erupt?, which co-occurred with the en-
tity, as the trigger of a ?volcanic eruption? event 
but not that of a ?spotty rash?. 
In spite of that, it is actually difficult to use an 
entity to directly infer an event occurrence because 
we normally don?t know the inevitable connection 
between the background of the entity and the event 
attributes. But we can well use the entities of the 
same background to perform the inference. In de-
tail, if we first know entity(a) has the same back-
ground with entity(b), and we also know that 
entity(a), as a certain role, participates in a specific 
event, then we can predict that entity(b) might par-
ticiptes in a similar event as the same role. 
Consider the two sentences2 from ACE corpus: 
(5) American case for war against Saddam. 
(6) Bush should torture the al Qaeda chief op-
erations officer. 
The sentences are two event mentions which 
have the same attributes: 
Event type Attack 
Trigger war 
American Role=Attacker 
(5) 
Arguments 
Saddam Role=Target 
Event type Attack 
Trigger torture 
Bush Role=Attacker 
(6) 
Arguments 
...Qaeda chief ... Role=Target 
Table 2: Cross-entity inference example 
From the sentences, we can find that the entities 
?Saddam? and ?Qaeda chief? have the same back-
ground (viz., terrorist leader), and they are both the 
arguments of Attack events as the role of Target. 
So if we previously know any of the event men-
tions, we can infer another one with the help of the 
entities of the same background. 
In a word, the cross-entity inference, we pro-
posed for event extraction, bases on the hypothesis: 
Entities of the consistent type normally partici-
pate in similar events as the same role. 
As we will introduce below, some statistical da-
ta from ACE training corpus can support the hy-
pothesis, which show the consistency of event type 
and role in event mentions where entities of the 
same type occur. 
4.1 Entity Consistency and Distribution 
Within the ACE corpus, there is a strong entity 
consistency: if one entity mention appears in a type 
                                                          
2 They are extracted from the files ?CNN_CF_20030305.1900. 
00-1? and ?CNN_CF_20030303.1900.06-1? respectively. 
of event, other entity mentions of the same type 
will appear in similar events, and even use the 
same word to trigger the events. To see this we 
calculated the conditional probability (in the ACE 
corpus) of a certain entity type appearing in the 33 
ACE event subtypes. 
0
50
100
150
200
250
Be?Born
M
arry
D
ivorce
Injure
D
ie
Transport
Transfer?
Transfer?
Start?O
rg
M
erge?
D
eclare?
End?O
rg
A
ttack
D
em
onstr
M
eet
Phone?
Start?
End?
N
om
inate
Elect
A
rrest?Jail
Release?
Trial?
Charge?
Sue
Convict
Sentence
Fine
Execute
Extradite
A
cquit
A
ppeal
Pardon
Event typeF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 1. Conditional probability of a certain entity 
type appearing in the 33 ACE event subtypes (Here 
only the probabilities of Population-Center, Ex-
ploding and Air entities as examples) 
0
50
100
150
200
250
Person
Place
Buyer
Seller
Beneficiary
Price
A
rtifact
O
rigin
D
estination
G
iver
Recipient
M
oney
O
rg
A
gent
Victim
Instrum
ent
Entity
A
ttacker
Target
D
efendant
A
djudicator
Prosecutor
Plaintiff
Crim
e
Position
Sentence
Vehicle
Tim
e?A
fter
Tim
e?Before
Tim
e?A
t?
Tim
e?A
t?End
Tim
e?
Tim
e?
Tim
e?H
olds
Tim
e?
RoleF
re
qu
en
cy
Population?Center
Exploding
Air
 
Figure 2. Conditional probability of an entity type 
appearing as the 34 ACE role types (Here only the 
probabilities of Population-Center, Exploding and 
Air entities as examples) 
As there are 33 event subtypes and 43 entity 
types, there are potentially 33*43=1419 entity-
event combinations. However, only a few of these 
appear with substantial frequency. For example, 
the Population-Center entities only occur in 4 
types of event mentions with the conditional prob-
ability more than 0.05. From Table 3, we can find 
that only Attack and Transport events co-occur 
frequently with Population-Center entities (see 
Figure 1 and Table 3). 
Event Cond.Prob. Freq. 
Transport 0.368 197 
Attack 0.295 158 
Meet 0.073 39 
Die 0.069 37 
Table 3: Events co-occurring with Population-
Center with the conditional probability > 0.05 
Actually we find that most entity types appear in 
more restricted event mentions than Population-
Center entity. For example, Air entity only co-
occurs with 5 event types (Attack, Transport, Die, 
Transfer-Ownership and Injure), and Exploding 
1130
entity co-occurs with 4 event types (see Figure 1). 
Especially, they only co-occur with one or two 
event types with the conditional probability more 
than 0.05. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 24 7 12 
Freq. >10 37 4 2 
Freq. >50 41 1 1 
Table 4: Distribution of entity-event combination 
corresponding to different co-occurrence frequency 
Table 4 gives the distributions of whole ACE 
entity types co-occurring with event types. We can 
find that there are 37 types of entities (out of 43 in 
total) appearing in less than 5 types of event men-
tions when entity-event co-occurrence frequency is 
larger than 10, and only 2 (e.g. Individual) appear-
ing in more than 10 event types. And when the fre-
quency is larger than 50, there are 41 (95%) entity 
types co-occurring with less than 5 event types. 
These distributions show the fact that most in-
stances of a certain entity type normally participate 
in events of the same type. And the distributions 
might be good predictors for event type detection 
and trigger determination. 
Air (Entity type) 
Attack 
event 
Fighter plane (subtype 1): 
?MiGs? ?enemy planes? ?warplanes? ?allied 
aircraft? ?U.S. jets? ?a-10 tank killer? ?b-1 
bomber? ?a-10 warthog? ?f-14 aircraft? 
?apache helicopter? 
Spacecraft (subtype 2): 
?russian soyuz capsule? ?soyuz? 
Civil aviation (subtype 3): 
?airliners? ?the airport? ?Hooters Air execu-
tive? 
Transport 
event 
Private plane (subtype 4): 
?Marine One? ?commercial flight? ?private 
plane? 
Table 5: Event types co-occurred with Air entities 
Besides, an ACE entity type actually can be di-
vided into more cohesive subtypes according to 
similarity of background of entity, and such a sub-
type nearly always co-occur with unique event 
type. For example, the Air entities can be roughly 
divided into 4 subtypes: Fighter plane, Spacecraft, 
Civil aviation and Private plane, within which the 
Fighter plane entities all appear in Attack event 
mentions, and other three subtypes all co-occur 
with Transport events (see Table 5). This consis-
tency of entities in a subtype is helpful to improve 
the precision of the event type predictor. 
4.2 Role Consistency and Distribution 
The same thing happens for entity-role combina-
tions: entities of the same type normally play the 
same role, especially in the event mentions of the 
same type. For example, the Population-Center 
entities occur in ACE corpus as only 4 role types: 
Place, Destination, Origin and Entity respectively 
with conditional probability 0.615, 0.289, 0.093, 
0.002 (see Figure 2). And They mainly appear in 
Transport event mentions as Place, and in Attack 
as Destination. Particularly the Exploding entities 
only occur as Instrument and Artifact respectively 
with the probability 0.986 and 0.014. They almost 
entirely appear in Attack events as Instrument. 
 Evnt.<=5 5<Evnt.<=10 Evnt.>10 
Freq. > 0 32 5 6 
Freq. >10 38 3 2 
Freq. >50 42 1 0 
Table 6: Distribution of entity-role combination 
corresponding to different co-occurrence frequency 
Table 6 gives the distributions of whole entity-
role combinations in ACE corpus. We can find that 
there are 38 entity types (out of 43 in total) occur 
as less than 5 role types when the entity-role co-
occurrence frequency is larger than 10. There are 
42 (98%) when the frequency is larger than 50, and 
only 2 (e.g. Individual) when larger than 10. The 
distributions show that the instances of an entity 
type normally occur as consistent role, which is 
helpful for cross-entity inference to predict roles. 
5 Cross-entity Approach  
In this section we present our approach to using 
blind cross-entity inference to improve sentence-
level ACE event extraction. 
Our event extraction system extracts events in-
dependently for each sentence, because the defini-
tion of event mention constrains them to appear in 
the same sentence. Every sentence that at least in-
volves one entity mention will be regarded as a 
candidate event mention, and a randomly selected 
entity mention from the candidate will be the star-
ing of the whole extraction process. For the entity 
mention, information retrieval is used to mine its 
background knowledge from Web, and its type is 
determined by comparing the knowledge with 
those in training corpus. Based on the entity type, 
the extraction system performs our step-by-step 
cross-entity inference to predict the attributes of 
1131
the candidate event mention: trigger, event type, 
arguments, roles and whether or not being an event 
mention. The main frame of our event extraction 
system is shown in Figure 3, which includes both 
training and testing processes. 
 
Figure 3. The frame of cross-entity inference for event extraction (including training and testing processes) 
In the training process, for every entity type in 
the ACE training corpus, a clustering technique 
(CLUTO toolkit)3 is used to divide it into different 
cohesive subtypes, each of which only contains the 
entities of the same background. For instance, the 
Air entities will be divided into Fighter plane, 
Spacecraft, Civil aviation, Private plane, etc (see 
Table 5). And for each subtype, we mine event 
mentions where this type of entities appear from 
ACE training corpus, and extract all the words 
which trigger the events to establish corresponding 
trigger list. Besides, a set of support vector ma-
chine (SVM) based classifiers are also trained: 
y Argument Classifier: to distinguish arguments 
of a potential trigger from non-arguments4; 
y Role Classifier: to classify arguments by ar-
gument role; 
y Reportable-Event Classifier (Trigger Classi-
fier): Given entity types, a potential trigger, an 
event type, and a set of arguments, to determine 
whether there is a reportable event mention. 
                                                          
3http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=h
tml&identifier=ADA439508 
4 It is noteworthy that a sentence may include more than one 
event (more than one trigger). So it is necessary to distinguish 
arguments of a potential trigger from that of others. 
In the test process, for each candidate event 
mention, our event extraction system firstly pre-
dicts its triggers and event types: given an ran-
domly selected entity mention from the candidate, 
the system determines the entity subtype it belong-
ing to and the corresponding trigger list, and then 
all non-entity words in the candidate are scanned 
for a instance of triggers from the list. When an 
instance is found, the system tags the candidate as 
the event type that the most frequently co-occurs 
with the entity subtype in the events that triggered 
by the instance. Secondly the argument classifier is 
applied to the remaining mentions in the candidate; 
for any argument passing that classifier, the role 
classifier is used to assign a role to it. Finally, once 
all arguments have been assigned, the reportable-
event classifier is applied to the candidate; if the 
result is successful, this event mention is reported. 
5.1 Further Division of Entity Type  
One of the most important pretreatments before 
our blind cross-entity inference is to divide the 
ACE entity type into more cohesive subtype. The 
greater consistency among backgrounds of entities 
in such a subtype might be good to improve the 
precision of cross-entity inference.  
1132
For each ACE entity type, we collect all entity 
mentions of the type from training corpus, and re-
gard each such mention as a query to retrieve the 
50 most relevant documents from Web. Then we 
select 50 key words that the most weighted by 
TFIDF in the documents to roughly describe back-
ground of entity. After establishing the vector 
space model (VSM) for each entity mention of the 
type, we adopt a clustering toolkit (CLUTO) to 
further divide the mentions into different subtypes. 
Finally, for each subtype, we describe its centroid 
by using 100 key words which the most frequently 
occurred in relevant documents of entities of the 
subtype. 
In the test process, for an entity mention in a 
candidate event mention, we determine its type by 
comparing its background against all centroids of 
subtypes in training corpus, and the subtype whose 
centroid has the most Cosine similarity with the 
background will be assigned to the entity. It is 
noteworthy that global information from the Web 
is only used to measure the entity-background con-
sistency and not directly in the inference process. 
Thus our event extraction system actually still per-
forms a sentence-level inference based on local 
information. 
5.2 Cross-Entity Inference 
Our event extraction system adopts a step-by-
step cross-entity inference to predict event. As dis-
cussed above, the first step is to determine the trig-
ger in a candidate event mention and tag its event 
type based on consistency of entity type. Given the 
domain of event mention that restrained by the 
known trigger, event type and entity subtype, the 
second step is to distinguish the most probable ar-
guments that co-occurring in the domain from the 
non-arguments. Then for each of the arguments, 
the third step can use the co-occurring arguments 
in the domain as important contexts to predict its 
role. Finally, the inference process determines 
whether the candidate is a reportable event men-
tion according to a confidence coefficient. In the 
following sections, we focus on introducing the 
three classifiers: argument classifier, role classifier 
and reportable-event classifier. 
5.2.1   Cross-Entity Argument Classifier 
For a candidate event mention, the first step 
gives its event type, which roughly restrains the 
domain of event mentions where the arguments of 
the candidate might co-occur. On the basis, given 
an entity mention in the candidate and its type (see 
the pretreatment process in section 5.1), the argu-
ment classifier could predict whether other entity 
mentions co-occur with it in such a domain, if yes, 
all the mentions will be the arguments of the can-
didate. In other words, if we know an entity of a 
certain type participates in some event, we will 
think of what entities also should participate in the 
event. For instance, when we know a defendant 
goes on trial, we can conclude that the judge, law-
yer and witness should appear in court. 
Argument Classifier 
Feature 1: an event type (an event-mention domain) 
Feature 2: an entity subtype 
Feature 3: entity-subtype co-occurrence in domain 
Feature 4: distance to trigger 
Feature 5: distances to other arguments 
Feature 6: co-occurrence with trigger in clause 
Role Classifier 
Feature 1 and Feature 2 
Feature 7: entity-subtypes of arguments 
Reportable-Event Classifier 
Feature 1 
Feature 8: confidence coefficient of trigger in domain 
Feature 9: confidence coefficient of role in domain 
Table 7: Features selected for SVM-based cross-
entity classifiers 
A SVM-based argument classifier is used to de-
termine arguments of candidate event mention. 
Each feature of this classifier is the conjunction of: 
y The subtype of an entity 
y The event type we are trying to assign an ar-
gument to 
y A binary indicator of whether this entity sub-
type co-occurs with other subtypes in such an 
event type (There are 266 entity subtypes, and so 
266 features for each instance) 
Some minor features, such as another binary indi-
cator of whether arguments co-occur with trigger 
in the same clause (see Table 7). 
5.2.2 Cross-Entity Role Classifier 
For a candidate event mention, the arguments 
that given by the second step (argument classifier) 
provide important contextual information for pre-
dicting what role the local entity (also one of the 
arguments) takes on. For instance, when citizens 
(Arg1) co-occur with terrorist (Arg2), most likely 
the role of Arg1 is Victim. On the basis, with the 
help of event type, the prediction might be more 
1133
precise. For instance, if the Arg1 and Arg2 co-
occur in an Attack event mention, we will have 
more confidence in the Victim role of Arg1. 
Besides, as discussed in section 4, entities of the 
same type normally take on the same role in simi-
lar events, especially when they co-occur with sim-
ilar arguments in the events (see Table 2). 
Therefore, all instances of co-occurrence model 
{entity subtype, event type, arguments} in training 
corpus could provide effective evidences for pre-
dicting the role of argument in the candidate event 
mention. Based on this, we trained a SVM-based 
role classifier which uses following features: 
y Feature 1 and Feature 2 (see Table 7) 
y Given the event domain that restrained by the 
entity and event types, an indicator of what sub-
types of arguments appear in the domain. (266 en-
tity subtypes make 266 features for each instance) 
5.2.3 Reportable-Event Classifier 
At this point, there are still two issues need to be 
resolved. First, some triggers are common words 
which often mislead the extraction of candidate 
event mention, such as ?it?, ?this?, ?what?, etc. 
These words only appear in a few event mentions 
as trigger, but when they once appear in trigger list, 
a large quantity of noisy sentences will be regarded 
as candidates because of their commonness in sen-
tences. Second, some arguments might be tagged 
as more than one role in specific event mentions, 
but as ACE event guideline, one argument only 
takes on one role in a sentence. So we need to re-
move those with low confidence. 
A confidence coefficient is used to distinguish 
the correct triggers and roles from wrong ones. The 
coefficient calculate the frequency of a trigger (or a 
role) appearing in specific domain of event men-
tions and that in whole training corpus, then com-
bines them to represent its confidence degree, just 
like TFIDF algorithm. Thus, the more typical trig-
gers (or roles) will be given high confidence. 
Based on the coefficient, we use a SVM-based 
classifier to determine the reportable events. Each 
feature of this classifier is the conjunction of: 
y An event type (domain of event mentions) 
y Confidence coefficients of triggers in domain 
y Confidence coefficients of roles in the domain. 
6 Experiments 
We followed Liao (2010)?s evaluation and ran-
domly select 10 newswire texts from the ACE 
2005 training corpus as our development set, 
which is used for parameter tuning, and then con-
duct a blind test on a separate set of 40 ACE 2005 
newswire texts. We use the rest of the ACE train-
ing corpus (549 documents) as training data for our 
event extraction system.  
To compare with the reported work on cross-
event inference (Liao, 2010) and its sentence-level 
baseline system, we cross-validate our method on 
10 separate sets of 40 ACE texts, and report the 
optimum, worst and mean performances (see Table 
8) on the data by using Precision (P), Recall (R) 
and F-measure (F). In addition, we also report the 
performance of two human annotators on 40 ACE 
newswire texts (a random blind test set): one 
knows the rules of event extraction; the other 
knows nothing about it. 
6.1 Main Results  
From the results presented in Table 8, we can 
see that using the cross-entity inference, we can 
improve the F score of sentence-level event extrac-
tion for trigger classification by 8.59%, argument 
classification by 11.86%, and role classification by 
11.9% (mean performance). Compared to the 
cross-event inference, we gains 2.87% improve-
ment for argument classification, and 3.81% for 
role classification (mean performance). Especially, 
our worst results also have better performances 
than cross-event inference. 
Nonetheless, the cross-entity inference has 
worse F score for trigger determination. As we can 
see, the low Recall score weaken its F score (see 
Table 8). Actually, we select the sentence which at 
least includes one entity mention as candidate 
event mention, but lots of event mentions in ACE 
never include any entity mention. Thus we have 
missed some mentions at the starting of inference 
process. 
In addition, the annotator who knows the rules 
of event extraction has a similar performance trend 
with systems: high for trigger classification, mid-
dle for argument classification, and low for role 
classification (see Table 8). But the annotator who 
never works in this field obtains a different trend: 
higher performance for argument classification. 
This phenomenon might prove that the step-by-
step inference is not the only way to predicate 
event mention because human can determine ar-
guments without considering triggers and event 
types. 
1134
                            Performance 
System/Human Trigger (%) Argument (%) Role (%) 
 P R F P R F P R F 
Sentence-level baseline 67.56 53.54 59.74 46.45 37.15 41.29 41.02 32.81 36.46
Cross-event inference 68.71 68.87 68.79 50.85 49.72 50.28 45.06 44.05 44.55
Cross-entity inference (optimum) 73.4 66.2 69.61 56.96 55.1 56 49.3 46.59 47.9 
Cross-entity inference (worst) 71.3 64.17 66.1 51.28 50.3 50.78 46.3 44.3 45.28
Cross-entity inference (mean) 72.9 64.3 68.33 53.4 52.9 53.15 51.6 45.5 48.36
Human annotation 1 (blind) 58.9 59.1 59.0 62.6 65.9 64.2 50.3 57.69 53.74
Human annotation 2 (know rules) 74.3 76.2 75.24 68.5 75.8 71.97 61.3 68.8 64.86
Table 8: Overall performance on blind test data
6.2 Influence of Clustering on Inference  
A main part of our blind inference system is the 
entity-type consistency detection, which relies 
heavily on the correctness of entity clustering and 
similarity measurement. In training, we used 
CLUTO clustering toolkit to automatically gener-
ate different types of entities based on their back-
ground-similarities. In testing, we use K-nearest 
neighbor algorithm to determine entity type. 
Fighter plane (subtype 1 in Air entities): 
?warplanes? ?allied aircraft? ?U.S. jets? ?a-10 tank killer? 
?b-1 bomber? ?a-10 warthog? ?f-14 aircraft? ?apache heli-
copter? ?terrorist? ?Saddam? ?Saddam Hussein? ?Bagh-
dad??
Table 9: Noises in subtype 1 of ?Air? entities (The 
blod fonts are noises) 
We obtained 129 entity subtypes from training 
set. By randomly inspecting 10 subtypes, we found 
nearly every subtype involves no less than 19.2% 
noises. For example, the subtype 1 of ?Air? in Ta-
ble 5 lost the entities of ?MiGs? and ?enemy 
planes?, but involved ?terrorist?, ?Saddam?, etc 
(See Table 9). Therefore, we manually clustered 
the subtypes and retry the step-by-step cross-entity 
inference. The results (denoted as ?Visible 1?) are 
shown in Table 10, within which, we additionally 
show the performance of the inference on the 
rough entity types provided by ACE (denoted as 
?Visible 2?), such as the type of ?Air?, ?Popula-
tion-Center?, ?Exploding?, etc., which normally 
can be divided into different more cohesive sub-
types. And the ?Blind? in Table 10 denotes the 
performances on our subtypes obtained by CLUTO. 
It is surprised that the performances (see Table 
10, F-score) on ?Visible 1? entity subtypes are just 
a little better than ?Blind? inference. So it seems 
that the noises in our blind entity types (CLUTO 
clusters) don?t hurt the inference much. But by re-
inspecting the ?Visible 1? subtypes, we found that 
their granularities are not enough small: the 89 
manual entity clusters actually can be divided into 
more cohesive subtypes. So the improvements of 
inference on noise-free ?Visible 1? subtypes are 
partly offset by loss on weakly consistent entities 
in the subtypes. It can be proved by the poor per-
formances on ?Visible 2? subtypes which are much 
more general than ?Visible 1?. Therefore, a rea-
sonable clustering method is important in our in-
ference process. 
F-score Trigger  Argument Role 
Blind 68.33 53.15 48.36 
Visible 1 69.15 53.65 48.83 
Visible 2 51.34 43.40 39.95 
Table 10: Performances on visible VS blind  
7 Conclusions and Future Work  
We propose a blind cross-entity inference method 
for event extraction, which well uses the consis-
tency of entity mention to achieve sentence-level 
trigger and argument (role) classification. Experi-
ments show that the method has better perform-
ance than cross-document and cross-event 
inferences in ACE event extraction. 
The inference presented here only considers the 
helpfulness of entity types of arguments to role 
classification. But as a superior feature, contextual 
roles can provide more effective assistance to role 
determination of local argument. For instance, 
when an Attack argument appears in a sentence, a 
Target might be there. So if we firstly identify 
simple roles, such as the condition that an argu-
ment has only a single role, and then use the roles 
as priori knowledge to classify hard ones, may be 
able to further improve performance.
Acknowledgments 
We thank Ruifang He. And we acknowledge the 
support of the National Natural Science Founda-
tion of China under Grant Nos. 61003152, 
60970057, 90920004. 
1135
References  
David Ahn. 2006. The stages of event extraction. In 
Proc. COLING/ACL 2006 Workshop on Annotating 
and Reasoning about Time and Events.Sydney, Aus-
tralia. 
Jenny Rose Finkel, Trond Grenager and Christopher 
Manning. 2005. Incorporating Non-local Information 
into Information Extraction Systems by Gibbs Sam-
pling. In Proc. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 363?370, 
Ann Arbor, MI, June. 
Prashant Gupta and Heng Ji. 2009. Predicting Unknown 
Time Arguments based on Cross-Event Propagation. 
In Proc. ACL-IJCNLP 2009. 
Ralph Grishman, David Westbrook and Adam Meyers. 
2005. NYU?s English ACE 2005 System Description. 
In Proc. ACE 2005 Evaluation Workshop, Gaithers-
burg, MD. 
Hilda Hardy, Vika Kanchakouskaya and Tomek Strzal-
kowski. 2006. Automatic Event Classification Using 
Surface Text Features. In Proc. AAAI06 Workshop on 
Event Extraction and Synthesis. Boston, MA. 
Heng Ji and Ralph Grishman. 2008. Refining Event 
Extraction through Cross-Document Inference. In 
Proc. ACL-08: HLT, pages 254?262, Columbus, OH, 
June. 
Shasha Liao and Ralph Grishman. 2010. Using Docu-
ment Level Cross-Event Inference to Improve Event 
Extraction. In Proc. ACL-2010, pages 789-797, Upp-
sala, Sweden, July. 
Mstislav Maslennikov and Tat-Seng Chua. 2007. A 
Multi resolution Framework for Information Extrac-
tion from Free Text. In Proc. 45th Annual Meeting of 
the Association of Computational Linguistics, pages 
592?599, Prague, Czech Republic, June. 
Siddharth Patwardhan and Ellen Riloff. 2007. Effective 
Information Extraction with Semantic Affinity Pat-
terns and Relevant Regions. In Proc. Joint Confer-
ence on Empirical Methods in Natural Language 
Processing and Computational Natural Language 
Learning, 2007, pages 717?727, Prague, Czech Re-
public, June. 
Siddharth Patwardhan and Ellen Riloff. 2009. A Unified 
Model of Phrasal and Sentential Evidence for Infor-
mation Extraction. In Proc. Conference on Empirical 
Methods in Natural Language Processing 2009, 
(EMNLP-09). 
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc. 
ACL 1995. Cambridge, MA. 
1136
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 569?573,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Effective Selection of Translation Model Training Data 
Le Liu  Yu Hong*  Hao Liu  Xing Wang  Jianmin Yao 
School of Computer Science & Technology, Soochow University, China 
{20124227052, hongy, 20134227035, 20114227047, jyao}@suda.edu.cn 
 
Abstract 
Data selection has been demonstrated to 
be an effective approach to addressing 
the lack of high-quality bitext for statisti-
cal machine translation in the domain of 
interest. Most current data selection 
methods solely use language models 
trained on a small scale in-domain data to 
select domain-relevant sentence pairs 
from general-domain parallel corpus. By 
contrast, we argue that the relevance be-
tween a sentence pair and target domain 
can be better evaluated by the combina-
tion of language model and translation 
model. In this paper, we study and exper-
iment with novel methods that apply 
translation models into domain-relevant 
data selection. The results show that our 
methods outperform previous methods. 
When the selected sentence pairs are 
evaluated on an end-to-end MT task, our 
methods can increase the translation per-
formance by 3 BLEU points.* 
1 Introduction 
Statistical machine translation depends heavily 
on large scale parallel corpora. The corpora are 
necessary priori knowledge for training effective 
translation model. However, domain-specific 
machine translation has few parallel corpora for 
translation model training in the domain of inter-
est. For this, an effective approach is to automat-
ically select and expand domain-specific sen-
tence pairs from large scale general-domain par-
allel corpus. The approach is named Data Selec-
tion. Current data selection methods mostly use 
language models trained on small scale in-
domain data to measure domain relevance and 
select domain-relevant parallel sentence pairs to 
expand training corpora. Related work in litera-
ture has proven that the expanded corpora can 
substantially improve the performance of ma-
                                                 
* Corresponding author 
chine translation (Duh et al, 2010; Haddow and 
Koehn, 2012). 
However, the methods are still far from satis-
factory for real application for the following rea-
sons: 
? There isn?t ready-made domain-specific 
parallel bitext. So it?s necessary for data se-
lection to have significant capability in min-
ing parallel bitext in those assorted free texts. 
But the existing methods seldom ensure 
parallelism in the target domain while se-
lecting domain-relevant bitext. 
? Available domain-relevant bitext needs keep 
high domain-relevance at both the sides of 
source and target language. But it?s difficult 
for current method to maintain two-sided 
domain-relevance when we aim at enhanc-
ing parallelism of bitext.   
In a word, current data selection methods can?t 
well maintain both parallelism and domain-
relevance of bitext. To overcome the problem, 
we first propose the method combining transla-
tion model with language model in data selection. 
The language model measures the domain-
specific generation probability of sentences, be-
ing used to select domain-relevant sentences at 
both sides of source and target language. Mean-
while, the translation model measures the trans-
lation probability of sentence pair, being used to 
verify the parallelism of the selected domain-
relevant bitext. 
2 Related Work 
The existing data selection methods are mostly 
based on language model. Yasuda et al (2008) 
and Foster et al (2010) ranked the sentence pairs 
in the general-domain corpus according to the 
perplexity scores of sentences, which are com-
puted with respect to in-domain language models. 
Axelrod et al (2011) improved the perplexity-
based approach and proposed bilingual cross-
entropy difference as a ranking function with in- 
and general- domain language models. Duh et al 
(2013) employed the method of (Axelrod et al, 
569
2011) and further explored neural language mod-
el for data selection rather than the conventional 
n-gram language model. Although previous 
works in data selection (Duh et al, 2013; Koehn 
and Haddow, 2012; Axelrod et al, 2011; Foster 
et al, 2010; Yasuda et al, 2008) have gained 
good performance, the methods which only 
adopt language models to score the sentence 
pairs are sub-optimal. The reason is that a sen-
tence pair contains a source language sentence 
and a target language sentence, while the existing 
methods are incapable of evaluating the mutual 
translation probability of sentence pair in the tar-
get domain. Thus, we propose novel methods 
which are based on translation model and lan-
guage model for data selection. 
3 Training Data Selection Methods 
We present three data selection methods for 
ranking and selecting domain-relevant sentence 
pairs from general-domain corpus, with an eye 
towards improving domain-specific translation 
model performance. These methods are based on 
language model and translation model, which are 
trained on small in-domain parallel data.  
3.1 Data Selection with Translation Model 
Translation model is a key component in statisti-
cal machine translation. It is commonly used to 
translate the source language sentence into the 
target language sentence. However, in this paper, 
we adopt the translation model to evaluate the 
translation probability of sentence pair and de-
velop a simple but effective variant of translation 
model to rank the sentence pairs in the general-
domain corpus. The formulations are detailed as 
below: 
 (   )  
 
(    )
  
? ?  (     )
  
   
  
       (1) 
  ? (   )
  
       (2) 
Where  (   ) is the translation model, which is 
IBM Model 1 in this paper, it represents the 
translation probability of target language sen-
tence   conditioned on source language sentence 
 .    and    are the number of words in sentence 
  and  respectively.  (     )  is the translation 
probability of word    conditioned on word   and 
is estimated from the small in-domain parallel 
data. The parameter   is a constant and is as-
signed with the value of 1.0.   is the length-
normalized IBM Model 1, which is used to score 
general-domain sentence pairs. The sentence pair 
with higher score is more likely to be generated 
by in-domain translation model, thus, it is more 
relevant to the in-domain corpus and will be re-
mained to expand the training data.  
3.2 Data Selection by Combining Transla-
tion and Language model  
As described in section 1, the existing data selec-
tion methods which only adopt language model 
to score sentence pairs are unable to measure the 
mutual translation probability of sentence pairs. 
To solve the problem, we develop the second 
data selection method, which is based on the 
combination of translation model and language 
model. Our method and ranking function are 
formulated as follows: 
   (   )   (   )   ( )        (3) 
    ? (   )
  
 ? ( )
  
             (4) 
Where  (   ) is a joint probability of sentence   
and   according to the translation model  (   ) 
and language model  ( ), whose parameters are 
estimated from the small in-domain text.   is the 
improved ranking function and used to score the 
sentence pairs with the length-normalized trans-
lation model  (   )and language model  ( ). 
The sentence pair with higher score is more simi-
lar to in-domain corpus, and will be picked out.  
3.3 Data Selection by Bidirectionally   
Combining Translation and Language 
Models  
As presented in subsection 3.2, the method com-
bines translation model and language model to 
rank the sentence pairs in the general-domain 
corpus. However, it does not evaluate the inverse 
translation probability of sentence pair and the 
probability of target language sentence. Thus, we 
take bidirectional scores into account and simply 
sum the scores in both directions.  
  ? (   )
  
 ? ( )
  
 ? (   )
  
 ? ( )
  
 
 (5) 
Again, the sentence pairs with higher scores are 
presumed to be better and will be selected to in-
corporate into the domain-specific training data. 
This approach makes full use of two translation 
models and two language models for sentence 
pairs ranking. 
570
4 Experiments 
4.1 Corpora 
We conduct our experiments on the Spoken Lan-
guage Translation English-to-Chinese task. Two 
corpora are needed for the data selection. The in-
domain data is collected from CWMT09, which 
consists of spoken dialogues in a travel setting, 
containing approximately 50,000 parallel sen-
tence pairs in English and Chinese. Our general-
domain corpus mined from the Internet contains 
16 million sentence pairs. Both the in- and gen-
eral- domain corpora are identically tokenized (in 
English) and segmented (in Chinese)1. The de-
tails of corpora are listed in Table 1. Additionally, 
we evaluate our work on the 2004 test set of 
?863? Spoken Language Translation task (?863? 
SLT), which consists of 400 English sentences 
with 4 Chinese reference translations for each. 
Meanwhile, the 2005 test set of ?863? SLT task, 
which contains 456 English sentences with 4 ref-
erences each, is used as the development set to 
tune our systems.  
Bilingual Cor-
pus 
#sentence #token 
Eng Chn Eng Chn 
In-domain 50K 50K 360K 310K 
General-domain 16M 16M 3933M 3602M 
Table 1. Data statics 
4.2 System settings 
We use the NiuTrans 2  toolkit which adopts 
GIZA++ (Och and Ney, 2003) and MERT (Och, 
2003) to train and tune the machine translation 
system. As NiuTrans integrates the mainstream 
translation engine, we select hierarchical phrase-
based engine (Chiang, 2007) to extract the trans-
lation rules and carry out our experiments. 
Moreover, in the decoding process, we use the 
NiuTrans decoder to produce the best outputs, 
and score them with the widely used NIST mt-
eval131a3  tool. This tool scores the outputs in 
several criterions, while the case-insensitive 
BLEU-4 (Papineni et al, 2002) is used as the 
evaluation for the machine translation system. 
4.3 Translation and Language models 
Our work relies on the use of in-domain lan-
guage models and translation models to rank the 
sentence pairs from the general-domain bilingual 
training set. Here, we employ ngram language 
                                                 
1http://www.nlplab.com/NiuPlan/NiuTrans.YourData.ch.html 
2http://www.nlplab.com/NiuPlan/NiuTrans.ch.html#download 
3 http://ww.itl.nist.gov/iad/mig/tools 
model and IBM Model 1 for data selection. Thus, 
we use the SRI Language Modeling Toolkit 
(Stolcke, 2002) to train the in-domain 4-gram 
language model with interpolated modified 
Kneser-Ney discounting (Chen and Goodman, 
1998). The language model is only used to score 
the general-domain sentences. Meanwhile, we 
use the language model training scripts integrat-
ed in the NiuTrans toolkit to train another 4-gram 
language model, which is used in MT tuning and 
decoding. Additionally, we adopt GIZA++ to get 
the word alignment of in-domain parallel data 
and form the word translation probability table. 
This table will be used to compute the translation 
probability of general-domain sentence pairs.  
4.4 Baseline Systems 
As described above, by using the NiuTrans 
toolkit, we have built two baseline systems to 
fulfill ?863? SLT task in our experiments. The 
In-domain baseline trained on spoken language 
corpus has 1.05 million rules in its hierarchical-
phrase table. While, the General-domain baseline 
trained on 16 million sentence pairs has a hierar-
chical phrase table containing 1.7 billion transla-
tion rules. These two baseline systems are 
equipped with the same language model which is 
trained on large-scale monolingual target lan-
guage corpus. The BLEU scores of the In-
domain and General-domain baseline system are 
listed in Table 2.  
Corpus 
Hierarchical 
phrase 
Dev Test 
In-domain 1.05M 15.01 21.99 
General-domain 1747M 27.72 34.62 
Table 2. Translation performances of In-domain and 
General-domain baseline systems 
The results show that General-domain system 
trained on a larger amount of bilingual resources 
outperforms the system trained on the in-domain 
corpus by over 12 BLEU points. The reason is 
that large scale parallel corpus maintains more 
bilingual knowledge and language phenomenon, 
while small in-domain corpus encounters data 
sparse problem, which degrades the translation 
performance. However, the performance of Gen-
eral-domain baseline can be improved further. 
We use our three methods to refine the general-
domain corpus and improve the translation per-
formance in the domain of interest. Thus, we 
build several contrasting systems trained on re-
fined training data selected by the following dif-
ferent methods.  
571
? Ngram: Data selection by 4-gram LMs with 
Kneser-Ney smoothing. (Axelrod et al, 
2011) 
? Neural net: Data selection by Recurrent 
Neural LM, with the RNNLM Tookit. (Duh 
et al, 2013) 
? Translation Model (TM): Data selection 
with translation model: IBM Model 1. 
? Translation model and Language Model 
(TM+LM): Data selection by combining 4-
gram LMs with Kneser-Ney smoothing and 
IBM model 1(equal weight).  
? Bidirectional TM+LM: Data selection by 
bidirectionally combining translation and 
language models (equal weight).  
4.5 Results of Training Data Selection 
We adopt five methods for extracting domain-
relevant parallel data from general-domain cor-
pus. Using the scoring methods, we rank the sen-
tence pairs of the general-domain corpus and 
select only the top N = {50k, 100k, 200k, 400k, 
600k, 800k, 1000k} sentence pairs as refined 
training data. New MT systems are then trained 
on these small refined training data. Figure 1 
shows the performances of systems trained on 
selected corpora from the general-domain corpus. 
The horizontal coordinate represents the number 
of selected sentence pairs and vertical coordinate 
is the BLEU scores of MT systems.  
 
Figure 1. Results of the systems trained on only a sub-
set of the general-domain parallel corpus. 
From Figure 1, we conclude that these five da-
ta selection methods are effective for domain-
specific translation. When top 600k sentence 
pairs are picked out from general-domain corpus 
to train machine translation systems, the systems 
perform higher than the General-domain baseline 
trained on 16 million parallel data. The results 
indicate that more training data for translation 
model is not always better. When the domain-
specific bilingual resources are deficient, the 
domain-relevant sentence pairs will play an im-
portant role in improving the translation perfor-
mance.  
Additionally, it turns out that our methods 
(TM, TM+LM and Bidirectional TM+LM) are 
indeed more effective in selecting domain-
relevant sentence pairs. In the end-to-end SMT 
evaluation, TM selects top 600k sentence pairs 
of general-domain corpus, but increases the 
translation performance by 2.7 BLEU points. 
Meanwhile, the TM+LM and Bidirectional 
TM+LM have gained 3.66 and 3.56 BLEU point 
improvements compared against the general-
domain baseline system. Compared with the 
mainstream methods (Ngram and Neural net), 
our methods increase translation performance by 
nearly 3 BLEU points, when the top 600k sen-
tence pairs are picked out. Although, in the fig-
ure 1, our three methods are not performing bet-
ter than the existing methods in all cases, their 
overall performances are relatively higher. We 
therefore believe that combining in-domain 
translation model and language model to score 
the sentence pairs is well-suited for domain-
relevant sentence pair selection. Furthermore, we 
observe that the overall performance of our 
methods is gradually improved. This is because 
our methods are combining more statistical char-
acteristics of in-domain data in ranking and se-
lecting sentence pairs. The results have proven 
the effectiveness of our methods again. 
5 Conclusion 
We present three novel methods for translation 
model training data selection, which are based on 
the translation model and language model. Com-
pared with the methods which only employ lan-
guage model for data selection, we observe that 
our methods are able to select high-quality do-
main-relevant sentence pairs and improve the 
translation performance by nearly 3 BLEU points. 
In addition, our methods make full use of the 
limited in-domain data and are easily implement-
ed. In the future, we are interested in applying 
20.00
22.00
24.00
26.00
28.00
30.00
32.00
34.00
36.00
38.00
40.00
0 200 400 600 800 1000
Axelord et al(2011) Duh et al(2013)
TM TM+LM
Bidirectional TM+LM
572
our methods into domain adaptation task of sta-
tistical machine translation in model level. 
Acknowledgments 
This research work has been sponsored by two 
NSFC grants, No.61373097 and No.61272259, 
and one National Science Foundation of Suzhou 
(Grants No. SH201212).  
Reference 
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 
2011. Domain adaptation via pseudo in-domain da-
ta selection. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language 
Processing, pages 355?362, Edinburgh, Scotland, 
UK, July. Association for Computational Linguis-
tics. 
Peter F.Brown, Vincent J. Della Pietra, Stephen A. 
Della Pietra and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: Pa-
rameter estimation. Computational linguistics, 
1993, 19(2): 263-311. 
Stanley Chen and Joshua Goodman. 1998. An Empir-
ical Study of Smoothing Techniques for Language 
Modeling. Technical Report 10-98, Computer Sci-
ence Group, Harvard University.  
Moore Robert C, Lewis William. 2010. Intelligent 
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Pa-
pers. Association for Computational Linguistics, 
2010: 220-224. 
Chiang David. A hierarchical phrase-based model for 
statistical machine translation. 2005. In Proceed-
ings of the 43rd Annual Meeting on Association for 
Computational Linguistics, pages: 263-270. Asso-
ciation for Computational Linguistics. 
Kevin Duh, Graham Neubig, Katsuhito Sudoh and  
Hajime Tsukada. Adaptation Data Selection using 
Neural Language Models: Experiments in Machine 
Translation. In Proceedings of the 51st Annual 
Meeting of the Association for Computational Lin-
guistics, pages 678-683, Sofia, Bulgaria, August 4-
9 2013.  
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 
2010.Analysis of translation model adaptation for 
statistical machine translation. In Proceedings of 
the International Workshop on Spoken Language 
Translation (IWSLT) - Technical Papers Track.  
George Foster, Cyril Goutte, and Roland Kuhn. 2010. 
Discriminative Instance Weighting for Domain 
Adaptation in Statistical Machine Translation. Em-
pirical Methods in Natural Language Processing. 
Barry Haddow and Philipp Koehn. 2012. Analysing 
the effect of out-of-domain data on smt systems. In 
Proceedings of the Seventh Workshop on Statistical 
Machine Translation, pages 422?432, Montreal, 
Canada, June. Association for Computational Lin-
guistics. 
Och, Franz Josef, and Hermann Ney. A systematic 
comparison of various statistical alignment models. 
Computational linguistics 29.1 (2003): 19-51.  
Och, Franz Josef. Minimum error rate training in sta-
tistical machine translation. Proceedings of the 41st 
Annual Meeting on Association for Computational 
Linguistics-Volume 1. Association for Computa-
tional Linguistics, 2003.  
Philipp Koehn and Barry Haddow. 2012. Towards 
effective use of training data in statistical machine 
translation. In WMT. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: A method for auto-
matic evaluation of machine translation. In ACL.  
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. Spoken Language Pro-
cessing. 
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li. 
NiuTrans: an open source toolkit for phrase-based 
and syntax-based machine translation. In Proceed-
ings of the ACL 2012 System Demonstrations. As-
sociation for Computational Linguistics, 2012: 19-
24. 
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, 
and Eiichiro Sumita. 2008. Method of selecting 
training data to build a compact and efficient trans-
lation model. International Joint Conference on 
Natural Language Processing.  
573
Jumping Distance based Chinese Person Name Disambiguation1
Yu Hong  Fei Pei  Yue-hui Yang  Jian-min Yao  Qiao-ming Zhu 
School of Computer Science and Technology, Soochow University 
No.1 Shizi street, Suzhou City, Jiansu Province, China 
{hongy, 20094527004, 0727401137, jyao, qmzhu}@suda.edu.cn
Abstract
In this paper, we describe a Chinese person 
name disambiguation system for news articles 
and report the results obtained on the data set of 
the CLP 2010 Bakeoff-31. The main task of the 
Bakeoff is to identify different persons from the 
news stories that contain the same person-name 
string. Compared to the traditional methods, 
two additional features are used in our system: 
1) n-grams co-occurred with target name string; 
2) Jumping distance among the n-grams. On the 
basis, we propose a two-stage clustering algo-
rithm to improve the low recall.
1   Our Novel Try
For this task, we propose a Jumping-Distance 
based n-gram model (abbr. DJ n-gram) to de-
scribe the semantics of the closest contexts of 
the target person-name strings. 
The generation of the DJ n-gram model 
mainly involves two steps. First, we mine the 
Jumping tree for the target string; second, we 
give the statistical description of the tree. 
z Jumping Tree 
Given a target string, we firstly extract the 
sentence where it locates as its closest context. 
Then we segment the sentence into n-
grams(Chen et al ,2009) (only Bi-gram and Tri-
gram are used in this paper). For each n-gram, 
we regard it as the beginning of a jumping jour-
ney. And the places where we jump are the sen-
tences which involve the n-gram. By the same 
way, we segment the sentences into n-grams 
which will be regarded as the new beginnings to 
open further jumping. The procedure will run 
iteratively until there are no sentences in the 
document (viz. the document which involves 
the target string) can be used to jump. Actually, 
we find there are only 3 jumps in average in our 
previous test and simultaneously 11 sentences 
in a document can be involved into the jumping 
journey. Thus, we can obtain a Jumping Tree 
where each jumping route from the initially n-
gram (viz. the gram in the closes context) refer 
to a branch. And for each intermediate node, its 
child-nodes are the n-grams co-occurred with it 
in the same sentences. 
The motivation to generate the Jumping Tree 
is to imitate the thinking model of human rec-
ognizing the word senses and semantics. In de-
tail, for each intermediate node of the tree, its 
child-nodes all come from its closest contexts, 
especially the nodes co-occur with it in the 
same sentences which involve the real grammar 
and semantic relations. Thus the child-nodes 
normally provide the natural inference for its 
word sense. For example, given the string 
?SARS?, we can deduce its sense from its child 
nodes ?Severe?, ?Acute?, ?Respiratory? and 
?Syndromes? even if we see the string for the 
first time. On the basis, the procedure of infer-
ence run iteratively, that is, the tree always use 
the child nodes deduce the meaning of their fa-
ther nodes then further ancestor nodes until the 
root. Thus the tree acts as a hierarchical under-
standing procedure. Additionally, the distances 
among nodes in the tree give the degree of se-
mantic relation.  
In the task of person-name disambiguation, 
we use the Jumping Tree to deduce the identi-
ties and backgrounds of a person. Each branch 
of the tree refers to a property of the person. 
z Jumping-Distance based n-gram model 
In this paper, we give a simple statistical 
model to describe the Jumping Tree. Given a 
node in the tree (viz. an n-gram), we record the Supported by the National Natural Science Foundation 
of China under Grant No. 60970057, No.60873105.
steps jumping from the root to it, viz. the depth 
of the node in the tree. Then based on the priori-
trained TFIDF value, we calculate the genera-
tion probability of the node as follows: 
depth
TFP D? 
where the D  denotes the smoothing factor.
In fact, we create more comprehensive mod-
els to describe the semantic correlations among 
the nodes in the Jumping Tree. The models well 
use the distances among the nodes in local 
Jumping Tree (viz. the tree generated based on 
the test document) and that normalized on the 
large-scale training data to calculate the prob-
ability of n-grams correboratively generate a 
semantics. They try to imitate the thinking 
model of human combine differents features to 
understand panoramic knowledge. In the task of 
name disambiguation, we can use the models to 
improve the distinguishment of different per-
sons who have the same name. And we have 
illustrate the well effectiveness on the topic de-
scription and relevance measurement in other 
tasks, such as Link Detection. But we actually 
didn?t use the models to perform the task of 
name diaambiguation this time with the aim to 
purely evaluate the usefulness of the Jumping 
Tree.
2    Systems
For the task of Chinese person name disam-
biguation, we submitted two systems as follows: 
z System1 
The system involves two main components: 
DJ-based name Identification error detection 
and DJ-based person name disambiguation. 
The first component, viz. DJ-based name 
segmentation error detection, aims to distin-
guish the target string referring to person name 
from that referring to something else. Such as, 
the string ???? can be a person name ?Hai 
Huang? but also a name of sea ?the Yellow 
Sea?. And the detection component focuses on 
obtaining the pure person name ?Hai Huang?. 
The detection component firstly establish two 
classes of features which respectively describe 
the nature of human and that of things. Such as, 
the features ?professor?, ?research?, ?honest? et 
al., can roughly be determined as the nature of 
human, and conversely the features ?solid?, 
?collapse?, ?deep? et al, can be that of things. 
For obtaining the features, we extract 10,000 
documents that discuss person, eg. ?Albert Ein-
stein? and 6000 documents that discuss tech-
nology, science, geography, et.al., from 
Wikipedia2. For each document, we generate its 
Jumping Tree, and regard the nodes in the tree 
as the features. After that, we combine the 
weights of the same features and normalized the 
value by dividing that by the average weight in 
the specific class of features. 
Based on the two classes of features, given a 
target string and the document where it occurs, 
the detection component firstly generate the 
Jumping Tree of the document, and then deter-
mines whether the string is person name or 
things by measuring the similarity of  the tree to 
the classes of features. Here, we simply use the 
VSM and Cosine metric ?Bagga and Baldwin, 
1998? to obtain the similarity. 
The second component, viz. DJ-based person 
name disambiguation, firstly generates the 
Jumping trees for all documents that involve 
specific person name. And a two-stage cluster-
ing algorithm is adopted to divide the docu-
ments and refer each cluster to a person. The 
first stage of the algorithm runs a strict division 
which focuses on obtaining high precision. The 
second stage performs a soft division which is 
used to improve recall. The two-stage clustering 
algorithm(Ikeda et al,2009) initially obtains the 
optimal parameters that respectively refer to the 
maximum precision and recall based on training 
data, and then regards a statistical tradeoff as 
the final value of the parameters. Here, the Af-
finity Propagation clustering tools (Frey BJ and 
Dueck D, 2007) is in use. 
z System2 
The system is similar to the system1 except 
that it additionally involve Named Entity Identi-
fication (Artiles et.al,2009B; Popescu,O. and 
Magnini, B.,2007)before the two-stage cluster-
ing in the component of person name disam-
biguation. In detail, given a person name and 
the documents that it occurs in, the disambigua-
tion component of System2 firstly adopt NER 
CRF++ toolkit3  provided by MSRA to identify 
Named Entities(Chen et al, 2006) that involve 
the given name string, such as the entity ???
?? (viz. Gao-ming Li in English) when given 
the target name string ????(viz. Ming Gao in 
English). Thus the documents can be roughly 
divided into different clusters of Named Entities 
without name segmentation errors. After that, 
we additionally adopt the two-stage clustering 
algorithm to further divide each cluster. Thus 
we can deal with the issue of disambiguation 
without the interruption of name segmentation 
errors.
3   Data sets 
z Training dataset: They contain about 30 
Chinese personal names, and a document set of 
about 100-300 news articles from collection of 
Xinhua news documents in a time span of four-
teen years are provided for each personal name. 
z External dataset: Chinese Wikipedia2 per-
sonal attribution (Cucerzan, 2007; Nguyen and 
Cao,2008).
z Test dataset: There are about 26 Chinese 
personal names, which are similar to train data 
sets.
4     Experiments 
The systems that run on test dataset are evalu-
ated by both B-Cubed (Bagga and  Baldwin, 
1998; Artiles et al,2009A) and P-IP (Artiles  et 
al., 2007 ;Artiles et al,2009A). And the systems 
that run on training dataset were only evaluated 
by B-Cubed. 
In experiments, we firstly evaluate the per-
formance of name segmentation error detection 
on the training dataset. For comparison, we ad-
ditionally perform another detection method 
which only using Name Entity Identifcation 
(NER CRF++ tools) to distinguish name-strings 
from the discarded ones. The results are shown 
in table 1. We can find that our error detection 
method can achieve more recall than NER, but 
lower precision. 
Besides, we evaluate the performance of the 
two-stage clustering in the component of name 
disambiguation step by step. Four steps are in 
use to evaluate the first-stage clustering method 
as follows: 
z DJ2
This step look like to run the system1 men-
tionedin in section 3 which don?t involve the 
prior-division of documents by using NER be-
fore the first-stage clustering in the component 
of name disambiguation. Especially it don?t 
perform the second-stage clustering to improve 
the recall probability. 
z DJ2+NER
This step is similar to the step of DJ2 men-
tioned above except that it perform the prior-
divison of documents by using NER. 
z NER+DJ 
This step is also similar to the step of DJ2 ex-
cept that its name segmentation error detection 
performs by using the NER. 
z NER2+DJ
This step is similar to the step of NER+DJ 
except that it involve the treatment of prior-
divison as that in DJ2+NER.
The performances of the four steps are shown 
in table 2. We can find that all steps achieve 
poor recall. And the step of DJ2 achieve the best 
F-score although it don?t involve the prior-
division. That is because NER is helpful to im-
prove precision but not recall, as shown in table 
1. Conversely, DJ2 can avoid the bias caused by 
the procedure of greatly maximizing the preci-
sion.
P recall F-score
DJ-based 0.62 0.81 0.70
NER-based 0.91 0.77 0.71
Table 1: Performance of name segmentation 
error detection 
P IP F-score
DJ2 80.49 53.85 60.12 
DJ2+NER 88.56 51.30 59.02 
NER+DJ 93.27 46.78 57.44 
NER2+DJ 97.79 42.13 55.47 
Table 2: Performances of the-stage clustering 
Additionally, another two steps are used to 
evluate the both two stages of clustering in 
name disambiguation. The steps are as follows: 
z DJ2+NER_2
This step is similar to the step of DJ2+NER 
except that it additionally run the second-stage 
clustering to improve recall. 
z NER2+DJ_2
This step also run the second-stage clustering 
on the basis of NER2+DJ. 
The performances of the two step are shown 
in table 3. We can find that the F-scores both 
have been improved substantially. And the two 
steps still maintain the original distribution be-
tween precision and recall. That is, the 
DJ2+NER_2, which has outperformance on re-
call in the name segmentation error detection, 
still maintain the higher recall at the second-
stage clustering. And NER2+DJ_2 also main-
tains higher precision. This illustrates that the 
clustering has no ability to remedy the short-
comings of NER in the prior-division. 
P IP F-score 
DJ2+NER_2 82.65 63.40    66.59 
NER2+DJ_2 87.71 60.45 66.23 
Table 3: Performances of two-stage clustering 
The test results of the two systems mentioned in 
section 3 are shown in the table 4. We also 
show the performances of each stage clustering 
as that on training dataset. We can find that the 
poor performance mainly come from the low 
recall, which illustrates that the DJ-based n-
gram disambiguation is not robust. 
B-Cubed
precision recall F-Score
System1(one 
t )
85.26 28.43 37.74
System1(both 
t )
84.51 44.17 51.42
P-IP
P IP F-Score
System2(one 
t )
88.4 39.47 50.52
System2(both 
t )
88.36 55.23 63.89
Table 4 :Test results 
5.Conclusions
In this paper, we report a hybrid Chinese per-
sonal disambiguation system and a novel algo-
rithm for extract useful global n-gram features 
from the context .Experiment showed that our 
algorithm performed high precision and poor 
recall. Furthermore, two-stage clustering can 
handl a change in the one-stage clustering algo-
rithm, especially for recall score. In the future, 
we will investigate global new types of features 
to improve the recall score and local new types 
of features to improve the precision score. For 
instance, the location and organization besides 
the person in the named-entities. And we try to 
use Hierarchical Agglomerative Clustering al-
gorithm to help raise the recall score.
References 
Artiles J, J Gonzalo and S Sekine. 2007. The 
SemEval-2007 WePS Evaluation: ?Establish-
ing a benchmark for the Web People Search 
Task.?, The SemEval-2007, 64-69, Associa-
tion for Computational Linguistics.
Artiles Javier, Julio Gonzalo and Satoshi Se-
kine.2009A. ?WePS 2 Evaluation Campaign: 
overview of the Web People Search Cluster-
ing Task,? In 2nd Web People Search 
Evaluation Workshop (WePS 2009), 18th 
WWW Conference. 
Artiles J, E Amig?o and J Gonzalo. 2009B.The 
Role of Named Entities in Web People 
Search. Proceedings of the 2009 Conference 
on Empirical Methods Natural Language 
Processing, 534?542,Singapore, August 2009.  
Bagga A and Baldwin B. 1998. Entity-based 
cross-document coreferenceing using the 
Vector Space Model.Proceedings of the 17th
international conference on computational 
linguistics. Volume 1, 79-85. 
Chen,Ying., Sophia Yat., Mei Lee and Chu-Ren 
Huang. 2009. PolyUHK:A Roubust Informa-
tion Extraction System for Web Personal 
Names In 2nd Web People Search Evaluation 
Workshop (WePS 2009), 18th WWW Con-
ference.
Chen Wen-liang, Zhang Yu-jie. 2006. Chinese 
Named Entity Recognition with Conditional 
Random Fields. Proceedings of the Fifth 
SIGHAN Workshop on Chinese Language 
Processing.
Cucerzan, Silviu. 2007. Large scale named en-
tity Disambiguation based on Wikipedia data. 
In The EMNLP-CoNLL-2007. 
Frey BJ and Dueck D. 2007. Clustering by 
Passing Messages Between Data 
Points .science, 2007 - sciencemag.org. 
Ikeda MS, Ono I, Sato MY and Nakagawa H. 
2009. Person Name disambiguation on the 
Web by Two-Stage Clustering. In 2nd Web 
People Search Evaluation Workshop(WePS 
2009),18th WWW Conference.
Popescu,O and Magnini, B. 2007. IRST-
BP:Web People Search Using Name Enti-
ties.Proceeding s of the 4th International 
Workshop on Semantic Evaluations (SemE-
val-2007), 195-198, Prague June 2007. Asso-
ciation for Computational Linguistics. 
