Proceedings of NAACL-HLT 2013, pages 868?877,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Open Information Extraction with Tree Kernels
Ying Xu, Mi-Young Kim, Kevin Quinn, Randy Goebel and Denilson Barbosa
Department of Computing Science
University of Alberta
{yx2,miyoung2,kfjquinn,goebel,denilson}@cs.ualberta.ca
Abstract
Traditional relation extraction seeks to iden-
tify pre-specified semantic relations within
natural language text, while open Information
Extraction (Open IE) takes a more general ap-
proach, and looks for a variety of relations
without restriction to a fixed relation set. With
this generalization comes the question, what
is a relation? For example, should the more
general task be restricted to relations medi-
ated by verbs, nouns, or both? To help answer
this question, we propose two levels of sub-
tasks for Open IE. One task is to determine if
a sentence potentially contains a relation be-
tween two entities? The other task looks to
confirm explicit relation words for two enti-
ties. We propose multiple SVM models with
dependency tree kernels for both tasks. For
explicit relation extraction, our system can ex-
tract both noun and verb relations. Our results
on three datasets show that our system is su-
perior when compared to state-of-the-art sys-
tems like REVERB and OLLIE for both tasks.
For example, in some experiments our system
achieves 33% improvement on nominal rela-
tion extraction over OLLIE. In addition we
propose an unsupervised rule-based approach
which can serve as a strong baseline for Open
IE systems.
1 Introduction
Relation Extraction (RE) systems are designed to
discover various semantic relations (e.g. <Obama,
president, the United States>) from natural lan-
guage text. Traditional RE systems extract spe-
cific relations for prespecified name-entity types
(Bunescu and Mooney, 2005; Chan and Dan, 2011;
Zhou and Zhu, 2011). To train such systems, ev-
ery relation needs manually annotated training ex-
amples, which supports limited scope and is diffi-
cult to extend. For this reason, Banko et al (2007)
proposed Open Information Extraction (Open IE),
whose goal is to extract general relations for two en-
tities. The idea is to avoid the need for specific train-
ing examples, and to extract a diverse range of rela-
tions. This generalized form has received significant
attention, e.g., (Banko et al, 2007; Akbik, 2009; Wu
and Weld, 2010; Fader et al, 2011; Mausam et al,
2012).
Because Open IE is not guided by or not restricted
to a prespecified list of relations, the immediate chal-
lenge is determining about what counts as a relation?
Most recent Open IE systems have targeted verbal
relations (Banko et al, 2007; Mausam et al, 2012),
claiming that these are the majority. However, Chan
and Dan (2011) show that only 20% of relations in
the ACE programs Relation Detection and Charac-
terization (RDC) are verbal. Our manually extracted
relation triple set from the Penn Treebank shows that
there are more nominal relations than verbal ones,
3 to 2. This difference arises because of the ambi-
guity of what constitutes a relation in Open IE. It
is often difficult even for humans to agree on what
constitutes a relation, and which words in the sen-
tence establish a relation between a pair of entities.
For example, in the sentence ?Olivetti broke Cocom
rules? is there a relation between Olivetti and Co-
com? This ambiguity in the problem definition leads
to significant challenges and confusion when eval-
uating and comparing the performance of different
methods and systems. An example are the results
in Fader et al (2011) and Mausam et al (2012). In
Fader et al (2011), REVERB ?is reported? as su-
868
perior to WOEparse, a system proposed in Wu and
Weld (2010); while in Mausam et al (2012), it is
reported the opposite.
To better answer the question, what counts as a
relation? we propose two tasks for Open IE. The
first task seeks to determine whether there is a re-
lation between two entities (called ?Binary task?).
The other is to confirm whether the relation words
extracted for the two entities are appropriate (the
?Triple task?). The Binary task does not restrict re-
lation word forms, whether they are mediated by
nouns, verbs, prepositions, or even implicit rela-
tions. The Triple task requires an abstract repre-
sentation of relation word forms, which we develop
here. We assume that relation words are nouns or
verbs; in our data, these two types comprise 71% of
explicit relations.
We adapt an SVM dependency tree kernel model
(Moschitti, 2006) for both tasks. The input to our
tasks is a dependency parse, created by Stanford
Parser. Selecting relevant features from a parse tree
for semantic tasks is difficult. SVM tree kernels
avoid extracting explicit features from parse trees
by calculating the inner product of the two trees.
For the Binary task, our dependency path is the path
between two entities. For the Triple task, the path
is among entities and relation words (i.e. relation
triples). Tree kernels have been used in traditional
RE and have helped achieve state of the art perfor-
mance (Culotta and Sorensen, 2004; Bunescu and
Mooney, 2005; Wang, 2008; Nguyen et al, 2009;
Zhou and Zhu, 2011). But one challenge of using
tree kernels on Open IE is that the lexicon of re-
lations is much larger than those of traditional RE,
making it difficult to include the lexical information
as features. Here we proposed an unlexicalized tree
structure for Open IE. As far as we know, this is the
first time an SVM tree kernel has been applied in
Open IE. Experimental results on multiple datasets
show our system outperforms state-of-the-art sys-
tems REVERB and OLLIE. Typically an Open IE
system is tested on one dataset. However, because
the definition of relation is ambiguous, we believe
that is necessary to test with multiple datasets.
In addition to the supervised model, we also pro-
pose an unsupervised model which relies on several
heuristic rules. Results with this approach show that
this simple unsupervised model provides a robust
strong baseline for other approaches.
In summary, our main contributions are:
? Use SVM tree kernels for Open IE. Our sys-
tem is robust comparing with other Open IE
systems, achieving superior scores in two test
sets and comparative scores in another set.
? Extend beyond verbal relations, which are
prevalent in current systems. Analyze implicit
relation problem in Open IE, which is ignored
by other work.
? Propose an unsupervised model for Open IE,
which can be a strong baseline for other ap-
proaches.
The rest of this paper is organized as follows. Sec-
tion 2 provides the problem description and system
structure, before summarizing previous work in Sec-
tion 3. Section 4 defines our representation of rela-
tion word patterns crucial to our task two, and Sec-
tion 5 describes tree kernels for SVM. Section 6 de-
scribes the unsupervised model, and Section 7 ex-
plains our experiment design and results. Section 8
concludes with a summary, and anticipation of fu-
ture work.
2 Problem Definition and System
Structure
The common definition of the Open IE task is a
function from a sentence, s, to a set of triples,
{< E1, R,E2 >}, where E1 and E2 are entities
(noun phrases) and R is a textual fragment indicat-
ing a semantic relation between the two entities. Our
?Triple task? is within this definition. However it is
often difficult to determine which textual fragments
to extract. In addition, semantic relations can be im-
plicit, e.g., consider the located in relation in the sen-
tence fragment ?Washington, US.? To illustrate how
much information is lost when restricting the rela-
tion forms, we add another task (the ?Binary task?),
determining if there is a relation between the two en-
tities. It is a function from s, to a set of binary rela-
tions over entities, {< E1, E2 >}. This binary task
is designed to overcome the disadvantage of current
Open IE systems, which suffer because of restricting
the relation form, e.g., to only verbs, or only nouns.
The two tasks are independent to each other.
869
	

	

		

		
		Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 55?63,
Beijing, August 2010
Application of the Tightness Continuum Measure
to Chinese Information Retrieval
Ying Xu?, Randy Goebel?, Christoph Ringlstetter? and Grzegorz Kondrak?
?Department of Computing Science ?Center for Language and
University of Alberta Information Processing (CIS)
Ludwig Maximilians University
{yx2,goebel,kondrak}@cs.ualberta.ca kristof@cis.uni-muenchen.de
Abstract
Most word segmentation methods em-
ployed in Chinese Information Retrieval
systems are based on a static dictionary
or a model trained against a manually
segmented corpus. These general seg-
mentation approaches may not be opti-
mal because they disregard information
within semantic units. We propose a novel
method for improving word-based Chi-
nese IR, which performs segmentation ac-
cording to the tightness of phrases. In
order to evaluate the effectiveness of our
method, we employ a new test collection
of 203 queries, which include a broad dis-
tribution of phrases with different tight-
ness values. The results of our experi-
ments indicate that our method improves
IR performance as compared with a gen-
eral word segmentation approach. The ex-
periments also demonstrate the need for
the development of better evaluation cor-
pora.
1 Introduction
What distinguishes Chinese Information Retrieval
from information retrieval (IR) in other languages
is the challenge of segmenting the queries and the
documents, created by the lack of word delimiters.
In general, there are two categories of segmenters:
character-based methods and word-based meth-
ods. Despite the superior performance of bigram
segmenters (Nie et al, 2000; Huang et al, 2000;
Foo and Li, 2004), word-based approaches con-
tinue to be investigated because of their applica-
tion in sophisticated IR tasks such as cross lan-
guage IR, and within techniques such as query ex-
pansion (Nie et al, 2000; Peng et al, 2002a).
Most word-based segmenters in Chinese IR are
either rule-based models, which rely on a lexi-
con, or statistical-based models, which are trained
on manually segmented corpora (Zhang et al,
2003). However, the relationship between the ac-
curacy of Chinese word segmentation and the per-
formance of Chinese IR is non-monotonic. Peng
et al (2002b) reported that segmentation meth-
ods achieving segmentation accuracy higher than
90% according to a manual segmentation standard
yield no improvement in IR performance. They
further argued that IR often benefits from splitting
compound words that are annotated as single units
by manual segmentation.
The essence of the problem is that there is no
clear definition of word in Chinese. Experiments
have shown only about 75% agreement among na-
tive speakers regarding the correct word segmen-
tation (Sproat et al, 1996). While units such as
??	? (peanut) and ???|? (match maker)
should clearly be considered as a single term in
Chinese IR, compounds such as ?????? (ma-
chine learning) are more controversial.1
Xu et al (2009) proposed a ?continuum hy-
pothesis? that rejects a clean binary classifica-
tion of Chinese semantic units as either compo-
sitional or non-compositional. Instead, they intro-
duced the notion of a tightness measure, which
quantifies the degree of compositionality. On
this tightness continuum, at one extreme are non-
1This issue is also present to a certain degree in languages
that do use explicit delimiters, including English (Halpern,
2000; McCarthy et al, 2003; Guenthner and Blanco, 2004).
55
compositional semantic units, such as ???
|? (match maker), and at the other end are se-
quences of consecutive words with no depen-
dency relationship, such as ??0??? (Shang-
hai where). In the middle of the spectrum are
compositional compounds such as ??????
(machine learning) and phrases such as ?thB
?? (legitimate income).
In this paper, we propose a method to ap-
ply the concept of semantic tightness to Chinese
IR, which refines the output of a general Chi-
nese word segmenter using tightness information.
In the first phase, we re-combine multiple units
that are considered semantically tight into single
terms. In the second phase, we break single units
that are not sufficiently tight. The experiments in-
volving two different IR systems demonstrate that
the newmethod improves IR performance as com-
pared to the general segmenter.
Most Chinese IR systems are evaluated on the
data from the TREC 5 and TREC 6 competi-
tions (Huang et al, 2000; Huang et al, 2003;
Nie et al, 2000; Peng et al, 2002a; Peng et al,
2002b; Shi and Nie, 2009). That data contains
only 54 queries, which are linked to relevancy-
judged documents. During our experiments, we
found the TREC query data is ill-suited for ana-
lyzing the effects of compound segmentation on
Chinese IR. For this reason, we created an addi-
tional set of queries based on the TREC corpus,
which includes a wide variety of semantic com-
pounds.
This paper is organized as follows. After sum-
marizing related work on Chinese IR and word
segmentation studies, we introduce the measure
of semantic tightness. Section 4 describes the in-
tegration of the semantic tightness measure into
an IR system. Section 5 discusses the available
data for Chinese IR evaluation, as well as an ap-
proach to acquire new data. Section 6 presents the
results of our method on word segmentation and
IR. A short conclusion wraps up and gives direc-
tions for future work.
2 Related Work
The impact of different Chinese word segmen-
tation methods on IR has received extensive at-
tention in the literature (Nie et al, 2000; Peng
et al, 2002a; Peng et al, 2002b; Huang et al,
2000; Huang et al, 2003; Liu et al, 2008; Shi
and Nie, 2009). For example, Foo and Li (2004)
tested the effects of manual segmentation and var-
ious character-based segmentations. In contrast
with most related work that only reports the over-
all performance, they provide an in-depth analysis
of query results. They note that a small test col-
lection diminishes the significance of the results.
In a series of papers on Chinese IR, Peng
and Huang compared various segmentation meth-
ods in IR, and proposed a new segmentation
method (Peng et al, 2002a; Peng et al, 2002b;
Huang et al, 2000; Huang et al, 2003). Their
experiments suggest that the relationship between
segmentation accuracy and retrieval performance
is non-monotonic, ranging from 44%-95%. They
hypothesize that weak word segmenters are able
to improve the accuracy of Chinese IR by break-
ing compound words into smaller constituents.
Shi and Nie (2009) proposed a probability-
based IR score function that combines a unigram
score with a word score according to ?phrase in-
separability.? Candidates for words in the query
are selected by a standard segmentation program.
Their results show a small improvement in com-
parison with a static combination of unigram and
word methods.
Liu et al (2008) is the research most similar
to our proposed method. They point out that cur-
rent segmentation methods which treat segmenta-
tion as a classification problem are not suitable
for Chinese IR. They propose a ranking support
vector machine (SVM) model to predict the inter-
nal association strength (IAS) between characters,
which is similar to our concept of tightness. How-
ever, they do not analyze their segmentation ac-
curacy with respect to a standard corpus, such as
Chinese Treebank. Their method does not reliably
segment function words, mistakenly identifying
?{|? (?s people) as tight, for example. Unlike
their approach, our segmentation method tackles
the problem by combining the tightness measure
with a general segmentation method.
Chinese word segmentation is closely related
to multiword expression extraction. McCarthy et
al. (2003) investigate various statistical measures
of compositionality of candidate multiword verbs.
56
Silva et al (1999) propose a new compositional-
ity measure based on statistical information. The
main difference with Xu et al?s measure is that
the latter is focused on word sense disambigua-
tion. In terms of multiword expressions in IR,
Vechtomova (2001) propose several approaches,
such as query expansion, to incorporating English
multiword expressions in IR. Braschler and Rip-
plinger (2004) analyze the effect of stemming and
decompounding on German text retrieval. How-
ever, Chinese compound segmentation in IR is a
thorny issue and needs more investigation for the
reasons mentioned earlier.
3 Semantic Tightness Continuum
We adopt the method developed by (Xu et al,
2009) for Chinese semantic unit tightness mea-
sure, which was shown to outperform the point-
wise mutual information method. For the sake
of completeness we briefly describe the basic ap-
proach here. The input of the measure is the prob-
ability distribution of a unit?s segmentation pat-
terns, i.e., potential segmentation candidates. The
output is a tightness value; the greater the value,
the tighter the unit. In this paper, we focus on 4-
gram sequences because 4-character compounds
are the most prominent in Chinese. There are
eight possible segmentations of any 4-character
sequence: ?ABCD,? ?A|BCD,? ?A|B|CD,? etc.
For a sequence of n characters, there are 2n?1 po-
tential segmentations. Equation 1 below defines
the tightness measure.
ratio =
?
??
??
?P t(s)
max(?P t(s1|s2))+ 1N
if ?P t(s) > ?
undef otherwise
(1)
In Equation 1, ?P t(s) stands for frequencies of
segmentation patterns of a potential semantic unit
s; Pt(s1|s2) is a pattern which segments the unit
s into two parts: s1 and s2; ? is a threshold to
exclude rare patterns; and N is a smoothing factor
which is set as the number of documents. Note
that when the first part of the denominator is zero,
the ratio of the unit will be very high. Intuitively,
the lack of certain separating patterns in the data
is evidence for the tightness of the units.
4 Application to Chinese IR
We propose a novel approach to segmentation
for Chinese IR which is based on the tight-
ness measure. Our segmenter revises the out-
put of a general segmenter according to the tight-
ness of units. The intuition behind our method
is that segmentation based on tightness of units
will lead to better IR performance. For exam-
ple, keeping ??CF? (Pinatubo) as a unit
should lead to better results than segmenting it
into ??(skin)|(include)|C(picture)|F(large)?.
On the other hand, segmenting the compositional
phrase ?)?)? (Kuwait country) into ?)?
(Kuwait)|)(country)? can improve recall. We
revise an initial segmentation in two steps: first,
we combine components that should not have
been separated, such as ??CF? (Pinatubo);
second, we split units which are compositional,
such as ?)?)? (Kuwait country).
In order to combine components, we first
extract 4-gram non-compositional compounds
whose tightness values are greater than a thresh-
old ?1 in a reference corpus, and then revise a
general segmenter by combining two separated
words if their combination is in the list. This ap-
proach is similar to the popular longest match first
method (LMF), but with segmentation chunks in-
stead of characters, and with the compound list
serving as the lexicon. For example, consider
a sequence ?ABCDEFGHIGK,? which a general
segmenter annotates as ?ABC|D|E|F|G|HI|GK.?
If our compound list constructed according to the
tightness measure contains {?DEFG?}, the re-
vised segmentation will be ?ABC|DEFG|HI|GK.?
Units of length less than 4 are segmented by using
the LMF rule against a dictionary.
In order to split a compositional unit, we set the
additional thresholds ?2, ?3, and ?4, and employ
the segmentation rules in Equation 2. The intu-
ition comes from the pattern lattice of a unit (Fig-
ure 1). For the patterns on the same level, the most
frequent pattern suggests the most reasonable seg-
mentation. For the patterns on different levels, the
frequency of each level indicates the tightness of
the unit.
57
Figure 1. The Lattice of the 8 Patterns.
if
v1 = ?Pt(ABCD)max(?Pt(A|BCD),?Pt(AB|CD),?Pt(ABC|D))+ 1N
> ?2
then ?ABCD? is one unit;
else if
v2 =
max(?Pt(A|BCD),?Pt(AB|CD),?Pt(ABC|D))+ 1N
max(?Pt(A|B|CD),?Pt(A|BC|D),?Pt(AB|C|D))+ 1N
> ?3
then ?ABCD? is segmented into two parts;
else if
v3 =
max(?Pt(A|B|CD),?Pt(A|BC|D),?Pt(AB|C|D))+ 1N
?Pt(A|B|C|D)+ 1N
> ?4
then ?ABCD? is segmented into three parts;
else
?ABCD? is segmented into four parts;
(2)
We apply the rules in Equation 2 to the se-
quence of 4-grams, with simple voting for select-
ing the segmentation pattern. For example, within
the sequence ?ABCDEF,? three 4-gram patterns
are considered: ?ABCD,? ?BCDE,? and ?CDEF.?
If only one of the 4-grams contains a segmentation
delimiter, the insertion of the delimiter depends
only upon that 4-gram. If two 4-grams contain the
same delimiter, the insertion of the delimiter de-
pends upon the two 4-grams. If the two 4-grams
disagree on the segmentation, a confidence value
is calculated as in Equation 3,
confidence = vi ? ?i+1, (3)
where i ? [1, 2, 3]. If three 4-grams contain the
same delimiter, voting is employed to decide the
segmentation. Returning to our example, suppose
that the first 4-gram is segmented as ?A|B|C|D,?
the second as ?BC|DE,? and the third as ?C|DE|F.?
Then the segmentation delimiter between ?A? and
?B? is inserted, but the delimiter between ?B? and
?C? depends on the confidence values of the first
two segmentation patterns. Finally, the delimiter
between ?C? and ?D? depends on the result of vot-
ing among the three 4-gram segmentations.
The two steps of combining and splitting can
either be applied in succession or separately. In
the former case, ?1 must be greater or equal to ?2.
In the remainder of this paper, we refer to the first
step as ?Tight Combine,? and to the second step
applied after the first step as ?Tight Split.? Note
that the second method can be used to segment
sentences directly instead of revising the output of
a general segmenter. This method, which we refer
to as ?Online Tight,? has the same shortcoming
as the method of Liu et al (2008), namely it fre-
quently fails to segment function words. For ex-
ample, it erroneously identifies ?{|? (?s people)
as tight. Therefore, we do not attempt to embed it
into the IR systems discussed in Section 6.
5 Test Collection
We analyzed the currently available Chinese test
collection of TREC, and found it unsuitable for
evaluating different strategies of compound seg-
mentation. One problem with the TREC data is
that the Chinese queries (topic titles) have too
many keywords. According to the output of ICT-
CLAS, a general segmenter, the average length of
Chinese queries is 12.2 words; in contrast, the av-
erage length of English ad-hoc queries in TREC-
5 and 6 (English topics 251-350) is 4.7. Even if
we use English translation of the Chinese queries
instead, the average length is still more than 7
words. The problem with long queries is that
they introduce complicating effects that interact
in ways difficult to understand. An example is
the co-occurrence between different keywords in
the base corpus. Sometimes a completely correct
segmentation causes a decrease in IR performance
because the score function assigns a higher score
to less important terms in a topic. For example,
for query 47 (Trec-6 dataset), ?9F5??C
F????????S??? (Philippines,
Mount Pinatubo, volcanic ash, magma, eruption),
preserving the unit Pinatubo makes the average
precision drop from 0.76 to 0.62 as compared to
the segmentation ??||C|F?. The score of the
58
unit is lower than that the sum of its components,
which results in a relatively low ranking for some
relevant documents. Another problem with the
TREC Chinese test collection is the small number
of queries (54). The number of of queries contain-
ing non-compositional words is smaller still. Sim-
ilarly, the other available corpus, NTCIR, com-
prises only 50 queries. In order to be confident of
our results, we would like to have a more substan-
tial number of queries containing units of varying
tightness.
Because of the shortcomings of available data
sets, we created our own test collection. There are
three components that define an IR test collection:
a query set, a corpus from which relevant docu-
ments are retrieved, and relevance judgements for
each query. Our criteria for gathering these com-
ponents are as follows.
First, the set of queries should contain both
tight queries and loose queries. For example,
there should be tight queries such as ???|?
(match maker), loose queries such as ??00?
(Shanghai customs), and queries with tightness
values in between, such as ?????? (machine
learning). Furthermore, the queries should be re-
alistic, rather than constructed by introspection.
In order to meet these requirements we randomly
chose 4-gram noun phrases (tagged by ICTCLAS)
from the TREC corpus. 51 queries are from a real
data set, the Sogou query logs2. The remaining
152 queries, which are selected manually based
on the initial 51 queries, represent queries that IR
system users are likely to enter. For example,
queries of locations and organizations are more
likely than queries such as ?how are you.? Fi-
nally, the queries should not be too general (i.e.,
resulting in too many relevant documents found),
nor too specific (no relevant documents). There-
fore, we selected the 4-grams which had the cor-
responding document frequency in the TREC cor-
pus between 30 and 300.
The second set of criteria concerns the rele-
vance judgements of documents. As our retrieval
corpus, we adopted the TREC Mandarin corpus,
which contains 24,959 documents. Because of re-
source limitation, we used the Minimum Test Col-
2Sogou query logs 2007 can be downloaded at
http://www.sogou.com/labs/dl/q.html.
lection (MTC) method (Carterette et al, 2006).
The method pools documents in such a way that
the documents which are best for discriminating
between different IR systems are judged first. We
applied this method on a document set that con-
tains all of the top 100 results of 8 IR systems
(two score functions, tf*idf and BM25, 4 index-
ing methods, unigram, bigram, ICTCLAS seg-
mentation, and our Tight Combine segmentation).
The systems were implemented with the Lucene
framework (http://lucene.apache.org/).
The last criterion determines which document
is relevant to a query. Annotators? opinions vary
about whether a document is relevant to a topic.
Is having the query in a document enough to be
the criterion of relevance? For the query ?Bei-
jing airport,? should the document that contains
the sentence ?Chairman Mao arrived at the Bei-
jing airport yesterday,? be classified as relevant?
Since our goal is to analyze the relationship be-
tween Chinese word segmentation, and IR, we
use weak relevant judgements. It is more related
to score functions to distinguish weak relevance
from strong relevance, that is, whether the query
is the topic of the document. This means the above
document is judged as relevant for the query ?Bei-
jing airport.?
In summary, our own test collection has about
200 queries, and at least 100 judged documents
per query with the TREC corpus as our base cor-
pus3.
6 Experiments
We conducted a series of experiments in word-
based Chinese information retrieval, with the aim
of establishing which segmenter is best for CIR,
while pursuing the best segmentation performance
in terms of segmented corpus is not the main crux.
In this section, we first present the accuracy of dif-
ferent segmentation methods, and then discuss the
results of IR systems.
6.1 Chinese Word Segmentation
ICTCLAS is a Chinese segmentation tool built by
the Institute of Computing Technology, Chinese
Academy of Sciences. Its segmentation model is a
3The query set and relevance judgements are available at
http://www.cs.ualberta.ca/?yx2/research.html
59
class-based hidden Markov model (HMM) model
(Zhang et al, 2003). The segmenter is trained
from manually segmented corpus, which makes
it ignore both the tightness of units and unknown
words such as ??CF? (Pinatubo), which are
difficult to identify.
In this experiment, we segmented the Chinese
Treebank using ICTCLAS and our three methods
that employ the tightness measure. The evalua-
tion is based on the manual segmentation of the
corpus. We evaluated the methods on the entire
Treebank corpus, employing 10-cross validation
for result significance verification.
In order to measure the tightness of Chinese
semantic units, pattern distributions of every 4-
gram were extracted from the Chinese Gigaword
corpus. Tight Combine is the ICTCLAS refined
segmentation that employs the non-compositional
compound list from the Chinese Gigaword cor-
pus. The threshold for non-compositional com-
pound ?1 is set to 11. Tight Split is the refined
segmentation of Tight Combine using Equation 2.
Online Tight is the segmentation using Equation
2 directly. For Tight Split and Online Tight, we
employed a lexicon which contains 41,245 words,
and set the thresholds ?2, ?3, and ?4 to 11, 0.01,
and 0.01, respectively. The parameters ?1 and ?2
are set according to the observation that the per-
centage of non-compositional units is high when
the tightness is greater than 11 for all the 4-grams
in the Chinese Gigaword corpus. The other two
parameters were established after experimenting
with several parameter pairs, such as (1,1), (0.1,
0.1), and (0.1, 0.01). We chose the one with the
best segmentation accuracy according to the stan-
dard corpus.
Table 1 shows the mean accuracy result over the
10 folders. The accuracy is the ratio of the number
of correctly segmented intervals to the number of
all intervals. The result shows that our method
improves over the ICTCLAS segmentation result,
but the improvement is not statistically significant
(measured by t-test). The only significant result is
that Online tight is worse than other methods.
Surprisingly, there is a large gap between
Tight Split and Online Tight, although they em-
ploy the same parameters. It turns out the ma-
jor difference lies in the segmentation of function
ICTCLAS 88.8%
Tight Combine 89.0%
Tight Split 89.1%
Online Tight 80.5%
Table 1. Segmentation accuracy of different seg-
menters.
words. Since it is based on ICTCLAS, Tight Split
does a good job in segmenting function words
such as verbal particles which represent past tense
??? and the nominalizer ?{.? Online Tight
tends to combine these words with the consecu-
tive one. For example, considering ????? (cu-
mulated), the Treebank and Tight Split segment it
into ???|?? (cumulate + particle); while On-
line Tight leaves it unsegmented.
6.2 IR Experiment Setup
We conducted our information retrieval experi-
ments using the Lucene package (Hatcher and
Gospodnetic, 2004). The documents and queries
were segmented by our three approaches before
indexing and searching process. In order to ana-
lyze the performance of our segmentation meth-
ods with different retrieval systems, we employed
two score functions: the BM25 function (Peng et
al., 2002b) 4; and BM25Beta (Function 4), which
prefers documents with more query terms.
Score(Q,D) ={
T
(1+?)?N
?T
i=0 score(ti, D) if T < N?N
i=0 score(ti, D) if T = N
(4)
In the above equation, score(ti, D) is the score
of the term ti in the document D. Although
we used BM25 as our base score function for
score(ti, D), it can be replaced by other score
functions, such as tf*idf, or a probability language
model. ? is a parameter to control a penalty com-
ponent for those documents that do not contain
all the query terms; T is the number of distinc-
tive query terms in the document; and N is the
number of query terms. The function penalizes
documents that do not contain all the query terms,
4An implementation of BM25 into Lucene can be down-
loaded at http://arxiv.org/abs/0911.5046
60
BM25 BM25Beta
ICTCLAS 62.78% 70.79%
Tight Combine 65.92% 71.19%
Tight Split 63.40% 70.95%
Table 2. MAP of different IR systems with differ-
ent segmenters.
which is an indirect way of incorporating proxim-
ity distance 5.
6.3 IR Experiment Results
Table 2 shows the comparison of our two seg-
menters to ICTCLAS on the IR task. The per-
formance of IR systems was measured by mean
average precision (MAP) of the query set. The re-
sults show that Tight Combine is better than the
ICTCLAS segmentation, especially when using
BM25. The relationship between Tight Split and
ICTCLAS is not clear.
In order to give a more in-depth analysis of
the word segmentation methods with respect to
the targeted phenomenon of semantic units, we
classified the 200 queries into three categories ac-
cording to their tightness as measured by func-
tion 1. The three classes are queries with tight-
ness in ranges [+?, 10), [10, 1), and [1, 0),
which contain 54, 41, and 108 queries respec-
tively. Queries in the range [+?, 10) are tight
queries, such as ?v?