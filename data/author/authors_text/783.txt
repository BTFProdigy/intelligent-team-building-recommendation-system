Proceedings of NAACL HLT 2009: Short Papers, pages 185?188,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
MICA: A Probabilistic Dependency Parser
Based on Tree Insertion Grammars
Application Note
Srinivas Bangalore Pierre Boulllier
AT&T Labs ? Research INRIA
Florham Park, NJ, USA Rocquencourt, France
srini@research.att.com Pierre.Boullier@inria.fr
Alexis Nasr Owen Rambow Beno??t Sagot
Aix-Marseille Universite? CCLS, Columbia Univserity INRIA
Marseille, France New York, NY, USA Rocquencourt, France
alexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.fr
Abstract
MICA is a dependency parser which returns
deep dependency representations, is fast, has
state-of-the-art performance, and is freely
available.
1 Overview
This application note presents a freely avail-
able parser, MICA (Marseille-INRIA-Columbia-
AT&T).1 MICA has several key characteristics that
make it appealing to researchers in NLP who need
an off-the-shelf parser.
? MICA returns a deep dependency parse, in
which dependency is defined in terms of lex-
ical predicate-argument structure, not in terms
of surface-syntactic features such as subject-verb
agreement. Function words such as auxiliaries
and determiners depend on their lexical head, and
strongly governed prepositions (such as to for give)
are treated as co-heads rather than as syntactic heads
in their own right. For example, John is giving books
to Mary gets the following analysis (the arc label is
on the terminal).
giving
John
arc=0
is
arc=adj
books
arc=1
to
arc=co-head
Mary
arc=2
The arc labels for the three arguments John,
books, and Mary do not change when the sentence
is passivized or Mary undergoes dative shift.
1We would like to thank Ryan Roth for contributing the
MALT data.
? MICA is based on an explicit phrase-structure
tree grammar extracted from the Penn Treebank.
Therefore, MICA can associate dependency parses
with rich linguistic information such as voice, the
presence of empty subjects (PRO), wh-movement,
and whether a verb heads a relative clause.
?MICA is fast (450 words per second plus 6 sec-
onds initialization on a standard high-end machine
on sentences with fewer than 200 words) and has
state-of-the-art performance (87.6% unlabeled de-
pendency accuracy, see Section 5).
? MICA consists of two processes: the supertag-
ger, which associates tags representing rich syntac-
tic information with the input word sequence, and
the actual parser, which derives the syntactic struc-
ture from the n-best chosen supertags. Only the su-
pertagger uses lexical information, the parser only
sees the supertag hypotheses.
? MICA returns n-best parses for arbitrary n;
parse trees are associated with probabilities. A
packed forest can also be returned.
? MICA is freely available2, easy to install under
Linux, and easy to use. (Input is one sentence per
line with no special tokenization required.)
There is an enormous amount of related work,
and we can mention only the most salient, given
space constraints. Our parser is very similar to the
work of (Shen and Joshi, 2005). They do not em-
ploy a supertagging step, and we do not restrict our
trees to spinal projections. Other parsers using su-
pertagging include the LDA of Bangalore and Joshi
(1999), the CCG-based parser of Clark and Curran
(2004), and the constraint-based approach of Wang
2http://www1.ccls.columbia.edu/?rambow/mica.html
185
and Harper (2004). Widely used dependency parsers
which generate deep dependency representations in-
clude Minipar (Lin, 1994), which uses a declarative
grammar, and the Stanford parser (Levy and Man-
ning, 2004), which performs a conversion from a
standard phrase-structure parse. All of these systems
generate dependency structures which are slightly
different from MICA?s, so that direct comparison
is difficult. For comparison purposes, we therefore
use the MALT parser generator (Nivre et al, 2004),
which allows us to train a dependency parser on our
own dependency structures. MALT has been among
the top performers in the CoNLL dependency pars-
ing competitions.
2 Supertags and Supertagging
Supertags are elementary trees of a lexicalized
tree grammar such as a Tree-Adjoining Gram-
mar (TAG) (Joshi, 1987). Unlike context-free gram-
mar rules which are single level trees, supertags are
multi-level trees which encapsulate both predicate-
argument structure of the anchor lexeme (by includ-
ing nodes at which its arguments must substitute)
and morpho-syntactic constraints such as subject-
verb agreement within the supertag associated with
the anchor. There are a number of supertags for each
lexeme to account for the different syntactic trans-
formations (relative clause, wh-question, passiviza-
tion etc.). For example, the verb give will be associ-
ated with at least these two trees, which we will call
tdi and tdi-dat. (There are also many other trees.)
tdi tdi-dat
S
NP0 ? VP
V? NP1 ? PP
P
to
NP2 ?
S
NP0 ? VP
V? NP2 ?NP1 ?
Supertagging is the task of disambiguating among
the set of supertags associated with each word in
a sentence, given the context of the sentence. In
order to arrive at a complete parse, the only step
remaining after supertagging is establishing the at-
tachments among the supertags. Hence the result of
supertagging is termed as an ?almost parse? (Banga-
lore and Joshi, 1999).
The set of supertags is derived from the Penn
Treebank using the approach of Chen (2001). This
extraction procedure results in a supertag set of
4,727 supertags and about one million words of su-
pertag annotated corpus. We use 950,028 annotated
words for training (Sections 02-21) and 46,451 (Sec-
tion 00) annotated words for testing in our exper-
iments. We estimate the probability of a tag se-
quence directly as in discriminative classification
approaches. In such approaches, the context of the
word being supertagged is encoded as features for
the classifier. Given the large scale multiclass la-
beling nature of the supertagging task, we train su-
pertagging models as one-vs-rest binary classifica-
tion problems. Detailed supertagging experiment re-
sults are reported in (Bangalore et al, 2005) which
we summarize here. We use the lexical, part-of-
speech attributes from the left and right context
in a 6-word window and the lexical, orthographic
(e.g. capitalization, prefix, suffix, digit) and part-
of-speech attributes of the word being supertagged.
Crucially, this set does not use the supertags for the
words in the history. Thus during decoding the su-
pertag assignment is done locally and does not need
a dynamic programming search. We trained a Max-
ent model with such features using the labeled data
set mentioned above and achieve an error rate of
11.48% on the test set.
3 Grammars and Models
MICA grammars are extracted in a three steps pro-
cess. In a first step, a Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995) is extracted from the
treebank, along with a table of counts. This is the
grammar that is used for supertagging, as described
in Section 2. In a second step, the TIG and the count
table are used to build a PCFG. During the last step,
the PCFG is ?specialized? in order to model more
finely some lexico-syntactic phenomena. The sec-
ond and third steps are discussed in this section.
The extracted TIG is transformed into a PCFG
which generates strings of supertags as follows. Ini-
tial elementary trees (which are substituted) yield
rules whose left hand side is the root category of
the elementary tree. Left (respectively right) aux-
iliary trees (the trees for which the foot node is the
186
left (resp. right) daughter of the root) give birth to
rules whose left-hand side is of the form Xl (resp.
Xr), where X is the root category of the elementary
tree. The right hand side of each rule is built during
a top down traversal of the corresponding elemen-
tary tree. For every node of the tree visited, a new
symbol is added to the right hand side of rule, from
left to right, as follows:
? The anchor of the elementary tree adds the su-
pertag (i.e., the name of the tree), which is a terminal
symbol, to the context-free rule.
? A substitution node in the elementary tree adds
its nonterminal symbol to the context-free rule.
? A interior node in the elementary tree at which
adjunction may occur adds to the context-free rule
the nonterminal symbol X ?r or X ?l , where X is the
node?s nonterminal symbol, and l (resp. r) indicates
whether it is a left (resp. right) adjunction. Each
interior node is visited twice, the first time from the
left, and then from the right. A set of non-lexicalized
rules (i.e., rules that do not generate a terminal sym-
bol) allow us to generate zero or more trees anchored
by Xl from the symbol X ?l . No adjunction, the first
adjunction, and the second adjunction are modeled
explicitly in the grammar and the associated prob-
abilistic model, while the third and all subsequent
adjunctions are modeled together.
This conversion method is basically the same as
that presented in (Schabes and Waters, 1995), ex-
cept that our PCFG models multiple adjunctions at
the same node by positions (a concern Schabes and
Waters (1995) do not share, of course). Our PCFG
construction differs from that of Hwa (2001) in that
she does not allow multiple adjunction at one node
(Schabes and Shieber, 1994) (which we do since we
are interested in the derivation structure as a repre-
sentation of linguistic dependency). For more in-
formation about the positional model of adjunction
and a discussion of an alternate model, the ?bigram
model?, see (Nasr and Rambow, 2006).
Tree tdi from Section 2 gives rise to the following
rule (where tdi and tCO are terminal symbols and
the rest are nonterminals): S ? S?l NP VP?l V?l tdiV?r NP PP?l P?l tCO P?r NP PP?r VP?r S?r
The probabilities of the PCFG rules are estimated
using maximum likelihood. The probabilistic model
refers only to supertag names, not to words. In the
basic model, the probability of the adjunction or sub-
stitution of an elementary tree (the daughter) in an-
other elementary tree (the mother) only depends on
the nonterminal, and does not depend on the mother
nor on the node on which the attachment is per-
formed in the mother elementary tree. It is well
known that such a dependency is important for an
adequate probabilistic modelling of syntax. In order
to introduce such a dependency, we condition an at-
tachment on the mother and on the node on which
the attachment is performed, an operation that we
call mother specialization. Mother specialization is
performed by adding to all nonterminals the name of
the mother and the address of a node. The special-
ization of a grammar increase vastly the number of
symbols and rules and provoke severe data sparse-
ness problems, this is why only a subset of the sym-
bols are specialized.
4 Parser
SYNTAX (Boullier and Deschamp, 1988) is a sys-
tem used to generate lexical and syntactic analyzers
(parsers) (both deterministic and non-deterministic)
for all kind of context-free grammars (CFGs) as
well as some classes of contextual grammars. It
has been under development at INRIA for several
decades. SYNTAX handles most classes of determin-
istic (unambiguous) grammars (LR, LALR, RLR)
as well as general context-free grammars. The
non-deterministic features include, among others,
an Earley-like parser generator used for natural lan-
guage processing (Boullier, 2003).
Like most SYNTAX Earley-like parsers, the archi-
tecture of MICA?s PCFG-based parser is the follow-
ing:
? The Earley-like parser proper computes a shared
parse forest that represents in a factorized (polyno-
mial) way all possible parse trees according to the
underlying (non-probabilistic) CFG that represents
the TIG;
? Filtering and/or decoration modules are applied
on the shared parse forest; in MICA?s case, an n-
best module is applied, followed by a dependency
extractor that relies on the TIG structure of the CFG.
The Earley-like parser relies on Earley?s algo-
rithm (Earley, 1970). However, several optimiza-
tions have been applied, including guiding tech-
niques (Boullier, 2003), extensive static (offline)
187
computations over the grammar, and efficient data
structures. Moreover, Earley?s algorithm has been
extended so as to handle input DAGs (and not only
sequences of forms). A particular effort has been
made to handle huge grammars (over 1 million
symbol occurrences in the grammar), thanks to ad-
vanced dynamic lexicalization techniques (Boullier
and Sagot, 2007). The resulting efficiency is satisfy-
ing: with standard ambiguous NLP grammars, huge
shared parse forest (over 1010 trees) are often gener-
ated in a few dozens of milliseconds.
Within MICA, the first module that is applied on
top of the shared parse forest is SYNTAX?s n-best
module. This module adapts and implements the al-
gorithm of (Huang and Chiang, 2005) for efficient
n-best trees extraction from a shared parse forest. In
practice, and within the current version of MICA,
this module is usually used with n = 1, which iden-
tifies the optimal tree w.r.t. the probabilistic model
embedded in the original PCFG; other values can
also be used. Once the n-best trees have been ex-
tracted, the dependency extractor module transforms
each of these trees into a dependency tree, by ex-
ploiting the fact that the CFG used for parsing has
been built from a TIG.
5 Evaluation
We compare MICA to the MALT parser. Both
parsers are trained on sections 02-21 of our de-
pendency version of the WSJ PennTreebank, and
tested on Section 00, not counting true punctuation.
?Predicted? refers to tags (PTB-tagset POS and su-
pertags) predicted by our taggers; ?Gold? refers to
the gold POS and supertags. We tested MALT using
only POS tags (MALT-POS), and POS tags as well
as 1-best supertags (MALT-all). We provide unla-
beled (?Un?) and labeled (?Lb?) dependency accu-
racy (%). As we can see, the predicted supertags do
not help MALT. MALT is significantly slower than
MICA, running at about 30 words a second (MICA:
450 words a second).
MICA MALT-POS MALT-all
Pred Gold Pred Gold Pred Gold
Lb 85.8 97.3 86.9 87.4 86.8 96.9
Un 87.6 97.6 88.9 89.3 88.5 97.2
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?266.
Srinivas Bangalore, Patrick Haffner, and Gae?l Emami.
2005. Factoring global inference by enriching local rep-
resentations. Technical report, AT&T Labs ? Reserach.
Pierre Boullier and Philippe Deschamp.
1988. Le syste`me SYNTAXTM ? manuel
d?utilisation et de mise en ?uvre sous UNIXTM.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Beno??t Sagot. 2007. Are very large
grammars computationnaly tractable? In Proceedings of
IWPT?07, Prague, Czech Republic.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 7th International Workshop on =20 Pars-
ing Technologies, pages 43?54, Nancy, France.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D. the-
sis, University of Delaware.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL?04.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, Vancouver, Canada.
Rebecca Hwa. 2001. Learning Probabilistic Lexicalized
Grammars for Natural Language Processing. Ph.D. the-
sis, Harvard University.
Aravind K. Joshi. 1987. An introduction to Tree Ad-
joining Grammars. In A. Manaster-Ramer, editor, Math-
ematics of Language. John Benjamins, Amsterdam.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Correct-
ing the surface dependency approximation. In ACL?04.
Dekang Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Coling?94.
Alexis Nasr and Owen Rambow. 2006. Parsing with
lexicalized probabilistic recursive transition networks. In
Finite-State Methods and Natural Language Processing,
Springer Verlag Lecture Notes in Commputer Science.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL-2004.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computational
Linguistics, 1(20):91?124.
Yves Schabes and Richard C. Waters. 1995. Tree Inser-
tion Grammar. Computational Linguistics, 21(4).
Libin Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP?05.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proceed-
ings of the ACL Workshop on Incremental Parsing.
188
Guided Parsing of Range Concatenation Languages
Franc?ois Barthe?lemy, Pierre Boullier, Philippe Deschamp and ?Eric de la Clergerie
INRIA-Rocquencourt
Domaine de Voluceau
B.P. 105
78153 Le Chesnay Cedex, France
 
Francois.Barthelemy Pierre.Boullier
Philippe.Deschamp Eric.De La Clergerie  @inria.fr
Abstract
The theoretical study of the range
concatenation grammar [RCG] formal-
ism has revealed many attractive prop-
erties which may be used in NLP.
In particular, range concatenation lan-
guages [RCL] can be parsed in poly-
nomial time and many classical gram-
matical formalisms can be translated
into equivalent RCGs without increas-
ing their worst-case parsing time com-
plexity. For example, after transla-
tion into an equivalent RCG, any tree
adjoining grammar can be parsed in

	
time. In this paper, we study a
parsing technique whose purpose is to
improve the practical efficiency of RCL
parsers. The non-deterministic parsing
choices of the main parser for a lan-
guage  are directed by a guide which
uses the shared derivation forest output
by a prior RCL parser for a suitable su-
perset of  . The results of a practi-
cal evaluation of this method on a wide
coverage English grammar are given.
1 Introduction
Usually, during a nondeterministic process, when
a nondeterministic choice occurs, one explores all
possible ways, either in parallel or one after the
other, using a backtracking mechanism. In both
cases, the nondeterministic process may be as-
sisted by another process to which it asks its way.
This assistant may be either a guide or an oracle.
An oracle always indicates all the good ways that
will eventually lead to success, and those good
ways only, while a guide will indicate all the good
ways but may also indicate some wrong ways. In
other words, an oracle is a perfect guide (Kay,
2000), and the worst guide indicates all possi-
ble ways. Given two problems  and  and
their respective solutions  and  , if they are
such that  , any algorithm which solves

 is a candidate guide for nondeterministic al-
gorithms solving  . Obviously, supplementary
conditions have to be fulfilled for  to be a guide.
The first one deals with relative efficiency: it as-
sumes that problem  can be solved more effi-
ciently than problem   . Of course, parsers are
privileged candidates to be guided. In this pa-
per we apply this technique to the parsing of a
subset of RCLs that are the languages defined by
RCGs. The syntactic formalism of RCGs is pow-
erful while staying computationally tractable. In-
deed, the positive version of RCGs [PRCGs] de-
fines positive RCLs [PRCLs] that exactly cover
the class PTIME of languages recognizable in de-
terministic polynomial time. For example, any
mildly context-sensitive language is a PRCL.
In Section 2, we present the definitions of
PRCGs and PRCLs. Then, in Section 3, we de-
sign an algorithm which transforms any PRCL 
into another PRCL  , ffTools and resources for Tree Adjoining Grammars
Fran?ois Barth?lemy,
CEDRIC ? CNAM,
92 Rue St Martin
FR-75141 Paris Cedex 03
barthe@cnam.fr
Pierre Boullier, Philippe Deschamp
Linda Kaouane, Abdelaziz Khajour
?ric Villemonte de la Clergerie
ATOLL - INRIA,
Domaine de Voluceau - BP 105
FR-78153 Le Chesnay Cedex
Eric.De_La_Clergerie@inria.fr
Abstract
This paper presents a workbench for
Tree Adjoining Grammars that we are
currently developing. This workbench
includes several tools and resources
based on the markup language XML,
used as a convenient language to format
and exchange linguistic resources.
1 Introduction
Our primary concern lies in the developmen-
t of efficient parsers for various grammatical for-
malisms of interest for Natural Language Pro-
cessing. Tree Adjoining Grammars [TAG] is one
of these formalisms, important from a linguistic
point of view but also because it is possible to de-
sign efficient parsers.
However, during our work on TAG, we were
confronted with a lack of standardization of gram-
mars, especially when dealing with wide cover-
age grammars. The XTAG System1 (The XTAG
Research Group, 1995) provides an implicit stan-
dard, but it is not very readable and lacks explic-
it specifications. The various grammars we stud-
ied presented many variations. Moreover, we also
noted many problems of consistencies in most of
them.
Following others, amongst whom LT XML2
and especially (Bonhomme and Lopez, 2000),
we considered that the markup language XML3
1http://www.cis.upenn.edu/~xtag/
2http://www.ltg.ed.ac.uk/
3http://www.w3c.org/XML/
would be a good choice to represent TAG, espe-
cially with the possibility of providing an explicit
and logical specification via a DTD. Being textu-
al, resources in XML can be read by humans and
easily exchanged and maintained. Finally, there
exists more and more supports to handle XML re-
sources. We have also found that XML is a conve-
nient language to store linguistic results, such as
the shared derivation forests output by our TAG
parsers.
The paper starts with a brief introduction to
TAGs. Section 3 presents the different XML en-
codings that we have designed for the representa-
tion of grammars and derivation forests. Section 4
presents several different maintenance tools we
are developing to handle grammars and deriva-
tion forests. Section 5 presents servers used to
access different kind of informations. Interfaces
for these servers are presented in Section 6.
2 Tree Adjoining Grammars
The TAG formalism (Joshi, 1987) is particular-
ly suitable to describe many linguistic phenome-
na. A TAG is given by a set of elementary trees
partitioned into initial trees and auxiliary trees.
Internal nodes are labeled by non-terminals and
leaves by non-terminal or terminals. Each auxil-
iary tree
 
has a distinguished leaf, called its foot
and labeled by a non-terminal, the same as the
root node of
 
.
Two operations may be used to derive trees
from elementary trees. The first one, called sub-
stitution, replaces a leaf node labeled by a non-
terminal  by an initial tree  whose root is also
labeled by  . The second operation, called ad-
Adjunction
node v
Root
Foot
Spine
Auxiliary Tree 
Adjunction
A
A
A
A
A
?
Figure 1: Adjunction
junction, is illustrated by Figure 1. An auxiliary
tree
 
whose root is labeled by  may be adjoined
at any node  labeled by  . The subtree rooted
at  is grafted to the foot of
 
.
Feature TAGs extend TAGs by attaching to n-
odes a pair of first-order terms represented by
Feature Structures [FS] and called top and bot-
tom arguments. These feature structures may be
used, for instance, to handle agreement or enforce
semantic restrictions.
Lexicalized (Feature) TAGs assumes that each
elementary tree has at least one lexical node la-
beled by a terminal. However, explicit lexicalized
grammars would be huge, with one or more ele-
mentary trees for each entry in the lexicon. The
choice made by the XTAG group and by all the
designers of wide coverage TAGs is to factor-
ize the grammars and gives enough information
to lexicalize parts of the grammars when needed.
Morphological entries (or inflected forms) refer-
ence one or more lemma entries, which, in turn,
refer to families of tree schema. A tree schema is
an elementary tree with a distinguished leaf called
anchor node that is to be replaced by a morpho-
logical entry. Each reference may be completed
by additional constraints.
For instance, extracted from a small French
grammar, Figure 2 shows the basic elements
(morphological entry donne, lemma \DONNER\,
and tree schema tn1pn2) used to build the tree
tn1pn2(donne) corresponding to the syntactic
pattern (1) and illustrated by sentence (2). The
lemma part states that the subject NP  and the
prepositional complement NP  must both be hu-
man and that NP  is introduced by the preposi-
tion ? (co-anchoring). In the tree tn1pn2, the
substitution nodes are marked with  and the an-
chor node with 	 .
(1) quelqu?un
somebody
donne
gives
quelque chose
something
?
to
quelqu?un
somebody
(2) Yves
Yves
donne
gives
un
a
joli
nice
livre
book
?
to
Sabine
Sabine
donne: \DONNER,\ V
{mode=,indnum=sing}
\DONNER,\V: tn1pn2[p_2=?]
{NP_0.t:restr=+,hum
NP_2.t:restr=+hum}
S
tn1pn2
NP 
 VP
V
<>V
NP  PP
P  NP 
Figure 2: Tree schema
3 XML Encoding
3.1 Representing grammars
We have designed a DTD4 that clearly specifies
the relations between the various components of
a grammar. For instance, the following DTD frag-
ment states that a morphological entry is char-
acterized by a field lex and includes zero or
more description entries (for documentation) and
at least one reference to a lemma (lemmaref).
Similarly, an element lemmaref is character-
ized by the fields name and cat, and may be
completed by a FS argument (fs).
<!ELEMENT morph (desc*,lemmaref+)>
<!ATTLIST morph lex CDATA #REQUIRED>
<!ELEMENT lemmaref (fs?)>
<!ATTLIST lemmaref name CDATA #REQUIRED
cat CDATA #REQUIRED>
Following the DTD, the various elements de-
scribed in Figure 2 may be represented by the
(tiny) following XML fragment, omitting the FS
specification on nodes for sake of space and clar-
ity.
<tag axiom="s">
<morph lex="donne">
<lemmaref cat="v" name="*DONNER*">
<fs>
<f name="mode">
<val>ind</val>
<val>subj</val>
4http://atoll.inria.fr/~clerger/tag.dtd,xml
</f>
<f name="num">
<val>sing</val>
</f>
</fs>
</lemmaref>
</morph>
<lemma cat="v" name="*DONNER*">
<anchor tree_id="family[@name=tn1pn2]">
<coanchor node_id="p_2">
<lex>?</lex>
</coanchor>
<equation node_id="np_0" type="top">
<fs>
<f name="restr">
<val>plushum</val>
</f>
</fs>
</equation>
<equation node_id="np_2" type="top">
<fs>
<f name="restr">
<val>plushum</val>
</f>
</fs>
</equation>
</anchor>
</lemma>
<family name="tn1pn2">
<tree name="tn1pn2">
<node cat="s" adj="yes" type="std">
<node cat="np" id="np_0" type="subst" />
<node cat="vp" adj="yes" type="std">
<node cat="v" adj="yes" type="anchor" />
<node cat="np" type="subst" />
<node cat="pp" adj="yes" type="std">
<node cat="p" id="p_2" type="subst"/>
<node cat="np" id="np_2" type="subst"/>
</node>
</node>
</node>
</tree>
</family>
</tag>
Currently, we have encoded a small French
grammar (50 tree schemata, 117 lemmas and
345 morphological entries) and an English gram-
mar (456 tree schemata, 333 lemmas and
507 morphological entries). We are processing
some other larger grammars (for both English and
French).
3.2 Encoding derivations
A (deterministic) TAG parser may return either
the result of the analysis as a parse tree, or the
steps of the derivation as a derivation tree. These
two alternatives are illustrated for sentence (3) by
Figures 3 and 4 (with Figure 5 showing the ele-
mentary lexicalized trees). A derivation tree in-
dicates which operation (substitution or adjunc-
tion of some tree) has taken place on which node
of which tree, for instance the adjunction of tree
a  joli  at node labeled N. It is worth noting that
the parse tree may be retrieved from the deriva-
tion tree, which motivates our interest in deriva-
tion trees.
(3) Yves
Yves
donne
gives
un
a
joli
nice
livre
book
?
to
Sabine
Sabine
S
NP
Yves
VP
V
donne
NP
NP
D
un
N
Adj
joli
N
livre
PP
P
?
NP
Sabine
Figure 3: Parse Tree
subst 
tn1pn2(donne,?)
subst  
np(Yves)
subst 
npdn(livre)
subst  
d(un)
adj ffProceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 1?10,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Efficient and robust LFG parsing: SXLFG
Pierre Boullier and Beno?t Sagot
INRIA-Rocquencourt, Projet Atoll,
Domaine de Voluceau, Rocquencourt B.P. 105
78 153 Le Chesnay Cedex, France
{pierre.boullier, benoit.sagot}@inria.fr
Abstract
In this paper, we introduce a new parser,
called SXLFG, based on the Lexical-
Functional Grammars formalism (LFG).
We describe the underlying context-free
parser and how functional structures are
efficiently computed on top of the CFG
shared forest thanks to computation shar-
ing, lazy evaluation, and compact data
representation. We then present vari-
ous error recovery techniques we imple-
mented in order to build a robust parser.
Finally, we offer concrete results when
SXLFG is used with an existing gram-
mar for French. We show that our parser
is both efficient and robust, although the
grammar is very ambiguous.
1 Introduction
In order to tackle the algorithmic difficulties of
parsers when applied to real-life corpora, it is nowa-
days usual to apply robust and efficient methods
such as Markovian techniques or finite automata.
These methods are perfectly suited for a large num-
ber of applications that do not rely on a complex rep-
resentation of the sentence. However, the descriptive
expressivity of resulting analyses is far below what
is needed to represent, e.g., phrases or long-distance
dependencies in a way that is consistent with seri-
ous linguistic definitions of these concepts. For this
reason, we designed a parser that is compatible with
a linguistic theory, namely LFG, as well as robust
and efficient despite the high variability of language
production.
Developing a new parser for LFG (Lexical-
Functional Grammars, see, e.g., (Kaplan, 1989)) is
not in itself very original. Several LFG parsers al-
ready exist, including those of (Andrews, 1990) or
(Briffault et al, 1997). However, the most famous
LFG system is undoubtedly the Xerox Linguistics
Environment (XLE) project which is the successor
of the Grammars Writer?s Workbench (Kaplan and
Maxwell, 1994; Riezler et al, 2002; Kaplan et al,
2004). XLE is a large project which concentrates a
lot of linguistic and computational technology, relies
on a similar point of view on the balance between
shallow and deep parsing, and has been successfully
used to parse large unrestricted corpora.
Nevertheless, these parsers do not always use in
the most extensive way all existing algorithmic tech-
niques of computation sharing and compact infor-
mation representation that make it possible to write
an efficient LFG parser, despite the fact that the LFG
formalism, as many other formalisms relying on uni-
fication, is NP-hard. Of course our purpose is not to
make a new XLE system but to study how robust-
ness and efficiency can be reached in LFG parsing
on raw text.
Building constituent structures (c-structures) does
not raise any particular problem in theory,1 be-
cause they are described in LFG by a context-free
grammar (CFG), called (CF) backbone in this pa-
per. Indeed, general parsing algorithms for CFGs
are well-known (Earley, GLR,. . . ). On the other
hand, the efficient construction of functional struc-
tures (f-structures) is much more problematic. The
first choice that a parser designer must face is that
of when f-structures are processed: either during CF
1In practice, the availability of a good parser is sometimes
less straightforward.
1
parsing (interleaved method) or in a second phase
(two-pass computation). The second choice is be-
tween f-structures evaluation on single individual
[sub-]parses ([sub-]trees) or on a complete represen-
tation of all parses. We choose to process all phrasal
constraints by a CF parser which produces a shared
forest2 of polynomial size in polynomial time. Sec-
ond, this shared forest is used, as a whole, to de-
cide which functional constraints to process. For
ambiguous CF backbones, this two pass computa-
tion is more efficient than interleaving phrasal and
functional constraints.3 Another advantage of this
two pass vision is that the CF parser may be easily
replaced by another one. It may also be replaced
by a more powerful parser.4 We choose to evalu-
ate functional constraints directly on the shared for-
est since it has been proven (See (Maxwell and Ka-
plan, 1993)), as one can easily expect, that tech-
niques which evaluate functional constraints on an
enumeration of the resulting phrase-structure trees
are a computational disaster. This article explores
the computation of f-structures directly (without un-
folding) on shared forests. We will see how, in some
cases, our parser allows to deal with potential com-
binatorial explosion. Moreover, at all levels, error
recovering mechanisms turn our system into a robust
parser.
Our parser, called SXLFG, has been evaluated
with two large-coverage grammars for French, on
corpora of various genres. In the last section of this
paper, we present quantitative results of SXLFG us-
ing one of these grammars on a general journalistic
corpus.
2 The SXLFG parser: plain parsing
This section describes the parsing process for fully
grammatical sentences. Error recovery mechanisms,
that are used when this is not the case, are described
in the next section.
2Informally, a shared forest is a structure which can repre-
sent a set of parses (even an unbounded number) in a way that
shares all common sub-parses.
3This fact can be easily understood by considering that func-
tional constraints may be constructed in exponential time on
a sub-forest that may well be discarded later on by (future)
phrasal constraints.
4For example, we next plan to use an RCG backbone (see
(Boullier, 2004) for an introduction to RCGs), with the func-
tional constraints being evaluated on the shared forest output by
an RCG parser.
2.1 Architecture overview
The core of SXLFG is a general CF parser that pro-
cesses the CF backbone of the LFG. It is an Earley-
like parser that relies on an underlying left-corner
automaton and is an evolution of (Boullier, 2003).
The set of analyses produced by this parser is rep-
resented by a shared parse forest. In fact, this parse
forest may itself be seen as a CFG whose produc-
tions are instantiated productions of the backbone.5
The evaluation of the functional equations is per-
formed during a bottom-up left-to-right walk in this
forest. A disambiguation module, which discards
unselected f-structures, may be invoked on any node
of the forest including of course its root node.
The input of the parser is a word lattice (all words
being known by the lexicon, including special words
representing unknown tokens of the raw text). This
lattice is converted by the lexer in a lexeme lattice
(a lexeme being here a CFG terminal symbol asso-
ciated with underspecified f-structures).
2.2 The context-free parser
The evolutions of the Earley parser compared to that
described in (Boullier, 2003) are of two kinds: it ac-
cepts lattices (or DAGs) as input and it has syntac-
tic error recovery mechanisms. This second point
will be examined in section 3.1. Dealing with DAGs
as input does not require, at least from a theoreti-
cal standpoint, considerable changes in the Earley
algorithm.6 Since the Earley parser is guided by
a left-corner finite automaton that defines a regular
super-set of the CF backbone language, this automa-
ton also deals with DAGs as input (this corresponds
to an intersection of two finite automata).
5If A is a non-terminal symbol of the backbone, A
ij
is an in-
stantiated non-terminal symbol if and only if A
ij
+
?
G
a
i+1
. . . a
j
where w = a
1
. . . a
n
is the input string and
+
?
G
the transitive
closure of the derives relation.
6If i is a node of the DAG and if we have a transition on the
terminal t to the node j (without any loss in generality, we can
suppose that j > i) and if the Earley item [A ? ?.t?, k] is
an element of the table T [i], then we can add to the table T [j]
the item [A ? ?t.?, k] if it is not already there. One must
take care to begin a PREDICTOR phase in a T [j] table only if
all Earley phases (PREDICTOR, COMPLETOR and SCANNER)
have already been performed in all tables T [i], i < j.
2
2.3 F-Structures computation
As noted in (Kaplan and Bresnan, 1982), if the num-
ber of CF parses (c-structures) grows exponentially
w.r.t. the length of the input, it takes exponential
time to build and check the associated f-structures.
Our experience shows that the CF backbone for large
LFGs may be highly ambiguous (cf. Section 4).
This means that (full) parsing of long sentences
would be intractable. Although in CF parsing an ex-
ponential (or even unbounded) number of parse trees
can be computed and packed in polynomial time in
a shared forest, the same result cannot be achieved
with f-structures for several reasons.7 However, this
intractable behavior (and many others) may well not
occur in practical NL applications, or some tech-
niques (See Section 2.4) may be applied to restrict
this combinatorial explosion.
Efficient computation of unification-based struc-
tures on a shared forest is still a evolving research
field. However, this problem is simplified if struc-
tures are monotonic, as is the case in LFG. In such
a case the support (i.e., the shared forest) does not
need to be modified during the functional equation
resolution. If we adopt a bottom-up left-to-right
traversal strategy in the shared forest, information
in f-structures is cumulated in a synthesized way.
This means that the evaluation of a sub-forest8 is
only performed once, even when this sub-forest is
shared by several parent nodes. In fact, the effect
of a complete functional evaluation is to associate
to each node of the parse forest a set of partial f-
structures which only depends upon the descendants
of that node (excluding its parent or sister nodes).
The result of our LFG parser is the set of (com-
plete and consistent, if possible) main f-structures
(i.e., the f-structures associated to the root of the
shared forest), or, when a partial analysis occurs,
7As an example, it is possible, in LFG, to define f-structures
which encode individual parses. If a polynomial sized shared
forest represents an exponential number of parses, the number
of different f-structures associated to the root of the shared for-
est would be that exponential number of parses. In other words,
there are cases where no computational sharing of f-structures
is possible.
8If the CF backbone G is cyclic (i.e., ?A s.t. A +?
G
A), the
forest may be a general graph, and not only a DAG. Though our
CF parser supports this case, we exclude it in SXLFG. Of course
this (little) restriction does not mean that cyclic f-structures are
also prohibited. SXLFG does support cyclic f-structures, which
can be an elegant way to represent some linguistic relations.
the sets of (partial) f-structures which are associated
with maximal internal nodes). Such sets of (partial
or not) f-structures could be factored out in a single
f-structure containing disjunctive values, as in XLE.
We decided not to use these complex disjunctive val-
ues, except for some atomic types, but rather to asso-
ciate to any (partial) f-structure a unique identifier:
two identical f-structures will always have the same
identifier throughout the whole process. Experi-
ments (not reported here) show that this strategy is
worth using and that the total number of f-structures
built during a complete parse remains very tractable,
except maybe in some pathological cases.
As in XLE, we use a lazy copying strategy during
unification. When two f-structures are unified, we
only copy their common parts which are needed to
check whether these f-structures are unifiable. This
restricts the quantity of copies between two daughter
nodes to the parts where they interact. Of course,
the original daughter f-structures are left untouched
(and thus can be reused in another context).
2.4 Internal and global disambiguation
Applications of parsing systems often need a dis-
ambiguated result, thus calling for disambiguation
techniques to be applied on the ambiguous output
of parsers such as SXLFG. In our case, this im-
plies developing disambiguation procedures in order
to choose the most likely one(s) amongst the main f-
structures. Afterwards, the shared forest is pruned,
retaining only c-structures that are compatible with
the chosen main f-structure(s).
On the other hand, on any internal node of the for-
est, a possibly huge number of f-structures may be
computed. If nothing is done, these numerous struc-
tures may lead to a combinatorial explosion that pre-
vents parsing from terminating in a reasonable time.
Therefore, it seems sensible to allow the grammar
designer to point out in his or her grammar a set of
non-terminal symbols that have a linguistic property
of (quasi-)saturation, making it possible to apply
on them disambiguation techniques.9 Hence, some
non-terminals of the CF backbone that correspond
9Such an approach is indeed more satisfying than a blind
skimming that stops the full processing of the sentence when-
ever the amount of time or memory spent on a sentence exceeds
a user-specified limit, replacing it by a partial processing that
performs a bounded amount of work on each remaining non-
terminal (Riezler et al, 2002; Kaplan et al, 2004).
3
to linguistically saturated phrases may be associ-
ated with an ordered list of disambiguation meth-
ods, each of these non-terminals having its own list.
This allows for swift filtering out on relevant internal
nodes of f-structures that could arguably only lead
to inconsistent and/or incomplete main f-structures,
or that would be discarded later on by applying the
same method on the main f-structures. Concomi-
tantly, this leads to a significant improvement of
parsing times. This view is a generalization of the
classical disambiguation method described above,
since the pruning of f-structures (and incidentally
of the forest itself) is not reserved any more to the
axiom of the CF backbone. We call global disam-
biguation the pruning of the main f-structures, and
internal disambiguation the same process applied
on internal nodes of the forest. It must be noticed
that neither disambiguation type necessarily leads
to a unique f-structure. Disambiguation is merely
a shortcut for partial or total disambiguation.
Disambiguation methods are generally divided
into probabilistic and rule-based techniques. Our
parsing architecture allows for implementing both
kinds of methods, provided the computations can
be performed on f-structures. It allows to asso-
ciate a weight with all f-structures of a given in-
stantiated non-terminal.10 Applying a disambigua-
tion rule consists in eliminating of all f-structures
that are not optimal according to this rule. Each op-
tional rule is applied in a cascading fashion (one can
change the order, or even not apply them at all).
After this disambiguation mechanism on f-
structures, the shared forest (that represent c-
structures) is filtered out so as to correspond exactly
to the f-structure(s) that have been kept. In partic-
ular, if the disambiguation is complete (only one f-
structure has been kept), this filtering yields in gen-
eral a unique c-structure (a tree).
10See (Kinyon, 2000) for an argumentation on the impor-
tance of performing disambiguation on structures such as TAG
derivation trees or LFG f-structures and not constituent(-like)
structures.
3 Techniques for robust parsing
3.1 Error recovery in the CF parser
The detection of an error in the Earley parser11 can
be caused by two different phenomena: the CF back-
bone has not a large enough coverage or the input is
not in its language. Of course, although the parser
cannot make the difference between both causes,
parser and grammar developers must deal with them
differently. In both cases, the parser has to be able
to perform recovery so as to resume parsing as well
as, if possible, to correctly parse valid portions of
incorrect inputs, while preserving a sensible relation
between these valid portions. Dealing with errors
in parsers is a field of research that has been mostly
addressed in the deterministic case and rarely in the
case of general CF parsers.
We have implemented two recovery strategies in
our Earley parser, that are tried one after the other.
The first strategy is called forward recovery, the
second one backward recovery.12 Both generate a
shared forest, as in the regular case.
The mechanism is the following. If, at a certain
point, the parsing is blocked, we then jump forward
a certain amount of terminal symbols so as to be able
to resume parsing. Formally, in an Earley parser
whose input is a DAG, an error is detected when,
whatever the active table T [j], items of the form
I = [A ? ?.t?, i] in this table are such that in the
DAG there is no out-transition on t from node j. We
say that a recovery is possible in k on ? if in the suf-
fix ? = ?
1
X?
2
there exists a derived phrase from
the symbol X which starts with a terminal symbol
r and if there exists a node k in the DAG, k ? j,
with an out-transition on r. If it is the case and if
this possible recovery is selected, we put the item
[A ? ?t?
1
.X?
2
, i] in table T [k]. This will ensure
11Let us recall here that the Earley algorithm, like the GLR
algorithm, has the valid prefix property. This is still true when
the input is a DAG.
12The combination of these two recovery techniques leads to
a more general algorithm than the skipping of the GLR* algo-
rithm (Lavie and Tomita, 1993). Indeed, we can not only skip
terminals, but in fact replace any invalid prefix by a valid pre-
fix (of a right sentential form) with an increased span. In other
words, both terminals and non-terminals may be skipped, in-
serted or changed, following the heuristics described later on.
However, in (Lavie and Tomita, 1993), considering only the
skipping of terminal symbols was fully justified since their aim
was to parse spontaneous speech, full of noise and irrelevances
that surround the meaningful words of the utterance.
4
S2
S
NP
N
pn
Jean
V
v
essaye
PVP
prep
de
VP
?
spunct
...
Figure 1: Simplified constituents structure for in-
complete sentence Jean essaye de... (?Jean tries
to...?). The derivation of VP in the empty string is
the result of a forward recovery, and will lead to
an incomplete functional structure (no ?pred? in the
sub-structure corresponding to node VP).
at least one valid transition from T [k] on r. The ef-
fect of such a recovery is to assume that between
the nodes j and k in the DAG there is a path that is a
phrase generated by t?
1
. We select only nodes k that
are as close as possible to j. This economy principle
allows to skip (without analysis) the smallest pos-
sible number of terminal symbols, and leads pretty
often to no skipping, thus deriving ?
1
into the empty
string and producing a recovery in k = j. This re-
covery mechanism allows the parsing process to go
forward, hence the name forward recovery.
If this strategy fails, we make use of backward
recovery.13 Instead of trying to apply the current
item, we jump backward over terminal symbols that
have already been recognized by the current item,
until we find its calling items, items on which we
try to perform a forward recovery at turn. In case
of failure, we can go up recursively until we suc-
ceed. Indeed, success is guaranteed, but in the worst
case it is obtained only at the axiom. In this ex-
treme case, the shared forest that is produced is only
a single production that says that the input DAG
is derivable from the axiom. We call this situation
trivial recovery. Formally, let us come back to the
item I = [A ? ?.t?, i] of table T [j]. We know
that there exists in table T [i] an item J of the form
[B ? ?.A?, h] on which we can hazard a forward
13This second strategy could be also used before or even in
parallel with the forward recovery.
recovery in l on ?, where i ? j ? l. If this fails, we
go on coming back further and further in the past,
until we reach the initial node of the DAG and the
root item [S? ? .S$, 0] of table T [0] ($ is the end-
of-sentence mark and S? the super-axiom). Since
any input ends with an $ mark, this strategy always
succeeds, leading in the worst case to trivial recov-
ery.
An example of an analysis produced is shown in
Figure 1: in this case, no out-transition on spunct is
available after having recognized prep. Hence a for-
ward recovery is performed that inserts an ?empty?
VP after the prep, so as to build a valid parse.
3.2 Inconsistent or partial f-structures
The computation of f-structures fails if and only
if no consistent and complete main f-structure is
found. This occurs because unification constraints
specified by functional equations could not have
been verified or because resulting f-structures are
inconsistent or incomplete. Without entering into
details, inconsistency mostly occurs because sub-
categorization constraints have failed.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
pred = ?essayer <subj, de-vcomp>?, v[2..3]
subj =
?
?
?
pred = ?Jean <(subj)>?, pn[1..2]
det = +
hum = +
A
ij
=
{
R182
9
, R177
26
, R170
28
}
?
?
?
F68
de-vcomp =
?
?
?
?
?
pred = ?de <obj|...>?, prep[3..4]
vcomp =
[
subj = []
F68
A
ij
=
{
R162
84
}
2
]
F69
pcase = de
A
ij
= {}
2
?
?
?
?
?
F70
number = sg
person = 3
mode = indicative
tense = present
A
ij
=
{
R130
33
, R119
48
, R134
49
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 2: Simplified incomplete functional structure
for incomplete sentence Jean essaye de... (?Jean
tries to...?). Sub-structure identifiers are indicated as
subscripts (like F70). In the grammar, a rule can tell
the parser to store the current instantiated production
in the special field A
ij
of its associated left-hand
side structure. Hence, atoms of the form Rq
p
rep-
resent instantiated production, thus allowing to link
sub-structures to non-terminals of the c-structure.
5
A first failure leads to a second evaluation of f-
structures on the shared forest, during which consis-
tency and completeness checks are relaxed (an ex-
ample thereof is given in Figure 2). In case of suc-
cess, we obtain inconsistent or incomplete main f-
structures. Of course, this second attempt can also
fail. We then look in the shared forest for a set of
maximal nodes that have f-structures (possibly in-
complete or inconsistent) and whose mother nodes
have no f-structures. They correspond to partial
disjoint analyses. The disambiguation process pre-
sented in section 2.4 applies to all maximal nodes.
3.3 Over-segmentation of unparsable sentences
Despite all these recovery techniques, parsing some-
times fails, and no analysis is produced. This can
occur because a time-out given as parameter has ex-
pired before the end of the process, or because the
Earley parser performed a trivial recovery (because
of the insufficient coverage of the grammar, or be-
cause the input sentence is simply too far from being
correct: grammatical errors, incomplete sentences,
too noisy sentences, . . . ).
For this reason, we developed a layer over SXLFG
that performs an over-segmentation of ungrammat-
ical sentences. The idea is that it happens fre-
quently that portions of the input sentence are ana-
lyzable as sentences, although the full input sentence
is not. Therefore, we split in segments unparsable
sentences (level 1 segmentation); then, if needed,
we split anew unparsable level 1 segments14 (level
2 segmentation), and so on with 5 segmentation lev-
els.15 Such a technique supposes that the grammar
recognizes both chunks (which is linguistically jus-
tified, e.g., in order to parse nominal sentences) and
isolated terminals (which is linguistically less rele-
vant). In a way, it is a generalization of the use of a
FRAGMENT grammar as described in (Riezler et al,
2002; Kaplan et al, 2004).
14A sentence can be split into two level 1 segments, the first
one being parsable. Then only the second one will be over-
segmented anew into level 2 segments. And only unparsable
level 2 segments will be over-segmented, and so on.
15The last segmentation level segments the input string into
isolated terminals, in order to guarantee that any input is parsed,
and in particular not to abandon parsing on sentences in which
some level 1 or 2 segments are parsable, but in which some parts
are only parsable at level 5.
4 Quantitative results
4.1 Grammar, disambiguation rules, lexicon
To evaluate the SXLFG parser, we used our system
with a grammar for French that is an adaptation of an
LFG grammar originally developed by Cl?ment for
his XLFG system (Cl?ment and Kinyon, 2001). In
its current state, the grammar has a relatively large
coverage. Amongst complex phenomena covered
by this grammar are coordinations (without ellip-
sis), juxtapositions (of sentences or phrases), inter-
rogatives, post-verbal subjects and double subjects
(Pierre dort-il ?), all kinds of verbal kernels (in-
cluding clitics, auxiliaries, passive, negation), com-
pletives (subcategorized or adjuncts), infinitives (in-
cluding raising verbs and all three kinds of control
verbs), relatives or indirect interrogatives, including
when arbitrarily long-distance dependencies are in-
volved. However, comparatives, clefts and elliptical
coordinations are not specifically covered, inter alia.
Moreover, we have realized that the CF backbone is
too ambiguous (see below).
Besides the grammar itself, we developed a set of
disambiguation heuristics. Following on this point
(Cl?ment and Kinyon, 2001), we use a set of rules
that is an adaptation and extension of the three sim-
ple principles they describe and that are applied
on f-structures, rather than a stochastic model.16
Our rules are based on linguistic considerations and
can filter out functional structures associated to a
given node of the forest. This includes two special
rules that eliminate inconsistent and incomplete f-
structures either in all cases or when consistent and
complete structures exist (these rules are not applied
during the second pass, if any). As explained above,
some non-terminals of the CF backbone, that corre-
spond to linguistically saturated phrases, have been
associated with an ordered list of these rules, each of
these non-terminal having its own list.17.
16As sketched before, this could be easily done by defining a
rule that uses a stochastic model to compute a weight for each
f-structure (see e.g., (Miyao and Tsujii, 2002)) and retains only
those with the heaviest weights (Riezler et al, 2002; Kaplan
et al, 2004). However, our experiments show that structural
rules can be discriminative enough to enable efficient parsing,
without the need for statistical data that have to be acquired on
annotated corpora that are rare and costly, in particular if the
considered language is not English.
17Our rules, in their order of application on main f-structures,
i.e. on the axiom of the backbone, are the following (note that
6
 0
 100
 200
 300
 400
 500
 600
 700
 800
 0  20  40  60  80  100N
um
be
r o
f s
en
te
nc
es
 in
 th
e 
sa
m
e 
cl
as
s
(le
ng
th
 b
et
w
ee
n 
10
i a
nd
 1
0(
i+
1)
-1
)
Sentence length (number of transitions in the corresponding word lattice)
Figure 3: Repartition of sentences of the test corpus
w.r.t. their length. We show the cardinal of classes
of sentences of length 10i to 10(i + 1) ? 1, plotted
with a centered x-coordinate (10(i + 1/2)).
The lexicon we used is the latest version of
Lefff (Lexique des formes fl?chies du fran?ais18),
which contains morphosyntactic and syntactic infor-
mation for more than 600,000 entries corresponding
to approximately 400,000 different tokens (words or
components of multi-word units).
The purpose of this paper is not however to val-
idate the grammar and these disambiguation rules,
since the grammar has only the role of enabling eval-
uation of parsing techniques developed in the current
work.
4.2 Results
As for any parser, the evaluation of SXLFG has been
carried out by testing it in a real-life situation. We
used the previously cited grammar on a raw journal-
istic corpus of 3293 sentences, not filtered and pro-
when used on other non-terminal symbols than the axiom, some
rules may not be applied, or in a different order):
Rule 1: Filter out inconsistent and incomplete structures, if
there is at least one consistent and complete structure.
Rule 2: Prefer analyses that maximize the sum of the weights
of involved lexemes; amongst lexical entries that have a
weight higher than normal are multi-word units.
Rule 3: Prefer nominal groups with a determiner.
Rule 4: Prefer arguments to modifiers, and auxiliary-
participle relations to arguments (the computation is
performed recursively on all (sub-)structures).
Rule 5: Prefer closer arguments (same remark).
Rule 6: Prefer deeper structures.
Rule 7: Order structures according to the mode of verbs (we
recursively prefer structures with indicative verbs, sub-
junctive verbs, and so on).
Rule 8: Order according to the category of adverb governors.
Rule 9: Choose one analysis at random (to guarantee that the
output is a unique analysis).
18Lexicon of French inflected forms
 1
 100000
 1e+10
 1e+15
 1e+20
 1e+25
 1e+30
 0  20  40  60  80  100
N
um
be
r o
f t
re
es
 in
 th
e 
C
FG
 p
ar
se
 fo
re
st
 (l
og
 s
ca
le
)
Sentence length (number of transitions in the corresponding word lattice)
Median number of trees
Number of trees at percentile rank 90
Number of trees at percentile rank 10
Figure 4: CFG ambiguity (medians are computed on
classes of sentences of length 10i to 10(i+1)?1 and
plotted with a centered x-coordinate (10(i + 1/2)).
cessed by the SXPipe pre-parsing system described
in (Sagot and Boullier, 2005). The repartition of sen-
tences w.r.t. their length is plotted in Figure 3.
In all Figures, the x-coordinate is bounded so as
to show results only on statistically significant data,
although we parse all sentences, the longest one be-
ing of length 156.
However, in order to evaluate the performance of
our parser, we had to get rid of, as much as possible,
the influence of the grammar and the corpus in the
quantitative results. Indeed, the performance of the
SXLFG parser does not depend on the quality and
the ambiguity of the grammar, which is an input for
SXLFG. On the contrary, our aim is to develop a
parser which is as efficient and robust as possible
given the input grammar, and in spite of its (possibly
huge) ambiguity and of its (possibly poor) intrinsic
coverage.
4.2.1 CFG parser evaluation
Therefore, Figure 4 demonstrates the level of am-
biguity of the CF backbone by showing the median
number of CF parses given the number of transitions
in the lattice representing the sentence. Although
the number of trees is excessively high, Figure 5
shows the efficiency of our CF parser19 (the max-
imum number of trees reached in our corpus is as
high as 9.12 1038 for a sentence of length 140, which
19Our experiments have been performed on a AMD Athlon
2100+ (1.7 GHz).
7
 0
 100
 200
 300
 400
 500
 0  20  40  60  80  100
C
FG
 p
ar
si
ng
 ti
m
e 
(m
ill
is
ec
on
ds
)
Sentence length (number of transitions in the corresponding word lattice)
Median CFG parsing time
CFG parsing time at percentile rank 90
CFG parsing time at percentile rank 10
Figure 5: CF parsing time (same remark as for Fig. 4).
is parsed in only 0.75 s). Moreover, the error re-
covery algorithms described in section 3.1 are suc-
cessful in most cases where the CF backbone does
not recognize the input sentences: out of the 3292
sentences, 364 are not recognized (11.1%), and the
parser proposes a non-trivial recovery for all but 13
(96.3%). We shall see later the relevance of the pro-
posed recovered forests. We should however notice
that the ambiguity of forests is significantly higher
in case of error recovery.
4.2.2 Evaluation of f-structures computation
Although the CF backbone is massively ambigu-
ous, results show that our f-structures evaluation
system is pretty efficient. Indeed, with a timeout of
20 seconds, it takes only 6 301 seconds to parse the
whole corpus, and only 5, 7% of sentences reach the
timeout before producing a parse. These results can
be compared to the result with the same grammar on
the same corpus, but without internal disambigua-
tion (see 2.4), which is 30 490 seconds and 41.2%
of sentences reaching the timeout.
The coverage of the grammar on our corpus with
internal disambiguation is 57.6%, the coverage be-
ing defined as the proportion of sentences for which
a consistent and complete main f-structure is output
by the parser. This includes cases where the sen-
tence was agrammatical w.r.t. the CF backbone, but
for which the forest produced by the error recov-
ery techniques made it possible to compute a consis-
tent and complete main f-structure (this concerns 86
sentences, i.e., 2.6% of all sentences, and 24.5% of
all agrammatical sentences w.r.t. the backbone; this
shows that CF error recovery gives relevant results).
The comparison with the results with the same
grammar but without internal disambiguation is in-
teresting (see Table 1): in this case, the high propor-
tion of sentences that reach the timeout before being
parsed leads to a coverage as low as 40.2%. Amid
the sentences covered by such a system, 94.6% are
also covered by the full-featured parser (with inter-
nal disambiguation), which means that only 72 sen-
tences covered by the grammar are lost because of
the internal disambiguation. This should be com-
pared with the 645 sentences that are not parsed be-
cause of the timeout when internal disambiguation
is disabled, but that are covered by the grammar and
correctly parsed if internal disambiguation is used:
the risk that is taken by pruning f-structures during
the parsing process is much smaller than the benefit
it gives, both in terms of coverage and parsing time.
Since we do not want the ambiguity of the CF
backbone to influence our results, Figure 6 plots the
total parsing time, including the evaluation of fea-
tures structures, against the number of trees pro-
duced by the CF parser.
8
Results With internal Without internal
disambiguation disambiguation
Total number of sentences 3293
Recognized by the backbone 2929 88.9%
CF parsing with non-trivial recovery 351 10.6%
CF parsing with trivial recovery 13 0.4%
Consistent and complete main f-structure 1896 57.6% 1323 40.2%
Inconsistent and incomplete main f-structure 734 22.3% 316 9.6%
Partial f-structures 455 13.8% 278 8.4%
No f-structure 6 0.2% 6 0.2%
No result (trivial recovery) 13 0.4% 13 0.4%
Timeout (20 s) 189 5.7% 1357 40.2%
Table 1: Coverage results with and without internal ranking, with the same grammar and corpus.
 10
 100
 1000
 10000
 1  100000  1e+10  1e+15  1e+20
To
ta
l p
ar
si
ng
 ti
m
e 
(m
ill
is
ec
on
ds
)
Number of trees in the forest
Median total parsing time
Total parsing time at percentile rank 90
Total parsing time at percentile rank 10
Figure 6: Total parsing time w.r.t. the number of trees in the forest produced by the CF backbone (medians
are computed on classes of sentences whose number of trees lies between 102i and 102i+2 ? 1 and plotted
with a centered x-coordinate (102i+1)).
9
5 Conclusion
This paper shows several important results.
It shows that wide-coverage unification-based
grammars can be used to define natural languages
and that their parsers can, in practice, analyze raw
text.
It shows techniques that allow to compute fea-
ture structures efficiently on a massively ambiguous
shared forest.
It also shows that error recovery is worth doing
both at the phrasal and functional levels. We have
shown that a non-negligible portion of input texts
that are not in the backbone language can neverthe-
less, after CF error recovery, be qualified as valid
sentences for the functional level.
Moreover, the various robustness techniques that
are applied at the functional level allow to gather
(partial) useful information. Note that these ro-
bust techniques, which do not alter the overall ef-
ficiency of SXLFG, apply in the two cases of incom-
plete grammar (lack of covering) and agrammati-
cal phrases (w.r.t. the current definition), though it
seems to be more effective in this latter case.
References
Avery Andrews. 1990. Functional closure in LFG. Tech-
nical report, The Australian National University.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 8th International Workshop on Parsing
Technologies (IWPT?03), pages 43?54, Nancy, France,
April.
Pierre Boullier. 2004. Range concatenation grammars.
In New developments in parsing technology, pages
269?289. Kluwer Academic Publishers.
Xavier Briffault, Karim Chibout, G?rard Sabah, and
J?r?me Vapillon. 1997. An object-oriented lin-
guistic engineering environment using LFG (Lexical-
Functional Grammar) and CG (Conceptual Graphs).
In Proceedings of Computational Environments for
Grammar Development and Linguistic Engineering,
ACL?97 Workshop.
Lionel Cl?ment and Alexandra Kinyon. 2001. XLFG ?
an LFG parsing scheme for French. In Proceedings of
LFG?01, Hong Kong.
Ronald Kaplan and Joan Bresnan. 1982. Lexical-
functional grammar: a formal system for grammatical
representation. In J. Bresnan, editor, The Mental Rep-
resentation of Grammatical Relations, pages 173?281.
MIT Press, Cambridge, MA.
Ronald M. Kaplan and John T. Maxwell. 1994. Gram-
mar writer?s workbench, version 2.0. Technical report,
Xerox Corporation.
Ronald Kaplan, Stefan Riezler, Tracey King, John
Maxwell, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Proceedings of HLT/NAACL,
Boston, Massachusetts.
Ronald Kaplan. 1989. The formal architecture of lexical
functionnal grammar. Journal of Informations Science
and Engineering.
Alexandra Kinyon. 2000. Are structural principles use-
ful for automatic disambiguation ? In Proceedings
of in COGSCI?00, Philadelphia, Pennsylvania, United
States.
Alon Lavie and Masaru Tomita. 1993. GLR* ? an effi-
cient noise-skipping parsing algorithm for context-free
grammars. In Proceedings of the Third International
Workshop on Parsing Technologies, pages 123?134,
Tilburg, Netherlands and Durbuy, Belgium.
John Maxwell and Ronald Kaplan. 1993. The interface
between phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum en-
tropy estimation for feature forests. In Proceedings of
HLT, San Diego, California.
Stefan Riezler, Tracey King, Ronald Kaplan, Richard
Crouch, John Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a Lexical-
Functional Grammar and discriminative estimation
techniques. In Proceedings of the Annual Meeting of
the ACL, University of Pennsylvania.
Beno?t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing. In
Proceedings of L&TC 2005, Poznan?, Pologne.
10
Proceedings of the 10th Conference on Parsing Technologies, pages 94?105,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Are Very Large Context-Free Grammars Tractable?
Pierre Boullier & Beno??t Sagot
INRIA-Rocquencourt
Domaine de Voluceau, Rocquencourt BP 105
78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
Abstract
In this paper, we present a method which, in
practice, allows to use parsers for languages
defined by very large context-free grammars
(over a million symbol occurrences). The
idea is to split the parsing process in two
passes. A first pass computes a sub-grammar
which is a specialized part of the large gram-
mar selected by the input text and various
filtering strategies. The second pass is a tra-
ditional parser which works with the sub-
grammar and the input text. This approach
is validated by practical experiments per-
formed on a Earley-like parser running on
a test set with two large context-free gram-
mars.
1 Introduction
More and more often, in real-word natural lan-
guage processing (NLP) applications based upon
grammars, these grammars are no more written by
hand but are automatically generated, this has sev-
eral consequences. This paper will consider one of
these consequences: the generated grammars may
be very large. Indeed, we aim to deal with grammars
that have, say, over a million symbol occurrences
and several hundred thousands rules. Traditional
parsers are not usually prepared to handle them,
either because these grammars are simply too big
(the parser?s internal structures blow up) or the time
spent to analyze a sentence becomes prohibitive.
This paper will concentrate on context-free gram-
mars (CFG) and their associated parsers. However,
virtually all Tree Adjoining Grammars (TAG, see
e.g., (Schabes et al, 1988)) used in NLP applica-
tions can (almost) be seen as lexicalized Tree In-
sertion Grammars (TIG), which can be converted
into strongly equivalent CFGs (Schabes and Waters,
1995). Hence, the parsing techniques and tools de-
scribed here can be applied to most TAGs used for
NLP, with, in the worst case, a light over-generation
which can be easily and efficiently eliminated in a
complementary pass. This is indeed what we have
achieved with a TAG automatically extracted from
(Villemonte de La Clergerie, 2005)?s large-coverage
factorized French TAG, as we will see in Section 4.
Even (some kinds of) non CFGs may benefit from
the ideas described in this paper.
The reason why the run-time of context-free (CF)
parsers for large CFGs is damaged relies on a theo-
retical result. A well-known result is that CF parsers
may reach a worst-case running time ofO(|G|?n3)
where |G| is the size of the CFG and n is the length
of the source text.1 In typical NLP applications
which mainly work at the sentence level, the length
of a sentence does not often go beyond a value of
say 100, while its average length is around 20-30
words.2 In these conditions, the size of the grammar,
despite its linear impact on the complexity, may be
the prevailing factor: in (Joshi, 1997), the author re-
marks that ?the real limiting factor in practice is the
size of the grammar?.
The idea developed in this paper is to split the
parsing process in two passes. A first pass called
filtering pass computes a sub-grammar which is the
1These two notions will be defined precisely later on.
2At least for French, English and similar languages.
94
sub-part of the large input grammar selected by the
input sentence and various filtering strategies. The
second pass is a traditional parser which works with
the sub-grammar and the input sentence. The pur-
pose is to find a filtering strategy which, in typical
practical situations, minimizes on the average the
total run-time of the filtering pass followed by the
parser pass.
A filtering pass may be seen as a (filtering) func-
tion that uses the input sentence to select a sub-
grammar out of a large input CFG. Our hope, us-
ing such a filter, is that the time saved by the parser
pass which uses a (smaller) sub-grammar will not
totally be used by the filter pass to generate this sub-
grammar.
It must be clear that this method cannot improve
the worst-case parse-time because there exists gram-
mars for which the sub-grammar selected by the fil-
tering pass is the input grammar itself. In such a
case, the filtering pass is simply a waste of time. Our
purpose in this paper is to argue that this technique
may profit from typical grammars used in NLP. To
do that we put aside the theoretical view point and
we will consider instead the average behaviour of
our processors.
More precisely we will study on two large NL
CFGs the behaviour of our filtering strategies on a
set of test sentences. The purpose being to choose
the best filtering strategy, if any. By best, we mean
the one which, on the average, minimizes the total
run-time of both the filtering pass followed by the
parsing pass.
Useful formal notions and notations are recalled
in Section 2. The filtering strategies are presented
in Section 3 while the associated experiments are
reported in Section 4. This paper ends with some
concluding remarks in Section 5.
2 Preliminaries
2.1 Context-free grammars
A CFG G is a quadruple (N,T, P, S) where N is
a non-empty finite set of nonterminal symbols, T is
a finite set of terminal symbols, P is a finite set of
(context-free rewriting) rules (or productions) and
S is a distinguished nonterminal symbol called the
axiom. The sets N and T are disjoint and V = N?T
is the vocabulary. The rules in P have the form A?
?, with A ? N and ? ? V ?.
For a given string ? ? V ?, its size (length)
is noted |?|. As an example, for the input string
w = a1 ? ? ? an, ai ? T , we have |w| = n. The empty
string is denoted ? and we have |?| = 0. The size |G|
of a CFG G is defined by |G| = ?A???P |A?|.
For G, on strings of V ?, we define the binary re-
lation derive, noted ?, by ?1A?2 A???G ?1??2 if
A ? ? ? P and ?1, ?2 ? V ?. The subscript G
or even the superscript A ? ? may be omitted. As
usual, its transitive (resp. reflexive transitive) clo-
sure is noted +?
G
(resp. ??
G
). We call derivation any
sequence of the form ?1 ?G ? ? ? ?G ?2. A complete
derivation is a derivation which starts with the ax-
iom and ends with a terminal string w. In that case
we have S ??
G
? ??
G
w, and ? is a sentential form.
The string language defined (generated, recog-
nized) by G is the set of all the terminal strings that
are derived from the axiom: L(G) = {w | S +?
G
w,w ? T ?}. We say that a CFG is empty iff its
language is empty.
A nonterminal symbol A is nullable iff it can de-
rive the empty string (i.e., A +?
G
?). A CFG is ?-free
iff its nonterminal symbols are non-nullable.
A CFG is reduced iff every symbol of every pro-
duction is a symbol of at least one complete deriva-
tion. A reduced grammar is empty iff its production
set is empty (P = ?). We say that a non-empty
reduced grammar is in canonical form iff its vocab-
ulary only contains symbols that appear in the pro-
ductions of P .3,4
Two CFGs G and G? are weakly equivalent iff
they generate the same string language. They are
strongly equivalent iff they generate the same set of
structural descriptions (i.e., parse trees). It is a well
known result (See Section 3.2) that every CFG G
can be transformed in time linear w.r.t. |G| into a
strongly equivalent (canonical) reduced CFG G?.
For a given input string w ? T ?, we define its
3We may say that the canonical form of the empty reduced
grammar is ({S}, ?, ?, S) though the axiom S does not appear
in any production.
4Note that the pair (P, S) completely defines a reduced CFG
G = (N,T, P, S) in canonical form since we have N = {X0 |
X0 ? ? ? P} ? {S}, T = {Xi | X0 ? X1 ? ? ?Xp ?
P ?1 ? i ? p}?N . Thus, in the sequel, we often note simply
G = (P, S) grammars in canonical form.
95
ranges as the set Rw = {[i..j] | 1 ? i ? j ?
|w| + 1}. If w = w1tw3 ? T ? is a terminal string,
and if t ? T ? {?} is a (terminal or empty) sym-
bol, the instantiation of t in w is the triple noted
t[i..j] where [i..j] is a range with i = |w1| + 1 and
j = i + |t|. More generally, the instantiation of the
terminal string w2 in w1w2w3 is noted w2[i..j] with
i = |w1| + 1 and j = i + |w2|. Obviously, the in-
stantiation of w itself is then w[1..1 + |w|].
Let us consider an input string w = w1w2w3
and a CFG G. If we have a complete derivation
d = S ??
G
w1Aw3 A???G w1?w3
??
G
w1w2w3, we
see that A derives w2 (we have A +?G w2). More-
over, in this complete derivation, we also know a
range in Rw, namely [i..j], which covers the sub-
string w2 which is derived by A (i = |w1| + 1
and j = i + |w2|). This is represented by the in-
stantiated nonterminal symbol A[i..j]. In fact, each
symbol which appears in a complete derivation may
be transformed into its instantiated counterpart. We
thus talk of instantiated productions or (complete)
instantiated derivations. For a given input text w,
and a CFG G, let PwG be the set of instantiated pro-
ductions that appears in all complete instantiated
derivations.5 The pair (PwG , S[1..|w|+1]) is the (re-
duced) shared parse forest in canonical form.6
2.2 Finite-state automata
A finite-state automaton (FSA) is the 5-tuple A =
(Q,?, ?, q0, F ) where Q is a non empty finite set
of states, ? is a finite set of terminal symbols, ? is
the transition relation ? = {(qi, t, qj)|qi, qj ? Q ?
t ? T ? {?}}, q0 is a distinguished element of Q
called the initial state and F is a subset of Q whose
elements are called final states. The size of A is
defined by |A| = |?|.
As usual, we define both a configuration as an ele-
ment of Q?T ? and derive a binary relation between
5For example, in the previous complete derivation
d, let the right-hand side ? be the (vocabulary) string
X1 ? ? ?Xk ? ? ?Xp in which each symbol Xk derives the ter-
minal string xk ? T ? (we have Xk ??
G
xk and w2 =
x1 ? ? ?xk ? ? ?xp), then the instantiated production A[i0..ip] ?
X1[i0..i1] ? ? ?Xk[ik?1..ik] ? ? ?Xp[ip?1..ip] with i0 = |w1| +
1, i1 = i0 + |x1|, . . . , ik = ik?1 + |xk| . . . and ip = i0 + |w2|
is an element of PwG .
6The popular notion of shared forests mainly comes from
(Billot and Lang, 1989).
configurations, noted ?
A
by (q, tx) ?
A
(q?, x), iff
(q, t, q?) ? ?. If w?w?? ? T ?, we call derivation any
sequence of the form (q?, w?w??) ?
A
? ? ? ?
A
(q??, w??).
If w ? T ?, the initial configuration is noted c0 and
is the pair (q0, w). A final configuration is noted cf
and has the form (qf , ?) with qf ? F . A complete
derivation is a derivation which starts with c0 and
ends in a final configuration cf . In that case we have
c0
?
?
A
cf .
The language L(A) defined (generated, recog-
nized) by the FSA A is the set of all terminal strings
w for which there exists a complete derivation. We
say that an FSA is empty iff its language is empty.
Two FSAs A and A? are equivalent iff they defined
the same language.
An FSA is ?-free iff its transition relation has the
form ? = {(qi, t, qj)|qi, qj ? Q, t ? ?}, except per-
haps for a distinguished transition, the ?-transition
which has the form (q0, ?, qf ), qf ? F and allows
the empty string ? to be in L(A). Every FSA can be
transformed into an equivalent ?-free FSA.
An FSA A = (Q,?, ?, q0, F ) is reduced iff every
element of ? appears in a complete derivation. A
reduced FSA is empty iff we have ? = ?. We say
that a non-empty reduced FSA is in canonical form
iff its set of states Q and its set of terminal symbols
? only contain elements that appear in the transition
relation ?.7 It is a well known result that every FSA
A can be transformed in time linear with |A| into an
equivalent (canonical) reduced FSA A?.
2.3 Input strings and input DAGs
In many NLP applications8 the source text cannot
be considered as a single string of terminal symbols
but rather as a finite set of terminal strings. These
sets are finite languages which can be defined by
particular FSAs. These particular type of FSAs are
called directed-acyclic graphs (DAGs). In a DAG
w = (Q,?, ?, q0, F ), the initial state q0 is 1 and we
assume that there is a single final state f (F = {f}),
Q is a finite subset of the positive integers less than
or equal to f : Q = {i|1 ? i ? f}, ? is the set of
terminal symbols. For the transition relation ?, we
7We may say that the canonical form of the empty reduced
FSA is ({q0}, ?, ?, q0, ?) though the initial state q0 does not
appear in any transition.
8Speech processing, lexical ambiguity representation, . . .
96
require that its elements (i, t, j) are such that i < j
(there are no loops in a DAG). Without loss of gen-
erality, we will assume that DAGs are ?-free reduced
FSAs in canonical form and that any DAG w is noted
by a triple (?, ?, f) since its initial state is always 1
and its set of states is {i | 1 ? i ? f}.
For a given CFG G, the recognition of an input
DAG w is equivalent to the emptiness of its inter-
section with G. This problem can be solved in time
linear in |G| and cubic in |Q| the number of states of
w.
If the input text is a DAG, the previous notions of
range, instantiations and parse forest easily general-
ize: the indices i and j which in the string case locate
the positions of substrings are changed in the DAG
case into DAG states. For example if A[i0..ip] ?
X1[i0..i1] ? ? ?Xp[ip?1..ip] is an instantiated produc-
tion of the parse forest for G = (N,T, P, S) and
w = (?, ?, f), we have A ? X1 ? ? ?Xp ? P and
there is a path in the input DAG from state i0 to state
ip via states i1, . . . , ip?1.
Of course, any nonempty terminal string w ? T+,
may be viewed as a DAG (?, ?, f) where ? = {t |
w = w1tw2 ? t ? T}, ? = {(i, t, i + 1) | w =
w1tw2?t ? T?i = 1+|w1|} and f = 1+|w|. If the
input string w is the empty string ?, the associated
DAG is (?, ?, f) where ? = ?, ? = {(1, ?, 2)} and
f = 2. Thus, in the sequel, we will assume that the
inputs of our parsers are not strings but DAGs. As a
consequence the size (or length) of a sentence is the
size of its DAG (i.e., its number of transitions).
3 Filtering Strategies
3.1 Gold Strategy
Let G = (N,T, P, S) be a CFG, w = (?, ?, f)
be an input DAG of size n = |?| and ?Fw? =
(?Pw?, S[1..f ]) be the reduced output parse for-
est in canonical form. From ?Pw?, it is pos-
sible to extract a set of (reduced) uninstanti-
ated productions P gw = {A ? X1 ? ? ?Xp |
A[i0..ip] ? X1[i0..i1]X2[i1..i2] ? ? ?Xp[ip?1..ip] ?
?Pw?}, which, together with the axiom S, defines a
new reduced CFG Ggw = (P gw, S) in canonical form.
This grammar is called the gold grammar of G for
w, hence the superscript g. Now, if we use Ggw to
reparse the same input DAG w, we will get the same
output forest ?Fw?. But in that case, we are sure that
every production in P gw is used in at least one com-
plete derivation. Now, if this process is viewed as
a filtering strategy that computes a filtering function
as introduced in Section 1, it is clear that this strat-
egy is size-optimal in the sense that P gw is of minimal
size, we call it the gold strategy and the associated
gold filtering function is noted g. Since we do not
want that a filtering strategy looses parses, the result
Gfw = (P fw , S) of any filtering function f must be
such that, for every sentence w, P fw is a superset of
P gw. In other words the recall score of any filtering
function f must be of 100%. We can note that the
parsing pass which generates Ggw may be led by any
filtering strategy f .
As usual, the precision score (precision for short)
of a filtering strategy f (w.r.t. the gold case) is, for
a given w, defined by the quotient |P
g
w|
|P fw|
which ex-
presses the number of useful productions selected by
f on w (for some G).
However, it is clear that we are interested in strate-
gies that are time-optimal and size-optimal strategies
are not necessarily also time-optimal: the time taken
at filtering-time to get a smaller grammar will not
necessarily be won back at parse-time.
For a given CFG G, an input DAG w and a filter-
ing strategy c, we only have to plot the times taken
by the filtering pass and by the parsing pass to make
some estimations on their average (median, decile)
parse times and then to decide which is the winner.
However, it may well happens that a strategy which
has not received the award (with the sample of CFGs
and the test sets tried) would be the winner in an-
other context!
All the following filtering strategies exhibit nec-
essary conditions that any production must hold in
order to be in a parse.
3.2 The make-a-reduced-grammar Algorithm
An algorithm which takes as input any CFG
G = (N,T, P, S) and generates as output a
strongly equivalent reduced CFG G? and which
runs in O(|G|) can be found in many text books
(See (Hopcroft and Ullman, 1979) for example).
So as to eliminate from all our intermediate sub-
grammars all useless productions, each filtering
strategy will end by a call to such an algorithm
named make-a-reduced-grammar.
97
The make-a-reduced-grammar algorithm works
as follows. It first finds all productive9 symbols. Af-
terwards it finds all reachable10 symbols. A symbol
is useful (otherwise useless) if it is both productive
and reachable. A production A? X1 ? ? ?Xp is use-
ful (otherwise useless) iff all its symbols are useful.
A last scan over the grammar erases all useless pro-
duction and leaves the reduced form. The canonical
form is reached in only retaining in the nonterminal
and terminal sets of the sub-grammar the symbols
which occur in the (useful) production set.
3.3 Basic Filtering Strategy: b-filter
The basic filtering strategy (b-filter for short) which
is described in this section will always be tried the
first. Thus, its input is the couple (G,w) where
G = (N,T, P, S) is the large initial CFG and the in-
put sentence w is a reduced DAG in canonical form
w = (?, ?, f) of size n. It generates a reduced CFG
in canonical form noted Gb = (P b, S) in which the
references to both G and w are assumed. Besides
this b-filter, we will examine in Sections 3.4 and 3.5
two others filtering strategies named a and d. These
filters will always have as input a couple (Gc, w)
where Gc = (P c, S) is a reduced CFG in canonical
form which has already been filtered by a previous
sequence of strategies noted c. They generate a re-
duced CFG in canonical form noted Gcf = (P cf , S)
with f = a or f = d respectively. Of course it may
happens that Gcf is identical to Gc if the f -filter is
not effective. A filtering strategy or a combination of
filtering strategies may be applied several times and
lead to a filtered grammar of the form say Gba2da
in which the sequence ba2da explicits the order in
which the filtering strategies have been performed.
We may even repeatedly apply a until a fixed point
is reached before applying d, and thus get something
of the form Gba?d.
The idea behind the b-filter is very simple and has
largely been used in lexicalized formalisms parsing,
in particular in LTAG (Schabes et al, 1988) parsing.
The filter rejects productions of P which contain ter-
minal symbols that do not occur in ? (i.e., that are
not terminal symbols of the DAG w) and thus takes
9X ? V is productive iff we have X ??
G
w,w ? T ?.
10X ? V is reachable iff we have S ??
G
w1Xw2, w1w2 ?
T ?.
S ? AB (1)
S ? BA (2)
A ? a (3)
A ? ab (4)
B ? b (5)
B ? bc (6)
Table 1: A simple grammar
O(|G|) time if we assume that the access to the ele-
ments of the terminal set ? is performed in constant
time. Unlexicalized productions whose right-hand
sides are in N? are kept. It also rejects productions
in which several terminal symbol occurs, in an order
which is not compatible with the linear order of the
input.
Consider for example the set of productions
shown in Table 1 and assume that the source text
is the terminal string ab. It is clear that the b-filter
will erase production 6 since c is not in the source
text.
The execution of the b-filter produces a (non-
reduced) CFG G? such that |G?| ? |G|. However, it
may be the case that some productions of G? are use-
less, it will thus be the task of the make-a-reduced-
grammar algorithm to transform G? into its reduced
canonical form Gb in time O(|G?|). The worst-case
total running time of the whole b-filter pass is thus
O(|G| ? n).
We can remark that, after the execution of the b-
filter, the set of terminal symbols of Gb is a subset
of T ? ?.
3.4 Adjacent Filtering Strategy: a-filter
As explained before, we assume that the input to
the adjacent filtering strategy (a-filter for short) de-
scribed in this section is a couple (Gc, w) where
Gc = (N c, T c, P c, S) is a reduced CFG in canon-
ical form. However, the a-filter would also work
for a non-reduced CFG. As usual, we define the
symbols of Gc as the elements of the vocabulary
V c = N c ? T c.
The idea is to erase productions that cannot be
part of any parses for w in using an adjacency crite-
ria: if two symbols are adjacent in a rule, they must
98
derive terminal symbols that are also adjacent in w.
To give a (very) simple practical idea of what we
mean by adjacency criteria, let us consider again the
source string ab and the grammar defined in Table 1
in which the last production has already been erased
by the b-filter.
The fact that the B-production ends with a b and
that the A-productions all start with an a, implies
that production 2 is in a complete parse only if the
source text is such that b is immediately followed
by a. Since it is not the case, production 2 can be
erased.
More generally, consider a production of the form
A ? ? ? ?XY ? ? ? . If for each couple (a, b) ? T 2 in
which a is a terminal symbol that can terminate (the
terminal strings generated by) X and b is a terminal
symbol that can lead (the terminal strings generated
by) Y , there is no transition on b that can follow a
transition on a in the DAG w, it is clear that the pro-
duction A? ? ? ?XY ? ? ? can be safely erased.
Now assume that we have the following (left)
derivation Y ?? Y1?1 ?? Yi?i ? ? ? ?1 ??
? ? ? Yp?1??pYp?p? ?pYp?p ? ? ? ?1 ?? Yp?p ? ? ? ?1,
with ?p ?? ?. If for each couple (a, b?) in which
a has the previous definition and b? is a terminal
symbol that can lead (the terminal strings gener-
ated by) Yp, there is no transition on b? that can fol-
low a transition on a in the DAG w, the production
Yp?1 ? ?pYp?p can be erased if it is not valid in
another context.
Moreover, consider a (right) derivation of the
form X ?? ?1X1 ?? ?1 ? ? ??iXi ??
? ? ? Xp?1??pXp?p? ?1 ? ? ??pXp?p ?? ?1 ? ? ??pXp,
with ?p ?? ?. If for each couple (a?, b) in which b
has the previous definition and a? is a terminal sym-
bol that can terminate (the terminal strings gener-
ated by) Xp, there is no transition on b that can fol-
low a transition on a? in the DAG w, the production
Xp?1 ? ?pXp?p can be erased if it is not valid in
another context.
In order to formalize these notions we define sev-
eral binary relations together with their (reflexive)
transitive closure.
Within a CFG G = (N,T, P, S), we first define
left-corner noted x. Left-corner (Nederhof, 1993;
Moore, 2000), hereafter LC, is a well-known rela-
tion since many parsing strategies are based upon it.
We say that X is in the LC of A and we write A x X
iff (A,X) ? {(B,Y ) | B ? ?Y ? ? P ? ? ??
G
?}.
We can write A x
A??X?
X to enforce how the cou-
ple (A,X) may be produced.
For its dual relation, right-corner, noted y, we say
that X is in the right corner of A and we write X y A
iff (X,A) ? {(Y,B) | B ? ?Y ? ? P ? ? ??
G
?}. We can write X y
A??X?
A to enforce how the
couple (X,A) may be produced.
We also define the first (resp. last) relation noted
??t (resp. ?? t) by ??t= {(X, t) | X ? V ? t ?
T ?X ??
G
tx ? x ? T ?} (resp. ?? t= {(X, t) | X ?
V ? t ? T ?X ??
G
xt ? x ? T ?}).
We define the adjacent ternary relation on V ?
N? ? V noted ? and we write X ?? Y iff
(X,?, Y ) ? {(U, ?, V ) | A? ?U?V ? ? P ?? ??
G
?}. This means that X and Y occur in that order in
the right-hand side of some production and are sep-
arated by a nullable string ?. Note that X or Y may
or may not be nullable.
On the input DAG w = (?, ?, f), we define the
immediately precede relation noted < and we write
a < b for a, b ? ? iff w1abw3 ? L(w), w1, w3 ?
??.
We also define the precede relation noted ? and
we write a ? b for a, b ? ? iff w1aw2bw3 ?
L(w), w1, w2, w3 ? ??.We can note that ? is not
the transitive closure of <.11
For each production A ? ?X0X1 ? ? ?Xp?1Xp?
in P c and for each symbol pairs (X0,Xp) of non-
nullable symbols s.t. X1 ? ? ?Xp?1 ??Gc ?, we com-
pute two sets A1 and A2 of couples (a, b), a, b ? T c
defined by A1 = ?0<i?p = {(a, b) | a ?? t
X0
X1???Xi?1? Xi ??t b} and A2 = ?0?i<p =
{(a, b) | a ?? t Xi
Xi+1???Xp?1? Xp ??t b}. Any
11Consider the source string bcab for which we have a
+
< c,
but not a ? c.
99
pair (a, b) of A1 is such that the terminal symbol
a may terminate a phrase of X0 while the terminal
symbol b may lead a phrase of X1 ? ? ?Xp. Since
X0 and Xp are not nullable, A1 is not empty. If
none of its elements (a, b) is such that a < b, the
production A ? ?X0X1 ? ? ?Xp?1Xp? is useless
and can be erased. Analogously, any pair (a, b) of
A2 is such that the terminal symbol a may termi-
nate a phrase of X0X1 ? ? ?Xp?1 while the terminal
symbol b may lead a phrase of Xp. Since X0 and
Xp are not nullable, A2 is not empty. If none of
its elements (a, b) is such that a < b, the produc-
tion A ? ?X0X1 ? ? ?Xp?1Xp? is useless and can
be erased. Of course if X1 ? ? ?Xp?1 = ?, we have
A1 = A2.12
The previous method has checked some adjacent
properties inside the right-hand sides of productions.
The following will perform some analogous checks
but at the beginning and at the end of the right-hand
sides of productions.
Let us go back to Table 1 to illustrate our pur-
pose. Recall that, with source text ab, productions 6
and 2 have already been erased. Consider produc-
tion 4 whose left-hand side is an A, the terminal
string ab that it generates ends by b. If we look for
the occurrences of A in the right-hand sides of the
(remaining) productions, we only find production 1
which indicates that A is followed by B. Since the
phrases of B all start with b (See production 5) and
since in the source text b does not immediately fol-
low another b, production 4 can be erased.
In order to check that the input sentence w starts
and ends by valid terminal symbols, we augment
the adjacent relation with two elements ($, ?, S) and
(S, ?, $) where $ is a new terminal symbol which is
supposed to start and to end every sentence.13
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If X is a non-nullable
symbol, we compute the set L = {(a, b) | a ?? t
X ?? Y ?x Z x
Z??U?
U ??t b}. Since Gc is reduced
and since $ < S, we are sure that the set X ?? Y ?x
12It can be shown that the previous check can be performed
on (Gc, w) in worst-case timeO(|Gc|?|?|3) (recall that |?| ?
n). This time reduces to O(|Gc| ? |?|2) if the input sentence
is not a DAG but a string.
13This is equivalent to assume the existence in the grammar
of a super-production whose right-hand side has the form $S$.
Z is non-empty, thus L is also non-empty.14
We can associate with each couple (a, b) ?
L at least one (left) derivation of the form
X?Y ??
Gc
w0aw1?Y ??Gc w0aw1w2Y
??
Gc
w0aw1w2w3Z?2
Z??U??
Gc
w0aw1w2w3?U??2 ??Gc
w0aw1w2w3w4U??2 ??Gc w0aw1w2w3w4w5b?1??2
in which w1w2w3w4w5 ? T c?. These derivations
contains all possible usages of the production Z ?
?U? in a parse. If for every couple (a, b) ? L, the
statement a? b does not hold, we can conclude that
the production Z ? ?U? is not used in any parse
and can thus be deleted.
Analogously, we can check that the order of ter-
minal symbols is compatible with both a production
and its right grammatical context.
Let Z ? ?U? be a production in P c in which U
is non-nullable and ? ??
Gc
?. If Y is a non-nullable
symbol, we compute the set R = {(a, b) | a ?? t
U y
Z??U?
Z ?y X ?? Y ??t b}. Since Gc is reduced
and since S < $, we are sure that the set Z ?y X ??
Y is non-empty, thus R is also non-empty.14
To each couple (a, b) ? R we can asso-
ciate at least one (right) derivation of the form
X?Y ??
Gc
X?w1bw0 ??Gc Xw2w1bw0
??
Gc
?1Zw3w2w1bw0
Z??U??
Gc
?1?U?w3w2w1bw0 ??Gc
?1?Uw4w3w2w1bw0 ??Gc ?1??2aw5w4w3w2w1bw0
in which w5w4w3w2w1 ? T c?. These deriva-
tions contains all possible usages of the production
Z ? ?U? in a partial parse. If for every couple
(a, b) ? L, the statement a ? b does not hold, we
can conclude that the production Z ? ?U? is not
used in any parse and can thus be deleted.
Now, a call to the make-a-reduced-grammar al-
gorithm produces a reduced CFG in canonical form
named Gca = (N ca, T ca, P ca, S).
14This statement does not hold any more if we exclude from
P c the productions that have been previously erased during the
current a-filter. In that case, an empty set indicates that the
production Z ? ?U? can be erased.
100
3.5 Dynamic Set Automaton Filtering
Strategy: d-filter
In (Boullier, 2003) the author has presented a
method that takes a CFG G and computes a FSA
that defines a regular superset of L(G). However his
method would produce intractable gigantic FSAs.
Thus he uses his method to dynamically compute
the FSA at parse time on a given source text. Based
on experimental results, he shows that his method
called dynamic set automaton (DSA) is tractable.
He uses it to guide an Earley parser (See (Ear-
ley, 1970)) and shows improvements over the non
guided version. The DSA method can directly be
used as a filtering strategy since the states of the un-
derlying FSA are in fact sets of items. For a CFG
G = (N,T, P, S), an item (or dotted production)
is an element of {[A ? ?.?] | A ? ?? ? P}.
A complete item has the form [A ? ?.], it indi-
cates that the production A ? ? has been, in some
sense, recognized. Thus, the complete items of the
DSA states gives the set of productions selected by
the DSA. This selection can be further refined if we
also use the mirror DSA which processes the source
text from right to left and if we only select complete
items that both belong to the DSA and to its mirror.
Thus, if we assume that the input to the DSA fil-
tering strategy (d-filter) is a couple (Gc, w) where
Gc = (P c, S) is a reduced CFG in canonical form,
we will eventually get a set of productions which is
a subset of P c. If it is a strict subset, we then ap-
ply the make-a-reduced-grammar algorithm which
produces a reduced CFG in canonical form named
Gcd = (P cd, S).
The Section 4 will give measures that may help to
compare the practical merits of the a and d-filtering
strategies.
4 Experiments
The measures presented in this section have been
taken on a 1.7GHz AMD Athlon PC with 1.5 Gb
of RAM running Linux. All parsers are written in C
and have been compiled with gcc 2.96 with the O2
optimization flag.
4.1 Grammars and corpus
We have performed experiments with two large
grammars described below. The first one is an auto-
matically generated CFG, the other one is the CFG
equivalent of a TIG automatically extracted from a
factorized TAG.
The first grammar, named GT>N , is a variant of
the CFG backbone of a large-coverage LFG gram-
mar for French used in the French LFG parser de-
scribed in (Boullier and Sagot, 2005). In this vari-
ant, the set T of terminal symbols is the whole set of
French inflected forms present in the Lefff , a large-
coverage syntactic lexicon for French (Sagot et al,
2006). This leads to as many as 407,863 different
terminal symbols and 520,711 lexicalized produc-
tions (hence, the average number of categories ?
which are here non-terminal symbols ? for an in-
flected form is 1.27). Moreover, this CFG entails
a non-neglectible amount of syntactic constraints
(including over-generating sub-categorization frame
checking), which implies as many as |Pu| = 19, 028
non-lexicalized productions. All in all, GT>N has
539,739 productions.
The second grammar, named GTIG, is a CFG
which represents a TIG. To achieve this, we applied
(Boullier, 2000)?s algorithm on the unfolded version
of (Villemonte de La Clergerie, 2005)?s factorized
TAG. The number of productions in GTIG is com-
parable to that of GT>N . However, these two gram-
mars are completely different. First, GTIG has much
less terminal and non-terminal symbols than GT>N .
This means that the basic filter may be less efficient
on GTIG than on GT>N . Second, the size of GTIG
is enormous (more than 10 times that of GT>N ),
which shows that right-hand sides of GTIG?s pro-
ductions are huge (the average number of right-hand
side symbols is more than 24). This may increase
the usefulness of a- and d-filtering strategies.
Global quantitative data about these grammars is
shown in Table 2.
Both grammars, as evoked in the introduction,
have not been written by hand. On the contrary, they
are automatically generated from a more abstract
and more compact level (a meta-level over LFG for
GT>N , and a metagrammar for GTIG). These gram-
mars are not artificial grammars set up only for this
experiment. On the contrary, they are automatically
generated huge real-life CFGs that are variants of
grammars used in real NLP applications.
Our test suite is a set of 3093 French journalistic
sentences. These sentences are the general lemonde
101
G |N | |T | |P | |Pu| |G|
GT>N 7,862 407,863 539,739 19,028 1,123,062
GTIG 448 173 493,408 4,338 12,455,767
Table 2: Sizes of the grammars GT>N and GTIG
used in our experiments
part of the EASy parsing evaluation campaign cor-
pus. Raw sentences have been turned into DAGs
of inflected forms known by both grammar/lexicon
couples.15 This step has been achieved by the pre-
syntactic processing chain SXPipe (Sagot and Boul-
lier, 2005). They are all recognized by both gram-
mars.16 The resulting DAGs have a median size of
28 and an average size of 31.7.
Before entering into details, let us give here the
first important result of these experiments: it was
actually possible to build parsers out of GT>N and
GTIG and to parse efficiently with the resulting
parsers (we shall detail later on efficiency results).
Given the fact that we are dealing with grammars
whose sizes are respectively over 1,000,000 and over
12,000,000, this is in itself a very satisfying result.
4.2 Precision results
Let us recall informally that the precision of a filter-
ing strategy is the proportion of productions in the
resulting sub-grammar that are in the gold grammar,
i.e., that have effectively instantiated counterparts in
the final parse forest.
We have applied different strategies so as to com-
pare their precisions. The results on GT>N and
GTIG are summed up in Table 3. These results give
several valuable results. First, as we expected, the
basic b-filter drastically reduces the size of the gram-
mar. The result is even better on GT>N thanks to its
large number of terminal symbols. Second, both the
adjacency a-filter and the DSA d-filter efficiently re-
duce the size of the grammar: on GT>N , the a-filter
eliminates 20% of the productions they receive as
input, a bit less for the d-filter. Indeed, the a-filter
performs better than the d-filter introduced in (Boul-
15As seen above, inflected forms are directly terminal sym-
bols of GT>N , while GTIG uses a lexicon to map these in-
flected forms into its own terminal symbols, thereby possibly
introducing lexical ambiguity.
16Approx. 15% of the original set of sentences were not rec-
ognized, and required error recovery techniques; we decided to
discard them for this experiment.
Strategy Average precision
GT>N GTIG
no filter 0.04% 0.03%
b 62.87% 39.43%
bd 74.53% 66.56%
ba 77.31% 66.94%
ba? 77.48% 67.48%
bad 80.27% 77.16%
ba?d 80.30% 77.41%
gold 100% 100%
Table 3: Average precision of six different filtering
strategies on our test corpus with GT>N and GTIG.
lier, 2003), at least as precision is concerned. We
shall see later that this is still the case on global
parsing times. However, applying the d-filter after
the a-filter still removes a non-neglectible amount
of productions:17 each technique is able to eliminate
productions that are kept by the other one. The result
of these filters is suprisingly good: in average, after
all filters, only approx. 20% of the productions that
have been kept will not be successfully instantiated
in the final parse forest. Third, the adjacency filter
can be used in its one-pass mode, since almost all
the benefit from the full (fix-point) mode is already
reached after the first application. This is practically
a very valuable result, since the one-pass mode is
obviously faster than the full mode.
However, all these filters do require computing
time, and it is necessary to evaluate not only the pre-
cision of these filters, but also their execution time
as well as the influence they have on the global (in-
cluding filtering) parsing time .
4.3 Parsing time and best filter
Filter execution times for the six filtering strategies
introduced in Table 3 are illustrated for GT>N in
Figure 1. These graphics show three extremely valu-
able pieces of information. First, filtering times are
extremely low: the average filtering time for the
slowest filter (ba?d, i.e., basic plus full adjacency
plus DSA) on 40-word sentences is around 20 ms.
Second, on small sentences, filtering times are virtu-
ally zero. This is important, since it means that there
17Although not reported here, applying the a before d leads
to the same conclusion.
102
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
b-filter bd-filter ba-filter
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0  20  40  60  80  100
Fi
lte
r e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Median filtering time
Filtering time at percentile rank 90
Filtering time at percentile rank 10
ba?-filter bad-filter ba?d-filter
Figure 1: Filtering times for six different strategies with GT>N
is almost no fixed cost to pay when we use these
filters (let us recall that without any filter, building
efficient parsers for such a huge grammar is highly
problematic). Third, all these filters, at least when
used with GT>N , are executed in a time which is
linear w.r.t. the size of the input sentence (i.e., the
size of the input DAG).
The results on GTIG lead to the same conclusions,
with one exception: with this extremely huge gram-
mar with so long right-hand sides, the basic filter
is not as fast as on GT>N (and not as precise, as
we will see below, which slows down the make-a-
reduced-grammar algorithm since it is applied on
a larger filtered grammars). For example, the me-
dian execution time for the basic filter on sentences
whose size is approximately 40 is 0.25 seconds,
to be compared with the 0.00 seconds reached on
GT>N (this zero value means a median time strictly
lower than 0.01 seconds, which is the granularity of
our time measurments).
Figure 2 and 3 show the global (filtering+parsing)
execution time for the 6 different filters. We only
show median times computed on classes of sen-
tences of length 10i to 10(i + 1) ? 1 and plotted
with a centered x-coordinate (10(i + 1/2)), but re-
sults with other percentiles or average times on the
same classes draw the same overall picture.
 0
 0.05
 0.1
 0.15
 0.2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 2: Global (filtering+parsing) times for six
different strategies with GT>N
One can see that the results are completely differ-
ent, showing a strong dependency on the character-
istics of the grammar. In the case of GT>N , the huge
number of terminal symbols and the reasonable av-
erage size of right-hand sides of productions, the ba-
sic filtering strategy is the best strategy: although it
is fast because relatively simple, it reduces the gram-
mar extremely efficiently (it has a 60.56% precision,
to be compared with the precision of the void filter
which is 0.04%). Hence, for GT>N , our only result
103
 0
 0.5
 1
 1.5
 2
 0  20  40  60  80  100
Av
er
ag
e 
gl
ob
al
 e
xe
cu
tio
n 
tim
e 
(se
co
nd
s)
Sentence length
Basic filter only
DSA filter
One-pass adjacency filter
Full adjacency filter
One-pass adjacency filter and DSA filter
Full adjacency filter and DSA filter
Figure 3: Global (filtering+parsing) times for six
different strategies with GTIG
is that this basic filter does allow us to build an effi-
cient parser (the most efficient one), but that refined
additionnal filtering strategies are not useful.
The picture is completely different with GTIG.
Contrary to GT>N , this grammar has comparatively
very few terminal and non-terminal symbols, and
very long right-hand sides. These two facts lead
to a lower precision of the basic filter (39.43%),
which keeps many more productions when applied
on GTIG than when applied on GT>N , and leads,
when applied alone, to the less efficient parser. This
gives to the adjacency filter much more opportunity
to improve the global execution time. However, the
complexity of the grammar makes the construction
of the DSA filter relatively costly despite its preci-
sion, leading to the following conclusion: on GTIG
(and probably on any grammar with similar charac-
teristics), the best filtering strategy is the one-pass
adjacency strategy. In particular, this leads to an im-
provement over the work of (Boullier, 2003) which
only introduced the DSA filter. Incidentally, the
extreme size of GTIG leads to much higher pars-
ing times, approximately 10 times higher than with
GT>N , which is consistent with the ratio between
the sizes of both involved grammars.
5 Conclusion
It is a well known result in optimization techniques
that the key to practically improve these processes is
to reduce their search space. This is also the case in
parsing and in particular in CF parsing.
Many parsers process their inputs from left to
right but we can find in the literature other parsing
strategies. In particular, in NLP, (van Noord, 1997)
and (Satta and Stock, 1994) propose bidirectional al-
gorithms. These parsers have the reputation to have
a better efficiency than their left-to-right counterpart.
This reputation is not only based upon experimental
results (van Noord, 1997) but also upon mathemat-
ical arguments in (Nederhof and Satta, 2000). This
is specially true when the productions of the CFG
strongly depend on lexical information. In that case
the parsing search space is reduced because the con-
straints associated to lexical elements are evaluated
as early as possible. We can note that our filtering
strategies try to reach the same purpose by a totally
different mean: we reduce the parsing search space
by eliminating as many productions as possible, in-
cluding possibly non-lexicalized productions whose
irrelevance to parse the current input can not be di-
rectly deduced from that input.
We can also remark that our results are not in con-
tradiction with the claims of (Nederhof and Satta,
2000) in which they argue that ?Earley algorithm
and related standard parsing techniques [. . . ] can-
not be directly extended to allow left-to-right and
correct-prefix-property parsing in acceptable time
bound?. First, as already noted in Section 1, our
method does not work for any large CFG. In order
to work well, the first step of our basic strategy must
filter out a great amount of (lexicalized) productions.
To do that, it is clear that the set of terminals in the
input text must select a small ratio of lexicalized pro-
ductions. To give a more concrete idea we advo-
cate that the selected productions produce roughly a
grammar of normal size out of the large grammar.
Second, our method as a whole clearly does not pro-
cess the input text from left-to-right and thus does
not enter in the categories studied in (Nederhof and
Satta, 2000). Moreover, the authors bring strong evi-
dences that in case of polynomial-time off-line com-
pilation of the grammar, left-to-right parsing cannot
be performed in polynomial time, independently of
the size of the lexicon. Once again, if our filter pass
is viewed as an off-line processing of the large input
grammar, our output is not a compilation of the large
grammar, but a (compilation of a) smaller grammar,
specialized in (some abstractions of) the source text
only. In other words their negative results do not
104
necessarily apply to our specific case.
The experiment campaign as been conducted in
using an Earley-like parser.18 We have also success-
fuly tried the coupling of our filtering strategies with
a CYK parser (Kasami, 1967; Younger, 1967) as
post-processor. However the coupling with a GLR
parser (See (Satta, 1992) for example) is perhaps
more problematic since the time taken to build up
the underlying nondeterministic LR automaton from
the sub-grammar can be prohibitive.
Though no definitive answer can be made to the
question asked in the title, we have shown that, in
some cases, the answer is certainly yes.
References
Sylvie Billot and Bernard Lang. 1989. The structure of
shared forests in ambiguous parsing. In Meeting of
the Association for Computational Linguistics, pages
143?151.
Pierre Boullier and Beno??t Sagot. 2005. Efficient and ro-
bust LFG parsing: SxLfg. In Proceedings of IWPT?05,
pages 1?10, Vancouver, Canada.
Pierre Boullier. 2000. On TAG parsing. Traitement Au-
tomatique des Langues (T.A.L.), 41(3):759?793.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT 03, pages 43?54, Nancy, France.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Jeffrey D. Hopcroft and John E. Ullman. 1979. Intro-
duction to Automata Theory, Languages, and Compu-
tation. Addison-Wesley, Reading, Mass.
Aravind Joshi. 1997. Parsing techniques. In Sur-
vey of the state of the art in human language tech-
nology, pages 351?356. Cambridge University Press,
New York, NY, USA.
Tadao Kasami. 1967. An efficient recognition and syntax
algorithm for context-free languages. Scientific Re-
port AFCRL-65?758, Air Force Cambridge Research
Laboratory, Bedford, Massachusetts, USA.
Robert C. Moore. 2000. Improved left-corner
chart parsing for large context-free gram-
mars. In Proceedings of IWPT 2000, pages
18Contrarily to classical Earley parsers, its predictor phase
uses a pre-computed structure which is roughly an LC relation.
Note that this feature forces our filters to compute an LC rela-
tion on the generated sub-grammar. This also shows that LC
parsers may also benefit from our filtering techniques.
171?182, Trento, Italy. Revised version at
http://www.cogs.susx.ac.uk/lab/nlp/
carroll/cfg-resources/iwpt2000-rev2.ps.
Mark-Jan Nederhof and Giorgio Satta. 2000. Left-to-
right parsing and bilexical context-free grammars. In
Proceedings of the first conference on North American
chapter of the ACL, pages 272?279, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Mark-Jan Nederhof. 1993. Generalized left-corner pars-
ing. In Proceedings of the sixth conference on Euro-
pean chapter of the ACL, pages 305?314, Morristown,
NJ, USA. ACL.
Beno??t Sagot and Pierre Boullier. 2005. From raw cor-
pus to word lattices: robust pre-parsing processing. In
Proceedings of L&TC 2005, pages 348?351, Poznan?,
Poland.
Beno??t Sagot, Lionel Cle?ment, ?Eric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2 syn-
tactic lexicon for french: architecture, acquisition, use.
In Proc. of LREC?06.
Giorgio Satta and Oliviero Stock. 1994. Bidirectional
context-free grammar parsing for natural language
processing. Artif. Intell., 69(1-2):123?164.
Giorgio Satta. 1992. Review of ?generalized lr parsing?
by masaru tomita. kluwer academic publishers 1991.
Comput. Linguist., 18(3):377?381.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: Cubic-time, parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21(4):479?513.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Comput.
Linguist. (COLING?88), Budapest, Hungary.
Gertjan van Noord. 1997. An efficient implementation of
the head-corner parser. Comput. Linguist., 23(3):425?
456.
?Eric Villemonte de La Clergerie. 2005. From metagram-
mars to factorized TAG/TIG parsers. In Proceedings
of IWPT?05, pages 190?191, Vancouver, Canada.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
105
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 117?128,
Paris, October 2009. c?2009 Association for Computational Linguistics
Constructing parse forests that include exactly the n-best PCFG trees
Pierre Boullier1, Alexis Nasr2 and Beno??t Sagot1
1. Alpage, INRIA Paris-Rocquencourt & Universite? Paris 7
Domaine de Voluceau ? Rocquencourt, BP 105 ? 78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
2. LIF, Univ. de la Me?diterranne?e
163, avenue de Luminy - Case 901 ? 13288 Marseille Cedex 9, France
Alexis.Nasr@lif.univ-mrs.fr
Abstract
This paper describes and compares two al-
gorithms that take as input a shared PCFG
parse forest and produce shared forests
that contain exactly the n most likely trees
of the initial forest. Such forests are
suitable for subsequent processing, such
as (some types of) reranking or LFG f-
structure computation, that can be per-
formed ontop of a shared forest, but that
may have a high (e.g., exponential) com-
plexity w.r.t. the number of trees contained
in the forest. We evaluate the perfor-
mances of both algorithms on real-scale
NLP forests generated with a PCFG ex-
tracted from the Penn Treebank.
1 Introduction
The output of a CFG parser based on dynamic
programming, such as an Earley parser (Earley,
1970), is a compact representation of all syntac-
tic parses of the parsed sentence, called a shared
parse forest (Lang, 1974; Lang, 1994). It can rep-
resent an exponential number of parses (with re-
spect to the length of the sentence) in a cubic size
structure. This forest can be used for further pro-
cessing, as reranking (Huang, 2008) or machine
translation (Mi et al, 2008).
When a CFG is associated with probabilistic in-
formation, as in a Probabilistic CFG (PCFG), it
can be interesting to process only the n most likely
trees of the forest. Standard state-of-the-art algo-
rithms that extract the n best parses (Huang and
Chiang, 2005) produce a collection of trees, los-
ing the factorization that has been achieved by the
parser, and reproduce some identical sub-trees in
several parses.
This situation is not satisfactory since post-
parsing processes, such as reranking algorithms
or attribute computation, cannot take advantage
of this lost factorization and may reproduce some
identical work on common sub-trees, with a com-
putational cost that can be exponentally high.
One way to solve the problem is to prune the
forest by eliminating sub-forests that do not con-
tribute to any of the n most likely trees. But this
over-generates: the pruned forest contains more
than the n most likely trees. This is particularly
costly for post-parsing processes that may require
in the worst cases an exponential execution time
w.r.t. the number of trees in the forest, such as
LFG f-structures construction or some advanced
reranking techniques. The experiments detailed
in the last part of this paper show that the over-
generation factor of pruned sub-forest is more or
less constant (see 6): after pruning the forest so as
to keep the n best trees, the resulting forest con-
tains approximately 103n trees. At least for some
post-parsing processes, this overhead is highly
problematic. For example, although LFG parsing
can be achieved by computing LFG f-structures
on top of a c-structure parse forest with a reason-
able efficiency (Boullier and Sagot, 2005), it is
clear that a 103 factor drastically affects the overall
speed of the LFG parser.
Therefore, simply pruning the forest is not an
adequate solution. However, it will prove useful
for comparison purposes.
The new direction that we explore in this pa-
per is the production of shared forests that con-
tain exactly the n most likely trees, avoiding both
the explicit construction of n different trees and
the over-generation of pruning techniques. This
can be seen as a transduction which is applied on
a forest and produces another forest. The trans-
duction applies some local transformations on the
structure of the forest, developing some parts of
the forest when necessary.
The structure of this paper is the following. Sec-
tion 2 defines the basic objects we will be dealing
with. Section 3 describes how to prune a shared
117
forest, and introduces two approaches for build-
ing shared forests that contain exactly the n most
likely parses. Section 4 describes experiments that
were carried out on the Penn Treebank and sec-
tion 5 concludes the paper.
2 Preliminaries
2.1 Instantiated grammars
Let G = ?N ,T ,P, S? be a context-free grammar
(CFG), defined in the usual way (Aho and Ullman,
1972). Throughout this paper, we suppose that we
manipulate only non-cyclic CFGs,1 but they may
(and usually do) include ?-productions. Given a
production p ? P, we note lhs(p) its left-hand
side, rhs(p) its right-hand side and |p| the length
of rhs(p). Moreover, we note rhsk(p), with 1 ?
k ? |p|, the kth symbol of rhs(p). We call A-
production any production p ? P of G such that
lhs(p) = A.
A complete derivation of a sentence w =
t1 . . . t|w| (?i ? |w|, ti ? T ) w.r.t. G is of the form
S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w. By def-
inition, A ? X1X2 . . . Xr is a production of G.
Each of A, X1, X2, . . . , Xr spans a unique oc-
currence of a substring ti+1 . . . tj of w, that can
be identified by the corresponding range, noted
i..j. A complete derivation represents a parse tree
whose yield is w, in which each symbol X of
range i..j roots a subtree whose yield is ti+1 . . . tj
(i.e., a derivation of the form X ??
G,w
ti+1 . . . tj).
Let us define the w-instantiation operation (or
instantiation). It can be applied to symbols and
productions of G, and to G itself, w.r.t. a string
w. It corresponds to the well-known intersection
of G with the linear automaton that corresponds
to the string w. We shall go into further detail for
terminology, notation and illustration purposes.
An instantiated non terminal symbol is a triple
noted Ai..j where A ? N and 0 ? i ? j ? |w|.
Similarly, an instantiated terminal symbol is a
triple noted Ti..j where T ? T and 0 ? i ? j =
i + 1 ? |w|. An instantiated symbol, terminal or
non terminal, is noted Xi..j . For any instantiated
symbol Xi..j , i (resp. j) is called its lower bound
1Actually, cyclic CFG can be treated as well, but not
cyclic parse forests. Therefore, if using a cyclic CFG which,
on a particular sentence, builds a cyclic parse forest, cycles
have to be removed before the algorithms descibed in the next
sections are applied. This is the case in the SYNTAX system
(see below).
(resp. upper bound), and can be extracted by the
operator lb() (resp. ub()).
An instantiated production (or instantiated
rule) is a context-free production Ai..j ?
X1i1..j1X2i2..j2 . . . Xrir ..jr whose left-hand side is an
instantiated non terminal symbol and whose right-
hand side is a (possibly empty) sequence of in-
stantiated (terminal or non terminal) symbols, pro-
vided the followings conditions hold:
1. the indexes involved are such that i = i1, j =
jr , and ?l such that 1 ? l < r, jl = il+1;
2. the corresponding non-instantiated produc-
tion A ? X1X2 . . . Xr is a production of
G.
If lhs(p) = Ai..j , we set lb(p) = i and ub(p) = j.
In a complete derivation S ??
G,w
?A? ?
G,w
?X1X2 . . . Xr? ??
G,w
w, any symbol X that spans
the range i..j can be replaced by the instantiated
symbols Xi..j . For example, the axiom S can be
replaced by the instantiated axiom S0..|w| in the
head of the derivation. If applied to the whole
derivation, this operation creates an instantiated
derivation, whose rewriting operations define a
particular set of instantiated productions. Given
G and w, the set of all instantiated productions in-
volved in at least one complete derivation of w is
unique, and noted Pw. An instantiated derivation
represents an instantiated parse tree, i.e., a parse
tree whose node labels are instantiated symbols.
In an instantiated parse tree, each node label is
unique, and therefore we shall not distinguish be-
tween a node in an instantiated parse tree and its
label (i.e., an instantiated symbol).
Then, the w-instantiated grammar Gw for G
and w is a CFG ?Nw,Tw,Pw, S0..|w|? such that:
1. Pw is defined as explained above;
2. Nw is a set of instantiated non terminal sym-
bols;
3. Tw is a set of instantiated terminal symbols.
It follows from the definition of Pw that (instan-
tiated) symbols of Gw have the following prop-
erties: Ai..j ? Nw ? A ??G,w ti+1 . . . tj , and
Ti..j ? Tw ? T = tj .
The w-instantiated CFG Gw represents all parse
trees for w in a shared (factorized) way. It is the
grammar representation of the parse forest of w
118
w.r.t. G.2 In fact, L(Gw) = {w} and the set
of parses of w with respect to Gw is isomorphic
to the set of parses of w with respect to G, the
isomorphism being the w-instantiation operation.
The size of a forest is defined as the size of the
grammar that represents it, i.e., as the number of
symbol occurrences in this grammar, which is de-
fined as the number of productions plus the sum of
the lengths of all right-hand sides.
Example 1: First running example.
Let us illustrate these definitions by an example.
Given the sentence w = the boy saw a man with a
telescope and the grammar G (that the reader has
in mind), the instantiated productions of Gw are:
Det0..1 ? the0..1 N1..2 ? boy1..2
NP0..2 ? Det0..1 N1..2 V2..3 ? saw2..3
Det3..4 ? a3..4 N4..5 ? man4..5
NP3..5 ? Det3..4 N4..5 Prep5..6 ? with5..6
Det6..7 ? a6..7 N7..8 ? telescope7..8
NP6..8 ? Det6..7 N7..8 PP5..8 ? Prep5..6 NP6..8
NP3..8 ? NP3..5 PP5..8 VP2..8 ? V2..3 NP3..8
VP2..5 ? V2..3 NP3..5 VP2..8 ? VP2..5 PP5..8
S0..8 ? NP0..2 VP2..8
They represent the parse forest of w according to
G. This parse forest contains two trees, since there
is one ambiguity: VP2..8 can be rewritten in two
different ways.
The instantiated grammar Gw can be repre-
sented as an hypergraph (as in (Klein and Man-
ning, 2001) or (Huang and Chiang, 2005)) where
the instantiated symbols of Gw correspond to the
vertices of the hypergraph and the instantiated pro-
ductions to the hyperarcs.
We define the extension of an instantiated sym-
bol Xi..j , noted E(Xi..j), as the set of instantiated
parse trees that have Xi..j as a root. The set of all
parse trees of w w.r.t. G is therefore E(S0..|w|). In
the same way, we define the extension of an in-
stantiated production Xi..j ? ? to be the subset
of E(Xi..j) that corresponds to derivations of the
form Xi..j ?G,w ?
??
G,w
ti+1 . . . tj (i.e., trees rooted
in Xi..j and where the daughters of the node Xi..j
are the symbols of ?).
2.2 Forest traversals
Let us suppose that we deal with non-cyclic
forests, i.e., we only consider forests that are rep-
2In particular, if G is a binary grammar, its w-instantation
(i.e., the parse forest of w) has a size O(|w|3), whereas it rep-
resents a potentially exponential number of parse trees w.r.t
|w| since we manipulate only non-cyclic grammars.
resented by a non-recursive instantiated CFG. In
this case, we can define two different kinds of for-
est traversals.
A bottom-up traversal of a forest is a traversal
with the following constraint: an Ai..j-production
is visited if and only if all its instantiated right-
hand side symbols have already been visited; the
instantiated symbol Ai..j is visited once all Ai..j-
productions have been visited. The bottom-up
visit starts by visiting all instantiated productions
with right-hand sides that are empty or contain
only (instantiated) terminal symbols.
A top-down traversal of a forest is a traversal
with the following constraint: a node Ai..j is vis-
ited if and only if all the instantiated productions
in which it occurs in right-hand side have already
been visited; once an instantiated production Ai..j
has been visited, all its Ai..j-productions are vis-
ited as well. Of course the top-down visit starts by
the visit of the axiom S0..|w|.
2.3 Ranked instantiated grammar
When an instantiated grammar Gw =
?Nw,Tw,Pw, S0..|w|? is built on a PCFG, ev-
ery parse tree in E(S0..|w|) has a probability that
is computed in the usual way (Booth, 1969). We
might be interested in extracting the kth most
likely tree of the forest represented by Gw,3 with-
out unfolding the forest, i.e., without enumerating
trees. In order to do so, we need to add some
extra structure to the instantiated grammar. The
augmented instantiated grammar will be called a
ranked instantiated grammar.
This extra structure takes the form of n-best ta-
bles that are associated with each instantiated non
terminal symbol (Huang and Chiang, 2005), thus
leading to ranked instantiated non terminal sym-
bols, or simply instantiated symbols when the con-
text is non ambiguous. A ranked instantiated non
terminal symbol is written ?Ai..j,T (Ai..j)?, where
T (Ai..j) is the n-best table associated with the in-
stantiated symbol Ai..j .
T (Ai..j) is a table of at most n entries. The
k-th entry of the table, noted e, describes how to
build the k-th most likely tree of E(Ai..j). This
tree will be called the k-th extention of Ai..j , noted
Ek(Ai..j). More precisely, e indicates the instanti-
ated Ai..j-production p such that Ek(Ai..j) ? E(p).
It indicates furthermore which trees of the exten-
3In this paper, we shall use the kth most likely tree and the
tree of rank k as synonyms.
119
sions of p?s right-hand side symbols must be com-
bined together in order to build Ek(Ai..j).
We also define the m,n-extension of Ai..j as
follows: Em,n(Ai..j) = ?m?k?nEk(Ai..j).
Example 2: n-best tables for the first running
example.
Let us illustrate this idea on our first running ex-
ample. Recall that in Example 1, the symbol VP2..8
can be rewritten using the two following produc-
tions :
VP2..8 ? V2..3 NP3..8
VP2..8 ? VP2..5 PP5..8
T (VP2..8) has the following form:
1 P1 VP2..8 ? V2..3 NP3..8 ?1, 1? 1
2 P2 VP2..8 ? VP2..5 PP5..8 ?1, 1? 1
This table indicates that the most likely tree
associated with VP2..8 (line one) has probability
P1 and is built using the production VP2..8 ?
V2..3 NP3..8 by combining the most likely tree of
E(V2..3) (indicated by the first 1 in ?1, 1?) with the
most likely tree of E(NP3..8) (indicated by the sec-
ond 1 in ?1, 1?). It also indicates that the most
likely tree of E(VP2..8) is the most likely tree of
E(VP2..8 ? V2..3 NP3..8) (indicated by the pres-
ence of 1 in the last column of entry 1) and the
second most likely tree of E(VP2..8) is the most
likely tree of E(VP2..8 ? VP2..5 PP5..8). This last
integer is called the local rank of the entry.
More formally, the entry T (Ai..j)[k] is defined
as a 4-tuple ?Pk, pk, ~vk, lk? where k is the rank
of the entry, Pk is the probability of the tree
Ek(Ai..j), pk is the instantiated production such
that Ek(Ai..j) ? E(pk), ~vk is a tuple of |rhs(pk)|
integers and lk is the local rank.
The tree Ek(Ai..j) is rooted by Ai..j , and its
daughters root N = |rhs(pk)| subtrees that are
E ~vk[1](rhs1(pk)), . . . , E ~vk [N ](rhsN (pk)).
Given an instantiated symbol Ai..j and an in-
stantitated production p ? P (Ai..j), we define
the n-best table of p to be the table composed
of the entries ?Pk, pk, ~vk, lk? of T (Ai..j) such that
pk = p.
Example 3: Second running example.
The following is a standard PCFG (probabili-
ties are shown next to the corresponding clauses).
S ? A B 1
A ? A1 0.7 A1 ? a 1
A ? A2 0.3 A2 ? a 1
B ? B1 0.6 B1 ? b 1
B ? B2 0.4 B2 ? b 1
The instantiation of the underlying (non-
probabilistic) CFG grammar by the input text
w = a b is the following.
S1..3 ? A1..2 B2..3
A1..2 ? A11..2 A11..2 ? a1..2
A1..2 ? A21..2 A21..2 ? a1..2
B2..3 ? B12..3 B12..3 ? b2..3
B2..3 ? B22..3 B22..3 ? b2..3
This grammar represents a parse forest that con-
tains four different trees, since on the one hand one
can reach (parse) the instantiated terminal symbol
a1..2 through A1 or A2, and on the other hand one
can reach (parse) the instantiated terminal sym-
bol b1..2 through B1 or B2. Therefore, when dis-
cussing this example in the remainder of the paper,
each of these four trees will be named accordingly:
the tree obtained by reaching a through Ai and b
through Bj (i and j are 1 or 2) shall be called
Ti,j .
The corresponding n-best tables are trivial
(only one line) for all instantiated symbols but
A1..2, B2..3 and S1..3. That of A1..2 is the follow-
ing 2-line table.
1 0.7 A ? A1 ?1? 1
2 0.3 A ? A2 ?1? 1
The n-best table for B2..3 is similar. The n-best
table for S1..3 is:
1 0.42 S1..3 ? A1..2 B2..3 ?1, 1? 1
2 0.28 S1..3 ? A1..2 B2..3 ?1, 2? 2
3 0.18 S1..3 ? A1..2 B2..3 ?2, 1? 3
4 0.12 S1..3 ? A1..2 B2..3 ?2, 2? 4
Thanks to the algorithm sketched in section 2.4,
these tables allow to compute the following obvi-
ous result: the best tree is T1,1, the second-best
tree is T1,2, the third-best tree is T2,1 and the worst
tree is T2,2.
If n = 3, the pruned forest over-generates: all
instantiated productions take part in at least one
of the three best trees, and therefore the pruned
forest is the full forest itself, which contains four
trees.
We shall use this example later on so as to il-
lustrate both methods we introduce for building
forests that contain exactly the n best trees, with-
out overgenerating.
2.4 Extracting the kth-best tree
An efficient algorithm for the extraction of the n-
best trees is introduced in (Huang and Chiang,
2005), namely the authors? algorithm 3, which
120
is a re-formulation of a procedure originally pro-
posed by (Jime?nez and Marzal, 2000). Contrar-
ily to (Huang and Chiang, 2005), we shall sketch
this algorithm with the terminology introduced
above (whereas the authors use the notion of hy-
pergraph). The algorithm relies on the n-best ta-
bles described above: extracting the kth-best tree
consists in extending the n-best tables as much as
necessary by computing all lines in each n-best ta-
ble up to those that concern the kth-best tree.4
The algorithm can be divided in two sub-
algorithms: (1) a bottom-up traversal of the for-
est for extracting the best tree; (2) a top-down
traversal for extracting the kth-best tree provided
the (k ? 1)th-best has been already extracted.
The extraction of the best tree can be seen as a
bottom-up traversal that initializes the n-best ta-
bles: when visiting a node Ai..j , the best probabil-
ity of each Ai..j-production is computed by using
the tables associated with each of their right-hand
side symbols. The best of these probabilities gives
the first line of the n-best table for Ai..j (the result
for other productions are stored for possible later
use). Once the traversal is completed (the instanti-
ated axiom has been reached), the best tree can be
easily output by following recursively where the
first line of the axiom?s n-best table leads to.
Let us now assume we have extracted all k?-best
trees, 1 ? k? < k, for a given k ? n. We want
to extract the kth-best tree. We achieve this recur-
sively by a top-down traversal of the forest. In or-
der to start the construction of the kth-best tree, we
need to know the following:
? which instantiated production p must be used
for rewriting the instantiated axiom,
? for each of p?s right-hand side symbols Ai..j ,
which subtree rooted in Ai..j must be used;
this subtree is identified by its local rank
kAi..j , i.e., the rank of its probability among
all subtrees rooted in Ai..j.
This information is given by the kth line of the n-
best table associated with the instantiated axiom.
If this kth line has not been filled yet, it is com-
puted recursively.5 Once the kth line of the n-best
4In the remainder of this paper, we shall use ?extracting
the kth-best tree? as a shortcut for ?extending the n-best ta-
bles up to what is necessary to extract the kth-best tree? (i.e.,
we do not necessarily really build or print the kth-best tree).
5Because the k ? 1th-best tree has been computed, this n-
best table is filled exactly up to line k?1. The kth line is then
table is known, i.e., p and all kAi..j ?s are known,
the rank k is added to p?s so-called rankset, noted
?(p). Then, the top-down traversal extracts recur-
sively for each Ai..j the appropriate subtree as de-
fined by kAi..j . After having extracted the n-th
best tree, we know that a given production p is in-
cluded in the kth-best tree, 1 ? k ? n, if and only
if k ? ?(p).
3 Computing sub-forests that only
contain the n best trees
Given a ranked instantiated grammar Gw, we are
interested in building a new instantiated grammar
which contains exactly the n most likely trees of
E(Gw). In this section, we introduce two algo-
rithms that compute such a grammar (or forest).
Both methods rely on the construction of new
symbols, obtained by decorating instantiated sym-
bols of Gw.
An empirical comparison of the two methods is
described in section 4. In order to evaluate the
size of the new constructed grammars (forests),
we consider as a lower bound the so-called pruned
forest, which is the smallest sub-grammar of the
initial instantiated grammar that includes the n
best trees. It is built simply by pruning produc-
tions with an empty rankset: no new symbols
are created, original instantiated symbols are kept.
Therefore, it is a lower bound in terms of size.
However, the pruned forest usually overgenerates,
as illustrated by Example 3.
3.1 The ranksets method
The algorithm described in this section builds an
instantiated grammar Gnw by decorating the sym-
bols of Gw. The new (decorated) symbols have
the form A?i..j where ? is a set of integers called
a rankset. An integer r is a rank iff we have
1 ? r ? n.
The starting point of this algorithm is set of n-
best tables, built as explained in section 2.4, with-
out explicitely unfolding the forest.
computed as follows: while constructing the k?th-best trees
for each k? between 1 and k?1, we have identified many pos-
sible rewritings of the instantiated axiom, i.e., many (produc-
tion, right-hand side local ranks) pairs; we know the proba-
bility of all these rewritings, although only some of them con-
situte a line of the instantiated axiom?s n-best table; we now
identify new rewritings, starting from known rewritings and
incrementing only one of their local ranks; we compute (re-
cursively) the probability of these newly identified rewritings;
the rewriting that has the best probability among all those that
are not yet a line of the n-best table is then added: it is its kth
line.
121
A preliminary top-down step uses these n-best
tables for building a parse forest whose non-
terminal symbols (apart from the axiom) have the
form A?i..j where ? is a singleton {r}: the sub-
forest rooted in A{r}i..j contains only one tree, that
of local rank r. Only the axiom is not decorated,
and remains unique. Terminal symbols are not af-
fected either.
At this point, the purpose of the algorithm is to
merge productions with identical right-hand sides,
whenever possible. This is achieved in a bottom-
up fashion as follows. Consider two symbols A?1i..j
and A?2i..j , which differ only by their underlying
ranksets. These symbols correspond to two dif-
ferent production sets, namely the set of all A?1i..j-
productions (resp. A?2i..j-productions). Each of
these production sets define a set of right-hand
sides. If these two right-hand side sets are iden-
tical we say that A?1i..j and A
?2
i..j are equivalent. In
that case introduce the rankset ? = ?1 ? ?2 and
create a new non-terminal symbol A?i..j . We now
simply replace all occurrences of A?1i..j and A
?2
i..j
in left- and right-hand sides by A?i..j . Of course
(newly) identical productions are erased. After
such a transformation, the newly created symbol
may appear in the right-hand side of productions
that now only differ by their left-hand sides; the
factorization spreads to this symbol in a bottom-
up way. Therefore, we perform this transforma-
tion until no new pair of equivalent symbols is
found, starting from terminal leaves and percolat-
ing bottom-up as far as possible.
Example 4: Applying the ranksets method to
the second running example.
Let us come back to the grammar of Example 3,
and the same input text w = a b as before. As
in Example 3, we consider the case when we are
interested in the n = 3 best trees.
Starting from the instantiated grammar and the
n-best tables given in Example 3, the preliminary
top-down step builds the following forest (for clar-
ity, ranksets have not been shown on symbols that
root sub-forests containing only one tree):
S1..3 ? A{1}1..2 B
{1}
2..3
S1..3 ? A{1}1..2 B
{2}
2..3
S1..3 ? A{2}1..2 B
{1}
2..3
A{1}1..2 ? A11..2 A11..2 ? a1..2
A{2}1..2 ? A21..2 A21..2 ? a1..2
B{1}2..3 ? B12..3 B12..3 ? b2..3
B{2}2..3 ? B22..3 B22..3 ? b2..3
In this example, the bottom-up step doesn?t fac-
torize out any other symbols, and this is therefore
the final output of the ranksets method. It con-
tains 2 more productions and 3 more symbols than
the pruned forest (which is the same as the origi-
nal forest), but it contains exactly the 3 best trees,
contrarily to the pruned forest.
3.2 The rectangles method
In this section only, we assume that the grammar
G is binary (and therefore the forest, i.e., the gram-
mar Gw, is binary). Standard binarization algo-
rithms can be found in the litterature (Aho and Ull-
man, 1972).
The algorithm described in this section per-
forms, as the preceding one, a decoration of the
symbols of Gw. The new (decorated) symbols
have the form Ax,yi..j , where x and y denote ranks
such that 1 ? x ? y ? n. The semantics of the
decoration is closely related to the x, y extention
of Ai..j , introduced in 2.3:
E(Ax,yi..j) = Ex,y(Ai..j)
It corresponds to ranksets (in the sense of the
previous section) that are intervals: Ax,yi..j is equiv-
alent to the previous section?s A{x,x+1,...,y?1,y}i..j . In
other words, the sub-forest rooted with Ax,yi..j con-
tains exactly the trees of the initial forest, rooted
with Ai..j , which rank range from x to y.
The algorithm performs a top-down traversal of
the initial instantiated grammar Gw. This traver-
sal also takes as input two parameters x and y. It
starts with the symbol S0..|w| and parameters 1 and
n. At the end of the traversal, a new decorated for-
est is built which contains exactly n most likely
the parses. During the traversal, every instantiated
symbol Ai..j will give birth to decorated instanti-
ated symbols of the form Ax,yi..j where x and y are
determined during the traversal. Two different ac-
tions are performed depending on whether we are
122
visiting an instantiated symbol or an instantiated
production.
3.2.1 Visiting an instantiated symbol
When visiting an instantiated symbol Ai..j with
parameters x and y, a new decorated instan-
tiated symbol Ax,yi,j is created and the traver-
sal continues on the instantiated productions of
P (Ai..j) with parameters that have to be com-
puted. These parameters depend on how the el-
ements of Ex,y(Ai..j) are ?distributed? among the
sets E(p) with p ? P (Ai..j). In other words, we
need to determine xk?s and yk?s such that:
Ex,y(Ai..j) =
?
pk?P (Ai..j)
Exk,yk(pk)
The idea can be easily illustrated on an exam-
ple. Suppose we are visiting the instantiated sym-
bol Ai..j with parameters 5 and 10. Suppose also
that Ai..j can be rewritten using the two instanti-
ated productions p1 and p2. Suppose finally that
the 5 to 10 entries of T (Ai..j) are as follows6:
5 p1 4
6 p2 2
7 p2 3
8 p1 5
9 p2 4
10 p1 6
This table says that E5(Ai..j) = E4(p1) i.e. the
5th most likely analysis of E(Ai..j) is the 4th most
likely analysis of E(p1) and E6(Ai..j) = E2(p2)
and so on. From this table we can deduce that:
E5,10(Ai..j) = E4,6(p1) ? E2,4(p2)
The traversal therefore continues on p1 and p2
with parameters 4, 6 and 2, 4.
3.2.2 Visiting an instantiated production
When visiting an instantiated production p of the
form Ai..j ? Bi..l Cl..j with parameters x and y,
a collection of q instantiated productions pr of the
form Ax,yi..j ? B
x1r,x2r
i..l C
y1r ,y2r
l..j , with 1 ? r ? q,
are built, where the parameters x1r, x2r , y1r , y2r and
q have to be computed.
Once the parameters q and x1r, x2r , y1r , y2r with
1 ? r ? q, have been computed, the traversal
continues independently on Bi..l with parameters
x1r and x2r and on Cl..j with parameters y1r and y2r .
6Only the relevant part of the table have been kept in the
figure.
The computation of the parameters x1r, x2r , y1r
and y2r for 1 ? r ? q, is the most complex part of
the algorithm, it relies on the three notions of rect-
angles, q-partitions and n-best matrices, which are
defined below.
Given a 4-tuple of parameters x1r , x2r, y1r , y2r ,
a rectangle is simply a pairing of the form
??x1r , x2r?, ?y1r , y2r ??. A rectangle can be interpreted
as a couple of rank ranges : ?x1r , y1r ? and ?x2r , y2r?.
It denotes the cartesian product
[
x1r, x2r
]?[y1r , y2r
]
.
Let ??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q ??
be a collection of q rectangles. It will be called a
q-partition of the instantiated production p iff the
following is true:
Ex,y(p) =
?
1?r?q
E(Ax,yi..j ? Bx
1
r,x2r
i..l C
y1r ,y2r
l..j )
To put it differently, this definition means that
??x11, x21?, ?y11 , y21??, . . . , ??x1q , x2q?, ?y1q , y2q?? is a q
partition of p if any tree of E(Bx1r,x2ri..l ) combined
with any tree of E(Cy1r ,y2rl..j ) is a tree of Ex,y(p) and,
conversely, any tree of Ex,y(p) is the combination
of a tree of E(Bx1r,x2ri..l ) and a tree of E(Cy
1
r ,y2r
l..j ).
The n-best matrix associated with an instanti-
ated production p, introduced in (Huang and Chi-
ang, 2005), is merely a two dimensional represen-
tation of the n-best table of p. Such a matrix, rep-
resents how the n most likely trees of E(p) are
built. An example of an n-best matrix is repre-
sented in figure 1. This matrix says that the first
most likely tree of p is built by combining the
tree E1(Bi..l) with the tree E1(Cl..j) (there is a 1
in the cell of coordinate ?1, 1?). The second most
likely tree is built by combining the tree E1(Bi..l)
and E2(Cl..j) (there is a 2 in the cell of coordinate
?1, 2?) and so on.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Cl..j
Bi..l
Figure 1: n-best matrix
An n-best matrix M has, by construction, the
remarkable following properties:
123
M(i, y) < M(x, y) ?i 1 ? i < x
M(x, j) < M(x, y) ?j 1 ? j < y
Given an n-best matrix M of dimensions d =
X ? Y and two integers x and y such that 1 ? x <
y ? d, M can be decomposed into three regions:
? the lower region, composed of the cells
which contain ranks i with 1 ? i < x
? the intermediate region, composed of the
cells which contain ranks i with x ? i ? y
? the upper region, composed of the cells
which contain ranks i such that y < i ? d.
The three regions of the matrix of figure 1, for
x = 4 and y = 27 have been delimited with bold
lines in figure 2.
1 2
3
4
6
7
8
9
10
11
12
13
14 15
16
17
18
20 21
23
5
24
26
2 3 5 6
2
3
4
5
6
19
41
1
22 2725
28
29
30
31 32 34
33
35
36
Bi..l
Cl..j
Figure 2: Decomposition of an n-best matrix into
a lower, an intermediate and an upper region with
parameters 4 and 27.
It can be seen that a rectangle, as introduced
earlier, defines a sub-matrix of the n-best matrix.
For example the rectangle ??2, 5?, ?2, 5?? defines
the sub-matrix which north west corner is M(2, 2)
and south east corner is M(5, 5), as represented in
figure 3.
When visiting an instantiated production p, hav-
ing M as an n-best matrix, with the two parame-
ters x and y, the intermediate region of M , with
respect to x and y, contains, by definition, all the
ranks that we are interested in (the ranks rang-
ing from x to y). This region can be partitioned
into a collection of disjoint rectangular regions.
Each such partition therefore defines a collection
of rectangles or a q-partition.
The computation of the parameters x1r, y1r , x2r
and y2r for an instantiated production p therefore
boils down to the computation of a partition of the
intermediate region of the n-best matrix of p.
9
10
11
12
13
17
18
20 21
5
24
26
2 5
2
5 19 22 2725
Cl..j
Bi..l
Figure 3: The sub-matrix corresponding to the
rectangle ??2, 5?, ?2, 5??
We have represented schematically, in figure 4,
two 4-partitions and a 3-partition of the interme-
diate region of the matrix of figure 2. The left-
most (resp. rightmost) partition will be called the
vertical (resp. horizontal) partition. The middle
partition will be called an optimal partition, it de-
composes the intermediate region into a minimal
number of sub-matrices.
   
   
   



III
IV
I
II
   
   
   



I
III
II
   
   
   



II
I 
III
IV
Figure 4: Three partitions of an n-best matrix
The three partitions of figure 4 will give birth to
the following instantiated productions:
? Vertical partition
A4,27i..j ? B3,6i..l C
1,1
l..j A
4,27
i..j ? B2,5i..l C
2,2
l..j
A4,27i..j ? B1,5i..l C
3,5
l..j A
4,27
i..j ? B1,1i..l C
6,6
l..j
? Optimal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,5i..l C
2,5
l..j
A4,27i..j ? B3,6i..l C
1,1
l..j
? Horizontal partition
A4,27i..j ? B1,1i..l C
3,6
l..j A
4,27
i..j ? B2,2i..l C
2,5
l..j
A4,27i..j ? B3,5i..l C
1,5
l..j A
4,27
i..j ? B6,6i..l C
1,1
l..j
Vertical and horizontal partition of the interme-
diate region of a n-best matrix can easily be com-
puted. We are not aware of an efficient method that
computes an optimal partition. In the implemen-
tation used for experiments described in section 4,
124
a simple heuristic has been used which computes
horizontal and vertical partitions and keeps the
partition with the lower number of parts.
The size of the new forest is clearly linked to
the partitions that are computed: a partition with
a lower number of parts will give birth to a lower
number of decorated instantiated productions and
therefore a smaller forest. But this optimization
is local, it does not take into account the fact that
an instantiated symbol may be shared in the initial
forest. During the computation of the new forest,
an instantiated production p can therefore be vis-
ited several times, with different parameters. Sev-
eral partitions of p will therefore be computed. If
a rectangle is shared by several partitions, this will
tend to decrease the size of the new forest. The
global optimal must therefore take into account all
the partitions of an instantiated production that are
computed during the construction of the new for-
est.
Example 5: Applying the rectangles method to
the second running example.
We now illustrate more concretely the rectan-
gles method on our second running example intro-
duced in Example 3. Let us recall that we are in-
terested in the n = 3 best trees, the original forest
containing 4 trees.
As said above, this method starts on the instan-
tiated axiom S1..3. Since it is the left-hand side
of only one production, this production is visited
with parameters 1, 3. Moreover, its n-best table is
the same as that of S1..3, given in Example 3. We
show here the corresponding n-best matrix, with
the empty lower region, the intermediate region
(cells corresponding to ranks 1 to 3) and the upper
region:
4
1 2
3
2
2
1
1A1..2
B2..3
As can be seen on that matrix, there are two op-
timal 2-partitions, namely the horizontal and the
vertical partitions, illustrated as follows:
II
I
II I
Let us arbitrarily chose the vertical partition. It
gives birth to two S 1..3-productions, namely:
S 1,31..3 ? A1,21..2 B1,12..3
S 1,31..3 ? A1,11..2 B2,22..3
Since this is the only non-trivial step while apply-
ing the rectangles algorithm to this example, we
can now give its final result, in which the axiom?s
(unnecessary) decorations have been removed:
S1..3 ? A1,21..2 B
{1,1}
2..3
S1..3 ? A1,11..2 B
{2,2}
2..3
A1,21..2 ? A11..2 A11..2 ? a1..2
A1,21..2 ? A21..2 A21..2 ? a1..2
B1,22..3 ? B12..3 B12..3 ? b2..3
B2,22..3 ? B22..3 B22..3 ? b2..3
Compared to the forest built by the ranksets algo-
rithm, this forest has one less production and one
less non-terminal symbol. It has only one more
production than the over-generating pruned for-
est.
4 Experiments on the Penn Treebank
The methods described in section 3 have been
tested on a PCFG G extracted from the Penn Tree-
bank (Marcus et al, 1993). G has been extracted
naively: the trees have been decomposed into bi-
nary context free rules, and the probability of ev-
ery rule has been estimated by its relative fre-
quency (number of occurrences of the rule divided
by the number of occurrences of its left hand side).
Rules occurring less than 3 times and rules with
probabilities lower than 3? 10?4 have been elim-
inated. The grammar produced contains 932 non
terminals and 3, 439 rules.7
The parsing has been realized using the SYN-
TAX system which implements, and optimizes, the
Earley algorithm (Boullier, 2003).
The evaluation has been conducted on the 1, 845
sentences of section 1, which constitute our test
set. For every sentence and for increasing values
of n, an n-best sub-forest has been built using the
rankset and the rectangles method.
The performances of the algorithms have been
measured by the average compression rate they
7We used this test set only to generate practical NLP
forests, with a real NLP grammar, and evaluate the perfor-
mances of our algorithms for constucting sub-forests that
contain only the n-best trees, both in terms of compression
rate and execution time. Therefore, the evaluation carried out
here has nothing to do with the usual evaluation of the pre-
cision and recall of parsers based on the Penn Treebank. In
particular, we are not interested here in the accuracy of such
a grammar, its only purpose is to generate parse forests from
which n-best sub-forests will be built.
125
0e+00
1e+05
2e+05
3e+05
4e+05
5e+05
6e+05
7e+05
8e+05
9e+05
 0  100  200  300  400  500  600  700  800  900 1000
av
g.
 n
b 
of
 t
re
es
 i
n 
th
e 
pr
un
ed
 f
or
es
t
n
Figure 5: Overgeneration of the pruned n-best forest
 1
 10
 100
 1000
 1  10  100  1000
co
mp
re
ss
io
n 
ra
te
n
pruned forest
rectangles
ranksets
Figure 6: Average compression rates
achieve for different values of n. The compres-
sion rate is obtained by dividing the size of the
n-best sub-forest of a sentence, as defined in sec-
tion 2, by the size of the (unfolded) n-best forest.
The latter is the sum of the sizes of all trees in the
forest, where every tree is seen as an instantiated
grammar, its size is therefore the size of the corre-
sponding instantiated grammar.
The size of the n-best forest constitutes a natu-
ral upper bound for the representation of the n-best
trees. Unfortunately, we have no natural lower
bound for the size of such an object. Neverthe-
less, we have computed the compression rates of
the pruned n-best forest and used it as an imperfect
lower bound. As already mentioned, its imper-
fection comes from the fact that a pruned n-best
forest contains more trees than the n best ones.
This overgeneration appears clearly in Figure 5
which shows, for increasing values of n, the av-
erage number of trees in the n-best pruned forest
for all sentences in our test set.
Figure 6 shows the average compression rates
achieved by the three methods (forest pruning,
rectangles and ranksets) on the test set for increas-
ing values of n. As predicted, the performances lie
between 1 (no compression) and the compression
of the n-best pruned forest. The rectangle method
outperforms the ranksets algorithm for every value
of n.
The time needed to build an 100-best forest with
the rectangle and the ranksets algorithms is shown
in Figure 7. This figure shows the average parsing
126
 0
 200
 400
 600
 800
 1000
 1200
 5  10  15  20  25  30  35  40  45
ti
me
 i
n 
mi
ll
is
ec
on
ds
sentence length
parsing
ranksets
rectangles
Figure 7: Processing time
time for sentences of a given length, as well as the
average time necessary for building the 100-best
forest using the two aforementioned algorithms.
This time includes the parsing time i.e. it is the
time necessary for parsing a sentence and build-
ing the 100-best forest. As shown by the figure,
the time complexities of the two methods are very
close.
5 Conclusion and perspectives
This work presented two methods to build n-
best sub-forests. The so called rectangle meth-
ods showed to be the most promising, for it al-
lows to build efficient sub-forests with little time
overhead. Future work will focus on computing
optimized partitions of the n-best matrices, a cru-
cial part of the rectangle method, and adapting the
method to arbitrary (non binary) CFG. Another
line of research will concentrate on performing
re-ranking of the n-best trees directly on the sub-
forest.
Acknowledgments
This research is supported by the French National
Research Agency (ANR) in the context of the
SEQUOIA project (ANR-08-EMER-013).
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling, vol-
ume 1. Prentice-Hall, Englewood Cliffs, NJ.
Taylor L. Booth. 1969. Probabilistic representation of
formal languages. In Tenth Annual Symposium on
Switching and Automata Theory, pages 74?81.
Pierre Boullier and Philippe Deschamp. 1988.
Le syste`me SYNTAXTM - manuel d?utilisation.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Benot Sagot. 2005. Efficient and
robust LFG parsing: SXLFG. In Proceedings of
IWPT?05, Vancouver, Canada.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of IWPT?03, pages 43?54.
Jay Earley. 1970. An efficient context-free parsing
algorithm. Communication of the ACM, 13(2):94?
102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, pages 53?64.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL?08, pages 586?594.
V??ctor M. Jime?nez and Andre?s Marzal. 2000. Com-
putation of the n best parse trees for weighted and
stochastic context-free grammars. In Proceedings
of the Joint IAPR International Workshops on Ad-
vances in Pattern Recognition, pages 183?192, Lon-
don, United Kingdom. Springer-Verlag.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT?01.
Bernard Lang. 1974. Deterministic techniques for ef-
ficient non-deterministic parsers. In J. Loeckx, ed-
itor, Proceedings of the Second Colloquium on Au-
tomata, Languages and Programming, volume 14 of
Lecture Notes in Computer Science, pages 255?269.
Springer-Verlag.
127
Bernard Lang. 1994. Recognition can be harder then
parsing. Computational Intelligence, 10:486?494.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn treebank. Computa-
tional Linguistics, 19(2):313?330, June.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
128
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 254?265,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Directed Acyclic Graphs
with Range Concatenation Grammars
Pierre Boullier and Beno??t Sagot
Alpage, INRIA Paris-Rocquencourt & Universite? Paris 7
Domaine de Voluceau ? Rocquencourt, BP 105 ? 78153 Le Chesnay Cedex, France
{Pierre.Boullier,Benoit.Sagot}@inria.fr
Abstract
Range Concatenation Grammars (RCGs)
are a syntactic formalism which possesses
many attractive properties. It is more pow-
erful than Linear Context-Free Rewriting
Systems, though this power is not reached
to the detriment of efficiency since its sen-
tences can always be parsed in polynomial
time. If the input, instead of a string, is a
Directed Acyclic Graph (DAG), only sim-
ple RCGs can still be parsed in polyno-
mial time. For non-linear RCGs, this poly-
nomial parsing time cannot be guaranteed
anymore. In this paper, we show how the
standard parsing algorithm can be adapted
for parsing DAGs with RCGs, both in the
linear (simple) and in the non-linear case.
1 Introduction
The Range Concatenation Grammar (RCG)
formalism has been introduced by Boullier ten
years ago. A complete definition can be
found in (Boullier, 2004), together with some
of its formal properties and a parsing algorithm
(qualified here of standard) which runs in
polynomial time. In this paper we shall only
consider the positive version of RCGs which
will be abbreviated as PRCG.1 PRCGs are
very attractive since they are more powerful
than the Linear Context-Free Rewriting Systems
(LCFRSs) by (Vijay-Shanker et al, 1987). In fact
LCFRSs are equivalent to simple PRCGs which
are a subclass of PRCGs. Many Mildly Context-
Sensitive (MCS) formalisms, including Tree
Adjoining Grammars (TAGs) and various kinds
of Multi-Component TAGs, have already been
1Negative RCGs do not add formal power since both
versions exactly cover the class PTIME of languages
recognizable in deterministic polynomial time (see (Boullier,
2004) for an indirect proof and (Bertsch and Nederhof, 2001)
for a direct proof).
translated into their simple PRCG counterpart in
order to get an efficient parser for free (see for
example (Barthe?lemy et al, 2001)).
However, in many Natural Language Process-
ing applications, the most suitable input for a
parser is not a sequence of words (forms, ter-
minal symbols), but a more complex representa-
tion, usually defined as a Direct Acyclic Graph
(DAG), which correspond to finite regular lan-
guages, for taking into account various kinds of
ambiguities. Such ambiguities may come, among
others, from the output of speech recognition sys-
tems, from lexical ambiguities (and in particular
from tokenization ambiguities), or from a non-
deterministic spelling correction module.
Yet, it has been shown by (Bertsch and
Nederhof, 2001) that parsing of regular languages
(and therefore of DAGs) using simple PRCGs is
polynomial. In the same paper, it is also proven
that parsing of finite regular languages (the DAG
case) using arbitrary RCGs is NP-complete.
This papers aims at showing how these
complexity results can be made concrete in a
parser, by extending a standard RCG parsing
algorithm so as to handle input DAGs. We
will first recall both some basic definitions and
their notations. Afterwards we will see, with a
slight modification of the notion of ranges, how
it is possible to use the standard PRCG parsing
algorithm to get in polynomial time a parse forest
with a DAG as input.2 However, the resulting
parse forest is valid only for simple PRCGs. In
the non-linear case, and consistently with the
complexity results mentioned above, we show that
the resulting parse forest needs further processing
for filtering out inconsistent parses, which may
need an exponential time. The proposed filtering
algorithm allows for parsing DAGs in practice
with any PRCG, including non-linear ones.
2The notion of parse forest is reminiscent of the work
of (Lang, 1994).
254
2 Basic notions and notations
2.1 Positive Range Concatenation Grammars
A positive range concatenation grammar (PRCG)
G = (N,T, V, P, S) is a 5-tuple in which:
? T and V are disjoint alphabets of terminal
symbols and variable symbols respectively.
? N is a non-empty finite set of predicates of
fixed arity (also called fan-out). We write
k = arity(A) if the arity of the predicate A is
k. A predicate A with its arguments is noted
A(~?) with a vector notation such that |~?| = k
and ~?[j] is its jth argument. An argument is a
string in (V ? T )?.
? S is a distinguished predicate called the start
predicate (or axiom) of arity 1.
? P is a finite set of clauses. A clause c
is a rewriting rule of the form A0( ~?0) ?
A1( ~?1) . . . Ar( ~?r) where r, r ? 0 is its
rank, A0( ~?0) is its left-hand side or LHS,
and A1( ~?1) . . . Ar( ~?r) its right-hand side or
RHS. By definition c[i] = Ai(~?i), 0 ? i ? r
where Ai is a predicate and ~?i its arguments;
we note c[i][j] its jth argument; c[i][j] is of
the form X1 . . . Xnij (the Xk?s are terminal
or variable symbols), while c[i][j][k], 0 ?
k ? nij is a position within c[i][j].
For a given clause c, and one of its predicates
c[i] a subargument is defined as a substring of an
argument c[i][j] of the predicate c[i]. It is denoted
by a pair of positions (c[i][j][k], c[i][j][k? ]), with
k ? k?.
Let w = a1 . . . an be an input string in T ?,
each occurrence of a substring al+1 . . . au is a pair
of positions (w[l], w[u]) s.t. 0 ? l ? u ? n
called a range and noted ?l..u?w or ?l..u? when
w is implicit. In the range ?l..u?, l is its lower
bound while u is its upper bound. If l = u,
the range ?l..u? is an empty range, it spans an
empty substring. If ?1 = ?l1..u1?, . . . and
?m = ?lm..um? are ranges, the concatenation of
?1, . . . , ?m noted ?1 . . . ?m is the range ? = ?l..u?
if and only if we have ui = li+1, 1 ? i < m,
l = l1 and u = um.
If c = A0( ~?0) ? A1( ~?1) . . . Ar( ~?r) is a
clause, each of its sub-
arguments (c[i][j][k], c[i][j][k? ]) may take a range
? = ?l..u? as value: we say that it is instantiated
by ?. However, the instantiation of a subargument
is subjected to the following constraints.
? If the subargument is the empty string (i.e.,
k = k?), ? is an empty range.
? If the subargument is a terminal symbol (i.e.,
k + 1 = k? and Xk? ? T ), ? is such that
l + 1 = u and au = Xk? . Note that several
occurrences of the same terminal symbol
may be instantiated by different ranges.
? If the subargument is a variable symbol
(i.e., k + 1 = k? and Xk? ? V ),
any occurrence (c[i?][j?][m], c[i?][j?][m?]) of
Xk? is instantiated by ?. Thus, each
occurrence of the same variable symbol must
be instantiated by the same range.
? If the subargument is the string Xk+1 . . . Xk? ,
? is its instantiation if and only if we have
? = ?k+1 . . . ?k? in which ?k+1, . . . , ?k? are
respectively the instantiations of Xk+1, . . . ,
Xk? .
If in c we replace each argument by its
instantiation, we get an instantiated clause noted
A0(~?0) ? A1(~?1) . . . Ar(~?r) in which each
Ai(~?i) is an instantiated predicate.
A binary relation called derive and noted ?
G,w
is
defined on strings of instantiated predicates. If ?1
and ?2 are strings of instantiated predicates, we
have
?1 A0(~?0) ?2 ?G,w ?1 A1(~?1) . . . Am( ~?m) ?2
if and only if A0(~?0) ? A1(~?1) . . . Am( ~?m) is an
instantiated clause.
The (string) language of a PRCG G is the
set L(G) = {w | S(?0..|w|?w) +?G,w ?}. In
other words, an input string w ? T ?, |w| =
n is a sentence of G if and only there exists a
complete derivation which starts from S(?0..n?)
(the instantiation of the start predicate on the
whole input text) and leads to the empty string
(of instantiated predicates). The parse forest of w
is the CFG whose axiom is S(?0..n?) and whose
productions are the instantiated clauses used in all
complete derivations.3
We say that the arity of a PRCG is k, and we
call it a k-PRCG, if and only if k is the maximum
3Note that this parse forest has no terminal symbols (its
language is the empty string).
255
arity of its predicates (k = maxA?N arity(A)).
We say that a k-PRCG is simple, we have a simple
k-PRCG, if and only if each of its clause is
? non-combinatorial: the arguments of its RHS
predicates are single variables;
? non-erasing: each variable which occur in
its LHS (resp. RHS) also occurs in its RHS
(resp. LHS);
? linear: there are no variables which occur
more than once in its LHS and in its RHS.
The subclass of simple PRCGs is of importance
since it is MCS and is the one equivalent to
LCFRSs.
2.2 Finite Automata
A non-deterministic finite automaton (NFA) is
the 5-tuple A = (Q,?, ?, q0, F ) where Q is a
non empty finite set of states, ? is a finite set
of terminal symbols, ? is the ternary transition
relation ? = {(qi, t, qj)|qi, qj ? Q? t ? ??{?}},
q0 is a distinguished element of Q called the initial
state and F is a subset of Q whose elements are
called final states. The size of A, noted |A|, is its
number of states (|A| = |Q|).
We define the ternary relation ?? on Q????Q
as the smallest set s.t. ?? = {(q, ?, q) | q ? Q} ?
{(q1, xt, q3) | (q1, x, q2) ? ?? ? (q2, t, q3) ? ?}. If
(q, x, q?) ? ??, we say that x is a path between q
and q?. If q = q0 and q? ? F , x is a complete path.
The language L(A) defined (generated, recog-
nized, accepted) by the NFA A is the set of all its
complete paths.
We say that a NFA is empty if and only if its
language is empty. Two NFAs are equivalent if
and only if they define the same language. A
NFA is ?-free if and only if its transition relation
does not contain a transition of the form (q1, ?, q2).
Every NFA can be transformed into an equivalent
?-free NFA (this classical result and those recalled
below can be found, e.g., in (Hopcroft and Ullman,
1979)).
As usual, a NFA is drawn with the following
conventions: a transition (q1, t, q2) is an arrow
labelled t from state q1 to state q2 which are
printed with a surrounded circle. Final states are
doubly circled while the initial state has a single
unconnected, unlabelled input arrow.
A deterministic finite automaton (DFA) is a
NFA in which the transition relation ? is a
transition function, ? : Q ? ? ? Q. In
other words, there are no ?-transitions and if
(q1, t, q2) ? ?, t 6= ? and ?(q1, t, q?2) ? ? with
q?2 6= q2. Each NFA can be transformed by
the subset construction into an equivalent DFA.
Moreover, each DFA can be transformed by a
minimization algorithm into an equivalent DFA
which is minimal (i.e., there is no other equivalent
DFA with fewer states).
2.3 Directed acyclic graphs
Formally, a directed acyclic graph (DAG) D =
(Q,?, ?, q0, F ) is an NFA for which there exists
a strict order relation < on Q such that (p, t, q) ?
? ? p < q. Without loss of generality we may
assume that < is a total order.
Of course, as NFAs, DAGs can be transformed
into equivalent deterministic or minimal DAGs.
3 DAGs and PRCGs
A DAG D is recognized (accepted) by a PRCG
G if and only if L(D) ? L(G) 6= ?. A trivial
way to solve this recognition (or parsing) problem
is to extract the complete paths of L(D) (which
are in finite number) one by one and to parse
each such string with a standard PRCG parser, the
(complete) parse forest for D being the union of
each individual forest.4 However since DAGs may
define an exponential number of strings w.r.t. its
own size,5 the previous operation would take an
exponential time in the size of D, and the parse
forest would also have an exponential size.
The purpose of this paper is to show that
it is possible to directly parse a DAG (without
any unfolding) by sharing identical computations.
This sharing may lead to a polynomial parse time
for an exponential number of sentences, but, in
some cases, the parse time remains exponential.
3.1 DAGs and Ranges
In many NLP applications the source text cannot
be considered as a sequence of terminal symbols,
but rather as a finite set of finite strings. As
4These forests do not share any production (instantiated
clause) since ranges in a particular forest are all related
to the corresponding source string w (i.e., are all of the
form ?i..j?w). To be more precise the union operation on
individual forests must be completed in adding productions
which connect the new (super) axiom (say S?) with each root
and which are, for each w of the form S? ? S(?0..|w|?w).
5For example the language (a|b)n, n > 0 which contains
2n strings can be defined by a minimal DAG whose size is
n + 1.
256
mentioned in th introduction, this non-unique
string could be used to encode not-yet-solved
ambiguities in the input. DAGs are a convenient
way to represent these finite sets of strings by
factorizing their common parts (thanks to the
minimization algorithm).
In order to use DAGs as inputs for PRCG
parsing we will perform two generalizations.
The first one follows. Let w = t1 . . . tn be a
string in some alphabet ? and let Q = {qi | 0 ?
i ? n} be a set of n + 1 bounds with a total order
relation <, we have q0 < q1 < . . . < qn. The
sequence ? = q0t1q1t2q2 . . . tnqn ? Q?(??Q)n
is called a bounded string which spells w. A range
is a pair of bounds (qi, qj) with qi < qj noted
?pi..pj?pi and any triple of the form (qi?1tiqi)
is called a transition. All the notions around
PRCGs defined in Section 2.1 easily generalize
from strings to bounded strings. It is also the case
for the standard parsing algorithm of (Boullier,
2004).
Now the next step is to move from bounded
strings to DAGs. Let D = (Q,?, ?, q0, F ) be a
DAG. A string x ? ?? s.t. we have (q1, x, q2) ?
?? is called a path between q1 and q2 and a string
? = qt1q1 . . . tpqp ? Q ? (? ? {?} ? Q)? is a
bounded path and we say that ? spells t1t2 . . . tp.
A path x from q0 to f ? F is a complete path
and a bounded path of the form q0t1 . . . tnf with
f ? F is a complete bounded path. In the
context of a DAG D, a range is a pair of states
(qi, qj) with qi < qj noted ?qi..qj?D. A range
?qi..qj?D is valid if and only if there exists a
path from qi to qj in D. Of course, any range
?p..q?D defines its associated sub-DAG D?p..q? =
(Q?p..q?,??p..q?, ??p..q?, p, {q}) as follows. Its
transition relation is ??p..q? = {(r, t, s) | (r, t, s) ?
? ? (p, x?, r), (s, x??, q) ? ??}. If ??p..q? = ?
(i.e., there is no path between p and q), D?p..q? is
the empty DAG, otherwise Q?p..q? (resp. ??p..q?)
are the states (resp. terminal symbols) of the
transitions of ??p..q?. With this new definition of
ranges, the notions of instantiation and derivation
easily generalize from bounded strings to DAGs.
The language of a PRCG G for a DAG
D is defined by
?
L (G,D) = ?f?F {x |
S(?q0..f?D) +?G,D ?}. Let x ? L(D), it is not very
difficult to show that if x ? L(G) then we have
x ?
?
L (G,D). However, the converse is not true
(see Example 1), a sentence of L(D)? ?L (G,D)
may not be in L(G). To put it differently, if we
use the standard RCG parser, with the ranges of
a DAG, we produce the shared parse-forest for
the language
?
L (G,D) which is a superset of
L(D) ? L(G).
However, if G is a simple PRCG, we have
the equality L(G) = ?D is a DAG
?
L (G,D).
Note that the subclass of simple PRCGs is of
importance since it is MCS and it is the one
equivalent to LCFRSs. The informal reason of
the equality is the following. If an instantiated
predicate Ai(~?i) succeeds in some RHS, this
means that each of its ranges ~?i[j] = ?k..l?D has
been recognized as being a component of Ai, more
precisely their exists a path from k to l in D which
is a component of Ai. The range ?k..l?D selects
in D a set ??k..l?D of transitions (the transitions
used in the bounded paths from k to l). Because
of the linearity of G, there is no other range in that
RHS which selects a transition in ??k..l?D . Thus
the bounded paths selected by all the ranges of that
RHS are disjoints. In other words, any occurrence
of a valid instantiated range ?i..j?D selects a set of
paths which is a subset of L(D?i..j?).
Now, if we consider a non-linear PRCG, in
some of its clauses, there is a variable, say X,
which has several occurrences in its RHS (if we
consider a top-down non-linearity). Now assume
that for some input DAG D, an instantiation of
that clause is a component of some complete
derivation. Let ?p..q?D be the instantiation of X
in that instantiated clause. The fact that a predicate
in which X occurs succeeds means that there exist
paths from p to q in D?p..q?. The same thing stands
for all the other occurrences of X but nothing
force these paths to be identical or not.
Example 1.
Let us take an example which will be used
throughout the paper. It is a non-linear 1-PRCG
which defines the language anbncn, n ? 0 as
the intersection of the two languages a?bncn and
anbnc?. Each of these languages is respectively
defined by the predicates a?bncn and anbnc?; the
start predicate is anbncn.
257
1
2
3
4
a b
b c
Figure 1: Input DAG associated with ab|bc.
anbncn(X) ? a?bncn(X) anbnc?(X)
a?bncn(aX) ? a?bncn(X)
a?bncn(X) ? bncn(X)
bncn(bXc) ? bncn(X)
bncn(?) ? ?
anbnc?(Xc) ? anbnc?(X)
anbnc?(X) ? anbn(X)
anbn(aXb) ? anbn(X)
anbn(?) ? ?
If we use this PRCG to parse the DAG of
Figure 1 which defines the language {ab, bc},
we (erroneously) get the non-empty parse for-
est of Figure 2 though neither ab nor bc is in
anbncn.6 It is not difficult to see that the problem
comes from the non-linear instantiated variable
X?1..4? in the start node, and more precisely from
the actual (wrong) meaning of the three differ-
ent occurrences of X?1..4? in anbncn(X?1..4?) ?
a?bncn(X?1..4?) anbnc?(X?1..4?). The first occur-
rence in its RHS says that there exists a path in
the input DAG from state 1 to state 4 which is an
a?bncn. The second occurrence says that there
exists a path from state 1 to state 4 which is an
anbnc?. While the LHS occurrence (wrongly) says
that there exists a path from state 1 to state 4 which
is an anbncn. However, if the two X?1..4??s in the
RHS had selected common paths (this is not pos-
sible here) between 1 and 4, a valid interpretation
could have been proposed.
With this example, we see that the difficulty of
DAG parsing only arises with non-linear PRCGs.
If we consider linear PRCGs, the sub-class of
the PRCGs which is equivalent to LCFRSs, the
6In this forest oval nodes denote different instantiated
predicates, while its associated instantiated clauses are
presented as its daughter(s) and are denoted by square nodes.
The LHS of each instantiated clause shows the instantiation
of its LHS symbols. The RHS is the corresponding sequence
of instantiated predicates. The number of daughters of each
square node is the number of its RHS instantiated predicates.
standard algorithm works perfectly well with input
DAGs, since a valid instantiation of an argument
of a predicate in a clause by some range ?p..q?
means that there exists (at least) one path between
p and q which is recognized.
The paper will now concentrate on non-linear
PRCGs, and will present a new valid parsing
algorithm and study its complexities (in space and
time).
In order to simplify the presentation we
introduce this algorithm as a post-processing pass
which will work on the shared parse-forest output
by the (slightly modified) standard algorithm
which accepts DAGs as input.
3.2 Parsing DAGs with non-linear PRCGs
The standard parsing algorithm of (Boullier, 2004)
working on a string w can be sketched as follows.
It uses a single memoized boolean function
predicate(A, ~?) where A is a predicate and ~? is a
vector of ranges whose dimension is arity(A). The
initial call to that function has the form predicate
(S, ?0..|w|?). Its purpose is, for each A0-clause, to
instantiate each of its symbols in a consistant way.
For example if we assume that the ith argument of
the LHS of the current A0-clause is ??iXaY ???i and
that the ith component of ~?0 is the range ?pi..qi? an
instantiation of X, a an Y by the ranges ?pX ..qX?,
?pa..qa? and ?pY ..qY ? is such that we have pi ?
pX ? qX = pa < qa = pa + 1 = pY ? qY ? qi
and w = w?aw?? with |w?| = pa. Since the PRCG
is non bottom-up erasing, the instantiation of all
the LHS symbols implies that all the arguments
of the RHS predicates Ai are also instantiated and
gathered into the vector of ranges ~?i. Now, for
each i (1 ? i ? |RHS|), we can call predicate
(Ai, ~?i). If all these calls succeed, the instantiated
clause can be stored as a component of the shared
parse forest.7
In the case of a DAG D = (Q,?, ?, q0, F ) as
input, there are two slight modifications, the ini-
tial call is changed by the conjunctive call pred-
icate(S, ?q0..f1?) ? . . .? predicate (S, ?q0..f|F |?)
with fi ? F 8 and the terminal symbol a can be in-
stantiated by the range ?pa..qa?D only if (pa, a, qa)
7Note that such an instantiated clause could be
unreachable from the (future) instantiated start symbol which
will be the axiom of the shared forest considered as a CFG.
8Technically, each of these calls produces a forest. These
individual forests may share subparts but their roots are all
different. In order to have a true forest, we introduce a
new root, the super-root whose daughters are the individual
forests.
258
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
bncn?1..4?
bncn(b?1..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
bncn?3..3?
bncn(??3..3?) ? ?
anbn?2..2?
anbn(??2..2?) ? ?
Figure 2: Parse forest for the input DAG ab|bc.
is a transition in ?. The variable symbol X can
be instantiated by the range ?pX ..qX?D only if
?pX ..qX?D is valid.
3.3 Forest Filtering
We assume here that for a given PRCG G we
have built the parse forest of an input DAG D as
explained above and that each instantiated clause
of that forest contains the range ?pX ..qX?D of
each of its instantiated symbols X. We have seen
in Example 1 that this parse forest is valid if G is
linear but may well be unvalid if G is non-linear.
In that latter case, this happens because the range
?pX ..qX?D of each instantiation of the non-linear
variable X selects the whole sub-DAG D?pX ..qX?
while each instantiation should only select a sub-
language of L(D?pX ..qX?). For each occurrence of
X in the LHS or RHS of a non-linear clause, its
sub-languages could of course be different from
the others. In fact, we are interested in their
intersections: If their intersections are non empty,
this is the language which will be associated with
?pX ..qX?D, otherwise, if their intersections are
empty, then the instantiation of the considered
clause fails and must thus be removed from the
forest. Of course, we will consider that the
language (a finite number of strings) associated
with each occurrence of each instantiated symbol
is represented by a DAG.
The idea of the forest filtering algorithm
is to first compute the DAGs associated with
each argument of each instantiated predicate
during a bottom-up walk. These DAGs are
called decorations. This processing will perform
DAG compositions (including intersections, as
suggested above), and will erase clauses in which
empty intersections occur. If the DAG associated
with the single argument of the super-root is
empty, then parsing failed.
Otherwise, a top-down walk is launched
(see below), which may also erase non-valid
instantiated clauses. If necessary, the algorithm
is completed by a classical CFG algorithm which
erase non productive and unreachable symbols
leaving a reduced grammar/forest.
In order to simplify our presentation we will
assume that the PRCGs are non-combinatorial
and bottom-up non-erasing. However, we
can note that the following algorithm can be
generalized in order to handle combinatorial
PRCGs and in particular with overlapping
arguments.9 Moreover, we will assume that the
forest is non cyclic (or equivalently that all cycles
have previously been removed).10
9For example the non-linear combinatorial clause
A(XY Z) ? B(XY ) B(Y Z) has overlapping arguments.
10By a classical algorithm from the CFG technology.
259
3.3.1 The Bottom-Up Walk
For this principle algorithm, we assume that for
each instantiated clause in the forest, a DAG
will be associated with each occurrence of each
instantiated symbol. More precisely, for a given
instantiated A0-clause, the DAGs associated with
the RHS symbol occurrences are composed (see
below) to build up DAGs which will be associated
with each argument of its LHS predicate. For each
LHS argument, this composition is directed by the
sequence of symbols in the argument itself.
The forest is walked bottom-up starting from its
leaves. The constraint being that an instantiated
clause is visited if and only if all its RHS
instantiated predicates have already all been
visited (computed). This constraint can be
satisfied for any non-cyclic forest.
To be more precise, consider an instantiation
c? = A0(~?0) ? A1(~?1) . . . Ap( ~?p) of the clause
c = A0( ~?0) ? A1( ~?1) . . . Am( ~?m), we perform
the following sequence:
1. If the clause is not top-down linear (i.e.,
there exist multiple occurrences of the same
variables in its RHS arguments), for such
variable X let the range ?pX ..qX? be its
instantiation (by definition, all occurrences
are instantiated by the same range), we
perform the intersection of the DAGs
associated with each instantiated predicate
argument X. If one intersection results in
an empty DAG, the instantiated clause is
removed from the forest. Otherwise, we
perform the following steps.
2. If a RHS variable Y is linear, it occurs once in
the jth argument of predicate Ai. We perform
a brand new copy of the DAG associated with
the jth argument of the instantiation of Ai.
3. At that moment, all instantiated variables
which occur in c? are associated with a DAG.
For each occurrence of a terminal symbol t
in the LHS arguments we associate a (new)
DAG whose only transition is (p, t, q) where
p and q are brand new states with, of course,
p < q.
4. Here, all symbols (terminals or variables) are
associated with disjoints DAGs. For each
LHS argument ~?0[i] = Xi1 . . . Xij . . . Xipi ,
we associate a new DAG which is the
concatenation of the DAGs associated with
the symbols Xi1, . . . , Xij , . . . and Xipi .
5. Here each LHS argument of c? is associated
with a non empty DAG, we then report
the individual contribution of c? into the
(already computed) DAGs associated with
the arguments of its LHS A0(~?0). The DAG
associated with the ith argument of A0(~?0) is
the union (or a copy if it is the first time) of its
previous DAG value with the DAG associated
with the ith argument of the LHS of c?.
This bottom-up walk ends on the super-root with a
final decoration say R. In fact, during this bottom-
up walk, we have computed the intersection of the
languages defined by the input DAG and by the
PRCG (i.e., we have L(R) = L(D) ? L(G)).
Example 2.
1 2 3 4a
b
b c
b
Figure 3: Input DAG associated with abc|ab|bc.
With the PRCG of Example 1 and the input
DAG of Figure 3, we get the parse forest of
Figure 4 whose transitions are decorated by the
DAGs computed by the bottom-up algorithm.11
The crucial point to note here is the intersection
which
is performed between {abc, bc} and {abc, ab} on
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4? . The
non-empty set {abc} is the final result assigned to
the instantiated start symbol. Since this result is
non empty, it shows that the input DAG D is rec-
ognized by G. More precisely, this shows that the
sub-language of D which is recognized by G is
{abc}.
However, as shown in the previous example, the
(undecorated) parse forest is not the forest built
for the DAG L(D) ? L(G) since it may contain
non-valid parts (e.g., the transitions labelled {bc}
or {ab} in our example). In order to get the
11For readability reasons these DAGs are represented by
their languages (i.e., set of strings). Bottom-up transitions
from instantiated clauses to instantiated predicates reflects
the computations performed by that instantiated clause
while bottom-up transitions from instantiated predicates to
instantiated clauses are the union of the DAGs entering that
instantiated predicate.
260
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
a?bncn(a?1..2? X?2..4?) ? a?bncn?2..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
anbnc?(X?1..3? c?3..4? ) ? anbnc??1..3?
bncn?1..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
a?bncn?2..4?
a?bncn(X?2..4?) ? bncn?2..4?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
anbnc??1..3?
anbnc?(X?1..3?) ? anbn?1..3?
bncn?3..3?
bncn(??3..3?) ? ?
bncn?2..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?2..2?
anbn(??2..2?) ? ?
anbn?1..3?
anbn(a?1..2? X?2..2? b?2..3?) ? anbn?2..2?
{abc}
{abc, bc} {abc, ab}
{bc}
{abc}
{ab}
{abc}
{bc} {abc} {ab} {abc}
{bc} {bc} {ab} {ab}
{?}
{bc}
{?}
{ab}
{?}
{bc}
{?}
{ab}
{?} {?}
Figure 4: Bottom-up decorated parse forest for the input DAG abc|ab|bc.
261
right forest (i.e., to get a PRCG parser ? not
a recognizer ? which accepts a DAG as input)
we need to perform another walk on the previous
decorated forest.
3.3.2 The Top-Down Walk
The idea of the top-down walk on the parse
forest decorated by the bottom-up walk is to
(re)compute all the previous decorations starting
from the bottom-up decoration associated with
the instantiated start predicate. It is to be noted
that (the language defined by) each top-down
decoration is a subset of its bottom-up counterpart.
However, when a top-down decoration becomes
empty, the corresponding subtree must be erased
from the forest. If the bottom-up walk succeeds,
we are sure that the top-down walk will not
result in an empty forest. Moreover, if we
perform a new bottom-up walk on this reduced
forest, the new bottom-up decorations will denote
the same language as their top-down decorations
counterpart.
The forest is walked top-down starting from
the super-root. The constraint being that an
instantiated A(~?)-clause is visited if and only if all
the occurrences of A(~?) occurring in the RHS of
instantiated clauses have all already been visited.
This constraint can be satisfied for any non-cyclic
forest.
Initially, we assume that each argument of each
instantiated predicate has an empty decoration,
except for the argument of the super-root which is
decorated by the DAG R computed by the bottom-
up pass.
Now, assume that a top-down decoration has
been (fully) computed for each argument of
the instantiated predicate A0(~?0). For each
instantiated clause of the form c? = A0(~?0) ?
A1(~?1) . . . Ai(~?i) . . . Am( ~?m), we perform the
following sequence:12
1. We perform the intersection of the top-down
decoration of each argument of A0(~?0) with
the decoration computed by the bottom-up
pass for the same argument of the LHS
predicate of c?. If the result is empty, c? is
erased from the forest.
2. For each LHS argument, the previous results
are dispatched over the symbols of this
12The decoration of each argument of Ai(~?i) is either
initially empty or has already been partially computed.
argument.13 Thus, each instantiated LHS
symbol occurrence is decorated by its own
DAG. If the considered clause has several
occurrences of the same variable in the LHS
arguments (i.e., is bottom-up non-linear),
we perform the intersection of these DAGs
in order to leave a single decoration per
instantiated variable. If an intersection results
in an empty DAG, the current clause is erased
from the forest.
3. The LHS instantiated variable decorations
are propagated to the RHS arguments. This
propagation may result in DAG concatena-
tions when a RHS argument is made up of
several variables (i.e., is combinatorial).
4. At last, we associate to each argument
of Ai(~?i) a new decoration which is
computed as the union of its previous top-
down decoration with the decoration just
computed.
Example 3. When we apply the previous al-
gorithm to the bottom-up parse forest of Exam-
ple 2, we get the top-down parse forest of Fig-
ure 5. In this parse forest, erased parts are
laid out in light gray. The more noticable points
w.r.t. the bottom-up forest are the decorations be-
tween anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
and its RHS predicates a?bncn?1..4? and
anbnc??1..4? which are changed both to {abc}
instead of {abc, bc} and {abc, ab}. These two
changes induce the indicated erasings.
13Assume that ~?0[k] = ?p..q?D, that the decoration DAG
associated with the kth argument of A0( ~?0) is D??p..q? =
(Q??p..q?,??p..q?, ???p..q?, p?, F ??p..q?) (we have L(D??p..q?) ?
L(D?p..q?)) and that ~?0[k] = ?1kX?2k and that ?i..j?D is the
instantiation of the symbol X in c?. Our goal is to extract
from D??p..q? the decoration DAG D??i..j? associated with
that instantiated occurrence of X. This computation can be
helped if we maintain, associated with each decoration DAG
a function, say d, which maps each state of the decoration
DAG to a set of states (bounds) of the input DAG D. If, as we
have assumed, D is minimal, each set of states is a singleton,
we can write d(p?) = p, d(f ?) = q for all f ? ? F ??p..q?
and more generally d(i?) ? Q if i? ? Q?. Let I ? = {i? |
i? ? Q??p..q? ? d(i?) = i} and J ? = {j? | j? ? Q??p..q? ?
d(j?) = j}. The decoration DAG D??i..j? is such that
L(D??i..j?) =
S
i??I?,j??J?{x | x is a path from i? to j?}.
Of course, together with the construction of D??i..j? , its
associated function d must also be built.
262
anbncn?1..4?
anbncn(X?1..4?) ? a?bncn?1..4? anbnc??1..4?
a?bncn?1..4?
a?bncn(X?1..4?) ? bncn?1..4?
a?bncn(a?1..2? X?2..4?) ? a?bncn?2..4?
anbnc??1..4?
anbnc?(X?1..4?) ? anbn?1..4?
anbnc?(X?1..3? c?3..4? ) ? anbnc??1..3?
bncn?1..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
a?bncn?2..4?
a?bncn(X?2..4?) ? bncn?2..4?
anbn?1..4?
anbn(a?1..2? X?2..2? b?2..4?) ? anbn?2..2?
anbnc??1..3?
anbnc?(X?1..3?) ? anbn?1..3?
bncn?3..3?
bncn(??3..3?) ? ?
bncn?2..4?
bncn(b?2..3? X?3..3? c?3..4?) ? bncn?3..3?
anbn?2..2?
anbn(??2..2?) ? ?
anbn?1..3?
anbn(a?1..2? X?2..2? b?2..3?) ? anbn?2..2?
{abc}
{abc} {abc}
?
{abc}
?
{abc}
? {abc} ? {abc}
? {bc} ? {ab}
?
{bc}
?
{ab}
{?}
{bc}
{?}
{ab}
{?} {?}
Figure 5: Top-down decorated parse forest for the input DAG abc|ab|bc.
263
3.4 Time and Space Complexities
In this Section we study the time and size
complexities of the forest filtering algorithm.
Let us consider the sub-DAG D?p..q? of the
minimal input DAG D and consider any (finite)
regular language L ? L(D?p..q?), and let DL be
the minimal DAG s.t. L(DL) = L. We show, on
an example, that |DL| may be an exponential w.r.t.
|D?p..q?|.
Consider, for a given h > 0, the language
(a|b)h. We know that this language can be
represented by the minimal DAG with h+1 states
of Figure 6.
Assume that h = 2k and consider the
sub-language L2k of (a|b)2k (nested well-
parenthesized strings) which is defined by
1. L2 = {aa, bb} ;
2. k > 1, L2k = {axa, bxb | x ? L2k?2},
It is not difficult to see that the DAG in Figure 7
defines L2k and is minimal, but its size 2k+2 ? 2
is an exponential in the size 2k+1 of the minimal
DAG for the language (a|b)2k .
This results shows that, there exist cases in
which some minimal DAGs D? that define sub-
languages of minimal DAGs D may have a
exponential size (i.e., |D?| = O(2|D|). In other
words, when, during the bottom-up or top-down
walk, we compute union of DAGs, we may fall
on these pathologic DAGs that will induce a
combinatorial explosion in both time and space.
3.5 Implementation Issues
Of course, many improvements may be brought
to the previous principle algorithms in practical
implementations. Let us cite two of them. First it
is possible to restrict the number of DAG copies:
a DAG copy is not useful if it is the last reference
to that DAG.
We shall here devel the second point on a little
more: if an argument of a predicate is never
used in ant non-linearity, it is only a waste of
time to compute its decoration. We say that Ak,
the kth argument of the predicate A is a non-
linear predicate argument if there exists a clause
c in which A occurs in the RHS and whose
kth argument has at least one common variable
another argument Bh of some predicate B of
the RHS (if B = A, then of course k and h
must be different). It is clear that Bh is then
non-linear as well. It is not difficult to see that
decorations needs only to be computed if they are
associated with a non-linear predicate argument. It
is possible to compute those non-linear predicate
arguments statically (when building the parser)
when the PRCG is defined within a single module.
However, if the PRCG is given in several modules,
this full static computation is no longer possible.
The non-linear predicate arguments must thus
be identified at parse time, when the whole
grammar is available. This rather trivial algorithm
will not be described here, but it should be
noted that it is worth doing since in practice it
prevents decoration computations which can take
an exponential time.
4 Conclusion
In this paper we have shown how PRCGs can
handle DAGs as an input. If we consider the linear
PRCG, the one equivalent to LCFRS, the parsing
time remains polynomial. Moreover, input DAGs
necessitate only rather cosmetic modifications in
the standard parser.
In the non-linear case, the standard parser may
produce illegal parses in its output shared parse
forest. It may even produce a (non-empty) shared
parse forest though no sentences of the input DAG
are in the language defined by our non-linear
PRCG. We have proposed a method which uses
the (slightly modified) standard parser but prunes,
within extra passes, its output forest and leaves all
and only valid parses. During these extra bottom-
up and top-down walks, this pruning involves
the computation of finite languages by means of
concatenation, union and intersection operations.
The sentences of these finite languages are always
substrings of the words of the input DAG D.
We choose to represent these intermediate finite
languages by DAGs instead of sets of strings
because the size of a DAG is, at worst, of the same
order as the size of a set of strings but it could, in
some cases, be exponentially smaller.
However, the time taken by this extra pruning
pass cannot be guaranteed to be polynomial,
as expected from previously known complexity
results (Bertsch and Nederhof, 2001). We have
shown an example in which pruning takes an
exponential time and space in the size of D. The
deep reason comes from the fact that if L is a
finite (regular) language defined by some minimal
DAG D, there are cases where a sub-language of
264
0 1 2 h? 1 h
a
b
a
b
a
b
Figure 6: Input DAG associated with the language (a|b)h, h > 0.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
.
.
.
.
.
.
.
.
.
.
.
.
2k+2 ? 4
2k+2 ? 3
2k+2 ? 2
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
a
b
Figure 7: DAG associated with the language of nested well-parenthesized strings of length 2k.
L may require to be defined by a DAG whose size
is an exponential in the size of D. Of course this
combinatorial explosion is not a fatality, and we
may wonder whether, in the particular case of NLP
it will practically occur?
References
Franois Barthe?lemy, Pierre Boullier, Philippe De-
schamp, and ?Eric de la Clergerie. 2001. Guided
parsing of range concatenation languages. In Pro-
ceedings of the 39th Annual Meeting of the Associ-
ation for Comput. Linguist. (ACL?01), pages 42?49,
University of Toulouse, France.
Eberhard Bertsch and Mark-Jan Nederhof. 2001. On
the complexity of some extensions of rcg parsing. In
Proceedings of IWPT?01, Beijing, China.
Pierre Boullier, 2004. New Developments in Pars-
ing Technology, volume 23 of Text, Speech and
Language Technology, chapter Range Concatena-
tion Grammars, pages 269?289. Kluwer Academic
Publishers, H. Bunt, J. Carroll, and G. Satta edition.
Jeffrey D. Hopcroft and John E. Ullman. 1979.
Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley, Reading, Mass.
Bernard Lang. 1994. Recognition can be harder than
parsing. Computational Intelligence, 10(4):486?
494.
K. Vijay-Shanker, David Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proceedings of the 25th Meeting of the Association
for Comput. Linguist. (ACL?87), pages 104?111,
Stanford University, CA.
265
