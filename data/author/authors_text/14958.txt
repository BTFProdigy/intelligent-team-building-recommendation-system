Attention Shifting for Parsing Speech ?
Keith Hall
Department of Computer Science
Brown University
Providence, RI 02912
kh@cs.brown.edu
Mark Johnson
Department of Cognitive and Linguistic Science
Brown University
Providence, RI 02912
Mark Johnson@Brown.edu
Abstract
We present a technique that improves the efficiency
of word-lattice parsing as used in speech recogni-
tion language modeling. Our technique applies a
probabilistic parser iteratively where on each iter-
ation it focuses on a different subset of the word-
lattice. The parser?s attention is shifted towards
word-lattice subsets for which there are few or no
syntactic analyses posited. This attention-shifting
technique provides a six-times increase in speed
(measured as the number of parser analyses evalu-
ated) while performing equivalently when used as
the first-stage of a multi-stage parsing-based lan-
guage model.
1 Introduction
Success in language modeling has been dominated
by the linear n-gram for the past few decades. A
number of syntactic language models have proven
to be competitive with the n-gram and better than
the most popular n-gram, the trigram (Roark, 2001;
Xu et al, 2002; Charniak, 2001; Hall and Johnson,
2003). Language modeling for speech could well be
the first real problem for which syntactic techniques
are useful.
John ate the pizza on a plate with a fork .
NP:plate NP:fork
PP:withPP:on
IN INVB NP
VP:ate
Figure 1: An incomplete parse tree with head-word an-
notations.
One reason that we expect syntactic models to
perform well is that they are capable of model-
ing long-distance dependencies that simple n-gram
? This research was supported in part by NSF grants 9870676
and 0085940.
models cannot. For example, the model presented
by Chelba and Jelinek (Chelba and Jelinek, 1998;
Xu et al, 2002) uses syntactic structure to identify
lexical items in the left-context which are then mod-
eled as an n-gram process. The model presented
by Charniak (Charniak, 2001) identifies both syn-
tactic structural and lexical dependencies that aid in
language modeling. While there are n-gram mod-
els that attempt to extend the left-context window
through the use of caching and skip models (Good-
man, 2001), we believe that linguistically motivated
models, such as these lexical-syntactic models, are
more robust.
Figure 1 presents a simple example to illustrate
the nature of long-distance dependencies. Using
a syntactic model such as the the Structured Lan-
guage Model (Chelba and Jelinek, 1998), we pre-
dict the word fork given the context {ate, with}
where a trigram model uses the context {with, a}.
Consider the problem of disambiguating between
. . . plate with a fork and . . . plate with effort. The
syntactic model captures the semantic relationship
between the words ate and fork. The syntactic struc-
ture allows us to find lexical contexts for which
there is some semantic relationship (e.g., predicate-
argument).
Unfortunately, syntactic language modeling tech-
niques have proven to be extremely expensive in
terms of computational effort. Many employ the
use of string parsers; in order to utilize such tech-
niques for language modeling one must preselect a
set of strings from the word-lattice and parse each of
them separately, an inherently inefficient procedure.
Of the techniques that can process word-lattices di-
rectly, it takes significant computation to achieve
the same levels of accuracy as the n?best rerank-
ing method. This computational cost is the result of
increasing the search space evaluated with the syn-
tactic model (parser); the larger space resulting from
combining the search for syntactic structure with the
search for paths in the word-lattice.
In this paper we propose a variation of a proba-
bilistic word-lattice parsing technique that increases
0 1
yesterday/0
2and/4.004
3
in/14.73
4tuesday/0
14
tuesday/0 5
to/0.000
6
two/8.769
7it/51.59
to/0
8
outlaw/83.57
9
outline/2.573
10
outlined/12.58
outlines/10.71
outline/0
outlined/8.027
outlines/7.14013
to/0
in/0
of/115.4
a/71.30
the/115.3 11
strategy/0
strategy/0
outline/0
12/0
</s>/0
Figure 2: A partial word-lattice from the NIST HUB-1 dataset.
efficiency while incurring no loss of language mod-
eling performance (measured as Word Error Rate ?
WER). In (Hall and Johnson, 2003) we presented
a modular lattice parsing process that operates in
two stages. The first stage is a PCFG word-lattice
parser that generates a set of candidate parses over
strings in a word-lattice, while the second stage
rescores these candidate edges using a lexicalized
syntactic language model (Charniak, 2001). Under
this paradigm, the first stage is not only responsible
for selecting candidate parses, but also for selecting
paths in the word-lattice. Due to computational and
memory requirements of the lexicalized model, the
second stage parser is capable of rescoring only a
small subset of all parser analyses. For this reason,
the PCFG prunes the set of parser analyses, thereby
indirectly pruning paths in the word lattice.
We propose adding a meta-process to the first-
stage that effectively shifts the selection of word-
lattice paths to the second stage (where lexical in-
formation is available). We achieve this by ensuring
that for each path in the word-lattice the first-stage
parser posits at least one parse.
2 Parsing speech word-lattices
P (A,W ) = P (A|W )P (W ) (1)
The noisy channel model for speech is presented in
Equation 1, where A represents the acoustic data ex-
tracted from a speech signal, and W represents a
word string. The acoustic model P (A|W ) assigns
probability mass to the acoustic data given a word
string and the language model P (W ) defines a dis-
tribution over word strings. Typically the acoustic
model is broken into a series of distributions condi-
tioned on individual words (though these are based
on false independence assumptions).
P (A|w
1
. . . w
i
. . . w
n
) =
n
?
i=1
P (A|w
i
) (2)
The result of the acoustic modeling process is a set
of string hypotheses; each word of each hypothesis
is assigned a probability by the acoustic model.
Word-lattices are a compact representation of
output of the acoustic recognizer; an example is pre-
sented in Figure 2. The word-lattice is a weighted
directed acyclic graph where a path in the graph cor-
responds to a string predicted by the acoustic recog-
nizer. The (sum) product of the (log) weights on the
graph (the acoustic probabilities) is the probability
of the acoustic data given the string. Typically we
want to know the most likely string given the acous-
tic data.
arg maxP (W |A) (3)
= arg max P (A,W )
= arg max P (A|W )P (W )
In Equation 3 we use Bayes? rule to find the opti-
mal string given P (A|W ), the acoustic model, and
P (W ), the language model. Although the language
model can be used to rescore1 the word-lattice, it is
typically used to select a single hypothesis.
We focus our attention in this paper to syntactic
language modeling techniques that perform com-
plete parsing, meaning that parse trees are built
upon the strings in the word-lattice.
2.1 n?best list reranking
Much effort has been put forth in developing effi-
cient probabilistic models for parsing strings (Cara-
ballo and Charniak, 1998; Goldwater et al, 1998;
Blaheta and Charniak, 1999; Charniak, 2000; Char-
niak, 2001); an obvious solution to parsing word-
lattices is to use n?best list reranking. The n?best
list reranking procedure, depicted in Figure 3, uti-
lizes an external language model that selects a set
of strings from the word-lattice. These strings are
analyzed by the parser which computes a language
model probability. This probability is combined
1To rescore a word-lattice, each arch is assigned a new score
(probability) defined by a new model (in combination with the
acoustic model).
w1, ..., wi, ..., wn1
...
Language
Model
w1, ..., wi, ..., wn2
w1, ..., wi, ..., wn3
w1, ..., wi, ..., wn4
w1, ..., wi, ..., wnm
o1, ..., oi, ..., on
8
2
3
5
1 6
4
7 10
9
the/0
man/0
is/0
duh/1.385
man/0 is/0
surely/0
early/0
mans/1.385
man's/1.385
surly/0
surly/0.692
early/0
early/0 n-best 
list
extractor
Figure 3: n?best list reranking
with the acoustic model probability to reranked the
strings according to the joint probability P (A,W ).
There are two significant disadvantages to this ap-
proach. First, we are limited by the performance
of the language model used to select the n?best
lists. Usually, the trigram model is used to se-
lect n paths through the lattice generating at most
n unique strings. The maximum performance that
can be achieved is limited by the performance of
this extractor model. Second, of the strings that
are analyzed by the parser, many will share com-
mon substrings. Much of the work performed by
the parser is duplicated for these substrings. This
second point is the primary motivation behind pars-
ing word-lattices (Hall and Johnson, 2003).
2.2 Multi-stage parsing
?
PCFG Parser
?
?
? ?
Lexicalized 
Parser
Figure 4: Coarse-to-fine lattice parsing.
In Figure 4 we present the general overview of
a multi-stage parsing technique (Goodman, 1997;
Charniak, 2000; Charniak, 2001). This process
1. Parse word-lattice with PCFG parser
2. Overparse, generating additional candidates
3. Compute inside-outside probabilities
4. Prune candidates with probability threshold
Table 1: First stage word-lattice parser
is know as coarse-to-fine modeling, where coarse
models are more efficient but less accurate than
fine models, which are robust but computation-
ally expensive. In this particular parsing model a
PCFG best-first parser (Bobrow, 1990; Caraballo
and Charniak, 1998) is used to search the uncon-
strained space of parses ? over a string. This first
stage performs overparsing which effectively al-
lows it to generate a set of high probability candi-
date parses ??. These parses are then rescored us-
ing a lexicalized syntactic model (Charniak, 2001).
Although the coarse-to-fine model may include any
number of intermediary stages, in this paper we con-
sider this two-stage model.
There is no guarantee that parses favored by the
second stage will be generated by the first stage. In
other words, because the first stage model prunes
the space of parses from which the second stage
rescores, the first stage model may remove solutions
that the second stage would have assigned a high
probability.
In (Hall and Johnson, 2003), we extended the
multi-stage parsing model to work on word-lattices.
The first-stage parser, Table 1, is responsible for
positing a set of candidate parses over the word-
lattice. Were we to run the parser to completion it
would generate all parses for all strings described
by the word-lattice. As with string parsing, we stop
the first stage parser early, generating a subset of
all parses. Only the strings covered by complete
parses are passed on to the second stage parser. This
indirectly prunes the word-lattice of all word-arcs
that were not covered by complete parses in the first
stage.
We use a first stage PCFG parser that performs
a best-first search over the space of parses, which
means that it depends on a heuristic ?figure-of-
merit? (FOM) (Caraballo and Charniak, 1998). A
good FOM attempts to model the true probability
of a chart edge2 P (N i
j,k
). Generally, this proba-
bility is impossible to compute during the parsing
process as it requires knowing both the inside and
outside probabilities (Charniak, 1993; Manning and
Schu?tze, 1999). The FOM we describe is an ap-
proximation to the edge probability and is computed
using an estimate of the inside probability times an
approximation to the outside probability 3.
The inside probability ?(Ni
j,k
) can be computed
incrementally during bottom-up parsing. The nor-
malized acoustic probabilities from the acoustic rec-
ognizer are included in this calculation.
??(N i
j,k
) (4)
=
?
i,l,q,r
fwd(T q
i,j
)p(N i|T q)p(T
r
|N i)bkwd(T r
k,l
)
The outside probability is approximated with a
bitag model and the standard tag/category bound-
ary model (Caraballo and Charniak, 1998; Hall and
Johnson, 2003). Equation 4 presents the approx-
imation to the outside probability. Part-of-speech
tags T q and T r are the candidate tags to the left
and right of the constituent Ni
j,k
. The fwd() and
bkwd() functions are the HMM forward and back-
ward probabilities calculated over a lattice con-
taining the part-of-speech tag, the word, and the
acoustic scores from the word-lattice to the left and
right of the constituent, respectively. p(Ni|T q) and
p(T
r
|N i) are the boundary statistics which are esti-
mated from training data (details of this model can
be found in (Hall and Johnson, 2003)).
FOM(N i
j,k
) = ??(N i
j,k
)?(N i
j,k
)?C(j, k) (5)
The best-first search employed by the first stage
parser uses the FOM defined in Equation 5, where
? is a normalization factor based on path length
C(j, k). The normalization factor prevents small
constituents from consistently being assigned a
2A chart edge Ni
j,k
indicates a grammar category Ni can
be constructed from nodes j to k.
3An alternative to the inside and outside probabilities are
the Viterbi inside and outside probabilities (Goldwater et al,
1998; Hall and Johnson, 2003).
higher probability than larger constituents (Goldwa-
ter et al, 1998).
Although this heuristic works well for directing
the parser towards likely parses over a string, it
is not an ideal model for pruning the word-lattice.
First, the outside approximation of this FOM is
based on a linear part-of-speech tag model (the
bitag). Such a simple syntactic model is unlikely
to provide realistic information when choosing a
word-lattice path to consider. Second, the model is
prone to favoring subsets of the word-lattice caus-
ing it to posit additional parse trees for the favored
sublattice rather than exploring the remainder of the
word-lattice. This second point is the primary moti-
vation for the attention shifting technique presented
in the next section.
3 Attention shifting4
We explore a modification to the multi-stage parsing
algorithm that ensures the first stage parser posits
at least one parse for each path in the word-lattice.
The idea behind this is to intermittently shift the at-
tention of the parser to unexplored parts of the word
lattice.
Identify
Used Edges
Clear Agenda/
Add Edges for 
Unused Words
Is Agenda
Empty? no
Continue 
Multi-stage
Parsing
yes
PCFG
Word-lattice
Parser
Figure 5: Attention shifting parser.
Figure 5 depicts the attention shifting first stage
parsing procedure. A used edge is a parse edge that
has non-zero outside probability. By definition of
4The notion of attention shifting is motivated by the work on
parser FOM compensation presented in (Blaheta and Charniak,
1999).
the outside probability, used edges are constituents
that are part of a complete parse; a parse is com-
plete if there is a root category label (e.g., S for sen-
tence) that spans the entire word-lattice. In order to
identify used edges, we compute the outside prob-
abilities for each parse edge (efficiently computing
the outside probability of an edge requires that the
inside probabilities have already been computed).
In the third step of this algorithm we clear the
agenda, removing all partial analyses evaluated by
the parser. This forces the parser to abandon analy-
ses of parts of the word-lattice for which complete
parses exist. Following this, the agenda is popu-
lated with edges corresponding to the unused words,
priming the parser to consider these words. To en-
sure the parser builds upon at least one of these
unused edges, we further modify the parsing algo-
rithm:
? Only unused edges are added to the agenda.
? When building parses from the bottom up, a
parse is considered complete if it connects to a
used edge.
These modifications ensure that the parser focuses
on edges built upon the unused words. The sec-
ond modification ensures the parser is able to de-
termine when it has connected an unused word with
a previously completed parse. The application of
these constraints directs the attention of the parser
towards new edges that contribute to parse anal-
yses covering unused words. We are guaranteed
that each iteration of the attention shifting algorithm
adds a parse for at least one unused word, meaning
that it will take at most |A| iterations to cover the en-
tire lattice, where A is the set of word-lattice arcs.
This guarantee is trivially provided through the con-
straints just described. The attention-shifting parser
continues until there are no unused words remain-
ing and each parsing iteration runs until it has found
a complete parse using at least one of the unused
words.
As with multi-stage parsing, an adjustable param-
eter determines how much overparsing to perform
on the initial parse. In the attention shifting algo-
rithm an additional parameter specifies the amount
of overparsing for each iteration after the first. The
new parameter allows for independent control of the
attention shifting iterations.
After the attention shifting parser populates a
parse chart with parses covering all paths in the
lattice, the multi-stage parsing algorithm performs
additional pruning based on the probability of the
parse edges (the product of the inside and outside
probabilities). This is necessary in order to con-
strain the size of the hypothesis set passed on to the
second stage parsing model.
The Charniak lexicalized syntactic language
model effectively splits the number of parse states
(an edges in a PCFG parser) by the number of
unique contexts in which the state is found. These
contexts include syntactic structure such as parent
and grandparent category labels as well as lexical
items such as the head of the parent or the head of a
sibling constituent (Charniak, 2001). State splitting
on this level causes the memory requirement of the
lexicalized parser to grow rapidly.
Ideally, we would pass all edges on to the sec-
ond stage, but due to memory limitations, pruning
is necessary. It is likely that edges recently discov-
ered by the attention shifting procedure are pruned.
However, the true PCFG probability model is used
to prune these edges rather than the approximation
used in the FOM. We believe that by considering
parses which have a relatively high probability ac-
cording to the combined PCFG and acoustic models
that we will include most of the analyses for which
the lexicalized parser assigns a high probability.
4 Experiments
The purpose of attention shifting is to reduce the
amount of work exerted by the first stage PCFG
parser while maintaining the same quality of lan-
guage modeling (in the multi-stage system). We
have performed a set of experiments on the NIST
?93 HUB?1 word-lattices. The HUB?1 is a collec-
tion of 213 word-lattices resulting from an acoustic
recognizer?s analysis of speech utterances. Profes-
sional readers reading Wall Street Journal articles
generated the utterances.
The first stage parser is a best-first PCFG parser
trained on sections 2 through 22, and 24 of the Penn
WSJ treebank (Marcus et al, 1993). Prior to train-
ing, the treebank is transformed into speech-like
text, removing punctuation and expanding numer-
als, etc.5 Overparsing is performed using an edge
pop6 multiplicative factor. The parser records the
number of edge pops required to reach the first com-
plete parse. The parser continues to parse a until
multiple of the number of edge pops required for
the first parse are popped off the agenda.
The second stage parser used is a modified ver-
sion of the Charniak language modeling parser de-
scribed in (Charniak, 2001). We trained this parser
5Brian Roark of AT&T provided a tool to perform the
speech normalization.
6An edge pop is the process of the parser removing an edge
from the agenda and placing it in the parse chart.
on the BLLIP99 corpus (Charniak et al, 1999); a
corpus of 30million words automatically parsed us-
ing the Charniak parser (Charniak, 2000).
In order to compare the work done by the n?best
reranking technique to the word-lattice parser, we
generated a set of n?best lattices. 50?best lists were
extracted using the Chelba A* decoder7. A 50?
best lattice is a sublattice of the acoustic lattice that
generates only the strings found in the 50?best list.
Additionally, we provide the results for parsing the
full acoustic lattices (although these work measure-
ments should not be compared to those of n?best
reranking).
We report the amount of work, shown as the
cumulative # edge pops, the oracle WER for the
word-lattices after first stage pruning, and the WER
of the complete multi-stage parser. In all of the
word-lattice parsing experiments, we pruned the set
of posited hypothesis so that no more than 30,000
local-trees are generated8. We chose this thresh-
old due to the memory requirements of the sec-
ond stage parser. Performing pruning at the end of
the first stage prevents the attention shifting parser
from reaching the minimum oracle WER (most no-
table in the full acoustic word-lattice experiments).
While the attention-shifting algorithm ensures all
word-lattice arcs are included in complete parses,
forward-backward pruning, as used here, will elim-
inate some of these parses, indirectly eliminating
some of the word-lattice arcs.
To illustrate the need for pruning, we computed
the number of states used by the Charniak lexi-
calized syntactic language model for 30,000 local
trees. An average of 215 lexicalized states were
generated for each of the 30,000 local trees. This
means that the lexicalized language model, on av-
erage, computes probabilities for over 6.5 million
states when provided with 30,000 local trees.
Model # edge pops O-WER WER
n?best (Charniak) 2.5 million 7.75 11.8
100x LatParse 3.4 million 8.18 12.0
10x AttShift 564,895 7.78 11.9
Table 2: Results for n?best lists and n?best lattices.
Table 2 shows the results for n?best list rerank-
ing and word-lattice parsing of n?best lattices.
We recreated the results of the Charniak language
model parser used for reranking in order to measure
the amount of work required. We ran the first stage
parser with 4-times overparsing for each string in
7The n?best lists were provided by Brian Roark (Roark,
2001)
8A local-tree is an explicit expansion of an edge and its chil-
dren. An example local tree is NP
3,8
? DT
3,4
NN
4,8
.
the n?best list. The LatParse result represents run-
ning the word-lattice parser on the n?best lattices
performing 100?times overparsing in the first stage.
The AttShift model is the attention shifting parser
described in this paper. We used 10?times overpars-
ing for both the initial parse and each of the attention
shifting iterations. When run on the n?best lattice,
this model achieves a comparable WER, while re-
ducing the amount of parser work sixfold (as com-
pared to the regular word-lattice parser).
Model # edge pops O-WER WER
acoustic lats N/A 3.26 N/A
100x LatParse 3.4 million 5.45 13.1
10x AttShift 1.6 million 4.17 13.1
Table 3: Results for acoustic lattices.
In Table 3 we present the results of the word-
lattice parser and the attention shifting parser when
run on full acoustic lattices. While the oracle WER
is reduced, we are considering almost half as many
edges as the standard word-lattice parser. The in-
creased size of the acoustic lattices suggests that it
may not be computationally efficient to consider the
entire lattice and that an additional pruning phase is
necessary.
The most significant constraint of this multi-stage
lattice parsing technique is that the second stage
process has a large memory requirement. While the
attention shifting technique does allow the parser to
propose constituents for every path in the lattice, we
prune some of these constituents prior to performing
analysis by the second stage parser. Currently, prun-
ing is accomplished using the PCFG model. One
solution is to incorporate an intermediate pruning
stage (e.g., lexicalized PCFG) between the PCFG
parser and the full lexicalized model. Doing so will
relax the requirement for aggressive PCFG pruning
and allows for a lexicalized model to influence the
selection of word-lattice paths.
5 Conclusion
We presented a parsing technique that shifts the at-
tention of a word-lattice parser in order to ensure
syntactic analyses for all lattice paths. Attention
shifting can be thought of as a meta-process around
the first stage of a multi-stage word-lattice parser.
We show that this technique reduces the amount of
work exerted by the first stage PCFG parser while
maintaining comparable language modeling perfor-
mance.
Attention shifting is a simple technique that at-
tempts to make word-lattice parsing more efficient.
As suggested by the results for the acoustic lattice
experiments, this technique alone is not sufficient.
Solutions to improve these results include modify-
ing the first-stage grammar by annotating the cat-
egory labels with local syntactic features as sug-
gested in (Johnson, 1998) and (Klein and Manning,
2003) as well as incorporating some level of lexical-
ization. Improving the quality of the parses selected
by the first stage should reduce the need for gen-
erating such a large number of candidates prior to
pruning, improving efficiency as well as overall ac-
curacy. We believe that attention shifting, or some
variety of this technique, will be an integral part of
efficient solutions for word-lattice parsing.
References
Don Blaheta and Eugene Charniak. 1999. Au-
tomatic compensation for parser figure-of-merit
flaws. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics,
pages 513?518.
Robert J. Bobrow. 1990. Statistical agenda pars-
ing. In DARPA Speech and Language Workshop,
pages 222?224.
Sharon Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilis-
tic chart parsing. Computational Linguistics,
24(2):275?298, June.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith
Hall, John Hale, and Mark Johnson. 1999.
BLLIP 1987?89 wsj corpus release 1. LDC cor-
pus LDC2000T43.
Eugene Charniak. 1993. Statistical Language
Learning. MIT Press.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics., ACL,
New Brunswick, NJ.
Eugene Charniak. 2001. Immediate-head parsing
for language models. In Proceedings of the 39th
Annual Meeting of the Association for Computa-
tional Linguistics.
Ciprian Chelba and Frederick Jelinek. 1998. A
study on richer syntactic dependencies for struc-
tured language modeling. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and 17th International
Conference on Computational Linguistics, pages
225?231.
Sharon Goldwater, Eugene Charniak, and Mark
Johnson. 1998. Best-first edge-based chart pars-
ing. In 6th Annual Workshop for Very Large Cor-
pora, pages 127?133.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the Sec-
ond Conference on Empirical Methods in Natural
Language Processing, pages 11?25.
Joshua Goodman. 2001. A bit of progress in lan-
guage modeling, extendend version. In Microsoft
Research Technical Report MSR-TR-2001-72.
Keith Hall and Mark Johnson. 2003. Language
modeling using efficient best-first bottom-up
parsing. In Proceedings of IEEE Automated
Speech Recognition and Understanding Work-
shop.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24:617?636.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of
the 41st Meeting of the Association for Computa-
tional Linguistics (ACL-03).
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of statistical natural lan-
guage processing. MIT Press.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics, 19:313?330.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(3):249?276.
Peng Xu, Ciprian Chelba, and Frederick Jelinek.
2002. A study on richer syntactic dependencies
for structured language modeling. In Proceed-
ings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 191?
198.
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 962?966,
Prague, June 2007. c?2007 Association for Computational Linguistics
Log-linear Models of Non-projective Trees, k-best MST Parsing and
Tree-ranking
Keith Hall1 and Jir??? Havelka2 and David A. Smith1
1Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD USA
keith hall@jhu.edu
dasmith@cs.jhu.edu
2Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
havelka@ufal.mff.cuni.cz
Abstract
We present our system used in the CoNLL
2007 shared task on multilingual parsing.
The system is composed of three compo-
nents: a k-best maximum spanning tree
(MST) parser, a tree labeler, and a reranker
that orders the k-best labeled trees. We
present two techniques for training the
MST parser: tree-normalized and graph-
normalized conditional training. The tree-
based reranking model allows us to explic-
itly model global syntactic phenomena. We
describe the reranker features which include
non-projective edge attributes. We provide
an analysis of the errors made by our system
and suggest changes to the models and fea-
tures that might rectify the current system.
1 Introduction
Reranking the output of a k-best parser has been
shown to improve upon the best results of a state-
of-the-art constituency parser (Charniak and John-
son, 2005). This is primarily due to the ability to
incorporate complex structural features that cannot
be modeled under a CFG. Recent work shows that
k-best maximum spanning tree (MST) parsing and
reranking is also viable (Hall, 2007). In the current
work, we explore the k-best MST parsing paradigm
along with a tree-based reranker. A system using
the parsing techniques presented in this paper was
entered in the CoNLL 2007 shared task competi-
tion (Nivre et al, 2007). This task evaluated pars-
ing performance on 10 languages: Arabic, Basque,
Catalan, Chinese, Czech, English, Greek, Hungar-
ian, Italian, and Turkish using data originating from
a wide variety of dependency treebanks, and trans-
formations of constituency-based treebanks (Hajic?
et al, 2004; Aduriz et al, 2003; Mart?? et al, 2007;
Chen et al, 2003; Bo?hmova? et al, 2003; Marcus et
al., 1993; Johansson and Nugues, 2007; Prokopidis
et al, 2005; Csendes et al, 2005; Montemagni et al,
2003; Oflazer et al, 2003).
We show that oracle parse accuracy1 of the out-
put of our k-best parser is generally higher than the
best reported results. We also present the results
of a reranker based on a rich set of structural fea-
tures, including features explicitly targeted at mod-
eling non-projective configurations. Labeling of the
dependency edges is accomplished by an edge la-
beler based on the same feature set as used in train-
ing the k-best MST parser.
2 Parser Description
Our parser is composed of three components: a k-
best MST parser, a tree-labeler, and a tree-reranker.
Log-linear models are used for each of the com-
ponents independently. In this section we give an
overview of the models, the training techniques, and
the decoders.
2.1 MST Parsing, Reranking, and Labeling
The connection between the maximum spanning
tree problem and dependency parsing stems from
the observation that a dependency parse is simply an
oriented spanning tree on the graph of all possible
1The oracle accuracy for a set of hypotheses is the maximal
accuracy for any of the hypotheses.
962
dependency links (the fully connected dependency
graph). Unfortunately, by mapping the problem to
a graph, we assume that the scores associated with
edges are independent, and thus, are limited to edge-
factored models.
Edge-factored models are severely limited in their
capacity to predict structure. In fact, they can only
directly model parent-child links. In order to allevi-
ate this, we use a k-best MST parser to generate a
set of candidate hypotheses. Then, we rerank these
trees using a model based on rich structural features
that model features such as valency, subcategoriza-
tion, ancestry relationships, and sibling interactions,
as well as features capturing the global structure of
dependency trees, aimed primarily at modeling lan-
guage specific non-projective configurations.
We assign dependency labels to entire trees, rather
than predicting the labels during tree construction.
Given that we have a reranking process, we can la-
bel the k-best tree hypotheses output from our MST
parser, and rerank the labeled trees. We have ex-
plored both labeled and unlabeled reranking. In the
latter case, we simply label the maximal unlabeled
tree.
2.1.1 MST Training
McDonald et al (2005) present a technique for
training discriminative models for dependency pars-
ing. The edge-factored models we use for MST
parsing are closely related to those described in the
previous work, but allow for the efficient compu-
tation of normalization factors which are required
for first and second-order (gradient-based) training
techniques.
We consider two estimation procedures for
parent-prediction models. A parent-prediction
model assigns a conditional score s(g|d) for ev-
ery parent-child pair (we denote the parent/governor
g, and the child/dependent d), where s(g|d) =
s(g, d)/
?
g? s(g
?, d). In our work, we compute
probabilities p(g|d) based on conditional log-linear
models. This is an approximation to a generative
model that predicts each node once (i.e.,
?
d p(d|g)).
In the graph-normalized model, we assume that
the conditional distributions are independent of one
another. In particular, we find the model parameters
that maximize the likelihood of p(g?|d), where g?
is the correct parent in the training data. We per-
form the optimization over the entire training set,
tying the feature parameters. In particular, we per-
form maximum entropy (MaxEnt) estimation over
the conditional distribution using second-order gra-
dient descent optimization techniques.2 An advan-
tage of the parent-prediction model is that we can
frame the estimation problem as that of minimum-
error training with a zero-one loss term:
p(e, g|d) =
exp(
?
i ?ifi(e, g, d))
Zd
(1)
where e ? {0, 1} is the error term (e is 1 for
the correct parent and 0 for all other nodes) and
Zd =
?
j exp(
?
i ?ifi(ej , gj , d)) is the normaliza-
tion constant for node d. Note that the normaliza-
tion factor considers all graphs with in-degree zero
for the root node and in-degree one for other nodes.
At parsing time, of course, our parent predictions
are constrained to produce a (non-projective) tree
structure. We can sum over all non-projective span-
ning trees by taking the determinant of the Kirchhoff
matrix of the graph defined above, minus the row
and column corresponding to the root node (Smith
and Smith, 2007). Training graph-normalized and
tree-normalized models under identical conditions,
we find tree normalization wins by 0.5% to 1% ab-
solute dependency accuracy. Although tree normal-
ization also shows a (smaller) advantage in k-best
oracle accuracy, we do not believe it would have a
large effect on our reranking results.
2.1.2 Reranker Training
The reranker is based on a conditional log-linear
model subject to the MaxEnt constraints using the
same second-order optimization procedures as the
graph-normalized MST models. The primary dif-
ference here is that there is no single correct tree in
the set of k candidate parse trees. Instead, we have
k trees that are generated by our k-best parser, each
with a score assigned by the parser. If we are per-
forming labeled reranking, we label each of these
hypotheses with l possible labelings, each with a
score assigned by the labeler.
As with the parent-prediction, graph-normalized
model, we perform minimum-error training. The
2For the graph-normalized models, we use L-BFGS opti-
mization provided through the TAO/PETSC optimization li-
brary (Benson et al, 2005; Balay et al, 2004).
963
optimization is achieved by assuming the oracle-best
parse(s) are correct and the remaining hypotheses
are incorrect. Furthermore, the feature values are
scaled according to the relative difference between
the oracle-best score and the score assigned to the
non-oracle-best hypothesis.
Note that any reranker could be used in place of
our current model. We have chosen to keep the
reranker model closely related to the MST parsing
model so that we can share feature representations
and training procedures.
2.1.3 Labeler Training
We used the same edge features to train a sep-
arate log-linear labeling model. Each edge feature
was conjoined with a potential label, and we then
maximized the likelihood of the labeling in the train-
ing data. Since this model is also edge-factored, we
can store the labeler scores for each of the n2 po-
tential edges in the dependency tree. In the submit-
ted system, we simply extracted the Viterbi predic-
tions of the labeler for the unlabeled trees selected
by the reranker. We also (see below) ran experiments
where each entry in the k-best lists input as training
data to the reranker was augmented by its l-best la-
belings. We hoped thereby to inject more diversity
into the resulting structures.
2.1.4 Model Features
Our MST models are based on the features de-
scribed in (Hall, 2007); specifically, we use features
based on a dependency nodes? form, lemma, coarse
and fine part-of-speech tag, and morphological-
string attributes. Additionally, we use surface-string
distance between the parent and child, buckets of
features indicating if a particular form/lemma/tag
occurred between or next to the parent and child, and
a branching feature indicating whether the child is
to the left or right of the parent. Composite features,
combining the above features are also included (e.g.,
a single feature combining branching, parent & child
form, parent & child tag).
The tree-based reranker includes the features de-
scribed in (Hall, 2007) as well as features based on
non-projective edge attributes explored in (Havelka,
2007a; Havelka, 2007b). One set of features mod-
els relationships of nodes with their siblings, in-
cluding valency and subcategorization. A second
set of features models global tree structure and in-
cludes features based on a node?s ancestors and the
depth and size of its subtree. A third set of fea-
tures models the interaction of word order and tree
structure as manifested on individual edges, i.e., the
features model language specific projective and non-
projective configurations. They include edge-based
features corresponding to the global constraints of
projectivity, planarity and well-nestedness, and for
non-projective edges, they furthermore include level
type, level signature and ancestor-in-gap features.
All features allow for an arbitrary degree of lexical-
ization; in the reported results, the first two sets of
features use coarse and fine part-of-speech lexical-
izations, while the features in the third set are used
in their unlexicalized form due to time limitations.
3 Results and Analysis
Hall (2007) shows that the oracle parsing accuracy
of a k-best edge-factored MST parser is consid-
erably higher than the one-best score of the same
parser, even when k is small. We have verified that
this is true for the CoNLL shared-task data by evalu-
ating the oracle rates on a randomly sampled devel-
opment set for each language.
In order to select optimal model parameters for
the MST parser, the labeler, and reranker, we sam-
pled approximately 200 sentences from each train-
ing set to use as a development test set. Training the
reranker requires a jackknife n-fold training proce-
dure where n?1 partitions are used to train a model
that parses the remaining partition. This is done n
times to generate k-best parses for the entire training
set without using models trained on the data they are
run on.
For lack of space, we report only results on the
CoNLL evaluation data set here, but note that the
trends observed on the evaluation data are identical
to those observed on our development sets.
In Table 1 we present results for labeled (and un-
labeled) dependency accuracy on the CoNLL 2007
evaluation data set. We report the oracle accu-
racy for different sized k-best hypothesis sets. The
columns are labeled by the number of trees output
from the MST parser, k;3 and by the number of al-
3All results are reported for the graph-normalized training
technique.
964
Language Oracle Accuracy New CoNLL07 CoNLL07
k = 1, l = 1 k = 10, l = 5 k = 50, l = 1 k = 50, l = 2 Reranked Reported Best
Arabic (83.10) (85.56) (86.96) (83.67) 73.40 (83.45) 76.52 (86.09)
Basque 67.92 (76.88) 76.25 (82.19) 69.93 (84.99) 76.81 (77.76) 69.80 (78.52) 76.92 (82.80)
Catalan 82.28 (87.82) 85.11 (90.87) 86.82 (92.68) 86.82 (89.43) 82.38 (87.80) 88.70 (93.40)
Chinese 73.86 (85.58) 91.32 (93.39) 82.39 (95.80) 92.21 (87.87) 82.77 (87.91) 84.69 (88.94)
Czech 74.05 (80.21) 78.58 (85.08) 80.97 (87.60) 80.97 (82.20) 72.27 (78.47) 80.19 (86.28)
English 82.21 (83.63) 85.95 (87.59) 87.99 (89.75) 87.99 (85.31) 81.93 (83.21) 89.61 (90.63)
Greek 72.21 (81.16) 78.58 (84.89) 74.13 (86.95) 79.48 (81.81) 74.21 (82.04) 76.31 (84.08)
Hungarian 71.68 (78.57) 79.70 (83.03) 74.32 (85.12) 80.75 (80.05) 74.20 (79.34) 80.27 (83.55)
Italian 77.92 (83.16) 85.05 (87.54) 80.30 (89.66) 86.42 (84.71) 80.69 (84.81) 84.40 (87.91)
Turkish 75.34 (83.63) 83.96 (89.65) 77.78 (92.40) 84.98 (84.13) 77.42 (85.18) 79.81 (86.22)
Table 1: Labeled (unlabeled) attachment accuracy for k-best MST oracle results and reranked data on the evaluation set. The
1-best results (k = 1, l = 1) represent the performance of the MST parser without reranking. The New Reranked field shows recent
unlabeled reranking results of 50-best trees using a modified feature set. For arabic, we only report unlabeled accuracy for different
k and l.
ternative labelings for each tree, l. When k = 1,
the score is the best achievable by the edge-factored
MST parser using our models. As k increases, the
oracle parsing accuracy increases. The most ex-
treme difference between the one-best accuracy and
the 50-best oracle accuracy can be seen for Turkish
where there is a difference of 9.64 points of accu-
racy (8.77 for the unlabeled trees). This means that
the reranker need only select the correct tree from
a set of 50 to increase the score by 9.64%. As our
reranking results show, this is not as simple as it may
appear.
We report the results for our CoNLL submission
as well as recent results based on alternative param-
eters optimization on the development set. We re-
port the latest results only for unlabeled accuracy of
reranking 50-best MST output.
4 Conclusion
Our submission to the CoNLL 2007 shared task
on multilingual parsing supports the hypothesis that
edge-factored MST parsing is viable given an effec-
tive reranker. The reranker used in our submission
was unable to achieve the oracle rates. We believe
this is primarily related to a relatively impoverished
feature set. Due to time constraints, we have not
been able to train lexicalized reranking models. The
introduction of lexicalized features in the reranker
should influence the selection of better trees, which
we know exist in the k-best hypothesis sets.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Diaz
de Ilarraza, A. Garmendia, and M. Oronoz. 2003. Con-
struction of a Basque dependency treebank. In Proc. of the
2nd Workshop on Treebanks and Linguistic Theories (TLT),
pages 201?204.
Satish Balay, Kris Buschelman, Victor Eijkhout, William D.
Gropp, Dinesh Kaushik, Matthew G. Knepley, Lois Curf-
man McInnes, Barry F. Smith, and Hong Zhang. 2004.
PETSc users manual. Technical Report ANL-95/11 - Re-
vision 2.1.5, Argonne National Laboratory.
Steven J. Benson, Lois Curfman McInnes, Jorge More?, and
Jason Sarich. 2005. TAO user manual (revision 1.8).
Technical Report ANL/MCS-TM-242, Mathematics and
Computer Science Division, Argonne National Laboratory.
http://www.mcs.anl.gov/tao.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7, pages 103?127.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005. The
Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of the NEMLAR Intern. Conf. on Arabic
Language Resources and Tools, pages 110?117.
Keith Hall. 2007. k-best spanning tree parsing. In (To Appear)
Proceedings of the 45th Annual Meeting of the Association
for Computational Linguistics.
965
Jir??? Havelka. 2007a. Beyond projectivity: Multilingual eval-
uation of constraints and measures on non-projective struc-
tures. In (To Appear) Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics.
Jir??? Havelka. 2007b. Relationship between non-projective
edges, their level types, and well-nestedness. In Human
Language Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Lin-
guistics; Companion Volume, Short Papers, pages 61?64.
R. Johansson and P. Nugues. 2007. Extended constituent-to-
dependency conversion for English. In Proc. of the 16th
Nordic Conference on Computational Linguistics (NODAL-
IDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Build-
ing a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Online large-margin training of dependency parsers. In Pro-
ceedings of the 43nd Annual Meeting of the Association for
Computational Linguistics.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, O. Coraz-
zari, A. Lenci, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino, F. Zan-
zotto, N. Nana, F. Pianesi, and R. Delmonte. 2003. Build-
ing the Italian Syntactic-Semantic Treebank. In Abeille?
(Abeille?, 2003), chapter 11, pages 189?210.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson, S. Riedel,
and D. Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proc. of the Joint Conf. on Empiri-
cal Methods in Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-CoNLL).
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papageor-
giou, and S. Piperidis. 2005. Theoretical and practical is-
sues in the construction of a Greek dependency treebank. In
Proc. of the 4th Workshop on Treebanks and Linguistic The-
ories (TLT), pages 149?160.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In (To Appear) Pro-
ceedings of the 2007 Conference on Empirical Methods in
Natural Language Processing.
966
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 392?399,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
k-best Spanning Tree Parsing
Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
keith hall@jhu.edu
Abstract
This paper introduces a Maximum Entropy
dependency parser based on an efficient k-
best Maximum Spanning Tree (MST) algo-
rithm. Although recent work suggests that
the edge-factored constraints of the MST al-
gorithm significantly inhibit parsing accu-
racy, we show that generating the 50-best
parses according to an edge-factored model
has an oracle performance well above the
1-best performance of the best dependency
parsers. This motivates our parsing ap-
proach, which is based on reranking the k-
best parses generated by an edge-factored
model. Oracle parse accuracy results are
presented for the edge-factored model and
1-best results for the reranker on eight lan-
guages (seven from CoNLL-X and English).
1 Introduction
The Maximum Spanning Tree algorithm1 was re-
cently introduced as a viable solution for non-
projective dependency parsing (McDonald et al,
2005b). The dependency parsing problem is nat-
urally a spanning tree problem; however, effi-
cient spanning-tree optimization algorithms assume
a cost function which assigns scores independently
to edges of the graph. In dependency parsing, this
effectively constrains the set of models to those
which independently generate parent-child pairs;
1In this paper we deal only with MSTs on directed graphs.
These are often referred to in the graph-theory literature asMax-
imum Spanning Arborescences.
these are known as edge-factored models. These
models are limited to relatively simple features
which exclude linguistic constructs such as verb
sub-categorization/valency, lexical selectional pref-
erences, etc.2
In order to explore a rich set of syntactic fea-
tures in the MST framework, we can either approx-
imate the optimal non-projective solution as in Mc-
Donald and Pereira (2006), or we can use the con-
strained MST model to select a subset of the set
of dependency parses to which we then apply less-
constrained models. An efficient algorithm for gen-
erating the k-best parse trees for a constituency-
based parser was presented in Huang and Chiang
(2005); a variation of that algorithm was used for
generating projective dependency trees for parsing
in Dreyer et al (2006) and for training in McDonald
et al (2005a). However, prior to this paper, an effi-
cient non-projective k-best MST dependency parser
has not been proposed.3
In this paper we show that the na??ve edge-factored
models are effective at selecting sets of parses on
which the oracle parse accuracy is high. The or-
acle parse accuracy for a set of parse trees is the
highest accuracy for any individual tree in the set.
We show that the 1-best accuracy and oracle accu-
racy can differ by as much as an absolute 9% when
the oracle is computed over a small set generated by
edge-factored models (k = 50).
2Labeled edge-factored models can capture selectional pref-
erence; however, the unlabeled models presented here are lim-
ited to modeling head-child relationships without predicting the
type of relationship.
3The work of McDonald et al (2005b) would also benefit
from a k-best non-projective parser for training.
392
ROOT
two
share
a
house
almost
devoid
of
furniture
.
Figure 1: A dependency graph for an English sen-
tence in our development set (Penn WSJ section 24):
Two share a house almost devoid of furniture.
The combination of two discriminatively trained
models, a k-best MST parser and a parse tree
reranker, results in an efficient parser that includes
complex tree-based features. In the remainder of the
paper, we first describe the core of our parser, the
k-best MST algorithm. We then introduce the fea-
tures that we use to compute edge-factored scores
as well as tree-based scores. Following, we outline
the technical details of our training procedure and fi-
nally we present empirical results for the parser on
seven languages from the CoNLL-X shared-task and
a dependency version of the WSJ Penn Treebank.
2 MST in Dependency Parsing
Work on statistical dependency parsing has utilized
either dynamic-programming (DP) algorithms or
variants of the Edmonds/Chu-Liu MST algorithm
(see Tarjan (1977)). The DP algorithms are gener-
ally variants of the CKY bottom-up chart parsing al-
gorithm such as that proposed by Eisner (1996). The
Eisner algorithm efficiently (O(n3)) generates pro-
jective dependency trees by assembling structures
over contiguous words in a clever way to minimize
book-keeping. Other DP solutions use constituency-
based parsers to produce phrase-structure trees, from
which dependency structures are extracted (Collins
et al, 1999). A shortcoming of the DP-based ap-
proaches is that they are unable to generate non-
projective structures. However, non-projectivity is
necessary to capture syntactic phenomena in many
languages.
McDonald et al (2005b) introduced a model for
dependency parsing based on the Edmonds/Chu-Liu
algorithm. The work we present here extends their
work by exploring a k-best version of the MST algo-
rithm. In particular, we consider an algorithm pro-
posed by Camerini et al (1980) which has a worst-
case complexity of O(km log(n)), where k is the
number of parses we want, n is the number of words
in the input sentence, and m is the number of edges
in the hypothesis graph. This can be reduced to
O(kn2) in dense graphs4 by choosing appropriate
data structures (Tarjan, 1977). Under the models
considered here, all pairs of words are considered
as candidate parents (children) of another, resulting
in a fully connected graph, thus m = n2.
In order to incorporate second-order features
(specifically, sibling features), McDonald et al pro-
posed a dependency parser based on the Eisner algo-
rithm (McDonald and Pereira, 2006). The second-
order features allow for more complex phrasal rela-
tionships than the edge-factored features which only
include parent/child features. Their algorithm finds
the best solution according to the Eisner algorithm
and then searches for the single valid edge change
that increases the tree score. The algorithm iter-
ates until no better single edge substitution can im-
prove the score of the tree. This greedy approxi-
mation allows for second-order constraints and non-
projectivity. They found that applying this method
to trees generated by the Eisner algorithm using
second-order features performs better than applying
it to the best tree produced by the MST algorithm
with first-order (edge-factored) features.
In this paper we provide a new evaluation of the
efficacy of edge-factored models, k-best oracle re-
sults. We show that even when k is small, the
edge-factored models select k-best sets which con-
tain good parses. Furthermore, these good parses
are even better than the parses selected by the best
dependency parsers.
2.1 k-best MST Algorithm
The k-best MST algorithm we introduce in this pa-
per is the algorithm described in Camerini et al
(1980). For proofs of complexity and correctness,
we defer to the original paper. This section is in-
tended to provide the intuitions behind the algo-
rithm and allow for an understanding of the key data-
structures necessary to ensure the theoretical guar-
antees.
4A dense graph is one in which the number of edges is close
to the number of edges in a fully connected graph (i.e., n2).
393
B C
4
9
8
5
11
10
1
1
5
v
1
v
2
v
3
R
4
-2
-3
5
-10
1
-5
v
4
v
3
R
v
1
v
2
v
1
v
2
10
v
1
v
2
10
11
v
4
v
1
v
2
v
3
v
1
v
2
v
3
v
3
v
4
v
3
-2
v
1
v
2
v
4
v
3
v
4
v
3
-2
5
-7
-4
-3
v
5
R
e
32
e
23
e
R2
e
R1
e
13
e
31
4
-2
-3
5
-10
1
-5
v
4
v
3
R
e
32
e
23
e
R2
e
R1
e
13
e
31
4
-2
-3
5
-10
1
-5
v
4
v
3
R
e
32
e
23
e
R2
e
R1
e
13
e
31
e
R1
e
R2
e
R3
v
1
v
2
v
4
v
3
v
1
v
2
v
4
v
3
v
5
4
8
8
5
11
10
1
1
5
v
1
v
2
v
3
R
-7
-4
-3
v
5
R
e
R1
e
R2
e
R3
v
5
R
-3
e
R1
e
23
e
31
e
31
G
v
1
v
2
v
4
v
3
v
5
S1
S2
S3
S4
S5
S6
S7
Figure 2: Simulated 1-best MST algorithm.
Let G = {V,E} be a directed graph
where V = {R, v1, . . . , vn} and E =
{e11, e12, . . . , e1n, e21, . . . , enn}. We refer to
edge eij as the edge that is directed from vi into
vj in the graph. The initial dependency graph in
Figure 2 (column G) contains three regular nodes
and a root node.
Algorithm 1 is a version of the MST algorithm
as presented by Camerini et al (1980); subtleties of
the algorithm have been omitted. Arguments Y (a
branching5) and Z (a set of edges) are constraints on
the edges that can be part of the solution, A. Edges
in Y are required to be in the solution and edges in
5A branching is a subgraph that contains no cycles and no
more than one edge directed into each node.
Algorithm 1 Sketch of 1-best MST algorithm
procedure BEST(G, Y, Z)
G = (G ? Y )? Z
B = ?
C = V
5: for unvisited vertex vi ? V do
mark vi as visited
get best in-edge b ? {ejk : k = i} for vi
B = B ? b
?(vi) = b
10: if B contains a cycle C then
create a new node vn+1
C = C ? vn+1
make all nodes of C children of vn+1 in C
COLLAPSE all nodes of C into vn+1
15: ADD vn+1 to list of unvisited vertices
n = n + 1
B = B ? C
end if
end for
20: EXPAND C choosing best way to break cycles
Return best A = {b ? E|?v ? V : ?(v) = b}
and C
end procedure
Z cannot be part of the solution. The branching C
stores a hierarchical history of cycle collapses, en-
capsulating embedded cycles and allowing for an ex-
panding procedure, which breaks cycles while main-
taining an optimal solution.
Figure 2 presents a view of the algorithm when
run on a three node graphs (plus a specified root
node). Steps S1, S2, S4, and S5 depict the process-
ing of lines 5 to 8, recording in ? the best input edges
for each vertex. Steps S3 and S6 show the process of
collapsing a cycle into a new node (lines 10 to 16).
The main loop of the algorithm processes each
vertex that has not yet been visited. We look up the
best incoming edge (which is stored in a priority-
queue). This value is recorded in ? and the edge is
added to the current best graph B. We then check
to see if adding this new edge would create a cycle
in B. If so, we create a new node and collapse the
cycle into it. This can be seen in Step S3 in Figure 2.
The process of collapsing a cycle into a node in-
volves removing the edges in the cycle from B, and
adjusting the weights of all edges directed into any
node in the cycle. The weights are adjusted so that
they reflect the relative difference of choosing the
new in-edge rather than the edge in the cycle. In
step S3, observe that edge eR1 had a weight of 5, but
now that it points into the new node v4, we subtract
the weight of the edge e21 that also pointed into v1,
394
which was 10. Additionally, we record in C the re-
lationship between the new node v4 and the original
nodes v1 and v2.
This process continues until we have visited all
original and newly created nodes. At that point, we
expand the cycles encoded in C. For each node not
originally in G (e.g., v5, v4), we retrieve the edge er
pointing into this node, recorded in ?. We identify
the node vs to which er pointed in the original graph
G and set ?(vs) = er.
Algorithm 2 Sketch of next-best MST algorithm
procedure NEXT(G, Y, Z,A,C)
? ? +?
for unvisited vertex v do
get best in-edge b for v
5: if b ? A? Y then
f ? alternate edge into v
if swapping f with b results in smaller ? then
update ?, let e? f
end if
10: end if
if b forms a cycle then
Resolve as in 1-best
end if
end for
15: Return edge e and ?
end procedure
Algorithm 2 returns the single edge, e, of the 1-
best solution A that, when removed from the graph,
results in a graph for which the best solution is the
next best solution after A. Additionally, it returns
?, the difference in score between A and the next
best tree. The branching C is passed in from Algo-
rithm 1 and is used here to efficiently identify alter-
nate edges, f , for edge e.
Y and Z in Algorithms 1 and 2 are used to con-
struct the next best solutions efficiently. We call
GY,Z a constrained graph; the constraints being that
Y restricts the in-edges for a subset of nodes: for
each vertex with an in-edge in Y , only the edge of
Y can be an in-edge of the vertex. Also, edges in
Z are removed from the graph. A constrained span-
ning tree for GY,Z (a tree covering all nodes in the
graph) must satisfy: Y ? A ? E ? Z.
Let A be the (constrained) solution to a (con-
strained) graph and let e be the edge that leads to the
next best solution. The third-best solution is either
the second-best solution to GY,{Z?e} or the second-
best solution to G{Y ?e},Z . The k-best ranking al-
gorithm uses this fact to incrementally partition the
solution space: for each solution, the next best either
will include e or will not include e.
Algorithm 3 k-best MST ranking algorithm
procedure RANK(G, k)
A,C ? best(E, V, ?, ?)
(e, ?)? next(E, V, ?, ?, A, C)
bestList? A
5: Q? enqueue(s(A)? ?, e, A,C, ?, ?)
for j ? 2 to k do
(s, e, A,C, Y, Z) = dequeue(Q)
Y ? = Y ? e
Z? = Z ? e
10: A?, C? ? best(E, V, Y, Z?)
bestList? A?
e?, ?? ? next(E, V, Y ?, Z,A?, C?)
Q? enqueue(s(A)? ??, e?, A?, C?, Y ?, Z)
e?, ?? ? next(E, V, Y, Z?, A?, C?)
15: Q? enqueue(s(A)? ??, e?, A?, C?, Y, Z?)
end for
Return bestList
end procedure
The k-best ranking procedure described in Algo-
rithm 3 uses a priority queue, Q, keyed on the first
parameter to enqueue to keep track of the horizon
of next best solutions. The function s(A) returns the
score associated with the tree A. Note that in each
iteration there are two new elements enqueued rep-
resenting the sets GY,{Z?e} and G{Y ?e},Z .
Both Algorithms 1 and 2 run inO(m log(n)) time
and can run in quadratic time for dense graphs with
the use of an efficient priority-queue6 (i.e., based
on a Fibonacci heap). Algorithm 3 runs in con-
stant time, resulting in anO(km log n) algorithm (or
O(kn2) for dense graphs).
3 Dependency Models
Each of the two stages of our parser is based on a dis-
criminative training procedure. The edge-factored
model is based on a conditional log-linear model
trained using the Maximum Entropy constraints.
3.1 Edge-factored MST Model
One way in which dependency parsing differs from
constituency parsing is that there is a fixed amount of
structure in every tree. A dependency tree for a sen-
tence of n words has exactly n edges,7 each repre-
6Each vertex keeps a priority queue of candidate parents.
When a cycles is collapsed, the new vertex inherits the union of
queues associated with the vertices of the cycle.
7We assume each tree has a root node.
395
senting a syntactic or semantic relationship, depend-
ing on the linguistic model assumed for annotation.
A spanning tree (equivalently, a dependency parse)
is a subgraph for which each node has one in-edge,
the root node has zero in-edges, and there are no cy-
cles.
Edge-factored features are defined over the edge
and the input sentence. For each of the n2 par-
ent/child pairs, we extract the following features:
Node-type There are three basic node-type fea-
tures: word form, morphologically reduced
lemma, and part-of-speech (POS) tag. The
CoNLL-X data format8 describes two part-of-
speech tag types, we found that features derived
from the coarse tags are more reliable. We con-
sider both unigram (parent or child) and bigram
(composite parent/child) features. We refer to
parent features with the prefix p- and child fea-
ture with the prefix c-; for example: p?pos,
p?form, c?pos, and c?form. In our model we
use both word form and POS tag and include
the composite form/POS features: p?form/c?
pos and p?pos/c?form.
Branch A binary feature which indicates whether
the child is to the left or right of the parent
in the input string. Additionally, we provide
composite features p?pos/branch and p?pos/c?
pos/branch.
Distance The number of words occurring between
the parent and child word. These distances are
bucketed into 7 buckets (1 through 6 plus an ad-
ditional single bucket for distances greater than
6). Additionally, this feature is combined with
node-type features: p?pos/dist, c?pos/dist, p?
pos/c?pos/dist.
Inside POS tags of the words between the parent
and child. A count of each tag that occurs is
recorded, the feature is identified by the tag and
the feature value is defined by the count. Addi-
tional composite features are included combin-
ing the inside and node-type: for each type ti
the composite features are: p?pos/ti, c?pos/ti,
p?pos/c?pos/ti.
8The 2006 CoNLL-X data format can be found on-line at:
http://nextens.uvt.nl/?conll/.
Outside Exactly the same as the Inside feature ex-
cept that it is defined over the features to the
left and right of the span covered by this parent-
child pair.
Extra-Feats Attribute-value pairs from the CoNLL
FEATS field including combinations with par-
ent/child node-types. These features represent
word-level annotations provided in the tree-
bank and include morphological and lexical-
semantic features. These do not exist in the En-
glish data.
Inside Edge Similar to Inside features, but only
includes nodes immediately to left and right
within the span covered by the parent/child
pair. We include the following features where
il and ir are the inside left and right POS tags
and ip is the inside POS tag closest to the par-
ent: il/ir, p?pos/ip, p?pos/il/ir/c?pos,
Outside Edge An Outside version of the Inside
Edge feature type.
Many of the features above were introduced in
McDonald et al (2005a); specifically, the node-
type, inside, and edge features. The number of fea-
tures can grow quite large when form or lemma fea-
tures are included. In order to handle large training
sets with a large number of features we introduce a
bagging-based approach, described in Section 4.2.
3.2 Tree-based Reranking Model
The second stage of our dependency parser is a
reranker that operates on the output of the k-best
MST parser. Features in this model are not con-
strained as in the edge-factored model. Many
of the model features have been inspired by the
constituency-based features presented in Charniak
and Johnson (2005). We have also included features
that exploit non-projectivity where possible. The
node-type is the same as defined for the MST model.
MST score The score of this parse given by the
first-stage MST model.
Sibling The POS-tag of immediate siblings. In-
tended to capture the preference for particular
immediate siblings such as modifiers.
Valency Count of the number of children for each
word (indexed by POS-tag of the word). These
396
counts are bucketed into 4 buckets. For ex-
ample, a feature may look like p?pos=VB/v=4,
meaning the POS tag of the parent is ?VB? and
it had 4 dependents.
Sub-categorization A string representing the se-
quence of child POS tags for each parent POS-
tag.
Ancestor Grandparent and great grandparent POS-
tag for each word. Composite features are gen-
erated with the label c?pos/p?pos/gp?pos and
c?pos/p?pos/ggp?pos (where gp is the grand-
parent and ggp is the great grand-parent).
Edge POS-tag to the left and right of the subtree,
both inside and outside the subtree. For exam-
ple, say a subtree with parent POS-tag p?pos
spans from i to j, we include composite out-
side features: p?pos/ni?1?pos/nj+1?pos, p?
pos/ni?1?pos, p?pos/nj+1?pos; and composite
inside features: p?pos/ni+1?pos/nj?1?pos, p?
pos/ni+1?pos, p?pos/nj?1?pos.
Branching Factor Average number of left/right
branching nodes per POS-tag. Additionally, we
include a boolean feature indicating the overall
left/right preference.
Depth Depth of the tree and depth normalized by
sentence length.
Heavy Number of dominated nodes per POS-tag.
We also include the average number of nodes
dominated by each POS-tag.
4 MaxEnt Training
We have adopted the conditional Maximum Entropy
(MaxEnt) modeling paradigm as outlined in Char-
niak and Johnson (2005) and Riezler et al (2002).
We can partition the training examples into indepen-
dent subsets, Ys: for the edge-factored MST models,
each set represents a word and its candidate parents;
for the reranker, each set represents the k-best trees
for a particular sentence. We wish to estimate the
conditional distribution over hypotheses in the set yi,
given the set: p(yi|Ys) =
exp(
P
k ?kfik)P
j:yj?Ys
exp(
P
k? ?k
?fjk? )
,
where fik is the kth feature function in the model
for example yi.
4.1 MST Training
Our MST parser training procedure involves enu-
merating the n2 potential tree edges (parent/child
pairs). Unlike the training procedure employed by
McDonald et al (2005b) and McDonald and Pereira
(2006), we provide positive and negative examples
in the training data. A node can have at most one
parent, providing a natural split of the n2 training
examples. For each node ni, we wish to estimate
a distribution over n nodes9 as potential parents,
p(vi, eji|e i), the probability of the correct parent of
vi being vj given the set of edges associated with
its candidate parents e i. We call this the parent-
prediction model.
4.2 MST Bagging
The complexity of the training procedure is a func-
tion of the number of features and the number of ex-
amples. For large datasets, we use an ensemble tech-
nique inspired by Bagging (Breiman, 1996). Bag-
ging is generally used to mitigate high variance in
datasets by sampling, with replacement, from the
training set. Given that we wish to include some
of the less frequent examples and therefore are not
necessarily avoiding high variance, we partition the
data into disjoint sets.
For each of the sets, we train a model indepen-
dently. Furthermore, we only allow the parame-
ters to be changed for those features observed in the
training set. At inference time, we apply each model
to the training data and then combine the prediction
probabilities.
p??(yi|Ys) = max
m
p?m(yi|Ys) (1)
p??(yi|Ys) =
1
M
?
m
p?m(yi|Ys) (2)
p??(yi|Ys) =
(
?
m
p?m(yi|Ys)
)1/M
(3)
p??(yi|Ys) =
M
?
m
1
p?m (yi|Ys)
(4)
Equations 1, 2, 3, and 4 are the maximum, aver-
age, geometric mean, and harmonic mean, respec-
tively. We performed an exploration of these on the
9Recall that in addition to the n?1 other nodes in the graph,
there is a root node for which we know has no parents.
397
development data and found that the geometric mean
produces the best results (Equation 3); however, we
observed only very small differences in the accuracy
among models where only the combination function
differed.
4.3 Reranker Training
The second stage of parsing is performed by our
tree-based reranker. The input to the reranker is a
list of k parses generated by the k-best MST parser.
For each input sentence, the hypothesis set is the k
parses. At inference time, predictions are made in-
dependently for each hypothesis set Ys and therefore
the normalization factor can be ignored.
5 Empirical Evaluation
The CoNLL-X shared task on dependency parsing
provided data for a number of languages in a com-
mon data format. We have selected seven of these
languages for which the data is available to us. Ad-
ditionally, we have automatically generated a depen-
dency version of the Penn WSJ treebank.10 As we
are only interested in the structural component of a
parse in this paper, we present results for unlabeled
dependency parsing. A second labeling stage can be
applied to get labeled dependency structures as de-
scribed in (McDonald et al, 2006).
In Table 1 we report the accuracy for seven of
the CoNLL languages and English.11 Already, at
k = 50, we see the oracle rate climb as much as
9.25% over the 1-best result (Dutch). Continuing to
increase the size of the k-best lists adds to the oracle
accuracy, but the relative improvement appears to be
increasing at a logarithmic rate. The k-best parser is
used both to train the k-best reranker and, at infer-
ence time, to select a set of hypotheses to rerank. It
is not necessary that training is done with the same
size hypothesis set as test, we explore the matched
and mismatched conditions in our reranking experi-
ments.
10The Penn WSJ treebank was converted using the con-
version program described in (Johansson and Nugues, 2007)
and available on the web at: http://nlp.cs.lth.se/
pennconverter/
11The Best Reported results is from the CoNLL-X competi-
tion. The best result reported for English is the Charniak parser
(without reranking) on Section 23 of the WSJ Treebank using
the same head-finding rules as for the evaluation data.
Table 2 shows the reranking results for the set of
languages. For each language, we select model pa-
rameters on a development set prior to running on
the test data. These parameters include a feature
count threshold (the minimum number of observa-
tions of a feature before it is included in a model)
and a mixture weight controlling the contribution of
a quadratic regularizer (used in MaxEnt training).
For Czech, German, and English, we use the MST
bagging technique with 10 bags. These test results
are for the models which performed best on the de-
velopment set (using 50-best parses).
We see minor improvements over the 1-best base-
line MST output (repeated in this table for compar-
ison). We believe this is due to the overwhelming
number of parameters in the reranking models and
the relatively small amount of training data. Inter-
estingly, increasing the number of hypotheses helps
for some languages and hurts the others.
6 Conclusion
Although the edge-factored constraints of MST
parsers inhibit accuracy in 1-best parsing, edge-
factored models are effective at selecting high accu-
racy k-best sets. We have introduced the Camerini
et al (1980) k-best MST algorithm and have shown
how to efficiently train MaxEnt models for depen-
dency parsing. Additionally, we presented a uni-
fied modeling and training setting for our two-stage
parser; MaxEnt training is used to estimate the pa-
rameters in both models. We have introduced a
particular ensemble technique to accommodate the
large training sets generated by the first-stage edge-
factored modeling paradigm. Finally, we have pre-
sented a reranker which attempts to select the best
tree from the k-best set. In future work we wish
to explore more robust feature sets and experiment
with feature selection techniques to accommodate
them.
Acknowledgments
This work was partially supported by U.S. NSF
grants IIS?9982329 and OISE?0530118. We thank
Ryan McDonald for directing us to the Camerini et
al. paper and Liang Huang for insightful comments.
398
Language Best Oracle Accuracy
Reported k = 1 k = 10 k = 50 k = 100 k = 500
Arabic 79.34 77.92 80.72 82.18 83.03 84.47
Czech 87.30 83.56 88.50 90.88 91.80 93.50
Danish 90.58 89.12 92.89 94.79 95.29 96.59
Dutch 83.57 81.05 87.43 90.30 91.28 93.12
English 92.36 85.04 89.04 91.12 91.87 93.42
German 90.38 87.02 91.51 93.39 94.07 95.47
Portuguese 91.36 89.86 93.11 94.85 95.39 96.47
Swedish 89.54 86.50 91.20 93.37 93.83 95.42
Table 1: k-best MST oracle results. The 1-best results represent the performance of the parser in isolation.
Results are reported for the CoNLL test set and for English, on Section 23 of the Penn WSJ Treebank.
Language Best Reranked Accuracy
Reported 1-best 10-best 50-best 100-best 500-best
Arabic 79.34 77.61 78.06 78.02 77.94 77.76
Czech 87.30 83.56 83.94 84.14 84.48 84.46
Danish 90.58 89.12 89.48 89.76 89.68 89.74
Dutch 83.57 81.05 82.01 82.91 82.83 83.21
English 92.36 85.04 86.54 87.22 87.38 87.81
German 90.38 87.02 88.24 88.72 88.76 88.90
Portuguese 91.36 89.38 90.00 89.98 90.02 90.02
Swedish 89.54 86.50 87.87 88.21 88.26 88.53
Table 2: Second-stage results from the k-best parser and reranker. The Best Reported and 1-best fields are
copied from table 1. Only non-lexical features were used for the reranking models.
References
Leo Breiman. 1996. Bagging predictors. Machine Learning,
26(2):123?140.
Paolo M. Camerini, Luigi Fratta, and Francesco Maffioli. 1980.
The k best spanning arborescences of a network. Networks,
10:91?110.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
Michael Collins, Lance Ramshaw, Jan Hajic?, and Christoph
Tillmann. 1999. A statistical parser for Czech. In Pro-
ceedings of the 37th annual meeting of the Association for
Computational Linguistics, pages 505?512.
Markus Dreyer, David A. Smith, and Noah A. Smith. 2006.
Vine parsing and minimum risk reranking for speed and pre-
cision. In Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning.
Jason Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proceedings of the
16th International Conference on Computational Linguistics
(COLING), pages 340?345.
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proceedings of the 9th International Workshop on Parsing
Technologies.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In Pro-
ceedings of NODALIDA 2007, Tartu, Estonia, May 25-26.
To appear.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proceed-
ings of the Annual Meeting of the European Association for
Computational Linguistics.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005a. Online large-margin training of dependency parsers.
In Proceedings of the 43nd Annual Meeting of the Associa-
tion for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005b. Non-projective dependency parsing using
spanning tree algorithms. In Proceedings of Human Lan-
guage Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 523?530,
October.
Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006.
Multilingual dependency parsing with a two-stage discrimi-
native parser. In Conference on Natural Language Learning.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard
Crouch, John T. III Maxwell, and Mark Johnson. 2002.
Parsing the Wall Street Journal using a lexical-functional
grammar and discriminative estimation techniques. In Pro-
ceedings of the 40th Annual Meeting of the Association for
Computational Linguistics. Morgan Kaufmann.
R.E. Tarjan. 1977. Finding optimal branchings. Networks,
7:25?35.
399
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 42?52,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Corrective Modeling for Non-Projective Dependency Parsing
Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
keith hall@jhu.edu
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
novak@ufal.mff.cuni.cz
Abstract
We present a corrective model for recov-
ering non-projective dependency struc-
tures from trees generated by state-of-the-
art constituency-based parsers. The con-
tinuity constraint of these constituency-
based parsers makes it impossible for
them to posit non-projective dependency
trees. Analysis of the types of depen-
dency errors made by these parsers on a
Czech corpus show that the correct gov-
ernor is likely to be found within a local
neighborhood of the governor proposed
by the parser. Our model, based on a
MaxEnt classifier, improves overall de-
pendency accuracy by .7% (a 4.5% reduc-
tion in error) with over 50% accuracy for
non-projective structures.
1 Introduction
Statistical parsing models have been shown to
be successful in recovering labeled constituencies
(Collins, 2003; Charniak and Johnson, 2005; Roark
and Collins, 2004) and have also been shown to
be adequate in recovering dependency relationships
(Collins et al, 1999; Levy and Manning, 2004;
Dubey and Keller, 2003). The most successful mod-
els are based on lexicalized probabilistic context
free grammars (PCFGs) induced from constituency-
based treebanks. The linear-precedence constraint
of these grammars restricts the types of dependency
structures that can be encoded in such trees.1 A
shortcoming of the constituency-based paradigm for
parsing is that it is inherently incapable of repre-
senting non-projective dependencies trees (we de-
fine non-projectivity in the following section). This
is particularly problematic when parsing free word-
order languages, such as Czech, due to the frequency
of sentences with non-projective constructions.
In this work, we explore a corrective model which
recovers non-projective dependency structures by
training a classifier to select correct dependency
pairs from a set of candidates based on parses gen-
erated by a constituency-based parser. We chose to
use this model due to the observations that the de-
pendency errors made by the parsers are generally
local errors. For the nodes with incorrect depen-
dency links in the parser output, the correct gov-
ernor of a node is often found within a local con-
text of the proposed governor. By considering al-
ternative dependencies based on local deviations of
the parser output we constrain the set of candidate
governors for each node during the corrective proce-
dure. We examine two state-of-the-art constituency-
based parsers in this work: the Collins Czech parser
(1999) and a version of the Charniak parser (2001)
that was modified to parse Czech.
Alternative efforts to recover dependency struc-
ture from English are based on reconstructing the
movement traces encoded in constituency trees
(Collins, 2003; Levy and Manning, 2004; Johnson,
2002; Dubey and Keller, 2003). In fact, the fea-
1In order to correctly capture the dependency structure, co-
indexed movement traces are used in a form similar to govern-
ment and Binding theory, GPSG, etc.
42
wc
wa
wb
b ca
wc
b ca
wa wb
wc
wa
wb
b ca
Figure 1: Examples of projective and non-projective trees. The trees on the left and center are both projec-
tive. The tree on the right is non-projective.
tures we use in the current model are similar to those
proposed by Levy and Manning (2004). However,
the approach we propose discards the constituency
structure prior to the modeling phase; we model cor-
rective transformations of dependency trees.
The technique proposed in this paper is similar to
that of recent parser reranking approaches (Collins,
2000; Charniak and Johnson, 2005); however, while
reranking approaches allow a parser to generate a
likely candidate set according to a generative model,
we consider a set of candidates based on local per-
turbations of the single most likely tree generated.
The primary reason for such an approach is that we
allow dependency structures which would never be
hypothesized by the parser. Specifically, we allow
for non-projective dependencies.
The corrective algorithm proposed in this paper
shares the motivation of the transformation-based
learning work (Brill, 1995). We do consider local
transformations of the dependency trees; however,
the technique presented here is based on a generative
model that maximizes the likelihood of good depen-
dents. We consider a finite set of local perturbations
of the tree and use a fixed model to select the best
tree by independently choosing optimal dependency
links.
In the remainder of the paper we provide a defini-
tion of a dependency tree and the motivation for us-
ing such trees as well as a description of the particu-
lar dataset that we use in our experiments, the Prague
Dependency Treebank (PDT). In Section 3 we de-
scribe the techniques used to adapt constituency-
based parsers to train from and generate dependency
trees. Section 4 describes corrective modeling as
used in this work and Section 4.2 describes the par-
ticular features with which we have experimented.
Section 5 presents the results of a set of experiments
we performed on data from the PDT.
2 Syntactic Dependency Trees and the
Prague Dependency Treebank
A dependency tree is a set of nodes ? =
{w
0
, w
1
, . . . , wk} where w0 is the imaginary root
node2 and a set of dependency links G =
{g
1
, . . . , gk} where gi is an index into ? represent-
ing the governor of wi. In other words g3 = 1 in-
dicates that the governor of w
3
is w
1
. Finally, every
node has exactly one governor except for w
0
, which
has no governor (the tree constraints).3 The index of
the nodes represents the surface order of the nodes
in the sequence (i.e., wi precedes wj in the sentence
if i < j).
A tree is projective if for every three nodes: wa,
wb, and wc where a < b < c; if wa is governed by
wc then wb is transitively governed by wc or if wc
is governed by wa then wb is transitively governed
by wa.4 Figure 1 shows examples of projective and
non-projective trees. The rightmost tree, which is
non-projective, contains a subtree consisting of wa
and wc but not wb; however, wb occurs between wa
and wc in the linear ordering of the nodes. Projec-
tivity in a dependency tree is akin to the continuity
constraint in a constituency tree; such a constraint is
2The imaginary root node simplifies notation.
3The dependency structures here are very similar to those
described by Mel?c?uk (1988); however the nodes of the depen-
dency trees discussed in this paper are limited to the words of
the sentence and are always ordered according to the surface
word-order.
4Node w
a
is said to transitively govern node w
b
if w
b
is a
descendant of w
a
in the dependency tree.
43
implicitly imposed by trees generated from context
free grammars (CFGs).
Strict word-order languages, such as English, ex-
hibit non-projective dependency structures in a rel-
atively constrained set of syntactic configurations
(e.g., right-node raising). Traditionally, these move-
ments are encoded in syntactic analyses as traces.
In languages with free word-order, such as Czech,
constituency-based representations are overly con-
strained (Sgall et al, 1986). Syntactic dependency
trees encode syntactic subordination relationships
allowing the structure to be non-specific about the
underlying deep representation. The relationship
between a node and its subordinates expresses a
sense of syntactic (functional) entailment.
In this work we explore the dependency struc-
tures encoded in the Prague Dependency Treebank
(Hajic?, 1998; Bo?hmova? et al, 2002). The PDT 1.0
analytical layer is a set of Czech syntactic depen-
dency trees; the nodes of which contain the word
forms, morphological features, and syntactic anno-
tations. These trees were annotated by hand and
are intended as an intermediate stage in the annota-
tion of the Tectogrammatical Representation (TR),
a deep-syntactic or syntacto-semantic theory of lan-
guage (Sgall et al, 1986). All current automatic
techniques for generating TR structures are based on
syntactic dependency parsing.
When evaluating the correctness of dependency
trees, we only consider the structural relationships
between the words of the sentence (unlabeled depen-
dencies). However, the model we propose contains
features that are considered part of the dependency
rather than the nodes in isolation (e.g., agreement
features). We do not propose a model for correctly
labeling dependency structures in this work.
3 Constituency Parsing for Dependency
Trees
A pragmatic justification for using constituency-
based parsers in order to predict dependency struc-
tures is that currently the best Czech dependency-
tree parser is a constituency-based parser (Collins et
al., 1999; Zeman, 2004). In fact both Charniak?s
and Collins? generative probabilistic models con-
tain lexical dependency features.5 From a gener-
ative modeling perspective, we use the constraints
imposed by constituents (i.e., projectivity) to enable
the encapsulation of syntactic substructures. This di-
rectly leads to efficient parsing algorithms such as
the CKY algorithm and related agenda-based pars-
ing algorithms (Manning and Schu?tze, 1999). Addi-
tionally, this allows for the efficient computation of
the scores for the dynamic-programming state vari-
ables (i.e., the inside and outside probabilities) that
are used in efficient statistical parsers. The computa-
tional complexity advantages of dynamic program-
ming techniques along with efficient search tech-
niques (Caraballo and Charniak, 1998; Klein and
Manning, 2003) allow for richer predictive models
which include local contextual information.
In an attempt to extend a constituency-based pars-
ing model to train on dependency trees, Collins
transforms the PDT dependency trees into con-
stituency trees (Collins et al, 1999). In order to
accomplish this task, he first normalizes the trees
to remove non-projectivities. Then, he creates ar-
tificial constituents based on the parts-of-speech of
the words associated with each dependency node.
The mapping from dependency tree to constituency
tree is not one-to-one. Collins describes a heuristic
for choosing trees that work well with his parsing
model.
3.1 Training a Constituency-based Parser
We consider two approaches to creating projec-
tive trees from dependency trees exhibiting non-
projectivities. The first is based on word-reordering
and is the model that was used with the Collins
parser. This algorithm identifies non-projective
structures and deterministically reorders the words
of the sentence to create projective trees. An alter-
native method, used by Charniak in the adaptation
of his parser for Czech6 and used by Nivre and Nils-
son (2005), alters the dependency links by raising
the governor to a higher node in the tree whenever
5Bilexical dependencies are components of both the Collins
and Charniak parsers and effectively model the types of syntac-
tic subordination that we wish to extract in a dependency tree.
(Bilexical models were also proposed by Eisner (Eisner, 1996)).
In the absence of lexicalization, both parsers have dependency
features that are encoded as head-constituent to sibling features.
6This information was provided by Eugene Charniak in a
personal communication.
44
D
en
si
ty
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
0.005 0.006 0.02
0.843
0.084
0.023 0.009 0.005 0.004
less ?2 ?1 1 2 3 4 5 more
D
en
si
ty
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
0.005 0.006 0.022
0.824
0.092
0.029 0.012 0.005 0.005
less ?2 ?1 1 2 3 4 5 more
(a)Charniak (b)Collins
Figure 2: Statistical distribution of correct governor positions in the Charniak (left) and Collins (right) parser output of parsed
PDT development data.
a non-projectivity is observed. The trees are then
transformed into Penn Treebank style constituen-
cies using the technique described in (Collins et al,
1999).
Both of these techniques have advantages and dis-
advantages which we briefly outline here:
Reordering The dependency structure is preserved,
but the training procedure will learn statistics
for structures over word-strings that may not be
part of the language. The parser, however, may
be capable of constructing parses for any string
of words if a smoothed grammar is being used.
Governor?Raising The dependency structure is
corrupted leading the parser to incorporate ar-
bitrary dependency statistics into the model.
However, the parser is trained on true sen-
tences, the words of which are in the correct
linear order. We expect the parser to predict
similar incorrect dependencies when sentences
similar to the training data are observed.
Although the results presented in (Collins et al,
1999) used the reordering technique, we have exper-
imented with his parser using the governor?raising
technique and observe an increase in dependency ac-
curacy. For the remainder of the paper, we assume
the governor?raising technique.
The process of generating dependency trees from
parsed constituency trees is relatively straight-
forward. Both the Collins and Charniak parsers pro-
vide head-word annotation on each constituent. This
is precisely the information that we encode in an un-
labeled dependency tree, so the dependency struc-
ture can simply be extracted from the parsed con-
stituency trees. Furthermore, the constituency labels
can be used to identify the dependency labels; how-
ever, we do not attempt to identify correct depen-
dency labels in this work.
3.2 Constituency-based errors
We now discuss a quantitative measure for the types
of dependency errors made by constituency-based
parsing techniques. For node wi and the correct gov-
ernor wg?
i
the distance between the two nodes in the
hypothesized dependency tree is:
dist(wi, wg?
i
)
=
?
?
?
?
?
d(wi, wg?
i
) iff wg?
i
is ancestor of wi
d(wi, wg?
i
) iff wg?
i
is sibling/cousin of wi
?d(wi, wg?
i
) iff wg?
i
is descendant of wi
Ancestor, sibling, cousin, and descendant have the
standard interpretation in the context of a tree. The
dependency distance d(wi, wg?
i
) is the minimum
number of dependency links traversed on the undi-
rected path from wi to wg?
i
in the hypothesized de-
pendency tree. The definition of the dist function
makes a distinction between paths through the par-
ent of wi (positive values) and paths through chil-
45
CORRECT(W )
1 Parse sentence W using the constituency-based parser
2 Generate a dependency structure from the constituency tree
3 for wi ? W
4 do for wc ? N (wgh
i
) // Local neighborhood of proposed governor
5 do l(c) ? P (g?i = c|wi,N (wgh
i
))
6 g?i ? arg maxc l(c) // Pick the governor in which we are most confident
Table 1: Corrective Modeling Procedure
dren of wi (negative values). We found that a vast
majority of the correct governors were actually hy-
pothesized as siblings or grandparents (a dist values
of 2) ? an extreme local error.
Figure 2 shows a histogram of the fraction of
nodes whose correct governor was within a particu-
lar dist in the hypothesized tree. A dist of 1 indicates
the correct governor was selected by the parser; in
these graphs, the density at dist = 1 (on the x axis)
shows the baseline dependency accuracy of each
parser. Note that if we repaired only the nodes that
are within a dist of 2 (grandparents and siblings),
we can recover more than 50% of the incorrect de-
pendency links (a raw accuracy improvement of up
to 9%). We believe this distribution to be indirectly
caused by the governor raising projectivization rou-
tine. In the cases where non-projective structures
can be repaired by raising the node?s governor to its
parent, the correct governor becomes a sibling of the
node.
4 Corrective Modeling
The error analysis of the previous section suggests
that by looking only at a local neighborhood of the
proposed governor in the hypothesized trees, we can
correct many of the incorrect dependencies. This
fact motivates the corrective modeling procedure
employed here.
Table 1 presents the pseudo-code for the correc-
tive procedure. The set gh contains the indices of
governors as predicted by the parser. The set of gov-
ernors predicted by the corrective procedure is de-
noted as g? . The procedure independently corrects
each node of the parsed trees meaning that there
is potential for inconsistent governor relationships
to exist in the proposed set; specifically, the result-
ing dependency graph may have cycles. We em-
ploy a greedy search to remove cycles when they are
present in the output graph.
The final line of the algorithm picks the governor
in which we are most confident. We use the correct-
governor classification likelihood,
P (g?i = j|wi,N (wgh
i
)), as a measure of the confi-
dence that wc is the correct governor of wi where
the parser had proposed wgh
i
as the governor. In ef-
fect, we create a decision list using the most likely
decision if we can (i.e., there are no cycles). If the
dependency graph resulting from the most likely de-
cisions does not result in a tree, we use the decision
lists to greedily select the tree for which the product
of the independent decisions is maximal.
Training the corrective model requires pairs of
dependency trees; each pair contains a manually-
annotated tree (i.e., the gold standard tree) and a tree
generated by the parser. This data is trivially trans-
formed into per-node samples. For each node wi in
the tree, there are |N (wgh
i
)| samples; one for each
governor candidate in the local neighborhood.
One advantage to the type of corrective algorithm
presented here is that it is completely disconnected
from the parser used to generate the tree hypotheses.
This means that the original parser need not be sta-
tistical or even constituency based. What is critical
for this technique to work is that the distribution of
dependency errors be relatively local as is the case
with the errors made by the Charniak and Collins
parsers. This can be determined via data analysis
using the dist metric. Determining the size of the lo-
cal neighborhood is data dependent. If subordinate
nodes are considered as candidate governors, then a
more robust cycle removal technique is be required.
46
4.1 MaxEnt Estimation
We have chosen a MaxEnt model to estimate the
governor distributions, P (g?i = j|wi,N (wgh
i
)). In
the next section we outline the feature set with which
we have experimented, noting that the features are
selected based on linguistic intuition (specifically
for Czech). We choose not to factor the feature vec-
tor as it is not clear what constitutes a reasonable
factorization of these features. For this reason we
use the MaxEnt estimator which provides us with
the flexibility to incorporate interdependent features
independently while still optimizing for likelihood.
The maximum entropy principle states that we
wish to find an estimate of p(y|x) ? C that maxi-
mizes the entropy over a sample set X for some set
of observations Y , where x ? X is an observation
and y ? Y is a outcome label assigned to that obser-
vation,
H(p) ? ?
?
x?X,y?Y
p?(x)p(y|x) log p(y|x)
The set C is the candidate set of distributions from
which we wish to select p(y|x). We define this set
as the p(y|x) that meets a feature-based expectation
constraint. Specifically, we want the expected count
of a feature, f(x, y), to be equivalent under the dis-
tribution p(y|x) and under the observed distribution
p?(y|x).
?
x?X,y?Y
p?(x)p(y|x)fi(x, y)
=
?
x?X,y?Y
p?(x)p?(y|x)fi(x, y)
fi(x, y) is a feature of our model with which we
capture correlations between observations and out-
comes. In the following section, we describe a set of
features with which we have experimented to deter-
mine when a word is likely to be the correct governor
of another word.
We incorporate the expected feature-count con-
straints into the maximum entropy objective using
Lagrange multipliers (additionally, constraints are
added to ensure the distributions p(y|x) are consis-
tent probability distributions):
H(p)
+
?
i
?i
?
x?X,y?Y
(
p?(x)p(y|x)fi(x, y)
?p?(x)p?(y|x)fi(x, y)
)
+ ?
?
y?Y
p(y|x) ? 1
Holding the ?i?s constant, we compute the uncon-
strained maximum of the above Lagrangian form:
p?(y|x) =
1
Z?(x)
exp(
?
i
?ifi(x, y))
Z?(x) =
?
y?Y
exp(
?
i
?ifi(x, y))
giving us the log-linear form of the distributions
p(y|x) in C (Z is a normalization constant). Finally,
we compute the ?i?s that maximize the objective
function:
?
?
x?X
p?(x) log Z?(x) +
?
i
?ip?(x, y)fi(x, y)
A number of algorithms have been proposed to ef-
ficiently compute the optimization described in this
derivation. For a more detailed introduction to max-
imum entropy estimation see (Berger et al, 1996).
4.2 Proposed Model
Given the above formulation of the MaxEnt estima-
tion procedure, we define features over pairs of ob-
servations and outcomes. In our case, the observa-
tions are simply wi, wc, and N (wgh
i
) and the out-
come is a binary variable indicating whether c = g?i
(i.e., wc is the correct governor). In order to limit
the dimensionality of the feature space, we consider
feature functions over the outcome, the current node
wi, the candidate governor node wc and the node
proposed as the governor by the parser wgh
i
.
Table 2 describes the general classes of features
used. We write Fi to indicate the form of the current
child node, Fc for the form of the candidate, and Fg
as the form of the governor proposed by the parser.
A combined feature is denoted as LiTc and indicates
we observed a particular lemma for the current node
with a particular tag of the candidate.
47
Feature Type Id Description
Form F the fully inflected word form as it appears in the data
Lemma L the morphologically reduced lemma
MTag T a subset of the morphological tag as described in (Collins et al, 1999)
POS P major part-of-speech tag (first field of the morphological tag)
ParserGov G true if candidate was proposed as governor by parser
ChildCount C the number of children
Agreement A(x, y) check for case/number agreement between word x and y
Table 2: Description of the classes of features used
In all models, we include features containing the
form, the lemma, the morphological tag, and the
ParserGov feature. We have experimented with dif-
ferent sets of feature combinations. Each combina-
tion set is intended to capture some intuitive linguis-
tic correlation. For example, the feature component
LiTc will fire if a particular child?s lemma Li is ob-
served with a particular candidate?s morphological
tag Tc. This feature is intended to capture phenom-
ena surrounding particles; for example, in Czech,
the governor of the reflexive particle se will likely
be a verb.
4.3 Related Work
Recent work by Nivre and Nilsson introduces a tech-
nique where the projectivization transformation is
encoded in the non-terminals of constituents dur-
ing parsing (Nivre and Nilsson, 2005). This al-
lows for a deterministic procedure that undoes the
projectivization in the generated parse trees, creat-
ing non-projective structures. This technique could
be incorporated into a statistical parsing frame-
work, however we believe the sparsity of such non-
projective configurations may be problematic when
using smoothed backed-off grammars. We suspect
that the deterministic procedure employed by Nivre
and Nilsson enables their parser to greedily consider
non-projective constructions when possible. This
may also explain the relatively low overall perfor-
mance of their parser.
A primary difference between the Nivre and Nils-
son approach and what we propose in this paper is
that of determining the projectivization procedure.
While we exploit particular side-effects of the pro-
jectivization procedure, we do not assume any par-
ticular algorithm. Additionally, we consider trans-
formations for all dependency errors where their
technique explicitly addresses non-projectivity er-
rors.
We mentioned above that our approach appears to
be similar to that of reranking for statistical parsing
(Collins, 2000; Charniak and Johnson, 2005). While
it is true that we are improving upon the output of the
automatic parser, we are not considering multiple al-
ternate parses. Instead, we consider a complete set
of alternate trees that are minimal perturbations of
the best tree generated by the parser. In the context
of dependency parsing, we do this in order to gen-
erate structures that constituency-based parsers are
incapable of generating (i.e., non-projectivities).
Recent work by Smith and Eisner (2005) on con-
trastive estimation suggests similar techniques to
generate local neighborhoods of a parse; however,
the purpose in their work is to define an approxi-
mation to the partition function for log-linear esti-
mation (i.e., the normalization factor in a MaxEnt
model).
5 Empirical Results
In this section we report results from experiments on
the PDT Czech dataset. Approximately 1.9% of the
words? dependencies are non-projective in version
1.0 of this corpus and these occur in 23.2% of the
sentences (Hajic?ova? et al, 2004). We used the stan-
dard training, development, and evaluation datasets
defined in the PDT documentation for all experi-
ments.7 We use Zhang Lee?s implementation of the
7We have used PDT 1.0 (2002) data for the Charniak experi-
ments and PDT 2.0 (2005) data for the Collins experiments. We
use the most recent version of each parser; however we do not
have a training program for the Charniak parser and have used
the pretrained parser provided by Charniak; this was trained on
the training section of the PDT 1.0. We train our model on the
48
Model Features Description
Count ChildCount count of children for the three nodes
MTagL TiTc, LiLc, LiTc, TiLc, TiPg conjunctions of MTag and Lemmas
MTagF TiTc, FiFc, FiTc, TiFc, TiPg conjunctions of MTag and Forms
POSL Pi, Pc, Pg, PiPcPg, PiPg, PcLc conjunctions of POS and Lemma
TTT TiTcTg conjunction of tags for each of the three nodes
Agr A(Ti, Tc), A(Ti, Tg) binary feature if case/number agree
Trig LiLgTc, TiLgTc, LiLgLc trigrams of Lemma/Tag
Table 3: Model feature descriptions.
Model Charniak Parse Trees Collins Parse Trees
Devel. Accuracy NonP Accuracy Devel. Accuracy NonP Accuracy
Baseline 84.3% 15.9% 82.4% 12.0%
Simple 84.3% 16.0% 82.5% 12.2%
Simple + Count 84.3% 16.7% 82.5% 13.8%
Simple + MtagL 84.8% 43.5% 83.2% 44.1%
Simple + MtagF 84.8% 42.2% 83.2% 43.2%
Simple + POS 84.3% 16.0% 82.4% 12.1%
Simple + TTT 84.3% 16.0% 82.5% 12.2%
Simple + Agr 84.3% 16.2% 82.5% 12.2%
Simple + Trig 84.9% 47.9% 83.1% 47.7%
All Features 85.0% 51.9% 83.5% 57.5%
Table 4: Comparative results for different versions of our model on the Charniak and Collins parse trees for
the PDT development data.
MaxEnt estimator using the L-BFGS optimization
algorithms and Gaussian smoothing.8
Table 4 presents results on development data for
the correction model with different feature sets. The
features of the Simple model are the form (F),
lemma (L), and morphological tag (M) for the each
node, the parser-proposed governor node, and the
candidate node; this model also contains the Parser-
Gov feature. In the table?s following rows, we show
the results for the simple model augmented with fea-
ture sets of the categories described in Table 2. Ta-
ble 3 provides a short description of each of the mod-
els. As we believe the Simple model provides the
minimum information needed to perform this task,
Collins trees via a 20-fold Jackknife training procedure.
8Using held-out development data, we determined a Gaus-
sian prior parameter setting of 4 worked best. The optimal num-
ber of training iterations was chosen on held-out data for each
experiment. This was generally in the order of a couple hun-
dred iterations of L-BFGS. The MaxEnt modeling implemen-
tation can be found at http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html.
we experimented with the feature-classes as addi-
tions to it. The final row of Table 4 contains results
for the model which includes all features from all
other models.
We define NonP Accuracy as the accuracy for
the nodes which were non-projective in the original
trees. Although both the Charniak and the Collins
parser can never produce non-projective trees, the
baseline NonP accuracy is greater than zero. This
is due to the parser making mistakes in the tree such
that the originally non-projective node?s dependency
is projective.
Alternatively, we report the Non-Projective Preci-
sion and Recall for our experiment suite in Table 5.
Here the numerator of the precision is the number
of nodes that are non-projective in the correct tree
and end up in a non-projective configuration; how-
ever, this new configuration may be based on incor-
rect dependencies. Recall is the obvious counterpart
to precision. These values correspond to the NonP
49
Model Charniak Parse Trees Collins Parse Trees
Precision Recall F-measure Precision Recall F-measure
Baseline N/A 0.0% 0.000 N/A 0.0% 0.000
Simple 22.6% 0.3% 0.592 5.0% 0.2% 0.385
Simple + Count 37.3% 1.1% 2.137 16.8% 2.0% 3.574
Simple + MtagL 78.0% 29.7% 43.020 62.4% 35.0% 44.846
Simple + MtagF 78.7% 28.6% 41.953 62.0% 34.3% 44.166
Simple + POS 23.3% 0.3% 0.592 2.5% 0.1% 0.192
Simple + TTT 20.7% 0.3% 0.591 6.1% 0.2% 0.387
Simple + Agr 40.0% 0.5% 0.988 5.7% 0.2% 0.386
Simple + Trig 74.6% 35.0% 47.646 52.3% 40.2% 45.459
All Features 75.7% 39.0% 51.479 48.1% 51.6% 49.789
Table 5: Alternative non-projectivity scores for different versions of our model on the Charniak and Collins
parse trees.
accuracy results reported in Table 4. From these ta-
bles, we see that the most effective features (when
used in isolation) are the conjunctive MTag/Lemma,
MTag/Form, and Trigram MTag/Lemma features.
Model Dependency NonP
Accuracy Accuracy
Collins 81.6% N/A
Collins + Corrective 82.8% 53.1%
Charniak 84.4% N/A
Charniak + Corrective 85.1% 53.9%
Table 6: Final results on PDT evaluation datasets
for Collins? and Charniak?s trees with and without
the corrective model
Finally, Table 6 shows the results of the full model
run on the evaluation data for the Collins and Char-
niak parse trees. It appears that the Charniak parser
fares better on the evaluation data than does the
Collins parser. However, the corrective model is
still successful at recovering non-projective struc-
tures. Overall, we see a significant improvement in
the dependency accuracy.
We have performed a review of the errors that
the corrective process makes and observed that the
model does a poor job dealing with punctuation.
This is shown in Table 7 along with other types of
nodes on which we performed well and poorly, re-
spectively. Collins (1999) explicitly added features
to his parser to improve punctuation dependency
parsing accuracy. The PARSEVAL evaluation met-
Top Five Good/Bad Repairs
Well repaired child se i si az? jen
Well repaired false governor v vs?ak li na o
Well repaired real governor a je sta?t ba ,
Poorly repaired child , se na z?e -
Poorly repaired false governor a , vs?ak mus?? li
Poorly repaired real governor root sklo , je -
Table 7: Categorization of corrections and errors
made by our model on trees from the Charniak
parser. root is the artificial root node of the PDT
tree. For each node position (child, proposed parent,
and correct parent), the top five words are reported
(based on absolute count of occurrences). The par-
ticle ?se? occurs frequently explaining why it occurs
in the top five good and top five bad repairs.
Charniak Collins
Correct to incorrect 13.0% 20.0%
Incorrect to incorrect 21.6% 25.8%
Incorrect to correct 65.5% 54.1%
Table 8: Categorization of corrections made by our
model on Charniak and Collins trees.
ric for constituency-based parsing explicitly ignores
punctuation in determining the correct boundaries of
constituents (Harrison et al, 1991) and so should the
dependency evaluation. However, the reported re-
sults include punctuation for comparative purposes.
Finally, we show in Table 8 a coarse analysis of the
corrective performance of our model. We are repair-
50
ing more dependencies than we are corrupting.
6 Conclusion
We have presented a Maximum Entropy-based cor-
rective model for dependency parsing. The goal is
to recover non-projective dependency structures that
are lost when using state-of-the-art constituency-
based parsers; we show that our technique recovers
over 50% of these dependencies. Our algorithm pro-
vides a simple framework for corrective modeling
of dependency trees, making no prior assumptions
about the trees. However, in the current model, we
focus on trees with local errors. Overall, our tech-
nique improves dependency parsing and provides
the necessary mechanism to recover non-projective
structures.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and
Barbora Vidova? Hladka?. 2002. The prague depen-
dency treebank: Three-level annotation scenario. In
Anne Abeille, editor, In Treebanks: Building and
Using Syntactically Annotated Corpora. Dordrecht,
Kluwer Academic Publishers, The Neterlands.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565, December.
Sharon Caraballo and Eugene Charniak. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
Computational Linguistics, 24(2):275?298, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics.
Michael Collins, Lance Ramshaw, Jan Hajic?, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th annual meeting of
the Association for Computational Linguistics, pages
505?512.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 17th In-
ternational Conference on Machine Learning 2000.
Michael Collins. 2003. Head-driven statistical models
for natural language processing. Computational Lin-
guistics, 29(4):589?637.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Sapporo.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pages 340?345.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova? Hladka?.
2005. The prague dependency treebank 2.0.
http://ufal.mff.cuni.cz/pdt2.0.
51
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The prague dependency treebank. In Issues
of Valency and Meaning, pages 106?132. Karolinum,
Praha.
Eva Hajic?ova?, Jir??? Havelka, Petr Sgall, Kater?ina Vesela?,
and Daniel Zeman. 2004. Issues of projectivity in
the prague dependency treebank. Prague Bulletin of
Mathematical Linguistics, 81:5?22.
P. Harrison, S. Abney, D. Fleckenger, C. Gdaniec, R. Gr-
ishman, D. Hindle, B. Ingria, M. Marcus, B. Santorini,
, and T. Strzalkowski. 1991. Evaluating syntax perfor-
mance of parser/grammars of english. In Proceedings
of the Workshop on Evaluating Natural Language Pro-
cessing Systems, ACL.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Factored
A* search for models over sequences and trees. In
Proceedings of IJCAI 2003.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Cor-
recting the surface dependency approximation. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 327?334,
Barcelona, Spain.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany, NY.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 99?106, Ann Arbor.
Brian Roark and Michael Collins. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics, Barcelona.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. Kluwer Academic, Boston.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the Association for Computational
Linguistics (ACL 2005), Ann Arbor, Michigan.
Daniel Zeman. 2004. Parsing with a statistical de-
pendency model. Ph.D. thesis, Charles University in
Prague.
52
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 390?398,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Corrective Models for Speech Recognition of Inflected Languages
Izhak Shafran and Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{zakshafran,keith hall}@jhu.edu
Abstract
This paper presents a corrective model
for speech recognition of inflected lan-
guages. The model, based on a discrim-
inative framework, incorporates word n-
grams features as well as factored mor-
phological features, providing error reduc-
tion over the model based solely on word
n-gram features. Experiments on a large
vocabulary task, namely the Czech portion
of the MALACH corpus, demonstrate per-
formance gain of about 1.1?1.5% absolute
in word error rate, wherein morphologi-
cal features contribute about a third of the
improvement. A simple feature selection
mechanism based on ?2 statistics is shown
to be effective in reducing the number of
features by about 70% without any loss in
performance, making it feasible to explore
yet larger feature spaces.
1 Introduction
N -gram models have long been the stronghold of
statistical language modeling approaches. Within
the n-gram paradigm, straightforward approaches
for increasing accuracy include using larger train-
ing sets and augmenting the contextual informa-
tion within the n-gram window. Incorporating
syntactic features into the context has been at the
forefront of recent research (Collins et al, 2005;
Rosenfeld et al, 2001; Chelba and Jelinek, 2000;
Hall and Johnson, 2004). However, much of the
previous work has focused on English language
syntax. This paper addresses syntax as captured
by the inflectional morphology of highly inflected
language.
High inflection in a language is generally cor-
related with some level of word-order flexibil-
ity. Morphological features either directly identify
or help disambiguate the syntactic participants of
a sentence. Inflectional morphology works as a
proxy for structured syntax in a language. Model-
ing morphological features in these languages not
only provides an additional source of information
but can also alleviate data sparsity problems.
Czech speech recognition needs to deal with
two sources of errors which are absent in En-
glish, namely, the inflectional morphology and the
differences in the formal (written) and colloquial
(spoken) forms. Table 1 presents an example out-
put of our speech recognizer on an utterance from
a Holocaust survivor, who is recounting General
Romel?s desert campaign during the Second World
War. In this example, the feminine past-tense
form of the Czech verb for to be is chosen mis-
takenly, which is followed by a sequence of in-
correct words chosen primarily to maintain agree-
ment with the feminine form of the verb. This is
an example of what we refer to as the morpho-
logical grouping effect. When the acoustic model
prefers a word with an incorrect inflection, the lan-
guage model effectively propagates the error to
later words. A language model based on word-
forms prefers sequences observed in the training
data, which will implicitly force an agreement
with the inflections of preceding words, making it
difficult to stop propagating errors. Although this
analysis is anecdotal in nature, the grouping effect
appears to be prevalent in the Czech dataset used
in this work. The proposed corrective model with
morphological features is expected to alleviate the
grouping effect as well as to improve the recogni-
tion of inflected languages in general.
In the following section, we present a brief
review of related work on morphological lan-
guage modeling and discriminative language mod-
390
REF no Jez???s? to uz? byl Romel hnedle pr?ed Alexandri??
gloss well Jesus by that time already was Romel just in front of Alexandria
translation oh Jesus, Romel was already just in front of Alexandria by that time
HYP no Jez???s? to uz? byla sama hned leps??? Alexandrie
gloss well Jesus by that time already (she) was herself just better Alexandria
translation oh Jesus, she was herself just better Alexandria by that time
Table 1: An example of the grouping effect. The incorrect form of the verb to be begins a group of
incorrect words in the hypothesis, but these words agree in their morphological inflection.
els. We begin the description of our work in sec-
tion 3 with the type of morphological features
modeled as well as their computation from the out-
put word-lattices of a speech recognizer. Section 4
presents the corrective model and the training ap-
proach explored in the current work. A simple and
effective feature selection mechanism is described
in section 5. In section 6, the proposed framework
is evaluated on a large vocabulary Czech speech
recognition task. Results show that the morpho-
logical features provide a significant improvement
over models lacking these features; subsequently,
two different analyses are provided to understand
the contribution of different morphological fea-
tures.
2 Related Work
It has long been assumed that incorporating mor-
phological features into a language models should
help improve the performance of speech recogni-
tion systems. Early models for German showed
little improvements over bigram language mod-
els and almost no improvement over trigram mod-
els (Geutner, 1995). More recently, morphology-
based models have been shown to help reduce er-
ror rate for out-of-vocabulary words (Carki et al,
2000; Podvesky and Machek, 2005).
Much of the early work on morphological lan-
guage modeling was focused on utilizing compos-
ite morphological tags, largely due to the difficulty
in teasing apart the intricate interdependencies of
the morphological features. Apart from a few ex-
ceptions, there has been little work done in explor-
ing the morphological systems of highly inflected
languages.
Kirchhoff and colleagues (2004) successfully
incorporated morphological features for Arabic
using a factored language model. In their ap-
proach, morphological inflections are modeled in
a generative framework, and the space of factored
morphological tags is explored using a genetic al-
gorithm.
Adopting a different tactic, Choueiter and
colleagues (2006) exploited morphological con-
straints to prune illegal morpheme sequences from
ASR output. They noticed that the gains obtained
from the application of such constraints in Arabic
depends on the size of the vocabulary ? an absolute
gain of 2.4% in word error rate (WER) reduced
to 0.2% when the size was increased from 64k to
800k.
Our approach to modeling morphology differs
from that of Vergyri et al (2004) and Choueiter et
al. (2006). By choosing a discriminative frame-
work and maximum entropy based estimation, we
allow arbitrary features or constraints and their
combinations without the need for explicit elab-
oration of the factored space and its backoff ar-
chitecture. Thus, morphological features can be
incorporated in the absence of knowledge about
their interdependencies.
Several researchers have investigated tech-
niques for improving automatic speech recogni-
tion (ASR) results by modeling the errors (Collins
et al, 2005; Shafran and Byrne, 2004). Collins
et al (2005) present a corrective language model
based on a discriminative framework. Initially, a
set of hypotheses is generated by a baseline de-
coder with standard acoustic and language models.
A corrective model is estimated such that it scores
desired or oracle hypotheses higher than compet-
ing hypotheses. The parameters are learned via
the perceptron algorithm which shifts weight away
from features associated with poor hypotheses and
towards those associated with better hypotheses.
By the appropriate choice of desired hypotheses,
the model parameters can be estimated to mini-
mize WER in speech recognition. During decod-
ing, the model can then be used to rerank a set
of hypotheses, and hence, it is also known as a
reranking framework. This paradigm allows mod-
eling arbitrary input features, even syntactic fea-
tures obtained from a parser. We adopt a vari-
ant of this framework where the corrective model
is based on a conditional model estimated by the
maximum entropy procedure (Charniak and John-
391
son, 2005) and we investigate its effectiveness in
modeling morphological features for highly in-
flected languages, in particular, Czech.
3 Inflectional Morphology
Inflectional abundance in a language generally
corresponds to some flexibility in word order. In
a free word-order language, the order of senten-
tial participants is relatively unconstrained. This
does not mean a speaker of the language can ar-
bitrarily choose an order. Word-order choice may
change the semantic and/or pragmatic interpreta-
tion of an utterance. Czech is known as a free
word-order language allowing for subject, object,
and verbal components to come in any order. Mor-
phological inflection in these languages must in-
clude a syntactic case marker to allow the determi-
nation of which participants are subjects (nomina-
tive case), objects (accusative or dative) and other
such entities. Additionally, morphological inflec-
tion encodes features such as gender and number.
The agreement of these features between senten-
tial components (adjectives with nouns, subjects
with verbs, etc.) may further disambiguate the tar-
get of a modifier (e.g., identifying the noun that is
modified by a particular adjective).
The increased flexibility in word order aggra-
vates the data sparsity of standard n-gram lan-
guage model for two reasons: first, the number of
valid configurations of a group of words increases
with the free order; and second, lexical items are
decorated with the inflectional morphemes, multi-
plying the number of word-forms that appear.
In addition to modeling sequences of word-
forms, we model sequences of morphologically
reduced lemmas, sequence of morphological tags
and sequences of various factored representations
of the morphological tags. Factoring a word
into the semantics-bearing lemma and syntax-
bearing morphological tag alleviates the data spar-
sity problem to some extent. However, the number
of possible factorizations of n-grams is large. The
approach adopted in this work is to provide a rich
class of features and defer the modeling of their
interaction to the learning procedure.
3.1 Extracting Morphological Features
The extraction of reliable morphological features
critically effects further morphological modeling.
Here, we first select the most likely morphologi-
cal analysis for each word using a morphological
Label Description # Values
lemma Reduced lexeme < |vocab|
POS Coarse part-of-speech 12
D-POS Detailed part-of-speech 65
gen Grammatical Gender 10
num Grammatical Number 5
case Grammatical Case 8
Table 2: Czech morphological features used in the
current work. The # Values field indicates the size
of the closed set of possible values. Not all values
are used in the annotated data.
tagger. In particular, we use the Czech feature-
based tagger distributed with the Prague Depen-
dency Treebank (Hajic? et al, 2005). The tagger is
based on a morphological analyzer which uses a
lexicon and a rule-based tag guesser for words not
found in the lexicon. Trained by the maximum en-
tropy procedure, the tagger uses left and right con-
textual features from the input string. Currently,
this is the best available Czech-language tagger.
See Hajic? and Vidova?-Hladka? (1998) for further
details on the tagger.
A disadvantage of such an approach is that
the tagger works on strings rather than the word-
lattices that we expect from an ASR system.
Therefore, we must extract a set of strings from the
lattices prior to tagging. An alternative approach is
to hypothesize all morphological analyses for each
word in the lattice, thereby considering the entire
set of analyses as features in the model. In the cur-
rent implementation we have chosen to use a tag-
ger to reduce the complexity of the model by lim-
iting the number of active features while still ob-
taining relatively reliable features. Moreover, sys-
tematic errors in tagging can be potentially com-
pensated by the corrective model.
The initial stage of feature extraction begins
with an analysis of the data on which we train and
test our models. The process follows:
1. Extract the n-best hypotheses according to a
baseline model, where n varies from 50 to
1000 in the current work.
2. Tag each of the hypotheses with the morpho-
logical tagger.
3. Re-encode the original word strings along
with their tagged morphological analysis in
a weighted finite state transducer to allow
392
Word-form to obdob?? bylo pome?rne? kra?tke?
gloss that period was relatively short
lemma ten obdob?? by?t pome?rne? kra?tky?
tag PDNS1 NNNS1 VpNS- Dg? AAFS2
Table 3: A morphological analysis of Czech. This analyses was generated by the Hajic? tagger.
form to obdob?? bylo pome?rne? kra?tke?
to obdob?? obdob?? bylo bylo pome?rne? pome?rne? kra?tke?
lemma ten obdob?? by?t pome?rne? kra?tky?
ten obdob?? obdob?? by?t by?t pome?rne? pome?rne? kra?tky?
tag PDNS1 NNNS1 VpNS- Dg? AAFS2
PDNS1 NNNS1 NNNS1 VpNS- VpNS- Dg? Dg? AAFS2
POS P N V D A
P N N V V D D A
. . . . . .
case 1 1 - - 2
1 1 1 - - 0 - 2
num/case S1 S1 S- ? S2
S1 S1 S1 S- S- ? ? S2
. . . . . .
Table 4: Examples of the n-grams extracted from the Czech sentence To obdob?? bylo pome?rne? kra?tke?. A
subset of the feature classes is presented here. The morphological feature values are those assigned by
the Hajic? tagger.
an efficient means of projecting the hypothe-
ses from word-form to morphology and vice
versa.
4. Extract appropriately factored n-gram fea-
tures for each hypothesis as described below.
Each word state in the original lattice has an
associated lemma/tag from which a variety of n-
gram features can be extracted.
From the morphological features assigned by
the tagger, we chose to retain only a subset and dis-
card the less reliable features which are semantic
in nature. The basic morphological features used
are detailed in Table 2. In the tag-based model, a
string of 5 characters representing the 5 morpho-
logical fields is used as a unique identifier. The
derived features include n-grams of POS, D-POS,
gender (gen), number (num), and case features as
well as their combinations.
POS, D-POS Captures the sub-categorization of
the part-of-speech tags.
gen, num Captures complex gender-number
agreement features.
num, case Captures number agreement between
specific case markers.
POS, case Captures associated POS/Case fea-
tures (e.g., adjectives associated with nomi-
native elements).
The paired features allow for complex inflec-
tional interactions and are less sparse than the
composite 5-component morphological tags. Ad-
ditionally, the morphologically reduced lemma
and n-grams of lemmas are used as features in the
models.
Table 3 presents a morphological analysis of the
Czech sentence To obdob?? bylo pome?rne? kra?tke?.
The encoded tags represent the first 5 fields of the
Prague Dependency Treebank morphological en-
coding and correspond to the last 5 rows of Ta-
ble 2. Features for this sentence include the word-
form, lemma, and composite tag features as well
as the components of each tag and the above men-
tioned concatenation of tag fields. Additionally,
n-grams of each of these features are included. Bi-
gram features extracted from an example sentence
are illustrated in Table 4.
The following section describes how the fea-
393
tures extracted above are modeled in a discrimi-
native framework to reduce word error rate.
4 Corrective Model and Estimation
In this work, we adopt the reranking framework
of Charniak and Johnson (2005) for incorporating
morphological features. The model scores each
test hypothesis y using a linear function, v?(y), of
features extracted from the hypothesis fj(y) and
model parameters ?j , i.e., v?(y) =
?
j ?jfj(y).
The hypothesis with the highest score is then cho-
sen as the output.
The model parameters, ?, are learned from a
training set by maximum entropy estimation of the
following conditional model:
?
s
?
yi?Ys:g(yi)=maxjg(yj)
P?(yi|Ys)
Here, Ys = {yj} is the set of hypotheses for each
training utterance s and the function g returns an
extrinsic evaluation score, which in our case is
the WER of the hypothesis. P?(yi|Ys) is modeled
by a maximum entropy distribution of the form,
P?(yi|Ys) = exp v?(yi)/
?
j exp v?(yj). This
choice simplifies the numerical estimation proce-
dure since the gradient of the log-likelihood with
respect to a parameter, say ?j , reduces to differ-
ence in expected counts of the associated feature,
E?[fj |Ys]?E?[fj |yi ? Ys : g(yi) = maxjg(yj)].
To allow good generalization properties, a Gaus-
sian regularization term is also included in the cost
function.
A set of hypotheses Ys is generated for each
training utterance using a baseline ASR system.
Care is taken to reduce the bias in decoding the
training set by following a jack-knife procedure.
The training set is divided into 20 subsets and each
subset is decoded after excluding the transcripts
of that subset from the language model of the de-
coder.
The model allows the exploration of a large fea-
ture space, including n-grams of words, morpho-
logical tags, and factored tags. In a large vocab-
ulary system, this could be an enormous space.
However, in a discriminative maximum entropy
framework, only the observed features are consid-
ered. Among the observed features, those associ-
ated with words that are correct in all hypotheses
do not provide any additional discrimination ca-
pability. Mathematically, the gradient of the log-
likelihood with respect to the parameters of these
features tends to zero and they may be discarded.
Additionally, the parameters associated with fea-
tures that are rarely observed in the training set are
difficult to learn reliably and may be discarded.
To avoid redundant features, we focus on words
which are frequently incorrect; this is the error re-
gion we aim to model. In the training utterance,
the error regions of a hypothesis are identified us-
ing the alignment corresponding to the minimum
edit distance from the reference, akin to comput-
ing word error rate. To mark all the error regions in
an ASR lattice, the minimum edit distance align-
ment is obtained using equivalent finite state ma-
chine operations (Mohri, 2002). From amongst all
the error regions in the training lattices, the most
frequent 12k words in error are shortlisted. Fea-
tures are computed in the corrective model only if
they involve words for the shortlist. The parame-
ters, ?, are estimated by numerical optimization as
in (Charniak and Johnson, 2005).
5 Feature Selection
The space of features spanned by the cross-
product space of words, lemmas, tags, factored-
tags and their n-gram can potentially be over-
whelming. However, not all of these features
are equally important and many of the features
may not have a significant impact on the word
error rate. The maximum entropy framework af-
fords the luxury of discarding such irrelevant fea-
tures without much bookkeeping, unlike maxi-
mum likelihood models. In the context of mod-
eling morphological features, we investigate the
efficacy of simple feature selection based on the
?2 statistics, which has been shown to effective
in certain text categorization problems. e.g. (Yang
and Pedersen, 1997).
The ?2 statistics measures the lack of indepen-
dence by computing the deviation of the observed
counts Oi from the expected counts Ei.
?2 =
?
i
(Oi ? Ei)
2/Ei
In our case, there are two classes ? oracle hy-
potheses c and competing hypotheses c?. The
expected count is the count marginalized over
classes.
?2(f, c) =
(P (f, c)? P (f))2
P (f)
+
(P (f, c?)? P (f))2
P (f)
+
(P (f? , c)? P (f?))2
P (f?)
+
(P (f? , c?)? P (f?))2
P (f?)
394
This can be simplified using a two-way contin-
gency table of feature and class, where A is the
number of times f and c co-occur, B is the num-
ber of times f occurs without c, C is the number
of times c occurs without f , and D is the number
of times neither f nor c occurs, and N is the total
number of examples. Then, the ?2 is defined to
be:
?2(f, c) =
N ? (AD ? CB)2
(A+ C)? (B +D)? (A+B)? (C +D)
The ?2 statistics are computed for all the fea-
tures and the features with larger value are re-
tained. Alternatives feature selection mechanisms
such as those based on mutual information and in-
formation gain are less reliable than ?2 statistics
for heavy-tailed distributions. More complex fea-
ture selection mechanism would entail computing
higher order interaction between features which is
computationally expensive and so is not explored
in this work.
6 Empirical Evaluation
The corrective model presented in this work is
evaluated on a large vocabulary task consisting
of spontaneous spoken testimonies in Czech lan-
guage, which is a subset of the multilingual
MALACH corpus (Psutka et al, 2003).
6.1 Task
For acoustic model training, transcripts are avail-
able for about 62 hours of speech from 336 speak-
ers, amounting to 507k spoken words from a vo-
cabulary of 79k. A portion of this data containing
speech from 44 speakers, about 21k words in all
is treated as development set (dev). The test set
(eval) consists of about 2 hours of speech from 10
new speakers and contains about 15k words.
6.2 Baseline ASR System
The baseline ASR system uses perceptual linear
prediction (PLP) features which is computed on
44KHz input speech at the rate of 10 frames per
second, and is normalized to have zero mean and
unit variance per speaker. The acoustic models are
made of 3-state HMM triphones, whose observa-
tion distributions are clustered into about 4500 al-
lophonic (triphone) states. Each state is modeled
by a 16 component Gaussian mixture with diag-
onal covariances. The parameters of the acoustic
models are initially estimated by maximum likeli-
hood and then refined by five iterations of maxi-
mum mutual information estimation (MMI).
Unlike other comparable corpora, this corpus
contains a relatively high percentage of colloquial
words ? about 9% of the vocabulary and 7% of the
tokens. For the sake of downstream application,
the colloquial variants are subsumed in the lexi-
con. As a result, common words contain several
pronunciation variants, and a few have as many as
14 variants.
For the first pass decoding, a language model
was created by interpolating the in-domain model
(weight=0.75), estimated from 600k words of
transcripts with an out-of-domain model, esti-
mated from 15M words of Czech National Cor-
pus (Psutka et al, 2003). Both models are param-
eterized by a trigram language model with Katz
back-off. The decoding graph was built by com-
posing the language model, the lexical transducer
and the context-dependent transducer (phones to
triphones) into a single compact finite state ma-
chine.
The baseline ASR system decodes test utter-
ance in two passes. A first pass decoding is per-
formed with MMIE acoustic models, whose out-
put transcripts are bootstrapped to estimate two
maximum likelihood linear regression transforms
for each speaker using five iterations. A second
pass decoding is then performed with the new
speaker adapted acoustic models. The resulting
performance is given in Table 5. The performance
reflects the difficulty of transcribing spontaneous
speech from the elderly speakers whose speech is
also heavily accented and emotional in this corpus.
1-best 1000-best
Dev 29.9 21.5
Eval 35.9 22.4
Table 5: The performance of the baseline ASR
system is reported, showing the word error rate
of 1-best MAP hypothesis and the oracle in 1000-
best hypotheses for dev and eval sets.
6.3 Experiments With Morphology
We present a set of contrastive experiments to
gauge the performance of the corrective models
and the contribution of morphological features.
For training the corrective models, 50 best hy-
potheses are generated for each utterance using the
395
 
28.6
 
28.8 29
 
29.2
 
29.4
 
29.6
 
29.8 30  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
WER
Fract
ion of
 featu
res us
ed
?base
line?
word 
n-gram
+ m
orph n
-gram
 
34.2
 
34.4
 
34.6
 
34.8 35
 
35.2
 
35.4
 
35.6
 
35.8 36  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
WER
Fract
ion of
 featu
res us
ed
?base
line?
word 
n-gram
+ m
orph n
-gram
(a)Devel (b)Eval
Figure 1: Feature selection via ?2 statistics helps reduce the number of parameters by 70% without any
loss in performance, as observed in dev (a) and eval (b) sets.
jack-knife procedure mentioned earlier. For each
hypothesis, bigram and unigram features are com-
puted which consist of word-forms, lemmas, mor-
phologoical tags, factored morphological tags, and
the likelihood from the baseline ASR system. For
testing, the baseline ASR system is used to gener-
ate 1000 best hypotheses for each utterance. These
are then evaluated using the corrective models and
the best scored hypothesis is chosen as the output.
Table 6 summarizes the results on two test sets
? the dev and the eval set. A corrective model with
word bigram features improve the word error rate
by about an absolute 1% over the baseline. Mor-
phological features provide a further gain on both
the test sets consistently.
Features Dev Eval
Baseline 29.9 35.9
Word bigram 29.0 34.8
+ Morph bigram 28.7 34.4
Table 6: The word error rate of the corrective
model is compared with that of the baseline ASR
system, illustrating the improvement in perfor-
mance with morphological features.
The gains on the dev set are significant at the
level of p < 0.001 for three standard NIST tests,
namely, matched pair sentence segment, signed
pair comparison, and Wilcoxon signed rank tests.
For the smaller eval set the significant levels were
lower for morphological features. The relative
gains observed are consistent over a variety of con-
ditions that we have tested including the ones re-
ported below.
Subsequently, we investigated the impact of re-
ducing the number of features using ?2 statistics,
as described in section 5. The experiments with
bigram features of word-forms and morphology
were repeated using reduced feature sets, and the
performance was measured at 10%, 30% and 60%
of their original features. The results, as illustrated
in Figure 1, show that the word error rate does not
change significantly even after the number of fea-
tures are reduced by 70%. We have also observed
that most of the gain can be achieved by evalu-
ating 200 best hypotheses from the baseline ASR
system, which could further reduce the computa-
tional cost for time-sensitive applications.
6.4 Analysis of Feature Classes
The impact of feature classes can be analyzed by
excluding all features from a particular class and
evaluating the performance of the resulting model
without re-estimation. Figure 2 illustrates the ef-
fectiveness of different features class. The y-axis
shows the gain in F-score, which is monotonic
with the word error rate, on the entire develop-
ment dataset. In this analysis, the likelihood score
from the baseline ASR system was omitted since
our interest is in understanding the effectiveness
of categorical features such as words, lemmas and
tags.
The most independently influential feature class
is the factored tag features. This corresponds with
396
-0.00100.001
0.0020.0030.004
0.005
TNG#1 TNG#2 LNG#2 FNG#2 TFAC#1 LNG#1 FNG#1 TFAC#2
Figure 2: Analysis of features classes for a bigram
form, lemma, tag, and factored tag model. Y -axis
is the contribution of this feature if added to an
otherwise complete model. Feature classes are la-
beled: TNG ? tag n-gram, LNG ? lemma n-gram,
FNG ? form n-gram and TFAC ? factored tag n-
grams. The number following the # represents the
order of the n-gram.
our belief that modeling morphological features
requires detailed models of the morphology; in
this model the composite morphological tag n-
gram features (TNG) offer little contribution in the
presence of the factored features.
Analysis of feature reduction by the ?2 statistics
reveals a similar story. When features are ranked
according to their ?2 statistics, about 57% of the
factored tag n-grams occur in the top 10% while
only 7% of the word n-grams make it. The lemma
and composite tag n-grams give about 6.2% and
19.2% respectively. Once again, the factored tag
is the most influential feature class.
7 Conclusion
We have proposed a corrective modeling frame-
work for incorporating inflectional morphology
into a discriminative language model. Empirical
results on a difficult Czech speech recognition task
support our claim that morphology can help im-
prove speech recognition results for these types of
languages. Additionally, we present a feature se-
lection method that effectively reduces the model
size by about 70% while having little or no im-
pact on recognition accuracy. Model size reduc-
tion greatly reduces training time which can often
be prohibitively expensive for maximum entropy
training.
Analysis of the models learned on our task show
that factored morphological tags along with word-
forms provide most of the discriminative power;
and, in the presence of these features, composite
morphological tags are of little use.
The corrective model outlined here operates on
the word lattices produced by an ASR system. The
morphological tags are inferred from the word se-
quences in the lattice. Alternatively, by employ-
ing an ASR system that models the morphological
constraints in the acoustics as in (Chung and Sen-
eff, 1999), the corrective model could be applied
directly to a lattice with morphological tags.
When dealing with ASR word lattices, the ef-
ficacy of the proposed feature selection mecha-
nism can be exploited to eliminate the intermedi-
ate tagger, a potential source of errors. Instead of
considering the best morphological analysis, the
model could consider all possible analyses of the
words. Further, the feature space could be en-
riched with syntactic features which are known to
be useful (Collins et al, 2005). The task of mod-
eling is then tackled by feature selection and the
maximum entropy training procedure.
8 Acknowledgements
The authors would like to thank William Byrne for
discussions on modeling aspects, and Jan Hajic?,
Petr Ne?mec, and Vaclav Nova?k for discussions
regarding Czech morphology and tagging. This
work was supported by the NSF (U.S.A) under the
Information Technology Research (ITR) program,
NSF IIS Award No. 0122466.
References
Kenan Carki, Petra Geutner, and Tanja Schultz. 2000.
Turkish LVCSR: towards better speech recognition
for agglutinative languages. In Proceedings of the
2000 IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 3688?3691.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14(4):283?332.
Ghinwa Choueiter, Daniel Povey, Stanley Chen, and
Geoffrey Zweig. 2006. Morpheme-based language
modeling for Arabic LVCSR. In Proceedings of the
2006 IEEE International Conference on Acoustics,
Speech, and Signal Processing, Toulouse, France.
Grace Chung and Stephanie Seneff. 1999. A hierar-
chical duration model for speech recognition based
397
on the ANGIE framework. Speech Communication,
27:113?134.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling
for speech recognition. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 507?514, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Petra Geutner. 1995. Using morphology towards bet-
ter large-vocabulary speech recognition systems. In
Proceedings of the 1995 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 445?448, Detroit, MI.
Jan Hajic? and Barbora Vidova?-Hladka?. 1998. Tagging
inflective languages: Prediction of morphological
categories for a rich, structured tagset. In Proceed-
ings of the COLING-ACL Conference, pages 483?
490, Montreal, Canada.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova? Hladka?.
2005. The prague dependency treebank 2.0.
http://ufal.mff.cuni.cz/pdt2.0.
Keith Hall and Mark Johnson. 2004. Attention shifting
for parsing speech. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 41?47, Barcelona.
Mehryar Mohri. 2002. Edit-distance of weighted
automata. In Proceedings of the 7th Interna-
tional Conference on Implementation and Applica-
tion of Automata, Jean-Marc Champarnaud and De-
nis Maurel, Eds.
Petr Podvesky and Pavel Machek. 2005. Speech
recognition of Czech?inclusion of rare words
helps. In Proceedings of the ACL Student Research
Workshop, pages 121?126, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Josef Psutka, Pavel Ircing, Josef V. Psutka, Vlasta
Radovic, William Byrne, Jan Hajic?, Jiri Mirovsky,
and Samuel Gustman. 2003. Large vocabulary ASR
for spontaneous Czech in the MALACH project.
In Proceedings of the 8th European Conference on
Speech Communication and Technology, Geneva,
Switzerland.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Izhak Shafran and William Byrne. 2004. Task-specific
minimum Bayes-risk decoding using learned edit
distance. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 3, pages 1945?48, Jeju Islands, Korea.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for arabic speech recognition. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing (ICSLP/Interspeech 2004).
Yiming Yang and Jan 0. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412 ? 420, San Fran-
cisco, CA, USA.
398
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 57?64,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Generation in Machine Translation from Deep Syntactic Trees
Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
keith hall@jhu.edu
Petr Ne?mec
Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
nemec@ufal.mff.cuni.cz
Abstract
In this paper we explore a generative
model for recovering surface syntax and
strings from deep-syntactic tree structures.
Deep analysis has been proposed for a
number of language and speech process-
ing tasks, such as machine translation and
paraphrasing of speech transcripts. In an
effort to validate one such formalism of
deep syntax, the Praguian Tectogrammat-
ical Representation (TR), we present a
model of synthesis for English which gen-
erates surface-syntactic trees as well as
strings. We propose a generative model
for function word insertion (prepositions,
definite/indefinite articles, etc.) and sub-
phrase reordering. We show by way of
empirical results that this model is ef-
fective in constructing acceptable English
sentences given impoverished trees.
1 Introduction
Syntactic models for language are being reintro-
duced into language and speech processing sys-
tems thanks to the success of sophisticated statisti-
cal models of parsing (Charniak and Johnson, 2005;
Collins, 2003). Representing deep syntactic rela-
tionships is an open area of research; examples of
such models are exhibited in a variety of grammat-
ical formalisms, such as Lexical Functional Gram-
mars (Bresnan and Kaplan, 1982), Head-driven
Phrase Structure Grammars (Pollard and Sag, 1994)
and the Tectogrammatical Representation (TR) of
the Functional Generative Description (Sgall et al,
1986). In this paper we do not attempt to analyze the
differences of these formalisms; instead, we show
how one particular formalism is sufficient for au-
tomatic analysis and synthesis. Specifically, in this
paper we provide evidence that TR is sufficient for
synthesis in English.
Augmenting models of machine translation (MT)
with syntactic features is one of the main fronts of
the MT research community. The Hiero model has
been the most successful to date by incorporating
syntactic structure amounting to simple tree struc-
tures (Chiang, 2005). Synchronous parsing mod-
els have been explored with moderate success (Wu,
1997; Quirk et al, 2005). An extension to this work
is the exploration of deeper syntactic models, such
as TR. However, a better understanding of the syn-
thesis of surface structure from the deep syntax is
necessary.
This paper presents a generative model for surface
syntax and strings of English given tectogrammati-
cal trees. Sentence generation begins by inserting
auxiliary words associated with autosemantic nodes;
these include prepositions, subordinating conjunc-
tions, modal verbs, and articles. Following this, the
linear order of nodes is modeled by a similar gen-
erative process. These two models are combined in
order to synthesize a sentence.
The Amalgam system provides a similar model
for generation from a logical form (Corston-Oliver
et al, 2002). The primary difference between our
approach and that of the Amalgam system is that
we focus on an impoverished deep structure (akin to
57
logical form); we restrict the deep analysis to con-
tain only the features which transfer directly across
languages; specifically, those that transfer directly
in our Czech-English machine translation system.
Amalgam targets different issues. For example,
Amalgam?s generation of prepositions and subordi-
nating conjunctions is severely restricted as most of
these are considered part of the logical form.
The work of Langkilde-Geary (2002) on the Halo-
gen system is similar to the work we present here.
The differences that distinguish their work from
ours stem from the type of deep representation from
which strings are generated. Although their syntac-
tic and semantic representations appear similar to
the Tectogrammatical Representation, more explicit
information is preserved in their representation. For
example, the Halogen representation includes mark-
ings for determiners, voice, subject position, and
dative position which simplifies the generation pro-
cess. We believe their minimally specified results are
based on input which most closely resembles the in-
put from which we generate in our experiments.
Amalgam?s reordering model is similar to the one
presented here; their model reorders constituents in
a similar way that we reorder subtrees. Both the
model of Amalgam and that presented here differ
considerably from the n-gram models of Langkilde
and Knight (1998), the TAG models of Bangalore
and Rambow (2000), and the stochastic generation
from semantic representation approach of Soricut
and Marcu (2006). In our work, we order the local-
subtrees1 of an augmented deep-structure tree based
on the syntactic features of the nodes in the tree. By
factoring these decisions to be independent for each
local-subtree, the set of strings we consider is only
constrained by the projective strucutre of the input
tree and the local permutation limit described below.
In the following sections we first provide a brief
description of the Tectogrammatical Representation
as used in our work. Both manually annotated and
synthetic TR trees are utilized in our experiments;
we present a description of each type of tree as well
as the motivation for using it. We then describe the
generative statistical process used to model the syn-
thesis of analytical (surface-syntactic) trees based
1A local subtree consists of a parent node (governor) and it?s
immediate children.
FORM: 
LEMM: 
FUNC: 
FORM: 
LEMM: 
FUNC: 
POS:  'NN'
FORM: 
LEMM: 
FUNC: 
POS:  'RB'
FORM: 
LEMM: 
FUNC: 
POS:  'VBN'
T_M:  'SIM'_'IND'
FORM: 
LEMM: 
FUNC: 
POS:  'NN'
FORM: 
LEMM: 
FUNC: 
POS:  'NN'
FORM: 
LEMM: 
FUNC: 
POS:  'NN'
FORM: 
LEMM: 
FUNC: 
POS:  'JJ'
#2
#
SENT
network
network
ACT
Now
now
TWHEN
opened
open
PRED
bureau
bureau
PAT
news
news
RSTR
capital
capital
LOC
Hungarian
hungarian
RSTR
Figure 1: Example of a manually annotated, Synthetic TR
tree (see Section 2.2).
Reference: Now the network has opened a news bureau in
the Hungarian capital
Each sentence has an artificial root node labeled #. Verbs con-
tain their tense and mood (labeled T M).
on the TR trees. Details of the model?s features
are presented in the following section. Finally we
present empirical results for experiments using both
the manually annotated and automatically generated
data.
2 Tectogrammatical (Deep) Syntax
The Tectogrammatical Representation (TR) comes
out of the Praguian linguistic theory known as
the Functional Generative Description of language
(Sgall et al, 1986). TR attempts to capture deep
syntactic relationships based on the valency of pred-
icates (i.e., function-argument structure) and modifi-
cation of participants (i.e., nouns used as actors, pa-
tients, etc.). A key feature of TR is that dependency
relationships are represented only for autosemantic
words (content words), meaning that synsemantic
words (syntactic function words) are encoded as fea-
tures of the grammatical relationships rather than the
actual words. Abstracting away from specific syn-
tactic lexical items allows for the representation to
be less language-specific making the representation
attractive as a medium for machine translation and
summarization.
Figure 1 shows an example TR tree, the nodes of
58
which represent the autosemantic words of the sen-
tence. Each node is labeled with a morphologically
reduced word-form called the lemma and a functor
that describes the deep syntactic relationship to its
governor (function-argument form). Additionally,
the nodes are labeled with grammatemes that cap-
ture morphological and semantic information asso-
ciated with the autosemantic words. For example,
English verb forms are represented by the infinitive
form as the lemma and the grammatemes encode
the tense, aspect, and mood of the verb. For a de-
tailed description of the TR annotation scheme see
Bo?hmova? et al (2002). In Figure 1 we show only
those features that are present in the TR structures
used throughout this paper.
Both the synsemantic nodes and the left-to-right
surface order2 in the TR trees is under-specified. In
the context of machine translation, we assume the
TR word order carries no information with the ex-
ception of a single situation: the order of coordi-
nated phrases is preserved in one of our models.
2.1 Analytic Representation
While it is not part of the formal TR description, the
authors of the TR annotation scheme have found it
useful to define an intermediate representation be-
tween the sentence and the TR tree (Bo?hmova? et
al., 2002). The analytical representation (AR) is a
surface-syntactic dependency tree that encodes syn-
tactic relationships between words (i.e., object, sub-
ject, attribute, etc.). Unlike the TR layer, the analyti-
cal layer contains all words of the sentence and their
relative ordering is identical to the surface order.
2.2 Manually Annotated TR
In order to evaluate the efficacy of the generation
model, we construct a dataset from both manually
annotated data and automatically generated data.
The information contained in the originally manu-
ally annotated TR all but specifies the surface form.
We have modified the annotated data by removing
all features except those that could be directly trans-
fered across languages. Specifically, we preserve
the following features: lemma, functor, verbal gram-
2In a TR tree, a subtree is always between the nodes to the
left and right of its governor. More specifically, all TR trees
are projective. For this reason, the relative ordering of subtrees
imposes an absolute ordering for the tree.
matemes, and part-of-speech tags. The lemma is
the morphologically reduced form of the word; for
verbs this is the infinitive form and for nouns this is
the singular form. The functor is the deep-syntactic
function of the node; for example, the deep functor
indicates whether a node is a predicate, an actor, or a
patient. Modifiers can be labeled as locative, tempo-
ral, benefactive, etc. Additionally we include a ver-
bal grammateme which encodes tense and mood as
well as a Penn Treebank style part-of-speech tag.
3 Generative Process
In this section we describe the generative process
that inserts the synsemantic auxiliary words, re-
orders the trees, and produces a sentence. Our eval-
uation will be on English data, so we describe the
models and the model features in the context of En-
glish. While the model is language independent, the
specific features and the size of the necessary condi-
tioning contexts is a function of the language.
Given a TR tree T , we wish to predict the cor-
rect auxiliary nodes A and an ordering of the words
associated with {T ? A}, defined by the function
f({T ? A}). The functions f determine the surface
word order of the words associated with nodes of the
auxiliary-inserted TR tree: N = {T ?A}. The node
features that we use from the nodes in the TR and
AR trees are: the word lemma, the part-of-speech
(POS) tag, and the functor.3 The objective of our
model is:
argmax
A,f
P (A, f |T )
= argmax
A,f
P (f |A,T )P (A|T ) (1)
? argmax
f
P (f |T, argmax
A
P (A|T )) (2)
In Equation 2 we approximate the full model with a
greedy procedure. First, we predict the most likely
A according to the model P (A|T ). Given A, we
compute the best ordering of the nodes of the tree,
including those introduced in A.
There is an efficient dynamic-programming solu-
tion to the objective function in Equation 1; how-
3The type of functor used (deep syntactic or surface-
syntactic) depends on the tree to which we are applying the
model. One form of the reordering model operates on AR trees
and therefore uses surface syntactic functors. The other model
is based on TR trees and uses deep-syntactic functors.
59
ever, in this work we experiment with the greedy
approximation.
3.1 Insertion Model
The specific English auxiliary nodes which are not
present in TR include articles, prepositions, subor-
dinating conjunctions, and modal verbs.4 For each
node in the TR tree, the generative process predicts
which synsemantic word, if any, should be inserted
as a dependent of the current node. We make the
assumption that these decisions are determined in-
dependently.
Let T = {w
1
, . . . , w
i
, . . . , w
k
} be the nodes of
the TR tree. For each node w
i
, we define the asso-
ciated node a
i
to be the auxiliary node that should
be inserted as a dependent of w
i
. Given a tree T ,
we wish to find the set of auxiliary nodes A =
{a
1
, . . . , a
k
} that should be inserted5:
P (A|T )
=
?
i
P (a
i
|a
1
, . . . , a
i?1
, T ) (3)
?
?
i
P (a
i
|T ) (4)
?
?
i
P (a
i
|w
i
, w
g(i)
) (5)
Equation 3 is simply a factorization of the origi-
nal model, Equation 4 shows the independence as-
sumption, and in Equation 5 we make an additional
conditional independence assumption that in order
to predict auxiliary a
i
, we need only know the asso-
ciated node w
i
and its governor w
g(i)
.6
We further divide the model into three compo-
nents: one that models articles, such as the En-
glish articles the and a; one that models preposi-
tions and subordinating conjunctions; and one that
models modal verbs. The first two models are of the
form described by Equation 5. The modal verb in-
sertion model is a deterministic mapping based on
4The function of synsemantic nodes are encoded by func-
tors. For example, the prepositions to, at, in, by, and on may be
used to indicate time or location. An autosemantic modifier will
be labeled as temporal or locative, but the particular preposition
is not specified.
5Note that we include the auxiliary node labeled NOAUX to
be inserted, which in fact means a node is not inserted.
6In the case of nodes whose governor is a coordinating con-
junction, the governor information comes from the governor of
the coordination node.
grammatemes expressing the verb modality of the
main verb. Additionally, each model is independent
of the other and therefore up to two insertions per
TR node are possible (an article and another syntac-
tic modifier). In a variant of our model, we perform
a small set of deterministic transformations in cases
where the classifier is relatively uncertain about the
predicted insertion node (i.e., the entropy of the con-
ditional distribution is high).
We note here that unlike the Amalgam system
(Corston-Oliver et al, 2002), we do not address fea-
tures which are determined (or almost completely
determined) by the underlying deep-structure. For
example, the task of inserting prepositions is non-
trivial given we only know a node?s functor (e.g.,
the node?s valency role).
3.2 Analytical Representation Tree Generation
We have experimented with two paradigms for syn-
thesizing sentences from TR trees. The first tech-
nique involves first generating AR trees (surface
syntax). In this model, we predict the node inser-
tions, transform the functors from TR to AR func-
tions (deep valency relationship to surface-syntactic
relationships), and then reorder the nodes. In the
second framework, we reorder the nodes directly in
the TR trees with inserted auxiliary nodes.
3.3 Surface-order Model
The node ordering model is used to determine a pro-
jection of the tree to a string. We assume the order-
ing of the nodes in the input TR trees is arbitrary,
the reordering model proposed here is based only on
the dependency structure and the node?s attributes
(words, POS tags, etc.). In a variant of the reorder-
ing model, we assume the deep order of coordinating
conjunctions to be the surface order.
Algorithm 1 presents the bottom-up node reorder-
ing algorithm. In the first part of the algorithm, we
determine the relative ordering of child nodes. We
maximize the likelihood of a particular order via the
precedence operator ?. If node c
i
? c
i+1
, then
the subtree of the word associated with c
i
imme-
diately precedes the subtree of the word associated
with c
i+1
in the projected sentence.
In the second half of the algorithm (starting at
line 13), we predict the position of the governor
within the previously ordered child nodes. Recall
60
Algorithm 1 Subtree Reordering Algorithm
procedure REORDER(T,A, O)  Result in O
N ?bottomUp(T ? A); O ? {}
for g ? N do
bestScore ? 0; o
g
? {}
5: for C ?permutation of g?s children do
for i ? 1 . . . |C| do
s ? s ? P (c
i
? c
i+1
|c
i
, c
i+1
, g)
end for
if s > bestScore then
10: bestScore ? s; o
g
? C
end if
end for
bestScore ? 0; m ? 0
for i ? 1 . . . |bestOrder| do
15: s ? P (c
i
? g ? c
i+1
|c
i
, c
i+1
, g)
if s > bestScore then
s ? bestScore ; m ? i
end if
end for
20: Insert governor c
g
after mth child in o
g
O ? O ? o
g
end for
end procedure
that this is a dependency structure; knowing the gov-
ernor does not tell us where it lies on the surface
with respect to its children. The model is similar
to the general reordering model, except we consider
an absolute ordering of three nodes (left child, gov-
ernor, right child). Finally, we can reconstruct the
total ordering from the subtree ordering defined in
O = {o
1
, . . . , o
n
}.
The procedure described here is greedy; first we
choose the best child ordering and then we choose
the location of the governor. We do this to minimize
the computational complexity of the algorithm. The
current algorithm?s runtime complexity isO(n!), but
the complexity of the alternative algorithm for which
we consider triples of child nodes is O(n!(n? 1)!).
The actual complexity is determined by the maxi-
mum number of child nodes k = |C| and is O(n
k
k!).
3.4 Morphological Generation
In order to produce true English sentences, we con-
vert the lemma and POS tag to a word form. We
use John Carroll?s morphg tool7 to generate English
word forms given lemma/POS tag pairs. This is
not perfect, but it performs an adequate job at re-
covering English inflected forms. In the complete-
system evaluation, we report scores based on gener-
7Available on the web at:
http://www.informatics.susx.ac.uk/research/nlp/carroll/morph.html.
ated morphological forms.
3.5 Insertion Features
Features for the insertion model come from the cur-
rent node being examined and the node?s governor.
When the governor is a coordinating conjunction,
we use features from the governor of the conjunc-
tion node. The features used are the lemma, POS
tag, and functor for the current node, and the lemma,
POS tag, and functor of the governor.
?
i
P (a
i
|w
i
, w
g
) (6)
=
?
i
P (a
i
|l
i
, t
i
, f
i
, l
g
, t
g
, f
g
)
The left-hand side of Equation 6 is repeated from
Equation 5 above. Equation 6 shows the expanded
model for auxiliary insertion where l
i
is the lemma ,
t
i
is the POS tag, and f
i
is the functor of node w
i
3.6 Reordering Features
Our reordering model for English is based primar-
ily on non-lexical features. We use the POS tag
and functor from each node as features. The two
distributions in our reordering model (used in Algo-
rithm 1) are:
P (c
i
? c
i+1
|c
i
, c
i+1
, g) (7)
= (c
i
? c
i+1
|f
i
, t
i
, f
i+1
, t
i+1
, f
g
, t
g
)
P (c
i
? g ? c
i+1
|c
i
, c
i+1
, g) (8)
= P (c
i
? g ? c
i+1
|f
i
, t
i
, f
i+1
, t
i+1
, t
g
, f
g
)
In both Equation 7 and Equation 8, only the func-
tor and POS tag of each node is used.
4 Empirical Evaluation
We have experimented with the above models on
both manually annotated TR trees and synthetic
trees (i.e., automatically generated trees). The data
comes from the PCEDT 1.0 corpus8, a version of the
Penn WSJ Treebank that has been been translated to
Czech and automatically transformed to TR in both
English and Czech. The English TR was automat-
ically generated from the Penn Treebank?s manu-
ally annotated surface syntax trees (English phrase-
structure trees). Additionally, a small set of 497 sen-
tences were manually annotated at the TR level: 248
8LDC catalog number: LDC2004T25.
61
Model Manual Data Synthetic Data
Ins. Rules No Rules Ins. Rules No Rules
Model Articles Prep & SC Articles Prep & SC Articles Prep & SC Articles Prep & SC
Baseline N/A N/A 77.93 76.78 N/A N/A 78.00 78.40
w/o g. functor 87.29 89.65 86.25 89.31 88.07 91.83 87.34 91.06
w/o g. lemma 86.77 89.48 85.68 89.02 87.53 90.95 86.55 91.16
w/o g. POS 87.29 89.45 86.10 89.14 87.68 91.86 86.89 92.07
w/o functor 86.10 85.02 84.86 84.56 86.01 85.60 84.79 85.65
w/o lemma 81.34 89.02 80.88 88.91 81.28 91.03 81.42 91.33
w/o POS 84.81 88.01 84.01 87.29 85.53 91.08 84.69 90.98
All Features 87.49 89.68 86.45 89.28 87.87 91.83 87.24 92.02
Table 1: Classification accuracy for insertion models on development data from PCEDT 1.0. Article accuracy is computed over
the set of nouns. Preposition and subordinating conjunction accuracy (P & SC) is computed over the set of nodes that appear on
the surface (excluding hidden nodes in the TR ? these will not exist in automatically generated data). Models are shown for all
features minus the specified feature. Features with the prefix ?g.? indicate governor features, otherwise the features are from the
node?s attributes. The Baseline model is one which never inserts any nodes (i.e., the model which inserts the most probable value ?
NOAUX).
for development and 249 for evaluation; results are
presented for these two datasets.
All models were trained on the PCEDT 1.0 data
set, approximately 49,000 sentences, of which 4,200
were randomly selected as held-out training data, the
remainder was used for training. We estimate the
model distributions with a smoothed maximum like-
lihood estimator, using Jelinek-Mercer EM smooth-
ing (i.e., linearly interpolated backoff distributions).
Lower order distributions used for smoothing are es-
timated by deleting the rightmost conditioning vari-
able (as presented in the above models).
Similar experiments were performed at the 2002
Johns Hopkins summer workshop. The results re-
ported here are substantially better than those re-
ported in the workshop report (Hajic? et al, 2002);
however, the details of the workshop experiments
are not clear enough to ensure the experimental con-
ditions are identical.
4.1 Insertion Results
For each of the two insertion models (the article
model and the preposition and subordinating con-
junction model), there is a finite set of values for
the dependent variable a
i
. For example, the articles
are the complete set of English articles as collected
from the Penn Treebank training data (these have
manual POS tag annotations). We add a dummy
value to this set which indicates no article should
be inserted.9 The preposition and auxiliary model
9In the classifier evaluation we consider the article a and an
to be equivalent.
assumes the set of possible modifiers to be all those
seen in the training data that were removed when
modifying the manual TR trees.
The classification accuracy is the percentage of
nodes for which we predicted the correct auxiliary
from the set of candidate nodes for the auxiliary
type. Articles are only predicted and evaluated for
nouns (determined by the POS tag). Prepositions
and subordinating conjunctions are predicted and
evaluated for all nodes that appear on the surface.
We do not report results for the modal verb inser-
tion as it is primarily determined by the features of
the verb being modified (accuracy is approximately
100%). We have experimented with different fea-
tures sets and found that the model described in
Equation 6 performs best when all features are used.
In a variant of the insertion model, when the clas-
sifier prediction is of low certainty (probability less
than .5) we defer to a small set of deterministic rules.
For infinitives, we insert ?to?; for origin nouns, we
insert ?from?, for actors we insert ?of?, and we at-
tach ?by? to actors of passive verbs. In the article
insertion model, we do not insert anything if there
is another determiner (e.g., ?none? or ?any?) or per-
sonal pronoun; we insert ?the? if the word appeared
within the previous four sentences or if there is a
suggestive adjective attached to the noun.10
Table 1 shows that the classifiers perform better
on automatically generated data (Synthetic Data),
but also perform well on the manually annotated
10Any adjective that is always followed by the definite article
in the training data.
62
Model Manual Data Synthetic Data
Coord. Rules No Rules Coord. Rules No Rules
All Interior All Interior All Interior All Interior
Baseline N/A N/A 68.43 21.67 N/A N/A 69.00 21.42
w/o g. functor 94.51 86.44 92.42 81.27 94.90 87.25 93.37 83.42
w/o g. tag 93.43 83.75 90.89 77.50 93.82 84.56 91.64 79.12
w/o c. functors 91.38 78.70 89.71 74.57 91.91 79.79 90.41 76.04
w/o c. tags 88.85 72.44 82.29 57.36 88.91 72.29 83.04 57.60
All Features 94.43 86.24 92.01 80.26 95.21 88.04 93.37 83.42
Table 2: Reordering accuracy for TR trees on development data from PCEDT 1.0. We include performance on the interior nodes
(excluding leaf nodes) for the Manual data to show a more detailed analysis of the performance. ?g.? are the governor features and
?c.? are the child features. The baseline model sorts subtrees of each node randomly.
data. Prediction of articles is primarily dependent on
the lemma and the tag of the node. The lemma and
tag of the governing node and the node?s functor is
important to a lesser degree. In predicting the prepo-
sitions and subordinating conjunctions, the node?s
functor is the most critical factor.
% Errors Reference?Hypothesis
41 the ? NULL
19 a/an ? NULL
16 NULL ? the
11 a/an ? the
11 the ? a/an
2 NULL ? a/an
Table 3: Article classifier errors on development data.
Manual Synthetic
Det. P & SC Det. P & SC
85.53 89.18 85.31 91.54
Table 4: Accuracy of best models on the evaluation data.
Table 3 presents a confusion set from the best ar-
ticle classifier on the development data. Our model
is relatively conservative, incurring 60% of the error
by choosing to insert nothing when it should have in-
serted an article. The model requires more informed
features as we are currently being overly conserva-
tive.
In Table 4 we report the overall accuracy on evalu-
ation data using the model that performed best on the
development data. The results are consistent with
the results for the development data; however, the
article model performs slightly worse on the evalua-
tion set.
4.2 Reordering Results
Evaluation of the final sentence ordering was based
on predicting the correct words in the correct po-
sitions. We use the reordering metric described in
Hajic? et al (2002) which computes the percentage
of nodes for which all children are correctly ordered
(i.e., no credit for partially correct orderings).
Table 2 shows the reordering accuracy for the
full model and variants where a particular feature
type is removed. These results are for ordering
the correct auxiliary-inserted TR trees (using deep-
syntactic functors and the correctly inserted auxil-
iaries). In the model variant that preserves the deep
order of coordinating conjunctions, we see a signif-
icant increase in performance. The child node tags
are critical for the reordering model, followed by the
child functors.
4.3 Combined System Results
Model Manual Synthetic
TR w/ Rules .4614 .4777
TR w/o Rules .4532 .4657
AR .2337 .2451
Table 5: BLEU scores for complete generation system for TR
trees (with and without rules applied) and the AR trees.
In order to evaluate the combined system, we used
the multiple-translation dataset in the PCEDT cor-
pus. This data contains four retranslations from
Czech to English of each of the original English sen-
tences in the development and evaluation datasets.
In Table 5 we report the BLEU scores on develop-
ment data for our TR generation model (including
the morphological generation module) and the AR
generation model. Results for the system that uses
AR trees as an intermediate stage are very poor; this
is likely due to the noise introduced when generating
AR trees. Additionally, the results for the TR model
with the additional rules are consistent with the pre-
63
vious results; the rules provide only a marginal im-
provement. Finally, we have run the complete sys-
tem on the evaluation data and achieved a BLEU
score of .4633 on the manual data and .4750 on
the synthetic data. These can be interpreted as the
upper-bound for Czech-English translation systems
based on TR tree transduction.
5 Conclusion
We have provided a model for sentence synthesis
from Tectogrammatical Representation trees. We
provide a number of models based on relatively sim-
ple, local features that can be extracted from impov-
erished TR trees. We believe that further improve-
ments will be made by allowing for more flexible
use of the features. The current model uses sim-
ple linear interpolation smoothing which limits the
types of model features used (forcing an explicit fac-
torization). The advantage of simple models of the
type presented in this paper is that they are robust
to errors in the TR trees ? which are expected when
the TR trees are generated automatically (e.g., in a
machine translation system).
Acknowledgments
This work was partially supported by U.S.
NSF grants IIS?9982329 and OISE?0530118; by
the project of the Czech Ministry of Educa-
tion #LC536; by the Information Society Project
No. 1ET201120505 of the Grant Agency of the
Academy of Sciences of the Czech Republic; and
Grant No. 352/2006 of the Grant Agency of Charles
University.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In Proceed-
ings of the 18th International Conference on Computational
Linguistics (COLING 2000), Saarbru?cken, Germany.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora Vidova?
Hladka?. 2002. The prague dependency treebank: Three-
level annotation scenario. In Anne Abeille, editor, In Tree-
banks: Building and Using Syntactically Annotated Cor-
pora. Dordrecht, Kluwer Academic Publishers, The Neter-
lands.
Joan Bresnan and Ronald M. Kaplan. 1982. Lexical-functional
grammar: A formal system for grammatical representation.
In The Mental Representation of Grammatical Relations.
MIT Press.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of the 43rd
Annual Meeting of the Association for Computational Lin-
guistics, pages 263?270, Ann Arbor, MI.
Michael Collins. 2003. Head-driven statistical models for
natural language processing. Computational Linguistics,
29(4):589?637.
Simon Corston-Oliver, Michael Gamon, Eric Ringger, and
Robert Moore. 2002. An overview of Amalgam: A
machine-learned generation module. In Proceedings of
the International Natural Language Generation Conference,
pages 33?40, New York, USA.
Jan Hajic?, Martin C?mejrek, Bonnie Dorr, Yuan Ding, Jason
Eisner, Dan Gildea, Terry Koo, Kristen Parton, Dragomir
Radev, and Owen Rambow. 2002. Natural language genera-
tion in the context of machine translation. Technical report,
Center for Language and Speech Processing, Johns Hopkins
University, Balitmore. Summer Workshop Final Report.
Irene Langkilde and Kevin Knight. 1998. The practical value of
n-grams in generation. In Proceedings of the International
Natural Language Generation Workshop.
Irene Langkilde-Geary. 2002. An empirical verification of cov-
erage and correctness for a general-purpose sentence gener-
ator. In Proceedings of the International Natural Language
Generation Conference.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed phrasal
SMT. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05), pages
271?279, Ann Arbor, Michigan, June. Association for Com-
putational Linguistics.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986. The
Meaning of the Sentence in Its Semantic and Pragmatic As-
pects. Kluwer Academic, Boston.
Radu Soricut and Daniel Marcu. 2006. Stochastic language
generation using WIDL?expressions and its application in
machine translation and summarization. In Proceedings of
the 44th Annual Meeting of the Association for Computa-
tional Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?404.
64
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 103?110,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur
Center for Language and Speech Processing
Johns Hopkins University
3400 North Charles Street, Baltimore, MD 21218 USA
{dreyer,keith hall,khudanpur}@jhu.edu
Abstract
This paper describes a new method to
compare reordering constraints for Statis-
tical Machine Translation. We investi-
gate the best possible (oracle) BLEU score
achievable under different reordering con-
straints. Using dynamic programming, we
efficiently find a reordering that approxi-
mates the highest attainable BLEU score
given a reference and a set of reordering
constraints. We present an empirical eval-
uation of popular reordering constraints:
local constraints, the IBM constraints,
and the Inversion Transduction Grammar
(ITG) constraints. We present results for a
German-English translation task and show
that reordering under the ITG constraints
can improve over the baseline by more
than 7.5 BLEU points.
1 Introduction
Reordering the words and phrases of a foreign sen-
tence to obtain the target word order is a fundamen-
tal, and potentially the hardest, problem in machine
translation. The search space for all possible per-
mutations of a sentence is factorial in the number
of words/phrases; therefore a variety of models have
been proposed that constrain the set of possible per-
mutations by allowing certain reorderings while dis-
allowing others. Some models (Brown et al (1996),
Kumar and Byrne (2005)) allow words to change
place with their local neighbors, but disallow global
reorderings. Other models (Wu (1997), Xiong et al
(2006)) explicitly allow global reorderings, but do
not allow all possible permutations, including some
local permutations.
We present a novel technique to compare achiev-
able translation accuracies under different reorder-
ing constraints. While earlier work has trained and
tested instantiations of different reordering models
and then compared the translation results (Zens and
Ney, 2003) we provide a more general mechanism
to evaluate the potential efficacy of reordering con-
straints, independent of specific training paradigms.
Our technique attempts to answer the question:
What is the highest BLEU score that a given trans-
lation system could reach when using reordering
constraints X? Using this oracle approach, we ab-
stract away from issues that are not inherent in the
reordering constraints, but may nevertheless influ-
ence the comparison results, such as model and fea-
ture design, feature selection, or parameter estima-
tion. In fact, we compare several sets of reorder-
ing constraints empirically, but do not train them as
models. We merely decode by efficiently search-
ing over possible translations allowed by each model
and choosing the reordering that achieves the high-
est BLEU score.
We start by introducing popular reordering con-
straints (Section 2). Then, we present dynamic-
programming algorithms that find the highest-
scoring permutations of sentences under given re-
ordering constraints (Section 3). We use this tech-
nique to compare several reordering constraints em-
pirically. We combine a basic translation framework
with different reordering constraints (Section 4) and
103
present results on a German-English translation task
(Section 5). Finally, we offer an analysis of the
results and provide a review of related work (Sec-
tions 6?8).
2 Reordering Constraints
Reordering constraints restrict the movement of
words or phrases in order to reach or approximate
the word order of the target language. Some of
the constraints considered in this paper were origi-
nally proposed for reordering words, but we will de-
scribe all constraints in terms of reordering phrases.
Phrases are units of consecutive words read off a
phrase translation table.
2.1 Local Constraints
Local constraints allow phrases to swap with one
another only if they are adjacent or very close to
each other. Kumar and Byrne (2005) define two
local reordering models for their Translation Tem-
plate Model (TTM): In the first one, called MJ-1,
only adjacent phrases are allowed to swap, and the
movement has to be done within a window of 2. A
sequence consisting of three phrases abc can there-
fore become acb or bac, but not cba. One phrase
can jump at most one phrase ahead and cannot take
part in more than one swap. In their second strategy,
called MJ-2, phrases are allowed to swap with their
immediate neighbor or with the phrase next to the
immediate neighbor; the maximum jump length is 2.
This allows for all six possible permutations of abc.
The movement here has to take place within a win-
dow of 3 phrases. Therefore, a four-phrase sequence
abcd cannot be reordered to cadb, for example. MJ-
1 and MJ-2 are shown in Figure 1.
2.2 IBM Constraints
First introduced by Brown et al (1996), the IBM
constraints are among the most well-known and
most widely used reordering paradigms. Transla-
tion is done from the beginning of the sentence to
the end, phrase by phrase; at each point in time, the
constraints allow one of the first k still untranslated
phrases to be selected for translation (see Figure 1d,
for k=2). The IBM constraints are much less restric-
tive than local constraints. The first word of the in-
put, for example, can move all the way to the end,
independent of the value of k. Typically, k is set to
4 (Zens and Ney, 2003). We write IBM with k=4 as
IBM(4). The IBM constraints are supersets of the
local constraints.
0
1
if
2
you
3
to-m
e
4
that
5
expla
in
6
cou
ld
(a) The sentence in foreign word order.
0
3
you
1
if
4
if you
2
to-m
e
5
to-m
e
7
that you
8
that
6
expla
in
to-m
e
9
expla
in
10
cou
ld that
11
cou
ld
expla
in
(b) MJ-1
0
8
you
6
to-m
e
1
if
9
if
60
to-m
e you
7
ifyou
3
that
2
to-m
e
15
to-m
e
12
that
10
expla
in
4
to-m
e
5
you youthat
17
that
19
cou
ld
16
expla
inyouto-me
18
expla
in
21
cou
ld
if you
to-m
e
13
expla
inthat
11
to-m
e
to-m
ethat
22
cou
ld
20
expla
inthatthat could
thatexpla
in
(c) MJ-2
0
6
you
1
if
7
if
11
to-m
e you
2
to-m
e
12
to-m
e
8
that you
3
that
16
that
13
expla
in you
4
expla
in
19
expla
in
17
cou
ld you
5
cou
ld
21
cou
ld you
if
15
that
to-m
e
9
expla
in
to-m
e
10
cou
ld
to-m
e
if
18
expla
in
that
60
cou
ld
that
if
20
cou
ld
expla
inif
(d) IBM(2)
Figure 1: The German word order if you to-me that explain
could (?wenn Sie mir das erkla?ren ko?nnten?) and all possible
reorderings under different constraints, represented as lattices.
None of these lattices contains the correct English order if you
could explain that to-me. See also Table 1.
2.3 ITG Constraints
The Inversion Transduction Grammar (ITG) (Wu,
1997), a derivative of the Syntax Directed Trans-
duction Grammars (Aho and Ullman, 1972), con-
strains the possible permutations of the input string
by defining rewrite rules that indicate permutations
of the string. In particular, the ITG allows all per-
mutations defined by all binary branching struc-
tures where the children of any constituent may be
swapped in order. The ITG constraint is different
from the other reordering constraints presented in
that it is not based on finite-state operations. An
104
Model # perm. ?Best? sentence n-gram precisions BLEU
MJ-1 13 if you that to-me could explain 100.0/66.7/20.0/0.0 0.0
MJ-2 52 to-me if you could explain that 100.0/83.3/60.0/50.0 70.71
IBM(2) 32 if to-me that you could explain 100.0/50.0/20.0/0.0 0.0
IBM(4) 384 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
IBM(4) (prune) 42 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG 394 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
ITG (prune) 78 if you could explain that to-me 100.0/100.0/100.0/100.0 100.0
Table 1: Illustrating example: The number of permutations (# perm.) that different reordering paradigms consider for the input
sequence if you to-me that explain could, and the permutation with highest BLEU score. The sentence length is 7, but there are
only 6! possible permutations, since the phrase to-me counts as one word during reordering. ITG (prune) is the ITG BLEU decoder
with the pruning settings we used in our experiments (beam threshold 10?4). For comparison, IBM(4) (prune) is the lattice
BLEU decoder with the same pruning settings, but we use pruning only for ITG permutations in our experiments.
Figure 2: The example if
you to-me that explain could
and its reordering to if you
could explain that to-me us-
ing an ITG. The alignments
are added below the tree, and
the horizontal bars in the tree
indicate a swap.
ITG decoder runs in polynomial time and allows for
long-distance phrasal reordering. A phrase can, for
example, move from the first position in the input
to the last position in the output and vice versa, by
swapping the topmost node in the constructed bi-
nary tree. However, due to the binary bracketing
constraint, some permutations are not modeled. A
four-phrase sequence abcd cannot be permuted into
cadb or bdac. Therefore, the ITG constraints are not
supersets of the IBM constraints. IBM(4), for exam-
ple, allows abcd to be permuted into cadb and bdac.
3 Factored BLEU Computation
The different reordering strategies described allow
for different permutations and restrict the search
space in different ways. We are concerned with
the maximal achievable accuracy under given con-
straints, independent of feature design or parameter
estimation. This is what we call the oracle accuracy
under the reordering constraints and it is computed
on a dataset with reference translations.
We now describe algorithms that can be used
to find such oracle translations among unreordered
translation candidates. There are two equivalent
strategies: The reordering constraints that are be-
ing tested can be expressed as a special dynamic-
programming decoder which, when applied to an
unreordered hypothesis, searches the space of per-
mutations defined by the reordering constraints and
returns the highest-scoring permutation. We employ
this strategy for the ITG reorderings (Section 3.2).
For the other reordering constraints, we employ a
more generic strategy: Given the set of reorder-
ing constraints, all permutations of an unreordered
translation candidate are precomputed and explicitly
represented as a lattice. This lattice is passed as in-
put to a Dijkstra-style decoder (Section 3.1) which
traverses it and finds the solution that reachest the
highest BLEU score.1
3.1 Dijkstra BLEU Decoder
The Dijkstra-style decoder takes as input a lattice in
which each path represents one possible permutation
of an unreordered hypothesis under a given reorder-
ing paradigm, as in Figure 1. It traverses the lat-
tice and finds the solution that has the highest ap-
proximate BLEU score, given the reference. The
dynamic-programming algorithm divides the prob-
lem into subproblems that are solved independently,
the solutions of which contribute to the solutions
of other subproblems. The general procedure is
sketched in Figure 3: for each subpath of the lat-
tice containing the precomputed permutations, we
store the three most recently attached words (Fig-
1For both strategies, several unreordered translation candi-
dates do not have to be regarded separately, but can be repre-
sented as a weighted lattice and be used as input to the special
dynamic program or to the process that precomputes possible
permutations.
105
?([0, k, len + 1, w2, w3, wnew]) = max
w1
( get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) ) (1)
function get bleu ( [0, j, len, w1, w2, w3], [j, k, wnew] ) :=
update ngrams (0, j, k, len, w1, w2, w3, wnew) ;
return exp
(
1
4
4?
n=1
log
(
ngramsi([0, k, len + 1, w2, w3, wnew])
len ? n + 1
))
;
(2)
Figure 3: Top: The BLEU score is used as inside score for a subpath from 0 to k with the rightmost words w2, w3, wnew in the
Dijkstra decoder. Bottom: Pseudo code for a function get bleu which updates the n-gram matches ngrams1(. . . ), ngrams2(. . . ),
ngrams3(. . . ), ngrams4(. . . ) for the resulting subpath in a hash table [0, k, len + 1, w2, w3, wnew] and returns its approximate
BLEU score.
("",
"","
")
0/0
/0/
0
("",
"to"
,"m
e")
2/1
/0/
0
("to
","m
e","
if")
3/1
/0/
0
("m
e","
if",
"yo
u")
4/2
/0/
0
("if
","y
ou"
,"co
uld
")
5/3
/1/
0
("y
ou"
,"co
uld
","e
xpl
ain
")
6/4
/2/
1
("co
uld
","e
xpl
ain
","t
hat
")
7/5
/3/
2
0
6
to-
me if yo
u
7
if yo
u
15
yo
u
19
co
uld tha
t
ex
pla
in
20
ex
pla
in
tha
t
22
tha
t
Figure 4: Three right-most words and n-gram matches: This shows the best path for the MJ-2 reordering of if you to-me that
explain could, along with the words stored at each state and the progressively updated n-gram matches. The full path to-me if you
could explain that has 7 unigram matches, 5 bigram, 3 trigram, and 2 fourgram matches. See the full MJ-2 lattice in Figure 1c.
ure 4). A context of three words is needed to com-
pute fourgram precisions used in the BLEU score.
Starting from the start state, we recursively extend
a subpath word by word, following the paths in
the lattice. Whenever we extend the path by a
word to the right we incorporate that word and use
update ngrams to update the four n-gram counts
for the subpath. The function update ngrams has
access to the reference string2 and stores the updated
n-gram counts for the resulting path in a hash table.3
The inside score of each subpath is the approximate
BLEU score, calculated as the average of the four
n-gram log precisions. An n-gram precision is al-
ways the number of n-gram matches divided by the
length len of the path minus (n ? 1). A path of
length 4 with 2 bigram matches, for example, has
a bigram precision of 2/3. This method is similar to
Dijkstra?s algorithm (Dijkstra, 1959) composed with
a fourgram finite-state language model, where the
scoring is done using n-gram counts and precision
2Multiple reference strings can be used if available.
3An epsilon value of 1?10 is used for zero precisions.
scores. We call this the Dijkstra BLEU decoder.
3.2 ITG BLEU Decoder
For the ITG reordering constraints, we use a dy-
namic program that computes the permutations im-
plicitly. It takes only the unreordered hypothesis
as input and creates the possible reorderings under
the ITG constraints during decoding, as it creates
a parse chart. The algorithm is similar to a CKY
parsing algorithm in that it proceeds bottom-up and
combines smaller constituents into larger ones re-
cursively. Figure 5 contains details of the algo-
rithm. The ITG BLEU decoder stores the three left-
most and the three rightmost words in each con-
stituent. A constituent from position i to posi-
tion k, with wa, wb, and wc as leftmost words,
and wx, wy, wz as rightmost words is written as
[i, k, (wa, wb, wc), (wx, wy, wz)]. Such a constituent
can be built by straight or inverted rules. Using an
inverted rule means swapping the order of the chil-
dren in the built constituent. The successive bottom-
up combinations of adjacent constituents result in hi-
erarchical binary bracketing with swapped and non-
106
? ([i, k, (wa, wb, wc), (wx, wy, wz)]) = max
(
?() ([i, k, (wa, wb, wc), (wx, wy, wz)]) ,
?<> ([i, k, (wa, wb, wc), (wx, wy, wz)])
)
(3)
?<>([i, k, (wa, wb, wc), (wx, wy, wz)]) =
max
j,wa? ,wb? ,wc? ,wx? ,wy? ,wz?
(
get bleu
( [
j, k, (wa, wb, wc), (wx? , wy? , wz?)
]
,
[i, j, (wa? , wb? , wc?), (wx, wy, wz)]
) ) (4)
Figure 5: Equations for the ITG oracle BLEU decoder. [i, k, (wa, wb, wc), (wx, wy, wz)] is a constituent from i to k with leftmost
words wa,wb,wc and rightmost words wx,wy ,wz . Top: A constituent can be built with a straight or a swapped rule. Bottom: A
swapped rule. The get bleu function can be adapted from Figure 3
swapped constituents. Our ITG BLEU decoder uses
standard beam search pruning. As in Zens and Ney
(2003), phrases are not broken up, but every phrase
is, at the beginning of reordering, stored in the chart
as one lexical token together with the precomputed
n-gram matches and the n-gram precision score.
In addition to standard ITG we run experiments
with a constrained ITG, in which we impose a bound
? on the maximum length of reordered constituents,
measured in phrases. If the combined length of two
constituents exceeds this bound they can only be
combined in the given monotone order. Experiments
with this ITG variant give insight into the effect that
various long-distance reorderings have on the final
BLEU scores (see Table 3). Such bounds are also
effective speedup techniques(Eisner and Tromble,
2006).
3.3 BLEU Approximations
BLEU is defined to use the modified n-gram preci-
sion, which means that a correct n-gram that oc-
curs once in the reference, but several times in the
system translation will be counted only once as
correct. The other occurrences are clipped. We
do not include this global feature since we want
a dynamic-programming solution with polynomial
size and runtime. The decoder processes subprob-
lems independently; words are attached locally and
stored only as boundary words of covered paths/
constituents. Therefore we cannot discount a locally
attached word that has already been attached else-
where to an alternative path/constituent. However,
clipping affects most heavily the unigram scores
which are constant, like the length of the sentence.4
4Since the sentence lengths are constant for all reorderings
of a given sentence we can in our experiments also ignore the
brevity penalty which cancels out. If the input consists of sev-
We also adopt the approximation that treats every
sentence with its reference as a separate corpus (Till-
mann and Zhang, 2006) so that ngram counts are not
accumulated, and parallel processing of sentences
becomes possible. Due to these two approximations,
our method is not guaranteed to find the best reorder-
ing defined by the reordering constraints. However,
we have found on our heldout data that an oracle
that does not accumulate n-gram counts is only min-
imally worse than an oracle that does accumulate
them (up to 0.25 BLEU points).5 If, in addition,
clipping is ignored, the resulting oracle stays virtu-
ally the same, at most 0.02 BLEU points worse than
the oracle found otherwise. All results in this paper
are computed with the original BLEU formula on the
sentences found by the oracle algorithms.
4 Creating a Monotone Translation
Baseline
To compare the reordering constraints under ora-
cle conditions we first obtain unreordered candi-
date translations from a simple baseline translation
model. For each reordering paradigm, we take the
candidate translations, get the best oracle reorder-
ings under the given reordering constraints and pick
the best sentence according to the BLEU score.
The baseline translation system is created using
probabilistic word-to-word and phrase-to-phrase ta-
eral sentences of different lengths (see fn. 1) then the brevity
penalty can be built in by keeping track of length ratios of at-
tached phrases.
5The accumulating oracle algorithm makes a greedy deci-
sion for every sentence given the ngram counts so far accumu-
lated (Zens and Ney, 2005). The result of such a greedy or-
acle method may depend on the order of the input sentences.
We tried 100 shuffles of these and received 100 very simi-
lar results, with a variance of under 0.006 BLEU points. The
non-accumulating oracles use an epsilon value (1?10) for zero
counts.
107
bles. Using the translation probabilities, we create
a lattice that contains word and phrase translations
for every substring of the source sentence. The re-
sulting lattice is made of English words and phrases
of different lengths. Every word or phrase transla-
tion probability p is a mixture of p(f |e) and p(e|f).
We discard short phrase translations exponentially
by a parameter that is trained on heldout data. Inser-
tions and deletions are handled exclusively by the
use of a phrase table: an insertion takes place wher-
ever the English side of a phrase translation is longer
than the foreign side (e.g. English presidential can-
didate for German Pra?sidentschaftskandidat), and
vice versa for deletions (e.g. we discussed for wir
haben diskutiert). Gaps or discontinuous phrases
are not handled. The baseline decoder outputs the
n-best paths through the lattice according to the lat-
tice scores6, marking consecutive phrases so that the
oracle reordering algorithms can recognize them and
keep them together. Note that the baseline system is
trained on real data, while the reordering constraints
that we want to test are not trained.
5 Empirical Comparison of Reordering
Constraints
We use the monotone translation baseline model and
the oracle BLEU computation to evaluate different
popular reordering strategies. We now describe the
experimental settings. The word and phrase transla-
tion probabilities of the baseline model are trained
on the Europarl German-English training set, using
GIZA++ and the Pharaoh phrase extraction algo-
rithm. For testing we use the NAACL 2006 SMT
Shared Task test data. For each sentence of the test
set, a lattice is created in the way described in Sec-
tion 4, with parameters optimized on a small heldout
set.7 For each sentence, the 1000-best candidates ac-
cording to the lattice scores are extracted. We take
the 10-best oracle candidates, according to the ref-
erence, and use a BLEU decoder to create the best
permutation of each of them and pick the best one.
Using this procedure, we make sure that we get the
highest-scoring unreordered candidates and choose
the best one among their oracle reorderings. Table 2
6We use a straightforward adaption of Algorithm 3 in Huang
and Chiang (2005)
7We fill the initial phrase and word lattice with the 20 best
candidates, using phrases of 3 or less words.
and Figure 6 show the resulting BLEU scores for dif-
ferent sentence lengths. Table 3 shows results of the
ITG runs with different length bounds ?. The aver-
age phrase length in the candidate translations of the
test set is 1.42 words.
Oracle decodings under the ITG and under
IBM(4) constraints were up to 1000 times slower
than under the other tested oracle reordering meth-
ods in our implementations. Among the faster meth-
ods, decoding under MJ-2 constraints was up to 40%
faster than under IBM(2) constraints in our imple-
mentation.
 
20
 
25
 
30
 
35
 
40
 
45  5
 
10
 
15
 
20
 
25
 
30
 
35
 
40
BLEU
Senten
ce leng
thI
TG
IBM, 
k=4
IBM, 
k=2 MJ-2 MJ-1 Baseli
ne
Figure 6: Reordering oracle scores for different sentence
lengths. See also Table 2.
6 Discussion
The empirical results show that reordering un-
der sufficiently permissive constraints can improve
a monotone baseline oracle by more than 7.5
BLEU points. This gap between choosing the best
unreordered sentences versus choosing the best op-
timally reordered sentences is small for short sen-
tences and widens dramatically (more than nine
BLEU points) for longer sentences.
The ITG constraints and the IBM(4) constraints
both give very high oracle translation accuracies on
the German-English translation task. Overall, their
BLEU scores are about 2 to more than 4 points bet-
ter than the BLEU scores of the best other meth-
ods. This gap between the two highest-scoring con-
straints and the other methods becomes bigger as
the sentence lengths grow and is greater than 4
108
Sen
tenc
e le
ngth
# of
test
sen
ten
ces
BLEU (NIST) scores
ITG (prune) IBM, k=4 IBM, k=2 MJ-2 MJ-1 No reordering
1?5 61 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.21 (5.35) 48.17 (5.68)
6?10 230 43.83 (6.75) 43.71 (6.74) 41.94 (6.68) 42.50 (6.71) 40.85 (6.66) 39.21 (6.99)
11?15 440 33.66 (6.71) 33.37 (6.71) 31.23 (6.62) 31.49 (6.64) 29.67 (6.56) 28.21 (6.76)
16?20 447 30.47 (6.66) 29.99 (6.65) 27.00 (6.52) 27.06 (6.50) 25.15 (6.45) 23.34 (6.52)
21?25 454 30.13 (6.80) 29.83 (6.79) 27.21 (6.67) 27.22 (6.65) 25.46 (6.58) 23.32 (6.63)
26?30 399 26.85 (6.42) 26.36 (6.42) 22.79 (6.25) 22.47 (6.22) 20.38 (6.12) 18.31 (6.11)
31?35 298 28.11 (6.45) 27.47 (6.43) 23.79 (6.25) 23.28 (6.21) 21.09 (6.12) 18.94 (6.06)
36?40 242 27.65 (6.37) 26.97 (6.35) 23.31 (6.19) 22.73 (6.16) 20.70 (6.06) 18.22 (5.94)
1?40 2571 29.63 (7.48) 29.17 (7.46) 26.07 (7.24) 25.89 (7.22) 23.95 (7.08) 21.89 (7.07)
Table 2: BLEU and NIST results for different reordering methods on binned sentence lengths. The ITG results are, unlike the
other results, with pruning (beam 10?4). The BLEU results are plotted in Figure 6. All results are computed with the original
BLEU formula on the sentences found by the oracle algorithms.
BLEU scores for sentences longer than 30 sentences.
This advantage in translation accuracy comes with
high computational cost, as mentioned above.
Among the computationally more lightweight re-
ordering methods tested, IBM(2) and MJ-2 are very
close to each other in translation accuracy, with
IBM(2) obtaining slightly better scores on longer
sentences, while MJ-2 is more efficient. MJ-1 is
less successful in reordering, improving the mono-
tone baseline by only about 2.5 BLEU points at best,
but is the best choice if speed is an issue.
As described above, the reorderings defined by
the local constraints MJ-1 and MJ-2 are subsets of
IBM(2) and IBM(3). We did not test IBM(3), but
the values can be interpolated between IBM(2) and
IBM(4). The ITG constraints do not belong in this
family of finite-state contraints; they allow reorder-
ings that none of the other methods allow, and vice
versa. The fact that ITG constraints can reach such
high translation accuracies supports the findings in
Zens et al (2004) and is an empirical validation of
the ITG hypothesis.
The experiments with the constrained ITG show
the effect of reorderings spanning different lengths
(see Table 3). While most reorderings are short-
distance (<5 phrases) a lot of improvements can still
be obtained when ? is increased from length 5 to 10
and even from 10 to 20 phrases.
7 Related Work
There exist related algorithms that search the space
of reorderings and compute BLEU oracle approxi-
Len. ?=0 ?=5 ?=10 ?=20 ?=30 ?=40
26?30 18.31 24.07 26.40 26.79 26.85 26.85
31?35 18.94 25.10 27.21 28.00 28.09 28.11
36?40 18.22 24.46 26.66 27.53 27.64 27.65
26?40 18.49 24.74 26.74 27.41 27.50 27.51
Table 3: BLEU results of ITGs that are constrained to reorder-
ings not exceeding a certain span length ?. Results shown for
different sentence lengths.
mations. Zens and Ney (2005) describe a dynamic-
programming algorithm in which at every state the
number of n-gram matches is stored, along with a
multiset that contains all words from the reference
that have not yet been matched. This makes it pos-
sible to compute the modified ngram precision, but
the search space is exponential. Tillmann and Zhang
(2006) use a BLEU oracle decoder for discrimina-
tive training of a local reordering model. No de-
tails about the algorithm are given. Zens and Ney
(2003) perform a comparison of different reorder-
ing strategies. Their study differs from ours in that
they use reordering models trained on real data and
may therefore be influenced by feature selection,
parameter estimation and other training-specific is-
sues. In our study, only the baseline translation
model is trained on data. Zens et al (2004) con-
duct a study similar to Zens and Ney (2003) and note
that the results for the ITG reordering constraints
were quite dependent on the very simple probability
model used. Our study avoids this issue by using the
109
BLEU oracle approach. In Wellington et al (2006),
hand-aligned data are used to compare the standard
ITG constraints to ITGs that allow gaps.
8 Conclusions
We have presented a training-independent method
to compare different reordering constraints for ma-
chine translation. Given a sentence in foreign word
order, its reference translation(s) and reordering
constraints, our dynamic-programming algorithms
efficiently find the oracle reordering that has the ap-
proximately highest BLEU score. This allows eval-
uating different reordering constraints experimen-
tally, but abstracting away from specific features,
the probability model or training methods of the re-
ordering strategies. The presented method evaluates
the theoretical capabilities of reordering constraints,
as opposed to more arbitrary accuracies of specifi-
cally trained instances of reordering models.
Using our oracle method, we presented an em-
pirical evaluation of different reordering constraints
for a German-English translation task. The results
show that a good reordering of a given monotone
translation can improve the translation quality dra-
matically. Both short- and long-distance reorderings
contribute to the BLEU score improvements, which
are generally greater for longer sentences. Reorder-
ing constraints that allow global reorderings tend
to reach better oracles scores than ones that search
more locally. The ITG constraints and the IBM(4)
constraints both give the highest oracle scores.
The presented BLEU decoder algorithms can be
useful in many ways: They can generally help de-
cide what reordering constraints to choose for a
given translation system. They can be used for
discriminative training of reordering models (Till-
mann and Zhang, 2006). Furthermore, they can help
detecting insufficient parameterization or incapable
training algorithms: If two trained reordering model
instances show similar performances on a given task,
but the oracle scores differ greatly then the training
methods might not be optimal.
Acknowledgments
This work was partially supported by the National
Science Foundation via an ITR grant (No 0121285),
the Defense Advanced Research Projects Agency
via a GALE contract (No HR0011-06-2-0001), and
the Office of Naval Research via a MURI grant (No
N00014-01-1-0685). We thank Jason Eisner, David
Smith, Roy Tromble and the anonymous reviewers
for helpful comments and suggestions.
References
A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing,
Translation, and Compiling. Prentice Hall.
A.L. Berger P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
J. R. Gillett, J. D. Lafferty, R. L. Mercer, H. Printz, and
L. Ures. 1996. Language translation apparatus and method
using context-based translation models. United States Patent
No. 5,510,981.
E.W. Dijkstra. 1959. A note on two problems in connexion
with graphs. Numerische Mathematik., 1:269?271.
J. Eisner and R. W. Tromble. 2006. Local search with very
large-scale neighborhoods for optimal permutations in Ma-
chine Translation. In Proc. of the Workshop on Computa-
tionally Hard Problems and Joint Inference, New York.
L. Huang and D. Chiang. 2005. Better k-best parsing. In Proc.
of IWPT, Vancouver, B.C., Canada.
S. Kumar and W. Byrne. 2005. Local phrase reordering
models for Statistical Machine Translation. In Proc. of
HLT/EMNLP, pages 161?168, Vancouver, B.C., Canada.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for Statistical MT. In Proc. of ACL, pages
721?728, Sydney, Australia.
B. Wellington, S. Waxmonsky, and D. Melamed. 2006. Empir-
ical lower bounds on the complexity of translational equiv-
alence. In Proc. of COLING-ACL, pages 977?984, Sydney,
Australia.
D. Wu. 1997. Stochastic inversion transduction grammars and
bilingual parsing of parallel corpora. Computational Lin-
guistics, 23(3):377?404.
D. Xiong, Q. Liu, and S. Lin. 2006. Maximum entropy based
phrase reordering model for Statistical Machine Translation.
In Proc. of COLING-ACL, pages 521?528, Sydney, Aus-
tralia.
R. Zens and H. Ney. 2003. A comparative study on reordering
constraints in Statistical Machine Translation. In Proc. of
ACL, pages 144?151, Sapporo, Japan.
R. Zens and H. Ney. 2005. Word graphs for Statistical Machine
Translation. In Proc. of the ACL Workshop on Building and
Using Parallel Texts, pages 191?198, Ann Arbor, MI.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Reorder-
ing constraints for phrase-based Statistical Machine Transla-
tion. In Proc. of CoLing, pages 205?211, Geneva.
110
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 19?27,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Study on Similarity and Relatedness
Using Distributional and WordNet-based Approaches
Eneko Agirre? Enrique Alfonseca? Keith Hall? Jana Kravalova?? Marius Pas?ca? Aitor Soroa?
? IXA NLP Group, University of the Basque Country
? Google Inc.
? Institute of Formal and Applied Linguistics, Charles University in Prague
{e.agirre,a.soroa}@ehu.es {ealfonseca,kbhall,mars}@google.com
kravalova@ufal.mff.cuni.cz
Abstract
This paper presents and compares WordNet-
based and distributional similarity approaches.
The strengths and weaknesses of each ap-
proach regarding similarity and relatedness
tasks are discussed, and a combination is pre-
sented. Each of our methods independently
provide the best results in their class on the
RG and WordSim353 datasets, and a super-
vised combination of them yields the best pub-
lished results on all datasets. Finally, we pio-
neer cross-lingual similarity, showing that our
methods are easily adapted for a cross-lingual
task with minor losses.
1 Introduction
Measuring semantic similarity and relatedness be-
tween terms is an important problem in lexical se-
mantics. It has applications in many natural lan-
guage processing tasks, such as Textual Entailment,
Word Sense Disambiguation or Information Extrac-
tion, and other related areas like Information Re-
trieval. The techniques used to solve this problem
can be roughly classified into two main categories:
those relying on pre-existing knowledge resources
(thesauri, semantic networks, taxonomies or ency-
clopedias) (Alvarez and Lim, 2007; Yang and Pow-
ers, 2005; Hughes and Ramage, 2007) and those in-
ducing distributional properties of words from cor-
pora (Sahami and Heilman, 2006; Chen et al, 2006;
Bollegala et al, 2007).
In this paper, we explore both families. For the
first one we apply graph based algorithms to Word-
Net, and for the second we induce distributional
similarities collected from a 1.6 Terabyte Web cor-
pus. Previous work suggests that distributional sim-
ilarities suffer from certain limitations, which make
them less useful than knowledge resources for se-
mantic similarity. For example, Lin (1998b) finds
similar phrases like captive-westerner which made
sense only in the context of the corpus used, and
Budanitsky and Hirst (2006) highlight other prob-
lems that stem from the imbalance and sparseness of
the corpora. Comparatively, the experiments in this
paper demonstrate that distributional similarities can
perform as well as the knowledge-based approaches,
and a combination of the two can exceed the per-
formance of results previously reported on the same
datasets. An application to cross-lingual (CL) sim-
ilarity identification is also described, with applica-
tions such as CL Information Retrieval or CL spon-
sored search. A discussion on the differences be-
tween learning similarity and relatedness scores is
provided.
The paper is structured as follows. We first
present the WordNet-based method, followed by the
distributional methods. Section 4 is devoted to the
evaluation and results on the monolingual and cross-
lingual tasks. Section 5 presents some analysis, in-
cluding learning curves for distributional methods,
the use of distributional similarity to improve Word-
Net similarity, the contrast between similarity and
relatedness, and the combination of methods. Sec-
tion 6 presents related work, and finally, Section 7
draws the conclusions and mentions future work.
2 WordNet-based method
WordNet (Fellbaum, 1998) is a lexical database of
English, which groups nouns, verbs, adjectives and
adverbs into sets of synonyms (synsets), each ex-
pressing a distinct concept. Synsets are interlinked
with conceptual-semantic and lexical relations, in-
cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-
sentation of WordNet, our method has basically two
19
steps: We first compute the personalized PageR-
ank over WordNet separately for each of the words,
producing a probability distribution over WordNet
synsets. We then compare how similar these two dis-
crete probability distributions are by encoding them
as vectors and computing the cosine between the
vectors.
We represent WordNet as a graph G = (V,E) as
follows: graph nodes represent WordNet concepts
(synsets) and dictionary words; relations among
synsets are represented by undirected edges; and
dictionary words are linked to the synsets associated
to them by directed edges.
For each word in the pair we first compute a per-
sonalized PageRank vector of graph G (Haveliwala,
2002). Basically, personalized PageRank is com-
puted by modifying the random jump distribution
vector in the traditional PageRank equation. In our
case, we concentrate all probability mass in the tar-
get word.
Regarding PageRank implementation details, we
chose a damping value of 0.85 and finish the calcula-
tion after 30 iterations. These are default values, and
we did not optimize them. Our similarity method is
similar, but simpler, to that used by (Hughes and Ra-
mage, 2007), which report very good results on sim-
ilarity datasets. More details of our algorithm can be
found in (Agirre and Soroa, 2009). The algorithm
and needed resouces are publicly available1.
2.1 WordNet relations and versions
The WordNet versions that we use in this work are
the Multilingual Central Repository or MCR (At-
serias et al, 2004) (which includes English Word-
Net version 1.6 and wordnets for several other lan-
guages like Spanish, Italian, Catalan and Basque),
and WordNet version 3.02. We used all the rela-
tions in MCR (except cooccurrence relations and se-
lectional preference relations) and in WordNet 3.0.
Given the recent availability of the disambiguated
gloss relations for WordNet 3.03, we also used a
version which incorporates these relations. We will
refer to the three versions as MCR16, WN30 and
WN30g, respectively. Our choice was mainly moti-
vated by the fact that MCR contains tightly aligned
1http://http://ixa2.si.ehu.es/ukb/
2Available from http://http://wordnet.princeton.edu/
3http://wordnet.princeton.edu/glosstag
wordnets of several languages (see below).
2.2 Cross-linguality
MCR follows the EuroWordNet design (Vossen,
1998), which specifies an InterLingual Index (ILI)
that links the concepts across wordnets of differ-
ent languages. The wordnets for other languages in
MCR use the English WordNet synset numbers as
ILIs. This design allows a decoupling of the rela-
tions between concepts (which can be taken to be
language independent) and the links from each con-
tent word to its corresponding concepts (which is
language dependent).
As our WordNet-based method uses the graph of
the concepts and relations, we can easily compute
the similarity between words from different lan-
guages. For example, consider a English-Spanish
pair like car ? coche. Given that the Spanish Word-
Net is included in MCR we can use MCR as the
common knowledge-base for the relations. We can
then compute the personalized PageRank for each
of car and coche on the same underlying graph, and
then compare the similarity between both probabil-
ity distributions.
As an alternative, we also tried to use pub-
licly available mappings for wordnets (Daude et al,
2000)4 in order to create a 3.0 version of the Span-
ish WordNet. The mapping was used to link Spanish
variants to 3.0 synsets. We used the English Word-
Net 3.0, including glosses, to construct the graph.
The two Spanish WordNet versions are referred to
as MCR16 and WN30g.
3 Context-based methods
In this section, we describe the distributional meth-
ods used for calculating similarities between words,
and profiting from the use of a large Web-based cor-
pus.
This work is motivated by previous studies that
make use of search engines in order to collect co-
occurrence statistics between words. Turney (2001)
uses the number of hits returned by a Web search
engine to calculate the Pointwise Mutual Informa-
tion (PMI) between terms, as an indicator of syn-
onymy. Bollegala et al (2007) calculate a number
of popular relatedness metrics based on page counts,
4http://www.lsi.upc.es/?nlp/tools/download-map.php.
20
like PMI, the Jaccard coefficient, the Simpson co-
efficient and the Dice coefficient, which are com-
bined with lexico-syntactic patterns as model fea-
tures. The model parameters are trained using Sup-
port Vector Machines (SVM) in order to later rank
pairs of words. A different approach is the one taken
by Sahami and Heilman (2006), who collect snip-
pets from the results of a search engine and repre-
sent each snippet as a vector, weighted with the tf?idf
score. The semantic similarity between two queries
is calculated as the inner product between the cen-
troids of the respective sets of vectors.
To calculate the similarity of two words w1 and
w2, Ruiz-Casado et al (2005) collect snippets con-
taining w1 from a Web search engine, extract a con-
text around it, replace it with w2 and check for the
existence of that modified context in the Web.
Using a search engine to calculate similarities be-
tween words has the drawback that the data used will
always be truncated. So, for example, the numbers
of hits returned by search engines nowadays are al-
ways approximate and rounded up. The systems that
rely on collecting snippets are also limited by the
maximum number of documents returned per query,
typically around a thousand. We hypothesize that
by crawling a large corpus from the Web and doing
standard corpus analysis to collect precise statistics
for the terms we should improve over other unsu-
pervised systems that are based on search engine
results, and should yield results that are competi-
tive even when compared to knowledge-based ap-
proaches.
In order to calculate the semantic similarity be-
tween the words in a set, we have used a vector space
model, with the following three variations:
In the bag-of-words approach, for each word w
in the dataset we collect every term t that appears in
a window centered in w, and add them to the vector
together with its frequency.
In the context window approach, for each word
w in the dataset we collect every window W cen-
tered in w (removing the central word), and add it
to the vector together with its frequency (the total
number of times we saw windowW around w in the
whole corpus). In this case, all punctuation symbols
are replaced with a special token, to unify patterns
like , the <term> said to and ? the <term> said to.
Throughout the paper, when we mention a context
window of size N it means N words at each side of
the phrase of interest.
In the syntactic dependency approach, we parse
the entire corpus using an implementation of an In-
ductive Dependency parser as described in Nivre
(2006). For each word w we collect a template of
the syntactic context. We consider sequences of gov-
erning words (e.g. the parent, grand-parent, etc.) as
well as collections of descendants (e.g., immediate
children, grandchildren, etc.). This information is
then encoded as a contextual template. For example,
the context template cooks <term> delicious could
be contexts for nouns such as food, meals, pasta, etc.
This captures both syntactic preferences as well as
selectional preferences. Contrary to Pado and Lap-
ata (2007), we do not use the labels of the syntactic
dependencies.
Once the vectors have been obtained, the fre-
quency for each dimension in every vector is
weighted using the other vectors as contrast set, with
the ?2 test, and finally the cosine similarity between
vectors is used to calculate the similarity between
each pair of terms.
Except for the syntactic dependency approach,
where closed-class words are needed by the parser,
in the other cases we have removed stopwords (pro-
nouns, prepositions, determiners and modal and
auxiliary verbs).
3.1 Corpus used
We have used a corpus of four billion documents,
crawled from the Web in August 2008. An HTML
parser is used to extract text, the language of each
document is identified, and non-English documents
are discarded. The final corpus remaining at the end
of this process contains roughly 1.6 Terawords. All
calculations are done in parallel sharding by dimen-
sion, and it is possible to calculate all pairwise sim-
ilarities of the words in the test sets very quickly
on this corpus using the MapReduce infrastructure.
A complete run takes around 15 minutes on 2,000
cores.
3.2 Cross-linguality
In order to calculate similarities in a cross-lingual
setting, where some of the words are in a language l
other than English, the following algorithm is used:
21
Method Window size RG dataset WordSim353 dataset
MCR16 0.83 [0.73, 0.89] 0.53 (0.56) [0.45, 0.60]
WN30 0.79 [0.67, 0.86] 0.56 (0.58) [0.48, 0.63]
WN30g 0.83 [0.73, 0.89] 0.66 (0.69) [0.59, 0.71]
CW 1 0.83 [0.73, 0.89] 0.63 [0.57, 0.69]
2 0.83 [0.74, 0.90] 0.60 [0.53, 0.66]
3 0.85 [0.76, 0.91] 0.59 [0.52, 0.65]
4 0.89 [0.82, 0.93] 0.60 [0.53, 0.66]
5 0.80 [0.70, 0.88] 0.58 [0.51, 0.65]
6 0.75 [0.62, 0.84] 0.58 [0.50, 0.64]
7 0.72 [0.58, 0.82] 0.57 [0.49, 0.63]
BoW 1 0.81 [0.70, 0.88] 0.64 [0.57, 0.70]
2 0.80 [0.69, 0.87] 0.64 [0.58, 0.70]
3 0.79 [0.67, 0.86] 0.64 [0.58, 0.70]
4 0.78 [0.66, 0.86] 0.65 [0.58, 0.70]
5 0.77 [0.64, 0.85] 0.64 [0.58, 0.70]
6 0.76 [0.63, 0.85] 0.65 [0.58, 0.70]
7 0.75 [0.62, 0.84] 0.64 [0.58, 0.70]
Syn G1,D0 0.81 [0.70, 0.88] 0.62 [0.55, 0.68]
G2,D0 0.82 [0.72, 0.89] 0.55 [0.48, 0.62]
G3,D0 0.81 [0.71, 0.88] 0.62 [0.56, 0.68]
G1,D1 0.82 [0.72, 0.89] 0.62 [0.55, 0.68]
G2,D1 0.82 [0.73, 0.89] 0.62 [0.55, 0.68]
G3,D1 0.82 [0.72, 0.88] 0.62 [0.55, 0.68]
CW+ 4; G1,D0 0.88 [0.81, 0.93] 0.66 [0.59, 0.71]
Syn 4; G2,D0 0.87 [0.80, 0.92] 0.64 [0.57, 0.70]
4; G3,D0 0.86 [0.77, 0.91] 0.63 [0.56, 0.69]
4; G1,D1 0.83 [0.73, 0.89] 0.48 [0.40, 0.56]
4; G2,D1 0.83 [0.73, 0.89] 0.49 [0.40, 0.56]
4; G3,D1 0.82 [0.72, 0.89] 0.48 [0.40, 0.56]
Table 1: Spearman correlation results for the various WordNet-based
models and distributional models. CW=Context Windows, BoW=bag
of words, Syn=syntactic vectors. For Syn, the window size is actually
the tree-depth for the governors and descendants. For examples, G1
indicates that the contexts include the parents and D2 indicates that both
the children and grandchildren make up the contexts. The final grouping
includes both contextual windows (at width 4) and syntactic contexts in
the template vectors. Max scores are bolded.
1. Replace each non-English word in the dataset
with its 5-best translations into English using
state-of-the-art machine translation technology.
2. The vector corresponding to each Spanish word
is calculated by collecting features from all the
contexts of any of its translations.
3. Once the vectors are generated, the similarities
are calculated in the same way as before.
4 Experimental results
4.1 Gold-standard datasets
We have used two standard datasets. The first
one, RG, consists of 65 pairs of words collected by
Rubenstein and Goodenough (1965), who had them
judged by 51 human subjects in a scale from 0.0 to
4.0 according to their similarity, but ignoring any
other possible semantic relationships that might ap-
pear between the terms. The second dataset, Word-
Sim3535 (Finkelstein et al, 2002) contains 353 word
pairs, each associated with an average of 13 to 16 hu-
man judgements. In this case, both similarity and re-
5Available at http://www.cs.technion.ac.il/
?gabr/resources/data/wordsim353/wordsim353.html
Context RG terms and frequencies
ll never forget the * on his face when grin,2,smile,10
he had a giant * on his face and grin,3,smile,2
room with a huge * on her face and grin,2,smile,6
the state of every * will be updated every automobile,2,car,3
repair or replace the * if it is stolen automobile,2,car,2
located on the north * of the Bay of shore,14,coast,2
areas on the eastern * of the Adriatic Sea shore,3,coast,2
Thesaurus of Current English * The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2
wizard,4,glass,4,crane,5,smile,5
implement,5,oracle,2,lad,2
food,3,car,2,madhouse,3,jewel,3
asylum,4,tool,8,journey,6,etc.
be understood that the * 10 may be designed crane,3,tool,3
a fight between a * and a snake and bird,3,crane,5
Table 2: Sample of context windows for the terms in the RG dataset.
latedness are annotated without any distinction. Sev-
eral studies indicate that the human scores consis-
tently have very high correlations with each other
(Miller and Charles, 1991; Resnik, 1995), thus val-
idating the use of these datasets for evaluating se-
mantic similarity.
For the cross-lingual evaluation, the two datasets
were modified by translating the second word in
each pair into Spanish. Two humans translated
simultaneously both datasets, with an inter-tagger
agreement of 72% for RG and 84% for Word-
Sim353.
4.2 Results
Table 1 shows the Spearman correlation obtained on
the RG and WordSim353 datasets, including the in-
terval at 0.95 of confidence6.
Overall the distributional context-window ap-
proach performs best in the RG, reaching 0.89 corre-
lation, and both WN30g and the combination of con-
text windows and syntactic context perform best on
WordSim353. Note that the confidence intervals are
quite large in both RG and WordSim353, and few of
the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of
the glosses and WordNet 3.0 (WN30g) yields the
best results in both datasets. While MCR16 is close
to WN30g for the RG dataset, it lags well behind
on WordSim353. This discrepancy is further ana-
lyzed is Section 5.3. Note that the performance of
WordNet in the WordSim353 dataset suffers from
unknown words. In fact, there are nine pairs which
returned null similarity for this reason. The num-
6To calculate the Spearman correlations values are trans-
formed into ranks, and we calculate the Pearson correlation on
them. The confidence intervals refer to the Pearson correlations
of the rank vectors.
22
Figure 1: Effect of the size of the training corpus, for the best distributional similarity model in each dataset. Left: WordSim353 with bag-of-words,
Right: RG with context windows.
Dataset Method overall ? interval
RG MCR16 0.78 -0.05 [0.66, 0.86]
WN30g 0.74 -0.09 [0.61, 0.84]
Bag of words 0.68 -0.23 [0.53, 0.79]
Context windows 0.83 -0.05 [0.73, 0.89]
WS353 MCR16 0.42 (0.53) -0.11 (-0.03) [0.34, 0.51]
WN30g 0.58 (0.67) -0.07 (-0.02) [0.51, 0.64]
Bag of words 0.53 -0.12 [0.45, 0.61]
Context windows 0.52 -0.11 [0.44, 0.59]
Table 3: Results obtained by the different methods on the Span-
ish/English cross-lingual datasets. The ? column shows the perfor-
mance difference with respect to the results on the original dataset.
ber in parenthesis in Table 1 for WordSim353 shows
the results for the 344 remaining pairs. Section 5.2
shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-
gether terms that can have a similar distribution of
contextual terms. Therefore, terms that are topically
related can appear in the same textual passages and
will get high values using this model. We see this
as an explanation why this model performed better
than the context window approach for WordSim353,
where annotators were instructed to provide high
ratings to related terms. On the contrary, the con-
text window approach tends to group together words
that are exchangeable in exactly the same context,
preserving order. Table 2 illustrates a few exam-
ples of context collected. Therefore, true synonyms
and hyponyms/hyperonyms will receive high simi-
larities, whereas terms related topically or based on
any other semantic relation (e.g. movie and star) will
have lower scores. This explains why this method
performed better for the RG dataset. Section 5.3
confirms these observations.
4.3 Cross-lingual similarity
Table 3 shows the results for the English-Spanish
cross-lingual datasets. For RG, MCR16 and the
context windows methods drop only 5 percentage
points, showing that cross-lingual similarity is feasi-
ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is
the best for this dataset, with the rest of the meth-
ods falling over 10 percentage points relative to the
monolingual experiment. A closer look at the Word-
Net results showed that most of the drop in perfor-
mance was caused by out-of-vocabulary words, due
to the smaller vocabulary of the Spanish WordNet.
Though not totally comparable, if we compute the
correlation over pairs covered in WordNet alne, the
correlation would drop only 2 percentage points. In
the case of the distributional approaches, the fall in
performance was caused by the translations, as only
61% of the words were translated into the original
word in the English datasets.
5 Detailed analysis and system
combination
In this section we present some analysis, including
learning curves for distributional methods, the use
of distributional similarity to improve WordNet sim-
ilarity, the contrast between similarity and related-
ness, and the combination of methods.
5.1 Learning curves for distributional methods
Figure 1 shows that the correlation improves with
the size of the corpus, as expected. For the re-
sults using the WordSim353 corpus, we show the
results of the bag-of-words approach with context
size 10. Results improve from 0.5 Spearman correla-
tion up to 0.65 when increasing the corpus size three
orders of magnitude, although the effect decays at
the end, which indicates that we might not get fur-
23
Method Without similar words With similar words
WN30 0.56 (0.58) [0.48, 0.63] 0.58 [0.51, 0.65]
WN30g 0.66 (0.69) [0.59, 0.71] 0.68 [0.62, 0.73]
Table 4: Results obtained replacing unknown words with their most
similar three words (WordSim353 dataset).
Method overall Similarity Relatedness
MCR16 0.53 [0.45, 0.60] 0.65 [0.56, 0.72] 0.33 [0.21, 0.43]
WN30 0.56 [0.48, 0.63] 0.73 [0.65, 0.79] 0.38 [0.27, 0.48]
WN30g 0.66 [0.59, 0.71] 0.72 [0.64, 0.78] 0.56 [0.46, 0.64]
BoW 0.65 [0.59, 0.71] 0.70 [0.63, 0.77] 0.62 [0.53, 0.69]
CW 0.60 [0.53, 0.66] 0.77 [0.71, 0.82] 0.46 [0.36, 0.55]
Table 5: Results obtained on the WordSim353 dataset and on the two
similarity and relatedness subsets.
ther gains going beyond the current size of the cor-
pus. With respect to results for the RG dataset, we
used a context-window approach with context radius
4. Here, results improve even more with data size,
probably due to the sparse data problem collecting
8-word context windows if the corpus is not large
enough. Correlation improves linearly right to the
end, where results stabilize around 0.89.
5.2 Combining both approaches: dealing with
unknown words in WordNet
Although the vocabulary of WordNet is very ex-
tensive, applications are bound to need the similar-
ity between words which are not included in Word-
Net. This is exemplified in the WordSim353 dataset,
where 9 pairs contain words which are unknown to
WordNet. In order to overcome this shortcoming,
we could use similar words instead, as provided by
the distributional thesaurus. We used the distribu-
tional thesaurus defined in Section 3, using context
windows of width 4, to provide three similar words
for each of the unknown words in WordNet. Results
improve for both WN30 and WN30g, as shown in
Table 4, attaining our best results for WordSim353.
5.3 Similarity vs. relatedness
We mentioned above that the annotation guidelines
of WordSim353 did not distinguish between simi-
lar and related pairs. As the results in Section 4
show, different techniques are more appropriate to
calculate either similarity or relatedness. In order to
study this effect, ideally, we would have two ver-
sions of the dataset, where annotators were given
precise instructions to distinguish similarity in one
case, and relatedness in the other. Given the lack
of such datasets, we devised a simpler approach in
order to reuse the existing human judgements. We
manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-
ing synonyms of each other, antonyms, iden-
tical, hyperonym-hyponym, hyponym-hyperonym,
holonym-meronym, meronym-holonym, and none-
of-the-above. The inter-tagger agreement rate was
0.80, with a Kappa score of 0.77. This anno-
tation was used to group the pairs in three cate-
gories: similar pairs (those classified as synonyms,
antonyms, identical, or hyponym-hyperonym), re-
lated pairs (those classified as meronym-holonym,
and pairs classified as none-of-the-above, with a hu-
man average similarity greater than 5), and unrelated
pairs (those classified as none-of-the-above that had
average similarity less than or equal to 5). We then
created two new gold-standard datasets: similarity
(the union of similar and unrelated pairs), and relat-
edness (the union of related and unrelated)7.
Table 5 shows the results on the relatedness and
similarity subsets of WordSim353 for the different
methods. Regarding WordNet methods, both WN30
and WN30g perform similarly on the similarity sub-
set, but WN30g obtains the best results by far on
the relatedness data. These results are congruent
with our expectations: two words are similar if their
synsets are in close places in the WordNet hierarchy,
and two words are related if there is a connection
between them. Most of the relations in WordNet
are of hierarchical nature, and although other rela-
tions exist, they are far less numerous, thus explain-
ing the good results for both WN30 and WN30g on
similarity, but the bad results of WN30 on related-
ness. The disambiguated glosses help find connec-
tions among related concepts, and allow our method
to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some
comments. Given the fact that MCR16 performed
very well on the RG dataset, it comes as a surprise
that it performs so poorly for the similarity subset
of WordSim353. In an additional evaluation, we at-
tested that MCR16 does indeed perform as well as
MCR30g on the similar pairs subset. We believe
that this deviation could be due to the method used to
construct the similarity dataset, which includes some
pairs of loosely related pairs labeled as unrelated.
7Available at http://alfonseca.org/eng/research/wordsim353.html
24
Methods combined in the SVM RG dataset WordSim353 dataset WordSim353 similarity WordSim353 relatedness
WN30g, bag of words 0.88 [0.82, 0.93] 0.78 [0.73, 0.81] 0.81 [0.76, 0.86] 0.72 [0.65, 0.77]
WN30g, context windows 0.90 [0.84, 0.94] 0.73 [0.68, 0.79] 0.83 [0.78, 0.87] 0.64 [0.56, 0.71]
WN30g, syntax 0.89 [0.83, 0.93] 0.75 [0.70, 0.79] 0.83 [0.78, 0.87] 0.67 [0.60, 0.74]
WN30g, bag of words, context windows, syntax 0.96 [0.93, 0.97] 0.78 [0.73, 0.82] 0.83 [0.78, 0.87] 0.71 [0.65, 0.77]
Table 6: Results using a supervised combination of several systems. Max values are bolded for each dataset.
Concerning the techniques based on distributional
similarities, the method based on context windows
provides the best results for similarity, and the bag-
of-words representation outperforms most of the
other techniques for relatedness.
5.4 Supervised combination
In order to gain an insight on which would be the up-
per bound that we could obtain when combining our
methods, we took the output of three systems (bag
of words with window size 10, context window with
size 4, and the WN30g run). Each of these outputs is
a ranking of word pairs, and we implemented an or-
acle that chooses, for each pair, the rank that is most
similar to the rank of the pair in the gold-standard.
The outputs of the oracle have a Spearman correla-
tion of 0.97 for RG and 0.92 for WordSim353, which
gives as an indication of the correlations that could
be achieved by choosing for each pair the rank out-
put by the best classifier for that pair.
The previous results motivated the use of a su-
pervised approach to combine the output of the
different systems. We created a training cor-
pus containing pairs of pairs of words from the
datasets, having as features the similarity and rank
of each pair involved as given by the differ-
ent unsupervised systems. A classifier is trained
to decide whether the first pair is more simi-
lar than the second one. For example, a train-
ing instance using two unsupervised classifiers is
0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative
meaning that the similarities given by the first clas-
sifier to the two pairs were 0.001364 and 0.327515
respectively, which ranked them in positions 31 and
64. The second classifier gave them similarities of
0.084805 and 0.109061 respectively, which ranked
them in positions 57 and 59. The class negative in-
dicates that in the gold-standard the first pair has a
lower score than the second pair.
We have trained a SVM to classify pairs of pairs,
and use its output to rank the entries in both datasets.
It uses a polynomial kernel with degree 4. We did
Method Source Spearman (MC) Pearson (MC)
(Sahami et al, 2006) Web snippets 0.62 [0.32, 0.81] 0.58 [0.26, 0.78]
(Chen et al, 2006) Web snippets 0.69 [0.42, 0.84] 0.69 [0.42, 0.85]
(Wu and Palmer, 1994) WordNet 0.78 [0.59, 0.90] 0.78 [0.57, 0.89]
(Leacock et al, 1998) WordNet 0.79 [0.59, 0.90] 0.82 [0.64, 0.91]
(Resnik, 1995) WordNet 0.81 [0.62, 0.91] 0.80 [0.60, 0.90]
(Lin, 1998a) WordNet 0.82 [0.65, 0.91] 0.83 [0.67, 0.92]
(Bollegala et al, 2007) Web snippets 0.82 [0.64, 0.91] 0.83 [0.67, 0.92]
(Jiang and Conrath, 1997) WordNet 0.83 [0.67, 0.92] 0.85 [0.69, 0.93]
(Jarmasz, 2003) Roget?s 0.87 [0.73, 0.94] 0.87 [0.74, 0.94]
(Patwardhan et al, 2006) WordNet n/a 0.91
(Alvarez and Lim, 2007) WordNet n/a 0.91
(Yang and Powers, 2005) WordNet 0.87 [0.73, 0.91] 0.92 [0.84, 0.96]
(Hughes et al, 2007) WordNet 0.90 n/a
Personalized PageRank WordNet 0.89 [0.77, 0.94] n/a
Bag of words Web corpus 0.85 [0.70, 0.93] 0.84 [0.69, 0.93]
Context window Web corpus 0.88 [0.76, 0.95] 0.89 [0.77, 0.95]
Syntactic contexts Web corpus 0.76 [0.54, 0.88] 0.74 [0.51, 0.87]
SVM Web, WN 0.92 [0.84, 0.96] 0.93 [0.85, 0.97]
Table 7: Comparison with previous approaches for MC.
not have a held-out set, so we used the standard set-
tings of Weka, without trying to modify parameters,
e.g. C. Each word pair is scored with the number
of pairs that were considered to have less similar-
ity using the SVM. The results using 10-fold cross-
validation are shown in Table 6. A combination of
all methods produces the best results reported so far
for both datasets, statistically significant for RG.
6 Related work
Contrary to the WordSim353 dataset, common prac-
tice with the RG dataset has been to perform the
evaluation with Pearson correlation. In our believe
Pearson is less informative, as the Pearson correla-
tion suffers much when the scores of two systems are
not linearly correlated, something which happens
often given due to the different nature of the tech-
niques applied. Some authors, e.g. Alvarez and Lim
(2007), use a non-linear function to map the system
outputs into new values distributed more similarly
to the values in the gold-standard. In their case, the
mapping function was exp (?x4 ), which was chosenempirically. Finding such a function is dependent
on the dataset used, and involves an extra step in the
similarity calculations. Alternatively, the Spearman
correlation provides an evaluation metric that is in-
dependent of such data-dependent transformations.
Most similarity researchers have published their
25
Word pair M&C SVM Word pair M&C SVM
automobile, car 3.92 62 crane, implement 1.68 26
journey, voyage 3.84 54 brother, lad 1.66 39
gem, jewel 3.84 61 car, journey 1.16 37
boy, lad 3.76 57 monk, oracle 1.1 32
coast, shore 3.7 53 food, rooster 0.89 3
asylum, madhouse 3.61 45 coast, hill 0.87 34
magician, wizard 3.5 49 forest, graveyard 0.84 27
midday, noon 3.42 61 monk, slave 0.55 17
furnace, stove 3.11 50 lad, wizard 0.42 13
food, fruit 3.08 47 coast, forest 0.42 18
bird, cock 3.05 46 cord, smile 0.13 5
bird, crane 2.97 38 glass, magician 0.11 10
implement, tool 2.95 55 rooster, voyage 0.08 1
brother, monk 2.82 42 noon, string 0.08 5
Table 8: Our best results for the MC dataset.
Method Source Spearman
(Strube and Ponzetto, 2006) Wikipedia 0.19?0.48
(Jarmasz, 2003) WordNet 0.33?0.35
(Jarmasz, 2003) Roget?s 0.55
(Hughes and Ramage, 2007) WordNet 0.55
(Finkelstein et al, 2002) Web corpus, WN 0.56
(Gabrilovich and Markovitch, 2007) ODP 0.65
(Gabrilovich and Markovitch, 2007) Wikipedia 0.75
SVM Web corpus, WN 0.78
Table 9: Comparison with previous work for WordSim353.
complete results on a smaller subset of the RG
dataset containing 30 word pairs (Miller and
Charles, 1991), usually referred to as MC, making it
possible to compare different systems using differ-
ent correlation. Table 7 shows the results of related
work on MC that was available to us, including our
own. For the authors that did not provide the de-
tailed data we include only the Pearson correlation
with no confidence intervals.
Among the unsupervised methods introduced in
this paper, the context window produced the best re-
ported Spearman correlation, although the 0.95 con-
fidence intervals are too large to allow us to accept
the hypothesis that it is better than all others meth-
ods. The supervised combination produces the best
results reported so far. For the benefit of future re-
search, our results for the MC subset are displayed
in Table 8.
Comparison on the WordSim353 dataset is eas-
ier, as all researchers have used Spearman. The
figures in Table 9) show that our WordNet-based
method outperforms all previously published Word-
Net methods. We want to note that our WordNet-
based method outperforms that of Hughes and Ram-
age (2007), which uses a similar method. Although
there are some differences in the method, we think
that the main performance gain comes from the use
of the disambiguated glosses, which they did not
use. Our distributional methods also outperform all
other corpus-based methods. The most similar ap-
proach to our distributional technique is Finkelstein
et al (2002), who combined distributional similar-
ities from Web documents with a similarity from
WordNet. Their results are probably worse due to
the smaller data size (they used 270,000 documents)
and the differences in the calculation of the simi-
larities. The only method which outperforms our
non-supervised methods is that of (Gabrilovich and
Markovitch, 2007) when based on Wikipedia, prob-
ably because of the dense, manually distilled knowl-
edge contained in Wikipedia. All in all, our super-
vised combination gets the best published results on
this dataset.
7 Conclusions and future work
This paper has presented two state-of-the-art dis-
tributional and WordNet-based similarity measures,
with a study of several parameters, including per-
formance on similarity and relatedness data. We
show that the use of disambiguated glosses allows
for the best published results for WordNet-based
systems on the WordSim353 dataset, mainly due to
the better modeling of relatedness (as opposed to
similarity). Distributional similarities have proven
to be competitive when compared to knowledge-
based methods, with context windows being better
for similarity and bag of words for relatedness. Dis-
tributional similarity was effectively used to cover
out-of-vocabulary items in the WordNet-based mea-
sure providing our best unsupervised results. The
complementarity of our methods was exploited by
a supervised learner, producing the best results so
far for RG and WordSim353. Our results include
confidence values, which, surprisingly, were not in-
cluded in most previous work, and show that many
results over RG and WordSim353 are indistinguish-
able. The algorithm for WordNet-base similarity
and the necessary resources are publicly available8.
This work pioneers cross-lingual extension and
evaluation of both distributional and WordNet-based
measures. We have shown that closely aligned
wordnets provide a natural and effective way to
compute cross-lingual similarity with minor losses.
A simple translation strategy also yields good results
for distributional methods.
8http://ixa2.si.ehu.es/ukb/
26
References
E. Agirre and A. Soroa. 2009. Personalizing pager-
ank for word sense disambiguation. In Proc. of EACL
2009, Athens, Greece.
M.A. Alvarez and S.J. Lim. 2007. A Graph Modeling
of Semantic Similarity between Words. Proc. of the
Conference on Semantic Computing, pages 355?362.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The meaning multi-
lingual central repository. In Proc. of Global WordNet
Conference, Brno, Czech Republic.
D. Bollegala, Matsuo Y., and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using web
search engines. In Proceedings of WWW?2007.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based Measures of Lexical Semantic Relatedness.
Computational Linguistics, 32(1):13?47.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of COCLING/ACL 2006.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets using structural information. In Proceedings of
ACL?2000, Hong Kong.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press,
Cambridge, Mass.
L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. 2002. Plac-
ing Search in Context: The Concept Revisited. ACM
Transactions on Information Systems, 20(1):116?131.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proc of IJCAI, pages 6?12.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02: Proceedings of the 11th international con-
ference on World Wide Web, pages 517?526.
T. Hughes and D. Ramage. 2007. Lexical semantic re-
latedness with random graph walks. In Proceedings of
EMNLP-CoNLL-2007, pages 581?589.
M. Jarmasz. 2003. Roget?s Thesuarus as a lexical re-
source for Natural Language Processing.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of International Conference on Research
in Computational Linguistics, volume 33. Taiwan.
C. Leacock and M. Chodorow. 1998. Combining local
context and WordNet similarity for word sense iden-
tification. WordNet: An Electronic Lexical Database,
49(2):265?283.
D. Lin. 1998a. An information-theoretic definition of
similarity. In Proc. of ICML, pages 296?304, Wiscon-
sin, USA.
D. Lin. 1998b. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of ACL-98.
G.A. Miller and W.G. Charles. 1991. Contextual corre-
lates of semantic similarity. Language and Cognitive
Processes, 6(1):1?28.
J. Nivre. 2006. Inductive Dependency Parsing, vol-
ume 34 of Text, Speech and Language Technology.
Springer.
S. Pado and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Re-
latedness of Concepts. In Proceedings of the EACL
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Pycholinguistics Together,
pages 1?8, Trento, Italy.
P. Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity in a Taxonomy. Proc. of IJCAI,
14:448?453.
H. Rubenstein and J.B. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
M Ruiz-Casado, E. Alfonseca, and P. Castells. 2005.
Using context-window overlapping in Synonym Dis-
covery and Ontology Extension. In Proceedings of
RANLP-2005, Borovets, Bulgaria,.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. Proc. of WWW, pages 377?386.
M. Strube and S.P. Ponzetto. 2006. WikiRelate! Com-
puting Semantic Relatedness Using Wikipedia. In
Proceedings of the AAAI-2006, pages 1419?1424.
P.D. Turney. 2001. Mining the Web for Synonyms: PMI-
IR versus LSA on TOEFL. Lecture Notes in Computer
Science, 2167:491?502.
P. Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers.
Z. Wu and M. Palmer. 1994. Verb semantics and lex-
ical selection. In Proc. of ACL, pages 133?138, Las
Cruces, New Mexico.
D. Yang and D.M.W. Powers. 2005. Measuring semantic
similarity in the taxonomy of WordNet. Proceedings
of the Australasian conference on Computer Science.
27
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 765?774,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Integrating sentence- and word-level error identification
for disfluency correction
Erin Fitzgerald
Johns Hopkins University
Baltimore, MD, USA
erinf@jhu.edu
Frederick Jelinek
Johns Hopkins University
Baltimore, MD, USA
jelinek@jhu.edu
Keith Hall
Google Inc.
Z?urich, Switzerland
kbhall@google.com
Abstract
While speaking spontaneously, speakers
often make errors such as self-correction
or false starts which interfere with the
successful application of natural language
processing techniques like summarization
and machine translation to this data. There
is active work on reconstructing this error-
ful data into a clean and fluent transcript
by identifying and removing these simple
errors.
Previous research has approximated the
potential benefit of conducting word-level
reconstruction of simple errors only on
those sentences known to have errors. In
this work, we explore new approaches
for automatically identifying speaker con-
struction errors on the utterance level, and
quantify the impact that this initial step has
on word- and sentence-level reconstruc-
tion accuracy.
1 Introduction
A system would accomplish reconstruction of its
spontaneous speech input if its output were to rep-
resent, in flawless, fluent, and content-preserving
text, the message that the speaker intended to con-
vey. While full speech reconstruction would likely
require a range of string transformations and po-
tentially deep syntactic and semantic analysis of
the errorful text (Fitzgerald, 2009), in this work we
will attempt only to resolve less complex errors,
correctable by deletion alone, in a given manually-
transcribed utterance.
The benefit of conducting word-level recon-
struction of simple errors only on those sen-
tences known to have errors was approximated in
(Fitzgerald et al, 2009). In the current work, we
explore approaches for automatically identifying
speaker-generated errors on the utterance level,
and calculate the gain in accuracy that this initial
step has on word- and sentence-level accuracy.
1.1 Error classes in spontaneous speech
Common simple disfluencies in sentence-like ut-
terances (SUs) include filler words (i.e., ?um?, ?ah?,
and discourse markers like ?you know?), as well as
speaker edits consisting of a reparandum, an inter-
ruption point (IP), an optional interregnum (like ?I
mean?), and a repair region (Shriberg, 1994), as
seen in Figure 1.
[that
?
s]
? ?? ?
reparandum
IP
????
+ {uh}
????
interregnum
that
?
s
? ?? ?
repair
a relief
Figure 1: Typical edit region structure.
These reparanda, or edit regions, can be classified
into three main groups:
1. In a repetition (above), the repair phrase is
approximately identical to the reparandum.
2. In a revision, the repair phrase alters reparan-
dum words to correct the previously stated
thought.
EX1: but [when he] + {i mean} when she put it
that way
EX2: it helps people [that are going to quit] + that
would be quitting anyway
3. In a restart fragment an utterance is aborted
and then restarted with a new train of thought.
EX3: and [i think he?s] + he tells me he?s glad he
has one of those
EX4: [amazon was incorporated by] {uh} well i
only knew two people there
In simple cleanup (a precursor to full speech re-
construction), all detected filler words are deleted,
and the reparanda and interregna are deleted while
the repair region is left intact. This is a strong ini-
tial step for speech reconstruction, though more
765
1 he that ?s uh that ?s a relief
2 E E E FL - - - -
3 NC RC RC FL - - - -
Figure 2: Example of word class and refined word
class labels, where - denotes a non-error, FL de-
notes a filler, E generally denotes reparanda, and
RC and NC indicate rough copy and non-copy
speaker errors, respectively. Line 3 refines the la-
bels of Line 2.
complex and less deterministic changes may be
required for generating fluent and grammatical
speech text in all cases.
1.2 Related Work
Stochastic approaches for simple disfluency de-
tection use features such as lexical form, acous-
tic cues, and rule-based knowledge. State-of-
the-art methods for edit region detection such as
(Johnson and Charniak, 2004; Zhang and Weng,
2005; Kahn et al, 2005; Honal and Schultz, 2005)
model speech disfluencies as a noisy channel
model, though direct classification models have
also shown promise (Fitzgerald et al, 2009; Liu
et al, 2004). The final output is a word-level tag-
ging of the error condition of each word in the se-
quence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach,
referred to in this document as JC04, combines
the noisy channel paradigm with a tree-adjoining
grammar (TAG) to capture approximately re-
peated elements. The TAG approach models the
crossed word dependencies observed when the
reparandum incorporates the same or very simi-
lar words in roughly the same word order, which
JC04 refer to as a rough copy. Line 3 of Figure
2 refines ?edits? (E) into rough copies (RC) and
non-copies (NC).
As expected given the assumptions of the
TAG approach, JC04 identifies repetitions and
most revisions in spontaneous data, but is less
successful in labeling false starts and other
speaker self-interruptions without cross-serial cor-
relations. These non-copy errors hurt the edit de-
tection recall and overall accuracy.
Fitzgerald et al (2009) (referred here as FHJ)
used conditional random fields (CRFs) and the
Spontaneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008) corpus for word-
level error identification, especially targeting im-
provement of these non-copy errors. The CRF was
trained using features based on lexical, language
model, and syntactic observations along with fea-
tures based on JC04 system output.
Alternate experimental setup showed that train-
ing and testing only on SUs known from the la-
beled corpus to contain word-level errors yielded
a notable improvement in accuracy, indicating that
the described system was falsely identifying many
non-error words as errors.
Improved sentence-level identification of error-
ful utterances was shown to help improve word-
level error identification and overall reconstruction
accuracy. This paper describes attempts to extend
these efforts.
2 Approach
2.1 Data
We conducted our experiments on the recently re-
leased Spontaneous Speech Reconstruction (SSR)
corpus (Fitzgerald and Jelinek, 2008), a medium-
sized set of disfluency annotations atop Fisher
conversational telephone speech data (Cieri et al,
2004)
1
. Advantages of the SSR data include
? aligned parallel original and cleaned sen-
tences
? several levels of error annotations, allowing
for a coarse-to-fine reconstruction approach
? multiple annotations per sentence reflecting
the occasional ambiguity of corrections
As reconstructions are sometimes non-
deterministic, the SSR provides two manual
reconstructions for each utterance in the data. We
use these dual annotations to learn complemen-
tary approaches in training and to allow for more
accurate evaluation.
The Spontaneous Speech Reconstruction cor-
pus is partitioned into three subcorpora: 17,162
training sentences (119,693 words), 2,191 sen-
tences (14,861 words) in the development set, and
2,288 sentences (15,382 words) in the test set. Ap-
proximately 17% of the total utterances contain a
reparandum-type error. In constructing the data,
two approaches were combined to filter out the
utterances considered most likely to be errorful
(6,384 in total) and only those SUs were manually
reconstructed. However the entire data set was in-
cluded in the distribution ? and used in training for
this work ? to maintain data balance.
1
The Spontaneous Speech Reconstruction corpus can be
downloaded from http://www.clsp.jhu.edu/PIRE/ssr.
766
The training of the TAG model for JC04, used
as a feature in this work, requires a very specific
data format, and thus is trained not with SSR but
with Switchboard (SWBD) data (Godfrey et al,
1992). Key differences in these corpora, besides
the granularity and form of their annotations, in-
clude:
? SSR aims to correct speech output, while
SWBD edit annotation aims to identify
reparandum structures specifically. SSR only
marks those reparanda which annotators be-
lieve must be deleted to generate a grammat-
ical and content-preserving reconstruction.
? SSR includes more complex error identifi-
cation and correction, not considered in this
work.
While the SWBD corpus has been used in
some previous simple disfluency labeling work
(e.g., Johnson and Charniak, 2004; Kahn et al,
2005), we consider the SSR for its fine-grained er-
ror annotations.
3 Identifying poor constructions
Prior to reconstruction, it is to our advantage to au-
tomatically identify poorly constructed sentences,
defined as being ungrammatical, incomplete, or
missing necessary sentence boundaries. Accu-
rately extracting ill-formed sentences prior to sub-
sentential error correction helps to minimize the
risk of information loss posed by unnecessarily
and incorrectly reconstructing well-formed text.
To evaluate the efforts described below, we
manually label each SU s in the SSR test set S
(including those not originally annotated with re-
constructions but still included in the SSR distri-
bution) as well-formed or poorly-formed, form-
ing the set of poorly constructed SUs P ? S,
|P | = 531 and |S| = 2288 utterances.
To identify speaker errors on the sentence level,
we consider and combine a collection of features
into a single framework using a maximum entropy
model (implemented with the Daum?e III (2004)
MEGA Model toolkit).
3.1 SU-level error features
Six feature types are presented in this section.
? Features #1 and #2 are the two methods in-
cluded in a similar though less exhaustive ef-
fort by (Fitzgerald and Jelinek, 2008) in error
filtering for the creation of the SSR corpus it-
self.
? Feature types #3 and #4 extract features from
automatic parses assigned to the given sen-
tence. It is expected that these parses will
contain some errors and the usefulness of
these features may be parser-specific. The
value of these features though is the con-
sistent, if not always accurate, treatment of
similar construction errores given a particu-
lar state-of-the-art parser.
? Feature type #5 investigates the relationship
between the probability of a SU-internal error
and the number of words it contains.
? Feature type #6 serves to bias the probabil-
ity against assigning a backchannel acknowl-
edgement SU as an error instance.
Feature #1 (JC04): Consider only sentences with
JC04 detected edit regions. This approach takes
advantage of the high precision, low recall JC04
disfluency detection approach described in Section
1.2. We apply the out-of-box JC04 system and
consider any sentence with one or more labeled
reparanda as a ?poor? indicator. Since speakers re-
pairing their speech once are often under a higher
cognitive load and thus more likely to make more
serious speech errors (in other words, there is a
higher probability of making an error given that an
error has already been made (Bard et al, 2001)).
This is a reasonable first order approach for find-
ing deeper problems.
Feature #2 (HPSG): Use deep linguistic parsers
to confirm well-formedness. Statistical context-
free parsers are highly robust and, due to smooth-
ing, can assign a non-zero probability syntac-
tic structure even for text and part-of-speech se-
quences never seen during training. However,
sometimes no output is preferable to highly er-
rorful output. Hand-built rule-based parsers can
produce extremely accurate and context-sensitive
syntactic structures, but are also brittle and do not
adapt well to never before seen input. We use this
inflexibility to our advantage.
Head-driven Phrase Structure Grammar
(HPSG) is a deep-syntax phrase structure gram-
mar which produces rich, non-context-free
syntactic analyses of input sentences based on
a collection of carefully constructed rules and
lexical item structures (Pollard and Sag, 1994;
Wahlster, 2000). Each utterance is parsed using
767
the PET deep parser produced by the inter-
institutional DELPH-IN group
2
. The manually
compiled English Resource Grammar (ERG)
(Flickinger, 2002) rules have previously been
extended for the Verbmobil (Wahlster, 2000)
project to allow for the parsing of basic conversa-
tional elements such as SUs with no verb or basic
backchannel acknowledgements like ?last thursday?
or ?sure?, but still produce strict HPSG parses
based on these rules. We use the binary result of
whether or not each SU is parsable by the HPSG
ERG as binary indicator functions in our models.
There has been some work on producing partial
parses for utterances for which a full HPSG analy-
sis is not deemed possible by the grammar (Zhang
et al, 2007). This work has shown early promise
for identifying coherent substrings within error-
ful SUs given subjective analysis; as this technol-
ogy progresses, HPSG may offer informative sub-
sentential features for word-level error analysis as
well.
Feature #3 (Rules): Mark unseen phrase rule ex-
pansions. Phrase-based parses are composed of
a recursive sequence of non-terminal (NT) rule ex-
pansions, such as those detailed for the example
parse shown in Figure 3. These rules are learned
from training data such as the Switchboard tree-
bank, where telephone conversation transcripts
were manually parsed. In many statistical parsers,
new structures are generated based on the relative
frequencies of such rules in the training treebank,
conditioned on the terminal words and some local
context, and the most probable parse (roughly the
joint probability of its rule expansions) is selected.
Because parsers are often required to produce
output for words and contexts never seen in the
training corpus, smoothing is required. The
Charniak (1999) parser accomplishes this in part
through a Markov grammar which works top-
down, expanding rules to the left and right of an
expansion head M of a given rule. The non-
terminal (NT) M is first predicted from the parent
P , then ? in order ?L
1
throughL
m
(stopping sym-
bol ?#?) and R
1
through R
n
(again ?#?), as shown
in Equation 1.
parent P ? #L
m
. . . L
1
MR
1
. . . R
n
# (1)
In this manner, it is possible to produce rules
never before seen in the training treebank. While
2
The DEep Linguistic Processing with HPSG INitiative
(see http://www.delph-in.net/)
this may be required for parsing grammatical sen-
tences with rare elements, this SU-level error pre-
diction feature indicates whether the automatic
parse for a given SU includes an expansion never
seen in the training treebank. If an expansion rule
in the one-best parse was not seen in training (here
meaning in the SWBD treebank after EDITED
nodes have been removed), the implication is that
new rule generation is an indicator of a speaker
error within a SU.
Feature #4 (C-comm): Mark unseen rule c-
commanding NTs. In X? theory (Chomsky,
1970), lexical categories such as nouns and verbs
are often modified by a specifier (such as the DT ?a?
modifying the NN ?lot? in the NP
3
phrase in Figure
3 or an auxiliary verb for a verb in a verb phrase
(VBZ for VP
3
) and a complement (such as the ob-
ject of a verb NP
3
for VBG in the phrase VP
3
).
In each of these cases, an NT tree node A has
the following relationship with a second NT P :
? Neither does node A dominate P nor node P
dominateA, (i.e., neither is directly above the
other in the parse tree), and
? Node A immediately precedes P in the tree
(precedence is represented graphically in left-
to-right order in the tree).
Given these relationships, we say that A locally
c-commands P and its descendants. We further
extend this definition to say that, if node
?
A is the
only child of nodeA (a unary expansion) andA lo-
cally c-commands P , then
?
A locally c-commands
P (so both [SBAR ? S] and [S ? NP
2
VP
2
] are
c-commanded by VBP). See Figure 3 for other ex-
amples of non-terminal nodes in c-commanding
relationships, and the phrase expansion rule they
c-command.
The c-command relationship is fundamental in
syntactic theory, and has uses such as predicting
the scope of pronoun antecedents. In this case,
however, we use it to describe two nodes which are
in a specifier?category relationship or a category?
complement relationship (e.g., subject?verb and
verb?object, respectively). This is valuable to us
because it takes advantage of a weakness of sta-
tistical parsers: the context used to condition the
probability of a given rule expansion generally
does not reach beyond dominance relationships,
and thus parsers rarely penalize for the juxtapo-
sition of A c-commanding P and its children as
768
a) S
NP
1
PRP
they
VP
1
VBP
are
SBAR
S
NP
2
DT
that
VP
2
VBZ
is
VP
3
VBG
saying
NP
3
DT
a
NN
lot
b) Rules expansions:
S? NP VP
NP
1
? PRP
VP
1
? VBP SBAR
SBAR? S
S? NP
2
VP
2
NP
2
? DT
VP
2
? VBZ VP
VP
3
? VBG NP
NP
3
? DT NN
c) Rule expansions + c-commanding NT:
S? NP VP no local c-command
NP
1
? PRP no local c-command
VP
1
? V SBAR NP
1
SBAR? S VBP
S? NP
2
VP
2
VBP
NP
2
? DT no local c-command
VP
2
? VBZ VP NP
2
VP
3
? VBG NP VBZ
NP
3
? DT NN VBG
Figure 3: The automatically generated parse (a) for an errorful sentence-like unit (SU), with accompa-
nying rule expansions (b) and local c-commands (c). Non-terminal indices such as NP
2
are for reader
clarification only and are not considered in the feature extraction process.
long as they have previously seen NT type A pre-
ceding NT type P . Thus, we can use the children
of a parent node P as a way to enrich a NT type P
and make it more informative.
For example, in Figure 3, the rule [S ? NP
2
VP
2
] is routinely seen in the manual parses of
the SWBD treebank, as is [VP
1
? VBP SBAR].
However, it is highly unusual for VBP to immedi-
ately precede SBAR or S when this rule expands
to NP
2
VP
2
. So, not only does SBAR/S comple-
ment VBP, but a very specific type of [SBAR/S
? NP VP] is the complement of VBP. This con-
ditional infrequency serves as an indication of
deeper structural errors.
Given these category relationship observations,
we include in our maximum entropy model a fea-
ture indicating whether a given parse includes a
c-command relationship not seen in training data.
Feature #5 (Length): Threshold sentences based
on length. Empirical observation indicates that
long sentences are more likely to contain speaker
errors, while very short sentences tend to be
backchannel acknowledgments like ?yeah? or ?I
know? which are not considered errorful. Oviatt
(1995) quantifies this, determining that the dis-
fluency rate in human-computer dialog increases
roughly linearly with the number of words in an
utterance.
The length-based feature value for each sen-
tence therefore is defined to be the number of word
tokens in that sentence.
Feature #6 (Backchannel): Bias backchannel
acknowledgements as non-errors A backchan-
nel acknowledgement is a short sentence-like unit
(SU) which is produced to indicate that the speaker
is still paying attention to the other speaker, with-
out requesting attention or adding new content to
the dialog. These SUs include ?uh-huh?, ?sure?,
or any combination of backchannel acknowledge-
ments with fillers (ex. ?sure uh uh-huh?).
To assign this feature, fifty-two common
backchannel acknowledgement tokens are consid-
ered. The indicator feature is one (1) if the SU in
question is some combination of these backchan-
nel acknowledgements, and zero (0) otherwise.
3.2 SU-level error identification results
We first observe the performance of each feature
type in isolation in our maximum entropy frame-
work (Table 1(a)). The top-performing individual
769
Features included
Setup JC04 HPSG Rules C-comm Length Backchannel F
1
-score
a) Individual features
1
?
? ? ? ? ? 79.9
2 ?
?
? ? ? ? 77.1
5 ? ? ? ?
?
? 59.7
4 ? ? ?
?
? ? 42.2
3 ? ?
?
? ? ? 23.2
6 ? ? ? ? ?
?
0.0
b) All features combined
7
? ? ? ? ? ?
83.3
c) All-but-one
8 ?
? ? ? ? ?
78.4 (-4.9)
9
? ? ?
?
? ?
81.2 (-2.1)
10
?
?
? ? ? ?
81.3 (-2.0)
11
? ?
?
? ? ?
82.1 (-1.2)
12
? ? ? ? ?
? 82.9 (-0.4)
13
? ? ? ?
?
?
83.2 (-0.1)
Table 1: Comparison of poor construction identification features, tested on the SSR test corpus.
feature is the JC04 edit indicator, which is not sur-
prising as this is the one feature whose existence
was designed specifically to predict speaker errors.
Following JC04 in individual performance are the
HPSG parsability feature, length feature, and un-
seen c-command rule presence feature. Backchan-
nel acknowledgements had no predictive power on
their own. This was itself unsurprising as the fea-
ture was primarily meant to reduce the probability
of selecting these SUs as errorful.
Combining all rules together (Table 1(b)), we
note an F
1
-score gain of 3.4 as compared to the top
individual feature JC04. (JC04 has a precision of
97.6, recall of 67.6, and F of 79.9; the combined
feature model has a precision of 93.0, a recall of
75.3, and an F of 83.3, so unsurprisingly our gain
primarily comes from increased error recall).
In order to understand the contribution of an in-
dividual feature, it helps not only to see the pre-
diction results conditioned only on that feature,
but the loss in accuracy seen when only that fea-
ture is removed from the set. We see in Table 1(c)
that, though the c-command prediction feature was
only moderately accurate in predicting SU errors
on its own, it has the second largest impact after
JC04 (an F-score loss of 2.1) when removed from
the set of features. Such a change indicates the
orthogonality of the information within this fea-
ture to the other features studied. Length, on the
other hand, while moderately powerful as a sin-
gle indicator, had negligible impact on classifica-
tion accuracy when removed from the feature set.
This indicates that the relationship between error-
ful sentences and length can be explained away by
the other features in our set.
We also note that the combination of all features
excluding JC04 is competitive with JC04 itself.
Additional complementary features seem likely to
further compete with the JC04 prediction feature.
4 Combining efforts
The FHJ work shows that the predictive power of
a CRF model could greatly improve (given a re-
striction on only altering SUs suspected to contain
errors) from an F-score of 84.7 to as high as 88.7
for rough copy (RC) errors and from an F-score of
47.5 to as high as 73.8 for non-copy (NC) errors.
Now that we have built a model to predict con-
struction errors on the utterance level, we combine
the two approaches to analyze the improvement
possible for word-level identification (measured
again by precision, recall, and F-score) and for
SU-level correction (measured by the SU Match
metric defined in Section 4.2).
4.1 Word-level evaluation of error
identification, post SU filtering
We first evaluate edit detection accuracy on those
test SUs predicted to be errorful on a per-word ba-
sis. To evaluate our progress identifying word-
770
level error classes, we calculate precision, recall
and F-scores for each labeled class c in each exper-
imental scenario. As usual, these metrics are cal-
culated as ratios of correct, false, and missed pre-
dictions. However, to take advantage of the double
reconstruction annotations provided in SSR (and
more importantly, in recognition of the occasional
ambiguities of reconstruction) we modified these
calculations slightly to account for all references.
Analysis of word-level label evaluation, post SU
filtering. Word-level F
1
-score results for error
region identification are shown in Table 2.
By first automatically selecting testing as de-
scribed in Section 3 (with a sentence-level F-score
of 83.3, Table 1(b)), we see in Table 2 some gain in
F-score for all three error classes, though much po-
tential improvement remains based on the oracle
gain (rows indicated as having ?Gold errors? test-
ing data). Note that there are no results from train-
ing only on errorful data but testing on all data, as
this was shown to yield dramatically worse results
due to data mismatch issues.
Unlike in the experiments where all data was
used for testing and training, the best NC and RC
detection performance given the automatically se-
lected testing data was achieved when training a
CRF model to detect each class separately (RC
or NC alone) and not in conjunction with filler
word detection FL. As in FHJ, training RC and NC
models separately instead of in a joint FL+RC+NC
model yielded higher accuracy.
We notice also that the F-score for RC identi-
fication is lower when automatically filtering the
test data. There are two likely causes. The most
likely issue is that the automatic SU-error clas-
sifier filtered out some SUs with true RC errors
which had previously been correctly identified, re-
ducing the overall precision ratio as well as re-
call (i.e., we no longer receive accuracy credit for
some easier errors once caught). A second, related
possibility is that the errorful SUs identified by
the Section 3 method had a higher density of er-
rors that the current CRF word-level classification
model is unable to identify (i.e. the more difficult
errors are now a higher relative percentage of the
errors we need to catch). While the former pos-
sibility seems more likely, both causes should be
investigated in future work.
The F-score gain in NC identification from 42.5
to 54.6 came primarily from a gain in precision (in
the original model, many non-errorful SUs were
mistakenly determined to include errors). Though
capturing approximately 55% of the non-copy NC
errors (for SUs likely to have errors) is an im-
provement, this remains a challenging and un-
solved task which should be investigated further
in the future.
4.2 Sentence-level evaluation of error
identification and region deletion, post
SU identification
Depending on the downstream task of speech re-
construction, it may be imperative not only to
identify many of the errors in a given spoken ut-
terance, but indeed to identify all errors (and only
those errors), yielding the exact cleaned sentence
that a human annotator might provide.
In these experiments we apply simple cleanup
(as described in Section 1.1) to both JC04 out-
put and the predicted output for each experimental
setup, deleting words when their error class is a
filler, rough copy or non-copy.
Taking advantage of the dual annotations pro-
vided for each sentence in the SSR corpus, we
can report double-reference evaluation. Thus, we
judge that if a hypothesized cleaned sentence ex-
actly matches either reference sentence cleaned in
the same manner we count the cleaned utterance as
correct, and otherwise we assign no credit. We re-
port double-reference exact match evaluation be-
tween a given SU s and references r ? R, as de-
fined below.
SU match =
1
S
?
s?S
max
r?R
?(s, r) (2)
Analysis of sentence level evaluation, post SU
identification. Results from this second evalua-
tion of rough copy and non-copy error reconstruc-
tion can be seen in Table 3.
As seen in word-level identification results (Ta-
ble 2), automatically selecting a subset of testing
data upon which to apply simple cleanup recon-
struction does not perform at the accuracy shown
to be possible given an oracle filtering. While
measuring improvement is difficult (here, non-
filtered data is incomparable to filtered test data
results since a majority of these sentences require
no major deletions at all), we note again that our
methods (MaxEnt/FHJ-x) outperform the baseline
of deleting nothing but filled pauses like ?eh? and
?um?, as well as the state-of-the-art baseline JC04.
771
Class labeled Training SUs for Testing FL RC NC
All data All SU data 71.0 80.3 47.4
FL+RC+NC Errorful only Auto ID?d SU errors 87.9 79.9 49.0
Errorful only Gold SU errors 91.6 84.1 52.2
All data All SU data - - 42.5
NC Errorful only Auto ID?d SU errors - - 54.6
Errorful only Gold SU errors - - 73.8
All data All SU data 70.8 - 47.5
NC+FL Errorful only Auto ID?d SU errors 88.8 - 53.3
Errorful only Gold SU errors 90.7 - 69.8
All data All SU data - /84.2/ -
RC Errorful only Auto ID?d SU errors - 81.3 -
Errorful only Gold SU errors - 88.7 -
All data All SU data 67.8 /84.7/ -
RC+FL Errorful only Auto ID?d SU errors 88.1 80.5 -
Errorful only Gold SU errors 92.3 87.4 -
Table 2: Error predictions, post-SU identification: F
1
-score results. Automatically identified ?SUs for
testing? were determined via the maximum entropy classification model described earlier in this paper,
and feature set #7 from Table 1. Filler (FL), rough copy error (RC) and non-copy error (NC) results are
given in terms of word-level F
1
-score. Bold numbers indicate the highest performance post-automatic
filter for each of the three classes. Italicized values indicate experiments where no filtering outperformed
automatic filtering (for RC errors).
# SUs # SUs that %
Setup Classed deleted Testing (filt/unfilt) match ref accuracy
Baseline-1 only filled pauses All data 2288 1800 78.7%
JC04-1 E+FL All data 2288 1858 81.2%
MaxEnt/FHJ-1 FL+RC+NC All data 2288 1922 84.0%
Baseline-2 only filled pauses Auto ID?d 430 84 19.5%
JC04-2 E+FL Auto ID?d 430 187 43.5%
MaxEnt/FHJ-2 FL+RC+NC Auto ID?d 430 223 51.9%
Baseline-3 only filled pauses Gold errors 281 5 1.8%
JC04-3 E+FL Gold errors 281 126 44.8%
MaxEnt/FHJ-3 FL+RC+NC Gold errors 281 156 55.5%
Table 3: Error predictions, post-SU identification: Exact Sentence Match Results.
For the baseline, we delete only filled pause filler words like ?eh? and ?um?. For JC04 output, we deleted
any word assigned the class E or FL. Finally, for the MaxEnt/FHJ models, we used the jointly trained
FL+RC+NC CRF model and deleted all words assigned any of the three classes.
5 Future Work
While some success and improvements for the
automatic detection and deletion of fillers and
reparanda (i.e., ?simple cleanup?) have been
demonstrated in this work, much remains to be
done to adequately address the issues and criteria
considered here for full reconstruction of sponta-
neous speech.
Included features for both the word level and
SU-level error detection have only skimmed the
surface of potentially powerful features for spon-
taneous speech reconstruction. There should be
continued development of complementary parser-
based features (such as those from dependency
parsers or even deep syntax parsers such as im-
plementations of HPSG as well as additional syn-
tactic features based on automatic constituent or
context-free grammar based parsers). Prosodic
772
features, though demonstrated to be unnecessary
for at least moderately successful detection of sim-
ple errors, also hold promise for additional gains.
Future investigators should evaluate the gains pos-
sible by integrating this information into the fea-
tures and ideas presented here.
6 Summary and conclusions
This work was an extension of the results in FHJ,
which showed that automatically determining
which utterances contain errors before attempting
to identify and delete fillers and reparanda has the
potential to increase accuracy significantly.
In Section 3, we built a maximum entropy clas-
sification model to assign binary error classes to
spontaneous speech utterances. Six features ?
JC04, HPSG, unseen rules, unseen c-command re-
lationships, utterance length, and backchannel ac-
knowledgement composition ? were considered.
The combined model achieved a precision of 93.0,
a recall of 75.3, and an F
1
-score of 83.3.
We then, in Section 4, cascaded the sentence-
level error identification system output into the
FHJ word-level error identification and simple
cleanup system. This combination lead to non-
copy error identification with an F
1
-score of 54.6,
up from 47.5 in the experiments conducted on all
data instead of data identified to be errorful, while
maintaining accuracy for rough copy errors and in-
creasing filler detection accuracy as well. Though
the data setup is slightly different, the true errors
are common across both sets of SUs and thus the
results are comparable.
This work demonstrates that automatically se-
lecting a subset of SUs upon which to imple-
ment reconstruction improves the accuracy of non-
copy (restart fragment) reparanda identification
and cleaning, though less improvement results
from doing the same for rough copy identification.
Acknowledgments
The authors thank our anonymous reviewers for
their valuable comments. Support for this work
was provided by NSF PIRE Grant No. OISE-
0530118. Any opinions, findings, conclusions,
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the supporting agency.
References
Ellen G. Bard, Robin J. Lickley, and Matthew P. Aylett.
2001. Is disfluency just difficult? In Disfluencies in
Spontaneous Speech Workshop, pages 97?100.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Proceedings of the Annual Meet-
ing of the North American Association for Compu-
tational Linguistics.
Noam Chomsky, 1970. Remarks on nominalization,
pages 184?221. Waltham: Ginn.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Hal Daum?e III. 2004. Notes on CG and
LM-BFGS optimization of logistic regression.
Paper available at http://pub.hal3.name\
#daume04cg-bfgs, implementation available at
http://hal3.name/megam/, August.
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference.
Erin Fitzgerald, Keith Hall, and Frederick Jelinek.
2009. Reconstructing false start errors in sponta-
neous speech text. In Proceedings of the Annual
Meeting of the European Association for Computa-
tional Linguistics.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen,
Dan Flickinger, Jun?ichi Tsujii, and Hans Uszkoreit,
editors, Collaborative Language Engineering, pages
1?17. CSLI Publications, Stanford.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 517?520,
San Francisco.
Matthias Honal and Tanja Schultz. 2005. Au-
tomatic disfluency removal on recognized spon-
taneous speech ? rapid adaptation to speaker-
dependent disfluenices. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
773
Jeremy Kahn, Matthew Lease, Eugene Charniak, Mark
Johnson, and Mari Ostendorf. 2005. Effective use
of prosody in parsing conversational speech. In Pro-
ceedings of the Conference on Human Language
Technology, pages 561?568.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, and Mary Harper. 2004. The ICSI/UW
RT04 structural metadata extraction system. In Rich
Transcription Fall Workshop.
Sharon L. Oviatt. 1995. Predicting and managing
spoken disfluencies during human-computer interac-
tion. Computer Speech and Language, 9:19?35.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. University of Chiacgo
Press and CSLI Publications, Chicago and Stanford.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Wolfgang Wahlster, editor. 2000. Verbmobil: Foun-
dations of Speech-to-Speech Translation. Springer,
Berlin.
Qi Zhang and Fuliang Weng. 2005. Exploring fea-
tures for identifying edited regions in disfluent sen-
tences. In Proceedings of the International Work-
shop on Parsing Techniques, pages 179?185.
Yi Zhang, Valia Kordoni, and Erin Fitzgerald. 2007.
Partial parse selection for robust deep processing. In
Proceedings of ACL Workshop on Deep Linguistic
Processing, pages 128?135.
774
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1046?1055,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Gazpacho and summer rash:
lexical relationships from temporal patterns of web search queries
Enrique Alfonseca Massimiliano Ciaramita Keith Hall
Google
Z?urich, Switzerland
ealfonseca@google.com, massi@google.com, kbhall@google.com
Abstract
In this paper we investigate temporal pat-
terns of web search queries. We carry out
several evaluations to analyze the proper-
ties of temporal profiles of queries, reveal-
ing promising semantic and pragmatic re-
lationships between words. We focus on
two applications: query suggestion and
query categorization. The former shows
a potential for time-series similarity mea-
sures to identify specific semantic relat-
edness between words, which results in
state-of-the-art performance in query sug-
gestion while providing complementary
information to more traditional distribu-
tional similarity measures. The query cat-
egorization evaluation suggests that the
temporal profile alone is not a strong in-
dicator of broad topical categories.
1 Introduction
The temporal patterns of word occurrences in hu-
man communication carry an implicit measure of
their relationship to real-world events and behav-
ioral patterns. For example, when there is an event
affecting a given entity (such as a natural disaster
in a country), the entity name will turn up more
frequently in human conversation, newswire arti-
cles and web documents; and people will search
for it more often. Two entities that are closely
related in the real world, such as the name of a
country and a prominent region inside the coun-
try are likely to share common events and there-
fore be closely associated in human communica-
tion. Finally, two instances of the same class
are also likely to share common usage patterns.
For example, names of airlines or retail stores are
more likely to be used by day rather than by night
(Chien, 2005).
In this paper we explore the linguistic relation-
ship between phrases that are judged to be sim-
ilar based on their frequency time series correla-
tion in search query logs. For every phrase
1
avail-
able in WordNet 3.0
2
(Miller, 1995), we have ob-
tained its temporal signature from query logs, and
calculated all their pairwise correlations. Next,
we study the relationship in the top-ranked pairs
with respect to their distribution in WordNet and a
human-annotated labelling.
We also discuss possible applications of this
data to solve open problems and present the results
of two experiments: one where time series corre-
lations turned out to be highly discriminative; and
another where they were not particularly informa-
tive but shed some light on the nature of temporal
semantics and topical categorization:
? Query suggestion, i.e. given a query, generate
a ranked list of alternative queries in which
the user may be interested.
? Query categorization, i.e. given a predefined
set of categories, find the top categories to
which the query can be assigned.
Finally, we illustrate with an example another ap-
plication of time series in solving information ex-
traction problems.
Although query logs are typically proprietary
data, there are ongoing initiatives, like the Lemur
toolbar
3
, which make this kind of information
available for research purposes. Other work
(Bansal and Koudas, 2007b; Bansal and Koudas,
2007a) shows that temporal information can also
be extracted from public data, such as blogs. More
traditional types of text, such as news, are also typ-
ically associated with temporal labels; e.g., dates
and timestamps.
This paper is structured in the following way:
1
We use the term phrase to refer to any single word or
multi-word expression that belongs to a synset in WordNet.
Examples of phrases are person, causal entity or william
shakespeare. We focused on the nouns hierarchy only.
2
http://wordnet.princeton.edu
3
http://www.lemurproject.org/
querylogtoolbar/
1046
Section 2 summarizes the related work. Section 3
describes the correlation analysis between all pairs
of phrases from WordNet. Next, Section 4 de-
scribes the application to query suggestion, and
Section 5 the application to labelling queries in
topical categories. Section 7 summarizes the con-
clusions and outlines ideas for future research.
2 Related work
The study of query time series explores a particu-
lar instance of the so-called wisdom of the crowds
effect. Within this area, we can distinguish two
kinds of phenomena. Knowledge and resources
assembled by people explicitly, either individu-
ally, such as the case of blogs, or in a collabora-
tive way, as in forums or wikis. These resources
are valuable for human-consumption and can also
be exploited in order to learn computational re-
sources (Medelyan et al, 2008; Weld et al, 2008;
Zesch et al, 2008b; Zesch et al, 2008a). On
the other hand, it is possible to acquire useful re-
sources and knowledge from aggregating behav-
ioral patterns of large groups of people, even in
the absence of a conscious effort. There is exten-
sive ongoing research on the use of web search
usage patterns to develop knowledge resources.
Some examples are clustering co-click patterns to
learn semantically related queries (Beeferman and
Berger, 2000), combining co-click patterns with
hitting times (Mei et al, 2008), analyzing query
revisions made by users when querying search en-
gines (Jones et al, 2006), replacing query words
with other words that have the highest pointwise
mutual information (Terra and Clarke, 2004), or
using the temporal distribution of words in docu-
ments to improve ranking of search results (Jones
and Diaz, 2007).
Within this second category, an important area
is dedicated to the study of time-related features
of search queries. News aggregators use real-time
frequencies of user queries to detect spikes and
identify news shortly after the spikes occur (Mu-
rata, 2008). Web users? query patterns have also
proved useful for building a real-time surveillance
system that accurately estimates region-by-region
influenza activity with a lag of one day (Ginsberg
et al, 2009). Search engines specifically devel-
oped for real-time searches, like Twitter search,
will most likely provide new use cases and sce-
narios for quickly detecting trends in user search
query patterns.
Figure 1: Time series obtained for the queries
[gazpacho] and [summertime] (normalized
scales).
Our study builds upon the work of Chien
(2005), who observed that queries with highly-
correlated temporal usage patterns are typically
semantically related, and described a procedure
for calculating the correlations efficiently. We
have extended the analysis described in this work,
by performing a more extensive evaluation of the
kinds of semantic relationships that we can find
among temporally-similar queries. We also pro-
pose, to our knowledge for the first time, areas
of applications in solving well-established prob-
lems which shed some light on the nature of time-
based semantic similarity. This work is also re-
lated to the analysis of temporal properties of
information streams in data mining (Kleinberg,
2006) and information retrieval from time series
databases (Agrawal et al, 1993).
3 Time-based similarities between
phrases
Similarly to the method described in Chien (2005),
we take a time interval, divide it into equally
spaced subintervals, and represent each phrase of
interest as the sequence of frequencies with which
the phrase was observed in the subintervals. In
our experiments, we have used as source data
the set of fully anonymized query logs from the
Google search engine between January 1st, 2004
and March 1st, 2009.
4
.
These data have been aggregated on a daily ba-
sis so that we have the daily frequency of the
4
Part of this data is publicly available from http://
www.google.com/trends
1047
queries of interest for over five years. The frequen-
cies are then normalized with the total number of
queries that happened on that day. The normaliza-
tion is necessary to avoid daily and seasonal varia-
tions as there are typically more queries on week-
days than on weekends and fewer queries during
holiday seasons than in the rest of the year. It
also helps reducing the effect deriving from the
fact that the population with Internet access is still
monotonically growing, so we can expect that the
number of queries will become higher and higher
over time.
Given two phrases and their associated time se-
ries, the similarity metric used is the correlation
coefficient between the two series (Chien, 2005).
For illustration, Figure 1 shows the time series ob-
tained for two sample queries, gazpacho and sum-
mertime, whose time series yield a correlation of
0.92. Similar high correlations can be observed
with other queries related to phenomena that oc-
cur mainly in summer in the countries from which
most queries come, like summer rash.
3.1 WordNet-based evaluation
In this section, we describe a study carried out
with the purpose of discovering the traditional
lexico-semantic relationships which hold between
the queries that are most strongly related accord-
ing to their temporal profiles.
For this evaluation, we have taken the nomi-
nal phrases appearing in WordNet 3.0. Given that
users, when writing queries, typically do not pay
attention to punctuation and case, we have normal-
ized all phrases by lowercasing them and remov-
ing all punctuation. Next, we collected the time se-
ries for each phrase by computing the normalized
daily frequency of each of them as exact queries
in the query logs. The computation of the pair-
wise correlations was performed in parallel using
the MapReduce infrastructure running over 2048
cores with 500 MB of RAM each. The total ex-
ecution (including data shuffling and networking
time) took approximately three hours.
Next, we represented the data as a complete
graph where phrases are nodes and the edge be-
tween each pair of nodes is weighted by their time
series correlation. Using a simple graph-cut we
obtained clusters of related terms. A minimum
weight threshold equal to 0.9 was applied;
5
thus,
5
This threshold is the same used by Chien (2005), and was
confirmed after a manual inspection of a sample of the data
two phrases belong to the same cluster if there is
a path between them only via edges with weight
over 0.9.
The previous procedure produced a set of 604
clusters, with highly different sizes. The first ob-
servation is that 70% of the phrases in WordNet
do not have a correlation over 0.9 with any other
phrase, so they are placed alone in singleton clus-
ters. There are several reasons for this. The clus-
ters obtained are very specific: only phrases that
have a very strong temporal association have tem-
poral correlations exceeding the threshold. This is
combined with the fact that we are using a very
restricted vocabulary, namely the terms included
in WordNet, which is many orders of magnitude
smaller than the vocabulary of all possible queries
from the users. Few phrase pairs in WordNet
have a temporal association and popularity strong
enough to be clustered together. Finally, many of
the phrases in WordNet are rare, including scien-
tific names of animals and plants, genuses or fami-
lies, which are not commonly used. Therefore, the
clusters extracted here correspond to very salient
sets of phrases. If, instead of WordNet, we choose
a vocabulary from known user queries (cf. Sec-
tion 4), there would be many fewer singleton clus-
ters, as the options of similar phrases to choose
from would be much larger.
From the phrases that belong to clusters, 25%
of the WordNet phrases do not have strong daily
temporal profiles. The typical pattern for these
terms is an almost flat time series, usually with
small drops at summertime and Christmas (when
seasonal leisure-related queries dominate). There-
fore, these phrases were collected in just one clus-
ter containing them all. Typical examples of the
elements of this set are names of famous scientists
and mathematicians (Gauss, Isaac Newton, Al-
bert Einstein, Thomas Alva Edison, Hipprocrates,
Gregor Mendel, ...), common terms (fertilization,
famine, macroeconomics, genus, nationalism, ...),
numbers and common first names, among other
things. It is possible that using sub-day intervals
might help to discriminate within this cluster.
The items in this big cluster contrast with pe-
riodical events, which display recurring patterns
(e.g., queries related to elections or tax-returns),
and names of famous people and other entities
which appeared in the news in the past few years.
All of these are associated with irregular, spiky
time series. These constitute the final 5% of the
1048
Type Pairs Examples
Synonyms 283 (angel cake, angel food cake), (thames, river thames), (armistice day, Nov 11)
Hyponym/hyperonyms 86 (howard hughes, aviator), (muhammad, prophet), (olga korbut, gymnast)
Siblings in hyponym taxonomy 611 (hiroshima, nagasaki), (junior school, primary school), (aids, welt)
Meronym/holonyms 53 (tutsi, rwanda), (july 4, july), (pyongyang, north korea)
Siblings in meronymy taxonomy 7 (everglades, everglades national park), (mississipi, orleans)
Other paths 471 (maundy thursday, maundy money), (tap water, water tap), (gren party, liberal)
Not structurally related 1009 (poppy, veterans day), (olympic games, gimnast), (belmont park, horse racing)
Table 1: Relationships between pairs of WordNet phrases belonging to the same cluster.
phrases belonging to small, highly focused, clus-
ters.
Table 1 shows the relationships that hold be-
tween all pairs of phrases belonging to any of the
smaller clusters. Out of 2520 pairs, 283 belong
to the same synset, 697 are related via hyponymy
links, 60 via meronymy links, and 471 by alternat-
ing hyponymy and meronymy links in the path.
When the phrases were polysemous, the short-
est path between any of their meaning was used.
About 40% of the relations do not have a clear
structural interpretation in WordNet.
The majority of pairs are related via more or
less complex paths in the WordNet graph. Inter-
estingly, even the structurally unrelated terms are
characterized by transparent relations in terms of
world knowledge, as it is the case between poppy
and veteran day. Note as well that sometimes a
WordNet term is used with a meaning not present
in WordNet or in a different language, which may
explain why aids has a very high correlation with
welt (AIDS and welt are both hyponyms of health
problem, but the correlation may be explained bet-
ter by the AIDS World Day, Welt Aids Tag in Ger-
man), and it also has a very high correlation with
sida, defined in WordNet as a genus of tropical
herbs, but which is in fact the translation of AIDS
into Spanish. These observations motivated an ad-
ditional manual labelling of the extracted pairs.
3.2 Hand labelled evaluation
As can be seen in Table 2, most of the terms that
constitute a cluster are related to each other, al-
though the kinds of semantic relationships that
hold between them can vary significantly. Exam-
ples of the following kinds can be observed:
? True synonyms, as in the case of november
and nov, or architeuthis and giant squid.
? Variations of people names, especially if a
person?s first name or surname is typically
used to refer to that person, as in the case of
john lennon and lennon, or janis joplin and
joplin. Sometimes the variations include per-
sonal titles, as it is the case of president carter
and president nixon, which are highly corre-
lated with jimmy carter and richard nixon.
? Geographically-related terms, referring to
locations which are located close to each
other, as in the clusters {korea, north ko-
rean, south korea, pyongyang, north korea}
and {strasbourg, grenoble, toulouse, poitiers,
lyon, lille, nantes, reims}.
? Synonyms of location names, like bahrain
and bahrein.
? Derived words, like north korea and north
korean, or lebanese and lebanon.
? Generic word optionalizations, which hap-
pen when one word in a multi-word phrase
is very correlated to the phrase, as in the
case of spanish inquisition and inquisition,
or red bone marrow and red marrow, where
the most common interpretation for the short-
ened version of the phrase is the same as for
the long version.
? Word reordering, where the two related
phrases have the same words in a different or-
der, as in the case of maple sugar and sugar
maple, or oil palm and palm oil.
? Morphological variants: WordNet does not
contain many morphological variants in the
main dataset, but there are a few, like station
of the cross and stations of the cross.
? Acronyms, like federal emergency manage-
ment agency and fema.
? Hyperonym-hyponym, like fern and plant.
? Sibling terms in a taxonomy, as in the clus-
ter {lutheran, methodist, presbyterian, united
methodist church, lutheran church,methodist
church, presbyterian church,baptist, baptist
church}, which contains mostly names of
Christian denominations.
? Co-occurring events in time, as is the case
of hitch and pacifier, both titles of movies
which were launched at almost the same
1049
hydrant,fire hydrant
inauguration day,inauguration,swearing,investiture,inaugural address,inaugural,benediction,oath
indulgence,self indulgence
insulation,heating
interstate highway,interstate, intestine,small intestine
iq,iq test
irish people,irish,irish potato,irish gaelic,gaelic,irish soda bread,irish stew,st patrick,saint patrick,leprechaun,
march 17,irish whiskey,shillelagh
ironsides,old ironsides
james,joyce,james joyce
janis joplin,joplin
jesus christ,pilate,pontius pilate,passion of christ,passion,aramaic
jewish new year,rosh hashana,rosh hashanah,shofar
john lennon,lennon
julep,mint julep,kentucky derby,kentucky
keynote,keynote address
kickoff,time off
korea,north korean,south korea,pyongyang,north korea
l ron hubbard,scientology
leap,leap year,leap day,february 29
left brain,right brain
leftover,leftovers,turkey stew
linseed oil,linseed
listeria,listeriosis,maple leaf
lobster tail,lobster,tails
lohan,lindsay
loire,rhone,rhone alpes
looking,looking for
lutheran,methodist,presbyterian,united methodist church,lutheran church,methodist church,presbyterian church,
baptist,baptist church
mahatma gandhi,mahatma
malignant hyperthermia,hyperthermia
maple sugar,sugar maple
martin luther,martin luther king,luther,martin,martin luther king day
matzo,matzah,matzoh,passover,seder,matzo meal,pesach,haggadah,gefilte fish
mestizo,half blood,half and half
meteorology,weather bureau
moslem,muslim,prophet,mohammed,mohammad,muhammad,mahomet
movie star,star,revenge,film star,menace,george lucas
mt st helens,mount saint helens,mount st helens
myeloma,multiple myeloma
ness,loch ness,loch ness monster,loch,nessie
new guinea,papua new guinea,papua
november,nov
pacifier,hitch
papa,pope,vatican,vatican city,karol wojtyla,john paul ii,holy see,pius xii,papacy,paul vi,john xxiii,the holy see,
vatican ii,pontiff,gulp,pater,nostradamus,ii,pontifex
parietal lobe,glioma,malignant tumor
particle accelerator,atom smasher,hadron,large,tallulah bankhead,bankhead,tanner
pledge,allegiance
president carter,jimmy carter
president nixon,richard nixon,richard m nixon
sept 11,september 11,sep 11,twin towers,wtc,ground zero,world trade center
slum,millionaire,pinto
strasbourg,grenoble,toulouse,poitiers,lyon,lille,nantes,reims
valentine,valentine day,february 14,romantic
aeon,flux
alien,predator
anne hathaway,hathaway
architeuthis,giant squid
basal temperature,basal body temperature
execution,saddam hussein,hussein,saddam,hanging,husain
flood,flooding
george herbert walker bush,george walker bush
intifada,palestine
may 1,may day,maypole
Table 2: Sample of clusters obtained from the temporal correlations.
1050
Type Clusters
True synonyms 19
Variations of people names 42
People names with and without titles 4
First name and surname from the same person 4
Geographically-related terms 18
Synonyms of location names 4
Derived words 4
Word optionalizations 87
Word reordering 7
Morphological variants 1
Acronyms 1
Cross-language synonyms 3
Hyperonym/hyponym 10
Sibling terms 10
Co-ocurring events in time 8
Topically related 38
Unrelated 72
Table 3: Results of the manual annotation of 2-
item clusters.
time. A particular example of this is when
the two terms are part of a named entity, as in
the case of quantum and solace, which have
a similar correlation because they appear to-
gether in a movie title.
? Topically-related terms, as the cluster
{jesus christ, pilate, pontius pilate, passion of
christ, passion, aramaic}, or the cluster con-
taining popes and the Vatican. A similar ex-
ample, execution is highly correlated to sad-
dam hussein, because his execution attracted
more interest worldwide during this time pe-
riod than any other execution. Interestingly,
topical correlation emerges at very specific
granularity.
For the manual analysis of the results, we ran-
domly selected 332 clusters containing only two
items (so that 664 phrases were considered in to-
tal). Each of these pairs has been classified in one
of the previous categories. The results of this anal-
ysis are shown in Table 3.
4 Application to query suggestion
Query suggestion is a feature of search engines
that helps users reformulate queries in order to bet-
ter describe their information need with the pur-
pose of reducing the time needed to find the de-
sired information (Beeferman and Berger, 2000;
Kraft and Zien, 2004; Sahami and Heilman, 2006;
Cucerzan and White, 2007; Yih and Meek, 2008).
In this section, we explore the application of a sim-
ilarity metric based on time series correlations for
finding related queries to suggest to the users.
As a test set, we have used the query sugges-
Method P@1 P@3 P@5 mAP
Random 0.37 0.37 0.37 0.43
Web Kernel 0.51 0.47 0.42 0.51
Dist. simil. 0.72 0.63 0.60 0.64
Time series 0.74 0.63 0.53 0.67
Combination 0.79 0.68 0.60 0.69
Table 4: Results for the query suggestion task.
tion dataset from (Alfonseca et al, 2009). It con-
tains a set of 57 queries and an average of 22 can-
didate query suggestions for each of them. Each
suggestion was rated by two human raters using
the 5-point Likert scale defined in (Sahami and
Heilman, 2006), from irrelevant to highly relevant.
The task involves providing a ranking of the sug-
gestions that most closely resembles the human
scores. The evaluation is based on standard IR
metrics: precision at 1, 3 and 5, and mean average
precision. In order to compute the precision- and
recall-based metrics, we infer a binary distinction
from the ratings: related or not related. The inter-
annotator agreement for this dataset given the bi-
nary classification as computed by Cohen?s Kappa
is 0.6171.
We used three baselines: the average values that
would be produced by a random scorer of the can-
didate suggestions, Sahami and Heilman (2006)?s
system (based on calculating similarities between
the retrieved snippets), and a recent competitive
ranker based on calculating standard distributional
similarities (Alfonseca et al, 2009) between the
original query and the suggestion. Please refer to
the referenced work for details.
In order to produce the ranked lists of candi-
date suggestions for each query, due to the lack of
training data, we have opted for the unsupervised
procedure described in the previous section:
1. Collect the daily time series of each of the
queries and the candidate suggestions.
2. Calculate the correlation between the original
query and each of the candidate suggestions
provided for it, and use it as the candidate?s
score.
3. For each query, rank its candidate sugges-
tions in decreasing order of correlation.
Finally, taking into account that the source of
similarity is very different to the one used for dis-
tributional similarity, we tested the hypothesis that
1051
a combination of the two techniques would be ben-
eficial to capture different features of the queries
and suggestions. We have trained a linear mixture
model combining both scores (time series and dis-
tributional similarities), using 10-fold cross vali-
dation.
The results are displayed in Table 4. For eval-
uating the results, whenever a system produced a
tie between several suggestions, we generated 100
random orderings of the elements in the tie, and
report the average scores.
Using distributional similarities and the tempo-
ral series turned out to be indistinguishable for the
precision scores at 0.95 confidence, and both are
significantly better than the similarity metric based
on the web kernel. The combination produced an
improvement across all metrics, although not sta-
tistically significant at p=0.05.
This is quite a positive finding as the time series
method relies on stored information requiring only
simple and highly optimized lookups.
5 Application to query categorization
The results from the manual evaluation in Sec-
tion 3.2 support the conclusion that time series
from query logs provide powerful signals for clus-
tering at a fine-grained level, in some cases un-
covering synonyms (may 1st, may day) and even
causal relations (insulation, heating). A natural
question is if temporal information is correlated
with other types of categorizations. In this sec-
tion we carry out a preliminary exploration of the
relation between query time series and query cat-
egorization. To this extent we adapt the data from
the KDD 2005 CUP (Li et al, 2005), which pro-
vides a set of queries classified into 67 broad topi-
cal categories. Since the data is rather sparse (678
queries) we applied Fourier analysis to ?smooth?
the time series.
5.1 The KDD CUP data
The KDD Cup 2005
6
introduced a query catego-
rization task and dataset consisting of 800,000 un-
labeled queries for unsupervised training, and an
evaluation set of 911 queries, 111 for development
and 800 for the final evaluation. The systems sub-
mitted for this task can be quite complex and made
full use of the large unlabeled set. Our goal here is
not to provide a comparative evaluation, but only
6
http://www.sigkdd.org/kdd2005/kddcup.
html
?
101234567
TIME
STANDARDIZED FREQUENCY  
 
Figure 2: RDFT reconstruction for the query
?brush cutters? using the first 25 Fourier coeffi-
cients. The squares represent the original time
series datapoints, while the continuous line repre-
sents the reconstructed signal.
to use the labelled data
7
in a simplified manner to
better understand the semantic properties of query
time series. Each query in the dataset is assessed
by three editors who can assign multiple topic la-
bels from a set of 67 categories belonging to seven
broad topics: Computers, Entertainment, Informa-
tion, Living, Online Community, Shopping and
Sports. We merged the KDD Cup development
and test set, out of the 911 queries we were able to
retrieve significant temporal information for 678
queries. We joined the sets of labels from each as-
sessor for each query. On average, each query is
assigned five labels.
5.2 DFT analysis
Assessing the similarity of data represented as
time series has been addressed mostly my means
of Fourier analysis; e.g., Agrawal et al (1993) in-
troduce a method for efficiently retrieving time
series from databases based on Discrete Fourier
Transform (DFT). Several other methods have
been proposed, e.g., Discrete Wavelet Trans-
form (DWT), however DFT provide a competitive
benchmark approach (Wu et al, 2000).
We use DFT to generate the Fourier coefficients
of the time series and Reverse DFT (RDFT) to re-
construct the original signal using only a subset
of the coefficients. This analysis effectively com-
presses the time series producing a smoother ap-
proximate representation. DFT can be computed
efficiently via Fast Fourier Transform (FFT), with
7
The KDD Cup dataset is probably the only public query
log providing topical categorization information.
1052
Method Accuracy ? std-err
Random 0.107 0.03
MostFrequent 0.490 0.07
DFT-c10 0.425 0.06
DFT-c50 0.456 0.05
DFT-c100 0.502 0.05
DFT-c200 0.456 0.04
DFT-c400 0.506 0.05
DFT-c600 0.481 0.06
DFT-c800 0.478 0.04
DFT-c1000 0.466 0.05
Table 5: Results of the KDD dataset exploration.
complexityO(n log n) where n is the length of the
sequence. The approximate representation is use-
ful not only to address sparsity but can also be used
to efficiently estimate the similarity of two time
series using only a small subset of coefficients as
in (Agrawal et al, 1993). As an example, Fig-
ure 2 shows the original time series for the query
?brush cutters? and its reconstructed signal using
only the first 25 Fourier coefficients. The recon-
structed signal captures the essence of the period-
icity of the query and highlights the yearly peaks
registered for the query in spring and summer.
5.3 Experiment and discussion
To explore the correlation between the structured
temporal representation of queries provided by the
time series and topical categorization we run the
following experiment. Each KDD Cup query was
reconstructed via RDFT using a variable number
of coefficients. The set of 679 queries was parti-
tioned in 10 sets and a 10-fold evaluation was per-
formed. For each fold we trained a classifier on the
remaining 9 folds. We used an average multi-class
perceptron (Freund and Schapire, 1999) adapted to
multi-label learning (Crammer and Singer, 2003).
Each model was trained on a fixed number of 10
iterations. The accuracy of each model was eval-
uated as the fraction of test items for which the
selected highest scoring class was in the gold stan-
dard set provided by the editors. As a lower bound
we estimated the accuracy of randomly choosing
a label for each test instance, and as a baseline we
used the most frequent label. The latter is a pow-
erful predictor: baselines based on class frequency
outperformmost of the systems that participated in
the KDD Cup (Lin and Wu, 2009).
Table 5 reports the average accuracy over the
10 runs with relative standard errors. Each DFT-
based model is characterized by the number of co-
efficients used for the reconstruction. Two main
patterns are noticeable. First, none of the differ-
ences between the frequency-based baseline and
the DFT models is significant, this seems to indi-
cate that temporal structure alone is not a good dis-
criminator of topic, at least of broad categories. In
retrospect, this is somewhat predictable. The tem-
poral dimension is a basic semantic component of
lexical meaning and world knowledge which is not
necessarily associated with any broad, and to some
extent subjective, categorization. An inspection of
the patterns found in each category shows in fact
that similar patterns often emerge in different cat-
egories; e.g., ?Halloween costume? and ?cheese-
cake recipe? have a similar yearly periodical pat-
tern with spikes in early winter, while monotoni-
cally decaying patterns are shared across all cate-
gories; e.g., between computer hardware and kids
toys.
The second interesting finding is the trend of
the DFT system results, higher at low-intermediate
values, providing some initial promising evidence
that DFT analysis generates useful compressed
representations which could be indexed and ap-
plied efficiently. Notice that the sequences recon-
structed using 1,000 coefficients reproduce almost
identically the original signals.
6 Applications in information extraction
Time series from query logs are particularly rel-
evant for phrases that refer to entities which are
involved in recent events. Therefore, we expect
them to be useful for solving other applications
that require handling entities, such as named en-
tity recognition and classification, relation extrac-
tion or disambiguation.
To illustrate this point, we mention an example
of relation extraction between actors and movies:
movies usually have spikes when they are re-
leased, and then the frequency again drops sharply.
At the same times, when a movie is released, the
search engine users have a renewed interest in
their actors. Figure 3 displays the time series for
the five most recent movies by Jim Carrey (as of
march 2009), and the time series for Jim Carrey.
As can be seen, the spikes are at exactly the same
points in time. If we add up the series (a) through
(e) into a single series and calculate the correlation
with (f), it turns out to be very high (0.88).
1053
(a) (b) (c)
(d) (e) (f)
Figure 3: Time series obtained for the five most recent movies with Jim Carrey, and (f) time serie for the
query [jim carrey] (normalized scales).
System Precision Recall F-measure
Random 0.24 0.14 0.17
Time series 0.53 0.66 0.57
Table 6: Results for the query suggestion task.
To validate the hypothesis that this data should
be useful for identifying related entities, we have
performed a small experiment in the following
way: by choosing five popular actors
8
and the cin-
ema movies in which they appear since the year
2004, obtained from IMDB
9
. Using the time se-
ries, for each actor we choose the combination of
movies such that, by adding up the time series of
those movies, we maximise the correlation with
the actor?s time series. It has been implemented
with a greedy beam search, with a beam size of
100. The results are shown in Table 6. The random
baseline randomly associates the movies from the
dataset with the five actors.
We do not believe this to be a perfect feature as,
for example, actors may have a peak in the time se-
ries related to their personal lives, not necessarily
to movies. However, the high correlations that can
be obtained when the pairing between actors and
movies is correct, and the improvement with re-
spect a random baseline, indicates this is a feature
which can probably be integrated with other re-
lation extraction systems when handling relation-
ships between entities that have big temporal de-
pendencies.
8
Ben Stiller, Edward Norton, Jim Carrey, Leonardo Di-
caprio, and Tom Hanks.
9
www.imdb.com.
7 Conclusions and future work
This paper explores the relationships between
queries whose associated time series obtained
from query logs are highly correlated. The use
of time series in semantic similarity has been dis-
cussed by Chien (2005), but only a very prelimi-
nary evaluation was described, and, to our knowl-
edge, they had never been applied and evaluated
in solving existing problems. Our results indicate
that, for a substantial percentage of phrases in a
thesaurus, it is possible to find other highly-related
phrases; and we have categorized the kind of se-
mantic relationships that hold between them.
We have found that in a query suggestion
task, somewhat surprisingly, results are compara-
ble with other state-of-the-art techniques based on
distributional similarities. Furthermore, informa-
tion obtained from time series seems to be com-
plementary with them, as a simple combination of
similarity metrics produces an important increase
in performance..
From an analysis on a query categorization task
the initial evidence suggests that there is no strong
correlation between broad topics and temporal
profiles. This agrees with the intuition that time
provides a fundamental semantic dimension possi-
bly orthogonal to broad topical classification. This
issue however deserves further investigation. An-
other issue which is worth a deeper investigation
is the application of Fourier transform methods
which offer tools for studying the periodic struc-
ture of the temporal sequences.
1054
References
R. Agrawal, C. Faloutsos, and A.N. Swami. 1993. Ef-
ficient similarity search in sequence databases. In
Proceedings of the 4th International Conference on
Foundations of Data Organization and Algorithms,
pages 69?84.
E. Alfonseca, K. Hall, and S. Hartmann. 2009. Large-
scale computation of distributional similarities for
queries. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics
- Human Language Technologies conference.
N. Bansal and N. Koudas. 2007a. BlogScope: a sys-
tem for online analysis of high volume text streams.
In Proceedings of the 33rd international conference
on Very large data bases, pages 1410?1413.
N. Bansal and N. Koudas. 2007b. BlogScope: Spatio-
temporal analysis of the blogosphere. In Proceed-
ings of the 16th international conference on World
Wide Web, pages 1269?1270.
D. Beeferman and A. Berger. 2000. Agglomerative
clustering of a search engine query log. In Proceed-
ings of the sixth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 407?416.
S. Chien. 2005. Semantic similarity between search
engine queries using temporal correlation. In Pro-
ceedings of the 14th international conference on
World Wide Web, pages 2?11.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. Journal
of Machine Learning Research, 3:951?991.
S. Cucerzan and R.W. White. 2007. Query sugges-
tion based on user landing pages. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 875?876.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
J. Ginsberg, M.H. Mohebbi, R.S. Patel, L. Brammer,
M.S. Smolinski, and L. Brilliant. 2009. Detecting
influenza epidemics using search engine query data.
Nature, 457, February.
R. Jones and F. Diaz. 2007. Temporal profiles of
queries. ACM Transactions on Information Systems,
25(3):14.
R. Jones, B. Rey, O. Madani, and W. Greiner. 2006.
Generating query substitutions. In Proceedings of
the 15th international conference on World Wide
Web, pages 387?396.
J. Kleinberg. 2006. Temporal dynamics of on-line in-
formation streams. In Data Stream Management:
Processing High-Speed Data. Springer.
R. Kraft and J. Zien. 2004. Mining anchor text for
query refinement. In Proceedings of the 13th inter-
national conference on World Wide Web, pages 666?
674.
Y. Li, Z. Zheng, and H. Dai. 2005. KDD Cup-2005
report: Facing a grat challenge. SIGKDD Explor.
Newsl., 7(2):91?99.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics and the International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing.
O. Medelyan, C. Legg, D. Milne, and I.H. Witten.
2008. Mining meaning from Wikipedia. Dept. of
Computer Science, University of Waikato.
Q. Mei, D. Zhou, and K. Church. 2008. Query sug-
gestion using hitting time. In Proceeding of the
17th ACM conference on Information and knowl-
edge management, pages 469?478.
G.A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39?
41.
T. Murata. 2008. Detection of breaking news from
online web search queries. New Generation Com-
puting, 26(1):63?73.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international
conference on World Wide Web, pages 377?386.
E. Terra and C.L.A. Clarke. 2004. Scoring missing
terms in information retrieval tasks. In Proceedings
of the thirteenth ACM international conference on
Information and knowledge management, pages 50?
58.
D.S. Weld, F. Wu, E. Adar, S. Amershi, J. Fogarty,
R. Hoffmann, K. Patel, and M. Skinner. 2008. In-
telligence in Wikipedia. In Proceedings of the 23rd
Conference on Artificial Intelligence.
Y. Wu, D. Agrawal, and A. El Abbadi. 2000. A com-
parison of DFT and DWT based similarity search
in time-series databases. In Proceedings of the 9th
International ACM Conference on Information and
Knowledge Management, pages 488?495.
W. Yih and C. Meek. 2008. Consistent Phrase Rel-
evance Measures. Workshop on Data Mining and
Audience Intelligence for Advertising, page 37.
T. Zesch, C. Muller, and I. Gurevych. 2008a. Extract-
ing lexical semantic knowledge from Wikipedia and
Wiktionary. In Proceedings of the Conference on
Language Resources and Evaluation.
T. Zesch, C. Muller, and I. Gurevych. 2008b. Using
Wiktionary for computing semantic relatedness. In
Proceedings of the Conference on Artificial Intelli-
gence, pages 861?867.
1055
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 255?263,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Reconstructing false start errors in spontaneous speech text
Erin Fitzgerald
Johns Hopkins University
Baltimore, MD, USA
erinf@jhu.edu
Keith Hall
Google, Inc.
Zurich, Switzerland
kbhall@google.com
Frederick Jelinek
Johns Hopkins University
Baltimore, MD, USA
jelinek@jhu.edu
Abstract
This paper presents a conditional ran-
dom field-based approach for identifying
speaker-produced disfluencies (i.e. if and
where they occur) in spontaneous speech
transcripts. We emphasize false start re-
gions, which are often missed in cur-
rent disfluency identification approaches
as they lack lexical or structural similar-
ity to the speech immediately following.
We find that combining lexical, syntac-
tic, and language model-related features
with the output of a state-of-the-art disflu-
ency identification system improves over-
all word-level identification of these and
other errors. Improvements are reinforced
under a stricter evaluation metric requiring
exact matches between cleaned sentences
annotator-produced reconstructions, and
altogether show promise for general re-
construction efforts.
1 Introduction
The output of an automatic speech recognition
(ASR) system is often not what is required for sub-
sequent processing, in part because speakers them-
selves often make mistakes (e.g. stuttering, self-
correcting, or using filler words). A cleaner speech
transcript would allow for more accurate language
processing as needed for natural language process-
ing tasks such as machine translation and conver-
sation summarization which often assume a gram-
matical sentence as input.
A system would accomplish reconstruction of
its spontaneous speech input if its output were
to represent, in flawless, fluent, and content-
preserving text, the message that the speaker in-
tended to convey. Such a system could also be ap-
plied not only to spontaneous English speech, but
to correct common mistakes made by non-native
speakers (Lee and Seneff, 2006), and possibly ex-
tended to non-English speaker errors.
A key motivation for this work is the hope that a
cleaner, reconstructed speech transcript will allow
for simpler and more accurate human and natu-
ral language processing, as needed for applications
like machine translation, question answering, text
summarization, and paraphrasing which often as-
sume a grammatical sentence as input. This ben-
efit has been directly demonstrated for statistical
machine translation (SMT). Rao et al (2007) gave
evidence that simple disfluency removal from tran-
scripts can improve BLEU (a standard SMT eval-
uation metric) up to 8% for sentences with disflu-
encies. The presence of disfluencies were found to
hurt SMT in two ways: making utterances longer
without adding semantic content (and sometimes
adding false content) and exacerbating the data
mismatch between the spontaneous input and the
clean text training data.
While full speech reconstruction would likely
require a range of string transformations and po-
tentially deep syntactic and semantic analysis of
the errorful text (Fitzgerald, 2009), in this work
we will first attempt to resolve less complex errors,
corrected by deletion alone, in a given manually-
transcribed utterance.
We build on efforts from (Johnson et al, 2004),
aiming to improve overall recall ? especially of
false start or non-copy errors ? while concurrently
maintaining or improving precision.
1.1 Error classes in spontaneous speech
Common simple disfluencies in sentence-like ut-
terances (SUs) include filler words (i.e. ?um?, ?ah?,
and discourse markers like ?you know?), as well as
speaker edits consisting of a reparandum, an inter-
ruption point (IP), an optional interregnum (like ?I
mean?), and a repair region (Shriberg, 1994), as
seen in Figure 1.
255
[that?s]
? ?? ?
reparandum
IP
????
+ {uh}
????
interregnum
that?s? ?? ?
repair
a relief
Figure 1: Typical edit region structure. In these
and other examples, reparandum regions are in
brackets (?[?, ?]?), interregna are in braces (?{?,
?}?), and interruption points are marked by ?+?.
These reparanda, or edit regions, can be classified
into three main groups:
1. In a repetition (above), the repair phrase is
approximately identical to the reparandum.
2. In a revision, the repair phrase alters reparan-
dum words to correct the previously stated
thought.
EX1: but [when he] + {i mean} when she put it
that way
EX2: it helps people [that are going to quit] + that
would be quitting anyway
3. In a restart fragment (also called a false
start), an utterance is aborted and then
restarted with a new train of thought.
EX3: and [i think he?s] + he tells me he?s glad he
has one of those
EX4: [amazon was incorporated by] {uh} well i
only knew two people there
In simple cleanup (a precursor to full speech re-
construction), all detected filler words are deleted,
and the reparanda and interregna are deleted while
the repair region is left intact. This is a strong ini-
tial step for speech reconstruction, though more
complex and less deterministic changes are of-
ten required for generating fluent and grammatical
speech text.
In some cases, such as the repetitions men-
tioned above, simple cleanup is adequate for re-
construction. However, simply deleting the identi-
fied reparandum regions is not always optimal. We
would like to consider preserving these fragments
(for false starts in particular) if
1. the fragment contains content words, and
2. its information content is distinct from that in
surrounding utterances.
In the first restart fragment example (EX3 in Sec-
tion 1.1), the reparandum introduces no new ac-
tive verbs or new content, and thus can be safely
deleted. The second example (EX4) however
demonstrates a case when the reparandum may be
considered to have unique and preservable con-
tent of its own. Future work should address how
to most appropriately reconstruct speech in this
and similar cases; this initial work will for risk
information loss as we identify and delete these
reparandum regions.
1.2 Related Work
Stochastic approaches for simple disfluency de-
tection use features such as lexical form, acoustic
cues, and rule-based knowledge. Most state-of-
the-art methods for edit region detection such as
(Johnson and Charniak, 2004; Zhang and Weng,
2005; Liu et al, 2004; Honal and Schultz, 2005)
model speech disfluencies as a noisy channel
model. In a noisy channel model we assume that
an unknown but fluent string F has passed through
a disfluency-adding channel to produce the ob-
served disfluent string D, and we then aim to re-
cover the most likely input string F? , defined as
F? = argmaxFP (F |D)
= argmaxFP (D|F )P (F )
where P (F ) represents a language model defin-
ing a probability distribution over fluent ?source?
strings F , and P (D|F ) is the channel model defin-
ing a conditional probability distribution of ob-
served sentences D which may contain the types
of construction errors described in the previous
subsection. The final output is a word-level tag-
ging of the error condition of each word in the se-
quence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach,
referred to in this document as JC04, combines
the noisy channel paradigm with a tree-adjoining
grammar (TAG) to capture approximately re-
peated elements. The TAG approach models the
crossed word dependencies observed when the
reparandum incorporates the same or very similar
words in roughly the same word order, which JC04
refer to as a rough copy. Our version of this sys-
tem does not use external features such as prosodic
classes, as they use in Johnson et al (2004), but
otherwise appears to produce comparable results
to those reported.
While much progress has been made in sim-
ple disfluency detection in the last decade, even
top-performing systems continue to be ineffec-
tive at identifying words in reparanda. To bet-
ter understand these problems and identify areas
256
Label % of words Precision Recall F-score
Fillers 5.6% 64% 59% 61%
Edit (reparandum) 7.8% 85% 68% 75%
Table 1: Disfluency detection performance on the SSR test subcorpus using JC04 system.
Label % of edits Recall
Rough copy (RC) edits 58.8% 84.8%
Non-copy (NC) edits 41.2% 43.2%
Total edits 100.0% 67.6%
Table 2: Deeper analysis of edit detection performance on the SSR test subcorpus using JC04 system.
1 he that ?s uh that ?s a relief
2 E E E FL - - - -
3 NC RC RC FL - - - -
Figure 2: Example of word class and refined word
class labels, where - denotes a non-error, FL de-
notes a filler, E generally denotes reparanda, and
RC and NC indicate rough copy and non-copy
speaker errors, respectively.
for improvement, we used the top-performing1
JC04 noisy channel TAG edit detector to produce
edit detection analyses on the test segment of the
Spontaneous Speech Reconstruction (SSR) corpus
(Fitzgerald and Jelinek, 2008). Table 1 demon-
strates the performance of this system for detect-
ing filled pause fillers, discourse marker fillers,
and edit words. The results of a more granular
analysis compared to a hand-refined reference (as
shown in line 3 of Figure 2) are shown in Table 2.
The reader will recall that precision P is defined
as P = |correct||correct|+|false| and recall R =
|correct|
|correct|+|miss| .
We denote the harmonic mean of P and R as F-
score F and calculate it F = 21/P+1/R .
As expected given the assumptions of the TAG
approach, JC04 identifies repetitions and most
revisions in the SSR data, but less success-
fully labels false starts and other speaker self-
interruptions which do not have a cross-serial cor-
relations. These non-copy errors (with a recall of
only 43.2%), are hurting the overall edit detection
recall score. Precision (and thus F-score) cannot
be calculated for the experiment in Table 2; since
the JC04 does not explicitly label edits as rough
copies or non-copies, we have no way of knowing
whether words falsely labeled as edits would have
1As determined in the RT04 EARS Metadata Extraction
Task
been considered as false RCs or false NCs. This
will unfortunately hinder us from using JC04 as a
direct baseline comparison in our work targeting
false starts; however, we consider these results to
be further motivation for the work.
Surveying these results, we conclude that there
is still much room for improvement in the
field of simple disfluency identification, espe-
cially the cases of detecting non-copy reparandum
and learning how and where to implement non-
deletion reconstruction changes.
2 Approach
2.1 Data
We conducted our experiments on the recently re-
leased Spontaneous Speech Reconstruction (SSR)
corpus (Fitzgerald and Jelinek, 2008), a medium-
sized set of disfluency annotations atop Fisher
conversational telephone speech (CTS) data (Cieri
et al, 2004). Advantages of the SSR data include
? aligned parallel original and cleaned sen-
tences
? several levels of error annotations, allowing
for a coarse-to-fine reconstruction approach
? multiple annotations per sentence reflecting
the occasional ambiguity of corrections
As reconstructions are sometimes non-
deterministic (illustrated in EX6 in Section
1.1), the SSR provides two manual reconstruc-
tions for each utterance in the data. We use
these dual annotations to learn complementary
approaches in training and to allow for more
accurate evaluation.
The SSR corpus does not explicitly label all
reparandum-like regions, as defined in Section 1.1,
but only those which annotators selected to delete.
257
Thus, for these experiments we must implicitly
attempt to replicate annotator decisions regarding
whether or not to delete reparandum regions when
labeling them as such. Fortunately, we expect this
to have a negligible effect here as we will empha-
size utterances which do not require more complex
reconstructions in this work.
The Spontaneous Speech Reconstruction cor-
pus is partitioned into three subcorpora: 17,162
training sentences (119,693 words), 2,191 sen-
tences (14,861 words) in the development set, and
2,288 sentences (15,382 words) in the test set. Ap-
proximately 17% of the total utterances contain a
reparandum-type error.
The output of the JC04 model ((Johnson and
Charniak, 2004) is included as a feature and used
as an approximate baseline in the following exper-
iments. The training of the TAG model within this
system requires a very specific data format, so this
system is trained not with SSR but with Switch-
board (SWBD) (Godfrey et al, 1992) data as de-
scribed in (Johnson and Charniak, 2004). Key dif-
ferences in these corpora, besides the form of their
annotations, include:
? SSR aims to correct speech output, while
SWBD edit annotation aims to identify
reparandum structures specifically. Thus, as
mentioned, SSR only marks those reparanda
which annotators believe must be deleted
to generate a grammatical and content-
preserving reconstruction.
? SSR considers some phenomena such as
leading conjunctions (?and i did? ? ?i did?) to
be fillers, while SWBD does not.
? SSR includes more complex error identifi-
cation and correction, though these effects
should be negligible in the experimental
setup presented herein.
While we hope to adapt the trained JC04 model
to SSR data in the future, for now these difference
in task, evaluation, and training data will prevent
direct comparison between JC04 and our results.
2.2 Conditional random fields
Conditional random fields (Lafferty et al, 2001),
or CRFs, are undirected graphical models whose
prediction of a hidden variable sequence Y is
globally conditioned on a given observation se-
quence X , as shown in Figure 3. Each observed
Figure 3: Illustration of a conditional random
field. For this work, x represents observable in-
puts for each word as described in Section 3.1 and
y represents the error class of each word (Section
3.2).
state xi ? X is composed of the corresponding
word wi and a set of additional features Fi, de-
tailed in Section 3.1.
The conditional probability of this model can be
represented as
p?(Y |X) =
1
Z?(X)
exp(
?
k
?kFk(X,Y )) (1)
where Z?(X) is a global normalization factor and
? = (?1 . . . ?K) are model parameters related to
each feature function Fk(X,Y ).
CRFs have been widely applied to tasks in
natural language processing, especially those in-
volving tagging words with labels such as part-
of-speech tagging and shallow parsing (Sha and
Pereira, 2003), as well as sentence boundary
detection (Liu et al, 2005; Liu et al, 2004).
These models have the advantage that they model
sequential context (like hidden Markov models
(HMMs)) but are discriminative rather than gen-
erative and have a less restricted feature set. Ad-
ditionally, as compared to HMMs, CRFs offer
conditional (versus joint) likelihood, and directly
maximizes posterior label probabilities P (E|O).
We used the GRMM package (Sutton, 2006) to
implement our CRF models, each using a zero-
mean Gaussian prior to reduce over-fitting our
model. No feature reduction is employed, except
where indicated.
3 Word-Level ID Experiments
3.1 Feature functions
We aim to train our CRF model with sets of
features with orthogonal analyses of the errorful
text, integrating knowledge from multiple sources.
While we anticipate that repetitions and other
rough copies will be identified primarily by lexical
258
and local context features, this will not necessarily
help for false starts with little or no lexical overlap
between reparandum and repair. To catch these er-
rors, we add both language model features (trained
with the SRILM toolkit (Stolcke, 2002) on SWBD
data with EDITED reparandum nodes removed),
and syntactic features to our model. We also in-
cluded the output of the JC04 system ? which had
generally high precision on the SSR data ? in the
hopes of building on these results.
Altogether, the following features F were ex-
tracted for each observation xi.
? Lexical features, including
? the lexical item and part-of-speech
(POS) for tokens ti and ti+1,
? distance from previous token to the next
matching word/POS,
? whether previous token is partial word
and the distance to the next word with
same start, and
? the token?s (normalized) position within
the sentence.
? JC04-edit: whether previous, next, or cur-
rent word is identified by the JC04 system as
an edit and/or a filler (fillers are classified as
described in (Johnson et al, 2004)).
? Language model features: the unigram log
probability of the next word (or POS) token
p(t), the token log probability conditioned on
its multi-token history h (p(t|h))2, and the
log ratio of the two (log p(t|h)p(t) ) to serve as
an approximation for mutual information be-
tween the token and its history, as defined be-
low.
I(t;h) =
?
h,t
p(h, t) log
p(h, t)
p(h)p(t)
=
?
h,t
p(h, t)
[
log
p(t|h)
p(t)
]
This aims to capture unexpected n-grams
produced by the juxtaposition of the reparan-
dum and the repair. The mutual information
feature aims to identify when common words
are seen in uncommon context (or, alterna-
tively, penalize rare n-grams normalized for
rare words).
2In our model, word historys h encompassed the previous
two words (a 3-gram model) and POS history encompassed
the previous four POS labels (a 5-gram model)
? Non-terminal (NT) ancestors: Given an au-
tomatically produced parse of the utterance
(using the Charniak (1999) parser trained on
Switchboard (SWBD) (Godfrey et al, 1992)
CTS data), we determined for each word all
NT phrases just completed (if any), all NT
phrases about to start to its right (if any), and
all NT constituents for which the word is in-
cluded.
(Ferreira and Bailey, 2004) and others have
found that false starts and repeats tend to end
at certain points of phrases, which we also
found to be generally true for the annotated
data.
Note that the syntactic and POS features we
used are extracted from the output of an automatic
parser. While we do not expect the parser to al-
ways be accurate, especially when parsing errorful
text, we hope that the parser will at least be con-
sistent in the types of structures it assigns to par-
ticular error phenomena. We use these features in
the hope of taking advantage of that consistency.
3.2 Experimental setup
In these experiments, we attempt to label the
following word-boundary classes as annotated in
SSR corpus:
? fillers (FL), including filled pauses and dis-
course markers (?5.6% of words)
? rough copy (RC) edit (reparandum incor-
porates the same or very similar words in
roughly the same word order, including repe-
titions and some revisions) (?4.6% of words)
? non-copy (NC) edit (a speaker error where the
reparandum has no lexical or structural re-
lationship to the repair region following, as
seen in restart fragments and some revisions)
(?3.2% of words)
Other labels annotated in the SSR corpus (such
as insertions and word reorderings), have been ig-
nored for these error tagging experiments.
We approach our training of CRFs in several
ways, detailed in Table 3. In half of our exper-
iments (#1, 3, and 4), we trained a single model
to predict all three annotated classes (as defined
at the beginning of Section 3.3), and in the other
half (#2, 5, and 6), we trained the model to predict
NCs only, NCs and FLs, RCs only, or RCs and FLs
(as FLs often serve as interregnum, we predict that
these will be a valuable cue for other edits).
259
Setup Train data Test data Classes trained per model
#1 Full train Full test FL + RC + NC
#2 Full train Full test {RC,NC}, FL+{RC,NC}
#3 Errorful SUs Errorful SUs FL + RC + NC
#4 Errorful SUs Full test FL + RC + NC
#5 Errorful SUs Errorful SUs {RC,NC}, FL+{RC,NC}
#6 Errorful SUs Full test {RC,NC}, FL+{RC,NC}
Table 3: Overview of experimental setups for word-level error predictions.
We varied the subcorpus utterances used in
training. In some experiments (#1 and 2) we
trained with the entire training set3, including sen-
tences without speaker errors, and in others (#3-6)
we trained only on those sentences containing the
relevant deletion errors (and no additionally com-
plex errors) to produce a densely errorful train-
ing set. Likewise, in some experiments we pro-
duced output only for those test sentences which
we knew to contain simple errors (#3 and 5). This
was meant to emulate the ideal condition where
we could perfectly predict which sentences con-
tain errors before identifying where exactly those
errors occurred.
The JC04-edit feature was included to help us
build on previous efforts for error classification.
To confirm that the model is not simply replicating
these results and is indeed learning on its own with
the other features detailed, we also trained models
without this JC04-edit feature.
3.3 Evaluation of word-level experiments
3.3.1 Word class evaluation
We first evaluate edit detection accuracy on a per-
word basis. To evaluate our progress identify-
ing word-level error classes, we calculate preci-
sion, recall and F-scores for each labeled class c in
each experimental scenario. As usual, these met-
rics are calculated as ratios of correct, false, and
missed predictions. However, to take advantage of
the double reconstruction annotations provided in
SSR (and more importantly, in recognition of the
occasional ambiguities of reconstruction) wemod-
3Using both annotated SSR reference reconstructions for
each utterance
ified these calculations slightly as shown below.
corr(c) =
?
i:cwi=c
?(cwi = cg1,i or cwi = cg2,i)
false(c) =
?
i:cwi=c
?(cwi 6= cg1,i and cwi 6= cg2,i)
miss(c) =
?
i:cg1,i=c
?(cwi 6= cg1,i)
where cwi is the hypothesized class forwi and cg1,i
and cg2,i are the two reference classes.
Setup Class labeled FL RC NC
Train and test on all SUs in the subcorpus
#1 FL+RC+NC 71.0 80.3 47.4
#2 NC - - 42.5
#2 NC+FL 70.8 - 47.5
#2 RC - 84.2 -
#2 RC+FL 67.8 84.7 -
Train and test on errorful SUs
#3 FL+RC+NC 91.6 84.1 52.2
#4 FL+RC+NC 44.1 69.3 31.6
#5 NC - - 73.8
#6 w/ full test - - 39.2
#5 NC+FL 90.7 - 69.8
#6 w/ full test 50.1 - 38.5
#5 RC - 88.7 -
#6 w/ full test - 75.0 -
#5 RC+FL 92.3 87.4 -
#6 w/ full test 62.3 73.9 -
Table 4: Word-level error prediction F1-score re-
sults: Data variation. The first column identifies
which data setup was used for each experiment
(Table 3). The highest performing result for each
class in the first set of experiments has been high-
lighted.
Analysis: Experimental results can be seen in
Tables 4 and 5. Table 4 shows the impact of
260
Features FL RC NC
JC04 only 56.6 69.9-81.9 1.6-21.0
lexical only 56.5 72.7 33.4
LM only 0.0 15.0 0.0
NT bounds only 44.1 35.9 11.5
All but JC04 58.5 79.3 33.1
All but lexical 66.9 76.0 19.6
All but LM 67.9 83.1 41.0
All but NT bounds 61.8 79.4 33.6
All 71.0 80.3 47.4
Table 5: Word-level error prediction F-score re-
sults: Feature variation. All models were trained
with experimental setup #1 and with the set of fea-
tures identified.
training models for individual features and of con-
straining training data to contain only those ut-
terances known to contain errors. It also demon-
strates the potential impact on error classification
after prefiltering test data to those SUs with er-
rors. Table 5 demonstrates the contribution of each
group of features to our CRF models.
Our results demonstrate the impact of varying
our training data and the number of label classes
trained for. We see in Table 4 from setup #5 exper-
iments that training and testing on error-containing
utterances led to a dramatic improvement in F1-
score. On the other hand, our results for experi-
ments using setup #6 (where training data was fil-
tered to contain errorful data but test data was fully
preserved) are consistently worse than those of ei-
ther setup #2 (where both train and test data was
untouched) or setup #5 (where both train and test
data were prefiltered). The output appears to suf-
fer from sample bias, as the prior of an error oc-
curring in training is much higher than in testing.
This demonstrates that a densely errorful training
set alne cannot improve our results when testing
data conditions do not match training data condi-
tions. However, efforts to identify errorful sen-
tences before determining where errors occur in
those sentences may be worthwhile in preventing
false positives in error-less utterances.
We next consider the impact of the four feature
groups on our prediction results. The CRF model
appears competitive even without the advantage
of building on JC04 results, as seen in Table 54.
4JC04 results are shown as a range for the reasons given in
Section 1.2: since JC04 does not on its own predict whether
an ?edit? is a rough copy or non-copy, it is impossible to cal-
Interestingly and encouragingly, the NT bounds
features which indicate the linguistic phrase struc-
tures beginning and ending at each word accord-
ing to an automatic parse were also found to be
highly contribututive for both fillers and non-copy
identification. We believe that further pursuit of
syntactic features, especially those which can take
advantage of the context-free weakness of statisti-
cal parsers like (Charniak, 1999) will be promising
in future research.
It was unexpected that NC classification would
be so sensitive to the loss of lexical features while
RC labeling was generally resilient to the drop-
ping of any feature group. We hypothesize that
for rough copies, the information lost from the re-
moval of the lexical items might have been com-
pensated for by the JC04 features as JC04 per-
formed most strongly on this error type. This
should be further investigated in the future.
3.3.2 Strict evaluation: SU matching
Depending on the downstream task of speech re-
construction, it could be imperative not only to
identify many of the errors in a given spoken ut-
terance, but indeed to identify all errors (and only
those errors), yielding the precise cleaned sentence
that a human annotator might provide.
In these experiments we apply simple cleanup
(as described in Section 1.1) to both JC04 out-
put and the predicted output for each experimental
setup in Table 3, deleting words when their right
boundary class is a filled pause, rough copy or
non-copy.
Taking advantage of the dual annotations for
each sentence in the SSR corpus, we can report
both single-reference and double-reference eval-
uation. Thus, we judge that if a hypothesized
cleaned sentence exactly matches either reference
sentence cleaned in the same manner, we count the
cleaned utterance as correct and otherwise assign
no credit.
Analysis: We see the outcome of this set of ex-
periments in Table 6. While the unfiltered test sets
of JC04-1, setup #1 and setup #2 appear to have
much higher sentence-level cleanup accuracy than
the other experiments, we recall that this is natu-
ral also due to the fact that the majority of these
sentences should not be cleaned at all, besides
culate precision and thus F1 score precisely. Instead, here we
show the resultant F1 for the best case and worst case preci-
sion range.
261
Setup Classes deleted # SUs # SUs which match gold % accuracy
Baseline only filled pauses 2288 1800 78.7%
JC04-1 E+FL 2288 1858 81.2%
CRF-#1 RC, NC, and FL 2288 1922 84.0%
CRF-#2
?
{RC,NC} 2288 1901 83.1%
Baseline only filled pauses 281 5 1.8%
JC04-2 E+FL 281 126 44.8%
CRF-#3 RC, NC, and FL 281 156 55.5%
CRF-#5
?
{RC,NC} 281 132 47.0%
Table 6: Word-level error predictions: exact SU match results. JC04-2 was run only on test sentences
known to contain some error to match the conditions of Setup #3 and #5 (from Table 3). For the baselines,
we delete only filled pause filler words like ?eh? and ?um?.
occasional minor filled pause deletions. Look-
ing specifically on cleanup results for sentences
known to contain at least one error, we see, once
again, that our system outperforms our baseline
JC04 system at this task.
4 Discussion
Our first goal in this work was to focus on an area
of disfluency detection currently weak in other
state-of-the-art speaker error detection systems ?
false starts ? while producing comparable classi-
fication on repetition and revision speaker errors.
Secondly, we attempted to quantify how far delet-
ing identified edits (both RC and NC) and filled
pauses could bring us to full reconstruction of
these sentences.
We?ve shown in Section 3 that by training and
testing on data prefiltered to include only utter-
ances with errors, we can dramatically improve
our results, not only by improving identification
of errors but presumably by reducing the risk of
falsely predicting errors. We would like to further
investigate to understand how well we can auto-
matically identify errorful spoken utterances in a
corpus.
5 Future Work
This work has shown both achievable and demon-
strably feasible improvements in the area of iden-
tifying and cleaning simple speaker errors. We be-
lieve that improved sentence-level identification of
errorful utterances will help to improve our word-
level error identification and overall reconstruction
accuracy; we will continue to research these areas
in the future. We intend to build on these efforts,
adding prosodic and other features to our CRF and
maximum entropy models,
In addition, as we improve the word-level clas-
sification of rough copies and non-copies, we will
begin to move forward to better identify more
complex speaker errors such as missing argu-
ments, misordered or redundant phrases. We will
also work to apply these results directly to the out-
put of a speech recognition system instead of to
transcripts alone.
Acknowledgments
The authors thank our anonymous reviewers for
their valuable comments. Support for this work
was provided by NSF PIRE Grant No. OISE-
0530118. Any opinions, findings, conclusions,
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the supporting agency.
References
J. Kathryn Bock. 1982. Toward a cognitive psy-
chology of syntax: Information processing contri-
butions to sentence formulation. Psychological Re-
view, 89(1):1?47, January.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. In Meeting of the North American
Association for Computational Linguistics.
Christopher Cieri, Stephanie Strassel, Mohamed
Maamouri, Shudong Huang, James Fiumara, David
Graff, Kevin Walker, and Mark Liberman. 2004.
Linguistic resource creation and distribution for
EARS. In Rich Transcription Fall Workshop.
Fernanda Ferreira and Karl G. D. Bailey. 2004. Disflu-
encies and human language comprehension. Trends
in Cognitive Science, 8(5):231?237, May.
262
Erin Fitzgerald and Frederick Jelinek. 2008. Linguis-
tic resources for reconstructing spontaneous speech
text. In Proceedings of the Language Resources and
Evaluation Conference, May.
Erin Fitzgerald. 2009. Reconstructing Spontaneous
Speech. Ph.D. thesis, The Johns Hopkins University.
John J. Godfrey, Edward C. Holliman, and Jane Mc-
Daniel. 1992. SWITCHBOARD: Telephone speech
corpus for research and development. In Proceed-
ings of the IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 517?520,
San Francisco.
Matthias Honal and Tanja Schultz. 2005. Au-
tomatic disfluency removal on recognized spon-
taneous speech ? rapid adaptation to speaker-
dependent disfluenices. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing.
Mark Johnson and Eugene Charniak. 2004. A TAG-
based noisy channel model of speech repairs. In
Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Rich Transcription
Fall Workshop.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. 18th International Conf. on
Machine Learning, pages 282?289. Morgan Kauf-
mann, San Francisco, CA.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Proceedings of the International Conference on
Spoken Language Processing.
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, and Mary Harper. 2004. The ICSI/UW
RT04 structural metadata extraction system. In Rich
Transcription Fall Workshop.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech. In
Proceedings of the Annual Meeting of the Associa-
tion for Computational Linguistics, pages 451?458,
Ann Arbor, MI.
Sharath Rao, Ian Lane, and Tanja Schultz. 2007. Im-
proving spoken language translation by automatic
disfluency removal: Evidence from conversational
speech transcripts. In Machine Translation Summit
XI, Copenhagen, Denmark, October.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In HLT-NAACL.
Elizabeth Shriberg. 1994. Preliminaries to a Theory
of Speech Disfluencies. Ph.D. thesis, University of
California, Berkeley.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and
Signal Processing, Denver, CO, September.
Charles Sutton. 2006. GRMM: A graphical models
toolkit. http://mallet.cs.umass.edu.
Qi Zhang and Fuliang Weng. 2005. Exploring fea-
tures for identifying edited regions in disfluent sen-
tences. In Proceedings of the International Work-
shop on Parsing Techniques, pages 179?185.
263
Proceedings of NAACL HLT 2009: Short Papers, pages 29?32,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Computation of Distributional Similarities for Queries
Enrique Alfonseca
Google Research
Zurich, Switzerland
ealfonseca@google.com
Keith Hall
Google Research
Zurich, Switzerland
kbhall@google.com
Silvana Hartmann
University of Stuttgart
Stuttgart, Germany
silvana.hartmann@ims.uni-stuttgart.de
Abstract
We present a large-scale, data-driven approach
to computing distributional similarity scores
for queries. We contrast this to recent web-
based techniques which either require the off-
line computation of complete phrase vectors,
or an expensive on-line interaction with a
search engine interface. Independent of the
computational advantages of our approach, we
show empirically that our technique is more
effective at ranking query alternatives that the
computationally more expensive technique of
using the results from a web search engine.
1 Introduction
Measuring the semantic similarity between queries
or, more generally, between pairs of very short texts,
is increasingly receiving attention due to its many
applications. An accurate metric of query simi-
larities is useful for query expansion, to improve
recall in Information Retrieval systems; for query
suggestion, to propose to the user related queries
that might help reach the desired information more
quickly; and for sponsored search, where advertisers
bid for keywords that may be different but semanti-
cally equivalent to user queries.
In this paper, we study the problem of measuring
similarity between queries using corpus-based unsu-
pervised methods. Given a query q, we would like
to rank all other queries according to their similarity
to q. The proposed approach compares favorably to
a state-of-the-art unsupervised system.
2 Related work
Distributional similarity methods model the similar-
ity or relatedness of words using a metric defined
over the set of contexts in which the words appear
(Firth, 1957). One of the most common representa-
tions for contexts is the vector space model (Salton
et al, 1975). This is the basic idea of approaches
such as (Grefenstette, 1992; Bordag, 2008; Lin,
1998; Riloff and Shepherd, 1997), with some varia-
tions; e.g., whether syntactic information is used ex-
plicitly, or which weight function is applied. Most of
the existing work has focused on similarity between
single words or syntactically-correct multiword ex-
pressions. In this work, we adapt these techniques
to calculate similarity metrics between pairs of com-
plete queries, which may or may not be syntactically
correct.
Other approaches for query similarity use sta-
tistical translation models (Riezler et al, 2008),
analysing search engine logs (Jones et al, 2006),
looking for different anchor texts pointing to the
same pages (Kraft and Zien, 2004), or replacing
query words with other words that have the high-
est pointwise mutual information (Terra and Clarke,
2004).
Sahami and Helman (Sahami and Heilman, 2006)
define a web kernel function for semantic similarity
based on the snippets of the search results returned
by the queries. The algorithm used is the following:
(a) Issue a query x to a search engine and collect
the set of n snippets returned by the search engine;
(b) Compute the tf?idf vector vi for each document
snippet di; (c) Truncate each vector to include its m
29
highest weighted terms; (d) Construct the centroid
of the L2-normalized vectors vi; (e) Calculate the
similarity of two queries as the dot product of their
L2-normalized vectors, i.e. as the cosine of both
vectors.
This work was followed up by Yih and Meek (Yih
and Meek, 2007), who combine the web kernel with
other simple metrics of similarity between word vec-
tors (Dice Coefficient, Jaccard Coefficient, Overlap,
Cosine, KL Divergence) in a machine learning sys-
tem to provide a ranking of similar queries.
3 Proposed method
Using a search engine to collect snippets (Sahami
and Heilman, 2006; Yih and Meek, 2007; Yih and
Meek, 2008) takes advantage of all the optimizations
performed by the retrieval engine (spelling correc-
tion, relevance scores, etc.), but it has several disad-
vantages: first, it is not repeatable, as the code un-
derlying search engines is in a constant state of flux;
secondly, it is usually very expensive to issue a large
number of search requests; sometimes the APIs pro-
vided limit the number of requests. In this section,
we describe a method which overcomes these draw-
backs. The distributional methods we propose for
calculating similarities between words and multi-
word expressions profit from the use of a large Web-
based corpus.
The contextual vectors for a query can be col-
lected by identifying the contexts in which the query
appears. Queries such as [buy a book] and [buy
some books] are supposed to appear close to simi-
lar context words in a bag-of-words model, and they
should have a high similarity. However, there are
two reasons why this would yield poor results:
First, as the length of the queries grows, the prob-
ability of finding exact queries in the corpus shrinks
quickly. As an example, when issuing the queries
[Lindsay Lohan pets] and [Britney Spears pets] to
Google enclosed in double quotes, we obtain only
6 and 760 results, respectively. These are too few
occurrences in order to collect meaningful statistics
about the contexts of the queries.
Secondly, many user queries are simply a concate-
nation of keywords with weak or no underlying syn-
tax. Therefore, even if they are popular queries, they
may not appear as such in well-formed text found
in web documents. For example, queries like [hol-
lywood dvd cheap], enclosed in double quotes, re-
trieve less than 10 results. Longer queries, such as
[hotel cheap new york fares], are still meaningful,
but do not appear frequently in web documents.
In order to use of distributional similarities in the
query setting, we propose the following method.
Given a query of interest p = [w1, w2, ..., wn]:
1. For each word wi collect all words that appear
close to wi in the web corpus (i.e., a bag-fo-
words models). Empirically we have chosen
all the words whose distance to wi is less or
equal to 3. This gives us a vector of context
words and frequencies for each of the words in
the query, ~vi = (fi1, fi2, ..., fi|V |), where |V | is
the size of the corpus vocabulary.
2. Represent the query p with a vector of words,
and the weight associated to each word is the
geometric mean of the frequencies for the word
in the original vectors:
~qv =
0
B
@
0
@
|n|Y
i=1
fi1
1
A
1
n
,
0
@
|n|Y
i=1
fi2
1
A
1
n
, ...,
0
@
|n|Y
i=1
fi|V |
1
A
1
n
1
C
A
3. Apply the ?2 test as a weighting function test to
measure whether the query and the contextual
feature are conditionally independent.
4. Given two queries, use the cosine between their
vectors to calculate their similarity.
The motivations for this approach are: the geo-
metric mean is a way to approximate a boolean AND
operation between the vectors, while at the same
time keeping track of the magnitude of the frequen-
cies. Therefore, if two queries only differ on a very
general word, e.g. [books] and either [buy books]
or [some books], the vector associated to the general
words (buy or some in the example) will have non-
zero values for most of the contextual features, be-
cause they are not topically constrained; and the vec-
tors for the queries will have similar sets of features
with non-zero values. Equally relevant, terms that
are closely related will appear in the proximity of a
similar set of words and will have similar vectors.
For example, if the two queries are Sir Arthur Co-
nan Doyle books and Sir Arthur Conan Doyle nov-
els, given that the vectors for books and novels are
expected to have similar features, these two queries
30
Contextual word acid fast bacteria Query
acidogenicity 11 6 4 6.41506
auramin 2 5 2 2.71441
bacillae 3 10 4 4.93242
carbolfuchsin 1 28 2 8.24257
dehydrogena 5 3 3 3.55689
diphtheroid 5 9 92 16.05709
fuchsine 42 3 4 7.95811
glycosilation 3 2 3 2.62074
Table 1: Example of context words for the query [acid fast bacteria].
will receive a high similarity score.
On the other hand, this combination also helps in
reducing word ambiguity. Consider the query bank
account; the bag-of-words vector for bank will con-
tain words related to the various senses of the word,
but when combining it to account only the terms that
belong to the financial domain and are shared be-
tween the two vectors will be included in the final
query vector.
Finally, we note that the geometric mean provides
a clean way to encode the pair-wise similarities of
the individual words of the phrase. One can inter-
pret the cosine similarity metric as the magnitude of
the vector constructed by the scalar product of the
individual vectors. Our approach scales this up by
taking the scalar product of the vectors for all words
in the phrase and then scaling them by the number of
words (i.e., the geometric mean). Instead of comput-
ing the magnitude of this vector, we use it to com-
pute similarities for the entire phrase.
As an example of the proposed procedure, Table 1
shows a random sample of the contextual features
collected for the words in the query [acid fast bac-
teria], and how the query?s vector is generated by
using the geometric mean of the frequencies of the
features in the vectors for the query words.
4 Experiments and results
4.1 Experimental settings
To collect the contextual features for words and
phrases, we have used a corpus of hundreds of mil-
lions of documents crawled from the Web in August
2008. An HTML parser is used to extract text and
non-English documents are discarded. After pro-
cess, the remaining corpus contains hundreds of bil-
lions of words.
As a source of keywords, we have used the top
0 1 2 3 4
0 280 95 14 1 0
1 108 86 65 4 0
2 11 47 83 16 0
3 1 2 17 45 2
4 0 0 1 1 2
Table 2: Confusion matrix for the pairs in the goldstandard. Rows
represent first rater scores, and columns second rater scores.
one and a half million English queries sent to the
Google search engine after being fully anonymized.
We have calculated the pairwise similarity between
all queries, which would potentially return 2.25 tril-
lion similarity scores, but in practice returns a much
smaller number as many pairs have non-overlapping
contexts.
As a baseline, we have used a new implementa-
tion of the Web Kernel similarity (Sahami and Heil-
man, 2006). The parameters are set the same as re-
ported in the paper with the exception of the snip-
pet size; in their study, the size was limited to 1,000
characters and in our system, the normal snippet re-
turned by Google is used (around 160 characters).
In order to evaluate our system, we prepared a
goldstandard set of query similarities. We have ran-
domly sampled 65 queries from our full dataset, and
obtained the top 20 suggestions from both the Sa-
hami system and the distributional similarities sys-
tem. Two human raters have rated the original query
and the union of the sets of suggestions, using the
same 5-point Likert scale that Sahami used. Table 2
shows the confusion matrix of scores between the
two raters. Most of the disagreements are between
the scores 0 and 1, which means that probably it was
not clear enough whether the queries were unrelated
or only slightly related. It is also noteworthy that
in this case, very few rewritten queries were clas-
sified as being better than the original, which also
suggests to us that probably we could remove the
topmost score from the classifications scale.
We have evaluated inter-judge agreement in the
following two ways: first, using the weighted Kappa
score, which has a value of 0.7111. Second, by
grouping the pairs judged as irrelevant or slightly
relevant (scores 0 and 1) as a class containing nega-
tive examples, and the pairs judged as very relevant,
equal or better (scores 2 through 4) as a class con-
taining positive examples. Using this two-class clas-
31
Method Prec@1 Prec@3 Prec@5 mAP AUC
Web Kernel 0.39 0.35 0.32 0.49 0.22
Unigrams 0.47 0.53 0.47 0.57 0.26
N-grams 0.70 0.57 0.52 0.71 0.54
Table 3: Results. mAP is mean average precision, and AUC is the
area under the precision/recall curve.
sification, Cohen?s Kappa score becomes 0.6171.
Both scores indicates substantial agreement amongst
the raters.
The data set thus collected is a ranked list of sug-
gestions for each query1, and can be used to evaluate
any other suggestion-ranking system.
4.2 Experiments and results
As an evolution of the distributional similarities
approach, we also implemented a second version
where the queries are chunked into phrases. The
motivation for the second version is that, in some
queries, like [new york cheap hotel], it makes sense
to handle new york as a single phrase with a sin-
gle associated context vector collected from the web
corpus. The list of valid n-grams is collected by
combining several metrics, e.g. whether Wikipedia
contains an entry with that name, or whether they
appear quoted in query logs. The queries are then
chunked greedily always preferring the longer n-
gram from our list.
Table 3 shows the results of trying both systems
on the same set of queries. The original system is
the one called Unigrams, and the one that chunks
the queries is the one called N-grams. The distri-
butional similarity approaches outperform the web-
based kernel on all the metrics, and chunking queries
shows a good improvement over using unigrams.
5 Conclusions
This paper extends the vector-space model of dis-
tributional similarities to query-to-query similarities
by combining different vectors using the geometric
mean. We show that using n-grams to chunk the
queries improves the results significantly. This out-
performs the web-based kernel method, a state-of-
the-art unsupervised query-to-query similarity tech-
nique, which is particularly relevant as the corpus-
based method does not benefit automatically from
1We plan to make it available to the research community.
search engine features.
References
S. Bordag. 2008. A Comparison of Co-occurrence and
Similarity Measures as Simulations of Context. Lec-
ture Notes in Computer Science, 4919:52.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, pages 1?32.
G. Grefenstette. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the 15th annual international ACM SI-
GIR conference on Research and development in infor-
mation retrieval, pages 89?97. ACM New York, NY,
USA.
R. Jones, B. Rey, O. Madani, andW. Greiner. 2006. Gen-
erating query substitutions. In Proceedings of the 15th
international conference on World Wide Web, pages
387?396. ACM New York, NY, USA.
Reiner Kraft and Jason Zien. 2004. Mining anchor text
for query refinement. In WWW ?04: Proceedings of
the 13th international conference on World Wide Web,
pages 666?674, New York, NY, USA. ACM.
D. Lin. 1998. Extracting Collocations from Text Cor-
pora. In First Workshop on Computational Terminol-
ogy, pages 57?63.
Stefan Riezler, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Improved
Query Expansion. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING?08).
E. Riloff and J. Shepherd. 1997. A corpus-based ap-
proach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124. As-
sociation for Computational Linguistics.
M. Sahami and T.D. Heilman. 2006. A web-based ker-
nel function for measuring the similarity of short text
snippets. In Proceedings of the 15th international con-
ference on World Wide Web, pages 377?386.
G. Salton, A. Wong, and CS Yang. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11):613?620.
Egidio Terra and Charles L.A. Clarke. 2004. Scoring
missing terms in information retrieval tasks. In CIKM
?04: Proceedings of the thirteenth ACM international
conference on Information and knowledge manage-
ment, pages 50?58, New York, NY, USA. ACM.
W. Yih and C. Meek. 2007. Improving Similarity Mea-
sures for Short Segments of Text. In Proceedings of
the Natural Conference on Artificial Intelligence, vol-
ume 2, page 1489. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999.
W. Yih and C. Meek. 2008. Consistent Phrase Relevance
Measures. Data Mining and Audience Intelligence for
Advertising (ADKDD 2008), page 37.
32
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Semantic Networks: Annotation and Evaluation
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University in Prague, Czech Republic
novak@ufal.mff.cuni.cz
Sven Hartrumpf
Computer Science Department
University of Hagen, Germany
Sven.Hartrumpf@FernUni-Hagen.de
Keith Hall?
Google Research
Zu?rich, Switzerland
kbhall@google.com
Abstract
We introduce a large-scale semantic-network
annotation effort based on the MutliNet for-
malism. Annotation is achieved via a pro-
cess which incorporates several independent
tools including a MultiNet graph editing tool,
a semantic concept lexicon, a user-editable
knowledge-base for semantic concepts, and a
MultiNet parser. We present an evaluation
metric for these semantic networks, allowing
us to determine the quality of annotations in
terms of inter-annotator agreement. We use
this metric to report the agreement rates for a
pilot annotation effort involving three annota-
tors.
1 Introduction
In this paper we propose an annotation frame-
work which integrates the MultiNet semantic net-
work formalism (Helbig, 2006) and the syntactico-
semantic formalism of the Prague Dependency Tree-
bank (Hajic? et al, 2006) (PDT). The primary goal of
this task is to increase the interoperability of these
two frameworks in order to facilitate efforts to an-
notate at the semantic level while preserving intra-
sentential semantic and syntactic annotations as are
found in the PDT.
The task of annotating text with global semantic
interactions (e.g., semantic interactions within some
discourse) presents a cognitively demanding prob-
lem. As with many other annotation formalisms,
?Part of this work was completed while at the Johns Hop-
kins University Center for Language and Speech Processing in
Baltimore, MD USA.
we propose a technique that builds from cognitively
simpler tasks such as syntactic and semantic anno-
tations at the sentence level including rich morpho-
logical analysis. Rather than constraining the se-
mantic representations to those compatible with the
sentential annotations, our procedure provides the
syntacitco-semantic tree as a reference; the annota-
tors are free to select nodes from this tree to create
nodes in the network. We do not attempt to measure
the influence this procedure has on the types of se-
mantic networks generated. We believe that using a
soft-constraint such as the syntactico-semantic tree,
allows us to better generate human labeled seman-
tic networks with links to the interpretations of the
individual sentence analyses.
In this paper, we present a procedure for com-
puting the annotator agreement rate for MultiNet
graphs. Note that a MultiNet graph does not rep-
resent the same semantics as a syntactico-semantic
dependency tree. The nodes of the MultiNet graph
are connected based on a corpus-wide interpretation
of the entities referred to in the corpus. These global
connections are determined by the intra-sentential
interpretation but are not restricted to that inter-
pretation. Therefore, the procedure for computing
annotator agreement differs from the standard ap-
proaches to evaluating syntactic and semantic de-
pendency treebanks (e.g., dependency link agree-
ment, label agreement, predicate-argument structure
agreement).
As noted in (Bos, 2008), ?Even though the de-
sign of annotation schemes has been initiated for
single semantic phenomena, there exists no anno-
tation scheme (as far as I know) that aims to inte-
37
grate a wide range of semantic phenomena all at
once. It would be welcome to have such a resource
at ones disposal, and ideally a semantic annotation
scheme should be multi-layered, where certain se-
mantic phenomena can be properly analysed or left
simply unanalysed.?
In Section 1 we introduce the theoretical back-
ground of the frameworks on which our annotation
tool is based: MultiNet and the Tectogrammatical
Representation (TR) of the PDT. Section 2 describes
the annotation process in detail, including an intro-
duction to the encyclopedic tools available to the an-
notators. In Section 3 we present an evaluation met-
ric for MultiNet/TR labeled data. We also present an
evaluation of the data we have had annotated using
the proposed procedure. Finally, we conclude with
a short discussion of the problems observed during
the annotation process and suggest improvements as
future work.
1.1 MultiNet
The representation of the Multilayered Extended
Semantic Networks (MultiNet), which is described
in (Helbig, 2006), provides a universal formalism
for the treatment of semantic phenomena of natu-
ral language. To this end, they offer distinct ad-
vantages over the use of the classical predicate
calculus and its derivatives. For example, Multi-
Net provides a rich ontology of semantic-concept
types. This ontology has been constructed to be
language independent. Due to the graphical inter-
pretation of MultiNets, we believe manual anno-
tation and interpretation is simpler and thus more
cognitively compatible. Figure 1 shows the Multi-
Net annotation of a sentence from the WSJ corpus:
?Stephen Akerfeldt, currently vice president fi-
nance, will succeed Mr. McAlpine.?
In this example, there are a few relationships that il-
lustrate the representational power of MultiNet. The
main predicate succeed is a ANTE dependent of the
node now, which indicates that the outcome of the
event described by the predicate occurs at some time
later than the time of the statement (i.e., the succes-
sion is taking place after the current time as captured
by the future tense in the sentence). Intra-sentential
coreference is indicated by the EQU relationship.
From the previous context, we know that the vice
president is related to a particular company, Magna
International Inc. The pragmatically defined rela-
tionship between Magna International Inc. and vice
president finance is captured by the ATTCH (con-
ceptual attachment) relationship. This indicates that
there is some relationship between these entities for
which one is a member of the other (as indicated by
the directed edge). Stephen Akerfeldt is the agent of
the predicate described by this sub-network.
The semantic representation of natural language
expressions by means of MultiNet is generally in-
dependent of the considered language. In contrast,
the syntactic constructs used in different languages
to express the same content are obviously not iden-
tical. To bridge the gap between different languages
we employ the deep syntactico-semantic representa-
tion available in the Functional Generative Descrip-
tion framework (Sgall et al, 1986).
1.2 Prague Dependency Treebank
The Prague Dependency Treebank (PDT) presents a
language resource containing a deep manual analy-
sis of texts(Sgall et al, 2004). The PDT contains
annotations on three layers:
Morphological A rich morphological annotation is
provided when such information is available in
the language. This includes lemmatization and
detailed morphological tagging.
Analytical The analytical layer is a dependency
analysis based purely on the syntactic interpre-
tation.
Tectogrammatical The tectogrammatical annota-
tion provides a deep-syntactic (syntactico-
semantic) analysis of the text. The formal-
ism abstracts away from word-order, function
words (syn-semantic words), and morphologi-
cal variation.
The units of each annotation level are linked with
corresponding units on the preceding level. The
morphological units are linked directly with the
original tokenized text. Linking is possible as most
of these interpretations are directly tied to the words
in the original sentence. In MultiNet graphs, addi-
tional nodes are added and nodes are removed.
The PDT 2.0 is based on the long-standing
Praguian linguistic tradition, adapted for the current
38
Figure 1: MultiNet annotation of sentence ?Stephen Akerfeldt, currently vice president finance, will succeed Mr.
McAlpine.? Nodes C4 and C8 are re-used from previous sentences. Node C2 is an unexpressed (not explicitly stated
in the text) annotator-created node used in previous annotations.
computational-linguistics research needs. The theo-
retical basis of the tectogrammatical representation
lies in the Functional Generative Description of lan-
guage systems (Sgall et al, 1986). Software tools
for corpus search, lexicon retrieval, annotation, and
language analysis are included. Extensive documen-
tation in English is provided as well.
2 Integrated Annotation Process
We propose an integrated annotation procedure
aimed at acquiring high-quality MultiNet semantic
annotations. The procedure is based on a combi-
nation of annotation tools and annotation resources.
We present these components in the this section.
2.1 Annotation Tool
The core annotation is facilitated by the cedit
tool1, which uses PML (Pajas and S?te?pa?nek, 2005),
an XML file format, as its internal representa-
tion (Nova?k, 2007). The annotation tool is an
application with a graphical user interface imple-
mented in Java (Sun Microsystems, Inc., 2007). The
1The cedit annotation tool can be downloaded from
http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
cedit tool is platform independent and directly con-
nected to the annotators? wiki (see Section 2.4),
where annotators can access the definitions of indi-
vidual MultiNet semantic relations, functions and at-
tributes; as well as examples, counterexamples, and
discussion concerning the entity in question. If the
wiki page does not contain the required information,
the annotator is encouraged to edit the page with
his/her questions and comments.
2.2 Online Lexicon
The annotators in the semantic annotation project
have the option to look up examples of MultiNet
structures in an online version of the semantically
oriented computer lexicon HaGenLex (Hartrumpf et
al., 2003). The annotators can use lemmata (instead
of reading IDs formed of the lemma and a numer-
ical suffix) for the query, thus increasing the recall
of related structures. English and German input is
supported with outputs in English and/or German;
there are approximately 3,000 and 25,000 seman-
tic networks, respectively, in the lexicon. An exam-
ple sentence for the German verb ?borgen.1.1? (?to
borrow?) plus its automatically generated and val-
39
Figure 2: HaGenLex entry showing an example sentence
for the German verb ?borgen.1.1? (?to borrow?). The
sentence is literally ?The man borrows himself money
from the friend.?
idated semantic representation is displayed in Fig-
ure 2. The quality of example parses is assured by
comparing the marked-up complements in the ex-
ample to the ones in the semantic network. In the
rare case that the parse is not optimal, it will not be
visible to annotators.
2.3 Online Parser
Sometimes the annotator needs to look up a phrase
or something more general than a particular noun
or verb. In this case, the annotator can use
the workbench for (MultiNet) knowledge bases
(MWR (Gno?rlich, 2000)), which provides conve-
nient and quick access to the parser that translates
German sentences or phrases into MultiNets.
2.4 Wiki Knowledge Base
Awiki (Leuf and Cunningham, 2001) is used collab-
oratively to create and maintain the knowledge base
used by all the annotators. In this project we use
Dokuwiki (Badger, 2007). The entries of individ-
ual annotators in the wiki are logged and a feed of
changes can be observed using an RSS reader. The
cedit annotation tool allows users to display appro-
priate wiki pages of individual relation types, func-
tion types and attributes directly from the tool using
their preferred web browser.
3 Network Evaluation
We present an evaluation which has been carried
out on an initial set of annotations of English arti-
cles from The Wall Street Journal (covering those
annotated at the syntactic level in the Penn Tree-
bank (Marcus et al, 1993)). We use the annotation
from the Prague Czech-English Dependency Tree-
bank (Cur???n et al, 2004), which contains a large por-
tion of the WSJ Treebank annotated according to the
PDT annotation scheme (including all layers of the
FGD formalism).
We reserved a small set of data to be used to train
our annotators and have excluded these articles from
the evaluation. Three native English-speaking anno-
tators were trained and then asked to annotate sen-
tences from the corpus. We have a sample of 67
sentences (1793 words) annotated by two of the an-
notators; of those, 46 sentences (1236 words) were
annotated by three annotators.2 Agreement is mea-
sured for each individual sentences in two steps.
First, the best match between the two annotators?
graphs is found and then the F-measure is computed.
In order to determine the optimal graph match be-
tween two graphs, we make use of the fact that
the annotators have the tectogrammatical tree from
which they can select nodes as concepts in theMulti-
Net graph. Many of the nodes in the annotated
graphs remain linked to the tectogrammatical tree,
therefore we have a unique identifier for these nodes.
When matching the nodes of two different annota-
tions, we assume a node represents an identical con-
cept if both annotators linked the node to the same
tectogrammatical node. For the remaining nodes,
we consider all possible one-to-one mappings and
construct the optimal mapping with respect to the F-
measure.
Formally, we start with a set of tectogrammatical
trees containing a set of nodes N . The annotation is
a tuple G = (V,E, T,A), where V are the vertices,
E ? V ? V ?P are the directed edges and their la-
bels (e.g., agent of an action: AGT ? P ), T ? V ?N
is the mapping from vertices to the tectogrammati-
cal nodes, and finally A are attributes of the nodes,
which we ignore in this initial evaluation.3 Analo-
gously, G? = (V ?, E?, T ?, A?) is another annotation
2The data associated with this experiment can be down-
loaded from http://ufal.mff.cuni.cz/?novak/files/data.zip. The
data is in cedit format and can be viewed using the cedit editor
at http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
3We simplified the problem also by ignoring the mapping
from edges to tectogrammatical nodes and the MultiNet edge
attribute knowledge type.
40
of the same sentence and our goal is to measure the
similarity s(G,G?) ? [0, 1] of G and G?.
To measure the similarity we need a set ? of ad-
missible one-to-one mappings between vertices in
the two annotations. A mapping is admissible if
it connects vertices which are indicated by the an-
notators as representing the same tectogrammatical
node:
? =
{
? ? V ? V ?
??? (1)
?
n?N
v?V
v??V ?
((
(v,n)?T?(v?,n)?T ?
)
?(v,v?)??
)
? ?v?V
v?,w??V ?
((
(v,v?)???(v,w?)??
)
?(v?=w?)
)
? ?v,w?V
v??V ?
((
(v,v?)???(w,v?)??
)
?(v=w)
)}
In Equation 1, the first condition ensures that ? is
constrained by the mapping induced by the links to
the tectogrammatical layer. The remaining two con-
ditions guarantee that ? is a one-to-one mapping.
We define the annotation agreement s as:
sF (G,G?) = max??? (F (G,G
?, ?))
where F is the F1-measure:
Fm(G,G?, ?) = 2 ?m(?)|E|+ |E?|
wherem(?) is the number of edges that match given
the mapping ?.
We use four versions of m, which gives us four
versions of F and consequently four scores s for ev-
ery sentence:
Directed unlabeled: mdu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
((
v?, w?, ??
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected unlabeled: muu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
(
((v?, w?, ??) ? E? ? (w?, v?, ??) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Directed labeled: mdl(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
((
v?, w?, ?
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected labeled: mul(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
(
((v?, w?, ?) ? E? ? (w?, v?, ?) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
These four m(?) functions give us four possible
Fm measures, which allows us to have four scores
for every sentence: sdu, suu, sdl and sul.
Figure 3 shows that the inter-annotator agreement
is not significantly correlated with the position of the
sentence in the annotation process. This suggests
that the annotations for each annotator had achieved
a stable point (primarily due to the annotator training
process).
10 20 30 40 50
0.2
0.4
0.6
0.8
1.0
Sentence length
Inte
r?a
nno
tato
r F?
mea
sure
 ? U
ndir
ecte
d U
nlab
eled
Annotators
CB?CWSM?CWSM?CB
Figure 4: Inter-annotator agreement depending on the
sentence length. Each point represents a sentence.
Figure 4 shows that the agreement is not corre-
lated with the sentence length. It means that longer
41
0 10 20 30 40
0.2
0.4
0.6
0.8
1.0
Index
Und
irec
ted
 Un
labe
led 
F?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
0 10 20 30 40
0.0
0.2
0.4
0.6
Index
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 3: Inter-annotator agreement over time. Left: unlabeled, right: labeled. Each point represents a sentence; CB,
CW, and SM are the annotators? IDs.
sentences are not more difficult than short sentences.
The variance decreases with the sentence length as
expected.
In Figure 5 we show the comparison of directed
and labeled evaluations with the undirected unla-
beled case. By definition the undirected unlabeled
score is the upper bound for all the other scores.
The directed score is well correlated and not very
different from the undirected score, indicating that
the annotators did not have much trouble with de-
termining the correct direction of the edges. This
might be, in part, due to support from the formal-
ism and its tool cedit: each relation type is speci-
fied by a semantic-concept type signature; a relation
that violates its signature is reported immediately to
the annotator. On the other hand, labeled score is
significantly lower than the unlabeled score, which
suggests that the annotators have difficulties in as-
signing the correct relation types. The correlation
coefficient between suu and sul (approx. 0.75) is
also much lower than than the correlation coefficient
between suu and sdu (approx. 0.95).
Figure 6 compares individual annotator pairs. The
scores are similar to each other and also have a sim-
ilar distribution shape.
Undirected Unlabeled F?measure
Den
sity
0.0
0.5
1.0
1.5
2.0
2.5
0.2 0.4 0.6 0.8 1.0
CB ? CW 0.0
0.5
1.0
1.5
2.0
2.5
SM ? CB0.0
0.5
1.0
1.5
2.0
2.5
SM ? CW
Figure 6: Comparison of individual annotator pairs.
A more detailed comparison of individual anno-
tator pairs is depicted in Figure 7. The graph shows
that there is a significant positive correlation be-
tween scores, i.e. if two annotators can agree on the
42
0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
Undirected Unlabeled F?measure
Dire
cte
d U
nlab
eled
 F?
me
asu
re
Annotators
CB?CWSM?CWSM?CB
0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
Undirected Unlabeled F?measure
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 5: Left: Directed vs. undirected inter-annotator agreement. Right: Labeled vs. unlabeled inter-annotator agree-
ment. Each point represents a sentence.
annotation, the third is likely to also agree, but this
correlation is not a very strong one. The actual cor-
relation coefficients are shown under the main diag-
onal of the matrix.
Sample Annotators Agreement F-measure
suu sdu sul sdl
Smaller CB-CW 61.0 56.3 37.1 35.0
Smaller SM-CB 54.9 48.5 27.1 25.7
Smaller SM-CW 58.5 50.7 31.3 30.2
Smaller average 58.1 51.8 31.8 30.3
Larger CB-CW 64.6 59.8 40.1 38.5
Table 1: Inter-annotator agreement in percents. The re-
sults come from the two samples described in the first
paragraph of Section 3.
Finally, we summarize the raw result in Table 1.
Note that we report simple annotator agreement
here.
4 Conclusion and Future Work
We have presented a novel framework for the anno-
tation of semantic network for natural language dis-
course. Additionally we present a technique to eval-
uate the agreement between the semantic networks
annotated by different annotators.
Our evaluation of an initial dataset reveals that
given the current tools and annotation guidelines, the
annotators are able to construct the structure of the
semantic network (i.e., they are good at building the
directed graph). They are not, however, able to con-
sistently label the semantic relations between the se-
mantic nodes. In our future work, we will investigate
the difficulty in labeling semantic annotations. We
would like to determine whether this is a product of
the annotation guidelines, the tool, or the formalism.
Our ongoing research include the annotation of
inter-sentential coreference relationships between
the semantic concepts within the sentence-based
graphs. These relationships link the local structures,
allowing for a complete semantic interpretation of
the discourse. Given the current level of consistency
in structural annotation, we believe the data will be
useful in this analysis.
43
CB_CW
0.2 0.4 0.6 0.8
ll
l
l
l
l
l
ll
l
l
l
l
l l
l l
l
l
l
l
l
l
l l
l
l l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
1.0
ll
l
l
l
l
l
ll
l
l
l
l
ll
l l
l
l
l
l
l
l
l l
l
ll
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
0.34 SM_CW
ll
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
0.2 0.4 0.6 0.8 1.0
0.55 0.56
0.2 0.4 0.6 0.8
0.2
0.4
0.6
0.8
SM_CB
Undirected Unlabeled F?measure with Correlation Coefficients
Figure 7: Undirected, unlabeled F-measure correlation of annotator pairs. Each cell represents two different pairs of
annotators; cells with graphs show scatter-plots of F-scores for the annotator pairs along with the optimal linear fit;
cells with values show the correlation coefficient (each point in the plot corresponds to a sentence). For example,
the top row, right-most column, we are comparing the F-score agreement of annotators CB and CW with that of the
F-score agreement of annotators SM and CB. This should help identify an outlier in the consistency of the annotations.
Acknowledgment
This work was partially supported by Czech
Academy of Science grants 1ET201120505 and
1ET101120503; by Czech Ministry of Educa-
tion, Youth and Sports projects LC536 and
MSM0021620838; and by the US National Science
Foundation under grant OISE?0530118. The views
expressed are not necessarily endorsed by the spon-
sors.
References
Mike Badger. 2007. Dokuwiki ? A Practical Open
Source Knowledge Base Solution. Enterprise Open
Source Magazine.
Johan Bos. 2008. Let?s not Argue about Semantics. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Jan Cur???n, Martin C?mejrek, Jir??? Havelka, and Vladislav
Kubon?. 2004. Building parallel bilingual syntacti-
cally annotated corpus. In Proceedings of The First
International Joint Conference on Natural Language
Processing, pages 141?146, Hainan Island, China.
Carsten Gno?rlich. 2000. MultiNet/WR: A Knowledge
Engineering Toolkit for Natural Language Informa-
tion. Technical Report 278, University Hagen, Hagen,
Germany.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank 2.0.
44
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia, Pennsylvania.
Sven Hartrumpf, Hermann Helbig, and Rainer Osswald.
2003. The Semantically Based Computer Lexicon Ha-
GenLex ? Structure and Technological Environment.
Traitement Automatique des Langues, 44(2):81?105.
Hermann Helbig. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer, Berlin,
Germany.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way.
Quick Collaboration on the Web. Addison-Wesley,
Reading, Massachusetts.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Va?clav Nova?k. 2007. Cedit ? semantic networks man-
ual annotation tool. In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-HLT), pages 11?12,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Petr Pajas and Jan S?te?pa?nek. 2005. A Generic XML-
Based Format for Structured Linguistic Annotation
and Its Application to Prague Dependency Treebank
2.0. Technical Report 29, UFAL MFF UK, Praha,
Czech Republic.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel, Dordrecht, The Netherlands.
Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?. 2004.
Deep syntactic annotation: Tectogrammatical repre-
sentation and beyond. In Adam Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32?38, Boston,
Massachusetts, May. Association for Computational
Linguistics.
Sun Microsystems, Inc. 2007. Java Platform, Standard
Edition 6. http://java.sun.com/javase/6/webnotes/
README.html.
45
Coling 2010: Poster Volume, pages 819?827,
Beijing, August 2010
Instance Sense Induction from Attribute Sets
Ricardo Martin-Brualla
Google Inc
rmbrualla@gmail.com
Enrique Alfonseca
Google Inc
ealfonseca@google.com
Marius Pasca
Google Inc
mars@google.com
Keith Hall
Google Inc
kbhall@google.com
Enrique Robledo-Arnuncio
Google Inc
era@google.com
Massimiliano Ciaramita
Google Inc
massi@google.com
Abstract
This paper investigates the new problem
of automatic sense induction for instance
names using automatically extracted at-
tribute sets. Several clustering strategies
and data sources are described and eval-
uated. We also discuss the drawbacks of
the evaluation metrics commonly used in
similar clustering tasks. The results show
improvements in most metrics with re-
spect to the baselines, especially for pol-
ysemous instances.
1 Introduction
Recent work on information extraction increas-
ingly turns its attention to the automatic acqui-
sition of open-domain information from large
text collections (Etzioni et al, 2008). The ac-
quired information typically includes instances
(e.g. barack obama or hillary clinton), class la-
bels (e.g. politician or presidential candidate)
and relations and attributes of the instances (e.g.
president-country or date-of-birth) (Sekine, 2006;
Banko et al, 2007).
Within the larger area of relation extraction,
the acquisition of instance attributes (e.g. pres-
ident for instances of countries, or side effects
for instances of drugs) plays an important role,
since attributes may serve as building blocks in
any knowledge base constructed around open-
domain classes of instances. Thus, a variety
of attribute extraction methods mine textual data
sources ranging from unstructured (Tokunaga et
al., 2005) or structured (Cafarella et al, 2008) text
within Web documents, to human-compiled ency-
clopedia (Wu et al, 2008; Cui et al, 2009) and
Web search query logs (Pas?ca and Van Durme,
2007), attempting to extract, for a given class, a
ranked list of attributes that is as comprehensive
and accurate as possible.
Previous work on attribute extraction, however,
does not capture or address attributes of polyse-
mous instances. An instance may have differ-
ent meanings, and the extracted attributes may
not apply to all of them. For example, the
most salient meanings of darwin are the scientist
Charles Darwin, an Australian city, and an op-
erating system, plus many less-known meanings.
For these ambiguous instances, it is common for
the existing procedures to extract mixed lists of
attributes that belong to incompatible meanings,
e.g. {biography, population, hotels, books}.
This paper explores the problem of automati-
cally inducing instance senses from the learned
attribute lists, and describes several clustering so-
lutions based on a variety of data sources. For
that, it brings together research on attribute acqui-
sition and on word sense induction. Results show
that we can generate meaninful groupings of at-
tributes for polysemous instance names, while not
harming much the monosemous instance names
by generating unwanted clusters for them. The
results are much better than for a random base-
line, and are superior to the one-in-all and the all-
singleton baselines.
2 Previous Work
Previous work on attribute extraction uses a va-
riety of types of textual data as sources for mining
attributes. Some methods take advantage of struc-
tured and semi-structured text available within
Web documents. Examples of this are the use of
markup information in HTML documents to ex-
819
tract patterns and clues around attributes (Yoshi-
naga and Torisawa, 2007; Wong and Lam, 2009;
Ravi and Pas?ca, 2008), or the use of articles
within online encyclopedia as sources of struc-
tured text for attribute extraction (Suchanek et al,
2007; Nastase and Strube, 2008; Wu and Weld,
2008). Regarding unstructured text in Web docu-
ments, the method described in (Tokunaga et al,
2005) takes various class labels as input, and ap-
plies manually-created lexico-syntactic patterns to
document sentences to extract candidate attributes
ranked using several frequency statistics. In (Bel-
lare et al, 2007), the extraction is guided by a set
of seed instances and attributes rather than hand-
crafted patterns, with the purpose of generating
training data and extract new instance-attribute
pairs from text.
Web search queries have also been used as a
data source for attribute extraction, using lexico-
syntactic patterns (Pas?ca and Van Durme, 2007) or
seed attributes (Pas?ca, 2007) to guide the extrac-
tion, and leading to attributes of higher accuracy
than those extracted with equivalent techniques
from Web documents (Pas?ca et al, 2007).
Another related area to this work is the field of
word sense induction: the task of identifying the
possible senses of a word in a corpus using unsu-
pervised methods (Yarowsky, 1995), as opposed
to traditional disambiguation methods which rely
on the availability of a finite and static list of pos-
sible meanings. In (Agirre and Soroa, 2007) a
framework is proposed for evaluating such sys-
tems. Word sense induction can be naturally for-
mulated as a clustering task. This introduces
the complication of choosing the right number
of possible senses, hence a Bayesian approach to
WSI was proposed which deals with this problem
within a principled generative framework (Brody
and Lapata, 2009). Another related line of work
Turkey Attributes Darwin Attributes
maps1 capital1 maps1 definition1,3
recipes2 culture1 awards2 jobs1
pictures1,2 history1 shoes1 tourism1
calories2 tourism1 evolution3 biography3
facts1,2 nutrition facts2 theory3 attractions1
nutrition2 beaches1 weather1 hotels1
cooking time2 brands2 pictures1,3 ports4
religion1 language1 quotes3 population1
Table 1: Attributes extracted for the instances
Turkey and Darwin.
is the disambiguation of people names (Mann and
Yarowsky, 2003). In SEMEVAL-1, a shared task
was introduced dedicated to this problem, the Web
People Search task (Artiles et al, 2007; Artiles et
al., 2009). Disambiguating names is also often ap-
proached as a clustering problem. One challenge
shared by word sense induction and name disam-
biguation (and most unsupervised settings), is the
evaluation. In both tasks, simple baselines such as
predicting one single cluster tend to outperform
more sophisticated approaches (Agirre and Soroa,
2007; Artiles et al, 2007).
3 Instance Sense Induction
3.1 Problem description
This paper assumes the existence of an attribute
extraction procedure. Using those attributes, our
aim is to identify the coarse-grained meanings
with which each attribute is associated. As an
example, Table 1 shows the top 16 attributes ex-
tracted using the procedure described in (Pas?ca
and Van Durme, 2007). Salient meanings for
turkey are the country name (labeled as 1 in the
table), and the bird name (labeled as 2). Some at-
tributes are applicable to both meanings (pictures
and facts). The second example, darwin, can re-
fer to a city (sense 1), the Darwin Awards (sense
2), the person (sense 3), and an operating system
(sense 4).
Examples of applications that need to dis-
criminate between the several meanings of in-
stances are user-facing applications requiring the
attributes to be organized logically and informa-
tion extraction pipelines that depend on the ex-
tracted attributes to find values in documents.
The problem we are addressing is the automatic
induction of instance senses from the attribute
sets, by grouping together the attributes that can
be applied to a particular sense. As in related work
on sense induction (Agirre and Soroa, 2007; Ar-
tiles et al, 2007), we approach this as a clustering
problem: finding the right similarity metrics and
clustering procedures to identify sets of related at-
tributes in an instance. We propose a clustering
based on the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977), exploring differ-
ent parameters, similarity sources, and prior dis-
tributions.
820
3.2 Instance and attributes input data
The input data of instances and attributes has been
obtained, in a fully automated way, following
the method described in (Pas?ca and Van Durme,
2007). The input dataset is a set of fully anony-
mized set of English queries submitted to a popu-
lar (anonymized) search engine. The set contains
millions of unique isolated, individual queries that
are independent from one another. Each query
is accompanied by its frequency of occurrence
in the query logs. The sum of frequencies of
all queries in the dataset is hundreds of millions.
Other sources of similar data are available pub-
licly for research purposes (Gao et al, 2007). This
extraction method applies a few patterns (e.g., the
A of I, or I?s A, or A of I) to queries within
query logs, where an instance I is one of the most
frequent 5 million queries from the repository of
isolated queries, and A is a candidate attribute.
For each instance, the method extracts ranked lists
containing zero, one or more attributes, along with
frequency-based scores. For this work, only the
top 32 attributes of each instance were used, in or-
der to have an input set for the clustering with a
reasonable size, but to keep precision at high lev-
els.
3.3 Per-attribute clustering information
For each (instance, attribute) pair, the following
information is collected:
Search results: The top 20 search results (in-
cluding titles and snippets) returned by a popular
search engine for a query created by concatenat-
ing the instance and the attribute. The motivation
for this data source is that the attributes that re-
fer to the same meaning of the instance should
help the search engine in selecting web pages that
refer to that meaning. The titles and snippets of
these search results are expected to contain other
terms related to that meaning. For example, for
the queries [turkey maps] and [turkey culture] the
search results will contain information related to
the country, whereas [turkey recipes] and [turkey
nutritional value] should share many terms about
the poultry.
Query sessions: A query session is a series of
queries submitted by a single user within a small
range of time (Silverstein et al, 1999). Informa-
tion stored in the session logs may include the text
For each (instance, attribute) pair:
? Retrieve all the sessions that contained the query [in-
stance attribute].
? Collect the set of all the queries that appeared in the
same session and which are a superstring of instance.
? Remove instance from each of those queries, and out-
put the resulting set of query words.
Figure 1: Algorithm to collect session phrases as-
sociated to attributes.
of the queries and metadata, such as the time, the
type of query (e.g., using the normal or the ad-
vance form), and user settings such as the Web
browser used (Silverstein et al, 1999).
Users often search for related queries within
a session: queries on the culture of the coun-
try Turkey will tend to be surrounded by queries
about topics related to the country; similarly,
queries about turkey recipes will tend to be sur-
rounded by other queries on recipes. Therefore,
if two attributes refer to the same meaning of the
instance, the distributions of terms that co-occur
with them in the same search sessions is expected
to be similar. To ensure that the user did not
change intent during the session, we also require
the queries from which we extract phrases to con-
tain the instance of interest. The pseudocode of
the procedure is shown in Figure 1.
Class labels: As described in (Pas?ca and Van
Durme, 2008), we collect for each instance (e.g.,
turkey), a ranked list of class labels (e.g., country,
location, poultry, food). The procedure uses a col-
lection of Web documents and applies some IsA
extraction patterns selected from (Hearst, 1992).
Using the (instance, ranked-attributes) and the (in-
stance, ranked-class labels) lists, it is possible to
aggregate the two datasets to obtain, for each at-
tribute, the class labels that are most strongly as-
sociated to it (Figure 2).
3.4 EM clustering
We run a set of EM clusterings separately for the
attributes of each instance. The model imple-
mented is the following: given an instance, let
A = {a1, a2, ..., an} be the set of attributes as-
sociated with that instance. Let T be the vocabu-
lary for the terms found in the search results, S the
vocabulary of session log terms co-occurring with
821
For each attribute:
? Collect all the instances that contain that attribute.
? For each class label, average its ranks for those in-
stances. If an instance does not contain a particular
class label, use as rank the size of the longest list of
class labels plus one.
? Rank the class labels from smaller to larger average
rank.
Figure 2: Algorithm to collect class labels associ-
ated to attributes.
the attribute, and C be the set of all the possible
class labels. Let K be the cluster function which
assigns cluster indexes to the attributes.
We assume that the distributions for snippet
terms, session terms and class labels are condi-
tionally independent given the clustering. Further-
more, we assume that the distribution of terms for
queries in a cluster are also conditionally indepen-
dent given the cluster assignments:
p?(T |K,A) ?
Y
j
p?(tj |K,A)
p?(S|K,A) ?
Y
k
p?(sk|K,A)
p?(C|K,A) ?
Y
l
p?(cl|K,A)
The clustering model for each instance (the ex-
pectation step) is, therefore:
p?(KT SC|A,?) =
N?
i
p?(K|A)p?(T |K,A)p?(S|K,A)p?(C|K,A)
To estimate the parameters of the model, we must
be able to estimate the following distributions dur-
ing the maximization step:
? p?(tj |K,A) = E?(tj ,K|A)E?(K|A)
? p?(sk|K,A) = E?(sk,K|A)E?(K|A)
? p?(cl|K,A) = E?(cl,K|A)E?(K|A)One advantage of this approach is that it allows
using a subset of the available data sources to eval-
uate their relative influence on the clustering qual-
ity. In the experiments we have tried all possible
combinations of the three data sources to find the
settings that give the best results.
3.5 Initialization strategies
The initial assignment of attributes to clusters is
important, since a bad seed clustering can lead
EM to local optima. We have tried the following
two strategies:
Random assignment: the attributes are assigned
to clusters randomly. To make the results repeat-
able, for each instance we use the instance name
as the seed for the random number generator.
K-means: the initial assignments of attributes
to clusters is performed using K-means. In this
model, we use a simple vector-space-model in the
following way:
1. Each attribute is represented with a bag-of-
words of the snippets of the search results for
a concatenation of the instance name and the
attribute. This is the same data already col-
lected for EM.
2. Each of the snippet terms in these bag-of-
words is weighted using the tf ? idf score,
with inverse document frequencies estimated
from an English web corpus with hundreds
of millions of documents.
3. The cosine of the angle of the vectors is used
as the similarity metric between each pair of
attributes.
Several values of K have been tried in our exper-
iments, as mentioned in Section 4.
3.6 Post-processing
EM works with a fixed set of clusters. In order
to decide which is the optimal number of clusters,
we have run all the experiments with a number of
clusters K that is large enough to accommodate
most of the queries in our dataset, and we run a
post-processing step that merges clusters for in-
stances that have less than K meanings.
Since we have, for each attribute, a distribution
of the most likely class labels (Section 3.3), the
post-processing performs as follows:
1. Generate a list of class labels per cluster, by
combining the ranked lists of per-attribute
class labels as was done in Section 3.3.
2. Merge together all the clusters such that their
sets of top k class labels are the same.
The values ofK and k are chosen by doing several
runs with different values on the development set,
as described in Section 4.
822
4 Evaluation and Results
4.1 Evaluation metrics
There does not exist a fully agreed evaluation
metric for clustering tasks in NLP (Geiss, 2009;
Amigo? et al, 2009). Each metric has its own
idiosyncrasies, so we have chosen to compute
six different evaluation metrics as described in
(Amigo? et al, 2009). Empirical results show they
are highly correlated, i.e., tuning a parameter by
hill-climbing on F-score typically also improves
the B3 F-score.
Purity (Zhao and Karypis, 2002): Let C be
the clusters to evaluate, L the set of cate-
gories (the clusters in the gold-standard), and
N the number of clustered items. Purity is
the average of the precision values: Purity =?
i
|Ci|
N maxj Prec(Ci, Lj), where the precisionfor cluster Ci with respect to category Lj is
Prec(Ci, Lj) = |Ci?Lj ||Ci| . Purity is a precision met-ric. Inverting the roles of the categories L and the
clusters C gives a recall metric, inverse purity,
which rewards grouping items together. The two
metrics can be combined in an F-score.
B3 Precision (Bagga and Baldwin, 1998): Let
L(e) and C(e) denote the gold-standard-category
and the cluster of an item e. The correctness of the
relation between e and other element e? is defined
as
Correctness(e, e?) =
?
1 iffL(e) = L(e?)? C(e) = C(e?)
0 otherwise
The B3 Precision of an item is the proportion
of items in its cluster which belong to its cat-
egory, including itself. The total precision is
the average of the item precisions: B3 Prec =
avge[avge?:C(e)=C(e?)Correctness(e, e?)]
B3 Recall: is calculated in a similar way, inverting
the roles of clusters and categories. The B3 F-
score is obtained by combining B3 precision and
B3 recall.
4.2 Gold standards
We have built two annotated sets, one to be used
as a development set for adjusting the parame-
ters, and a second one as a test set. The evalu-
ation settings were chosen without knowledge of
Purity Inv. F-score B3 B3 B3
Purity Precision Recall F-score
0.94 0.95 0.92 0.90 0.92 0.91
Table 2: Inter-judge agreement scores.
Polysemous Main meanings
airplane machine, movie
apple fruit, company
armstrong unit, company, person
chain reaction company, film, band, chemistry
chf airport, currency, heart attack
darwin person, city
david copperfield book, performer, movie
delta letter, airways
Table 3: Examples of polysemous instances.
the test set. Each of the two sets contains 75 in-
stances chosen randomly from the complete set of
instances with ranked attributes (Section 3.2 de-
scribed the input data). For the random sampling,
the instances were weighted with their frequency
in the query logs as full queries, so that more
frequent instances have higher chance to be cho-
sen. This ensures that uncommon instances are
not overrepresented in the gold-standard.
The annotators contributed 50 additional in-
stances (25 for development and 25 for testing)
that they considered interesting to study, e.g., be-
cause of having several salient meanings.
Five human annotators were shown the top 32
attributes for each instance, and they were asked
to cluster them. We decided to start with a sim-
plified version of the problem by considering it a
hard clustering task.
Table 2 shows that the average agreement
scores between judge pairs, measured with the
same evaluation metrics used for the system out-
put, are quite high. In the first three metrics, the
F-score is not an average of precision and recall,
but a weighted average calculated separately for
each cluster, so it may have a value that is not be-
tween the values of precision and recall.
The annotated instances were classified as
monosemous/polysemous, depending on wether
or not they had more than one cluster with enough
(five) attributes. This classification allows to re-
port separate results for the whole set (where in-
stances with just one major sense dominate) and
for the subset of polysemous instances. Table 3
shows examples of polysemous instances. Exam-
823
All instances polysemous instances
Weights Purity Inv. F B3 B3 B3 F Purity Inv. F B3 B3 B3 F
Purity score Prec. Recall score Purity score Prec. Recall score
All-in-one 0.797 1.000 0.766 0.700 1.000 0.797 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242 1.000 0.205 0.266 1.000 0.205 0.333
Random 0.888 0.322 0.451 0.851 0.246 0.373 0.685 0.362 0.447 0.595 0.276 0.373
Random Only snippets 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
Init. Only sessions 0.797 0.948 0.728 0.700 0.944 0.753 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.798 0.983 0.760 0.701 0.969 0.785 0.561 0.990 0.541 0.415 0.981 0.574
No snippets 0.798 0.934 0.723 0.702 0.918 0.744 0.561 0.990 0.541 0.415 0.981 0.574
No sessions 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
No class labels 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399
All 0.809 0.380 0.420 0.736 0.316 0.414 0.596 0.430 0.400 0.483 0.361 0.399
K-Means Only snippets 0.844 0.765 0.700 0.771 0.654 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Init. Only sessions 0.798 0.957 0.736 0.702 0.949 0.759 0.558 1.000 0.540 0.410 1.000 0.573
Only class labels 0.824 0.656 0.622 0.747 0.568 0.604 0.641 0.768 0.565 0.519 0.699 0.575
No snippets 0.824 0.655 0.622 0.748 0.562 0.598 0.640 0.768 0.565 0.518 0.698 0.574
No sessions 0.843 0.770 0.701 0.769 0.661 0.677 0.671 0.806 0.587 0.556 0.719 0.611
No class labels 0.844 0.762 0.698 0.771 0.651 0.673 0.671 0.806 0.587 0.556 0.719 0.611
All 0.843 0.767 0.699 0.770 0.657 0.675 0.671 0.806 0.587 0.556 0.719 0.611
Table 4: Scores over all instances and over polysemous instances.
ples of monosemous instances are activision, am-
ctheaters, american airlines, ask.com, bebo, dis-
ney or einstein. 22% of the instances in the devel-
opment set and 13% of the instances in the test set
are polysemous.
4.3 Parameter tuning
We tuned the different parameters of the algorithm
using the development set. We performed several
EM runs including all three data sources, modi-
fying the following parameters: the smoothing 
added to the cluster soft-assignment in the Maxi-
mization step (Manning et al, 2008), the number
K of clusters for K-Means and EM, and the num-
ber k of top ranked class labels that two clusters
need to have in common in order to be merged
at the post-processing step. The best results were
obtained with  = 0.4, K = 5 and k = 1. These
are the values used in the experiments mentioned
from now on.
4.4 EM initialization and data sources
Table 4 shows the results after running EM over
the development set, using every possible combi-
nation of data sources, and the two initialization
strategies (random and K-Means). Several obser-
vations can be drawn from this table:
First, as mentioned in Section 2, the evalua-
tion metrics are biased towards the all-in-one solu-
tion. This is worsened by the fact that the majority
of the instances in our dataset are monosemous.
Therefore, the highest F-scores and B3 F-scores
are obtained by the all-in-one baseline, although
it is not the most useful clustering.
When using only class labels, EM tends to pro-
duce results similar to the all-in-one baseline This
can be explained by the limited class vocabulary
which makes most of the attributes share class la-
bels. The bad results when using only sessions are
caused by the presence of attributes with no ses-
sion terms, due to insufficient data.
The random clustering baseline (third line in
Table 4) tends to give smaller clusters than EM,
because it distributes instances uniformly across
the clusters. This leads to better precision scores,
and much worse recall and F-score metrics.
From these results, we conclude that snippet
terms are the most useful resource for clustering.
The other data sources do not provide a signifi-
cant improvement over it. The best results overall
for the polysemous instances, and the highest re-
sults for the whole dataset (excluding the outliers
that are too similar to the all-in-one baseline) are
obtained using snippet terms. For these configura-
tions, as we expected, the K-Means initialization
does a better job in avoiding local optima during
EM than the random one.
4.5 Post-processing
Table 5 includes the results on the development
set after post-processing, using the best configu-
ration for EM (K-Means initialization and snippet
terms for EM). Post-processing slightly hurts the
B3 F-score for polysemous terms, but it improves
results for the whole dataset, as it merges many
clusters for the monosemous instances.
824
Data Method Purity Inv. Purity F-score B3 Prec. B3 Recall B3 F-score
All instances All-in-one 0.797 1.000 0.766 0.700 1.000 0.797
All-singletons 1.000 0.145 0.187 1.000 0.145 0.242
K-Means + EM (snippets) 0.844 0.765 0.700 0.771 0.654 0.675
K-Means + EM (snippets) + postprocessing 0.825 0.837 0.728 0.743 0.761 0.722
Polysemous All-in-one 0.558 1.000 0.540 0.410 1.000 0.573
All-singletons 1.000 0.205 0.266 1.000 0.205 0.333
K-Means + EM (snippets) 0.671 0.806 0.587 0.556 0.719 0.611
K-Means + EM (snippets) + postprocessing 0.644 0.846 0.592 0.518 0.777 0.607
Table 5: Scores only over all and polysemous instances, without and with postprocessing.
K-Means output EM output Post-processing
pictures, family, logo, biography pictures, biography, inauguration pictures, biography, inauguration
inauguration, song, lyrics, foods, song, lyrics, foods, timeline, song, lyrics, goods, timeline,
quotes, timeline, shoes, health care camping, shoes, maps, art, history, camping, shoes, maps, art, history
maps, art, kids, history, speeches official website, facts, speeches official website, facts, speeches
official website, facts, scandal scandal, blog, music scandal, blog, music, family, kids
economy, blog, music, flag, camping approval rating, health care, daughters
approval rating economy approval rating, health care,
daughters family, kids, daughters economy
symbol logo, quotes, symbol, flag logo, quotes, symbol, definition
definition, religion, definition, religion, slogan, books religion, slogan, books, flag
slogan, books
Table 6: Attributes extracted for the monosemous instance obama, using snippet terms for EM.
4.6 Clustering examples
Tables 6 and 7 show examples of clustering results
for three instances chosen as representatives of the
monosemous and the polysemous subsets. These
show that the output of the K-Means initialization
can uncover some meaningful clusters, but tends
to generate a dominant cluster and a few small or
singleton clusters. EM distributes the attributes
more evenly across clusters, combining attributes
that are closely related.
For monosemous instances like obama, EM
generates small clusters of highly related at-
tributes (e.g, family, kids and daughters). Post-
processing merges some of the clusters together,
but it fails to merge all into a single cluster.
For darwin, two of the small clusters given by
K-Means are actually good, as ports is the only at-
tribute of the operating system, and lyrics is one of
the two attributes referring to a song titled Darwin.
EM again redistributes the attributes, creating two
large and mostly correct clusters.
For david copperfield, EM creates two clusters
for the performer, one for the book, one for the
movie, and one for tattoo (off-topic for this in-
stance). The two clusters referring to the per-
former are merged in the post-processing, with
some errors remaining, e.g, trailer and second
wife are in the wrong cluster.
4.7 Results on the test set
Table 8 show the results of the EM clustering and
the postprocessing step when executed on the test
set. The settings are those that produced the best
results on the development set: using EM initial-
ized with K-Means, and using only snippet terms
for the generative model.
As mentioned above, the test set has a higher
proportion of monosemous queries than the de-
velopment set, so the all-in-one baseline pro-
duces better results than before. Still, we can see
the same trend happening: for the whole dataset
the F-score metrics are somewhat worse than the
best baseline, given that the evaluation metrics all
overvalue the all-in-one baseline, but this can be
considered an artifact of the metrics. As with the
development set, using EM produces the best pre-
cision scores (except for the all-singletons base-
line), and the postprocessing improves precision
and F-score over the all-in-one baseline. The
whole system improves considerably the F-score
for the polysemous terms.
5 Conclusions
This paper investigates the new task of inducing
instance senses using ranked lists of attributes as
input. It describes a clustering procedure based
on the EM model, capable of integrating differ-
825
Instance K-Means output EM output Post-processing
Darwin maps, shoes, logo, awards, maps, shoes, logo, maps, shoes, logo,
weather pictures, quotes, weather jobs, tourism weather jobs, tourism
definition, jobs, tourism, hotels, attractions, hotels, attractions,
biography, hotels, beaches, accommodation, beaches, accommodation,
attractions, beaches, tv show, clothing, tv show, clothing,
accommodation, tv show, postcode, music, review postcode, music, review
clothing, postcode, music side effects, airlines, side effects, airlines,
facts, review, history prices, lighting prices, lighting
side effects, airlines, awards, ports definition, population
prices, lighting evolution, theory, quotes awards, ports
ports pictures, biography, evolution, theory, quotes
evolution, theory, books facts, history, books pictures, biography,
lyrics lyrics facts, history, books
population definition, population lyrics
David Copperfield summary, biography, pictures, biography, pictures, quotes, biography, pictures, girlfriend
quotes, strokes, book review, strokes, tricks, tour dates, quotes, strokes, tricks, tattoo
tricks, tour dates, characters, lyrics, dating, logo, tour dates, secrets, lyrics,
lyrics, plot, synopsis, dating, filmography, cast members, wives, music, dating, logo,
logo, themes, author, official website, trailer, filmography, blog, cast members,
filmography, cast members, setting, religion official website, trailer,
official website, trailer, book review, review, house, setting, religion
setting, religion reviews book review, review, house,
house, reviews tattoo reviews
tattoo summary, second wife, summary, second wife,
second wife characters, plot, synopsis, characters, plot, synopsis,
girlfriend, secrets, wives, themes, author themes, author
review, music, blog girlfriend, secrets, wives,
music, blog
Table 7: Attributes extracted for three polysemous instances, using snippet terms for EM.
Set Solution Purity Inverse Purity F-score B3 Precision B3 Recall B3 F-score
All All-in-one 0.907 1.000 0.892 0.858 1.000 0.908
All-singletons 1.000 0.076 0.114 1.000 0.076 0.136
Random 0.936 0.325 0.463 0.914 0.243 0.377
EM 0.927 0.577 0.664 0.896 0.426 0.561
EM+postprocessing 0.919 0.806 0.804 0.878 0.717 0.764
Polysemous All-in-one 0.588 1.000 0.586 0.457 1.000 0.613
All-singletons 1.000 0.141 0.210 1.000 0.141 0.239
Random 0.643 0.382 0.441 0.549 0.288 0.369
EM 0.706 0.631 0.556 0.626 0.515 0.547
EM+postprocessing 0.675 0.894 0.650 0.564 0.842 0.661
Table 8: Scores in the test set.
ent data sources, and explores cluster initializa-
tion and post-processing strategies. The evalu-
ation shows that the most important of the con-
sidered data sources is the snippet terms obtained
from search engine results to queries made by
concatenating the instance and the attribute. A
simple post-processing that merges attribute clus-
ters that have common class labels can improve
recall for monosemous queries. The results show
improvements across most metrics with respect to
a random baseline, and F-score improvements for
polysemous instances.
Future work includes extending the generative
model to be applied across the board, linking the
clustering models of different instances with each
other. We also intend to explore applications of
the clustered attributes in order to perform extrin-
sic evaluations on these data.
References
Agirre, Eneko and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimination
systems. In Proceedings of SemEval-2007, pages 7?12.
Association for Computational Linguistics.
Amigo?, E., J. Gonzalo, J. Artiles, and F. Verdejo. 2009.
A comparison of extrinsic clustering evaluation met-
rics based on formal constraints. Information Retrieval,
12(4):461?486.
Artiles, Javier, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 WePS evaluation: Establishing a
benchmark for the web people search task. In Proceed-
ings of SemEval-2007, pages 64?69.
Artiles, J., J. Gonzalo, and S. Sekine. 2009. Weps 2 evalua-
tion campaign: overview of the web people search cluster-
ing task. In 2nd Web People Search Evaluation Workshop
(WePS 2009), 18th WWW Conference.
Bagga, A. and B. Baldwin. 1998. Entity-based cross-
document co-referencing using the vector space model,
Proceedings of the 17th international conference on Com-
putational linguistics. In Proceedings of ACL-98.
826
Banko, M., Michael J Cafarella, S. Soderland, M. Broad-
head, and O. Etzioni. 2007. Open information extraction
from the Web. In Proceedings of IJCAI-07, pages 2670?
2676, Hyderabad, India.
Bellare, K., P.P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. In NIPS 2007 Workshop
on Machine Learning for Web Search.
Brody, Samuel and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of EACL ?09, pages 103?
111.
Cafarella, M.J., A. Halevy, D.Z. Wang, and Y. Zhang. 2008.
Webtables: Exploring the Power of Tables on the Eeb.
Proceedings of the VLDB Endowment archive, 1(1):538?
549.
Cui, G., Q. Lu, W. Li, and Y. Chen. 2009. Automatic Acqui-
sition of Attributes for Ontology Construction. In Pro-
ceedings of the 22nd International Conference on Com-
puter Processing of Oriental Languages, pages 248?259.
Springer.
Dempster, A.P., N.M. Laird, D.B. Rubin, et al 1977. Max-
imum likelihood from incomplete data via the EM algo-
rithm. Journal of the Royal Statistical Society. Series B
(Methodological), 39(1):1?38.
Etzioni, O., M. Banko, S. Soderland, and S. Weld. 2008.
Open Information Extraction from the Web. Communica-
tions of the ACM, 51(12), December.
Gao, W., C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, and
H. Hon. 2007. Cross-lingual query suggestion using
query logs of different languages. In Proceedings of
SIGIR-07, pages 463?470, Amsterdam, The Netherlands.
Geiss, J. 2009. Creating a Gold Standard for Sentence Clus-
tering in Multi-Document Summarization. ACL-IJCNLP
2009.
Hearst, M. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proceedings of COLING-92, pages
539?545, Nantes, France.
Mann, Gideon S. and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings of
HLT-NAACL 2003, pages 33?40. Association for Compu-
tational Linguistics.
Manning, C.D., P. Raghavan, and H. Schtze. 2008. Intro-
duction to Information Retrieval. Cambridge University
Press New York, NY, USA.
Nastase, V. and M. Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI-08, pages 1219?1224, Chicago, Illinois.
Pas?ca, M. and B. Van Durme. 2007. What you seek is what
you get: Extraction of class attributes from query logs. In
Proceedings of IJCAI-07, pages 2832?2837, Hyderabad,
India.
Pas?ca, M. and B. Van Durme. 2008. Weakly-supervised ac-
quisition of open-domain classes and class attributes from
web documents and query logs. In Proceedings of ACL-
08, pages 19?27, Columbus, Ohio.
Pas?ca, M., B. Van Durme, and N. Garera. 2007. The role of
documents vs. queries in extracting class attributes from
text. In Proceedings of CIKM-07, pages 485?494, Lis-
bon, Portugal.
Pas?ca, M. 2007. Organizing and searching the World Wide
Web of facts - step two: Harnessing the wisdom of the
crowds. In Proceedings of WWW-07, pages 101?110,
Banff, Canada.
Ravi, S. and M. Pas?ca. 2008. Using Structured Text for
Large-Scale Attribute Extraction. In CIKM. ACM New
York, NY, USA.
Sekine, S. 2006. On-Demand Information Extraction. In
Proceedings of the COLING/ACL on Main conference
poster sessions, pages 731?738. Association for Compu-
tational Linguistics Morristown, NJ, USA.
Silverstein, C., H. Marais, M. Henzinger, and M. Moricz.
1999. Analysis of a very large web search engine query
log. In ACM SIGIR Forum, pages 6?12. ACM New York,
NY, USA.
Suchanek, F., G. Kasneci, and G. Weikum. 2007. Yago:
a core of semantic knowledge unifying WordNet and
Wikipedia. In Proceedings of WWW-07, pages 697?706,
Banff, Canada.
Tokunaga, K., J. Kazama, and K. Torisawa. 2005. Au-
tomatic discovery of attribute words from Web docu-
ments. In Proceedings of the 2nd International Joint Con-
ference on Natural Language Processing (IJCNLP-05),
pages 106?118, Jeju Island, Korea.
Wong, T.L. and W. Lam. 2009. An Unsupervised Method
for Joint Information Extraction and Feature Mining
Across Different Web Sites. Data & Knowledge Engi-
neering, 68(1):107?125.
Wu, F. and D. Weld. 2008. Automatically refining the
Wikipedia infobox ontology. In Proceedings of WWW-
08, pages 635?644, Beijing, China.
Wu, F., R. Hoffmann, and D. Weld. 2008. Information
extraction from Wikipedia: Moving down the long tail.
In Proceedings of KDD-08, pages 731?739, Las Vegas,
Nevada.
Yarowsky, David. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings of
ACL-95, pages 189?196. Association for Computational
Linguistics.
Yoshinaga, N. and K. Torisawa. 2007. Open-Domain
Attribute-Value Acquisition from Semi-Structured Texts.
In Proceedings of the Workshop on Ontolex, pages 55?66.
Zhao, Y. and G. Karypis. 2002. Criterion functions for docu-
ment clustering. Technical report, Experiments and Anal-
ysis University of Minnesota, Department of Computer
Science/Army HPC Research Center.
827
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62?72,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multi-Source Transfer of Delexicalized Dependency Parsers
Ryan McDonald
Google
New York, NY
ryanmcd@google.com
Slav Petrov
Google
New York, NY
slav@google.com
Keith Hall
Google
Zu?rich
kbhall@google.com
Abstract
We present a simple method for transferring
dependency parsers from source languages
with labeled training data to target languages
without labeled training data. We first demon-
strate that delexicalized parsers can be di-
rectly transferred between languages, produc-
ing significantly higher accuracies than unsu-
pervised parsers. We then use a constraint
driven learning algorithm where constraints
are drawn from parallel corpora to project the
final parser. Unlike previous work on project-
ing syntactic resources, we show that simple
methods for introducing multiple source lan-
guages can significantly improve the overall
quality of the resulting parsers. The projected
parsers from our system result in state-of-the-
art performance when compared to previously
studied unsupervised and projected parsing
systems across eight different languages.
1 Introduction
Statistical parsing has been one of the most active ar-
eas of research in the computational linguistics com-
munity since the construction of the Penn Treebank
(Marcus et al, 1993). This includes work on phrase-
structure parsing (Collins, 1997; Charniak, 2000;
Petrov et al, 2006), dependency parsing (McDonald
et al, 2005; Nivre et al, 2006) as well as a num-
ber of other formalisms (Clark and Curran, 2004;
Wang and Harper, 2004; Shen and Joshi, 2008).
As underlying modeling techniques have improved,
these parsers have begun to converge to high lev-
els of accuracy for English newswire text. Subse-
quently, researchers have begun to look at both port-
ing these parsers to new domains (Gildea, 2001; Mc-
Closky et al, 2006; Petrov et al, 2010) and con-
structing parsers for new languages (Collins et al,
1999; Buchholz and Marsi, 2006; Nivre et al, 2007).
One major obstacle in building statistical parsers
for new languages is that they often lack the manu-
ally annotated resources available for English. This
observation has led to a vast amount of research
on unsupervised grammar induction (Carroll and
Charniak, 1992; Klein and Manning, 2004; Smith
and Eisner, 2005; Cohen and Smith, 2009; Berg-
Kirkpatrick and Klein, 2010; Naseem et al, 2010;
Spitkovsky et al, 2010; Blunsom and Cohn, 2010).
Grammar induction systems have seen large ad-
vances in quality, but parsing accuracies still signif-
icantly lag behind those of supervised systems. Fur-
thermore, they are often trained and evaluated under
idealized conditions, e.g., only on short sentences
or assuming the existence of gold-standard part-of-
speech (POS) tags.1 The reason for these assump-
tions is clear. Unsupervised grammar induction is
difficult given the complexity of the analysis space.
These assumptions help to give the model traction.
The study of unsupervised grammar induction has
many merits. Most notably, it increases our under-
standing of how computers (and possibly humans)
learn in the absence of any explicit feedback. How-
ever, the gold POS tag assumption weakens any con-
clusions that can be drawn, as part-of-speech are
also a form of syntactic analysis, only shallower.
Furthermore, from a practical standpoint, it is rarely
the case that we are completely devoid of resources
for most languages. This point has been made by
1A notable exception is the work of Seginer (2007).
62
studies that transfer parsers to new languages by
projecting syntax across word alignments extracted
from parallel corpora (Hwa et al, 2005; Ganchev et
al., 2009; Smith and Eisner, 2009). Although again,
most of these studies also assume the existence of
POS tags.
In this work we present a method for creating de-
pendency parsers for languages for which no labeled
training data is available. First, we train a source
side English parser that, crucially, is delexicalized so
that its predictions rely soley on the part-of-speech
tags of the input sentence, in the same vein as Ze-
man and Resnik (2008). We empirically show that
directly transferring delexicalized models (i.e. pars-
ing a foreign language POS sequence with an En-
glish parser) already outperforms state-of-the-art un-
supervised parsers by a significant margin. This re-
sult holds in the presence of both gold POS tags as
well as automatic tags projected from English. This
emphasizes that even for languages with no syntac-
tic resources ? or possibly even parallel data ? sim-
ple transfer methods can already be more powerful
than grammar induction systems.
Next, we use this delexicalized English parser to
seed a perceptron learner for the target language.
The model is trained to update towards parses that
are in high agreement with a source side English
parse based on constraints drawn from alignments in
the parallel data. We use the augmented-loss learn-
ing procedure (Hall et al, 2011) which is closely
related to constraint driven learning (Chang et al,
2007; Chang et al, 2010). The resulting parser con-
sistently improves on the directly transferred delex-
icalized parser, reducing relative errors by 8% on
average, and as much as 18% on some languages.
Finally, we show that by transferring parsers from
multiple source languages we can further reduce er-
rors by 16% over the directly transferred English
baseline. This is consistent with previous work on
multilingual part-of-speech (Snyder et al, 2009) and
grammar (Berg-Kirkpatrick and Klein, 2010; Cohen
and Smith, 2009) induction, that shows that adding
languages leads to improvements.
We present a comprehensive set of experiments
on eight Indo-European languages for which a sig-
nificant amount of parallel data exists. We make
no language specific enhancements in our experi-
ments. We report results for sentences of all lengths,
??????????????????????????????????????????????????????????????
Figure 1: An example (unlabeled) dependency tree.
as well as with gold and automatically induced
part-of-speech tags. We also report results on sen-
tences of length 10 or less with gold part-of-speech
tags to compare with previous work. Our results
consistently outperform the previous state-of-the-art
across all languages and training configurations.
2 Preliminaries
In this paper we focus on transferring dependency
parsers between languages. A dependency parser
takes a tokenized input sentence (optionally part-of-
speech tagged) and produces a connected tree where
directed arcs represent a syntactic head-modifier re-
lationship. An example of such a tree is given in
Figure 1. Dependency tree arcs are often labeled
with the role of the syntactic relationship, e.g., is to
hearing might be labeled as SUBJECT. However, we
focus on unlabeled parsing in order to reduce prob-
lems that arise due to different treebank annotation
schemes. Of course, even for unlabeled dependen-
cies, significant variations in the annotation schemes
remain. For example, in the Danish treebank deter-
miners govern adjectives and nouns in noun phrases,
while in most other treebanks the noun is the head of
the noun phrase. Unlike previous work (Zeman and
Resnik, 2008; Smith and Eisner, 2009), we do not
apply any transformations to the treebanks, which
makes our results easier to reproduce, but systemat-
ically underestimates accuracy.
2.1 Data Sets
The treebank data in our experiments are from the
CoNLL shared-tasks on dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al, 2007). We use
English (en) only as a source language throughout
the paper. Additionally, we use the following eight
languages as both source and target languages: Dan-
ish (da), Dutch (nl), German (de), Greek (el), Italian
(it), Portuguese (pt), Spanish (es) and Swedish (sv).
For languages that were included in both the 2006
and 2007 tasks, we used the treebank from the lat-
63
ter. We focused on this subset of languages because
they are Indo-European and a significant amount of
parallel data exists for each language. By present-
ing results on eight languages our study is already
more comprehensive than most previous work in this
area. However, the restriction to Indo-European lan-
guages does make the results less conclusive when
one wishes to transfer a parser from English to Chi-
nese, for example. To account for this, we report
additional results in the discussion for non-Indo-
European languages. For all data sets we used the
predefined training and testing splits.
Our approach relies on a consistent set of part-
of-speech tags across languages and treebanks. For
this we used the universal tagset from Petrov et
al. (2011), which includes: NOUN (nouns), VERB
(verbs), ADJ (adjectives), ADV (adverbs), PRON
(pronouns), DET (determiners), ADP (prepositions
or postpositions), NUM (numerals), CONJ (conjunc-
tions), PRT (particles), PUNC (punctuation marks)
and X (a catch-all tag). Similar tagsets are used by
other studies on grammar induction and projection
(Naseem et al, 2010; Zeman and Resnik, 2008). For
all our experiments we replaced the language spe-
cific part-of-speech tags in the treebanks with these
universal tags.
Like all treebank projection studies we require a
corpus of parallel text for each pair of languages we
study. For this we used the Europarl corpus version
5 (Koehn, 2005). The corpus was preprocessed in
standard ways and word aligned by running six it-
erations of IBM Model 1 (Brown et al, 1993), fol-
lowed by six iterations of the HMM model (Vogel et
al., 1996) in both directions. We then intersect word
alignments to generate one-to-one alignments.
2.2 Parsing Model
All of our parsing models are based on the
transition-based dependency parsing paradigm
(Nivre, 2008). Specifically, all models use an
arc-eager transition strategy and are trained using
the averaged perceptron algorithm as in Zhang and
Clark (2008) with a beam size of 8. The features
used by all models are: the part-of-speech tags of
the first four words on the buffer and of the top two
words on the stack; the word identities of the first
two words on the buffer and of the top word on the
stack; the word identity of the syntactic head of
the top word on the stack (if available). All feature
conjunctions are included. For treebanks with
non-projective trees we use the pseudo-projective
parsing technique to transform the treebank into
projective structures (Nivre and Nilsson, 2005).
We focus on using this parsing system for two
reasons. First, the parser is near state-of-the-art on
English parsing benchmarks and second, and more
importantly, the parser is extremely fast to train and
run, making it easy to run a large number of exper-
iments. Preliminary experiments using a different
dependency parser ? MSTParser (McDonald et al,
2005) ? resulted in similar empirical observations.
2.3 Evaluation
All systems are evaluated using unlabeled attach-
ment score (UAS), which is the percentage of words
(ignoring punctuation tokens) in a corpus that mod-
ify the correct head (Buchholz and Marsi, 2006).
Furthermore, we evaluate with both gold-standard
part-of-speech tags, as well as predicted part-of-
speech tags from the projected part-of-speech tagger
of Das and Petrov (2011).2 This tagger relies only on
labeled training data for English, and achieves accu-
racies around 85% on the languages that we con-
sider. We evaluate in the former setting to compare
to previous studies that make this assumption. We
evaluate in the latter setting to measure performance
in a more realistic scenario ? when no target lan-
guage resources are available.
3 Transferring from English
To simplify discussion, we first focus on the most
common instantiation of parser transfer in the liter-
ature: transferring from English to other languages.
In the next section we expand our system to allow
for the inclusion of multiple source languages.
3.1 Direct Transfer
We start with the observation that discriminatively
trained dependency parsers rely heavily on part-of-
speech tagging features. For example, when train-
ing and testing a parser on our English data, a parser
with all features obtains an UAS of 89.3%3 whereas
2Available at http://code.google.com/p/pos-projection/
3The best system at CoNLL 2007 achieved 90.1% and used
a richer part-of-speech tagset (Nivre et al, 2007).
64
a delexicalized parser ? a parser that only has non-
lexical features ? obtains an UAS of 82.5%. The
key observation is that part-of-speech tags contain a
significant amount of information for unlabeled de-
pendency parsing.
This observation combined with our universal
part-of-speech tagset, leads to the idea of direct
transfer, i.e., directly parsing the target language
with the source language parser without relying on
parallel corpora. This idea has been previously ex-
plored by Zeman and Resnik (2008) and recently by
S?gaard (2011). Because we use a mapping of the
treebank specific part-of-speech tags to a common
tagset, the performance of a such a system is easy to
measure ? simply parse the target language data set
with a delexicalized parser trained on the source lan-
guage data. We conducted two experiments. In the
first, we assumed that the test set for each target lan-
guage had gold part-of-speech tags, and in the sec-
ond we used predicted part-of-speech tags from the
projection tagger of Das and Petrov (2011), which
also uses English as the source language.
UAS for all sentence lengths without punctuation
are given in Table 1. We report results for both the
English direct transfer parser (en-dir.) as well as a
baseline unsupervised grammar induction system ?
the dependency model with valence (DMV) of Klein
and Manning (2004), as obtained by the implemen-
tation of Ganchev et al (2010). We trained on sen-
tences of length 10 or less and evaluated on all sen-
tences from the test set.4 For DMV, we reversed the
direction of all dependencies if this led to higher per-
formance. From this table we can see that direct
transfer is a very strong baseline and is over 20%
absolute better than the DMV model for both gold
and predicted POS tags. Table 4, which we will dis-
cuss in more detail later, further shows that the direct
transfer parser also significantly outperforms state-
of-the-art unsupervised grammar induction models,
but in a more limited setting of sentences of length
less than 10.
Direct transfer works for a couple of reasons.
First, part-of-speech tags contain a significant
amount of information for parsing unlabeled depen-
dencies. Second, this information can be transferred,
4Training on all sentences results in slightly lower accura-
cies on average.
to some degree, across languages and treebank stan-
dards. This is because, at least for Indo-European
languages, there is some regularity in how syntax
is expressed, e.g., primarily SVO, prepositional, etc.
Even though there are some differences with respect
to relative location of certain word classes, strong
head-modifier POS tag preferences can still help re-
solve these, especially when no other viable alter-
natives are available. Consider for example an arti-
ficial sentence with a tag sequence: ?VERB NOUN
ADJ DET PUNC?. The English parser still predicts
that the NOUN and PUNC modify the VERB and the
ADJ and DET modify the NOUN, even though in the
English data such noun phrases are unlikely.5
3.2 Projected Transfer
Unlike most language transfer systems for parsers,
the direct transfer approach does not rely on project-
ing syntax across aligned parallel corpora (modulo
the fact that non-gold tags come from a system that
uses parallel corpora). In this section we describe
a simple mechanism for projecting from the direct
transfer system using large amounts of parallel data
in a similar vein to Hwa et al (2005), Ganchev et
al. (2009), Smith and Eisner (2009) inter alia. The
algorithm is based on the work of Hall et al (2011)
for training extrinsic parser objective functions and
borrows heavily from ideas in learning with weak
supervision including work on learning with con-
straints (Chang et al, 2007) and posterior regular-
ization (Ganchev et al, 2010). In our case, the
weak signals come from aligned source and target
sentences, and the agreement in their corresponding
parses, which is similar to posterior regularization
or the bilingual view of Smith and Smith (2004) and
Burkett et al (2010).
The algorithm is given in Figure 2. It starts by
labeling a set of target language sentences with a
parser, which in our case is the direct transfer parser
from the previous section (line 1). Next, it uses
these parsed target sentences to ?seed? a new parser
by training a parameter vector using the predicted
parses as a gold standard via standard perceptron
updates for J rounds (lines 3-6). This generates a
parser that emulates the direct transfer parser, but
5This requires a transition-based parser with a beam greater
than 1 to allow for ambiguity to be resolved at later stages.
65
Notation:
x: input sentence
y: dependency tree
a: alignment
w: parameter vector
?(x, y): feature vector
DP : dependency parser, i.e., DP : x? y
Input:
X = {xi}ni=1: target language sentences
P = {(xsi , xti, ai)}mi=1: aligned source-target sentences
DPdelex: delexicalized source parser
DPlex: lexicalized source parser
Algorithm:
1. Let X ? = {(xi, yi)}ni=1 where yi = DPdelex(xi)
2. w = 0
see
d-s
tag
e 3. for j : 1 . . . J
4. for xi : x1 . . . xn
5. Let y = argmaxy w ? ?(xi, y)
6. w = w + ?(xt, yi)? ?(xi, y)
pro
jec
tio
n-s
tag
e 7. for (xsi , xti, ai) : (xs1, xt1, a1) . . . (xsm, xsm, am)8. Let ys = DPlex(xsi )
9. Let Yt = {y1i , . . . , yki }, where:
yki = argmaxy/?{y1i ,...,yk?1i } w ? ?(x
t
i, y)
10. Let yt = argmaxyt?Yt ALIGN(ys, yt, ai)11. w = w + ?(xi, yt)? ?(xi, y1i )
return DP ? such that DP ?(x) = argmaxy w ? ?(x, y)
Figure 2: Perceptron-based learning algorithm for train-
ing a parser by seeding the model with a direct transfer
parser and projecting constraints across parallel corpora.
has now been lexicalized and is working in the space
of target language sentences. Next, the algorithm it-
erates over the sentences in the parallel corpus. It
parses the English sentence with an English parser
(line 8, again a lexicalized parser). It then uses the
current target language parameter vector to create
a k-best parse list for the target sentence (line 9).
From this list, it selects the parse whose dependen-
cies align most closely with the English parse via the
pre-specified alignment (line 10, also see below for
the definition of the ALIGN function). It then uses
this selected parse as a proxy to the gold standard
parse to update the parameters (line 11).
The intuition is simple. The parser starts with
non-random accuracies by emulating the direct
transfer model and slowly tries to induce better pa-
rameters by selecting parses from its k-best list
that are considered ?good? by some external met-
ric. The algorithm then updates towards that out-
put. In this case ?goodness? is determined through
the pre-specified sentence alignment and how well
the target language parse aligns with the English
parse. As a result, the model will, ideally, converge
to a state where it predicts target parses that align as
closely as possible with the corresponding English
parses. However, since we seed the learner with the
direct transfer parser, we bias the parameters to se-
lect parses that both align well and also have high
scores under the direct transfer model. This helps
to not only constrain the search space at the start
of learning, but also helps to bias dependencies be-
tween words that are not part of the alignment.
So far we have not defined the ALIGN function
that is used to score potential parses. Let a =
{(s(1), t(1)), . . . , (s(n), t(n))} be an alignment where
s(i) is a word in the source sentence xs (not nec-
essarily the ith word) and t(i) is similarly a word
in the target sentence xt (again, not necessarily the
ith word). The notation (s(i), t(i)) ? a indicates
two words are the ith aligned pair in a. We define
the ALIGN function to encode the Direct Correspon-
dence Assumption (DCA) from Hwa et al (2005):
ALIGN(ys, yt, a)
=
?
(s(i),t(i))?a
(s(j),t(j))?a
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
SCORE(ys, yt, (s(i), s(j)), (t(i), t(j)))
=
?
???
???
+1 if (s(i), s(j)) ? ys and (t(i), t(j)) ? yt
?1 if (s(i), s(j)) ? ys and (t(i), t(j)) /? yt
?1 if (s(i), s(j)) /? ys and (t(i), t(j)) ? yt
0 otherwise
The notation (i, j) ? y indicates that a dependency
from head i to modifier j is in tree y. The ALIGN
function rewards aligned head-modifier pairs and
penalizes unaligned pairs when a possible alignment
exists. For all other cases it is agnostic, i.e., when
one or both of the modifier or head are not aligned.
Figure 3 shows an example of aligned English-
Greek sentences, the English parse and a potential
Greek parse. In this case the ALIGN function re-
turns a value of 2. This is because there are three
aligned dependencies: took?book, book?the and
66
?????????????????????????????????????
?????????????????????????????????????
Figure 3: A Greek and English sentence pair. Word
alignments are shown as dashed lines, dependency arcs
as solid lines.
from?John. These add 3 to the score. There is
one incorrectly aligned dependency: the preposi-
tion mistakenly modifies the noun on the Greek side.
This subtracts 1. Finally, there are two dependencies
that do not align: the subject on the English side
and a determiner to a proper noun on the Greek side.
These do not effect the result.
The learning algorithm in Figure 2 is an instance
of augmented-loss training (Hall et al, 2011) which
is closely related to the constraint driven learning al-
gorithms of Chang et al (2007). In that work, ex-
ternal constraints on output structures are used to
help guide the learner to good parameter regions.
In our model, we use constraints drawn from paral-
lel data exactly in the same manner. Since posterior
regularization is closely related to constraint driven
learning, this makes our algorithm also similar to the
parser projection approach of Ganchev et al (2009).
There are a couple of differences. First, we bias our
model towards the direct transfer model, which is
already quite powerful. Second, our alignment con-
straints are used to select parses from a k-best list,
whereas in posterior regularization they are used as
soft constraints on full model expectations during
training. The latter is beneficial as the use of k-best
lists does not limit the class of parsers to those whose
parameters and search space decompose neatly with
the DCA loss function. An empirical comparison to
Ganchev et al (2009) is given in Section 5.
Results are given in Table 1 under the column en-
proj. For all experiments we train the seed-stage
perceptron for 5 iterations (J = 5) and we use one
hundred times as much parallel data as seed stage
non-parallel data (m = 100n). The seed-stage non-
parallel data is the training portion of each treebank,
stripped of all dependency annotations. After train-
ing the projected parser we average the parameters
gold-POS pred-POS
DMV en-dir. en-proj. DMV en-dir. en-proj.
da 33.4 45.9 48.2 18.4 44.0 45.5
de 18.0 47.2 50.9 30.3 44.7 47.4
el 39.9 63.9 66.8 21.2 63.0 65.2
es 28.5 53.3 55.8 19.9 50.2 52.4
it 43.1 57.7 60.8 37.7 53.7 56.3
nl 38.5 60.8 67.8 19.9 62.1 66.5
pt 20.1 69.2 71.3 21.0 66.2 67.7
sv 44.0 58.3 61.3 33.8 56.5 59.7
avg 33.2 57.0 60.4 25.3 55.0 57.6
Table 1: UAS for the unsupervised DMV model (DMV),
a delexicalized English direct transfer parser (en-dir.)
and a English projected parser (en-proj.). Measured on
all sentence lengths for both gold and predicted part-of-
speech tags as input.
of the model (Collins, 2002). The parsers evaluated
using predicted part-of-speech tags use the predicted
tags at both training and testing time and are thus
free of any target language specific resources.
When compared with the direct transfer model
(en-dir. in Table 1), we can see that there is an im-
provement for every single language, reducing rela-
tive error by 8% on average (57.0% to 60.4%) and
up to 18% for Dutch (60.8 to 67.8%). One could
wonder whether the true power of the projection
model comes from the re-lexicalization step ? lines
3-6 of the algorithm. However, if just this step is run,
then the average UAS only increases from 57.0%
to 57.4%, showing that most of the improvement
comes from the projection stage. Note that the re-
sults in Table 1 indicate that parsers using predicted
part-of-speech tags are only slightly worse than the
parsers using gold tags (about 2-3% absolute), show-
ing that these methods are robust to tagging errors.
4 Multi-Source Transfer
The previous section focused on transferring an En-
glish parser to a new target language. However,
there are over 20 treebanks available for a variety
of language groups including Indo-European, Altaic
(including Japanese), Semitic, and Sino-Tibetan.
Many of these are even in standardized formats
(Buchholz and Marsi, 2006; Nivre et al, 2007). Past
studies have shown that for both part-of-speech tag-
ging and grammar induction, learning with multiple
comparable languages leads to improvements (Co-
hen and Smith, 2009; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010). In this section we ex-
67
Source Training Language
da de el en es it nl pt sv
Ta
rge
tT
est
La
ng
ua
ge
da 79.2 45.2 44.0 45.9 45.0 48.6 46.1 48.1 47.8
de 34.3 83.9 53.2 47.2 45.8 53.4 55.8 55.5 46.2
el 33.3 52.5 77.5 63.9 41.6 59.3 57.3 58.6 47.5
en 34.4 37.9 45.7 82.5 28.5 38.6 43.7 42.3 43.7
es 38.1 49.4 57.3 53.3 79.7 68.4 51.2 66.7 41.4
it 44.8 56.7 66.8 57.7 64.7 79.3 57.6 69.1 50.9
nl 38.7 43.7 62.1 60.8 40.9 50.4 73.6 58.5 44.2
pt 42.5 52.0 66.6 69.2 68.5 74.7 67.1 84.6 52.1
sv 44.5 57.0 57.8 58.3 46.3 53.4 54.5 66.8 84.8
Table 2: UAS for all source-target language pairs. Each column represents which source language was used to train a
delexicalized parser and each row represents which target language test data was used. Bold numbers are when source
equals target and underlined numbers are the single best UAS for a target language. Results are for all sentence lengths
without punctuation.
amine whether this is also true for parser transfer.
Table 2 shows the matrix of source-target lan-
guage UAS for all nine languages we consider (the
original eight target languages plus English). We
can see that there is a wide range from 33.3% to
74.7%. There is also a wide range of values depend-
ing on the source training data and/or target testing
data, e.g., Portuguese as a source tends to parse tar-
get languages much better than Danish, and is also
more amenable as a target testing language. Some
of these variations are expected, e.g., the Romance
languages (Spanish, Italian and Portuguese) tend to
transfer well to one another. However, some are
unexpected, e.g., Greek being the best source lan-
guage for Dutch, as well as German being one of the
worst. This is almost certainly due to different an-
notation schemes across treebanks. Overall, Table 2
does indicate that there are possible gains in accu-
racy through the inclusion of additional languages.
In order to take advantage of treebanks in multi-
ple languages, our multi-source system simply con-
catenates the training data from all non-target lan-
guages. In other words, the multi-source direct
transfer parser for Danish will be trained by first
concatenating the training corpora of the remain-
ing eight languages, training a delexicalized parser
on this data and then directly using this parser to
analyze the Danish test data. For the multi-source
projected parser, the procedure is identical to that
in Section 3.2 except that we use the multi-source
direct transfer model to seed the algorithm instead
of the English-only direct transfer model. For these
experiments we still only use English-target parallel
data because that is the format of the readily avail-
able data in the Europarl corpus.
Table 3 presents four sets of results. The first
(best-source) is the direct transfer results for the ora-
cle single-best source language per target language.
The second (avg-source) is the mean UAS over all
source languages per target language. The third
(multi-dir.) is the multi-source direct transfer sys-
tem. The fourth and final result set (multi-proj.)
is the multi-source projected system. The resulting
parsers are typically much more accurate than the
English direct transfer system (Table 1). On aver-
age, the multi-source direct transfer system reduces
errors by 10% relative over the English-only direct
transfer system. These improvements are not consis-
tent. For Greek and Dutch we see significant losses
relative to the English-only system. An inspection of
Table 2 shows that for these two languages English
is a particularly good source training language.
For the multi-source projected system the results
are mixed. Some languages see basically no change
relative the multi-source direct transfer model, while
some languages see modest to significant increases.
But again, there is an overall trend to better mod-
els. In particular, starting with an English-only di-
rect transfer parser with 57.0% UAS on average,
by adding parallel corpora and multiple source lan-
guages we finish with parser having 63.8% UAS
on average, which is a relative reduction in error
of roughly 16% and more than doubles the perfor-
mance of a DMV model (Table 1).
Interestingly, the multi-source systems provide,
on average, accuracies near that of the single-best
source language and significantly better than the av-
erage source UAS. Thus, even this simple method of
68
best-source avg-source gold-POS pred-POS
source gold-POS gold-POS multi-dir. multi-proj. multi-dir. multi-proj.
da it 48.6 46.3 48.9 49.5 46.2 47.5
de nl 55.8 48.9 56.7 56.6 51.7 52.0
el en 63.9 51.7 60.1 65.1 58.5 63.0
es it 68.4 53.2 64.2 64.5 55.6 56.5
it pt 69.1 58.5 64.1 65.0 56.8 58.9
nl el 62.1 49.9 55.8 65.7 54.3 64.4
pt it 74.8 61.6 74.0 75.6 67.7 70.3
sv pt 66.8 54.8 65.3 68.0 58.3 62.1
avg 63.7 51.6 61.1 63.8 56.1 59.3
Table 3: UAS for multi-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best
source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the
source models for the target (excluding target language).
multi-source transfer already provides strong perfor-
mance gains. We expect that more principled tech-
niques will lead to further improvements. For exam-
ple, recent work by S?gaard (2011) explores data set
sub-sampling methods. Unlike our work, S?gaard
found that simply concatenating all the data led to
degradation in performance. Cohen et al (2011) ex-
plores the idea learning language specific mixture
coefficients for models trained independently on the
target language treebanks. However, their results
show that this method often did not significantly out-
perform uniform mixing.
5 Comparison
Comparing unsupervised and parser projection sys-
tems is difficult as many publications use non-
overlapping sets of languages or different evaluation
criteria. We compare to the following three systems
that do not augment the treebanks and report results
for some of the languages that we considered:
? USR: The weakly supervised system of
Naseem et al (2010), in which manually de-
fined universal syntactic rules (USR) are used
to constrain a probabilistic Bayesian model. In
addition to their original results, we also report
results using the same part-of-speech tagset as
the systems described in this paper (USR?).
This is useful for two reasons. First, it makes
the comparison more direct. Second, we can
generate USR results for all eight languages
and not just for the languages that they report.
? PGI: The phylogenetic grammar induction
(PGI) model of Berg-Kirkpatrick and Klein
(2010), in which the parameters of completely
unsupervised DMV models for multiple lan-
guages are coupled via a phylogenetic prior.
? PR: The posterior regularization (PR) approach
of Ganchev et al (2009), in which a supervised
English parser is used to generate constraints
that are projected using a parallel corpus and
used to regularize a target language parser. We
report results without treebank specific rules.
Table 4 gives results comparing the models pre-
sented in this work to those three systems. For this
comparison we use sentences of length 10 or less
after punctuation has been removed in order to be
consistent with reported results. The overall trends
carry over from the full treebank setting to this re-
duced sentence length setup: the projected mod-
els outperform the direct transfer models and multi-
source transfer gives higher accuracy than transfer-
ring only from English. Most previous work has as-
sumed gold part-of-speech tags, but as the code for
USR is publicly available we were able to train it
using the same projected part-of-speech tags used
in our models. These results are also given in Ta-
ble 4 under USR?. Again, we can see that the multi-
source systems (both direct and projected) signifi-
cantly outperform the unsupervised models.
It is not surprising that a parser transferred from
annotated resources does significantly better than
unsupervised systems since it has much more in-
formation from which to learn. The PR system of
Ganchev et al (2009) is similar to ours as it also
projects syntax across parallel corpora. For Span-
ish we can see that the multi-source direct trans-
fer parser is better (75.1% versus 70.6%), and this
is also true for the multi-source projected parser
69
?? gold-POS ?? ? pred-POS?
en-dir. en-proj. multi-dir. multi-proj. USR? USR PGI PR multi-dir. multi-proj. USR?
da 53.2 57.4 58.4 58.8 55.1 51.9 41.6 54.9 54.6 41.7
de 65.9 67.0 74.9 72.0 60.0 63.7 63.4 55.1
el 73.9 73.9 73.5 78.7 60.3 65.2 74.3 53.4
es 58.0 62.3 75.1 73.2 68.3 67.2 58.4 70.6 59.1 56.8 43.3
it 65.5 69.9 75.5 75.5 47.9 65.5 70.2 41.4
nl 67.6 72.2 58.8 70.7 44.0 45.1 56.3 67.2 38.8
pt 77.9 80.6 81.1 86.2 70.9 71.5 63.0 74.0 79.2 66.4
sv 70.4 71.3 76.0 77.6 52.6 58.3 72.0 73.9 59.4
avg 66.6 69.4 71.7 74.1 57.4 63.9 67.5 49.9
Table 4: UAS on sentences of length 10 or less without punctuation, comparing the systems presented in this work
to three representative systems from related work. en-dir./en-proj. are the direct/projected English parsers and multi-
dir./multi-proj. are the multi-source direct/projected parsers. Section 5 contains a description of the baseline systems.
(73.2%). Ganchev et al also report results for
Bulgarian. We trained a multi-source direct trans-
fer parser for Bulgarian which obtained a score of
72.8% versus 67.8% for the PR system. If we only
use English as a source language, as in Ganchev et
al., the English direct transfer model achieves 66.1%
on Bulgarian and 69.3% on Spanish versus 67.8%
and 70.6% for PR. In this setting the English pro-
jected model gets 72.0% on Spanish. Thus, under
identical conditions the direct transfer model obtains
accuracies comparable to PR.6
Another projection based system is that of Smith
and Eisner (2009), who report results for German
(68.5%) and Spanish (64.8%) on sentences of length
15 and less inclusive of punctuation. Smith and Eis-
ner use custom splits of the data and modify a sub-
set of the dependencies. The multi-source projected
parser obtains 71.9% for German and 67.8% for
Spanish on this setup.7 If we cherry-pick the source
language the results can improve, e.g., for Spanish
we can obtain 71.7% and 70.8% by directly transfer-
ring parsers form Italian or Portuguese respectively.
6 Discussion
One fundamental point the above experiments il-
lustrate is that even for languages for which no
resources exist, simple methods for transferring
parsers work remarkably well. In particular, if
6Note that the last set of results was obtained by using the
same English training data as Ganchev et al Using the CoNLL
2007 English data set for training, the English direct transfer
model is 63.2% for Bulgarian and 58.0% for Spanish versus
67.8% and 70.6% for PR, highlighting the large impact that dif-
ference treebank annotation standards can have.
7Data sets and evaluation criteria obtained via communica-
tions with David Smith and Jason Eisner.
one can transfer part-of-speech tags, then a large
part of transferring unlabeled dependencies has been
solved. This observation should lead to a new base-
line in unsupervised and projected grammar induc-
tion ? the UAS of a delexicalized English parser.
Of course, our experiments focus strictly on Indo-
European languages. Preliminary experiments for
Arabic (ar), Chinese (zh), and Japanese (ja) suggest
similar direct transfer methods are applicable. For
example, on the CoNLL test sets, a DMV model
obtains UAS of 28.7/41.8/34.6% for ar/zh/ja re-
spectively, whereas an English direct transfer parser
obtains 32.1/53.8/32.2% and a multi-source direct
transfer parser obtains 39.9/41.7/43.3%. In this
setting only Indo-European languages are used as
source data. Thus, even across language groups di-
rect transfer is a reasonable baseline. However, this
is not necessary as treebanks are available for a num-
ber of language groups, e.g., Indo-European, Altaic,
Semitic, and Sino-Tibetan.
The second fundamental observation is that when
available, multiple sources should be used. Even
through naive multi-source methods (concatenating
data), it is possible to build a system that has compa-
rable accuracy to the single-best source for all lan-
guages. This advantage does not come simply from
having more data. In fact, if we randomly sam-
pled from the multi-source data until the training set
size was equivalent to the size of the English data,
then the results still hold (and in fact go up slightly
for some languages). This suggests that even bet-
ter transfer models can be produced by separately
weighting each of the sources depending on the tar-
get language ? either weighting by hand, if we know
the language group of the target language, or auto-
70
matically, if we do not. As previously mentioned,
the latter has been explored in both S?gaard (2011)
and Cohen et al (2011).
7 Conclusions
We presented a simple, yet effective approach
for projecting parsers from languages with labeled
training data to languages without any labeled train-
ing data. Central to our approach is the idea of
delexicalizing the models, which combined with a
standardized part-of-speech tagset alows us to di-
rectly transfer models between languages. We then
use a constraint driven learning algorithm to adapt
the transferred parsers to the respective target lan-
guage, obtaining an additional 16% error reduc-
tion on average in a multi-source setting. Our final
parsers achieve state-of-the-art accuracies on eight
Indo-European languages, significantly outperform-
ing previous unsupervised and projected systems.
Acknowledgements: We would like to thank Kuz-
man Ganchev, Valentin Spitkovsky and Dipanjan
Das for numerous discussions on this topic and com-
ments on earlier drafts of this paper. We would
also like to thank Shay Cohen, Dipanjan Das, Noah
Smith and Anders S?gaard for sharing early drafts
of their recent related work.
References
T. Berg-Kirkpatrick and D. Klein. 2010. Phylogenetic
grammar induction. In Proc. of ACL.
P. Blunsom and T. Cohn. 2010. Unsupervised induction
of tree substitution grammars for dependency parsing.
Proc. of EMNLP.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
Learning better monolingual models with unannotated
bilingual text. In Proc. of CoNLL.
G. Carroll and E. Charniak. 1992. Two experiments on
learning probabilistic dependency grammars from cor-
pora. In Proc. of the Working Notes of the Workshop
Statistically-Based NLP Techniques.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of NAACL.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. of ACL.
S.B. Cohen and N.A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In Proc. of NAACL.
S.B. Cohen, D. Das, and N.A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proc. of EMNLP.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. of ACL.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Proc. of ACL-HLT.
K. Ganchev, J. Gillenwater, and B. Taskar. 2009. De-
pendency grammar induction via bitext projection con-
straints. In Proc. of ACL-IJCNLP.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc of EMNLP.
K. Hall, R. McDonald, J. Katz-Brown, and M. Ringgaard.
2011. Training dependency parsers by jointly optimiz-
ing multiple objectives. In Proc. of EMNLP.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Natural Language Engineer-
ing, 11(03):311?325.
D. Klein and C. D. Manning. 2004. Corpus-based induc-
tion of syntactic structure: models of dependency and
constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
71
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Naseem, H. Chen, R. Barzilay, and M. Johnson. 2010.
Using universal linguistic knowledge to guide gram-
mar induction. In Proc. of EMNLP.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. of ACL.
J. Nivre, J. Hall, and J. Nilsson. 2006. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of LREC.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In Proc. of ACL.
S. Petrov, P. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In EMNLP ?10.
S. Petrov, D. Das, and R. McDonald. 2011. A universal
part-of-speech tagset. In ArXiv:1104.2086.
Y. Seginer. 2007. Fast unsupervised incremental parsing.
In Proc. of ACL.
L. Shen and A.K. Joshi. 2008. Ltag dependency parsing
with bidirectional incremental construction. In Proc.
of EMNLP.
N.A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
D.A. Smith and J. Eisner. 2009. Parser adaptation and
projection with quasi-synchronous grammar features.
In Proc. of EMNLP.
D.A. Smith and N.A. Smith. 2004. Bilingual parsing
with factored estimation: Using english to parse ko-
rean. In Proc. of EMNLP.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2009. Adding more languages improves unsupervised
multilingual part-of-speech tagging: A Bayesian non-
parametric approach. In Proc. of NAACL.
A. S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In Proc.
ACL.
V.I. Spitkovsky, H. Alshawi, and D. Jurafsky. 2010.
From baby steps to leapfrog: How ?less is more? in un-
supervised dependency parsing. In Proc. of NAACL-
HLT.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
W. Wang and M. P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proc. of
the Workshop on Incremental Parsing: Bringing Engi-
neering and Cognition Together.
D. Zeman and P. Resnik. 2008. Cross-language parser
adaptation between related languages. In NLP for Less
Privileged Languages.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
72
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1489?1499,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Training dependency parsers by jointly optimizing multiple objectives
Keith Hall Ryan McDonald Jason Katz-Brown Michael Ringgaard
Google Research
{kbhall|ryanmcd|jasonkb|ringgaard}@google.com
Abstract
We present an online learning algorithm for
training parsers which allows for the inclusion
of multiple objective functions. The primary
example is the extension of a standard su-
pervised parsing objective function with addi-
tional loss-functions, either based on intrinsic
parsing quality or task-specific extrinsic mea-
sures of quality. Our empirical results show
how this approach performs for two depen-
dency parsing algorithms (graph-based and
transition-based parsing) and how it achieves
increased performance on multiple target tasks
including reordering for machine translation
and parser adaptation.
1 Introduction
The accuracy and speed of state-of-the-art depen-
dency parsers has motivated a resumed interest in
utilizing the output of parsing as an input to many
downstream natural language processing tasks. This
includes work on question answering (Wang et al,
2007), sentiment analysis (Nakagawa et al, 2010),
MT reordering (Xu et al, 2009), and many other
tasks. In most cases, the accuracy of parsers de-
grades when run on out-of-domain data (Gildea,
2001; McClosky et al, 2006; Blitzer et al, 2006;
Petrov et al, 2010). But these accuracies are mea-
sured with respect to gold-standard out-of-domain
parse trees. There are few tasks that actually depend
on the complete parse tree. Furthermore, when eval-
uated on a downstream task, often the optimal parse
output has a model score lower than the best parse
as predicted by the parsing model. While this means
that we are not properly modeling the downstream
task in the parsers, it also means that there is some
information from small task or domain-specific data
sets which could help direct our search for optimal
parameters during parser training. The goal being
not necessarily to obtain better parse performance,
but to exploit the structure induced from human la-
beled treebank data while targeting specific extrinsic
metrics of quality, which can include task specific
metrics or external weak constraints on the parse
structure.
One obvious approach to this problem is to em-
ploy parser reranking (Collins, 2000). In such a
setting, an auxiliary reranker is added in a pipeline
following the parser. The standard setting involves
training the base parser and applying it to a devel-
opment set (this is often done in a cross-validated
jack-knife training framework). The reranker can
then be trained to optimize for the downstream or
extrinsic objective. While this will bias the reranker
towards the target task, it is limited by the oracle
performance of the original base parser.
In this paper, we propose a training algorithm for
statistical dependency parsers (Ku?bler et al, 2009)
in which a single model is jointly optimized for a
regular supervised training objective over the tree-
bank data as well as a task-specific objective ? or
more generally an extrinsic objective ? on an ad-
ditional data set. The case where there are both
gold-standard trees and a task-specific objective for
the entire training set is a specific instance of the
larger problem that we address here. Specifically,
the algorithm takes the form of an online learner
where a training instance is selected and the param-
1489
eters are optimized based on the objective function
associated with the instance (either intrinsic or ex-
trinsic), thus jointly optimizing multiple objectives.
An update schedule trades-off the relative impor-
tance of each objective function. We call our algo-
rithm augmented-loss training as it optimizes mul-
tiple losses to augment the traditional supervised
parser loss.
There have been a number of efforts to exploit
weak or external signals of quality to train better pre-
diction models. This includes work on generalized
expectation (Mann and McCallum, 2010), posterior
regularization (Ganchev et al, 2010) and constraint
driven learning (Chang et al, 2007; Chang et al,
2010). The work of Chang et al (2007) on constraint
driven learning is perhaps the closest to our frame-
work and we draw connections to it in Section 5.
In these studies the typical goal is to use the weak
signal to improve the structured prediction models
on the intrinsic evaluation metrics. For our setting
this would mean using weak application specific sig-
nals to improve dependency parsing. Though we
explore such ideas in our experiments, in particular
for semi-supervised domain adaptation, we are pri-
marily interested in the case where the weak signal
is precisely what we wish to optimize, but also de-
sire the benefit from using both data with annotated
parse structures and data specific to the task at hand
to guide parser training.
In Section 2 we outline the augmented-loss algo-
rithm and provide a convergence analysis. In Sec-
tion 3 and 4 we present a set of experiments defin-
ing diffent augmented losses covering a task-specific
extrinsic loss (MT reordering), a domain adapta-
tion loss, and an alternate intrinsic parser loss. In
all cases we show the augmented-loss framework
can lead to significant gains in performance. In
Section 5 we tie our augmented-loss algorithm to
other frameworks for encoding auxiliary informa-
tion and/or joint objective optimization.
2 Methodology
We present the augmented-loss algorithm in the con-
text of the structured perceptron. The structured
perceptron (Algorithm 1) is an on-line learning al-
gorithm which takes as input: 1) a set of training
examples di = (xi, yi) consisting of an input sen-
Algorithm 1 Structured Perceptron
{Input data sets: D = {d1 = (x1, y1) . . . dN = (xN , yN )}}
{Input 0/1 loss: L(F?(x), y) = [F?(x) 6= y ? 1 : 0]}
{Let: F?(x) = arg maxy?Y ? ? ?(y)}
{Initialize model parameters: ? = ~0}
repeat
for i = 1 . . . N do
{Compute structured loss}
y?i = F?(xi)
if L(y?i, yi) > 0 then
{Update model Parameters}
? = ? + ?(yi)? ?(y?i)
end if
end for
until converged
{Return model ?}
tence xi and an output yi; and 2) a loss-function,
L(y?, y), that measures the cost of predicting out-
put y? relative to the gold standard y and is usu-
ally the 0/1 loss (Collins, 2002). For dependency
parser training, this set-up consists of input sen-
tences x and the corresponding gold dependency
tree y ? Yx, where Yx is the space of possible
parse trees for sentence x. In the perceptron setting,
F?(x) = arg maxy?Yx ? ??(y) where ? is mappingfrom a parse tree y for sentence x to a high dimen-
sional feature space. Learning proceeds by predict-
ing a structured output given the current model, and
if that structure is incorrect, updating the model: re-
warding features that fire in the gold-standard ?(yi),
and discounting features that fire in the predicted
output, ?(y?i).
The structured perceptron, as given in Algo-
rithm 1, only updates when there is a positive loss,
meaning that there was a prediction mistake. For
the moment we will abstract away from details such
as the precise definition of F (x) and ?(y). We
will show in the next section that our augmented-
loss method is general and can be applied to any de-
pendency parsing framework that can be trained by
the perceptron algorithm, such as transition-based
parsers (Nivre, 2008; Zhang and Clark, 2008) and
graph-based parsers (McDonald et al, 2005).
2.1 Augmented-Loss Training
The augmented-loss training algorithm that we pro-
pose is based on the structured perceptron; however,
the augmented-loss training framework is a general
1490
mechanism to incorporate multiple loss functions in
online learner training. Algorithm 2 is the pseudo-
code for the augmented-loss structured perceptron
algorithm. The algorithm is an extension to Algo-
rithm 1 where there are 1) multiple loss functions
being evaluated L1, . . . , LM ; 2) there are multiple
datasets associated with each of these loss functions
D1, . . . ,DM ; and 3) there is a schedule for pro-
cessing examples from each of these datasets, where
Sched(j, i) is true if the jth loss function should be
updated on the ith iteration of training. Note that
for data point dji = (x, y), which is the ith training
instance of the jth data set, that y does not neces-
sarily have to be a dependency tree. It can either
be a task-specific output of interest, a partial tree, or
even null, in the case where learning will be guided
strictly by the loss Lj . The training algorithm is ef-
fectively the same as the perceptron, the primary dif-
ference is that if Lj is an extrinsic loss, we cannot
compute the standard updates since we do not nec-
essarily know the correct parse (the line indicated by
?). Section 2.2 shows one method for updating the
parser parameters for extrinsic losses.
In the experiments in this paper, we only consider
the case where there are two loss functions: a super-
vised dependency parsing labeled-attachment loss;
and an additional loss, examples of which are pre-
sented in Section 3.
2.2 Inline Ranker Training
In order to make Algorithm 2 more concrete, we
need a way of defining the loss and resulting pa-
rameter updates for the case when Lj is not a stan-
dard supervised parsing loss (? from Algorithm 2).
Assume that we have a cost function C(xi, y?, yi)
which, given a training example (xi, yi) will give a
score for a parse y? ? Yxi relative to some output
yi. While we can compute the score for any parse,
we are unable to determine the features associated
with the optimal parse, as yi need not be a parse
tree. For example, consider a machine translation re-
ordering system which uses the parse y? to reorder the
words of xi, the optimal reordering being yi. Then
C(xi, y?, yi) is a reordering cost which is large if the
predicted parse induces a poor reordering of xi.
We propose a general purpose loss function which
is based on parser k-best lists. The inline reranker
uses the currently trained parser model ? to parse
Algorithm 2 Augmented-Loss Perceptron
{Input data sets}:
D1 = {d11 = (x11, y11) . . . d1N1 = (x1N1 , y1N1)},
. . .
DM = {dM1 = (xM1 , yM1 ) . . . dMNM = (xMNM , yMNM )}
{Input loss functions: L1 . . . LM}
{Initialize indexes: c1 . . . cM = ~0}
{Initialize model parameters: ? = ~0}
i = 0
repeat
for j = 1 . . .M do
{Check whether to update Lj on iteration i}
if Sched(j, i) then
{Compute index of instance ? reset if cj ? N j}
cj = [(cj ? N j) ? 0 : cj + 1]
{Compute structured loss for instance}
if Lj is intrinsic loss then
y? = F?(xjcj )
if Lj(y?, yjcj ) > 0 then
? = ? + ?(yjcj )? ?(y?) {yjcj is a tree}end if
else if Lj is an extrinsic loss then
{See Section 2.2}?
end if
end if
end for
i = i+ 1
until converged
{Return model ?}
the external input, producing a k-best set of parses:
Fk-best? (xi) = {y?1, . . . , y?k}. We can compute the
cost function C(xi, y?, yi) for all y? ? Fk-best? (xi). If
the 1-best parse, y?1, has the lowest cost, then there is
no lower cost parse in this k-best list. Otherwise, the
lowest-cost parse in Fk-best? (xi) is taken to be the
correct output structure yi, and the 1-best parse is
taken to be an incorrect prediction. We can achieve
this by substituting the following into Algorithm 2
at line ?.
Algorithm 3 Reranker Loss
{y?1, . . . , y?k} = Fk-best? (xi)
? = min? C(xjcj , y?? , yjcj ) {? is min const index}
Lj(y?1, yjcj ) = C(xjcj , y?1, yjcj )? C(xjcj , y?? , yjcj )
if Lj(y?1, yjcj ) > 0 then
? = ? + ?(y?? )? ?(y?1)
end if
Again the algorithm only updates when there is
an error ? when the 1-best output has a higher cost
than any other output in the k-best list ? resulting
1491
in positive Lj . The intuition behind this method is
that in the presence of only a cost function and a
k-best list, the parameters will be updated towards
the parse structure that has the lowest cost, which
over time will move the parameters of the model to
a place with low extrinsic loss.
We exploit this formulation of the general-
purpose augmented-loss function as it allows one to
include any extrinsic cost function which is depen-
dent of parses. The scoring function used does not
need to be factored, requiring no internal knowledge
of the function itself. Furthermore, we can apply this
to any parsing algorithm which can generate k-best
lists. For each parse, we must retain the features
associated with the parse (e.g., for transition-based
parsing, the features associated with the transition
sequence resulting in the parse).
There are two significant differences from the in-
line reranker loss function and standard reranker
training. First, we are performing this decision per
example as each data item is processed (this is done
in the inner loop of the Algorithm 2). Second, the
feedback function for selecting a parse is based on
an external objective function. The second point is
actually true for many minimum-error-rate training
scenarios, but in those settings the model is updated
as a post-processing stage (after the base-model is
trained).
2.3 Convergence of Inline Ranker Training
A training setD is loss-separable with margin ? > 0
if there exists a vector u with ?u? = 1 such that
for all y?, y?? ? Yx and (x, y) ? D, if L(y?, y) <
L(y??, y), then u??(y?)?u??(y??) ? ?. Furthermore,
let R ? ||?(y)? ?(y?)||, for all y, y?.
Assumption 1. Assume training set D is loss-
separable with margin ?.
Theorem 1. Given Assumption 1. Letm be the num-
ber of mistakes made when training the perceptron
(Algorithm 2) with inline ranker loss (Algorithm 3)
on D, where a mistake occurs for (x, y) ? D with
parameter vector ? when ?y?j ? F k-best? (x) where
y?j 6= y?1 and L(y?j , y) < L(y?1, y). If training is run
indefinitely, then m ? R2?2 .
Proof. Identical to the standard perceptron proof,
e.g., Collins (2002), by inserting in loss-separability
for normal separability.
Like the original perceptron theorem, this implies
that the algorithm will converge. However, unlike
the original theorem, it does not imply that it will
converge to a parameter vector ? such that for all
(x, y) ? D, if y? = arg maxy? ? ??(y?) then L(y?, y) =
0. Even if we assume for every x there exists an out-
put with zero loss, Theorem 1 still makes no guar-
antees. Consider a training set with one instance
(x, y). Now, set k = 2 for the k-best output list and
let y?1, y?2, and y?3 be the top-3 scoring outputs and
let L(y?1, y) = 1, L(y?2, y) = 2 and L(y?3, y) = 0.
In this case, no updates will ever be made and y?1
will remain unchanged even though it doesn?t have
minimal loss. Consider the following assumption:
Assumption 2. For any parameter vector ? that ex-
ists during training, either 1) for all (x, y) ? D,
L(y?1, y) = 0 (or some optimal minimum loss),
or 2) there exists at least one (x, y) ? D where
?y?j ? F k-best? (x) such that L(y?j , y) < L(y?1, y).
Assumption 2 states that for any ? that exists
during training, but before convergence, there is at
least one example in the training data where k is
large enough to include one output with a lower loss
when y?1 does not have the optimal minimal loss. If
k = ?, then this is the standard perceptron as it
guarantees the optimal loss output to be in the k-best
list. But we are assuming something much weaker
here, i.e., not that the k-best list will include the min-
imal loss output, only a single output with a lower
loss than the current best guess. However, it is strong
enough to show the following:
Theorem 2. Given Assumption 1 and Assumption 2.
Training the perceptron (Algorithm 2) with inline
ranker loss (Algorithm 3) on D 1) converges in fi-
nite time, and 2) produces parameters ? such that
for all (x, y) ? D, if y? = arg maxy? ? ? ?(y?) then
L(y?, y) = 0 (or equivalent minimal loss).
Proof. It must be the case for all (x, y) ? D that
L(y?1, y) = 0 (and y?1 is the argmax) after a finite
amount of time. Otherwise, by Assumption 2, there
exists some x, such that when it is next processed,
there would exist an output in the k-best list that
had a lower loss, which will result in an additional
mistake. Theorem 1 guarantees that this can not
continue indefinitely as the number of mistakes is
bounded.
1492
Thus, the perceptron algorithm will converge to
optimal minimal loss under the assumption that k
is large enough so that the model can keep improv-
ing. Note that this does not mean k must be large
enough to include a zero or minimum loss output,
just large enough to include a better output than
the current best hypothesis. Theorem 2, when cou-
pled with Theorem 1, implies that augmented-loss
learning will make at most R2/?2 mistakes at train-
ing, but does not guarantee the rate at which these
mistakes will be made, only that convergence is fi-
nite, providing that the scheduling time (defined by
Sched()) between seeing the same instance is always
finite, which is always true in our experiments.
This analysis does not assume anything about the
loss L. Every instance (x, y) can use a different loss.
It is only required that the loss for a specific input-
output pair is fixed throughout training. Thus, the
above analysis covers the case where some training
instances use an extrinsic loss and others an intrin-
sic parsing loss. This also suggests more efficient
training methods when extracting the k-best list is
prohibitive. One can parse with k = 2, 4, 8, 16, . . .
until an k is reached that includes a lower loss parse.
It may be the case that for most instances a small
k is required, but the algorithm is doing more work
unnecessarily if k is large.
3 Experimental Set-up
3.1 Dependency Parsers
The augmented-loss framework we present is gen-
eral in the sense that it can be combined with any
loss function and any parser, provided the parser can
be parameterized as a linear classifier, trained with
the perceptron and is capable of producing a k-best
list of trees. For our experiments we focus on two
dependency parsers.
? Transition-based: An implementation of the
transition-based dependency parsing frame-
work (Nivre, 2008) using an arc-eager transi-
tion strategy and are trained using the percep-
tron algorithm as in Zhang and Clark (2008)
with a beam size of 8. Beams with varying
sizes can be used to produce k-best lists. The
features used by all models are: the part-of-
speech tags of the first four words on the buffer
and of the top two words on the stack; the word
identities of the first two words on the buffer
and of the top word on the stack; the word iden-
tity of the syntactic head of the top word on the
stack (if available); dependency arc label iden-
tities for the top word on the stack, the left and
rightmost modifier of the top word on the stack,
and the left most modifier of the first word in
the buffer (if available). All feature conjunc-
tions are included.
? Graph-based: An implementation of graph-
based parsing algorithms with an arc-factored
parameterization (McDonald et al, 2005). We
use the non-projective k-best MST algorithm to
generate k-best lists (Hall, 2007), where k = 8
for the experiments in this paper. The graph-
based parser features used in the experiments
in this paper are defined over a word, wi at po-
sition i; the head of this word w?(i) where ?(i)
provides the index of the head word; and part-
of-speech tags of these words ti. We use the
following set of features similar to McDonald
et al (2005):
isolated features: wi, ti, w?(i), t?(i)
word-tag pairs: (wi, ti); (w?(i), t?(i))
word-head pairs: (wi, w?(i)), (ti, t?(i))
word-head-tag triples: (t?(i), wi, ti)
(w?(i), wi, ti)
(w?(i), t?(i), ti)
(w?(i), t?(i), wi)
tag-neighbourhood: (t?(i), t?(i)+1, ti?1, ti)
(t?(i), t?(i)+1, ti+1, ti)
(t?(i), t?(i)?1, ti?1, ti)
(t?(i), t?(i)?1, ti+1, ti)
between features: ?j i < j < ?(i) || ?(i) < j < i
(t?(i), tj , ti)
arc-direction/length : (i? ?(i) > 0, |i? ?(i)|)
3.2 Data and Tasks
In the next section, we present a set of scoring func-
tions that can be used in the inline reranker loss
framework, resulting in a new augmented-loss for
each one. Augmented-loss learning is then applied
to target a downstream task using the loss functions
to measure gains. We show empirical results for two
extrinsic loss-functions (optimizing for the down-
stream task): machine translation and domain adap-
tation; and for one intrinsic loss-function: an arc-
length parsing score. For some experiments we also
1493
measure the standard intrinsic parser metrics unla-
beled attachment score (UAS) and labeled attach-
ment score (LAS) (Buchholz and Marsi, 2006).
In terms of treebank data, the primary training
corpus is the Penn Wall Street Journal Treebank
(PTB) (Marcus et al, 1993). We also make use
of the Brown corpus, and the Question Treebank
(QTB) (Judge et al, 2006). For PTB and Brown
we use standard training/development/testing splits
of the data. For the QTB we split the data into
three sections: 2000 training, 1000 development,
and 1000 test. All treebanks are converted to de-
pendency format using the Stanford converter v1.6
(de Marneffe et al, 2006).
4 Experiments
4.1 Machine Translation Reordering Score
As alluded to in Section 2.2, we use a reordering-
based loss function to improve word order in a ma-
chine translation system. In particular, we use a sys-
tem of source-side reordering rules which, given a
parse of the source sentence, will reorder the sen-
tence into a target-side order (Collins et al, 2005).
In our experiments we work with a set of English-
Japanese reordering rules1 and gold reorderings
based on human generated correct reordering of an
aligned target sentences. We use a reordering score
based on the reordering penalty from the METEOR
scoring metric. Though we could have used a fur-
ther downstream measure like BLEU, METEOR has
also been shown to directly correlate with translation
quality (Banerjee and Lavie, 2005) and is simpler to
measure.
reorder-score = 1? # chunks? 1# unigrams matched? 1
reorder-cost = 1? reorder-score
All reordering augmented-loss experiments are
run with the same treebank data as the baseline
(the training portions of PTB, Brown, and QTB).
The extrinsic reordering training data consists of
10930 examples of English sentences and their cor-
rect Japanese word-order. We evaluate our results on
an evaluation set of 6338 examples of similarly cre-
ated reordering data. The reordering cost, evaluation
1Our rules are similar to those from Xu et al (2009).
Exact Reorder
trans?PTB + Brown + QTB 35.29 76.49
trans?0.5?aug.-loss 38.71 78.19
trans?1.0?aug.-loss 39.02 78.39
trans?2.0?aug.-loss 39.58 78.67
graph?PTB + Brown + QTB 25.71 69.84
graph?0.5? aug.-loss 28.99 72.23
graph?1.0?aug.-loss 29.99 72.88
graph?2.0?aug.-loss 30.03 73.15
Table 1: Reordering scores for parser-based reordering
(English-to-Japanese). Exact is the number of correctly
reordered sentences. All models use the same treebank-
data (PTB, QTB, and the Brown corpus). Results for
three augmented-loss schedules are shown: 0.5 where for
every two treebank updates we make one augmented-loss
update, 1 is a 1-to-1 mix, and 2 is where we make twice
as many augmented-loss updates as treebank updates.
criteria and data used in our experiments are based
on the work of Talbot et al (2011).
Table 1 shows the results of using the reordering
cost as an augmented-loss to the standard treebank
objective function. Results are presented as mea-
sured by the reordering score as well as a coarse
exact-match score (the number of sentences which
would have correct word-order given the parse and
the fixed reordering rules). We see continued im-
provements as we adjust the schedule to process the
extrinsic loss more frequently, the best result being
when we make two augmented-loss updates for ev-
ery one treebank-based loss update.
4.2 Semi-supervised domain adaptation
Another application of the augmented-loss frame-
work is to improve parser domain portability in the
presence of partially labeled data. Consider, for ex-
ample, the case of questions. Petrov et al (2010)
observed that dependency parsers tend to do quite
poorly when parsing questions due to their lim-
ited exposure to them in the news corpora from
the PennTreebank. Table 2 shows the accuracy
of two parsers (LAS, UAS and the F1 of the root
dependency attachment) on the QuestionBank test
data. The first is a parser trained on the standard
training sections of the PennTreebank (PTB) and
the second is a parser trained on the training por-
tion of the QuestionBank (QTB). Results for both
1494
LAS UAS Root-F1
trans?PTB 67.97 73.52 47.60
trans?QTB 84.59 89.59 91.06
trans?aug.-loss 76.27 86.42 83.41
graph?PTB 65.27 72.72 43.10
graph?QTB 82.73 87.44 91.58
graph?aug.-loss 72.82 80.68 86.26
Table 2: Domain adaptation results. Table shows (for
both transition and graph-based parsers) the labeled ac-
curacy score (LAS), unlabeled accuracy score (UAS)
and Root-F1 for parsers trained on the PTB and QTB
and tested on the QTB. The augmented-loss parsers are
trained on the PTB but with a partial tree loss on QTB
that considers only root dependencies.
transition-based parsers and graph-based parsers are
given. Clearly there is significant drop in accu-
racy for a parser trained on the PTB. For example,
the transition-based PTB parser achieves a LAS of
67.97% relative to 84.59% for the parser trained on
the QTB.
We consider the situation where it is possible to
ask annotators a single question about the target do-
main that is relatively easy to answer. The question
should be posed so that the resulting answer pro-
duces a partially labeled dependency tree. Root-F1
scores from Table 2 suggest that one simple ques-
tion is ?what is the main verb of this sentence?? for
sentences that are questions. In most cases this task
is straight-forward and will result in a single depen-
dency, that from the root to the main verb of the sen-
tence. We feel this is a realistic partial labeled train-
ing setting where it would be possible to quickly col-
lect a significant amount of data.
To test whether such weak information can signif-
icantly improve the parsing of questions, we trained
an augmented-loss parser using the training set of
the QTB stripped of all dependencies except the de-
pendency from the root to the main verb of the sen-
tence. In other words, for each sentence, the parser
may only observe a single dependency at training
from the QTB ? the dependency to the main verb.
Our augmented-loss function in this case is a simple
binary function: 0 if a parse has the correct root de-
pendency and 1 if it does not. Thus, the algorithm
will select the first parse in the k-best list that has the
correct root as the proxy to a gold standard parse.2
The last row in each section of Table 2 shows the
results for this augmented-loss system when weight-
ing both losses equally during training. By simply
having the main verb annotated in each sentence ?
the sentences from the training portion of the QTB
? the parser can eliminate half of the errors of the
original parser. This is reflected by both the Root-
F1 as well as LAS/UAS. It is important to point out
that these improvements are not limited to simply
better root predictions. Due to the fact that parsing
algorithms make many parsing decisions jointly at
test time, all such decisions influence each other and
improvements are seen across the board. For exam-
ple, the transition-based PTB parser has an F1 score
of 41.22% for verb subjects (nsubj), whereas the
augmented-loss parser has an F1 of 73.52%. Clearly
improving just a single (and simple to annotate) de-
pendency leads to general parser improvements.
4.3 Average Arc Length Score
The augmented-loss framework can be used to in-
corporate multiple treebank-based loss functions as
well. Labeled attachment score is used as our base
model loss function. In this set of experiments we
consider adding an additional loss function which
weights the lengths of correct and incorrect arcs, the
average (labeled) arc-length score:
ALS =
?
i ?(??i, ?i)(i? ?i)?
i(i? ?i)
For each word of the sentence we compute the dis-
tance between the word?s position i and the posi-
tion of the words head ?i. The arc-length score is
the summed length of all those with correct head as-
signments (?(??i, ?i) is 1 if the predicted head and
the correct head match, 0 otherwise). The score is
normalized by the summed arc lengths for the sen-
tence. The labeled version of this score requires that
the labels of the arc are also correct. Optimizing
for dependency arc length is particularly important
as parsers tend to do worse on longer dependencies
(McDonald and Nivre, 2007) and these dependen-
cies are typically the most meaningful for down-
stream tasks, e.g., main verb dependencies for tasks
2For the graph-based parser one can also find the higest scor-
ing tree with correct root by setting the score of all competing
arcs to ??.
1495
LAS UAS ALS
trans?PTB 88.64 91.64 82.96
trans?unlabeled aug.-loss 88.74 91.91 83.65
trans?labeled aug.-loss 88.84 91.91 83.46
graph?PTB 85.75 88.70 73.88
graph?unlabeled aug.-loss 85.80 88.81 74.26
graph?labeled aug.-loss 85.85 88.93 74.40
Table 3: Results for both parsers on the development set
of the PTB. When training with ALS (labeled and unla-
beled), we see an improvement in UAS, LAS, and ALS.
Furthermore, if we use a labeled-ALS as the metric for
augmented-loss training, we also see a considerable in-
crease in LAS.
like information extraction (Yates and Etzioni, 2009)
and textual entailment (Berant et al, 2010).
In Table 3 we show results for parsing with the
ALS augmented-loss objective. For each parser, we
consider two different ALS objective functions; one
based on unlabeled-ALS and the other on labeled-
ALS. The arc-length score penalizes incorrect long-
distance dependencies more than local dependen-
cies; long-distance dependencies are often more de-
structive in preserving sentence meaning and can be
more difficult to predict correctly due to the larger
context on which they depend. Combining this with
the standard attachment scores biases training to fo-
cus on the difficult head dependencies.
For both experiments we see that by adding the
ALS augmented-loss we achieve an improvement in
LAS and UAS in addition to ALS. The augmented-
loss not only helps us improve on the longer depen-
dencies (as reflected in the increased ALS), but also
in the main parser objective function of LAS and
UAS. Using the labeled loss function provides better
reinforcement as can be seen in the improvements
over the unlabeled loss-function. As with all experi-
ments in this paper, the graph-based parser baselines
are much lower than the transition-based parser due
to the use of arc-factored features. In these experi-
ments we used an inline-ranker loss with 8 parses.
We experimented with larger sizes (16 and 64) and
found very similar improvements: for example, the
transition parser?s LAS for the labeled loss is 88.68
and 88.84, respectively).
We note that ALS can be decomposed locally and
could be used as the primary objective function for
parsing. A parse with perfect scores under ALS
and LAS will match the gold-standard training tree.
However, if we were to order incorrect parses of a
sentence, ALS and LAS will suggest different order-
ings. Our results show that by optimizing for losses
based on a combination of these metrics we train a
more robust parsing model.
5 Related Work
A recent study by Katz-Brown et al (2011) also in-
vestigates the task of training parsers to improve MT
reordering. In that work, a parser is used to first
parse a set of manually reordered sentences to pro-
duce k-best lists. The parse with the best reordering
score is then fixed and added back to the training set
and a new parser is trained on resulting data. The
method is called targeted self-training as it is simi-
lar in vein to self-training (McClosky et al, 2006),
with the exception that the new parse data is targeted
to produce accurate word reorderings. Our method
differs as it does not statically fix a new parse, but
dynamically updates the parameters and parse selec-
tion by incorporating the additional loss in the inner
loop of online learning. This allows us to give guar-
antees of convergence. Furthermore, we also evalu-
ate the method on alternate extrinsic loss functions.
Liang et al (2006) presented a perceptron-based
algorithm for learning the phrase-translation param-
eters in a statistical machine translation system.
Similar to the inline-ranker loss function presented
here, they use a k-best lists of hypotheses in order to
identify parameters which can improve a global ob-
jective function: BLEU score. In their work, they
are interested in learning a parameterization over
translation phrases (including the underlying word-
alignment) which optimizes the BLEU score. Their
goal is considerably different; they want to incor-
porate additional features into their model and de-
fine an objective function which allows them to do
so; whereas, we are interested in allowing for mul-
tiple objective functions in order to adapt the parser
model parameters to downstream tasks or alternative
intrinsic (parsing) objectives.
The work that is most similar to ours is that
of Chang et al (2007), who introduced the Con-
straint Driven Learning algorithm (CODL). Their al-
gorithm specifically optimizes a loss function with
1496
the addition of constraints based on unlabeled data
(what we call extrinsic datasets). For each unla-
beled example, they use the current model along
with their set of constraints to select a set of k au-
tomatically labeled examples which best meet the
constraints. These induced examples are then added
to their training set and, after processing each unla-
beled dataset, they perform full model optimization
with the concatenation of training data and newly
generated training items. The augmented-loss al-
gorithm can be viewed as an online version of this
algorithm which performs model updates based on
the augmented-loss functions directly (rather than
adding a set of examples to the training set). Un-
like the CODL approach, we do not perform com-
plete optimization on each iteration over the unla-
beled dataset; rather, we incorporate the updates in
our online learning algorithm. As mentioned earlier,
CODL is one example of learning algorithms that
use weak supervision, others include Mann and Mc-
Callum (2010) and Ganchev et al (2010). Again,
these works are typically interested in using the ex-
trinsic metric ? or, in general, extrinsic information
? to optimize the intrinsic metric in the absence of
any labeled intrinsic data. Our goal is to optimize
both simultaneously.
The idea of jointly training parsers to optimize
multiple objectives is related to joint learning and in-
ference for tasks like information extraction (Finkel
and Manning, 2009) and machine translation (Bur-
kett et al, 2010). In such works, a large search space
that covers both the space of parse structures and
the space of task-specific structures is defined and
parameterized so that standard learning and infer-
ence algorithms can be applied. What sets our work
apart is that there is still just a single parameter set
that is being optimized ? the parser parameters. Our
method only uses feedback from task specific objec-
tives in order to update the parser parameters, guid-
ing it towards better downstream performance. This
is advantageous for two reasons. First, it decouples
the tasks, making inference and learning more effi-
cient. Second, it does not force arbitrary paraemter
factorizations in order to define a joint search space
that can be searched efficiently.
Finally, augmented-loss training can be viewed
as multi-task learning (Caruana, 1997) as the model
optimizes multiple objectives over multiple data sets
with a shared underlying parameter space.
6 Discussion
The empirical results show that incorporating an
augmented-loss using the inline-ranker loss frame-
work achieves better performance under metrics as-
sociated with the external loss function. For the in-
trinsic loss, we see that the augmented-loss frame-
work can also result in an improvement in parsing
performance; however, in the case of ALS, this is
due to the fact that the loss function is very closely
related to the standard evaluation metrics of UAS
and LAS.
Although our analysis suggests that this algorithm
is guaranteed to converge only for the separable
case, it makes a further assumption that if there is
a better parse under the augmented-loss, then there
must be a lower cost parse in the k-best list. The em-
pirical evaluation presented here is based on a very
conservative approximation by choosing lists with
at most 8 parses. However, in our experiments, we
found that increasing the size of the lists did not sig-
nificantly increase our accuracy under the external
metrics. If we do have at least one improvement
in our k-best lists, the analysis suggests that this is
enough to move in the correct direction for updating
the model. The assumption that there will always
be an improvement in the k-best list if there is some
better parse breaks down as training continues. We
suspect that an increasing k, as suggested in Sec-
tion 2.3, will allow for continued improvements.
Dependency parsing, as presented in this pa-
per, is performed over (k-best) part-of-speech tags
and is therefore dependent on the quality of the
tagger. The experiments presented in this paper
made use of a tagger trained on the source treebank
data which severely limits the variation in parses.
The augmented-loss perceptron algorithm presented
here can be applied to any online learning prob-
lem, including part-of-speech tagger training. To
build a dependency parser which is better adapted
to a downstream task, one would want to perform
augmented-loss training on the tagger as well.
7 Conclusion
We introduced the augmented-loss training algo-
rithm and show that the algorithm can incorporate
1497
additional loss functions to adapt the model towards
extrinsic evaluation metrics. Analytical results are
presented that show that the algorithm can opti-
mize multiple objective functions simultaneously.
We present an empirical analysis for training depen-
dency parsers for multiple parsing algorithms and
multiple loss functions.
The augmented-loss framework supports both in-
trinsic and extrinsic losses, allowing for both com-
binations of objectives as well as multiple sources
of data for which the results of a parser can be eval-
uated. This flexibility makes it possible to tune a
model for a downstream task. The only requirement
is a metric which can be defined over parses of the
downstream data. Our dependency parsing results
show that we are not limited to increasing parser
performance via more data or external domain adap-
tation techniques, but that we can incorporate the
downstream task into parser training.
Acknowledgements: We would like to thank Kuz-
man Ganchev for feedback on an earlier draft of this
paper as well as Slav Petrov for frequent discussions
on this topic.
References
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Berant, I. Dagan, and J. Goldberger. 2010. Global
learning of focused entailment graphs. In Proc. of
ACL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of EMNLP.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
D. Burkett, J. Blitzer, and D. Klein. 2010. Joint parsing
and alignment with weakly synchronized grammars.
In Proc. of NAACL.
R. Caruana. 1997. Multitask learning. Machine Learn-
ing, 28(1):41?75.
M.W. Chang, L. Ratinov, and D. Roth. 2007. Guiding
semi-supervision with constraint-driven learning. In
Proc. of ACL.
M. Chang, D. Goldwasser, D. Roth, and V. Srikumar.
2010. Structured output learning with indirect super-
vision. In Proc. of ICML.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proc. of ICML.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
M.C. de Marneffe, B. MacCartney, and C. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC, Genoa,
Italy.
J.R. Finkel and C.D. Manning. 2009. Joint parsing and
named entity recognition. In Proc. of NAACL.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
K. Hall. 2007. k-best spanning tree parsing. In Proc. of
ACL, June.
J. Judge, A. Cahill, and J. Van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proc. of ACL, pages 497?504.
J. Katz-Brown, S. Petrov, R. McDonald, D. Talbot,
F. Och, H. Ichikawa, M. Seno, and H. Kazawa. 2011.
Training a parser for machine translation reordering.
In Proc. of EMNLP.
S. Ku?bler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies. Morgan & Claypool Publishers.
P. Liang, A. Bouchard-Ct, D. Klein, and B. Taskar. 2006.
An end-to-end discriminative approach to machine
translation. In Proc. of COLING/ACL.
G.S. Mann and A. McCallum. 2010. Generalized Ex-
pectation Criteria for Semi-Supervised Learning with
Weakly Labeled Data. The Journal of Machine Learn-
ing Research, 11:955?984.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19:313?330.
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proc. of ACL.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of EMNLP-CoNLL.
1498
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
of ACL.
T. Nakagawa, K. Inui, and S. Kurohashi. 2010. De-
pendency tree-based sentiment classification using crfs
with hidden variables. In Proc. of NAACL.
J. Nivre. 2008. Algorithms for deterministic incremen-
tal dependency parsing. Computational Linguistics,
34(4):513?553.
S. Petrov, P.C. Chang, M. Ringgaard, and H. Alshawi.
2010. Uptraining for accurate deterministic question
parsing. In Proc. of EMNLP, pages 705?713.
D. Talbot, H. Kazawa, H. Ichikawa, J. Katz-Brown,
M. Seno, and F. Och. 2011. A lightweight evalu-
ation framework for machine translation reordering.
In Proc. of the Sixth Workshop on Statistical Machine
Translation.
M. Wang, N.A. Smith, and T. Mitamura. 2007. What is
the Jeopardy model? A quasi-synchronous grammar
for QA. In Proc. of EMNLP-CoNLL.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Us-
ing a dependency parser to improve SMT for Subject-
Object-Verb languages. In Proc. of NAACL.
A. Yates and O. Etzioni. 2009. Unsupervised meth-
ods for determining object and relation synonyms on
the web. Journal of Artificial Intelligence Research,
34(1):255?296.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP, pages 562?571.
1499
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 879?883,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Russian Stress Prediction using Maximum Entropy Ranking
Keith Hall Richard Sproat
Google, Inc
New York, NY, USA
{kbhall,rws}@google.com
Abstract
We explore a model of stress prediction
in Russian using a combination of lo-
cal contextual features and linguistically-
motivated features associated with the
word?s stem and suffix. We frame this
as a ranking problem, where the objec-
tive is to rank the pronunciation with the
correct stress above those with incorrect
stress. We train our models using a simple
Maximum Entropy ranking framework al-
lowing for efficient prediction. An empir-
ical evaluation shows that a model com-
bining the local contextual features and
the linguistically-motivated non-local fea-
tures performs best in identifying both
primary and secondary stress.
1 Introduction
In many languages, one component of accu-
rate word pronunciation prediction is predict-
ing the placement of lexical stress. While in
some languages (e.g. Spanish) the lexical stress
system is relatively simple, in others (e.g. En-
glish, Russian) stress prediction is quite compli-
cated. Much as with other work on pronuncia-
tion prediction, previous work on stress assign-
ment has fallen into two camps, namely systems
based on linguistically motivated rules (Church,
1985, for example) and more recently data-
driven techniques where the models are derived
directly from labeled training data (Dou et al,
2009). In this work, we present a machine-
learned system for predicting Russian stress
which incorporates both data-driven contextual
features as well as linguistically-motivated word
features.
2 Previous Work on Stress
Prediction
Pronunciation prediction, of which stress pre-
diction is a part, is important for many speech
applications including automatic speech recog-
nition, text-to-speech synthesis, and translit-
eration for, say, machine translation. While
there is by now a sizable literature on pro-
nunciation prediction from spelling (often
termed ?grapheme-to-phoneme? conversion),
work that specifically focuses on stress predic-
tion is more limited. One of the best-known
early pieces of work is (Church, 1985), which
uses morphological rules and stress pattern
templates to predict stress in novel words. An-
other early piece of work is (Williams, 1987).
The work we present here is closer in spirit to
data-driven approaches such as (Webster, 2004;
Pearson et al, 2000) and particularly (Dou et
al., 2009), whose features we use in the work
described below.
3 Russian Stress Patterns
Russian stress preserves many features of Indo-
European accenting patterns (Halle, 1997). In
order to know the stress of a morphologically
complex word consisting of a stem plus a suf-
fix, one needs to know if the stem has an accent,
and if so on what syllable; and similarly for the
suffix. For words where the stem is accented,
879
acc unacc postacc
Dat Sg ???'??? ?'????? ?????'?
gor?oxu g?orodu korolj?u
Dat Pl ???'???? ?????'?? ?????'??
gor?oxam gorod?am korolj?am
?pea? ?town? ?king?
Table 1: Examples of accented, unaccented and
postaccented nouns in Russian, for dative singular
and plural forms.
this accent overrides any accent that may oc-
cur on the suffix. With unaccented stems, if
the suffix has an accent, then stress for the
whole word will be on the suffix; if there is
also no stress on the suffix, then a default rule
places stress on the first syllable of the word.
In addition to these patterns, there are also
postaccented words, where accent is placed uni-
formly on the first syllable of the suffix ? an
innovation of East and South Slavic languages
(Halle, 1997). These latter cases can be handled
by assigning an accent to the stem, indicating
that it is associated with the syllable after the
stem. Some examples of each of these classes,
from (Halle, 1997, example 11), are given in
Table 1. According to Halle (1997), consid-
ering just nouns, 91.6% are accented (on the
stem), 6.6% are postaccented and 0.8% are un-
accented, with about 1.0% falling into other
patterns.
Stress placement in Russian is important for
speech applications since over and above the
phonetic effects of stress itself (prominence, du-
ration, etc.), the position of stress strongly in-
fluences vowel quality. To take an example
of the lexically unaccented noun ????? gorod
?city?, the genitive singular ?'????? g?oroda
/g"Or@d@/ contrasts with the nominative plural
?????'? gorod?a /g@r2d"a/. All non-stressed
/a/ are reduced to schwa ? or by most ac-
counts if before the stressed syllable to /2/; see
(Wade, 1992).
The stress patterns of Russian suggest that
useful features for predicting stress might in-
clude (string) prefix and suffix features of the
word in order to capture properties of the stem,
since some stems are (un)accented, or of the
suffix, since some suffixes are accented.
4 Maximum Entropy Rankers
Similarly to Dou et al (2009), we frame the
stress prediction problem as a ranking problem.
For each word, we identify stressable vowels and
generate a set of alternatives, each represent-
ing a different primary stress placement. Some
words also have secondary stress which, if it oc-
curs, always occurs before the primary stressed
syllable. For each primary stress alternative,
we generate all possible secondary stressed al-
ternatives, including an alternative that has no
secondary stress. (In the experiments reported
below we actually consider two conditions: one
where we ignore secondary stress in training
and evaluation; and one where we include it.)
Formally, we model the problem using a Max-
imum Entropy ranking framework similar to
that presented in Collins and Koo (2005). For
each example, xi, we generate the set of possible
stress patterns Yi. Our goal is to rank the items
in Yi such that all of the valid stress patterns
Y?i are above all of the invalid stress patterns.
Our objective function is the likelihood, L of
this conditional distribution:
L =
?
i
p(Y?i |Yi, xi) (1)
logL =
?
i
log p(Y?i |Yi, xi) (2)
=
?
i
log
?
y??Y?i
e
?
k ?kfk(y?,x)
Z
(3)
Z is defined as the sum of the conditional like-
lihood over all hypothesized stress predictions
for example xi:
Z =
?
y???Yi
e
?
k ?kfk(y??,x) (4)
The objective function in Equation 3 can be
optimized using a gradient-based optimization.
In our case, we use a variety of stochastic gra-
dient descent (SGD) which can be parallelized
for efficient training.
During training, we provide all plausibly cor-
rect primary stress patterns as the positive set
880
Y?i . At prediction-time, we evaluate all possi-
ble stress predictions and pick the one with the
highest score under the trained model ?:
argmax
y??Yi
p(y?|Yi) = argmax
y??Yi
?
k
?kfk(y?, x) (5)
The primary motivation for using Maximum
Entropy rather the ranking-SVM is for efficient
training and inference. Under the above Max-
imum Entropy model, we apply a linear model
to each hypothesis (i.e., we compute the dot-
product) and sort according to this score. This
makes inference (prediction) fast in comparison
to the ranking SVM-based approach proposed
in Dou et al (2009).
All experiments presented in this paper used
the Iterative Parameter Mixtures distributed
SGD training optimizer (Hall et al, 2010). Un-
der this training approach, per-iteration aver-
aging has a regularization-like effect for sparse
feature spaces. We also experimented with L1-
regularization, but it offered no additional im-
provements.
5 Features
The features used in (Dou et al, 2009) are
based on trigrams consisting of a vowel letter,
the preceding consonant letter (if any) and the
following consonant letter (if any). Attached
to each trigram is the stress level of the tri-
gram?s vowel ? 1, 2 or 0 (for no stress). For
the English word overdo with the stress pattern
2-0-1, the basic features would be ov:2, ver:0,
and do:1. Notating these pairs as si : ti, where
si is the triple, ti is the stress pattern and i is
the position in the word, the complete feature
set is given in Table 2, where the stress pat-
tern for the whole word is given in the last row
as t1t2...tN . Dou and colleagues use an SVM-
based ranking approach, so they generated fea-
tures for all possible stress assignments for each
word, assigning the highest rank to the correct
assignment. The ranker was then trained to
associate feature combinations to the correct
ranking of alternative stress possibilities.
Given the discussion in Section 3, plausible
additional features are all prefixes and suffixes
Substring si, ti
si, i, ti
Context si1, ti
si1si, ti
si+1, ti
sisi+1, ti
si1sisi+1, ti
Stress Pattern t1t2...tN
Table 2: Features used in (Dou et al, 2009, Table 2).
vowel ?,?,?,?,?,?,?,?,?
stop ?,?,?,?,?,?
nasal ?,?
fricative ?,?,?,?,?,?,?
hard/soft ?,?
yo ?
semivowel ?,?
liquid ?,?
affricate ?,?
Table 3: Abstract phonetic classes used for con-
structing ?abstract? versions of a word. Note that
etymologically, and in some ways phonologically, ?
v behaves like a semivowel in Russian.
of the word, which might be expected to better
capture some of the properties of Russian stress
patterns discussed above, than the much more
local features from (Dou et al, 2009). In this
case for all stress variants of the word we collect
prefixes of length 1 through the length of the
word, and similarly for suffixes, except that for
the stress symbol we treat that together with
the vowel it marks as a single symbol. Thus for
the word gorod?a, all prefixes of the word would
be g, go, gor, goro, gorod, gorod?a.
In addition, we include prefixes and suffixes
of an ?abstract? version of the word where most
consonants and vowels have been replaced by
a phonetic class. The mappings for these are
shown in Table 3.
Note that in Russian the vowel ? /jO/ is al-
ways stressed, but is rarely written in text: it
is usually spelled as ?, whose stressed pronun-
cation is /(j)E/. Since written ? is in general
ambiguous between ? and ?, when we compute
stress variants of a word for the purpose of rank-
881
ing, we include both variants that have ? and
?.
6 Data
Our data were 2,004,044 fully inflected words
with assigned stress expanded from Zaliznyak?s
Grammatical Dictionary of the Russian Lan-
guage (Zaliznyak, 1977). These were split ran-
domly into 1,904,044 training examples and
100,000 test examples. The 100,000 test ex-
amples obviously contain no forms that were
found in the training data, but most of them
are word forms that derive from lemmata from
which some training data forms are also de-
rived. Given the fact that Russian stress is lex-
ically determined as outlined in Section 3, this
is perfectly reasonable: in order to know how
to stress a form, it is often necessary to have
seen other words that share the same lemma.
Nonetheless, it is also of interest to know how
well the system works on words that do not
share any lemmata with words in the training
data. To that end, we collected a set of 248
forms that shared no lemmata with the train-
ing data. The two sets will be referred to in the
next section as the ?shared lemmata? and ?no
shared lemmata? sets.
7 Results
Table 4 gives word accuracy results for the dif-
ferent feature combinations, as follows: Dou et
al?s features (Dou et al, 2009); our affix fea-
tures; our affix features plus affix features based
on the abstract phonetic class versions of words;
Dou et als features plus our affix features; Dou
et als features plus our affix features plus the
abstract affix features.
When we consider only primary stress (col-
umn 2 in Table 4, for the shared-lemmata test
data, Dou et als features performed the worst
at 97.2% accuracy, with all feature combina-
tions that include the affix features performing
at the same level, 98.7%. For the no-shared-
lemmata test data, using Dou et als features
alone achieved an accuracy of 80.6%. The affix
features alone performed worse, at 79.8%, pre-
sumably because it is harder for them to gener-
Features 1 stress 1+2 stress
shared lemmata
Dou et al0.972 0.965
Aff 0.987 0.985
Aff+Abstr Aff 0.987 0.985
Dou et alAff 0.987 0.986
Dou et alAff+Abstr Aff 0.987 0.986
no shared lemmata
Dou et al0.806 0.798
Aff 0.798 0.782
Aff+Abstr 0.810 0.790
Dou et alAff 0.823 0.810
Dou et alAff+Abstr Aff 0.839 0.815
Table 4: Word accuracies for various feature combi-
nations for both shared lemmata and no-shared lem-
mata conditions. The second column reports results
where we consider only primary stress, the third col-
umn results where we also predict secondary stress.
alize to unseen cases, but using the abstract af-
fix features increased the performance to 81.0%,
better than that of using Dou et als features
alone. As can be seen combining Dou et als
features with various combinations of the affix
features improved the performance further.
For primary and secondary stress prediction
(column 3 in the table), the results are over-
all degraded for most conditions but otherwise
very similar in terms of ranking of the fea-
tures to what we find with primary stress alone.
Note though that for the shared-lemmata con-
dition the results with affix features are almost
as good as for the primary-stress-only case,
whereas there is a significant drop in perfor-
mance for the Dou et al features. For the
no-shared-lemmata condition, Dou et al?s fea-
tures fare rather better compared to the affix
features. On the other hand there is a sub-
stantial benefit to combining the features, as
the results for ?Dou et alAff? and ?Dou et
al+Aff+Abstr Aff? show. Note that in the
no-shared-lemmata condition, there is only one
word that is marked with a secondary stress,
and that stress is actually correctly predicted
by all methods. Much of the difference between
the Dou et al features and the affix condition
can be accounted for by three cases involving
the same root, which the affix condition misas-
882
signs secondary stress to.
For the shared-lemmata task however there
were a substantial number of differences, as
one might expect given the nature of the fea-
tures. Comparing just the Dou et al fea-
tures and the all-features condition, system-
atic benefit for the all-features condition was
found for secondary stress assignment for pro-
ductive prefixes where secondary stress is typ-
ically found. For example, the prefix ????
(?aero-?) as in ?`???????'???? (?aerodynam-
ics?) typically has secondary stress. This is usu-
ally missed by the Dou et al features, but is
uniformly correct for the all-features condition.
Since the no-shared-lemmata data set is
small, we tested significance using two permu-
tation tests. The first computed a distribu-
tion of scores for the test data where succes-
sive single test examples were removed. The
second randomly permuted the test data 248
times, after each random permutation, remov-
ing the first ten examples, and computing the
score. Pairwise t-tests between all conditions
for the primary-stress-only and for the primary
plus secondary stress predictions, were highly
significant in all cases.
We also experimented with a postaccent fea-
ture to model the postaccented class of nouns
described in Section 3. For each prefix of the
word, we record whether the following vowel
is stressed or unstressed. This feature yielded
only very slight improvements, and we do not
report these results here.
8 Discussion
In this paper we have presented a Maximum
Entropy ranking-based approach to Russian
stress prediction. The approach is similar in
spirit to the SVM-based ranking approach pre-
sented in (Dou et al, 2009), but incorporates
additional affix-based features, which are moti-
vated by linguistic analyses of the problem. We
have shown that these additional features gen-
eralize better than the Dou et al features in
cases where we have seen a related form of the
test word, and that combing the additional fea-
tures with the Dou et al features always yields
an improvement.
References
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In Associ-
ation for Computational Linguistics, pages 246?
253.
Michael Collins and Terry Koo. 2005. Discrim-
inative reranking for natural language parsing.
Computational Linguistics, 31:25?69, March.
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn,
and Grzegorz Kondrak. 2009. A ranking ap-
proach to stress prediction for letter-to-phoneme
conversion. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Nat-
ural Language Processing of the AFNLP, pages
118?126, Suntec, Singapore, August. Association
for Computational Linguistics.
Keith B. Hall, Scott Gilpin, and Gideon Mann.
2010. Mapreduce/bigtable for distributed opti-
mization. In Neural Information Processing Sys-
tems Workshop on Leaning on Cores, Clusters,
and Clouds.
Morris Halle. 1997. On stress and accent in Indo-
European. Language, 73(2):275?313.
Steve Pearson, Roland Kuhn, Steven Fincke, and
Nick Kibre. 2000. Automatic methods for lexical
stress assignment and syllabification. In Interna-
tional Conference on Spoken Language Process-
ing, pages 423?426.
Terence Wade. 1992. A Comprehensive Russian
Grammar. Blackwell, Oxford.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
International Conference on Spoken Language
Processing, pages 2573?2576.
Briony Williams. 1987. Word stress assignment
in a text-to-speech synthesis system for British
English. Computer Speech and Language, 2:235?
272.
Andrey Zaliznyak. 1977. Grammaticheskij slovar?
russkogo jazyka. Russkiy Yazik, Moscow.
883
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 28?32,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Projecting the Knowledge Graph to Syntactic Parsing
Andrea Gesmundo and Keith B. Hall
Google, Inc.
{agesmundo,kbhall}@google.com
Abstract
We present a syntactic parser training
paradigm that learns from large scale
Knowledge Bases. By utilizing the
Knowledge Base context only during
training, the resulting parser has no
inference-time dependency on the Knowl-
edge Base, thus not decreasing the speed
during prediction. Knowledge Base infor-
mation is injected into the model using an
extension to the Augmented-loss training
framework. We present empirical results
that show this approach achieves a signif-
icant gain in accuracy for syntactic cat-
egories such as coordination and apposi-
tion.
1 Introduction
Natural Language Processing systems require
large amounts of world knowledge to achieve
state-of-the-art performance. Leveraging Knowl-
edge Bases (KB) provides allows us to inject hu-
man curated world-knowledge into our systems.
As these KBs have increased in size, we are now
able to leverage this information to improve upon
the state-of-the-art. Large scale KB have been de-
veloped rapidly in recent years, adding large num-
bers of entities and relations between the entities.
Such entities can be of any kind: an object, a per-
son, a place, a company, a book, etc. Entities
and relations are stored in association with rele-
vant data that describes the particular entity or re-
lation; for example, the name of a book, it?s author,
other books by the same author, etc.. Large scale
KB annotation efforts have focused on the collec-
tion of both current and historical entities, but are
biased towards the contemporary entities.
Of the many publicly available KBs, we focus
this study on the use of Freebase
1
: a large collab-
orative Knowledge Base composed and updated
by a member community. Currently it contains
roughly 40 million entities and 1.1 billion rela-
tions.
The aim of the presented work is to use the in-
formation provided by the KB to improve the ac-
curacy of the statistical dependency parsing task
(Kubler et al., 2009). In particular we focus on the
recognition of relations such as coordination and
apposition. This choice is motivated by the fact
that the KB stores information about real-world
entities while many of the errors associated with
coordination and apposition is the lack of knowl-
edge of these real-world entities.
We begin by defining the task (section 2). Fol-
lowing, we present the modified augmented-loss
training framework (section 3). In section 4, we
define how the Knowledge Base data is integrated
into the training process. Finally, we discuss the
empirical results (section 5).
2 Task
Apposition is a relation between two adjacent
noun-phrases, where one noun-phrase specifies or
modifying the other. For example, in the sentence
?My friend Anna?, the nouns ?friend? and ?Anna?
are in apposition. Coordination between nouns
relates two or more elements of the same kind.
The coordination is often signaled by the appear-
ance of a coordinating conjunction. For example,
in the sentence ?My friend and Anna?, the nouns
?friend? and ?Anna? are in coordination. The se-
mantic difference between the two relations is that
the nouns in apposition refer to the same entity,
1
www.freebase.com
28
while the nouns in coordination refer to distinct
entities of the same kind or sharing some proper-
ties.
Statistical parsers are inaccurate in classifying
relations involving proper nouns that appear rarely
in the training set. In the sentence:
?They invested in three companies, Google,
Microsoft, and Yahoo.?
?companies? is in apposition with the coordina-
tion ?Google, Microsoft, and Yahoo?. By integrat-
ing the information provided by a large scale KB
into the syntactic parser, we attempt to increase
the ability to disambiguate the relations involving
these proper nouns, even if the parser has been
trained on a different domain.
3 Model
We present a Syntactic Parsing model that learns
from the KB. An important constraint that we im-
pose, is that the speed of the Syntactic Parser must
not decrease when this information is integrated.
As the queries to the KB would significantly slow
down the parser, we limit querying the KB to train-
ing. This constraint reduces the impact that the KB
can have on the accuracy, but allows us to design a
parser that can be substituted in any setting, even
in the absence of the KB.
We propose a solution based on the Augmented-
loss framework (Hall et al., 2011a). Augmented-
loss is a training framework for structured predic-
tion tasks such as parsing. It can be used to ex-
tend a standard objective function with additional
loss-functions and be integrated with the struc-
tured perceptron training algorithm. The input
is enriched with multiple datasets each associated
with a loss function. The algorithm iterates over
the datasets triggering parameter updates when-
ever the loss function is positive.
Loss functions return a positive value if the pre-
dicted output is ?worse? than the gold standard.
Augmented-loss allows for the inclusion of mul-
tiple objective functions, either based on intrinsic
parsing quality or task-specific extrinsic measures
of quality. In the original formalization, both the
intrinsic and extrinsic losses require gold standard
information. Thus, each dataset must specify a
gold standard output for each input.
We extend the Augmented-loss framework to
apply it when the additional dataset gold-standard
is unknown. Without the gold standard, it is not
possible to trigger updates using a loss function.
Instead, we use a sampling function, S(?), that is
defined such that: if y? is a candidate parse tree,
then S(y?) returns a parse tree that is guaranteed to
be ?not worse? than y?. In other words:
L
S
(y?, S(y?)) ? 0 (1)
Where the L
S
(?) is the implicit loss function. This
formalization will allow us to avoid stating explic-
itly the loss function. Notice that S(y?) is not guar-
anteed to be the ?best? parse tree. It can be any
parse tree in the search space that is ?not worse?
than y?. S(y?) can represent an incremental im-
provement over y?.
Algorithm 1 Augmented-loss extension
1: {Input loss function: L(?)}
2: {Input sample function: S(?)}
3: {Input data sets}:
4: D
L
= {d
L
i
= (x
L
i
, y
L
i
) | 1 ? i ? N
L
}
5: D
S
= {d
S
i
= (x
S
i
) | 1 ? i ? N
S
}
6: ? =
~
0
7: repeat
8: for i = 1 . . . N
L
do
9: y? = F
?
(x
L
i
)
10: if L(y?, y
L
i
) > 0 then
11: ? = ? + ?(y
L
i
)? ?(y?)
12: end if
13: end for
14: for i = 1 . . . N
S
do
15: y? = F
?
(x
S
i
)
16: y
?
= S(y?)
17: ? = ? + ?(y
?
)? ?(y?)
18: end for
19: until converged
20: {Return model ?}
Algorithm 1 summarizes the extension to the
Augmented-loss algorithm.
The algorithm takes as input: the loss func-
tion L(?); the sample function S(?); the loss func-
tion data samples D
L
; and the sample function
data samples D
S
. Notice that D
L
specifies the
gold standard parse y
L
i
for each input sentence x
L
i
.
While, D
S
specifies only the input sentence x
S
i
.
The model parameter are initialized to the zero
vector (line 6). The main loop iterates until the
model reaches convergence (lines 7-19). After
which the model parameters are returned.
The first inner loop iterates over D
L
(lines 8-
13) executing the standard on-line training. The
candidate parse, y?, for the current input sentence,
29
xL
i
, is predicted given the current model parame-
ters, ? (line 9). In the structured perceptron setting
(Collins and Roark, 2004; Daum?e III et al., 2009),
we have that:
F
?
(x) = argmax
y?Y
? ? ?(y) (2)
Where ?(?) is the mapping from a parse tree y to
a high dimensional feature space. Then, the algo-
rithm tests if the current prediction is wrong (line
10). In which case the model is updated promot-
ing features that fire in the gold-standard ?(y
L
i
),
and penalizing features that fire in the predicted
output, ?(y?) (line 11).
The second inner loop iterates over D
S
(lines
14-18). First, the candidate parse, y?, is predicted
(line 15). Then the sample parse, y
?
, is pro-
duced by the sample function (line 16). Finally,
the parameters are updated promoting the features
of y
?
. The updates are triggered without test-
ing if the loss is positive, since it is guaranteed
that L
S
(y?, y
?
) ? 0. Updating in cases where
L
S
(y?, y
?
) = 0 does not harm the model. To opti-
mize the algorithm, updates can be avoided when
y? = y
?
.
In order to simplify the algorithmic descrip-
tion, we define the algorithm with only one loss
function and one sample function, and we formal-
ized it for the specific task we are considering.
This definitions can be trivially generalized to in-
tegrate multiple loss/sample functions and to be
formalized for a generic structured prediction task.
This generalization can be achieved following the
guidelines of (Hall et al., 2011a). Furthermore, we
defined the algorithm such that it first iterates over
D
L
and then over D
S
. In practice, the algorithm
can switch between the data sets with a desired fre-
quency by using a scheduling policy as described
in (Hall et al., 2011a). For the experiments, we
trained on 8 samples ofD
L
followed by 1 samples
of D
S
, looping over the training sets.
4 Sample Function
We integrate the Knowledge Base data into the
training algorithm using a sampling function. The
idea is to correct errors in the candidate parse
by using the KB. The sample function corrects
only relations among entities described in the KB.
Thus, it returns a better or equal parse tree that
may still contain errors. This is sufficient to guar-
antee the constraint on the implicit loss function
(equation 1).
The sample function receives as input the can-
didate dependency parse and the input sentence
enriched with KB annotation. Then, it corrects
the labels of each arc in the dependency tree con-
necting two entities. The labels are corrected ac-
cording to the predictions produced by a classifier.
As classifier we use a standard multi-class percep-
tron (Crammer and Singer, 2003). The classifier is
trained in a preprocessing step on a parsed corpus
enriched with KB data. The features used by the
classifier are:
? Lexical features of the head and modifier.
? Sentence level features: words distance be-
tween head and modifier; arc direction (L/R);
neighboring words.
? Syntactic features: POS and syntactic label of
head and modifier and modifier?s left sibling.
? Knowledge Base features: types defined for
entities and for their direct relations.
5 Experiments
The primary training corpus is composed of manu-
ally annotated sentences with syntactic tress which
are converted to dependency format using the
Stanford converter v1.6 (de Marneffe et al., 2006).
We run experiments using 10k sentences or 70k
sentences from this corpus. The test set contains
16k manually syntactically annotated sentences
crawled from the web. The test and train sets are
from different domains. This setting may degrade
the parser accuracy in labelling out-of-domain en-
tities, as we discussed in section 2. Thus, we use
web text as secondary training set to be used for
the Augmented-loss loss sample training. Web
text is available in any quantity, and we do not
need to provide gold-standard parses in order to
integrate it in the Augmented-loss sample train-
ing. The classifier is trained on 10k sentences ex-
tracted from news text which has been automati-
cally parsed. We chose to train the classifier on
news data as the quality of the automatic parses is
much higher than on general web text. We do this
despite the fact that we will apply the classifier to
a different domain (the web text).
As dependency parser, we use an implemen-
tation of the transition-based dependency parsing
framework (Nivre, 2008) with the arc-eager tran-
sition strategy. The part of Augmented-loss train-
ing based on the standard loss function, applies
30
Training set size Model appos F1 conj F1 LAS UAS
70k sentences Baseline 54.36 83.72 79.55 83.50
Augmented-loss 55.64 84.47 79.71 83.71
10k sentences Baseline 45.13 80.36 75.99 86.02
Augmented-loss 48.06 81.63 76.16 86.18
Table 1: Accuracy Comparison.
the perceptron algorithm as in (Zhang and Clark,
2008) with a beam size of 16. The baseline is the
same model but trained only the primary training
corpus without Augmented-loss.
Table 1 reports the results of the accuracy com-
parison. It reports the metrics for Labeled At-
tachment Score (LAS) and Unlabeled Attachment
Score (UAS) to measure the overall accuracy. The
syntactic classes that are affected the most are ap-
position (appos) and conjunction (conj). On the
development set we measured that the percentage
of arcs connecting 2 entities that are labeled as
conjunction is 36.11%. While those that are la-
belled as apposition is 25.06%. Each of the other
40 labels cover a small portion of the remaining
38.83%.
Training the models with the full primary train-
ing corpus (70k sentences), shows a significant
gain for the Augmented-loss model. Apposition
F1 gains 1.28, while conjunction gains 0.75. The
LAS gain is mainly due to the gain of the two men-
tioned classes. It is surprising to measure a simi-
lar gain also for the unlabeled accuracy. Since the
classifier can correct the label of an arc but never
change the structure of the parse. This implies
that just by penalizing a labeling action, the model
learns to construct better parse structures.
Training the model with 10k sentences shows a
significantly bigger gain on all the measures. This
results shows that, in cases where the set of la-
beled data is small, this approach can be applied
to integrate in unlimited amount of unlabeled data
to boost the learning.
6 Related Work
As we mentioned, Augmented-loss (Hall et al.,
2011a; Hall et al., 2011b) is perhaps the closest to
our framework. Another difference with its origi-
nal formalization is that it was primarily aimed to
cases where the additional weak signal is precisely
what we wish to optimize. Such as cases where
we wish to optimize parsing to be used as an input
to a downstream natural language processing tasks
and the accuracies to be optimized are those of the
downstream task and not directly the parsing ac-
curacy. While our work is focused on integrating
additional data in a semi-supervised fashion with
the aim of improving the primary task?s accuracy
and/or adapt it to a different domain.
Another similar idea is (Chang et al., 2007)
which presents a constraint driven learning. In this
study, they integrate a weak signal into the training
framework with the aim to improve the structured
prediction models on the intrinsic evaluation met-
rics.
7 Conclusion
We extended the Augmented-loss framework
defining a method for integrating new types of sig-
nals that require neither gold standard data nor an
explicit loss function. At the same time, they al-
low the integration of additional information that
can inform training to learn for specific types of
phenomena.
This framework allows us to effectively inte-
grate large scale KB in the training of structured
prediction tasks. This approach integrates the data
at training time without affecting the prediction
time.
Experiments on syntactic parsing show that a
significant gain for categories that model relation
between entities defined in the KB.
References
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint-driven
learning. In ACL ?07: Proceedings of the 45th Con-
ference of the Association for Computational Lin-
guistics.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In ACL ?04:
Proceedings of the 42rd Conference of the Associa-
tion for Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
31
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Submit-
ted to Machine Learning Journal.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure trees. In
LREC.
Keith Hall, Ryan McDonald, Jason Katz-brown, and
Michael Ringgaard. 2011a. Training dependency
parsers by jointly optimizing multiple objectives. In
EMNLP ?11: Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing.
Keith Hall, Ryan McDonald, and Slav Petrov. 2011b.
Training structured prediction models with extrinsic
loss functions. In Domain Adaptation Workshop at
NIPS, October.
Sandra Kubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. In Synthesis Lectures
on Human Language Technologies. Morgan & Clay-
pool Publishers.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. volume 34, pages
513?553.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: Investigating and combining graph-
based and transition-based dependency parsing. In
EMNLP ?08: Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562?571.
32
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456?464,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Distributed Training Strategies for the Structured Perceptron
Ryan McDonald Keith Hall Gideon Mann
Google, Inc., New York / Zurich
{ryanmcd|kbhall|gmann}@google.com
Abstract
Perceptron training is widely applied in the
natural language processing community for
learning complex structured models. Like all
structured prediction learning frameworks, the
structured perceptron can be costly to train
as training complexity is proportional to in-
ference, which is frequently non-linear in ex-
ample sequence length. In this paper we
investigate distributed training strategies for
the structured perceptron as a means to re-
duce training times when computing clusters
are available. We look at two strategies and
provide convergence bounds for a particu-
lar mode of distributed structured perceptron
training based on iterative parameter mixing
(or averaging). We present experiments on
two structured prediction problems ? named-
entity recognition and dependency parsing ?
to highlight the efficiency of this method.
1 Introduction
One of the most popular training algorithms for
structured prediction problems in natural language
processing is the perceptron (Rosenblatt, 1958;
Collins, 2002). The structured perceptron has many
desirable properties, most notably that there is no
need to calculate a partition function, which is
necessary for other structured prediction paradigms
such as CRFs (Lafferty et al, 2001). Furthermore,
it is robust to approximate inference, which is of-
ten required for problems where the search space
is too large and where strong structural indepen-
dence assumptions are insufficient, such as parsing
(Collins and Roark, 2004; McDonald and Pereira,
2006; Zhang and Clark, 2008) and machine trans-
lation (Liang et al, 2006). However, like all struc-
tured prediction learning frameworks, the structure
perceptron can still be cumbersome to train. This
is both due to the increasing size of available train-
ing sets as well as the fact that training complexity
is proportional to inference, which is frequently non-
linear in sequence length, even with strong structural
independence assumptions.
In this paper we investigate distributed training
strategies for the structured perceptron as a means
of reducing training times when large computing
clusters are available. Traditional machine learning
algorithms are typically designed for a single ma-
chine, and designing an efficient training mechanism
for analogous algorithms on a computing cluster ?
often via a map-reduce framework (Dean and Ghe-
mawat, 2004) ? is an active area of research (Chu
et al, 2007). However, unlike many batch learning
algorithms that can easily be distributed through the
gradient calculation, a distributed training analog for
the perceptron is less clear cut. It employs online up-
dates and its loss function is technically non-convex.
A recent study by Mann et al (2009) has shown
that distributed training through parameter mixing
(or averaging) for maximum entropy models can
be empirically powerful and has strong theoretical
guarantees. A parameter mixing strategy, which can
be applied to any parameterized learning algorithm,
trains separate models in parallel, each on a disjoint
subset of the training data, and then takes an average
of all the parameters as the final model. In this paper,
we provide results which suggest that the percep-
tron is ill-suited for straight-forward parameter mix-
ing, even though it is commonly used for large-scale
structured learning, e.g., Whitelaw et al (2008) for
named-entity recognition. However, a slight mod-
456
ification we call iterative parameter mixing can be
shown to: 1) have similar convergence properties to
the standard perceptron algorithm, 2) find a sepa-
rating hyperplane if the training set is separable, 3)
reduce training times significantly, and 4) produce
models with comparable (or superior) accuracies to
those trained serially on all the data.
2 Related Work
Distributed cluster computation for many batch
training algorithms has previously been examined
by Chu et al (2007), among others. Much of the
relevant prior work on online (or sub-gradient) dis-
tributed training has been focused on asynchronous
optimization via gradient descent. In this sce-
nario, multiple machines run stochastic gradient de-
scent simultaneously as they update and read from
a shared parameter vector asynchronously. Early
work by Tsitsiklis et al (1986) demonstrated that
if the delay between model updates and reads is
bounded, then asynchronous optimization is guaran-
teed to converge. Recently, Zinkevich et al (2009)
performed a similar type of analysis for online learn-
ers with asynchronous updates via stochastic gra-
dient descent. The asynchronous algorithms in
these studies require shared memory between the
distributed computations and are less suitable to
the more common cluster computing environment,
which is what we study here.
While we focus on the perceptron algorithm, there
is a large body of work on training structured pre-
diction classifiers. For batch training the most com-
mon is conditional random fields (CRFs) (Lafferty
et al, 2001), which is the structured analog of maxi-
mum entropy. As such, its training can easily be dis-
tributed through the gradient or sub-gradient com-
putations (Finkel et al, 2008). However, unlike per-
ceptron, CRFs require the computation of a partition
function, which is often expensive and sometimes
intractable. Other batch learning algorithms include
M3Ns (Taskar et al, 2004) and Structured SVMs
(Tsochantaridis et al, 2004). Due to their efficiency,
online learning algorithms have gained attention, es-
pecially for structured prediction tasks in NLP. In
addition to the perceptron (Collins, 2002), others
have looked at stochastic gradient descent (Zhang,
2004), passive aggressive algorithms (McDonald et
Perceptron(T = {(xt,yt)}
|T |
t=1)
1. w(0) = 0; k = 0
2. for n : 1..N
3. for t : 1..T
4. Let y? = argmaxy? w
(k) ? f(xt,y?)
5. if y? 6= yt
6. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
7. k = k + 1
8. return w(k)
Figure 1: The perceptron algorithm.
al., 2005; Crammer et al, 2006), the recently intro-
duced confidence weighted learning (Dredze et al,
2008) and coordinate descent algorithms (Duchi and
Singer, 2009).
3 Structured Perceptron
The structured perceptron was introduced by Collins
(2002) and we adopt much of the notation and pre-
sentation of that study. The structured percetron al-
gorithm ? which is identical to the multi-class per-
ceptron ? is shown in Figure 1. The perceptron is an
online learning algorithm and processes training in-
stances one at a time during each epoch of training.
Lines 4-6 are the core of the algorithm. For a input-
output training instance pair (xt,yt) ? T , the algo-
rithm predicts a structured output y? ? Yt, where Yt
is the space of permissible structured outputs for in-
put xt, e.g., parse trees for an input sentence. This
prediction is determined by a linear classifier based
on the dot product between a high-dimensional fea-
ture representation of a candidate input-output pair
f(x,y) ? RM and a corresponding weight vector
w ? RM , which are the parameters of the model1.
If this prediction is incorrect, then the parameters
are updated to add weight to features for the cor-
responding correct output yt and take weight away
from features for the incorrect output y?. For struc-
tured prediction, the inference step in line 4 is prob-
lem dependent, e.g., CKY for context-free parsing.
A training set T is separable with margin ? >
0 if there exists a vector u ? RM with ?u? = 1
such that u ? f(xt,yt) ? u ? f(xt,y?) ? ?, for all
(xt,yt) ? T , and for all y? ? Yt such that y? 6= yt.
Furthermore, letR ? ||f(xt,yt)?f(xt,y?)||, for all
(xt,yt) ? T and y? ? Yt. A fundamental theorem
1The perceptron can be kernalized for non-linearity.
457
of the perceptron is as follows:
Theorem 1 (Novikoff (1962)). Assume training set
T is separable by margin ?. Let k be the number of
mistakes made training the perceptron (Figure 1) on
T . If training is run indefinitely, then k ? R
2
?2 .
Proof. See Collins (2002) Theorem 1.
Theorem 1 implies that if T is separable then 1) the
perceptron will converge in a finite amount of time,
and 2) will produce a w that separates T . Collins
also proposed a variant of the structured perceptron
where the final weight vector is a weighted average
of all parameters that occur during training, which
he called the averaged perceptron and can be viewed
as an approximation to the voted perceptron algo-
rithm (Freund and Schapire, 1999).
4 Distributed Structured Perceptron
In this section we examine two distributed training
strategies for the perceptron algorithm based on pa-
rameter mixing.
4.1 Parameter Mixing
Distributed training through parameter mixing is a
straight-forward way of training classifiers in paral-
lel. The algorithm is given in Figure 2. The idea is
simple: divide the training data T into S disjoint
shards such that T = {T1, . . . , TS}. Next, train
perceptron models (or any learning algorithm) on
each shard in parallel. After training, set the final
parameters to a weighted mixture of the parameters
of each model using mixture coefficients ?. Note
that we call this strategy parameter mixing as op-
posed to parameter averaging to distinguish it from
the averaged perceptron (see previous section). It is
easy to see how this can be implemented on a cluster
through a map-reduce framework, i.e., the map step
trains the individual models in parallel and the re-
duce step mixes their parameters. The advantages of
parameter mixing are: 1) that it is parallel, making
it possibly to scale to extremely large data sets, and
2) it is resource efficient, in particular with respect
to network usage as parameters are not repeatedly
passed across the network as is often the case for
exact distributed training strategies.
For maximum entropy models, Mann et al (2009)
show it is possible to bound the norm of the dif-
PerceptronParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w(i) = Perceptron(Ti) ?
3. w =
?
i ?iw
(i) ?
4. return w
Figure 2: Distributed perceptron using a parameter mix-
ing strategy. ? Each w(i) is computed in parallel. ? ? =
{?1, . . . , ?S}, ??i ? ? : ?i ? 0 and
?
i ?i = 1.
ference between parameters trained on all the data
serially versus parameters trained with parameter
mixing. However, their analysis requires a stabil-
ity bound on the parameters of a regularized max-
imum entropy model, which is not known to hold
for the perceptron. In Section 5, we present empir-
ical results showing that parameter mixing for dis-
tributed perceptron can be sub-optimal. Addition-
ally, Dredze et al (2008) present negative parame-
ter mixing results for confidence weighted learning,
which is another online learning algorithm. The fol-
lowing theorem may help explain this behavior.
Theorem 2. For a any training set T separable by
margin ?, the perceptron algorithm trained through
a parameter mixing strategy (Figure 2) does not nec-
essarily return a separating weight vector w.
Proof. Consider a binary classification setting
where Y = {0, 1} and T has 4 instances.
We distribute the training set into two shards,
T1 = {(x1,1,y1,1), (x1,2,y1,2)} and T2 =
{(x2,1,y2,1), (x2,2,y2,2)}. Let y1,1 = y2,1 = 0 and
y1,2 = y2,2 = 1. Now, let w, f ? R6 and using
block features, define the feature space as,
f(x1,1, 0) = [1 1 0 0 0 0] f(x1,1, 1) = [0 0 0 1 1 0]
f(x1,2, 0) = [0 0 1 0 0 0] f(x1,2, 1) = [0 0 0 0 0 1]
f(x2,1, 0) = [0 1 1 0 0 0] f(x2,1, 1) = [0 0 0 0 1 1]
f(x2,2, 0) = [1 0 0 0 0 0] f(x2,2, 1) = [0 0 0 1 0 0]
Assuming label 1 tie-breaking, parameter mixing re-
turns w1=[1 1 0 -1 -1 0] and w2=[0 1 1 0 -1 -1]. For
any ?, the mixed weight vector w will not separate
all the points. If both ?1/?2 are non-zero, then all
examples will be classified 0. If ?1=1 and ?2=0,
then (x2,2,y2,2) will be incorrectly classified as 0
and (x1,2,y1,2) when ?1=0 and ?2=1. But there is a
separating weight vector w = [-1 2 -1 1 -2 1].
This counter example does not say that a parameter
mixing strategy will not converge. On the contrary,
458
if T is separable, then each of its subsets is separa-
ble and converge via Theorem 1. What it does say
is that, independent of ?, the mixed weight vector
produced after convergence will not necessarily sep-
arate the entire data, even when T is separable.
4.2 Iterative Parameter Mixing
Consider a slight augmentation to the parameter
mixing strategy. Previously, each parallel percep-
tron was trained to convergence before the parame-
ter mixing step. Instead, shard the data as before, but
train a single epoch of the perceptron algorithm for
each shard (in parallel) and mix the model weights.
This mixed weight vector is then re-sent to each
shard and the perceptrons on those shards reset their
weights to the new mixed weights. Another single
epoch of training is then run (again in parallel over
the shards) and the process repeats. This iterative
parameter mixing algorithm is given in Figure 3.
Again, it is easy to see how this can be imple-
mented as map-reduce, where the map computes the
parameters for each shard for one epoch and the re-
duce mixes and re-sends them. This is analogous
to batch distributed gradient descent methods where
the gradient for each shard is computed in parallel in
the map step and the reduce step sums the gradients
and updates the weight vector. The disadvantage of
iterative parameter mixing, relative to simple param-
eter mixing, is that the amount of information sent
across the network will increase. Thus, if network
latency is a bottleneck, this can become problematic.
However, for many parallel computing frameworks,
including both multi-core computing as well as clus-
ter computing with high rates of connectivity, this is
less of an issue.
Theorem 3. Assume a training set T is separable
by margin ?. Let ki,n be the number of mistakes that
occurred on shard i during the nth epoch of train-
ing. For any N , when training the perceptron with
iterative parameter mixing (Figure 3),
N?
n=1
S?
i=1
?i,nki,n ?
R2
?2
Proof. Let w(i,n) to be the weight vector for the
ith shard after the nth epoch of the main loop and
let w([i,n]?k) be the weight vector that existed on
shard i in the nth epoch k errors before w(i,n). Let
PerceptronIterParamMix(T = {(xt,yt)}
|T |
t=1)
1. Shard T into S pieces T = {T1, . . . , TS}
2. w = 0
3. for n : 1..N
4. w(i,n) = OneEpochPerceptron(Ti,w) ?
5. w =
?
i ?i,nw
(i,n) ?
6. return w
OneEpochPerceptron(T , w?)
1. w(0) = w?; k = 0
2. for t : 1..T
3. Let y? = argmaxy? w
(k) ? f(xt,y?)
4. if y? 6= yt
5. w(k+1) = w(k) + f(xt,yt)? f(xt,y?)
6. k = k + 1
7. return w(k)
Figure 3: Distributed perceptron using an iterative param-
eter mixing strategy. ? Each w(i,n) is computed in paral-
lel. ? ?n = {?1,n, . . . , ?S,n}, ??i,n ? ?n: ?i,n ? 0 and
?n:
?
i ?i,n = 1.
w(avg,n) be the mixed vector from the weight vec-
tors returned after the nth epoch, i.e.,
w(avg,n) =
S?
i=1
?i,nw(i,n)
Following the analysis from Collins (2002) Theorem
1, by examining line 5 of OneEpochPerceptron in
Figure 3 and the fact that u separates the data by ?:
u ?w(i,n) = u ?w([i,n]?1)
+ u ? (f(xt,yt)? f(xt,y?))
? u ?w([i,n]?1) + ?
? u ?w([i,n]?2) + 2?
. . . ? u ?w(avg,n?1) + ki,n? (A1)
That is, u ? w(i,n) is bounded below by the average
weight vector for the n-1st epoch plus the number
of mistakes made on shard i during the nth epoch
times the margin ?. Next, by OneEpochPerceptron
line 5, the definition ofR, and w([i,n]?1)(f(xt,yt)?
f(xt,y?)) ? 0 when line 5 is called:
?w(i,n)?2 = ?w([i,n]?1)?2
+?f(xt,yt)? f(xt,y?)?2
+ 2w([i,n]?1)(f(xt,yt)? f(xt,y?))
? ?w([i,n]?1)?2 +R2
? ?w([i,n]?2)?2 + 2R2
. . . ? ?w(avg,n?1)?2 + ki,nR2 (A2)
459
That is, the squared L2-norm of a shards weight vec-
tor is bounded above by the same value for the aver-
age weight vector of the n-1st epoch and the number
of mistakes made on that shard during the nth epoch
times R2.
Using A1/A2 we prove two inductive hypotheses:
u ?w(avg,N) ?
N?
n=1
S?
i=1
?i,nki,n? (IH1)
?w(avg,N)?2 ?
N?
n=1
S?
i=1
?i,nki,nR
2 (IH2)
IH1 implies ?w(avg,N)? ?
?N
n=1
?S
i=1 ?i,nki,n?
since u ?w ? ?u??w? and ?u? = 1.
The base case is w(avg,1), where we can observe:
u ?wavg,1 =
S?
i=1
?i,1u ?w(i,1) ?
S?
i=1
?i,1ki,1?
using A1 and the fact that w(avg,0) = 0 for the sec-
ond step. For the IH2 base case we can write:
?w(avg,1)?2 =
?
?
?
?
?
S?
i=1
?i,1w(i,1)
?
?
?
?
?
2
?
S?
i=1
?i,1?w(i,1)?2 ?
S?
i=1
?i,1ki,1R
2
The first inequality is Jensen?s inequality, and the
second is true by A2 and ?w(avg,0)?2 = 0.
Proceeding to the general case, w(avg,N):
u ?w(avg,N) =
S?
i=1
?i,N (u ?w(i,N))
?
S?
i=1
?i,N (u ?w(avg,N?1) + ki,N?)
= u ?w(avg,N?1) +
S?
i=1
?i,Nki,N?
?
[
N?1?
n=1
S?
i=1
?i,nki,n?
]
+
S?
i=1
?i,Nki,N
=
N?
n=1
S?
i=1
?i,nki,n?
The first inequality uses A1, the second step
?
i ?i,N = 1 and the second inequality the induc-
tive hypothesis IH1. For IH2, in the general case,
we can write:
?w(avg,N)?2 ?
S?
i=1
?i,N?w(i,N)?2
?
S?
i=1
?i,N (?w(avg,N?1)?2 + ki,NR2)
= ?w(avg,N?1)?2 +
S?
i=1
?i,Nki,NR
2
?
[
N?1?
n=1
S?
i=1
?i,nki,nR
2
]
+
S?
i=1
?i,Nki,NR
2
=
N?
n=1
S?
i=1
?i,nki,nR
2
The first inequality is Jensen?s, the second A2, and
the third the inductive hypothesis IH2. Putting to-
gether IH1, IH2 and ?w(avg,N)? ? u ?w(avg,N):
[
N?
n=1
S?
i=1
?i,nki,n
]2
?2 ?
[
N?
n=1
S?
i=1
?i,nki,n
]
R2
which yields:
?N
n=1
?S
i=1 ?i,nki,n ?
R2
?2
4.3 Analysis
If we set each ?n to be the uniform mixture, ?i,n =
1/S, then Theorem 3 guarantees convergence to
a separating hyperplane. If
?S
i=1 ?i,nki,n = 0,
then the previous weight vector already separated
the data. Otherwise,
?N
n=1
?S
i=1 ?i,nki,n is still in-
creasing, but is bounded and cannot increase indefi-
nitely. Also note that if S = 1, then ?1,n must equal
1 for all n and this bound is identical to Theorem 1.
However, we are mainly concerned with how fast
convergence occurs, which is directly related to the
number of training epochs each algorithm must run,
i.e., N in Figure 1 and Figure 3. For the non-
distributed variant of the perceptron we can say that
Nnon dist ? R2/?2 since in the worst case a single
mistake happens on each epoch.2 For the distributed
case, consider setting ?i,n = ki,n/kn, where kn =?
i ki,n. That is, we mix parameters proportional to
the number of errors each made during the previous
epoch. Theorem 3 still implies convergence to a sep-
arating hyperplane with this choice. Further, we can
2It is not hard to derive such degenerate cases.
460
bound the required number of epochs Ndist:
Ndist ?
Ndist?
n=1
S?
i=1
[ki,n]
ki,n
kn ?
Ndist?
n=1
S?
i=1
ki,n
kn
ki,n ?
R2
?2
Ignoring when all ki,n are zero (since the algorithm
will have converged), the first inequality is true since
either ki,n ? 1, implying that [ki,n]ki,n/kn ? 1, or
ki,n = 0 and [ki,n]ki,n/kn = 1. The second inequal-
ity is true by the generalized arithmetic-geometric
mean inequality and the final inequality is Theo-
rem 3. Thus, the worst-case number of epochs is
identical for both the regular and distributed percep-
tron ? but the distributed perceptron can theoreti-
cally process each epoch S times faster. This ob-
servation holds only for cases where ?i,n > 0 when
ki,n ? 1 and ?i,n = 0 when ki,n = 0, which does
not include uniform mixing.
5 Experiments
To investigate the distributed perceptron strategies
discussed in Section 4 we look at two structured pre-
diction tasks ? named entity recognition and depen-
dency parsing. We compare up to four systems:
1. Serial (All Data): This is the classifier returned
if trained serially on all the available data.
2. Serial (Sub Sampling): Shard the data, select
one shard randomly and train serially.
3. Parallel (Parameter Mix): Parallel strategy
discussed in Section 4.1 with uniform mixing.
4. Parallel (Iterative Parameter Mix): Parallel
strategy discussed in Section 4.2 with uniform
mixing (Section 5.1 looks at mixing strategies).
For all four systems we compare results for both the
standard perceptron algorithm as well as the aver-
aged perceptron algorithm (Collins, 2002).
We report the final test set metrics of the con-
verged classifiers to determine whether any loss in
accuracy is observed as a consequence of distributed
training strategies. We define convergence as ei-
ther: 1) the training set is separated, or 2) the train-
ing set performance measure (accuracy, f-measure,
etc.) does not change by more than some pre-defined
threshold on three consecutive epochs. As with most
real world data sets, convergence by training set sep-
aration was rarely observed, though in both cases
training set accuracies approached 100%. For both
tasks we also plot test set metrics relative to the user
wall-clock taken to obtain the classifier. The results
were computed by collecting the metrics at the end
of each epoch for every classifier. All experiments
used 10 shards (Section 5.1 looks at convergence rel-
ative to different shard size).
Our first experiment is a named-entity recogni-
tion task using the English data from the CoNLL
2003 shared-task (Tjong Kim Sang and De Meul-
der, 2003). The task is to detect entities in sentences
and label them as one of four types: people, organi-
zations, locations or miscellaneous. For our exper-
iments we used the entire training set (14041 sen-
tences) and evaluated on the official development
set (3250 sentences). We used a straight-forward
IOB label encoding with a 1st order Markov fac-
torization. Our feature set consisted of predicates
extracted over word identities, word affixes, orthog-
raphy, part-of-speech tags and corresponding con-
catenations. The evaluation metric used was micro
f-measure over the four entity class types.
Results are given in Figure 4. There are a num-
ber of things to observe here: 1) training on a single
shard clearly provides inferior performance to train-
ing on all data, 2) the simple parameter mixing strat-
egy improves upon a single shard, but does not meet
the performance of training on all data, 3) iterative
parameter mixing achieves performance as good as
or better than training serially on all the data, and
4) the distributed algorithms return better classifiers
much quicker than training serially on all the data.
This is true regardless of whether the underlying al-
gorithm is the regular or the averaged perceptron.
Point 3 deserves more discussion. In particular, the
iterative parameter mixing strategy has a higher final
f-measure than training on all the data serially than
the standard perceptron (f-measure of 87.9 vs. 85.8).
We suspect this happens for two reasons. First, the
parameter mixing has a bagging like effect which
helps to reduce the variance of the per-shard classi-
fiers (Breiman, 1996). Second, the fact that parame-
ter mixing is just a form of parameter averaging per-
haps has the same effect as the averaged perceptron.
Our second set of experiments looked at the much
more computationally intensive task of dependency
parsing. We used the Prague Dependency Tree-
bank (PDT) (Hajic? et al, 2001), which is a Czech
461
Wall Clock
0.65
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Parameter Mix)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock
0.7
0.75
0.8
0.85
Test 
Data
 F-m
easu
re
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- Parallel (Parameter Mix)Averaged Perceptron -- Parallel (Iterative Parameter Mix)
Reg. Perceptron Avg. Perceptron
F-measure F-measure
Serial (All Data) 85.8 88.2
Serial (Sub Sampling) 75.3 76.6
Parallel (Parameter Mix) 81.5 81.6
Parallel (Iterative Parameter Mix) 87.9 88.1
Figure 4: NER experiments. Upper figures plot test data f-measure versus wall clock for both regular perceptron (left)
and averaged perceptron (right). Lower table is f-measure for converged models.
language treebank and currently one of the largest
dependency treebanks in existence. We used the
CoNLL-X training (72703 sentences) and testing
splits (365 sentences) of this data (Buchholz and
Marsi, 2006) and dependency parsing models based
on McDonald and Pereira (2006) which factors fea-
tures over pairs of dependency arcs in a tree. To
parse all the sentences in the PDT, one must use a
non-projective parsing algorithm, which is a known
NP-complete inference problem when not assuming
strong independence assumptions. Thus, the use of
approximate inference techniques is common in or-
der to find the highest weighted tree for a sentence.
We use the approximate parsing algorithm given in
McDonald and Pereira (2006), which runs in time
roughly cubic in sentence length. To train such a
model is computationally expensive and can take on
the order of days to train on a single machine.
Unlabeled attachment scores (Buchholz and
Marsi, 2006) are given in Figure 5. The same trends
are seen for dependency parsing that are seen for
named-entity recognition. That is, iterative param-
eter mixing learns classifiers faster and has a final
accuracy as good as or better than training serially
on all data. Again we see that the iterative parame-
ter mixing model returns a more accurate classifier
than the regular perceptron, but at about the same
level as the averaged perceptron.
5.1 Convergence Properties
Section 4.3 suggests that different weighting strate-
gies can lead to different convergence properties,
in particular with respect to the number of epochs.
For the named-entity recognition task we ran four
experiments comparing two different mixing strate-
gies ? uniform mixing (?i,n=1/S) and error mix-
ing (?i,n=ki,n/kn) ? each with two shard sizes ?
S = 10 and S = 100. Figure 6 plots the number
of training errors per epoch for each strategy.
We can make a couple observations. First, the
mixing strategy makes little difference. The rea-
son being that the number of observed errors per
epoch is roughly uniform across shards, making
both strategies ultimately equivalent. The other ob-
servation is that increasing the number of shards
can slow down convergence when viewed relative to
epochs3. Again, this appears in contradiction to the
analysis in Section 4.3, which, at least for the case
of error weighted mixtures, implied that the num-
ber of epochs to convergence was independent of
the number of shards. But that analysis was based
on worst-case scenarios where a single error occurs
on a single shard at each epoch, which is unlikely to
occur in real world data. Instead, consider the uni-
3As opposed to raw wall-clock/CPU time, which benefits
from faster epochs the more shards there are.
462
Wall Clock
0.74
0.76
0.78
0.8
0.82
0.84
Unla
beled
 Atta
chme
nt Sc
ore
Perceptron -- Serial (All Data)Perceptron -- Serial (Sub Sampling)Perceptron -- Parallel (Iterative Parameter Mix)
Wall Clock0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
Unla
beled
 Atta
chme
nt Sc
ore
Averaged Perceptron -- Serial (All Data)Averaged Perceptron -- Serial (Sub Sampling)Averaged Perceptron -- (Iterative Parameter Mix) 
Reg. Perceptron Avg. Perceptron
Unlabeled Attachment Score Unlabeled Attachment Score
Serial (All Data) 81.3 84.7
Serial (Sub Sampling) 77.2 80.1
Parallel (Iterative Parameter Mix) 83.5 84.5
Figure 5: Dependency Parsing experiments. Upper figures plot test data unlabeled attachment score versus wall clock
for both regular perceptron (left) and averaged perceptron (right). Lower table is unlabeled attachment score for
converged models.
0 10 20 30 40 50Training Epochs
0
2000
4000
6000
8000
10000
# Tra
ining
 Mist
akes
Error mixing (10 shards)Uniform mixing (10 shards)Error mixing (100 shards)Uniform mixing (100 shards)
Figure 6: Training errors per epoch for different shard
size and parameter mixing strategies.
form mixture case. Theorem 3 implies:
N?
n=1
S?
i=1
ki,n
S
?
R2
?2
=?
N?
n=1
S?
i=1
ki,n ? S ?
R2
?2
Thus, for cases where training errors are uniformly
distributed across shards, it is possible that, in the
worst-case, convergence may slow proportional the
the number of shards. This implies a trade-off be-
tween slower convergence and quicker epochs when
selecting a large number of shards. In fact, we ob-
served a tipping point for our experiments in which
increasing the number of shards began to have an ad-
verse effect on training times, which for the named-
entity experiments occurred around 25-50 shards.
This is both due to reasons described in this section
as well as the added overhead of maintaining and
summing multiple high-dimensional weight vectors
after each distributed epoch.
It is worth pointing out that a linear term S in
the convergence bound above is similar to conver-
gence/regret bounds for asynchronous distributed
online learning, which typically have bounds lin-
ear in the asynchronous delay (Mesterharm, 2005;
Zinkevich et al, 2009). This delay will be on aver-
age roughly equal to the number of shards S.
6 Conclusions
In this paper we have investigated distributing the
structured perceptron via simple parameter mixing
strategies. Our analysis shows that an iterative pa-
rameter mixing strategy is both guaranteed to sepa-
rate the data (if possible) and significantly reduces
the time required to train high accuracy classifiers.
However, there is a trade-off between increasing
training times through distributed computation and
slower convergence relative to the number of shards.
Finally, we note that using similar proofs to those
given in this paper, it is possible to provide theoreti-
cal guarantees for distributed online passive aggres-
sive learning (Crammer et al, 2006), which is a form
of large-margin perceptron learning. Unfortunately
space limitations prevent exploration here.
Acknowledgements: We thank Mehryar Mohri, Fer-
nando Periera, Mark Dredze and the three anonymous re-
views for their helpful comments on this work.
463
References
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
C.T. Chu, S.K. Kim, Y.A. Lin, Y.Y. Yu, G. Bradski, A.Y.
Ng, and K. Olukotun. 2007. Map-Reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proceedings of the Con-
ference of the Association for Computational Linguis-
tics.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithm. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive algo-
rithms. The Journal of Machine Learning Research,
7:551?585.
J. Dean and S. Ghemawat. 2004. MapReduce: Simpli-
fied data processing on large clusters. In Sixth Sym-
posium on Operating System Design and Implementa-
tion.
M. Dredze, K. Crammer, and F. Pereira. 2008.
Confidence-weighted linear classification. In Pro-
ceedings of the International Conference on Machine
learning.
J. Duchi and Y. Singer. 2009. Efficient learning using
forward-backward splitting. In Advances in Neural In-
formation Processing Systems.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In Proceedings of the Conference of the Association
for Computational Linguistics.
Y. Freund and R.E. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
J. Hajic?, B. Vidova Hladka, J. Panevova?, E. Hajic?ova?,
P. Sgall, and P. Pajas. 2001. Prague Dependency Tree-
bank 1.0. LDC, 2001T10.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proceedings of the Conference of
the Association for Computational Linguistics.
G. Mann, R. McDonald, M. Mohri, N. Silberman, and
D. Walker. 2009. Efficient large-scale distributed
training of conditional maximum entropy models. In
Advances in Neural Information Processing Systems.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Pro-
ceedings of the Conference of the European Chapter
of the Association for Computational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the Conference of the Association for
Computational Linguistics.
C. Mesterharm. 2005. Online learning with delayed la-
bel feedback. In Proceedings of Algorithmic Learning
Theory.
A.B. Novikoff. 1962. On convergence proofs on percep-
trons. In Symposium on the Mathematical Theory of
Automata.
F. Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
B. Taskar, C. Guestrin, and D. Koller. 2004. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 Shared Task: Language-
Independent Named Entity Recognition. In Proceed-
ings of the Conference on Computational Natural Lan-
guage Learning.
J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986.
Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. IEEE Transactions
on Automatic Control, 31(9):803?812.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the International Conference on Machine learning.
C. Whitelaw, A. Kehlenbeck, N. Petrovic, and L. Ungar.
2008. Web-scale named entity recognition. In Pro-
ceedings of the International Conference on Informa-
tion and Knowledge Management.
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
vestigating and combining graph-based and transition-
based dependency parsing using beam-search. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
T. Zhang. 2004. Solving large scale linear prediction
problems using stochastic gradient descent algorithms.
In Proceedings of the International Conference on Ma-
chine Learning.
M. Zinkevich, A. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems.
464
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 474?482,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning Dense Models of Query Similarity from User Click Logs
Fabio De Bona?
Friedrich Miescher Laboratory
of the Max Planck Society
Tu?bingen, Germany
fabio@tuebingen.mpg.de
Stefan Riezler
Google Research
Zu?rich, Switzerland
riezler@google.com
Keith Hall
Google Research
Zu?rich, Switzerland
kbhall@google.com
Massimiliano Ciaramita
Google Research
Zu?rich, Switzerland
massi@google.com
Amac? Herdag?delen?
University of Trento
Rovereto, Italy
amac@herdagdelen.com
Maria Holmqvist?
Linkopings University
Linkopings, Sweden
marho@ida.liu.se
Abstract
The goal of this work is to integrate query
similarity metrics as features into a dense
model that can be trained on large amounts
of query log data, in order to rank query
rewrites. We propose features that incorpo-
rate various notions of syntactic and semantic
similarity in a generalized edit distance frame-
work. We use the implicit feedback of user
clicks on search results as weak labels in train-
ing linear ranking models on large data sets.
We optimize different ranking objectives in a
stochastic gradient descent framework. Our
experiments show that a pairwise SVM ranker
trained on multipartite rank levels outperforms
other pairwise and listwise ranking methods
under a variety of evaluation metrics.
1 Introduction
Measures of query similarity are used for a wide
range of web search applications, including query
expansion, query suggestions, or listings of related
queries. Several recent approaches deploy user
query logs to learn query similarities. One set of ap-
proaches focuses on user reformulations of queries
that differ only in one phrase, e.g., Jones et al
(2006). Such phrases are then identified as candi-
date expansion terms, and filtered by various signals
such as co-occurrence in similar sessions, or log-
likelihood ratio of original and expansion phrase.
Other approaches focus on the relation of queries
and search results, either by clustering queries based
?The work presented in this paper was done while the au-
thors were visiting Google Research, Zu?rich.
on their search results, e.g., Beeferman and Berger
(2000), or by deploying the graph of queries and re-
sults to find related queries, e.g., Sahami and Heil-
man (2006).
The approach closest to ours is that of Jones et al
(2006). Similar to their approach, we create a train-
ing set of candidate query rewrites from user query
logs, and use it to train learners. While the dataset
used in Jones et al (2006) is in the order of a few
thousand query-rewrite pairs, our dataset comprises
around 1 billion query-rewrite pairs. Clearly, man-
ual labeling of rewrite quality is not feasible for our
dataset, and perhaps not even desirable. Instead, our
intent is to learn from large amounts of user query
log data. Such data permit to learn smooth mod-
els because of the effectiveness of large data sets to
capture even rare aspects of language, and they also
are available as in the wild, i.e., they reflect the ac-
tual input-output behaviour that we seek to automate
(Halevy et al, 2009). We propose a technique to au-
tomatically create weak labels from co-click infor-
mation in user query logs of search engines. The
central idea is that two queries are related if they
lead to user clicks on the same documents for a large
amount of documents. A manual evaluation of a
small subset showed that a determination of positive
versus negative rewrites by thresholding the number
of co-clicks correlates well with human judgements
of similarity, thus justifying our method of eliciting
labels from co-clicks.
Similar to Jones et al (2006), the features of our
models are not based on word identities, but instead
on general string similarity metrics. This leads to
dense rather than sparse feature spaces. The dif-
474
ference of our approach to Jones et al (2006) lies
in our particular choice of string similarity metrics.
While Jones et al (2006) deploy ?syntactic? fea-
tures such as Levenshtein distance, and ?semantic?
features such as log-likelihood ratio or mutual in-
formation, we combine syntactic and semantic as-
pects into generalized edit-distance features where
the cost of each edit operation is weighted by vari-
ous term probability models.
Lastly, the learners used in our approach are appli-
cable to very large datasets by an integration of lin-
ear ranking models into a stochastic gradient descent
framework for optimization. We compare several
linear ranking models, including a log-linear prob-
ability model for bipartite ranking, and pairwise and
listwise SVM rankers. We show in an experimen-
tal evaluation that a pairwise SVM ranker trained on
multipartite rank levels outperforms state-of-the-art
pairwise and listwise ranking methods under a vari-
ety of evaluation metrics.
2 Query Similarity Measures
2.1 Semantic measures
In several of the similarity measures we describe be-
low, we employ pointwise mutual information (PMI)
as a measure of the association between two terms or
queries. Let wi and wj be two strings that we want
to measure the amount of association between. Let
p(wi) and p(wj) be the probability of observing wi
and wj in a given model; e.g., relative frequencies
estimated from occurrence counts in a corpus. We
also define p(wi, wj) as the joint probability of wi
and wj ; i.e., the probability of the two strings occur-
ring together. We define PMI as follows:
PMI(wi, wj) = log
p(wi, wj)
p(wi)p(wj)
. (1)
PMI has been introduced by Church and Hanks
(1990) as word assosiatio ratio, and since then
been used extensively to model semantic similar-
ity. Among several desirable properties, it correlates
well with human judgments (Recchia and Jones,
2009).
2.2 Taxonomic normalizations
As pointed out in earlier work, query transitions tend
to correlate with taxonomic relations such as gener-
alization and specialization (Lau and Horvitz, 1999;
Rieh and Xie, 2006). Boldi et al (2009) show how
knowledge of transition types can positively impact
query reformulation. We would like to exploit this
information as well. However, rather than building a
dedicated supervised classifier for this task we try to
capture it directly at the source. First, we notice how
string features; e.g., length, and edit distance already
model this phenomenon to some extent, and in fact
are part of the features used in Boldi et al (2009).
However, these measures are not always accurate
and it is easy to find counterexamples both at the
term level (e.g., ?camping? to ?outdoor activities? is
a generalization) and character level (?animal pic-
tures? to ?cat pictures? is a specialization). Sec-
ondly, we propose that by manipulating PMI we can
directly model taxonomic relations to some extent.
Rather than using raw PMI values we re-
normalize them. Notice that it is not obvious in our
context how to interpret the relation between strings
co-occurring less frequently than random. Such
noisy events will yield negative PMI values since
p(wi, wj) < p(wi)p(wj). We enforce zero PMI val-
ues for such cases. If PMI is thus constrained to
non-negative values, normalization will bound PMI
to the range between 0 and 1.
The first type of normalization, called joint nor-
malization, uses the negative log joint probability
and is defined as
PMI(J)(wi, wj) = PMI(wi, wj)/?log(p(wi, wj)).
The jointly normalized PMI(J) is a symmetric
measure between wi and wj in the sense that
PMI(J)(wi, wj) = PMI(J)(wj , wi). Intuitively it
is a measure of the amount of shared information
between the two strings relative to the sum of indi-
vidual strings information. The advantages of the
joint normalization of PMI have been noticed be-
fore (Bouma, 2009).
To capture asymmetries in the relation between
two strings, we introduce two non-symmetric nor-
malizations which also bound the measure between
0 and 1. The second normalization is called special-
ization normalization and is defined as
PMI(S)(wi, wj) = PMI(wi, wj)/? log(p(wi)).
The reason we call it specialization is that PMI(S)
favors pairs where the second string is a specializa-
475
tion of the first one. For instance, PMI(S) is at its
maximum when p(wi, wj) = p(wj) and that means
the conditional probability p(wi|wj) is 1 which is an
indication of a specialization relation.
The last normalization is called the generalization
normalization and is defined in the reverse direction
as
PMI(G)(wi, wj) = PMI(wi, wj)/? log(p(wj)).
Again, PMI(G) is a measure between 0 and 1 and is
at its maximum value when p(wj |wi) is 1.
The three normalizations provide a richer rep-
resentation of the association between two strings.
Furthermore, jointly, they model in an information-
theoretic sense the generalization-specialization di-
mension directly. As an example, for the query
transition ?apple? to ?mac os? PMI(G)=0.2917 and
PMI(S)=0.3686; i.e., there is more evidence for a
specialization. Conversely for the query transition
?ferrari models? to ?ferrari? we get PMI(G)=1 and
PMI(S)=0.5558; i.e., the target is a ?perfect? gener-
alization of the source1.
2.3 Syntactic measures
Let V be a finite vocabulary and ? be the null
symbol. An edit operation: insertion, deletion or
substitution, is a pair (a, b) ? {V ? {?} ? V ?
{?}} \ {(?, ?)}. An alignment between two se-
quences wi and wj is a sequence of edit oper-
ations ? = (a1, b1), ..., (an, bn). Given a non-
negative cost function c, the cost of an alignment is
c(?) =
?n
i=1 c(?i). The Levenshtein distance, or
edit distance, defined over V , dV (wi, wj) between
two sequences is the cost of the least expensive se-
quence of edit operations which transforms wi into
wj (Levenshtein, 1966). The distance computation
can be performed via dynamic programming in time
O(|wi||wj |). Similarity at the string, i.e., character
or term, level is an indicator of semantic similar-
ity. Edit distance captures the amount of overlap be-
tween the queries as sequences of symbols and has
been previously used in information retrieval (Boldi
et al, 2009; Jones et al, 2006).
We use two basic Levenshtein distance models.
The first, called Edit1 (E1), employs a unit cost func-
tion for each of the three operations. That is, given
1The values are computed from Web counts.
a finite vocabulary T containing all terms occurring
in queries:
?a, b ? T, cE1(a, b) = 1 if(a 6= b), 0 else.
The second, called Edit2 (E2), uses unit costs for
insertion and deletion, but computes the character-
based edit distance between two terms to decide on
the substitution cost. If two terms are very similar
at the character level, then the cost of substitution is
lower. Given a finite vocabulary T of terms and a
finite vocabulary A of characters, the cost function
is defined as:
?a, b ? T, cE2(a, b) = dA(a, b) ifa ? b 6= ?, 1 else.
where dA(a, b) is linearly scaled between 0 and 1
dividing by max(|a|, |b|).
We also investigate a variant of the edit distance
algorithm in which the terms in the input sequences
are sorted, alphabetically, before the distance com-
putation. The motivation behind this variant is the
observation that linear order in queries is not always
meaningful. For example, it seems reasonable to as-
sume that ?brooklyn pizza? and ?pizza brooklyn?
denote roughly the same user intent. However, the
pair has an edit distance of two (delete-insert), while
the distance between ?brooklyn pizza? and the less
relevant ?brooklyn college? is only one (substitute).
The sorted variant relaxes the ordering constraint.
2.4 Generalized measures
In this section we extend the edit distance frame-
work introduced in Section 2.3 with the semantic
similarity measures described in Section 2.1, using
the taxonomic normalizations defined in Section 2.2.
Extending the Levenshtein distance framework
to take into account semantic similarities between
terms is conceptually simple. As in the Edit2 model
above we use a modified cost function. We introduce
a cost matrix encoding individual costs for term sub-
stitution operations; the cost is defined in terms of
the normalized PMI measures of Section 2.2, recall
that these measures range between 0 and 1. Given a
normalized similarity measure f , an entry in a cost
matrix S for a term pair (wi, wj) is defined as:
s(wi, wj) = 2? 2f(wi, wj) + 
476
We call these models SEdit (SE), where S specifies
the cost matrix used. Given a finite term vocabulary
T and cost matrix S, the cost function is defined as:
?a, b ? T, cSE(a, b) = s(a, b) ifa ? b 6= ?, 1 else.
The cost function has the following properties.
Since insertion and deletion have unit cost, a term
is substituted only if a substitution is ?cheaper? than
deleting and inserting another term, namely, if the
similarity between the terms is not zero. The 
correction, coupled with unit insertion and deletion
cost, guarantees that for an unrelated term pair a
combination of insertion and deletion will always be
less costly then a substitution. Thus in the compu-
tation of the optimal alignment, each operation cost
ranges between 0 and 2.
As a remark on efficiency, we notice that here the
semantic similarities are computed between terms,
rather than full queries. At the term level, caching
techniques can be applied more effectively to speed
up feature computation. The cost function is imple-
mented as a pre-calculated matrix, in the next sec-
tion we describe how the matrix is estimated.
2.5 Cost matrix estimation
In our experiments we evaluated two different
sources to obtain the PMI-based cost matrices. In
both cases, we assumed that the cost of the substitu-
tion of a term with itself (i.e. identity substitution)
is always 0. The first technique uses a probabilis-
tic clustering model trained on queries and clicked
documents from user query logs. The second model
estimates cost matrices directly from user session
logs, consisting of approximately 1.3 billion U.S.
English queries. A session is defined as a sequence
of queries from the same user within a controlled
time interval. Let qs and qt be a query pair observed
in the session data where qt is issued immediately
after qs in the same session. Let q?s = qs \ qt and
q?t = qt \ qs, where \ is the set difference opera-
tor. The co-occurrence count of two terms wi and
wj from a query pair qs, qt is denoted by ni,j(qs, qt)
and is defined as:
ni,j(qs, qt) =
?
?
?
1 if wi = wj ? wi ? qs ? wj ? qt
1/(|q?s| |q
?
t|) if wi ? q
?
s ? wj ? q
?
t
0 else.
In other words, if a term occurs in both queries,
it has a co-occurrence count of 1. For all other term
pairs, a normalized co-occurrence count is computed
in order to make sure the sum of co-occurrence
counts for a term wi ? qs sums to 1 for a given
query pair. The normalization is an attempt to avoid
the under representation of terms occurring in both
queries.
The final co-occurrence count of two arbitrary
terms wi and wj is denoted by Ni,j and it is defined
as the sum over all query pairs in the session logs,
Ni,j =
?
qs,qt ni,j(qs, qt). Let N =
?
wi,wj
Ni,j be
the sum of co-occurrence counts over all term pairs.
Then we define a joint probability for a term pair as
p(wi, wj) =
Ni,j
N . Similarly, we define the single-
occurrence counts and probabilities of the terms
by computing the marginalized sums over all term
pairs. Namely, the probability of a termwi occurring
in the source query is p(i, ?) =
?
wj
Ni,j/N and
similarly the probability of a term wj occurring in
the target query is p(?, j) =
?
wi
Ni,j/N . Plugging
in these values in Eq. (1), we get the PMI(wi, wj)
for term pair wi and wj , which are further normal-
ized as described in Section 2.2.
More explanation and evaluation of the features
described in this section can be found in Ciaramita
et al (2010).
3 Learning to Rank from Co-Click Data
3.1 Extracting Weak Labels from Co-Clicks
Several studies have shown that implicit feedback
from clickstream data is a weaker signal than human
relevance judgements. Joachims (2002) or Agrawal
et al (2009) presented techniques to convert clicks
into labels that can be used for machine learning.
Our goal is not to elicit relevance judgments from
user clicks, but rather to relate queries by pivoting on
commonly clicked search results. The hypothesis is
that two queries are related if they lead to user clicks
on the same documents for a large amount of docu-
ments. This approach is similar to the method pro-
posed by Fitzpatrick and Dent (1997) who attempt
to measure the relatedness between two queries by
using the normalized intersection of the top 200 re-
trieval results. We add click information to this
setup, thus strengthening the preference for preci-
sion over recall in the extraction of related queries.
477
Table 1: Statistics of co-click data sets.
train dev test
number of queries 250,000 2,500 100
average number of
rewrites per query 4,500 4,500 30
percentage of rewrites
with ? 10 coclicks 0.2 0.2 43
In our experiments we created two ground-truth
ranking scenarios from the co-click signals. In a first
scenario, called bipartite ranking, we extract a set
of positive and a set of negative query-rewrite pairs
from the user logs data. We define positive pairs as
queries that have been co-clicked with at least 10 dif-
ferent results, and negative pairs as query pairs with
fewer than 10 co-clicks. In a second scenario, called
multipartite ranking, we define a hierarchy of levels
of ?goodness?, by combining rewrites with the same
number of co-clicks at the same level, with increas-
ing ranks for higher number of co-clicks. Statistics
on the co-click data prepared for our experiments are
given in Table 1.
For training and development, we collected
query-rewrite pairs from user query logs that con-
tained at least one positive rewrite. The training set
consists of about 1 billion of query-rewrite pairs; the
development set contains 10 million query-rewrite
pairs. The average number of rewrites per query is
around 4,500 for the training and development set,
with a very small amount of 0.2% positive rewrites
per query. In order to confirm the validity of our co-
click hypothesis, and for final evaluation, we held
out another sample of query-rewrite pairs for man-
ual evaluation. This dataset contains 100 queries for
each of which we sampled 30 rewrites in descending
order of co-clicks, resulting in a high percentage of
43% positive rewrites per query. The query-rewrite
pairs were annotated by 3 raters as follows: First the
raters were asked to rank the rewrites in descend-
ing order of relevance using a graphical user inter-
face. Second the raters assigned rank labels and bi-
nary relevance scores to the ranked list of rewrites.
This labeling strategy is similar to the labeling strat-
egy for synonymy judgements proposed by Ruben-
stein and Goodenough (1965). Inter-rater agree-
ments on binary relevance judgements, and agree-
ment between rounded averaged human relevance
scores and assignments of positive/negative labels
by the co-click threshold of 10 produced a Kappa
value of 0.65 (Siegel and Castellan, 1988).
3.2 Learning-to-Rank Query Rewrites
3.2.1 Notation
Let S = {(xq, yq)}nq=1 be a training sample
of queries, each represented by a set of rewrites
xq = {xq1, . . . , xq,n(q)}, and set of rank labels
yq = {yq1, . . . , yq,n(q)}, where n(q) is the num-
ber of rewrites for query q. For full rankings of
all rewrites for a query, a total order on rewrites is
assumed, with rank labels taking on values yqi ?
{1, . . . , n(q)}. Rewrites of equivalent rank can be
specified by assuming a partial order on rewrites,
where a multipartite ranking involves r < n(q) rele-
vance levels such that yqi ? {1, . . . , r} , and a bipar-
tite ranking involves two rank values yqi ? {1, 2}
with relevant rewrites at rank 1 and non-relevant
rewrites at rank 2.
Let the rewrites in xq be identified by the integers
{1, 2, . . . , n(q)}, and let a permutation piq on xq be
defined as a bijection from {1, 2, . . . , n(q)} onto it-
self. Let ?q denote the set of all possible permuta-
tions on xq, and let piqi denote the rank position of
xqi. Furthermore, let (i, j) denote a pair of rewrites
in xq and let Pq be the set of all pairs in xq.
We associate a feature function ?(xqi) with each
rewrite i = 1, . . . , n(q) for each query q. Further-
more, a partial-order feature map as used in Yue et
al. (2007) is created for each rewrite set as follows:
?(xq, piq) =
1
|Pq|
?
(i,j)?Pq
?(xqi)??(xqj)sgn(
1
piqi
?
1
piqj
).
The goal of learning a ranking over the rewrites
xq for a query q can be achieved either by sorting the
rewrites according to the rewrite-level ranking func-
tion f(xqi) = ?w, ?(xqi)?, or by finding the permu-
tation that scores highest according to a query-level
ranking function f(xq, piq) = ?w, ?(xq, piq)?.
In the following, we will describe a variety
of well-known ranking objectives, and extensions
thereof, that are used in our experiments. Optimiza-
tion is done in a stochastic gradient descent (SGD)
framework. We minimize an empirical loss objec-
tive
min
w
?
xq ,yq
`(w)
478
by stochastic updating
wt+1 = wt ? ?tgt
where ?t is a learning rate, and gt is the gradient
gt = ?`(w)
where
?`(w) =
?
?
?w1
`(w),
?
?w2
`(w), . . . ,
?
?wn
`(w)
?
.
3.2.2 Listwise Hinge Loss
Standard ranking evaluation metrics such as
(Mean) Average Precision (Manning et al, 2008)
are defined on permutations of whole lists and are
not decomposable over instances. Joachims (2005),
Yue et al (2007), or Chakrabarti et al (2008) have
proposed multivariate SVM models to optimize such
listwise evaluation metrics. The central idea is to
formalize the evaluation metric as a prediction loss
function L, and incorporate L via margin rescal-
ing into the hinge loss function, such that an up-
per bound on the prediction loss is achieved (see
Tsochantaridis et al (2004), Proposition 2).
The loss function is given by the following list-
wise hinge loss:
`lh(w) = (L(yq, pi
?
q )?
?
w, ?(xq, yq)? ?(xq, pi
?
q )
?
)+
where pi?q is the maximizer of the
maxpiq??q\yq L(yq, pi
?
q ) +
?
w, ?(xq, pi?q )
?
ex-
pression, (z)+ = max{0, z} and L(yq, piq) ? [0, 1]
denotes a prediction loss of a predicted ranking piq
compared to the ground-truth ranking yq.2
In this paper, we use Average Precision (AP) as
prediction loss function s.t.
LAP (yq, piq) = 1?AP (yq, piq)
where AP is defined as follows:
AP (yq, piq) =
?n(q)
j=1 Prec(j) ? (|yqj ? 2|)
?n(q)
j=1 (|yqj ? 2|)
,
P rec(j) =
?
k:piqk?piqj
(|yqk ? 2|)
piqj
.
2We slightly abuse the notation yq to denote the permutation
on xq that is induced by the rank labels. In case of full rankings,
the permutation piq corresponding to ranking yq is unique. For
multipartite and bipartite rankings, there is more than one pos-
sible permutation for a given ranking, so that we let piq denote
a permutation that is consistent with ranking yq .
Note that the ranking scenario is in this case bipartite
with yqi ? {1, 2}.
The derivatives for `lh are as follows:
?
?wk
`lh =
?
?
?
0 if
(?
w, ?(xq, yq)? ?(xq, pi?q )
?)
> L(yq, pi?q ),
?(?k(xq, yq)? ?k(xq, pi?q )) else.
SGD optimization involves computing pi?q for each
feature and each query, which can be done effi-
ciently using the greedy algorithm proposed by Yue
et al (2007). We will refer to this method as the
SVM-MAP model.
3.2.3 Pairwise Hinge Loss for Bipartite and
Multipartite Ranking
Joachims (2002) proposed an SVM method that
defines the ranking problem as a pairwise classifi-
cation problem. Cortes et al (2007) extended this
method to a magnitude-preserving version by penal-
izing a pairwise misranking by the magnitude of the
difference in preference labels. A position-sensitive
penalty for pairwise ranking SVMs was proposed
by Riezler and De Bona (2009) and Chapelle and
Keerthi (2010), and earlier for perceptrons by Shen
and Joshi (2005). In the latter approaches, the mag-
nitude of the difference in inverted ranks is accrued
for each misranked pair. The idea is to impose an
increased penalty for misrankings at the top of the
list, and for misrankings that involve a difference of
several rank levels.
Similar to the listwise case, we can view the
penalty as a prediction loss function, and incor-
porate it into the hinge loss function by rescaling
the margin by a pairwise prediction loss function
L(yqi, yqj). In our experiments we used a position-
sensitive prediction loss function
L(yqi, yqj) = |
1
yqi
?
1
yqj
|
defined on the difference of inverted ranks. The
margin-rescaled pairwise hinge loss is then defined
as follows:
`ph(w) =
?
(i,j)?Pq
(L(yqi, yqj)?
?w, ?(xqi)? ?(xqj)? sgn(
1
yqi
?
1
yqj
))+
479
Table 2: Experimental evaluation of random and best feature baselines, and log-linear, SVM-MAP, SVM-bipartite,
SVM-multipartite, and SVM-multipartite-margin-rescaled learning-to-rank models on manually labeled test set.
MAP NDCG@10 AUC Prec@1 Prec@3 Prec@5
Random 51.8 48.7 50.4 45.6 45.6 46.6
Best-feature 71.9 70.2 74.5 70.2 68.1 68.7
SVM-bipart. 73.7 73.7 74.7 79.4 70.1 70.1
SVM-MAP 74.3 75.2 75.3 76.3 71.8 72.0
Log-linear 74.7 75.1 75.7 75.3 72.2 71.3
SVM-pos.-sens. 75.7 76.0 76.6 82.5 72.9 73.0
SVM-multipart. 76.5 77.3 77.2 83.5 74.2 73.6
The derivative of `ph is calculated as follows:
?
?wk
`lp =
?
????
????
0 if (?w, ?(xqi)? ?(xqj)?
sgn( 1yqi ?
1
yqj
)) > L(yqi, yqj),
?(?k(xqi)? ?k(xqj))sgn( 1yqi ?
1
yqj
)
else.
Note that the effect of inducing a position-
sensitive penalty on pairwise misrankings applies
only in case of full rankings on n(q) rank levels,
or in case of multipartite rankings involving 2 <
r < n(q) rank levels. Henceforth we will refer to
margin-rescaled pairwise hinge loss for multipartite
rankings as the SVM-pos.-sens. method.
Bipartite ranking is a special case where
L(yqi, yqj) is constant so that margin rescaling does
not have the effect of inducing position-sensitivity.
This method will be referred to as the SVM-bipartite
model.
Also note that for full ranking or multipartite
ranking, predicting a low ranking for an instance
that is ranked high in the ground truth has a domino
effect of accruing an additional penalty at each
rank level. This effect is independent of margin-
rescaling. The method of pairwise hinge loss
for multipartite ranking with constant margin will
henceforth be referred to as the SVM-multipartite
model.
Computation in SGD optimization is dominated
by the number of pairwise comparisons |Pq| for
each query. For full ranking, a comparison of
|Pq| =
(n(q)
2
)
pairs has to be done. In the case
of multipartite ranking at r rank levels, each in-
cluding |li| rewrites, pairwise comparisons between
rewrites at the same rank level can be ignored.
This reduces the number of comparisons to |Pq| =
?r?1
i=1
?r
j=i+1 |li||lj |. For bipartite ranking of p
positive and n negative instances, |Pq| = p ? n com-
parisons are necessary.
3.2.4 Log-linear Models for Bipartite Ranking
A probabilistic model for bipartite ranking can be
defined as the conditional probability of the set of
relevant rewrites, i.e., rewrites at rank level 1, given
all rewrites at rank levels 1 and 2. A formalization in
the family of log-linear models yields the following
logistic loss function `llm that was used for discrim-
inative estimation from sets of partially labeled data
in Riezler et al (2002):
`llm(w) = ? log
?
xqi?xq |yqi=1
e?w,?(xqi)?
?
xqi?xq
e?w,?(xqi)?
.
The gradient of `llm is calculated as a difference be-
tween two expectations:
?
?wk
`llm = ?pw [?k|xq; yqi = 1] + pw [?k|xq] .
The SGD computation for the log-linear model is
dominated by the computation of expectations for
each query. The logistic loss for bipartite ranking is
henceforth referred to as the log-linear model.
4 Experimental Results
In the experiments reported in this paper, we trained
linear ranking models on 1 billion query-rewrite
pairs using 60 dense features, combined of the build-
ing blocks of syntactic and semantic similarity met-
rics under different estimations of cost matrices. De-
velopment testing was done on a data set that was
held-out from the training set. Final testing was car-
ried out on the manually labeled dataset. Data statis-
tics for all sets are given in Table 1.
480
Table 3: P-values computed by approximate randomization test for 15 pairwise comparisons of result differences.
Best-feature SVM-bipart. SVM-MAP Log-linear SVM-pos.-sens. SVM-multipart.
Best-feature - < 0.005 < 0.005 < 0.005 < 0.005 < 0.005
SVM-bipart. - - 0.324 < 0.005 < 0.005 < 0.005
SVM-MAP - - - 0.374 < 0.005 < 0.005
Log-linear - - - - 0.053 < 0.005
SVM-pos.-sens. - - - - - < 0.005
SVM-multipart. - - - - - -
Model selection was performed by adjusting
meta-parameters on the development set. We
trained each model at constant learning rates ? ?
{1, 0.5, 0.1, 0.01, 0.001}, and evaluated each variant
after every fifth out of 100 passes over the training
set. The variant with the highest MAP score on the
development set was chosen and evaluated on the
test set. This early stopping routine also served for
regularization.
Evaluation results for the systems are reported in
Table 2. We evaluate all models according to the fol-
lowing evaluation metrics: Mean Average Precision
(MAP), Normalized Discounted Cumulative Gain
with a cutoff at rank 10 (NDCG@10), Area-under-
the-ROC-curve (AUC), Precision@n3. As baselines
we report a random permutation of rewrites (ran-
dom), and the single dense feature that performed
best on the development set (best-feature). The latter
is the log-probability assigned to the query-rewrite
pair by the probabilistic clustering model used for
cost matrix estimation (see Section 2.5). P-values
are reported in Table 3 for all pairwise compar-
isons of systems (except the random baseline) us-
ing an Approximate Randomization test where strat-
ified shuffling is applied to results on the query level
(see Noreen (1989)). The rows in Tables 2 and 3
are ranked according to MAP values of the systems.
SVM-multipartite outperforms all other ranking sys-
tems under all evaluation metrics at a significance
level ? 0.995. For all other pairwise comparisons
of result differences, we find result differences of
systems ranked next to each other to be not statis-
tically significant. All systems outperform the ran-
dom and best-feature baselines with statistically sig-
nificant result differences. The distinctive advantage
of the SVM-multipartite models lies in the possibil-
3For a definition of these metrics see Manning et al (2008)
ity to rank rewrites with very high co-click num-
bers even higher than rewrites with reasonable num-
bers of co-clicks. This preference for ranking the
top co-clicked rewrites high seems the best avenue
for transferring co-click information to the human
judgements encoded in the manually labeled test set.
Position-sensitive margin rescaling does not seem to
help, but rather seems to hurt.
5 Discussion
We presented an approach to learn rankings of query
rewrites from large amounts of user query log data.
We showed how to use the implicit co-click feed-
back about rewrite quality in user log data to train
ranking models that perform well on ranking query
rewrites according to human quality standards. We
presented large-scale experiments using SGD opti-
mization for linear ranking models. Our experimen-
tal results show that an SVM model for multipartite
ranking outperforms other linear ranking models un-
der several evaluation metrics. In future work, we
would like to extend our approach to other models,
e.g., sparse combinations of lexicalized features.
References
R. Agrawal, A. Halverson, K. Kenthapadi, N. Mishra,
and P. Tsaparas. 2009. Generating labels from clicks.
In Proceedings of the 2nd ACM International Con-
ference on Web Search and Data Mining, Barcelona,
Spain.
Doug Beeferman and Adam Berger. 2000. Agglom-
erative clustering of a search engine query log. In
Proceedings of the 6th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing (KDD?00), Boston, MA.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese cakes?: Query reformula-
481
tion models and patterns. In Proceedings of Web Intel-
ligence. IEEE Cs Press.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. In Proceedings of
GSCL.
Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and
Chiru Bhattacharayya. 2008. Structured learning for
non-smooth ranking losses. In Proceedings of the 14th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), Las Vegas, NV.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 16(1):22?29.
Massimiliano Ciaramita, Amac? Herdag?delen, Daniel
Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler,
and Enrique Alfonseca. 2010. Generalized syntactic
and semantic models of query reformulation. In Pro-
ceedings of the 33rd ACM SIGIR Conference, Geneva,
Switzerland.
Corinna Cortes, Mehryar Mohri, and Asish Rastogi.
2007. Magnitude-preserving ranking algorithms. In
Proceedings of the 24th International Conference on
Machine Learning (ICML?07), Corvallis, OR.
Larry Fitzpatrick and Mei Dent. 1997. Automatic feed-
back using past queries: Social searching? In Pro-
ceedings of the 20th Annual International ACM SIGIR
Conference, Philadelphia, PA.
Alon Halevy, Peter Norvig, and Fernando Pereira. 2009.
The unreasonable effectiveness of data. IEEE Intelli-
gent Systems, 24:8?12.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the 8th
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD?08), New York, NY.
Thorsten Joachims. 2005. A support vector method for
multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine Learn-
ing (ICML?05), Bonn, Germany.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th International World Wide Web
conference (WWW?06), Edinburgh, Scotland.
T. Lau and E. Horvitz. 1999. Patterns of search: analyz-
ing and modeling web query refinement. In Proceed-
ings of the seventh international conference on User
modeling, pages 119?128. Springer-Verlag New York,
Inc.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
G. Recchia and M.N. Jones. 2009. More data trumps
smarter algorithms: comparing pointwise mutual in-
formation with latent semantic analysis. Behavioral
Research Methods, 41(3):647?656.
S.Y. Rieh and H. Xie. 2006. Analysis of multiple query
reformulations on the web: the interactive information
retrieval context. Inf. Process. Manage., 42(3):751?
768.
Stefan Riezler and Fabio De Bona. 2009. Simple risk
bounds for position-sensitive max-margin ranking al-
gorithms. In Proceedings of the Workshop on Ad-
vances in Ranking at the 23rd Annual Conference
on Neural Information Processing Systems (NIPS?09),
Whistler, Canada.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communications
of the ACM, 10(3):627?633.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th Inter-
national World Wide Web conference (WWW?06), Ed-
inburgh, Scotland.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Sidney Siegel and John Castellan. 1988. Nonparametric
Statistics for the Behavioral Sciences. Second Edition.
MacGraw-Hill, Boston, MA.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the 21st International
Conference on Machine Learning (ICML?04), Banff,
Canada.
Yisong Yue, Thomas Finley, Filip Radlinski, and
Thorsten Joachims. 2007. A support vector method
for optimizing average precision. In Proceedings of
the 30th Annual International ACM SIGIR Confer-
ence, Amsterdam, The Netherlands.
482
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 440?449,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Beam-Width Prediction for Efficient Context-Free Parsing
Nathan Bodenstab? Aaron Dunlop? Keith Hall? and Brian Roark?
? Center for Spoken Language Understanding, Oregon Health & Science University, Portland, OR
?Google, Inc., Zurich, Switzerland
{bodensta,dunlopa,roark}@cslu.ogi.edu kbhall@google.com
Abstract
Efficient decoding for syntactic parsing has
become a necessary research area as statisti-
cal grammars grow in accuracy and size and
as more NLP applications leverage syntac-
tic analyses. We review prior methods for
pruning and then present a new framework
that unifies their strengths into a single ap-
proach. Using a log linear model, we learn
the optimal beam-search pruning parameters
for each CYK chart cell, effectively predicting
the most promising areas of the model space
to explore. We demonstrate that our method
is faster than coarse-to-fine pruning, exempli-
fied in both the Charniak and Berkeley parsers,
by empirically comparing our parser to the
Berkeley parser using the same grammar and
under identical operating conditions.
1 Introduction
Statistical constituent parsers have gradually in-
creased in accuracy over the past ten years. This
accuracy increase has opened the door to automati-
cally derived syntactic information within a number
of NLP tasks. Prior work incorporating parse struc-
ture into machine translation (Chiang, 2010) and Se-
mantic Role Labeling (Tsai et al, 2005; Punyakanok
et al, 2008) indicate that such hierarchical structure
can have great benefit over shallow labeling tech-
niques like chunking and part-of-speech tagging.
Although syntax is becoming increasingly impor-
tant for large-scale NLP applications, constituent
parsing is slow ? too slow to scale to the size of
many potential consumer applications. The exhaus-
tive CYK algorithm has computational complexity
O(n3|G|) where n is the length of the sentence and
|G| is the number of grammar productions, a non-
negligible constant. Increases in accuracy have pri-
marily been accomplished through an increase in
the size of the grammar, allowing individual gram-
mar rules to be more sensitive to their surround-
ing context, at a considerable cost in efficiency.
Grammar transformation techniques such as linguis-
tically inspired non-terminal annotations (Johnson,
1998; Klein and Manning, 2003b) and latent vari-
able grammars (Matsuzaki et al, 2005; Petrov et al,
2006) have increased the grammar size |G| from a
few thousand rules to several million in an explic-
itly enumerable grammar, or even more in an im-
plicit grammar. Exhaustive search for the maximum
likelihood parse tree with a state-of-the-art grammar
can require over a minute of processing for a sin-
gle sentence of 25 words, an unacceptable amount
of time for real-time applications or when process-
ing millions of sentences. Deterministic algorithms
for dependency parsing exist that can extract syntac-
tic dependency structure very quickly (Nivre, 2008),
but this approach is often undesirable as constituent
parsers are more accurate and more adaptable to new
domains (Petrov et al, 2010).
The most accurate constituent parsers, e.g., Char-
niak (2000), Petrov and Klein (2007a), make use
of approximate inference, limiting their search to
a fraction of the total search space and achieving
speeds of between one and four newspaper sen-
tences per second. The paradigm for building state-
of-the-art parsing models is to first design a model
structure that can achieve high accuracy and then,
after the model has been built, design effective ap-
proximate inference methods around that particu-
lar model; e.g., coarse-to-fine non-terminal hierar-
chies for a given model, or agenda-based methods
440
that are empirically tuned to achieve acceptable ef-
ficiency/accuracy operating points. While both of
the above mentioned papers use the CYK dynamic
programming algorithm to search through possible
solutions, their particular methods of approximate
inference are quite distinct.
In this paper, we examine a general approach to
approximate inference in constituent parsing that
learns cell-specific thresholds for arbitrary gram-
mars. For each cell in the CYK chart, we sort all
potential constituents in a local agenda, ordered by
an estimate of their posterior probability. Given fea-
tures extracted from the chart cell context ? e.g.,
span width; POS-tags and words surrounding the
boundary of the cell ? we train a log linear model
to predict how many constituents should be popped
from the local agenda and added to the chart. As
a special case of this approach, we simply pre-
dict whether the number to add should be zero or
greater than zero, in which case the method can be
seen as a cell-by-cell generalization of Roark and
Hollingshead?s (2008; 2009) tagger-derived Chart
Constraints. More generally, instead of a binary
classification decision, we can also use this method
to predict the desired cell population directly and
get cell closure for free when the classifier predicts
a beam-width of zero. In addition, we use a non-
symmetric loss function during optimization to ac-
count for the imbalance between over-predicting or
under-predicting the beam-width.
A key feature of our approach is that it does
not rely upon reference syntactic annotations when
learning to search. Rather, the beam-width predic-
tion model is trained to learn the rank of constituents
in the maximum likelihood trees.1 We will illus-
trate this by presenting results using a latent-variable
grammar, for which there is no ?true? reference la-
tent variable parse. We simply parse sections 2-21
of the WSJ treebank and train our search models
from the output of these trees, with no prior knowl-
edge of the non-terminal set or other grammar char-
acteristics to guide the process. Hence, this ap-
1Note that we do not call this method ?unsupervised? be-
cause all grammars used in this paper are induced from super-
vised data, although our framework can also accommodate un-
supervised grammars. We emphasize that we are learning to
search using only maximum likelihood trees, not that we are
doing unsupervised parsing.
Figure 1: Inside (grey) and outside (white) representations of
an example chart edge Ni,j .
proach is broadly applicable to a wide range of sce-
narios, including tuning the search to new domains
where domain mismatch may yield very different ef-
ficiency/accuracy operating points.
In the next section, we present prior work on
approximate inference in parsing, and discuss how
our method to learn optimal beam-search param-
eters unite many of their strengths into a single
framework. We then explore using our approach to
open or close cells in the chart as an alternative to
Roark and Hollingshead (2008; 2009). Finally, we
present results which combine cell closure and adap-
tive beam-width prediction to achieve the most effi-
cient parser.
2 Background
2.1 Preliminaries and notation
Let S = w1 . . . w|S| represent an input string of
|S| words. Let wi,j denote the substring from word
wi+1 to wj ; i.e., S = w0,|S|. We use the term chart
edge to refer to a non-terminal spanning a specific
substring of the input sentence. Let Ni,j denote the
edge labeled with non-terminalN spanning wi,j , for
example NP3,7. We define an edge?s figure-of-merit
(FOM) as an estimate of the product of its inside
(?) and outside (?) scores, conceptually the relative
merit the edge has to participate in the final parse
tree (see Figure 1). More formally:
?(Ni,j) = P (w0,i, Ni,j , wj,n)
?(Ni,j) = P (wi,j |N)
FOM(Ni,j) = ??(Ni,j)??(Ni,j)
441
With bottom-up parsing, the true inside probability
is accumulated and ?(Ni,j) does not need to be esti-
mated, improving the FOMs ability to represent the
true inside/outside distribution.
In this paper, we use a modified version of the
Caraballo and Charniak Boundary FOM (1998)
for local edge comparison, which computes ??(Ni,j)
using POS forward-backward scores and POS-to-
nonterminal constituent boundary transition proba-
bilities. Details can be found in (?).
We also note that in this paper we only use
the FOM scoring function to rank constituents in
a local agenda. Alternative approaches to rank-
ing competitors are also possible, such as Learning
as Search Optimization (Daume? and Marcu, 2005).
The method we present in this paper to learn the op-
timal beam-search parameters is applicable to any
ranking function, and we demonstrate this by com-
puting results with both the Boundary FOM and
only the inside probability in Section 6.
2.2 Agenda-based parsing
Agenda-based parsers maintain a global agenda of
edges, ranked by FOM score. At each iteration, the
highest-scoring edge is popped off of the agenda,
added to the chart, and combined with other edges
already in the chart. The agenda-based approach
includes best-first parsing (Bobrow, 1990) and A*
parsing (Klein and Manning, 2003a), which differ
in whether an admissible FOM estimate ??(Ni,j) is
required. A* uses an admissible FOM, and thus
guarantees finding the maximum likelihood parse,
whereas an inadmissible heuristic (best-first) may
require less exploration of the search space. Much
work has been pursued in both admissible and in-
admissible heuristics for agenda parsing (Caraballo
and Charniak, 1998; Klein and Manning, 2003a;
Pauls et al, 2010).
In this paper, we also make use of agendas, but
at a local rather than a global level. We maintain an
agenda for each cell, which has two significant ben-
efits: 1) Competing edges can be compared directly,
avoiding the difficulty inherent in agenda-based ap-
proaches of comparing edges of radically differ-
ent span lengths and characteristics; and 2) Since
the agendas are very small, the overhead of agenda
maintenance ? a large component of agenda-based
parse time ? is minimal.
2.3 Beam-search parsing
CYK parsing with a beam-search is a local pruning
strategy, comparing edges within the same chart cell.
The beam-width can be defined in terms of a thresh-
old in the number of edges allowed, or in terms of
a threshold on the difference in probability relative
to the highest scoring edge (Collins, 1999; Zhang et
al., 2010). For the current paper, we use both kinds
of thresholds, avoiding pathological cases that each
individual criteria is prone to encounter. Further, un-
like most beam-search approaches we will make use
of a FOM estimate of the posterior probability of an
edge, defined above, as our ranking function. Fi-
nally, we will learn log linear models to assign cell-
specific thresholds, rather than relying on a single
search parameter.
2.4 Coarse-to-Fine Parsing
Coarse-to-fine parsing, also known as multiple pass
parsing (Goodman, 1997; Charniak, 2000; Char-
niak and Johnson, 2005), first parses the input sen-
tence with a simplified (coarse) version of the tar-
get (fine) grammar in which multiple non-terminals
are merged into a single state. Since the coarse
grammar is quite small, parsing is much faster than
with the fine grammar, and can quickly yield an es-
timate of the outside probability ?(?) for use in sub-
sequent agenda or beam-search parsing with the fine
grammar. This approach can also be used iteratively
with grammars of increasing complexity (Petrov and
Klein, 2007a).
Building a coarse grammar from a fine gram-
mar is a non-trivial problem, and most often ap-
proached with detailed knowledge of the fine gram-
mar being used. For example, Goodman (1997)
suggests using a coarse grammar consisting of reg-
ular non-terminals, such as NP and VP, and then
non-terminals augmented with head-word informa-
tion for the more accurate second-pass grammar.
Such an approach is followed by Charniak (2000) as
well. Petrov and Klein (2007a) derive coarse gram-
mars in a more statistically principled way, although
the technique is closely tied to their latent variable
grammar representation.
To the extent that our cell-specific threshold clas-
sifier predicts that a chart cell should contain zero
edges or more than zero edges, it is making coarse
442
predictions about the unlabeled constituent structure
of the target parse tree. This aspect of our work is
can be viewed as a coarse-to-fine process, though
without considering specific grammatical categories
or rule productions.
2.5 Chart Constraints
Roark and Hollingshead (2008; 2009) introduced
a pruning technique that ignores entire chart cells
based on lexical and POS features of the input sen-
tence. They train two finite-state binary taggers:
one that allows multi-word constituents to start at
a word, and one that allows constituents to end at a
word. Given these tags, it is straightforward to com-
pletely skip many chart cells during processing.
In this paper, instead of tagging word positions to
infer valid constituent spans, we classify chart cells
directly. We further generalize this cell classification
to predict the beam-width of the chart cell, where a
beam-width of zero indicates that the cell is com-
pletely closed. We discuss this in detail in the next
section.
3 Open/Closed Cell Classification
3.1 Constituent Closure
We first look at the binary classification of chart cells
as either open or closed to full constituents, and pre-
dict this value from the input sentence alone. This
is the same problem that Roark and Hollingshead
(2008; 2009) solve with Chart Constraints; however,
where they classify lexical items as either beginning
or ending a constituent, we classify individual chart
cells as open or closed, an approach we call Con-
stituent Closure. Although the number of classifi-
cations scales quadratically with our approach, the
total parse time is still dominated by the O(n3|G|)
parsing complexity and we find that the added level
of specificity reduces the search space significantly.
To learn to classify a chart cell spanning words
wi+1 . . . wj of a sentence S as open or closed to full
constituents, we first map cells in the training corpus
to tuples:
?(S, i, j) = (x, y) (1)
where x is a feature-vector representation of the
chart cell and y is the target class 1 if the cell con-
tains an edge from the maximum likelihood parse
tree, 0 otherwise. The feature vector x is encoded
with the chart cell?s absolute and relative span width,
as well as unigram and bigram lexical and part-of-
speech tag items from wi?1 . . . wj+2.
Given feature/target tuples (x, y) for every chart
cell in every sentence of a training corpus ? , we train
a weight vector ? using the averaged perceptron al-
gorithm (Collins, 2002) to learn an open/closed bi-
nary decision boundary:
?? = argmin
?
?
(x,y)??(?)
L?(H(? ? x), y) (2)
where H(?) is the unit step function: 1 if the inner
product ? ?x > 0, and 0 otherwise; and L?(?, ?) is an
asymmetric loss function, defined below.
When predicting cell closure, all misclassifica-
tions are not equal. If we leave open a cell which
contains no edges in the maximum likelihood (ML)
parse, we incur the cost of additional processing, but
are still able to recover the ML tree. However, if we
close a chart cell which contains an ML edge, search
errors occur. To deal with this imbalance, we intro-
duce an asymmetric loss functionL?(?, ?) to penalize
false-negatives more severely during training.
L?(h, y) =
?
??
??
0 if h = y
1 if h > y
? if h < y
(3)
We found the value ? = 102 to give the best per-
formance on our development set, and we use this
value in all of our experiments.
Figures 2a and 2b compare the pruned charts of
Chart Constraints and Constituent Closure for a sin-
gle sentence in the development set. Note that both
of these methods are predicting where a complete
constituent may be located in the chart, not partial
constituents headed by factored nonterminals within
a binarized grammar. Depending on the grammar
factorization (right or left) we can infer chart cells
that are restricted to only edges with a factored left-
hand-side non-terminal. In Figure 2 these chart cells
are colored gray. Note that Constituent Closure re-
duces the number of completely open cells consider-
ably vs. Chart Constraints, and the number of cells
open to factored categories somewhat.
443
3.2 Complete Closure
Alternatively, we can predict whether a chart cell
contains any edge, either a partial or a full con-
stituent, an approach we call Complete Closure.
This is a more difficult classification problem as par-
tial constituents occur in a variety of contexts. Nev-
ertheless, learning this directly allows us to remove a
large number of internal chart cells from considera-
tion, since no additional cells need to be left open to
partial constituents. The learning algorithm is iden-
tical to Equation 2, but training examples are now
assigned a positive label if the chart cell contains any
edge from the binarized maximum likelihood tree.
Figure 2c gives a visual representation of Complete
Closure for the same sentence; the number of com-
pletely open cells increases somewhat, but the total
number of open cells (including those open to fac-
tored categories) is greatly reduced.
We compare the effectiveness of Constituent Clo-
sure, Complete Closure, and Chart Constraints, by
decreasing the percentage of chart cells closed un-
til accuracy over all sentences in our development
set start to decline. For Constituent and Complete
Closure, we also vary the loss function, adjusting
the relative penalty between a false-negative (clos-
ing off a chart cell that contains a maximum like-
lihood edge) and a false-positive. Results show that
using Chart Constrains as a baseline, we prune (skip)
33% of the total chart cells. Constituent Closure im-
proves on this baseline only slightly (36%), but we
see our biggest gains with Complete Closure, which
prunes 56% of all chart cells in the development set.
All of these open/closed cell classification meth-
ods can improve the efficiency of the exhaustive
CYK algorithm, or any of the approximate infer-
ence methods mentioned in Section 2. We empir-
ically evaluate them when applied to CYK parsing
and beam-search parsing in Section 6.
4 Beam-Width Prediction
The cell-closing approaches discussed in Section 3
make binary decisions to either allow or completely
block all edges in each cell. This all-on/all-off tactic
ignores the characteristics of the local cell popula-
tion, which, given a large statistical grammar, may
contain hundred of edges, even if very improbable.
Retaining all of these partial derivations forces the
(a) Chart Constraints (Roark and Hollingshead, 2009)
(b) Constituent Closure (this paper)
(c) Complete Closure (this paper)
Figure 2: Comparison of Chart Constraints (Roark and
Hollingshead, 2009) to Constituent and Complete Closure for a
single example sentence. Black cells are open to all edges while
grey cells only allow factored edges (incomplete constituents).
search in larger spans to continue down improbable
paths, adversely affecting efficiency. We can further
improve parsing speed in these open cells by lever-
aging local pruning methods, such as beam-search.
When parsing with a beam-search, finding the op-
timal beam-width threshold(s) to balance speed and
accuracy is a necessary step. As mentioned in Sec-
444
tion 2.3, two variations of the beam-width are of-
ten considered: a fixed number of allowed edges,
or a relative probability difference from the highest
scoring local edge. For the remainder of this pa-
per we fix the relative probability threshold for all
experiments and focus on adapting the number of
allowed edges per cell. We will refer to this number-
of-allowed-edges value as the beam-width, notated
by b, and leave adaptation of the relative probability
difference to future work.
The standard way to tune the beam-width is a sim-
ple sweep over possible values until accuracy on
a heldout data set starts to decline. The optimal
point will necessarily be very conservative, allowing
outliers (sentences or sub-phrases with above aver-
age ambiguity) to stay within the beam and produce
valid parse trees. The majority of chart cells will
require much fewer than b entries to find the max-
imum likelihood (ML) edge, yet, constrained by a
constant beam-width, the cell will continue to be
filled with unfruitful edges, exponentially increasing
downstream computation.
For example, when parsing with the Berkeley
latent-variable grammar and Boundary FOM, we
find we can reduce the global beam-width b to 15
edges in each cell before accuracy starts to decline.
However we find that 73% of the ML edges are
ranked first in their cell and 96% are ranked in the
top three. Thus, in 24 of every 25 cells, 80% of the
edges are unnecessary (12 of the top 15). Clearly,
it would be advantageous to adapt the beam-width
such that it is restrictive when we are confident in
the FOM ranking and more forgiving in ambiguous
contexts.
To address this problem, we learn the optimal
beam-width for each chart cell directly. We define
Ri,j as the rank of the ML edge in the chart cell
spanning wi+1 . . . wj . If no ML edge exists in the
cell, then Ri,j = 0. Given a global maximum beam-
width b, we train b different binary classifiers, each
using separate mapping functions ?k, where the tar-
get value y produced by ?k is 1 if Ri,j > k and 0
otherwise.
The same asymmetry noted in Section 3 applies
in this task as well. When in doubt, we prefer to
over-predict the beam-width and risk an increase in
processing time opposed to under-predicting at the
expense of accuracy. Thus we use the same loss
function L?, this time training several classifiers:
??k = argmin
?
?
(x,y)??k(?)
L?(H(? ? x), y) (4)
Note that in Equation 4 when k = 0, we re-
cover the open/closed cell classification of Equa-
tion 2, since a beam width of 0 indicates that the
chart cell is completely closed.
During decoding, we assign the beam-width
for chart cell spanning wi+1 . . . wj given models
?0, ?1, ...?b?1 by finding the lowest value k such that
the binary classifier ?k classifiesRi,j ? k. If no such
k exists, R?i,j is set to the maximum beam-width
value b:
R?i,j = argmin
k
?k ? xi ? 0 (5)
In Equation 5 we assume there are b unique clas-
sifiers, one for each possible beam-width value be-
tween 0 and b? 1, but this level of granularity is not
required. Choosing the number of classification bins
to minimize total parsing time is dependent on the
FOM function and how it ranks ML edges. With the
Boundary FOM we use in this paper, 97.8% of ML
edges have a local rank less than five and we find that
the added cost of computing b decision boundaries
for each cell is not worth the added specificity. We
searched over possible classification bins and found
that training four classifiers with beam-width deci-
sion boundaries at 0, 1, 2, and 4 is faster than 15 in-
dividual classifiers and more memory efficient, since
each model ?k has over 800,000 parameters. All
beam-width prediction results reported in this paper
use these settings.
Figure 3 is a visual representation of beam-width
prediction on a single sentence of the development
set using the Berkeley latent-variable grammar and
Boundary FOM. In this figure, the gray scale repre-
sents the relative size of the beam-width, black being
the maximum beam-width value, b, and the lightest
gray being a beam-width of size one. We can see
from this figure that very few chart cells are classi-
fied as needing the full 15 edges, apart from span-1
cells which we do not classify.
445
Figure 3: Visualization of Beam-Width Prediction for a single example sentence. The grey scale represents the size of the predicted
beam-width: white is 0 (cell is skipped) and black is the maximum value b (b=15 in this example).
5 Experimental Setup
We run all experiments on the WSJ treebank (Mar-
cus et al, 1999) using the standard splits: section
2-21 for training, section 22 for development, and
section 23 for testing. We preprocess the treebank
by removing empty nodes, temporal labels, and spu-
rious unary productions (X?X), as is standard in
published works on syntactic parsing.
The pruning methods we present in this paper can
be used to parse with any grammar. To achieve state-
of-the-art accuracy levels, we parse with the Berke-
ley SM6 latent-variable grammar (Petrov and Klein,
2007b) where the original treebank non-terminals
are automatically split into subclasses to optimize
parsing accuracy. This is an explicit grammar con-
sisting of 4.3 million productions, 2.4 million of
which are lexical productions. Exhaustive CYK
parsing with the grammar takes more than a minute
per sentence.
Accuracy is computed from the 1-best Viterbi
(max) tree extracted from the chart. Alternative de-
coding methods, such as marginalizing over the la-
tent variables in the grammar or MaxRule decod-
ing (Petrov and Klein, 2007a) are certainly possible
in our framework, but it is unknown how effective
these methods will be given the heavily pruned na-
ture of the chart. We leave investigation of this to
future work. We compute the precision and recall
of constituents from the 1-best Viterbi trees using
the standard EVALB script (?), which ignores punc-
tuation and the root symbol. Accuracy results are
reported as F-measure (F1), the harmonic mean be-
tween precision and recall.
We ran all timing tests on an Intel 3.00GHz pro-
cessor with 6MB of cache and 16GB of memory.
Our parser is written in Java and publicly available
at http://nlp.csee.ogi.edu.
6 Results
We empirically demonstrate the advantages of our
pruning methods by comparing the total parse time
of each system, including FOM initialization, chart
cell classification, and beam-width prediction. The
parse times reported for Chart Constraints do not in-
clude tagging times as we were provided with this
pre-tagged data, but tagging all of Section 22 takes
less than three seconds and we choose to ignore this
contribution for simplicity.
Figure 4 contains a timing comparison of the three
components of our final parser: Boundary FOM ini-
tialization (which includes the forward-backward al-
gorithm over ambiguous part-of-speech tags), beam-
446
Figure 4: Timing breakdown by sentence length for major
components of our parser.
width prediction, and the final beam-search, includ-
ing 1-best extraction. We bin these relative times
with respect to sentence length to see how each com-
ponent scales with the number of input words. As
expected, theO(n3|G|) beam-search begins to dom-
inate as the sentence length grows, but Boundary
FOM initialization is not cheap, and absorbs, on
average, 20% of the total parse time. Beam-width
prediction, on the other hand, is almost negligible
in terms of processing time even though it scales
quadratically with the length of the sentence.
We compare the accuracy degradation of beam-
width prediction and Chart Constraints in Figure 5
as we incrementally tighten their respective prun-
ing parameters. We also include the baseline beam-
search parser with Boundary FOM in this figure
to demonstrate the accuracy/speed trade-off of ad-
justing a global beam-width alone. In this figure
we see that the knee of the beam-width prediction
curve (Beam-Predict) extends substantially further
to the left before accuracy declines, indicating that
our pruning method is intelligently removing a sig-
nificant portion of the search space that remains un-
pruned with Chart Constraints.
In Table 1 we present the accuracy and parse time
for three baseline parsers on the development set:
exhaustive CYK parsing, beam-search parsing using
only the inside score ?(?), and beam-search parsing
using the Boundary FOM. We then apply our two
cell-closing methods, Constituent Closure and Com-
plete Closure, to all three baselines. As expected,
the relative speedup of these methods across the var-
ious baselines is similar since the open/closed cell
classification does not change across parsers. We
Figure 5: Time vs. accuracy curves comparing beam-width
prediction (Beam-Predict) and Chart Constraints.
also see that Complete Closure is between 22% and
31% faster than Constituent Closure, indicating that
the greater number of cells closed translates directly
into a reduction in parse time. We can further apply
beam-width prediction to the two beam-search base-
line parsers in Table 1. Dynamically adjusting the
beam-width for the remaining open cells decreases
parse time by an additional 25% when using the In-
side FOM, and 28% with the boundary FOM.
We apply our best model to the test set and report
results in Table 2. Beam-width prediction, again,
outperforms the baseline of a constant beam-width
by 65% and the open/closed classification of Chart
Constraints by 49%. We also compare beam-width
prediction to the Berkeley Coarse-to-Fine parser.
Both our parser and the Berkeley parser are written
in Java, both are run with Viterbi decoding, and both
parse with the same grammar, so a direct compari-
son of speed and accuracy is fair.2
7 Conclusion and Future Work
We have introduced three new pruning methods, the
best of which unites figure-of-merit estimation from
agenda-based parsing, local pruning from beam-
search parsing, and unlabeled constituent structure
2We run the Berkeley parser with the default search param-
eterization to achieve the fastest possible parsing time. We note
that 3 of 2416 sentences fail to parse under these settings. Using
the ?-accurate? option provides a valid parse for all sentences,
but increases parsing time of section 23 to 0.293 seconds per
sentence with no increase in F-score. We assume a back-off
strategy for failed parses could be implemented to parse all sen-
tences with a parsing time close to the default parameterization.
447
Parser Sec/Sent F1
CYK 70.383 89.4
CYK + Constituent Closure 47.870 89.3
CYK + Complete Closure 32.619 89.3
Beam + Inside FOM (BI) 3.977 89.2
BI + Constituent Closure 2.033 89.2
BI + Complete Closure 1.575 89.3
BI + Beam-Predict 1.180 89.3
Beam + Boundary FOM (BB) 0.326 89.2
BB + Constituent Closure 0.279 89.2
BB + Complete Closure 0.199 89.3
BB + Beam-Predict 0.143 89.3
Table 1: Section 22 development set results for CYK and
Beam-Search (Beam) parsing using the Berkeley latent-variable
grammar.
prediction from coarse-to-fine parsing and Chart
Constraints. Furthermore, our pruning method is
trained using only maximum likelihood trees, allow-
ing it to be tuned to specific domains without labeled
data. Using this framework, we have shown that we
can decrease parsing time by 65% over a standard
beam-search without any loss in accuracy, and parse
significantly faster than both the Berkeley parser and
Chart Constraints.
We plan to explore a number of remaining ques-
tions in future work. First, we will try combin-
ing our approach with constituent-level Coarse-to-
Fine pruning. The two methods prune the search
space in very different ways and may prove to be
complementary. On the other hand, our parser cur-
rently spends 20% of the total parse time initializing
the FOM, and adding additional preprocessing costs,
such as parsing with a coarse grammar, may not out-
weigh the benefits gained in the final search.
Second, as with Chart Constraints we do not
prune lexical or unary edges in the span-1 chart cells
(i.e., chart cells that span a single word). We ex-
pect pruning entries in these cells would notably re-
duce parse time since they cause exponentially many
chart edges to be built in larger spans. Initial work
constraining span-1 chart cells has promising results
(Bodenstab et al, 2011) and we hope to investigate
its interaction with beam-width prediction even fur-
ther.
Parser Sec/Sent F1
CYK 64.610 88.7
Berkeley CTF MaxRule 0.213 90.2
Berkeley CTF Viterbi 0.208 88.8
Beam + Boundary FOM (BB) 0.334 88.6
BB + Chart Constraints 0.244 88.7
BB + Beam-Predict (this paper) 0.125 88.7
Table 2: Section 23 test set results for multiple parsers using
the Berkeley latent-variable grammar.
Finally, the size and structure of the grammar is
the single largest contributor to parse efficiency. In
contrast to the current paradigm, we plan to inves-
tigate new algorithms that jointly optimize accuracy
and efficiency during grammar induction, leading to
more efficient decoding.
Acknowledgments
We would like to thank Kristy Hollingshead for
her valuable discussions, as well as the anony-
mous reviewers who gave very helpful feedback.
This research was supported in part by NSF Grants
#IIS-0447214, #IIS-0811745 and DARPA grant
#HR0011-09-1-0041. Any opinions, findings, con-
clusions or recommendations expressed in this pub-
lication are those of the authors and do not necessar-
ily reflect the views of the NSF or DARPA.
References
Robert J. Bobrow. 1990. Statistical agenda parsing. In
DARPA Speech and Language Workshop, pages 222?
224.
Nathan Bodenstab, Kristy Hollingshead, and Brian
Roark. 2011. Unary constraints for efficient context-
free parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics,
Portland, Oregon.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24:275?298.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 173?
180, Ann Arbor, Michigan.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st North American
448
chapter of the Association for Computational Linguis-
tics conference, pages 132?139, Seattle, Washington.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proceedings of the 48rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 1443?1452.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. PhD dissertation, Uni-
versity of Pennsylvania.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical Methods in
Natural Language Processing, volume 10, pages 1?8,
Philadelphia.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In Proceedings of the
22nd international conference on Machine learning,
ICML ?05, pages 169?176, New York, NY, USA.
Joshua Goodman. 1997. Global thresholding and
Multiple-Pass parsing. Proceedings of the Second
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 11?25.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Christopher D. Manning. 2003a. A* pars-
ing. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy (NAACL ?03), pages 40?47, Edmonton, Canada.
Dan Klein and Christopher D. Manning. 2003b. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan.
Mitchell P Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3,
Philadelphia.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics - ACL ?05, pages
75?82, Ann Arbor, Michigan.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Comput. Linguist.,
34:513?553.
Adam Pauls, Dan Klein, and Chris Quirk. 2010. Top-
down k-best a* parsing. In In proceedings of the An-
nual Meeting on Association for Computational Lin-
guistics Short Papers, ACLShort ?10, pages 200?204,
Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007a. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages 404?
411, Rochester, New York.
Slav Petrov and Dan Klein. 2007b. Learning and in-
ference for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, pages 433?440, Syd-
ney, Australia.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705?713, Cambridge, MA,
October.
Vasin Punyakanok, Dan Roth, and Wen tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Donia Scott and Hans Uszkoreit, editors,
Proceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 745?
752, Manchester, UK.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity Context-Free parsing pipelines via chart
constraints. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 647?655, Boulder, Colorado.
Tzong-Han Tsai, Chia-Wei Wu, Yu-Chun Lin, and Wen-
Lian Hsu. 2005. Exploiting full parsing information
to label semantic roles using an ensemble of ME and
SVM via integer linear programming. In Proceed-
ings of the Ninth Conference on Computational Natu-
ral Language Learning, CONLL ?05, pages 233?236,
Morristown, NJ, USA.
Yue Zhang, Byung gyu Ahn, Stephen Clark, Curt Van
Wyk, James R. Curran, and Laura Rimell. 2010.
Chart pruning for fast Lexicalised-Grammar parsing.
In Proceedings of the 23rd International Conference
on Computational Linguistics, pages 1472?1479, Bei-
jing, China.
449
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Using Search-Logs to Improve Query Tagging
Kuzman Ganchev Keith Hall Ryan McDonald Slav Petrov
Google, Inc.
{kuzman|kbhall|ryanmcd|slav}@google.com
Abstract
Syntactic analysis of search queries is im-
portant for a variety of information-retrieval
tasks; however, the lack of annotated data
makes training query analysis models diffi-
cult. We propose a simple, efficient proce-
dure in which part-of-speech tags are trans-
ferred from retrieval-result snippets to queries
at training time. Unlike previous work, our
final model does not require any additional re-
sources at run-time. Compared to a state-of-
the-art approach, we achieve more than 20%
relative error reduction. Additionally, we an-
notate a corpus of search queries with part-
of-speech tags, providing a resource for future
work on syntactic query analysis.
1 Introduction
Syntactic analysis of search queries is important for
a variety of tasks including better query refinement,
improved matching and better ad targeting (Barr
et al, 2008). However, search queries differ sub-
stantially from traditional forms of written language
(e.g., no capitalization, few function words, fairly
free word order, etc.), and are therefore difficult
to process with natural language processing tools
trained on standard corpora (Barr et al, 2008). In
this paper we focus on part-of-speech (POS) tagging
queries entered into commercial search engines and
compare different strategies for learning from search
logs. The search logs consist of user queries and
relevant search results retrieved by a search engine.
We use a supervised POS tagger to label the result
snippets and then transfer the tags to the queries,
producing a set of noisy labeled queries. These la-
beled queries are then added to the training data and
the tagger is retrained. We evaluate different strate-
gies for selecting which annotation to transfer and
find that using the result that was clicked by the user
gives comparable performance to using just the top
result or to aggregating over the top-k results.
The most closely related previous work is that of
Bendersky et al (2010, 2011). In their work, un-
igram POS tag priors generated from a large cor-
pus are blended with information from the top-50
results from a search engine at prediction time. Such
an approach has the disadvantage that it necessitates
access to a search engine at run-time and is com-
putationally very expensive. We re-implement their
method and show that our direct transfer approach is
more effective, while being simpler to instrument:
since we use information from the search engine
only during training, we can train a stand-alone POS
tagger that can be run without access to additional
resources. We also perform an error analysis and
find that most of the remaining errors are due to er-
rors in POS tagging of the snippets.
2 Direct Transfer
The main intuition behind our work, Bendersky et
al. (2010) and Ru?d et al (2011), is that standard NLP
annotation tools work better on snippets returned by
a search engine than on user supplied queries. This
is because snippets are typically well-formed En-
glish sentences, while queries are not. Our goal is to
leverage this observation and use a supervised POS
tagger trained on regular English sentences to gen-
erate annotations for a large set of queries that can
be used for training a query-specific model. Perhaps
the simplest approach ? but also a surprisingly pow-
erful one ? is to POS tag some relevant snippets for
238
a given query, and then to transfer the tags from the
snippet tokens to matching query tokens. This ?di-
rect? transfer idea is at the core of all our experi-
ments. In this work, we provide a comparison of
techniques for selecting snippets associated with the
query, as well as an evaluation of methods for align-
ing the matching words in the query to those in the
selected snippets.
Specifically, for each query1 with a corresponding
set of ?relevant snippets,? we first apply the baseline
tagger to the query and all the snippets. We match
any query terms in these snippets, and copy over the
POS tag to the matching query term. Note that this
can produce multiple labelings as the relevant snip-
pet set can be very diverse and varies even for the
same query. We choose the most frequent tagging
as the canonical one and add it to our training set.
We then train a query tagger on all our training data:
the original human annotated English sentences and
also the automatically generated query training set.
The simplest way to match query tokens to snip-
pet tokens is to allow a query token to match any
snippet token. This can be problematic when we
have queries that have a token repeated with differ-
ent parts-of-speech such as in ?tie a tie.? To make a
more precise matching we try a sequence of match-
ing rules: First, exact match of the query n-gram.
Then matching the terms in order, so the query ?tiea
a tieb? matched to the snippet ?to tie1 a neck tie2?
would match tiea:tie1 and tieb:tie2. Finally, we
match as many query terms as possible. An early
observation showed that when a query term occurs
in the result URL, e.g., searching for ?irs mileage
rate? results in the page irs.gov, the query term
matching the URL domain name is usually a proper
noun. Consequently we add this rule.
In the context of search logs, a relevant snippet
set can refer to the top k snippets (including the case
where k = 1) or the snippet(s) associated with re-
sults clicked by users that issued the query. In our
experiments we found that different strategies for se-
lecting relevant snippets, such as selecting the snip-
pets of the clicked results, using the top-10 results
or using only the top result, perform similarly (see
Table 1).
1We skip navigational queries, e.g, amazon or amazon.com,
since syntactic analysis of such queries is not useful.
Query budget/NN rent/VB a/DET car/NN Clicks
Snip 1 . . . Budget/NNP Rent/NNP 2
A/NNP Car/NNP . . .
Snip 2 . . . Go/VB to/TO Budget/NNP 1
to/TO rent/VB a/DET car/NN . . .
Snip 3 . . . Rent/VB a/DET car/NN 1
from/IN Budget/NNP . . .
Figure 1: Example query and snippets as tagged by a
baseline tagger as well as associated clicks.
By contrast Bendersky et al (2010) use a lin-
ear interpolation between a prior probability and the
snippet tagging. They define pi(t|w) as the relative
frequency of tag t given by the baseline tagger to
word w in some corpus and ?(t|w, s) as the indica-
tor function for word w in the context of snippet s
has tag t. They define the tagging of a word as
argmax
t
0.2pi(t|w) + 0.8mean
s:w?s
?(t|w, s) (1)
We illustrate the difference between the two ap-
proaches in Figure 1. The numbered rows of the
table correspond to three snippets (with non-query
terms elided). The strategy that uses the clicks to se-
lect the tagging would count two examples of ?Bud-
get/NNP Rent/NNP A/NNP Car/NNP? and one for
each of two other taggings. Note that snippet 1
and the query get different taggings primarily due
to orthographic variations. It would then add ?bud-
get/NNP rent/NNP a/NNP car/NNP? to its training
set. The interpolation approach of Bendersky et al
(2010) would tag the query as ?budget/NNP rent/VB
a/DET car/NN?. To see why this is the case, consider
the probability for rent/VB vs rent/NNP. For rent/VB
we have 0.2 + 0.8? 23 , while for rent/NNP we have
0 + 0.8? 13 assuming that pi(VB|rent) = 1.
3 Experimental Setup
We assume that we have access to labeled English
sentences from the PennTreebank (Marcus et al,
1993) and the QuestionBank (Judge et al, 2006), as
well as large amounts of unlabeled search queries.
Each query is paired with a set of relevant results
represented by snippets (sentence fragments con-
taining the search terms), as well as information
about the order in which the results were shown to
the user and possibly the result the user clicked on.
Note that different sets of results are possible for the
239
same query, because of personalization and ranking
changes over time.
3.1 Evaluation Data
We use two data sets for evaluation. The first is the
set of 251 queries from Microsoft search logs (MS-
251) used in Bendersky et al (2010, 2011). The
queries are annotated with three POS tags represent-
ing nouns, verbs and ?other? tags (MS-251 NVX).
We additionally refine the annotation to cover 14
POS tags comprising the 12 universal tags of Petrov
et al (2012), as well as proper nouns and a special
tag for search operator symbols such as ?-? (for
excluding the subsequent word). We refer to this
evaluation set as MS-251 in our experiments. We
had two annotators annotate the whole of the MS-
251 data set. Before arbitration, the inter-annotator
agreement was 90.2%. As a reference, Barr et al
(2008) report 79.3% when annotating queries with
19 POS tags. We then examined all the instances
where the annotators disagreed, and corrected
the discrepancy. Our annotations are available at
http://code.google.com/p/query-syntax/.
The second evaluation set consists of 500 so
called ?long-tail? queries. These are queries that oc-
curred rarely in the search logs, and are typically
difficult to tag because they are searching for less-
frequent information. They do not contain naviga-
tional queries.
3.2 Baseline Model
We use a linear chain tagger trained with the aver-
aged perceptron (Collins, 2002). We use the follow-
ing features for our tagger: current word, suffixes
and prefixes of length 1 to 3; additionally we use
word cluster features (Uszkoreit and Brants, 2008)
for the current word, and transition features of the
cluster of the current and previous word. When
training on Sections 1-18 of the Penn Treebank
and testing on sections 22-24, our tagger achieves
97.22% accuracy with the Penn Treebank tag set,
which is state-of-the-art for this data set. When we
evaluate only on the 14 tags used in our experiments,
the accuracy increases to 97.88%.
We experimented with 4 baseline taggers (see Ta-
ble 2). WSJ corresponds to training on only the
standard training sections of Wall Street Journal por-
tion of the Penn Treebank. WSJ+QTB adds the
Method
MS-251
NVX
MS-251 long-tail
DIRECT-CLICK 93.43 84.11 78.15
DIRECT-ALL 93.93 84.39 77.73
DIRECT-TOP-1 93.93 84.60 77.60
Table 1: Evaluation of snippet selection strategies.
QuestionBank as training data. WSJ NOCASE and
WSJ+QTB NOCASE use case-insensitive version of
the tagger (conceptually lowercasing the text before
training and before applying the tagger). As we will
see, all our baseline models are better than the base-
line reported in Bendersky et al (2010); our lower-
cased baseline model significantly outperforms even
their best model.
4 Experiments
First, we compared different strategies for selecting
relevant snippets from which to transfer the tags.
These systems are: DIRECT-CLICK, which uses
snippets clicked on by users; DIRECT-ALL, which
uses all the returned snippets seen by the user;2
and DIRECT-TOP-1, which uses just the snippet in
the top result. Table 1 compares these systems on
our three evaluation sets. While DIRECT-ALL and
DIRECT-TOP-1 perform best on the MS-251 data
sets, DIRECT-CLICK has an advantage on the long
tail queries. However, these differences are small
(<0.6%) suggesting that any strategy for selecting
relevant snippet sets will return comparable results
when aggregated over large amounts of data.
We then compared our method to the baseline
models and a re-implementation of Bendersky et al
(2010), which we denote BSC. We use the same
matching scheme for both BSC and our system, in-
cluding the URL matching described in Section 2.
The URL matching improves performance by 0.4-
3.0% across all models and evaluation settings.
Table 2 summarizes our final results. For com-
parison, Bendersky et al (2010) report 91.6% for
their final system, which is comparable to our im-
plementation of their system when the baseline tag-
ger is trained on just the WSJ corpus. Our best sys-
tem achieves a 21.2% relative reduction in error on
their annotations. Some other trends become appar-
2Usually 10 results, but more if the user viewed the second
page of results.
240
Method
MS-251
NVX
MS-251 long-tail
WSJ 90.54 75.07 53.06
BSC 91.74 77.82 57.65
DIRECT-CLICK 93.36 85.81 76.13
WSJ + QTB 90.18 74.86 53.48
BSC 91.74 77.54 57.65
DIRECT-CLICK 93.01 85.03 76.97
WSJ NOCASE 92.87 81.92 74.31
BSC 93.71 84.32 76.63
DIRECT-CLICK 93.50 84.46 77.48
WSJ + QTB NOCASE 93.08 82.70 74.65
BSC 93.57 83.90 77.27
DIRECT-CLICK 93.43 84.11 78.15
Table 2: Tagging accuracies for different baseline settings
and two transfer methods.DIRECT-CLICK is the approach
we propose (see text). Column MS-251 NVX evaluates
with tags from Bendersky et al (2010). Their baseline
is 89.3% and they report 91.6% for their method. MS-
251 and Long-tail use tags from Section 3.1. We observe
snippets for 2/500 long-tail queries and 31/251 MS-251
queries.
ent in Table 2. Firstly, a large part of the benefit of
transfer has to do with case information that is avail-
able in the snippets but is missing in the query. The
uncased tagger is insensitive to this mismatch and
achieves significantly better results than the cased
taggers. However, transferring information from the
snippets provides additional benefits, significantly
improving even the uncased baseline taggers. This
is consistent with the analysis in Barr et al (2008).
Finally, we see that the direct transfer method from
Section 2 significantly outperforms the method de-
scribed in Bendersky et al (2010). Table 3 confirms
this trend when focusing on proper nouns, which are
particularly difficult to identify in queries.
We also manually examined a set of 40 queries
with their associated snippets, for which our best
DIRECT-CLICK system made mistakes. In 32 cases,
the errors in the query tagging could be traced back
to errors in the snippet tagging. A better snippet
tagger could alleviate that problem. In the remain-
ing 8 cases there were problems with the matching
? either the mis-tagged word was not found at all,
or it was matched incorrectly. For example one of
the results for the query ?bell helmet? had a snippet
containing ?Bell cycling helmets? and we failed to
match helmet to helmets.
Method P R F
WSJ + QTB NOCASE 72.12 79.80 75.77
BSC 82.87 69.05 75.33
BSC + URL 83.01 70.80 76.42
DIRECT-CLICK 79.57 76.51 78.01
DIRECT-ALL 75.88 78.38 77.11
DIRECT-TOP-1 78.38 76.40 77.38
Table 3: Precision and recall of the NNP tag on the long-
tail data for the best baseline method and the three trans-
fer methods using that baseline.
5 Related Work
Barr et al (2008) manually annotate a corpus of
2722 queries with 19 POS tags and use it to train
and evaluate POS taggers, and also describe the lin-
guistic structures they find. Unfortunately their data
is not available so we cannot use it to compare to
their results. Ru?d et al (2011) create features based
on search engine results, that they use in an NER
system applied to queries. They report report sig-
nificant improvements when incorporating features
from the snippets. In particular, they exploit capital-
ization and query terms matching URL components;
both of which we have used in this work. Li et al
(2009) use clicks in a product data base to train a tag-
ger for product queries, but they do not use snippets
and do not annotate syntax. Li (2010) and Manshadi
and Li (2009) also work on adding tags to queries,
but do not use snippets or search logs as a source of
information.
6 Conclusions
We described a simple method for training a search-
query POS tagger from search-logs by transfer-
ring context from relevant snippet sets to query
terms. We compared our approach to previous work,
achieving an error reduction of 20%. In contrast to
the approach proposed by Bendersky et al (2010),
our approach does not require access to the search
engine or index when tagging a new query. By ex-
plicitly re-training our final model, it has the ability
to pool knowledge from several related queries and
incorporate the information into the model param-
eters. An area for future work is to transfer other
syntactic information, such as parse structures or su-
pertags using a similar transfer approach.
241
References
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English web-search queries.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1021?1030, Honolulu, Hawaii, October. Association
for Computational Linguistics.
M. Bendersky, W.B. Croft, and D.A. Smith. 2010.
Structural annotation of search queries using pseudo-
relevance feedback. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 1537?1540. ACM.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of EMNLP.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: Creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, pages 497?504, Sydney, Australia, July.
Association for Computational Linguistics.
X. Li, Y.Y. Wang, and A. Acero. 2009. Extracting
structured information from user queries with semi-
supervised conditional random fields. In Proceedings
of the 32nd international ACM SIGIR conference on
Research and development in information retrieval,
pages 572?579. ACM.
X. Li. 2010. Understanding the semantic structure of
noun phrase queries. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 1337?1345. Association for Com-
putational Linguistics.
M. Manshadi and X. Li. 2009. Semantic tagging of web
search queries. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 2-Volume 2, pages
861?869. Association for Computational Linguistics.
M. P. Marcus, Mary Ann Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated corpus of
English: the Penn treebank. Computational Linguis-
tics, 19.
S. Petrov, D. Das, and R. McDonald. 2012. A universal
part-of-speech tagset. In Proc. of LREC.
Stefan Ru?d, Massimiliano Ciaramita, Jens Mu?ller, and
Hinrich Schu?tze. 2011. Piggyback: Using search en-
gines for robust cross-domain named entity recogni-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 965?975, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
J. Uszkoreit and T. Brants. 2008. Distributed word clus-
tering for large scale class-based language modeling in
machine translation. In Proc. of ACL.
242
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92?97,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Universal Dependency Annotation for Multilingual Parsing
Ryan McDonald? Joakim Nivre?? Yvonne Quirmbach-Brundage? Yoav Goldberg??
Dipanjan Das? Kuzman Ganchev? Keith Hall? Slav Petrov? Hao Zhang?
Oscar Ta?ckstro?m?? Claudia Bedini? Nu?ria Bertomeu Castello?? Jungmee Lee?
Google, Inc.? Uppsala University? Appen-Butler-Hill? Bar-Ilan University?
Contact: ryanmcd@google.com
Abstract
We present a new collection of treebanks
with homogeneous syntactic dependency
annotation for six languages: German,
English, Swedish, Spanish, French and
Korean. To show the usefulness of such a
resource, we present a case study of cross-
lingual transfer parsing with more reliable
evaluation than has been possible before.
This ?universal? treebank is made freely
available in order to facilitate research on
multilingual dependency parsing.1
1 Introduction
In recent years, syntactic representations based
on head-modifier dependency relations between
words have attracted a lot of interest (Ku?bler et
al., 2009). Research in dependency parsing ? com-
putational methods to predict such representations
? has increased dramatically, due in large part to
the availability of dependency treebanks in a num-
ber of languages. In particular, the CoNLL shared
tasks on dependency parsing have provided over
twenty data sets in a standardized format (Buch-
holz and Marsi, 2006; Nivre et al, 2007).
While these data sets are standardized in terms
of their formal representation, they are still hetero-
geneous treebanks. That is to say, despite them
all being dependency treebanks, which annotate
each sentence with a dependency tree, they sub-
scribe to different annotation schemes. This can
include superficial differences, such as the renam-
ing of common relations, as well as true diver-
gences concerning the analysis of linguistic con-
structions. Common divergences are found in the
1Downloadable at https://code.google.com/p/uni-dep-tb/.
analysis of coordination, verb groups, subordinate
clauses, and multi-word expressions (Nilsson et
al., 2007; Ku?bler et al, 2009; Zeman et al, 2012).
These data sets can be sufficient if one?s goal
is to build monolingual parsers and evaluate their
quality without reference to other languages, as
in the original CoNLL shared tasks, but there are
many cases where heterogenous treebanks are less
than adequate. First, a homogeneous represen-
tation is critical for multilingual language tech-
nologies that require consistent cross-lingual anal-
ysis for downstream components. Second, consis-
tent syntactic representations are desirable in the
evaluation of unsupervised (Klein and Manning,
2004) or cross-lingual syntactic parsers (Hwa et
al., 2005). In the cross-lingual study of McDonald
et al (2011), where delexicalized parsing models
from a number of source languages were evalu-
ated on a set of target languages, it was observed
that the best target language was frequently not the
closest typologically to the source. In one stun-
ning example, Danish was the worst source lan-
guage when parsing Swedish, solely due to greatly
divergent annotation schemes.
In order to overcome these difficulties, some
cross-lingual studies have resorted to heuristics to
homogenize treebanks (Hwa et al, 2005; Smith
and Eisner, 2009; Ganchev et al, 2009), but we
are only aware of a few systematic attempts to
create homogenous syntactic dependency anno-
tation in multiple languages. In terms of auto-
matic construction, Zeman et al (2012) attempt
to harmonize a large number of dependency tree-
banks by mapping their annotation to a version of
the Prague Dependency Treebank scheme (Hajic?
et al, 2001; Bo?hmova? et al, 2003). Addition-
ally, there have been efforts to manually or semi-
manually construct resources with common syn-
92
tactic analyses across multiple languages using al-
ternate syntactic theories as the basis for the repre-
sentation (Butt et al, 2002; Helmreich et al, 2004;
Hovy et al, 2006; Erjavec, 2012).
In order to facilitate research on multilingual
syntactic analysis, we present a collection of data
sets with uniformly analyzed sentences for six lan-
guages: German, English, French, Korean, Span-
ish and Swedish. This resource is freely avail-
able and we plan to extend it to include more data
and languages. In the context of part-of-speech
tagging, universal representations, such as that of
Petrov et al (2012), have already spurred numer-
ous examples of improved empirical cross-lingual
systems (Zhang et al, 2012; Gelling et al, 2012;
Ta?ckstro?m et al, 2013). We aim to do the same for
syntactic dependencies and present cross-lingual
parsing experiments to highlight some of the bene-
fits of cross-lingually consistent annotation. First,
results largely conform to our expectations of
which target languages should be useful for which
source languages, unlike in the study of McDon-
ald et al (2011). Second, the evaluation scores
in general are significantly higher than previous
cross-lingual studies, suggesting that most of these
studies underestimate true accuracy. Finally, un-
like all previous cross-lingual studies, we can re-
port full labeled accuracies and not just unlabeled
structural accuracies.
2 Towards A Universal Treebank
The Stanford typed dependencies for English
(De Marneffe et al, 2006; de Marneffe and Man-
ning, 2008) serve as the point of departure for our
?universal? dependency representation, together
with the tag set of Petrov et al (2012) as the under-
lying part-of-speech representation. The Stanford
scheme, partly inspired by the LFG framework,
has emerged as a de facto standard for depen-
dency annotation in English and has recently been
adapted to several languages representing different
(and typologically diverse) language groups, such
as Chinese (Sino-Tibetan) (Chang et al, 2009),
Finnish (Finno-Ugric) (Haverinen et al, 2010),
Persian (Indo-Iranian) (Seraji et al, 2012), and
Modern Hebrew (Semitic) (Tsarfaty, 2013). Its
widespread use and proven adaptability makes it a
natural choice for our endeavor, even though ad-
ditional modifications will be needed to capture
the full variety of grammatical structures in the
world?s languages.
Alexandre re?side avec sa famille a` Tinqueux .
NOUN VERB ADP DET NOUN ADP NOUN P
NSUBJ ADPMOD
ADPOBJ
POSS
ADPMOD
ADPOBJ
P
Figure 1: A sample French sentence.
We use the so-called basic dependencies (with
punctuation included), where every dependency
structure is a tree spanning all the input tokens,
because this is the kind of representation that most
available dependency parsers require. A sample
dependency tree from the French data set is shown
in Figure 1. We take two approaches to generat-
ing data. The first is traditional manual annotation,
as previously used by Helmreich et al (2004) for
multilingual syntactic treebank construction. The
second, used only for English and Swedish, is to
automatically convert existing treebanks, as in Ze-
man et al (2012).
2.1 Automatic Conversion
Since the Stanford dependencies for English are
taken as the starting point for our universal annota-
tion scheme, we begin by describing the data sets
produced by automatic conversion. For English,
we used the Stanford parser (v1.6.8) (Klein and
Manning, 2003) to convert the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993) to basic dependency trees, including punc-
tuation and with the copula verb as head in cop-
ula constructions. For Swedish, we developed a
set of deterministic rules for converting the Tal-
banken part of the Swedish Treebank (Nivre and
Megyesi, 2007) to a representation as close as pos-
sible to the Stanford dependencies for English.
This mainly consisted in relabeling dependency
relations and, due to the fine-grained label set used
in the Swedish Treebank (Teleman, 1974), this
could be done with high precision. In addition,
a small number of constructions required struc-
tural conversion, notably coordination, which in
the Swedish Treebank is given a Prague style anal-
ysis (Nilsson et al, 2007). For both English and
Swedish, we mapped the language-specific part-
of-speech tags to universal tags using the map-
pings of Petrov et al (2012).
2.2 Manual Annotation
For the remaining four languages, annotators were
given three resources: 1) the English Stanford
93
guidelines; 2) a set of English sentences with Stan-
ford dependencies and universal tags (as above);
and 3) a large collection of unlabeled sentences
randomly drawn from newswire, weblogs and/or
consumer reviews, automatically tokenized with a
rule-based system. For German, French and Span-
ish, contractions were split, except in the case of
clitics. For Korean, tokenization was more coarse
and included particles within token units. Annota-
tors could correct this automatic tokenization.
The annotators were then tasked with producing
language-specific annotation guidelines with the
expressed goal of keeping the label and construc-
tion set as close as possible to the original English
set, only adding labels for phenomena that do not
exist in English. Making fine-grained label dis-
tinctions was discouraged. Once these guidelines
were fixed, annotators selected roughly an equal
amount of sentences to be annotated from each do-
main in the unlabeled data. As the sentences were
already randomly selected from a larger corpus,
annotators were told to view the sentences in or-
der and to discard a sentence only if it was 1) frag-
mented because of a sentence splitting error; 2) not
from the language of interest; 3) incomprehensible
to a native speaker; or 4) shorter than three words.
The selected sentences were pre-processed using
cross-lingual taggers (Das and Petrov, 2011) and
parsers (McDonald et al, 2011).
The annotators modified the pre-parsed trees us-
ing the TrEd2 tool. At the beginning of the annota-
tion process, double-blind annotation, followed by
manual arbitration and consensus, was used itera-
tively for small batches of data until the guidelines
were finalized. Most of the data was annotated
using single-annotation and full review: one an-
notator annotating the data and another reviewing
it, making changes in close collaboration with the
original annotator. As a final step, all annotated
data was semi-automatically checked for annota-
tion consistency.
2.3 Harmonization
After producing the two converted and four an-
notated data sets, we performed a harmonization
step, where the goal was to maximize consistency
of annotation across languages. In particular, we
wanted to eliminate cases where the same label
was used for different linguistic relations in dif-
ferent languages and, conversely, where one and
2Available at http://ufal.mff.cuni.cz/tred/.
the same relation was annotated with different la-
bels, both of which could happen accidentally be-
cause annotators were allowed to add new labels
for the language they were working on. Moreover,
we wanted to avoid, as far as possible, labels that
were only used in one or two languages.
In order to satisfy these requirements, a number
of language-specific labels were merged into more
general labels. For example, in analogy with the
nn label for (element of a) noun-noun compound,
the annotators of German added aa for compound
adjectives, and the annotators of Korean added vv
for compound verbs. In the harmonization step,
these three labels were merged into a single label
compmod for modifier in compound.
In addition to harmonizing language-specific la-
bels, we also renamed a small number of relations,
where the name would be misleading in the uni-
versal context (although quite appropriate for En-
glish). For example, the label prep (for a mod-
ifier headed by a preposition) was renamed adp-
mod, to make clear the relation to other modifier
labels and to allow postpositions as well as prepo-
sitions.3 We also eliminated a few distinctions in
the original Stanford scheme that were not anno-
tated consistently across languages (e.g., merging
complm with mark, number with num, and purpcl
with advcl).
The final set of labels is listed with explanations
in Table 1. Note that relative to the universal part-
of-speech tagset of Petrov et al (2012) our final
label set is quite rich (40 versus 12). This is due
mainly to the fact that the the former is based on
deterministic mappings from a large set of annota-
tion schemes and therefore reduced to the granu-
larity of the greatest common denominator. Such a
reduction may ultimately be necessary also in the
case of dependency relations, but since most of our
data sets were created through manual annotation,
we could afford to retain a fine-grained analysis,
knowing that it is always possible to map from
finer to coarser distinctions, but not vice versa.4
2.4 Final Data Sets
Table 2 presents the final data statistics. The num-
ber of sentences, tokens and tokens/sentence vary
3Consequently, pobj and pcomp were changed to adpobj
and adpcomp.
4The only two data sets that were created through con-
version in our case were English, for which the Stanford de-
pendencies were originally defined, and Swedish, where the
native annotation happens to have a fine-grained label set.
94
Label Description
acomp adjectival complement
adp adposition
adpcomp complement of adposition
adpmod adpositional modifier
adpobj object of adposition
advcl adverbial clause modifier
advmod adverbial modifier
amod adjectival modifier
appos appositive
attr attribute
aux auxiliary
auxpass passive auxiliary
cc conjunction
ccomp clausal complement
Label Description
compmod compound modifier
conj conjunct
cop copula
csubj clausal subject
csubjpass passive clausal subject
dep generic
det determiner
dobj direct object
expl expletive
infmod infinitival modifier
iobj indirect object
mark marker
mwe multi-word expression
neg negation
Label Description
nmod noun modifier
nsubj nominal subject
nsubjpass passive nominal subject
num numeric modifier
p punctuation
parataxis parataxis
partmod participial modifier
poss possessive
prt verb particle
rcmod relative clause modifier
rel relative
xcomp open clausal complement
Table 1: Harmonized label set based on Stanford dependencies (De Marneffe et al, 2006).
source(s) # sentences # tokens
DE N, R 4,000 59,014
EN PTB? 43,948 1,046,829
SV STB? 6,159 96,319
ES N, B, R 4,015 112,718
FR N, B, R 3,978 90,000
KO N, B 6,194 71,840
Table 2: Data set statistics. ?Automatically con-
verted WSJ section of the PTB. The data release
includes scripts to generate this data, not the data
itself. ?Automatically converted Talbanken sec-
tion of the Swedish Treebank. N=News, B=Blogs,
R=Consumer Reviews.
due to the source and tokenization. For example,
Korean has 50% more sentences than Spanish, but
?40k less tokens due to a more coarse-grained to-
kenization. In addition to the data itself, anno-
tation guidelines and harmonization rules are in-
cluded so that the data can be regenerated.
3 Experiments
One of the motivating factors in creating such a
data set was improved cross-lingual transfer eval-
uation. To test this, we use a cross-lingual transfer
parser similar to that of McDonald et al (2011).
In particular, it is a perceptron-trained shift-reduce
parser with a beam of size 8. We use the features
of Zhang and Nivre (2011), except that all lexical
identities are dropped from the templates during
training and testing, hence inducing a ?delexical-
ized? model that employs only ?universal? proper-
ties from source-side treebanks, such as part-of-
speech tags, labels, head-modifier distance, etc.
We ran a number of experiments, which can be
seen in Table 3. For these experiments we ran-
domly split each data set into training, develop-
ment and testing sets.5 The one exception is En-
glish, where we used the standard splits. Each
row in Table 3 represents a source training lan-
guage and each column a target evaluation lan-
guage. We report both unlabeled attachment score
(UAS) and labeled attachment score (LAS) (Buch-
holz and Marsi, 2006). This is likely the first re-
liable cross-lingual parsing evaluation. In partic-
ular, previous studies could not even report LAS
due to differences in treebank annotations.
We can make several interesting observations.
Most notably, for the Germanic and Romance tar-
get languages, the best source language is from
the same language group. This is in stark contrast
to the results of McDonald et al (2011), who ob-
serve that this is rarely the case with the heteroge-
nous CoNLL treebanks. Among the Germanic
languages, it is interesting to note that Swedish
is the best source language for both German and
English, which makes sense from a typological
point of view, because Swedish is intermediate be-
tween German and English in terms of word or-
der properties. For Romance languages, the cross-
lingual parser is approaching the accuracy of the
supervised setting, confirming that for these lan-
guages much of the divergence is lexical and not
structural, which is not true for the Germanic lan-
guages. Finally, Korean emerges as a very clear
outlier (both as a source and as a target language),
which again is supported by typological consider-
ations as well as by the difference in tokenization.
With respect to evaluation, it is interesting to
compare the absolute numbers to those reported
in McDonald et al (2011) for the languages com-
5These splits are included in the release of the data.
95
Source
Training
Language
Target Test Language
Unlabeled Attachment Score (UAS) Labeled Attachment Score (LAS)
Germanic Romance Germanic Romance
DE EN SV ES FR KO DE EN SV ES FR KO
DE 74.86 55.05 65.89 60.65 62.18 40.59 64.84 47.09 53.57 48.14 49.59 27.73
EN 58.50 83.33 70.56 68.07 70.14 42.37 48.11 78.54 57.04 56.86 58.20 26.65
SV 61.25 61.20 80.01 67.50 67.69 36.95 52.19 49.71 70.90 54.72 54.96 19.64
ES 55.39 58.56 66.84 78.46 75.12 30.25 45.52 47.87 53.09 70.29 63.65 16.54
FR 55.05 59.02 65.05 72.30 81.44 35.79 45.96 47.41 52.25 62.56 73.37 20.84
KO 33.04 32.20 27.62 26.91 29.35 71.22 26.36 21.81 18.12 18.63 19.52 55.85
Table 3: Cross-lingual transfer parsing results. Bolded are the best per target cross-lingual result.
mon to both studies (DE, EN, SV and ES). In that
study, UAS was in the 38?68% range, as compared
to 55?75% here. For Swedish, we can even mea-
sure the difference exactly, because the test sets
are the same, and we see an increase from 58.3%
to 70.6%. This suggests that most cross-lingual
parsing studies have underestimated accuracies.
4 Conclusion
We have released data sets for six languages with
consistent dependency annotation. After the ini-
tial release, we will continue to annotate data in
more languages as well as investigate further au-
tomatic treebank conversions. This may also lead
to modifications of the annotation scheme, which
should be regarded as preliminary at this point.
Specifically, with more typologically and morpho-
logically diverse languages being added to the col-
lection, it may be advisable to consistently en-
force the principle that content words take func-
tion words as dependents, which is currently vi-
olated in the analysis of adpositional and copula
constructions. This will ensure a consistent analy-
sis of functional elements that in some languages
are not realized as free words or are not obliga-
tory, such as adpositions which are often absent
due to case inflections in languages like Finnish. It
will also allow the inclusion of language-specific
functional or morphological markers (case mark-
ers, topic markers, classifiers, etc.) at the leaves of
the tree, where they can easily be ignored in appli-
cations that require a uniform cross-lingual repre-
sentation. Finally, this data is available on an open
source repository in the hope that the community
will commit new data and make corrections to ex-
isting annotations.
Acknowledgments
Many people played critical roles in the pro-
cess of creating the resource. At Google, Fer-
nando Pereira, Alfred Spector, Kannan Pashu-
pathy, Michael Riley and Corinna Cortes sup-
ported the project and made sure it had the re-
quired resources. Jennifer Bahk and Dave Orr
helped coordinate the necessary contracts. Andrea
Held, Supreet Chinnan, Elizabeth Hewitt, Tu Tsao
and Leigha Weinberg made the release process
smooth. Michael Ringgaard, Andy Golding, Terry
Koo, Alexander Rush and many others provided
technical advice. Hans Uszkoreit gave us per-
mission to use a subsample of sentences from the
Tiger Treebank (Brants et al, 2002), the source of
the news domain for our German data set. Anno-
tations were additionally provided by Sulki Kim,
Patrick McCrae, Laurent Alamarguy and He?ctor
Ferna?ndez Alcalde.
References
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Parsed Cor-
pora, pages 103?127. Kluwer.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL.
Miriam Butt, Helge Dyvik, Tracy Holloway King,
Hiroshi Masuichi, and Christian Rohrer. 2002.
The parallel grammar project. In Proceedings of
the 2002 workshop on Grammar engineering and
evaluation-Volume 15.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative
reordering with Chinese grammatical relations fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation (SSST-3)
at NAACL HLT 2009.
96
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation.
Marie-Catherine De Marneffe, Bill MacCartney, and
Chris D. Manning. 2006. Generating typed depen-
dency parses from phrase structure parses. In Pro-
ceedings of LREC.
Tomaz Erjavec. 2012. MULTEXT-East: Morphosyn-
tactic resources for Central and Eastern European
languages. Language Resources and Evaluation,
46:131?142.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction
via bitext projection constraints. In Proceedings of
ACL-IJCNLP.
Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joao
Grac?a. 2012. The pascal challenge on grammar in-
duction. In Proceedings of the NAACL-HLT Work-
shop on the Induction of Linguistic Structure.
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking finnish. In Proceedings of
The Ninth International Workshop on Treebanks and
Linguistic Theories (TLT9).
Stephen Helmreich, David Farwell, Bonnie Dorr, Nizar
Habash, Lori Levin, Teruko Mitamura, Florence
Reeder, Keith Miller, Eduard Hovy, Owen Rambow,
and Advaith Siddharthan. 2004. Interlingual anno-
tation of multilingual text corpora. In Proceedings
of the HLT-EACL Workshop on Frontiers in Corpus
Annotation.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% solution. In Proceedings of NAACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL.
Dan Klein and Chris D. Manning. 2004. Corpus-based
induction of syntactic structure: models of depen-
dency and constituency. In Proceedings of ACL.
Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan and Claypool.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: the Penn treebank. Compu-
tational Linguistics, 19(2):313?330.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of EMNLP.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2007.
Generalizing tree transformations for inductive de-
pendency parsing. In Proceedings of ACL.
Joakim Nivre and Bea?ta Megyesi. 2007. Bootstrap-
ping a Swedish treebank using cross-corpus harmo-
nization and annotation projection. In Proceedings
of the 6th International Workshop on Treebanks and
Linguistic Theories.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012.
A universal part-of-speech tagset. In Proceedings of
LREC.
Mojgan Seraji, Bea?ta Megyesi, and Nivre Joakim.
2012. Bootstrapping a Persian dependency tree-
bank. Linguistic Issues in Language Technology,
7(18):1?10.
David A. Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the ACL.
Ulf Teleman. 1974. Manual fo?r grammatisk beskrivn-
ing av talad och skriven svenska. Studentlitteratur.
Reut Tsarfaty. 2013. A unified morpho-syntactic
scheme of stanford dependencies. Proceedings of
ACL.
Daniel Zeman, David Marecek, Martin Popel,
Loganathan Ramasamy, Jan S?tepa?nek, Zdene?k
Z?abokrtsky`, and Jan Hajic. 2012. Hamledt: To
parse or not to parse. In Proceedings of LREC.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL-HLT.
Yuan Zhang, Roi Reichart, Regina Barzilay, and Amir
Globerson. 2012. Learning to map into a universal
pos tagset. In Proceedings of EMNLP.
97
