Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 742?751,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexically-Triggered Hidden Markov Models
for Clinical Document Coding
Svetlana Kiritchenko Colin Cherry
Institute for Information Technology
National Research Council Canada
{Svetlana.Kiritchenko,Colin.Cherry}@nrc-cnrc.gc.ca
Abstract
The automatic coding of clinical documents
is an important task for today?s healthcare
providers. Though it can be viewed as
multi-label document classification, the cod-
ing problem has the interesting property that
most code assignments can be supported by
a single phrase found in the input docu-
ment. We propose a Lexically-Triggered Hid-
den Markov Model (LT-HMM) that leverages
these phrases to improve coding accuracy. The
LT-HMM works in two stages: first, a lexical
match is performed against a term dictionary
to collect a set of candidate codes for a docu-
ment. Next, a discriminative HMM selects the
best subset of codes to assign to the document
by tagging candidates as present or absent.
By confirming codes proposed by a dictio-
nary, the LT-HMM can share features across
codes, enabling strong performance even on
rare codes. In fact, we are able to recover
codes that do not occur in the training set at
all. Our approach achieves the best ever per-
formance on the 2007Medical NLP Challenge
test set, with an F-measure of 89.84.
1 Introduction
The clinical domain presents a number of interesting
challenges for natural language processing. Con-
ventionally, most clinical documentation, such as
doctor?s notes, discharge summaries and referrals,
are written in a free-text form. This narrative form
is flexible, allowing healthcare professionals to ex-
press any kind of concept or event, but it is not
particularly suited for large-scale analysis, search,
or decision support. Converting clinical narratives
into a structured form would support essential activi-
ties such as administrative reporting, quality control,
biosurveillance and biomedical research (Meystre
et al, 2008). One way of representing a docu-
ment is to code the patient?s conditions and the per-
formed procedures into a nomenclature of clinical
codes. The International Classification of Diseases,
9th and 10th revisions, Clinical Modification (ICD-
9-CM, ICD-10-CM) are the official administrative
coding schemes for healthcare organizations in sev-
eral countries, including the US and Canada. Typi-
cally, coding is performed by trained coding profes-
sionals, but this process can be both costly and error-
prone. Automated methods can speed-up the cod-
ing process, improve the accuracy and consistency
of internal documentation, and even result in higher
reimbursement for the healthcare organization (Ben-
son, 2006).
Traditionally, statistical document coding is
viewed as multi-class multi-label document classifi-
cation, where each clinical free-text document is la-
belled with one or several codes from a pre-defined,
possibly very large set of codes (Patrick et al, 2007;
Suominen et al, 2008). One classification model is
learned for each code, and then all models are ap-
plied in turn to a new document to determine which
codes should be assigned to the document. The
drawback of this approach is poor predictive perfor-
mance on low-frequency codes, which are ubiqui-
tous in the clinical domain.
This paper presents a novel approach to document
coding that simultaneously models code-specific as
well as general patterns in the data. This allows
742
us to predict any code label, even codes for which
no training data is available. Our approach, the
lexically-triggered HMM (LT-HMM), is based on
the fact that a code assignment is often indicated
by short lexical triggers in the text. Consequently,
a two-stage coding method is proposed. First, the
LT-HMM identifies candidate codes by matching
terms from a medical terminology dictionary. Then,
it confirms or rejects each of the candidates by ap-
plying a discriminative sequence model. In this ar-
chitecture, low-frequency codes can still be matched
and confirmed using general characteristics of their
trigger?s local context, leading to better prediction
performance on these codes.
2 Document Coding and Lexical Triggers
Document coding is a special case of multi-class
multi-label text classification. Given a fixed set of
possible codes, the ultimate goal is to assign a set of
codes to documents, based on their content. Further-
more, we observe that for each code assigned to a
document, there is generally at least one correspond-
ing trigger term in the text that accounts for the
code?s assignment. For example, if an ICD-9-CM
coding professional were to see ?allergic bronchitis?
somewhere in a clinical narrative, he or she would
immediately consider adding code 493.9 (Asthma,
unspecified) to the document?s code set. The pres-
ence of these trigger terms separates document cod-
ing from text classification tasks, such as topic or
genre classification, where evidence for a particular
label is built up throughout a document. However,
this does not make document coding a term recogni-
tion task, concerned only with the detection of trig-
gers. Codes are assigned to a document as a whole,
and code assignment decisions within a document
may interact. It is an interesting combination of sen-
tence and document-level processing.
Formally, we define the document coding task
as follows: given a set of documents X and a set
of available codes C, assign to each document xi
a subset of codes Ci ? C. We also assume ac-
cess to a (noisy) mechanism to detect candidate trig-
gers in a document. In particular, we will assume
that an (incomplete) dictionary D(c) exists for each
code c ? C, which lists specific code terms asso-
ciated with c.1 To continue our running example:
D(493.9) would include the term ?allergic bron-
chitis?. Each code can have several corresponding
terms while each term indicates the presence of ex-
actly one code. A candidate code c is proposed each
time a term from D(c) is found in a document.
2.1 From triggers to codes
The presence of a term from D(c) does not automat-
ically imply the assignment of code c to a document.
Even with extremely precise dictionaries, there are
three main reasons why a candidate code may not
appear in a document?s code subset.
1. The context of the trigger term might indicate
the irrelevancy of the code. In the clinical do-
main, such irrelevancy can be specified by a
negative or speculative statement (e.g., ?evalu-
ate for pneumonia?) or a family-related context
(e.g., ?family history of diabetes?). Only defi-
nite diagnosis of the patient should be coded.
2. There can be several closely related candidate
codes; yet only one, the best fitted code should
be assigned to the document. For example, the
triggers ?left-sided flank pain? (code 789.09)
and ?abdominal pain? (code 789.00) may both
appear in the same clinical report, but only the
most specific code, 789.09, should end up in
the document code set.
3. The domain can have code dependency rules.
For example, the ICD-9-CM coding rules state
that no symptom codes should be given to
a document if a definite diagnosis is present.
That is, if a document is coded with pneumo-
nia, it should not be coded with a fever or
cough. On the other hand, if the diagnosis is
uncertain, then codes for the symptoms should
be assigned.
This suggests a paradigm where a candidate code,
suggested by a detected trigger term, is assessed
in terms of both its local context (item 1) and the
presence of other candidate codes for the document
(items 2 and 3).
1Note that dictionary-based trigger detection could be re-
placed by tagging approaches similar to those used in named-
entity-recognition or information extraction.
743
2.2 ICD-9-CM Coding
As a specific application we have chosen the task
of assigning ICD-9-CM codes to free-form clinical
narratives. We use the dataset collected for the 2007
Medical NLP Challenge organized by the Compu-
tational Medicine Center in Cincinnati, Ohio, here-
after refereed to as ?CMC Challenge? (Pestian et al,
2007). For this challenge, 1954 radiology reports
on outpatient chest x-ray and renal procedures were
collected, disambiguated, and anonymized. The re-
ports were annotated with ICD-9-CM codes by three
coding companies, and the majority codes were se-
lected as a gold standard. In total, 45 distinct codes
were used.
For this task, our use of a dictionary to detect lex-
ical triggers is quite reasonable. The medical do-
main is rich with manually-created and carefully-
maintained knowledge resources. In particular, the
ICD-9-CM coding guidelines come with an index
file that contains hundreds of thousands of terms
mapped to corresponding codes. Another valuable
resource is Metathesaurus from the Unified Medical
Language System (UMLS) (Lindberg et al, 1993).
It has millions of terms related to medical problems,
procedures, treatments, organizations, etc. Often,
hospitals, clinics, and other healthcare organizations
maintain their own vocabularies to introduce con-
sistency in their internal and external documenta-
tion and to support reporting, reviewing, and meta-
analysis.
This task has some very challenging properties.
As mentioned above, the ICD-9-CM coding rules
create strong code dependencies: codes are assigned
to a document as a set and not individually. Fur-
thermore, the code distribution throughout the CMC
training documents has a very heavy tail; that is,
there are a few heavily-used codes and a large
number of codes that are used only occasionally.
An ideal approach will work well with both high-
frequency and low-frequency codes.
3 Related work
Automated clinical coding has received much atten-
tion in the medical informatics literature. Stanfill et
al. reviewed 113 studies on automated coding pub-
lished in the last 40 years (Stanfill et al, 2010). The
authors conclude that there exists a variety of tools
covering different purposes, healthcare specialties,
and clinical document types; however, these tools
are not generalizable and neither are their evaluation
results. One major obstacle that hinders the progress
in this domain is data privacy issues. To overcome
this obstacle, the CMC Challenge was organized in
2007. The purpose of the challenge was to provide
a common realistic dataset to stimulate the research
in the area and to assess the current level of perfor-
mance on the task. Forty-four teams participated in
the challenge. The top-performing system achieved
micro-averaged F1-score of 0.8908, and the mean
score was 0.7670.
Several teams, including the winner, built pure
symbolic (i.e., hand-crafted rule-based) systems
(e.g., (Goldstein et al, 2007)). This approach is fea-
sible for the small code set used in the challenge,
but it is questionable in real-life settings where thou-
sands of codes need to be considered. Later, the
winning team showed how their hand-crafted rules
can be built in a semi-automatic way: the initial set
of rules adopted from the official coding guidelines
were automatically extended with additional syn-
onyms and code dependency rules generated from
the training data (Farkas and Szarvas, 2008).
Statistical systems trained on only text-derived
features (such as n-grams) did not show good per-
formance due to a wide variety of medical language
and a relatively small training set (Goldstein et al,
2007). This led to the creation of hybrid systems:
symbolic and statistical classifiers used together in
an ensemble or cascade (Aronson et al, 2007; Cram-
mer et al, 2007) or a symbolic component provid-
ing features for a statistical component (Patrick et
al., 2007; Suominen et al, 2008). Strong competi-
tion systems had good answers for dealing with neg-
ative and speculative contexts, taking advantage of
the competition?s limited set of possible code com-
binations, and handling of low-frequency codes.
Our proposed approach is a combination system
as well. We combine a symbolic component that
matches lexical strings of a document against a med-
ical dictionary to determine possible codes (Lussier
et al, 2000; Kevers and Medori, 2010) and a sta-
tistical component that finalizes the assignment of
codes to the document. Our statistical component
is similar to that of Crammer et al (2007), in that
we train a single model for all codes with code-
744
specific and generic features. However, Crammer
et al (2007) did not employ our lexical trigger step
or our sequence-modeling formulation. In fact, they
considered all possible code subsets, which can be
infeasible in real-life settings.
4 Method
To address the task of document coding, our
lexically-triggered HMM operates using a two-stage
procedure:
1. Lexically match text to the dictionary to get a
set of candidate codes;
2. Using features derived from the candidates and
the document, select the best code subset.
In the first stage, dictionary terms are detected in the
document using exact string matching. All codes
corresponding to matches become candidate codes,
and no other codes can be proposed for this docu-
ment.
In the second stage, a single classifier is trained to
select the best code subset from the matched candi-
dates. By training a single classifier, we use all of
the training data to assign binary labels (present or
absent) to candidates. This is the key distinction of
our method from the traditional statistical approach
where a separate classifier is trained for each code.
The LT-HMM allows features learned from a doc-
ument coded with ci to transfer at test time to pre-
dict code cj , provided their respective triggers ap-
pear in similar contexts. Training one common clas-
sifier improves our chances to reliably predict codes
that have few training instances, and even codes that
do not appear at all in the training data.
4.1 Trigger Detection
We have manually assembled a dictionary of terms
for each of the 45 codes used in the CMC chal-
lenge.2 The dictionaries were built by collecting rel-
evant medical terminology from UMLS, the ICD-9-
CM coding guidelines, and the CMC training data.
The test data was not consulted during dictionary
construction. The dictionaries contain 440 terms,
with 9.78 terms per code on average. Given these
dictionaries, the exact-matching of terms to input
2Online at https://sites.google.com/site/
colinacherry/ICD9CM ACL11.txt
documents is straightforward. In our experiments,
this process finds on average 1.83 distinct candidate
codes per document.
The quality of the dictionary significantly affects
the prediction performance of the proposed two-
stage approach. Especially important is the cover-
age of the dictionary. If a trigger term is missing
from the dictionary and, as the result, the code is not
selected as a candidate code, it will not be recov-
ered in the following stage, resulting in a false neg-
ative. Preliminary experiments show that our dictio-
nary recovers 94.42% of the codes in the training set
and 93.20% in the test set. These numbers provide
an upper bound on recall for the overall approach.
4.2 Sequence Construction
After trigger detection, we view the input document
as a sequence of candidate codes, each correspond-
ing to a detected trigger (see Figure 1). By tagging
these candidates in sequence, we can label each can-
didate code as present or absent and use previous
tagging decisions to model code interactions. The
final code subset is constructed by collecting all can-
didate codes tagged as present.
Our training data consists of [document, code set]
pairs, augmented with the trigger terms detected
through dictionary matching. We transform this into
a sequence to be tagged using the following steps:
Ordering: The candidate code sequence is pre-
sented in reverse chronological order, according to
when their corresponding trigger terms appear in the
document. That is, the last candidate to be detected
by the dictionary will be the first code to appear in
our candidate sequence. Reverse order was chosen
because clinical documents often close with a final
(and informative) diagnosis.
Merging: Each detected trigger corresponds to
exactly one code; however, several triggers may be
detected for the same code throughout a document.
If a code has several triggers, we keep only the last
occurrence. When possible, we collect relevant fea-
tures (such as negation information) of all occur-
rences and associate them with this last occurrence.
Labelling: Each candidate code is assigned a bi-
nary label (present or absent) based on whether it
appears in the gold-standard code set. Note that this
745
Cough,	 ?fever	 ?in	 ?9-??year-??
old	 ?male.	 ?IMPRESSION:	 ?
1.	 ?Right	 ?middle	 ?lobe	 ?
pneumonia.	 ?2.	 ?Minimal	 ?
pleural	 ?thickening	 ?on	 ?
the	 ?right	 ?may	 ?represent	 ?
small	 ?pleural	 ?effusion.	 ?	 ?
486	 ?
pneumonia	 ?
context=pos	 ?
sem=disease	 ?
	 ?
N	 ?Y	 ? N	 ?N	 ?
511.9	 ?
pleural	 ?effusion	 ?
context=neg	 ?
sem=disease	 ?
	 ?
780.6	 ?
fever	 ?
context=pos	 ?
sem=symptom	 ?
	 ?
786.2	 ?
cough	 ?
context=pos	 ?
sem=symptom	 ?
	 ?
Gold	 ?code	 ?set:	 ?{486}	 ?
Figure 1: An example document and its corresponding gold-standard tag sequence. The top binary layer is the correct
output tag sequence, which confirms or rejects the presence of candidate codes. The bottom layer shows the candidate
code sequence derived from the text, with corresponding trigger phrases and some prominent features.
process can not introduce gold-standard codes that
were not proposed by the dictionary.
The final output of these steps is depicted in Fig-
ure 1. To the left, we have an input text with un-
derlined trigger phrases, as detected by our dictio-
nary. This implies an input sequence (bottom right),
which consists of detected codes and their corre-
sponding trigger phrases. The gold-standard code
set for the document is used to infer a gold-standard
label sequence for these codes (top right). At test
time, the goal of the classifier is to correctly predict
the correct binary label sequence for new inputs. We
discuss the construction of the features used to make
this prediction in section 4.3.
4.3 Model
We model this sequence data using a discriminative
SVM-HMM (Taskar et al, 2003; Altun et al, 2003).
This allows us to use rich, over-lapping features of
the input while also modeling interactions between
labels. A discriminative HMM has two major cate-
gories of features: emission features, which charac-
terize a candidate?s tag in terms of the input docu-
ment x, and transition features, which characterize
a tag in terms of the tags that have come before it.
We describe these two feature categories and then
our training mechanism. All feature engineering dis-
cussed below was carried out using 10-fold cross-
validation on the training set.
Transition Features
The transition features are modeled as simple in-
dicators over n-grams of present codes, for values of
n up to 10, the largest number of codes proposed by
our dictionary in the training set.3 This allows the
system to learn sequences of codes that are (and are
not) likely to occur in the gold-standard data.
We found it useful to pad our n-grams with ?be-
ginning of document? tokens for sequences when
fewer than n codes have been labelled as present,
but found it harmful to include an end-of-document
tag once labelling is complete. We suspect that the
small training set for the challenge makes the system
prone to over-fit when modeling code-set length.
Emission Features
The vast majority of our training signal comes
from emission features, which carefully model both
the trigger term?s local context and the document as
a whole. For each candidate code, three types of
features are generated: document features, ConText
features, and code-semantics features (Table 1).
Document: Document features include indicators
on all individual words, 2-grams, 3-grams, and 4-
grams found in the document. These n-gram fea-
tures have the candidate code appended to them,
making them similar to features traditionally used
in multiclass document categorization.
ConText: We take advantage of the ConText algo-
rithm?s output. ConText is publicly available soft-
ware that determines the presence of negated, hypo-
thetical, historical, and family-related context for a
given phrase in a clinical text (Harkema et al, 2009).
3We can easily afford such a long history because input se-
quences are generally short and the tagging is binary, resulting
in only a small number of possible histories for a document.
746
Features gen. spec.
Document
n-gram x
ConText
current match
context x x
only in context x x
more than once in context x x
other matches
present x x
present in context = pos x x
code present in context x x
Code Semantics
current match
sem type x
other matches
sem type, context = pos x x
Table 1: The emission features used in LT-HMM.
Typeset words represent variables replaced with spe-
cific values, i.e. context ? {pos,neg}, sem type ?
{symptom,disease}, code is one of 45 challenge codes,
n-gram is a document n-gram. Features can come in
generic and/or code-specific version.
The algorithm is based on regular expression match-
ing of the context to a precompiled list of context
indicators. Regardless of its simplicity, the algo-
rithm has shown very good performance on a vari-
ety of clinical document types. We run ConText for
each trigger term located in the text and produce two
types of features: features related to the candidate
code in question and features related to other candi-
date codes of the document. Negated, hypothetical,
and family-related contexts are clustered into a sin-
gle negative context for the term. Absence of the
negative context implies the positive context.
We used the following ConText derived indicator
features: for the current candidate code, if there is at
least one trigger term found in a positive (negative)
context, if all trigger terms for this code are found
in a positive (negative) context, if there are more
than one trigger terms for the code found in a posi-
tive (negative) context; for other candidate codes of
the document, if there is at least one other candidate
code, if there is another candidate code with at least
one trigger term found in a positive context, if there
is a trigger term for candidate code ci found in a pos-
itive (negative) context.
Code Semantics: We include features that indi-
cate if the code itself corresponds to a disease or a
symptom. This assignment was determined based
on the UMLS semantic type of the code. Like the
ConText features, code features come in two types:
those regarding the candidate code in question and
those regarding other candidate codes from the same
document.
Generic versus Specific: Most of our features
come in two versions: generic and code-specific.
Generic features are concerned with classifying any
candidate as present or absent based on characteris-
tics of its trigger or semantics. Code-specific fea-
tures append the candidate code to the feature. For
example, the feature context=pos represents that
the current candidate has a trigger term in a positive
context, while context=pos:486 adds the infor-
mation that the code in question is 486. Note that
n-grams features are only code-specific, as they are
not connected to any specific trigger term.
To an extent, code-specific features allow us
to replicate the traditional classification approach,
which focuses on one code at a time. Using these
features, the classifier is free to build complex sub-
models for a particular code, provided that this code
has enough training examples. Generic versions of
the features, on the other hand, make it possible to
learn common rules applicable to all codes, includ-
ing low-frequency ones. In this way, even in the ex-
treme case of having zero training examples for a
particular code, the model can still potentially assign
the code to new documents, provided it is detected
by our dictionary. This is impossible in a traditional
document-classification setting.
Training
We train our SVM-HMM with the objective of
separating the correct tag sequence from all others
by a fixed margin of 1, using a primal stochastic
gradient optimization algorithm that follows Shalev-
Shwartz et al (2007). Let S be a set of training
points (x, y), where x is the input and y is the cor-
responding gold-standard tag sequence. Let ?(x, y)
be a function that transforms complete input-output
pairs into feature vectors. We also use ?(x, y?, y)
as shorthand for the difference in features between
747
begin
Input: S, ?, n
Initialize: Set w0 to the 0 vector
for t = 1, 2 . . . , n|S|
Choose (x, y) ? S at random
Set the learning rate: ?t = 1?t
Search:
y? = argmaxy?? [?(y, y
??) + wt ? ?(x, y??)]
Update:
wt+1 = wt + ?t
(
?(x, y, y?) ? ?wt
)
Adjust:
wt+1 = wt+1 ? min
[
1, 1/
?
?
?wt+1?
]
end
Output: wn|S|+1
end
Figure 2: Training an SVM-HMM
two outputs: ?(x, y?, y) = ?(x, y?) ? ?(x, y). With
this notation in place, the SVM-HMM minimizes
the regularized hinge-loss:
min
w
?
2
w2 +
1
|S|
?
(x,y)?S
`(w; (x, y)) (1)
where
`(w; (x, y)) = max
y?
[
?(y, y?) + w ? ?(x, y?, y)
]
(2)
and where ?(y, y?) = 0 when y = y? and 1 oth-
erwise.4 Intuitively, the objective attempts to find
a small weight vector w that separates all incorrect
tag sequences y? from the correct tag sequence y by
a margin of 1. ? controls the trade-off between reg-
ularization and training hinge-loss.
The stochastic gradient descent algorithm used
to optimize this objective is shown in Figure 2. It
bears many similarities to perceptron HMM train-
ing (Collins, 2002), with theoretically-motivated al-
terations, such as selecting training points at ran-
dom5 and the explicit inclusion of a learning rate ?
4We did not experiment with structured versions of ? that
account for the number of incorrect tags in the label sequence
y?, as a fixed margin was already working very well. We intend
to explore structured costs in future work.
5Like many implementations, we make n passes through S,
shuffling S before each pass, rather than sampling from S with
replacement n|S| times.
training test
# of documents 978 976
# of distinct codes 45 45
# of distinct code subsets 94 94
# of codes with < 10 ex. 24 24
avg # of codes per document 1.25 1.23
Table 2: The training and test set characteristics.
and a regularization term ?. The search step can be
carried out with a two-best version of the Viterbi al-
gorithm; if the one-best answer y?1 matches the gold-
standard y, that is ?(y, y?1) = 0, then y
?
2 is checked
to see if its loss is higher.
We tune two hyper-parameters using 10-fold
cross-validation: the regularization parameter ? and
a number of passes n through the training data. Us-
ing F1 as measured by 10-fold cross-validation on
the training set, we found values of ? = 0.1 with
n = 5 to prove optimal. Training time is less than
one minute on modern hardware.
5 Experiments
5.1 Data
For testing purposes, we use the CMC Challenge
dataset. The data consists of 978 training and 976
test medical records labelled with one or more ICD-
9-CM codes from a set of 45 codes. The data statis-
tics are presented in Table 2. The training and test
sets have similar, very imbalanced distributions of
codes. In particular, all codes in the test set have at
least one training example. Moreover, for any code
subset assigned to a test document there is at least
one training document labelled with the same code
subset. Notably, more than half of the codes have
less than 10 instances in both training and test sets.
Following the challenge?s protocol, we use micro-
averaged F1-measure for evaluation.
5.2 Baseline
As the first baseline for comparison, we built a
one-classifier-per-code statistical system. A docu-
ment?s code subset is implied by the set of classi-
fiers that assign it a positive label. The classifiers
use a feature set designed to mimic our LT-HMM
as closely as possible, including n-grams, dictionary
matches, ConText output, and symptom/disease se-
748
mantic types. Each classifier is trained as an SVM
with a linear kernel.
Unlike our approach, this baseline cannot share
features across codes, and it does not allow coding
decisions for a document to inform one another. It
also cannot propose codes that have not been seen in
the training data as it has no model for these codes.
However, one should note that it is a very strong
baseline. Like our proposed system, it is built with
many features derived from dictionary matches and
their contexts, and thus it shares many of our sys-
tem?s strengths. In fact, this baseline system outper-
forms all published statistical approaches tested on
the CMC data.
Our second baseline is a symbolic system, de-
signed to evaluate the quality of our rule-based com-
ponents when used alone. It is based on the same
hand-crafted dictionary, filtered according to the
ConText algorithm and four code dependency rules
from (Farkas and Szarvas, 2008). These rules ad-
dress the problem of overcoding: some symptom
codes should be omitted when a specific disease
code is present.6
This symbolic system has access to the same
hand-crafted resources as our LT-HMM and, there-
fore, has a good chance of predicting low-frequency
and unseen codes. However, it lacks the flexibility of
our statistical solution to accept or reject code candi-
dates based on the whole document text, which pre-
vents it from compensating for dictionary or Con-
Text errors. Similarly, the structure of the code de-
pendency rules may not provide the same flexibility
as our features that look at other detected triggers
and previous code assignments.
5.3 Coding Accuracy
We evaluate the proposed approach on both the
training set (using 10-fold cross-validation) and the
test set (Table 3). The experiments demonstrate the
superiority of the proposed LT-HMM approach over
the one-per-code statistical scheme as well as our
symbolic baseline. Furthermore, the new approach
shows the best results ever achieved on the dataset,
beating the top-performing system in the challenge,
a symbolic method.
6Note that we do not match the performance of the Farkas
and Szarvas system, likely due to our use of a different (and
simpler) dictionary.
Cross-fold Test
Symbolic baseline N/A 85.96
Statistical baseline 87.39 88.26
LT-HMM 89.39 89.84
CMC Best N/A 89.08
Table 3: Micro-averaged F1-scores for statistical and
symbolic baselines, the proposed LT-HMM approach,
and the best CMC hand-crafted rule-based system.
System Prec. Rec. F1
Full 90.91 88.80 89.84
-ConText 88.54 85.89 87.19
-Document 89.89 88.55 89.21
-Code Semantics 90.10 88.38 89.23
-Append code-specific 88.96 88.30 88.63
-Transition 90.79 88.38 89.57
-ConText & Transition 86.91 85.39 86.14
Table 4: Results on the CMC test data with each major
component removed.
5.4 Ablation
Our system employs a number of emission feature
templates. We measure the impact of each by re-
moving the template, re-training, and testing on the
challenge test data, as shown in Table 4. By far the
most important component of our system is the out-
put of the ConText algorithm.
We also tested a version of the system that does
not create a parallel code-specific feature set by ap-
pending the candidate code to emission features.
This system tags code-candidates without any code-
specific components, but it still does very well, out-
performing the baselines.
Removing the sequence-based transition features
from our system has only a small impact on accu-
racy. This is because several of our emission fea-
tures look at features of other candidate codes. This
provides a strong approximation to the actual tag-
ging decisions for these candidates. If we remove
the ConText features, the HMM?s transition features
become more important (compare line 2 of Table 4
to line 7).
5.5 Low-frequency codes
As one can see from Table 2, more than half of the
available codes appear fewer than 10 times in the
749
System Prec. Rec. F1
Symbolic baseline 42.53 56.06 48.37
Statistical baseline 73.33 33.33 45.83
LT-HMM 70.00 53.03 60.34
Table 5: Results on the CMC test set, looking only at the
codes with fewer than 10 examples in the training set.
System Prec. Rec. F1
Symbolic baseline 60.00 80.00 68.57
All training data 72.92 74.47 73.68
One code held out 79.31 48.94 60.53
Table 6: Results on the CMC test set when all instances
of a low-frequency code are held-out during training.
training documents. This does not provide much
training data for a one-classifier-per-code approach,
which has been a major motivating factor in the de-
sign of our LT-HMM. In Table 5, we compare our
system to the baselines on the CMC test set, con-
sidering only these low-frequency codes. We show
a 15-point gain in F1 over the statistical baseline
on these hard cases, brought on by an substantial
increase in recall. Similarly, we improve over the
symbolic baseline, due to a much higher precision.
In this way, the LT-HMM captures the strengths of
both approaches.
Our system also has the ability to predict codes
that have not been seen during training, by labelling
a dictionary match for a code as present according to
its local context. We simulate this setting by drop-
ping training data. For each low-frequency code c,
we hold out all training documents that include c in
their gold-standard code set. We then train our sys-
tem on the reduced training set and measure its abil-
ity to detect c on the unseen test data. 11 of the 24
low-frequency codes have no dictionary matches in
our test data; we omit them from our analysis as we
are unable to predict them. The micro-averaged re-
sults for the remaining 13 low-frequency codes are
shown in Table 6, with the results from the symbolic
baseline and from our system trained on the com-
plete training data provided for comparison.
We were able to recover 49% of the test-time oc-
currences of codes withheld from training, while
maintaining our full system?s precision. Consider-
ing that traditional statistical strategies would lead
to recall dropping uniformly to 0, this is a vast im-
provement. However, the symbolic baseline recalls
80% of occurrences in aggregate, indicating that we
are not yet making optimal use of the dictionary for
cases when a code is missing from the training data.
By holding out only correct occurrences of a code
c, our system becomes biased against it: all trigger
terms for c that are found in the training data must
be labelled absent. Nonetheless, out of the 13 codes
with dictionary matches, there were 9 codes that we
were able to recall at a rate of 50% or more, and 5
codes that achieved 100% recall.
6 Conclusion
We have presented the lexically-triggered HMM, a
novel and effective approach for clinical document
coding. The LT-HMM takes advantage of lexical
triggers for clinical codes by operating in two stages:
first, a lexical match is performed against a trigger
term dictionary to collect a set of candidates codes
for a document; next, a discriminative HMM se-
lects the best subset of codes to assign to the docu-
ment. Using both generic and code-specific features,
the LT-HMM outperforms a traditional one-per-
code statistical classification method, with substan-
tial improvements on low-frequency codes. Also,
it achieves the best ever performance on a common
testbed, beating the top-performer of the 2007 CMC
Challenge, a hand-crafted rule-based system. Fi-
nally, we have demonstrated that the LT-HMM can
correctly predict codes never seen in the training set,
a vital characteristic missing from previous statisti-
cal methods.
In the future, we would like to augment our
dictionary-based matching component with entity-
recognition technology. It would be interesting to
model triggers as latent variables in the document
coding process, in a manner similar to how latent
subjective sentences have been used in document-
level sentiment analysis (Yessenalina et al, 2010).
This would allow us to employ a learned matching
component that is trained to compliment our classi-
fication component.
Acknowledgements
Many thanks to Berry de Bruijn, Joel Martin, and
the ACL-HLT reviewers for their helpful comments.
750
References
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In ICML.
A. R. Aronson, O. Bodenreider, D. Demner-Fushman,
K. W. Fung, V. K. Lee, J. G. Mork, A. Nvol, L. Peters,
and W. J. Rogers. 2007. From indexing the biomed-
ical literature to coding clinical text: Experience with
MTI and machine learning approaches. In BioNLP,
pages 105?112.
S. Benson. 2006. Computer assisted coding software
improves documentation, coding, compliance and rev-
enue. Perspectives in Health Information Manage-
ment, CAC Proceedings, Fall.
M. Collins. 2002. Discriminative training methods for
Hidden Markov Models: Theory and experiments with
perceptron algorithms. In EMNLP.
K. Crammer, M. Dredze, K. Ganchev, P. P. Talukdar, and
S. Carroll. 2007. Automatic code assignment to med-
ical text. In BioNLP, pages 129?136.
R. Farkas and G. Szarvas. 2008. Automatic construc-
tion of rule-based ICD-9-CM coding systems. BMC
Bioinformatics, 9(Suppl 3):S10.
I. Goldstein, A. Arzumtsyan, and Uzuner. 2007. Three
approaches to automatic assignment of ICD-9-CM
codes to radiology reports. In AMIA, pages 279?283.
H. Harkema, J. N. Dowling, T. Thornblade, and W. W.
Chapman. 2009. Context: An algorithm for determin-
ing negation, experiencer, and temporal status from
clinical reports. Journal of Biomedical Informatics,
42(5):839?851, October.
L. Kevers and J. Medori. 2010. Symbolic classifica-
tion methods for patient discharge summaries encod-
ing into ICD. In Proceedings of the 7th International
Conference on NLP (IceTAL), pages 197?208, Reyk-
javik, Iceland, August.
D. A. Lindberg, B. L. Humphreys, and A. T. McCray.
1993. The Unified Medical Language System. Meth-
ods of Information in Medicine, 32(4):281?291.
Y. A. Lussier, L. Shagina, and C. Friedman. 2000. Au-
tomating ICD-9-CM encoding using medical language
processing: A feasibility study. In AMIA, page 1072.
S. M. Meystre, G. K. Savova, K. C. Kipper-Schuler, and
J. F. Hurdle. 2008. Extracting information from tex-
tual documents in the electronic health record: a re-
view of recent research. Methods of Information in
Medicine, 47(Suppl 1):128?144.
J. Patrick, Y. Zhang, and Y. Wang. 2007. Developing
feature types for classifying clinical notes. In BioNLP,
pages 191?192.
J. P. Pestian, C. Brew, P. Matykiewicz, D. J. Hovermale,
N. Johnson, K. B. Cohen, and W. Duch. 2007. A
shared task involving multi-label classification of clin-
ical free text. In BioNLP, pages 97?104.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pega-
sos: Primal Estimated sub-GrAdient SOlver for SVM.
In ICML, Corvallis, OR.
M. H. Stanfill, M. Williams, S. H. Fenton, R. A. Jenders,
and W. R. Hersh. 2010. A systematic literature re-
view of automated clinical coding and classification
systems. JAMIA, 17:646?651.
H. Suominen, F. Ginter, S. Pyysalo, A. Airola,
T. Pahikkala, S. Salanter, and T. Salakoski. 2008.
Machine learning to automate the assignment of di-
agnosis codes to free-text radiology reports: a method
description. In Proceedings of the ICML Workshop
on Machine Learning for Health-Care Applications,
Helsinki, Finland.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
markov networks. In Neural Information Processing
Systems Conference (NIPS03), Vancouver, Canada,
December.
A. Yessenalina, Y. Yue, and C. Cardie. 2010. Multi-
level structured models for document-level sentiment
classification. In EMNLP.
751
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 304?313,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
An Empirical Study on the Effect of Negation Words on Sentiment
Xiaodan Zhu, Hongyu Guo, Saif Mohammad and Svetlana Kiritchenko
National Research Council Canada
1200 Montreal Road
Ottawa, K1A 0R6, ON, Canada
{Xiaodan.Zhu,Hongyu.Guo,Saif.Mohammad,Svetlana.Kiritchenko}
@nrc-cnrc.gc.ca
Abstract
Negation words, such as no and not, play
a fundamental role in modifying sentiment
of textual expressions. We will refer to a
negation word as the negator and the text
span within the scope of the negator as the
argument. Commonly used heuristics to
estimate the sentiment of negated expres-
sions rely simply on the sentiment of ar-
gument (and not on the negator or the ar-
gument itself). We use a sentiment tree-
bank to show that these existing heuristics
are poor estimators of sentiment. We then
modify these heuristics to be dependent on
the negators and show that this improves
prediction. Next, we evaluate a recently
proposed composition model (Socher et
al., 2013) that relies on both the negator
and the argument. This model learns the
syntax and semantics of the negator?s ar-
gument with a recursive neural network.
We show that this approach performs bet-
ter than those mentioned above. In ad-
dition, we explicitly incorporate the prior
sentiment of the argument and observe that
this information can help reduce fitting er-
rors.
1 Introduction
Morante and Sporleder (2012) define negation to
be ?a grammatical category that allows the chang-
ing of the truth value of a proposition?. Nega-
tion is often expressed through the use of nega-
tive signals or negators?words like isn?t and never,
and it can significantly affect the sentiment of
its scope. Understanding the impact of negation
on sentiment is essential in automatic analysis of
sentiment. The literature contains interesting re-
search attempting to model and understand the
behavior (reviewed in Section 2). For example,
Figure 1: Effect of a list of common negators
in modifying sentiment values in Stanford Senti-
ment Treebank. The x-axis is s(~w), and y-axis
is s(w
n
, ~w). Each dot in the figure corresponds
to a text span being modified by (composed with)
a negator in the treebank. The red diagonal line
corresponds to the sentiment-reversing hypothesis
that simply reverses the sign of sentiment values.
a simple yet influential hypothesis posits that a
negator reverses the sign of the sentiment value
of the modified text (Polanyi and Zaenen, 2004;
Kennedy and Inkpen, 2006). The shifting hypoth-
esis (Taboada et al, 2011), however, assumes that
negators change sentiment values by a constant
amount. In this paper, we refer to a negation word
as the negator (e.g., isn?t), a text span being mod-
ified by and composed with a negator as the ar-
gument (e.g., very good), and entire phrase (e.g.,
isn?t very good) as the negated phrase.
The recently available Stanford Sentiment Tree-
bank (Socher et al, 2013) renders manually anno-
tated, real-valued sentiment scores for all phrases
in parse trees. This corpus provides us with the
data to further understand the quantitative behav-
ior of negators, as the effect of negators can now
be studied with arguments of rich syntactic and se-
mantic variety. Figure 1 illustrates the effect of a
common list of negators on sentiment as observed
304
on the Stanford Sentiment Treebank.1 Each dot in
the figure corresponds to a negated phrase in the
treebank. The x-axis is the sentiment score of its
argument s(~w) and y-axis the sentiment score of
the entire negated phrase s(w
n
, ~w).
We can see that the reversing assumption (the
red diagonal line) does capture some regularity of
human perception, but rather roughly. Moreover,
the figure shows that same or similar s(~w) scores
(x-axis) can correspond to very different s(w
n
, ~w)
scores (y-axis), which, to some degree, suggests
the potentially complicated behavior of negators.2
This paper describes a quantitative study of
the effect of a list of frequent negators on sen-
timent. We regard the negators? behavior as an
underlying function embedded in annotated data;
we aim to model this function from different as-
pects. By examining sentiment compositions of
negators and arguments, we model the quantita-
tive behavior of negators in changing sentiment.
That is, given a negated phrase (e.g., isn?t very
good) and the sentiment score of its argument
(e.g., s(?very good??) = 0.5), we focus on un-
derstanding the negator?s quantitative behavior in
yielding the sentiment score of the negated phrase
s(?isn
?
t very good
??
).
We first evaluate the modeling capabilities of
two influential heuristics and show that they cap-
ture only very limited regularity of negators? ef-
fect. We then extend the models to be dependent
on the negators and demonstrate that such a sim-
ple extension can significantly improve the per-
formance of fitting to the human annotated data.
Next, we evaluate a recently proposed composi-
tion model (Socher, 2013) that relies on both the
negator and the argument. This model learns the
syntax and semantics of the negator?s argument
with a recursive neural network. This approach
performs significantly better than those mentioned
above. In addition, we explicitly incorporate the
prior sentiment of the argument and observe that
this information helps reduce fitting errors.
1The sentiment values have been linearly rescaled from
the original range [0, 1] to [-0.5, 0.5]; in the figure a negative
or positive value corresponds to a negative or a positive sen-
timent respectively; zero means neutral. The negator list will
be discussed later in the paper.
2Similar distribution is observed in other data such as
Tweets (Kiritchenko et al, 2014).
2 Related work
Automatic sentiment analysis The expression of
sentiment is an integral component of human lan-
guage. In written text, sentiment is conveyed with
word senses and their composition, and in speech
also via prosody such as pitch (Mairesse et al,
2012). Early work on automatic sentiment anal-
ysis includes the widely cited work of (Hatzivas-
siloglou and McKeown, 1997; Pang et al, 2002;
Turney, 2002), among others. Since then, there has
been an explosion of research addressing various
aspects of the problem, including detecting sub-
jectivity, rating and classifying sentiment, label-
ing sentiment-related semantic roles (e.g., target
of sentiment), and visualizing sentiment (see sur-
veys by Pang and Lee (2008) and Liu and Zhang
(2012)).
Negation modeling Negation is a general gram-
matical category pertaining to the changing of the
truth values of propositions; negation modeling is
not limited to sentiment. For example, paraphrase
and contradiction detection systems rely on detect-
ing negated expressions and opposites (Harabagiu
et al, 2006). In general, a negated expression and
the opposite of the expression may or may not con-
vey the same meaning. For example, not alive has
the same meaning as dead, however, not tall does
not always mean short. Some automatic methods
to detect opposites were proposed by Hatzivas-
siloglou and McKeown (1997) and Mohammad et
al. (2013).
Negation modeling for sentiment An early yet
influential reversing assumption conjectures that a
negator reverses the sign of the sentiment value
of the modified text (Polanyi and Zaenen, 2004;
Kennedy and Inkpen, 2006), e.g., from +0.5 to -
0.5, or vice versa. A different hypothesis, called
the shifting hypothesis in this paper, assumes that
negators change the sentiment values by a con-
stant amount (Taboada et al, 2011; Liu and Sen-
eff, 2009). Other approaches to negation modeling
have been discussed in (Jia et al, 2009; Wiegand
et al, 2010; Lapponi et al, 2012; Benamara et al,
2012).
In the process of semantic composition, the ef-
fect of negators could depend on the syntax and
semantics of the text spans they modify. The ap-
proaches of modeling this include bag-of-word-
based models. For example, in the work of
(Kennedy and Inkpen, 2006), a feature not good
will be created if the word good is encountered
305
within a predefined range after a negator.
There exist different ways of incorporating
more complicated syntactic and semantic infor-
mation. Much recent work considers sentiment
analysis from a semantic-composition perspec-
tive (Moilanen and Pulman, 2007; Choi and
Cardie, 2008; Socher et al, 2012; Socher et al,
2013), which achieved the state-of-the-art perfor-
mance. Moilanen and Pulman (2007) used a col-
lection of hand-written compositional rules to as-
sign sentiment values to different granularities of
text spans. Choi and Cardie (2008) proposed a
learning-based framework. The more recent work
of (Socher et al, 2012; Socher et al, 2013) pro-
posed models based on recursive neural networks
that do not rely on any heuristic rules. Such mod-
els work in a bottom-up fashion over the parse
tree of a sentence to infer the sentiment label of
the sentence as a composition of the sentiment ex-
pressed by its constituting parts. The approach
leverages a principled method, the forward and
backward propagation, to learn a vector represen-
tation to optimize the system performance. In
principle neural network is able to fit very compli-
cated functions (Mitchell, 1997), and in this paper,
we adapt the state-of-the-art approach described in
(Socher et al, 2013) to help understand the behav-
ior of negators specifically.
3 Negation models based on heuristics
We begin with previously proposed methods that
leverage heuristics to model the behavior of nega-
tors. We then propose to extend them to consider
lexical information of the negators themselves.
3.1 Non-lexicalized assumptions and
modeling
In previous research, some influential, widely
adopted assumptions posit the effect of negators
to be independent of both the specific negators and
the semantics and syntax of the arguments. In this
paper, we call a model based on such assumptions
a non-lexicalized model. In general, we can sim-
ply define this category of models in Equation 1.
That is, the model parameters are only based on
the sentiment value of the arguments.
s(w
n
, ~w)
def
= f(s(~w)) (1)
3.1.1 Reversing hypothesis
A typical model falling into this category is the
reversing hypothesis discussed in Section 2, where
a negator simply reverses the sentiment score s(~w)
to be ?s(~w); i.e., f(s(~w)) = ?s(~w).
3.1.2 Shifting hypothesis
Basic shifting Similarly, a shifting based model
depends on s(~w) only, which can be written as:
f(s(~w)) = s(~w) ? sign(s(~w)) ? C (2)
where sign(.) is the standard sign function
which determines if the constant C should be
added to or deducted from s(w
n
): the constant is
added to a negative s(~w) but deducted from a pos-
itive one.
Polarity-based shifting As will be shown in our
experiments, negators can have different shifting
power when modifying a positive or a negative
phrase. Thus, we explore the use of two different
constants for these two situations, i.e., f(s(~w)) =
s(~w)?sign(s(~w))?C(sign(s(~w))). The constant
C now can take one of two possible values. We
will show that this simple modification improves
the fitting performance statistically significantly.
Note also that instead of determining these con-
stants by human intuition, we use the training data
to find the constants in all shifting-based models
as well as for the parameters in other models.
3.2 Simple lexicalized assumptions
The above negation hypotheses rely on s(~w). As
intuitively shown in Figure 1, the capability of the
non-lexicalized heuristics might be limited. Fur-
ther semantic or syntactic information from either
the negators or the phrases they modify could be
helpful. The most straightforward way of expand-
ing the non-lexicalized heuristics is probably to
make the models to be dependent on the negators.
s(w
n
, ~w)
def
= f(w
n
, s(~w)) (3)
Negator-based shifting We can simply extend the
basic shifting model above to consider the lexi-
cal information of negators: f(s(~w)) = s(~w) ?
sign(s(~w)) ?C(w
n
). That is, each negator has its
own C . We call this model negator-based shift-
ing. We will show that this model also statistically
significantly outperforms the basic shifting with-
out overfitting, although the number of parameters
have increased.
306
Combined shifting We further combine the
negator-based shifting and polarity-based shift-
ing above: f(s(~w)) = s(~w) ? sign(s(~w)) ?
C(w
n
, sign(s(~w))). This shifting model is
based on negators and the polarity of the text
they modify: constants can be different for each
negator-polarity pair. The number of parameters
in this model is the multiplication of number
of negators by two (the number of sentiment
polarities). This model further improves the fitting
performance on the test data.
4 Semantics-enriched modeling
Negators can interact with arguments in complex
ways. Figure 1 shows the distribution of the ef-
fect of negators on sentiment without considering
further semantics of the arguments. The question
then is that whether and how much incorporating
further syntax and semantic information can help
better fit or predict the negation effect. Above, we
have considered the semantics of the negators. Be-
low, we further make the models to be dependent
on the arguments. This can be written as:
s(w
n
, ~w)
def
= f(w
n
, s(~w), r(~w)) (4)
In the formula, r(~w) is a certain type of repre-
sentation for the argument ~w and it models the se-
mantics or/and syntax of the argument. There ex-
ist different ways of implementing r(~w). We con-
sider two models in this study: one drops s(~w) in
Equation 4 and directly models f(w
n
, r(~w)). That
is, the non-uniform information shown in Figure 1
is not directly modeled. The other takes into ac-
count s(~w) too.
For the former, we adopt the recursive neu-
ral tensor network (RNTN) proposed recently by
Socher et al (2013), which has showed to achieve
the state-of-the-art performance in sentiment anal-
ysis. For the latter, we propose a prior sentiment-
enriched tensor network (PSTN) to take into ac-
count the prior sentiment of the argument s(~w).
4.1 RNTN: Recursive neural tensor network
A recursive neural tensor network (RNTN) is
a specific form of feed-forward neural network
based on syntactic (phrasal-structure) parse tree
to conduct compositional sentiment analysis. For
completeness, we briefly review it here. More de-
tails can be found in (Socher et al, 2013).
As shown in the black portion of Figure 2, each
instance of RNTN corresponds to a binary parse
Figure 2: Prior sentiment-enriched tensor network
(PSTN) model for sentiment analysis.
tree of a given sentence. Each node of the parse
tree is a fixed-length vector that encodes composi-
tional semantics and syntax, which can be used to
predict the sentiment of this node. The vector of a
node, say p
2
in Figure 2, is computed from the d-
dimensional vectors of its two children, namely a
and p
1
(a, p
1
? R
d?1
), with a non-linear function:
p
2
= tanh(
[
a
p
1
]
T
V
[1:d]
[
a
p
1
]
+ W
[
a
p
1
]
) (5)
where, W ? Rd?(d+d) and V ? R(d+d)?(d+d)?d
are the matrix and tensor for the composition func-
tion. A major difference of RNTN from the con-
ventional recursive neural network (RRN) (Socher
et al, 2012) is the use of the tensor V in order
to directly capture the multiplicative interaction of
two input vectors, although the matrix W implic-
itly captures the nonlinear interaction between the
input vectors. The training of RNTN uses conven-
tional forward-backward propagation.
4.2 PSTN: Prior sentiment-enriched tensor
network
The non-uniform distribution in Figure 1 has
showed certain correlations between the sentiment
values of s(w
n
, ~w) and s(~w), and such informa-
tion has been leveraged in the models discussed in
Section 3. We intend to devise a model that imple-
ments Equation 4. It bridges between the models
we have discussed above that use either s(~w) or
r(~w).
We extend RNTN to directly consider the senti-
ment information of arguments. Consider the node
p
2
in Figure 2. When calculating its vector, we
aim to directly engage the sentiment information
of its right child, i.e., the argument. To this end,
we make use of the sentiment class information of
307
p1
, noted as psen
1
. As a result, the vector of p
2
is
calculated as follows:
p
2
= tanh(
[
a
p
1
]
T
V
[1:d]
[
a
p
1
]
+ W
[
a
p
1
]
(6)
+
[
a
p
sen
1
]
T
V
sen
[1:d]
[
a
p
sen
1
]
+ W
sen
[
a
p
sen
1
]
)
As shown in Equation 6, for the node vector
p
1
? R
d?1
, we employ a matrix, namely W sen ?
R
d?(d+m) and a tensor, V sen ? R(d+m)?(d+m)?d,
aiming at explicitly capturing the interplays be-
tween the sentiment class of p
1
, denoted as psen
1
(?
R
m?1), and the negator a. Here, we assume the
sentiment task has m classes. Following the idea
of Wilson et al (2005), we regard the sentiment of
p
1
as a prior sentiment as it has not been affected
by the specific context (negators), so we denote
our method as prior sentiment-enriched tensor net-
work (PSTN). In Figure 2, the red portion shows
the added components of PSTN.
Note that depending on different purposes, psen
1
can take the value of the automatically predicted
sentiment distribution obtained in forward propa-
gation, the gold sentiment annotation of node p
1
,
or even other normalized prior sentiment value or
confidence score from external sources (e.g., sen-
timent lexicons or external training data). This
is actually an interesting place to extend the cur-
rent recursive neural network to consider extrinsic
knowledge. However, in our current study, we fo-
cus on exploring the behavior of negators. As we
have discussed above, we will use the human an-
notated sentiment for the arguments, same as in
the models discussed in Section 3.
With the new matrix and tensor, we then have
? = (V, V
sen
,W,W
sen
,W
label
, L) as the PSTN
model?s parameters. Here, L denotes the vector
representations of the word dictionary.
4.2.1 Inference and Learning
Inference and learning in PSTN follow a forward-
backward propagation process similar to that in
(Socher et al, 2013), and for completeness, we
depict the details as follows. To train the model,
one first needs to calculate the predicted sentiment
distribution for each node:
p
sen
i
= W
label
p
i
, p
sen
i
? R
m?1
and then compute the posterior probability over
the m labels:
y
i
= softmax(psen
i
)
During learning, following the method used by
the RNTN model in (Socher et al, 2013), PSTN
also aims to minimize the cross-entropy error be-
tween the predicted distribution yi ? Rm?1 at
node i and the target distribution ti ? Rm?1 at that
node. That is, the error for a sentence is calculated
as:
E(?) =
?
i
?
j
t
i
j
logyi
j
+ ? ???
2 (7)
where, ? represents the regularization hyperpa-
rameters, and j ? m denotes the j-th element of
the multinomial target distribution.
To minimize E(?), the gradient of the objec-
tive function with respect to each of the param-
eters in ? is calculated efficiently via backprop-
agation through structure, as proposed by Goller
and Kchler (1996). Specifically, we first compute
the prediction errors in all tree nodes bottom-up.
After this forward process, we then calculate the
derivatives of the softmax classifiers at each node
in the tree in a top-down fashion. We will discuss
the gradient computation for the V sen and W sen
in detail next. Note that the gradient calculations
for the V,W,W label, L are the same as that of pre-
sented in (Socher et al, 2013).
In the backpropogation process of the training,
each node (except the root node) in the tree car-
ries two kinds of errors: the local softmax error
and the error passing down from its parent node.
During the derivative computation, the two errors
will be summed up as the complement incoming
error for the node. We denote the complete incom-
ing error and the softmax error vector for node i
as ?i,com ? Rd?1 and ?i,s ? Rd?1, respectively.
With this notation, the error for the root node p
2
can be formulated as follows.
?
p
2
,com
= ?
p
2
,s
= (W
T
(y
p
2
? t
p
2
)) ? f
?
([a; p
1
]) (8)
where ? is the Hadamard product between the two
vectors and f ? is the element-wise derivative of
f = tanh. With the results from Equation 8, we
then can calculate the derivatives for the W sen at
node p
2
using the following equation:
?E
p
2
W
sen
= ?
p
2
,com
([a; p
sen
1
])
T
Similarly, for the derivative of each slice k(k =
308
1, . . . , d) of the V sen tensor, we have the follow-
ing:
?E
p
2
V
sen
[k]
= ?
p
2
,com
k
[
a
p
sen
1
] [
a
p
sen
1
]
T
Now, let?s form the equations for computing the
error for the two children of the p
2
node. The dif-
ference for the error at p
2
and its two children is
that the error for the latter will need to compute the
error message passing down from p
2
. We denote
the error passing down as ?p2,down, where the left
child and the right child of p
2
take the 1st and 2nd
half of the error ?p2,down, namely ?p2,down[1 : d]
and ?p2,down[d + 1 : 2d], respectively. Follow-
ing this notation, we have the error message for
the two children of p
2
, provided that we have the
?
p
2
,down:
?
p
1
,com
= ?
p
1
,s
+ ?
p
2
,down
[d + 1 : 2d]
= (W
T
(y
p
1
? t
p
1
)) ? f
?
([b; c])
+ ?
p
2
,down
[d + 1 : 2d]
The incoming error message of node a can be
calculated similarly. Finally, we can finish the
above equations with the following formula for
computing ?p2,down:
?
p
2
,down
= (W
T
?
p
2
,com
) ? f
?
([a; p
1
]) + ?
tensor
where
?
tensor
= [?
V
[1 : d] + ?
V
sen
[1 : d], ?
V
[d + 1 : 2d]]
=
d
?
k=1
?
p
2
,com
k
(V
[k]
+ (V
[k]
)
T
)? f
?
([a; p
1
])[1 : d]
+
d
?
k=1
?
p
2
,com
k
(V
sen
[k]
+ (V
sen
[k]
)
T
)? f
?
([a; p
sen
1
])[1 : d]
+
d
?
k=1
?
p
2
,com
k
(V
[k]
+ (V
[k]
)
T
)? f
?
([a; p
1
])[d + 1 : 2d]
After the models are trained, they are applied to
predict the sentiment of the test data. The orig-
inal RNTN and the PSTN predict 5-class senti-
ment for each negated phrase; we map the out-
put to real-valued scores based on the scale that
Socher et al (2013) used to map real-valued senti-
ment scores to sentiment categories. Specifically,
we conduct the mapping with the formula: preal
i
=
y
i
? [0.1 0.3 0.5 0.7 0.9]; i.e., we calculate the dot
product of the posterior probability yi and the scal-
ing vector. For example, if yi = [0.5 0.5 0 0 0],
meaning this phrase has a 0.5 probability to be
in the first category (strong negative) and 0.5 for
the second category (weak negative), the resulting
p
real
i
will be 0.2 (0.5*0.1+0.5*0.3).
5 Experiment set-up
Data As described earlier, the Stanford Sentiment
Treebank (Socher et al, 2013) has manually anno-
tated, real-valued sentiment values for all phrases
in parse trees. This provides us with the training
and evaluation data to study the effect of negators
with syntax and semantics of different complex-
ity in a natural setting. The data contain around
11,800 sentences from movie reviews that were
originally collected by Pang and Lee (2005). The
sentences were parsed with the Stanford parser
(Klein and Manning, 2003). The phrases at all
tree nodes were manually annotated with one of 25
sentiment values that uniformly span between the
positive and negative poles. The values are nor-
malized to the range of [0, 1].
In this paper, we use a list of most frequent
negators that include the words not, no, never, and
their combinations with auxiliaries (e.g., didn?t).
We search these negators in the Stanford Senti-
ment Treebank and normalize the same negators to
a single form; e.g., ?is n?t?, ?isn?t?, and ?is not?
are all normalized to ?is not?. Each occurrence of
a negator and the phrase it is directly composed
with in the treebank, i.e., ?w
n
, ~w?, is considered
a data point in our study. In total, we collected
2,261 pairs, including 1,845 training and 416 test
cases. The split of training and test data is same as
specified in (Socher et al, 2013).
Evaluation metrics We use the mean absolute er-
ror (MAE) to evaluate the models, which mea-
sures the averaged absolute offsets between the
predicted sentiment values and the gold stan-
dard. More specifically, MAE is calculated as:
MAE =
1
N
?
?w
n
, ~w?
|(s?(w
n
, ~w) ? s(w
n
, ~w))|,
where s?(w
n
, ~w) denotes the gold sentiment value
and s(w
n
, ~w) the predicted one for the pair
?w
n
, ~w?, and N is the total number of test in-
stances. Note that mean square error (MSE) is an-
other widely used measure for regression, but it is
less intuitive for out task here.
6 Experimental results
Overall regression performance Table 1 shows
the overall fitting performance of all models. The
first row of the table is a random baseline, which
309
simply guesses the sentiment value for each test
case randomly in the range [0,1]. The table shows
that the basic reversing and shifting heuristics do
capture negators? behavior to some degree, as their
MAE scores are higher than that of the baseline.
Making the basic shifting model to be dependent
on the negators (model 4) reduces the prediction
error significantly as compared with the error of
the basic shifting (model 3). The same is true
for the polarity-based shifting (model 5), reflect-
ing that the roles of negators are different when
modifying positive and negative phrases. Merging
these two models yields additional improvement
(model 6).
Assumptions MAE
Baseline
(1) Random 0.2796
Non-lexicalized
(2) Reversing 0.1480*
(3) Basic shifting 0.1452*
Simple-lexicalized
(4) Negator-based shifting 0.1415?
(5) Polarity-based shifting 0.1417?
(6) Combined shifting 0.1387?
Semantics-enriched
(7) RNTN 0.1097**
(8) PSTN 0.1062??
Table 1: Mean absolute errors (MAE) of fitting
different models to Stanford Sentiment Treebank.
Models marked with an asterisk (*) are statisti-
cally significantly better than the random baseline.
Models with a dagger sign (?) significantly outper-
form model (3). Double asterisks ** indicates a
statistically significantly different from model (6),
and the model with the double dagger ??is signif-
icantly better than model (7). One-tailed paired
t-test with a 95% significance level is used here.
Furthermore, modeling the syntax and seman-
tics with the state-of-the-art recursive neural net-
work (model 7 and 8) can dramatically improve
the performance over model 6. The PSTN model,
which takes into account the human-annotated
prior sentiment of arguments, performs the best.
This could suggest that additional external knowl-
edge, e.g., that from human-built resources or au-
tomatically learned from other data (e.g., as in
(Kiritchenko et al, 2014)), including sentiment
that cannot be inferred from its constituent expres-
sions, might be incorporated to benefit the current
is
_n
ev
e
r
w
ill
_n
ot
is
_n
ot
do
es
_n
ot
ba
re
ly
w
a
s
_
n
o
t
c
o
u
ld
_n
ot
n
o
t
di
d_
no
t
u
n
lik
e
ly
do
_n
ot
c
a
n
_
n
o
t
n
o
ha
s_
no
t
s
u
pe
rfi
ci
al
w
o
u
ld
_n
ot
s
ho
ul
d_
no
t
0.05
0.10
0.15
0.20
0.25
0.30
Figure 3: Effect of different negators in shifting
sentiment values.
neural-network-based models as prior knowledge.
Note that the two neural network based models
incorporate the syntax and semantics by represent-
ing each node with a vector. One may consider
that a straightforward way of considering the se-
mantics of the modified phrases is simply memo-
rizing them. For example, if a phrase very good
modified by a negator not appears in the train-
ing and test data, the system can simply memorize
the sentiment score of not very good in training
and use this score at testing. When incorporating
this memorizing strategy into model (6), we ob-
served a MAE score of 0.1222. It?s not surprising
that memorizing the phrases has some benefit, but
such matching relies on the exact reoccurrences of
phrases. Note that this is a special case of what the
neural network based models can model.
Discriminating negators The results in Table 1
has demonstrated the benefit of discriminating
negators. To understand this further, we plot in
Figure 3 the behavior of different negators: the
x-axis is a subset of our negators and the y-axis
denotes absolute shifting in sentiment values. For
example, we can see that the negator ?is never?
on average shifts the sentiment of the arguments
by 0.26, which is a significant change considering
the range of sentiment value is [0, 1]. For each
negator, a 95% confidence interval is shown by
the boxes in the figure, which is calculated with
the bootstrapping resampling method. We can ob-
serve statistically significant differences of shift-
ing abilities between many negator pairs such as
that between ?is never? and ?do not? as well as
between ?does not? and ?can not?.
Figure 3 also includes three diminishers (the
310
is
_n
ot
(n
n)
is
_n
ot
(n
p)
do
es
_n
ot
(n
n)
do
es
_n
ot
(n
p)
n
o
t(n
n)
n
o
t(n
p)
do
_n
ot
(n
n)
do
_n
ot
(n
p)
n
o
(n
n)
n
o
(n
p)
0.15
0.20
0.25
0.30
Figure 4: The behavior of individual negators in
negated negative (nn) and negated positive (np)
context.
white bars), i.e., barely, unlikely, and superficial.
By following (Kennedy and Inkpen, 2006), we ex-
tracted 319 diminishers (also called understate-
ment or downtoners) from General Inquirer3. We
calculated their shifting power in the same man-
ner as for the negators and found three diminish-
ers having shifting capability in the shifting range
of these negators. This shows that the boundary
between negators and diminishers can by fuzzy.
In general, we argue that one should always con-
sider modeling negators individually in a senti-
ment analysis system. Alternatively, if the model-
ing has to be done in groups, one should consider
clustering valence shifters by their shifting abili-
ties in training or external data.
Figure 4 shows the shifting capacity of negators
when they modify positive (blue boxes) or nega-
tive phrases (red boxes). The figure includes five
most frequently used negators found in the sen-
timent treebank. Four of them have significantly
different shifting power when composed with pos-
itive or negative phrases, which can explain why
the polarity-based shifting model achieves im-
provement over the basic shifting model.
Modeling syntax and semantics We have seen
above that modeling syntax and semantics through
the-state-of-the-art neural networks help improve
the fitting performance. Below, we take a closer
look at the fitting errors made at different depths
of the sentiment treebank. The depth here is de-
fined as the longest distance between the root of a
negator-phrase pair ?w
n
, ~w? and their descendant
3http://www.wjh.harvard.edu/ inquirer/
Figure 5: Errors made at different depths in the
sentiment tree bank.
leafs. Negators appearing at deeper levels of the
tree tend to have more complicated syntax and se-
mantics. In Figure 5, the x-axis corresponds to
different depths and y-axis is the mean absolute
errors (MAE).
The figure shows that both RNTN and PSTN
perform much better at all depths than the model
6 in Table 1. When the depths are within 4,
the RNTN performs very well and the (human
annotated) prior sentiment of arguments used
in PSTN does not bring additional improvement
over RNTN. PSTN outperforms RNTN at greater
depths, where the syntax and semantics are more
complicated and harder to model. The errors made
by model 6 is bumpy, as the model considers
no semantics and hence its errors are not depen-
dent on the depths. On the other hand, the er-
rors of RNTN and PSTN monotonically increase
with depths, indicating the increase in the task dif-
ficulty.
7 Conclusions
Negation plays a fundamental role in modifying
sentiment. In the process of semantic compo-
sition, the impact of negators is complicated by
the syntax and semantics of the text spans they
modify. This paper provides a comprehensive
and quantitative study of the behavior of negators
through a unified view of fitting human annota-
tion. We first measure the modeling capabilities of
two influential heuristics on a sentiment treebank
and find that they capture some effect of negation;
however, extending these non-lexicalized models
to be dependent on the negators improves the per-
311
formance statistically significantly. The detailed
analysis reveals the differences in the behavior
among negators, and we argue that they should al-
ways be modeled separately. We further make the
models to be dependent on the text being modi-
fied by negators, through adaptation of a state-of-
the-art recursive neural network to incorporate the
syntax and semantics of the arguments; we dis-
cover this further reduces fitting errors.
References
Farah Benamara, Baptiste Chardon, Yannick Mathieu,
Vladimir Popescu, and Nicholas Asher. 2012. How
do negation and modality impact on opinions? In
Proceedings of the ACL-2012 Workshop on Extra-
Propositional Aspects of Meaning in Computational
Linguistics, pages 10?18, Jeju, Republic of Korea.
Yejin Choi and Claire Cardie. 2008. Learning with
compositional semantics as structural inference for
subsentential sentiment analysis. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?08, pages 793?801,
Honolulu, Hawaii.
Christoph Goller and Andreas Kchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In In Proc. of
the ICNN-96, pages 347?352, Bochum, Germany.
IEEE.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In AAAI, volume 6, pages 755?762.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 8th Conference of Euro-
pean Chapter of the Association for Computational
Linguistics, EACL ?97, pages 174?181, Madrid,
Spain.
Lifeng Jia, Clement Yu, and Weiyi Meng. 2009. The
effect of negation on sentiment analysis and retrieval
effectiveness. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Manage-
ment, CIKM ?09, pages 1827?1830, Hong Kong,
China. ACM.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment classification of movie reviews using contex-
tual valence shifters. Computational Intelligence,
22(2):110?125.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014. Sentiment analysis of short informal
texts. (to appear) Journal of Artificial Intelligence
Research.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 423?
430, Sapporo, Japan. Association for Computational
Linguistics.
Emanuele Lapponi, Jonathon Read, and Lilja Ovre-
lid. 2012. Representing and resolving negation
for sentiment analysis. In Jilles Vreeken, Charles
Ling, Mohammed Javeed Zaki, Arno Siebes, Jef-
frey Xu Yu, Bart Goethals, Geoffrey I. Webb, and
Xindong Wu, editors, ICDM Workshops, pages 687?
692. IEEE Computer Society.
Jingjing Liu and Stephanie Seneff. 2009. Review sen-
timent scoring via a parse-and-paraphrase paradigm.
In EMNLP, pages 161?169, Singapore.
Bing Liu and Lei Zhang. 2012. A survey of opin-
ion mining and sentiment analysis. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 415?463. Springer US.
Franc?ois Mairesse, Joseph Polifroni, and Giuseppe
Di Fabbrizio. 2012. Can prosody inform sentiment
analysis? experiments on short spoken reviews. In
ICASSP, pages 5093?5096, Kyoto, Japan.
Tom M Mitchell. 1997. Machine learning. 1997. Burr
Ridge, IL: McGraw Hill, 45.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555?590.
Karo Moilanen and Stephen Pulman. 2007. Senti-
ment composition. In Proceedings of RANLP 2007,
Borovets, Bulgaria.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223?260.
Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics, ACL ?05, pages 115?124.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, USA.
Livia Polanyi and Annie Zaenen. 2004. Contextual
valence shifters. In Exploring Attitude and Affect in
Text: Theories and Applications (AAAI Spring Sym-
posium Series).
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
312
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?12,
Jeju, Korea. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?13, Seattle, USA. Association for Compu-
tational Linguistics.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In ACL, pages 417?424, Philadel-
phia, USA.
Michael Wiegand, Alexandra Balahur, Benjamin Roth,
Dietrich Klakow, and Andre?s Montoyo. 2010. A
survey on the role of negation in sentiment analysis.
In Proceedings of the Workshop on Negation and
Speculation in Natural Language Processing, NeSp-
NLP ?10, pages 60?68, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Association for Computational Linguistics.
313
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 321?327, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
NRC-Canada: Building the State-of-the-Art in
Sentiment Analysis of Tweets
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{saif.mohammad,svetlana.kiritchenko,xiaodan.zhu}@nrc-cnrc.gc.ca
Abstract
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to de-
tect the sentiment of messages such as tweets
and SMS (message-level task) and one to de-
tect the sentiment of a term within a message
(term-level task). Among submissions from
44 teams in a competition, our submissions
stood first in both tasks on tweets, obtaining
an F-score of 69.02 in the message-level task
and 88.93 in the term-level task. We imple-
mented a variety of surface-form, semantic,
and sentiment features. We also generated
two large word?sentiment association lexi-
cons, one from tweets with sentiment-word
hashtags, and one from tweets with emoticons.
In the message-level task, the lexicon-based
features provided a gain of 5 F-score points
over all others. Both of our systems can be
replicated using freely available resources.1
1 Introduction
Hundreds of millions of people around the world ac-
tively use microblogging websites such as Twitter.
Thus there is tremendous interest in sentiment anal-
ysis of tweets across a variety of domains such as
commerce (Jansen et al, 2009), health (Chew and
Eysenbach, 2010; Salathe? and Khandelwal, 2011),
and disaster management (Verma et al, 2011; Man-
del et al, 2012).
1The three authors contributed equally to this paper. Svet-
lana Kiritchenko developed the system for the message-level
task, Xiaodan Zhu developed the system for the term-level task,
and Saif Mohammad led the overall effort, co-ordinated both
tasks, and contributed to feature development.
In this paper, we describe how we created two
state-of-the-art SVM classifiers, one to detect the
sentiment of messages such as tweets and SMS
(message-level task) and one to detect the sentiment
of a term within a message (term-level task). The
sentiment can be one out of three possibilities: posi-
tive, negative, or neutral. We developed these classi-
fiers to participate in an international competition or-
ganized by the Conference on Semantic Evaluation
Exercises (SemEval-2013) (Wilson et al, 2013).2
The organizers created and shared sentiment-labeled
tweets for training, development, and testing. The
distributions of the labels in the different datasets is
shown in Table 1. The competition, officially re-
ferred to as Task 2: Sentiment Analysis in Twitter,
had 44 teams (34 for the message-level task and 23
for the term-level task). Our submissions stood first
in both tasks, obtaining a macro-averaged F-score
of 69.02 in the message-level task and 88.93 in the
term-level task.
The task organizers also provided a second test
dataset, composed of Short Message Service (SMS)
messages (no training data of SMS messages was
provided). We applied our classifiers on the SMS
test set without any further tuning. Nonetheless, the
classifiers still obtained the first position in identify-
ing sentiment of SMS messages (F-score of 68.46)
and second position in detecting the sentiment of
terms within SMS messages (F-score of 88.00, only
0.39 points behind the first ranked system).
We implemented a number of surface-form, se-
mantic, and sentiment features. We also gener-
ated two large word?sentiment association lexicons,
2http://www.cs.york.ac.uk/semeval-2013/task2
321
Table 1: Class distributions in the training set (Train), de-
velopment set (Dev) and testing set (Test). The Train set
was accessed through tweet ids and a download script.
However, not all tweets were accessible. Below is the
number of Train examples we were able to download.
The Dev and Test sets were provided by FTP.
Dataset Positive Negative Neutral Total
Tweets
Message-level task:
Train 3,045 (37%) 1,209 (15%) 4,004 (48%) 8,258
Dev 575 (35%) 340 (20%) 739 (45%) 1,654
Test 1,572 (41%) 601 (16%) 1,640 (43%) 3,813
Term-level task:
Train 4,831 (62%) 2,540 (33%) 385 (5%) 7,756
Dev 648 (57%) 430 (38%) 57 (5%) 1,135
Test 2,734 (62%) 1,541 (35%) 160 (3%) 4,435
SMS
Message-level task:
Test 492 (23%) 394 (19%) 1,208 (58%) 2,094
Term-level task:
Test 1,071 (46%) 1,104 (47%) 159 (7%) 2,334
one from tweets with sentiment-word hashtags, and
one from tweets with emoticons. The automatically
generated lexicons were particularly useful. In the
message-level task for tweets, they alone provided a
gain of more than 5 F-score points over and above
that obtained using all other features. The lexicons
are made freely available.3
2 Sentiment Lexicons
Sentiment lexicons are lists of words with associa-
tions to positive and negative sentiments.
2.1 Existing, Automatically Created Sentiment
Lexicons
The manually created lexicons we used include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011) (about 14,000
words), the MPQA Lexicon (Wilson et al, 2005)
(about 8,000 words), and the Bing Liu Lexicon (Hu
and Liu, 2004) (about 6,800 words).
2.2 New, Tweet-Specific, Automatically
Generated Sentiment Lexicons
2.2.1 NRC Hashtag Sentiment Lexicon
Certain words in tweets are specially marked with
a hashtag (#) to indicate the topic or sentiment. Mo-
3www.purl.com/net/sentimentoftweets
hammad (2012) showed that hashtagged emotion
words such as joy, sadness, angry, and surprised are
good indicators that the tweet as a whole (even with-
out the hashtagged emotion word) is expressing the
same emotion. We adapted that idea to create a large
corpus of positive and negative tweets.
We polled the Twitter API every four hours from
April to December 2012 in search of tweets with ei-
ther a positive word hashtag or a negative word hash-
tag. A collection of 78 seed words closely related
to positive and negative such as #good, #excellent,
#bad, and #terrible were used (32 positive and 36
negative). These terms were chosen from entries for
positive and negative in the Roget?s Thesaurus.
A set of 775,000 tweets were used to generate a
large word?sentiment association lexicon. A tweet
was considered positive if it had one of the 32 pos-
itive hashtagged seed words, and negative if it had
one of the 36 negative hashtagged seed words. The
association score for a term w was calculated from
these pseudo-labeled tweets as shown below:
score(w) = PMI(w, positive)? PMI(w, negative)
(1)
where PMI stands for pointwise mutual informa-
tion. A positive score indicates association with pos-
itive sentiment, whereas a negative score indicates
association with negative sentiment. The magni-
tude is indicative of the degree of association. The
final lexicon, which we will refer to as the NRC
Hashtag Sentiment Lexicon has entries for 54,129
unigrams and 316,531 bigrams. Entries were also
generated for unigram?unigram, unigram?bigram,
and bigram?bigram pairs that were not necessarily
contiguous in the tweets corpus. Pairs with cer-
tain punctuations, ?@? symbols, and some function
words were removed. The lexicon has entries for
308,808 non-contiguous pairs.
2.2.2 Sentiment140 Lexicon
The sentiment140 corpus (Go et al, 2009) is a
collection of 1.6 million tweets that contain pos-
itive and negative emoticons. The tweets are la-
beled positive or negative according to the emoti-
con. We generated a sentiment lexicon from this
corpus in the same manner as described above (Sec-
tion 2.2.1). This lexicon has entries for 62,468
unigrams, 677,698 bigrams, and 480,010 non-
contiguous pairs.
322
3 Task: Automatically Detecting the
Sentiment of a Message
The objective of this task is to determine whether a
given message is positive, negative, or neutral.
3.1 Classifier and features
We trained a Support Vector Machine (SVM) (Fan
et al, 2008) on the training data provided. SVM
is a state-of-the-art learning algorithm proved to be
effective on text categorization tasks and robust on
large feature spaces. The linear kernel and the value
for the parameter C=0.005 were chosen by cross-
validation on the training data.
We normalized all URLs to http://someurl and all
userids to @someuser. We tokenized and part-of-
speech tagged the tweets with the Carnegie Mellon
University (CMU) Twitter NLP tool (Gimpel et al,
2011). Each tweet was represented as a feature vec-
tor made up of the following groups of features:
? word ngrams: presence or absence of contigu-
ous sequences of 1, 2, 3, and 4 tokens; non-
contiguous ngrams (ngrams with one token re-
placed by *);
? character ngrams: presence or absence of con-
tiguous sequences of 3, 4, and 5 characters;
? all-caps: the number of words with all charac-
ters in upper case;
? POS: the number of occurrences of each part-
of-speech tag;
? hashtags: the number of hashtags;
? lexicons: the following sets of features were
generated for each of the three manually con-
structed sentiment lexicons (NRC Emotion
Lexicon, MPQA, Bing Liu Lexicon) and for
each of the two automatically constructed lex-
icons (Hashtag Sentiment Lexicon and Senti-
ment140 Lexicon). Separate feature sets were
produced for unigrams, bigrams, and non-
contiguous pairs. The lexicon features were
created for all tokens in the tweet, for each part-
of-speech tag, for hashtags, and for all-caps to-
kens. For each token w and emotion or po-
larity p, we used the sentiment/emotion score
score(w, p) to determine:
? total count of tokens in the tweet with
score(w, p) > 0;
? total score =
?
w?tweet score(w, p);
? the maximal score =
maxw?tweetscore(w, p);
? the score of the last token in the tweet with
score(w, p) > 0;
? punctuation:
? the number of contiguous sequences of
exclamation marks, question marks, and
both exclamation and question marks;
? whether the last token contains an excla-
mation or question mark;
? emoticons: The polarity of an emoticon was
determined with a regular expression adopted
from Christopher Potts? tokenizing script:4
? presence or absence of positive and nega-
tive emoticons at any position in the tweet;
? whether the last token is a positive or neg-
ative emoticon;
? elongated words: the number of words with one
character repeated more than two times, for ex-
ample, ?soooo?;
? clusters: The CMU pos-tagging tool provides
the token clusters produced with the Brown
clustering algorithm on 56 million English-
language tweets. These 1,000 clusters serve as
alternative representation of tweet content, re-
ducing the sparcity of the token space.
? the presence or absence of tokens from
each of the 1000 clusters;
? negation: the number of negated contexts. Fol-
lowing (Pang et al, 2002), we defined a negated
context as a segment of a tweet that starts
with a negation word (e.g., no, shouldn?t) and
ends with one of the punctuation marks: ?,?,
?.?, ?:?, ?;?, ?!?, ???. A negated context af-
fects the ngram and lexicon features: we add
? NEG? suffix to each word following the nega-
tion word (?perfect? becomes ?perfect NEG?).
The ? NEG? suffix is also added to polarity and
emotion features (?POLARITY positive? be-
comes ?POLARITY positive NEG?). The list
of negation words was adopted from Christo-
pher Potts? sentiment tutorial.5
4http://sentiment.christopherpotts.net/tokenizing.html
5http://sentiment.christopherpotts.net/lingstruc.html
323
3.2 Experiments
We trained the SVM classifier on the set of 9,912
annotated tweets (8,258 in the training set and 1,654
in the development set). We applied the model to the
test set of 3,813 unseen tweets. The same model was
applied unchanged to the other test set of 2,094 SMS
messages as well. The bottom-line score used by the
task organizers was the macro-averaged F-score of
the positive and negative classes. The results ob-
tained by our system on the training set (ten-fold
cross-validation), development set (when trained on
the training set), and test sets (when trained on the
combined set of tweets in the training and devel-
opment sets) are shown in Table 2. The table also
shows baseline results obtained by a majority clas-
sifier that always predicts the most frequent class as
output. Since the bottom-line F-score is based only
on the F-scores of positive and negative classes (and
not on neutral), the majority baseline chose the most
frequent class among positive and negative, which
in this case was the positive class. We also show
baseline results obtained using an SVM and unigram
features alone. Our system (SVM and all features)
obtained a macro-averaged F-score of 69.02 on the
tweet set and 68.46 on the SMS set. In the SemEval-
2013 competition, our submission ranked first on
both datasets. There were 48 submissions from 34
teams for this task.
Table 3 shows the results of the ablation experi-
ments where we repeat the same classification pro-
cess but remove one feature group at a time. The
most influential features for both datasets turned out
to be the sentiment lexicon features: they provided
gains of more than 8.5%. It is interesting to note
that tweets benefited mostly from the automatic sen-
timent lexicons (NRC Hashtag Lexicon and the Sen-
timent140 Lexicon) whereas the SMS set benefited
more from the manual lexicons (MPQA, NRC Emo-
tion Lexicon, Bing Liu Lexicon). Among the au-
tomatic lexicons, both the Hashtag Sentiment Lex-
icon and the Sentiment140 Lexicon contributed to
roughly the same amount of improvement in perfor-
mance on the tweet set.
The second most important feature group for
the message-level task was that of ngrams (word
and character ngrams). Expectedly, the impact of
ngrams on the SMS dataset was less extensive since
Table 2: Message-level Task: The macro-averaged F-
scores on different datasets.
Classifier Tweets SMS
Training set: Majority 26.94 -
SVM-all 67.20 -
Development set: Majority 26.85 -
SVM-all 68.72 -
Test set: Majority 29.19 19.03
SVM-unigrams 39.61 39.29
SVM-all 69.02 68.46
Table 3: Message-level Task: The macro-averaged F-
scores obtained on the test sets with one of the feature
groups removed. The number in the brackets is the dif-
ference with the all features score. The biggest drops are
shown in bold.
Experiment Tweets SMS
all features 69.02 68.46
all - lexicons 60.42 (-8.60) 59.73 (-8.73)
all - manual lex. 67.45 (-1.57) 65.64 (-2.82)
all - auto. lex. 63.78 (-5.24) 67.12 (-1.34)
all - Senti140 lex. 65.25 (-3.77) 67.33 (-1.13)
all - Hashtag lex. 65.22 (-3.80) 70.28 (1.82)
all - ngrams 61.77 (-7.25) 67.27 (-1.19)
all - word ngrams 64.64 (-4.38) 66.56 (-1.9)
all - char. ngrams 67.10 (-1.92) 68.94 (0.48)
all - negation 67.20 (-1.82) 66.22 (-2.24)
all - POS 68.38 (-0.64) 67.07 (-1.39)
all - clusters 69.01 (-0.01) 68.10 (-0.36)
all - encodings (elongated, emoticons, punctuations,
all-caps, hashtags) 69.16 (0.14) 68.28 (-0.18)
the classifier model was trained only on tweets.
Attention to negations improved performance on
both datasets. Removing the sentiment encoding
features like hashtags, emoticons, and elongated
words, had almost no impact on performance, but
this is probably because the discriminating informa-
tion in them was also captured by some other fea-
tures such as character and word ngrams.
4 Task: Automatically Detecting the
Sentiment of a Term in a Message
The objective of this task is to detect whether a term
(a word or phrase) within a message conveys a pos-
itive, negative, or neutral sentiment. Note that the
same term may express different sentiments in dif-
ferent contexts.
324
4.1 Classifier and features
We trained an SVM using the LibSVM package
(Chang and Lin, 2011) and a linear kernel. In ten-
fold cross-validation over the training data, the lin-
ear kernel outperformed other kernels implemented
in LibSVM as well as a maximum-entropy classi-
fier. Our model leverages a variety of features, as
described below:
? word ngrams:
? presence or absence of unigrams, bigrams,
and the full word string of a target term;
? leading and ending unigrams and bigrams;
? character ngrams: presence or absence of two-
and three-character prefixes and suffixes of all
the words in a target term (note that the target
term may be a multi-word sequence);
? elongated words: presence or absence of elon-
gated words (e.g., ?sooo?);
? emoticons: the numbers and categories of
emoticons that a term contains6;
? punctuation: presence or absence of punctua-
tion sequences such as ??!? and ?!!!?;
? upper case:
? whether all the words in the target start
with an upper case letter followed by
lower case letters;
? whether the target words are all in upper-
case (to capture a potential named entity);
? stopwords: whether a term contains only stop-
words. If so, separate features indicate whether
there are 1, 2, 3, or more stop-words;
? lengths:
? the length of a target term (number of
words);
? the average length of words (number of
characters) in a term;
? a binary feature indicating whether a term
contains long words;
6http://en.wikipedia.org/wiki/List of emoticons
? negation: similar to those described for the
message-level task. Whenever a negation word
was found immediately before the target or
within the target, the polarities of all tokens af-
ter the negation term were flipped;
? position: whether a term is at the beginning,
end, or another position;
? sentiment lexicons: we used automatically cre-
ated lexicons (NRC Hashtag Sentiment Lexi-
con, Sentiment140 Lexicon) as well as manu-
ally created lexicons (NRC Emotion Lexicon,
MPQA, Bing Liu Lexicon).
? total count of tokens in the target term
with sentiment score greater than 0;
? the sum of the sentiment scores for all to-
kens in the target;
? the maximal sentiment score;
? the non-zero sentiment score of the last to-
ken in the target;
? term splitting: when a term contains a hash-
tag made of multiple words (e.g., #biggest-
daythisyear), we split the hashtag into compo-
nent words;
? others:
? whether a term contains a Twitter user
name;
? whether a term contains a URL.
The above features were extracted from target
terms as well as from the rest of the message (the
context). For unigrams and bigrams, we used four
words on either side of the target as the context. The
window size was chosen through experiments on the
development set.
4.2 Experiments
We trained an SVM classifier on the 8,891 annotated
terms in tweets (7,756 terms in the training set and
1,135 terms in the development set). We applied the
model to 4,435 terms in the tweets test set. The same
model was applied unchanged to the other test set of
2,334 terms in unseen SMS messages as well. The
bottom-line score used by the task organizers was
the macro-averaged F-score of the positive and neg-
ative classes.
325
The results on the training set (ten-fold cross-
validation), the development set (trained on the
training set), and the test sets (trained on the com-
bined set of tweets in the training and development
sets) are shown in Table 4. The table also shows
baseline results obtained by a majority classifier that
always predicts the most frequent class as output,
and an additional baseline result obtained using an
SVM and unigram features alone. Our submission
obtained a macro-averaged F-score of 88.93 on the
tweet set and was ranked first among 29 submissions
from 23 participating teams. Even with no tuning
specific to SMS data, our SMS submission still ob-
tained second rank with an F-score of 88.00. The
score of the first ranking system on the SMS set was
88.39. A post-competition bug-fix in the bigram fea-
tures resulted in a small improvement: F-score of
89.10 on the tweets set and 88.34 on the SMS set.
Note that the performance is significantly higher
in the term-level task than in the message-level task.
This is largely because of the ngram features (see
unigram baselines in Tables 2 and 4). We analyzed
the labeled data provided to determine why ngrams
performed so strongly in this task. We found that the
percentage of test tokens already seen within train-
ing data targets was 85.1%. Further, the average ra-
tio of instances pertaining to the most dominant po-
larity of a target term to the total number of instances
of that target term was 0.808.
Table 5 presents the ablation F-scores. Observe
that the ngram features were the most useful. Note
also that removing just the word ngram features or
just the character ngram features results in only a
small drop in performance. This indicates that the
two feature groups capture similar information.
The sentiment lexicon features are the next most
useful group?removing them leads to a drop in F-
score of 3.95 points for the tweets set and 4.64 for
the SMS set. Modeling negation improves the F-
score by 0.72 points on the tweets set and 1.57 points
on the SMS set.
The last two rows in Table 5 show the results ob-
tained when the features are extracted only from the
target (and not from its context) and when they are
extracted only from the context of the target (and
not from the target itself). Observe that even though
the context may influence the polarity of the tar-
get, using target features alone is substantially more
Table 4: Term-level Task: The macro-averaged F-scores
on the datasets. The official scores of our submission are
shown in bold. SVM-all* shows results after a bug fix.
Classifier Tweets SMS
Training set: Majority 38.38 -
SVM-all 86.80 -
Development set: Majority 36.34 -
SVM-all 86.49 -
Test set: Majority 38.13 32.11
SVM-unigrams 80.28 78.71
official SVM-all 88.93 88.00
SVM-all* 89.10 88.34
Table 5: Term-level Task: The F-scores obtained on the
test sets with one of the feature groups removed. The
number in brackets is the difference with the all features
score. The biggest drops are shown in bold.
Experiment Tweets SMS
all features 89.10 88.34
all - ngrams 83.86 (-5.24) 80.49 (-7.85)
all - word ngrams 88.38 (-0.72) 87.37 (-0.97)
all - char. ngrams 89.01 (-0.09) 87.31 (-1.03)
all - lexicons 85.15 (-3.95) 83.70 (-4.64)
all - manual lex. 87.69 (-1.41) 86.84 (-1.5)
all - auto lex. 88.24 (-0.86) 86.65 (-1.69)
all - negation 88.38 (-0.72) 86.77 (-1.57)
all - stopwords 89.17 (0.07) 88.30 (-0.04)
all - encodings (elongated words, emoticons, punctns.,
uppercase) 89.16 (0.06) 88.39 (0.05)
all - target 72.97 (-16.13) 68.96 (-19.38)
all - context 85.02 (-4.08) 85.93 (-2.41)
useful than using context features alone. Nonethe-
less, adding context features improves the F-scores
by roughly 2 to 4 points.
5 Conclusions
We created two state-of-the-art SVM classifiers, one
to detect the sentiment of messages and one to de-
tect the sentiment of a term within a message. Our
submissions on tweet data stood first in both these
subtasks of the SemEval-2013 competition ?Detect-
ing Sentiment in Twitter?. We implemented a variety
of features based on surface form and lexical cate-
gories. The sentiment lexicon features (both manu-
ally created and automatically generated) along with
ngram features (both word and character ngrams)
led to the most gain in performance.
326
Acknowledgments
We thank Colin Cherry for providing his SVM code
and for helpful discussions.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Cynthia Chew and Gunther Eysenbach. 2010. Pan-
demics in the Age of Twitter: Content Analysis of
Tweets during the 2009 H1N1 Outbreak. PLoS ONE,
5(11):e14118+, November.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
Lin C.-J. 2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine Learning
Research, 9:1871?1874.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-Speech Tagging for
Twitter: Annotation, Features, and Experiments. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter
Sentiment Classification using Distant Supervision. In
Final Projects from CS224N for Spring 2008/2009 at
The Stanford Natural Language Processing Group.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?04, pages 168?
177, New York, NY, USA. ACM.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as electronic
word of mouth. Journal of the American Society for
Information Science and Technology, 60(11):2169?
2188.
Benjamin Mandel, Aron Culotta, John Boulahanis,
Danielle Stark, Bonnie Lewis, and Jeremy Rodrigue.
2012. A demographic analysis of online sentiment
during hurricane irene. In Proceedings of the Second
Workshop on Language in Social Media, LSM ?12,
pages 27?36, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions Evoked by Common Words and Phrases: Using
Mechanical Turk to Create an Emotion Lexicon. In
Proceedings of the NAACL-HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California.
Saif Mohammad and Tony Yang. 2011. Tracking Sen-
timent in Mail: How Genders Differ on Emotional
Axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 70?79, Portland, Ore-
gon. Association for Computational Linguistics.
Saif Mohammad. 2012. #Emotional Tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*SEM), pages 246?
255, Montre?al, Canada. Association for Computa-
tional Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment Classification Using
Machine Learning Techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86, Philadelphia, PA.
Marcel Salathe? and Shashank Khandelwal. 2011. As-
sessing vaccination sentiments with online social me-
dia: Implications for infectious disease dynamics and
control. PLoS Computational Biology, 7(10).
Sudha Verma, Sarah Vieweg, William Corvey, Leysia
Palen, James Martin, Martha Palmer, Aaron Schram,
and Kenneth Anderson. 2011. Natural language pro-
cessing to the rescue? extracting ?situational aware-
ness? tweets during mass emergency. In International
AAAI Conference on Weblogs and Social Media.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter. In
Proceedings of the International Workshop on Seman-
tic Evaluation, SemEval ?13, Atlanta, Georgia, USA,
June.
327
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437?442,
Dublin, Ireland, August 23-24, 2014.
NRC-Canada-2014: Detecting Aspects and Sentiment
in Customer Reviews
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. Mohammad
National Research Council Canada
1200 Montreal Rd., Ottawa, ON, Canada
{Svetlana.Kiritchenko, Xiaodan.Zhu, Colin.Cherry, Saif.Mohammad}
@nrc-cnrc.gc.ca
Abstract
Reviews depict sentiments of customers
towards various aspects of a product or
service. Some of these aspects can be
grouped into coarser aspect categories.
SemEval-2014 had a shared task (Task 4)
on aspect-level sentiment analysis, with
over 30 teams participated. In this pa-
per, we describe our submissions, which
stood first in detecting aspect categories,
first in detecting sentiment towards aspect
categories, third in detecting aspect terms,
and first and second in detecting senti-
ment towards aspect terms in the laptop
and restaurant domains, respectively.
1 Introduction
Automatically identifying sentiment expressed in
text has a number of applications, including track-
ing sentiment towards products, movies, politi-
cians, etc.; improving customer relation models;
and detecting happiness and well-being. In many
applications, it is important to associate sentiment
with a particular entity or an aspect of an entity.
For example, in reviews, customers might express
different sentiment towards various aspects of a
product or service they have availed. Consider:
The lasagna was great, but the service
was a bit slow.
The review is for a restaurant, and we can gather
from it that the customer has a positive sentiment
towards the lasagna they serve, but a negative sen-
timent towards the service.
The SemEval-2014 Task 4 (Aspect Based Sen-
timent Analysis) is a shared task where given a
customer review, automatic systems are to deter-
mine aspect terms, aspect categories, and senti-
ment towards these aspect terms and categories.
An aspect term is defined to be an explicit men-
tion of a feature or component of the target prod-
uct or service. The example sentence above has
Restaurants Laptops
Term T-Sent. Cat. C-Sent. Term T-Sent.
3 2 1 1 3 1
Table 1: Rank obtained by NRC-Canada in vari-
ous subtasks of SemEval-2014 Task 4.
the aspect term lasagna. Similar aspect terms can
be grouped into aspect categories. For example,
lasagna and other food items can be grouped into
the aspect category of ?food?. In Task 4, customer
reviews are provided for two domains: restaurants
and laptops. A fixed set of five aspect categories
is defined for the restaurant domain: food, ser-
vice, price, ambiance, and anecdotes. Automatic
systems are to determine if any of those aspect
categories are described in a review. The exam-
ple sentence above describes the aspect categories
of food (positive sentiment) and service (negative
sentiment). For the laptop reviews, there is no as-
pect category detection subtask. Further details of
the task and data can be found in the task descrip-
tion paper (Pontiki et al., 2014).
We present an in-house sequence tagger to de-
tect aspect terms and supervised classifiers to de-
tect aspect categories, sentiment towards aspect
terms, and sentiment towards aspect categories. A
summary of the ranks obtained by our submissions
to the shared task is provided in Table 1.
2 Lexical Resources
2.1 Unlabeled Reviews Corpora
Apart from the training data provided for Task 4,
we compiled large corpora of reviews for restau-
rants and laptops that were not labeled for aspect
terms, aspect categories, or sentiment. We gen-
erated lexicons from these corpora and used them
as a source of additional features in our machine
learning systems.
437
Yelp restaurant reviews corpus: The Yelp
Phoenix Academic Dataset
1
contains customer re-
views posted on the Yelp website. The businesses
for which the reviews are posted are classified into
over 500 categories. Further, many of the busi-
nesses are assigned multiple business categories.
We identified all food-related business categories
(58 categories) that were grouped along with the
category ?restaurant? and extracted all customer
reviews for these categories. We will refer to this
corpus of 183,935 reviews as the Yelp restaurant
reviews corpus.
Amazon laptop reviews corpus: McAuley and
Leskovec (2013) collected reviews posted on
Amazon.com from June 1995 to March 2013. A
subset of this corpus is marked as reviews for elec-
tronic products. We extracted from this subset all
reviews that mention either laptop or notebook.
We will refer to this collection of 124,712 reviews
as the Amazon laptop reviews corpus.
Both the Yelp and the Amazon reviews have one
to five star ratings associated with each review. We
treated the one- and two-star reviews as negative
reviews, and the four- and five-star reviews as pos-
itive reviews.
2.2 Lexicons
Sentiment Lexicons: From the Yelp restaurant
reviews corpus, we automatically created an in-
domain sentiment lexicon for restaurants. Follow-
ing Turney and Littman (2003) and Mohammad
et al. (2013), we calculated a sentiment score for
each term w in the corpus:
score (w) = PMI (w , pos)?PMI (w ,neg) (1)
where pos denotes positive reviews and neg de-
notes negative reviews. PMI stands for pointwise
mutual information:
PMI (w , pos) = log
2
freq (w , pos) ?N
freq (w) ? freq (pos)
(2)
where freq (w, pos) is the number of times a term
w occurs in positive reviews, freq (w) is the to-
tal frequency of term w in the corpus, freq (pos)
is the total number of tokens in positive reviews,
and N is the total number of tokens in the cor-
pus. PMI (w ,neg) was calculated in a similar
way. Since PMI is known to be a poor estima-
tor of association for low-frequency events, we ig-
nored terms that occurred less than five times in
each (positive and negative) groups of reviews.
1
http://www.yelp.com/dataset_challenge
A positive sentiment score indicates a greater
overall association with positive sentiment,
whereas a negative score indicates a greater asso-
ciation with negative sentiment. The magnitude is
indicative of the degree of association.
Negation words (e.g., not, never) can signifi-
cantly affect the sentiment of an expression (Zhu
et al., 2014). Therefore, when generating the sen-
timent lexicons we distinguished terms appearing
in negated contexts (defined as text spans between
a negation word and a punctuation mark) and af-
firmative (non-negated) contexts. The sentiment
scores were then calculated separately for the two
types of contexts. For example, the term good in
affirmative contexts has a sentiment score of 1.2
whereas the same term in negated contexts has a
score of -1.4. We built two lexicons, Yelp Restau-
rant Sentiment AffLex and Yelp Restaurant Senti-
ment NegLex, as described in (Kiritchenko et al.,
2014).
Similarly, we generated in-domain sentiment
lexicons from the Amazon laptop reviews corpus.
In addition, we employed existing out-of-
domain sentiment lexicons: (1) large-coverage au-
tomatic tweet sentiment lexicons, Hashtag Sen-
timent lexicons and Sentiment140 lexicons (Kir-
itchenko et al., 2014), and (2) three manually cre-
ated sentiment lexicons, NRC Emotion Lexicon
(Mohammad and Turney, 2010), Bing Liu?s Lex-
icon (Hu and Liu, 2004), and the MPQA Subjec-
tivity Lexicon (Wilson et al., 2005).
Yelp Restaurant Word?Aspect Association
Lexicon: The Yelp restaurant reviews corpus was
also used to generate a lexicon of terms associated
with the aspect categories of food, price, service,
ambiance, and anecdotes. Each sentence of the
corpus was labeled with zero, one, or more of the
five aspect categories by our aspect category clas-
sification system (described in Section 5). Then,
for each term w and each category c an associa-
tion score was calculated as follows:
score (w , c) = PMI (w , c)? PMI (w ,?c) (3)
2.3 Word Clusters
Word clusters can provide an alternative represen-
tation of text, significantly reducing the sparsity
of the token space. Using Brown clustering algo-
rithm (Brown et al., 1992), we generated 1,000
word clusters from the Yelp restaurant reviews
corpus. Additionally, we used publicly available
438
word clusters generated from 56 million English-
language tweets (Owoputi et al., 2013).
3 Subtask 1: Aspect Term Extraction
The objective of this subtask is to detect aspect
terms in sentences. We approached this problem
using in-house entity-recognition software, very
similar to the system used by de Bruijn et al.
(2011) to detect medical concepts. First, sen-
tences were tokenized to split away punctuation,
and then the token sequence was tagged using a
semi-Markov tagger (Sarawagi and Cohen, 2004).
The tagger had two possible tags: O for outside,
and T for aspect term, where an aspect term could
tag a phrase of up to 5 consecutive tokens. The
tagger was trained using the structured Passive-
Aggressive (PA) algorithm with a maximum step-
size of C = 1 (Crammer et al., 2006).
Our features can be divided into two categories:
emission and transition features. Emission fea-
tures couple the tag sequence y to the input w.
Most of these work on the token level, and con-
join features of each token with the tag covering
that token. If a token is the first or last token cov-
ered by a tag, then we produce a second copy of
each of its features to indicate its special position.
Let w
i
be the token being tagged; its token fea-
ture templates are: token-identity within a win-
dow (w
i?2
. . . w
i+2
), lower-cased token-identity
within a window (lc(w
i?2
) . . . lc(w
i+2
)), and pre-
fixes and suffixes of w
i
(up to 3 characters in
length). There are only two phrase-level emission
feature templates: the cased and uncased identity
of the entire phrase covered by a tag, which al-
low the system to memorize complete terms such
as, ?getting a table? or ?fish and chips.? Transi-
tion features couple tags with tags. Let the cur-
rent tag be y
j
. Its transition feature templates are
short n-grams of tag identities: y
j
; y
j
, y
j?1
; and
y
j
, y
j?1
, y
j?2
.
During development, we experimented with the
training algorithm, trying both PA and the simpler
structured perceptron (Collins, 2002). We also
added the lowercased back-off features. In Ta-
ble 2, we re-test these design decisions on the test
set, revealing that lower-cased back-off features
made a strong contribution, while PA training was
perhaps not as important. Our complete system
achieved an F1-score of 80.19 on the restaurant
domain and 68.57 on the laptop domain, ranking
third among 24 teams in both.
Restaurants
System P R F1
NRC-Canada (All) 84.41 76.37 80.19
All ? lower-casing 83.68 75.49 79.37
All ? PA + percep 83.37 76.45 79.76
Laptops
System P R F1
NRC-Canada (All) 78.77 60.70 68.57
All ? lower-casing 78.11 60.55 68.22
All ? PA + percep 77.76 61.47 68.66
Table 2: Test set ablation experiments for Sub-
task 1: Aspect Term Detection.
4 Subtask 2: Aspect Term Polarity
In this subtask, the goal is to detect sentiment ex-
pressed towards a given aspect term. For example,
in sentence ?The asian salad is barely eatable.? the
aspect term asian salad is referred to with negative
sentiment. There were defined four categories of
sentiment: positive, negative, neutral, or conflict.
The conflict category is assigned to cases where
an aspect term is mentioned with both positive and
negative sentiment.
To address this multi-class classification prob-
lem, we trained a linear SVM classifier using
the LibSVM software (Chang and Lin, 2011).
Sentences were first tokenized and parsed with
the Stanford CoreNLP toolkits
2
to obtain part-of-
speech (POS) tags and (collapsed) typed depen-
dency parse trees (de Marneffe et al., 2006). Then,
features were extracted from (1) the target term it-
self; (2) its surface context, i.e., a window of n
words surrounding the term; (3) the parse context,
i.e., the nodes in the parse tree that are connected
to the target term by at most three edges.
Surface features: (1) unigrams (single words)
and bigrams (2-word sequences) extracted from a
term and its surface context; (2) context-target bi-
grams (i.e., bigrams formed by a word from the
surface context and a word from the term itself).
Lexicon features: (1) the number of posi-
tive/negative tokens; (2) the sum of the tokens?
sentiment scores; (3) the maximal sentiment score.
The lexicon features were calculated for each
manually and automatically created sentiment lex-
icons described in Section 2.2.
Parse features: (1) word- and POS-ngrams in
2
http://nlp.stanford.edu/software/corenlp.shtml
439
Laptops Rest.
System Acc. Acc.
NRC-Canada (All) 70.49 80.16
All ? sentiment lexicons 63.61 77.13
All ? Yelp lexicons 68.65 77.85
All ? Amazon lex. 68.13 80.11
All ? manual lexicons 67.43 78.66
All ? tweet lexicons 69.11 78.57
All ? parse features 69.42 78.40
Table 3: Test set ablation experiments for Sub-
task 2: Aspect Term Polarity.
the parse context; (2) context-target bigrams, i.e.,
bigrams composed of a word from the parse con-
text and a word from the term; (3) all paths that
start or end with the root of the target terms. The
idea behind the use of the parse features is that
sometimes an aspect term is separated from its
modifying sentiment phrase and the surface con-
text is insufficient or even misleading for detect-
ing sentiment expressed towards the aspect. For
example, in sentence ?The food, though different
from what we had last time, is actually great? the
word great is much closer to the word food in the
parse tree than in the surface form. Furthermore,
the features derived from the parse context can
help resolve local syntactic ambiguity (e.g., the
word bad in the phrase ?a bad sushi lover? modi-
fies lover and not sushi).
Table 3 presents the results of our official sub-
mission on the test sets for the laptop and restau-
rant domains. On the laptop dataset, our system
achieved the accuracy of 70.49 and was ranked
first among 32 submissions from 29 teams. From
the ablation experiments we see that the most sig-
nificant gains come from the use of the sentiment
lexicons; without the lexicon features the perfor-
mance of the system drops by 6.88 percentage
points. Observe that the features derived from
the out-of-domain Yelp Restaurant Sentiment lex-
icon are very helpful on the laptop domain. The
parse features proved to be useful as well; they
contribute 1.07 percentage points to the final per-
formance. On the restaurant data, our system ob-
tained the accuracy of 80.16 and was ranked sec-
ond among 36 submissions from 29 teams.
5 Subtask 3: Aspect Category Detection
The objective of this subtask is to detect aspect
categories discussed in a given sentence. There
Restaurants
System P R F1
NRC-Canada (All) 91.04 86.24 88.58
All ? lex. resources 86.53 78.34 82.23
All ?W?A lexicon 88.47 80.10 84.08
All ? word clusters 90.84 86.15 88.43
All ? post-processing 91.47 84.78 88.00
Table 4: Test set ablation experiments for Sub-
task 3: Aspect Category Detection. ?W?A lexicon?
stands for Yelp Restaurant Word?Aspect Associa-
tion Lexicon.
are 5 pre-defined categories for the restaurant do-
main: food, price, service, ambience, and anec-
dotes/miscellaneous. Each sentence can be la-
beled with one or more categories from the pre-
defined set. No aspect categories were defined for
the laptop domain.
We addressed the subtask as a multi-class multi-
label text classification problem. Five binary one-
vs-all Support Vector Machine (SVM) classifiers
were built, one for each category. The parameter C
was optimized through cross-validation separately
for each classifier. Sentences were tokenized
and stemmed with Porter stemmer (Porter, 1980).
Then, the following sets of features were gener-
ated for each sentence: ngrams, stemmed ngrams,
character ngrams, non-contiguous ngrams, word
cluster ngrams, and lexicon features. For the lex-
icon features, we used the Yelp Restaurant Word?
Aspect Association Lexicon and calculated the cu-
mulative scores of all terms appeared in the sen-
tence for each aspect category. Separate scores
were calculated for unigram and bigram entries.
Sentences with no category assigned by any of the
five classifiers went through the post-processing
step. For each such sentence, a category c with the
maximal posterior probability P (c|d) was identi-
fied and the sentence was labeled with the category
c if P (c|d) ? 0.4.
Table 4 presents the results on the restaurant test
set. Our system obtained the F1-score of 88.58
and was ranked first among 21 submissions from
18 teams. Among the lexical resources (lexicons
and word clusters) employed in the system, the
Word?Aspect Association Lexicon provided the
most gains: an increase in F1-score of 4.5 points.
The post-processing step also proved to be benefi-
cial: the recall improved by 1.46 points increasing
the overall F1-score by 0.58 points.
440
6 Subtask 4: Aspect Category Polarity
In the Aspect Category Polarity subtask, the goal
is to detect the sentiment expressed towards a
given aspect category in a given sentence. For
each input pair (sentence, aspect category), the
output is a single sentiment label: positive, neg-
ative, neutral, or conflict.
We trained one multi-class SVM classifier
(Crammer and Singer, 2002) for all aspect cate-
gories. The feature set was extended to incorpo-
rate the information about a given aspect category
c using a domain adaptation technique (Daum?e III,
2007) as follows: each feature f had two copies,
f general (for all the aspect categories) and f c
(for the specific category of the instance). For ex-
ample, for the input pair (?The bread is top notch
as well.?, ?food?) two copies of the unigram top
would be used: top general and top food . With
this setup the classifier can take advantage of the
whole training dataset to learn common sentiment
features (e.g., the word good is associated with
positive sentiment for all aspect categories). At the
same time, aspect-specific sentiment features can
be learned from the training instances pertaining
to a specific aspect category (e.g., the word deli-
cious is associated with positive sentiment for the
category ?food?).
Sentences were tokenized and part-of-speech
tagged with CMU Twitter NLP tool (Gimpel et al.,
2011). Then, each sentence was represented as a
feature vector with the following groups of fea-
tures: ngrams, character ngrams, non-contiguous
ngrams, POS tags, cluster ngrams, and lexicon
features. The lexicon features were calculated as
described in Section 4.
A sentence can refer to more than one aspect
category with different sentiment. For example,
in the sentence ?The pizza was delicious, but the
waiter was rude.?, food is described with posi-
tive sentiment while service with negative. If the
words delicious and rude occur in the training set,
the classifier can learn that delicious usually refers
to food (with positive sentiment) and rude to ser-
vice (with negative sentiment). If these terms do
not appear in the training set, their polarities can
still be inferred from sentiment lexicons. How-
ever, sentiment lexicons do not distinguish among
aspect categories and would treat both words, de-
licious and rude, as equally applicable to both cat-
egories, ?food? and ?service?. To (partially) over-
come this problem, we applied the Yelp Restau-
Restaurants
System Accuracy
NRC-Canada (All) 82.93
All ? lexical resources 74.15
All ? lexicons 75.32
All ? Yelp lexicons 79.22
All ? manual lexicons 82.44
All ? tweet lexicons 84.10
All ? word clusters 82.93
All ? aspect term features 82.54
Table 5: Test set ablation experiments for Sub-
task 4: Aspect Category Polarity.
rant Word?Aspect Association Lexicon to collect
all the terms having a high or moderate associ-
ation with the given aspect category (e.g., pizza,
delicious for the category ?food? and waiter, rude
for the category ?service?). Then, the feature set
described above was augmented with the same
groups of features generated just for the terms as-
sociated with the given category. We call these
features aspect term features.
Table 5 presents the results on the test set for
the restaurant domain. Our system achieved the
accuracy of 82.93 and was ranked first among 23
submissions from 20 teams. The ablation exper-
iments demonstrate the significant impact of the
lexical resources employed in the system: 8.78
percentage point gain in accuracy. The major ad-
vantage comes from the sentiment lexicons, and
specifically from the in-domain Yelp Restaurant
Sentiment lexicons. The out-of-domain tweet sen-
timent lexicons did not prove useful on this sub-
task. Also, word clusters did not offer additional
benefits on top of those provided by the lexicons.
The use of aspect term features resulted in gains
of 0.39.
7 Conclusion
The paper describes supervised machine-learning
approaches to detect aspect terms and aspect cat-
egories and to detect sentiment expressed towards
aspect terms and aspect categories in customer re-
views. Apart from common surface-form features
such as ngrams, our approaches benefit from the
use of existing and newly created lexical resources
such as word?aspect association lexicons and sen-
timent lexicons. Our submissions stood first on 3
out of 4 subtasks, and within the top 3 best results
on all 6 task-domain evaluations.
441
References
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265?292.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 256 ? 263.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,
Joel Martin, and Xiaodan Zhu. 2011. Machine-
learned solutions for three stages of clinical infor-
mation extraction: the state of the art at i2b2 2010.
Journal of the American Medical Informatics Asso-
ciation, 18(5):557?562.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC ?06.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL ?11.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal of Artificial Intelligence Research
(to appear).
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165?172. ACM.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ?13, Atlanta, Georgia, USA,
June.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ?14, Dublin, Ireland, August.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 3:130?137.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1185?1192.
Peter Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354, Stroudsburg, PA, USA.
Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, ACL ?14.
442
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443?447,
Dublin, Ireland, August 23-24, 2014.
NRC-Canada-2014: Recent Improvements in
the Sentiment Analysis of Tweets
Xiaodan Zhu, Svetlana Kiritchenko, and Saif M. Mohammad
National Research Council Canada
Ottawa, Ontario, Canada K1A 0R6
{xiaodan.zhu,svetlana.kiritchenko,saif.mohammad}@nrc-cnrc.gc.ca
Abstract
This paper describes state-of-the-art statis-
tical systems for automatic sentiment anal-
ysis of tweets. In a Semeval-2014 shared
task (Task 9), our submissions obtained
highest scores in the term-level sentiment
classification subtask on both the 2013 and
2014 tweets test sets. In the message-level
sentiment classification task, our submis-
sions obtained highest scores on the Live-
Journal blog posts test set, sarcastic tweets
test set, and the 2013 SMS test set. These
systems build on our SemEval-2013 senti-
ment analysis systems (Mohammad et al.,
2013) which ranked first in both the term-
and message-level subtasks in 2013. Key
improvements over the 2013 systems are
in the handling of negation. We create
separate tweet-specific sentiment lexicons
for terms in affirmative contexts and in
negated contexts.
1 Introduction
Automatically detecting sentiment of tweets (and
other microblog posts) has attracted extensive
interest from both the academia and industry.
The Conference on Semantic Evaluation Exercises
(SemEval) organizes a shared task on the senti-
ment analysis of tweets with two subtasks. In the
message-level task, the participating systems are
to identify whether a tweet as a whole expresses
positive, negative, or neutral sentiment. In the
term-level task, the objective is to determine the
sentiment of a marked target term (a single word
or a multi-word expression) within the tweet. Our
submissions stood first in both subtasks in 2013.
This paper describes improvements over that sys-
Evaluation Set Term-level Task Message-level Task
Twt14 1 4
Twt13 1 2
Sarc14 3 1
LvJn14 2 1
SMS13 2 1
Table 1: Overall rank of NRC-Canada sentiment
analysis models in Semeval-2014 Task 9 under the
constrained condition. The rows are five evalua-
tion datasets and the columns are the two subtasks.
tem and the subsequent submissions to the 2014
shared task (Rosenthal et al., 2014).
The training data for the SemEval-2014 shared
task is same as that of SemEval-2013 (about
10,000 tweets). The 2014 test set has five sub-
categories: a tweet set provided newly in 2014
(Twt14), the tweet set used for testing in the 2013
shared task (Twt13), a set of tweets that are sarcas-
tic (Sarc14), a set of sentences from the blogging
website LiveJournal (LvJn14), and the set of SMS
messages used for testing in the 2013 shared task
(SMS13). Instances from these categories were in-
terspersed in the provided test set. The partici-
pants were not told about the source of the indi-
vidual messages. The objective was to determine
how well a system trained on tweets generalizes to
texts from other domains.
Our submissions to SemEval-2014 Task 9,
ranked first in five out of the ten subtask?dataset
combinations. In the other evaluation sets as well,
our submissions performed competitively. The
results are summarized in Table 1. As we will
show, automatically generated tweet-specific lexi-
cons were especially helpful in all subtask?dataset
combinations. The results also show that even
though our models are trained only on tweets, they
generalize well to data from other domains.
443
Our systems are based on supervised SVMs and
a number of surface-form, semantic, and senti-
ment features. The major improvement in our
2014 system over the 2013 system is in the way it
handles negation. Morante and Sporleder (2012)
define negation to be ?a grammatical category that
allows the changing of the truth value of a propo-
sition?. Negation is often expressed through the
use of negative signals or negators, words such as
isnt and never, and it can significantly affect the
sentiment of its scope. We create separate tweet-
specific sentiment lexicons for terms in affirmative
contexts and in negated contexts. That is, we au-
tomatically determine the average sentiment of a
term when occurring in an affirmative context, and
separately the average sentiment of a term when
occurring in a negated context.
2 Our Systems
Our SemEval-2014 systems are based on our
SemEval-2013 systems (Mohammad et al., 2013).
For completeness, we briefly revisit our previ-
ous approach, which uses support vector machine
(SVM) as the classification algorithm and lever-
ages the following features.
Lexicon features These features are generated by
using three manually constructed sentiment lexi-
cons and two automatically constructed lexicons.
The manually constructed lexicons include the
NRC Emotion Lexicon (Mohammad and Turney,
2010; Mohammad and Yang, 2011), the MPQA
Lexicon (Wilson et al., 2005), and the Bing Liu
Lexicon (Hu and Liu, 2004). The two automati-
cally constructed lexicons, the Hashtag Sentiment
Lexicon and the Sentiment140 Lexicon, were cre-
ated specifically for tweets (Mohammad et al.,
2013).
The sentiment score of each term (e.g., a word
or bigram) in the automatically constructed lexi-
cons is computed by measuring the PMI (point-
wise mutual information) between the term and
the positive or negative category of tweets using
the formula:
SenScore (w) = PMI(w, pos)? PMI(w, neg)
(1)
where w is a term in the lexicons. PMI(w, pos)
is the PMI score between w and the positive class,
and PMI(w, neg) is the PMI score between w
and the negative class. Therefore, a positive Sen-
Score (w) suggests a stronger association of word
w with positive sentiment and vice versa. The
magnitude indicates the strength of association.
Note that the sentiment class of the tweets used
to construct the lexicons was automatically iden-
tified either from hashtags or from emoticons as
described in (Mohammad et al., 2013).
With these lexicons available, the following fea-
tures were extracted for a text span. Here a text
span can be a target term, its context, or an en-
tire tweet, depending on the task. The lexicon
features include: (1) the number of sentiment to-
kens in a text span; sentiment tokens are word
tokens whose sentiment scores are not zero in a
lexicon; (2) the total sentiment score of the text
span:
?
w?textSpan
SenScore (w); (3) the maxi-
mal score: max
w?textSpan
SenScore (w); (4) the
total positive and negative sentiment scores of the
text span; (5) the sentiment score of the last token
in the text span. Note that all these features are
generated, when applicable, by using each of the
sentiment lexicons mentioned above.
Ngrams We employed two types of ngram fea-
tures: word ngrams and character ngrams. The
former reflect the presence or absence of contigu-
ous or non-contiguous sequences of words, and
the latter are sequences of prefix/suffix characters
in each word. These features are same as in our
last year?s submission.
Negation The number of negated contexts. Our
definition of a negated context follows Pang et al.
(2002), which will be described in more details be-
low in Section 2.1.
POS The number of occurrences of each part-
of-speech tag. We tokenized and part-of-speech
tagged the tweets with the Carnegie Mellon Uni-
versity (CMU) Twitter NLP tool (Gimpel et al.,
2011).
Cluster features The CMU POS-tagging tool pro-
vides the token clusters produced with the Brown
clustering algorithm from 56 million English-
language tweets. These 1,000 clusters serve as an
alternative representation of tweet content, reduc-
ing the sparsity of the token space.
Encodings The encoding features are derived
from hashtags, punctuation marks, emoticons,
elongated words, and uppercased words.
For the term-level task, all the above features
are extracted for target terms and their context,
where a context is a window of words surround-
ing a target term. For the message-level task, the
features are extracted from the whole tweet.
444
In the term-level task, we used the LIB-
SVM (Chang and Lin, 2011) tool with the follow-
ing parameters: -t 0 -b 1 -m 1000. The total num-
ber of features is about 115,000. In the message-
level task, we used an in-house implementation of
SVM with a linear kernel. The parameter C was
set to 0.005. The total number of features was
about 1.5 million.
2.1 Improving Lexicons and Negation
Models
An important advantage of our SemEval-2013
systems comes from the use of the two high-
coverage tweet-specific sentiment lexicons. In
the SemEval-2014 submissions, we improve these
lexicons by incorporating negation modeling into
the lexicon generation process.
2.1.1 Improving Sentiment Lexicons
A word in a negated context has a different eval-
uative nature than the same word in an affirma-
tive (non-negated) context. We have proposed a
lexicon-based approach (Kiritchenko et al., 2014)
to determining the sentiment of words in these two
situations by automatically creating separate senti-
ment lexicons for the affirmative and negated con-
texts. In this way, we do not need to employ any
explicit assumptions to model negation.
To achieve this, a tweet corpus is split into two
parts: Affirmative Context Corpus and Negated
Context Corpus. Following the work of Pang et al.
(2002), we define a negated context as a segment
of a tweet that starts with a negation word (e.g., no,
shouldn?t) and ends with one of the punctuation
marks: ?,?, ?.?, ?:?, ?;?, ?!?, ???. The list of negation
words was adopted from Christopher Potts? senti-
ment tutorial.
1
Thus, part of a tweet that is marked
as negated is included into the negated context cor-
pus while the rest of the tweet becomes part of the
affirmative context corpus. The sentiment label
for the tweet is kept unchanged in both corpora.
Then, we generate an affirmative context lexicon
from the affirmative context corpus and a negated
context lexicon from the negated context corpus
using the technique described in (Kiritchenko et
al., 2014).
Furthermore, we refined the method of con-
structing the negated context lexicons by split-
ting a negated context into two parts: the imme-
diate context consisting of a single token that di-
rectly follows a negation word, and the distant
1
http://sentiment.christopherpotts.net/lingstruc.html
context consisting of the rest of the tokens in the
negated context. This has two benefits. Intu-
itively, negation affects words directly following
the negation words more strongly than more dis-
tant words. Second, immediate-context scores are
less noisy. Our simple negation scope identifica-
tion algorithm can at times fail and include parts
of a tweet that are not actually negated (e.g., if a
punctuation mark is missing). Overall, a sentiment
word can have up to three scores, one for affirma-
tive context, one for immediate negated context,
and one for distant negated context.
We reconstructed the Hashtag Sentiment Lexi-
con and the Sentiment140 Lexicon with this ap-
proach and used them in our SemEval-2014 sys-
tems.
2.1.2 Discriminating Negation Words
Different negation words, e.g., never and didn?t,
can have different effects on sentiment (Zhu et al.,
2014; Taboada et al., 2011). In our SemEval-2014
submission, we discriminate negation words in the
term-level models. For example, the word accept-
able appearing in a sentence this is never accept-
able is marked as acceptable beNever, while in
the sentence this is not acceptable, it is marked
as acceptable beNot. In this way, different nega-
tors (e.g., be not and be never) are treated differ-
ently. Note that we do not differentiate the tense
and person of auxiliaries in order to reduce sparse-
ness (e.g., was not and am not are treated in the
same way). This new representation is used to ex-
tract ngrams and lexicon-based features.
3 Results
Overall performance The evaluation metric used
in the competition is the macro-averaged F-
measure calculated over the positive and negative
categories. Table 2 presents the overall perfor-
mance of our models. NRC13 and NRC14 are
the systems we submitted to SemEval-2013 and
SemEval-2014, respectively. The integers in the
brackets are our official ranks in SemEval-2014
under the constrained condition.
In the term-level task, our submission ranked
first on the two Tweet datasets among 14 teams.
The results show that we achieved significant im-
provements over our last year?s submission: the F-
score improves from 85.19 to 86.63 on the Twt14
data and from 89.10 to 90.14 on the Twt13 data.
More specifically, on the Twt14 data, the approach
described in Section 2.1.1 improved our F-score
445
Term-level Message-level
NRC13 NRC14 NRC13 NRC14
Twt14 85.19 86.63(1) 68.88 69.85(4)
Twt13 89.10 90.14(1) 69.02 70.75(2)
Sarc14 78.16 77.13(3) 47.64 58.16(1)
LvJn14 84.96 85.49(2) 74.01 74.84(1)
SMS13 88.34 88.03(2) 68.34 70.28(1)
Table 2: Overall performance of the NRC-Canada
sentiment analysis systems.
from 85.19 to 86.37, and discriminating nega-
tion words (discussed in Section 2.1.2) further im-
proved the F-score from 86.37 to 86.63.
Our system ranked second on the LvJn14 and
SMS13 dataset. Note that the term-level system
that ranked first on LvJn14 performed worse than
our system on SMS13 and the system that ranked
first on SMS13 showed worse results than ours on
LvJn14, indicating that our term-level models in
general have good generalizability on these two
out-of-domain datasets.
On the message-level task, again the NRC14
system showed significant improvements over the
last year?s system on all five datasets. It achieved
the second best result on the Twt13 data and the
fourth result on the Twt14 data among 42 teams.
It was also the best system to predict sentiment in
sarcastic tweets (Sarc14). Furthermore, the system
proved to generalize well to other types of short
informal texts; it placed first on the two out-of-
domain datasets: SMS13 and LvJn14. We observe
a major improvement of our message-level model
on Sarc14 over our last year?s model, but as the
size of Sarc14 is small (86 tweets), more data and
analysis would be desirable to help better under-
stand this phenomenon.
Contribution of features Table 3 presents the re-
sults of ablation experiments on all five test sets for
the term-level task. The features derived from the
manual and automatic lexicons proved to be useful
on four datasets. The only exception is the Sarc14
data where removing lexicon features results in no
performance improvement. Considering that this
test set is very small (only about 100 test terms),
further investigation would be desirable if a larger
dataset becomes available. Also, in sarcasm the
real sentiment of a text span may be different from
its literal sentiment. In such a situation, a system
that correctly recognizes the literal sentiment may
actually make mistakes in capturing the real sen-
timent. The last two rows in Table 3 show the re-
sults obtained when the features are extracted only
from the target (and not from its context) and when
they are extracted only from the context of the tar-
get (and not from the target itself). Observe that
even though the context may influence the polar-
ity of the target, using target features alone is sub-
stantially more useful than using context features
alone. Nonetheless, adding context features im-
proves the F-scores in general.
On the message-level task (Table 4), the fea-
tures derived from the sentiment lexicons and, in
particular, from our large-coverage tweet-specific
lexicons turned out to be the most influential. The
use of the lexicons provided consistent gains of 9?
11 percentage points not only on tweet datasets,
but also on out-of-domain SMS and LiveJournal
data. Note that removing the features derived from
the manual lexicons as well as removing the ngram
features improves the performance on the Twt14
dataset. However, this effect is not observed on
the Twt13 and the out-of-domain test sets. The
possible explanation of this phenomenon is minor
overfitting on the tweet data.
4 Conclusions
We presented supervised statistical systems for
message-level and term-level sentiment analysis
of tweets. They incorporate many surface-form,
semantic, and sentiment features. Among sub-
missions from over 40 teams in the Semeval-
2014 shared task ?Sentiment Analysis in Twit-
ter?, our submissions ranked first in five out of
the ten subtask-dataset combinations. The sin-
gle most useful set of features are those obtained
from automatically generated tweet-specific lexi-
cons. We obtained significant improvements over
our previous system (which ranked first in the
2013 shared task) notably by estimating the senti-
ment of words in affirmative and negated contexts
separately. Also, since different negation words
impact sentiment differently, we modeled different
negation words separately in our term-level sys-
tem. This too led to an improvement in F-score.
The results on different kinds of evaluation sets
show that even though our systems are trained only
on tweets, they generalize well to text from other
domains such as blog posts and SMS messages.
Many of the resources we created and used are
made freely available.
2
2
www.purl.com/net/sentimentoftweets
446
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13
all features 86.63 90.14 77.13 85.49 88.03
all - lexicons 81.98 86.25 80.74 80.00 83.91
all - manu. lex. 86.08 89.25 75.32 84.13 87.69
all - auto. lex. 86.05 88.32 80.38 83.96 86.18
all - ngrams 83.31 86.67 72.95 81.58 82.41
all - target 72.93 74.19 63.09 72.21 69.34
all - context 84.40 88.83 77.22 82.99 87.97
Table 3: Term-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the feature
groups removed.
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13
all features 69.85 70.75 58.16 74.84 70.28
all - lexicons 60.59 60.04 47.17 65.80 60.56
all - manu. lex. 71.84 69.84 53.34 73.41 66.60
all - auto. lex. 63.40 65.08 47.57 71.76 66.94
all - ngrams 70.02 67.90 44.58 74.43 68.45
Table 4: Message-level Task: The macro-averaged F-scores obtained on the 5 test sets with one of the
feature groups removed.
Acknowledgments
We thank Colin Cherry for providing his SVM
code and for helpful discussions.
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1?27:27.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of KDD,
pages 168?177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif Moham-
mad. 2014. Sentiment analysis of short informal
texts. (To appear) Journal of Artificial Intelligence
Research.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad and Tony (Wenda) Yang. 2011.
Tracking sentiment in mail: How genders differ on
emotional axes. In Proceedings of the ACL Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, Portland, OR, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the International Workshop on Semantic Evalua-
tion, SemEval ?13, Atlanta, Georgia, USA, June.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational linguistics, 38(2):223?260.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, PA.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Proceedings of
SemEval-2014, Dublin, Ireland.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, HLT ?05, pages 347?354, Stroudsburg, PA,
USA.
Xiaodan Zhu, Hongyu Guo, Saif Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of ACL, Baltimore, Maryland, USA, June.
447
