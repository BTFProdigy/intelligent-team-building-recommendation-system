Proceedings of the ACL 2010 System Demonstrations, pages 48?53,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Personalising speech-to-speech translation in the EMIME project
Mikko Kurimo1?, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,
Yong Guan5, Teemu Hirsima?ki1, Reima Karhila1, Simon King2, Hui Liang3, Keiichiro
Oura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,
Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi2
1 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,
Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,
6 University of Cambridge, UK
?Corresponding author: Mikko.Kurimo@tkk.fi
Abstract
In the EMIME project we have studied un-
supervised cross-lingual speaker adapta-
tion. We have employed an HMM statisti-
cal framework for both speech recognition
and synthesis which provides transfor-
mation mechanisms to adapt the synthe-
sized voice in TTS (text-to-speech) using
the recognized voice in ASR (automatic
speech recognition). An important ap-
plication for this research is personalised
speech-to-speech translation that will use
the voice of the speaker in the input lan-
guage to utter the translated sentences in
the output language. In mobile environ-
ments this enhances the users? interaction
across language barriers by making the
output speech sound more like the origi-
nal speaker?s way of speaking, even if she
or he could not speak the output language.
1 Introduction
A mobile real-time speech-to-speech translation
(S2ST) device is one of the grand challenges in
natural language processing (NLP). It involves
several important NLP research areas: auto-
matic speech recognition (ASR), statistical ma-
chine translation (SMT) and speech synthesis, also
known as text-to-speech (TTS). In recent years
significant advance have also been made in rele-
vant technological devices: the size of powerful
computers has decreased to fit in a mobile phone
and fast WiFi and 3G networks have spread widely
to connect them to even more powerful computa-
tion servers. Several hand-held S2ST applications
and devices have already become available, for ex-
ample by IBM, Google or Jibbigo1, but there are
still serious limitations in vocabulary and language
selection and performance.
When an S2ST device is used in practical hu-
man interaction across a language barrier, one fea-
ture that is often missed is the personalization of
the output voice. Whoever speaks to the device in
what ever manner, the output voice always sounds
the same. Producing high-quality synthesis voices
is expensive and even if the system had many out-
put voices, it is hard to select one that would sound
like the input voice. There are many features in the
output voice that could raise the interaction expe-
rience to a much more natural level, for example,
emotions, speaking rate, loudness and the speaker
identity.
After the recent development in hidden Markov
model (HMM) based TTS, it has become possi-
ble to adapt the output voice using model trans-
formations that can be estimated from a small
number of speech samples. These techniques, for
instance the maximum likelihood linear regres-
sion (MLLR), are adopted from HMM-based ASR
where they are very powerful in fast adaptation of
speaker and recording environment characteristics
(Gales, 1998). Using hierarchical regression trees,
the TTS and ASR models can further be coupled
in a way that enables unsupervised TTS adaptation
(King et al, 2008). In unsupervised adaptation
samples are annotated by applying ASR. By elimi-
nating the need for human intervention it becomes
possible to perform voice adaptation for TTS in
almost real-time.
The target in the EMIME project2 is to study
unsupervised cross-lingual speaker adaptation for
S2ST systems. The first results of the project have
1http://www.jibbigo.com
2http://emime.org
48
been, for example, to bridge the gap between the
ASR and TTS (Dines et al, 2009), to improve
the baseline ASR (Hirsima?ki et al, 2009) and
SMT (de Gispert et al, 2009) systems for mor-
phologically rich languages, and to develop robust
TTS (Yamagishi et al, 2010). The next step has
been preliminary experiments in intra-lingual and
cross-lingual speaker adaptation (Wu et al, 2008).
For cross-lingual adaptation several new methods
have been proposed for mapping the HMM states,
adaptation data and model transformations (Wu et
al., 2009).
In this presentation we can demonstrate the var-
ious new results in ASR, SMT and TTS. Even
though the project is still ongoing, we have an
initial version of mobile S2ST system and cross-
lingual speaker adaptation to show.
2 Baseline ASR, TTS and SMT systems
The baseline ASR systems in the project are devel-
oped using the HTK toolkit (Young et al, 2001)
for Finnish, English, Mandarin and Japanese. The
systems can also utilize various real-time decoders
such as Julius (Kawahara et al, 2000), Juicer at
IDIAP and the TKK decoder (Hirsima?ki et al,
2006). The main structure of the baseline sys-
tems for each of the four languages is similar and
fairly standard and in line with most other state-of-
the-art large vocabulary ASR systems. Some spe-
cial flavors for have been added, such as the mor-
phological analysis for Finnish (Hirsima?ki et al,
2009). For speaker adaptation, the MLLR trans-
formation based on hierarchical regression classes
is included for all languages.
The baseline TTS systems in the project utilize
the HTS toolkit (Yamagishi et al, 2009) which
is built on top of the HTK framework. The
HMM-based TTS systems have been developed
for Finnish, English, Mandarin and Japanese. The
systems include an average voice model for each
language trained over hundreds of speakers taken
from standard ASR corpora, such as Speecon
(Iskra et al, 2002). Using speaker adaptation
transforms, thousands of new voices have been
created (Yamagishi et al, 2010) and new voices
can be added using a small number of either su-
pervised or unsupervised speech samples. Cross-
lingual adaptation is possible by creating a map-
ping between the HMM states in the input and the
output language (Wu et al, 2009).
Because the resources of the EMIME project
have been focused on ASR, TTS and speaker
adaptation, we aim at relying on existing solu-
tions for SMT as far as possible. New methods
have been studied concerning the morphologically
rich languages (de Gispert et al, 2009), but for the
S2ST system we are currently using Google trans-
late3.
3 Demonstrations to show
3.1 Monolingual systems
In robust speech synthesis, a computer can learn
to speak in the desired way after processing only a
relatively small amount of training speech. The
training speech can even be a normal quality
recording outside the studio environment, where
the target speaker is speaking to a standard micro-
phone and the speech is not annotated. This differs
dramatically from conventional TTS, where build-
ing a new voice requires an hour or more careful
repetition of specially selected prompts recorded
in an anechoic chamber with high quality equip-
ment.
Robust TTS has recently become possible us-
ing the statistical HMM framework for both ASR
and TTS. This framework enables the use of ef-
ficient speaker adaptation transformations devel-
oped for ASR to be used also for the TTS mod-
els. Using large corpora collected for ASR, we can
train average voice models for both ASR and TTS.
The training data may include a small amount of
speech with poor coverage of phonetic contexts
from each single speaker, but by summing the ma-
terial over hundreds of speakers, we can obtain
sufficient models for an average speaker. Only a
small amount of adaptation data is then required to
create transformations for tuning the average voice
closer to the target voice.
In addition to the supervised adaptation us-
ing annotated speech, it is also possible to em-
ploy ASR to create annotations. This unsu-
pervised adaptation enables the system to use a
much broader selection of sources, for example,
recorded samples from the internet, to learn a new
voice.
The following systems will demonstrate the re-
sults of monolingual adaptation:
1. In EMIME Voice cloning in Finnish and En-
glish the goal is that the users can clone their
own voice. The user will dictate for about
3http://translate.google.com
49
Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.
Blue markers show male speakers and red markers show female speakers. Available online via
http://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.1
10 minutes and then after half an hour of
processing time, the TTS system has trans-
formed the average model towards the user?s
voice and can speak with this voice. The
cloned voices may become especially valu-
able, for example, if a person?s voice is later
damaged in an accident or by a disease.
2. In EMIME Thousand voices map the goal is
to browse the world?s largest collection of
synthetic voices by using a world map in-
terface (Yamagishi et al, 2010). The user
can zoom in the world map and select any
voice, which are organized according to the
place of living of the adapted speaker, to ut-
ter the given sentence. This interactive ge-
ographical representation is shown in Figure
1. Each marker corresponds to an individual
speaker. Blue markers show male speakers
and red markers show female speakers. Some
markers are in arbitrary locations (in the cor-
rect country) because precise location infor-
mation is not available for all speakers. This
geographical representation, which includes
an interactive TTS demonstration of many of
the voices, is available from the URL pro-
vided. Clicking on a marker will play syn-
thetic speech from that speaker4. As well as
4Currently the interactive mode supports English and
Spanish only. For other languages this only provides pre-
being a convenient interface to compare the
many voices, the interactive map is an attrac-
tive and easy-to-understand demonstration of
the technology being developed in EMIME.
3. The models developed in the HMM frame-
work can be demonstrated also in adapta-
tion of an ASR system for large-vocabulary
continuous speech recognition. By utilizing
morpheme-based language models instead of
word-based models the Finnish ASR system
is able to cover practically an unlimited vo-
cabulary (Hirsima?ki et al, 2006). This is
necessary for morphologically rich languages
where, due to inflection, derivation and com-
position, there exists so many different word
forms that word based language modeling be-
comes impractical.
3.2 Cross-lingual systems
In the EMIME project the goal is to learn cross-
lingual speaker adaptation. Here the output lan-
guage ASR or TTS system is adapted from speech
samples in the input language. The results so far
are encouraging, especially for TTS: Even though
the cross-lingual adaptation may somewhat de-
grade the synthesis quality, the adapted speech
now sounds more like the target speaker. Sev-
eral recent evaluations of the cross-lingual speaker
synthesised examples, but we plan to add an interactive type-
in text-to-speech feature in the near future.
50
Figure 2: All English HTS voices can be used as online TTS on the geographical map.
adaptation methods can be found in (Gibson et al,
2010; Oura et al, 2010; Liang et al, 2010; Oura
et al, 2009).
The following systems have been created to
demonstrate cross-lingual adaptation:
1. In EMIME Cross-lingual Finnish/English
and Mandarin/English TTS adaptation the
input language sentences dictated by the user
will be used to learn the characteristics of her
or his voice. The adapted cross-lingual model
will be used to speak output language (En-
glish) sentences in the user?s voice. The user
does not need to be bilingual and only reads
sentences in their native language.
2. In EMIME Real-time speech-to-speech mo-
bile translation demo two users will interact
using a pair of mobile N97 devices (see Fig-
ure 3). The system will recognize the phrase
the other user is speaking in his native lan-
guage and translate and speak it in the native
language of the other user. After a few sen-
tences the system will have the speaker adap-
tation transformations ready and can apply
them in the synthesized voices to make them
sound more like the original speaker instead
of a standard voice. The first real-time demo
version is available for the Mandarin/English
language pair.
3. The morpheme-based translation system for
Finnish/English and English/Finnish can be
compared to a word based translation for
arbitrary sentences. The morpheme-based
approach is particularly useful for language
pairs where one or both languages are mor-
phologically rich ones where the amount and
complexity of different word forms severely
limits the performance for word-based trans-
lation. The morpheme-based systems can
learn translation models for phrases where
morphemes are used instead of words (de
Gispert et al, 2009). Recent evaluations (Ku-
rimo et al, 2009) have shown that the perfor-
mance of the unsupervised data-driven mor-
pheme segmentation can rival the conven-
tional rule-based ones. This is very useful if
hand-crafted morphological analyzers are not
available or their coverage is not sufficient for
all languages.
Acknowledgments
The research leading to these results was partly
funded from the European Communitys Seventh
51
  
ASR SMT TTS
Cross-lingualSpeaker adaptation
Speakeradaptation
input outputspeech
Figure 3: EMIME Real-time speech-to-speech
mobile translation demo
Framework Programme (FP7/2007-2013) under
grant agreement 213845 (the EMIME project).
References
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum Bayes risk combination of transla-
tion hypotheses from alternative morphological de-
compositions. In Proc. NAACL-HLT.
J. Dines, J. Yamagishi, and S. King. 2009. Measur-
ing the gap between HMM-based ASR and TTS. In
Proc. Interspeech ?09, Brighton, UK.
M. Gales. 1998. Maximum likelihood linear transfor-
mations for HMM-based speech recognition. Com-
puter Speech and Language, 12(2):75?98.
M. Gibson, T. Hirsima?ki, R. Karhila, M. Kurimo,
and W. Byrne. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis using two-pass decision tree construction. In
Proc. of ICASSP, page to appear, March.
T. Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S.
Virpioja, and J. Pylkko?nen. 2006. Unlimited vo-
cabulary speech recognition with morph language
models applied to finnish. Computer Speech & Lan-
guage, 20(4):515?541, October.
T. Hirsima?ki, J. Pylkko?nen, and M Kurimo. 2009.
Importance of high-order n-gram models in morph-
based speech recognition. IEEE Trans. Audio,
Speech, and Language Process., 17:724?732.
D. Iskra, B. Grosskopf, K. Marasek, H. van den
Heuvel, F. Diehl, and A. Kiessling. 2002.
SPEECON speech databases for consumer devices:
Database specification and validation. In Proc.
LREC, pages 329?333.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda,
N. Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-
mamoto, A. Yamada, T. Utsuro, and K. Shikano.
2000. Free software toolkit for japanese large vo-
cabulary continuous speech recognition. In Proc.
ICSLP-2000, volume 4, pages 476?479.
S. King, K. Tokuda, H. Zen, and J. Yamagishi. 2008.
Unsupervised adaptation for HMM-based speech
synthesis. In Proc. Interspeech 2008, pages 1869?
1872, September.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
H. Liang, J. Dines, and L. Saheer. 2010. A
comparison of supervised and unsupervised cross-
lingual speaker adaptation approaches for HMM-
based speech synthesis. In Proc. of ICASSP, page
to appear, March.
Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-
jam Wester, and Keiichi Tokuda. 2009. Unsuper-
vised speaker adaptation for speech-to-speech trans-
lation system. In Proc. SLP (Spoken Language Pro-
cessing), number 356 in 109, pages 13?18.
K. Oura, K. Tokuda, J. Yamagishi, S. King, and
M. Wester. 2010. Unsupervised cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ICASSP, page to appear, March.
Y.-J. Wu, S. King, and K. Tokuda. 2008. Cross-lingual
speaker adaptation for HMM-based speech synthe-
sis. In Proc. of ISCSLP, pages 1?4, December.
Y.-J. Wu, Y. Nankaku, and K. Tokuda. 2009. State
mapping based method for cross-lingual speaker
adaptation in HMM-based speech synthesis. In
Proc. of Interspeech, pages 528?531, September.
J. Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,
K. Tokuda, S. King, and S. Renals. 2009. Robust
speaker-adaptive HMM-based text-to-speech syn-
thesis. IEEE Trans. Audio, Speech and Language
Process., 17(6):1208?1230. (in press).
J. Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,
J. Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, and
M. Kurimo. 2010. Thousands of voices for hmm-
based speech synthesis. IEEE Trans. Speech, Audio
& Language Process. (in press).
52
S. Young, G. Everman, D. Kershaw, G. Moore, J.
Odell, D. Ollason, V. Valtchev, and P. Woodland,
2001. The HTK Book Version 3.1, December.
53
Proceedings of the ACL-HLT 2011 System Demonstrations, pages 80?85,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
A Speech-based Just-in-Time Retrieval System using Semantic Search
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner
Idiap Research Institute
Rue Marconi 19, CP 592
1920 Martigny, Switzerland
{apbelis,myazdani,ananchen,pgarner}@idiap.ch
Abstract
The Automatic Content Linking Device is a
just-in-time document retrieval system which
monitors an ongoing conversation or a mono-
logue and enriches it with potentially related
documents, including multimedia ones, from
local repositories or from the Internet. The
documents are found using keyword-based
search or using a semantic similarity measure
between documents and the words obtained
from automatic speech recognition. Results
are displayed in real time to meeting partici-
pants, or to users watching a recorded lecture
or conversation.
1 Introduction
Enriching a monologue or a conversation with re-
lated content, such as textual or audio-visual docu-
ments on the same topic, is a task with multiple ap-
plications in the field of computer-mediated human-
human communication. In this paper, we describe
the Automatic Content Linking Device (ACLD), a
system that analyzes spoken input from one or more
speakers using automatic speech recognition (ASR),
in order to retrieve related content, in real-time, from
a variety of repositories. These include local doc-
ument databases or archives of multimedia record-
ings, as well as websites. Local repositories are
queried using a keyword-based search engine, or us-
ing a semantic similarity measure, while websites
are queried using commercial search engines.
We will first describe the scenarios of use of the
ACLD in Section 2, and review previous systems for
just-in-time retrieval in Section 3. The ACLD com-
ponents will be outlined in Sections 4.1 to 4.5. Four
types of evaluation results obtained with our system
will finally be summarized in Sections 5.1 to 5.4.
2 Content Linking: Scenarios of Use
Just-in-time information retrieval, i.e. finding useful
documents without the need for a user to initiate a di-
rect search for them, is one of the ways in which the
large quantity of knowledge that is available in net-
worked environments can be efficiently put to use.
To perform this task, a system must consider ex-
plicit and implicit input from users, mainly speech
or typed input, and attempt to model their context,
in order to provide recommendations, which users
are free to consult if they feel the need for additional
information.
One of the main scenarios of use for the ACLD
involves people taking part in meetings, who often
mention documents containing facts under discus-
sion, but do not have the time to search for them
without interrupting the discussion flow. The ACLD
performs this search for them. Moreover, as the
ACLD was developed on meetings from the AMI
Corpus, it can also perform the same operations on
a replayed meeting, as a complement to a meet-
ing browser, for development or demonstration pur-
poses.
In a second scenario, content linking is performed
over live or recorded lectures, for instance in a
computer-assisted learning environment for individ-
ual students. The ACLD enriches the lectures with
related material drawn from various repositories,
through a search process that can be guided in real
80
time by its user. The advantage of real-time con-
tent linking over a more static enrichment, such as
the Feynman lectures at Microsoft Research,1 is that
users can tune search parameters at will while view-
ing the lecture.
3 Just-in-Time Retrieval Systems
The first precursors to the ACLD were the Fixit
query-free search system (Hart and Graham, 1997),
the Remembrance Agent for just-in-time retrieval
(Rhodes and Maes, 2000), and the Implicit Queries
(IQ) system (Dumais et al, 2004). Fixit monitored
the state of a user?s interaction with a diagnostic
system, and excerpts from maintenance manuals de-
pending on the interaction state. The Remembrance
Agent was integrated to the Emacs text editor, and
ran searches over emails or notes at regular time in-
tervals (every few seconds) using the latest 20?500
words typed by the user. The IQ system generated
context-sensitive searches based on a user?s ongoing
activities on their computer, such as writing email.
A version of the Remembrance Agent called Jim-
miny was conceived as a wearable assistant for tak-
ing notes, but ASR was only simulated for evalua-
tion (Rhodes, 1997).
The Watson system (Budzik and Hammond,
2000) monitored the user?s operations in a text ed-
itor, but proposed a more complex mechanism than
the Remembrance Agent for selecting terms for
queries, which were directed to a web search engine.
Another assistant for an authoring environment was
developed in the A-Propos project (Puerta Melguizo
et al, 2008). A query-free system was designed for
enriching television news with articles from the Web
(Henziker et al, 2005).
The FAME interactive space (Metze and al.,
2006), which provides multi-modal access to record-
ings of lectures via a table top interface, bears many
similarities to the ACLD. However, it requires the
use of specific voice commands by one user only,
and does not spontaneously follow a conversation.
More recently, several speech-based search en-
gines have become available, including as smart
phone applications. Conversely, many systems al-
low searching of spoken document archives.2 Inspi-
1See http://research.microsoft.com/apps/tools/tuva/.
2See workshops at http://www.searchingspeech.org.
ration from these approaches, which are not query-
free, can nevertheless be useful to just-in-time re-
trieval. Other related systems are the Speech Spot-
ter (Goto et al, 2004) and a personal assistant using
dual-purpose speech (Lyons et al, 2004), which en-
able users to search for information using commands
that are identified in the speech flow.
The ACLD improves over numerous past ones by
giving access to indexed multimedia recordings as
well as websites, with fully operational ASR and se-
mantic search, as we now explain.
4 Description of the ACLD
The architecture of the ACLD comprises the follow-
ing functions: document preparation, text extraction
and indexing; input sensing and query preparation;
search and integration of results; user interface to
display the results.
4.1 Document Preparation and Indexing
The preparation of the local database of documents
for content linking involves mainly the extraction of
text, and then the indexing of the documents, which
is done using Apache Lucene software. Text can be
extracted from a large variety of formats (includ-
ing MS Office, PDF, and HTML) and hierarchies
of directories are recursively scanned. The docu-
ment repository is generally prepared before using
the ACLD, but users can also add files at will. Be-
cause past discussions are relevant to subsequent
ones, they are passed through offline ASR and then
chunked into smaller units (e.g. of fixed length, or
based on a homogeneous topic). The resulting texts
are indexed along with the other documents.
The ACLD uses external search engines to search
in external repositories, for instance the Google Web
search API or the Google Desktop application to
search the user?s local drives.
4.2 Sensing the User?s Information Needs
We believe that the most useful cues about the in-
formation needs of participants in a conversation,
or of people viewing a lecture, are the words that
are spoken during the conversation or the lecture.
For the ACLD, we use the AMI real-time ASR sys-
tem (Garner et al, 2009). One of its main features
is the use of a pre-compiled grammar, which al-
lows it to retain accuracy even when running in real-
81
time on a low resource machine. Of course, when
content linking is done over past meetings, or for
text extraction from past recordings, the ASR sys-
tem runs slower than real-time to maximize accuracy
of recognition. However, the accuracy of real-time
ASR is only about 1% lower than the unconstrained
mode which takes several times real-time.
For the RT07 meeting data, when using signals
from individual headset microphones, the AMI ASR
system reaches about 38% word error rate. With
a microphone array, this increases to about 41%.
These values indicate that enough correct words are
sensed by the real-time ASR to make it applicable
to the ACLD, and that a robust search mechanism
could help avoiding retrieval errors due to spurious
words.
The words obtained from the ASR are filtered for
stopwords, so that only content words are used for
search; our list has about 80 words. Furthermore,
we believe that existing knowledge about the impor-
tant terminology of a domain or project can be used
to increase the impact of specific words on search. A
list of pre-specified keywords can be defined based
on such knowledge and can be modified while run-
ning the ACLD. For instance, for remote control de-
sign as in the AMI Corpus scenario, this list includes
about 30 words such as ?chip?, ?button?, or ?mate-
rial?. If any of them is detected in the ASR output,
then their importance is increased for searching, but
otherwise all the other words from the ASR (minus
the stopwords) are used for constructing the query.
4.3 Querying the Document Database
The Query Aggregator (QA) uses the ASR words
to retrieve the most relevant documents from one or
more databases. The current version of the ACLD
makes use of semantic search (see next subsection),
while previous versions used word-based search
from Apache Lucene for local documents, or from
the Google Web or Google Desktop APIs. ASR
words from the latest time frame are put together
(minus the stopwords) to form queries, and recog-
nized keywords are boosted in the Lucene query.
Queries are formulated at regular time intervals, typ-
ically every 15-30 seconds, or on demand. This du-
ration is a compromise between the need to gather
enough words for search, and the need to refresh the
search results reasonably often.
The results are integrated with those from the
previous time frame, using a persistence model to
smooth variations over time. The model keeps track
of the salience of each result, initialized from their
ranking among the search results, then decreasing
in time unless the document is again retrieved. The
rate of decrease (or its inverse, persistence) can be
tuned by the user, but in any case, all past results are
saved by the user interface and can be consulted at
any time.
4.4 Semantic Search over Wikipedia
The goal of our method for semantic search is to
improve the relevance of the retrieved documents,
and to make the mechanism more robust to noise
from the ASR. We have applied to document re-
trieval the graph-based model of semantic rela-
tedness that we recently developed (Yazdani and
Popescu-Belis, 2010), which is also related to other
proposals (Strube and Ponzetto, 2006; Gabrilovich
and Markovitch, 2007; Yeh et al, 2009).
The model is grounded in a measure of seman-
tic relatedness between text fragments, which is
computed using random walk over the network of
Wikipedia articles ? about 1.2 million articles from
the WEX data set (Metaweb Technologies, 2010).
The articles are linked through hyperlinks, and also
through lexical similarity links that are constructed
upon initialization. The random walk model allows
the computation of a visiting probability (VP) from
one article to another, and then a VP between sets of
articles, which has been shown to function as a mea-
sure of semantic relatedness, and has been applied
to various NLP problems. To compute relatedness
between two text fragments, these are first projected
represented into the network by the ten closest arti-
cles in terms of lexical similarity.
For the ACLD, the use of semantic relatedness for
document retrieval amounts to searching, in a very
large collection, the documents that are the most
closely related to the words from the ASR in a given
timeframe. Here, the document collection is (again)
the set of Wikipedia articles from WEX, and the goal
is to return the eight most related articles. Such a
search is hard to perform in real time; hence, the so-
lution that was found makes use of several approx-
imations to compute average VP between the ASR
fragment and all articles in the Wikipedia network.
82
Figure 1: Unobtrusive UI displaying document results.
Hovering the mouse over a result (here, the most relevant
one) displays a pop-up window with more information
about it.
4.5 The User Interface (UI)
The main goal of the UI is to make available all
information produced by the system, in a config-
urable way, allowing users to see a larger or smaller
amount of information according to their needs. A
modular architecture with a flexible layout has been
implemented, maximizing the accessibility but also
the understandability of the results, and displaying
also intermediary data such as ASR words and found
keywords. The UI displays up to five widgets, which
can be arranged at will:
1. ASR results with highlighted keywords.
2. Tag-cloud of keywords, coding for recency and
frequency of keywords.
3. Names of documents and past meeting snippets
found by the QA.
4. Names of web pages found via the Google API.
5. Names of local files found via the Google
Desktop API.
Two main arrangements are intended, though
many others are possible: an informative full-screen
UI, shown in Figure 2 with widgets 1?4; and an un-
obtrusive widget UI, with superposed tabs, shown in
Figure 1 with widget 3.
The document names displayed in widgets 3?5
function as hyperlinks to the documents, launching
appropriate external viewers when the user clicks on
them. Moreover, when hovering over a document
name, a pop-up window displays metadata and doc-
ument excerpts that match words from the query, as
an explanation of why the document was retrieved.
5 Evaluation Experiments
Four types of evidence for the relevance and utility
of the ACLD are summarized in this section.
5.1 Feedback from Potential Users
The ACLD was demonstrated to about 50 potential
users (industrial partners, focus groups, etc.) in a
series of sessions of about 30 minutes, starting with
a presentation of the ACLD and continuing with a
discussion and elicitation of feedback. The overall
concept was generally found useful, with positive
verbal evaluations. Feedback for smaller and larger
improvements was collected: e.g. the importance of
matching context, linking on demand, and the UI un-
obtrusive mode.
5.2 Pilot Task-based Experiments
A pilot experiment was conducted by a team at the
University of Edinburgh with an earlier version of
the unobtrusive UI. Four subjects had to complete a
task that was started in previous meetings (ES2008a-
b-c from the AMI Corpus). The goal was to compare
two conditions, with vs. without the ACLD, in terms
of satisfied constraints, overall efficiency, and satis-
faction. Two pilot runs have shown that the ACLD
was being consulted about five times per meeting.
Therefore, many more runs are required to reach sta-
tistical significance of observations, and remain to
be executed depending on future resources.
5.3 Usability Evaluation of the UI
The UI was submitted to a usability evaluation ex-
periment with nine non-technical subjects. The
subjects used the ACLD over a replayed meeting
recording, and were asked to perform several tasks
with it, such as adding a keyword to monitor, search-
ing for a word, or changing the layout. The subjects
then rated usability-related statements, leading to an
assessment on the System Usability Scale (Brooke,
1996).
The overall usability score was 68% (SD: 10),
which is considered as ?acceptable usability? for the
SUS. The average task-completion time was 45?
75 seconds. In free-form feedback, subjects found
the system helpful to review meetings but also lec-
tures, appreciated the availability of documents, but
also noted that search results (with keyword-based
83
Figure 2: Full screen UI with four widgets: ASR, keywords, document and website results.
search) were often irrelevant. They also suggested
simplifying the UI (menus, layout) and embedding
a media player for use in the meeting or lecture re-
play scenario.
5.4 Comparing the Relevance of
Keyword-based vs. Semantic Search
We compared the output of semantic search with
that of keyword-based search. The ASR transcript
of one AMI meeting (ES2008d) was passed to both
search methods, and ?evaluation snippets? contain-
ing the manual transcript for one-minute excerpts,
accompanied by the 8-best Wikipedia articles found
by each method were produced. Overall, 36 snip-
pets were generated. The manual transcript shown to
subjects was enriched with punctuation and speak-
ers? names, and the names of the Wikipedia pages
were placed on each side of the transcript frame.
Subjects were then asked to read each snippet,
and decide which of the two document sets was the
most relevant to the discussion taking place, i.e. the
most useful as a suggestion to the participants. They
could also answer ?none?, and could consult the re-
sult if necessary.
Results were obtained from 8 subjects, each see-
ing 9 snippets out of 36. Every snippet was thus
seen by two subjects. The subjects agreed on 23
(64%) snippets and disagreed on 13 (36%). In fact,
the number of true disagreements not including the
answer ?none? was only 7 out of 36.
Over the 23 snippets on which subjects agreed,
the result of semantic search was judged more rel-
evant than that of keyword search for 19 snippets
(53% of the total), and the reverse for 4 snippets
only (11%). Alternatively, if one counts the votes
cast by subjects in favor of each system, regardless
of agreement, then semantic search received 72%
of the votes and keyword-based only 28%. These
numbers show that semantic search quite clearly im-
proves relevance in comparison to keyword-based
one, but there is still room for improvement.
6 Conclusion
The ACLD is, to the best of our knowledge, the
first just-in-time retrieval system to use spontaneous
speech and to support access to multimedia docu-
ments and web pages, using a robust semantic search
method. Future work will aim at improving the rel-
evance of semantic search, at modeling context to
84
improve timing of results, and at inferring relevance
feedback from users. The ACLD should also be ap-
plied to specific use cases, and an experiment with
group work in a learning environment is under way.
Acknowledgments
The authors gratefully acknowledge the support
of the EU AMI and AMIDA Integrated Projects
(http://www.amiproject.org) and of the Swiss IM2
NCCR on Interactive Multimodal Information Man-
agement (http://www.im2.ch).
References
John Brooke. 1996. SUS: A ?quick and dirty? us-
ability scale. In Patrick W. Jordan, Bruce Thomas,
Bernard A. Weerdmeester, and Ian L. McClelland, ed-
itors, Usability evaluation in industry, pages 189?194.
Taylor and Francis, London, UK.
Jay Budzik and Kristian J. Hammond. 2000. User inter-
actions with everyday applications as context for just-
in-time information access. In IUI 2000 (5th Interna-
tional Conference on Intelligent User Interfaces), New
Orleans, LA.
Susan Dumais, Edward Cutrell, Raman Sarin, and Eric
Horvitz. 2004. Implicit Queries (IQ) for contextual-
ized search. In SIGIR 2004 (27th ACM SIGIR Confer-
ence) Demonstrations, page 534, Sheffield, UK.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using Wikipedia-based
explicit semantic analysis. In IJCAI 2007 (20th Inter-
national Joint Conference on Artificial Intelligence),
pages 6?12, Hyderabad, India.
Philip N. Garner, John Dines, Thomas Hain, Asmaa
El Hannani, Martin Karafiat, Danil Korchagin, Mike
Lincoln, Vincent Wan, and Le Zhang. 2009. Real-
time ASR from meetings. In Interspeech 2009 (10th
Annual Conference of the Intl. Speech Communication
Association), pages 2119?2122, Brighton, UK.
Masataka Goto, Koji Kitayama, Katsunobu Itou, and Tet-
sunori Kobayashi. 2004. Speech Spotter: On-demand
speech recognition in human-human conversation on
the telephone or in face-to-face situations. In ICSLP
2004 (8th International Conference on Spoken Lan-
guage Processing), pages 1533?1536, Jeju Island.
Peter E. Hart and Jamey Graham. 1997. Query-free in-
formation retrieval. IEEE Expert: Intelligent Systems
and Their Applications, 12(5):32?37.
Monika Henziker, Bay-Wei Chang, Brian Milch, and
Sergey Brin. 2005. Query-free news search. World
Wide Web: Internet and Web Information Systems,
8:101?126.
Kent Lyons, Christopher Skeels, Thad Starner, Cor-
nelis M. Snoeck, Benjamin A. Wong, and Daniel Ash-
brook. 2004. Augmenting conversations using dual-
purpose speech. In UIST 2004 (17th Annual ACM
Symposium on User Interface Software and Technol-
ogy), pages 237?246, Santa Fe, NM.
Metaweb Technologies. 2010. Freebase Wikipedia Ex-
traction (WEX). http://download.freebase.com/wex/.
Florian Metze and al. 2006. The ?Fame? interactive
space. In Machine Learning for Multimodal Interac-
tion II, LNCS 3869, pages 126?137. Springer, Berlin.
Maria Carmen Puerta Melguizo, Olga Monoz Ramos,
Lou Boves, Toine Bogers, and Antal van den Bosch.
2008. A personalized recommender system for writ-
ing in the Internet age. In LREC 2008 Workshop on
NLP Resources, Algorithms, and Tools for Authoring
Aids, pages 21?26, Marrakech, Morocco.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time
information retrieval agents. IBM Systems Journal,
39(3-4):685?704.
Bradley J. Rhodes. 1997. The Wearable Remembrance
Agent: A system for augmented memory. Personal
Technologies: Special Issue on Wearable Computing,
1:218?224.
Michael Strube and Simone Paolo Ponzetto. 2006.
Wikirelate! Computing semantic relatedness using
Wikipedia. In AAAI 2006 (21st National Conference
on Artificial Intelligence), pages 1419?1424, Boston,
MA.
Majid Yazdani and Andrei Popescu-Belis. 2010. A ran-
dom walk framework to compute textual semantic sim-
ilarity: A unified model for three benchmark tasks. In
ICSC 2010 (4th IEEE International Conference on Se-
mantic Computing), pages 424?429, Pittsburgh, PA.
Eric Yeh, Daniel Ramage, Christopher D. Manning,
Eneko Agirre, and Aitor Soroa. 2009. WikiWalk: ran-
dom walks on Wikipedia for semantic relatedness. In
TextGraphs-4 (4th Workshop on Graph-based Methods
for NLP), pages 41?49, Singapore.
85
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 350?352,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
A Just-in-Time Document Retrieval System for Dialogues or Monologues
Andrei Popescu-Belis, Majid Yazdani, Alexandre Nanchen, and Philip N. Garner
Idiap Research Institute
Rue Marconi 19, Case Postale 592
1920 Martigny, Switzerland
{apbelis,myazdani,ananchen,pgarner}@idiap.ch
Abstract
The Automatic Content Linking Device is a
just-in-time document retrieval system that
monitors an ongoing dialogue or monologue
and enriches it with potentially related docu-
ments from local repositories or from the Web.
The documents are found using queries that
are built from the dialogue words, obtained
through automatic speech recognition. Re-
sults are displayed in real time to the dialogue
participants, or to people watching a recorded
dialogue or a talk. The system can be demon-
strated in both settings.
1 Introduction
The Automatic Content Linking Device (ACLD) is
a system that analyzes speech input from one or
more speakers using automatic speech recognition
(ASR), in order to retrieve related content, in real
time, from a variety of repositories. This paper de-
scribes the main components of the system and sum-
marizes evaluation results. The remainder of this
section introduces scenarios of use and previous sys-
tems with similar goals.
The first scenario of use involves people taking
part in meetings, who often mention documents con-
taining facts that are relevant to the current discus-
sion, but cannot search for them without interrupt-
ing the discussion flow. Our goal is to perform such
searches automatically. In a second scenario, search
is performed for live or recorded lectures, for in-
stance in a computer-assisted learning environment.
The ACLD enriches the lectures with related course
material, receiving real-time feedback from the user.
The ACLD improves over past systems by using
speech, by giving access to multimedia documents,
and by using semantic search. Its first precursors
were the Fixit query-free search system (Hart and
Graham, 1997), the Remembrance Agent for just-
in-time retrieval (Rhodes and Maes, 2000), and the
Implicit Queries system (Dumais et al, 2004). A
version of the Remembrance Agent called Jimminy
was conceived as a wearable assistant for taking
notes, but ASR was only simulated (Rhodes, 1997).
Watson monitored the user?s operations in a text
editor, and selected terms for web search (Budzik
and Hammond, 2000). Another authoring assistant
was developed in the A-Propos project (Puerta Mel-
guizo and al., 2008). Recently, several speech-
based search engines have been proposed, as well as
systems for searching spoken documents. For hu-
man dialogues in meetings, the FAME interactive
space (Metze and al., 2006) provided multi-modal
access to recordings of lectures via a table top in-
terface, but required specific voice commands from
one user only, and did not spontaneously follow a
conversation as the ACLD does.
2 Description of the ACLD
The architecture of the ACLD comprises modules
for: (1) document preparation and indexing; (2) in-
put sensing and query construction; (3) search and
integration of results; (4) user interaction.
2.1 Document Preparation and Indexing
The preparation of the local database of documents
available for search requires text extraction from
various file formats (like MS Office or PDF), and
350
document indexing, here using Apache Lucene. Past
meetings, when available, are automatically tran-
scribed, then chunked into smaller units, and in-
dexed along with the other documents. For search-
ing the Web, the system does not build indexes but
uses the Google Search API.
2.2 Sensing the User?s Information Needs
The ACLD uses the AMI real-time ASR system for
English (Garner and al., 2009), which has an ac-
ceptable accuracy for use with conversational speech
in the ACLD. When processing past recordings, the
ASR system can run slower than real-time to maxi-
mize its accuracy. If one or more pre-specified key-
words (based on domain knowledge) are detected in
the ASR output, then their importance is increased
for searching. Otherwise, all the words from the
ASR (except stopwords) are used for constructing
the query.
2.3 Querying the Document Database
The Query Aggregator component uses the ASR
words in order to retrieve the most relevant docu-
ments from a given database. The latest version
of the ACLD makes use of semantic search (see
below), but earlier versions used keyword-based
search from Apache Lucene for local documents.
Queries are formulated and launched at regular time
intervals, typically every 15-30 seconds, or on de-
mand. The search results are integrated with previ-
ous ones, using a persistence model that smoothes
variations in time by keeping track of the salience of
each result. Salience is initialized from the ranking
of search results, then decreases in time, or increases
if the document appears again among results. A his-
tory of all results is also accessible.
2.4 Semantic Search over Wikipedia
The goal of semantic search is to improve the rel-
evance of results with respect to the spoken words,
and to make search more robust to noise from ASR.
The method used here is adapted from a graph-based
measure of semantic relatedness between text frag-
ments (Yazdani and Popescu-Belis, 2010). Related-
ness is computed using random walk in a large net-
work of documents, here about 1.2 milion Wikipedia
articles from the WEX data set (Metaweb Technolo-
gies, 2010). These are linked by directional hy-
Figure 1: Unobtrusive UI of the ACLD displaying docu-
ment results. The pop-up window shows more details for
the first results.
perlinks, and also by lexical similarity links that
we construct upon initialization. The random walk
model allows the computation of the visiting proba-
bility (VP) from one document to another, and then
of the VP between sets of documents. This functions
as a measure of semantic relatedness, and has been
applied to several NLP problems by projecting the
text fragments to be compared onto the documents
in the network (Yazdani and Popescu-Belis, 2010).
For the ACLD, the use of semantic relatedness for
document retrieval amounts to searching, in a very
large collection, the documents that are the most
closely related to the words obtained from the ASR
in a given time frame. Here, we set the document
collection to Wikipedia (WEX). As the search is
hard to perform in real time, we made a series of
justified approximations to make it tractable.
2.5 The User Interface
The goal of the UI is to make ACLD information
available in a configurable way, allowing users to
see more or less information according to their own
needs. The UI displays up to four widgets, which
can be arranged at will, and contain: (1) ASR words
with highlighted keywords; (2) tag-cloud of key-
words, coding for recency and frequency; (3) links
to the current results from the local repository; (4)
links to the current Web search results.
Two main arrangements are intended: an infor-
mative full-screen UI (not shown here from lack of
space) and an unobtrusive UI, with superposed tabs,
shown in Figure 1 with the document result widget.
When hovering over a document name, a pop-up
window displays metadata and document excerpts
that match words from the query, as an explanation
for why the document was retrieved.
351
3 Evaluation of the ACLD
Four types of evidence for the relevance and util-
ity of the ACLD are summarized here. Firstly, the
ACLD was demonstrated to about 50 potential users
(industrial partners, focus groups, etc.), who found
the concept useful, and offered positive verbal eval-
uation, along with suggestions for smaller and larger
improvements.
Secondly, a pilot experiment was conducted with
a group using an earlier version of the UI. Two pilot
runs have shown that the ACLD was consulted about
five times per meeting, but many more runs are (still)
needed for statistical significance of observations.
Thirdly, the UI was tested in a usability evaluation
experiment with nine non-technical subjects, who
rated it as ?acceptable? (68%) on the System Usabil-
ity Scale, following a series of tasks they had to per-
form using it. Additional suggestions for changes
were received.
Finally, we compared offline the results of seman-
tic search with the keyword-based ones. We asked
eight subjects to read a series of nine meeting frag-
ments, and to decide which of the two results was
the most useful one (they could also answer ?none?).
Of a total of 36 snippets, each seen by two subjects,
there was agreement on 23 (64%) snippets and dis-
agreement on 13 (36%). In fact, if ?none? is ex-
cluded, there were only 7 true disagreements. Over
the 23 snippets on which the subjects agreed, the
result of semantic search was judged more relevant
than that of keyword search for 19 (53% of the to-
tal), and the reverse for 4 only (11%). Alternatively,
if one counts the votes cast by subjects in favor of
each system, regardless of agreement, then semantic
search received 72% of the votes and keyword-based
only 28%. Hence, semantic search already outper-
forms keyword based one.
4 Conclusion
The ACLD is, to the best of our knowledge, the
first just-in-time retrieval system to use spontaneous
speech and to support access to multimedia doc-
uments and to websites, using a robust semantic
search method. Future work should aim at improv-
ing the relevance of semantic search, at modeling
context to improve the timing of results, and at in-
ferring relevance feedback from users. The ACLD
should also be applied to specific use cases, and an
experiment with group discussions in a learning en-
vironment is under way.
Acknowledgments
We are grateful to the EU AMI and AMIDA Inte-
grated Projects and to the Swiss IM2 NCCR (In-
teractive Multimodal Information Management) for
supporting the development of the ACLD.
References
Jay Budzik and Kristian J. Hammond. 2000. User inter-
actions with everyday applications as context for just-
in-time information access. In IUI 2000 (5th Interna-
tional Conference on Intelligent User Interfaces), New
Orleans, LA.
Susan Dumais, Edward Cutrell, Raman Sarin, and Eric
Horvitz. 2004. Implicit Queries (IQ) for contextual-
ized search. In SIGIR 2004 (27th Annual ACM SIGIR
Conference) Demonstrations, page 534, Sheffield.
Philip N. Garner and al. 2009. Real-time ASR from
meetings. In Interspeech 2009 (10th Annual Confer-
ence of the International Speech Communication As-
sociation), pages 2119?2122, Brighton.
Peter E. Hart and Jamey Graham. 1997. Query-free in-
formation retrieval. IEEE Expert: Intelligent Systems
and Their Applications, 12(5):32?37.
Metaweb Technologies. 2010. Freebase Wikipedia Ex-
traction (WEX). http://download.freebase.com/wex/.
Florian Metze and al. 2006. The ?Fame? interactive
space. In Machine Learning for Multimodal Interac-
tion II, LNCS 3869, pages 126?137. Springer, Berlin.
Maria Carmen Puerta Melguizo and al. 2008. A person-
alized recommender system for writing in the Internet
age. In LREC 2008 Workshop on NLP Resources, Al-
gorithms, and Tools for Authoring Aids, pages 21?26,
Marrakech.
Bradley J. Rhodes and Pattie Maes. 2000. Just-in-time
information retrieval agents. IBM Systems Journal,
39(3-4):685?704.
Bradley J. Rhodes. 1997. The Wearable Remembrance
Agent: A system for augmented memory. Personal
Technologies: Special Issue on Wearable Computing,
1:218?224.
Majid Yazdani and Andrei Popescu-Belis. 2010. A ran-
dom walk framework to compute textual semantic sim-
ilarity: a unified model for three benchmark tasks. In
ICSC 2010 (4th IEEE International Conference on Se-
mantic Computing), pages 424?429, Pittsburgh, PA.
352
