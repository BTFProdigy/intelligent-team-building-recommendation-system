Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 376?383,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Multimodal Interface for Access to Content in the Home 
Michael Johnston 
AT&T Labs  
Research, 
Florham Park, 
New Jersey, USA 
johnston@ 
research. 
att.com 
Luis Fernando D?Haro 
Universidad Polit?cnica 
de Madrid,  
Madrid, Spain 
lfdharo@die. 
upm.es 
Michelle Levine 
AT&T Labs  
Research, 
Florham Park,  
New Jersey, USA 
mfl@research.
att.com 
Bernard Renger 
AT&T Labs  
Research, 
Florham Park,  
New Jersey, USA 
renger@ 
research. 
att.com 
 
Abstract 
In order to effectively access the rapidly 
increasing range of media content available 
in the home, new kinds of more natural in-
terfaces are needed.  In this paper, we ex-
plore the application of multimodal inter-
face technologies to searching and brows-
ing a database of movies.  The resulting 
system allows users to access movies using 
speech, pen, remote control, and dynamic 
combinations of these modalities. An ex-
perimental evaluation, with more than 40 
users, is presented contrasting two variants 
of the system: one combining speech with 
traditional remote control input and a sec-
ond where the user has a tablet display 
supporting speech and pen input. 
1 Introduction 
As traditional entertainment channels and the 
internet converge through the advent of technolo-
gies such as broadband access, movies-on-demand, 
and streaming video, an increasingly large range of 
content is available to consumers in the home.  
However, to benefit from this new wealth of con-
tent, users need to be able to rapidly and easily find 
what they are actually interested in, and do so ef-
fortlessly while relaxing on the couch in their liv-
ing room ? a location where they typically do not 
have easy access to the keyboard, mouse, and 
close-up screen display typical of desktop web 
browsing.  
Current interfaces to cable and satellite televi-
sion services typically use direct manipulation of a 
graphical user interface using a remote control. In 
order to find content, users generally have to either 
navigate a complex, pre-defined, and often deeply 
embedded menu structure or type in titles or other 
key phrases using an onscreen keyboard or triple 
tap input on a remote control keypad. These inter-
faces are cumbersome and do not scale well as the 
range of content available increases (Berglund, 
2004; Mitchell, 1999).  
 
Figure 1 Multimodal interface on tablet 
In this paper we explore the application of multi-
modal interface technologies (See Andr? (2002) 
for an overview) to the creation of more effective 
systems used to search and browse for entertain-
ment content in the home.  A number of previous 
systems have investigated the addition of unimodal 
spoken search queries to a graphical electronic 
program guide (Ibrahim and Johansson, 2002 
376
(NokiaTV); Goto et al, 2003; Wittenburg et al, 
2006). Wittenburg et alexperiment with unre-
stricted speech input for electronic program guide 
search, and use a highlighting mechanism to pro-
vide feedback to the user regarding the ?relevant? 
terms the system understood and used to make the 
query. However, their usability study results show 
this complex output can be confusing to users and 
does not correspond to user expectations. Others 
have gone beyond unimodal speech input and 
added multimodal commands combining speech 
with pointing (Johansson, 2003; Portele et al 
2006). Johansson (2003) describes a movie re-
commender system MadFilm where users can use 
speech and pointing to accept/reject recommended 
movies.  Portele et al(2006) describe the Smart-
Kom-Home system which includes multimodal 
electronic program guide on a tablet device. 
In our work we explore a broader range of inter-
action modalities and devices. The system provides 
users with the flexibility to interact using spoken 
commands, handwritten commands, unimodal 
pointing (GUI) commands, and multimodal com-
mands combining speech with one or more point-
ing gestures made on a display. We compare two 
different interaction scenarios. The first utilizes a 
traditional remote control for direct manipulation 
and pointing, integrated with a wireless micro-
phone for speech input. In this case, the only 
screen is the main TV display (far screen). In the 
second scenario, the user also has a second graphi-
cal display (close screen) presented on a mobile 
tablet which supports speech and pen input, includ-
ing both pointing and handwriting (Figure 1).  Our 
application task also differs, focusing on search 
and browsing of a large database of movies-on-
demand and supporting queries over multiple si-
multaneous dimensions.  This work also differs in 
the scope of the evaluation. Prior studies have pri-
marily conducted qualitative evaluation with small 
groups of users (5 or 6). A quantitative and qualita-
tive evaluation was conducted examining the inter-
action of 44 na?ve users with two variants of the 
system.  We believe this to be the first broad scale 
experimental evaluation of a flexible multimodal 
interface for searching and browsing large data-
bases of movie content.  
In Section 2, we describe the interface and illus-
trate the capabilities of the system. In Section 3, 
we describe the underlying multimodal processing 
architecture and how it processes and integrates 
user inputs.  Section 4 describes our experimental 
evaluation and comparison of the two systems. 
Section 5 concludes the paper. 
2 Interacting with the system 
The system described here is an advanced user in-
terface prototype which provides multimodal ac-
cess to databases of media content such as movies 
or television programming.  The current database 
is harvested from publicly accessible web sources 
and contains over 2000 popular movie titles along 
with associated metadata such as cast, genre, direc-
tor, plot, ratings, length, etc. 
The user interacts through a graphical interface 
augmented with speech, pen, and remote control 
input modalities. The remote control can be used to 
move the current focus and select items.  The pen 
can be used both for selecting items (pointing at 
them) and for handwritten input. The graphical 
user interface has three main screens. The main 
screen is the search screen (Figure 2). There is also 
a control screen used for setting system parameters 
and a third comparison display used for showing 
movie details side by side (Figure 4).  The user can 
select among the screens using three icons in the 
navigation bar at the top left of the screen. The ar-
rows provide ?Back? and ?Next? for navigation 
through previous searches.  Directly below, there is 
a feedback window which indicates whether the 
system is listening and provides feedback on 
speech recognition and search.  In the tablet vari-
ant, the microphone and speech recognizer are ac-
tivated by tapping on ?CLICK TO SPEAK? with 
the pen. In the remote control version, the recog-
nizer can also be activated using a button on the 
remote control.  The main section of the search 
display (Figure 2) contains two panels.  The right 
panel (results panel) presents a scrollable list of 
thumbnails for the movies retrieved by the current 
search.  The left panel (details panel) provides de-
tails on the currently selected title in the results 
panel.  These include the genre, plot summary, 
cast, and director.  
The system supports a speech modality, a hand-
writing modality, pointing (unimodal GUI) modal-
ity, and composite multimodal input where the user 
utters a spoken command which is combined with 
pointing ?gestures? the user has made towards 
screen icons using the pen or the remote control.  
 
377
 
 
 
 
 
Figure 2 Graphical user interface 
Speech: The system supports speech search over 
multiple different dimensions such as title, genre, 
cast, director, and year. Input can be more tele-
graphic with searches such as ?Legally Blonde?, 
?Romantic comedy?, and ?Reese Witherspoon?, or 
more verbose natural language queries such as 
?I?m looking for a movie called Legally Blonde? 
and ?Do you have romantic comedies?.  An impor-
tant advantage of speech is that it makes it easy to 
combine multiple constraints over multiple dimen-
sions within a single query (Cohen, 1992). For ex-
ample, queries can indicate co-stars: ?movies star-
ring Ginger Rogers and Fred Astaire?, or constrain 
genre and cast or director at the same time: ?Meg 
Ryan Comedies?, ?show drama directed by Woody 
Allen? and ?show comedy movies directed by 
Woody Allen and starring Mira Sorvino?.  
Handwriting: Handwritten pen input can also be 
used to make queries.  When the user?s pen ap-
proaches the feedback window, it expands allow-
ing for freeform pen input. In the example in Fig-
ure 3, the user requests comedy movies with Bruce 
Willis using unimodal handwritten input. This is an 
important input modality as it is not impacted by 
ambient noise such as crosstalk from other viewers 
or currently playing content. 
 
Figure 3 Handwritten query 
 Navigation Bar Feedback Window 
Pointing/GUI:  In addition to the recognition-
based modalities, speech and handwriting, the in-
terface also supports more traditional graphical 
user interface (GUI) commands. In the details 
panel, the actors and directors are presented as but-
tons. Pointing at (i.e., clicking on) these buttons 
results in a search for all of the movies with that 
particular actor or director, allowing users to 
quickly navigate from an actor or director in a spe-
cific title to other material they may be interested 
in. The buttons in the results panel can be pointed 
at (clicked on) in order to view the details in the 
left panel for that particular title.   
 
Actor/Director Buttons Details Results 
Figure 4 Comparison screen 
Composite multimodal input: The system also 
supports true composite multimodality when spo-
ken or handwritten commands are integrated with 
pointing gestures made using the pen (in the tablet 
version) or by selecting items (in the remote con-
trol version).  This allows users to quickly execute 
more complex commands by combining the ease 
of reference of pointing with the expressiveness of 
spoken constraints.  While by unimodally pointing 
at an actor button you can search for all of the ac-
tor?s movies, by adding speech you can narrow the 
search to, for example, all of their comedies by 
saying: ?show comedy movies with THIS actor?.  
Multimodal commands with multiple pointing ges-
tures are also supported, allowing the user to ?glue? 
together references to multiple actors or directors 
in order to constrain the search.  For example, they 
can say ?movies with THIS actor and THIS direc-
tor? and point at the ?Alan Rickman? button and 
then the ?John McTiernan? button in turn (Figure 
2). Comparison commands can also be multimo-
378
dal; for example, if the user says ?compare THIS 
movie and THIS movie? and clicks on the two but-
tons on the right display for ?Die Hard? and the 
?The Fifth Element? (Figure 2), the resulting dis-
play shows the two movies side-by-side in the 
comparison screen (Figure 4).  
3 Underlying multimodal architecture 
The system consists of a series of components 
which communicate through a facilitator compo-
nent (Figure 5). This develops and extends upon 
the multimodal architecture underlying the 
MATCH system (Johnston et al, 2002). 
 
Multimodal UI ASR
Server
ASR
Server
Multimodal
NLU
Multimodal
NLU
Movie DB
(XML)
NLU 
Model
Grammar Template
ASR 
Model
WordsGestures
Speech
Client
Speech
Client
Meaning
Grammar
Compiler
Grammar
Compiler
F
A
C
I
L
I
T
A
T
O
R
Handwriting
Handwriting
Recognition
 
Figure 5 System architecture 
The underlying database of movie information is 
stored in XML format.  When a new database is 
available, a Grammar Compiler component ex-
tracts and normalizes the relevant fields from the 
database. These are used in conjunction with a pre-
defined multimodal grammar template and any 
available corpus training data to build a multimo-
dal understanding model and speech recognition 
language model.   
The user interacts with the multimodal user in-
terface client (Multimodal UI), which provides the 
graphical display.  When the user presses ?CLICK 
TO SPEAK? a message is sent to the Speech Cli-
ent, which activates the microphone and ships au-
dio to a speech recognition server.  Handwritten 
inputs are processed by a handwriting recognizer 
embedded within the multimodal user interface 
client. Speech recognition results, pointing ges-
tures made on the display, and handwritten inputs, 
are all passed to a multimodal understanding server 
which uses finite-state multimodal language proc-
essing techniques (Johnston and Bangalore, 2005) 
to interpret and integrate the speech and gesture. 
This model combines alignment of multimodal 
inputs, multimodal integration, and language un-
derstanding within a single mechanism. The result-
ing combined meaning representation (represented 
in XML) is passed back to the multimodal user 
interface client, which translates the understanding 
results into an XPATH query and runs it against 
the movie database to determine the new series of 
results.  The graphical display is then updated to 
represent the latest query. 
The system first attempts to find an exact match 
in the database for all of the search terms in the 
user?s query.  If this returns no results, a back off 
and query relaxation strategy is employed. First the 
system tries a search for movies that have all of the 
search terms, except stop words, independent of 
the order (an AND query). If this fails, then it 
backs off further to an OR query of the search 
terms and uses an edit machine, using Levenshtein 
distance, to retrieve the most similar item to the 
one requested by the user.  
4 Evaluation 
After designing and implementing our initial proto-
type system, we conducted an extensive multimo-
dal data collection and usability study with the two 
different interaction scenarios: tablet versus remote 
control.  Our main goals for the data collection and 
statistical analysis were three-fold: collect a large 
corpus of natural multimodal dialogue for this me-
dia selection task, investigate whether future sys-
tems should be paired with a remote control or tab-
let-like device, and determine which types of 
search and input modalities are more or less desir-
able. 
4.1 Experimental set up 
The system evaluation took place in a conference 
room set up to resemble a living room (Figure 6). 
The system was projected on a large screen across 
the room from a couch. 
An adjacent conference room was used for data 
collection (Figure 7). Data was collected in sound 
files, videotapes, and text logs. Each subject?s spo-
ken utterances were recorded by three micro-
phones: wireless, array and stand alone. The wire-
less microphone was connected to the system 
while the array and stand alone microphones were 
379
around 10 feet away.1 Test sessions were recorded 
with two video cameras ? one captured the sys-
tem?s screen using a scan converter while the other 
recorded the user and couch area. Lastly, the user?s 
interactions and the state of the system were cap-
tured by the system?s logger. The logger is an addi-
tional agent added to the system architecture for 
the purposes of the evaluation.  It receives log mes-
sages from different system components as interac-
tion unfolds and stores them in a detailed XML log 
file. For the specific purposes of this evaluation, 
each log file contains: general information about 
the system?s components, a description and time-
stamp for each system event and user event, names 
and timestamps for the system-recorded sound 
files, and timestamps for the start and end of each 
scenario. 
 
Figure 6 Data collection environment 
Forty-four subjects volunteered to participate in 
this evaluation.  There were 33 males and 11 fe-
males, ranging from 20 to 66 years of age.  Each 
user interacted with both the remote control and 
tablet variants of the system, completing the same 
two sets of scenarios and then freely interacting 
with each system.  For counterbalancing purposes, 
half of the subjects used the tablet and then the re-
mote control and the other half used the remote 
                                                 
1 Here we report results for the wireless microphone only. 
Analysis of the other microphone conditions is ongoing. 
control and then the tablet.  The scenario set as-
signed to each version was also counterbalanced.   
 
Figure 7 Data collection room 
Each set of scenarios consisted of seven defined 
tasks, four user-specialized tasks and five open-
ended tasks. Defined tasks were presented in chart 
form and had an exact answer, such as the movie 
title that two specified actors/actresses starred in. 
For example, users had to find the movie in the 
database with Matthew Broderick and Denzel 
Washington. User-specialized tasks relied on the 
specific user?s preferences, such as ?What type of 
movie do you like to watch on a Sunday evening?  
Find an example from that genre and write down 
the title?. Open-ended tasks prompted users to 
search for any type of information with any input 
modality. The tasks in the two sets paralleled each 
other. For example, if one set of tasks asked the 
user to find the highest ranked comedy movie with 
Reese Witherspoon, the other set of tasks asked the 
user to find the highest ranked comedy movie with 
Will Smith. Within each task set, the defined tasks 
appeared first, then the user-specialized tasks and 
lastly the open-ended tasks. However, for each par-
ticipant, the order of defined tasks was random-
ized, as well as the order of user-specialized tasks. 
At the beginning of the session, users read a 
short tutorial about the system?s GUI, the experi-
ment, and available input modalities. Before inter-
acting with each version, users were given a man-
ual on operating the tablet/remote control. To 
minimize bias, the manuals gave only a general 
overview with few examples and during the ex-
periment users were alone in the room.  
At the end of each session, users completed a 
user-satisfaction/preference questionnaire and then 
a qualitative interview. The questionnaire consisted 
380
of 25 statements about the system in general, the 
two variants of the system, input modality options 
and search options. For example, statements 
ranged from ?If I had [the system], I would use the 
tablet with it? to ?If my spoken request was mis-
understood, I would want to try again with speak-
ing?.  Users responded to each statement with a 5-
point Likert scale, where 1 = ?I strongly agree?, 2 = 
?I mostly agree?, 3 = ?I can?t say one way or the 
other?, 4 = ?I mostly do not agree? and 5 = ?I do not 
agree at all?. The qualitative interview allowed for 
more open-ended responses, where users could 
discuss reasons for their preferences and their likes 
and dislikes regarding the system. 
4.2 Results 
Data was collected from all 44 participants. Due to 
technical problems, five participants? logs or sound 
files were not recorded in parts of the experiment.  
All collected data was used for the overall statistics 
but these five participants had to be excluded from 
analyses comparing remote control to tablet. 
Spoken utterances: After removing empty 
sound files, the full speech corpus consists of 3280 
spoken utterances.  Excluding the five participants 
subject to technical problems, the total is 3116 ut-
terances (1770 with the remote control and 1346 
with the tablet).   
The set of 3280 utterances averages 3.09 words 
per utterance.  There was not a significant differ-
ence in utterance length between the remote con-
trol and tablet conditions.  Users? averaged 2.97 
words per utterance with the remote control and 
3.16 words per utterance with the tablet, paired t 
(38) = 1.182, p = n.s.  However, users spoke sig-
nificantly more often with the remote control.  On 
average, users spoke 34.51 times with the tablet 
and 45.38 times with the remote control, paired t 
(38) = -3.921, p < .01. 
ASR performance: Over the full corpus of 
3280 speech inputs, word accuracy was 44% and 
sentence accuracy 38%.  In the tablet condition, 
word accuracy averaged 46% and sentence accu-
racy 41%.  In the remote control condition, word 
accuracy averaged 41% and sentence accuracy 
38%.  The difference across conditions was only 
significant for word accuracy, paired t (38) = 
2.469, p < .02.  In considering the ASR perform-
ance, it is important to note that 55% of the 3280 
speech inputs were out of grammar, and perhaps 
more importantly 34% were out of the functional-
ity of the system entirely.  On within functionality 
inputs, word accuracy is 62% and sentence accu-
racy 57%.  On the in grammar inputs, word accu-
racy is 86% and sentence accuracy 83%. The vo-
cabulary size was 3851 for this task. In the corpus, 
there are a total of 356 out-of-vocabulary words.  
Handwriting recognition: Performance was de-
termined by manual inspection of screen capture 
video recordings. 2   There were a total of 384 
handwritten requests with overall 66% sentence 
accuracy and 76% word accuracy. 
Task completion:  Since participants had to re-
cord the task answers on a paper form, task com-
pletion was calculated by whether participants 
wrote down the correct answer.  Overall, users had 
little difficulty completing the tasks.  On average, 
participants completed 11.08 out of the 14 defined 
tasks and 7.37 out of the 8 user-specialized tasks.  
The number of tasks completed did not differ 
across system variants. 3  For the seven defined 
tasks within each condition, users averaged 5.69 
with the remote control and 5.40 with the tablet, 
paired t (34) = -1.203, p = n.s.  For the four user-
specialized task within each condition, users aver-
aged 3.74 on the remote control and 3.54 on the 
tablet, paired t (34) = -1.268, p = n.s. 
Input modality preference: During the inter-
view, 55% of users reported preferring the pointing 
(GUI) input modality over speech and multimodal 
input. When asked about handwriting, most users 
were hesitant to place it on the list.  They also dis-
cussed how speech was extremely important, and 
given a system with a low error speech recognizer, 
using speech for input probably would be their first 
choice. In the questionnaire, the majority of users 
(93%) ?strongly agree? or ?mostly agree? with the 
importance of making a pointing request. The im-
portance of making a request by speaking had the 
next highest average, where 57% ?strongly agree? 
or ?mostly agree? with the statement. The impor-
tance of multimodal and handwriting requests had 
the lowest averages, where 39% agreed with the 
former and 25% for the latter.  However, in the 
open-ended interview, users mentioned handwrit-
ing as an important back-up input choice for cases 
when the speech recognizer fails. 
                                                 
2 One of the 44 participants videotape did not record and so is 
not included in the statistics.     
3 Four participants did not properly record their task answers 
and had to be eliminated from the 39 participants being used 
in the remote control versus tablet statistics.   
381
Further support for input modality preference was 
gathered from the log files, which showed that par-
ticipants mostly searched using unimodal speech 
commands and GUI buttons.  Out of a total of 
6082 user inputs to the systems, 48% were unimo-
dal speech and 39% were unimodal GUI (pointing 
and clicking). Participants requested information 
with composite multimodal commands 7% of the 
time and with handwriting 6% of the time. 
Search preference: Users most strongly agreed 
with movie title being the most important way to 
search. For searching by title, more than half the 
users chose ?strongly agree? and 91% of users 
chose ?strongly agree? or ?mostly agree?.  Slightly 
more than half chose ?strongly agree? with search-
ing by actor/actress and slightly less than half 
chose ?strongly agree? with the importance of 
searching by genre. During the open ended inter-
view, most users reported title as the most impor-
tant means for searching. 
Variant preference:  Results from the qualita-
tive interview indicate that 67% of users preferred 
the remote control over the tablet variant of the 
system. The most common reported reasons were 
familiarity, physical comfort and ease of use. Re-
mote control preference is further supported from 
the user-preference questionnaire, where 68% of 
participants ?mostly agree? or ?strongly agree? with 
wanting to use the remote control variant of the 
system, compared to 30% of participants choosing 
?mostly agree? or ?strongly agree? with wanting to 
use the tablet version of the system. 
5 Conclusion  
With the range of entertainment content available 
to consumers in their homes rapidly expanding, the 
current access paradigm of direct manipulation of 
complex graphical menus and onscreen keyboards, 
and remote controls with way too many buttons is 
increasingly ineffective and cumbersome. In order 
to address this problem, we have developed a 
highly flexible multimodal interface that allows 
users to search for content using speech, handwrit-
ing, pointing (using pen or remote control), and 
dynamic multimodal combinations of input modes. 
Results are presented in a straightforward graphical 
interface similar to those found in current systems 
but with the addition of icons for actors and direc-
tors that can be used both for unimodal GUI and 
multimodal commands. The system allows users to 
search for movies over multiple different dimen-
sions of classification (title, genre, cast, director, 
year) using the mode or modes of their choice. We 
have presented the initial results of an extensive 
multimodal data collection and usability study with 
the system. 
Users in the study were able to successfully use 
speech in order to conduct searches. Almost half of 
their inputs were unimodal speech (48%) and the 
majority of users strongly agreed with the impor-
tance of using speech as an input modality for this 
task. However, as also reported in previous work 
(Wittenburg et al2006), recognition accuracy re-
mains a serious problem. To understand the per-
formance of speech recognition here, detailed error 
analysis is important. The overall word accuracy 
was 44% but the majority of errors resulted from 
requests from users that lay outside the functional-
ity of the underlying system, involving capabilities 
the system did not have or titles/cast absent from 
the database (34% of the 3280 spoken and multi-
modal inputs). No amount of speech and language 
processing can resolve these problems. This high-
lights the importance of providing more detailed 
help and tutorial mechanisms in order to appropri-
ately ground users? understanding of system capa-
bilities. Of the remaining 66% of inputs (2166) 
which were within the functionality of the system, 
68% were in grammar. On the within functionality 
portion of the data, the word accuracy was 62%, 
and on in grammar inputs it is 86%.  Since this was 
our initial data collection, an un-weighted finite-
state recognition model was used. The perform-
ance will be improved by training stochastic lan-
guage models as data become available and em-
ploying robust understanding techniques. One in-
teresting issue in this domain concerns recognition 
of items that lie outside of the current database. 
Ideally the system would have a far larger vocabu-
lary than the current database so that it would be 
able to recognize items that are outside the data-
base. This would allow feedback to the user to dif-
ferentiate between lack of results due to recogni-
tion or understanding problems versus lack of 
items in the database. This has to be balanced 
against degradation in accuracy resulting from in-
creasing the vocabulary.  
In practice we found that users, while acknowl-
edging the value of handwriting as a back-up 
mode, generally preferred the more relaxed and 
familiar style of interaction with the remote con-
trol. However, several factors may be at play here. 
382
The tablet used in the study was the size of a small 
laptop and because of cabling had a fixed location 
on one end of the couch. In future, we would like 
to explore the use of a smaller, more mobile, tablet 
that would be less obtrusive and more conducive to 
leaning back on the couch. Another factor is that 
the in-lab data collection environment is somewhat 
unrealistic since it lacks the noise and disruptions 
of many living rooms. It remains to be seen 
whether in a more realistic environment we might 
see more use of handwritten input. Another factor 
here is familiarity. It may be that users have more 
familiarity with the concept of speech input than 
handwriting. Familiarity also appears to play a role 
in user preferences for remote control versus tablet. 
While the tablet has additional capabilities such 
handwriting and easier use of multimodal com-
mands, the remote control is more familiar to users 
and allows for a more relaxed interaction since 
they can lean back on the couch. Also many users 
are concerned about the quality of their handwrit-
ing and may avoid this input mode for that reason.   
Another finding is that it is important not to un-
derestimate the importance of GUI input. 39% of 
user commands were unimodal GUI (pointing) 
commands and 55% of users reported a preference 
for GUI over speech and handwriting for input. 
Clearly, the way forward for work in this area is to 
determine the optimal way to combine more tradi-
tional graphical interaction techniques with the 
more conversational style of spoken interaction. 
Most users employed the composite multimodal 
commands, but they make up a relatively small 
proportion of the overall number of user inputs in 
the study data (7%). Several users commented that 
they did not know enough about the multimodal 
commands and that they might have made more 
use of them if they had understood them better. 
This, along with the large number of inputs that 
were out of functionality, emphasizes the need for 
more detailed tutorial and online help facilities. 
The fact that all users were novices with the sys-
tem may also be a factor. In future, we hope to 
conduct a longer term study with repeat users to 
see how previous experience influences use of 
newer kinds of inputs such as multimodal and 
handwriting.   
Acknowledgements Thanks to Keith Bauer, Simon Byers, 
Harry Chang, Rich Cox, David Gibbon, Mazin Gilbert, 
Stephan Kanthak, Zhu Liu, Antonio Moreno, and Behzad 
Shahraray for their help and support.  Thanks also to the Di-
recci?n General de Universidades e Investigaci?n - Consejer?a 
de Educaci?n - Comunidad de Madrid, Espa?a for sponsoring 
D?Haro?s visit to AT&T. 
References 
Elisabeth Andr?. 2002. Natural Language in Multimodal 
and Multimedia systems. In Ruslan Mitkov (ed.) Ox-
ford Handbook of Computational Linguistics. Oxford 
University Press. 
Aseel Berglund. 2004. Augmenting the Remote Control: 
Studies in Complex Information Navigation for Digi-
tal TV. Link?ping Studies in Science and Technol-
ogy, Dissertation no. 872. Link?ping University. 
Philip R. Cohen. 1992. The Role of Natural Language in 
a Multimodal Interface. In Proceedings of ACM 
UIST Symposium on User Interface Software and 
Technology. pp. 143-149. 
Jun Goto, Kazuteru Komine, Yuen-Bae Kim and Nori-
yoshi Uratan. 2003. A Television Control System 
based on Spoken Natural Language Dialogue. In 
Proceedings of 9th International Conference on Hu-
man-Computer Interaction. pp. 765-768. 
Aseel Ibrahim and Pontus Johansson. 2002. Multimodal 
Dialogue Systems for Interactive TV Applications. In 
Proceedings of 4th IEEE International Conference 
on Multimodal Interfaces. pp. 117-222. 
Pontus Johansson. 2003. MadFilm - a Multimodal Ap-
proach to Handle Search and Organization in a 
Movie Recommendation System. In Proceedings of 
the 1st Nordic Symposium on Multimodal Communi-
cation. Helsing?r, Denmark. pp. 53-65. 
Michael Johnston, Srinivas Bangalore, Guna Vasireddy, 
Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve 
Whittaker, Preetam Maloor. 2002. MATCH: An Ar-
chitecture for Multimodal Dialogue Systems. In Pro-
ceedings of the 40th ACL. pp. 376-383. 
Michael Johnston and Srinivas Bangalore. 2005. Finite-
state Multimodal Integration and Understanding. 
Journal of Natural Language Engineering 11.2. 
Cambridge University Press. pp. 159-187. 
Russ Mitchell. 1999. TV?s Next Episode. U.S. News 
and World Report. 5/10/99. 
Thomas Portele, Silke Goronzy, Martin Emele, Andreas 
Kellner, Sunna Torge, and J?ergen te Vrugt. 2006. 
SmartKom?Home: The  Interface to Home Enter-
tainment. In Wolfgang Wahlster (ed.) SmartKom: 
Foundations of Multimodal Dialogue Systems. 
Springer.  pp. 493-503. 
Kent Wittenburg, Tom Lanning, Derek Schwenke, Hal 
Shubin and Anthony Vetro. 2006. The Prospects for 
Unrestricted Speech Input for TV Content Search. In 
Proceedings of  AVI?06. pp. 352-359. 
383
A System for Searching and Browsing Spoken Communications
Lee Begeja
Bernard Renger
Murat Saraclar
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
{lee, renger, murat}
@research.att.com
David Gibbon
Zhu Liu
Behzad Shahraray
AT&T Labs ? Research
200 Laurel Ave S
Middletown, NJ 07748
{dcg, zliu, behzad}
@research.att.com
Abstract
As the amount of spoken communications ac-
cessible by computers increases, searching and
browsing is becoming crucial for utilizing such
material for gathering information. It is desir-
able for multimedia content analysis systems
to handle various formats of data and to serve
varying user needs while presenting a simple
and consistent user interface. In this paper,
we present a research system for searching and
browsing spoken communications. The system
uses core technologies such as speaker segmen-
tation, automatic speech recognition, transcrip-
tion alignment, keyword extraction and speech
indexing and retrieval to make spoken commu-
nications easy to navigate. The main focus is
on telephone conversations and teleconferences
with comparisons to broadcast news.
1 Introduction
Archiving and organizing multimedia communications
for easy user access is becoming more important as such
information sources are becoming available in amounts
that can easily overwhelm a user. As storage and ac-
cess become cheaper, the types of multimedia communi-
cations are also becoming more diverse. Therefore, it is
necessary for multimedia content analysis and navigation
systems to handle various forms of data.
In this paper we present SpeechLogger, a research sys-
tem for searching and browsing spoken communications,
or the spoken component of multimedia communications.
In general, the information contained in a spoken com-
munication consists of more than just words. Our goal is
to make use of all the information within a spoken com-
munication. Our system uses automatic speech recogni-
tion (ASR) to convert speech into a format which makes
word and phonetic searching of the material possible. It
also uses speaker segmentation to aid navigation.
We are interested in a wide range of spoken communi-
cations with different characteristics, including broadcast
material, lectures, meetings, interviews, telephone con-
versations, call center recordings, and teleconferences.
Each of these communication types presents interesting
opportunities, requirements and challenges. For example,
lectures might have accompanying material that can aid
ASR and navigation. Prior knowledge about the speakers
and the topic may be available for meetings. Call center
recordings may be analyzed to create aggregate reports.
Spoken document retrieval (SDR) for Broadcast News
type of content has been well studied and there are many
research and commercial systems. There has also been
some interest in the Voicemail domain (Hirschberg et al,
2001) which consists of typically short duration human-
to-machine messages. Our focus here is on telephone
conversations and teleconferences with comparisons to
broadcast news.
The paper is organized as follows. In Section 2, we
motivate our approach by describing the user needs un-
der various conditions. Then we describe our system in
Section 3, giving the details of various components. Ex-
perimental results for some components are given in Sec-
tion 4. Finally, in Section 5 we present a summary.
2 User Needs
We are primarily interested in situations in which a per-
son needs to gather information from audio data but the
quality of that data is not always sufficient to produce
good ASR results. In the case of telephone conversations,
the information gatherer needs to know who was on the
call, how long the call was, what was said, a summary of
the call, the ability to listen to any part of the call based
on search parameters that s/he specifies, etc. Our users
want to be able to scan a database of many calls, across a
long period of time to look for specific phrases, speakers,
or patterns of speech.
In many cases, it is difficult to gather this type of infor-
mation from teleconference calls since the audio quality
is poor because of speaker phones, cell phones and line
noise. All of these combine to lower ASR results to a
point where the text of the call is not fully representative
of the conversation. Thus, using standard information re-
trieval techniques may not provide sufficient information
to the user. We focus on the navigation aspect of infor-
mation gathering with the goal of compensating for lower
ASR accuracy by presenting user interface elements rel-
evant to the specific task at hand (Stark et al, 2000).
Rather than looking at the recorded conversation as
merely audio information, we view it as a source of lin-
guistic information to which we can apply information
retrieval and data mining techniques. We use all avail-
able metadata to enhance the search and the presentation.
We wanted to have a set of interface elements that
would be useful no matter what the ASR accuracy was.
The main interface elements are:
? Timeline with tick marks indicates search hits within
the spoken document which allows for many search
results to be displayed without overwhelming the
user. This is particularly useful for cases where there
are many false positives.
? Keyword extraction summarizes a given communi-
cation, enables differentiation among a collection of
many spoken documents, and detects subtopics in a
large spoken document.
? Speaker segmentation and speaker identification
separate a long spoken document into inherently
useful pieces.
? Lattice search and phoneme search expand the pos-
sible search space.
In this paper we examine three classes of spoken doc-
uments and consider what tasks a user might want to per-
form on them.
? Broadcast News - excellent ASR conditions, one
speaker at a time, good audio quality and gener-
ally a good speaker. Task involves primarily inter-
document navigation. User needs to search text for
information with metadata possibly used to enhance
the search.
? Telephone Conversations - fair ASR conditions, two
speakers, decent quality audio. User needs to search
text but also wants speaker identification and some
classification (call type, urgency, importance).
? Teleconferences - poor ASR conditions, multiple
speakers, mixed to poor audio quality. Most time
is spent in intra-document navigation. User needs to
navigate through the calls and find relevant informa-
tion in the audio.
3 System Description
The system overview is shown in Figure 1. Our sys-
tem is flexible enough to support various forms of live
(via a VoiceXML Gateway) or prerecorded spoken com-
munications including the three classes of spoken docu-
ments discussed above. It can record the audio via tele-
phone for two-party or multi-party calls. Alternatively,
the system can support prerecorded audio input from var-
ious sources including telephone conversations or video
content in which case the audio is extracted from the
video. Once various speech processing techniques are ap-
plied and the speech is indexed, it is possible to search
and browse the audio content. Our system is scalable
and supports open source/industry standard components
(J2EE, VXML, XML, Microsoft SAMI, Microsoft Media
Player). It is also flexible enough to support other forms
of audio as input or to support new speech processing
techniques as they become available. The system was de-
signed with modularity in mind. For instance, it should
be possible to add a speaker identification module to the
processing.
Figure 1: System Overview
Once a new audio recording is available on the
File Server, the following processing steps can begin:
speaker segmentation, speech recognition, transcription
alignment, keyword extraction, audio compression, and
speech indexing. Each step will be described in more de-
tail below. We attempt to distinguish the different speak-
ers from each other in the speaker segmentation compo-
nent. The speech recognition component converts the au-
dio into a word or phone based representation including
alternative hypotheses in the form of a lattice. If a tran-
script is available, the transcript can be synchronized (or
aligned) in time with the speech recognition output. The
keyword extraction component generates the most salient
words found in the speech recognition output (one-best
word) or transcript (if available) and can be used to de-
termine the nature of the spoken communications. The
audio compression component compresses the audio file
and creates an MP3 audio file which is copied to the Me-
dia Server. The final step in the processing is text and
lattice indexing. This includes creating indices based on
one-best word and one-best phone strings or word and
phone lattices.
After processing, the user can search and browse the
audio using either the text index or the lattice index. The
audio is played back via media streaming. Alternatively,
the user can playback the audio file over the phone using
the VoiceGenie VoiceXML Gateway.
3.1 Speaker Segmentation
Speaker-based segmentation of multi-speaker audio data
has received considerable attention in recent years. Ap-
plications that have been considered include: indexing
archived recorded spoken documents by speaker to facil-
itate browsing and retrieval of desired portions; tagging
speaker specific portions of data to be used for adapt-
ing speech models in order to improve the quality of
automatic speech recognition transcriptions, and track-
ing speaker specific segments in audio streams to aid in
surveillance applications. In our system, speaker segmen-
tation is used for more effective visualization of the audio
document and speaker-based audio playback.
Figure 2 gives an overview of the speaker segmenta-
tion algorithm we developed. It consists of two steps:
preprocessing and iterative speaker segmentation. Dur-
ing the preprocessing step, the input audio stream is seg-
mented into frames and acoustic features are computed
for each frame. The features we extracted are energy, 12
Mel-frequency cepstral coefficients (MFCC), pitch, and
the first and second order temporal derivatives. Then,
all speaker boundary candidates are located, which in-
clude silent frames and frames with minimum energy in
a window of neighboring frames. The preprocessing step
generates a set of over-segmented audio segments, whose
durations may be as short as a fraction of a second to as
long as a couple of seconds.
The iterative speaker segmentation step, as depicted in
the bigger dotted rectangle in Figure 2, detects all seg-
ments of each speaker in an iterative way and then marks
the boundaries where speakers change. At the beginning,
all segments produced by the preprocessing step are un-
labeled. Assuming that the features within each segment
follow a Gaussian distribution, we compute the distances
between each pair of segments using the Kullback Leibler
distance (KLD) (Cover and Thomas, 1991). Here, we just
consider features extracted from voiced frames since only
voiced frames have pitch information. Based on the seg-
ment distance matrix, a hierarchical agglomerative clus-
tering (HAC) (Jain and Dubes, 1988) algorithm is applied
Figure 2: Overview of the Speaker Segmentation Algo-
rithm.
to all unlabeled segments. The biggest cluster will be hy-
pothesized as the set of segments for a new speaker and
the rest of the segments will be considered as background
audio. Accordingly, each unlabeled segment is labeled as
either the target speaker or background. Then an embed-
ded speaker segment refinement substep is activated to
iteratively refine the segments of the target speaker.
The refinement substep is depicted in the smaller dot-
ted rectangle in Figure 2. For each iteration, two Gaus-
sian mixture models (GMM) are built based on current
segment labels, one for the target speaker, one for back-
ground audio. Then all segments are relabeled as either
the target speaker or background audio using the maxi-
mum likelihood method based on the two GMM models.
If the set of segments for the target speaker converges
or the refinement iteration number reaches its maximum,
the refinement iteration stops. Otherwise, a new itera-
tion starts. Before the refinement substep terminates, it
assigns a new speaker label for all segments of the tar-
get speaker, and sets the background audio as unlabeled.
Then the iterative speaker segmentation step needs to test
for more speakers or needs to stop. The termination cri-
teria could be the given number of speakers (or major
speakers) in an audio document, the percentage of unla-
beled segments to the number of all segments, or the max-
imum distance among all pairs of unlabeled segments. If
any of the criteria are met, the speaker segmentation algo-
rithm merges all adjacent segments if their speaker labels
are the same, and then outputs a list of audio segments
with corresponding speaker labels.
Obviously, one advantage of our speaker segmentation
method is that the speaker labels are also extracted. Al-
though the real speaker identities are not available, the
Figure 3: Presentation of Speaker Segmentation Results.
labels are very useful for presenting, indexing, and re-
trieving audio documents. For more detailed description
of the speaker segmentation algorithm, please refer to
Rosenberg et al (2002).
Figure 3 illustrates a graphic interface for presenting
the speaker segmentation results. The audio stream is
shown in colored blocks along a timeline which goes
from top to bottom, and from left to right. Color is used to
differentiate the speaker labels. There are two layers for
each line: the bottom layer shows the manually labeled
speaker segments and the top layer displays the automat-
ically generated segments. This allows the segmentation
performance to be clearly observed.
3.2 Automatic Speech Recognition
We use two different state-of-the-art HMM based large
vocabulary continuous speech recognition (LVCSR) sys-
tems for telephone and microphone recordings. In both
cases the front-end uses 9 frames of 12 MFCC compo-
nents and energy summarized into a feature vector via
linear discriminant analysis. The acoustic models consist
of decision tree state clustered triphones and the output
distributions are mixtures of Gaussians. The models are
discriminatively trained using maximum mutual informa-
tion estimation. The language models are pruned backoff
trigram models.
For narrow-band telephone recordings we use the first
pass of the Switchboard evaluation system developed
by Ljolje et al (2002). The calls are automatically seg-
mented prior to ASR. The acoustic models are trained on
265 hours of speech. The recognition vocabulary of the
system has 45K words.
For wide-band recordings, we use the real-time
Broadcast News transcription system developed by
Saraclar et al (2002). The acoustic models are trained on
140 hours of speech. The language models are estimated
on a mixture of newspaper text, closed captions and high-
accuracy transcriptions from LDC. Since the system was
designed for SDR, the recognition vocabulary of the sys-
tem has over 200K words.
Both systems use the same Finite State Machine (FSM)
based LVCSR decoder (Allauzen et al, 2003). The out-
put of the ASR system is represented as a FSM and may
be in the form of a one-best hypothesis string or a lattice
of alternate hypotheses. The lattices are normalized so
that the probability of the set of all paths leading from
any state to the final state is 1. The labels on the arcs of
the FSM may be words or phones and the conversion be-
tween the two can easily be done using FSM composition
using the AT&T FSM Library (Mohri et al, 1997). The
costs on the arcs of the FSM are negative log likelihoods.
Additionally, timing information can also be present in
the output.
3.3 Alignment with Transcripts
Manual transcriptions of spoken communications are
available for certain application domains such as medical
diagnosis, legal depositions, television and radio broad-
casts. Most audio and video teleconferencing providers
offer transcription as an optional service. In these
cases, we can take advantage of this additional informa-
tion to create high quality multimedia representations of
the archived spoken communications using parallel text
alignment techniques (Gibbon, 1998). The obvious ad-
vantage is increased retrieval accuracy due to the lower
word error rate (manual transcriptions are seldom com-
pletely error free.) What is more compelling, however, is
that we can construct much more evolved user interfaces
for browsing speech by leveraging the fact that the tran-
scription is by its nature readable whereas the one-best
hypothesis from ASR is typically useful only in small
segments to establish context for a search term occur-
rence.
There are several methods for aligning text with
speech. We use dynamic programming techniques to
maximize the number or word correspondences between
the manual transcription and the one-best ASR word hy-
pothesis. For most applications, finding the start and end
times of the transcript sentences is sufficient; but we do
alignment at the word level and then derive the sentence
alignment from that. In cases where the first or last word
of a sentence is not recognized, we expand to the near-
est recognized word to avoid cropping even though we
may include small segments from neighboring sentences
during playback. The accuracy of the resulting align-
ment is directly related to the ASR word error rate; more
precisely it can be thought of as a sentence error rate
where we impose a minimum percentage of correspond-
ing words per sentence (typically 20%) before declar-
ing a sentence a match to avoid noise words triggering
false matches. For sentences without correspondences,
we must fall back to deriving the timings from the near-
est neighboring sentences with correspondences.
Figure 4: Illustration of Keyword Extraction.
3.4 Keyword Extraction
Playing back a spoken document or linearly skimming
the corresponding text transcript, either from automatic
speech recognition or manual transcription, is not an ef-
ficient way for a user to grasp the central topics of the
document within a short period of time. A list of repre-
sentative keywords, which serve as a dense summary for a
document, can effectively convey the essence of the docu-
ment to the user. The keywords have been widely used for
indexing and retrieval of documents in large databases. In
our system, we extract a list of keywords for each audio
document based on its transcript (ASR or manual tran-
script).
There are different ways to automatically extract key-
words for a text document within a corpus. A popular
approach is to select keywords that frequently occur in
one document but do not frequently occur in other doc-
uments based on the term frequency - inverse document
frequency (TF-IDF) feature. Our task is slightly differ-
ent. We are interested in choosing keywords for a sin-
gle document, independent of the remaining documents
in the database. Accordingly, we adopt a different fea-
ture, which is term frequency - inverse term probability
(TF-ITP) to serve our purpose. The term probability mea-
sures the probability that a term may appear in a general
document and it is a language dependent characteristic.
Assuming that a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of Tk is
defined as wk = tfk/tpk.
Figure 4 illustrates the keyword extraction method that
we have developed. For the transcript of a given doc-
ument, we first apply the Porter stemming algorithm
(Porter, 1980) to remove word variations. Then, the stop
words, which are common words that have no impact on
the document content (also called noise words), are re-
moved. Here we use two lists of noise words, one for gen-
eral purposes, which apply to all varieties of documents,
and one for specific domains, which can be customized
by the user when prior knowledge about the document is
available. For each remaining term in the document, a
value of TF-ITP is calculated. A vocabulary is created
based on the transcripts of 600 hours of broadcast news
data and corresponding term probabilities are estimated
using the same corpus. If a term in the document is not
in the vocabulary, and its term frequency is more than
2, then a default term probability value tpd will be used.
The tpd we use is the minimum term probability in the
vocabulary. After we get a list of terms and their TF-ITP
values, we sort the terms based on their TF-ITP values,
such that the most representative terms (highest TF-ITP
values) are on the top of the list. Depending on certain
criteria, for example, the number of keywords desired or
the minimum TF-ITP value required, a list of keywords
can be chosen from the top of the term list. In our sys-
tem, we choose the top ten terms as the keywords for a
document.
3.5 Speech Indexing and Retrieval
Two different indexing and retrieval modules are uti-
lized depending on the type of ASR output. In
the case of one-best word or phone strings, we use
an off-the-shelf text-based index server called Lucene
(http://jakarta.apache.org/lucene). In the case of word
and phone lattices, we use the method described in
Saraclar and Sproat (2004). Here we give a brief descrip-
tion of the latter.
The lattice output is a compact representation of likely
alternative hypotheses of an ASR system. Each path in
the lattice corresponds to a word (or phone) string and
has a probability attached to it. The expected count for
a substring can be defined as the sum of the probabilities
of all paths which contain that substring. Lattice based
retrieval makes the system more robust to recognition er-
rors, whereas phonetic search allows for retrieving words
that are not in the vocabulary of the recognizer.
The lattice index is similar to a standard inverted index
but contains enough information to compute the expected
count of an arbitrary substring for each lattice. This can
be achieved by storing a set of index files, one for each
label (word or phone) l. For each arc labeled with l in a
lattice, the index file for l records the lattice number, the
previous and next states of the arc, along with the prob-
ability mass leading to the previous state of the arc and
the probability of the arc itself. For a lattice, which is
normalized so that the probability of the set of all paths
leading from any state to the final state is 1, the poste-
rior probability of an arc is given by the multiplication of
the probability mass leading to the previous state and the
probability of the arc itself. The expected count of a label
given a lattice is equal to the sum of the posterior proba-
bilities of all arcs in the index for that label with the same
lattice number.
To search for a multi-label expression (e.g., a multi-
word phrase) w1w2 . . . wn we seek on each label in the
expression and then for each (wi, wi+1) join the next
states of wi with the matching previous states of wi+1.
In this way, we retrieve just those path segments in each
lattice that match the entire multi-label expression. The
probability of each match is defined as the multiplication
of the probability mass leading to the previous state of
the first arc and the probabilities of all the arcs in the path
segment. The expected count of a multi-label expression
for the lattice is computed as above.
The answer to a query contains an audio segment only
if the expected count of the query for the lattice corre-
sponding to that audio segment is higher than a threshold.
3.6 User Interface
The user interface description will apply for the three
types of spoken communications (Telephone Conversa-
tions, Teleconferences, Broadcast News) although the au-
dio and speaker quality do vary for each of these types
of spoken communications. Once the user has found the
desired call (or spoken communication) using one of the
retrieval modules (one-best word, one-best phone string,
word lattice, phone lattice, or both word and phone lat-
tice), the user can navigate the call using the user inter-
face elements described below.
For the one-best word index, the Web page in Fig-
ure 5 shows the user interface for searching, browsing,
and playing back this call. The user can browse the call
at any time by clicking on the timeline to start playing at
that location on the timeline. The compressed audio file
(MP3) that was created during the processing would be
streamed to the user. The user can at any time either enter
a word (or word phrase) in the Search box or use one of
the common keywords generated during the keyword ex-
traction process. The text index would be queried and the
results of the search would be shown. The timeline plot
at the top would show all the hits or occurrences of the
word as thin tick marks. The list of hits would be found
under the keyword list. In this case, the word ?chap-
ter? was found 4 times and the time stamps are shown.
The time stamps come from the results of the automatic
speech recognition process when the one-best words and
time stamps were generated. The search term ?chapter?
is shown in bold with 5 context words on either side. The
user can click on any of these 4 hits to start playing where
the hit occurred. The solid band in the timeline indicates
the current position of the audio being played back. The
entire call, in this case, is 9:59 minutes long and the au-
dio is playing at the beginning of the fourth hit at 5:20
minutes. As part of the processing, caption data is gener-
ated in Microsoft?s SAMI (Synchronized Accessible Me-
dia Interchange) format from the one-best word output in
order to show caption text during the playback. The cap-
tion text under the timeline will be updated as the audio
is played. At this point in the call, the caption text is ?but
i did any chapter in a?. This caption option can be dis-
Figure 5: User Interface for ASR One-Best Word Search.
Figure 6: User Interface for Lattice Search.
abled by clicking on the CC icon and can be enabled by
clicking on the CC icon again. The user can also speed
up or slow down the playback at any time by using the
?Speed? button. The speed will toggle from 50% (slow)
to 100% to 150% (fast) to 200% (faster) and then start
over at 50%. The speed, which is currently ?fast?, will be
shown next to the current time above the ?Stop? button.
This allows the user to more quickly peruse the audio file.
A similar Web page in Figure 6 shows the user inter-
face for searching a lattice index. Note that for the same
audio file (or call) and the same search term ?chapter?,
the results of the query show 6 hits compared to the 4
hits in the text index in Figure 5. In this particular case,
the manual transcript does indeed contain these 6 occur-
rences of the word ?chapter?. The search terms were
found in audio segments, which is why the time of the
hit is a time range. The information in brackets is the ex-
pected count and can exceed 1.0 if the search term occurs
more than once in the audio segment. The time range is
reflected in the timeline since the thin tick marks have
been replaced with colored segments. The colors of the
segments correspond to the colors of the hits in the list.
The darker the color, the higher the count and the lighter
the color, the lower the count. Finally, the search can be
refined by altering the threshold using the ?Better Hits?
and ?More Hits? buttons. In this example, the threshold
is set to 0.2 as can be seen under the timeline. If the
user clicks on the ?Better Hits? button, the threshold is
increased so that only better hits are shown. If the ?More
Hits? button is used, the threshold is decreased so more
hits are shown although the hits may not be as good. The
lattice index only returns hits where each hit has a count
above the threshold.
The lattice search user interface allows the user to more
easily find what the user wants and has additional controls
(threshold adjustments) and visual feedback (colored seg-
ments/hits) that are not possible for the text search user
interface.
4 Experimental Results
We used three different corpora to assess the effectiveness
of different techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types.
The second corpus is the Switchboard corpus consist-
ing of two-party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types.
The third corpus is named Teleconference since it con-
sists of multi-party teleconferences on various topics. A
test set of six teleconferences (about 3.5 hours) was tran-
scribed. It contains 31106 word tokens and 2779 word
types. Calls are automatically segmented into a total of
1157 segments prior to ASR.
4.1 Speaker Segmentation
The performance of the speaker segmentation is evalu-
ated as follows. For an audio document, assume there
are N true boundaries, and the algorithm generates M
speaker boundaries. If a detected boundary is within
1 second of a true boundary, it is a correctly detected
boundary, otherwise it is a falsely detected boundary. Let
C denote the number of correctly detected boundaries,
the recall and precision of the boundary detection can be
computed as R = C/N and P = C/M , respectively.
We can combine these two values using the F-measure
F = 2 ? P ? R/(P + R) to measure the speaker seg-
mentation performance.
We evaluated the developed method on three different
types of audio documents: Broadcast News recordings
(16KHz sampling rate, 16 bits/sample), two-party tele-
phone conversations (8KHz, 16bps), and multi-party tele-
conference recordings (8KHz, 16bps). Due to the high
audio quality and well controlled structure of the broad-
cast news program, the achieved F-measure for broadcast
news data is 91%. Teleconference data has the worst au-
dio quality given the various devices (headset, speaker-
phone, etc.) used and different channels (wired and wire-
less) involved. There are also a lot of spontaneous speech
segments less than 1 second long, for example, ?Yes?,
?No?, ?Uh?, etc. These characteristics make the telecon-
ference data the most challenging one to segment. The
F-measure we achieved for this type of data is 70%. The
F-measure for two-party telephone conversations is in the
middle at 82%.
4.2 Automatic Speech Recognition
For evaluating ASR performance, we use the standard
word error rate (WER) as our metric. Since we are in-
terested in retrieval, we use OOV (Out Of Vocabulary)
rate by type to measure the OOV word characteristics.
In Table 1, we present the ASR performance on these
three tasks as well as the OOV Rate by type of the cor-
pora. It is important to note that the recognition vocabu-
lary for the Switchboard and Teleconference tasks are the
same and no data from the Teleconference task was used
while building the ASR systems.
Task WER OOV Rate by Type
Broadcast News ?20% 0.6%
Switchboard ?40% 6%
Teleconference ?50% 12%
Table 1: Word Error Rate and OOV Rate Comparison.
4.3 Retrieval
Our task is to retrieve the audio segments in which the
user query appears. For evaluating retrieval performance,
we use precision and recall with respect to manual tran-
scriptions. Let C(q) be the number of times the query
q is found correctly, M(q) be the number of answers
to the query q, and N(q) be the number of times q is
found in the reference. We compute precision and re-
call rates for each query as P (q) = C(q)/M(q) and
R(q) = C(q)/N(q). We report the average of these
quantities over a set of queries Q, P =
?
q?Q P (q)/|Q|
and R =
?
q?Q R(q)/|Q|. The set of queries Q includes
all the words seen in the reference except for a stop list of
the 100 most common words.
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve. In addition to individual precision-recall
values we also compute the F-measure defined above and
report the maximum F-measure (maxF) to summarize the
information in a precision-recall curve.
In Table 2, a comparison of the maximum F-measure
(maxF) is given for various corpora. Using word lattices
yields a relative gain of 3-5% in maxF over using one-
best word hypotheses. Using both word and phone lat-
tices, the relative gain over the baseline increases to 8-
12%. In this approach, we first search the word index;
if no matches are found then we search the phone index.
This allows the system to return matches even if the user
query is not in the ASR vocabulary.
Task System
1-best W Lats W+P Lats
Broadcast News 84.0 84.8 86.0
Switchboard 57.1 58.4 60.5
Teleconference 47.4 50.3 52.8
Table 2: Maximum F-measure Comparison.
In Figure 7, we present the precision-recall curves.
The gain from using better techniques utilizing word
and phone lattices increases as retrieval performance gets
worse.
0 20 40 60 80 1000
20
40
60
80
100
Precision
Reca
ll
Teleconferences
SwitchboardBroadcast News
1?best Word HypothesesWord LatticesWord and Phone Lattices
Figure 7: Precision vs Recall Comparison.
5 Summary
We presented a system for searching and browsing spo-
ken communications. The system is flexible enough to
support various forms of spoken communications. In this
paper, our focus was on telephone conversations and tele-
conferences. We also presented experimental results for
the speaker segmentation, ASR and retrieval components
of the system.
Acknowledgments
We would like to thank Richard Sproat for useful dis-
cussions and for making his lattice indexing software
(lctools) available for our system.
References
C. Allauzen, M. Mohri, and M. Riley.
2003. DCD Library ? Decoder Library.
http://www.research.att.com/sw/tools/dcd.
T. M. Cover and J. A. Thomas. 1991. Elements of Infor-
mation Theory. John Wiley & Sons.
D. Gibbon. 1998. Generating hypermedia documents
from transcriptions of television programs using paral-
lel text alignment. In B. Furht, editor, Handbook of In-
ternet and Multimedia Systems and Applications. CR-
CPress.
J. Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,
A. Rosenberg, L. Stark, L. Stead, S. Whittaker, and
G. Zamchick. 2001. Scanmail: Browsing and search-
ing speech data by content. In Proceedings of the
European Conference on Speech Communication and
Technology (Eurospeech), Aalborg, Denmark.
A. K. Jain and R. C. Dubes. 1988. Algorithms for Clus-
tering Data. Prentice-Hall.
A. Ljolje, M. Saraclar, M. Bacchiani, M. Collins, and
B. Roark. 2002. The AT&T RT-02 STT system. In
Proc. RT02 Workshop, Vienna, Virginia.
M. Mohri, F. C. N. Pereira, and M. Riley. 1997.
AT&T FSM Library ? Finite-State Machine Library.
http://www.research.att.com/sw/tools/fsm.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
A. Rosenberg, A. Gorin, Z. Liu, and S. Parthasarathy.
2002. Unsupervised speaker segmentation of tele-
phone conversations. In Proceedings of the Inter-
national Conference on Spoken Language Processing
(ICSLP), Denver, Colorado, USA.
M. Saraclar and R. Sproat. 2004. Lattice-based search
for spoken utterance retrieval. In Proc. HLT-NAACL.
M. Saraclar, M. Riley, E. Bocchieri, and V. Goffin. 2002.
Towards automatic closed captioning: Low latency
real time broadcast news transcription. In Proceedings
of the International Conference on Spoken Language
Processing (ICSLP), Denver, Colorado, USA.
L. Stark, S. Whittaker, and J. Hirschberg. 2000. ASR
satisficing: the effects of ASR accuracy on speech re-
trieval. In Proceedings of the International Conference
on Spoken Language Processing (ICSLP).
Interactive Machine Learning Techniques for Improving SLU Models 
 
Lee Begeja  
Bernard Renger 
AT&T Labs-Research 
180 Park Ave 
Florham Park, NJ 07932 
{lee,renger} 
@research.att.com 
David Gibbon  
Zhu Liu 
Behzad Shahraray 
AT&T Labs-Research 
200 Laurel Ave S 
 Middletown, NJ  07748 
{dcg, zliu, behzad} 
@research.att.com 
 
 
 
Abstract 
Spoken language understanding is a critical 
component of automated customer service ap-
plications.  Creating effective SLU models is 
inherently a data driven process and requires 
considerable human intervention.  We de-
scribe an interactive system for speech data 
mining.  Using data visualization and interac-
tive speech analysis, our system allows a User 
Experience (UE) expert to browse and under-
stand data variability quickly.  Supervised 
machine learning techniques are used to cap-
ture knowledge from the UE expert.  This cap-
tured knowledge is used to build an initial 
SLU model, an annotation guide, and a train-
ing and testing system for the labelers.  Our 
goal is to shorten the time to market by in-
creasing the efficiency of the process and to 
improve the quality of the call types, the call 
routing, and the overall application. 
1 Introduction 
The use of spoken dialogue systems to automate ser-
vices in call centers is continually expanding.  In one 
such system, unconstrained speech recognition is used 
in a limited domain to direct call traffic in customer call 
centers (Gorin et al 1997).  The challenge in this envi-
ronment is not only the accuracy of the speech recogni-
tion but more importantly, the knowledge and 
understanding of how the customer request is mapped to 
the business requirement.   
The first step of the process is to collect utterances 
from customers, which are transcribed.  This gives us a 
baseline for the types of requests (namely, the user in-
tents) that customers make when they call a client.  A 
UE expert working with the business customer uses 
either a spreadsheet or a text document to classify these 
calls into call types.  For example, 
? ?I want a refund?   REFUND 
? ?May I speak with an operator?  
GET_CUSTOMER_REP 
The end result of this process is a document, the an-
notation guide, that describes the types of calls that may 
be received and how to classify them.  This guide is 
then given to a group of ?labelers? who are trained and 
given thousands of utterances to label.  The utterances 
and labels are then used to create the SLU model for the 
application.  The call flow which maps the call types to 
routing destinations (dialog trajectory) is finalized and 
the development of the dialogue application begins.  
After the field tests, the results are given to the UE ex-
pert, who then will refine the call types, create a new 
annotation guide, retrain the labelers, redo the labels and 
create new ones from new data and rebuild the SLU 
model.   
Previously, this knowledge was only captured in a 
document and was not formalized until the SLU model 
was generated.  Our goal in creating our system is not 
only to give the UE expert tools to classify the calls, but 
to capture and formalize the knowledge that is gained in 
the process and to pass it on to the labelers.  We can 
thus automatically generate training instances and test-
ing scenarios for the labelers, thereby creating more 
consistent results.  Additionally, we can use the SLU 
model generated by our system to ?pre-label? the utter-
ances.  The labelers can then view these ?pre-labeled? 
utterances and either agree or disagree with the gener-
ated labels.  This should speed up the overall labeling 
process. 
More importantly, this knowledge capture will en-
able the UE expert to generate and test a SLU model as 
part of the process of creating the call types for the 
speech data.  The feedback from this initial SLU test 
allows the UE expert to refine the call types and to im-
prove them without having to train a group of labelers 
and to run a live test with customers.  This results in an 
improved SLU model and makes it easier to find prob-
lems before deployment, thus saving time and money. 
 Figure 1. System Diagram 
 
At the same time, the process is more efficient due to 
the increased uniformity in the way different UE experts 
classify calls into call type labels. 
We will describe Annomate, an interactive system 
for speech data mining.  In this system, we employ sev-
eral machine learning techniques such as clustering and 
relevance feedback in concert with standard text search-
ing methods.  We focus on interactive dynamic tech-
niques and visualization of the data in the context of the 
application. 
 The paper is organized as follows.  The overview of 
the system is presented in Section 2.  Section 3 briefly 
discusses the different components of the system.  Some 
results are given in Section 4.  Finally, in Sections 5 and 
6, we give conclusions and point to some future direc-
tions. 
2 System Overview 
In this section, we will give a system overview and 
show how automation has sped up and improved the 
existing process.  The UE expert no longer needs to 
keep track of utterances or call type labels in spread-
sheets. Our system allows the UE expert to more easily 
and efficiently label collected utterances in order to 
automatically build a SLU model and an electronic an-
notation guide (see System Diagram in Figure 1). The 
box in Figure 1 contains the new components used in 
the improved and more automated process of creating 
SLU models and annotation guides.  
After data collection, the Preprocessing steps (the 
data reduction and clustering steps are described in 
more detail below) reduce the data that the UE expert 
needs to work with thus saving time and money.  The 
Processed Data, which initially only contains the tran-
scribed utterances but later will also contain call types, 
is stored in an XML database which is used by the Web 
Interface.  At this point, various components of the Web 
Interface are applied to create call types from utterances 
and the Processed Data (utterances and call types) con-
tinue to get updated as these changes are applied.  These 
include the Clustering Tool to fine-tune the optimal 
clustering performance by adjusting the clustering 
threshold. Using this tool, the UE expert can easily 
browse the utterances within each cluster and compare 
the members of one cluster with those of its neighboring 
clusters.  The Relevance Feedback component is im-
plemented by the Call Type Editor Tool. This tool pro-
vides an efficient way to move utterances between two 
call types and to search relevant utterances for a specific 
call type. The Search Engine is used to search text in the 
utterances in order to facilitate the use of relevance 
feedback. It is also used to get a handle on utterance and 
La-
belers Transcribed 
Utterances 
 
Annotation 
Guide 
Preprocessing 
?Data Reduction 
?Clustering 
 
Processed 
Data 
Web-Enabled User Interface 
Clustering 
Relevance 
Feedback 
Search Engine 
 
Report 
Generation 
 
Initial 
SLU 
User  
Experience 
Expert 
SLU  
Toolset 
V 
I 
S 
U 
A 
L 
I 
Z 
A 
T 
I 
O 
N 
 
call type proximity using utterance and cluster dis-
tances.  
After a reasonable percentage of the utterances are 
populated or labeled into call types, an initial SLU 
model can be built and tested using the SLU Toolset. 
Although a reduced dataset is used for labeling (see 
discussion on clone families and reduced dataset be-
low), all the utterances are used when building the SLU 
model in order to take advantage of more data and 
variations in the utterances. The UE expert can itera-
tively refine the SLU model.  If certain test utterances 
are being incorrectly classified or are not providing suf-
ficient differentiability among certain call types (the 
SLU metric described below is used to improve call 
type differentiation), then the UE expert can go back 
and modify the problem call types (by adding utterances 
from other call types or by removing utterances using 
the Web Interface). The updated Processed Data can 
then be used to rebuild the SLU model and it can be 
retested to ensure the desired result.  This initial SLU 
model can also be used as a guide in determining the 
call flow for the application. 
The Reporting component of the Web Interface can 
automatically create the annotation guide from the 
Processed Data in the XML database at any time using 
the Annotation Guide Generation Tool. If changes are 
made to utterances or call types, then the annotation 
guide can be regenerated almost instantly.  Thus, this 
Web Interface allows the UE expert to easily and more 
efficiently create the annotation guide in an automated 
fashion unlike the manual process that was used before. 
3 Components 
Many SLU systems require data collection and some 
form of utterance preprocessing and possibly utterance 
clustering.  Our system uses relevance feedback and 
SLU tools to improve the SLU process. 
3.1 Data Collection 
Natural language data exists in a variety of forms such 
as documents, e-mails, and text chat logs. We will focus 
here on transcriptions of telephone conversations, and in 
particular, on data collected in response to the first 
prompt from an open dialogue system. The utterances 
collected are typically short phrases or single sentences, 
although in some cases, the caller may make several 
statements. It is assumed that there may be multiple 
intents for each utterance. We have also found that the 
methods presented here work well when used with the 
one-best transcription from a large vocabulary auto-
matic speech recognition system instead of manual tran-
scription.  
3.2 Preprocessing 
Our tools add structure to the raw collected data through 
a series of preprocessing steps. Utterance redundancy 
(and even repetition) is inherent in the collection proc-
ess and this is tedious for UE experts to deal with as 
they examine and work with the dataset. This section 
describes taking the original utterance set and reducing 
the redundancy (using text normalization, named entity 
extraction, and feature extraction) and thereby the vol-
ume of data to be examined. The end product of this 
processing is a subset of the original utterances that 
represents the diversity of the input data in a concise 
way. Sets of identical or similar utterances are formed 
and one utterance is selected at random to represent 
each set (alternative selection methods are also possible, 
see the Future Work section). UE experts may choose to 
expand these clone families to view individual mem-
bers, but the bulk of the interaction needs to only in-
volve a single representative utterance from each set.  
Text Normalization 
There is a near continuous degree of similarity between 
utterances. At one extreme are exact text duplicates 
(data samples in which two different callers say the ex-
act same thing). At the next level, utterances may differ 
only by transcription variants like ?100? vs. ?one hun-
dred? or ?$50? vs. ?fifty dollars.? Text normalization is 
used to remove this variation. Moving further, utter-
ances may differ only by the inclusion of verbal pauses 
or of transcription markup such as: ?uh, eh, background 
noise.? Beyond this, for many applications it is insig-
nificant if the utterances differ only by contraction: ?I?d 
vs. I would? or ?I wanna? vs. ?I want to.? Acronym 
expansions can be included here: ?I forgot my personal 
identification number? vs. ?I forgot my P I N.? Up to 
this point it is clear that these variations are not relevant 
for the purposes of intent determination (but of course 
they are useful for training a SLU classifier). We could 
go further and include synonyms or synonymous 
phrases: ?I want? vs. ?I need.?  Synonyms however, 
quickly become too powerful at data reduction, collaps-
ing semantically distinct utterances or producing other 
undesirable effects (?I am in want of a doctor.?)  Also, 
synonyms may be application specific.  
Text normalization is handled by string replacement 
mappings using regular expressions. Note that these 
may be represented as context free grammars and com-
posed with named entity extraction (see below) to per-
form both operations in a single step.  In addition to 
one-to-one replacements, the normalization includes 
many-to-one mappings (you   	
	-
to-null mappings (to remove noise words). 
Named Entity Extraction 
Utterances that differ only by an entity value should 
also be collapsed. For example ?give me extension 
12345? and ?give me extension 54321? should be repre-
sented by ?give me extension extension_value.? Named 
entity extraction is implemented through rules encoded 
using context free grammars in Backus-Naur form.  A 
library of generic grammars is available for such things 
as phone numbers and the library may be augmented 
with application-specific grammars to deal with account 
number formats, for example. The grammars are view-
able and editable, through an interactive web interface. 
Note that any grammars developed or selected at this 
point may also be used later in the deployed application 
but that the named entity extraction process may also be 
data driven in addition to or instead of being rule based.  
Feature Extraction 
To perform processing such as clustering, relevance 
feedback, or building prototype classifiers, the utter-
ances are represented by feature vectors. At the simplest 
level, individual words can be used as features (i.e., a 
unigram language model). In this case, a lexis or vo-
cabulary for the corpus of utterances is formed and each 
word is assigned an integer index. Each utterance is then 
converted to a vector of indices and the subsequent 
processing operates on these feature vectors. Other 
methods for deriving features include using bi-grams or 
tri-grams as features, weighting features based upon the 
number of times a word appears in an utterance or how 
unusual the word is in the corpus (TF, TF-IDF), and 
performing word stemming (Porter, 1980). When the 
dataset available for training is very small (as is the case 
for relevance feedback) it is best to use less restrictive 
features to effectively amplify the training data. In this 
case, we have chosen to use features that are invariant to 
word position, word count and word morphology and 
we ignore noise words. With this, the following two 
utterances have identical feature vector representations: 
? I need to check medical claim status 
? I need check status of a medical claim 
Note that while these features are very useful for the 
process of initially analyzing the data and defining call 
types, it is appropriate to use a different set of features 
when training classifiers with large amounts of data 
when building the SLU model to be fielded. In that case, 
tri-grams may be used, and stemming is not necessary 
since the training data will contain all of the relevant 
morphological variations. 
Clustering 
After the data reductions steps above, we use clustering 
as a good starting point to partition the dataset into clus-
ters that roughly map to call types. 
Clustering is grouping data based on their intrinsic 
similarities. After the data reduction steps described 
above, clustering is used as a bootstrapping process to 
create a reasonable set of call types. 
In any clustering algorithm, we need to define the 
similarity (or dissimilarity, which is also called distance) 
between two samples, and the similarity between two 
clusters of samples. Specifically, the data samples in our 
task are call utterances. Each utterance is converted into 
a feature vector, which is an array of terms and their 
weights. The distance of two utterances is defined as the 
cosine distance between corresponding feature vectors. 
Assume x and y are two feature vectors, the distance 
d(x,y) between them is given by 
yx
yxyx
?
?
?= 1),(d  
As indicated in the previous section, there are differ-
ent ways to extract a feature vector from an utterance. 
The options include named entity extraction, stop word 
removal, word stemming, N-gram on terms, and binary 
or TF-IDF (Term frequency ? inverse document fre-
quency) based weights. Depending on the characteris-
tics of the applications in hand, certain combinations of 
these options are appropriate. For all the results pre-
sented in this paper, we applied named entity extraction, 
stop word removal, word stemming, and 1-gram term 
with binary weights to extract the feature vectors. 
The cluster distance is defined as the maximum dis-
tance between any pairs of two utterances, one from 
each cluster. Figure 2 illustrates the definition of the 
cluster distance.  
 
 
 
Figure 2. Illustration of Cluster Distance. 
 
The range of utterance distance is from 0 to 1, and 
the range of the cluster distance is the same. When the 
cluster distance is 1, it means that there exists at least 
one pair of utterances, one from each cluster, that are 
totally different (sharing no common term).  
The clustering algorithm we adopted is the Hierar-
chical Agglomerative Clustering (HAC) method. The 
details of agglomerative hierarchical clustering algo-
rithm can be found in (Jan and Dubes, 1988).  The fol-
lowing is a brief description of the HAC procedure. 
Initially, each utterance is a cluster on its own. Then, for 
each iteration, two clusters with a minimum distance 
value are merged. This procedure continues until the 
minimum cluster distance exceeds a preset threshold. 
The principle of HAC is straightforward, yet the compu-
tational complexity and memory requirements may be 
high for large size datasets. We developed an efficient 
implementation of HAC by on-the-fly cluster/utterance 
distance computation and by keeping track of the cluster 
distances from neighboring clusters, such that the mem-
ory usage is effectively reduced and the speed is signifi-
cantly increased. 
Our goal is to partition the dataset into call types 
recognized by the SLU model and the clustering results 
provide a good starting point. It is easier to transform a 
set of clusters into call types than to create call types 
directly from a large set of flat data. Depending on the 
distance threshold chosen in the clustering algorithm, 
the clustering results may either be conservative (with 
small threshold) or aggressive (with large threshold). If 
the clustering is conservative, the utterances of one call 
type may be scattered into several clusters, and the UE 
expert has to merge these clusters to create the call type. 
On the other hand, if the cluster is aggressive, there may 
be multiple call types in one cluster, and the UE expert 
needs to manually split the mixture cluster into different 
call types. In real applications, we tend to set a rela-
tively low threshold since it is easier to merge small 
homogeneous clusters than to split one big heterogene-
ous cluster.  
3.3 Relevance Feedback 
Although clustering provides a good starting point, find-
ing all representative utterances belonging to one call 
type is not a trivial task. Effective data mining tools are 
desirable to help the UE expert speed up this manual 
procedure. Our solution is to provide a relevance feed-
back mechanism based on support vector machine 
(SVM) techniques for the UE expert to perform this 
tedious task.  
Relevance feedback is a form of query-free retrieval 
where documents are retrieved according to a measure 
of relevance to given documents. In essence, a UE ex-
pert indicates to the retrieval system that it should re-
trieve ?more documents like the ones desired, not the 
ones ignored.? Selecting relevant documents based on 
UE expert?s inputs is basically a classification (rele-
vant/irrelevant) problem. We adopted support vector 
machine as the classifier for to two reasons: First, SVM 
efficiently handles high dimensional data, especially a 
text document with a large vocabulary. Second, SVM 
provides reliable performance with small amount of 
training data. Both advantages perfectly match the task 
at hand. For more details about SVM, please refer to 
(Vapnik, 1998; Drucker et al 2002).  
Relevance feedback is an iterative procedure. The 
UE expert starts with a cluster or a query result by cer-
tain keywords, and marks each utterance as either a 
positive or negative utterance for the working call type. 
The UE expert?s inputs are collected by the relevance 
feedback engine, and they are used to build a SVM clas-
sifier that attempts to capture the essence of the call type. 
The SVM classifier is then applied to the rest of the 
utterances in the dataset, and it assigns a relevance score 
for each utterance. A new set of the most relevant utter-
ances are generated and presented to the UE expert, and 
the second loop of relevance feedback begins. During 
each loop, the UE expert does not need to mark all the 
given utterances since the SVM is capable of building a 
reasonable classifier based on very few, e.g., 10, train-
ing samples. The superiority of relevance feedback is 
that instead of going through all the utterances one by 
one to create a specific call type, the UE expert only 
needs to check a small percentage of utterances to create 
a satisfactory call type.  
 
 
 
Figure 3. The Interface for Relevance Feedback. 
 
The relevance feedback engine is implemented by 
the Call Type Editor Tool. This tool provides an inte-
grated environment for the UE expert to create a variety 
of call types and assign relevant utterances to them.  
The tool provides an efficient way to move utterances 
between two call types and to search relevant utterances 
for a specific call type. The basic search function is to 
search a keyword or a set of keywords within the dataset 
and retrieve all utterances containing these search terms. 
The UE expert can then assign these utterances into the 
appropriate call types. Relevance feedback serves as an 
advanced searching option. Relevance feedback can be 
applied to the positive and negative utterances of a clus- 
 Table 1. Data Reduction Results 
 
ter or call type or can be applied to utterances, from a 
search query, which are marked as positive or negative. 
The interface for the relevance feedback is shown in 
Figure 3. In the interface, the UE expert can mark the 
utterances as positive or negative samples. The UE ex-
pert can also control the threshold of the relevance value 
such that the relevance feedback engine only returns 
utterances with high enough relevance values. In the 
tool, we are using an internally developed package for 
learning large margin classifiers to implement the SVM 
classifier (Haffner et al 2003). 
   
3.4 SLU Toolset 
The SLU toolset is based on an internally developed 
NLU Toolset. The underlying boosting algorithm for 
text classification used, BoosTexter, is described else-
where (Freund and Schapire, 1999; Schapire and Singer, 
2000; Rochery et al 2002). We added interactive input 
and display capabilities via a Web interface allowing the 
UE expert to easily build and test SLU models. 
Named entity grammars are constructed as described 
above. About 20% of the labeled data is set aside for 
testing. The remaining data is used to build the initial 
SLU model which is used to test the utterances set aside 
for testing. The UE expert can interactively test utter-
ances typed into a Web page or can evaluate the test 
results of the test data.  For each of the tested utterances 
in the test data, test logs show the classification confi-
dence scores for each call type. The confidence scores 
are replaced by probability thresholds that have been 
computed using a logistic function.  These scores are 
then used to calculate a simple metric which is a meas-
ure of call type differentiability. If the test utterance 
labeled by the UE expert is correctly classified, then the 
call type is the truth call type. The SLU metric is calcu-
lated as follows and it is averaged over the utterances: 
? if the call type is the truth, the score is the dif-
ference (positive) between the truth probability 
and the next highest probability 
? if the call type is not the truth, the score is the 
difference (negative) between the truth prob-
ability and the highest probability 
This metric allows the UE expert to easily spot prob-
lem call types or those that might give potential prob-
lems in the field.  It is critical that call types are easily 
differentiable in order to properly route the call.  The 
UE expert can iteratively build and test the initial SLU 
models until the UE expert has a set of self-consistent 
call types before creating the final annotation guide.  
The final annotation guide would then be used by the 
labelers to label all the utterance data needed to build 
the final SLU model.  Thus, the SLU Toolset is critical 
for creating the call types defined in the annotation 
guide which in turn is needed to label the data for creat-
ing the final SLU. 
Alternatively, the labeled utterances can easily be 
exported in a format compatible with the internally de-
veloped NLU Toolset if further SLU model tuning is to 
be performed by the NLU expert using just the com-
mand line interface. 
3.5 Reporting 
One of the reporting components is the Annotation 
Guide Generation Tool.  The UE expert can use this at 
any time to automatically generate the annotation guide 
from the Processed Data.  Other reporting components 
include summary statistics and spreadsheets containing 
utterance and call type information. 
4 Results 
The performance of the preprocessing techniques has 
been evaluated on several datasets from various industry 
sectors. Approximately 10,000 utterances were col-
lected for each application and the results of the data 
reduction at each processing stage are shown in Table 1. 
The Redundancy R is given by 
N
UR ?= 1  
where U is the number of unique utterances after feature 
extraction and N is the number of original utterances.  
Industry 
Sector 
Original 
Utterances 
Unique 
Utterances 
Unique  
Utterances after 
Text  
Normalization 
Unique 
Utterances  
after Entity 
Extraction 
Unique 
Utterances  
after Feature 
Extraction 
Redundancy 
Financial 11,623 10,021 9,670 9,165 7,929 31.8% 
Healthcare 12,080 10,255 9,452 9,382 7,946 34.2% 
Insurance 12,109 8,865 8,103 7,963 6,530 46.1% 
Retail 10,240 4,956 4,392 4,318 3,566 65.2% 
Initial UE experts of the tools have been successful 
in producing annotation guides more quickly and with 
very good initial F-measures.  
recallprecision
recallprecisionF
+
??
=
2
 
They have also reported that the task is much less 
tedious and that they have done a better job of covering 
all of the significant utterance clusters. Further studies 
are required to generate quantitative measures of the 
performance of the toolset. 
5 Future Work 
In the future, the system could be improved using other 
representative utterance selection algorithms (e.g., se-
lecting the utterance with the minimum string edit dis-
tance to all others).  
The grammars for entity extraction were not tuned 
for these applications and it is expected that further data 
reduction will be obtained with improved grammars. 
6 Conclusions 
We presented an interactive speech data analysis system 
for creating and testing spoken language understanding 
systems.  Spoken language understanding is a critical 
component of automated customer service applications.  
Creating effective SLU models is inherently a data 
driven process and requires considerable human inter-
vention.  The fact that this process relies heavily on hu-
man expertise prevents a total automation of the 
process. Our experience indicates that augmenting the 
human expertise with interactive data analysis tech-
niques made possible by machine learning techniques 
can go a long way towards increasing the efficiency of 
the process and the quality of the final results.  The 
automatic preprocessing of the utterance data prior to its 
use by the UE expert results in a considerable reduction 
in the number of utterances that needs to be manually 
examined. Clustering uncovers certain structures in the 
data that can then be refined by the UE expert. Super-
vised machine learning capabilities provided by interac-
tive relevance feedback tend to capture the knowledge 
of the UE expert to create the guidelines for labeling the 
data.  The ability to test the generated call types during 
the design process helps detect and remove problematic 
call types prior to their inclusion in the SLU model.  
This tool has been used to create the labeling guide for 
several applications by different UE experts.  Aside 
from the increased efficiency and improved quality of 
the generated SLU systems, the tool has resulted in in-
creased uniformity in the way different UE experts clas-
sify calls into call type labels.  
Acknowledgements 
We would like to thank Harris Drucker, Patrick Haffner, 
Steve Lewis, Maria Alvarez-Ryan, Barbara Hollister, 
Harry Blanchard, Liz Alba, Elliot Familant, Greg Pulz, 
David Neeves, Uyi Stewart, and Lan Zhang for their 
contributions to this work. 
 
 
References 
 
Harris Drucker, Behzad Shahraray, and David C. Gib-
bon, 2002. Support Vector Machines: Relevance 
Feedback and Information Retrieval, Information 
Processing and Management, 38(3):305-323. 
Yaov Freund and Robert Schapire, 1999. A Short Intro-
duction to Boosting, Journal of Japanese Society for 
Artificial Intelligence, 14(5):771-780. 
Patrick Haffner, Gokhan Tur, and Jerry Wright, 2003. 
Optimizing SVMs for complex Call Classification, 
ICASSP 2003. 
A. L. Gorin, G. Riccardi, and J. H. Wright. 1997. How 
May I Help You? Speech Communication, 23:113-
127. 
A. K. Jan and R. C. Dubes, 1988. Algorithms for Clus-
tering Data, Prentice Hall. 
M. F. Porter, 1980. An Algorithm For Suffix Stripping, 
Program, 14(3):130-137. 
M. Rochery, R. Schapire, M. Rahim, N. Gupta, G. Ric-
cardi, S. Bangalore, H. Alshawi and S. Douglas, 
2002. Combining prior knowledge and boosting for 
call classification in spoken language dialogue, 
ICASSP 2002. 
SAS Institute Press Release, 2002. New SAS? Text Min-
ing Software Surfaces Intelligence beyond the Num-
bers, 1/21/02. 
Robert Schapire and Yoram Singer, 2000. BoosTex-
ter: A Boosting-based System for Text Categorization, 
Machine Learning, 39(2/3):135-168. 
Gokhan Tur, Robert E. Schapire, and Dilek Hakkani-
T?r, 2003. Active Learning for Spoken Language 
Understanding, Proceedings of International Confer-
ence on Acoustics, Speech and Signal Processing, 
ICASSP 2003. 
V. N. Vapnik, 1998. Statistical Learning Theory, John 
Wiley & Sons, Inc. 
 
