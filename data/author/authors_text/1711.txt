Identifying Semitic Roots: Machine Learning
with Linguistic Constraints
Ezra Daya?
University of Haifa
Dan Roth??
University of Illinois
Shuly Wintner?
University of Haifa
Words in Semitic languages are formed by combining two morphemes: a root and a pattern. The
root consists of consonants only, by default three, and the pattern is a combination of vowels
and consonants, with non-consecutive ?slots? into which the root consonants are inserted.
Identifying the root of a given word is an important task, considered to be an essential part
of the morphological analysis of Semitic languages, and information on roots is important for
linguistics research as well as for practical applications. We present a machine learning approach,
augmented by limited linguistic knowledge, to the problem of identifying the roots of Semitic
words. Although programs exist which can extract the root of words in Arabic and Hebrew, they
are all dependent on labor-intensive construction of large-scale lexicons which are components of
full-scale morphological analyzers. The advantage of our method is an automation of this process,
avoiding the bottleneck of having to laboriously list the root and pattern of each lexeme in the
language. To the best of our knowledge, this is the first application of machine learning to this
problem, and one of the few attempts to directly address non-concatenative morphology using
machine learning. More generally, our results shed light on the problem of combining classifiers
under (linguistically motivated) constraints.
1. Introduction
The standard account of word-formation processes in Semitic languages describes
words as combinations of two morphemes: a root and a pattern.1 The root consists of
consonants only, by default three (although longer roots are known), called radicals.
The pattern is a combination of vowels and, possibly, consonants too, with ?slots? into
which the root consonants can be inserted. Words are created by interdigitating roots
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: edaya@cs.haifa.ac.il.
?? Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801. E-mail:
danr@cs.uiuc.edu.
? Department of Computer Science, University of Haifa, 31905 Haifa, Israel. E-mail: shuly@cs.haifa.ac.il
1 An additional morpheme, vocalization, is used to abstract the pattern further; for the present purposes,
this distinction is irrelevant.
Submission received: 19 June 2006; revised submission received: 30 May 2007; accepted for publication:
12 October 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 3
into patterns: The first radical is inserted into the first consonantal slot of the pattern,
the second fills the second slot, and the third fills the last slot. See Shimron (2003) for a
survey.
We present a machine learning approach, augmented by limited linguistic knowl-
edge, to the problem of identifying the roots of Semitic words. To the best of our
knowledge, this is the first application of machine learning to this problem, and one
of the few attempts to directly address the non-concatenative morphology of Semitic
languages using machine learning. Although there exist programs which can extract the
roots of words in Arabic (Beesley 1998a, 1998b) andHebrew (Choueka 1990), they are all
dependent on labor-intensive construction of large-scale lexicons which are components
of full-scale morphological analyzers. Note that the Arabic morphological analyzer of
Buckwalter (2002, software documentation) only uses ?word stems?rather than root
and pattern morphemes?to identify lexical items.? Buckwalter further notes that ?The
information on root and pattern morphemes could be added to each stem entry if
this were desired.? The challenge of our work is to automate this process, avoiding
the bottleneck of having to laboriously list the root and pattern of each lexeme in the
language.
Identifying the root of a given word is a non-trivial problem, due to the complex
nature of Semitic derivational and inflectional morphology and the peculiarities of the
orthography. It is also an important task. Although existingmorphological analyzers for
Hebrew only provide a lexeme (which is a combination of a root and a pattern), for other
Semitic languages, notably Arabic, the root is an essential part of any morphological
analysis simply because traditional dictionaries are organized by root, rather than by
lexeme (Owens 1997). Information on roots is important for linguistic research, because
roots can shed light on etymological processes, both within a single language and across
languages. Furthermore, roots are known to carry some meaning, albeit vague. This
information can be useful for computational applications: For example, several studies
show that indexing Arabic documents by root improves the performance of information
retrieval systems (Al-Kharashi and Evens 1994; Abu-Salem, Al-Omari, and Evens 1999;
Larkey, Ballesteros, and Connell 2002).2
The contributions of this article are manifold. First and foremost, we report on
a practical system which can be used to extract roots in Hebrew and Arabic (the
system is freely available; an on-line demo is provided at http://cl.haifa.ac.il/
projects/roots/index.shtml). The system can be used for practical applications or
for scientific (linguistic) research, and constitutes an important addition to the grow-
ing set of resources dedicated to Semitic languages. It is one of the few attempts to
directly address the non-concatenative morphology of Semitic languages and extract
non-contiguousmorphemes from surface forms. As amachine learning application, this
work describes a set of experiments in combination of classifiers under constraints. The
resulting insights can be used for other applications of the same techniques for similar
problems (see, e.g., Habash and Rambow 2005). Furthermore, this work demonstrates
that providing a data-driven classifier with limited linguistic knowledge significantly
improves the classification results.
We focus on Hebrew in the first part of this article. After sketching the linguistic
data in Section 2 and our methodology in Section 3, we discuss in Section 4 a simple,
baseline, learning approach.We then propose several methods for combining the results
2 These results are challenged by Darwish and Oard (2002), who conclude that roots are inferior to
character n-grams for this task.
430
Daya, Roth, and Wintner Identifying Semitic Roots
of interdependent classifiers in Section 5 and demonstrate the benefits of using limited
linguistic knowledge in the inference procedure. Then, the same technique is applied
to Arabic in Section 6 and we demonstrate comparable improvements. In Section 7
we discuss the influence of global constraints on local classifiers. We conclude with
suggestions for future research.
2. Linguistic Background
Root and pattern morphology is the major word formation device of Semitic languages.
As an example, consider the Hebrew roots g.d.l, k.t.b, and r.$.m and the patterns
haCCaCa, hitCaCCut, and miCCaC, where the ?C?s indicate the slots.3 When the
roots combine with these patterns the resulting lexemes are hagdala, hitgadlut, migdal,
haktaba, hitkatbut, miktab, har$ama, hitra$mut, and mir$am, respectively. After the
root combines with the pattern, some morpho-phonological alternations take place,
which may be non-trivial: For example, the hitCaCCut pattern triggers assimilation
when the first consonant of the root is t or d : thus, d.r.$+hitCaCCut yields hiddar$ut.
The same pattern triggers metathesis when the first radical is s or $ : s.d.r+hitCaCCut
yields histadrut rather than the expected *hitsadrut. Semi-vowels such as w or y in the
root are frequently combined with the vowels of the pattern, so that q.w.m+haCCaCa
yields haqama, and so on. Frequently, root consonants such as w or y are altogether
missing from the resulting form.
These matters are complicated further due to two sources: First, the standard
Hebrew orthography leaves most of the vowels unspecified.4 It does not explicate a and
e vowels, does not distinguish between o and u vowels and leaves many of the i vowels
unspecified. Furthermore, the single letter w is used both for the vowels o and u and for
the consonant v, whereas i is similarly used both for the vowel i and for the consonant y.
On top of that, the script dictates thatmany particles, including four of themost frequent
prepositions, the definite article, the coordinating conjunction, and some subordinating
conjunctions, all attach to the words which immediately follow them. Thus, a form such
as mhgr can be read as a lexeme (?immigrant?), as m-hgr (?from Hagar?), or even as
m-h-gr (?from the foreigner?). Note that there is no deterministic way to tell whether the
first m of the form is part of the pattern, the root, or a prefixing particle (the preposition
m ?from?).
The Hebrew script has 22 letters, all of which can be considered consonants. The
number of tri-consonantal roots is thus theoretically bounded by 223, although several
phonological constraints limit this number to a much smaller value. For example,
although roots whose second and third radicals are identical abound in Semitic lan-
guages, roots whose first and second radicals are identical are extremely rare (see
McCarthy 1981 for a theoretical explanation). To estimate the number of roots in Hebrew
we compiled a list of roots from two sources: a dictionary (Even-Shoshan 1993) and the
verb paradigm tables of Zdaqa (1974). The union of these yields a list of 2,152 roots.5
3 To facilitate readability we use a straight-forward transliteration of Hebrew using ASCII characters,
where the characters (in Hebrew alphabetic order) are: ?bgdhwzxviklmnsypcqr$t.
4 In this work we consider only texts in undotted, or unvocalized script. This is the standard script of both
Hebrew and Arabic.
5 Only tri-consonantal roots are counted. Ornan (2003) mentions 3,407 roots, whereas the number of roots
in Arabic is estimated to be 10,000 (Darwish 2002). We do not know why Arabic should have so many
more roots than Hebrew.
431
Computational Linguistics Volume 34, Number 3
Whereas most Hebrew roots are regular, many belong to weak paradigms, which
means that root consonants undergo changes in some patterns. Examples include i or n
as the first root consonant, w or i as the second, i as the third, and roots whose second
and third consonants are identical. For example, consider the pattern hCCCh. Regular
roots such as p.s.q yield forms such as hpsqh. However, the irregular roots n.p.l, i.c.g,
q.w.m, and g.n.n in this pattern yield the seemingly similar forms hplh, hcgh, hqmh,
and hgnh, respectively. Note that in the first and second examples, the first radical (n or
i ) is missing, in the third the second radical (w) is omitted, and in the last example one
of the two identical radicals is omitted. Consequently, a form such as hC1C2h can have
any of the roots n.C1.C2, C1.w.C2, C1.i.C2, C1.C2.C2 and even, in some cases, i.C1.C2.
Although root and pattern morphology is the major word formation device of Se-
mitic languages, both Hebrew and Arabic have words which are not generated through
this mechanism, and therefore have no root. These are either loan words (which are
oftentimes longer than originally Semitic words, in particular in the case of proper
names) or short functional or frequent wordswhose origin is more ancient. For example,
the most frequent token in Hebrew texts is the accusative preposition ?t, which is not
formed through root and pattern processes. Of course, theremay be surface formswhich
are ambiguous: one reading based on root and pattern morphology, the other a loan
word, for example, npl (either ?Nepal? or ?fall? (past, 3rd person masculine singular)).
Although the Hebrew script is highly ambiguous, ambiguity is somewhat reduced
for the task we consider here, as many of the possible lexemes of a given form share
the same root. Still, in order to correctly identify the root of a given word, context
must be taken into consideration. For example, the form $mnh has more than a dozen
readings, including the adjective ?fat? (feminine singular), which has the root $.m.n,
and the verb ?count,? whose root is m.n.i, preceded by a subordinating conjunction.
In the experiments we describe herein we ignore context completely, so our results
are handicapped by design. Adding contextual information renders the problem very
similar to that of word sense disambiguation (as different roots denote distinct senses),
and we opted to focus only on morphology here.
3. Data and Methodology
3.1 Machine-Learning Framework
In this work we apply several machine-learning techniques to the problem of root
identification. In all the experiments described in this article we use SNoW (Roth
1998; Carlson et al 1999) as the learning environment, with Winnow as the update
rule (using Perceptron yielded comparable results). SNoW is a multi-class classifier
that is specifically tailored for learning in domains in which the potential number of
information sources (features) taking part in decisions is very large. It is an on-line
linear classifier, as are most of the classifiers currently used in NLP, over a variable
number of expressive features. In addition to the ?standard? perceptron-like algorithms,
SNoW has a number of extensions such as regularization and good treatment of multi-
class classification. SNoW provides, in addition to classification, a reliable confidence in
the instance prediction which facilitates its use in an inference algorithm that combines
predictors to produce a coherent inference.
SNoW has already been used successfully as the learning vehicle in a large col-
lection of natural language related tasks, including part-of-speech tagging, shallow
parsing, information extraction tasks, and so forth, and compared favorably with other
classifiers (Roth 1998; Golding and Roth 1999; Banko and Brill 2001; Punyakanok and
432
Daya, Roth, and Wintner Identifying Semitic Roots
Roth 2001; Florian 2002; Punyakanok, Roth, and Yih 2005). The choice of SNoW as
the learning algorithm in this work is motivated by its good performance on other,
similar tasks and on the availability in SNoW of easy to tune state-of-the-art versions
of three linear algorithms (Perceptron, Winnow, and naive Bayes) in a single package.
As was shown by Roth (1998), Golding and Roth (1999), and in countless experimental
papers thereafter, most algorithms used today, from on-line variations of Winnow and
Perceptron to maximum entropy algorithms to SVMs, perform comparably if tuned
properly, and the eventual performance depends mostly on the selection of features.
3.2 Data and Evaluation
For training and testing, a Hebrew linguist manually tagged a corpus of 15,000 words
(from a set of newspaper articles). Of these, only 9,752 were annotated; the reason for
the gap is that some Hebrew words, mainly borrowed but also some frequent words
such as prepositions, are not formed by the root and pattern paradigm. Such words are
excluded from our experiments in this work; in an application, such words have to be
identified and handled separately. This can be rather easily done using simple heuristics
and a small list of frequent closed-class words, because words which do not conform
to the root and pattern paradigm are either (short, functional) closed-class words, or
loan words which tend to be longer and, in many cases, involve ?foreign? characters
(typically proper names). This problem is orthogonal to the problem of identifying the
root, and hence a pipeline approach is reasonable.
We further eliminated 168 roots with more than three consonants and were left
with 5,242 annotated word types, exhibiting 1,043 different roots. Table 1 shows the
distribution of word types according to root ambiguity.
Table 2 provides the distribution of the roots of the 5,242 word types in our corpus
according to root type, where Ri is the ith radical (note that some roots may belong to
more than one group).
Table 1
Root ambiguity in the corpus.
Number of roots 1 2 3 4
Number of word types 4,886 335 18 3
Table 2
Distribution of root paradigms.
Paradigm Number Percentage
R1 = i 414 7.90
R1 = w 28 0.53
R1 = n 419 7.99
R2 = i 297 5.66
R2 = w 517 9.86
R3 = h 18 0.19
R3 = i 677 12.92
R2 = R3 445 8.49
Regular 3,061 58.41
433
Computational Linguistics Volume 34, Number 3
As assurance for statistical reliability, in all the experiments discussed in the sequel
(unless otherwise mentioned) we performed 10-fold cross validation runs for every
classification task during evaluation. We also divided the annotated corpus into two
sets: a training set of 4,800 words and a test set of 442 words. Only the training set
was used for most experiments, and the results reported here refer to these data unless
stated otherwise. We used the training set to tune the parameter ? (see Section 5.4), and
once ? was set we report on results obtained by training on the training set and testing
on test data (Table 8).
A given example is a word type with all its (manually tagged) possible roots. In
the experiments we describe subsequently, our system produces one or more root
candidates for each example. For each example, we define tp as the number of correct
candidates produced by the system; fp as the number of candidates which are not cor-
rect roots; and fn as the number of roots the system did not produce. As usual, we define
recall as tp
tp+fp
, precision as tp
tp+fn
and F-score as 2?recall?precision
recall+precision
; we then (macro-) average
over all words to obtain the system?s overall F-score.
To estimate the difficulty of this task, we asked six human subjects to perform
it. Participants were asked to identify all the possible roots of all the words in a list
of 200 words (without context), randomly chosen from the training corpus. All par-
ticipants were computer science graduates, native Hebrew speakers with no linguistic
background. The average precision of humans on this task is 83.52%, and with recall at
80.27%, F-score is 81.86%. We conjecture that the main reasons for the low performance
of our subjects are the lack of context (people tend to pick the most prominent root and
ignore the less salient ones) and the ambiguity of some of the weak paradigms (Hebrew
speakers are unaware of the correct root in many of the weak paradigms, even when
only one exists).
3.3 Feature Design
All the experiments we describe in this work share the same features and differ only in
the target classifiers. One of the advantages of SNoW is that it makes use of variable-
sized feature vectors, represented as the list of the active (present) features in a given
instance, rather than the fixed-sized Boolean vectors. This facilitates the use of very long
(theoretically, unbounded) feature lists, which are typically very sparse. The features
that are used to characterize a word are both grammatical and statistical:
 Position of letters (e.g., the third letter of the word is b). We limit word
length to 20, as the longest string generated by a Hebrew morphological
generator (Yona and Wintner 2008) is 18. We thus obtain up to 440
features6 of this type (recall that the size of the alphabet is 22).
 Bigrams of letters, independently of their location (e.g., the substring gd
occurs in the word). This yields up to 484 features.
 Prefixes (e.g., the word is prefixed by k$h, ?when the?). We have 292
features of this type, corresponding to 17 prefixes and sequences thereof.
 Suffixes (e.g., the word ends with im, a plural suffix). There are 26 such
features.
6 Some of these features are never active and are thus never represented.
434
Daya, Roth, and Wintner Identifying Semitic Roots
The lists of suffixes and of prefix sequences were compiled from a morphological
grammar of Hebrew (Yona and Wintner 2008). In the general case, such features can be
elicited from (non-expert) native speakers or extracted from amorphologically analyzed
corpus, if one exists.
3.4 Linguistic Resources
One of our goals in this work is to demonstrate the contribution of limited linguistic
knowledge to a machine-learning approach to an NLP task. Specifically, we used the
following resources for Hebrew and Arabic:
 A list of roots (Section 2)
 Lists of common prefixes and suffixes (Section 3.3)
 Corpora annotated with roots (Section 3.2)
 Knowledge of word-formation processes, and in particular the behavior of
the weak roots in certain paradigms (see Section 5.4)
It is important to note that these resources do not constitute a method for identifying,
even approximately, the root of a given word. We are unaware of any set of rules which
attempts to address this task, or of the chances of solving this problem deterministically.
4. Naive Classification Methods
4.1 Direct Prediction
To establish a baseline, we first performed two experiments with simple, baseline clas-
sifiers. In the first of the two experiments, referred to as Experiment A, we trained a
classifier to learn roots as a single unit. The two obvious drawbacks of this approach
are the large set of targets and the sparseness of the training data. Of course, defining a
multi-class classification task with 2,152 targets, when only half of them are manifested
in the training corpus, does not leave much hope for ever learning to identify the
missing targets. There is no generalization when the whole root is predicted as a single
unit with a simple classifier.
In Experiment A, themacro-average precision of 10-fold cross validation runs of this
classification problem is 45.72%; recall is 44.37%, yielding an F-score of 45.03%. In order
to demonstrate the inadequacy of this method, we repeated the same experiment with
a different organization of the training data. We chose 30 roots and collected all their
occurrences in the corpus into a test file. We then trained the classifier on the remainder
of the corpus and tested on the test file. As expected, the accuracy was close to 0%.
4.2 Decoupling the Problem
In the second experiment, referred to as Experiment B, we separated the problem into
three different tasks. We trained three classifiers to learn each of the root consonants
in isolation and then combined the results in the straightforward way (a conjunction
of the decisions of the three classifiers). This is still a multi-class classification but the
435
Computational Linguistics Volume 34, Number 3
number of targets in every classification task is only 22 (the number of letters in the
Hebrew alphabet) and data sparseness is no longer a problem. Although each classifier
performs well in isolation, the clear limitation of this method is that it completely
ignores interdependencies between different targets: The decision on the first radical
is completely independent of the decision on the second and the third.
We observed a difference between recognizing the first and third radicals and
recognizing the second one, as can be seen in Table 3. These results correspond well
to our linguistic intuitions: The most difficult cases for humans are those in which
the second radical is w or i, and those where the second and the third consonants are
identical. Combining the three classifiers using logical conjunction yields an F-score of
52.84%. Repeating the same experiment, but testing only on unseen roots, yielded 18.1%
accuracy.
To demonstrate the difficulty of the problem, we conducted yet another experiment.
Here, we trained the system as described but we tested it on different wordswhose roots
were known to be in the training set. The results of Experiment A here were 46.35%,
whereas Experiment B was accurate in 57.66% of the cases. Evidently, even when testing
only on previously seen roots, both naive methods are unsuccessful.
5. Combining Interdependent Classifiers
5.1 Adding Linguistic Constraints
The experiments discussed previously are completely devoid of linguistic knowledge.
In particular, Experiment B inherently assumes that any sequence of three consonants
can be the root of a given word. This is obviously not the case: With few exceptions, all
radicals must be present in any inflected form. In fact, when roots and patterns combine,
the first radical can be deleted only when it is w, i, n, and in an exceptional case l ; the
second radical can only be deleted if it is w or i ; and the third, only if it is i. We therefore
trained the classifiers to consider as targets, during training and testing, only letters that
occurred in the observedword, plus w, i, n, and l (depending on the radical), rather than
any of the alphabet letters. The average number of targets is now 7.2 for the first radical,
5.7 for the second, and 5.2 for the third (compared to 22 each in the previous setup).
In this model, known as the sequential model (Even-Zohar and Roth 2001), SNoW?s
performance improved slightly, as can be seen in Table 4 (compare to Table 3). Com-
bining the results in the straight-forward way yields an F-score of 58.89%, a small
improvement over the 52.84% performance of the basic method. This new result should
be considered the baseline. In what follows we always employ the sequential model
for training and testing the classifiers, using the same constraints. However, we employ
more linguistic knowledge for a more sophisticated combination of the classifiers.
Table 3
Accuracy of identifying the correct radical.
R1 R2 R3 root
Precision 82.25 72.29 81.85 53.60
Recall 80.13 70.00 80.51 52.09
F-score 81.17 71.13 81.18 52.84
436
Daya, Roth, and Wintner Identifying Semitic Roots
Table 4
Accuracy of identifying the correct radical, sequential model.
R1 R2 R3 root
Precision 83.06 72.52 83.88 59.83
Recall 80.88 70.20 82.50 57.98
F-score 81.96 71.34 83.18 58.89
5.2 Sequential Combination
Evidently, simple combination of the results of the three classifiers leaves much room
for improvement. We therefore explore other ways for combining these results. We can
rely on the fact that SNoW provides insight into the decisions of the classifiers?it lists
not only the selected target, but rather all candidates, with an associated confidence
measure. Apparently, the correct radicals are chosen among SNoW?s top-n candidates
with high accuracy, as shown in Table 5. This observation calls for a different way
of combining the results of the classifiers which takes into account not only the first
candidate but also others, along with their confidence scores.
Given the sequential nature of the data and the fact that our classifier returns
a distribution over the possible outcomes for each radical, a natural approach is to
combine SNoW?s outcomes via a Markovian approach. Variations of this approach
are used in the context of several natural language problems, including part-of-speech
tagging (Schu?tze and Singer 1994), shallow parsing (Punyakanok and Roth 2001), and
named entity recognition (Tjong Kim Sang and De Meulder 2003).
However, perhaps not surprisingly given the difficulty of the problem, this model is
too simplistic. In fact, performance deteriorated to an F-score of 37.79%. We conjecture
that the static probabilities (the model) are too biased and cause the system to abandon
good choices obtained from SNoW in favor of worse candidates whose global behavior
is better. For example, the root q.r.n was correctly generated by SNoW as the best
candidate for the word mqrn, but because P(R3 = r | R2 = r), which is 0.066, is higher
than P(R3 = n | R2 = r), which is 0.025, the root q.r.r was produced instead.
Similar examples of interdependencies among radicals abound. In Hebrew and
Arabic, some letters cannot appear in a sequence, mostly due to phonetic restrictions.
For example, if the first radical is s, the second radical cannot be z, c, or e. Taking into
account the dependency between the root radicals is an interesting learning experiment
which may provide a better results. We therefore extend the naive HMM approach to
account for such dependencies, following the PMM model of Punyakanok and Roth
(2001).
Table 5
Recall of identifying the correct radical among top-n candidates
R1 R2 R3
top-1: 80.88 70.20 82.50
top-2: 92.98 86.99 93.85
top-5: 99.14 99.38 99.68
top-10: 99.69 99.90 99.70
437
Computational Linguistics Volume 34, Number 3
Consider a specific example of some word w. We already trained a classifier for
R1 (the first root radical), so we can look at the predictions of the R1 classifier for w.
Assume that this classifier predicts a1, a2, a3, . . . , ak with confidence scores c1, c2, . . . , ck
respectively (the maximum value of k is 22). For each value ai (1 ? i ? k) predicted by
the R1 classifier, we run the R2 classifier where the value of the feature for R1 is ai. That
is, we run the R2 classifier k times for each word w (k depends on w). Then, we check
which value of i (where i runs from 1 to k) gives the best sum of the two classifiers, both
the confidence measure of the R1 classifier on ai and the confidence measure of the R2
classifier. This gives a confidence ranking for R2. We perform the same evaluation for R3,
using the results of the R2 classifier as the value of the R3 added feature. We then select
the root which maximizes the confidence of R3; selecting the root which maximizes all
three classifiers yields similar results, as shown in Table 6.
As the results demonstrate, this is a promising approach which we believe can yield
even better results with more training data. However, as we show subsequently, adding
more linguistic knowledge can improve performance even further.
5.3 Learning Bigrams
In the first experiments of this research, we trained a classifier to learn roots as a single
unit. As already mentioned, the drawbacks of this approach are the large set of targets
and the sparseness of the training data. Then, we decoupled the problem into three
different classifiers to learn each of the root consonants in isolation and then combined
the results in various ways. Training the classifiers in the sequential model, considering
as targets only letters that occurred in the observed word, plus w, i, n, and l, reduced
the number of targets from 22 to approximately 7. This facilitates a different experiment
whereby bigrams of the root radicals, rather than each radical in isolation, are learned,
taking advantage of the reduced number of targets for each radical. On one hand, the
average number of targets is still relatively large (about 50), but on the other, we only
have to deal with a combination of two classifiers. In this method, each of the classifiers
should predict two letters at once. For example, we define one classifier to learn the
first and second radicals (R1R2), and a second classifier to learn the second and third
radicals (R2R3). Alternatively, the first and third radicals can be learned as a single unit
by a different classifier. In this case, we only need to combine this classifier with one of
the above mentioned classifiers to obtain the complete root.
It should be noted that the number of potential roots for a given word example
in combining three different classifiers (for each of the root radicals) is determined by
multiplying the number of targets of each of the classifiers. In this classification problem,
each classifier predicts two root radicals, meaning that the classifiers overlap in one
radical. This common radical should be identical in the combination (e.g., R1R2 and
Table 6
Results: Combining dependent classifiers.
Maximizing all radicals Maximizing R3 Baseline (Table 4)
Precision 76.99 76.87 59.83
Recall 84.78 84.78 57.98
F-score 80.70 80.63 58.89
438
Daya, Roth, and Wintner Identifying Semitic Roots
Table 7
Results: Combining classifiers of root radicals bigram.
R1R2&R2R3 R1R2&R1R3 R2R3&R1R3
Precision 82.11 79.71 79.11
Recall 85.28 86.40 86.64
F-score 83.67 82.92 82.71
R2R3 overlap in R2), and thus the number of potential roots is significantly reduced. The
results of these experiments are depicted in Table 7.
5.4 Combining Classifiers using Linguistic Knowledge
SNoW provides a ranking on all possible roots. We now describe the use of linguis-
tic constraints to re-rank this list. We implemented a function, dubbed the scoring
function, which uses knowledge pertaining to word-formation processes in Hebrew
in order to estimate the likelihood of a given candidate being the root of a given word.
The function practically classifies the candidate roots into one of three classes: good
candidates, which are likely to be the root of the word; bad candidates, which are highly
unlikely; and average cases.
It is important to note that the scoring function alone is not a function for extracting
roots from Hebrew words. First, it only scores a given root candidate against a given
word, rather than yield a root given a word. Although we could have used it exhaus-
tively on all possible roots in this case, in a general setting of a number of classifiers
the number of classes might be too high for this solution to be practical. Second, the
function only produces three different values; when given a number of candidate roots
it may return more than one root with the highest score. In the extreme case, when
called with all 223 potential roots, it returns on average more than 11 candidates which
score highest (and hence are ranked equally). The linguistic knowledge employed by the
system, although significant to the improved performance, is far from being sufficient
for devising a deterministic root extraction algorithm.
We now discuss the constraints employed by the scoring function in detail. In what
follows, a root r = r1r2r3 is said to be in Paradigm P1 if r1 ? {w, i,n}; in Paradigm P2 if
r2 ? {w, i}; in Paradigm P3 if r3 ? {h, i}; in Paradigm P4 if r2 = r3; and regular if none
of these holds. The constraints are deterministic, in the sense that they always hold in
the training data. They can be easily evaluated because determining the paradigm(s) a
given root belongs to is deterministic and efficient. In the examples, root consonants are
typeset in boldface.
1. If r is regular then r1, r2, r3 must occur in the word in this order.
Furthermore, either r1r2r3 are consecutive in the target word, or a single
letter intervenes between r1 and r2 or between r2 and r3 (or both). The
intervening letter between r1 and r2 can only be w, i, t (if r1 is $ or s),
d (if r1 is z), or v (if r1 is c). The intervening letter between r2 and r3
can only be w or i. For example, hgrywm hmsxri gdl bkt$yh ?xwzim;
hhstdrwt hzdrzh lhctlm wlhcvdq.
439
Computational Linguistics Volume 34, Number 3
2. If r is in Paradigm P1 and not in Paradigm P2, P3, or P4, then r2, r3 must
occur in the word in this order. Furthermore, either r2r3 are consecutive
in the target word, or a single letter intervenes between r2 and r3. The
intervening letter can only be w or i. Examples: l$bt (i.$.b); hwdiyh (i.d.y);
mpilim (n.p.l).
3. If r is in Paradigm P2 and not in Paradigm P1 or P3, then r1, r3 must
occur in the word in this order. Furthermore, either r1r3 are consecutive
in the target word, or a single letter intervenes between r1 and r3. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1 is
c). Examples: hqmt (q.w.m) hmlwn hcviirh (c.i.r) kmbi?h (b.w.?) rwwxim.
4. If r is in Paradigm P3 and not in Paradigm P1 or P2, then r1, r2 must
occur in the word in this order. Furthermore, either r1r2 are consecutive
in the target word, or a single letter intervenes between r1 and r2. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1
is c). Examples: tgliwt (g.l.i); hzdkw (z.k.i).
5. If r is in Paradigm P4 and not in Paradigm P1 or P2, then r1, r2 must
occur in the word in this order. Furthermore, either r1r2 are consecutive
in the target word, or a single letter intervenes between r1 and r2. The
intervening letter can only be w, i, t (if r1 is $ or s), d (if r1 is z), or v (if r1
is c). Examples: mgilh (g.l.l); hmginim (g.n.n).
6. r must occur in the pre-compiled list of roots.
The decision of the function is based on the observation that when a root is regular
it either occurs in a word consecutively or with a certain single letter between any two
of its radicals (constraint 1). The scoring function checks, given a root and a word,
whether this is the case. If the condition holds, the scoring function returns a high
value. The weak paradigm constraints (2?5) are assigned a middle score, because in
such paradigms we are limited to a partial check on the root as we only check for
the occurrence of two root radicals in the word. For roots that are in more than one
paradigm, the scoring function returns an average score as a default value. We also
make use in this function of our pre-compiled list of roots. A root candidate which does
not occur in the list (constraint 6) is assigned the low score.
The actual values that the function returns were chosen empirically by counting the
number of occurrences of each class in the training data. Thus, ?good? candidates make
up 74.26% of the data, hence the value the function returns for ?good? roots is set to
0.7426. Similarly, the middle value is set to 0.2416 and the low to 0.0155.
As an example, consider ki$lwn, whose only possible root is k.$.l. Here, the correct
candidate will be assigned the high score, because k.$.l is a regular root and its radicals
occur consecutively in the word with a single intervening letter i between k and $
(constraint 1). The candidate root $.l.i will be assigned a middle score, because this root
is in paradigm P3 and constraint 4 holds. The candidate root $.l.n will score low, as it
does not occur in the list of roots (constraint 6).
In addition to the scoring function we implemented a simple edit distance function
which returns, for a given root and a given word, the inverse of the edit distance
between the two. Edit distance (Levenshtein 1965) is the minimum number of character
insertions and deletions required to transform one string to another. For example, for
hipilw, the (correct) root n.p.l scores 1/5 whereas h.p.l scores 1/3. The inverse edit
440
Daya, Roth, and Wintner Identifying Semitic Roots
distance increases with the similarity between two strings; finer tuning of this similarity
measure is of course possible, but was not the focus of this work.
We then run SNoW on the test data and rank the results of the three classifiers glob-
ally, where the order is determined by the product of the three different classifiers. This
induces an order on roots, which are combinations of the decisions of three independent
classifiers. Each candidate root is assigned three scores: the product of the confidence
measures of the three classifiers; the result of the scoring function; and the inverse
edit distance between the candidate and the observed word. We rank the candidates
according to the product of the three scores (i.e., we give each score an equal weight in
the final ranking).
Recall that a given word formmay have several possible roots; our system therefore
has to determine how many roots to produce for each example. We observed that in the
?difficult? examples, the top ranking candidates are assigned close scores, whereas in
the easier cases, the top candidate is usually scored much higher than the next one.
We therefore decided to produce all those candidates whose scores are not much lower
than the score of the top ranking candidate. The drop in the score, ?, was determined
empirically on the training set and was set to ? = 0.4. With this value for ?, results for
the test data are presented in Table 8.
The results clearly demonstrate the added benefit of the linguistic knowledge.
Interestingly, even when testing the system on a set of roots which do not occur in the
training corpus, we obtain an F-score of 65.60%. This result demonstrates the robustness
of our method.
The additional linguistic knowledge is not merely eliminating illegitimate roots
from the ranking produced by SNoW. Using the linguistic constraints encoded in the
scoring function only to eliminate roots, while maintaining the ranking proposed by
SNoW, yields much lower accuracy. Specifically, when we use only the list of roots
as the single constraint when combining the three classifiers, thereby implementing
only a filter of infeasible results, we obtain a precision of 65.24%, recall of 73.87%,
and an F-score of 69.29%. Clearly, our linguistically motivated scoring does more than
elimination, and actually re-ranks the roots. We conclude that it is only the combination
of the classifiers with the linguistically motivated scoring function which boosts the
performance on this task.
5.5 Error Analysis
Looking at the questionnaires filled in by our subjects (Section 3.2), it is obvious that
humans have problems identifying the correct roots in two general cases: when the
root paradigm is weak (i.e., when the root is irregular) and when the word can be
read in more than one way and the subject chooses only one (presumably, the most
prominent one). Our system suffers from similar problems: First, its performance on
Table 8
Results: Using linguistic constraints for inference.
System Baseline
Precision 80.90 59.83
Recall 88.16 57.98
F-score 84.38 58.89
441
Computational Linguistics Volume 34, Number 3
the regular paradigms is far superior to its overall performance; second, it sometimes
cannot distinguish between several roots which are in principle possible, but only one
of which happens to be the correct one.
To demonstrate the first point, we evaluated the performance of the system on a
different organization of the data. We tested separately words whose roots are all reg-
ular, versus words all of whose roots are irregular. We also tested words which have at
least one regular root (this group is titled ?mixed? herein). As an additional experiment,
we extracted from the corpus a sample of 200 ?hard? words: these are surface forms in
which either one of the root characters is missing, or two root characters are transposed
due to metathesis. The results are presented in Table 9, and clearly demonstrate the
difficulty of the system on the weak paradigms, compared to almost 95% on the easier,
regular roots.
A more refined analysis reveals differences between the various weak paradigms.
Table 10 lists F-score for words whose roots are irregular, classified by paradigm. As
can be seen, the system has great difficulty in the cases of R2 = R3 and R3 = i. Refer to
Table 2 for the sizes of the different root classes.
Finally, we took a closer look at some of the errors, and in particular at cases where
the system produces several roots where fewer (usually only one) are correct. Such cases
include, for example, the word hkwtrt (?the title?), whose root is the regular k.t.r; but
the system produces, in addition, also w.t.r, mistaking the k to be a prefix. These are the
kinds of errors which are most difficult to fix.
However, in many cases the system?s errors are relatively easy to overcome. Con-
sider, for example, the word hmtndbim (?the volunteers?) whose root is the irregular
n.d.b. Our system produces as many as five possible roots for this word: n.d.b, i.t.d,
d.w.b, i.h.d, and i.d.d. Clearly some of these could be eliminated. For example, i.t.d
should not be produced, because if this were the root, nothing could explain the pres-
ence of the b in the word; i.h.d should be excluded because of the location of the h.
Similar phenomena abound in the errors the system makes; they indicate that a more
Table 9
Error analysis: Performance of the system on different cases.
Regular Irregular Mixed Hard
Number of words 2,598 2,019 2,781 200
Precision 92.79 60.02 92.54 47.87
Recall 96.92 73.45 94.28 55.11
F-score 94.81 66.06 93.40 51.23
Table 10
Error analysis: The weak paradigms.
Paradigm F-score
R1 = i 70.57
R1 = n 71.97
R2 = i/w 76.33
R3 = i 58.00
R2 = R3 47.42
442
Daya, Roth, and Wintner Identifying Semitic Roots
Table 11
Arabic root ambiguity in the corpus.
Number of roots 1 2 3 4 5 6
Number of words 28,741 2,258 664 277 48 3
careful design of the scoring function can yield still better results, and this is a direction
we intend to pursue in the future.
6. Extension to Arabic
Although Arabic and Hebrew have a very similar morphological system, being both
Semitic languages, the task of learning roots in Arabic is more difficult than in Hebrew,
for the following reasons.
 There are 28 letters in Arabic which are represented using
approximately 40 characters in the transliteration of Modern Standard
Arabic orthography of Buckwalter (2002). Thus, the learning problem is
more complicated due to the increased number of targets (potential root
radicals) as well as the number of characters available in a word.
 The number of roots in Arabic is significantly higher. We pre-compiled a
list of 3,822 trilateral roots from Buckwalter?s list of roots, 2,517 of which
occur in our corpus. According to our lists, Arabic has almost twice as
many roots as Hebrew.
 Not only is the number of roots high, the number of patterns in Arabic is
also much higher than in Hebrew.
 Whereas in Hebrew the only possible letters which can intervene between
root radicals in a word are i and w, in Arabic there are more possibilities.
The possible intervening letter sequences between r1 and r2 are y, w, A, t,
and wA, and between r2 and r3 y, w, A, and A}.7
We applied the same methods discussed previously to the problem of learning
(Modern Standard) Arabic roots. For training and testing, we produced a corpus
of 31,991 word types (we used the morphological analyzer of Buckwalter 2002 to ana-
lyze a corpus of 152,666 word tokens from which our annotated corpus was produced).
Table 11 shows the distribution of word types according to root ambiguity.
We then trained stand-alone classifiers to identify each radical of the root in iso-
lation, using features of the same categories as for Hebrew: location of letters, letter
bigrams (independently of their location), and prefixes and suffixes compiled manu-
ally from a morphological grammar (Buckwalter 2002). Despite the rather pessimistic
starting point, each classifier provides satisfying results, as shown in Table 12, probably
owing to the significantly larger training corpus. The first three columns present the
results of each of the three classifiers, and the fourth column is a straightforward
combination of the three classifiers.
7 ?}? is a character in Buckwalter?s transliteration.
443
Computational Linguistics Volume 34, Number 3
Table 12
Accuracy of identifying the correct radical in Arabic.
R1 R2 R3 root
Precision 86.02 70.71 82.95 54.08
Recall 89.84 80.29 88.99 68.10
F-score 87.89 75.20 85.86 60.29
We combined the classifiers using linguistic knowledge pertaining to word-
formation processes in Arabic, by implementing a function that approximates the like-
lihood of a given candidate to be the root of a given word. The function actually checks
the following cases:
 If a root candidate is indeed the root of a given word, then we expect
it to occur in the word consecutively or with one of {y, w, A, t, wA}
intervening between R1 and R2, or with one of { y, w, A, A} } between
R2 and R3 (or both).
 If a root candidate does not occur in our pre-compiled list of roots, it
cannot be a root of any word in the corpus.
We suppressed the constraints of weak paradigms in the Arabic experiments, be-
cause in such paradigms we are limited to a partial check on the root as we only check
for the occurrence of two root radicals instead of three in the word. This limitation seems
to be crucial in Arabic, considering the fact that the number of roots is much higher
and, in addition, there are more possible intervening letter sequences between the root
radicals. Consequently, more incorrect roots are wrongly extracted as correct ones. Of
course, this is an over-simplistic account of the linguistic facts, but it serves our purpose
of using very limited and very shallow linguistic constraints on the combination of
specialized ?expert? classifiers. Table 13 shows the final results.
The Arabic results are slightly worse than the Hebrew ones. One reason is that in
Hebrew the number of roots is smaller than in Arabic (2,152 vs. 3,822), which leaves
much room for wrong root selection. Another reason might be the fact that in Arabic
word formation is amore complicated process, for example by allowingmore characters
to occur in the word between the root letters as previously mentioned. This may have
caused the scoring function to wrongly tag some root candidates as possible roots.
7. Improving Local Classifiers by Applying Global Constraints
In Section 5 we presented several methods addressing the problem of learning roots. In
general, we trained stand-alone classifiers, each predicting a different root component,
Table 13
Results: Arabic root identification.
Precision 78.21
Recall 82.80
F-score 80.44
444
Daya, Roth, and Wintner Identifying Semitic Roots
in which the decision for the complete root depends on the outcomes of these different
but mutually dependent classifiers. The classifiers? outcomes need to respect some
constraints that arise from the dependency between the root radicals, requiring a level
of inference on top of the predictions, which is implemented by the scoring function
(Section 5.4).
In this section we show that applying global constraints, in the form of the scor-
ing function, not only improves global decisions but also significantly improves the
local classification task. Specifically, we show that the performance of identifying each
radical in isolation improves after the scoring function is applied. In this experiment
we trained each of the three radical classifiers as previously described, and then
applied inference to re-rank the results. The combined classifier now predicts the
complete root, and in particular, induces a new local classifier decision on each of
the radicals which, due to re-ranking, may differ from the original prediction of the
local classifiers.
Table 14 shows the results of each of the radical classifiers after inference with the
scoring function. There is a significant improvement in each of the three classifiers after
applying the global constraints (Table 14; cf. Table 4). The most remarkable improve-
ment is of the R2 classifier. The gap between R2 and other classifiers, as stand-alone
classifiers with no external knowledge, is 10?12%, due to linguistic reasons. Now, after
employing the global constraints, the gap is reduced to only 4%. In such scenarios,
global constraints can significantly aid local classification.
Because the most dominant constraint is the occurrence of the candidate root in
the pre-compiled list of roots, we examined the results of applying only this constraint
on each of the three classifiers, as a single global constraint. Although there is an
improvement in all classifiers, as shown in Table 15, applying this single constraint still
performs worse than applying all the constraints mentioned in Section 5.4. Again, we
conclude that re-ranking the candidates produced by the local classifiers is essential for
improving the accuracy, and filtering out infeasible results is not sufficient.
Finally, to further emphasize the contribution of global inference to local classifi-
cation, we repeated the same experiment, measuring accuracy of each of the radical
classifiers induced by the root identification system, for Arabic. The results are listed
Table 14
Accuracy of each classifier after applying global constraints.
R1 R2 R3
Precision 89.67 84.7 89.27
Recall 93.08 90.17 93.16
F-score 91.34 87.35 91.17
Table 15
Accuracy of each classifier applying the list of roots as a single constraint.
R1 R2 R3
Precision 86.33 78.58 83.63
Recall 88.59 83.75 88.82
F-score 87.45 81.08 86.15
445
Computational Linguistics Volume 34, Number 3
Table 16
Accuracy of each classifier after applying global constraints (Arabic).
R1 R2 R3
Precision 90.41 84.40 87.92
Recall 92.90 89.59 92.19
F-score 91.64 86.92 90.01
in Table 16, and show a significant improvement over the basic classifiers (compare to
Table 12).
8. Conclusions
We have shown that combining machine learning with limited linguistic knowledge
can produce state-of-the-art results on a difficult morphological task, the identification
of roots of Semitic words. Our best result, over 80% accuracy, was obtained using simple
classifiers for each of the root?s consonants, and then combining the outputs of the
classifiers using a linguistically motivated, yet extremely coarse and simplistic, scoring
function.
This work can be improved in a variety of ways. As is well known from other learn-
ing tasks, fine-tuning of the feature set can produce additional accuracy; we expect this
to be the case in this task, too. In particular, introducing features that capture contextual
information is likely to improve the results. Similarly, our scoring function is simplistic
andwe believe that it can be improved. The edit-distance function can be improved such
that the cost of replacing characters reflect phonological and orthographic constraints
(Kruskal 1999). Other, learning-based, re-ranking methods can also be used to improve
the results.
There are various other ways in which different inter-related classifiers can be
combined. Here we only used a simple multiplication of the three classifiers? confidence
measures, which is then combined with the linguistically motivated functions. We
intend to investigate more sophisticated methods for this combination.
Finally, we plan to extend these results to more complex cases of learning tasks with
a large number of targets, in particular such tasks in which the targets are structured.We
are currently working on morphological disambiguation in languages with non-trivial
morphology, which can be viewed as a part-of-speech tagging problem with a large
number of tags on which structure can be imposed using the various morphological
and morpho-syntactic features that morphological analyzers produce.
Acknowledgments
Previous versions of this work were
published as Daya, Roth, and Winter (2004,
2007). This work was supported by The
Caesarea Edmond Benjamin de Rothschild
Foundation Institute for Interdisciplinary
Applications of Computer Science at the
University of Haifa and the Israeli Ministry
of Science and Technology, under the
auspices of the Knowledge Center for
Processing Hebrew. Dan Roth is supported
by NSF grants CAREER IIS-9984168, ITR
IIS-0085836, and ITR-IIS 00-85980. We
are grateful to Ido Dagan, Alon Lavie,
and Idan Szpektor for useful comments.
We benefitted greatly from useful and
instructive comments by three reviewers.
References
Abu-Salem, Hani, Mahmoud Al-Omari,
and Martha W. Evens. 1999. Stemming
methodologies over individual query
words for an Arabic information retrieval
446
Daya, Roth, and Wintner Identifying Semitic Roots
system. Journal of the American Society for
Information Science, 50(6):524?529.
Al-Kharashi, Ibrahim A. and Martha W.
Evens. 1994. Comparing words, stems,
and roots as index terms in an Arabic
information retrieval system. Journal of the
American Society for Information Science,
45(8):548?560.
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, pages 26?33,
Morristown, NJ.
Beesley, Kenneth R. 1998a. Arabic
morphological analysis on the internet.
In Proceedings of the 6th International
Conference and Exhibition on Multi-lingual
Computing, Cambridge, UK.
Beesley, Kenneth R. 1998b. Arabic
morphology using only finite-state
operations. Proceedings of the Workshop
on Computational Approaches to Semitic
Languages, pages 50?57, Montreal, Quebec.
Buckwalter, Tim. 2002. Buckwalter Arabic
morphological analyzer. Linguistic Data
Consortium (LDC) catalog number
LDC2002L49 and ISBN 1-58563-257-0.
Carlson, Andrew J., Chad M. Cumby, Jeff L.
Rosen, and Dan Roth. 1999. The SNoW
learning architecture. Technical Report
UIUCDCS-R-99-2101, UIUC Computer
Science Department.
Choueka, Yaacov. 1990. MLIM?A system for
full, exact, on-line grammatical analysis
of Modern Hebrew. In Proceedings of the
Annual Conference on Computers in
Education, page 63, Tel Aviv. [In Hebrew.]
Darwish, Kareem. 2002. Building a shallow
Arabic morphological analyzer in one
day. In Computational Approaches to
Semitic Languages, an ACL?02 Workshop,
pages 47?54, Philadelphia, PA.
Darwish, Kareem and Douglas W. Oard.
2002. Term selection for searching printed
Arabic. In SIGIR ?02: Proceedings of the 25th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 261?268, New York, NY.
Daya, Ezra, Dan Roth, and Shuly Wintner.
2004. Learning Hebrew roots: Machine
learning with linguistic constraints. In
Proceedings of EMNLP?04, pages 357?364,
Barcelona, Spain.
Daya, Ezra, Dan Roth, and Shuly Wintner.
2007. Learning to identify Semitic roots.
In Abdelhadi Soudi, Guenter Neumann,
and Antal van den Bosch, editors, Arabic
Computational Morphology: Knowledge-based
and Empirical Methods, volume 38 of Text,
Speech and Language Technology. Springer,
New York, pages 143?158.
Even-Shoshan, Abraham. 1993. HaMillon
HaXadash (The New Dictionary). Kiryat
Sefer, Jerusalem. In Hebrew.
Even-Zohar, Yair and Dan Roth. 2001.
A sequential model for multi class
classification. In EMNLP-2001, the SIGDAT
Conference on Empirical Methods in Natural
Language Processing, pages 10?19,
Pittsburgh, PA.
Florian, Radu. 2002. Named entity
recognition as a house of cards: Classifier
stacking. In Proceedings of CoNLL-2002,
pages 175?178, Taiwan.
Golding, Andrew R. and Dan Roth.
1999. A Winnow based approach to
context-sensitive spelling correction.
Machine Learning, 34(1?3):107?130.
Habash, Nizar and Owen Rambow.
2005. Arabic tokenization, part-of-
speech tagging and morphological
disambiguation in one fell swoop.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL?05), pages 573?580,
Ann Arbor, MI.
Kruskal, Joseph. 1999. An overview of
sequence comparison. In David Sankoff
and Joseph Kruskal, editors, Time Warps,
String Edits and Macromolecules: The Theory
and Practice of Sequence Comparison. CSLI
Publications, Stanford, CA, pages 1?44.
Larkey, Leah S., Lisa Ballesteros, and
Margaret E. Connell. 2002. Improving
stemming for Arabic information retrieval:
Light stemming and co-occurrence
analysis. In SIGIR ?02: Proceedings of the
25th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval, pages 275?282,
New York, NY.
Levenshtein, Vladimir I. 1965. Binary codes
capable of correcting deletions, insertions
and reversals. Doklady Akademii Nauk
SSSR, 163(4):845?848.
McCarthy, John J. 1981. A prosodic theory of
nonconcatenative morphology. Linguistic
Inquiry, 12(3):373?418.
Ornan, Uzzi. 2003. The Final Word. University
of Haifa Press, Haifa, Israel. [In Hebrew.]
Owens, Jonathan. 1997. The Arabic
grammatical tradition. In Robert Hetzron,
editor, The Semitic Languages. Routledge,
London and New York, chapter 3,
pages 46?58.
Punyakanok, Vasin and Dan Roth. 2001. The
use of classifiers in sequential inference. In
447
Computational Linguistics Volume 34, Number 3
NIPS-13; The 2000 Conference on Advances in
Neural Information Processing Systems 13,
pages 995?1001, Denver, CO.
Punyakanok, Vasin, Dan Roth, and Wen-Tau
Yih. 2005. The necessity of syntactic
parsing for semantic role labeling. In
Proceedings of IJCAI 2005, pages 1117?1123,
Edinburgh.
Roth, Dan. 1998. Learning to resolve natural
language ambiguities: A unified approach.
In Proceedings of AAAI-98 and IAAI-98,
pages 806?813, Madison, WI.
Schu?tze, Hinrich and Yoram Singer. 1994.
Part-of-speech tagging using a variable
memory Markov model. In Proceedings
of the 32nd Annual Meeting of the
Association for Computational Linguistics,
pages 181?187, Las Cruses, NM.
Shimron, Joseph, editor. 2003. Language
Processing and Acquisition in Languages of
Semitic, Root-Based, Morphology. Number 28
in Language Acquisition and Language
Disorders. John Benjamins, Amsterdam.
Tjong Kim Sang, Erik F. and Fien
De Meulder. 2003. Introduction
to the CoNLL-2003 shared task:
Language-independent named entity
recognition. In Proceedings of CoNLL-2003,
pages 142?147, Edmonton, Canada.
Yona, Shlomo and Shuly Wintner. 2008. A
finite-state morphological grammar of
Hebrew. Natural Language Engineering,
14(2):173?190.
Zdaqa, Yizxaq. 1974. Luxot HaPoal [The
Verb Tables]. Kiryath Sepher, Jerusalem.
[In Hebrew.]
448
Learning Hebrew Roots: Machine Learning with Linguistic Constraints
Ezra Daya
Dept. of Computer Science
University of Haifa
31905 Haifa
Israel
edaya@cs.haifa.ac.il
Dan Roth
Dept. of Computer Science
University of Illinois
Urbana, IL 61801
USA
danr@cs.uiuc.edu
Shuly Wintner
Dept. of Computer Science
University of Haifa
31905 Haifa
Israel
shuly@cs.haifa.ac.il
Abstract
The morphology of Semitic languages is unique in
the sense that the major word-formation mechanism
is an inherently non-concatenative process of inter-
digitation, whereby two morphemes, a root and a
pattern, are interwoven. Identifying the root of a
given word in a Semitic language is an important
task, in some cases a crucial part of morphological
analysis. It is also a non-trivial task, which many
humans find challenging. We present a machine
learning approach to the problem of extracting roots
of Hebrew words. Given the large number of po-
tential roots (thousands), we address the problem as
one of combining several classifiers, each predict-
ing the value of one of the root?s consonants. We
show that when these predictors are combined by
enforcing some fairly simple linguistics constraints,
high accuracy, which compares favorably with hu-
man performance on this task, can be achieved.
1 Introduction
The standard account of word-formation processes
in Semitic languages describes words as combina-
tions of two morphemes: a root and a pattern.1 The
root consists of consonants only, by default three
(although longer roots are known), called radicals.
The pattern is a combination of vowels and, possi-
bly, consonants too, with ?slots? into which the root
consonants can be inserted. Words are created by
interdigitating roots into patterns: the first radical is
inserted into the first consonantal slot of the pattern,
the second radical fills the second slot and the third
fills the last slot. See Shimron (2003) for a survey.
Identifying the root of a given word is an im-
portant task. Although existing morphological an-
alyzers for Hebrew only provide a lexeme (which
is a combination of a root and a pattern), for other
Semitic languages, notably Arabic, the root is an
essential part of any morphological analysis sim-
1An additional morpheme, vocalization, is used to abstract
the pattern further; for the present purposes, this distinction is
irrelevant.
ply because traditional dictionaries are organized by
root, rather than by lexeme. Furthermore, roots are
known to carry some meaning, albeit vague. We be-
lieve that this information can be useful for compu-
tational applications and are currently experiment-
ing with the benefits of using root and pattern infor-
mation for automating the construction of a Word-
Net for Hebrew.
We present a machine learning approach, aug-
mented by limited linguistic knowledge, to the prob-
lem of identifying the roots of Hebrew words. To
the best of our knowledge, this is the first appli-
cation of machine learning to this problem. While
there exist programs which can extract the root of
words in Arabic (Beesley, 1998a; Beesley, 1998b)
and Hebrew (Choueka, 1990), they are all depen-
dent on labor intensive construction of large-scale
lexicons which are components of full-scale mor-
phological analyzers. Note that Tim Bockwalter?s
Arabic morphological analyzer2 only uses ?word
stems ? rather than root and pattern morphemes ? to
identify lexical items. (The information on root and
pattern morphemes could be added to each stem en-
try if this were desired.)? The challenge of our work
is to automate this process, avoiding the bottleneck
of having to laboriously list the root and pattern of
each lexeme in the language, and thereby gain in-
sights that can be used for more detailed morpho-
logical analysis of Semitic languages.
As we show in section 2, identifying roots is a
non-trivial problem even for humans, due to the
complex nature of Hebrew derivational and inflec-
tional morphology and the peculiarities of the He-
brew orthography. From a machine learning per-
spective, this is an interesting test case of interac-
tions among different yet interdependent classifiers.
After presenting the data in section 3, we discuss a
simple, baseline, learning approach (section 4) and
then propose two methods for combining the results
of interdependent classifiers (section 5), one which
is purely statistical and one which incorporates lin-
2http://www.qamus.org/morphology.htm
guistic constraints, demonstrating the improvement
of the hybrid approach. We conclude with sugges-
tions for future research.
2 Linguistic background
In this section we refer to Hebrew only, although
much of the description is valid for other Semitic
languages as well. As an example of root-and-
pattern morphology, consider the Hebrew roots
g.d.l, k.t.b and r.$.m and the patterns haCCaCa,
hitCaCCut and miCCaC, where the ?C?s indicate
the slots. When the roots combine with these pat-
terns the resulting lexemes are hagdala, hitgadlut,
migdal, haktaba, hitkatbut, miktab, har$ama, hi-
tra$mut, mir$am, respectively. After the root com-
bines with the pattern, some morpho-phonological
alternations take place, which may be non-trivial:
for example, the hitCaCCut pattern triggers assimi-
lation when the first consonant of the root is t or d :
thus, d.r.$+hitCaCCut yields hiddar$ut. The same
pattern triggers metathesis when the first radical is
s or $ : s.d.r+hitCaCCut yields histadrut rather than
the expected *hitsadrut. Semi-vowels such as w or
y in the root are frequently combined with the vow-
els of the pattern, so that q.w.m+haCCaCa yields
haqama, etc. Frequently, root consonants such as w
or y are altogether missing from the resulting form.
These matters are complicated further due to two
sources: first, the standard Hebrew orthography
leaves most of the vowels unspecified. It does not
explicate a and e vowels, does not distinguish be-
tween o and u vowels and leaves many of the i
vowels unspecified. Furthermore, the single letter
w is used both for the vowels o and u and for the
consonant v, whereas i is similarly used both for
the vowels i and for the consonant y. On top of
that, the script dictates that many particles, includ-
ing four of the most frequent prepositions, the def-
inite article, the coordinating conjunction and some
subordinating conjunctions all attach to the words
which immediately follow them. Thus, a form such
as mhgr can be read as a lexeme (?immigrant?), as
m-hgr ?from Hagar?or even as m-h-gr ?from the
foreigner?. Note that there is no deterministic way
to tell whether the first m of the form is part of the
pattern, the root or a prefixing particle (the preposi-
tion m ?from?).
The Hebrew script has 22 letters, all of which
can be considered consonants. The number of
tri-consonantal roots is thus theoretically bounded
by 223, although several phonological constraints
limit this number to a much smaller value. For
example, while roots whose second and third radi-
cals are identical abound in Semitic languages, roots
whose first and second radicals are identical are ex-
tremely rare (see McCarthy (1981) for a theoreti-
cal explanation). To estimate the number of roots
in Hebrew we compiled a list of roots from two
sources: a dictionary (Even-Shoshan, 1993) and the
verb paradigm tables of Zdaqa (1974). The union of
these yields a list of 2152 roots.3
While most Hebrew roots are regular, many be-
long to weak paradigms, which means that root con-
sonants undergo changes in some patterns. Exam-
ples include i or n as the first root consonant, w or
i as the second, i as the third and roots whose sec-
ond and third consonants are identical. For example,
consider the pattern hCCCh. Regular roots such as
p.s.q yield forms such as hpsqh. However, the irreg-
ular roots n.p.l, i.c.g, q.w.m and g.n.n in this pattern
yield the seemingly similar forms hplh, hcgh, hqmh
and hgnh, respectively. Note that in the first and sec-
ond examples, the first radical (n or i ) is missing, in
the third the second radical (w) is omitted and in
the last example one of the two identical radicals is
omitted. Consequently, a form such as hC1C2h can
have any of the roots n.C1.C2, C1.w.C2, C1.i.C2,
C1.C2.C2 and even, in some cases, i.C1.C2.
While the Hebrew script is highly ambiguous,
ambiguity is somewhat reduced for the task we con-
sider here, as many of the possible lexemes of a
given form share the same root. Still, in order to cor-
rectly identify the root of a given word, context must
be taken into consideration. For example, the form
$mnh has more than a dozen readings, including
the adjective ?fat? (feminine singular), which has
the root $.m.n, and the verb ?count?, whose root is
m.n.i, preceded by a subordinating conjunction. In
the experiments we describe below we ignore con-
text completely, so our results are handicapped by
design.
3 Data and methodology
We take a machine learning approach to the prob-
lem of determining the root of a given word. For
training and testing, a Hebrew linguist manually
tagged a corpus of 15,000 words (a set of newspa-
per articles). Of these, only 9752 were annotated;
the reason for the gap is that some Hebrew words,
mainly borrowed but also some frequent words such
as prepositions, do not have roots; we further elim-
inated 168 roots with more than three consonants
and were left with 5242 annotated word types, ex-
hibiting 1043 different roots. Table 1 shows the dis-
tribution of word types according to root ambiguity.
3Only tri-consonantal roots are counted. Ornan (2003) men-
tions 3407 roots, whereas the number of roots in Arabic is esti-
mated to be 10,000 (Darwish, 2002).
Number of roots 1 2 3 4
Number of words 4886 335 18 3
Table 1: Root ambiguity in the corpus
Table 2 provides the distribution of the roots of
the 5242 word types in our corpus according to root
type, where Ci is the i-th radical (note that some
roots may belong to more than one group).
Paradigm Number Percentage
C1 = i 414 7.90%
C1 = w 28 0.53%
C1 = n 419 7.99%
C2 = i 297 5.66%
C2 = w 517 9.86%
C3 = h 18 0.19%
C3 = i 677 12.92%
C2 = C3 445 8.49%
Regular 3061 58.41%
Table 2: Distribution of root paradigms
As assurance for statistical reliability, in all the
experiments discussed in the sequel (unless other-
wise mentioned) we performed 10-fold cross valida-
tion runs a for every classification task during evalu-
ation. We also divided the test corpus into two sets:
a development set of 4800 words and a held-out set
of 442 words. Only the development set was used
for parameter tuning. A given example is a word
type with all its (manually tagged) possible roots.
In the experiments we describe below, our system
produces one or more root candidates for each ex-
ample. For each example, we define tp as the num-
ber of candidates correctly produced by the system;
fp as the number of candidates which are not cor-
rect roots; and fn as the number of correct roots the
system did not produce. As usual, we define recall
as
tp
tp+fp and precision as
tp
tp+fn ; we then compute
f -measure for each example (with ? = 0.5) and
(macro-) average to obtain the system?s overall f -
measure.
To estimate the difficulty of this task, we asked
six human subjects to perform it. Subjects were
asked to identify all the possible roots of all the
words in a list of 200 words (without context), ran-
domly chosen from the test corpus. All subjects
were computer science graduates, native Hebrew
speakers with no linguistic background. The aver-
age precision of humans on this task is 83.52%, and
with recall at 80.27%, f -measure is 81.86%. Two
main reasons for the low performance of humans
are the lack of context and the ambiguity of some of
the weak paradigms.
4 A machine learning approach
To establish a baseline, we first performed two ex-
periments with simple, baseline classifiers. In all the
experiments described in this paper we use SNoW
(Roth, 1998) as the learning environment, with win-
now as the update rule (using perceptron yielded
comparable results). SNoW is a multi-class clas-
sifier that is specifically tailored for learning in do-
mains in which the potential number of information
sources (features) taking part in decisions is very
large, of which NLP is a principal example. It works
by learning a sparse network of linear functions
over a pre-defined or incrementally learned feature
space. SNoW has already been used successfully
as the learning vehicle in a large collection of nat-
ural language related tasks, including POS tagging,
shallow parsing, information extraction tasks, etc.,
and compared favorably with other classifiers (Roth,
1998; Punyakanok and Roth, 2001; Florian, 2002).
Typically, SNoW is used as a classifier, and predicts
using a winner-take-all mechanism over the activa-
tion values of the target classes. However, in addi-
tion to the prediction, it provides a reliable confi-
dence level in the prediction, which enables its use
in an inference algorithm that combines predictors
to produce a coherent inference.
4.1 Feature types
All the experiments we describe in this work share
the same features and differ only in the target clas-
sifiers. The features that are used to characterize a
word are both grammatical and statistical:
? Location of letters (e.g., the third letter of the
word is b ). We limit word length to 20, thus
obtaining 440 features of this type (recall the
the size of the alphabet is 22).
? Bigrams of letters, independently of their loca-
tion (e.g., the substring gd occurs in the word).
This yields 484 features.
? Prefixes (e.g., the word is prefixed by k$h
?when the?). We have 292 features of this type,
corresponding to 17 prefixes and sequences
thereof.
? Suffixes (e.g., the word ends with im, a plural
suffix). There are 26 such features.
4.2 Direct prediction
In the first of the two experiments, referred to as
Experiment A, we trained a classifier to learn roots
as a single unit. The two obvious drawbacks of
this approach are the large set of targets and the
sparseness of the training data. Of course, defin-
ing a multi-class classification task with 2152 tar-
gets, when only half of them are manifested in the
training corpus, does not leave much hope for ever
learning to identify the missing targets.
In Experiment A, the macro-average precision of
ten-fold cross validation runs of this classification
problem is 45.72%; recall is 44.37%, yielding an
f -score of 45.03%. In order to demonstrate the in-
adequacy of this method, we repeated the same ex-
periment with a different organization of the train-
ing data. We chose 30 roots and collected all their
occurrences in the corpus into a test file. We then
trained the classifier on the remainder of the corpus
and tested on the test file. As expected, the accuracy
was close to 0%,
4.3 Decoupling the problem
In the second experiment, referred to as Experi-
ment B, we separated the problem into three dif-
ferent tasks. We trained three classifiers to learn
each of the root consonants in isolation and then
combined the results in the straight-forward way
(a conjunction of the decisions of the three classi-
fiers). This is still a multi-class classification but
the number of targets in every classification task is
only 22 (the number of letters in the Hebrew al-
phabet) and data sparseness is no longer a problem.
As we show below, each classifier achieves much
better generalization, but the clear limitation of this
method is that it completely ignores interdependen-
cies between different targets: the decision on the
first radical is completely independent of the deci-
sion on the second and the third.
We observed a difference between recognizing
the first and third radicals and recognizing the sec-
ond one, as can be seen in table 3. These results cor-
respond well to our linguistic intuitions: the most
difficult cases for humans are those in which the
second radical is w or i, and those where the second
and the third consonants are identical. Combining
the three classifiers using logical conjunction yields
an f -measure of 52.84%. Here, repeating the same
experiment with the organization of the corpus such
that testing is done on unseen roots yielded 18.1%
accuracy.
To demonstrate the difficulty of the problem, we
conducted yet another experiment. Here, we trained
the system as above but we tested it on different
words whose roots were known to be in the training
set. The results of experiment A here were 46.35%,
whereas experiment B was accurate in 57.66% of
C1 C2 C3 root
Precision: 82.25 72.29 81.85 53.60
Recall: 80.13 70.00 80.51 52.09
f -measure: 81.17 71.13 81.18 52.84
Table 3: Accuracy of SNoW?s identifying the cor-
rect radical
the cases. Evidently, even when testing only on
previously seen roots, both na??ve methods are un-
successful (although method A here outperforms
method B).
5 Combining interdependent classifiers
Evidently, simple combination of the results of the
three classifiers leaves much room for improve-
ment. Therefore we explore other ways for com-
bining these results. We can rely on the fact that
SNoW provides insight into the decisions of the
classifiers ? it lists not only the selected target, but
rather all candidates, with an associated confidence
measure. Apparently, the correct radical is chosen
among SNoW?s top-n candidates with high accu-
racy, as the data in table 3 reveal.
This observation calls for a different way of com-
bining the results of the classifiers which takes into
account not only the first candidate but also others,
along with their confidence scores.
5.1 HMM combination
We considered several ways, e.g., via HMMs, of ap-
pealing to the sequential nature of the task (C1 fol-
lowed by C2, followed by C3). Not surprisingly, di-
rect applications of HMMs are too weak to provide
satisfactory results, as suggested by the following
discussion. The approach we eventually opted for
combines the predictive power of a classifier to es-
timate more accurate state probabilities.
Given the sequential nature of the data and the
fact that our classifier returns a distribution over
the possible outcomes for each radical, a natural
approach is to combine SNoW?s outcomes via a
Markovian approach. Variations of this approach
are used in the context of several NLP problems,
including POS tagging (Schu?tze and Singer, 1994),
shallow parsing (Punyakanok and Roth, 2001) and
named entity recognition (Tjong Kim Sang and
De Meulder, 2003).
Formally, we assume that the confidence supplied
by the classifier is the probability of a state (radical,
c) given the observation o (the word), P (c|o). This
information can be used in the HMM framework by
applying Bayes rule to compute
P (o|c) = P (c|o)P (o)P (c) ,
where P (o) and P (c) are the probabilities of ob-
serving o and being at c, respectively. That is,
instead of estimating the observation probability
P (o|c) directly from training data, we compute
it from the classifiers? output. Omitting details
(see Punyakanok and Roth (2001)), we can now
combine the predictions of the classifiers by finding
the most likely root for a given observation, as
r = argmaxP (c1c2c3|o, ?)
where ? is a Markov model that, in this case, can
be easily learned from the supervised data. Clearly,
given the short root and the relatively small number
of values of ci that are supported by the outcomes
of SNoW, there is no need to use dynamic program-
ming here and a direct computation is possible.
However, perhaps not surprisingly given the dif-
ficulty of the problem, this model turns out to be too
simplistic. In fact, performance deteriorated. We
conjecture that the static probabilities (the model)
are too biased and cause the system to abandon good
choices obtained from SNoW in favor of worse can-
didates whose global behavior is better.
For example, the root &.b.d was correctly gen-
erated by SNoW as the best candidate for the word
&obdim, but since P (C3 = b|C2 = b), which is
0.1, is higher than P (C3 = d|C2 = b), which is
0.04, the root &.b.b was produced instead. Note that
in the above example the root &.b.b cannot possibly
be the correct root of &obdim since no pattern in
Hebrew contains the letter d, which must therefore
be part of the root. It is this kind of observations that
motivate the addition of linguistic knowledge as a
vehicle for combining the results of the classifiers.
An alternative approach, which we intend to investi-
gate in the future, is the introduction of higher-level
classifiers which take into account interactions be-
tween the radicals (Punyakanok and Roth, 2001).
5.2 Adding linguistic constraints
The experiments discussed in section 4 are com-
pletely devoid of linguistic knowledge. In partic-
ular, experiment B inherently assumes that any se-
quence of three consonants can be the root of a
given word. This is obviously not the case: with
very few exceptions, all radicals must be present in
any inflected form (in fact, only w, i, n and in an ex-
ceptional case l can be deleted when roots combine
with patterns). We therefore trained the classifiers
to consider as targets only letters that occurred in
the observed word, plus w, i, n and l, rather than
any of the alphabet letters. The average number of
targets is now 7.2 for the first radical, 5.7 for the
second and 5.2 for the third (compared to 22 each in
the previous setup).
In this model, known as the sequential model
(Even-Zohar and Roth, 2001), SNoW?s perfor-
mance improved slightly, as can be seen in table 4
(compare to table 3). Combining the results in
the straight-forward way yields an f -measure of
58.89%, a small improvement over the 52.84% per-
formance of the basic method. This new result
should be considered baseline. In what follows we
always employ the sequential model for training and
testing the classifiers, using the same constraints.
However, we employ more linguistic knowledge for
a more sophisticated combination of the classifiers.
C1 C2 C3 root
Precision: 83.06 72.52 83.88 59.83
Recall: 80.88 70.20 82.50 57.98
f -measure: 81.96 71.34 83.18 58.89
Table 4: Accuracy of SNoW?s identifying the cor-
rect radical, sequential model
5.3 Combining classifiers using linguistic
knowledge
SNoW provides a ranking on all possible roots. We
now describe the use of linguistic constraints to re-
rank this list. We implemented a function which
uses knowledge pertaining to word-formation pro-
cesses in Hebrew in order to estimate the likeli-
hood of a given candidate being the root of a given
word. The function practically classifies the can-
didate roots into one of three classes: good candi-
dates, which are likely to be the root of the word;
bad candidates, which are highly unlikely; and av-
erage cases.
The decision of the function is based on the ob-
servation that when a root is regular it either occurs
in a word consecutively or with a single w or i be-
tween any two of its radicals. The scoring func-
tion checks, given a root and a word, whether this
is the case. Furthermore, the suffix of the word, af-
ter matching the root, must be a valid Hebrew suffix
(there is only a small number of such suffixes in He-
brew). If both conditions hold, the scoring function
returns a high value. Then, the function checks if
the root is an unlikely candidate for the given word.
For example, if the root is regular its consonants
must occur in the word in the same order they occur
in the root. If this is not the case, the function re-
turns a low value. We also make use in this function
of our pre-compiled list of roots. A root candidate
which does not occur in the list is assigned the low
score. In all other cases, a middle value is returned.
The actual values that the function returns were
chosen empirically by counting the number of oc-
currences of each class in the training data. For ex-
ample, ?good? candidates make up 74.26% of the
data, hence the value the function returns for ?good?
roots is set to 0.7426. Similarly, the middle value is
set to 0.2416 and the low ? to 0.0155.
As an example, consider hipltm, whose root is
n.p.l (note that the first n is missing in this form).
Here, the correct candidate will be assigned the mid-
dle score while p.l.t and l.t.m will score high.
In addition to the scoring function we imple-
mented a simple edit distance function which re-
turns, for a given root and a given word, the inverse
of the edit distance between the two. For exam-
ple, for hipltm, the (correct) root n.p.l scores 1/4
whereas p.l.t scores 1/3.
We then run SNoW on the test data and rank the
results of the three classifiers globally, where the
order is determined by the product of the three dif-
ferent classifiers. This induces an order on roots,
which are combinations of the decisions of three
independent classifiers. Each candidate root is as-
signed three scores: the product of the confidence
measures of the three classifiers; the result of the
scoring function; and the inverse edit distance be-
tween the candidate and the observed word. We
rank the candidates according to the product of
the three scores (i.e., we give each score an equal
weight in the final ranking).
In order to determine which of the candidates to
produce for each example, we experimented with
two methods. First, the system produced the top-i
candidates for a fixed value of i. The results on the
development set are given in table 5.
i = 1 2 3 4
Precision 82.02 46.17 32.81 25.19
Recall 79.10 87.83 92.93 94.91
f -measure 80.53 60.52 48.50 39.81
Table 5: Performance of the system when producing
top-i candidates.
Obviously, since most words have only one root,
precision drops dramatically when the system pro-
duces more than one candidate. This calls for a bet-
ter threshold, facilitating a non-fixed number of out-
puts for each example. We observed that in the ?dif-
ficult? examples, the top ranking candidates are as-
signed close scores, whereas in the easier cases, the
top candidate is usually scored much higher than the
next one. We therefore decided to produce all those
candidates whose scores are not much lower than
the score of the top ranking candidate. The drop
in the score, ?, was determined empirically on the
development set. The results are listed in table 6,
where ? varies from 0.1 to 1 (? is actually computed
on the log of the actual score, to avoid underflow).
These results show that choosing ? = 0.4 pro-
duces the highest f -measure. With this value for
?, results for the held-out data are presented in ta-
ble 7. The results clearly demonstrate the added
benefit of the linguistic knowledge. In fact, our re-
sults are slightly better than average human perfor-
mance, which we recall as well. Interestingly, even
when testing the system on a set of roots which do
not occur in the training corpus (see section 4), we
obtain an f -score of 65.60%. This result demon-
strates the robustness of our method.
Held-out data Humans
Precision: 80.90 83.52
Recall: 88.16 80.27
f -measure: 84.38 81.86
Table 7: Results: performance of the system on
held-out data.
It must be noted that the scoring function alone
is not a function for extracting roots from Hebrew
words. First, it only scores a given root candidate
against a given word, rather than yield a root given a
word. While we could have used it exhaustively on
all possible roots in this case, in a general setting of
a number of classifiers the number of classes might
be too high for this solution to be practical. Sec-
ond, the function only produces three different val-
ues; when given a number of candidate roots it may
return more than one root with the highest score. In
the extreme case, when called with all 223 potential
roots, it returns on the average more than 11 can-
didates which score highest (and hence are ranked
equally).
Similarly, the additional linguistic knowledge is
not merely eliminating illegitimate roots from the
ranking produced by SNoW. Using the linguistic
constraints encoded in the scoring function only
to eliminate roots, while maintaining the ranking
proposed by SNoW, yields much lower accuracy.
Clearly, our linguistically motivated scoring does
more than elimination, and actually re-ranks the
? = 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Precision 81.81 80.97 79.93 78.86 77.31 75.48 73.71 71.80 69.98 67.90
Recall 81.06 82.74 84.03 85.52 86.49 87.61 88.72 89.70 90.59 91.45
f -measure 81.43 81.85 81.93 82.06 81.64 81.10 80.52 79.76 78.96 77.93
Table 6: Performance of the system, producing candidates scoring no more than ? below the top score.
roots. It is only the combination of the classifiers
with the linguistically motivated scoring function
which boosts the performance on this task.
5.4 Error analysis
Looking at the questionnaires filled in by our sub-
jects (section 3), it is obvious that humans have
problems identifying the correct roots in two gen-
eral cases: when the root paradigm is weak (i.e.,
when the root is irregular) and when the word can be
read in more than way and the subject chooses only
one (presumably, the most prominent one). Our sys-
tem suffers from similar problems: first, its perfor-
mance on the regular paradigms is far superior to its
overall performance; second, it sometimes cannot
distinguish between several roots which are in prin-
ciple possible, but only one of which happens to be
the correct one.
To demonstrate the first point, we evaluated the
performance of the system on a different organiza-
tion of the data. We tested separately words whose
roots are all regular, vs. words all of whose roots are
irregular. We also tested words which have at least
one regular root (mixed). The results are presented
in table 8, and clearly demonstrate the difficulty of
the system on the weak paradigms, compared to al-
most 95% on the easier, regular roots.
Regular Irregular Mixed
Number of words 2598 2019 2781
Precision: 92.79 60.02 92.54
Recall: 96.92 73.45 94.28
f -measure: 94.81 66.06 93.40
Table 8: Error analysis: performance of the system
on different cases.
A more refined analysis reveals differences be-
tween the various weak paradigms. Table 9 lists f -
measure for words whose roots are irregular, classi-
fied by paradigm. As can be seen, the system has
great difficulty in the cases of C2 = C3 and C3 = i.
Finally, we took a closer look at some of the er-
rors, and in particular at cases where the system pro-
duces several roots where fewer (usually only one)
are correct. Such cases include, for example, the
Paradigm f -measure
C1 = i 70.57
C1 = n 71.97
C2 = i/w 76.33
C3 = i 58.00
C2 = C3 47.42
Table 9: Error analysis: the weak paradigms
word hkwtrt (?the title?), whose root is the regu-
lar k.t.r; but the system produces, in addition, also
w.t.r, mistaking the k to be a prefix. This is the kind
of errors which are most difficult to cope with.
However, in many cases the system?s errors are
relatively easy to overcome. Consider, for example,
the word hmtndbim (?the volunteers?) whose root is
the irregular n.d.b. Our system produces as many as
five possible roots for this word: n.d.b, i.t.d, d.w.b,
i.h.d, i.d.d. Clearly some of these could be elimi-
nated. For example, i.t.d should not be produced,
because if this were the root, nothing could explain
the presence of the b in the word; i.h.d should be
excluded because of the location of the h. Similar
phenomena abound in the errors the system makes;
they indicate that a more careful design of the scor-
ing function can yield still better results, and this is
the direction we intend to pursue in the future.
6 Conclusions
We have shown that combining machine learning
with limited linguistic knowledge can produce state-
of-the-art results on a difficult morphological task,
the identification of roots of Hebrew words. Our
best result, over 80% precision, was obtained using
simple classifiers for each of the root?s consonants,
and then combining the outputs of the classifiers us-
ing a linguistically motivated, yet extremely coarse
and simplistic, scoring function. This result is com-
parable to average human performance on this task.
This work can be improved in a variety of ways.
We intend to spend more effort on feature engineer-
ing. As is well-known from other learning tasks,
fine-tuning of the feature set can produce additional
accuracy; we expect this to be the case in this task,
too. In particular, introducing features that capture
contextual information is likely to improve the re-
sults. Similarly, our scoring function is simplistic
and we believe that it can be improved. We also in-
tend to improve the edit-distance function such that
the cost of replacing characters reflect phonological
and orthographic constraints (Kruskal, 1999).
In another track, there are various other ways in
which different inter-related classifiers can be com-
bined. Here we only used a simple multiplica-
tion of the three classifiers? confidence measures,
which is then combined with the linguistically mo-
tivated functions. We intend to investigate more so-
phisticated methods for this combination, including
higher-order machine learning techniques.
Finally, we plan to extend these results to more
complex cases of learning tasks with a large num-
ber of targets, in particular such tasks in which the
targets are structured. We are currently working on
similar experiments for Arabic root extraction. An-
other example is the case of morphological disam-
biguation in languages with non-trivial morphology,
which can be viewed as a POS tagging problem with
a large number of tags on which structure can be im-
posed using the various morphological and morpho-
syntactic features that morphological analyzers pro-
duce. We intend to investigate this problem for He-
brew in the future.
Acknowledgments
This work was supported by The Caesarea Edmond
Benjamin de Rothschild Foundation Institute for In-
terdisciplinary Applications of Computer Science.
Dan Roth is supported by NSF grants CAREER IIS-
9984168, ITR IIS-0085836, and ITR-IIS 00-85980.
We thank Meira Hess and Liron Ashkenazi for an-
notating the corpus and Alon Lavie and Ido Dagan
for useful comments.
References
Ken Beesley. 1998a. Arabic morphological analy-
sis on the internet. In Proceedings of the 6th In-
ternational Conference and Exhibition on Multi-
lingual Computing, Cambridge, April.
Kenneth R. Beesley. 1998b. Arabic morphology
using only finite-state operations. In Michael
Rosner, editor, Proceedings of the Workshop
on Computational Approaches to Semitic lan-
guages, pages 50?57, Montreal, Quebec, August.
COLING-ACL?98.
Yaacov Choueka. 1990. MLIM - a system for full,
exact, on-line grammatical analysis of Modern
Hebrew. In Yehuda Eizenberg, editor, Proceed-
ings of the Annual Conference on Computers in
Education, page 63, Tel Aviv, April. In Hebrew.
Kareem Darwish. 2002. Building a shallow Arabic
morphological analyzer in one day. In Mike Ros-
ner and Shuly Wintner, editors, Computational
Approaches to Semitic Languages, an ACL?02
Workshop, pages 47?54, Philadelphia, PA, July.
Abraham Even-Shoshan. 1993. HaMillon HaX-
adash (The New Dictionary). Kiryat Sefer,
Jerusalem. In Hebrew.
Y. Even-Zohar and Dan Roth. 2001. A sequential
model for multi class classification. In EMNLP-
2001, the SIGDAT Conference on Empirical
Methods in Natural Language Processing, pages
10?19.
Radu Florian. 2002. Named entity recognition as a
house of cards: Classifier stacking. In Proceed-
ings of CoNLL-2002, pages 175?178. Taiwan.
Joseph Kruskal. 1999. An overview of se-
quence comparison. In David Sankoff and Joseph
Kruskal, editors, Time Warps, String Edits and
Macromolecules: The Theory and Practice of Se-
quence Comparison, pages 1?44. CSLI Publica-
tions, Stanford, CA. Reprint, with a foreword by
John Nerbonne.
John J. McCarthy. 1981. A prosodic theory of non-
concatenative morphology. Linguistic Inquiry,
12(3):373?418.
Uzzi Ornan. 2003. The Final Word. University of
Haifa Press, Haifa, Israel. In Hebrew.
Vasin Punyakanok and Dan Roth. 2001. The use
of classifiers in sequential inference. In NIPS-
13; The 2000 Conference on Advances in Neural
Information Processing Systems 13, pages 995?
1001. MIT Press.
Dan Roth. 1998. Learning to resolve natural lan-
guage ambiguities: A unified approach. In Pro-
ceedings of AAAI-98 and IAAI-98, pages 806?
813, Madison, Wisconsin.
H. Schu?tze and Y. Singer. 1994. Part-of-speech tag-
ging using a variable memory markov model. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics.
Joseph Shimron, editor. 2003. Language Process-
ing and Acquisition in Languages of Semitic,
Root-Based, Morphology. Number 28 in Lan-
guage Acquisition and Language Disorders. John
Benjamins.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recog-
nition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of CoNLL-2003, pages 142?
147. Edmonton, Canada.
Yizxaq Zdaqa. 1974. Luxot HaPoal (The Verb Ta-
bles). Kiryath Sepher, Jerusalem. In Hebrew.
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 38?43,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sentence Clustering via Projection over Term Clusters
Lili Kotlerman, Ido Dagan
Bar-Ilan University
Israel
Lili.Kotlerman@biu.ac.il
dagan@cs.biu.ac.il
Maya Gorodetsky, Ezra Daya
NICE Systems Ltd.
Israel
Maya.Gorodetsky@nice.com
Ezra.Daya@nice.com
Abstract
This paper presents a novel sentence cluster-
ing scheme based on projecting sentences over
term clusters. The scheme incorporates exter-
nal knowledge to overcome lexical variability
and small corpus size, and outperforms com-
mon sentence clustering methods on two real-
life industrial datasets.
1 Introduction
Clustering is a popular technique for unsupervised
text analysis, often used in industrial settings to ex-
plore the content of large amounts of sentences. Yet,
as may be seen from the results of our research,
widespread clustering techniques, which cluster sen-
tences directly, result in rather moderate perfor-
mance when applied to short sentences, which are
common in informal media.
In this paper we present and evaluate a novel
sentence clustering scheme based on projecting
sentences over term clusters. Section 2 briefly
overviews common sentence clustering approaches.
Our suggested clustering scheme is presented in
Section 3. Section 4 describes an implementation of
the scheme for a particular industrial task, followed
by evaluation results in Section 5. Section 6 lists
directions for future research.
2 Background
Sentence clustering aims at grouping sentences with
similar meanings into clusters. Commonly, vector
similarity measures, such as cosine, are used to de-
fine the level of similarity over bag-of-words encod-
ing of the sentences. Then, standard clustering algo-
rithms can be applied to group sentences into clus-
ters (see Steinbach et al (2000) for an overview).
The most common practice is representing the
sentences as vectors in term space and applying the
K-means clustering algorithm (Shen et al (2011);
Pasquier (2010); Wang et al (2009); Nomoto and
Matsumoto (2001); Boros et al (2001)). An alterna-
tive approach involves partitioning a sentence con-
nectivity graph by means of a graph clustering algo-
rithm (Erkan and Radev (2004); Zha (2002)).
The main challenge for any sentence clustering
approach is language variability, where the same
meaning can be phrased in various ways. The
shorter the sentences are, the less effective becomes
exact matching of their terms. Compare the fol-
lowing newspaper sentence ?The bank is phasing out
the EZ Checking package, with no monthly fee charged
for balances over $1,500, and is instead offering cus-
tomers its Basic Banking account, which carries a fee?
with two tweets regarding the same event: ?Whats
wrong.. charging $$ for checking a/c? and ?Now they
want a monthly fee!?. Though each of the tweets can
be found similar to the long sentence by exact term
matching, they do not share any single term. Yet,
knowing that the words fee and charge are semanti-
cally related would allow discovering the similarity
between the two tweets.
External resources can be utilized to provide such
kind of knowledge, by which sentence representa-
tion can be enriched. Traditionally, WordNet (Fell-
baum, 1998) has been used for this purpose (She-
hata (2009); Chen et al (2003); Hotho et al (2003);
Hatzivassiloglou et al (2001)). Yet, other resources
38
of semantically-related terms can be beneficial, such
as WordNet::Similarity (Pedersen et al, 2004), sta-
tistical resources like that of Lin (1998) or DIRECT
(Kotlerman et al, 2010), thesauri, Wikipedia (Hu et
al., 2009), ontologies (Suchanek et al, 2007) etc.
3 Sentence Clustering via Term Clusters
This section presents a generic sentence clustering
scheme, which involves two consecutive steps: (1)
generating relevant term clusters based on lexical se-
mantic relatedness and (2) projecting the sentence
set over these term clusters. Below we describe each
of the two steps.
3.1 Step 1: Obtaining Term Clusters
In order to obtain term clusters, a term connectivity
graph is constructed for the given sentence set and is
clustered as follows:
1. Create initially an undirected graph with
sentence-set terms as nodes and use lexical re-
sources to extract semantically-related terms
for each node.
2. Augment the graph nodes with the extracted
terms and connect semantically-related nodes
with edges. Then, partition the graph into term
clusters through a graph clustering algorithm.
Extracting and filtering related terms. In Sec-
tion 2 we listed a number of lexical resources pro-
viding pairs of semantically-related terms. Within
the suggested scheme, any combination of resources
may be utilized.
Often resources contain terms, which are
semantically-related only in certain contexts. E.g.,
the words visa and passport are semantically-related
when talking about tourism, but cannot be consid-
ered related in the banking domain, where visa usu-
ally occurs in its credit card sense. In order to dis-
card irrelevant terms, filtering procedures can be em-
ployed. E.g., a simple filtering applicable in most
cases of sentence clustering in a specific domain
would discard candidate related terms, which do not
occur sufficiently frequently in a target-domain cor-
pus. In the example above, this procedure would
allow avoiding the insertion of passport as related to
visa, when considering the banking domain.
Clustering the graph nodes. Once the term
graph is constructed, a graph clustering algorithm
is applied resulting in a partition of the graph nodes
(terms) into clusters. The choice of a particular al-
gorithm is a parameter of the scheme. Many clus-
tering algorithms consider the graph?s edge weights.
To address this trait, different edge weights can be
assigned, reflecting the level of confidence that the
two terms are indeed validly related and the reliabil-
ity of the resource, which suggested the correspond-
ing edge (e.g. WordNet synonyms are commonly
considered more reliable than statistical thesauri).
3.2 Step 2: Projecting Sentences to Term
Clusters
To obtain sentence clusters, the given sentence set
has to be projected in some manner over the term
clusters obtained in Step 1. Our projection pro-
cedure resembles unsupervised text categorization
(Gliozzo et al, 2005), with categories represented
by term clusters that are not predefined but rather
emerge from the analyzed data:
1. Represent term clusters and sentences as vec-
tors in term space and calculate the similarity
of each sentence with each of the term clusters.
2. Assign each sentence to the best-scoring term
cluster. (We focus on hard clustering, but the
procedure can be adapted for soft clustering).
Various metrics for feature weighting and vector
comparison may be chosen. The top terms of term-
cluster vectors can be regarded as labels for the cor-
responding sentence clusters.
Thus each sentence cluster corresponds to a sin-
gle coherent cluster of related terms. This is con-
trasted with common clustering methods, where if
sentence A shares a term with B, and B shares an-
other term with C, then A and C might appear in the
same cluster even if they have no related terms in
common. This behavior turns out harmful for short
sentences, where each incidental term is influential.
Our scheme ensures that each cluster contains only
sentences related to the underlying term cluster, re-
sulting in more coherent clusters.
4 Application: Clustering Customer
Interactions
In industry there?s a prominent need to obtain busi-
ness insights from customer interactions in a contact
center or social media. Though the number of key
39
sentences to analyze is often relatively small, such
as a couple hundred, manually analyzing just a hand-
ful of clusters is much preferable. This section de-
scribes our implementation of the scheme described
in Section 3 for the task of clustering customer in-
teractions, as well as the data used for evaluation.
Results and analysis are presented in Section 5.
4.1 Data
We apply our clustering approach over two real-life
datasets. The first one consists of 155 sentences
containing reasons of account cancelation, retrieved
from automatic transcripts of contact center interac-
tions of an Internet Service Provider (ISP). The sec-
ond one contains 194 sentences crawled from Twit-
ter, expressing reasons for customer dissatisfaction
with a certain banking company. The sentences in
both datasets were gathered automatically by a rule-
based extraction algorithm. Each dataset is accom-
panied by a small corpus of call transcripts or tweets
from the corresponding domain.1
The goal of clustering these sentences is to iden-
tify the prominent reasons of cancelation and dissat-
isfaction. To obtain the gold-standard (GS) anno-
tation, sentences were manually grouped to clusters
according to the reasons stated in them.
Table 1 presents examples of sentences from the
ISP dataset. The sentences are short, with only one
or two words expressing the actual reason stated in
them. We see that exact term matching is not suffi-
cient to group the related sentences. Moreover, tra-
ditional clustering algorithms are likely to mix re-
lated and unrelated sentences, due to matching non-
essential terms (e.g. husband or summer). We note
that such short and noisy sentences are common
in informal media, which became a most important
channel of information in industry.
4.2 Implementation of the Clustering Scheme
Our proposed sentence clustering scheme presented
in Section 3 includes a number of choices. Below
we describe the choices we made in our current im-
plementation.
Input sentences were tokenized, lemmatized and
cleaned from stopwords in order to extract content-
word terms. Candidate semantically-related terms
1The bank dataset with the output of the tested methods will
be made publicly available.
he hasn?t been using it all summer long
it?s been sitting idle for about it almost a year
I?m getting married my husband has a computer
yeah I bought a new laptop this summer so
when I said faces my husband got laid off from work
well I?m them going through financial difficulties
Table 1: Example sentences expressing 3 reasons for can-
celation: the customer (1) does not use the service, (2)
acquired a computer, (3) cannot afford the service.
were extracted for each of the terms, using Word-
Net synonyms and derivations, as well as DIRECT2,
a directional statistical resource learnt from a news
corpus. Candidate terms that did not appear in the
accompanying domain corpus were filtered out as
described in Section 3.1.
Edges in the term graph were weighted with the
number of resources supporting the corresponding
edge. To cluster the graph we used the Chinese
Whispers clustering tool3 (Biemann, 2006), whose
algorithm does not require to pre-set the desired
number of clusters and is reported to outperform
other algorithms for several NLP tasks.
To generate the projection, sentences were rep-
resented as vectors of terms weighted by their fre-
quency in each sentence. Terms of the term-cluster
vectors were weighted by the number of sentences
in which they occur. Similarity scores were calcu-
lated using the cosine measure. Clusters were la-
beled with the top terms appearing both in the un-
derlying term cluster and in the cluster?s sentences.
5 Results and Analysis
In this section we present the results of evaluating
our projection approach, compared to the common
K-means clustering method4 applied to:
(A) Standard bag-of-words representation of sen-
tences;
2Available for download at www.cs.biu.ac.il/
?nlp/downloads/DIRECT.html. For each term we
extract from the resource the top-5 related terms.
3Available at http://wortschatz.informatik.
uni-leipzig.de/?cbiemann/software/CW.html
4We use the Weka (Hall et al, 2009) implementation. Due
to space limitations and for more meaningful comparison we re-
port here one value of K, which is equal to the number of clus-
ters returned by projection (60 for the ISP and 65 for the bank
dataset). For K = 20, 40 and 70 the performance was similar.
40
(B) Bag-of-words representation, where sentence?s
words are augmented with semantically-related
terms (following the common scheme of prior
work, see Section 2). We use the same set of
related terms as is used by our method.
(C) Representation of sentences in term-cluster
space, using the term clusters generated by our
method as vector features. A feature is acti-
vated in a sentence vector if it contains a term
from the corresponding term cluster.
Table 2 shows the results in terms of Purity, Recall
(R), Precision (P) and F1 (see ?Evaluation of clus-
tering?, Manning et al (2008)). Projection signifi-
cantly5 outperforms all baselines for both datasets.
Dataset Algorithm Purity R P F1
ISP
Projection .74 .40 .68 .50
K-means A .65 .18 .22 .20
K-means B .65 .13 .24 .17
K-means C .65 .18 .26 .22
Bank
Projection .79 .26 .53 .35
K-means A .61 .14 .14 .14
K-means B .64 .13 .19 .16
K-means C .67 .17 .21 .19
Table 2: Evaluation results.
For completeness we experimented with applying
Chinese Whispers clustering to sentence connectiv-
ity graphs, but the results were inferior to K-means.
Table 3 presents sample sentences from clusters
produced by projection and K-means for illustration.
Our initial analysis showed that our approach indeed
produces more homogenous clusters than the base-
line methods, as conjectured in Section 3.2. We con-
sider it advantageous, since it?s easier for a human to
merge clusters than to reveal sub-clusters. E.g., a GS
cluster of 20 sentences referring to fees and charges
is covered by three projection clusters labeled fee,
charge and interest rate, with 9, 8 and 2 sentences
correspondingly. On the other hand, K-means C
method places 11 out of the 20 sentences in a messy
cluster of 57 sentences (see Table 3), scattering the
remaining 9 sentences over 7 other clusters.
In our current implementation fee, charge and in-
terest rate were not detected by the lexical resources
we used as semantically similar and thus were not
5p=0.001 according to McNemar test (Dietterich, 1998).
grouped in one term cluster. However, adding more
resources may introduce additional noise. Such de-
pendency on coverage and accuracy of resources is
apparently a limitation of our approach. Yet, as
our experiments indicate, using only two generic re-
sources already yielded valuable results.
a. Projection
credit card, card, mastercard, visa (38 sentences)
XXX has the worst credit cards ever
XXX MasterCard is the worst credit card I?ve ever had
ntuc do not accept XXX visa now I have to redraw $150...
XXX card declined again , $40 dinner in SF...
b. K-means C
fee, charge (57 sentences)
XXX playing games wit my interest
arguing w incompetent pol at XXX damansara perdana
XXX?s upper management are a bunch of rude pricks
XXX are ninjas at catching fraudulent charges.
Table 3: Excerpt from resulting clusterings for the bank
dataset. Bank name is substituted with XXX. Cluster la-
bels are given in italics. Two most frequent terms are
assigned as cluster labels for K-means C.
6 Conclusions and Future Work
We presented a novel sentence clustering scheme
and evaluated its implementation, showing signifi-
cantly superior performance over common sentence
clustering techniques. We plan to further explore
the suggested scheme by utilizing additional lexical
resources and clustering algorithms. We also plan
to compare our approach with co-clustering meth-
ods used in document clustering (Xu et al (2003),
Dhillon (2001), Slonim and Tishby (2000)).
Acknowledgments
This work was partially supported by the MAGNE-
TON grant no. 43834 of the Israel Ministry of Indus-
try, Trade and Labor, the Israel Ministry of Science
and Technology, the Israel Science Foundation grant
1112/08, the PASCAL-2 Network of Excellence of
the European Community FP7-ICT-2007-1-216886
and the European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment no. 287923 (EXCITEMENT).
41
References
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, USA.
Endre Boros, Paul B. Kantor, and David J. Neu. 2001. A
clustering based approach to creating multi-document
summaries.
Hsin-Hsi Chen, June-Jei Kuo, and Tsei-Chun Su.
2003. Clustering and visualization in a multi-lingual
multi-document summarization system. In Proceed-
ings of the 25th European conference on IR re-
search, ECIR?03, pages 266?280, Berlin, Heidelberg.
Springer-Verlag.
Inderjit S. Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning. In
Proceedings of the seventh ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, KDD ?01, pages 269?274, New York, NY,
USA. ACM.
Thomas G. Dietterich. 1998. Approximate statistical
tests for comparing supervised classification learning
algorithms.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text sum-
marization. J. Artif. Int. Res., 22(1):457?479, Decem-
ber.
C. Fellbaum. 1998. WordNet ? An Electronic Lexical
Database. MIT Press.
Alfio Massimiliano Gliozzo, Carlo Strapparava, and Ido
Dagan. 2005. Investigating unsupervised learning for
text categorization bootstrapping. In HLT/EMNLP.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In In Proceedings of the
NAACL Workshop on Automatic Summarization, pages
41?49.
A. Hotho, S. Staab, and G. Stumme. 2003. Word-
net improves text document clustering. In Ying
Ding, Keith van Rijsbergen, Iadh Ounis, and Joe-
mon Jose, editors, Proceedings of the Semantic Web
Workshop of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formaion Retrieval (SIGIR 2003), August 1, 2003,
Toronto Canada. Published Online at http://de.
scientificcommons.org/608322.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, E. K. Park, and
Xiaohua Zhou. 2009. Exploiting wikipedia as exter-
nal knowledge for document clustering. In Proceed-
ings of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, KDD
?09, pages 389?396, New York, NY, USA. ACM.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. JNLE, 16:359?389.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics - Vol-
ume 2, COLING ?98, pages 768?774, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, Cambridge, Juli.
Tadashi Nomoto and Yuji Matsumoto. 2001. A new ap-
proach to unsupervised text summarization. In Pro-
ceedings of the 24th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, SIGIR ?01, pages 26?34, New York, NY,
USA. ACM.
Claude Pasquier. 2010. Task 5: Single document
keyphrase extraction using sentence clustering and la-
tent dirichlet alocation. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 154?157, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Shady Shehata. 2009. A wordnet-based semantic model
for enhancing text clustering. Data Mining Work-
shops, International Conference on, 0:477?482.
Chao Shen, Tao Li, and Chris H. Q. Ding. 2011. Integrat-
ing clustering and multi-document summarization by
bi-mixture probabilistic latent semantic analysis (plsa)
with sentence bases. In AAAI.
Noam Slonim and Naftali Tishby. 2000. Document clus-
tering using word clusters via the information bottle-
neck method. In Proceedings of the 23rd annual inter-
national ACM SIGIR conference on Research and de-
velopment in information retrieval, SIGIR ?00, pages
208?215, New York, NY, USA. ACM.
M. Steinbach, G. Karypis, and V. Kumar. 2000. A
comparison of document clustering techniques. KDD
Workshop on Text Mining.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: A large ontology from
wikipedia and wordnet.
42
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Multi-document summarization us-
ing sentence-based topic models. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
ACLShort ?09, pages 297?300, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, SIGIR ?03, pages 267?273, New
York, NY, USA. ACM.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement prin-
ciple and sentence clustering. In SIGIR, pages 113?
120.
43
