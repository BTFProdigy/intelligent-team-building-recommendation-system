Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 48?51,
Suntec, Singapore, 6 August 2009.
c
?2009 ACL and AFNLP
Building an Annotated Textual Inference Corpus for Motion and Space
Kirk Roberts
Human Language Technology Research Institute
University of Texas at Dallas
Richardson TX 75080
kirk@hlt.utdallas.edu
Abstract
This paper presents an approach for build-
ing a corpus for the domain of motion and
spatial inference using a specific class of
verbs. The approach creates a distribution
of inference features that maximize the
discriminatory power of a system trained
on the corpus. The paper addresses the is-
sue of using an existing textual inference
system for generating the examples. This
enables the corpus annotation method to
assert whether more data is necessary.
1 Introduction
Open-domain textual inference provides a vast ar-
ray of challenges to a textual entailment system.
In order to ensure a wide distribution of these chal-
lenges in building the PASCAL 2005 corpus (Da-
gan et al, 2005), seven different application set-
tings were used for inspiration: Information Re-
trieval, Comparable Documents, Reading Com-
prehension, Question Answering, Information Ex-
traction, Machine Translation, and Paraphrase Ac-
quisition. While PASCAL 2005 and its subse-
quent challenges have released numerous corpora
for open-domain textual inference, many types of
textual inference are sparsely represented. This
speaks not to a weakness in the mentioned cor-
pora, but rather the depth and complexity of chal-
lenges that textual inference presents.
Furthermore, the open-domain inference task
often forces systems to face more than one of
these challenges on a single inference pair (such
as requiring both an understanding of paraphrases
and part-whole relationships). In many cases, it
is desirable to isolate out most of these ?sub-
tasks? within textual inference and concentrate
on a single aspect. Partly for this reason, the
Boeing-Princeton-ISI (BPI) Textual Entailment
Test Suite
1
was developed. Its focus is real-world
knowledge and not syntactic constructions, so it
provides 250 syntactically simple but semantically
rich inference pairs.
This paper explores the creation of such a
specific textual inference corpus based on verb
classes, specifically focusing on the class of mo-
tion verbs and their nominalizations. The goal is
to develop a publicly available corpus for spatial
inference involving motion. Section 2 analyzes the
properties of such a corpus. Section 3 outlines the
effort to build a motion corpus. Finally, Section 4
discusses considerations for the size of the corpus.
2 Properties of an Inference Corpus
2.1 General Properties
Annotated corpora are designed for training and
evaluation for specific classification tasks, and
thus an optimal corpus is one that maximizes a
system?s ability to form a discriminative feature
space. However, knowing ahead of time what the
feature space will look like may be difficult. But,
at the same time the corpus should be also reflec-
tive of the real world.
One method for developing a useful corpus un-
der these conditions, especially for a specific do-
main, is to use an existed textual entailment sys-
tem that can aid in the example generation pro-
cess. By using such a system to suggest examples,
one is able to both reduce the time (and cost) of
annotation as well as producing a corpus with a
desirable distribution of features.
1
Available at http://www.cs.utexas.edu/?pclark/bpi-test-
suite/
48
Text: John flew to New York from LA.
Hypothesis: John left LA for New York.
Text: John will fly over the Atlantic during
his trip to London from New York on Tuesday.
Hypothesis: On Tuesday, John flew over water
when going from North America to Europe.
Table 1: Examples of textual inference for motion.
2.2 Properties of a Motion Corpus
Textual inference about motion requires an exter-
nal representation apart from the text. While many
inference pairs can be solved with strategies such
as lexical alignment or paraphrasing, many texts
assume the reader has knowledge of the proper-
ties of motion. Table 1 shows two such inference
pairs. The first can be solved through a paraphrase
strategy, while the second requires explicit knowl-
edge of the properties of motion that are difficult
to acquire through a paraphrasing method. Unfor-
tunately, most open-domain inference corpora are
sparsely populated with such types of inference
pairs, so a new corpus is required.
For the purpose of the corpus, the concept of
motion is strictly limited to the set of words in the
(Levin, 1993) verb-class MOTION. This greatly
benefits the annotation process: passages or sen-
tences without a verb or nominalization that fits
into the MOTION class can immediately be dis-
carded. Levin?s verb classes are easily accessible
via VERBNET (Kipper et al, 1998), which pro-
vides additional syntactic and semantic informa-
tion as well as mappings into WORDNET (Fell-
baum, 1998).
(Muller, 1998) proposes a qualitative theory of
motion based on spatio-temporal primitives, while
(Pustejovsky and Moszkowicz, 2008) shows an
annotation structure for motion. Furthermore, rep-
resenting motion requires the complete representa-
tion of spatial information, as motion is simply a
continuous function that transforms space. (Hobbs
and Narayanan, 2002) discuss many of the prop-
erties for spatial representation, including dimen-
sionality, frame of reference, regions, relative lo-
cation, orientation, shape, and motion. It is there-
fore desirable for a motion corpus to require infer-
ence over many different aspects of space as well
as motion. Table 2 shows the properties of motion
incorporated in the inference system.
In practice, these properties are far from uni-
formly distributed. Properties such as dest(M
x
)
are far more common than shape(M
x
). Clearly,
Property Description
motion(M
x
) Instance of motion in text
theme(M
x
) Object under motion
area(M
x
) Area of motion
src(M
x
) Source location
dest(M
x
) Destination location
path(M
x
) Path of motion
current(M
x
) Current position
orientation(M
x
) Direction/Orientation
shape(M
x
) Shape of object
t start(M
x
) Start of motion
t end(M
x
) End of motion
Table 2: Extracted properties of motion.
having a system that performs well on destinations
is more important than one that can draw infer-
ences from motion?s effects on an object?s shape
(?the car hit the barricade and was crushed?), but
it is still desirable to have a corpus that provides
systems with examples of such properties.
The corpus annotation process shall disregard
many discourse-related phenomena, including co-
reference. Further, the text and hypothesis for each
inference pair will be limited to one sentence. In
this way, knowledge of motion is emphasized over
other linguistic tasks.
3 Building a Corpus Focusing on
Knowledge about Motion
To build the motion inference corpus, we chose
to start with an existing, large document corpus,
AQUAINT-2.
2
This corpus is composed of 2.4GB
of raw files and contains over 900,000 documents.
Having a large corpus is important for finding
sparse verbs like escort and swing and sparse
properties like area(M
x
) and orientation(M
x
).
3.1 Text Annotation
In order to get a more diverse distribution of mo-
tion verbs and properties (hereafter, just referred
to as properties) than the given distribution from
the corpus, the following procedure is considered:
Let V
s
be the (static) distribution of motion
properties from the document corpus. Let V
d
be
the (dynamic) distribution of motion properties
from an (initially empty) set of annotated exam-
ples. Next, define a ?feedback? distribution V
f
,
such that for each property y:
P
f
(y) =
max(0, 2P
s
(y)? P
d
(y))
Z
(1)
Where P
s
(y), P
d
(y), and P
f
(y) are the proba-
bilities of property y in distributions V
s
, V
d
, and
2
Available through the Linguistic Data Consortium, id
LDC2008T25
49
Vf
, respectively, and Z is a normalization factor
(needed when the numerator is zero).
Let the parameter ? determine the likeli-
hood of sampling from this distribution V
f
or
from the uniform distribution U . The function
NextExampleType(V
f
, ?) then specifies which mo-
tion property should be in the next example. An
unannotated example is then drawn from an index,
annotated by the user, and placed in the set of an-
notated examples. V
d
is then updated to reflect the
new distribution of verbs and properties in the an-
notated example set.
There are several items to note. First, the exam-
ple might contain multiple properties not chosen
by the NextExampleType method. When a motion
event with a path(M
x
) is chosen, it is not uncom-
mon for a dest(M
x
) property to be a part of the
same event. This is why the V
d
and V
f
distribu-
tions are necessary: they are a feedback mecha-
nism to try to keep the actual distribution, V
d
, as
close to the desired distribution as possible.
Second, the value for ? is the sole pre-specified
parameter. It dictates the likelihood of choosing
an example despite its a priori probability. Setting
? to 1.0 will result in only sampling based on the
V
f
distribution, and setting it to 0.0 will generate
a uniform sampling. In practice, this is set to 0.8
to allow many of the sparse features through.
Third, V
d
and V
f
account even for proper-
ties generated from the uniform distribution. In
practice this means that low-probability events
will be generated from U and not V
f
, especially
later in the sampling process. Due to the non-
independence of the properties as discussed above,
this discrepancy is difficult to account for and is
considered acceptable: U will still dictate a much
higher distribution of low-probability properties
than would otherwise be the case.
3.2 Hypothesis Annotation
While the hypothesis itself must be written by the
annotator, one can apply some of the same prin-
ciples to ensure a coverage of motion concepts.
Since not every motion term in the text need be
tested by the hypothesis, it is beneficial to keep
track of which properties are tested within each.
For this reason, the annotator is responsible for in-
dicating which motion properties are used in the
hypothesis. This way, the annotator can be alerted
to any properties under-represented in the set of
hypotheses relative to the set of annotated texts.
Feature # Seq Ex Gen
dest(M
x
) 749 48 60
go 382 90 129
leave 105 376 454
. . . . . . . . . . . .
orientation(M
x
) 94 420 282
flee 4 9,991 5,508
steer 2 20,000 7,065
parachute 1 40,000 8,227
Table 3: Motion features with instance counts
from 2000 sample sentences. The Seq (Sequen-
tial) and Ex Gen (see Section 3.1) columns are
the expected number of annotated sentences for
20 instances of the feature to be found using that
method, assuming i.i.d.
3.3 Evaluation
The purpose of the algorithm from Section 3.1 is
not only to build a more balanced corpus, but to
do so more quickly. By looking through exam-
ples that are more likely to maintain a balanced
corpus, annotators are saved from looking through
hundreds (or thousands!) of examples that contain
overly redundant properties.
To illustrate this point, consider a random sam-
ple of 2000 sentences. Table 3 shows the extracted
counts for some of the least and most common
verbs and properties alongside projections of how
many motion sentences would need to be anno-
tated with and without the algorithm to attain a
rather modest 20 examples of each. The results
prove that, for many features, the example genera-
tion approach allows many more instances of that
feature to be placed in the corpus.
3.4 Comparison with Active Learning
The process presented in Section 3.1 bears a close
resemblence with active learning, so the differ-
ences between the two merit some discussion. Ac-
tive learning seeks to improve a specific classifier
by selecting training data based on some confi-
dence/score metric for the purpose of improving
an overall metric (usually the score across all an-
notated data). Often, examples on which the clas-
sifier is the least confident are presented to an an-
notator for manual classification. Then the system
is re-trained to include the new data, and the pro-
cess repeats.
The annotation process presented above, how-
ever, is not ?active? in this same sense. Instead
it seeks a certain distribution of properties regard-
less of a classifier?s ability to accurately perform
inferences. The primary advantage, then, is a cor-
pus that is not designed for a specific classification
50
Corpus # Dev # Test
RTE-1 567 800
RTE-2 800 800
RTE-3 800 800
BPI 250
Table 4: Number of annotated inferences for each
inference corpus.
technique or set of features. A secondary advan-
tage is that it avoids the risk of choosing poor ex-
amples but rather seeks a breadth of data.
4 Corpus Size Considerations
An important consideration?and an active area of
research?is the ideal size of an annotated corpus.
As one can see from Table 4, the RTE tasks make
800 examples available for an open-domain tex-
tual inference corpus.
But when the scope of the corpus is more lim-
ited, perhaps 800 examples is too few or too many.
If the intent is to provide a set on which systems
can be blindly evaluated for motion inference, then
a much smaller number is required than a corpus
intended for training machine-learned models. In
this case, we seek to do the latter.
It should be mentioned that if the corpus gen-
eration process follows the algorithm presented in
Section 3.1, then any reasonable number of infer-
ence pairs should follow the same distribution as
a much larger set. For this reason, it is possible
to adopt the active learning approach and build the
corpus incrementally by iteratively annotating un-
til satisfactory results are reached or gains are min-
imal.
5 Discussion
In addition to building a motion-specific corpus,
this paper argues for the creation of domain-
specific corpora for textual inference. Beyond
simply measuring a system?s ability to reason for
specific tasks, they enable the aquisition of world
knowledge through training data. They can then
be used by statistical learning techniques applied
to natural language processing. This is different
than generating axioms and using them in abduc-
tive reasoning, which is another approach to ap-
proximate world knowledge.
Levin?s verb classes (of which there are less
than fifty) are a useful way to organize corpora.
Levin?s classes are structured under the assump-
tion that syntactic and semantic frames are directly
linked within each class. Since all verbs within the
class have similar semantic arguments, knowledge
aquisition becomes manageable. A system that
has a wide coverage of knowledge trained on such
corpora could claim a wide coverage of knowledge
of all verb-based events within text.
6 Conclusion
This paper has presented an argument for the cre-
ation of domain-specific textual inference corpora
and, in general terms, what that corpus should look
like. In particular, it has described the ongoing
process of building an inference corpus for spa-
tial inference about motion. It has shown how an
existing system can be used to aid in the example
generation and annotation process with analysis as
to the effects of the algorithm on presenting more
balanced data. Finally, the paper discussed some
considerations for the size of such a corpus.
Upon completion, the corpus will be made pub-
lically available.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognizing textual entailment
challenge. In Proceedings of the First PASCAL
Challenges Workshop on Recognising Textual En-
tailment, pages 1?8.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Jerry R. Hobbs and Srini Narayanan. 2002. Spatial
representation and reasoning. In Intelligent Systems:
Concepts and Applications, pages 67?76. MacMil-
lan.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
1998. Class-based construction of a verb lexicon.
In In Proceedings of AAAI/IAAI.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. The University
of Chicago Press.
Philippe Muller. 1998. A Qualitative Theory of Mo-
tion Based on Spatio-Temporal Primitives. In KR
?98: Principles of Knowledge Representation and
Reasoning, pages 131?141.
James Pustejovsky and Jessica L. Moszkowicz. 2008.
Integrating Motion Predicate Classes with Spatial
and Temporal Annotations. In Proceedings of COL-
ING 2008, pages 95?98.
51
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 980?990,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Learning of Selectional Restrictions and Detection of
Argument Coercions
Kirk Roberts and Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083, USA
{kirk,sanda}@hlt.utdallas.edu
Abstract
Metonymic language is a pervasive phe-
nomenon. Metonymic type shifting, or ar-
gument type coercion, results in a selectional
restriction violation where the argument?s se-
mantic class differs from the class the predi-
cate expects. In this paper we present an un-
supervised method that learns the selectional
restriction of arguments and enables the de-
tection of argument coercion. This method
also generates an enhanced probabilistic reso-
lution of logical metonymies. The experimen-
tal results indicate substantial improvements
the detection of coercions and the ranking of
metonymic interpretations.
1 Introduction
Metonymic language is pervasive in today?s social
interactions. For example, it is typical to find ques-
tions that require metonymic resolution:
(Q1) Did you enjoy War and Peace?
(Q2) Does anyone have any advice on how to start
a bowling team?1
In order to process such questions and capture the
intention of the person that posed them, coercions
are needed. Question (Q1) is interpreted as whether
you enjoyed reading ?War and Peace?, while (Q2)
is interpreted as asking for advice on organizing,
forming, or registering a bowling team. The qual-
ity of the answers therefore depends on the ability
to (1) recognize when metonymic language is used,
and (2) to produce coercions that capture the user?s
intention. One important step in this direction was
1Both questions taken from Yahoo Answers.
taken by SemEval-2010 Task 7, which focused on
the ability to recognize (a) an argument?s selectional
restriction for predicates such as arrive at, cancel,
or hear, and (b) the type of coercion that licensed
a correct interpretation of the metonymy. Details of
the task are reported in (Pustejovsky et al, 2010).
Approaches to metonymy based on this task are lim-
ited, however, because (a) the task is focused only on
semantically non-ambiguous predicates and (b) the
selectional restrictions of the arguments were cho-
sen from a pre-defined set of six semantic classes
(artifact, document, event, location, proposition, and
sound). However, metonymy coercion systems ca-
pable of providing the interpretations of questions
(Q1) and (Q2) clearly cannot operate with the sim-
plifications designed for this task.
Inspired by recent advances in modeling selec-
tional preferences with latent-variable models (Rit-
ter et al, 2010; ?O Se?aghdha, 2010), we propose
an unsupervised model for learning selectional re-
strictions. The model assumes that (1) arguments
have a single selected class exemplified by the se-
lectional restriction, and (2) the selected class can
be inferred from the data, in part by modeling how
coercive each predicate is. The model is capable of
operating with both ambiguous and disambiguated
predicates, producing superior results for predicates
that have been disambiguated. The selectional re-
strictions and coercions detected by the model re-
ported in this paper can be used to enhance the logi-
cal metonymy approach reported in Lapata and Las-
carides (2003). The experimental results show a sig-
nificant improvement in the ranking of interpreta-
tions.
980
The rest of this paper is organized as follows.
Section 2 discusses related work. Section 3 de-
tails unsupervised models that inform detection of
metonymies. Section 4 outlines a method for disam-
biguating ambiguous predicates. Section 5 describes
the enhanced interpretation of logical metonymies
when conventional constraints are known. Section 6
outlines our implementation and experimental de-
sign. Section 7 presents our experimental results in
three broad tasks: (i) semantic class induction, (ii)
coercion detection, and (iii) logical metonymy inter-
pretation. Section 8 summarizes the conclusions.
2 Previous Work
Lapata and Lascarides (2003) propose a probabilis-
tic ranking model for logical metonymies. They es-
timate these probabilities using co-occurrence fre-
quencies of predicate-argument pairs in a corpus.
Shutova (2009) extends this approach to provide
sense-disambiguated interpretations from WordNet
(Fellbaum, 1998) by using the alternative interpre-
tations to disambiguate polysemous words. Shutova
and Teufel (2009) extend this approach further by
clustering these sense-disambiguated interpretations
into distinct groups of meaning (e.g., {read, browse,
look through} and {write, produce, work on} for
?enjoy book?). Not only do these approaches as-
sume logical metonymies have already been iden-
tified, but they are susceptible to providing interpre-
tations that are themselves logical metonymies (e.g.,
finish book). In this paper, we propose an enhance-
ment to resolving logical metonymies by ruling out
event-invoking predicates in order to provide more
semantically valid interpretations.
Recently, the resolution of several linguistic prob-
lems has benefited from Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003) models. ?O Se?aghdha
(2010) examines several selectional preference mod-
els based on LDA in predicting human judgements
on predicate-argument plausibility. Both LDA and
an extension, ROOTH-LDA (based on Rooth et al
(1999)), perform well at predicting plausibility on
unseen predicate-argument pairs. Inspired by these
results, we propose to extend selectional preference
models in order to learn selectional restrictions.
Alternatively, unsupervised algorithms exist that
both induce semantic classes (Rooth et al, 1999;
Lin and Pantel, 2001) and cluster predicates by their
selectional restrictions (Rumshisky et al, 2007) but
none of these provide a sufficient framework for de-
termining if a specific argument violates its predi-
cate?s selectional restriction.
3 Unsupervised Learning of Selectional
Restrictions
In predicate-argument structures, predicates impose
selectional restrictions in the form of semantic ex-
pectations on their arguments. Whenever the seman-
tic class of the argument meets these constraints a
selection occurs. For example, the predicate ?hear?
imposes the semantics related to sound on the ar-
gument ?voice?. Because the semantic class for
?voice? conforms to these constraints, we call its se-
mantic class the selected class. However, when the
semantic class of the argument violates these con-
straints, we follow Pustejovsky et al (2010) and re-
fer to this as a coercion. In this case, we call the
argument?s semantic class the coerced class. For ex-
ample, ?hear speaker? is a coercion where the ar-
gument class, person, is implicitly coerced into the
voice of the speaker, a sound.
3.1 A Baseline Model
We consider the LDA-based selectional prefer-
ence model reported in ?O Se?aghdha (2010) as a
baseline for modeling selectional restrictions. For-
mally, we define our LDA baseline model as follows.
Let V be the predicate vocabulary size, let A be the
argument vocabulary size, and let K be the number
of argument classes. Let avi be the ith (non-unique)
argument realized by predicate v. Let cvi be the class
for avi . Let ?v be the class distribution for predicate
v and ?k be the argument distribution for class k.
The graphical model for this LDA is shown in Fig-
ure 1(a). The generative process for LDA is:
For each argument class k = 1..K:
1. Choose ?k ? Dirichlet(?)
For each unique predicate v = 1..V :
2. Choose ?v ? Dirichlet(?)
For every argument i = 1..nv:
3. Choose cvi ? Multinomial(?v)
4. Choose avi ? Multinomial(?c
v
i )
Following Griffiths and Steyvers (2004), we col-
lapse ? and ? and estimate the model using Gibbs
981
? ? c a
? ?
V
N
K
(a)
?
?0
?1
?
s
?
c
x
a
? ?
V
N
K
(b)
Figure 1: Graphical models for (a) LDA, and (b) coercion LDA (cLDA).
Sampling. This yields the update equation:
p(cvi = k|av;?, ?) ?
fvk + ?
fv + K?
fak + ?
fk + A?
(1)
Where fak is the frequency of argument a being as-
signed class k; fk is the frequency of class k being
assigned to any argument; fvk is the frequency of
predicate v having an argument of class k; and fv is
the total number of non-unique arguments for pred-
icate v.
3.2 A Coercion Model
We now incorporate our assumptions for selec-
tional restriction modeling. Namely: (1) there is
one selected class per predicate, and (2) the predi-
cate?s selected class can be chosen from the classes
of its arguments. To accomplish this, we must also
account for the coerciveness of each predicate. We
assign a latent variable ?v for each predicate v that
controls how coercive v should be. The additional
hyper-parameters ?0 and ?1 act as priors on ?v. The
generative process for this coercion LDA model,
which we denote cLDA, is:
For each argument class k = 1..K:
1. Choose ?k ? Dirichlet(?)
For each unique predicate v = 1..V :
2. Choose sv ? Uniform(1,K)
3. Choose ?v ? Dirichlet(?)2
4. Choose ?v ? Beta(?0, ?1)
For every argument i = 1..nv:
5. Choose cvi ? Multinomial(?v)
6. Choose xvi ? Bernoulli(?v)
7. If xvi = 1, Choose avi ? Multinomial(?c
v
i )
Else Choose avi ? Multinomial(?s
v)
The model variable sv represents the selected class
for predicate v. The coerced class is represented
2With the exception that the probability of drawing the se-
lected class sv is zero. This can be seen as drawing the multi-
nomial ?v from a Dirichlet distribution with K-1 components.
for each argument i by cvi , where xvi chooses be-
tween the selected and coerced class. The variable
xvi is similar to switching variables in other graph-
ical models such as Chemudugunta et al (2007)
and Reisinger and Mooney (2010), where switch-
ing variables are used to choose between a back-
ground distribution and a document-specific distri-
bution. In this case, the switching variable chooses
between a specific class and a predicate-specific dis-
tribution. The graphical model for cLDA is shown
in Figure 1(b). Note that cLDA is virtually equiva-
lent to LDA when ?v is 1 and ?1 is small because the
selected class will be ignored. In this way, highly co-
ercive predicates have less of an impact on the argu-
ment clustering because they are more reliant on the
multinomial ?. We use Gibbs sampling to perform
model inference and collapse ?, ?, and ? , integrat-
ing them out using multinomial-Dirichlet conjugacy
(the Beta distribution used by ? is just a special case
of the Dirichlet with only two parameters).
The update formula for the selected class sv is:
p(sv = k|av, cv ,xv;?, ?)
?
nv?
i
P (avi |sv = k;?)
?
?
i?Sv
favi k + ?
fk + A?
(2)
Where nv is the number of argument observations
for predicate v; Sv is the set of arguments of v that
are selections; and favi k is the frequency of word a
v
i
being assigned to class k for any predicate. We then
sample cvi and xvi jointly:
p(cvi = k, xvi = q|sv, cvi? ,xvi? ,av;?, ?, ?)
? p(cvi =k;?)p(xvi =q; ?)p(avi |sv, cvi? ,xvi? ,av;?)
? fvk + ?fv + K?
fvq + ?q
fv0 + ?0 + fv1 + ?1
faz + ?
fz + A?
(3)
982
Where fvq, fv0, and fv1 is the frequency of x values
that equal q, 0, and 1, respectively, for predicate v;
faz is the frequency of word a being in class z and
fz is the frequency all words being in class z, where
z is defined as being equal to k when xvi = 1, or sv
when xvi = 0.
Note that Equation (2) results in a sampling of
the selected class for v proportional to the number
of arguments in each class for v, fulfilling our sec-
ond assumption. Also note from Equation (3), the
second term corresponds to the coerciveness of the
predicate. When the predicate is very coercive, the
marginal probability associated with xvi = 0 will be
very low. If all predicates become entirely coercive,
most x values will become 1 and the cLDA will be-
come almost equivalent to an LDA model.
3.3 Coercion Detection
After the latent parameters have been estimated,
we still require a method to determine if a given
predicate-argument pair is a coercion or not. We
assign a score in [0, 1] instead of a binary value.
Higher scores (near 1) indicate high likelihood of
selection, while lower scores (near 0) indicate coer-
cion. The LDA model must rely on a scoring method
using the predicate-class and argument-class mix-
tures:
C1(v, a) =
K?
k
P (k|v)P (a|k)
=
K?
k
?vk?ka (4)
Where ?vk represents the probability of any argument
of v being in the class k and ?ka represents the prob-
ability of the argument a being in class k for any
predicate. C1 is also available as a scoring method
for cLDA by including the proportion of the selected
class sv in ?. Note that since ? and ? are integrated
out for both LDA and cLDA, we instead use their
frequencies smoothed with ? and ?, respectively,
which is their maximum likelihood estimate.
The cLDA model contains two useful parameters
that can identify selections and coercions: the se-
lected class s and the coercion indicator x. This
yields two more coercion scoring metrics:
C2(v, a) = P (a|sv)
= ?sva (5)
C3(v, a) = P (xva = 0|v, a)
= 1.0 ?
?
i?Iva x
v
i
|Iva |
(6)
Where sv is the selected class for predicate v; Iva
is the set of predicate-argument instances for pred-
icate v and argument a; and xvi is 0 for a selection
and 1 for a coercion. Of the three metrics, C3 is the
most direct measure of a coercion as it represents
the average decision the model learned on the same
predicate-argument pair. However, C3 requires a
large sample of instances for a particular predicate
and argument, and so may be quite sparse. In prac-
tice, these different metrics have their own strengths
and weaknesses and the best performing method of-
ten depends on the final task.
4 Predicate Sense Induction
Our assumption of a single selected class per predi-
cate ignores predicate polysemy. However, the same
lexical item may have multiple meanings, each with
a separate selected class. We therefore propose a
method of partitioning a predicate?s arguments by
the induced senses of the predicate. This allows sep-
arate induced predicates to each select a separate ar-
gument class. Consider the verb fire, which has at
least two distinct common senses: (1) to shoot or
propel an object (e.g., to fire a gun), and (2) to lay
someone off (e.g., to fire an employee). The first
sense selects a weapon (e.g., gun, bullet, rocket),
while the second sense selects a person (e.g., em-
ployee, coach, apprentice).
Specifically, we employ tiered clustering
(Reisinger and Mooney, 2010) using the words
in the predicate?s context. Tiered clustering is a
discrete clustering method, as opposed to methods
such as (Brody and Lapata, 2009) that assign a
distribution of word senses to each word instance.
Tiered clustering has several advantages over
other discrete clustering approaches. First, tiered
clustering learns a background word distribution in
addition to the clusters. This reduces the impact that
words common to most senses have on the cluster-
ing process and allow clusters to form around only
the most salient words. Second, tiered clustering
983
Cluster 1 Cluster 2 Cluster 3 Cluster 4
(18,391) (16,651) (18,749) (11,833)
shots ball hire gun
gun puck letter imagination
Israeli hired Yeltsin grill
missiles owner minister laser
rockets shots workforce cells
officers coaches executives engine
soldiers net employee brain
rounds circle managers !
bullets Johnson hired engines
weapons Williams union fire
Table 1: Context word clusters resulting from tiered clus-
tering for the verb fire (includes the number of unique
words belonging to each cluster).
uses a Chinese Restaurant Process (CRP) prior to
control both the formation of new clusters (senses)
and the bias toward larger clusters (more common
senses). This conforms with our intuition of how
word senses are distributed: a few common senses
with a gradual transition to a long tail of rare senses.
When deciding which cluster to use for a given
predicate-argument pair, we use the cluster most
associated with the argument.
We use a 10-token window around the predicate
as features. The result of predicate induction on the
verb fire is shown in Table 4. The first three clusters
can be interpreted to be about (1) firing weapons, (2)
sport-related shots (e.g., ?fired the puck?), and (3)
lay-offs. One must be careful in choosing the param-
eters for induction, however, as it is possible to par-
tition a unique word sense such that coercions and
selections are placed in a separate clusters. Section 6
discusses our parameter selection experiments.
5 Logical Metonymy Interpretation
Logical metonymies are a unique class of coercions
due to the fact that their eventive interpretation can
be derived from verbal predicates. For instance, for
the logical metonymy ?enjoy book?, we know that
read is a good candidate interpretation because (1)
books are objects whose purpose is to be read and
(2) reading is an event that may be enjoyed. We
therefore expect to see many instances of both ?read
book? and ?enjoy reading? (Lapata and Lascarides,
2003). Conversely, for coercions with non-eventive
interpretations, such as ?arrive at meeting?, the in-
terpretation (location of) is more dependent on the
predicate (arrive) than the function of its argument
(meeting).
In this section, we limit our discussion of logical
metonymy to the verb-object case, its correspond-
ing baseline for ranking interpretations, and our pro-
posed enhancements. However, similar baselines
exist for other types of logical metonymy, such as
adjective-noun and noun-noun. Since our enhance-
ment does not depend on any syntactic information
beyond the predicate-argument instances needed for
Section 3.2, it could easily be applied to those as
well.
Lapata and Lascarides (2003) propose a proba-
bilistic ranking model where the probability of an
interpretation e for a verb-object pair (v, o) is pro-
portional to the probability of all three in a verb-
interpretation-object pattern.3 For example, the
probability that read is the correct interpretation of
?enjoy book? is proportional to the likelihood of see-
ing ?enjoy reading book? expressed as a syntactic
dependency in a sufficiently large corpus. Due to
data sparsity, they approximate this likelihood of
seeing the object given the verb and interpretation
to simply the likelihood of seeing the object given
the interpretation. We denote this logical metonymy
ranking method as LMLL, formally defined as:
LMLL(e; v, o) = Pc(v, e, o)
= Pc(e)Pc(v|e)Pc(o|e, v)
? Pc(e)Pc(v|e)Pc(o|e)
? fc(v, e)fc(o, e)Nfc(e)
(7)
Where Pc and fc indicate probability and frequency,
respectively, derived from corpus counts. See Lap-
ata and Lascarides (2003) for a detailed explanation
of how these frequencies are obtained.
This model, which we consider our baseline, is
only partially correct as the corpus will contain co-
ercions that form invalid interpretations. Consider
the phrases ?enjoy finishing a book? and ?enjoy
discussing a book?. Both ?finish book? and ?dis-
cuss book? are coercions (and logical metonymies)
themselves, and do not form a valid interpretation.4
3They use two patterns: ?v e-ing o? and ?v to e o?, where e
is tagged as a verb.
4For evidence of the frequency of these phrases, at the time
of this writing, ?enjoy finishing a book? and ?enjoy finishing
the book? have a combined 728 Google hits, while ?enjoy dis-
cussing a book? and ?enjoy discussing the book? have a com-
984
Thus, when discovering interpretations for logical
metonymies, we must be aware of the selectional re-
strictions of candidate interpretations.
We propose to incorporate the coercion probabil-
ity learned by our cLDA model in order to rank only
those interpretations that are considered selections:
LM ?(e; v, o) = P (v, e, o, xeo = 0) (8)
However, due to the approximations made to esti-
mate Pc(v, e, o), this probability cannot be directly
calculated as not all the frequencies reflect verb-
object counts. Instead, we can combine the corpus
probability Pc(v, e, o) with the probability that the
verb-object pair (e, o) is a coercion in our model.
We denote this probability Px(e, o), and it may be
derived from the scoring metrics in Equations (4),
(5), or (6) above. We further propose three methods
for enhancing the LMLL baseline using Px(e, o) to
approximate Equation 8.
A naive method for including information from
our cLDA model is to consider the corpus prob-
ability, Pc(v, e, o) and the coercion probability,
Px(e, o), to be independent:
LMIND(e; v, o) = Pc(v, e, o)Px(e, o) (9)
In other words, the rank of an interpretation is dic-
tated by the unweighted combination of its corpus
probability Pc and its coercion probability Px. How-
ever, these two quantities are not likely to be inde-
pendent. Most instances where e is used with either
v or o are in fact selective.5 We therefore experiment
with two shallow learning methods for combining
these two quantities.
The first method is a filtering approach where a
threshold is learned for Px:
LMTH(e; v, o) =
{
Pc(v, e, o) if Px(e, o) ? ?
0 otherwise (10)
Where the threshold ? is learned from a development
set. We expect this model could suffer from noisy Px
values or to simply choose a threshold of zero due to
the prominence of Pc.
Finally, we include a weighted linear model to
bined 7,040 Google hits.
5For comparison, ?enjoy reading a book? and ?enjoy read-
ing the book? have a combined 6.5 million Google hits
discover the relative value of Pc and Px:
LMWT (e; v, o) = w1Pc(v, e, o)+w2Px(e, o) (11)
Where w1 and w2 are learned weights. We dis-
cuss how the parameters for LMTH and LMWT are
learned in the experimental setup below.
6 Experimental Setup
We use the NYT subsection of the English Gigaword
Fourth Edition (Parker et al, 2009) for a total of
1.8M newswire articles. The Stanford Dependency
Parser (de Marneffe et al, 2006) is used to extract
verb-object relations (dobj) that form the input to our
model. To reduce noise, we keep only verbs listed in
VerbNet (Kipper et al, 1998) with at least 100 ar-
gument instances, discarding have and say, which
are too semantically flexible to select from clear se-
mantic classes and so common they distort the class
distributions. This results in 4,145 unique verbs with
51M argument instances (388K unique arguments).
Additionally we use the dependency parser to ex-
tract open clausal complements of verbs (e.g., ?like
to swim?) for use in logical metonymy interpreta-
tion. We believe this to be a more reliable alter-
native to the phrase chunk extraction patterns used
in Lapata and Lascarides (2003). We keep clausal
complements (xcomp) where the dependent is either
a gerund or infinitive in order to estimate Pc(v|e) in
Equation (7).
For tiered clustering we use the same implemen-
tation as Reisinger and Mooney (2010)6 to partition
the surface form of the verb into one or more in-
duced forms. Instead of using a fixed number of
iterations, the clustering was run for 100 iterations
past the best recorded log-likelihood in order to find
the best possible fit to the data. We tuned the hyper-
parameters by maximizing the log-likelihood on a
small held-out set of 20 predicate-argument pairs
(10 selections, 10 coercions). The resulting parti-
tions were fairly conservative, yielding 12,332 in-
duced verbs or about 3 induced verb forms for every
surface form, with 305 verbs not being partitioned at
all.
We implemented both LDA and cLDA as de-
scribed in Sections 3.1 and 3.2. For the ? and ?
6Available at http://github.com/joeraii/UTML-Latent-
Variable-Modeling-Toolkit
985
hyper-parameters, we used the MALLET (McCal-
lum, 2002) defaults of 1.0 and 0.1, respectively, for
both LDA and cLDA. We used the 20 predicate-
argument pairs mentioned above to tune the ? hyper-
parameters as well as the number of iterations. Both
?0 and ?1 were set to 100. We observed that for
both LDA and cLDA, longer runs (in iterations) re-
sulted in improved model log-likelihood but infe-
rior results in terms of detecting coercions. It is
not uncommon in topic modeling for model likeli-
hood to not be completely correlated with the score
on the task for which the topic model was intended
(see Chang et al (2009)). Both LDA and cLDA
were found to perform best at 50 iterations on this
data, after which their class distributions were less
?smooth? and became rigidly associated with just a
few classes, thus having a negative impact on coer-
cion detection. While further iterations hurt coer-
cion detection, only minor gains in model likelihood
are seen. We believe the small number of iterations
necessary for the model to converge is therefore a
function of the data. In traditional topic modeling,
documents are generally of similar size (i.e., within
an order of magnitude). But in our data, many pred-
icates have 10,000 times more instances than others.
We have not yet empirically explored the impact of
using a more uniform number of arguments for each
predicate. This issue also makes it difficult to take
multiple samples, which we experimented with un-
successfully.
Our a priori intuition was that as the number
of classes was increased, LDA would improve and
cLDA would degrade due to its assumption of a sin-
gle selected class. However, this did not always bear
out in the results for every task described below.
As such, instead of choosing a specific number of
classes for each model, we describe results for each
model with K = 10, 25, and 50.
For logical metonymy, both LMTH and LMWT
require learned parameters. LMTH needs a learned
threshold while LMWT needs two learned weights.
For both, we split the data set into two partitions,
learn the optimal threshold/weights on one partition,
and use it as the parameters for the other partition.
Both methods are trained on the final scoring metric,
described in Section 7.3. For threshold learning, this
involves finding the optimal cut-off to maximize the
score. For weight learning, we use an exhaustive
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA
NMI .382 .448 .389 .435 .391 .383
Rand .717 .731 .721 .760 .723 .730
F1 .425 .319 .192 .543 .311 .205
B3 (C) .553 .513 .444 .525 .476 .341
B3 (E) .453 .351 .223 .521 .324 .234
MUC .545 .545 .531 .500 .532 .544
cLDA
NMI .446 .403 .360 .510 .430 .366
Rand .736 .719 .716 .788 .734 .711
F1 .448 .291 .183 .567 .329 .184
B3 (C) .575 .484 .312 .593 .495 .313
B3 (E) .473 .321 .205 .556 .346 .205
MUC .500 .521 .507 .595 .541 .571
Table 2: Clustering scores for induced classes.
search over the range {1.0, 0.9, . . . , 0.2, 0.1, 10?2,
10?3, . . . , 10?14} for both w1 and w2.
7 Results and Discussion
7.1 Semantic Class Induction
For the evaluation of the argument classes in-
duced by our method, we use a subset of the Word-
Net lexicographer files, which correspond to coarse-
grained semantic classes. We chose this form of
evaluation because, unlike a named entity corpus,
no sentential context is required and is therefore
more consistent with the information available to
our model. We use six of the larger, more seman-
tically coherent WordNet classes: artifact, person,
plant, animal, location, and food. We consider each
of these a cluster and compare them to clusters com-
posed of the top ten non-polysemous words (accord-
ing to WordNet) in each of the classes generated
by both the baseline (LDA) and our model (cLDA).
Words not in both sets of clusters are removed. The
result of this evaluation, compared with six cluster-
ing metrics, is shown in Table 2. For descriptions of
NMI, Rand, and cluster F-measure, see Manning et
al. (2008); for the B3 metrics (Cluster and Element),
see Bagga and Baldwin (1998); for the MUC met-
ric, see Vilain et al (1995). Each metric has differ-
ent strengths and biases in regards to the number and
distribution of clusters, so all are provided to give a
general picture of class induction performance.
The best performing model on all metrics is cLDA
with induced predicates using 10 classes. However,
as the number of classes is increased and the gran-
ularity of the induced classes becomes more fine-
grained, LDA (predictably) outperforms cLDA on
most metrics. This is consistent with our intuition
986
induced predicates? N Y
# classes 10 25 50 10 25 50
LDA C1 74.4 78.7 80.5 69.7 70.1 73.4
cLDA
C1 80.6 81.2 80.9 76.2 78.4 77.5
C2 75.4 75.9 78.9 73.5 68.3 80.8
C3 67.8 70.8 67.4 70.9 67.4 74.1
Table 3: Accuracy on SemEval-2010 Task 7 data.
that a single-class assumption degrades as the num-
ber of classes increases.
For this evaluation, predicate induction also im-
proved LDA for smaller numbers of classes, but not
to the degree that it improved cLDA. Without pred-
icate induction, LDA outperforms cLDA on all six
metrics for 25 and 50 classes. With predicate in-
duction, LDA outperforms cLDA on only one metric
for 25 classes and five metrics for 50 classes. Thus
the induced predicates do reduce the negative im-
pact caused by the single selected class assumption
for semantic class induction.
7.2 Coercion Detection
For the evaluation of coercion detection, we use
the SemEval-2010 Task 7 data (Pustejovsky et al,
2010). This data uses the most common sense for
each of five predicates (arrive, cancel, deny, fin-
ish, and hear) with a total of 2,070 sentences an-
notated with the argument?s source type (the argu-
ment?s semantic class) and target type (the predi-
cate?s selected class for that argument). We ignore
the actual argument classes and evaluate on the coer-
cion type, which is a selection when the source and
target type match, and a coercion otherwise.
In order to evaluate unsupervised systems on this
data, we use the corresponding training set (1,031
examples) to learn a threshold for coercion detec-
tion. At test time, if the model output is below the
threshold, a coercion is inferred. Otherwise it is con-
sidered a selection. Therefore, the better a model
can rank selections over coercions, the more accu-
rate threshold it will learn. The results for this eval-
uation are shown in Table 3. The baseline for this
task (threshold = 0, or all selections) is 67.4.
The best overall model on this data is cLDA us-
ing the C1 coercion scoring method (Equation (4)).
This method consistently outperforms the baseline
LDA, especially for smaller numbers of classes, per-
forming best with K = 25. The second metric, C2,
was not as reliable. The third metric, C3, performed
poorly on the task. As discussed in Section 3.3, C3
is a direct result of the sampling for the predicate-
argument pair in question and can thus be expected
to perform poorly on rare predicate-argument pairs.
Given that many of the arguments in this data are
rare or unseen in the Gigaword data (e.g., ?cancel
Renault?), C3?s poor performance is understandable.
The use of predicate sense induction based on
tiered clustering to overcome the single-class as-
sumption caused significant degradation in perfor-
mance on this task. Using automatically induced
predicates instead of the surface form caused an av-
erage degradation of 2.6 points across the twelve
tests. A potential explanation for this is that
the evaluated predicates have a single dominant
sense, meaning the single class assumption may be
valid for these predicates (the task-defined selected
classes are: location for arrive, event for cancel and
finish, proposition for deny, and sound for hear).
Therefore it would be interesting to evaluate it on
a set of highly polysemous predicates with multi-
ple dominant senses. Furthermore, the introduction
of predicate sense induction was designed to help
cLDA, and the performance degradation for these
nine tests was not as large as it was for LDA. For
cLDA, C1 had an average degradation of 3.5 points
compared to LDA?s C1 average degradation of 6.5
points. cLDA?s C2 had an average degradation of
only 2.5 points and C3 was actually improved by 2.1
points. This suggests that there is value in assign-
ing different selected classes via sense induction, but
that the two-step approach is not beneficial for these
common predicates. This could be overcome by a
joint approach of inducing predicate classes while
simultaneously detecting coercions, as the presence
of many coercions would be an indicator that more
induced predicates are necessary.
7.3 Logical Metonymy Interpretation
For the evaluation of logical metonymy, we use
both an existing data set and a newly created data
set. Shutova and Teufel (2009) annotated 10 verb-
object logical metonymies from Lapata and Las-
carides (2003) with sense-disambiguated interpreta-
tions and organized the interpretations into clusters
representing different possible meanings. For evalu-
ation purposes we ignore the sense annotations and
clusters and consider all lexical matchings of one
of the annotated interpretations to be correct. The
987
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.381 0.365
LMIND
LDA C1 0.415 0.406 0.383 0.386 0.412 0.395
cLDA
C1 0.408 0.412 0.412 0.407 0.468 0.439
C2 0.415 0.447 0.419 0.414 0.415 0.434
C3 0.416 0.453 0.455 0.395 0.416 0.402
LMTH
LDA C1 0.599 0.568 0.588 0.479 0.520 0.551
cLDA
C1 0.571 0.644 0.751 0.497 0.620 0.708
C2 0.544 0.496 0.633 0.457 0.635 0.660
C3 0.601 0.677 0.767 0.472 0.622 0.571
LMWT
LDA C1 0.383 0.381 0.379 0.365 0.356 0.361
cLDA
C1 0.380 0.387 0.381 0.386 0.377 0.321
C2 0.317 0.342 0.350 0.338 0.340 0.345
C3 0.378 0.370 0.350 0.387 0.382 0.384
Table 4: Mean average precision (MAP) scores on the Shutova and Teufel (2009) data set. The bold items indicate the
best scores with/without induced predicates as well as using/not using a threshold-based interpretation method.
induced predicates? N Y
# classes 10 25 50 10 25 50
LMLL 0.274 0.248
LMIND
LDA C1 0.291 0.286 0.294 0.263 0.267 0.255
cLDA
C1 0.296 0.298 0.285 0.280 0.274 0.288
C2 0.291 0.287 0.288 0.283 0.271 0.285
C3 0.318 0.317 0.333 0.298 0.285 0.307
LMTH
LDA C1 0.478 0.534 0.534 0.414 0.495 0.479
cLDA
C1 0.449 0.504 0.541 0.391 0.495 0.513
C2 0.505 0.478 0.456 0.398 0.429 0.440
C3 0.449 0.496 0.577 0.382 0.439 0.446
LMWT
LDA C1 0.276 0.270 0.271 0.248 0.251 0.249
cLDA
C1 0.271 0.272 0.270 0.257 0.259 0.265
C2 0.274 0.274 0.266 0.250 0.259 0.261
C3 0.271 0.273 0.274 0.253 0.262 0.259
Table 5: Mean average precision (MAP) scores on 100 logical metonymies manually annotated with interpretations.
The bold items indicate the best scores with/without induced predicates as well as using/not using a threshold-based
interpretation method.
data contains an average of 11 interpretations per
metonymy and has a reported 70% recall.
In order to create a larger data set, we identified
100 verb-object logical metonymies, including those
used in Lapata and Lascarides (2003). Three anno-
tators were asked to provide up to five interpreta-
tions for each metonymy (they were not provided
with any verbs from which to choose, only the verb-
object pair). The annotators provided an average of
4.6 interpretations per metonymy. Because our goal
was recall, inter-annotator agreement was necessar-
ily low, and each logical metonymy had an average
of 11.7 unique interpretations. All annotators agreed
on at least one interpretation for 40 metonymies,
while for 14 they had no interpretations in common.7
Since logical metonymy interpretation is usually
evaluated as a ranking task, we score our methods
7 Data available at
http://www.hlt.utdallas.edu/?kirk/data/lmet.zip
using mean average precision (MAP):
MAP = 1Q
Q?
q=1
?N
n=1 prec(n)? rel(n)
interps(q) (12)
Where Q is the number of metonymies evaluated;
N is the number of interpretations ranked; prec(n)
is the precision at rank n; rel(n) = 1 if interpreta-
tion n is valid, 0 otherwise; and interps(q) is the
number of valid interpretations for the metonymy q.
We rank all 4,145 verbs as interpretations except for
those removed by the threshold technique, as they
have a score of zero. This can give LMTH artifi-
cially high MAP scores since it may remove some
valid interpretations that are low-ranking. However,
since a smaller, higher precision list may be useful
for many applications we still consider MAP a valid
metric and indicate both the highest scoring method
and the highest scoring non-threshold method. The
results on the Shutova and Teufel (2009) data are
988
shown in Table 4. The results on our own data are
shown in Table 5.
The scores reported in the Shutova and Teufel
(2009) data are noticeably higher than the data we
annotated. Since the metonymies in our data are a
super-set of those in their data, and since for those
metonymies our annotators provided approximately
the same number of interpretations (110 versus 120),
this likely indicates the remaining metonymies in
our data are more difficult.
In all cases the best reported scores use cLDA.
Unlike coercion detection on the SemEval data, C3
performs very well, achieving the highest scores
when no predicate sense induction is used. Also un-
like coercion detection, LDA scores do not increase
as the number of classes increase. We suspect both
these differences have to do with the fact that the ar-
guments in this data are far more common. Since
LDA is a selectional preference model and its co-
ercion scores correspond roughly to the plausibility
of seeing a predicate-argument pair, it is less able to
distinguish coercions in common arguments.
Of the logical metonymy ranking methods,
LMTH consistently produces the highest MAP
scores. However, as stated before, by using a cut-off
and removing low-ranking valid interpretations, the
MAP score is increased, which might not be applica-
ble to some applications. The best non-thresholded
ranking method is LMIND, which naively combines
the LMLL score with the coercion probability. In
almost every case this beats out LMWT . Upon in-
spection, we observed that the range of scale for the
LMLL scores are very inconsistent. This can make
it difficult to learn a linear model using these scores
as features, and as a result the learned weights were
forced to ignore the coercion score and rely entirely
on LMLL. We attempted other scaling methods,
such as a rank-based method, but these had poor re-
sults as well, so we leave the problem of the super-
vised learning these weights to future work.
Using induced senses did not result in the dras-
tic and consistent degradation in performance seen
on the SemEval data, and the highest non-threshold
result for the Shutova and Teufel (2009) data used
predicate induction. Both metonymy data sets were
limited to the verbs found in Lapata and Lascarides
(2003), which are still quite common (attempt, be-
gin, enjoy, expect, finish, prefer, start, survive, try,
want). However, the verbs used in our data set had a
greater number of WordNet senses attested in a cor-
pus than the SemEval data (an average of 4.4 senses
for our data versus 3.0 senses for the SemEval data).
This suggests the potential value of sense induction
for highly polysemous predicates and further moti-
vates the integration of sense induction within a se-
lectional restriction model.
8 Conclusion
We have presented a novel topic model that ex-
tends an unsupervised selectional preference model
(LDA) to an unsupervised selectional restriction
model (cLDA) using two assumptions. For the first
assumption, that each predicate has a single selected
class, we proposed a predicate induction method to
overcome predicate polysemy. This improved re-
sults for semantic class induction but proved harmful
for detecting coercions on common predicates with
a single, dominant sense. For the second assump-
tion, that the selected class can be inferred from the
data, we proposed a sampling method based on the
classes of the predicate?s arguments. Superior per-
formance on coercion detection shows the merit of
this assumption.
Additionally, we proposed methods for improving
an existing task, logical metonymy interpretation,
using the learned parameters of our model, showing
positive results.
It is clear that our model may be improved by
more accurate predicate sense induction. To this
end, we plan to develop a model that simultane-
ously induces predicates and learns coercions, using
knowledge of a predicate?s coerciveness to inform
the induction mechanism.
Acknowledgements
We would like to thank Diarmuid ?O Se?aghdha,
Bryan Rink, and Anna Rumshisky for several help-
ful conversations during the course of this work.
We thank Mirella Lapata and Ekaterina Shutova for
making the data from their experiments available
as well as the organizers of SemEval-2010 Task 7
for the associated data set. Additionally, we thank
Srikanth Gullapalli, Aileen McDermott, and Bryan
Rink for annotating the data set used in our exper-
iment. Finally, we thank the anonymous reviewers
for their suggestions on improving this work.
989
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In In Proceedings of
the First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic Coref-
erence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 103?111.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading tea
leaves: How humans interpret topic models. In Neural
Information Processing Systems, pages 1?9.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Language Re-
sources and Evaluation.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl 1):5228.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
1998. Class-based construction of a verb lexicon. In
Proceedings of AAAI/IAAI.
Maria Lapata and Alex Lascarides. 2003. A Probabilistic
Account of Logical Metonymy. Computational Lin-
guistics, 21(2):261?315.
Dekang Lin and Patrick Pantel. 2001. Induction of Se-
mantic Classes from Natural Language Text. In Pro-
ceedings of ACM SIGKDD Conference on Knowledge
Discovery and Data Mining, pages 317?322.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit.
Diarmuid ?O Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 435?444.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth Edi-
tion. The LDC Corpus Catalog., LDC2009T13.
James Pustejovsky, Anna Rumshisky, Alex Plotnick,
Elisabetta Jezek, Olga Batiukova, and Valeria Quochi.
2010. SemEval-2010 Task 7: Argument Selection
and Coercion. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 27?32.
Joseph Reisinger and Raymond J. Mooney. 2010. A
Mixture Model with Sharing for Lexical Semantics. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1173?1182.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A La-
tent Dirichlet Allocation method for Selectional Pref-
erences. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
424?434.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
Anna Rumshisky, Victor A. Grinberg, and James Puste-
jovsky. 2007. Detecting selectional behavior of com-
plex types in text. In Fourth International Workshop
on Generative Approaches to the Lexicon.
Ekaterina Shutova and Simone Teufel. 2009. Logical
Metonymy: Discovering Classes of Meaning. In Pro-
ceedings of the CogSci 2009 Workshop on Semantic
Space Models.
Ekaterina Shutova. 2009. Sense-based interpretation of
logical metonymy using a statistical method. In Pro-
ceedings of the ACL 2009 Student Workshop.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference, pages
45?52.
990
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 252?255,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
UTDMet: Combining WordNet and Corpus Data for
Argument Coercion Detection
Kirk Roberts and Sanda Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, Texas, USA
{kirk,sanda}@hlt.utdallas.edu
Abstract
This paper describes our system for the
classification of argument coercion for
SemEval-2010 Task 7. We present two ap-
proaches to classifying an argument?s se-
mantic class, which is then compared to
the predicate?s expected semantic class to
detect coercions. The first approach is
based on learning the members of an arbi-
trary semantic class using WordNet?s hy-
pernymy structure. The second approach
leverages automatically extracted seman-
tic parse information from a large corpus
to identify similar arguments by the pred-
icates that select them. We show the re-
sults these approaches obtain on the task
as well as how they can improve a tradi-
tional feature-based approach.
1 Introduction
Argument coercion (a type of metonymy) occurs
when the expected semantic class (relative to the
a predicate) is substituted for an object of a dif-
ferent semantic class. Metonymy is a pervasive
phenomenon in language and the interpretation of
metonymic expressions can impact tasks from se-
mantic parsing (Scheffczyk et al, 2006) to ques-
tion answering (Harabagiu et al, 2005). A seminal
example in metonymy from (Lakoff and Johnson,
1980) is:
(1) The ham sandwich is waiting for his check.
The ARG1 for the predicate wait is typically an
animate, but the ?ham sandwich? is clearly not an
animate. Rather, the argument is coerced to ful-
fill the predicate?s typing requirement. This coer-
cion is allowed because an object that would nor-
mally fulfill the typing requirement (the customer)
can be uniquely identified by an attribute (the ham
sandwich he ordered).
SemEval-2010 Task 7 (?Argument Selection
and Coercion?) (Pustejovsky and Rumshisky,
2009) was designed to evaluate systems that de-
tect such coercions and provide a ?compositional
history? of argument selection relative to the pred-
icate. In order to accomplish this, an argument is
annotated with both the semantic class to which it
belongs (the ?source? type) as well as the class ex-
pected by the predicate (the ?target? type). How-
ever, in the data provided, the target type was un-
ambiguous given the lemmatized predicate, so the
remainder of this paper discusses source type clas-
sification. The detection of coercion is then sim-
ply performed by checking if the classified source
type and target type are different.
In our system, we explore two approaches with
separate underlying assumptions about how arbi-
trary semantic classes can be learned. In our first
approach, we assume a semantic class can be de-
fined a priori from a set of seed terms and that
WordNet is capable of defining the membership
of that semantic class. We apply the PageRank al-
gorithm in order to weight WordNet synsets given
a set of seed concepts. In our second approach,
we assume that arguments in the same semantic
class will be selected by similar verbs. We apply a
statistical test to determine the most representative
predicates for an argument. This approach benefits
from a large corpus from which we automatically
extracted 200 million predicate-argument pairs.
The remainder of this paper is organized as fol-
lows. Section 2 discusses our WordNet-based ap-
proach. Section 3 describes our corpus approach.
Section 4 discusses our experiments and results.
Section 5 provides a conclusion and direction for
future work. Due to space limitations, previous
work is discussed when relevant.
2 PageRanking WordNet Hypernyms
Our first approach assumes that semantic class
members can be defined and acquired a priori.
252
Given a set of seed concepts, we mine WordNet
for other concepts that may be in the same seman-
tic class. Clearly, this approach has both practical
limitations (WordNet does not contain every pos-
sible concept) and linguistic limitations (concepts
may belong to different semantic classes based on
their context). However, given the often vague na-
ture of semantic classes (is a building an ARTI-
FACT or a LOCATION?), access to a weighted list
of semantic class members can prove useful for ar-
guments not seen in the train set.
Using (Esuli and Sebastiani, 2007) as inspira-
tion, we have implemented our own naive ver-
sion of WordNet PageRank. They use sense-
disambiguated glosses provided by eXtended
WordNet (Harabagiu et al, 1999) to link synsets
by starting with positive (or negative) sentiment
concepts in order to find other concepts with pos-
itive (or negative) sentiment values. For our
task, however, hypernymy relations are more ap-
propriate for determining a given synset?s mem-
bership in a semantic class. Hypernymy de-
fines an IS-A relationship between the parent
class (the hypernym) and one of its child classes
(the hyponym). Furthermore, while PageRank as-
sumes directed edges (e.g., hyperlinks in a web
page), we use undirected edges. In this way, if
HYPERNYMOF(A, B), then A?s membership in a
semantic class strengthens B?s and vice versa.
Briefly, the formula for PageRank is:
a
(k)
= ?a
(k?1)
W+ (1 ? ?)e (1)
where a(k) is the weight vector containing weights
for every synset in WordNet at time k; W
i,j
is the
inverse of the total number of hypernyms and hy-
ponyms for synset i if synset j is a hypernym or
hyponym of synset i; e is the initial score vector;
and ? is a tuning parameter. In our implementa-
tion, a(0) is initialized to all zeros; ? is fixed at
0.5; and e
i
= 1 if synset i is in the seed set S,
and zero otherwise. The process is then run until
convergence, defined by |a(k)
i
? a
(k?1)
i
| < 0.0001
for all i.
The result of this PageRank is a weighted list
containing every synset reachable by a hyper-
nym/hyponym relation from a seed concept. We
ran the PageRank algorithm six times, once for
each semantic class, using the arguments in the
train set as seeds. For arguments that are polyse-
mous, we make a first WordNet sense assumption.
Representative examples of the concepts gener-
ated from this approach are shown in Table 1.
ARTIFACT DOCUMENT
funny wagon .377 white paper .342
liquor .353 progress report .342
iced tea .338 screenplay .324
tartan .325 papyrus .313
alpaca .325 pie chart .308
EVENT LOCATION
rock concert .382 heliport .381
rodeo .369 mukataa .380
radium therapy .357 subway station .342
seminar .347 dairy farm .326
pub crawl .346 gateway .320
PROPOSITION SOUND
dibs .363 whoosh .353
white paper .322 squish .353
tall tale .319 yodel .339
commendation .310 theme song .320
field theory .309 oldie .312
Table 1: Some of the concepts (and scores) learned
from applying PageRank to WordNet hypernyms.
3 Leveraging a Large Corpus of
Semantic Parse Annotations
Our second approach assumes that semantic class
members are arguments of similar predicates. As
(Pustejovsky and Rumshisky, 2009) elaborate,
predicates select an argument from a specific se-
mantic class, therefore terms that belong in the
same semantic class should be selected by simi-
lar predicates. However, this assumption is often
violated: type coercion allows predicates to have
arguments outside their intended semantic class.
Our solution to this problem, partially inspired by
(Lapata and Lascarides, 2003), is to collect statis-
tics from an enormous amount of data in order to
statistically filter out these coercions.
The English Gigaword Forth Edition corpus1
contains over 8.5 million documents of newswire
text collected over a 15 year period. We processed
these documents with the SENNA2 (Collobert and
Weston, 2009) suite of natural language tools,
which includes a part-of-speech tagger, phrase
chunker, named entity recognizer, and PropBank
semantic role labeler. We chose SENNA due to its
speed, yet it still performs comparably with many
state-of-the-art systems. Of the 8.5 million doc-
uments in English Gigaword, 8 million were suc-
cessfully processed. For each predicate-argument
pair in these documents, we gathered counts by
argument type and argument head. The head was
determined with simple heuristics from the chunk
parse and parts-of-speech for each argument (ar-
guments consisting of more than three phrase
chunks were discarded). When available, named
entity types (e.g., PERSON, ORGANIZATION, LO-
1LDC2009T13
2http://ml.nec-labs.com/senna/
253
coffee book meeting station report voice
drink write hold own release hear
sip read attend build publish raise
brew publish schedule open confirm give
serve title chair attack issue add
spill sell convene close comment have
smell buy arrange operate submit silence
sell balance call fill deny sound
pour illustrate host shut file lend
buy research plan storm prepare crack
rise review make set voice find
Table 2: Top ten predicates for the most common
word in the train set for the six semantic classes.
CATION) were substituted for heads. This resulted
in over 511 million predicate-argument pairs for
argument types ARG0, ARG1, and ARG2. For this
task, however, we chose only to use ARG1 argu-
ments (direct objects), which resulted in 210 mil-
lion pairs, 7.65 million of which were unique. The
ARG1 argument was chosen because most of the
arguments in the data are direct objects 3.
The ?best? predicates for a given argument are
defined by a ranking based on Fisher?s exact test
(Fisher, 1922):
p =
(a + b)!(c + d)!(a + c)!(b + d)!
n!a!b!c!d!
(2)
where a is the number of times the given argument
was used with the given predicate, b is the number
of times the argument was used with a different
predicate, c is the number of times the predicate
was used with a different argument, d is the num-
ber of times neither the given argument or predi-
cate was used, and n = a+b+c+d. The top ranked
(lowest p) predicates for the most common argu-
ments in the training data are shown in Table 2.
4 Experiments
We have conducted several experiments to test
the performance of the approaches outlined in
Sections 2 and 3 along with additional features
commonly found in information extraction liter-
ature. All experiments were conducted using the
SVMmulticlass support vector machine library4.
4.1 WordNet PageRank
We experimented with the output of our WordNet
PageRank implementation along three separate di-
mensions: (1) which sense to use (since we did
not incorporate a word sense disambiguation sys-
tem), (2) whether to use the highest scoring se-
3The notable exception to this, however, is arrive, where
the data uses the destination argument. In the PropBank
scheme (Palmer et al, 2005), this would correspond to the
ARG4, which usually signifies an end state.
4http://svmlight.joachims.org/svm multiclass.html
mantic class or every class an argument belonged
to, and (3) how to use the weight output by the al-
gorithm. The results of these experiments yielded
a single feature for each class that returns true if
the argument is in that class, regardless of weight.
This resulted in a micro-precision score of 75.6%.
4.2 Gigaword Predicates
We experimented with both (i) the number of pred-
icates to use for an argument and (ii) the score
threshold to use. Ultimately, the Fisher score did
not prove nearly as useful as a classifier as it did
as a ranker. Since the distribution of predicates
for each argument varied significantly, choosing a
high number of predicates would yield good re-
sults for some arguments but not others. However,
because of size of the training data, we were able
to choose the top 5 predicates for each argument
as features and still achieve a reasonable micro-
precision score of 89.6%.
4.3 Other Features
Many other features common in information ex-
traction are well-suited for this task. Given that
SVMs can support millions of features, we chose
to add many features simpler than those previously
described in order to improve the final perfor-
mance of the classifier. These include the lemma
of the argument (both the last word?s lemma and
every word?s lemma), the lemma of the predicate,
the number of words in the argument, the casing of
the argument, the part-of-speech of the argument?s
last word, the WordNet synset and all (recursive)
hypernyms of the argument. Additionally, since
the EVENT class is both the most common and
the most often confused, we introduced two fea-
tures based on annotated resources. The first fea-
ture indicates the most common part-of-speech for
the un-lemmatized argument in the Treebank cor-
pus. This helped classify examples such as think-
ing which was confused with a PROPOSITION for
the predicate deny. Second, we introduced a fea-
ture that indicated if the un-lemmatized argument
was considered an event in the TimeBank cor-
pus (Pustejovsky et al, 2003) at least five times.
This helped to distinguish events such as meet-
ing, which was confused with a LOCATION for the
predicate arrive.
4.4 Ablation Test
We conducted an ablation test using combina-
tions of five feature sets: (1) our WordNet PageR-
254
+WNSH +WNPR +GWPA +EVNT
WORD 89.2 94.2 95.0 95.6 96.1
EVNT 31.1 89.7 89.9 90.8
GWPA 89.6 90.8 91.0
WNPR 75.6 89.4
WNSH 89.0
Table 3: Ablation test of feature sets showing
micro-precision scores.
Precision Recall
Selection vs. Coercion Macro 95.4 95.7Micro 96.3 96.3
Source Type Macro 96.5 95.7Micro 96.1 96.1
Target Type Macro 100.0 100.0Micro 100.0 100.0
Joint Type Macro 85.5 95.2Micro 96.1 96.1
Table 4: Results for UTDMET on SemEval-2010
Task 7.
ank feature (WNPR), (2) our Gigaword Predicates
feature (GWPA), (3) word, lemma, and part-of-
speech features (WORD), (4) WordNet synset and
hypernym features (WNSH), and (5) Treebank and
TimeBank features (EVNT). Of these 25 ? 1 =
31 tests, 15 are shown in Table 3. The Giga-
word Predicates (GWPA) was the best overall fea-
ture, but each feature set ended up helping the fi-
nal score. WordNet PageRank (WNPR) even im-
proved the score when combined WordNet hyper-
nym features (WNSH) despite the fact that they
are heavily related. Ultimately, WordNet PageR-
ank had a greater precision, while the other Word-
Net features had greater recall.
4.5 Task 7 Results
Table 4 shows the official results for UTDMET on
the Task 7 data. The target type was unambigu-
ous given the lemmatized predicate. For classify-
ing selection vs. coercion, we simply checked to
see if the classified source type was the same as
the target type. If this was the case, we returned
selection, otherwise a coercion existed.
5 Conclusion
We have presented two approaches for determin-
ing the semantic class of a predicate?s argument.
The two approaches capture different information
and combine well to classify the ?source? type in
SemEval-2010 Task 7. We showed how this can be
incorporated into a system to detect coercions as
well as the argument?s compositional history rel-
ative to its predicate. In future work we plan to
extend this system to more complex tasks such as
when the predicate may be polysemous or unseen
predicates may be encountered.
Acknowledgments
The authors would like to thank Bryan Rink for
several insights during the course of this work.
References
Ronan Collobert and Jason Weston. 2009. Deep
Learning in Natural Language Processing. Tutorial
at NIPS.
Andrea Esuli and Fabrizio Sebastiani. 2007. PageR-
anking WordNet Synsets: An Application to Opin-
ion Mining. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics, pages 424?431.
Ronald A. Fisher. 1922. On the interpretation of ?2
from contingency tables, and the calculation of p.
85(1):87?94.
Sanda Harabagiu, George Miller, and Dan Moldovan.
1999. WordNet 2 - A Morphologically and Se-
mantically Enhanced Resource. In Proceedings of
the SIGLEX Workshop on Standardizing Lexical Re-
sources, pages 1?7.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with Inter-
active Question-Answering. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics, pages 205?214.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. University of Chicago Press.
Maria Lapata and Alex Lascarides. 2003. A Proba-
bilistic Account of Logical Metonymy. Computa-
tional Linguistics, 21(2):261?315.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
James Pustejovsky and Anna Rumshisky. 2009.
SemEval-2010 Task 7: Argument Selection and Co-
ercion. In Proceedings of the NAACL HLT Work-
shop on Semantic Evaluations: Recent Achieve-
ments and Future Directions, pages 88?93.
James Pustejovsky, Patrick Hanks, Roser Saur??, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics,
pages 647?656.
Jan Scheffczyk, Adam Pease, and Michael Ellsworth.
2006. Linking FrameNet to the Suggested Upper
Merged Ontology. In Proceedings of Formal Ontol-
ogy in Information Systems, pages 289?300.
255
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 419?424,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UTD-SpRL: A Joint Approach to Spatial Role Labeling
Kirk Roberts and Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083, USA
{kirk, sanda}@hlt.utdallas.edu
Abstract
We present a joint approach for recognizing
spatial roles in SemEval-2012 Task 3. Can-
didate spatial relations, in the form of triples,
are heuristically extracted from sentences with
high recall. The joint classification of spatial
roles is then cast as a binary classification over
the candidates. This joint approach allows for
a rich feature set based on the complete rela-
tion instead of individual relation arguments.
Our best official submission achieves an F1-
measure of 0.573 on relation recognition, best
in the task and outperforming the previous
best result on the same data set (0.500).
1 Introduction
A significant amount of spatial information in natu-
ral language is encoded in spatial relationships be-
tween objects. In this paper, we present our ap-
proach for detecting the special case of spatial re-
lations evaluated in SemEval-2012 Task 3, Spatial
Role Labeling (SpRL) (Kordjamshidi et al, 2012).
This task considers the most common type of spa-
tial relationships between objects, namely those de-
scribed with a spatial preposition (e.g., in, on, over)
or a spatial phrase (e.g., in front of, on the left), re-
ferred to as the spatial INDICATOR. A spatial INDI-
CATOR connects an object of interest (the TRAJEC-
TOR) with a grounding location (the LANDMARK).
Examples of this type of spatial relationship include:
(1) [cars]T parked [in front of]I the [house]L .
(2) [bushes]T1 and small [trees]T2 [on]I the [hill]L .
(3) a huge [column]L with a [football]T [on top]I .
(4) [trees]T [on the right]I . [?]L
SpRL is a type of semantic role labeling (SRL)
(Palmer et al, 2010), where the spatial INDICA-
TOR is the predicate (or trigger) and the TRAJEC-
TOR and LANDMARK are its two arguments. Previ-
ous approaches to SpRL (Kordjamshidi et al, 2011)
have largely followed the commonly employed SRL
pipeline: (1) find predicates (i.e., the INDICATOR),
(2) recognize the predicate?s syntactic constituents,
and (3) classify the constituent?s role (i.e., TRA-
JECTOR, LANDMARK, or neither). The problem
with this approach is that arguments are considered
largely in isolation. Consider the following:
(5) there is a picture on the wall above the bed.
This sentence contains three objects (picture, wall,
and bed) and two INDICATORs (on and above).
Since the most common spatial relation pattern is
simply trajector-indicator-landmark (as in Examples
(1) and (2)), the triple wall-above-bed is a likely can-
didate relation. However, the semantics of these ob-
jects invalidates the relation (i.e., walls are beside
beds, ceilings are above them). Instead the correct
relation is picture-above-bed because the preposi-
tion above syntactically attaches to picture instead
of wall. Prepositional attachment, however, is a dif-
ficult syntactic problem solved largely through the
use of semantics, so an understanding of the con-
sistency of spatial relationships plays an important
role in their recognition. Consistency checking is
not possible under a pipeline approach that classifies
whether wall as the TRAJECTOR without any knowl-
edge of its LANDMARK.
We therefore propose an alternative to this
pipeline approach that jointly decides whether a
419
given TRAJECTOR-INDICATOR-LANDMARK triple
expresses a spatial relation. We utilize a high re-
call heuristic for recognizing objects capable of par-
ticipating in a spatial relation as well as a lexicon
of INDICATORs. All possible combinations of these
arguments (including undefined LANDMARKs) are
considered by a binary classifier in order to make a
joint decision. This allows us to incorporate features
based on all three relation elements such as the rela-
tion?s semantic consistency.
2 Joint Classification
2.1 Relation Candidate Selection
Previous joint approaches to SpRL have performed
poorly relative to the pipeline approach (Kord-
jamshidi et al, 2011). However, these approaches
have issues with data imbalance: if every token
could be a TRAJECTOR, LANDMARK, or INDICA-
TOR, then even short sentences may contain thou-
sands of negative relation candidates. Such unbal-
anced data sets are difficult for classifiers to reason
over (Japkowicz and Stephen, 2002). To reduce this
imbalance, we propose high recall heuristics to rec-
ognize candidate elements (INDICATORs, TRAJEC-
TORs, and LANDMARKs). Since INDICATORs are
taken from a closed set of prepositions and a small
set of spatial phrases, we simply use a lexicon con-
structed from the indicators in the training data (e.g.,
on, in front of). Thus, our approach is not capable of
detecting INDICATORs that were unseen in the train-
ing data. The effectiveness of this indicator lexicon
is evaluated in Section 3.2. For TRAJECTORs and
LANDMARKs, we observe that both may be consid-
ered spatial objects, which unlike INDICATORs are
not a closed class of words. Instead, we consider
noun phrase (NP) heads to be spatial objects. To
overcome part-of-speech errors and increase recall,
we incorporate three sources: (1) the NP heads from
a syntactic parse tree (Klein and Manning, 2003),
(2) the NP heads from a chunk parse1, and (3) words
that are marked as nouns in at least 66% of instances
in Treebank (Marcus et al, 1993). This approach
identifies all nouns, not just spatial nouns. But for
the SemEval-2012 Task 3 data, which is composed
of image descriptions, most nouns are spatial ob-
jects and no further refinements are necessary. Fur-
1http://www.surdeanu.name/mihai/bios/
ther heuristics (such as using WordNet (Fellbaum,
1998)) could be used to refine the set of spatial ob-
jects if other domains (such as newswire) were to
be used. Our main emphasis in this step, however,
is recall: by utilizing these heuristics we greatly re-
duce the number of negative instances while remov-
ing very few positive spatial relations. The effective-
ness of our heuristics are evaluated in Section 3.2.
Once all possible spatial INDICATORs and spa-
tial objects are marked, all possible combinations of
these are formed as candidate relations. Addition-
ally, for each spatial object and spatial INDICATOR
pair, an additional candidate relation is formed with
an undefined LANDMARK (such as in Example (4)).
2.2 Classification Framework
Given candidate spatial relations, we utilize a binary
support vector machine (SVM) classifier to indicate
which relation candidates are spatial relations. We
use the LibLINEAR (Fan et al, 2008) SVM imple-
mentation, adjusting the negative outcome weight
from 1.0 to 0.8 (tuned via cross-validation on the
training data). This adjustment sacrifices preci-
sion for recall, but raises the overall F1 score. For
type classification (REGION, DIRECTION, and DIS-
TANCE), we use LibLINEAR as a multi-class SVM
with no weight adjustment in order to maximize ac-
curacy. The features used in both classifiers are dis-
cussed in Sections 2.3 and 2.4.
2.3 Relation Detection Features
The difference between our two official submissions
(supervised1 and supervised2) is that different sets
of features were used to detect spatial relations. The
features for general type classification, discussed in
Section 2.4, were consistent across both submis-
sions. Based on previous approaches to spatial role
labeling, our own initial intuitions, and error analy-
sis, we created over 100 different features, choosing
the best feature set with a greedy forward/backward
automated feature selection technique (Pudil et al,
1994). This greedy method iteratively chooses the
best un-used feature to add to the feature set. At the
end of each iteration, there is a pruning step to re-
move any features made redundant by the addition
of the latest feature.
Before describing the individual features used in
our submission, we first enumerate some basic fea-
420
tures that form the building blocks of many of the
features in our submissions (with sample feature val-
ues from Example (1)):
(BF.1) The TRAJECTOR?s raw string (e.g., cars).
(BF.2) The LANDMARK?s raw string (house).
(BF.3) The INDICATOR?s raw string (in front of).
(BF.4) The TRAJECTOR?s lemma (car).
(BF.5) The LANDMARK?s lemma (house).
(BF.6) The dependency path from the TRAJECTOR to the
INDICATOR (?NSUBJ?PREP). Uses the Stanford
Dependency Parser (de Marneffe et al, 2006).
(BF.7) The dependency path from the INDICATOR to the
LANDMARK (?POBJ).
For BF.2, BF.5, and BF.7, if the relation?s
LANDMARK is undefined, the feature value is sim-
ply undefined. The features for our first submission
(supervised1), in the order they were chosen by the
feature selector, are as follows:
(JF1.1) The concatenation of BF.6, BF.3, and BF.7 (i.e.,
the dependency path from the TRAJECTOR to the
LANDMARK including the INDICATOR?s raw string),
for all spatial objects related to the TRAJECTOR under
consideration via a conjunction dependency relation
(including the TRAJECTOR itself). For instance,
TRAJECTOR1 in Example (2) would have two feature
values: ?CONJ?PREP?POBJ and ?PREP?POBJ.
Since objects connected via a conjunction should
participate in the same relation, this allows the
classifier to overcome the sparsity related to the low
number of training instances containing a conjunction.
(JF1.2) The concatenation of BF.1, BF.3, and BF.2
(cars::in front of::house).
(JF1.3) Whether or not the LANDMARK is part of a term
from the INDICATOR lexicon. Words like front and
side are common LANDMARKs but may also be part
of an INDICATOR as well.
(JF1.4) All the words between the left-most argument in
the relation and the right-most argument (parked, the).
Does not include any word in the arguments.
(JF1.5) The value of BF.7.
(JF1.6) The first word in the INDICATOR.
(JF1.7) The LANDMARK?s WordNet hypernyms.
(JF1.8) The TRAJECTOR?s WordNet hypernyms.
(JF1.9) Whether or not the relative order of the relation
arguments in the text is INDICATOR, LANDMARK,
TRAJECTOR. This order is rare and thus this feature
acts as a negative indicator.
(JF1.10) Whether or not the TRAJECTOR is a
prepositional object (POBJ from the dependency tree)
of a preposition that is not the relation?s INDICATOR
but is in the INDICATOR lexicon. Again, this is a
negative indicator.
(JF1.11) The concatenation of BF.4, BF.3, and BF.5
(car::in front of::house).
(JF1.12) The dependency path from the TRAJECTOR
to the LANDMARK. Differs from JF1.1 because it
does not consider conjunctions or differentiate
between INDICATORs.
(JF1.13) The concatenation of BF.3 and BF.7.
(JF1.14) Whether or not the relation under consideration
has an undefined LANDMARK and the sentence
contains no spatial objects other than the TRAJECTOR
under consideration. This helps to indicate relations
with undefined LANDMARKs in short sentences.
The first feature selected by the automated feature
selector (JF1.1) utilizes conjunctions (e.g., and, or,
either). However, conjunctions are difficult to detect
with high precision, so we decided to perform an-
other round of feature selection without this particu-
lar feature. The chosen features were then submitted
separately (supervised2):
(JF2.1) The same as JF1.2.
(JF2.2) The same as JF1.3.
(JF2.3) The same as JF1.4.
(JF2.4) The same as JF1.13.
(JF2.5) The value of BF.1.
(JF2.6) The same as JF1.5.
(JF2.7) Similar to JF1.1, but only using the concatenation
of BF.6 and BF.3 (i.e., leaving out the dependency
path from the INDICATOR to the LANDMARK).
(JF2.8) The same as JF1.7.
(JF2.9) The same as JF1.8.
(JF2.10) The lexical pattern from the left-most
argument to the right-most argument
(TRAJECTOR parked INDICATOR the LANDMARK).
(JF2.11) The raw string of the preposition in a PREP
dependency relation with the TRAJECTOR if that
preposition is not the relation?s INDICATOR.
(JF2.12) The PropBank role types for each argument in
the relation (TRAJECTOR=A1;INDICATOR=
AM LOC;LANDMARK=AM LOC). Uses SENNA
(Collobert and Weston, 2009) for the PropBank parse.
(JF2.13) The same as JF1.14.
(JF2.14) The concatenation of BF.4, BF.3, and BF.5.
(JF2.15) The same as JF1.10, but with no requirement to
be in the INDICATOR lexicon.
2.4 Type Classification Features
After joint detection of a relation?s arguments, a
separate classifier determines the relation?s general
type. The features used to classify a relation?s gen-
eral type (REGION, DIRECTION, and DISTANCE)
were also selected using an automated feature se-
lector from the same set of features. Both submis-
sions (supervised1 and supervised2) utilized these
421
supervised1 supervised2
Label Precision Recall F1 Precision Recall F1
TRAJECTOR 0.731 0.621 0.672 0.782 0.646 0.707
LANDMARK 0.871 0.645 0.741 0.894 0.680 0.772
INDICATOR 0.928 0.712 0.806 0.940 0.732 0.823
Relation 0.567 0.500 0.531 0.610 0.540 0.573
Relation + Type 0.561 0.494 0.526 0.603 0.534 0.566
Table 1: Official results for submissions.
features. The following features were used for clas-
sifying a spatial relation?s general type:
(TF.1) The last word of the INDICATOR.
(TF.2) The value of BF.3.
(TF.3) The value of BF.5.
(TF.4) The same as JF1.3.
(TF.5) The same as JF2.10.
3 Evaluation
3.1 Official Submission
The official results for both of our submissions is
shown in Table 1. The argument-specific results
for TRAJECTORs, LANDMARKs, and INDICATORs
are difficult to interpret in the joint approach. In a
pipeline method, these usually indicate the perfor-
mance of individual classifiers, but in our approach
these results are simply a derivative of our joint clas-
sification output. The first submission (supervised1)
achieved a triple F1 of 0.531 for relation detection
and 0.526 when the general type is included. Our
second submission (supervised2) performed better,
with an F1 of 0.573 for relation detection and 0.566
when the general type is included. This suggests that
the feature JF1.1, even though it is the best individ-
ual feature, introduces a significant amount of noise.
The only result to compare our official submis-
sions to is that of Kordjamshidi et al (2011), who
utilize a pipeline approach. Their method has a rela-
tion detection F1 of 0.500 (they do not report a score
with general type). We further compare our method
with theirs in Section 4.
3.2 Relation Candidate Evaluation
The heuristics described in Section 2.1 that enable
joint classification were tuned for the training data,
but their recall on the test data places a strict upper
bound on the recall to our overall approach. It is
therefore important to understand the performance
loss that occurs at this step.
Table 2 shows the performance of our heuristics
on the training and test data. The spatial INDICA-
TOR lexicon has perfect recall on the training data
because it was built from this data set. However, it
performs at only 0.951 recall on the test data, as al-
most 5% of the INDICATORs in the test data were not
seen in the training data. Most of these are phrasal
verbs (e.g., sailing over) or include the modifier very
(e.g., to the very left). Our spatial object recognizer
performed better, only dropping from 0.998 (2 er-
rors) to 0.989 (16 errors). Some of these errors re-
sulted from mis-spellings (e.g., housed instead of
houses), non-head spatial objects (mountain from
the NP mountain landscape), NPs containing con-
junctions (trees in two palm trees, lamps and flags,
which gets marked as one simple NP), as well as
parser errors. The significant drop in precision for
both spatial indicators and objects is an additional
concern. This does not indicate the extracted items
were not valid as potential indicators or objects, but
rather that no gold relation contained them. As ex-
plained in Section 4, this is likely caused by the dis-
parity in sentence length: longer sentences result in
more matches, but not necessarily more relations.
As evidence of this, despite the training and test data
containing almost the same number of sentences,
there are 36% more spatial indicators and 20% more
spatial objects in the test set.
3.3 Further Experiments
After the evaluation deadline, the task organizers
provided the gold test data, allowing us to perform
additional experiments. In this process we found
several annotation errors which we needed to fix in
order to process our gold results. These errors were
largely annotations that were given an incorrect to-
ken index, resulting in the annotation text not match-
ing the referenced text. These fixes increased our
performance, shown on Table 3, improving relation
detection for the supervised2 feature set from 0.573
422
# Precision Recall F1
Spatial Train 1,488 0.448 1.000 0.619
Indicators Test 2,335 0.328 0.951 0.487
Spatial Train 2,974 0.448 0.998 0.618
Objects Test 3,704 0.387 0.989 0.556
Table 2: Results of relation candidate selection heuristics.
Data Precision Recall F1
Train/Test 0.644 0.556 0.597
Train/Test -NSI 0.644 0.582 0.611
Train CV 0.824 0.743 0.781
Test CV 0.745 0.639 0.688
Train+Test CV 0.774 0.680 0.724
Table 3: Additional experiments on corrected test data
using the supervised2 data set. -NSI indicates that the
gold spatial INDICATORs that are not in the lexicon are
removed. CV indicates 10-fold cross validation.
to 0.597. We use this updated data set for the follow-
ing experiments. While the results aren?t compara-
ble to other methods, the goal of these experiments is
to analyze our system under various configurations
by their relative performance.
Table 3 also shows a 10-fold cross validation per-
formance on 3 data sets: (1) the training data, (2)
the test data, and (3) both the training and test data.
While our feature set is tuned to the training data,
the test data is clearly more difficult. Section 4 dis-
cusses the differences between the training and test
data that may lead to such a performance reduction.
Since our lexicon of spatial INDICATORs was
built from the training data, our method will not rec-
ognize any relations that use unseen INDICATORs.
To differentiate between how our method performs
on the full test data and just those INDICATORs that
are in the lexicon, we removed the 39 gold relations
with unseen INDICATORs and re-tested the system.
As can be seen in Table 3 (under -NSI), this im-
proves recall by 2.6 points.
3.4 Feature Experiments
To estimate the contribution of our features, we per-
formed an additive experiment to see how each fea-
ture contributes to the overall test score. Table 4
shows the feature contributions based on the order
they were added by the feature selector. For many of
the features the score goes down when added. How-
ever, without these features, the final score would
drop to 0.578, indicating they still provide valuable
information in the context of the other features. Ta-
ble 5 shows performance on the updated test set
Feature Precision Recall F1
JF2.1 0.333 0.156 0.212
+JF2.2 0.347 0.126 0.185
+JF2.3 0.708 0.115 0.197
+JF2.4 0.555 0.294 0.384
+JF2.5 0.636 0.402 0.493
+JF2.6 0.590 0.414 0.486
+JF2.7 0.621 0.553 0.585
+JF2.8 0.614 0.568 0.590
+JF2.9 0.573 0.568 0.571
+JF2.10 0.612 0.547 0.578
+JF2.11 0.625 0.571 0.597
+JF2.12 0.660 0.536 0.592
+JF2.13 0.633 0.573 0.601
+JF2.14 0.642 0.563 0.600
+JF2.15 0.644 0.556 0.597
Table 4: Additive feature experiment results using the su-
pervised2 features. Bold indicates increases in F1 over
the previous feature set.
Feature Precision Recall F1
? 0.644 0.556 0.597
JF2.1 0.627 0.571 0.598
JF2.2 0.629 0.542 0.582
JF2.3 0.540 0.494 0.516
JF2.4 0.591 0.412 0.485
JF2.5 0.631 0.558 0.592
JF2.6 0.657 0.515 0.577
JF2.7 0.636 0.547 0.589
JF2.8 0.641 0.562 0.599
JF2.9 0.678 0.539 0.601
JF2.10 0.607 0.569 0.587
JF2.11 0.640 0.565 0.600
JF2.12 0.646 0.566 0.603
JF2.13 0.646 0.553 0.596
JF2.14 0.618 0.572 0.594
JF2.15 0.642 0.563 0.600
Table 5: Results when individual features from the super-
vised2 submission are removed. Bold indicates improve-
ment when the feature is removed.
when individual features are removed. Here, six fea-
tures that were useful on the training data did not
prove useful on the test data.
4 Discussion
The only available work against which our method
may be compared is that of Kordjamshidi et al
(2011). They propose both a pipeline and joint ap-
proach to SpRL. In their case, their pipeline ap-
proach performs better than their joint approach.
Joint approaches increase data sparsity, so their
greatest value is in the ability to use a richer set of
features that describe the relationships between the
arguments. Kordjamshidi et al (2011) furthermore
423
did not employ heuristics to select relation candi-
dates such as those in Section 2.1. Given this dif-
ference it is difficult to assert that a joint approach
is better with complete certainty, but we believe the
ability to analyze the consistency of the entire rela-
tion provides a significant advantage. Many of our
features (JF2.1, JF2.3, JF2.10, JF2.12, JF2.13, and
JF2.14) were of this joint type.
The drop in performance from the training data to
the test data is significant. The possibility that this is
entirely due to over-training is dispelled by the cross
validation results in Table 3. While different features
might work better on the test set, they are unlikely
to overcome the cross validation difference of 9.3
points (0.781 vs. 0.688). Much of this comes from
the recall limit due to the use of the spatial indicator
lexicon. The other significant cause of performance
degradation seems to be caused by sentence length
and complexity. The test sentences are longer (18 to-
kens vs. 15 tokens in the training data), and have far
more conjunctions (389 and tokens vs. 256), indi-
cating greater syntactic complexity. But the largest
difference is the number of relation candidates gen-
erated by the heuristics: 60,377 relation candidates
from the training data vs. 167,925 relation candi-
dates from the test data (the data sets are roughly the
same size: 600 training and 613 test sentences). The
drop of precision in spatial objects in Table 2 reflects
this as well. Since the number of candidate relations
is quadratic in the number of spatial objects, it is
likely that just a few, long sentences result in this
dramatic increase in the number of candidates.
Since more general domains (such as newswire)
are likely to have this problem as well, one important
area of future work is the reduction of the number of
relation candidates (increasing precision) while still
maintaining near-perfect recall.
5 Conclusion
We have presented a joint approach for recogniz-
ing spatial roles in SemEval-2012 Task 3. Our ap-
proach improves over previous attempts at joint clas-
sification by extracting a more precise (but still ex-
tremely high recall) set of relation candidates, allow-
ing binary classification on a more balanced data set.
This joint approach allowed for a rich set of features
based on all the relation?s arguments. Our best of-
ficial submission achieved an F1-measure of 0.573
on relation recognition, best in the task and outper-
forming all previous work.
Acknowledgments
The authors would like to thank the SemEval-2012
Task 3 organizers for their work preparing the data
set and organizing the task.
References
Ronan Collobert and Jason Weston. 2009. Deep Learn-
ing in Natural Language Processing. Tutorial at NIPS.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference on
Language Resources and Evaluation.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Li-
brary for Large Linear Classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Nathalie Japkowicz and Shaju Stephen. 2002. The class
imbalance problem: A systematic study. Intelligent
Data Analysis, 6(5).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430.
Parisa Kordjamshidi, Martijn Van Otterlo, and Marie-
Francine Moens. 2011. Spatial Role Labeling: To-
wards Extraction of Spatial Relations from Natural
Language. ACM Transactions on Speech and Lan-
guage Processing, 8(3).
Parisa Kordjamshidi, Steven Bethard, and Marie-
Francine Moens. 2012. SemEval-2012 Task 3: Spatial
Role Labeling. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Martha Palmer, Daniel Gildea, and Nianwen Xue. 2010.
Semantic Role Labeling. Morgan and Claypool.
Pavel Pudil, Jana Novovic?ova?, and Josef Kittler. 1994.
Floating search methods in feature selection. Pattern
Recognition Letters, 15:1119?1125.
424
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 461?466,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UTDHLT: COPACETIC System for Choosing Plausible Alternatives
Travis Goodwin, Bryan Rink, Kirk Roberts, Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas Dallas
Richardson TX, 75080
{travis,bryan,kirk,sanda}@hlt.utdallas.edu
Abstract
The Choice of Plausible Alternatives (COPA)
task in SemEval-2012 presents a series of
forced-choice questions wherein each question
provides a premise and two viable cause or ef-
fect scenarios. The correct answer is the cause
or effect that is the most plausible. This paper
describes the COPACETIC system developed
by the University of Texas at Dallas (UTD) for
this task. We approach this task by casting it
as a classification problem and using features
derived from bigram co-occurrences, TimeML
temporal links between events, single-word po-
larities from the Harvard General Inquirer, and
causal syntactic dependency structures within
the gigaword corpus. Additionally, we show
that although each of these components im-
proves our score for this evaluation, the dif-
ference in accuracy between using all of these
features and using bigram co-occurrence infor-
mation alone is not statistically significant.
1 The Problem
?The surfer caught the wave.? This statement, al-
though almost tautological for human understanding,
requires a considerable depth of semantic reasoning.
What is a surfer? What does it mean to ?catch a
wave?? How are these concepts related? What if
we want to ascertain, given that the surfer caught the
wave, whether the most likely next event is that ?the
wave carried her to the shore? or that ?she paddled her
board into the ocean?? This type of causal and tempo-
ral reasoning requires a breadth of world-knowledge,
often called commonsense understanding.
Question 15 (Find the EFFECT)
Premise: I poured water on my sleeping friend.
Alternative 1: My friend awoke.
Alternative 2: My friend snored.
Question 379 (Find the CAUSE)
Premise: The man closed the umbrella.
Alternative 1: He got out of the car.
Alternative 2: He approached the building.
Figure 1: An example of each type of question, one target-
ing an effect, and another targeting a cause.
The seventh task of SemEval-2012 evaluates pre-
cisely this type of cogitation. COPA: Choice of Plau-
sible Alternatives presents 1,0001 sets of two-choice
questions (presented as a premise and two alterna-
tives) provided in simple English sentences. The
goal for each question is to choose the most plausible
cause or effect entailed by the premise (the dataset
provided an equal distribution of cause and effect
targetting questions). Additionally, each question is
labeled so as to describe whether the answer should
be a cause or an effect, as indicated in Figure 1.
The topics of these questions were drawn from two
sources:
1. Randomly selected accounts of personal stories
taken from a collection of Internet weblogs (Gor-
don and Swanson, 2009).
2. Randomly selected subject terms from the Li-
brary of Congress Thesaurus for Graphic Mate-
rials (of Congress. Prints et al, 1980).
Additionally, the incorrect alternatives were authored
1This data set was split into a 500 question development (or
training) set and a 500 question test set.
461
Figure 2: Architecture of the COPACETIC System
with the intent of impeding ?purely associative meth-
ods? (Roemmele et al, 2011). The task aims to
evaluate the state of commonsense causal reasoning
(Roemmele et al, 2011).
2 System Architecture
Given a question, such as Question 15 (as shown
in Figure 1), our system selects the most plausible
alternative by using the output of an SVM classifier,
trained on the 500 provided development questions
and tested on the 500 provided test questions. The
classifier operates with features describing informa-
tion extracted from the processing of the question?s
premise and alternatives. As illustrated by Figure 2,
the preprocessing involves part of speech (POS) tag-
ging, and syntactic dependency parsing provided
by the Stanford parser (Klein and Manning, 2003;
Toutanova et al, 2003), multi-word expression detec-
tion using Wikipedia, automatic TimeML annotation
using TARSQI (Verhagen et al, 2005; Pustejovsky
et al, 2003), and Brown clustering as provided in
(Turian, 2010).
The architecture of the COPACETIC system is di-
vided into offline (independent of any question) and
online (question dependent) processing. The online
aspect of our system inspects each question using
an SVM and selects the most likely alternative. Our
system?s offline functions focus on pre-processing
resources so that they may be used by components
of the online aspect of our system. In the next sec-
tion, we describe the offline processing upon which
our system is built, and in the following section, the
online manner in which we evaluate each question.
2.1 Offline Processing
Because the questions presented in this task require
a wealth of commonsense knowledge, we first ex-
tracted commonsense and temporal facts. This sub-
section describes the process of mining this informa-
tion from the fourth edition of the English Gigaword
corpus2 (Parker et al, 2009).
We collected commonsense facts by extracting
cause and effect pairs using twenty-four hand-crafted
patterns. Rather than lexical patterns, we used pat-
terns over syntactic dependency structures in order
to capture the syntactic role each word plays. Fig-
ure 3 illuminates two examples of the dependency
structures encoded by our causal patterns. Causal
Pattern 1 captures all cases of causality indicated by
the verb causes, while Causal Pattern 2 illustrates a
more sophisticated pattern, in which the phrasal verb
brought on indicates causality.
In order to extract this information, we first parsed
the syntactic dependence structure of each sentence
using the Stanford parser (Klein and Manning, 2003).
Next, we loaded each sentence?s dependence tree
2The LDC Catalog number of the English Gigaword Fourth
Edition corpus is LDC2009T13.
462
CAUSAL PATTERN 1:
"causes"
?cause
nsubj
?effect
dobj
CAUSAL PATTERN 2:
"causenb"
?cause
jdsco
"uj"
a
?effect
uco
Figure 3: The dependency structures associated with
the causal patterns: ?cause ?causes? ?effect, and
?cause ?brought on? ?effect.
into the RDF3X (Neumann and Weikum, 2008)
implementation of an RDF3 database. Then, we
represented our dependency structures using in the
SPARQL4query language and extracted cause and
effect pairs by issuing SPARQL queries against the
RDF3X database. We used SPARQL and RDF repre-
sentations because they allowed us to easily represent
and reason over graphical structures, such as those of
our dependency trees.
It has been shown that causality often manifests as
a temporal relation (Bethard, 2008; Bethard and Mar-
tin, 2008). The questions presented in this task are
no exception: many of the alternative-premise pairs
necessitate temporal understanding. For example,
consider question 63 provided in Figure 4.
Question 63 (Find the EFFECT)
Premise: The man removed his coat.
Alternative 1: He entered the house.
Alternative 2: He loosened his tie.
Figure 4: Example question 63, which illustrates the ne-
cessity for temporal reasoning.
3The Resource Description Framework (RDF) is is a spec-
ification from the W3C. Information on RDF is available at
http://www.w3.org/RDF/.
3The SPARQL Query Language is defined at http://www.
w3.org/TR/rdf-sparql-query/. An examples of the
WHERE clause for a SPARQL query associated with the brought
on pattern from Figure 3 is provided below:
{ ?a <nsubj> ?cause ;
<token> "brought" ;
<prep> ?b .
?b <token> "on" ;
<pobj> ?effect . }
In order to extract this temporal information, we
automatically annotated our corpus with TimeML
annotations using the TARSQI Toolkit (Verhagen
et al, 2005). Unfortunately, the events represented
in this corpus were too sparse to use directly. To
mitigate this sparsity, we clustered events using the
3,200 Brown clusters5 described in (Turian, 2010).
After all such offline processing has been com-
pleted, we incorporate the knowledge encoded by
this processing in the online components of our sys-
tem (online preprocessing, and feature extraction) as
described in the following section.
2.2 Online Processing
We cast the task of selecting the most plausible al-
ternative as a classification problem, using a support
vector machine (SVM) supervised classifier (using
a linear kernel). To this end, we pre-process each
question for lexical information. We extract parts
of speech (POS) and syntactic dependencies using
the Stanford CoreNLP parser (Klein and Manning,
2003; Toutanova et al, 2003). Stopwords are re-
moved using a manually curated list of one hundred
and one common stopwords; non-content words (de-
fined as words whose POS is not a noun, verb, or
adjective) are also discarded. Additionally, we ex-
tract multi-word expressions (noun collocations6 and
phrasal verbs7). Finally, in order to utilize our of-
fline TimeML annotations, we extract events using
POS. Examples of the retained content words are
underlined in Figures 5, 6, 7 and 8.
After preprocessing each question, we convert
it into two premise-alternative pairs (PREMISE-
ALTERNATIVE1, and PREMISE-ALTERNATIVE2).
For each of these pairs, we attempt to form a bridge
from the causal sentence to the effect sentence, with-
out distinction over whether the cause or effect origi-
nated from the premise or the alternative. This bridge
is provided by four measures, or features, described
in the following section.
5These clusters are available at http://metaoptimize.
com/projects/wordreprs/.
6These were detected using a list of English Wikipedia ar-
ticle titles available at http://dumps.wikimedia.org/
backup-index.html.
7Phrasal verbs were determined using a list avail-
able at http://www.learn-english-today.com/
phrasal-verbs/phrasal-verb-list.htm.
463
3 The Features of the COPACETIC
System
In determining the causal relatedness between a cause
and an effect sentence, we utilize four features. Each
feature calculates a value indicating the perceived
strength of the causal relationship between a cause
and an effect using a different measure of causality.
The four features used by our COPACETIC system
are described in the following subsections.
3.1 Bigram Relatedness
Our first feature measures the degree of relatedness
between all pairs of bigrams (at the token level) in the
cause and effect pair. We do this by calculating the
point-wise mutual Information (PMI) (Fano, 1961)
for all bigram combinations between the candidate
alternative and its premise in the English Gigaword
corpus (Parker et al, 2009) as shown in Equation 1.
PMI(x; y) ? log
p(x, y)
p(x)p(y)
(1)
Under the assumption that distance words are un-
likely to causally influence each other, we only con-
sider co-occurrences within a window of one hundred
tokens when calculating the joint probability of the
PMI. Additionally, we allow for up to two tokens
to occur within a single bigram?s occurrence (e.g.
the phrase pierced her ears would be considered a
match for the bigram pierced ears ). Although these
relaxations skew the values of our calculated PMIs
by artificially lowering the joint probability, we are
only concerned with how the values compare to each
other. Note that because we employ no smoothing,
the PMI of an unseen bigram is set to zero. The max-
imum PMI over all pairs of bigrams is retained as the
value for this feature. Figure 5 illustrates this feature
for Question 495.
3.2 Temporal Relatedness
Although most of the questions in this task focus on
causal relationships, for many questions, the nature
of this causal relationship manifests instead as a tem-
poral one (Bethard and Martin, 2008; Bethard, 2008).
We use temporal link information from TimeML
(Pustejovsky et al, 2005; Pustejovsky et al, 2003)
annotations on our corpus to determine how tempo-
rally related a given cause and effect sentence are.
Question 495 (Find the EFFECT)
Premise: The girl wanted to wear earrings.
Alternative 1: She got her ears pierced.
Alternative 2: She got a tattoo.
Alternative 1 Alternative 2
PMI(wear earrings, pierced ears) = -10.928 PMI(wear earrings, tattoo) = -12.77
PMI(wanted wear, pierced ears) = -13.284 PMI(wanted wear, tattoo) = -14.284
PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762
PMI(girl, pierced ears) = -15.711 PMI(girl, tattoo) = -14.859
Maximum PMI = -10.928 Maximum PMI = -12.77
Figure 5: Example PMI values for bigrams and unigrams
(with content words underlined). Alternative 1 is correctly
chosen as it has largest maxi mum PMI.
This is accomplished by using the point-wise mutual
information (PMI) between all pairs of events from
the cause to the effect (see Equation 1). We define
the relevant probabilities as follows:
? The joint probability (P (x, y)) of a cause and
effect event is defined as the number of times
the cause event participates in a temporal link
ending with the effect event.
? The probability of a cause event (P (x)) is de-
fined as the number of times the cause event
precipitates a temporal link to any event.
? The probability of an effect event (P (y)) is de-
fined as the number of times the effect event
ends a temporal link begun by any event.
We define the PMI to be zero for any unseen pair of
events (and for any pairs involving an unseen event).
The summation of all pairs of PMIs is used as the
value of this feature. Figure 6 shows how this feature
behaves.
Question 468 (Find the CAUSE)
Premise: The dog barked.
Alternative 1: The cat lounged on the couch.
Alternative 2: A knock sounded at the door.
Alternative 1 Alternative 2
PMI(lounge, bark) = 5.60436 PMI(knock, bark) = 5.77867
PMI(sound, bark) = 5.26971
Figure 6: Example temporal PMI values (with content
words underlined). Alternative 2 is correctly chosen as it
has the highest summation.
3.3 Causal Dependency Structures
We attempted to capture the degree of direct causal re-
latedness between a cause sentence and an effect sen-
tence. To determine the strength of this relationship,
464
we considered how often phrases from the cause and
effect sentences occur within a causal dependency
structure. We detect this through the use of twenty-
four8 manually crafted causal patterns (described in
Section 2.1). The alternative that has the maximum
number of matched dependency structures with the
premise is retained as the correct choice. Figure 7
illustrates this feature.
Question 490 (Find the EFFECT)
Premise: The man won the lottery.
Alternative 1: He became rich.
Alternative 2: He owed money.
Alternative 1 Alternative 2
won? rich = 15 won? owed = 5
Figure 7: Example casual dependency matches (with con-
tent words underlined). Alternative 1 is correctly selected
because more patterns extracted ?won? causing ?rich? than
?won? causing ?owed?.
3.4 Polarity Comparison
We observed that many of the questions involve the
dilemma of determining whether a positive premise
is more related to a positive or negative alternative
(and vice-versa). This differs from sentiment analysis
in that rather than determining if a sentence expresses
a negative statement or view, we instead desire the
overall sentimental connotation of a sentence (and
thus of each word). For example, the premise from
Question 494 (Figure 8) is ?the woman became fa-
mous.? Although this sentence makes no positive or
negative claims about the woman, the word ?famous?
? when considered on its own ? implies positive con-
notations.
We capture this information using the Harvard
General Inquirer (Stone et al, 1966). Originally de-
veloped in 1966, the Harvard General Inquirer pro-
vides a mapping from English words to their polarity
(POSITIVE, or NEGATIVE). For example, it de-
notes the word ?abandon? as NEGATIVE, and the
word ?abound? as POSITIVE. We use this informa-
tion by summing the score for all words in a sen-
tence (assigning POSITIVE words a score of 1.0,
NEGATIVE words a score of -1.0, and NEUTRAL or
unseen words a score of 0.0). The difference between
8Twenty-four patterns was deemed sufficient due to time
constraints.
these scores between the cause sentence and the ef-
fect sentence is used as the value of this feature. This
feature is illustrated in Figure 8.
Question 494 (Find the CAUSE)
Premise: The woman became famous.
Alternative 1: Photographers followed her.
Alternative 2: Her family avoided her.
Premise Alternative 1 Alternative 2
famous POSITIVE 1.0 follow NEUTRAL 0.0 avoid NEGATIVE?1.0
photographer NEUTRAL 0.0 family NEUTRAL 0.0
Sum 1.0 Sum 0.0 Sum ?1.0
Figure 8: Example polarity comparison (with content
words underlined). Alternative 1 is correctly chosen as it
has the least difference from the score of the premise.
4 Results
The COPA task of SemEval-2012 provided partici-
pants with 1,000 causal questions, divided into 500
questions for development or training, and 500 ques-
tions for testing. We submitted two systems to the
COPA Evaluation for SemEval-2012, both of which
are trained on the 500 development questions. Our
first system uses only the bigram PMI feature and is
denoted as bigram pmi. Our second system uses
all four features and is denoted as svm combined.
The accuracy of our two systems on the 500 provided
test questions is provided in Table 1 (Gordon et al,
2012). On this task, accuracy is defined as the quo-
tient of dividing the number of questions for which
the correct alternative was chosen by the number of
questions. Although multiple groups registered, ours
were the only submitted results. Note that the differ-
ence in performance between our two systems is not
statistically significant (p = 0.411) (Gordon et al,
2012).
Team ID System ID Score
UTDHLT bigram pmi 0.618
UTDHLT svm combined 0.634
Table 1: Accuracy of submitted systems
The primary hindrance to our approach is in com-
bining each feature ? that is, determining the con-
fidence of each feature?s judgement. Because the
questions vary significantly in their subject matter
and the nature of the causal relationship between
given causes and effects, a single approach is unlikely
465
to satisfy all scenarios. Unfortunately, the problem
of determining which feature best applies to a give
question requires non-trivial reasoning over implicit
semantics between the premise and alternatives.
5 Conclusion
This evaluation has shown that although common-
sense causal reasoning is trivial for humans, it belies
deep semantic reasoning and necessitates a breadth of
world knowledge. Additional progress towards cap-
turing world knowledge by leveraging a large number
of cross-domain knowledge resources is necessary.
Moreover, distilling information not specific to any
domain ? that is, a means of inferring basic and fun-
damental information about the world ? is not only
necessary but paramount to the success of any fu-
ture system desiring to build chains of commonsense
or causal reasoning. At this point, we are merely
approximating such possible distillation.
6 Acknowledgements
We would like to thank the organizers of SemEval-
2012 task 7 for their work constructing the dataset
and overseeing the task.
References
[Bethard and Martin2008] S. Bethard and J.H. Martin.
2008. Learning semantic links from a corpus of parallel
temporal and causal relations. Proceedings of the 46th
Annual Meeting of the ACL-HLT.
[Bethard2008] S Bethard. 2008. Building a corpus of
temporal-causal structure. Proceedings of the Sixth
LREC.
[Fano1961] RM Fano. 1961. Transmission of Information:
A Statistical Theory of Communication.
[Gordon and Swanson2009] A. Gordon and R. Swanson.
2009. Identifying personal stories in millions of weblog
entries. In Third International Conference on Weblogs
and Social Media, Data Challenge Workshop, San Jose,
CA.
[Gordon et al2012] Andrew Gordon, Zornitsa Kozareva,
and Melissa Roemmele. 2012. (2012) SemEval-2012
Task 7: Choice of Plausible Alternatives: An Evalua-
tion of Commonsense Causal Reasoning. In Proceed-
ings of the 6th International Workshop on Semantic
Evaluation (SemEval 2012), Montreal.
[Klein and Manning2003] D. Klein and C.D. Manning.
2003. Accurate unlexicalized parsing. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics-Volume 1, pages 423?430.
Association for Computational Linguistics.
[Neumann and Weikum2008] Thomas Neumann and Ger-
hard Weikum. 2008. RDF-3X: a RISC-style engine for
RDF. Proceedings of the VLDB Endowment.
[of Congress. Prints et al1980] Library
of Congress. Prints, Photographs Division, and
E.B. Parker. 1980. Subject headings used in the library
of congress prints and photographs division. Prints and
Photographs Division, Library of Congress.
[Parker et al2009] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2009. English
Gigaword Fourth Edition.
[Pustejovsky et al2003] J Pustejovsky, J Castano, and
R Ingria. 2003. TimeML: Robust specification of
event and temporal expressions in text. AAAI Spring
Symposium on New Directions in Question-Answering.
[Pustejovsky et al2005] J Pustejovsky, Bob Ingria, Roser
Sauri, Jose Castano, Jessica Littman, Rob Gaizauskas,
Andrea Setzer, G. Katz, and I. Mani. 2005. The speci-
fication language TimeML. The Language of Time: A
Reader.
[Roemmele et al2011] Melissa Roemmele, Cos-
min Adrian Bejan, and Andrew S. Gordon. 2011.
Choice of Plausible Alternatives: An Evaluation of
Commonsense Causal Reasoning. 2011 AAAI Spring
Symposium Series.
[Stone et al1966] P. J. Stone, D.C. Dunphy, and M. S.
Smith. 1966. The General Inquirer: A Computer
Approach to Content Analysis. MIT Press.
[Toutanova et al2003] K. Toutanova, D. Klein, C.D. Man-
ning, and Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proceed-
ings of the 2003 Conference of NAACL-HLT, pages
173?180. Association for Computational Linguistics.
[Turian2010] J Turian. 2010. Word representations: a sim-
ple and general method for semi-supervised learning.
Proceedings of the 48th Annual Meeting of the ACL,
pages 384?394.
[Verhagen et al2005] M Verhagen, I Mani, and R Sauri.
2005. Automating Temporal Annotation with TARSQI.
In Proceedings of the ACL 2005, pages 81?84.
466
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29?37,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Decomposing Consumer Health Questions
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
robertske@nih.gov, {kilicogluh,fiszmanm,ddemner}@mail.nih.gov
Abstract
This paper presents a method for decom-
posing long, complex consumer health
questions. Our approach largely decom-
poses questions using their syntactic struc-
ture, recognizing independent questions
embedded in clauses, as well as coordi-
nations and exemplifying phrases. Addi-
tionally, we identify elements specific to
disease-related consumer health questions,
such as the focus disease and background
information. To achieve this, our approach
combines rank-and-filter machine learning
methods with rule-based methods. Our
results demonstrate significant improve-
ments over the heuristic methods typically
employed for question decomposition that
rely only on the syntactic parse tree.
1 Introduction
Natural language questions provide an intuitive
method for consumers (non-experts) to query for
health-related content. The most intuitive way
for consumers to formulate written questions is
the same way they write to other humans: multi-
sentence, complex questions that contain back-
ground information and often more than one spe-
cific question. Consider the following:
? Will Fabry disease affect a transplanted kidney?
Previous to the transplant the disease was be-
ing managed with an enzyme supplement. Will
this need to be continued? What cautions or ad-
ditional treatments are required to manage the
disease with a transplanted kidney?
This complex question contains three question
sentences and one background sentence. The fo-
cus (Fabry disease) is stated in the first question
but is necessary for a full understanding of the
other questions as well. The background sentence
is necessary to understand the second question:
the anaphor this must be resolved to an enzyme
treatment, and the predicate continue?s implicit ar-
gument that must be re-constructed from the dis-
course (i.e., continue after a kidney transplant).
The final question sentence uses a coordination
to ask two separate questions (cautions and addi-
tional treatments). A decomposition of this com-
plex question would then result in four questions:
1. Will Fabry disease affect a transplanted kidney?
2. Will enzyme treatment for Fabry disease need to
be continued after a kidney transplant?
3. What cautions are required to manage Fabry
disease with a transplanted kidney?
4. What additional treatments are required to man-
age Fabry disease with a transplanted kidney?
Each question above could be independently an-
swered by a question answering (QA) system.
While previous work has discussed methods for
resolving co-reference and implicit arguments in
consumer health questions (Kilicoglu et al., 2013),
it does not address question decomposition.
In this work, we propose methods for auto-
matically recognizing six annotation types use-
ful for decomposing consumer health questions.
These annotations distinguish between sentences
that contain questions and background informa-
tion. They also identify when a question sentence
can be split in multiple independent questions, and
29
when they contain optional or coordinated infor-
mation embedded within a question.
For each of these decomposition annotations,
we propose a combination of machine learning
(ML) and rule based methods. The ML methods
largely take the form of a 3-step rank-and-filter
approach, where candidates are generated, ranked
by an ML classifier, then the top-ranked candidate
is passed through a separate ML filtering classi-
fier. We evaluate each of these methods on a set of
1,467 consumer health questions related to genetic
and rare diseases.
2 Background
QA in the biomedical domain has been well-
studied (Demner-Fushman and Lin, 2007; Cairns
et al., 2011; Cao et al., 2011) as a means for re-
trieving medical information. This work has typ-
ically focused, however, on questions posed by
medical professionals, and the methods proposed
for question analysis generally assume a single,
concise question. For example, Demner-Fushman
and Abhyankar (2012) propose a method for ex-
tracting frames from queries for the purpose of
cohort retrieval. Their method assumes syntactic
dependencies exist between the necessary frame
elements, and is thus not well-suited to handle
long, multi-sentence questions. Similarly, Ander-
sen et al. (2012) proposes a method for converting
a concise question into a structured query. How-
ever, many medical questions require background
information that is difficult to encode in a single
question sentence. Instead, it is often more natural
to ask multiple questions over several sentences,
providing background information to give context
to the questions. Yu and Cao (2008) use a ML
method to recognize question types in professional
health questions. Their method can identify more
than one type per complex question. Without de-
composing the full question into its sub-questions,
however, the type cannot be associated with its
specific span, or with other information specific to
the sub-question. This other information can in-
clude answer types, question focus, and other an-
swer constraints. By decomposing multi-sentence
questions, these question-specific attributes can be
extracted, and the discourse structure of the larger
question can be better understood.
Question decomposition has been utilized be-
fore in open-domain QA approaches, but rarely
evaluated on its own. Lacatusu et al. (2006)
demonstrates how question decomposition can im-
prove the performance of a multi-sentence sum-
marization system. They perform what we refer
to as syntactic question decomposition, where the
syntactic structure of the question is used to iden-
tify sub-questions that can be answered in isola-
tion. A second form of question decomposition is
semantic decomposition, which can semantically
break individual questions apart to answer them
in stages. For instance, the question ?When did
the third U.S. President die?? can be semantically
decomposed ?Who was the third U.S. President??
and ?When did X die??, where the answer to the
first question is substituted into the second. Katz
and Grau (2005) discusses this kind of decompo-
sition using the syntactic structure, though it is not
empirically validated. Hartrumpf (2008) proposes
a decomposition method using only the deep se-
mantic structure. Finally, Harabagiu et al. (2006)
proposes a different type of question decomposi-
tion based on a random walk over similar ques-
tions extracted from a corpus. In our work, we
focus on syntactic question decomposition. We
demonstrate the importance of empirical evalua-
tion of question decomposition, notably the pit-
falls of heuristic approaches that rely entirely on
the syntactic parse tree. Syntactic parsers trained
on Treebank are particularly poor at both analyz-
ing questions (Judge et al., 2006) and coordination
boundaries (Hogan, 2007). Robust question de-
composition methods, therefore, must be able to
overcome many of these difficulties.
3 Consumer Health Question
Decomposition
Our goal is to decompose multi-sentence, multi-
faceted consumer health questions into concise
questions coupled with important contextual in-
formation. To this end, we utilize a set of an-
notations that identify the decomposable elements
and important contextual elements. A more de-
tailed description of these annotations is provided
in Roberts et al. (2014). The annotations are pub-
licly available at our institution website
1
. Here, we
briefly describe each annotation:
(1) BACKGROUND - a sentence indicating useful
contextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-
cates an independent question.
1
http://lhncbc.nlm.nih.gov/project/consumer-health-
question-answering
30
Sentence Splitting 
Request 
Question 
Sentence 
Ignore 
Sentence 
Background 
Sentence 
Candidate Generation 
UMLS 
SVM Candidate Ranking 
Boundary Fixing 
Focus 
Focus Recognition 
Sentence Classification 
Background Classification 
SVM Comorbidity Classification 
SVM Diagnosis Classification 
SVM Family History Classification 
SVM ISF Classification 
SVM Lifestyle Classification 
SVM Symptom Classification 
SVM Test Classification 
Candidate Generation 
SVM Candidate Filter 
Question Recognition 
SVM Sentence Classification 
Question 
Candidate Generation 
SVM Candidate Ranking 
Exemplification Recognition 
Candidate Filter 
Candidate Generation 
SVM Candidate Ranking 
Coordination Recognition 
SVM Candidate Filter Coordination 
Exemplification 
Stanford 
Parser 
WordNet 
SVM Treatment Classification 
Figure 1: Question Decomposition Architecture. Modules with solid green lines indicate machine learn-
ing classifiers. Modules with dotted green lines indicate rule-based classifiers.
(3) COORDINATION - a phrase that spans a set of
decomposable items.
(4) EXEMPLIFICATION - a phrase that spans an
optional item.
(5) IGNORE - a sentence indicating nothing of
value is present.
(6) FOCUS - an NP indicating the theme of the
consumer health question.
Further explanations of each annotation are pro-
vided in Sections 4-9. To convert these annota-
tions into separate, decomposed questions, a sim-
ple set of recursive rules is used. The rules enu-
merate all ways of including one conjunct from
each COORDINATION as well as whether or not
to include the phrase within an EXEMPLIFICA-
TION. These rules must be applied recursively to
handle overlapping annotations (e.g., a COORDI-
NATION within an EXEMPLIFICATION). Our im-
plementation is straight-forward and not discussed
further in this paper. The BACKGROUND and FO-
CUS annotations do not play a direct role in this
process, though they provide important contextual
elements and are useful for co-reference, and are
thus still considered part of the overall decompo-
sition process.
It should also be noted that some questions are
syntactically decomposable, but doing so alters
their original meaning. Consider the following
two question sentences:
? Can this disease be cured or can we only treat
the symptoms?
? Are males or females worse affected?
While the first example contains two ?Can...?
questions and the second example contains the co-
ordination ?males or females?, both questions are
providing a choice between two alternatives and
decomposing them would alter the semantic na-
ture of the original question. In these cases, we do
not consider the questions to be decomposable.
Data We use a set of consumer health ques-
tions collected from the Genetic and Rare Dis-
eases Information Center (GARD), which main-
tains a website
2
with publicly available consumer-
submitted questions and professionally-authored
answers about genetic and rare diseases. We col-
lected 1,467 consumer health questions, consist-
ing of 4,115 sentences, 1,713 BACKGROUND sen-
tences, 37 IGNORE sentences, 2,465 QUESTIONs,
367 COORDINATIONs, 53 EXEMPLIFICATIONs,
and 1,513 FOCUS annotations. Questions with
more than one FOCUS are generally concerned
with the relation between diseases. Further infor-
mation about the corpus and the annotation pro-
cess can be found in Roberts et al. (2014).
System Architecture The architecture of our
question decomposition method is illustrated in
2
http://rarediseases.info.nih.gov/gard
31
Figure 1. To avoid confusion, in the rest of this
paper we refer to a complex consumer health ques-
tion simply as a request. Requests are sent to
the independent FOCUS recognition module (Sec-
tion 4), and then proceed through a pipeline that
includes the classification of sentences (Section 5),
the identification of separate QUESTIONs within
a question sentence (Section 6), the recognition
of COORDINATIONs (Section 7) and EXEMPLIFI-
CATIONs (Section 8), and the sub-classification of
BACKGROUND sentences (Section 9).
Experimental Setup The remainder of this pa-
per describes the individual modules in Figure 1.
For simplicity, we show results on the GARD data
for each task in its corresponding section. In all
cases, experiments are conducted using a 5-fold
cross-validation on the GARD data. The cross-
validation folds are organized at the request level
so that no two items from the same request will be
split between the training and testing data.
4 Identifying the Focal Disease
The FOCUS is the condition that disease-centered
questions are centered around. Many other dis-
eases may be mentioned, but the FOCUS is the dis-
ease of central concern. This is similar to the as-
sumption made about a central disease in Medline
abstracts (Demner-Fushman and Lin, 2007). Of-
ten the FOCUS is stated in the first sentence (typ-
ically a BACKGROUND) of the request while the
questions are near the end. The questions can-
not generally be answered outside the context of
the FOCUS, however, so its identification is a crit-
ical part of decomposition. As shown in Figure 1,
we use a 3-step process: (1) a high-recall method
identifies potential FOCUS diseases in the data, (2)
a support vector machine (SVM) ranks the FO-
CUS candidates, and (3) the highest-ranking can-
didate?s boundary is modified with a set of rules to
better match our annotation standard.
To identify candidates for the FOCUS, we use a
lexicon constructed from UMLS (Lindberg et al.,
1993). UMLS includes very generic terms, such as
disease and cancer, that are too general to exactly
match a FOCUS in our data. We allow these terms
to be candidates so as to not miss any FOCUS that
doesn?t exactly match an entry in UMLS. When
such a general term is selected as the top-ranked
FOCUS, the rules described below are capable of
expanding the term to the full disease name.
To rank candidates, we utilize an SVM (Fan et
E/R P R F
1
1st UMLS Disorder
E 19.6 19.0 19.3
R 28.2 27.4 27.8
SVM
E 56.4 54.7 55.6
R 89.2 86.5 87.9
SVM + Rules
E 74.8 72.5 73.6
R 89.5 86.8 88.1
Table 1: FOCUS recognition results. E = exact
match; R = relaxed match.
al., 2008) with a small number of feature types:
? Unigrams. Identifies generic words such as dis-
ease and syndrome that indicate good FOCUS
candidates, while also recognizing noisy UMLS
terms that are often false positives.
? UMLS semantic group (McCray et al., 2001).
? UMLS semantic type.
? Sentence Offset. The FOCUS is typically in the
first sentence, and is far more likely to be at the
beginning of the request than the end.
? Lexicon Offset. The FOCUS is typically the first
disease mentioned.
During training, the SVM considers any candidate
that overlaps the gold FOCUS to be correct. This
enables our approach to train on FOCUS examples
that do not perfectly align with a UMLS concept.
At test time, all candidates are classified, ranked
by the classifier?s confidence, and the top-ranked
candidate is considered the FOCUS.
As mentioned above, there are differences be-
tween how a FOCUS is annotated in our data and
how it is represented in the UMLS. We therefore
use a series of heuristics to alter the boundary to a
more usable FOCUS after it is chosen by the SVM.
The rules are applied iteratively to widen the FO-
CUS boundary until it cannot be expanded any fur-
ther. If a generic disease word is the only token
in the FOCUS, we add the token to the left. Con-
versely, if the token on the right is a generic dis-
ease word, it is added as well. If the word to the
left is capitalized, it is safe to assume it is part of
the disease?s name and so it is added as well. Fi-
nally, several rules recognize the various ways in
which a disease sub-type might be specified (e.g.,
Behcet?s syndrome vascular type, type 2 diabetes,
Charcot-Marie-Tooth disease type 2C).
We evaluate FOCUS recognition with both an
exact match, where the gold and automatic FOCUS
boundaries must line up perfectly, and a relaxed
match, which only requires a partial overlap. As a
baseline, we compare our results against a fully
rule-based system where the first UMLS Disor-
der term in the request is considered the FOCUS.
32
We also evaluate the effectiveness of our bound-
ary altering rules by measuring performance with-
out these rules. The results are shown in Table 1.
The baseline method shows significant problems
in precision and recall. It is not able to ignore
noisy UMLS terms (e.g., aim is both a gene and
a treatment). The SVM improves upon the rule-
based method by over 50 points in F
1
for relaxed
matching. Adding the boundary fixing rules has
little effect on relaxed matching, but greatly im-
proves exact matching: precision and recall are
improved by 18.4 and 17.8 points, respectively.
5 Classifying Sentences
Before precise question boundaries can be rec-
ognized, we first identify sentences that con-
tain QUESTIONs, as distinguished from BACK-
GROUND and IGNORE sentences. It should be
noted that many of the question sentences in our
data are not typical wh-word questions. About
20% of the questions in our data end in a period.
For instance:
? Please tell me more about this condition.
? I was wondering if you could let me know where
I can find more information on this topic.
? I would like to get in contact with other families
that have this illness.
We consider a sentence to be a question if it con-
tains any information request, explicit or implicit.
After sentence splitting, we identify sentences
using a multi-class SVM with three feature types:
? Unigrams with parts-of-speech (POS). Reduces
unigram ambiguities, such as what-WP (a pro-
noun, indicative of a question) versus what-
WDT (a determiner, not indicative).
? Bigrams.
? Parse tree tags. All Treebank tags from the syn-
tactic parse tree. Captures syntactic question
clues such as the phrase tags SQ (question sen-
tence) and WHNP (wh-word noun phrase).
The SVM classifier performs at 97.8%. For com-
parison, an SVM with only unigram features per-
forms at 97.2%. While the unigram model does a
good job classifying sentences, suggesting this is
a very easy task, the improved feature set reduces
the number of errors by 20%.
6 Identifying Questions
QUESTION recognition is the task of identifying
when a conjunction like and joins two independent
questions into a single sentence:
? [What causes the condition]
QUESTION
[and what
treatment is available?]
QUESTION
? [What is this disease]
QUESTION
[and what steps
can I take to protect my daughter?]
QUESTION
We consider the identification of separate QUES-
TIONs within a single sentence to be a differ-
ent task from COORDINATION recognition, which
finds phrases whose conjuncts can be treated in-
dependently. Linguistically, these tasks are quite
similar, but the distinction lies in whether the
right-conjunct syntactically depends on anything
to its left. For instance:
? I would like to learn [more about this condition
and what the prognosis is for a baby born with
it]
COORDINATION
.
Here, the right-conjunct starts with a question
stem (what), but is not a complete, grammatical
question on its own. Alternatively, this could be
re-formed into two separate QUESTIONs:
? [I would like to learn more about this
condition,]
QUESTION
[and what is the prognosis
is for a baby born with it.]
QUESTION
We make this distinction because the QUESTION
recognition task requires one fewer step since the
boundaries extend to the entire sentence, prevent-
ing error propagation from an input module. Fur-
ther, the features that differentiate our QUESTION
and COORDINATION annotations are different.
The two-step process for recognizing QUES-
TIONs includes: (1) a high-recall candidate gener-
ator, and (2) an SVM to eliminate candidates that
are not separate QUESTIONs. The candidates for
QUESTION recognition are simply all the ways a
sentence can be split by the conjunctions and, or,
as well as, and the forward slash (?/?). In our data,
this candidate generation process has a recall of
98.6, as a few examples were missed where candi-
dates were not separated by one of the above con-
junctions.
To filter candidates, we use an SVM with three
features types:
? The conjunction separating the QUESTIONs.
? Unigrams in the left-conjunct. Identifies when
the left-conjunct is not a QUESTION, or when a
question is part of a COORDINATION.
? The right-conjunct?s parse tree tag. Recog-
nizes when the right-conjunct is an independent
clause that may safely be split.
33
P R F
1
QUESTION split recognition
Baseline 24.7 82.4 38.0
SVM 67.7 64.7 66.2
Overall QUESTION recognition
Baseline 87.3 92.8 90.0
SVM 97.7 97.4 97.5
Table 2: QUESTION recognition results.
For evaluation, we measure both the F
1
score
for correct candidates, and the overall F
1
for all
QUESTION annotations (i.e., all QUESTION sen-
tences). We also evaluate a baseline method that
utilizes the parse tree to recognize separate QUES-
TIONs by splitting sentences where a conjunction
separates independent clauses. The results are
shown in Table 2. The baseline method has good
recall for recognizing where a sentence should be
split into multiple QUESTIONs, but it lacks preci-
sion. This is largely because it is unable to differ-
entiate clausal COORDINATIONs such as the above
example, as well as when the left-conjunct is not
actually a separate question. For instance:
? Our grandson was diagnosed recently with this
disease and I am wondering if you could send
me information on it.
The SVM-based method can overcome this prob-
lem by looking at the words in the left-conjunct.
Both methods, however, fail to recognize when
two independent question clauses are asking the
same question but providing alternative answers:
? Will this condition be with him throughout his
life, or is it possible that it will clear up?
While there are methods for handling this issue
for COORDINATION recognition, addressed be-
low, recognizing non-splittable QUESTIONs re-
quires far deeper semantic understanding which
we leave to future work.
7 Identifying Coordinations
COORDINATION recognition is the task of identi-
fying when a conjunction joins phrases within a
QUESTION that can in be separate questions:
? How can I learn more about [treatments and
clinical trials]
COORDINATION
?
? Are [muscle twitching, muscle cramps, and
muscle pain]
COORDINATION
effects of having sil-
icosis?
Unlike QUESTION recognition, the boundaries of
a COORDINATION need to be determined as well
as whether the conjuncts can semantically be split
into separate questions. We thus use a three-step
process for recognizing COORDINATIONs: (1) a
high-recall candidate generator, (2) an SVM to
rank all the candidates for a given conjunction, and
(3) an SVM to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid conjunctions within a QUESTION an-
notation. We use the same four conjunctions as in
QUESTION recognition: and, or, as well as, and
the forward slash. For each of these, all possi-
ble left and right boundaries are generated, so in
a QUESTION with 4 tokens on either side of the
conjunction, there would be 16 candidates. Addi-
tionally, two adjectives separated by a comma and
immediately followed by a noun are considered a
candidate (e.g., ?a [safe, permanent]
COORDINATION
treatment?). In our data, this candidate generation
process has a recall of 98.9, as a few instances ex-
ist in which a conjunction is not used, such as:
? I am looking for any information you have
about heavy metal toxicity, [treatment,
outcomes]
EXEMPLIFICATION+COORDINATION
.
To rank candidates, we use an SVM with the
following feature types:
? If the left-conjunct is congruent with the high-
est node in the syntactic parse tree whose right-
most leaf is also the right-most token in the left-
conjunct. Essentially, this is equivalent to say-
ing whether or not the syntactic parser agrees
with the left-conjunct?s boundary.
? The equivalent heuristic for the right-conjunct.
? If a noun is in both, just the left conjunct, just
the right conjunct, or neither conjunct.
? The Levenshtein distance between the POS tag
sequences for the left- and right-conjuncts.
The first two features encode the information a
rule-based method would use if it relied entirely
on the syntactic parse tree. The remaining features
help the classifier overcome cases where the parser
may be wrong.
At training time, all candidates for a given con-
junction are generated and only the candidate that
matches the gold COORDINATION is considered
a positive example. Additionally, we annotated
the boundaries for negative COORDINATIONs (i.e.,
syntactic coordinations that do not fit our annota-
tion standard). There were 203 such instances in
the GARD data. These are considered gold CO-
ORDINATIONs for boundary ranking only.
To filter the top-ranked candidates, we use an
SVM with several feature types:
34
E/R P R F
1
Baseline
E 28.1 36.5 31.8
R 62.9 75.8 68.7
Rank + Filter
E 38.2 34.8 36.4
R 78.5 69.0 73.5
Table 3: COORDINATION recognition results.
E = exact match; R = relaxed match.
? The conjunction.
? Unigrams in the left-conjunct.
? POS of the first word in both conjuncts. CO-
ORDINATIONs often have the same first POS in
both conjuncts.
? The word immediately before the candidate.
E.g., between is a good negative indicator.
? Unigrams in the question but not the candidate.
? If the candidate takes up almost the entire ques-
tion (all but 3 tokens). Typically, COORDINA-
TIONs are much smaller than the full question.
? If more than one conjunction is in the candidate.
? If a word in the left-conjunct has an antonym
in the right conjunct. Antonyms are recognized
via WordNet (Fellbaum, 1998).
At training time, the positive examples are drawn
from the annotated COORDINATIONs, while the
negative examples are drawn from the 203 non-
gold annotations mentioned above.
In addition to evaluating this method, we
evaluate a baseline method that relies entirely
on the syntactic parse to identify COORDINA-
TION boundaries without filtering. The results
are shown in Table 3. The rank-and-filter ap-
proach shows significant gains over the rule-based
method in precision and F
1
. As can be seen in
the difference between exact and relaxed match-
ing, most of the loss for both the baseline and ML
methods come in boundary detection. Most meth-
ods overly rely upon the syntactic parser, which
performs poorly both on questions and coordina-
tions. The ML method, though, is sometimes able
to overcome this problem.
8 Identifying Exemplifications
EXEMPLIFICATION recognition is the task of iden-
tifying when a phrase provides an optional, exem-
plifying example with a more specific type of in-
formation than that asked by the rest of the ques-
tion. For instance, the following contains both an
EXEMPLIFICATION and a COORDINATION:
? Is there anything out there that can help
him [such as [medications or alternative
therapies]
COORDINATION
]
EXEMPLIFICATION
?
We could consider this to denote 3 questions:
? Is there anything out there that can help him?
? Is there anything out there that can help him
such as medications?
? Is there anything out there that can help him
such as alternative therapies?
In the latter two questions, we consider the phrase
such as to now denote a mandatory constraint on
the answer to each question, whereas in the origi-
nal question it would be considered optional.
EXEMPLIFICATION recognition is similar to
COORDINATION recognition, and its three-step
process is thus similar as well: (1) a high-recall
candidate generator, (2) an SVM to rank all the
candidates for a given trigger phrase, and (3) a set
of rules to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid trigger words and phrases. These in-
clude: especially, including, particularly, specifi-
cally, and such as. For each of these, all possible
right boundaries are generated, thus EXEMPLIFI-
CATIONs have far fewer candidates than COORDI-
NATIONs. Additionally, all phrases within paren-
theses are added as EXEMPLIFICATIONs. In our
data, this candidate generation process has a recall
of 98.1, missing instances without a trigger (see
the example also missed by COORDINATION can-
didate generation in Section 7).
To rank candidates, we use an SVM with the
following feature types:
? If the right-conjunct is the highest parse node
as defined in the COORDINATION boundary fea-
ture.
? If a dependency relation crosses from the right-
conjunct to any word outside the candidate.
? POS of the word after the candidate.
As with COORDINATIONs, we annotated bound-
aries for negative EXEMPLIFICATIONs matching
the trigger words and used them as positive exam-
ples for boundary ranking.
To filter the top-ranked candidates, we use two
simple rules. First, EXEMPLIFICATIONs within
parentheses are filtered if they are acronyms or
acronym expansions. Second, cases such as the
below example are removed by looking at the
words before the candidate:
? I am particularly interested in learning more
about genetic testing for the syndrome.
In addition to evaluating this method, we eval-
uate a baseline method that relies entirely on the
35
E/R P R F
1
Baseline
E 28.9 62.3 39.5
R 39.5 84.9 53.9
Rank + Filter
E 60.8 58.5 59.6
R 80.4 77.4 78.8
Table 4: EXEMPLIFICATION recognition results.
E = exact match; R = relaxed match.
syntactic parser to identify EXEMPLIFICATION
boundaries and performs no filtering. The re-
sults are shown in Table 4. The rank-and-filter
approach shows significant gains over the rule-
based method in precision and F
1
, more than dou-
bling precision for both exact and relaxed match-
ing. There is still a drop in performance when go-
ing from relaxed to exact matching, again largely
due to the reliance on the syntactic parser.
9 Classifying Background Information
BACKGROUND sentences contain contextual in-
formation, such as whether or not a patient has
been diagnosed with the focal disease or what
symptoms they are experiencing. This informa-
tion was annotated at the sentence level, partly be-
cause of annotation convenience, but also because
phrase boundaries are not always clear for medical
concepts (Hahn et al., 2012; Forbush et al., 2013).
A difficult factor in this task, and especially on
the GARD dataset, is that consumers are not al-
ways asking about a disease for themselves. In-
stead, often they ask on behalf of another individ-
ual, often a family member. The BACKGROUND
types are thus annotated based on the person of
interest, who we refer to as the patient (in the lin-
guistic sense). For instance, if a mother has a dis-
ease but is asking about her son (e.g., asking about
the probability of her son inheriting the disease),
that sentence would be a FAMILY HISTORY, as
opposed to a DIAGNOSIS sentence.
The GARD corpus is annotated with eight
BACKGROUND types:
? COMORBIDITY
? DIAGNOSIS
? FAMILY HISTORY
? ISF (information
search failure)
? LIFESTYLE
? SYMPTOM
? TEST
? TREATMENT
ISF sentences indicate previous attempts to find
the requested information have failed, and are a
good signal to the QA system to enable more in-
depth search strategies. LIFESTYLE sentences de-
scribe the patient?s life habits (e.g., smoking, ex-
ercise). Currently, the automatic identification of
Type P R F
1
# Anns
COMORBIDITY 0.0 0.0 0.0 23
DIAGNOSIS 80.8 80.3 80.5 690
FAMILY HISTORY 67.4 38.4 48.9 151
ISF 75.0 65.9 70.1 41
LIFESTYLE 0.0 0.0 0.0 13
SYMPTOM 76.6 48.1 59.1 320
TEST 37.5 4.9 8.7 61
TREATMENT 87.3 35.0 50.0 137
Overall: Micro-F
1
: 67.3 Macro-F
1
: 39.7
Table 5: BACKGROUND results.
BACKGROUND types has not been a major focus
of our effort as no handling exists for it within our
QA system. We report a baseline method and re-
sults here to provide some insight into the diffi-
culty of the task.
BACKGROUND types are a multi-labeling prob-
lem, so we use eight binary classifiers, one for
each type. Each classifier utilizes only unigram
and bigram features. The results for the mod-
els are shown in Table 5. COMORBIDITY and
LIFESTYLE are too rare in the data (23 and 13
instances, respectively) for the classifier to iden-
tify. DIAGNOSIS questions are identified fairly
well because this is the most common type (690
instances) and because of the constrained vocabu-
lary for expressing a diagnosis. The performance
of the rest of the types is largely proportional to
the number of instances in the data, though ISF
performs quite well given only 41 instances.
10 Conclusion
We have presented a method for decomposing
consumer health questions by recognizing six an-
notation types. Some of these types are general
enough to use in open-domain question decom-
position (BACKGROUND, IGNORE, QUESTION,
COORDINATION, EXEMPLIFICATION), while oth-
ers are targeted specifically at consumer health
questions (FOCUS and the BACKGROUND sub-
types). We demonstrate that ML methods can
improve upon heuristic methods relying on the
syntactic parse tree, though parse errors are of-
ten difficult to overcome. Since significant im-
provements in performance would likely require
major advances in open-domain syntactic parsing,
we instead envision further integration of the key
tasks in consumer health question analysis: (1) in-
tegration of co-reference and implicit argument in-
formation, (2) improved identification of BACK-
GROUND types, and (3) identification of discourse
relations within questions to further leverage ques-
tion decomposition.
36
Acknowledgements
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health. We would
additionally like to thank Stephanie M. Morri-
son and Janine Lewis for their help accessing the
GARD data.
References
Ulrich Andersen, Anna Braasch, Lina Henriksen,
Csaba Huszka, Anders Johannsen, Lars Kayser,
Bente Maegaard, Ole Norgaard, Stefan Schulz, and
J?urgen Wedekind. 2012. Creation and use of Lan-
guage Resources in a Question-Answering eHealth
System. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 2536?2542.
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,
James H. Martin, Martha S. Palmer, Wayne H. Ward,
and Guergana K. Savova. 2011. The MiPACQ Clin-
ical Question Answering System. In Proceedings of
the AMIA Annual Symposium, pages 171?180.
YongGang Cao, Feifan Liu, Pippa Simpson, Lamont
Antieau, Andrew Bennett, James J. Cimino, John
Ely, and Hong Yu. 2011. AskHERMES: An on-
line question answering system for complex clini-
cal questions. Journal of Biomedical Informatics,
44:277?288.
Dina Demner-Fushman and Swapna Abhyankar. 2012.
Syntactic-Semantic Frames for Clinical Cohort
Identification Queries. In Data Integration in the
Life Sciences, volume 7348 of Lecture Notes in
Computer Science, pages 100?112.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge-Based
and Statistical Techniques. Computational Linguis-
tics, 33(1).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tyler B. Forbush, Adi V. Gundlapalli, Miland N.
Palmer, Shuying Shen, Brett R. South, Guy Divita,
Marjorie Carter, Andrew Redd, Jorie M. Butler, and
Matthew Samore. 2013. ?Sitting on Pins and Nee-
dles?: Characterization of Symptom Descriptions in
Clinical Notes. In AMIA Summit on Clinical Re-
search Informatics, pages 67?71.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik
Faessler, Jenny Traum?uller, Susann Schr?oder, and
Kerstin Hornbostel. 2012. Iterative Refinement
and Quality Checking of Annotation Guidelines ?
How to Deal Effectively with Semantically Sloppy
Named Entity Types, such as Pathological Phenom-
ena. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 3881?3885.
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answer Complex Questions with Random
Walk Models. In Proceedings of the 29th Annual
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 220?227.
Sven Hartrumpf. 2008. Semantic Decomposition
for Question Answering. In Proceedings on the
18th European Conference on Artificial Intelligence,
pages 313?317.
Dierdre Hogan. 2007. Coordinate Noun Phrase Dis-
ambiguation in a Generative Parsing Model. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680?687.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. QuestionBank: Creating a Corpus of Parse-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497?504.
Yarden Katz and Bernardo C. Grau. 2005. Repre-
senting Qualitative Spatial Information in OWL-DL.
Proceedings of OWL: Experiences and Directions.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting Consumer Health
Questions: The Role of Anaphora and Ellipsis. In
Proceedings of the 2013 BioNLP Workshop, pages
54?62.
Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.
2006. Impact of Question Decomposition on the
Quality of Answer Summaries. In Proceedings of
LREC, pages 1147?1152.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Alexa T McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS Semantic Types
for Reducing Conceptual Complexity. In Studies
in Health Technology and Informatics (MEDINFO),
volume 84(1), pages 216?220.
Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil
Kilicoglu, and Dina Demner-Fushman. 2014. An-
notating Question Decomposition on Complex Med-
ical Questions. In Proceedings of LREC.
Hong Yu and YongGang Cao. 2008. Automatically
Extracting Information Needs from Ad Hoc Clini-
cal Questions. In Proceedings of the AMIA Annual
Symposium.
37
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 68?76,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Structuring Operative Notes using Active Learning
Kirk Roberts
?
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
kirk.roberts@nih.gov
Sanda M. Harabagiu
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75080
sanda@hlt.utdallas.edu
Michael A. Skinner
University of Texas Southwestern Medical Center
Children?s Medical Center of Dallas
Dallas, TX 75235
michael.skinner@childrens.com
Abstract
We present an active learning method for
placing the event mentions in an operative
note into a pre-specified event structure.
Event mentions are first classified into ac-
tion, peripheral action, observation, and
report events. The actions are further clas-
sified into their appropriate location within
the event structure. We examine how uti-
lizing active learning significantly reduces
the time needed to completely annotate a
corpus of 2,820 appendectomy notes.
1 Introduction
Operative reports are written or dictated after ev-
ery surgical procedure. They describe the course
of the operation as well as any abnormal find-
ings in the surgical process. Template-based and
structured methods exist for recording the opera-
tive note (DeOrio, 2002), and in many cases have
been shown to increase the completeness of sur-
gical information (Park et al., 2010; Gur et al.,
2011; Donahoe et al., 2012). The use of natural
language, however, is still preferred for its expres-
sive power. This unstructured information is typi-
cally the only vehicle for conveying important de-
tails of the procedure, including the surgical in-
struments, incision techniques, and laparoscopic
methods employed.
The ability to represent and extract the infor-
mation found within operative notes would enable
?
Most of this work was performed while KR was at the
University of Texas at Dallas.
powerful post-hoc reasoning methods about surgi-
cal procedures. First, the completeness problem
may be alleviated by indicating gaps in the sur-
gical narrative. Second, deep semantic similarity
methods could be used to discover comparable op-
erations across surgeons and institutions. Third,
given information on the typical course and find-
ings of a procedure, abnormal aspects of an oper-
ation could be identified and investigated. Finally,
other secondary use applications would be enabled
to study the most effective instruments and tech-
niques across large amounts of surgical data.
In this paper, we present an initial method for
aligning the event mentions within an operative
note to the overall event structure for a procedure.
A surgeon with experience in a particular proce-
dure first describes the overall event structure. A
supervised method enhanced by active learning is
then employed to rapidly build an information ex-
traction model to classify event mentions into the
event structure. This active learning paradigm al-
lows for rapid prototyping while also taking ad-
vantage of the sub-language characteristics of op-
erative notes and the common structure of opera-
tive notes reporting the same type of procedure. A
further goal of this method is to aid in the eval-
uation of unsupervised techniques that can auto-
matically discover the event structure solely from
the narratives. This would enable all the objectives
outlined above for leveraging the unstructured in-
formation within operative notes.
This paper presents a first attempt at this ac-
tive learning paradigm for structuring appendec-
tomy reports. We intentionally chose a well-
understood and relatively simple procedure to en-
68
sure a straight-forward, largely linear event struc-
ture where a large amount of data would be eas-
ily available. Section 3 describes a generic frame-
work for surgical event structures and the particu-
lar structure chosen for appendectomies. Section 4
details the data used in this study. Section 5 de-
scribes the active learning experiment for filling in
this event structure for operative notes. Section 6
reports the results of this experiment. Section 7
analyzes the method and proposes avenues for fur-
ther research. First, however, we outline the small
amount of previous work in natural language pro-
cessing on operative notes.
2 Previous Work
An early tool for processing operative notes was
proposed by Lamiell et al. (1993). They develop
an auditing tool to help enforce completeness in
operative notes. A syntactic parser converts sen-
tences in an operative note into a graph structure
that can be queried to ensure the necessary surgical
elements are present in the narrative. For appen-
dectomies, they could determine whether answers
were specified for questions such as ?What was
the appendix abnormality?? and ?Was cautery or
drains used??. Unlike what we propose, they did
not attempt to understand the narrative structure of
the operative note, only ensure that a small num-
ber of important elements were present. Unfortu-
nately, they only tested their rule-based system on
four notes, so it is difficult to evaluate the robust-
ness and generalizability of their method.
More recently, Wang et al. (2014) proposed a
machine learning (ML) method to extract patient-
specific values from operative notes written in
Chinese. They specifically extract tumor-related
information from patients with hepatic carcinoma,
such as the size/location of the tumor, and whether
the tumor boundary is clear. In many ways this is
similar in purpose to Lamiell et al. (1993) in the
sense that there are operation-specific attributes to
extract. However, while the auditing function pri-
marily requires knowing whether particular items
were stated, their method extracts the particular
values for these items. Furthermore, they em-
ploy an ML-based conditional random field (CRF)
trained and tested on 114 operative notes. The pri-
mary difference between the purpose of these two
methods and the purpose of our method lies in the
attempt to model all the events that characterize a
surgery. Both the work of Lamiell et al. (1993)
and Wang et al. (2014) can be used for complete-
ness testing, and Wang et al. (2014) can be used
to find similar patients. The lack of understand-
ing of the event structure, however, prevents these
methods from identifying similar surgical methods
or unexpected surgical techniques, or from accom-
plishing many other secondary use objectives.
In a more similar vein to our own approach,
Wang et al. (2012) studies actions (a subset of
event mentions) within an operative note. They
note that various lexico-syntactic constructions
can be used to specify an action (e.g., incised, the
incision was carried, made an incision). Like our
approach, they observed sentences can be catego-
rized into actions, perceptions/reports, and other
(though we make this distinction at the event men-
tion level). They adapted the Stanford Parser
(Klein and Manning, 2003) with the Specialist
Lexicon (Browne et al., 1993) similar to Huang
et al. (2005). They do not, however, propose any
automatic system for recognizing and categoriz-
ing actions. Instead, they concentrate on evalu-
ating existing resources. They find that many re-
sources, such as UMLS (Lindberg et al., 1993) and
FrameNet (Baker et al., 1998) have poor coverage
of surgical actions, while Specialist and WordNet
(Fellbaum, 1998) have good coverage.
A notable limitation of their work is that they
only studied actions at the sentence level, look-
ing at the main verb of the independent clause.
We have found in our study that multiple actions
can occur within a sentence, and we thus study ac-
tions at the event mention level. Wang et al. (2012)
noted this shortcoming and provide the following
illustrative examples:
? The patient was taken to the operating room
where general anesthesia was administered.
? After the successful induction of spinal anes-
thesia, she was placed supine on the operat-
ing table.
The second event mention in the first sentence
(administered) and the first event mention in the
second sentence (induction) are ignored in Wang
et al. (2012)?s study. Despite the fact that they
are stated in dependent clauses, these mentions
may be more semantically important to the narra-
tive than the mentions in the independent clauses.
This is because a grammatical relation does not
necessarily imply event prominence. In a further
study, Wang et al. (2013) work toward the creation
of an automatic extraction system by annotating
69
PropBank (Palmer et al., 2005) style predicate-
argument structures on thirty common surgical ac-
tions.
3 Event Structures in Operative Notes
Since operations are considered to be one of the
riskier forms of clinical treatment, surgeons fol-
low strict procedures that are highly structured and
require significant training and oversight. Thus,
a surgeon?s description of a particular operation
should be highly similar with a different descrip-
tion of the same type of operation, even if writ-
ten by a different surgeon at a different hospital.
For instance, the two examples below were writ-
ten by two different surgeons to describe the event
of controlling the blood supply to the appendix:
? The 35 mm vascular Endo stapler device was
fired across the mesoappendix...
? The meso appendix was divided with electro-
cautery...
In these two examples, the surgeons use different
lexical forms (fired vs. divided), syntactic forms
(mesoappendix to the right or left of the EVENT),
different semantic predicate-argument structures
(INSTRUMENT-EVENT-ANATOMICALOBJECT
vs. ANATOMICALOBJECT-EVENT-METHOD),
and even different surgical techniques (stapling or
cautery). Still, these examples describe the same
step in the operation and thus can be mapped to
the same location in the event structure.
In order to recognize the event structure in op-
erative notes, we start by specifying an event
structure to a particular operation (e.g., mastec-
tomy, appendectomy, heart transplant) and create
a ground-truth structure based on expert knowl-
edge. Our goal is then to normalize the event men-
tions within a operative note to the specific surgi-
cal actions in the event structure. While the lex-
ical, syntactic, and predicate-argument structures
vary greatly across the surgeons in our data, many
event descriptions are highly consistent within
notes written by the same surgeon. This is es-
pecially true of events with little linguistic vari-
ability, typically largely procedural but necessary
events that are not the focus of the surgeon?s de-
scription of the operation. An example of low-
variability is the event of placing the patient on
the operating table, as opposed to the event of ma-
nipulating the appendix to prepare it for removal.
Additionally, while there is considerable lexical
variation in how an event is mentioned, the ter-
minology for event mentions is fairly limited, re-
sulting in reasonable similarity between surgeons
(e.g., the verbal description used for the dividing
of the mesoappendix is typically one of the fol-
lowing mentions: fire, staple, divide, separate, re-
move).
3.1 Event Structure Representation
Operative notes contain event mentions of many
different event classes. Some classes correspond
to actions performed by the surgeon, while oth-
ers describe findings, provide reasonings, or dis-
cuss interactions with patients or assistants. These
distinctions are necessary to recognizing the event
structure of an operation, in which we are primar-
ily concerned with surgical actions. We consider
the following event types:
? ACTION: the primary types of events in an
operation. These typically involve physi-
cal actions taken by the surgeon (e.g., cre-
ating/closing an incision, dividing tissue), or
procedural events (e.g., anesthesia, transfer
to recovery). With limited exceptions, AC-
TIONs occur in a strict order and the i
th
AC-
TION can be interpreted as enabling the (i +
1)
th
ACTION.
? P ACTION: the peripheral actions that are
optional, do not occur within a specific place
in the chain of ACTIONs, and are not consid-
ered integral to the event structure. Examples
include stopping unexpected bleeding and re-
moving benign cysts un-connected with the
operation.
? OBSERVATION: an event that denotes the
act of observing a given state. OBSERVA-
TIONs may lead to ACTION (e.g., the ap-
pendix is perforated and therefore needs to
be removed) or P ACTIONs (e.g., a cyst is
found). They may also be elaborations to pro-
vide more details about the surgical method
being used.
? REPORT: an event that denotes a verbal in-
teraction between the surgeon and a patient,
guardian, or assistant (such as obtaining con-
sent for an operation).
The primary class of events that we are interested
in here are ACTIONs. Abstractly, one can view a
type of operation as a directed graph with specified
start and end states. The nodes denote the events,
while the edges denote enablements. An instance
of an operation then can be represented as some
70
Figure 1: Graphical representation of a surgical
procedure with ACTIONs A, B, C , D, E, and F ,
OBSERVATION O, and P ACTION G. (a) strict
surgical graph (only actions), (b) surgical graph
with an observation invoking an action, (c) surgi-
cal graph with an observation invoking a periph-
eral action.
path between the start and end nodes.
In its simplest form, a surgical graph is com-
posed entirely of ACTION nodes (see Figure 1(a)).
It is possible to add expected OBSERVATIONs
that might trigger a different ACTION path (Fig-
ure 1(b)). Finally, P ACTIONs can be represented
as optional nodes in the surgical graph, which may
or may not be triggered by OBSERVATIONs (Fig-
ure 1(c)). This graphical model is simply a con-
ceptual aid to help design the action types. The
model currently plays no role in the automatic
classification. For the remainder of this section
we focus on a relatively limited surgical proce-
dure that can be interpreted as a linear chain of
ACTIONs.
3.2 Appendectomy Representation
Acute appendicitis is a common condition requir-
ing surgical management, and is typically treated
by removing the appendix, either laparoscopically
or by using an open technique. Appendectomies
are the most commonly performed urgent surgi-
cal procedure in the United States. The procedure
is relatively straight-forward, and the steps of the
procedure exhibit little variation between differ-
ent surgeons. The third author (MS), a surgeon
with more than 20 years of experience in pedi-
atric surgery, provided the following primary AC-
TIONs:
? APP01: transfer patient to operating room
? APP02: place patient on table
? APP03: anesthesia
? APP04: prep
? APP05: drape
? APP06: umbilical incision
? APP07: insert camera/telescope
? APP08: insert other working ports
? APP09: identify appendix
? APP10: dissect appendix away from other
structures
? APP11: divide blood supply
? APP12: divide appendix from cecum
? APP13: place appendix in a bag
? APP14: remove bag from body
? APP15: close incisions
? APP16: wake up patient
? APP17: transfer patient to post-anesthesia
care unit
In the laparoscopic setting, each of these actions is
a necessary part of the operation, and most should
be recorded in the operative note. Additionally,
any number of P ACTION, OBSERVATION, and
REPORT events may be interspersed.
4 Data
In accordance with generally accepted medical
practice and to comply with requirements of The
Joint Commission, a detailed report of any surgical
procedure is placed in the medical record within
24 hours of the procedure. These notes include the
preoperative diagnosis, the post-operative diagno-
sis, the procedure name, names of surgeon(s) and
assistants, anesthetic method, operative findings,
complications (if any), estimated blood loss, and a
detailed report of the conduct of the procedure. To
ensure accuracy and completeness, such notes are
typically dictated and transcribed shortly after the
procedure by the operating surgeon or one of the
assistants.
To obtain the procedure notes for this study,
The Children?s Medical Center (CMC) of Dal-
las electronic medical record (EMR) was queried
for operative notes whose procedure contained the
word ?appendectomy? (CPT codes 44970, 44950,
44960) for a preoperative diagnosis of ?acute ap-
pendicitis? (ICD9 codes 541, 540.0, 540.1). At
the time of record acquisition, the CMC EMR had
been in operation for about 3 years, and 2,820
notes were obtained, having been completed by 12
pediatric surgeons. In this set, there were 2,757
71
Surgeon Notes Events Words
surgeon
1
8 291 2,305
surgeon
2
311 16,379 134,748
surgeon
3
143 6,897 57,797
surgeon
4
400 8,940 62,644
surgeon
5
391 15,246 114,684
surgeon
6
307 9,880 77,982
surgeon
7
397 10,908 74,458
surgeon
8
34 2,401 20,391
surgeon
9
2 100 973
surgeon
10
355 9,987 89,085
surgeon
11
380 14,211 135,215
surgeon
12
92 2,417 19,364
Total 2,820 97,657 789,646
Table 1: Overview of corpus by surgeon.
laparoscopic appendectomies and 63 open proce-
dures. The records were then processed automat-
ically to remove any identifying information such
as names, hospital record numbers, and dates. For
the purposes of this investigation, only the sur-
geon?s name and the detailed procedure note were
collected for further study. Owing to the complete
anonymity of the records, the study received an ex-
emption from the University of Texas Southwest-
ern Medical Center and CMC Institutional Review
Boards. Table 1 contains statistics about the distri-
bution of notes by surgeon in our dataset.
5 Active Learning Framework
Active learning is becoming a more and more
popular framework for natural language annota-
tion in the biomedical domain (Hahn et al., 2012;
Figueroa et al., 2012; Chen et al., 2013a; Chen et
al., 2013b). In an active learning setting, instead of
performing manual annotation separate from auto-
matic system development, an existing ML classi-
fier is employed to help choose which examples
to annotate. Thus, human annotators can focus on
examples that would prove difficult for a classifier,
which can dramatically reduce overall annotation
time. However, active learning is not without pit-
falls, notably sampling bias (Dasgupta and Hsu,
2008), re-usability (Tomanek et al., 2007), and
class imbalance (Tomanek and Hahn, 2009). In
our work, the purpose of utilizing an active learn-
ing framework is to produce a fully-annotated cor-
pus of labeled event mentions in as small a period
of time as possible. To some extent, the goal of
full-annotation alleviates some of the active learn-
ing issues discussed above (re-usability and class
imbalance), but sampling bias could still lead to
significantly longer annotation time.
Our goal is to (1) distinguish event mentions in
one of the four classes introduced in Section 3.1
(event type annotation), and (2) further classify ac-
tions into their appropriate location in the event
structure (on this data, appendectomy type anno-
tation). While most active learning methods are
used with the intention of only manually labeling
a sub-set of the data, our goal is to annotate every
event mention so that we may ultimately evaluate
unsupervised techniques on this data. Our active
learning experiment thus proceeds in two paral-
lel tracks: (i) a traditional active learning process
where the highest-utility unlabeled event mentions
are classified by a human annotator, and (ii) a
batch annotation process where extremely simi-
lar, ?easy? examples are annotated in large groups.
Due to small intra-surgeon language variation, and
relatively small inter-surgeon variation due to the
limited terminology, this second process allows us
to annotate large numbers of unlabeled examples
at a time. The batch labeling largely annotates un-
labeled examples that would not be selected by the
primary active learning module because they are
too similar to the already-labeled examples. After
a sufficient amount of time being spent in tradi-
tional active learning, the batch labeling is used
to annotate until the batches produced are insuf-
ficiently similar and/or wrong classifications are
made. After a sufficent number of annotations are
made with the active learning method, the choice
of when to use the active learning or batch anno-
tation method is left to the discretion of the anno-
tator. This back-and-forth is then repeated itera-
tively until all the examples are annotated.
For both the active learning and batch labeling
processes, we use a multi-class support vector ma-
chine (SVM) using a simple set of features:
F1. Event mention?s lexical form (e.g., identified)
F2. Event mention?s lemma (identify)
F3. Previous words (3-the, 2-appendix, 1-was)
F4. Next words (1-and, 2-found, 3-to, 4-be,
5-ruptured)
F5. Whether the event is a gerund (false)
Features F3 and F4 were constrained to only return
words within the sentence.
To sample event mentions for the active learner,
we combine several sampling techniques to ensure
a diversity of samples to label. This meta-sampler
chooses from 4 different samplers with differing
probability p:
1. UNIFORM: Choose (uniformly) an unlabeled
instance (p = 0.1). Formally, let L be the
72
set of manually labeled instances. Then, the
probability of selecting an event e
i
is:
P
U
(e
i
) ? ?(e
i
/? L)
Where ?(x) is the delta function that returns
1 if the condition x is true, and 0 otherwise.
Thus, an unlabeled event has an equal prob-
ability of being selected as every other unla-
beled event.
2. JACCARD: Choose an unlabeled instance bi-
ased toward those whose word context is least
similar to the labeled instances using Jac-
card similarity (p = 0.2). This sampler pro-
motes diversity to help prevent sampling bias.
Let W
i
be the words in e
i
?s sentence. Then
the probability of selecting an event with the
JACCARD sampler is:
P
J
(e
i
) ? ?(e
i
/? L) min
e
j
?L
[(
1?
W
i
?W
j
W
i
?W
j
)
?
]
Here, ? is a parameter to give more weight to
dissimilar sentences (we set ? = 2).
3. CLASSIFIER: Choose an unlabeled instance
biased toward those the SVM assigned low
confidence values (p = 0.65). Formally, let
f
c
(e
i
) be the confidence assigned by the clas-
sifier to event e
i
. Then, the probability of se-
lecting an event with the CLASSIFIER sam-
pler is:
P
C
(e
i
) ? ?(e
i
/? L)(1? f
c
(e
i
))
The SVM we use provides confidence values
largely in the range (-1, 1), but for some very
confident examples this value can be larger.
We therefore constrain the raw confidence
value f
r
(e
i
) and place it within the range [0,
1] to achieve the modified confidence f
c
(e
i
)
above:
f
c
(e
i
) =
max(min(f
r
(e
i
), 1),?1) + 1
2
In this way, f
c
(e
i
) can be guaranteed to be
within [0, 1] and can thus be interpreted as a
probability.
4. MISCLASSIFIED: Choose (uniformly) a la-
beled instance that the SVM mis-classifies
during cross-validation (p = 0.05). Let f(e
i
)
be the classifier?s guess and L(e
i
) be the
manual label for event e
i
. Then the proba-
bility of selecting an event is:
P
M
(e
i
) ? ?(e
i
? L)?(f(e
i
) 6= L(e
i
))
Event Type Precision Recall F
1
ACTION 0.79 0.90 0.84
NOT EVENT 0.75 0.82 0.79
OBSERVATION 0.71 0.57 0.63
P ACTION 0.66 0.40 0.50
REPORT 1.00 0.58 0.73
Active Learning Accuracy: 76.4%
Batch Annotation Accuracy: 99.5%
Table 2: Classification results for event types. Ex-
cept when specified, results are for data annotated
using the active learning method, while the batch
annotation results include all data.
The first annotation was made using the UNIFORM
sampler. For every new annotation, the meta-
sampler chooses one of the above sampling meth-
ods according to the above p values, and that sam-
pler selects an example to annotate. For each se-
lected sample, it is first assigned an event type. If it
is assigned as an ACTION, the annotator further as-
signs its appropriate action type. The CLASSIFIER
and MISCLASSIFIED samplers alternate between
the event type and action type classifiers. These
four samplers were chosen to balance the tra-
ditional active learning approach (CLASSIFIER),
while trying to prevent classifier bias (UNIFORM
and JACCARD), while also allowing mis-labeled
data to be corrected (MISCLASSIFIED). An eval-
uation of the utility of the individual samplers is
beyond the scope of this work.
6 Results
For event type annotation, two annotators single-
annotated 1,014 events with one of five event types
(ACTION, P ACTION, OBSERVATION, REPORT,
and NOT EVENT). The classifier?s accuracy on
this data was 75.9% (see Table 2 for a breakdown
by event type). However, the examples were cho-
sen because they were very different from the cur-
rent labeled set, and thus we would expect them to
be more difficult than a random sampling. When
one includes the examples annotated using batch
labeling, the overall accuracy is 99.5%.
For action type annotation, the same two anno-
tators labeled 626 ACTIONs with one of the 17 ac-
tion types (APP01?APP17). The classifier?s accu-
racy on this data was again a relatively low 72.2%
(see Table 3 for a breakdown by action type).
However, again, these examples were expected to
be difficult for the classifier. When one includes
the examples annotated using batch labeling, the
overall accuracy is 99.4%.
73
Action Type Precision Recall F
1
APP01 0.91 0.77 0.83
APP02 1.00 0.67 0.80
APP03 1.00 0.67 0.80
APP04 0.95 0.95 0.95
APP05 1.00 1.00 1.00
APP06 0.79 0.72 0.76
APP07 0.58 0.58 0.58
APP08 0.65 0.75 0.70
APP09 0.82 0.93 0.87
APP10 0.63 0.73 0.68
APP11 0.50 0.50 0.50
APP12 0.61 0.56 0.58
APP13 0.94 0.94 0.94
APP14 0.71 0.73 0.72
APP15 0.84 0.79 0.82
APP16 0.93 0.81 0.87
APP17 0.84 0.89 0.86
Active Learning Accuracy: 71.4%
Batch Annotation Accuracy: 99.4%
Table 3: Classification results for action types.
7 Discussion
The total time allotted for annotation was approxi-
mately 12 hours, split between two annotators (the
first author and a computer science graduate stu-
dent). Prior to annotation, both annotators were
given a detailed description of an appendectomy,
including a video of a procedure to help asso-
ciate the actual surgical actions with the narrative
description. After annotation, 1,042 event types
were annotated using the active learning method,
90,335 event types were annotated using the batch
method, and 6,279 remained un-annotated. Sim-
ilarly, 658 action types were annotated using the
active learning method, 35,799 action types were
annotated using the batch method, and 21,151 re-
mained un-annotated. A greater proportion of ac-
tions remained un-annotated due to the lower clas-
sifier confidence associated with the task. Event
and action types were annotated in unison, but we
estimate during the active learning process it took
about 25 seconds to annotate each event (both the
event type and the action type if classified as an
ACTION). The batch process enabled the annota-
tion of an average of 3 event mentions per second.
This rapid annotation was made possible by
the repetitive nature of operative notes, especially
within an individual surgeon?s notes. For exam-
ple, the following statements were repeated over
100 times in our corpus:
? General anesthesia was induced.
? A Foley catheter was placed under sterile
conditions.
? The appendix was identified and seemed to
be acutely inflamed.
The first example was used by an individual sur-
geon in 95% of his/her notes, and only used three
times by a different surgeon. In the second exam-
ple, the sentence is used in 77% of the surgeon?s
notes while only used once by another surgeon.
The phrase ?Foley catheter was placed?, however,
was used 133 times by other surgeons. In the con-
text of an appendectomy, this action is unambigu-
ous, and so only a few annotations are needed to
recognize the hundreds of actual occurrences in
the data. Similarly, with the third example, the
phrase ?the appendix was identified? was used in
over 600 operative notes by 10 of the 12 surgeons.
After a few manual annotations to achieve suffi-
cient classification confidence, the batch process
can identify duplicate or near-duplicate events that
can be annotated at once, greatly reducing the time
needed to achieve full annotation.
Unfortunately, the most predictable parts of a
surgeon?s language are typically the least inter-
esting from the perspective of understanding the
critical points in the narrative. As shown in the
examples above, the highest levels of redundancy
are found in the most routine aspects of the op-
eration. The batch annotation, therefore, is quite
biased and the 99% accuracies it achieves cannot
be expected to hold up once the data is fully an-
notated. Conversely, the active learning process
specifically chooses examples that are different
from the current labeled set and thus are more dif-
ficult to classify. Active learning is more likely to
sample from the ?long tail? than the most frequent
events and actions, so the performance on the cho-
sen sample is certainly a lower bound on the per-
formance of a completely annotated data set. If
one assumes the remaining un-annotated data will
be of similar difficulty to the data sampled by the
active learner, one could project an overall event
type accuracy of 97% and an overall action type
accuracy of 89%. This furthermore assumes no
improvements are made to the machine learning
method based on this completed data.
One way to estimate the potential bias in batch
annotation is by observing the differences in the
distributions of the two data sets. Figure 2 shows
the total numbers of action types for both the
active learning and batch annotation portions of
the data. For the most part, the distributions
are similar. APP08 (insert other working ports),
APP10 (dissect appendix away from other struc-
tures), APP11 (divide blood supply), APP12 (di-
74
Figure 2: Frequencies of action types in the active learning (AL) portion of the data set (left vertical axis)
and the batch annotation (BA) portion of the data set (right vertical axis).
vide appendix from cecum), and APP14 (remove
bag from body) are the most under-represented in
the batch annotation data. This confirms our hy-
pothesis that some of the most interesting events
have the greatest diversity in expression.
In Section 2 we noted that a limitation of the an-
notation method of Wang et al. (2012) was that a
sentence could only have one action. We largely
overcame this problem by associating a single sur-
gical action with an event mention. This has one
notable limitation, however, as occasionally a sin-
gle event mention corresponds to more than one
action. In our data, APP11 and APP12 are com-
monly expressed together:
? Next, the mesoappendix and appendix is
stapled
APP11/APP12
and then the appendix is
placed
APP13
in an endobag.
Here, a coordination (?mesoappendix and ap-
pendix?) is used to associate two events (the sta-
pling of the mesoappendix and the stapling of
the appendix) with the same event mention. In
the event extraction literature, this is a well-
understood occurrence, as for instance TimeML
(Pustejovsky et al., 2003) can represent more than
one event with a single event mention. In practice,
however, few automatic TimeML systems handle
such phenomena. Despite this, for our purpose the
annotation structure should likely be amended so
that we can account for all the important actions
in the operative note. This way, gaps in our event
structure will correspond to actual gaps in the nar-
rative (e.g., dividing the blood supply is a critical
step in an appendectomy and therefore needs to fit
within the event structure).
Finally, the data in our experiment comes from
a relatively simple procedure (an appendectomy).
It is unclear how well this method would general-
ize to more complex operations. Most likely, the
difficulty will lie in actions that are highly ambigu-
ous, such as if more than one incision is made.
In this case, richer semantic information will be
necessary, such as the spatial argument that indi-
cates where a particular event occurs (Roberts et
al., 2012).
8 Conclusion
With the increasing availability of electronic oper-
ative notes, there is a corresponding need for deep
analysis methods to understand the note?s narra-
tive structure to enable applications for improving
patient care. In this paper, we have presented a
method for recognizing how event mentions in an
operative note fit into the event structure of the ac-
tual operation. We have proposed a generic frame-
work for event structures in surgical notes with a
specific event structure for appendectomy opera-
tions. We have described a corpus of 2,820 opera-
tive notes of appendectomies performed by 12 sur-
geons at a single institution. With the ultimate goal
of fully annotating this data set, which contains al-
most 100,000 event mentions, we have shown how
an active learning method combined with a batch
annotation process can quickly annotate the ma-
jority of the corpus. The method is not without
its weaknesses, however, and further annotation is
likely necessary.
Beyond finishing the annotation process, our ul-
timate goal is to develop unsupervised methods
for structuring operative notes. This would en-
able expanding to new surgical procedures without
human intervention while also leveraging the in-
creasing availability of this information. We have
shown in this work how operative notes have lin-
guistic characteristics that result in parallel struc-
tures. It is our goal to leverage these characteris-
tics in developing unsupervised methods.
75
Acknowledgments
The authors would like to thank Sanya Peshwani
for her help in annotating the data.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceed-
ings of ACL/COLING.
Allen C. Browne, Alexa T. McCray, and Suresh Srini-
vasan. 1993. The SPECIALIST Lexicon. Tech-
nical Report NLM-LHC-93-01, National Library of
Medicine.
Yukun Chen, Hongxin Cao, Qiaozhu Mei, Kai Zheng,
and Hua Xu. 2013a. Applying active learning to su-
pervised word sense disambiguation in MEDLINE.
J Am Med Inform Assoc, 20:1001?1006.
Yukun Chen, Robert Carroll, Eugenia R. McPeek Hinz,
Anushi Shah, Anne E. Eyler, Joshua C. Denny, ,
and Hua Xu. 2013b. Applying active learning
to high-throughput phenotyping algorithms for elec-
tronic health records data. J Am Med Inform Assoc,
20:e253?e259.
Sanjoy Dasgupta and Daniel Hsu. 2008. Hierarchical
Sampling for Active Learning. In Proceedings of the
International Conference on Maching Learning.
J.K. DeOrio. 2002. Surgical templates for orthopedic
operative reports. Orthopedics, 25(6):639?642.
Laura Donahoe, Sean Bennett, Walley Temple, Andrea
Hilchie-Pye, Kelly Dabbs, EthelMacIntosh, and Ge-
off Porter. 2012. Completeness of dictated oper-
ative reports in breast cancer?the case for synoptic
reporting. J Surg Oncol, 106(1):79?83.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Rosa L. Figueroa, Qing Zeng-Treitler, Long H. Ngo,
Sergey Goryachev, and Eduardo P. Wiechmann.
2012. Active learning for clinical text classification:
is it better than random sampling? J Am Med Inform
Assoc, 19:809?816.
I. Gur, D. Gur, and J.A. Recabaren. 2011. The com-
puterized synoptic operative report: A novel tool in
surgical residency education. Arch Surg, pages 71?
74.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, and
Erik Faessler. 2012. Active Learning-Based Corpus
Annotation ? The PATHOJEN Experience. In Pro-
ceedings of the AMIA Symposium, pages 301?310.
Yang Huang, Henry J Lowe, Dan Klein, and Rus-
sell J Cucina. 2005. Improved Identification of
Noun Phrases in Clinical Radiology Reports Using
a High-Performance Statistical Natural Language
Parser Augmented with the UMLS Specialist Lex-
icon. J Am Med Inform Assoc, 12:275?285.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL,
pages 423?430.
James M Lamiell, Zbigniew M Wojcik, and John
Isaacks. 1993. Computer Auditing of Surgical Op-
erative Reports Written in English. In Proc Annu
Symp Comput Appl Med Care, pages 269?273.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Martha Palmer, Paul Kingsbury, and Daniel Gildea.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?106.
Jason Park, Venu G. Pillarisetty, Murray F. Brennan,
and et al. 2010. Electronic Synoptic Operative Re-
porting: Assessing the Reliability and Completeness
of Synoptic Reports for Pancreatic Resection. J Am
Coll Surgeons, 211(3):308?315.
James Pustejovsky, Jose? Castano, Robert Ingria, Roser
Saur??, Robert Gaizauskas, Andrea Setzer, Graham
Katz, and Dragomir Radev. 2003. TimeML: Ro-
bust Specification of Event and Temporal Expres-
sions in Text. In Proceedings of the Fifth Interna-
tional Workshop on Computational Semantics.
Kirk Roberts, Bryan Rink, Sanda M. Harabagiu,
Richard H. Scheuermann, Seth Toomay, Travis
Browning, Teresa Bosler, and Ronald Peshock.
2012. A Machine Learning Approach for Identi-
fying Anatomical Locations of Actionable Findings
in Radiology Reports. In Proceedings of the AMIA
Symposium.
Katrin Tomanek and Udo Hahn. 2009. Reducing Class
Imbalance during Active Learning for Named Entity
Annotation. In Proceedings of KCAP.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007. An Approach to Text Corpus Construc-
tion which Cuts Annotation Costs and Maintains
Reusability of Annotated Data. In Proceedings of
EMNLP/CoNLL, pages 486?495.
Yan Wang, Serguei Pakhomov, Nora E. Burkart,
James O. Ryan, and Genevieve B. Melton. 2012.
A Study of Actions in Operative Notes. In Proceed-
ings of the AMIA Symposium, pages 1431?1440.
Yan Wang, Serguei Pakhomov, and Genevieve B
Melton. 2013. Predicate Argument Structure
Frames for Modeling Information in Operative
Notes. In Studies in Health Technology and Infor-
matics (MEDINFO), pages 783?787.
Hui Wang, Weide Zhang, Qiang Zeng, Zuofeng Li,
Kaiyan Feng, and Lei Liu. 2014. Extracting impor-
tant information from Chinese Operation Notes with
natural language processing methods. J Biomed In-
form.
76
