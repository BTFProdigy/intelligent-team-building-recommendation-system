Training a Perceptron with Global and Local Features
for Chinese Word Segmentation
Dong Song and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, Canada V5A1S6
{dsong,anoop}@cs.sfu.ca
Abstract
This paper proposes the use of global fea-
tures for Chinese word segmentation. These
global features are combined with local fea-
tures using the averaged perceptron algo-
rithm over N-best candidate word segmenta-
tions. The N-best candidates are produced
using a conditional random field (CRF)
character-based tagger for word segmenta-
tion. Our experiments show that by adding
global features, performance is significantly
improved compared to the character-based
CRF tagger. Performance is also improved
compared to using only local features. Our
system obtains an F-score of 0.9355 on the
CityU corpus, 0.9263 on the CKIP corpus,
0.9512 on the SXU corpus, 0.9296 on the
NCC corpus and 0.9501 on the CTB cor-
pus. All results are for the closed track in
the fourth SIGHAN Chinese Word Segmen-
tation Bakeoff.
1 Introduction
Most natural language processing tasks require that
the input be tokenized into individual words. For
some languages, including Chinese, this is challeng-
ing since the sentence is typically written as a string
of characters without spaces between words. Word
segmentation is the task of recovering the most plau-
sible grouping of characters into words. In this pa-
per, we describe the system we developed for the
fourth SIGHAN Chinese Word Segmentation Bake-
off1. We test our system in the closed track2 for all
five corpora: Academia Sinica (CKIP), City Uni-
versity of Hong Kong (CityU), National Chinese
1Further details at: www.china-language.gov.cn/bakeoff08/
bakeoff-08 basic.html
2We do not use any extra annotation, especially for punctu-
ation, dates, numbers or English letters.
Corpus (NCC), University of Colorado (CTB), and
Shanxi University (SXU).
2 System Description
The architecture of our system is shown in Figure 1.
For each of the training corpora in the bakeoff, we
produce a 10-fold split: in each fold, 90% of the cor-
pus is used for training and 10% is used to produce
an N-best list of candidates. The N-best list is pro-
duced using a character-based conditional random
field (CRF) (Lafferty et al, 2001; Kudo et al, 2004)
tagger. The true segmentation can now be compared
with the N-best list in order to train an averaged per-
ceptron algorithm (Collins, 2002a). This system is
then used to predict the best word segmentation from
an N-best list for each sentence in the test data.
Training Corpus
Weight Vector
N?best Candidates
Training With
Decoding With
Conditional Random
N?best Candidates
Field
Local Features Global Features
Average Perceptron Input Sentence
Average Perceptron
Output
Conditional RandomField
(10?Fold Split)
Figure 1: Outline of the segmentation process
2.1 Learning Algorithm
Given an unsegmented sentence x, the word seg-
mentation problem can be defined as finding the
143
Sixth SIGHAN Workshop on Chinese Language Processing
most probable segmentation F (x) from a set of pos-
sible segmentations of x.
F (x) = argmax
y?GEN(x)
?(x, y) ? w (1)
The set of possible segmentations is given by
GEN(x) and the na??ve method is to first generate all
possible segmented candidates. For a long sentence,
generating those candidates and picking the one with
the highest score is time consuming.
In our approach, N-best candidates for each train-
ing example are produced with the CRF++ soft-
ware (Kudo et al, 2004). The CRF is used as a tag-
ger that tags each character with the following tags:
for each multi-character word, its first character is
given a B (Beginning) tag , its last character is as-
signed an E (End) tag, while each of its remaining
characters is provided an M (Middle) tag. In addi-
tion, for a single-character word, S (Single) is used
as its tag3. Let c0 be the current character, c?1, c?2
are the two preceding characters, and c1, c2 are the
two characters to the right . Using this notation, the
features used in our CRF models are: c0, c?1, c1,
c?2, c2, c?1c0, c0c1, c?1c1, c?2c?1 and c0c2.
We use the now standard method for producing N-
best candidates in order to train our re-ranker which
uses global and local features: 10-folds of training
data are used to train the tagger on 90% of the data
and then produce N-best lists for the remaining 10%.
This process gives us an N-best candidate list for
each sentence and the candidate that is most similar
to the true segmentation, called yb. We map a seg-
mentation y to features associated with the segmen-
tation using the mapping ?(?). The score of a seg-
mentation y is provided by the dot-product ?(y) ?w.
The perceptron algorithm (Fig. 2) finds the weight
parameter vector w using online updates. The pre-
dicted segmentation y
?
i based on the current weight
vector is compared to the the best candidate yb, and
whenever there is a mismatch, the algorithm updates
the parameter vector by incrementing the parame-
ter value for features in yb, and by decrementing the
value for features in y
?
i.
The voted perceptron (Freund and Schapire,
1999) has considerable advantages over the standard
3Note that performance of the CRF tagger could be im-
proved with the use of other tagsets. However, this does not
affect our comparative experiments in this paper.
Inputs: Training Data ?(x1, y1), . . . , (xm, ym)?
Initialization: Set w = 0
Algorithm:
for t = 1, . . . , T do
for i = 1, . . . ,m do
Calculate y
?
i, where
y
?
i = argmax
y?N-best Candidates
?(y) ? w
if y
?
i 6= y
b then
w = w +?(yb) ? ?(y
?
i)
end if
end for
end for
Figure 2: Training using a perceptron algorithm over
N-best candidates.
perceptron. However, due to the computational is-
sues with the voted perceptron, the averaged per-
ceptron algorithm (Collins, 2002a) is used instead.
Rather than using w, we use the averaged weight
parameter ? over the m training examples for future
predictions on unseen data:
? =
1
mT
?
i=1..m,t=1..T
wi,t
In calculating ?, an accumulating parameter vec-
tor ?i,t is maintained and updated using w for each
training example; therefore, ?i,t =
?
wi,t. After
the last iteration, ?i,t/mT produces the final para-
meter vector ?.
When the number of features is large, it is time
consuming to calculate the total parameter ?i,t for
each training example. To reduce the time complex-
ity, we adapted the lazy update proposed in (Collins,
2002b), which was also used in (Zhang and Clark,
2007). After processing each training sentence, not
all dimensions of ?i,t are updated. Instead, an up-
date vector ? is used to store the exact location (i, t)
where each dimension of the averaged parameter
vector was last updated, and only those dimensions
corresponding to features appearing in the current
sentence are updated. While for the last example in
the last iteration, each dimension of ? is updated, no
matter whether the candidate output is correct.
2.2 Feature Templates
The feature templates used in our system include
both local features and global features. For local fea-
tures, we consider twomajor categories: word-based
144
Sixth SIGHAN Workshop on Chinese Language Processing
features and character-based features. Five specific
types of features from (Zhang and Clark, 2007) that
are shown in Table 1 were used in our system. In our
initial experiments, the other features used in (Zhang
and Clark, 2007) did not improve performance and
so we do not include them in our system.
1 word w
2 word bigram w1w2
3 single character word w
4 space-separated characters c1 and c2
5 character bi-gram c1c2 in any word
Table 1: local feature templates. Rows 1, 2 and 3
are word-based and rows 4 and 5 are character-based
features
In our system, we also used two types of global
features per sentence (see Table 2). By global, we
mean features over the entire segmented sentence.4
6 sentence confidence score
7 sentence language model score
Table 2: global feature template
The sentence confidence score is calculated by
CRF++ during the production of the N-best candi-
date list, and it measures how confident each candi-
date is close to the true segmentation.
The sentence language model score for each seg-
mentation candidate is produced using the SRILM
toolkit (Stolcke, 2002) normalized using the formula
P 1/L, where P is the probability-based language
model score and L is the length of the sentence in
words (not in characters). For global features, the
feature weights are not learned using the perceptron
algorithm but are determined using a development
set.
3 Experiments and Analysis
Our system is tested on all five corpora provided in
the fourth SIGHAN Bakeoff, in the closed track.
3.1 Parameter Pruning
First, the value of the parameter N, which is the
maximum number of N-best candidates, was deter-
mined. An oracle procedure proceeds as follows:
80% of the training corpus is used to train the CRF
4It is important to distinguish this kind of global feature
from another type of ?global? feature that either enforces con-
sistency or examines the use of a feature in the entire training
or testing corpus.
model, and produce N-best outputs for each sen-
tence on the remaining 20% of the data. Then these
N candidates are compared with the true segmen-
tation, and for each training sentence, the candidate
closest to the truth is chosen as the final output. Test-
ing on different values of N, we chose N to be 20
in all our experiments since that provided the best
tradeoff between accuracy and speed.
Next, the weight for sentence confidence score
Scrf and that for language model score Slm are de-
termined. To simplify the process, we assume that
the weights for both Scrf and Slm are equal. In this
step, each training corpus is separated into a train-
ing set (80% of the whole corpus) and a held-out
set (20% of the corpus). Then, the perceptron algo-
rithm is applied on the training set with different Scrf
and Slm values, and for various number of iterations.
The weight values we test include 2, 4, 6, 8, 10, 20,
30, 40, 50, 100 and 200. From the experiments, the
weights are chosen to be 100 for CKIP corpus, 10
for CityU corpus, 30 for NCC corpus, 20 for CTB
corpus, and 10 for SXU corpus.
While determining the weights for global fea-
tures, the number of training iterations can be deter-
mined as well. Experiments show that, as the num-
ber of iterations increases, the accuracy stabilizes in
most cases, reflecting the convergence of the learn-
ing algorithm. Analyzing the learning curves, we fix
the number of training iterations to be 5 for CKIP
corpus, 9 for NCC corpus, and 8 for the CityU, CTB
and SXU corpora.
3.2 Results on the Fourth SIGHAN Bakeoff
In each experiment, F-score (F ) is used to evalu-
ate the segmentation accuracy. Table 3 shows the
F-score on the fourth SIGHAN Bakeoff corpora. In
this table, we record the performance of our system,
the score from the character-based CRF method and
the score from the averaged perceptron using only
local features.
Our system outperforms the baseline character-
based CRF tagger. In addition, the use of global
features in the re-ranker produces better results than
only using local features.
The only data set on which the performance of
our system is lower than the character-based CRF
method is CKIP corpus. For this data set during the
parameter pruning step, the weight for Scrf and Slm
145
Sixth SIGHAN Workshop on Chinese Language Processing
CKIP NCC CityU CTB SXU
Character-based CRF method 0.9332 0.9248 0.9320 0.9468 0.9473
Averaged Perceptron with only
local features
0.9180 0.9125 0.9273 0.9450 0.9387
Our System 0.9263 0.9296 0.9355 0.9501 0.9512
Our System (With modified
weight for global features)
0.9354 ? ? ? ?
Significance (p-value) ? 1.19e-12 ? 4.43e-69 ? 3.55e-88 ? 2.17e-18 ? 2.18e-38
Table 3: F-scores on the Fourth SIGHAN Bakeoff Corpora
was too large. By lowering the weight from 100 to
4, we obtains an F-score of 0.9354, which is signifi-
cantly better than the baseline CRF tagger.
The significance values in Table 3 were produced
using the McNemar?s Test (Gillick, 1989)5. All our
results are significantly better.
4 Related Work
Re-ranking over N-best lists has been applied to so
many tasks in natural language that it is not possi-
ble to list them all here. Closest to our approach
is the work in (Kazama and Torisawa, 2007). They
proposed a margin perceptron approach for named
entity recognition with non-local features on an N-
best list. In contrast to their approach, in our sys-
tem, global features examine the entire sentence in-
stead of partial phrases. For word segmentation,
(Wang and Shi, 2006) implemented a re-ranking
method with POS tagging features. In their ap-
proach, character-based CRF model produces the N-
best list for each test sentence. The Penn Chinese
TreeBank is used to train a POS tagger, which is
used in re-ranking. However the POS tags are used
as local and not global features. Note that we would
not use POS tags in the closed track.
5 Conclusion
We have participated in the closed track of the fourth
SIGHAN Chinese word segmentation bakeoff, and
we provide results on all five corpora. We have
shown that by combining global and local features,
we can improve accuracy over simply using local
features, and we also show improved accuracy over
the baseline CRF character-based tagger for word
segmentation.
5www.fon.hum.uva.nl/Service/Statistics/McNemars test.html
References
M. Collins. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experiments
with Perceptron Algorithms. In Proc. of the Empirical
Methods in Natural Language Processing (EMNLP).
MACL, 2002, 1?8, 2000.
M. Collins. 2002. Ranking Algorithms for Named-
Entity Extractions: Boosting and the Voted Percep-
tron. In Proc. of ACL 2002.
Y. Freund and R. Schapire. 1999. Large Margin Classi-
fication using the Perceptron Algorithm. In Machine
Learning, 37(3): 277?296.
L. Gillick and S. Cox. 1989. Some Statistical Issues
in the Comparison of Speech Recognition Algorithms.
In Proc. of IEEE Conf. on Acoustics, Speech and Sig.
Proc., Glasgow, 1989, 532?535.
J. Kazama and K. Torisawa. 2007. A New Perceptron
Algorithm for Sequence Labeling with Non-local Fea-
tures. In Proc. of EMNLP-CoNLL 2007 , pages 315?
324.
T. Kudo, K. Yamamoto, and Y. Matsumoto. 2004. Ap-
pliying Conditional Random Fields to Japanese Mor-
phological Analysis. In Proc. of EMNLP 2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 591?598.
A. Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Proc. of EMNLP 1996.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado, September 2002.
M. Wang and Y. Shi. 2006. A Maximum Entropy Model
for Part-of-Speech Tagging. In Proc. of the Fifth
SIGHAN Workshop on Chinese Language Processing,
Sydney, 2006, pages 205?208.
Y. Zhang and S. Clark. 2007. Chinese Segmentation with
a Word-based Perceptron Algorithm. In Proc. of ACL
2007.
146
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 126?129,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Voting between Dictionary-based and Subword Tagging Models for
Chinese Word Segmentation
Dong Song and Anoop Sarkar
School of Computing Science, Simon Fraser University
Burnaby, BC, Canada V5A1S6
{dsong,anoop}@cs.sfu.ca
Abstract
This paper describes a Chinese word seg-
mentation system that is based on ma-
jority voting among three models: a for-
ward maximum matching model, a con-
ditional random field (CRF) model us-
ing maximum subword-based tagging, and
a CRF model using minimum subword-
based tagging. In addition, it contains a
post-processing component to deal with
inconsistencies. Testing on the closed
track of CityU, MSRA and UPUC corpora
in the third SIGHAN Chinese Word Seg-
mentation Bakeoff, the system achieves a
F-score of 0.961, 0.953 and 0.919, respec-
tively.
1 Introduction
Tokenizing input text into words is the first step of
any text analysis task. In Chinese, a sentence is
written as a string of characters, to which we shall
refer by their traditional name of hanzi, without
separations between words. As a result, before any
text analysis on Chinese, word segmentation task
has to be completed so that each word is ?isolated?
by the word-boundary information.
Participating in the third SIGHAN Chinese
Word Segmentation Bakeoff in 2006, our system
is tested on the closed track of CityU, MSRA and
UPUC corpora. The sections below provide a de-
tailed description of the system and our experi-
mental results.
2 System Description
In our segmentation system, a hybrid strategy
is applied (Figure 1): First, forward maximum
matching (Chen and Liu, 1992), which is a
dictionary-based method, is used to generate a
segmentation result. Also, the CRF model us-
ing maximum subword-based tagging (Zhang et
al., 2006) and the CRF model using minimum
subword-based tagging, both of which are statis-
tical methods, are used individually to solve the
problem. In the next step, the solutions from
these three methods are combined via the hanzi-
level majority voting algorithm. Then, a post-
processing procedure is applied in order to to get
the final output. This procedure merges adjoin-
ing words to match the dictionary entries and then
splits words which are inconsistent with entries in
the training corpus.
Forward
Post?processing
Majority Voting
Result
Input Sentence
Tagging
Subword?based
Minimum
CRF with
Tagging
Subword?based
Maximum
CRF with
Matching
Maximum
Figure 1: Outline of the segmentation process
2.1 Forward Maximum Matching
The maximum matching algorithm is a greedy
segmentation approach. It proceeds through the
sentence, mapping the longest word at each point
with an entry in the dictionary. In our system,
the well-known forward maximum matching algo-
rithm (Chen and Liu, 1992) is implemented.
The maximum matching approach is simple and
efficient, and it results in high in-vocabulary ac-
curacy; However, the small size of the dictionary,
which is obtained only from the training data, is
a major bottleneck for this approach to be applied
by itself.
126
2.2 CRF Model with Maximum
Subword-based Tagging
Conditional random fields (CRF), a statistical se-
quence modeling approach (Lafferty et al, 2001),
has been widely applied in various sequence
learning tasks including Chinese word segmen-
tation. In this approach, most existing methods
use the character-based IOB tagging. For ex-
ample, ??(all) ??(extremely important)?
is labeled as ??(all)/O ?(until)/B (close)/I
?(heavy)/I(demand)/I?.
Recently (Zhang et al, 2006) proposed a maxi-
mum subword-based IOB tagger for Chinese word
segmentation, and our system applies their ap-
proach which obtains a very high accuracy on the
shared task data from previous SIGHAN compe-
titions. In this method, all single-hanzi words and
the top frequently occurring multi-hanzi words are
extracted from the training corpus to form the lexi-
con subset. Then, each word in the training corpus
is segmented for IOB tagging, with the forward
maximum matching algorithm, using the formed
lexicon subset as the dictionary. In the above
example, the tagging labels become ??(all)/O
?(until)/B (close)/I ?(important)/I?, as-
suming that ??(important)? is the longest sub-
word in this word, and it is one of the top fre-
quently occurring words in the training corpus.
After tagging the training corpus, we use the
package CRF++1 to train the CRF model. Sup-
pose w0 represents the current word, w?1 is the
first word to the left, w?2 is the second word to
the left, w1 is the first word to the right, and w2
is the second word to the right, then in our experi-
ments, the types of unigram features used include
w0, w?1, w1, w?2, w2, w0w?1, w0w1, w?1w1,
w?2w?1, and w2w0. In addition, only combina-
tions of previous observation and current observa-
tion are exploited as bigram features.
2.3 CRF Model with Minimum
Subword-based Tagging
In our third model, we applies a similar approach
as in the previous section. However, instead of
finding the maximum subwords, we explore the
minimum subwords. At the beginning, we build
the dictionary using the whole training corpus.
Then, for each word in the training data, a forward
shortest matching is used to get the sequence of
minimum-length subwords, and this sequence is
1available from http://www/chasen.org/?taku/software
tagged in the same IOB format as before. Suppose
?a?, ?ac?, ?de? and ?acde? are the only entries in
the dictionary. Then, for the word ?acde?, the se-
quence of subwords is ?a?, ?c? and ?de?, and the
tags assigned to ?acde? are ?a/B c/I de/I?.
After tagging the training data set, CRF++
package is executed again to train this type of
model, using the identical unigram and bigram
feature sets that are used in the previous model.
Meanwhile, the unsegmented test data is seg-
mented by the forward shortest matching algo-
rithm. After this initial segmentation process, the
result is fed into the trained CRF model for re-
segmentation by assigning IOB tags.
2.4 Majority Voting
Having the segmentation results from the above
three models in hand, in this next step, we adopt
the hanzi-level majority voting algorithm. First,
for each hanzi in a segmented sentence, we tag it
either as ?B? if it is the first hanzi of a word or
a single-hanzi word, or as ?I? otherwise. Then,
for a given hanzi in the results from those three
models, if at least two of the models provide the
identical tag, it will be assigned that tag. For in-
stance, suppose ?a c de? is the segmentation result
via forward maximum matching, and it is also the
result from CRF model with maximum subword-
based tagging, and ?ac d e? is the result from the
third model. Then, for ?a?, since all of them as-
sign ?B? to it, ?a? is given the ?B? tag; for ?c?,
because two of segmentations tag it as ?B?, ?c? is
given the ?B? tag as well. Similarly, the tag for
each remaining hanzi is determined by this major-
ity voting process, and we get ?a c de? as the result
for this example.
To test the performance of each of the three
models and that of the majority voting, we di-
vide the MSRA corpus into training set and held-
out set. Throughout all the experiments we con-
ducted, we discover that those two CRF models
perform much better than the pure hanzi-based
CRF method, and that the voting process improves
the performance further.
2.5 Post-processing
While analyzing errors with the segmentation re-
sult from the held-out set, we find two incon-
sistency problems: First, the inconsistency be-
tween the dictionary and the result: that is, certain
words that appear in the dictionary are separated
into consecutive words in the test result; Second,
127
the inconsistency among words in the dictionary;
For instance, both ?)????(scientific research)
and ?)?(science)??(research)? appear in the
training corpus.
To deal with the first phenomena, for the seg-
mented result, we try to merge adjoining words to
match the dictionary entries. Suppose ?a b c de?
are the original voting result, and ?ab?, ?abc? and
?cd? form the dictionary. Then, we merge ?a?, ?b?
and ?c? together to get the longest match with the
dictionary. Therefore, the output is ?abc de?.
For the second problem, we introduce the split
procedure. In our system, we only consider two
consecutive words. First, all bigrams are extracted
from the training corpus, and their frequencies are
counted. After that, for example, if ?a b? appears
more often than ?ab?, then whenever in the test
result we encounter ?ab?, we split it into ?a b?.
The post-processing steps detailed above at-
tempt to maximize the value of known words in
the training data as well as attempting to deal with
the word segmentation inconsistencies in the train-
ing data.
3 Experiments and Analysis
The third International Chinese Language
Processing Bakeoff includes four different cor-
pora, Academia Sinica (CKIP), City University
of Hong Kong (CityU), Microsoft Research
(MSRA), and University of Pennsylvania and
University of Colorado, Boulder (UPUC), for the
word segmentation task.
In this bakeoff, we test our system in CityU,
MSRA and UPUC corpora, and follow the closed
track. That is, we only use training material from
the training data for the particular corpus we are
testing on. No other material or any type of ex-
ternal knowledge is used, including part-of-speech
information, externally generated word-frequency
counts, Arabic and Chinese numbers, feature char-
acters for place names and common Chinese sur-
names.
3.1 Results on SIGHAN Bakeoff 2006
To observe the result of majority voting and the
contribution of the post-processing step, the ex-
periment is ran for each corpus by first producing
the outcome of majority voting and then producing
the output from the post-processing. In each ex-
periment, the precision (P ), recall (R), F-measure
(F ), Out-of-Vocabulary rate (OOV ), OOV recall
rate (ROOV ), and In-Vocabulary rate (RIV ) are
recorded. Table 1,2,3 show the scores for the
CityU corpus, for the MSRA corpus, and for the
UPUC corpus, respectively.
Majority Voting Post-processing
P 0.956 0.958
R 0.962 0.963
F 0.959 0.961
OOV 0.04 0.04
ROOV 0.689 0.689
RIV 0.974 0.974
Table 1: Scores for CityU corpus
Majority Voting Post-processing
P 0.952 0.954
R 0.952 0.952
F 0.952 0.953
OOV 0.034 0.034
ROOV 0.604 0.604
RIV 0.964 0.964
Table 2: Scores for MSRA corpus
Majority Voting Post-processing
P 0.908 0.909
R 0.927 0.929
F 0.918 0.919
OOV 0.088 0.088
ROOV 0.628 0.628
RIV 0.956 0.958
Table 3: Scores for UPUC corpus
From those tables, we can see that a simple ma-
jority voting algorithm produces accuracy that is
higher than each individual system and reason-
ably high F-scores overall. In addition, the post-
processing step indeed helps to improve the per-
formance.
3.2 Error analysis
The errors that occur in our system are mainly due
to the following three factors:
First, there is inconsistency between the gold
segmentation and the training corpus. Although
the inconsistency problem within the training cor-
pus is intended to be tackled in the post-processing
step, we cannot conclude that the segmentation
128
for certain words in the gold test set alays fol-
lows the convention in the training data set. For
example, in the MSRA training corpus, ??)
u?(Chinese government) is usually considered
as a single word; while in the gold test set, it is
separated as two words ??)?(Chinese) and ?u
?(government). This inconsistency issue lowers
the system performance. This problem, of course,
affects all competing systems.
Second, we don?t have specific steps to deal
with words with postfixes such as ?V?(person).
Compared to our system, (Zhang, 2005) proposed
a segmentation system that contains morpholog-
ically derived word recognition post-processing
component to solve this problem. Lacking of such
a step prevents us from identifying certain types
of words such as ???V?(worker) to be a single
word.
In addition, the unknown words are still trou-
blesome because of the limited size of the training
corpora. In the class of unknown words, we en-
counter person names, numbers, dates, organiza-
tion names and words translated from languages
other than Chinese. For example, in the produced
CityU test result, the translated person name ??
-b???(Mihajlovic) is incorrectly separated
as ??-b? and ????. Moreover, in cer-
tain cases, person names can also create ambigu-
ity. Take the name ?B?0?(Qiu, Beifang) in
UPUC test set for example, without understand-
ing the meaning of the whole sentence, it is dif-
ficult even for human to determine whether it is
a person name or it represents ?B?(autumn), ??
0?(north), with the meaning of ?the autumn in the
north?.
4 Alternative to Majority Voting
In designing the voting procedure, we also attempt
to develop and use a segmentation lattice, which
proceeds using a similar underlying principle as
the one applied in (Xu et al, 2005).
In our approach, for an input sentence, the seg-
mentation result using each of our three models is
transformed into an individual lattice. Also, each
edge in the lattice is assigned a particular weight,
according to certain features such as whether or
not the output word from that edge is in the dictio-
nary. After building the three lattices, one for each
model, we merge them together. Then, the shortest
path, referring to the path that has the minimum
weight, is extracted from the merged lattice, and
therefore, the segmentation result is determined by
this shortest path.
However, in the time we had to run our experi-
ments on the test data, we were unable to optimize
the edge weights to obtain high accuracy on some
held-out set from the training corpora. So instead,
we tried a simple method for finding edge weights
by uniformly distributing the weight for each fea-
ture; Nevertheless, by testing on the shared task
data from the 2005 SIGHAN bakeoff, the perfor-
mance is not competitive, compared to our simple
majority voting method described above. As a re-
sult, we decide to abandon this approach for this
year?s SIGHAN bakeoff.
5 Conclusion
Our Chinese word segmentation system is based
on majority voting among the initial outputs from
forward maximum matching, from a CRF model
with maximum subword-based tagging, and from
a CRF model with minimum subword-based tag-
ging. In addition, we experimented with various
steps in post-processing which effectively boosted
the overall performance.
In future research, we shall explore more so-
phisticated ways of voting, including the contin-
uing investigation on the segmentation lattice ap-
proach. Also, more powerful methods on how
to accurately deal with unknown words, including
person and place names, without external knowl-
edge, will be studied as well.
References
Keh-jiann Chen, and Shing-Huan Liu. 1992. Word Identi-
fication for Mandarin Chinese Sentences. In Fifth Inter-
national Conference on Computational Linguistics, pages
101?107.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. In Proc. of
ICML-2001, pages 591?598.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney.
2005. Integrated Chinese Word Segmentation in Statisti-
cal Machine Translation. In Proc. of IWSLT-2005.
Huipeng Zhang, Ting Liu, Jinshan Ma, and Xiantao Liu.
2005. Chinese Word Segmentation with Multiple Post-
processors in HIT-IRLab. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Processing,
pages 172?175.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based Tagging by Conditional Random
Fields for Chinese Word Segmentation. In Proc. of HLT-
NAACL 2006.
129
