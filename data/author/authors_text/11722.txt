Proceedings of the 12th Conference of the European Chapter of the ACL, pages 354?362,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Learning-Based Named Entity Recognition for Morphologically-Rich,
Resource-Scarce Languages
Kazi Saidul Hasan and Md. Altaf ur Rahman and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,altaf,vince}@hlt.utdallas.edu
Abstract
Named entity recognition for morpholog-
ically rich, case-insensitive languages, in-
cluding the majority of semitic languages,
Iranian languages, and Indian languages,
is inherently more difficult than its English
counterpart. Worse still, progress on ma-
chine learning approaches to named entity
recognition for many of these languages
is currently hampered by the scarcity of
annotated data and the lack of an accu-
rate part-of-speech tagger. While it is
possible to rely on manually-constructed
gazetteers to combat data scarcity, this
gazetteer-centric approach has the poten-
tial weakness of creating irreproducible
results, since these name lists are not
publicly available in general. Motivated
in part by this concern, we present a
learning-based named entity recognizer
that does not rely on manually-constructed
gazetteers, using Bengali as our represen-
tative resource-scarce, morphologically-
rich language. Our recognizer achieves
a relative improvement of 7.5% in F-
measure over a baseline recognizer. Im-
provements arise from (1) using in-
duced affixes, (2) extracting information
from online lexical databases, and (3)
jointly modeling part-of-speech tagging
and named entity recognition.
1 Introduction
While research in natural language processing has
gained a lot of momentum in the past several
decades, much of this research effort has been fo-
cusing on only a handful of politically-important
languages such as English, Chinese, and Arabic.
On the other hand, being the fifth most spoken lan-
guage1 with more than 200 million native speakers
residing mostly in Bangladesh and the Indian state
of West Bengal, Bengali has far less electronic
resources than the aforementioned languages. In
fact, a major obstacle to the automatic processing
of Bengali is the scarcity of annotated corpora.
One potential solution to the problem of data
scarcity is to hand-annotate a small amount of
data with the desired linguistic information and
then develop bootstrapping algorithms for com-
bining this small amount of labeled data with
a large amount of unlabeled data. In fact, co-
training (Blum and Mitchell, 1998) has been suc-
cessfully applied to English named entity recog-
nition (NER) (Collins & Singer [henceforth C&S]
(1999)). In C&S?s approach, consecutive words
tagged as proper nouns are first identified as poten-
tial NEs, and each such NE is then labeled by com-
bining the outputs of two co-trained classifiers.
Unfortunately, there are practical difficulties in ap-
plying this technique to Bengali NER. First, one
of C&S?s co-trained classifiers uses features based
on capitalization, but Bengali is case-insensitive.
Second, C&S identify potential NEs based on
proper nouns, but unlike English, (1) proper noun
identification for Bengali is non-trivial, due to the
lack of capitalization; and (2) there does not ex-
ist an accurate Bengali part-of-speech (POS) tag-
ger for providing such information, owing to the
scarcity of annotated data for training the tagger.
In other words, Bengali NER is complicated not
only by the scarcity of annotated data, but also by
the lack of an accurate POS tagger. One could
imagine building a Bengali POS tagger using un-
1See http://en.wikipedia.org/wiki/Bengali language.
354
supervised induction techniques that have been
successfully developed for English (e.g., Schu?tze
(1995), Clark (2003)), including the recently-
proposed prototype-driven approach (Haghighi
and Klein, 2006) and Bayesian approach (Gold-
water and Griffiths, 2007). The majority of these
approaches operate by clustering distributionally
similar words, but they are unlikely to work well
for Bengali for two reasons. First, Bengali is a
relatively free word order language, and hence
the distributional information collected for Ben-
gali words may not be as reliable as that for En-
glish words. Second, many closed-class words
that typically appear in the distributional repre-
sentation of an English word (e.g., prepositions
and particles such as ?in? and ?to?) are realized
as inflections in Bengali, and the absence of these
informative words implies that the context vector
may no longer capture sufficient information for
accurately clustering the Bengali words.
In view of the above problems, many learning-
based Bengali NE recognizers have relied heavily
on manually-constructed name lists for identify-
ing persons, organizations, and locations. There
are at least two weaknesses associated with this
gazetteer-centric approach. First, these name lists
are typically not publicly available, making it dif-
ficult to reproduce the results of these NE recog-
nizers. Second, it is not clear how comprehen-
sive these lists are. Relying on comprehensive lists
that comprise a large portion of the names in the
test set essentially reduces the NER problem to a
dictionary-lookup problem, which is arguably not
very interesting from a research perspective.
In addition, many existing learning-based Ben-
gali NE recognizers have several common weak-
nesses. First, they use as features pseudo-affixes,
which are created by extracting the first n and the
last n characters of a word (where 1 ? n ? 4)
(e.g., Dandapat et al (2007)). While affixes en-
code essential grammatical information in Ben-
gali due to its morphological richness, this extrac-
tion method is arguably too ad-hoc and does not
cover many useful affixes. Second, they typically
adopt a pipelined NER architecture, performing
POS tagging prior to NER and encoding the result-
ing not-so-accurate POS information as a feature.
In other words, errors in POS tagging are propa-
gated to the NE recognizer via the POS feature,
thus limiting its performance.
Motivated in part by these weaknesses, we in-
vestigate how to improve a learning-based NE rec-
ognizer that does not rely on manually-constructed
gazetteers. Specifically, we investigate two learn-
ing architectures for our NER system. The first
one is the aforementioned pipelined architecture
in which the NE recognizer uses as features the
output of a POS tagger that is trained indepen-
dently of the recognizer. Unlike existing Bengali
POS and NE taggers, however, we examine two
new knowledge sources for training these taggers:
(1) affixes induced from an unannotated corpus
and (2) semantic class information extracted from
Wikipedia. In the second architecture, we jointly
learn the POS tagging and the NER tasks, allow-
ing features for one task to be accessible to the
other task during learning. The goal is to exam-
ine whether any benefits can be obtained via joint
modeling, which could address the error propaga-
tion problem with the pipelined architecture.
While we focus on Bengali NER in this pa-
per, none of the proposed techniques are language-
specific. In fact, we believe that these techniques
are of relevance and interest to the EACL com-
munity because they can be equally applicable to
the numerous resource-scarce European and Mid-
dle Eastern languages that share similar linguis-
tic and extra-linguistic properties as Bengali. For
instance, the majority of semitic languages and
Iranian languages are, like Bengali, morpholog-
ically productive; and many East European lan-
guages such as Czech and Polish resemble Bengali
in terms of not only their morphological richness,
but also their relatively free word order.
The rest of the paper is organized as follows.
In Section 2, we briefly describe the related work.
Sections 3 and 4 show how we induce affixes from
an unannotated corpus and extract semantic class
information from Wikipedia. In Sections 5 and
6, we train and evaluate a POS tagger and an NE
recognizer independently, augmenting the feature
set typically used for these two tasks with our new
knowledge sources. Finally, we describe and eval-
uate our joint model in Section 7.
2 Related Work
Cucerzan and Yarowsky (1999) exploit morpho-
logical and contextual patterns to propose a
language-independent solution to NER. They use
affixes based on the paradigm that named enti-
ties corresponding to a particular class have sim-
ilar morphological structure. Their bootstrapping
355
approach is tested on Romanian, English, Greek,
Turkish, and Hindi. The recall for Hindi is the
lowest (27.84%) among the five languages, sug-
gesting that the lack of case information can sig-
nificantly complicate the NER task.
To investigate the role of gazetteers in NER,
Mikheev et al (1999) combine grammar rules with
maximum entropy models and vary the gazetteer
size. Experimental results show that (1) the F-
scores for NE classes like person and organiza-
tion are still high without gazetteers, ranging from
85% to 92%; and (2) a small list of country names
can improve the low F-score for locations substan-
tially. It is worth noting that their recognizer re-
quires that the input data contain POS tags and
simple semantic tags, whereas ours automatically
acquires such linguistic information. In addition,
their approach uses part of the dataset to extend the
gazetteer. Therefore, the resulting gazetteer list is
specific to a particular domain; on the other hand,
our approach does not generate a domain-specific
list, since it makes use of Wikipedia articles.
Kozareva (2006) generates gazetteer lists for
person and location names from unlabeled data
using common patterns and a graph exploration
algorithm. The location pattern is essentially
a preposition followed by capitalized context
words. However, this approach is inadequate for a
morphologically-rich language like Bengali, since
prepositions are often realized as inflections.
3 Affix Induction
Since Bengali is morphologically productive, a lot
of grammatical information about Bengali words
is expressed via affixes. Hence, these affixes could
serve as useful features for training POS and NE
taggers. In this section, we show how to induce
affixes from an unannotated corpus.
We rely on a simple idea proposed by Keshava
and Pitler (2006) for inducing affixes. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) ?
and ? are two character sequences, and (3) ?? is
the concatenation of ? and ?. If ?? and ? are
found in V , we extract ? as a suffix. Similarly, if
?? and ? are found in V , we extract ? as a prefix.
In principle, we can use all of the induced af-
fixes as features for training a POS tagger and an
NE recognizer. However, we choose to use only
those features that survive our feature selection
process (to be described below), for the follow-
ing reasons. First, the number of induced affixes
is large, and using only a subset of them as fea-
tures could make the training process more effi-
cient. Second, the above affix induction method is
arguably overly simplistic and hence many of the
induced affixes could be spurious.
Our feature selection process is fairly simple:
we (1) score each affix by multiplying its fre-
quency (i.e., the number of distinct words in V to
which each affix attaches) and its length2, and (2)
select only those whose score is above a certain
threshold. In our experiments, we set this thresh-
old to 50, and generate our vocabulary of 140K
words from five years of articles taken from the
Bengali newspaper Prothom Alo. This enables us
to induce 979 prefixes and 975 suffixes.
4 Semantic Class Induction from
Wikipedia
Wikipedia has recently been used as a knowl-
edge source for various language processing tasks,
including taxonomy construction (Ponzetto and
Strube, 2007a), coreference resolution (Ponzetto
and Strube, 2007b), and English NER (e.g.,
Bunescu and Pas?ca (2006), Cucerzan (2007),
Kazama and Torisawa (2007), Watanabe et al
(2007)). Unlike previous work on using Wikipedia
for NER, our goal here is to (1) generate a list
of phrases and tokens that are potentially named
entities from the 16914 articles in the Bengali
Wikipedia3 and (2) heuristically annotate each of
them with one of four classes, namely, PER (per-
son), ORG (organization), LOC (location), or OTH-
ERS (i.e., anything other than PER, ORG and LOC).
4.1 Generating an Annotated List of Phrases
We employ the steps below to generate our anno-
tated list.
Generating and annotating the titles Recall
that each Wikipedia article has been optionally as-
signed to one or more categories by its creator
and/or editors. We use these categories to help an-
notate the title of an article. Specifically, if an ar-
ticle has a category whose name starts with ?Born
on? or ?Death on,? we label the corresponding ti-
tle with PER. Similarly, if it has a category whose
name starts with ?Cities of? or ?Countries of,? we
2The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
3See http://bn.wikipedia.org. In our experiments, we used
the Bengali Wikipedia dump obtained on October 22, 2007.
356
NE Class Keywords
PER ?born,? ?died,? ?one,? ?famous?
LOC ?city,? ?area,? ?population,? ?located,? ?part of?
ORG ?establish,? ?situate,? ?publish?
Table 1: Keywords for each named entity class
label the title as LOC. If an article does not be-
long to one of the four categories above, we label
its title with the help of a small set of seed key-
words shown in Table 1. Specifically, for each of
the three NE classes shown on the left of Table
1, we compute a weighted sum of its keywords:
a keyword that appears in the first paragraph has
a weight of 3, a keyword that appears elsewhere
in the article has a weight of 1, and a keyword
that does not appear in the article has a weight of
0. The rationale behind using different weights is
simple: the first paragraph is typically a brief ex-
position of the title, so it should in principle con-
tain words that correlate more closely with the ti-
tle than words appearing in the rest of the article.
We then label the title with the class that has the
largest weighted sum. Note, however, that we ig-
nore any article that contains fewer than two key-
words, since we do not have reliable evidence for
labeling its title as one of the NE classes. We put
all these annotated titles into a title list.
Getting more location names To get more loca-
tion names, we search for the character sequences
?birth place:? and ?death place:? in each article,
extracting the phrase following any of these se-
quences and label it as LOC. We put all such la-
beled locations into the title list.
Generating and annotating the tokens in the ti-
tles Next, we extract the word tokens from each
title in the title list and label each token with an
NE class. The reason for doing this is to improve
generalization: if ?Dhaka University? is labeled as
ORG in the title list, then it is desirable to also label
the token ?University? as ORG, because this could
help identify an unseen phrase that contains the
term ?University? as an organization. Our token
labeling method is fairly simple. First, we gener-
ate the tokens from each title in the title list, as-
signing to each token the same NE label as that
of the title from which it is generated. For in-
stance, from the title ?Anna Frank,? ?Anna? will
be labeled as PER; and from ?Anna University,? ?
Anna? will be labeled as LOC. To resolve such
ambiguities (i.e., assigning different labels to the
same token), we keep a count of how many times
?Anna? is labeled with each NE class, and set its
final label to be the most frequent NE class. We
put all these annotated tokens into a token list. If
the title list and the token list have an element in
common, we remove the element from the token
list, since we have a higher confidence in the la-
bels of the titles.
Merging the lists Finally, we append the token
list to the title list. The resulting title list contains
4885 PERs, 15176 LOCs, and 188 ORGs.
4.2 Applying the Annotated List to a Text
We can now use the title list to annotate a text.
Specifically, we process each word w in the text in
a left-to-right manner, using the following steps:
1. Check whether w has been labeled. If so, we
skip this word and process the next one.
2. Check whether w appears in the Samsad
Bengali-English Dictionary4. If so, we as-
sume that w is more likely to be used as a
non-named entity, thus leaving the word un-
labeled and processing the next word instead.
3. Find the longest unlabeled word sequence5
that begins with w and appears in the title
list. If no such sequence exists, we leave w
unlabeled and process the next word. Oth-
erwise, we label it with the NE tag given
by the title list. To exemplify, consider a
text that starts with the sentence ?Smith Col-
lege is in Massachusetts.? When processing
?Smith,? ?Smith College? is the longest se-
quence that starts with ?Smith? and appears
in the title list (as an ORG). As a result, we
label all occurrences of ?Smith College? in
the text as an ORG. (Note that without using
the longest match heuristic, ?Smith? would
likely be mislabeled as PER.) In addition, we
take the last word of the ORG (which in this
case is ?College?) and annotate each of its oc-
currence in the rest of the text as ORG.6
These automatic annotations will then be used
to derive a set of WIKI features for training our
POS tagger and NE recognizer. Hence, unlike
existing Bengali NE recognizers, our ?gazetteers?
are induced rather than manually created.
4See http://dsal.uchicago.edu/dictionaries/biswasbengali/.
5This is a sequence in which each word is unlabeled.
6However, if we have a PER match (e.g., ?Anna Frank?)
or a LOC match (e.g., ?Las Vegas?), we take each word in the
matched phrase and label each of its occurrence in the rest of
the text with the same NE tag.
357
Current word wi
Previous word wi?1
2nd previous word wi?2
Next word wi+1
2nd next word wi+2
Current pseudo-affixes pfi (prefix), sfi (suffix)
Current induced affixes pii (prefix), sii (suffix)
Previous induced affixes pii?1 (prefix), sii?1 (suffix)
Induced affix bigrams pii?1pii (prefix), sii?1sii (suffix)
Current Wiki tag wikii
Previous Wiki tag wikii?1
Wiki bigram wikii?1wikii
Word bigrams wi?2wi?1, wi?1wi, wiwi+1,
wi+1wi+2
Word trigrams wi?2wi?1wi
Current number qi
Table 2: Feature templates for the POS tagging
experiments
5 Part-of-Speech Tagging
In this section, we will show how we train and
evaluate our POS tagger. As mentioned before, we
hypothesize that introducing our two knowledge
sources into the feature set for the tagger could
improve its performance: using the induced affixes
could improve the extraction of grammatical infor-
mation from the words, and using the Wikipedia-
induced list, which in principle should comprise
mostly of names, could help improve the identifi-
cation of proper nouns.
Corpus Our corpus is composed of 77942 words
and is annotated with one of 26 POS tags in the
tagset defined by IIIT Hyderabad7. Using this cor-
pus, we perform 5-fold cross-validation (CV) ex-
periments in our evaluation. It is worth noting that
this dataset has a high unknown word rate of 15%
(averaged over the five folds), which is due to the
small size of the dataset. While this rate is compa-
rable to another Bengali POS dataset described in
Dandapat et al (2007), it is much higher than the
2.6% unknown word rate in the test set for Ratna-
parkhi?s (1996) English POS tagging experiments.
Creating training instances Following previ-
ous work on POS tagging, we create one train-
ing instance for each word in the training set. The
class value of an instance is the POS tag of the cor-
responding word. Each instance is represented by
a set of linguistic features, as described next.
7A detailed description of these POS tags can be found in
http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf,
and are omitted here due to space limitations. This tagset
and the Penn Treebank tagset differ in that (1) nouns do not
have a number feature; (2) verbs do not have a tense feature;
and (3) adjectives and adverbs are not subcategorized.
Features Our feature set consists of (1) base-
line features motivated by those used in Danda-
pat et al?s (2007) Bengali POS tagger and Singh
et al?s (2006) Hindi POS tagger, as well as (2)
features derived from our induced affixes and the
Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams, bi-
grams and trigrams; (2) pseudo-affix features that
are created by taking the first three characters and
the last three characters of the current word; and
(3) a binary feature that determines whether the
current word is a number. As far as our new fea-
tures are concerned, we create one induced prefix
feature and one induced suffix feature from both
the current word and the previous word, as well
as two bigrams involving induced prefixes and in-
duced suffixes. We also create three WIKI features,
including the Wikipedia-induced NE tag of the
current word and that of the previous word, as well
as the combination of these two tags. Note that
the Wikipedia-induced tag of a word can be ob-
tained by annotating the test sentence under con-
sideration using the list generated from the Ben-
gali Wikipedia (see Section 4). To make the de-
scription of these features more concrete, we show
the feature templates in Table 2.
Learning algorithm We used CRF++8, a C++
implementation of conditional random fields (Laf-
ferty et al, 2001), as our learning algorithm for
training a POS tagging model.
Evaluating the model To evaluate the resulting
POS tagger, we generate test instances in the same
way as the training instances. 5-fold CV results of
the POS tagger are shown in Table 3. Each row
consists of three numbers: the overall accuracy,
as well as the accuracies on the seen and the un-
seen words. Row 1 shows the accuracy when the
baseline feature set is used; row 2 shows the ac-
curacy when the baseline feature set is augmented
with our two induced affix features; and the last
row shows the results when both the induced af-
fix and the WIKI features are incorporated into
the baseline feature set. Perhaps not surprisingly,
(1) adding more features improves performance,
and (2) accuracies on the seen words are substan-
tially better than those on the unseen words. In
fact, adding the induced affixes to the baseline fea-
ture set yields a 7.8% reduction in relative error
in overall accuracy. We also applied a two-tailed
paired t-test (p < 0.01), first to the overall accura-
8Available from http://crfpp.sourceforge.net
358
Experiment Overall Seen Unseen
Baseline 89.83 92.96 72.08
Baseline+Induced Affixes 90.57 93.39 74.64
Baseline+Induced Affixes+Wiki 90.80 93.50 75.58
Table 3: 5-fold cross-validation accuracies for
POS tagging
Predicted Tag Correct Tag % of Error
NN NNP 22.7
NN JJ 9.6
JJ NN 7.4
NNP NN 5.0
NN VM 4.9
Table 4: Most frequent errors for POS tagging
cies in rows 1 and 2, and then to the overall accu-
racies in rows 2 and 3. Both pairs of numbers are
statistically significantly different from each other,
meaning that incorporating the two induced affix
features and then the WIKI features both yields sig-
nificant improvements.
Error analysis To better understand the results,
we examined the errors made by the tagger. The
most frequent errors are shown in Table 4. From
the table, we see that the largest source of errors
arises from mislabeling proper nouns as common
nouns. This should be expected, as proper noun
identification is difficult due to the lack of capital-
ization information. Unfortunately, failure to iden-
tify proper nouns could severely limit the recall of
an NE recognizer. Also, adjectives and common
nouns are difficult to distinguish, since these two
syntactic categories are morphologically and dis-
tributionally similar to each other. Finally, many
errors appear to involve mislabeling a word as a
common noun. The reason is that there is a larger
percentage of common nouns (almost 30%) in the
training set than other POS tags, thus causing the
model to prefer tagging a word as a common noun.
6 Named Entity Recognition
In this section, we show how to train and evaluate
our NE recognizer. The recognizer adopts a tradi-
tional architecture, assuming that POS tagging is
performed prior to NER. In other words, the NE
recognizer will use the POS acquired in Section 5
as one of its features. As in Section 5, we will fo-
cus on examining how our knowledge sources (the
induced affixes and the WIKI features) impact the
performance of our recognizer.
Corpus The corpus we used for NER evaluation
is the same as the one described in the previous
POS of current word ti
POS of previous word ti?1
POS of 2nd previous word ti?2
POS of next word ti+1
POS of 2nd next word ti+2
POS bigrams ti?2ti?1, ti?1ti, titi+1, ti+1ti+2
First word fwi
Table 5: Additional feature templates for the NER
experiments
section. Specifically, in addition to POS infor-
mation, each sentence in the corpus is annotated
with NE information. We focus on recognizing the
three major NE types in this paper, namely persons
(PER), organizations (ORG), and locations (LOC).
There are 1721 PERs, 104 ORGs, and 686 LOCs in
the corpus. As far as evaluation is concerned, we
conduct 5-fold CV experiments, dividing the cor-
pus into the same five folds as in POS tagging.
Creating training instances We view NE
recognition as a sequence labeling problem. In
other words, we combine NE identification and
classification into one step, labeling each word in
a test text with its NE tag. Any word that does not
belong to one of our three NE tags will be labeled
as OTHERS. We adopt the IOB convention, pre-
ceding an NE tag with a B if the word is the first
word of an NE and an I otherwise. Now, to train
the NE recognizer, we create one training instance
from each word in a training text. The class value
of an instance is the NE tag of the corresponding
word, or OTHERS if the word is not part of an NE.
Each instance is represented by a set of linguistic
features, as described next.
Features Our feature set consists of (1) base-
line features motivated by those used in Ekbal
et al?s (2008) Bengali NE recognizer, as well as
(2) features derived from our induced affixes and
the Wikipedia-induced list. More specifically, the
baseline feature set has (1) word unigrams; (2)
pseudo-affix features that are created by taking the
first three characters and the last three characters
of the current word; (3) a binary feature that deter-
mines whether the current word is the first word of
a sentence; and (4) a set of POS-related features,
including the POS of the current word and its sur-
rounding words, as well as POS bigrams formed
from the current and surrounding words. Our in-
duced affixes and WIKI features are incorporated
into the baseline NE feature set in the same man-
ner as in POS tagging. In essence, the feature tem-
359
Experiment R P F
Baseline 60.97 74.46 67.05
Person 66.18 74.06 69.90
Organization 29.81 44.93 35.84
Location 52.62 80.40 63.61
Baseline+Induced Affixes 60.45 73.30 66.26
Person 65.70 72.61 69.02
Organization 31.73 46.48 37.71
Location 51.46 80.05 62.64
Baseline+Induced Affixes+Wiki 63.24 75.19 68.70
Person 66.47 75.16 70.55
Organization 30.77 43.84 36.16
Location 60.06 79.69 68.50
Table 6: 5-fold cross-validation results for NER
plates employed by the NE recognizer are the top
12 templates in Table 2 and those in Table 5.
Learning algorithm We again use CRF++ as
our sequence learner for acquiring the recognizer.
Evaluating the model To evaluate the resulting
NE tagger, we generate test instances in the same
way as the training instances. To score the output
of the recognizer, we use the CoNLL-2000 scor-
ing program9, which reports performance in terms
of recall (R), precision (P), and F-measure (F). All
NE results shown in Table 6 are averages of the
5-fold CV experiments. The first block of the Ta-
ble 6 shows the overall results when the baseline
feature set is used; in addition, we also show re-
sults for each of the three NE tags. As we can see,
the baseline achieves an F-measure of 67.05. The
second block shows the results when the baseline
feature set is augmented with our two induced af-
fix features. Somewhat unexpectedly, F-measure
drops by 0.8% in comparison to the baseline. Ad-
ditional experiments are needed to determine the
reason. Finally, when the WIKI features are in-
corporated into the augmented feature set, the sys-
tem achieves an F-measure of 68.70 (see the third
block), representing a statistically significant in-
crease of 1.6% in F-measure over the baseline.
As we can see, improvements stem primarily from
dramatic gains in recall for locations.
Discussions Several points deserve mentioning.
First, the model performs poorly on the ORGs, ow-
ing to the small number of organization names
in the corpus. Worse still, the recall drops after
adding the WIKI features. We examined the list
of induced ORG names and found that it is fairly
noisy. This can be attributed in part to the diffi-
culty in forming a set of seed words that can ex-
tract ORGs with high precision (e.g., the ORG seed
?situate? extracted many LOCs). Second, using the
9http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt
WIKI features does not help recalling the PERs. A
closer examination of the corpus reveals the rea-
son: many sentences describe fictitious characters,
whereas Wikipedia would be most useful for arti-
cles that describe famous people. Overall, while
the WIKI features provide our recognizer with a
small, but significant, improvement, the useful-
ness of the Bengali Wikipedia is currently lim-
ited by its small size. Nevertheless, we believe the
Bengali Wikipedia will become a useful resource
for language processing as its size increases.
7 A Joint Model for POS Tagging and
NER
The NE recognizer described thus far has adopted
a pipelined architecture, and hence its perfor-
mance could be limited by the errors of the POS
tagger. In fact, as discussed before, the major
source of errors made by our POS tagger concerns
the confusion between proper nouns and common
nouns, and this type of error, when propagated
to the NE recognizer, could severely limit its re-
call. Also, there is strong empirical support for
this argument: the NE recognizers, when given ac-
cess to the correct POS tags, have F-scores rang-
ing from 76-79%, which are 10% higher on aver-
age than those with POS tags that were automat-
ically computed. Consequently, we hypothesize
that modeling POS tagging and NER jointly would
yield better performance than learning the two
tasks separately. In fact, many approaches have
been developed to jointly model POS tagging and
noun phrase chunking, including transformation-
based learning (Ngai and Florian, 2001), factorial
HMMs (Duh, 2005), and dynamic CRFs (Sutton
et al, 2007). Some of these approaches are fairly
sophisticated and also require intensive computa-
tions during inference. For instance, when jointly
modeling POS tagging and chunking, Sutton et al
(2007) reduce the number of POS tags from 45
to 5 when training a factorial dynamic CRF on a
small dataset (with only 209 sentences) in order to
reduce training and inference time.
In contrast, we propose a relatively simple
model for jointly learning Bengali POS tagging
and NER, by exploiting the limited dependencies
between the two tasks. Specifically, we make the
observation that most of the Bengali words that are
part of an NE are also proper nouns. In fact, based
on statistics collected from our evaluation corpus
(see Sections 5 and 6), this observation is correct
360
Experiment R P F
Baseline 54.76 81.70 65.57
Baseline+Induced Affixes 56.79 88.96 69.32
Baseline+Induced Affixes+Wiki 61.73 86.35 71.99
Table 7: 5-fold cross-validation joint modeling re-
sults for NER
97.3% of the time. Note, however, that this ob-
servation does not hold for English, since many
prepositions and determiners are part of an NE.
On the other hand, this observation largely holds
for Bengali because prepositions and determiners
are typically realized as noun suffixes.
This limited dependency between the POS tags
and the NE tags allows us to develop a simple
model for jointly learning the two tasks. More
specifically, we will use CRF++ to learn the joint
model. Training and test instances are generated
as described in the previous two subsections (i.e.,
one instance per word). The feature set will con-
sist of the union of the features that were used to
train the POS tagger and the NE tagger indepen-
dently, minus the POS-related features that were
used in the NE tagger. The class value of an in-
stance is computed as follows. If a word is not a
proper noun, its class is simply its POS tag. Oth-
erwise, its class is its NE tag, which can be PER,
ORG, LOC, or OTHERS. In other words, our joint
model exploits the observation that we made ear-
lier in the section by assuming that only proper
nouns can be part of a named entity. This allows
us to train a joint model without substantially in-
creasing the number of classes.
We again evaluate our joint model using 5-fold
CV experiments. The NE results of the model are
shown in Table 7. The rows here can be interpreted
in the same manner as those in Table 6. Compar-
ing these three experiments with their counterparts
in Table 6, we can see that, except for the base-
line, jointly modeling offers a significant improve-
ment of 3.3% in overall F-measure.10 In particu-
lar, the joint model benefits significantly from our
10The POS tagging results are not shown due to space lim-
itations. Overall, the POS accuracies drop insignificantly as
a result of joint modeling, for the following reason. Recall
from Section 5 that the major source of POS tagging errors
arises from the mislabeling of many proper nouns as com-
mon nouns, due primarily to the large number of common
nouns in the corpus. The joint model aggravates this prob-
lem by subcategorizing the proper nouns into different NE
classes, causing the tagger to have an even stronger bias to-
wards labeling a proper noun as a common noun than before.
Nevertheless, as seen from the results in Tables 6 and 7, such
a bias has yielded an increase in NER precision.
two knowledge sources, achieving an F-measure
of 71.99% when both of them are incorporated.
Finally, to better understand the value of the in-
duced affix features in the joint model as well as
the pipelined model described in Section 6, we
conducted an ablation experiment, in which we in-
corporated only the WIKI features into the base-
line feature set. With pipelined modeling, the F-
measure for NER is 68.87%, which is similar to
the case where both induced affixes and the WIKI
features are used. With joint modeling, however,
the F-measure for NER is 70.87%, which is 1%
lower than the best joint modeling score. These
results provide suggestive evidence that the in-
duced affix features play a significant role in the
improved performance of the joint model.
8 Conclusions
We have explored two types of linguistic fea-
tures, namely the induced affix features and the
Wikipedia-related features, to improve a Bengali
POS tagger and NE recognizer. Our experimen-
tal results have demonstrated that (1) both types of
features significantly improve a baseline POS tag-
ger and (2) the Wikipedia-related features signif-
icantly improve a baseline NE recognizer. More-
over, by exploiting the limited dependencies be-
tween Bengali POS tags and NE tags, we pro-
posed a new model for jointly learning the two
tasks, which not only avoids the error-propagation
problem present in the pipelined system architec-
ture, but also yields statistically significant im-
provements over the NE recognizer that is trained
independently of the POS tagger. When applied in
combination, our three extensions contributed to a
relative improvement of 7.5% in F-measure over
the baseline NE recognizer. Most importantly, we
believe that these extensions are of relevance and
interest to the EACL community because many
European and Middle Eastern languages resemble
Bengali in terms of not only their morphological
richness but also their scarcity of annotated cor-
pora. We plan to empirically verify our belief in
future work.
Acknowledgments
We thank the three anonymous reviewers for their
invaluable comments on the paper. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources. This work was
supported in part by NSF Grant IIS-0812261.
361
References
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of COLT, pages 92?100.
Razvan Bunescu and Marius Pas?ca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In Proceedings of EACL, pages 9?16.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of EACL, pages 59?66.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of EMNLP/VLC, pages 100?110.
Silviu Cucerzan and David Yarowsky. 1999. Lan-
guage independent named entity recognition com-
bining morphological and contextual evidence. In
Proceedings of EMNLP/VLC, pages 90?99.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on Wikipedia data. In Proceed-
ings of EMNLP-CoNLL, pages 708?716.
Sandipan Dandapat, Sudeshna Sarkar, and Anupam
Basu. 2007. Automatic part-of-speech tagging for
Bengali: An approach for morphologically rich lan-
guages in a poor resource scenario. In Proceedings
of the ACL Companion Volume, pages 221?224.
Kevin Duh. 2005. Jointly labeling multiple sequences:
A factorial HMM approach. In Proceedings of the
ACL Student Research Workshop, pages 19?24.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopad-
hyay. 2008. Named entity recognition in Bengali:
A conditional random field approach. In Proceed-
ings of IJCNLP, pages 589?594.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320?327.
Jun?ichi Kazama and Kentaro Torisawa. 2007. Ex-
ploiting Wikipedia as external knowledge for named
entity recognition. In Proceedings of EMNLP-
CoNLL, pages 698?707.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
Zornitsa Kozareva. 2006. Bootstrapping named entity
recognition with automatically generated gazetteer
lists. In Proceedings of the EACL Student Research
Workshop, pages 15?22.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Andrei Mikheev, Marc Moens, and Claire Grover.
1999. Named entity recognition without gazetteers.
In Proceedings of EACL, pages 1?8.
Grace Ngai and Radu Florian. 2001. Transformation
based learning in the fast lane. In Proceedings of
NAACL, pages 40?47.
Simone Paolo Ponzetto and Michael Strube. 2007a.
Deriving a large scale taxonomy from wikipedia. In
Proceedings of AAAI, pages 1440?1445.
Simone Paolo Ponzetto and Michael Strube. 2007b.
Knowledge derived from Wikipedia for computing
semantic relatedness. Journal of Artificial Intelli-
gence Research, 30:181?212.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP, pages 133?142.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148.
Smriti Singh, Kuhoo Gupta, Manish Shrivastava, and
Pushpak Bhattacharyya. 2006. Morphological rich-
ness offsets resource demand ? Experiences in con-
structing a POS tagger for Hindi. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 779?786.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. Journal of Machine
Learning Research, 8:693?723.
Yotaro Watanabe, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. A graph-based approach to named
entity categorization in Wikipedia using conditional
random fields. In Proceedings of EMNLP-CoNLL,
pages 649?657.
362
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 363?371,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Weakly Supervised Part-of-Speech Tagging for Morphologically-Rich,
Resource-Scarce Languages
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
This paper examines unsupervised ap-
proaches to part-of-speech (POS) tagging
for morphologically-rich, resource-scarce
languages, with an emphasis on Goldwa-
ter and Griffiths?s (2007) fully-Bayesian
approach originally developed for En-
glish POS tagging. We argue that ex-
isting unsupervised POS taggers unreal-
istically assume as input a perfect POS
lexicon, and consequently, we propose
a weakly supervised fully-Bayesian ap-
proach to POS tagging, which relaxes the
unrealistic assumption by automatically
acquiring the lexicon from a small amount
of POS-tagged data. Since such relaxation
comes at the expense of a drop in tag-
ging accuracy, we propose two extensions
to the Bayesian framework and demon-
strate that they are effective in improv-
ing a fully-Bayesian POS tagger for Ben-
gali, our representative morphologically-
rich, resource-scarce language.
1 Introduction
Unsupervised POS tagging requires neither man-
ual encoding of tagging heuristics nor the avail-
ability of data labeled with POS information.
Rather, an unsupervised POS tagger operates by
only assuming as input a POS lexicon, which con-
sists of a list of possible POS tags for each word.
As we can see from the partial POS lexicon for
English in Figure 1, ?the? is unambiguous with re-
spect to POS tagging, since it can only be a deter-
miner (DT), whereas ?sting? is ambiguous, since
it can be a common noun (NN), a proper noun
(NNP) or a verb (VB). In other words, the lexi-
con imposes constraints on the possible POS tags
Word POS tag(s)
... ...
running NN, JJ
sting NN, NNP, VB
the DT
... ...
Figure 1: A partial lexicon for English
of each word, and such constraints are then used
by an unsupervised tagger to label a new sentence.
Conceivably, tagging accuracy decreases with the
increase in ambiguity: unambiguous words such
as ?the? will always be tagged correctly; on the
other hand, unseen words (or words not present
in the POS lexicon) are among the most ambigu-
ous words, since they are not constrained at all
and therefore can receive any of the POS tags.
Hence, unsupervised POS tagging can present sig-
nificant challenges to natural language processing
researchers, especially when a large fraction of
the words are ambiguous. Nevertheless, the de-
velopment of unsupervised taggers potentially al-
lows POS tagging technologies to be applied to a
substantially larger number of natural languages,
most of which are resource-scarce and, in particu-
lar, have little or no POS-tagged data.
The most common approach to unsupervised
POS tagging to date has been to train a hidden
Markov model (HMM) in an unsupervised man-
ner to maximize the likelihood of an unannotated
corpus, using a special instance of the expectation-
maximization (EM) algorithm (Dempster et al,
1977) known as Baum-Welch (Baum, 1972).
More recently, a fully-Bayesian approach to un-
supervised POS tagging has been developed by
Goldwater and Griffiths (2007) [henceforth G&G]
as a viable alternative to the traditional maximum-
likelihood-based HMM approach. While unsuper-
vised POS taggers adopting both approaches have
363
demonstrated promising results, it is important to
note that they are typically evaluated by assuming
the availability of a perfect POS lexicon. This as-
sumption, however, is fairly unrealistic in practice,
as a perfect POS lexicon can only be constructed
by having a linguist manually label each word in
a language with its possible POS tags.1 In other
words, the labor-intensive POS lexicon construc-
tion process renders unsupervised POS taggers a
lot less unsupervised than they appear. To make
these unsupervised taggers practical, one could at-
tempt to automatically construct a POS lexicon, a
task commonly known as POS induction. How-
ever, POS induction is by no means an easy task,
and it is not clear how well unsupervised POS tag-
gers work when used in combination with an au-
tomatically constructed POS lexicon.
The goals of this paper are three-fold. First,
motivated by the successes of unsupervised ap-
proaches to English POS tagging, we aim to inves-
tigate whether such approaches, especially G&G?s
fully-Bayesian approach, can deliver similar per-
formance for Bengali, our representative resource-
scarce language. Second, to relax the unrealis-
tic assumption of employing a perfect lexicon as
in existing unsupervised POS taggers, we propose
a weakly supervised fully-Bayesian approach to
POS tagging, where we automatically construct a
POS lexicon from a small amount of POS-tagged
data. Hence, unlike a perfect POS lexicon, our au-
tomatically constructed lexicon is necessarily in-
complete, yielding a large number of words that
are completely ambiguous. The high ambiguity
rate inherent in our weakly supervised approach
substantially complicates the POS tagging pro-
cess. Consequently, our third goal of this paper is
to propose two potentially performance-enhancing
extensions to G&G?s Bayesian POS tagging ap-
proach, which exploit morphology and techniques
successfully used in supervised POS tagging.
The rest of the paper is organized as follows.
Section 2 presents related work on unsupervised
approaches to POS tagging. Section 3 gives an
introduction to G&G?s fully-Bayesian approach
to unsupervised POS tagging. In Section 4, we
describe our two extensions to G&G?s approach.
Section 5 presents experimental results on Bengali
POS tagging, focusing on evaluating the effective-
1When evaluating an unsupervised POS tagger, re-
searchers typically construct a pseudo-perfect POS lexicon
by collecting the possible POS tags of a word directly from
the corpus on which the tagger is to be evaluated.
ness of our two extensions in improving G&G?s
approach. Finally, we conclude in Section 6.
2 Related Work
With the notable exception of Synder et
al.?s (2008; 2009) recent work on unsupervised
multilingual POS tagging, existing approaches to
unsupervised POS tagging have been developed
and tested primarily on English data. For instance,
Merialdo (1994) uses maximum likelihood esti-
mation to train a trigram HMM. Schu?tze (1995)
and Clark (2000) apply syntactic clustering and
dimensionality reduction in a knowledge-free
setting to obtain meaningful clusters. Haghighi
and Klein (2006) develop a prototype-driven
approach, which requires just a few prototype
examples for each POS tag and exploits these
labeled words to constrain the labels of their
distributionally similar words. Smith and Eisner
(2005) train an unsupervised POS tagger using
contrastive estimation, which seeks to move
probability mass to a positive example e from
its neighbors (i.e., negative examples are created
by perturbing e). Wang and Schuurmans (2005)
improve an unsupervised HMM-based tagger by
constraining the learned structure to maintain
appropriate marginal tag probabilities and using
word similarities to smooth the lexical parameters.
As mentioned before, Goldwater and Griffiths
(2007) have recently proposed an unsupervised
fully-Bayesian POS tagging framework that op-
erates by integrating over the possible parameter
values instead of fixing a set of parameter values
for unsupervised sequence learning. Importantly,
this Bayesian approach facilitates the incorpora-
tion of sparse priors that result in a more practical
distribution of tokens to lexical categories (John-
son, 2007). Similar to Goldwater and Griffiths
(2007) and Johnson (2007), Toutanova and John-
son (2007) also use Bayesian inference for POS
tagging. However, their work departs from exist-
ing Bayesian approaches to POS tagging in that
they (1) introduce a new sparse prior on the dis-
tribution over tags for each word, (2) extend the
Latent Dirichlet Allocation model, and (3) explic-
itly model ambiguity class. While their tagging
model, like Goldwater and Griffiths?s, assumes as
input an incomplete POS lexicon and a large unla-
beled corpus, they consider their approach ?semi-
supervised? simply because of the human knowl-
edge involved in constructing the POS lexicon.
364
3 A Fully Bayesian Approach
3.1 Motivation
As mentioned in the introduction, the most com-
mon approach to unsupervised POS tagging is to
train an HMM on an unannotated corpus using the
Baum-Welch algorithm so that the likelihood of
the corpus is maximized. To understand what the
HMM parameters are, let us revisit how an HMM
simultaneously generates an output sequence w
= (w0, w1, ..., wn) and the associated hidden state
sequence t = (t0, t1, ..., tn). In the context of POS
tagging, each state of the HMM corresponds to a
POS tag, the output sequence w is the given word
sequence, and the hidden state sequence t is the
associated POS tag sequence. To generate w and
t, the HMM begins by guessing a state t0 and then
emitting w0 from t0 according to a state-specific
output distribution over word tokens. After that,
we move to the next state t1, the choice of which
is based on t0?s transition distribution, and emit
w1 according to t1?s output distribution. This gen-
eration process repeats until the end of the word
sequence is reached. In other words, the parame-
ters of an HMM, ?, are composed of a set of state-
specific (1) output distributions (over word tokens)
and (2) transition distributions, both of which can
be learned using the EM algorithm. Once learning
is complete, we can use the resulting set of param-
eters to find the most likely hidden state sequence
given a word sequence using the Viterbi algorithm.
Nevertheless, EM sometimes fails to find good
parameter values.2 The reason is that EM tries to
assign roughly the same number of word tokens to
each of the hidden states (Johnson, 2007). In prac-
tice, however, the distribution of word tokens to
POS tags is highly skewed (i.e., some POS cate-
gories are more populated with tokens than oth-
ers). This motivates a fully-Bayesian approach,
which, rather than committing to a particular set
of parameter values as in an EM-based approach,
integrates over all possible values of ? and, most
importantly, allows the use of priors to favor the
learning of the skewed distributions, through the
use of the term P (?|w) in the following equation:
P (t|w) =
?
P (t|w, ?)P (?|w)d? (1)
The question, then, is: which priors on ? would
allow the acquisition of skewed distributions? To
2When given good parameter initializations, however, EM
can find good parameter values for an HMM-based POS tag-
ger. See Goldberg et al (2008) for details.
answer this question, recall that in POS tagging, ?
is composed of a set of tag transition distributions
and output distributions. Each such distribution is
a multinomial (i.e., each trial produces exactly one
of some finite number of possible outcomes). For
a multinomial with K outcomes, a K-dimensional
Dirichlet distribution, which is conjugate to the
multinomial, is a natural choice of prior. For sim-
plicity, we assume that a distribution in ? is drawn
from a symmetric Dirichlet with a certain hyper-
parameter (see Teh et al (2006) for details).
The value of a hyperparameter, ?, affects the
skewness of the resulting distribution, as it as-
signs different probabilities to different distribu-
tions. For instance, when ? < 1, higher proba-
bilities are assigned to sparse multinomials (i.e.,
multinomials in which only a few entries are non-
zero). Intuitively, the tag transition distributions
and the output distributions in an HMM-based
POS tagger are sparse multinomials. As a re-
sult, it is logical to choose a Dirichlet prior with
? < 1. By integrating over all possible param-
eter values, the probability that i-th outcome, yi,
takes the value k, given the previous i ? 1 out-
comes y?i= (y1, y2, ..., yi?1), is
P (k|y?i, ?) =
?
P (k|?)P (?|y?i, ?)d? (2)
= nk + ?i? 1 + K? (3)
where nk is the frequency of k in y?i. See
MacKay and Peto (1995) for the derivation.
3.2 Model
Our baseline POS tagging model is a standard tri-
gram HMM with tag transition distributions and
output distributions, each of which is a sparse
multinomial that is learned by applying a symmet-
ric Dirichlet prior:
ti | ti?1, ti?2, ? (ti?1,ti?2) ? Mult(? (ti?1,ti?2))
wi | ti, ?(ti) ? Mult(?(ti))
? (ti?1,ti?2) | ? ? Dirichlet(?)
?(ti) | ? ? Dirichlet(?)
where wi and ti denote the i-th word and tag. With
a tagset of size T (including a special tag used as
sentence delimiter), each of the tag transition dis-
tributions has T components. For the output sym-
bols, each of the ?(ti) has Wti components, where
Wti denotes the number of word types that can be
emitted from the state corresponding to ti.
365
From the closed form in Equation 3, given pre-
vious outcomes, we can compute the tag transition
and output probabilities of the model as follows:
P (ti|t?i, ?) =
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
(4)
P (wi|ti, t?i,w?i, ?) =
n(ti,wi) + ?
nti + Wti?
(5)
where n(ti?2,ti?1,ti) and n(ti,wi) are the frequen-
cies of observing the tag trigram (ti?2, ti?1, ti)
and the tag-word pair (ti, wi), respectively. These
counts are taken from the i ? 1 tags and words
generated previously. The inference procedure de-
scribed next exploits the property that trigrams
(and outputs) are exchangeable; that is, the prob-
ability of a set of trigrams (and outputs) does not
depend on the order in which it was generated.
3.3 Inference Procedure
We perform inference using Gibbs sampling (Ge-
man and Geman, 1984), using the following pos-
terior distribution to generate samples:
P (t|w, ?, ?) ? P (w|t, ?)P (t|?)
Starting with a random assignment of a POS tag
to each word (subject to the constraints in the POS
lexicon), we resample each POS tag, ti, accord-
ing to the conditional distribution shown in Figure
2. Note that the current counts of other trigrams
and outputs can be used as ?previous? observa-
tions due to the property of exchangeability.
Following G&G, we use simulated annealing to
find the MAP tag sequence. The temperature de-
creases by a factor of exp(
log( ?2?1 )
N?1 ) after each iter-
ation, where ?1 is the initial temperature and ?2 is
the temperature after N sampling iterations.
4 Two Extensions
In this section, we present two extensions to
G&G?s fully-Bayesian framework to unsupervised
POS tagging, namely, induced suffix emission and
discriminative prediction.
4.1 Induced Suffix Emission
For morphologically-rich languages like Bengali,
a lot of grammatical information (e.g., POS) is ex-
pressed via suffixes. In fact, several approaches to
unsupervised POS induction for morphologically-
rich languages have exploited the observation that
some suffixes can only be associated with a small
number of POS tags (e.g., Clark (2003), Dasgupta
and Ng (2007)). To exploit suffixes in HMM-
based POS tagging, one can (1) convert the word-
based POS lexicon to a suffix-based POS lexicon,
which lists the possible POS tags for each suffix;
and then (2) have the HMM emit suffixes rather
than words, subject to the constraints in the suffix-
based POS lexicon. Such a suffix-based HMM,
however, may suffer from over-generalization. To
prevent over-generalization and at the same time
exploit suffixes, we propose as our first exten-
sion to G&G?s framework a hybrid approach to
word/suffix emission: a word is emitted if it is
present in the word-based POS lexicon; otherwise,
its suffix is emitted. In other words, our approach
imposes suffix-based constraints on the tagging of
words that are unseen w.r.t. the word-based POS
lexicon. Below we show how to induce the suffix
of a word and create the suffix-based POS lexicon.
Inducing suffixes To induce suffixes, we rely on
Keshava and Pitler?s (2006) method. Assume that
(1) V is a vocabulary (i.e., a set of distinct words)
extracted from a large, unannotated corpus, (2) C1
and C2 are two character sequences, and (3) C1C2
is the concatenation of C1 and C2. If C1C2 and
C1 are found in V , we extract C2 as a suffix.
However, this unsupervised suffix induction
method is arguably overly simplistic and hence
many of the induced affixes could be spurious. To
identify suffixes that are likely to be correct, we
employ a simple procedure: we (1) score each suf-
fix by multiplying its frequency (i.e., the number
of distinct words in V to which each suffix at-
taches) and its length3, and (2) select only those
whose score is above a certain threshold. In our
experiments, we set this threshold to 50, and gen-
erate our vocabulary from five years of articles
taken from the Bengali newspaper Prothom Alo.
This enables us to induce 975 suffixes.
Constructing a suffix-based POS lexicon
Next, we construct a suffix-based POS lexicon.
For each word w in the original word-based
POS lexicon, we (1) use the induced suffix list
obtained in the previous step to identify the
longest-matching suffix of w, and then (2) assign
all the POS tags associated with w to this suffix.
Incorporating suffix-based output distributions
Finally, we extend our trigram model by introduc-
3The dependence on frequency and length is motivated by
the observation that less frequent and shorter affixes are more
likely to be erroneous (see Goldsmith (2001)).
366
P (ti|t?i,w, ?, ?) ?
n(ti,wi) + ?
nti + Wti?
.
n(ti?2,ti?1,ti) + ?
n(ti?2,ti?1) + T?
.
n(ti?1,ti,ti+1) + I(ti?2 = ti?1 = ti = ti+1) + ?
n(ti?1,ti) + I(ti?2 = ti?1 = ti) + T?
.
n(ti,ti+1,ti+2) + I(ti?2 = ti = ti+2, ti?1 = ti+1) + I(ti?1 = ti = ti+1 = ti+2) + ?
n(ti,ti+1) + I(ti?2 = ti, ti?1 = ti+1) + I(ti?1 = ti = ti+1) + T?
Figure 2: The sampling distribution for ti (taken directly from Goldwater and Griffiths (2007)). All nx
values are computed from the current values of all tags except for ti. Here, I(arg) is a function that
returns 1 if arg is true and 0 otherwise, and t?i refers to the current values of all tags except for ti.
ing a state-specific probability distribution over in-
duced suffixes. Specifically, if the current word is
present in the word-based POS lexicon, or if we
cannot find any suffix for the word using the in-
duced suffix list, then we emit the word. Other-
wise, we emit its suffix according to a suffix-based
output distribution, which is drawn from a sym-
metric Dirichlet with hyperparameter ?:
si | ti, ?(ti) ? Mult(?(ti))
?(ti) | ? ? Dirichlet(?)
where si denotes the induced suffix of the i-th
word. The distribution, ?(ti), has Sti components,
where Sti denotes the number of induced suffixes
that can be emitted from the state corresponding to
ti. We compute the induced suffix emission prob-
abilities of the model as follows:
P (si|ti, t?i, s?i, ?) =
n(ti,si) + ?
nti + Sti?
(6)
where n(ti,si) is the frequency of observing the
tag-suffix pair (ti, si).
This extension requires that we slightly modify
the inference procedure. Specifically, if the cur-
rent word is unseen (w.r.t. the word-based POS
lexicon) and has a suffix (according to the induced
suffix list), then we sample from a distribution that
is almost identical to the one shown in Figure 2,
except that we replace the first fraction (i.e., the
fraction involving the emission counts) with the
one shown in Equation (6). Otherwise, we simply
sample from the distribution in Figure 2.
4.2 Discriminative Prediction
As mentioned in the introduction, the (word-
based) POS lexicons used in existing approaches
to unsupervised POS tagging were created some-
what unrealistically by collecting the possible
POS tags of a word directly from the corpus on
which the tagger is to be evaluated. To make the
lexicon formation process more realistic, we pro-
pose a weakly supervised approach to Bayesian
POS tagging, in which we automatically create the
word-based POS lexicon from a small set of POS-
tagged sentences that is disjoint from the test data.
Adopting a weakly supervised approach has an ad-
ditional advantage: the presence of POS-tagged
sentences makes it possible to exploit techniques
developed for supervised POS tagging, which is
the idea behind discriminative prediction, our sec-
ond extension to G&G?s framework.
Given a small set of POS-tagged sentences L,
discriminative prediction uses the statistics col-
lected from L to predict the POS of a word in a
discriminative fashion whenever possible. More
specifically, discriminative prediction relies on
two simple ideas typically exploited by supervised
POS tagging algorithms: (1) if the target word
(i.e., the word whose POS tag is to be predicted)
appears in L, we can label the word with its POS
tag in L; and (2) if the target word does not appear
in L but its context does, we can use its context to
predict its POS tag. In bigram and trigram POS
taggers, the context of a word is represented us-
ing the preceding one or two words. Nevertheless,
since L is typically small in a weakly supervised
setting, it is common for a target word not to sat-
isfy any of the two conditions above. Hence, if it is
not possible to predict a target word in a discrim-
inative fashion (due to the limited size of L), we
resort to the sampling equation in Figure 2.
To incorporate the above discriminative deci-
sion steps into G&G?s fully-Bayesian framework
for POS tagging, the algorithm estimates three
types of probability distributions from L. First,
to capture context, it computes (1) a distribu-
tion over the POS tags following a word bi-
gram, (wi?2, wi?1), that appears in L [henceforth
D1(wi?2, wi?1)] and (2) a distribution over the
POS tags following a word unigram, wi?1, that ap-
pears in L [henceforth D2(wi?1)]. Then, to cap-
367
Algorithm 1 Algorithm for incorporating discrim-
inative prediction
Input: wi: current word
wi?1: previous word
wi?2: second previous word
L: a set of POS-tagged sentences
Output: Predicted tag, ti
1: if wi ? L then
2: ti ? Tag drawn from the distribution of wi?s candi-
date tags
3: else if (wi?2, wi?1) ? L then
4: ti ? Tag drawn from the distribution of the POS tags
following the word bigram (wi?2, wi?1)
5: else if wi?1 ? L then
6: ti ? Tag drawn from the distribution of the POS tags
following the word unigram wi?1
7: else
8: ti ? Tag obtained using the sampling equation
9: end if
ture the fact that a word can have more than one
POS tag, it also estimates a distribution over POS
tags for each word wi that appears in L [hence-
forth D3(wi)].
Implemented as a set of if-else clauses, the al-
gorithm uses these three types of distributions to
tag a target word, wi, in a discriminative manner.
First, it checks whether wi appears in L (line 1). If
so, it tags wi according to D3(wi). Otherwise, it
attempts to label wi based on its context. Specifi-
cally, if (wi?2, wi?1), the word bigram preceding
wi, appears in L (line 3), then wi is tagged accord-
ing to D1(wi?2, wi?1). Otherwise, it backs off to
a unigram distribution: if wi?1, the word preced-
ing wi, appears in L (line 5), then wi is tagged
according to D2(wi?1). Finally, if it is not possi-
ble to tag the word discriminatively (i.e., if all the
above cases fail), it resorts to the sampling equa-
tion (lines 7?8). We apply simulated annealing to
all four cases in this iterative tagging procedure.
5 Evaluation
5.1 Experimental Setup
Corpus Our evaluation corpus is the one used
in the shared task of the IJCNLP-08 Workshop on
NER for South and South East Asian Languages.4
Specifically, we use the portion of the Bengali
dataset that is manually POS-tagged. IIIT Hy-
derabad?s POS tagset5, which consists of 26 tags
specifically developed for Indian languages, has
been used to annotate the data. The corpus is com-
posed of a training set and a test set with approxi-
4The corpus is available from http://ltrc.iiit.ac.in/ner-ssea-
08/index.cgi?topic=5.
5http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
mately 50K and 30K tokens, respectively. Impor-
tantly, all our POS tagging results will be reported
using only the test set; the training set will be used
for lexicon construction, as we will see shortly.
Tagset We collapse the set of 26 POS tags into
15 tags. Specifically, while we retain the tags cor-
responding to the major POS categories, we merge
some of the infrequent tags designed to capture
Indian language specific structure (e.g., reduplica-
tion, echo words) into a category called OTHERS.
Hyperparameter settings Recall that our tag-
ger consists of three types of distributions ? tag
transition distributions, word-based output distri-
butions, and suffix-based output distributions ?
drawn from a symmetric Dirichlet with ?, ?,
and ? as the underlying hyperparameters, respec-
tively. We automatically determine the values of
these hyperparameters by (1) randomly initializ-
ing them and (2) resampling their values by using
a Metropolis-Hastings update (Gilks et al, 1996)
at the end of each sampling iteration. Details of
this update process can be found in G&G.
Inference Inference is performed by running a
Gibbs sampler for 5000 iterations. The initial tem-
perature is set to 2.0, which is gradually lowered
to 0.08 over the iterations. Owing to the random-
ness involved in hyperparameter initialization, all
reported results are averaged over three runs.
Lexicon construction methods To better under-
stand the role of a POS lexicon in tagging perfor-
mance, we evaluate each POS tagging model by
employing lexicons constructed by three methods.
The first lexicon construction method, arguably
the most unrealistic among the three, follows that
of G&G: for each word, w, in the test set, we (1)
collect from each occurrence of w in the training
set and the test set its POS tag, and then (2) insert
w and all the POS tags collected for w into the
POS lexicon. This method is unrealistic because
(1) in practice, a human needs to list all possible
POS tags for each word in order to construct this
lexicon, thus rendering the resulting tagger con-
siderably less unsupervised than it appears; and
(2) constructing the lexicon using the dataset on
which the tagger is to be evaluated implies that
there is no unseen word w.r.t. the lexicon, thus un-
realistically simplifies the POS tagging task. To
make the method more realistic, G&G also create
a set of relaxed lexicons. Each of these lexicons
includes the tags for only the words that appear
at least d times in the test corpus, where d ranges
368
1 2 3 4 5 6 7 8 9 10
30
40
50
60
70
80
90
d
Ac
cu
ra
cy
 (%
)
(a) Lexicon 1
 
 
MLHMM
BHMM
BHMM+IS
1 2 3 4 5 6 7 8 9 10
30
35
40
45
50
55
60
65
70
75
d
Ac
cu
ra
cy
 (%
)
(b) Lexicon 2
 
 
MLHMM
BHMM
BHMM+IS
Figure 3: Accuracies of POS tagging models using (a) Lexicon 1 and (b) Lexicon 2
from 1 to 10 in our experiments. Any unseen (i.e.,
out-of-dictionary) word is ambiguous among the
15 possible tags. Not surprisingly, both ambigu-
ity and the unseen word rate increase with d. For
instance, the ambiguous token rate increases from
40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1
tags/token (d=10). Similarly, the unseen word rate
increases from 16% (d=2) to 46% (d=10). We will
refer to this set of tag dictionaries as Lexicon 1.
The second method generates a set of relaxed
lexicons, Lexicon 2, in essentially the same way
as the first method, except that these lexicons in-
clude only the words that appear at least d times
in the training data. Importantly, the words that
appear solely in the test data are not included in
any of these relaxed POS lexicons. This makes
Lexicon 2 a bit more realistic than Lexicon 1 in
terms of the way they are constructed. As a result,
in comparison to Lexicon 1, Lexicon 2 has a con-
siderably higher ambiguous token rate and unseen
word rate: its ambiguous token rate ranges from
64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6
tags/token (d=10), and its unseen word rate ranges
from 25% (d=1) to 50% (d=10).
The third method, arguably the most realistic
among the three, is motivated by our proposed
weakly supervised approach. In this method, we
(1) form ten different datasets from the (labeled)
training data of sizes 5K words, 10K words, . . .,
50K words, and then (2) create one POS lexicon
from each dataset L by listing, for each word w in
L, all the tags associated with w in L. This set of
tag dictionaries, which we will refer to as Lexicon
3, has an ambiguous token rate that ranges from
57.7% with 5.1 tags/token (50K) to 61.5% with
8.1 tags/token (5K), and an unseen word rate that
ranges from 25% (50K) to 50% (5K).
5.2 Results and Discussion
5.2.1 Baseline Systems
We use as our first baseline system G&G?s
Bayesian POS tagging model, as our goal is to
evaluate the effectiveness of our two extensions
in improving their model. To further gauge the
performance of G&G?s model, we employ another
baseline commonly used in POS tagging exper-
iments, which is an unsupervised trigram HMM
trained by running EM to convergence.
As mentioned previously, we evaluate each tag-
ging model by employing the three POS lexicons
described in the previous subsection. Figure 3(a)
shows how the tagging accuracy varies with d
when Lexicon 1 is used. Perhaps not surpris-
ingly, the trigram HMM (MLHMM) and G&G?s
Bayesian model (BHMM) achieve almost identi-
cal accuracies when d=1 (i.e., the complete lexi-
con with a zero unseen word rate). As d increases,
both ambiguity and the unseen word rate increase;
as a result, the tagging accuracy decreases. Also,
consistent with G&G?s results, BHMM outper-
forms MLHMM by a large margin (4?7%).
Similar performance trends can be observed
when Lexicon 2 is used (see Figure 3(b)). How-
ever, both baselines achieve comparatively lower
tagging accuracies, as a result of the higher unseen
word rate associated with Lexicon 2.
369
5 10 15 20 25 30 35 40 45 50
45
50
55
60
65
70
75
80
Training data (K)
Ac
cu
ra
cy
 (%
)
Lexicon 3
 
 
SHMM
BHMM
BHMM+IS
BHMM+IS+DP
Figure 4: Accuracies of the POS tagging models
using Lexicon 3
Results using Lexicon 3 are shown in Figure
4. Owing to the availability of POS-tagged sen-
tences, we replace MLHMM with its supervised
counterpart that is trained on the available labeled
data, yielding the SHMM baseline. The accuracies
of SHMM range from 48% to 67%, outperforming
BHMM as the amount of labeled data increases.
5.2.2 Adding Induced Suffix Emission
Next, we augment BHMM with our first
extension, induced suffix emission, yielding
BHMM+IS. For Lexicon 1, BHMM+IS achieves
the same accuracy as the two baselines when d=1.
The reason is simple: as all the test words are
in the POS lexicon, the tagger never emits an in-
duced suffix. More importantly, BHMM+IS beats
BHMM and MLHMM by 4?9% and 10?14%, re-
spectively. Similar trends are observed for Lex-
icon 2, where BHMM+IS outperforms BHMM
and MLHMM by a larger margin of 5?10% and
12?16%, respectively. For Lexicon 3, BHMM+IS
outperforms SHMM, the stronger baseline, by 6?
11%. Overall, these results suggest that induced
suffix emission is a strong performance-enhancing
extension to G&G?s approach.
5.2.3 Adding Discriminative Prediction
Finally, we augment BHMM+IS with discrimi-
native prediction, yielding BHMM+IS+DP. Since
this extension requires labeled data, it can only be
applied in combination with Lexicon 3. As seen
in Figure 4, BHMM+IS+DP outperforms SHMM
by 10?14%. Its discriminative nature proves to be
Predicted Tag Correct Tag % of Error
NN NNP 8.4
NN JJ 6.9
VM VAUX 5.9
Table 1: Most frequent POS tagging errors for
BHMM+IS+DP on the 50K-word training set
strong as it even beats BHMM+IS by 3?4%.
5.2.4 Error Analysis
Table 1 lists the most common types of er-
rors made by the best-performing tagging model,
BHMM+IS+DP (50K-word labeled data). As we
can see, common nouns and proper nouns (row
1) are difficult to distinguish, due in part to the
case insensitivity of Bengali. Also, it is difficult
to distinguish Bengali common nouns and adjec-
tives (row 2), as they are distributionally similar
to each other. The confusion between main verbs
[VM] and auxiliary verbs [VAUX] (row 3) arises
from the fact that certain Bengali verbs can serve
as both a main verb and an auxiliary verb, depend-
ing on the role the verb plays in the verb sequence.
6 Conclusions
While Goldwater and Griffiths?s fully-Bayesian
approach and the traditional maximum-likelihood
parameter-based approach to unsupervised POS
tagging have offered promising results for English,
we argued in this paper that such results were ob-
tained under the unrealistic assumption that a per-
fect POS lexicon is available, which renders these
taggers less unsupervised than they appear. As a
result, we investigated a weakly supervised fully-
Bayesian approach to POS tagging, which relaxes
the unrealistic assumption by automatically ac-
quiring the lexicon from a small amount of POS-
tagged data. Since such relaxation comes at the
expense of a drop in tagging accuracy, we pro-
posed two performance-enhancing extensions to
the Bayesian framework, namely, induced suffix
emission and discriminative prediction, which ef-
fectively exploit morphology and techniques from
supervised POS tagging, respectively.
Acknowledgments
We thank the three anonymous reviewers and
Sajib Dasgupta for their comments. We also thank
CRBLP, BRAC University, Bangladesh, for pro-
viding us with Bengali resources and Taufiq Hasan
Al Banna for his MATLAB code. This work was
supported in part by NSF Grant IIS-0812261.
370
References
Leonard E. Baum. 1972. An equality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of Markov processes. In-
equalities, 3:1?8.
Alexander Clark. 2000. Inducing syntactic categories
by context distribution clustering. In Proceedings of
CoNLL: Short Papers, pages 91?94.
Alexander Clark. 2003. Combining distributional and
morphological information for part-of-speech induc-
tion. In Proceedings of the EACL, pages 59?66.
Sajib Dasgupta and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of EMNLP-CoNLL, pages
218?227.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society. Series B (Methodological), 39:1?38.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721?741.
Walter R. Gilks, Sylvia Richardson, and David
J. Spiegelhalter (editors). 1996. Markov Chain
Monte Carlo in Practice. Chapman & Hall, Suffolk.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL-
08:HLT, pages 746?754.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 27(2):153?198.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT-NAACL, pages 320?327.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL, pages 296?305.
Samarth Keshava and Emily Pitler. 2006. A simpler,
intuitive approach to morpheme induction. In PAS-
CAL Challenge Workshop on Unsupervised Segmen-
tation of Words into Morphemes.
David J. C. MacKay and Linda C. Bauman Peto. 1995.
A hierarchical Dirichlet language model. Natural
Language Engineering, 1:289?307.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?172.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of EACL, pages 141?148.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the ACL, pages 354?362.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2008. Unsupervised multi-
lingual learning for POS tagging. In Proceedings of
EMNLP, pages 1041?1050.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2009. Adding more lan-
guages improves unsupervised multilingual tagging.
In Proceedings of NAACL-HLT.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1527?1554.
Kristina Toutanova and Mark Johnson. 2007. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
Qin Iris Wang and Dale Schuurmans. 2005. Im-
proved estimation for unsupervised part-of-speech
tagging. In Proceedings of the 2005 IEEE Interna-
tional Conference on Natural Language Processing
and Knowledge Engineering (IEEE NLP-KE), pages
219?224.
371
Coling 2010: Poster Volume, pages 365?373,
Beijing, August 2010
Conundrums in Unsupervised Keyphrase Extraction:
Making Sense of the State-of-the-Art
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
{saidul,vince}@hlt.utdallas.edu
Abstract
State-of-the-art approaches for unsuper-
vised keyphrase extraction are typically
evaluated on a single dataset with a single
parameter setting. Consequently, it is un-
clear how effective these approaches are
on a new dataset from a different domain,
and how sensitive they are to changes in
parameter settings. To gain a better under-
standing of state-of-the-art unsupervised
keyphrase extraction algorithms, we con-
duct a systematic evaluation and analysis
of these algorithms on a variety of stan-
dard evaluation datasets.
1 Introduction
The keyphrases for a given document refer to a
group of phrases that represent the document. Al-
though we often come across texts from different
domains such as scientific papers, news articles
and blogs, which are labeled with keyphrases by
the authors, a large portion of the Web content re-
mains untagged. While keyphrases are excellent
means for providing a concise summary of a doc-
ument, recent research results have suggested that
the task of automatically identifying keyphrases
from a document is by no means trivial. Re-
searchers have explored both supervised and un-
supervised techniques to address the problem of
automatic keyphrase extraction. Supervised meth-
ods typically recast this problem as a binary clas-
sification task, where a model is trained on anno-
tated data to determine whether a given phrase is
a keyphrase or not (e.g., Frank et al (1999), Tur-
ney (2000; 2003), Hulth (2003), Medelyan et al
(2009)). A disadvantage of supervised approaches
is that they require a lot of training data and yet
show bias towards the domain on which they are
trained, undermining their ability to generalize
well to new domains. Unsupervised approaches
could be a viable alternative in this regard.
The unsupervised approaches for keyphrase ex-
traction proposed so far have involved a number
of techniques, including language modeling (e.g.,
Tomokiyo and Hurst (2003)), graph-based rank-
ing (e.g., Zha (2002), Mihalcea and Tarau (2004),
Wan et al (2007), Wan and Xiao (2008), Liu
et al (2009a)), and clustering (e.g., Matsuo and
Ishizuka (2004), Liu et al (2009b)). While these
methods have been shown to work well on a par-
ticular domain of text such as short paper abstracts
and news articles, their effectiveness and portabil-
ity across different domains have remained an un-
explored issue. Worse still, each of them is based
on a set of assumptions, which may only hold for
the dataset on which they are evaluated.
Consequently, we have little understanding of
how effective the state-of the-art systems would be
on a completely new dataset from a different do-
main. A few questions arise naturally. How would
these systems perform on a different dataset with
their original configuration? What could be the
underlying reasons in case they perform poorly?
Is there any system that can generalize fairly well
across various domains?
We seek to gain a better understanding of the
state of the art in unsupervised keyphrase ex-
traction by examining the aforementioned ques-
tions. More specifically, we compare five unsu-
pervised keyphrase extraction algorithms on four
corpora with varying domains and statistical char-
acteristics. These algorithms represent the ma-
365
jor directions in this research area, including Tf-
Idf and four recently proposed systems, namely,
TextRank (Mihalcea and Tarau, 2004), SingleR-
ank (Wan and Xiao, 2008), ExpandRank (Wan
and Xiao, 2008), and a clustering-based approach
(Liu et al, 2009b). Since none of these systems
(except TextRank) are publicly available, we re-
implement all of them and make them freely avail-
able for research purposes.1 To our knowledge,
this is the first attempt to compare the perfor-
mance of state-of-the-art unsupervised keyphrase
extraction systems on multiple datasets.
2 Corpora
Our four evaluation corpora belong to different
domains with varying document properties. Ta-
ble 1 provides an overview of each corpus.
The DUC-2001 dataset (Over, 2001), which is
a collection of 308 news articles, is annotated by
Wan and Xiao (2008). We report results on all 308
articles in our evaluation.
The Inspec dataset is a collection of 2,000 ab-
stracts from journal papers including the paper ti-
tle. Each document has two sets of keyphrases as-
signed by the indexers: the controlled keyphrases,
which are keyphrases that appear in the In-
spec thesaurus; and the uncontrolled keyphrases,
which do not necessarily appear in the thesaurus.
This is a relatively popular dataset for automatic
keyphrase extraction, as it was first used by Hulth
(2003) and later by Mihalcea and Tarau (2004)
and Liu et al (2009b). In our evaluation, we use
the set of 500 abstracts designated by these previ-
ous approaches as the test set and its set of uncon-
trolled keyphrases. Note that the average docu-
ment length for this dataset is the smallest among
all our datasets.
The NUS Keyphrase Corpus (Nguyen and
Kan, 2007) includes 211 scientific conference pa-
pers with lengths between 4 to 12 pages. Each
paper has one or more sets of keyphrases assigned
by its authors and other annotators. We use all the
211 papers in our evaluation. Since the number
of annotators can be different for different docu-
ments and the annotators are not specified along
with the annotations, we decide to take the union
1See http://www.hlt.utdallas.edu/
?
saidul/code.html for details.
of all the gold standard keyphrases from all the
sets to construct one single set of annotation for
each paper. As Table 1 shows, each NUS pa-
per, both in terms of the average number of to-
kens (8291) and candidate phrases (2027) per pa-
per, is more than five times larger than any doc-
ument from any other corpus. Hence, the num-
ber of candidate keyphrases that can be extracted
is potentially large, making this corpus the most
challenging of the four.
Finally, the ICSI meeting corpus (Janin et al,
2003), which is annotated by Liu et al (2009a),
includes 161 meeting transcriptions. Following
Liu et al, we remove topic segments marked as
?chitchat? and ?digit? from the dataset and use all
the remaining segments for evaluation. Each tran-
script contains three sets of keyphrases produced
by the same three human annotators. Since it is
possible to associate each set of keyphrases with
its annotator, we evaluate each system on this
dataset three times, once for each annotator, and
report the average score. Unlike the other three
datasets, the gold standard keys for the ICSI cor-
pus are mostly unigrams.
3 Unsupervised Keyphrase Extractors
A generic unsupervised keyphrase extraction sys-
tem typically operates in three steps (Section 3.1),
which will help understand the unsupervised sys-
tems explained in Section 3.2.
3.1 Generic Keyphrase Extractor
Step 1: Candidate lexical unit selection The
first step is to filter out unnecessary word to-
kens from the input document and generate a list
of potential keywords using heuristics. Com-
monly used heuristics include (1) using a stop
word list to remove non-keywords (e.g., Liu et al
(2009b)) and (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to
be considered candidate keywords (Mihalcea and
Tarau (2004), Liu et al (2009a), Wan and Xiao
(2008)). In all of our experiments, we follow Wan
and Xiao (2008) and select as candidates words
with the following Penn Treebank tags: NN, NNS,
NNP, NNPS, and JJ, which are obtained using the
Stanford POS tagger (Toutanova and Manning,
2000).
366
Corpora
DUC-2001 Inspec NUS ICSI
Type News articles Paper abstracts Full papers Meeting transcripts
# Documents 308 500 211 161
# Tokens/Document 876 134 8291 1611
# Candidate words/Document 312 57 3271 453
# Candidate phrases/Document 207 34 2027 296
# Tokens/Candidate phrase 1.5 1.7 1.6 1.5
# Gold keyphrases 2484 4913 2327 582
# Gold keyphrases/Document 8.1 9.8 11.0 3.6
U/B/T/O distribution (%) 17/61/18/4 13/53/25/9 27/50/16/7 68/29/2/1
# Tokens/Gold keyphrase 2.1 2.3 2.1 1.3
Table 1: Corpus statistics for the four datasets used in this paper. A candidate word/phrase, typically a sequence
of one or more adjectives and nouns, is extracted from the document initially and considered a potential keyphrase. The
U/B/T/O distribution indicates how the gold standard keys are distributed among unigrams, bigrams, trigrams, and other
higher order n-grams.
Step 2: Lexical unit ranking Once the can-
didate list is generated, the next task is to rank
these lexical units. To accomplish this, it is nec-
essary to build a representation of the input text
for the ranking algorithm. Depending on the un-
derlying approach, each candidate word is repre-
sented by its syntactic and/or semantic relation-
ship with other candidate words. The relationship
can be defined using co-occurrence statistics, ex-
ternal resources (e.g., neighborhood documents,
Wikipedia), or other syntactic clues.
Step 3: Keyphrase formation In the final step,
the ranked list of candidate words is used to form
keyphrases. A candidate phrase, typically a se-
quence of nouns and adjectives, is selected as a
keyphrase if (1) it includes one or more of the
top-ranked candidate words (Mihalcea and Tarau
(2004), Liu et al (2009b)), or (2) the sum of the
ranking scores of its constituent words makes it a
top scoring phrase (Wan and Xiao, 2008).
3.2 The Five Keyphrase Extractors
As mentioned above, we re-implement five unsu-
pervised approaches for keyphrase extraction. Be-
low we provide a brief overview of each system.
3.2.1 Tf-Idf
Tf-Idf assigns a score to each term t in a doc-
ument d based on t?s frequency in d (term fre-
quency) and how many other documents include
t (inverse document frequency) and is defined as:
tfidft = tft ? log(D/Dt) (1)
where D is the total number of documents and Dt
is the number of documents containing t.
Given a document, we first compute the Tf-
Idf score of each candidate word (see Step 1 of
the generic algorithm). Then, we extract all the
longest n-grams consisting of candidate words
and score each n-gram by summing the Tf-Idf
scores of its constituent unigrams. Finally, we out-
put the top N n-grams as keyphrases.
3.2.2 TextRank
In the TextRank algorithm (Mihalcea and Ta-
rau, 2004), a text is represented by a graph. Each
vertex corresponds to a word type. A weight,
wij , is assigned to the edge connecting the two
vertices, vi and vj , and its value is the number
of times the corresponding word types co-occur
within a window of W words in the associated
text. The goal is to (1) compute the score of each
vertex, which reflects its importance, and then (2)
use the word types that correspond to the highest-
scored vertices to form keyphrases for the text.
The score for vi, S(vi), is initialized with a de-
fault value and is computed in an iterative manner
until convergence using this recursive formula:
S(vi) = (1? d) + d?
?
vj?Adj(vi)
wji?
vk?Adj(vj)
wjk
S(vj)
(2)
where Adj(vi) denotes vi?s neighbors and d is the
damping factor set to 0.85 (Brin and Page, 1998).
Intuitively, a vertex will receive a high score if it
has many high-scored neighbors. As noted before,
after convergence, the T% top-scored vertices are
367
selected as keywords. Adjacent keywords are then
collapsed and output as a keyphrase.
According to Mihalcea and Tarau (2004), Tex-
tRank?s best score on the Inspec dataset is
achieved when only nouns and adjectives are used
to create a uniformly weighted graph for the text
under consideration, where an edge connects two
word types only if they co-occur within a window
of two words. Hence, our implementation of Tex-
tRank follows this configuration.
3.2.3 SingleRank
SingleRank (Wan and Xiao, 2008) is essen-
tially a TextRank approach with three major dif-
ferences. First, while each edge in a TextRank
graph (in Mihalcea and Tarau?s implementation)
has the same weight, each edge in a SingleRank
graph has a weight equal to the number of times
the two corresponding word types co-occur. Sec-
ond, while in TextRank only the word types that
correspond to the top-ranked vertices can be used
to form keyphrases, in SingleRank, we do not fil-
ter out any low-scored vertices. Rather, we (1)
score each candidate keyphrase, which can be any
longest-matching sequence of nouns and adjec-
tives in the text under consideration, by summing
the scores of its constituent word types obtained
from the SingleRank graph, and (2) output the N
highest-scored candidates as the keyphrases for
the text. Finally, SingleRank employs a window
size of 10 rather than 2.
3.2.4 ExpandRank
ExpandRank (Wan and Xiao, 2008) is a
TextRank extension that exploits neighborhood
knowledge for keyphrase extraction. For a given
document d, the approach first finds its k near-
est neighboring documents from the accompany-
ing document collection using a similarity mea-
sure (e.g., cosine similarity). Then, the graph for
d is built using the co-occurrence statistics of the
candidate words collected from the document it-
self and its k nearest neighbors.
Specifically, each document is represented by
a term vector where each vector dimension cor-
responds to a word type present in the document
and its value is the Tf-Idf score of that word type
for that document. For a given document d0, its k
nearest neighbors are identified, and together they
form a larger document set of k+1 documents,
D = {d0, d1, d2, ..., dk}. Given this document
set, a graph is constructed, where each vertex cor-
responds to a candidate word type in D, and each
edge connects two vertices vi and vj if the corre-
sponding word types co-occur within a window of
W words in the document set. The weight of an
edge, w(vi, vj), is computed as follows:
w(vi, vj) =
?
dk?D
sim(d0, dk)? freqdk(vi, vj) (3)
where sim(d0, dk) is the cosine similarity be-
tween d0 and dk, and freqdk(vi, vj) is the co-
occurrence frequency of vi and vj in document dk.
Once the graph is constructed, the rest of the pro-
cedure is identical to SingleRank.
3.2.5 Clustering-based Approach
Liu et al (2009b) propose to cluster candidate
words based on their semantic relationship to en-
sure that the extracted keyphrases cover the en-
tire document. The objective is to have each clus-
ter represent a unique aspect of the document and
take a representative word from each cluster so
that the document is covered from all aspects.
More specifically, their algorithm (henceforth
referred to as KeyCluster) first filters out the stop
words from a given document and treats the re-
maining unigrams as candidate words. Second,
for each candidate, its relatedness with another
candidate is computed by (1) counting how many
times they co-occur within a window of size W
in the document and (2) using Wikipedia-based
statistics. Third, candidate words are clustered
based on their relatedness with other candidates.
Three clustering algorithms are used of which
spectral clustering yields the best score. Once
the clusters are formed, one representative word,
called an exemplar term, is picked from each clus-
ter. Finally, KeyCluster extracts from the docu-
ment all the longest n-grams starting with zero
or more adjectives and ending with one or more
nouns, and if such an n-gram includes one or more
exemplar words, it is selected as a keyphrase. As
a post-processing step, a frequent word list gener-
ated from Wikipedia is used to filter out the fre-
quent unigrams that are selected as keyphrases.
368
4 Evaluation
4.1 Experimental Setup
TextRank and SingleRank setup Following
Mihalcea and Tarau (2004) and Wan and Xiao
(2008), we set the co-occurrence window size for
TextRank and SingleRank to 2 and 10, respec-
tively, as these parameter values have yielded the
best results for their evaluation datasets.
ExpandRank setup Following Wan and Xiao
(2008), we find the 5 nearest neighbors for each
document from the remaining documents in the
same corpus. The other parameters are set in the
same way as in SingleRank.
KeyCluster setup As argued by Liu et al
(2009b), Wikipedia-based relatedness is computa-
tionally expensive to compute. As a result, we fol-
low them by computing the co-occurrence-based
relatedness instead, using a window of size 10.
Then, we cluster the candidate words using spec-
tral clustering, and use the frequent word list that
they generously provided us to post-process the
resulting keyphrases by filtering out those that are
frequent unigrams.
4.2 Results and Discussion
In an attempt to gain a better insight into the
five unsupervised systems, we report their perfor-
mance in terms of precision-recall curves for each
of the four datasets (see Figure 1). This contrasts
with essentially all previous work, where the per-
formance of a keyphrase extraction system is re-
ported in terms of an F-score obtained via a par-
ticular parameter setting on a particular dataset.
We generate the curves for each system as fol-
lows. For Tf-Idf, SingleRank, and ExpandRank,
we vary the number of keyphrases, N , predicted
by each system. For TextRank, instead of vary-
ing the number of predicted keyphrases, we vary
T , the percentage of top-scored vertices (i.e., un-
igrams) that are selected as keywords at the end
of the ranking step. The reason is that TextRank
only imposes a ranking on the unigrams but not
on the keyphrases generated from the high-ranked
unigrams. For KeyCluster, we vary the number
of clusters produced by spectral clustering rather
than the number of predicted keyphrases, again
because KeyCluster does not impose a ranking on
the resulting keyphrases. In addition, to give an
estimate of how each system performs in terms of
F-score, we also plot curves corresponding to dif-
ferent F-scores in these graphs.
Tf-Idf Consistent with our intuition, the preci-
sion of Tf-Idf drops as recall increases. Although
it is the simplest of the five approaches, Tf-Idf is
the best performing system on all but the Inspec
dataset, where TextRank and KeyCluster beat Tf-
Idf on just a few cases. It clearly outperforms all
other systems for NUS and ICSI.
TextRank The TextRank curves show a differ-
ent progression than Tf-Idf: precision does not
drop as much when recall increases. For instance,
in case of DUC and ICSI, precision is not sensi-
tive to changes in recall. Perhaps somewhat sur-
prisingly, its precision increases with recall for In-
spec, allowing it to even reach a point (towards
the end of the curve) where it beats Tf-Idf. While
additional experiments are needed to determine
the reason for this somewhat counter-intuitive re-
sult, we speculate that this may be attributed to
the fact that the TextRank curves are generated
by progressively increasing T (i.e., the percent-
age of top-ranked vertices/unigrams that are used
to generate keyphrases) rather than the number of
predicted keyphrases, as mentioned before. In-
creasing T does not necessarily imply an increase
in the number of predicted keyphrases, however.
To see the reason, consider an example in which
we want TextRank to extract the keyphrase ?ad-
vanced machine learning? for a given document.
Assume that TextRank ranks the unigrams ?ad-
vanced?, ?learning?, and ?machine? first, second,
and third, respectively in its ranking step. When
T = 2n , where n denotes the total number of
candidate unigrams, only the two highest-ranked
unigrams (i.e., ?advanced? and ?learning?) can
be used to form keyphrases. This implies that
?advanced? and ?learning? will each be predicted
as a keyphrase, but ?advanced machine learning?
will not. However, when T = 3n , all three un-
igrams can be used to form a keyphrase, and
since TextRank collapses unigrams adjacent to
each other in the text to form a keyphrase, it will
correctly predict ?advanced machine learning? as
a keyphrase. Note that as we increase T from 2n
to 3n , recall increases, and so does precision, since
369
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(a) DUC
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(b) Inspec
 0
 2
 4
 6
 8
 10
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(c) NUS
 0
 2
 4
 6
 8
 10
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
Tf-Idf
TextRank
SingleRank
ExpandRank
KeyCluster
(d) ICSI
Figure 1: Precision-recall curves for all four datasets
?advanced? and ?learning? are now combined to
form one keyphrase (and hence the number of pre-
dicted keyphrases decreases). In other words, it
is possible to see a simultaneous rise in precision
and recall in a TextRank curve. A natural ques-
tion is: why does is happen only for Inspec but
not the other datasets? The reason could be at-
tributed to the fact that Inspec is composed of ab-
stracts: since the number of keyphrases that can be
generated from these short documents is relatively
small, precision may not drop as severely as with
the other datasets even when all of the unigrams
are used to form keyphrases.
On average, TextRank performs much worse
compared to Tf-Idf. The curves also prove Tex-
tRank?s sensitivity to T on Inspec, but not on the
other datasets. This certainly gives more insight
into TextRank since it was evaluated on Inspec
only for T=33% by Mihalcea and Tarau (2004).
SingleRank SingleRank, which is supposed to
be a simple variant of TextRank, surprisingly ex-
hibits very different performance. First, it shows
a more intuitive nature: precision drops as recall
increases. Second, SingleRank outperforms Tex-
tRank by big margins on all the datasets. Later,
we will examine which of the differences between
them is responsible for the differing performance.
370
DUC Inspec NUS ICSI
Parameter F Parameter F Parameter F Parameter F
Tf-Idf N = 14 27.0 N = 14 36.3 N = 60 6.6 N = 9 12.1
TextRank T = 100% 9.7 T = 100% 33.0 T = 5% 3.2 T = 25% 2.7
SingleRank N = 16 25.6 N = 15 35.3 N = 190 3.8 N = 50 4.4
ExpandRank N = 13 26.9 N = 15 35.3 N = 177 3.8 N = 51 4.3
KeyCluster m = 0.9n 14.0 m = 0.9n 40.6 m = 0.25n 1.7 m = 0.9n 3.2
Table 2: Best parameter settings. N is the number of predicted keyphrases, T is the percentage of vertices selected as
keywords in TextRank, m is the number of clusters in KeyCluster, expressed in terms of n, the fraction of candidate words.
ExpandRank Consistent with Wan and Xiao
(2008), ExpandRank beats SingleRank on DUC
when a small number of phrases are predicted, but
their difference diminishes as more phrases are
predicted. On the other hand, their performance
is indistinguishable from each other on the other
three datasets. A natural question is: why does
ExpandRank improve over SingleRank only for
DUC but not for the other datasets? To answer
this question, we look at the DUC articles and
find that in many cases, the 5-nearest neighbors
of a document are on the same topic involving the
same entities as the document itself, presumably
because many of these news articles are simply
updated versions of an evolving event. Conse-
quently, the graph built from the neighboring doc-
uments is helpful for predicting the keyphrases of
the given document. Such topic-wise similarity
among the nearest documents does not exist in the
other datasets, however.
KeyCluster As in TextRank, KeyCluster does
not always yield a drop in precision as recall im-
proves. This, again, may be attributed to the fact
that the KeyCluster curves are generated by vary-
ing the number of clusters rather than the num-
ber of predicted keyphrases, as well as the way
keyphrases are formed from the exemplars. An-
other reason is that the frequent Wikipedia uni-
grams are excluded during post-processing, mak-
ing KeyCluster more resistant to precision drops.
Overall, KeyCluster performs slightly better than
TextRank on DUC and ICSI, yields the worst per-
formance on NUS, and scores the best on Inspec
when the number of clusters is high. These results
seem to suggest that KeyCluster works better if
more clusters are used.
Best parameter settings Table 2 shows for each
system the parameter values yielding the best F-
score on each dataset. Two points deserve men-
tion. First, in comparison to SingleRank and
ExpandRank, Tf-Idf outputs fewer keyphrases to
achieve its best F-score on most datasets. Second,
the systems output more keyphrases on NUS than
on other datasets to achieve their best F-scores
(e.g., 60 for Tf-Idf, 190 for SingleRank, and 177
for ExpandRank). This can be attributed in part to
the fact that the F-scores on NUS are low for all
the systems and exhibit only slight changes as we
output more phrases.
Our re-implementations Do our duplicated
systems yield scores that match the original
scores? Table 3 sheds light on this question.
First, consider KeyCluster, where our score
lags behind the original score by approximately
5%. An examination of Liu et al?s (2009b) re-
sults reveals a subtle caveat in keyphrase extrac-
tion evaluations. In Inspec, not all gold-standard
keyphrases appear in their associated document,
and as a result, none of the five systems we con-
sider in this paper can achieve a recall of 100.
While Mihalcea and Tarau (2004) and our re-
implementations use all of these gold-standard
keyphrases in our evaluation, Hulth (2003) and
Liu et al address this issue by using as gold-
standard keyphrases only those that appear in the
corresponding document when computing recall.2
This explains why our KeyCluster score (38.9) is
lower than the original score (43.6). If we fol-
low Liu et al?s way of computing recall, our re-
implementation score goes up to 42.4, which lags
behind their score by only 1.2.
Next, consider TextRank, where our score lags
behind Mihalcea and Tarau?s original score by
more than 25 points. We verified our implemen-
tation against a publicly available implementation
2As a result, Liu et al and Mihalcea and Tarau?s scores
are not directly comparable, but Liu et al did not point this
out while comparing scores in their paper.
371
Dataset F-scoreOriginal Ours
Tf-Idf DUC 25.4 25.7
TextRank Inspec 36.2 10.0
SingleRank DUC 27.2 24.9
ExpandRank DUC 31.7 26.4
KeyCluster Inspec 43.6 38.9
Table 3: Original vs. re-implementation scores
of TextRank3, and are confident that our imple-
mentation is correct. It is also worth mentioning
that using our re-implementation of SingleRank,
we are able to match the best scores reported by
Mihalcea and Tarau (2004) on Inspec.
We score 2 and 5 points less than Wan and
Xiao?s (2008) implementations of SingleRank and
ExpandRank, respectively. We speculate that doc-
ument pre-processing (e.g., stemming) has con-
tributed to the discrepancy, but additional exper-
iments are needed to determine the reason.
SingleRank vs. TextRank Figure 1 shows that
SingleRank behaves very differently from Tex-
tRank. As mentioned in Section 3.2.3, the two
algorithms differ in three major aspects. To de-
termine which aspect is chiefly responsible for the
large difference in their performance, we conduct
three ?ablation? experiments. Each experiment
modifies exactly one of these aspects in SingleR-
ank so that it behaves like TextRank, effectively
ensuring that the two algorithms differ only in the
remaining two aspects. More specifically, in the
three experiments, we (1) change SingleRank?s
window size to 2, (2) build an unweighted graph
for SingleRank, and (3) incorporate TextRank?s
way of forming keyphrases into SingleRank, re-
spectively. Figure 2 shows the resultant curves
along with the SingleRank and TextRank curves
on Inspec taken from Figure 1b. As we can see,
the way of forming phrases, rather than the win-
dow size or the weight assignment method, has
the largest impact on the scores. In fact, after in-
corporating TextRank?s way of forming phrases,
SingleRank exhibits a remarkable drop in perfor-
mance, yielding a curve that resembles the Tex-
tRank curve. Also note that SingleRank achieves
better recall values than TextRank. To see the rea-
son, recall that TextRank requires that every word
of a gold keyphrase must appear among the top-
3http://github.com/sharethis/textrank
 0
 10
 20
 30
 40
 50
 0  20  40  60  80  100
Pr
ec
isi
on
 (%
)
Recall (%)
F=10
F=20
F=30
F=40
SingleRank
SingleRank+Window size=2
SingleRank+Unweighted
SingleRank+TextRank phrases
TextRank
Figure 2: Ablation results for SingleRank on In-
spec
ranked unigrams. This is a fairly strict criterion,
especially in comparison to SingleRank, which
does not require all unigrams of a gold keyphrase
to be present in the top-ranked list. We observe
similar trends for the other datasets.
5 Conclusions
We have conducted a systematic evaluation of five
state-of-the-art unsupervised keyphrase extraction
algorithms on datasets from four different do-
mains. Several conclusions can be drawn from
our experimental results. First, to fully under-
stand the strengths and weaknesses of a keyphrase
extractor, it is essential to evaluate it on multi-
ple datasets. In particular, evaluating it on a sin-
gle dataset has proven inadequate, as good per-
formance can sometimes be achieved due to cer-
tain statistical characteristics of a dataset. Sec-
ond, as demonstrated by our experiments with
TextRank and SingleRank, post-processing steps
such as the way of forming keyphrases can have
a large impact on the performance of a keyphrase
extractor. Hence, it may be worthwhile to investi-
gate alternative methods for extracting candidate
keyphrases (e.g., Kumar and Srinathan (2008),
You et al (2009)). Finally, despite the large
amount of recent work on unsupervised keyphrase
extractor, our results indicated that Tf-Idf remains
a strong baseline, offering very robust perfor-
mance across different datasets.
372
Acknowledgments
We thank the three anonymous reviewers for their
comments. Many thanks to Anette Hulth and
Yang Liu for providing us with the Inspec and
ICSI datasets; Rada Mihalcea, Paco Nathan, and
Xiaojun Wan for helping us understand their al-
gorithms/implementations; and Peng Li for pro-
viding us with the frequent word list that he and
his co-authors used in their paper. This work was
supported in part by NSF Grant IIS-0812261.
References
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Frank, Eibe, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668?673.
Hulth, Anette. 2003. Improved automatic keyword
extraction given more linguistic knowledge. In
Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing, pages
216?223.
Janin, Adam, Don Baron, Jane Edwards, Dan El-
lis, David Gelbart, Nelson Morgan, Barbara Pe-
skin, Thilo Pfau, Elizabeth Shriberg, Andreas Stol-
cke, and Chuck Wooters. 2003. The ICSI meeting
corpus. In Proceedings of 2003 IEEE Conference
on Acoustics, Speech, and Signal Processing, pages
364?367.
Kumar, Niraj and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the Eighth ACM Symposium on Document Engi-
neering, pages 199?208.
Liu, Feifan, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic
keyword extraction using meeting transcripts. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 620?628.
Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257?266.
Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13(1):157?
169.
Medelyan, Olena, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404?411.
Nguyen, Thuy Dung and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317?326.
Over, Paul. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.
Tomokiyo, Takashi and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Toutanova, Kristina and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings of the 2000 Joint SIGDAT Conference on
Empirical Methods in Natural Language processing
and Very Large Corpora, pages 63?70.
Turney, Peter. 2000. Learning algorithms for
keyphrase extraction. Information Retrieval,
2:303?336.
Turney, Peter. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434?439.
Wan, Xiaojun and Jianguo Xiao. 2008. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855?860.
Wan, Xiaojun, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559.
You, Wei, Dominique Fontaine, and Jean-Paul Barthe`s.
2009. Automatic keyphrase extraction with a re-
fined candidate set. In Proceedings of the 2009
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576?579.
Zha, Hongyuan. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings of
the 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113?120.
373
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 751?762,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Why are You Taking this Stance?
Identifying and Classifying Reasons in Ideological Debates
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Recent years have seen a surge of interest
in stance classification in online debates.
Oftentimes, however, it is important to de-
termine not only the stance expressed by
an author in her debate posts, but also the
reasons behind her supporting or oppos-
ing the issue under debate. We therefore
examine the new task of reason classifi-
cation in this paper. Given the close in-
terplay between stance classification and
reason classification, we design computa-
tional models for examining how automat-
ically computed stance information can be
profitably exploited for reason classifica-
tion. Experiments on our reason-annotated
corpus of ideological debate posts from
four domains demonstrate that sophisti-
cated models of stances and reasons can
indeed yield more accurate reason and
stance classification results than their sim-
pler counterparts.
1 Introduction
In recent years, researchers have begun exploring
new opinion mining tasks. One such task is debate
stance classification (SC): given a post written for
a two-sided topic discussed in an online debate fo-
rum, determine which of the two sides (i.e., for or
against) its author is taking (Agrawal et al., 2003;
Thomas et al., 2006; Bansal et al., 2008; Soma-
sundaran and Wiebe, 2009; Burfoot et al., 2011;
Hasan and Ng, 2013b). For example, the author of
the post shown in Figure 1 is pro-abortion.
Oftentimes, however, it is important to deter-
mine not only the author?s stance expressed in her
debate posts, but also the reasons why she supports
or opposes the issue under debate. Intuitively,
given a debate topic such as ?Should abortion be
banned?? or ?Do you support Obamacare??, it
[I feel that abortion should remain legal, or rather, parents
should have the power to make the decision themselves and
not face any legal hindrance of any form.]
1
Let us take a
look from the social perspective. [If parents cannot afford
to provide for the child, or if the family is facing financial
constraints, it is understandable that abortion can remain as
one of the options.]
2
Reason 1: Woman?s right to abort
Reason 2: Unwanted babies are threat to their parents? fu-
ture
Figure 1: A sample post on abortion annotated
with reasons.
should not be difficult for us to come up with a set
of reasons people typically use to back up their
stances. Given a set of reasons associated with
each stance in an online debate, the goal of post-
level reason classification is to identify those rea-
son(s) an author uses to back up her stance in her
debate post. A more challenging version of this
task is sentence-level reason classification, where
the goal is to identify not only the reason(s) an au-
thor uses in her post, but also the sentence(s) in
the post that the author uses to describe each of
her reasons. For example, the author of the post
shown in Figure 1 mentions two reasons why she
supports abortion, namely it?s a woman?s right to
abort and unwanted babies are threat to their par-
ents? future, which are mentioned in the first and
third sentences in the post respectively.
Our goal in this paper is to examine post- and
sentence-level reason classification (RC) in ideo-
logical debates. Many online debaters use emo-
tional languages, which may involve sarcasm and
insults, to express their points, thereby making
RC and SC in ideological debates potentially more
challenging than that in other debate settings such
as congressional debates and company-internal
discussions (Walker et al., 2012).
Besides examining the new task of RC in ide-
ological debates, we believe that our work makes
three contributions. First, we propose to address
post-level RC by means of sentence-level RC by
751
(1) determining the reason(s) associated with each
of its sentences (if any), and then (2) taking the
union of the set of reasons associated with all of its
sentences to be the set of reasons associated with
the post. We hypothesize that this sentence-based
approach, which exploits a training set in which
each sentence in a post is labeled with its reason,
would achieve better performance than a multi-
label text classification approach to post-level RC,
which learns to determine the subset of reasons a
post contains directly from a training set in which
each post is labeled with the corresponding set of
reasons. In other words, we hypothesize that we
could achieve better results for post-level RC by
learning from sentence-level than from post-level
reason annotations, as sentence-level reason anno-
tations can enable a learning algorithm to accu-
rately attribute an annotated reason to a particular
portion of a post.
Second, we propose stance-supported RC sys-
tems, hypothesizing that automatically computed
stance information can be profitably exploited for
RC. Since we are exploiting automatically com-
puted (and thus potentially noisy) stance informa-
tion, we hypothesize that the effectiveness of such
information would depend in part on the way it is
exploited in RC systems. As a result, we introduce
a set of stance-supported models for RC, start-
ing with simple pipeline models and then mov-
ing on to joint models with increasing sophisti-
cation. Note that exploiting stance information
by no means guarantees that RC performance will
improve, as an incorrect determination of stance
could lead to an incorrect identification of rea-
sons. Hence, one of our goals is to examine how to
model stances and reasons so that RC can benefit
from stance information.
Finally, since progress on RC is hindered in part
by the lack of an annotated corpus, we make our
reason-annotated dataset publicly available.
1
To
our knowledge, this will be the first publicly avail-
able corpus for sentence- and post-level RC.
2 Corpus and Annotation
We collected debate posts from four popular
domains, Abortion (ABO), Gay Rights (GAY),
Obama (OBA), and Marijuana (MAR), from an
online debate forum
2
. All debates are two-sided,
1
http://www.hlt.utdallas.edu/
?
saidul/
stance/
2
http://www.createdebate.com/
so each post receives one of two stance labels, for
or against, depending on whether the author of
the post supports or opposes abortion, gay rights,
Obama, or the legalization of marijuana respec-
tively. A post?s stance label is given by its author.
Note that each post belongs to a thread, which
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post
y
i
is the parent of another post y
j
if y
j
is a reply
to y
i
. Given a thread, we generate post sequences,
each of which is a path from the root of the thread
to one of its leaves. Hence, a post sequence is an
ordered set of posts such that each post is a reply
to its immediately preceding post in the sequence.
Table 2a shows the statistics of the four stance-
labeled datasets.
While the debate posts contain the stance labels
given by their authors, they are not annotated with
reasons. As part of our study of RC, we annotate
each post with the reasons it gives for its stance.
Our annotation procedure is composed of three
steps. First, two human annotators independently
examined each post and identified the reasons au-
thors present to support their stances (i.e., for and
against) in each domain. Second, they discussed
and agreed on the reasons identified for each do-
main. Third, they independently annotated the text
of each post with reason labels from the post?s do-
main. To do this, they labeled each sentence of a
post with the set of reasons the author expressed in
that sentence. Any sentence that does not belong
to any reason class was assigned the NONE class.
After the annotators completed the aforemen-
tioned steps, they were asked to collapse all the
reason classes that occur in less than 2% of the
sentences annotated with non-NONE classes into
the OTHER class. In other words, all the sentences
that were originally annotated with one of these
infrequent reason classes will now be labeled as
OTHER. Our decision to merge infrequent classes
is motivated by two observations. First, from a
practical point of view, infrequent reasons do not
carry much weight. Second, from a modeling per-
spective, it is often not worth increasing model
complexity by handling infrequent classes. The
resulting set of reason classes for each domain is
shown in Table 1.
A closer examination of the resulting annota-
tions reveals that approximately 3% of the sen-
tences received multiple reason labels. Again, to
avoid the complexity of modeling multi-labeled
752
Domain Stance Reason classes
ABO
for [F1] Abortion is a woman?s right (26%); [F2] Rape victims need it to be legal (7%); [F3] A fetus
is not human (38%); [F4] Mother?s life in danger (5%); [F5] Unwanted babies are ill-treated by
parents (8%); [F6] Birth control fails at times (3%); [F7] Abortion is not murder (3%); [F8] Mother
is not healthy/financially solvent (4%); [F9] Others (6%)
against [A1] Put baby up for adoption (9%); [A2] Abortion kills a life (29%); [A3] An unborn baby is a
human and has the right to live (40%); [A4] Be willing to have the baby if you have sex (14%);
[A5] Abortion is harmful for women (5%); [A6] Others (3%)
GAY
for [F1] Gay marriage is like any other marriage (14%); [F2] Gay people should have the same rights
as straight people (36%); [F3] Gay parents can adopt and ensure a happy life for a baby (10%); [F4]
People are born gay (18%); [F5] Religion should not be used against gay rights (11%); [F6] Others
(11%)
against [A1] Religion does not permit gay marriages (18%); [A2] Gay marriages are not normal/against
nature (39%); [A3] Gay parents can not raise kids properly (11%); [A4] Gay people have problems
and create social issues (16%); [A5] Others (16%)
OBA
for [F1] Fixed the economy (21%); [F2] Ending the wars (7%); [F3] Better than the republican candi-
dates (25%); [F4] Makes good decisions/policies (8%); [F5] Has qualities of a good leader (14%);
[F6] Ensured better healthcare (8%); [F7] Executed effective foreign policies (6%); [F8] Created
more jobs (4%); [F9] Others (7%)
against [A1] Destroyed our economy (26%); [A2] Wars are still on (11%); [A3] Unemployment rate is high
(5%); [A4] Healthcare bill is a failure(9%); [A5] Poor decision-maker (7%); [A6] We have better
republicans than Obama (5%); [A7] Not eligible as a leader (20%); [A8] Ineffective foreign policies
(4%); [A9] Others (13%)
MAR
for [F1] Not addictive (23%); [F2] Used as a medicine (11%); [F3] Legalized marijuana can be con-
trolled and regulated by the government (33%); [F4] Prohibition violates human rights (15%); [F5]
Does not cause any damage to our bodies (6%); [F6] Others (12%)
against [A1] Damages our bodies (23%); [A2] Responsible for brain damage (22%); [A3] If legalized,
people will use marijuana and other drugs more (12%); [A4] Causes crime (9%); [A5] Highly
addictive (17%); [A6] Others (17%)
Table 1: Reason classes and their percentages in the corresponding stance for each domain.
sentences given their rarity, we asked each annota-
tor to pick the reason that was highlighted the most
in each multi-labeled sentence.
Inter-annotator agreement scores at the sentence
level and the post level, expressed in terms of Co-
hen?s Kappa (Carletta, 1996), are shown in Ta-
ble 2b. Given that the majority of sentences were
labeled as NONE, we avoid inflating agreement by
not considering the sentences labeled with NONE
by both annotators when computing Kappa. As we
can see, we achieved substantial post-level agree-
ment and high sentence-level agreement.
The major source of inter-annotator disagree-
ment for all four datasets stems from the fact that
in many cases, the annotators, while agreeing on
the reason class, differ on how long the text span
for a reason should be. This hurts sentence-level
agreement but not post-level agreement, since the
latter only concerns whether a reason was men-
tioned in a post, and explains why the sentence-
level agreement scores are lower than the corre-
sponding post-level scores. Minor sources of dis-
agreement arise from the facts that (1) the anno-
tators selected different reason labels for some of
the multi-labeled sentences, and (2) they tend to
disagree in some cases where authors use sarcasm
ABO GAY OBA MAR
Stance-labeled posts
1741 1376 985 626
for posts (%)
54.9 63.4 53.9 69.5
Average post sequence length
4.1 4.0 2.6 2.5
(a) Statistics of stance-labeled posts
ABO GAY OBA MAR
Reason-labeled posts
463 561 447 432
% of sentences w/ reason tags
20.4 29.8 34.4 43.7
Kappa (sentence)
0.66 0.63 0.61 0.67
Kappa (post)
0.82 0.80 0.78 0.83
(b) Statistics of reason-labeled posts
Table 2: Stance and reason annotation statistics.
to present a reason. Each case of disagreement is
resolved through discussion among the annotators.
3 Baseline RC System
Our baseline system uses a maximum entropy
(MaxEnt) classifier to determine whether a reason
is expressed in a post and/or its sentence(s). We
create one training instance for each sentence in
each post in the training set, using the reason label
as its class label. We represent each instance using
five types of features, as described below.
N-gram features. We encode each unigram and
bigram collected from the training sentences as a
753
binary feature indicating the n-gram?s presence or
absence in a given sentence.
Dependency-based features. To capture the
inter-word relationships that n-grams may not,
we employ the dependency-based features previ-
ously used for stance classification in Anand et
al. (2011). These features have three variants. In
the first variant, the pair of arguments involved in
each dependency relation extracted by a depen-
dency parser is used as a feature. The second vari-
ant is the same as the first except that the head (i.e.,
the first argument in a relation) is replaced by its
part-of-speech tag. The features in the third vari-
ant, the topic-opinion features, are created by re-
placing each sentiment-bearing word in features of
the first two types with its corresponding polarity
label (i.e., + or ?).
Frame-semantic features. While dependency-
based features capture the syntactic dependencies,
frame-semantic features encode the semantic rep-
resentation of the concepts in a sentence. Fol-
lowing our previous work on stance classification
(Hasan and Ng, 2013c), we employ three types
of features computed based on the frame-semantic
parse of each sentence in a post obtained from SE-
MAFOR (Das et al., 2010). Frame-word interac-
tion features encode whether two words appear in
different elements of the same frame. Hence, each
frame-word interaction feature consists of (1) the
name of the frame f from which it is created, and
(2) an unordered word pair in which the words are
taken from two frame elements of f . A frame-pair
feature is represented as a word pair corresponding
to the names of two frames and encodes whether
the target word of the first frame appears within
an element of the second frame. Finally, frame n-
gram features are a variant of word n-grams. For
each word n-gram in the sentence, a frame n-gram
feature is created by replacing one or more words
in the word n-gram with the name of the frame or
the frame element in which the word appears. A
detailed description of these features can be found
in Hasan and Ng (2013c).
Quotation features. We employ two quotation
features. IsQuote is a binary feature that indicates
whether a sentence is a quote or not (i.e., whether
it appeared in its parent post in the post sequence).
Note that if an instance is a quote from a previ-
ous post, it is unlikely that it represents a reason
the author is presenting to support her argument.
Instead, the author may have quoted this before
stating her counter-argument. FollowsQuote is a
binary feature that indicates whether a sentence
follows a sentence for which the IsQuote feature
value is true. Intuitively, a sentence following a
quote is likely to present a counter-argument.
Positional feature. We split each post into four
parts (such that each part contains roughly the
same number of sentences) and create one posi-
tional feature that encodes which part of the post
contains a given sentence. This feature is moti-
vated by our observations on the training data that
(1) reasons are more likely to appear in the second
half of a post and (2) on average more than one-
third of the reasons appear in the last quarter of a
post.
After training, we can apply the resulting RC
system to classify the test instances, which are
generated in the same way as the training in-
stances. Once the sentences of a test post are clas-
sified, we simply assume its post-level reason la-
bels to be the set of reason labels assigned by the
classifier to its sentences.
4 Stance-Supported RC Systems
In this section, we propose a set of systems for
RC. Unlike the baseline RC system, these RC
systems are stance-supported, enabling us to ex-
plore how different ways of modeling automati-
cally computed stances and reasons can improve
RC classification. Below we present our systems
in increasing order of modeling sophistication.
4.1 Pipeline Systems
We examine two pipeline systems, P1 and P2.
Given a set of test posts, both systems first de-
termine the stance of each post and then apply a
stance-specific reason classifier to each of them.
More specifically, both P1 and P2 employ two
stance-specific reason classifiers: one is trained on
all the posts labeled as for and the other is trained
on all the posts labeled as against. Each stance-
specific reason classifier is trained using MaxEnt
on the same feature set as that of the Baseline RC
system. It computes for a particular stance s the
probability P (r|s, t), where r is a reason label and
t is a sentence in a test post p.
P1 and P2 differ only with respect to the SC
model used to stance-label each post. In P1, the
stance s of a post p is determined by applying to p
a stance classifier that computes P (s|p). To train
the classifier, we employ MaxEnt. Each train-
754
ing instance corresponds to a training post and
is represented by all but the quotation and posi-
tional features used to train the Baseline RC sys-
tem, since these two feature types are sentence-
based rather than post-based. After training, the
resulting classifier can be used to stance-label a
post independently of the other posts.
In P2, on the other hand, we recast SC as a se-
quence labeling task. In other words, we train a
SC model that assumes as input a post sequence
and outputs a stance sequence, with one stance la-
bel for each post in the input post sequence. This
choice is motivated by an observation we made
previously (Hasan and Ng, 2013a): since each post
in a sequence is a reply to the preceding post, we
could exploit their dependencies by determining
their stance labels together.
3
As our sequence learner, we employ a maxi-
mum entropy Markov model (MEMM) (McCal-
lum et al., 2000). Given an input post sequence
P
S
= (p
1
, p
2
, . . . , p
n
), the MEMM finds the most
probable stance sequence S = (s
1
, s
2
, . . . , s
n
) by
computing P (S|P
S
), where:
P (S|P
S
) =
n
?
k=1
P (s
k
|s
k?1
, p
k
) (1)
This probability can be computed efficiently via
dynamic programming (DP), using a modified ver-
sion of the Viterbi algorithm (Viterbi, 1967).
There is a caveat, however. Recall that the post
sequences are generated from a thread. Since a
test post may appear in more than one sequence,
different occurrences of it may be assigned differ-
ent stance labels by the MEMM. To determine the
final stance label for the post, we average the prob-
abilities assigned to the for stance over all its oc-
currences; if the average is ? 0.5, then its final
label is for; otherwise, its label is against.
4.2 System based on Joint Inference
One weakness of the pipeline systems is that errors
may propagate from the SC system to the RC sys-
tem. If the stance of a post is incorrectly labeled,
its reasons will also be incorrectly labeled.
To avoid this problem, we employ joint infer-
ence. Specifically, we first train a SC system and
3
While we could similarly recast the problem of assigning
reasons to the sentences in a post as a sequence learning task,
we did not pursue this idea further because preliminary ex-
periments indicated that sequence learning for RC was inef-
fective: there is little, if any, dependency between the reason
labels in consecutive sentences.
a RC system independently of each other. We em-
ploy the Baseline as our RC system, since this is
the only RC system that is not stance-specific. For
the SC system, we employ P2.
Since the SC system and the RC system are
trained independently of each other, their outputs
may not be consistent. For instance, an inconsis-
tency arises if a post is labeled as for but one or
more of its reasons are associated with the oppos-
ing stance. In fact, an inconsistency can arise in
the output of the RC system alone: reasons associ-
ated with both stances may be assigned by the RC
systems to different sentences of a given post.
To enforce consistency, we apply integer lin-
ear programming (ILP) (Roth and Yih, 2004). We
formulate one ILP program for each debate post.
Each ILP program contains two post-stance vari-
ables (x
for
and x
against
) and |T | ? |L
R
| reason
variables (i.e., one indicator variable z
t,r
for each
reason class r and each sentence t), where |T | is
the number of sentences in the post and |L
R
| is the
number of reason labels. Our objective is to maxi-
mize the linear combination of these variables and
their corresponding probabilities assigned by their
respective classifiers (see (2) below) subject to two
types of constraints, the integrity constraints and
the post-reason constraints. The integrity con-
straints ensure that each post is assigned exactly
one stance and each sentence in a post is assigned
exactly one reason class (see the two equality con-
straints in (3)). The post-reason constraints ensure
consistency between the predictions made by the
SC and the RC systems. Specifically, (1) if there
is at least one reason supporting the for stance, the
post must be assigned a for label; and (2) a for
post must have at least one for reason. These con-
straints are defined for the against label as well
(see the constraints in (4)).
Maximize:
?
s?L
S
a
s
x
s
+
1
|T |
|T |
?
t=1
?
r?L
R
b
t,r
z
t,r
(2)
subject to:
?
s?L
S
x
s
= 1, ? t
?
r?L
R
z
t,r
= 1,
x
s
? {0, 1}, z
t,r
? {0, 1}
(3)
? t x
s
? z
t,r
,
|T |
?
t=1
z
t,r
? x
s
(4)
755
Note that (1) a
s
and b
t,r
are two sets of probabil-
ities assigned by the SC and RC systems respec-
tively; (2) L
S
and L
R
denote the set of stance
labels and reason labels respectively; and (3) the
fraction
1
|T |
ensures that both classifiers are con-
tributing equally to the objective function.
4.3 Systems based on Joint Learning
Another way to avoid the error propagation prob-
lem in pipeline systems is to perform joint learn-
ing. In joint learning, the two tasks, SC and RC,
are learned jointly. Below we propose three joint
models in increasing level of sophistication.
J1 is a joint model that, given a test post p, finds
the stance label s and the reason label for each of
the sentences that together maximize the probabil-
ity P (R
p
, s|p), where R
p
= (r
1
, r
2
, . . . , r
n
) is the
sequence of reason labels with r
i
(1 ? i ? n)
being the reason label assigned to t
i
, the i-th sen-
tence in p. Using Chain Rule,
P (R
p
, s|p) = P (s|p)P (R
p
|s, p)
= P (s|p)
n
?
i=1
P (r
i
|s, t
i
)
(5)
Hence, P (R
p
, s|p) can be computed by using
the stance-specific RC classifier and the SC classi-
fier employed in P1.
The second joint model, J2, is the same as
J1, except that we recast SC as a sequence la-
beling task. As before, we employ MEMM to
learn how to predict stance sequences. Given a
post sequence P
S
= (p
1
, p
2
, . . . , p
n
), J2 finds the
stance sequence S = (s
1
, s
2
, . . . , s
n
) and rea-
sons R = (R
1
, R
2
, . . . , R
n
) that jointly maximize
P (R,S|P
S
). Note that R
i
is the sequence of rea-
son labels assigned to the sentences in post i.
The R and S that jointly maximize P (R,S|P
S
)
can be found efficiently via DP, using a modified
version of the Viterbi algorithm. Unlike in P2, in
J2 the decoding process is slightly more compli-
cated because we have to take into account R
i
. Be-
low we show the recursive definitions used to com-
pute the entries in the DP table, where v
k
(h) is the
(k,h)-th entry of the table; P (h|p) is provided by
the MaxEnt stance classifier used in P1; P (h|j, p)
is provided by the MEMM stance classifier used
in P2; P (r
max
i
|h, t
i
) is provided by the stance-
specific reason classifier used in the pipeline sys-
tems; and r
max
i
is the reason label for sentence t
i
that has the highest probability according to the
reason classifier.
Base case:
v
1
(h) = P (h|p)
n
?
i=1
P (r
max
i
|h, t
i
) (6)
Recursive definition:
v
k
(h) = max
j
v
k?1
(j)P (h|j, p)
n
?
i=1
P (r
max
i
|h, t
i
)
(7)
To motivate our third joint model, J3, we make
the following observation. Recall that a post in
a post sequence is a reply to its preceding post.
An inspection of the training data reveals that in
many cases, a reply is a rebuttal to the preced-
ing post, where an author attempts to argue why
the points or reasons raised in the preceding post
are wrong and then provides her reasons for the
opposing stance. Motivated by this observation,
we hypothesize that the reasons mentioned in the
preceding post could be useful for predicting the
reasons in the current post. However, none of the
models we have presented so far makes use of the
reasons predicted for the preceding post.
This motivates the design of J3, which we build
on top of J2. Specifically, to incorporate the reason
labels predicted for the preceding post in a post se-
quence, we augment the feature set of the stance-
specific reason classifiers with a set of reason fea-
tures, with one binary feature for each reason. The
value of a reason feature is 1 if and only if the cor-
responding reason is predicted to be present in the
preceding post. Hence, in J3, we can apply the
same DP equations we used in J2 except that the
set of features used by the reason classifier is aug-
mented with the reason features.
5 Evaluation
While our primary goal is to evaluate the RC sys-
tems introduced in the previous section, we are
also interested in whether SC performance can im-
prove when SC is jointly modeled with RC. More
specifically, our evaluation is driven by the follow-
ing question: will RC performance and SC perfor-
mance improve as we employ more sophisticated
methods for modeling reasons and stances? Be-
fore showing the results, we describe the metrics
for evaluating RC and SC systems.
756
System
ABO GAY OBA MAR
Stance
Reason
Stance
Reason
Stance
Reason
Stance
Reason
Sentence Post Sentence Post Sentence Post Sentence Post
Baseline ? 32.7 45.0 ? 23.3 40.5 ? 19.5 31.5 ? 28.7 44.2
P1 62.8 34.5 46.3 63.4 24.5 43.2 61.0 20.3 33.5 67.2 30.5 47.3
P2 65.1 36.1 47.7 64.2 26.6 45.5 63.8 21.1 34.4 68.5 32.9 48.8
ILP 65.2 36.5 48.4 64.6 28.0 46.7 63.6 22.8 35.0 68.8 33.1 48.9
J1 62.5 36.0 47.6 64.0 26.7 45.6 61.2 23.1 35.7 67.8 33.3 49.2
J2 65.9 37.9 50.6 65.3 29.6 48.5 63.5 24.5 37.1 68.7 34.5 50.5
J3 66.3 39.5 52.3 65.7 31.4 49.8 64.0 25.1 38.0 69.0 35.1 51.1
Table 3: SC accuracies and RC F-scores for our five-fold cross-validation experiments.
5.1 Experimental Setup
We express SC results in terms of accuracy (i.e.,
the percentage of test posts labeled with the cor-
rect stance) and RC results in terms of F-score
micro-averaged over all reason classes except the
NONE class. For each RC system, we report its
sentence-level RC score and post-level RC score,
which are computed over sentences and posts re-
spectively. As mentioned at the end of Section 3,
the set of post-level reason labels of a given post
is automatically obtained by taking the union of
the set of reason labels assigned to each of its sen-
tences. Hence, a reason classifier will be rewarded
as long as it can predict, for any sentence in a test
post, a reason label that the annotators assigned to
some sentence in the same post.
We obtain these scores via five-fold cross-
validation experiments. During fold partition, all
posts that are in the same post sequence are as-
signed to the same fold. All reason and stance
classifiers are domain-specific, meaning that each
of them is trained on sentences/posts from ex-
actly one domain and is applied to classify sen-
tences/posts from the same domain. We use the
Stanford maximum entropy classifier
4
for classifi-
cation and solve ILP programs using lpsolve
5
.
5.2 Results and Discussion
Results are shown in Table 3. Each row corre-
sponds to one of our seven RC systems, showing
its SC accuracy as well as its sentence- and post-
level RC F-scores for each domain.
Let us begin by discussing the RC results. First,
P1 and P2 significantly beat the Baseline on all
4
http://nlp.stanford.edu/software/
classifier.shtml
5
http://sourceforge.net/projects/
lpsolve/
four domains by an average of 1.4 and 3.1 points
at the sentence level and by an average of 2.3 and
3.8 points at the post level respectively.
6
These re-
sults show that stance information can indeed be
profitably used for RC even if it is incorporated
into RC systems in a simple manner. Second,
improving SC through sequence learning can im-
prove RC: the systems in which SC is recast as se-
quence labeling (P2 and J2) perform significantly
better than the corresponding systems that do not
(P1 and J1). Third, ILP significantly beats P2 on
two domains (ABO and GAY) and achieves the
same level of performance as P2 on the remain-
ing domains. These results suggest that joint infer-
ence is no worse (and sometimes even better) than
pipeline learning as far as exploiting stance infor-
mation for RC is concerned. Fourth, the systems
trained via joint learning (J1 and J2) beat their cor-
responding pipeline counterparts (P1 and P2) on
all four domains, significantly so by an average of
2.3 and 2.5 points at the sentence level and by an
average of 2.0 and 2.6 points at the post level re-
spectively, suggesting that joint learning is indeed
a better way to incorporate stance information than
pipeline learning. Finally, J3, the joint system that
exploits reasons predicted for the previous post,
significantly beats J2, the system on which it is
built, by 1.6 and 1.8 points at the sentence level
and by 1.7 and 1.3 points at the post level for ABO
and GAY respectively. It also yields small, statis-
tically insignificant, improvements (0.6 points at
the sentence level and 0.6?0.9 points at the post
level) for the remaining two domains. These re-
sults suggest that the reasons predicted for the pre-
vious post indeed provide useful information for
predicting the current post?s reasons.
Overall, these results are consistent with our hy-
6
All significance tests are paired t-tests (p < 0.05).
757
pothesis that the usefulness of stance information
depends in part on the way it is exploited, and
that RC performance increases as we employ more
sophisticated methods for modeling reasons and
stances. Our best system, J3, significantly beats
the Baseline by an average of 6.7 and 7.5 points at
the sentence and post levels respectively.
As mentioned earlier, a secondary goal of this
work is to determine whether joint modeling can
improve SC as well. For that reason, we com-
pare the performances of the best pipeline model
(P2) and the best joint model (J3) on each domain.
We find that in terms of SC accuracy, J3 is signifi-
cantly better than P2 on ABO and GAY, and yields
slightly, though insignificantly, better performance
on the remaining two domains. In other words, our
results suggest that joint modeling of SC and RC
has a positive impact on SC performance on all
domains, and the impact can sometimes be large
enough to yield significantly better results.
5.3 Further Comparison
We hypothesized in the introduction that the
sentence-based approach to post-level RC would
yield better performance than the multi-label text
classification approach. In Section 5.2, we pre-
sented results of the sentence-based approach to
RC. So, to test this hypothesis, we next evaluate
the multi-label text classification approach to RC.
Recall that the multi-label text classification ap-
proach assumes the following setup. Given a set
of training posts where each post is multi-labeled
with the set of reasons it contains, the goal is to
train a system to determine the set of reasons a
test post contains. Hence, unlike in the sentence-
based approach, in this approach no sentence-level
reason annotations are exploited during training.
We implement this approach by recasting multi-
label text classification as n binary text classifica-
tion tasks, where n is the number of reason classes
for a domain. In the binary classification task for
predicting reason i, we train a binary classifier c
i
using the one-versus-all training scheme. Specif-
ically, to train c
i
, we create one training instance
for each post p in the training set, labeling it as
positive if and only if p contains reason i. Note
that if i is a minority reason, the class distribution
of the resulting training set will be highly skewed
towards the negatives, which will in turn cause the
resulting MaxEnt classifier to be biased towards
predicting a test instance as negative.
To address this problem, we adjust the classifi-
cation thresholds associated with the binary classi-
fiers. Recall that a test instance is classified as pos-
itive by a binary classifier if and only if its prob-
ability of belonging to the positive class is above
the classification threshold used. Hence, adjusting
the threshold amounts to adjusting the number of
test instances classified as positive, thus address-
ing the bias problem mentioned above. Specifi-
cally, we adjust the thresholds of the classifiers as
follows. We train the binary classifiers to optimize
the overall F-score by jointly tuning their classifi-
cation thresholds on 25% of the training data re-
served for development purposes. Since comput-
ing the exact solution to this optimization prob-
lem is computationally expensive, we employ a lo-
cal search algorithm that changes the value of one
threshold at a time to optimize F-score while keep-
ing the remaining thresholds fixed. During testing,
classifier c
i
will classify a test instance as positive
if its probability of belonging to the positive class
is above the corresponding threshold.
We apply this multi-label text classification ap-
proach to obtain post-level RC scores for the Base-
line, P1 and P2. Note that since P1 and P2 are
pipeline systems, the binary classifiers they use to
predict a test post?s reasons depend on the post?s
predicted stance. Specifically, if a test post is pre-
dicted to have a positive (negative) stance, then
only the reason classifiers associated with the pos-
itive (negative) stance will be used to predict the
reasons it contains. On the other hand, this ap-
proach cannot be used in combination with ILP or
the joint models to produce post-level RC scores:
they all require a reason classifier trained on
reason-annotated sentences, which are not avail-
able in the multi-label text classification approach.
Post-level RC results of the Baseline and the
two pipeline systems, P1 and P2, obtained via this
multi-label text classification approach are shown
in Table 4. These scores are significantly lower
than the corresponding scores in Table 3 by 3.2,
2.9, and 3.1 points for the Baseline, P1, and P2 re-
spectively, when averaged over the four domains.
They confirm our hypothesis that the sentence-
based approach to post-level RC is indeed better
than its multi-class text classification counterpart.
5.4 Error Analysis
To get a better understanding of our best-
performing RC system (J3), we examine its major
758
System ABO GAY OBA MAR
Baseline 39.8 37.9 30.1 40.8
P1 41.5 41.0 31.7 44.7
P2 43.3 42.6 32.0 46.3
Table 4: Post-level RC F-scores obtained via the
multi-class text classification approach.
sources of error in this subsection.
For the four domains, 75?83% of the errors
can be attributed to the system?s inability to de-
cide whether a sentence describes a reason or not.
Specifically, in 51?54% of the erroneous cases, a
reason sentence is misclassified as NONE. On the
other hand, 23?30% of the cases are concerned
with assigning a reason label to a NONE sentence.
The remaining 17?25% of the errors concern mis-
labeling a reason sentence with the wrong reason.
A closer examination of the errors reveals that
they resulted primarily from (1) the lack of access
to background knowledge, (2) the failure to pro-
cess complex discourse structures, and (3) the fail-
ure to process sarcastic statements and rhetorical
questions. We present two examples for each of
these three major sources of error from the ABO
and OBA domains in Table 5. In each example,
we show their predicted (P) and gold (G) labels.
Lack of access to background knowledge.
Consider the first example for ABO in Table 5.
Our system misclassifies this sentence in part be-
cause it lacks the background knowledge that ?ge-
netic code? is one of the characteristics of life and
a fetus having it means a fetus has life (A3). Sim-
ilarly, the system cannot determine the reason for
the first OBA example without the knowledge that
?deficit spending? is a term related to the econ-
omy and that increasing it is bad (A1). We believe
some of these relations can be extracted from lex-
ical knowledge bases such as YAGO2 (Suchanek
et al., 2007), Freebase (Bollacker et al., 2008), and
BabelNet (Navigli and Ponzetto, 2012).
Failure to process complex discourse struc-
tures. Our system misclassifies the second ex-
ample for ABO in part because the first part of
the sentence (i.e., Sure, the fetus has the potential
to one day be a person) expresses a meaning that
is completely inverted by the second part. Such
complex discourse structures often lead to classi-
fication errors even for sentences whose interpre-
tation requires no background knowledge. We be-
lieve that this problem can be addressed in part
by a better understanding of the structure of a dis-
course, particularly the relation between two dis-
course segments, using a discourse parser.
Failure to process sarcastic statements and
rhetorical questions. Owing to the nature of
our dataset (i.e., debate posts), many errors arise
from sentences containing sarcasm and/or rhetori-
cal questions. This is especially a problem in long
post sequences, where authors frequently restate
their opponents? positions, sometimes ironically.
A first step towards handling these errors would
therefore be to identify sentences containing sar-
casm and/or rhetorical questions.
6 Related Work
In this section, we discuss related work in the areas
of document-level RC, argument recognition, tex-
tual entailment in online debates, argumentation
mining, and sentiment analysis.
Document-level reason classification. Persing
and Ng (2009) apply a multi-label text classifi-
cation approach to document-level RC of aviation
safety incident reports. Given a set of pre-defined
reasons, their RC system seeks to identify the rea-
sons that can explain why the incident described
in a given report occurred. Their work is dif-
ferent from ours in at least two respects. First,
while our posts occur in post sequences (which
can be profitably exploited in RC, for example, as
in J3), their incident reports were written indepen-
dently of each other. Second, they do not perform
sentence-level RC, as the lack of sentence-level
reason annotations in their dataset prevented them
from training a sentence-level reason classifier.
Argument recognition. Boltuz?ic? and
?
Snajder
(2014) propose a multi-class classification task
called argument recognition in online discussions.
Given a post and a reason for a particular domain,
the task is to predict the extent to which the au-
thor of the post supports or opposes the reason
as measured on a five-point ordinal scale rang-
ing from ?explicitly supports? to ?explicitly op-
poses?. Hence, unlike RC, argument recognition
focuses on the magnitude rather than the exis-
tence of a post-reason relationship. In addition,
Boltuz?ic? and
?
Snajder focus on post-level (rather
than sentence-level) classification and employ per-
fect (rather than predicted) stance information.
Textual entailment in online debates. Given
the title of a debate and a post written in re-
sponse to it, this task seeks to detect arguments in
759
Domain
Background knowledge Complex discourse structure Sarcasm/rhetorical questions
Example P G Example P G Example P G
ABO Science does agree that
the fetus has an individ-
ual genetic code and fits
into the biological defi-
nition of life.
NONE A3 Sure, the fetus has the
potential to one day be
a person, but right now
it is not.
NONE F3 So are there enough
homes for 50,000,000
babies?
F9 F5
OBA Democrats have
increased deficit spend-
ing by 2 trillion dollars
over 2 years.
NONE A1 Bush raised the debt
by two billion for the
wars, Obama has out-
spent that in a week.
A2 A1 I agree, Bush put us in
debt for the next 100
years, so we can blame
Obama forever.
NONE F3
Table 5: Examples of the major sources of error. P and G stand for predicted tag and gold tag respectively.
the post that entail or contradict the title (Cabrio
and Villata, 2012). Hence, this task is concerned
with identifying text segments that correspond to
rationales without a predefined set of rationales,
whereas RC is concerned with both identifying
text segments and classifying them based on a
given set of reasons.
Argumentation mining. The goal of this task is
to extract the argumentative structure of a docu-
ment. Researchers have proposed approaches to
mine the structure of scientific papers (Teufel and
Moens, 2000; Teufel, 2001), product reviews (Vil-
lalba and Saint-Dizier, 2012; Wyner et al., 2012),
newspaper articles (Feng and Hirst, 2011), and le-
gal documents (Bru?ninghaus and Ashley, 2005;
Wyner et al., 2010; Palau and Moens, 2011; Ash-
ley and Walker, 2013). A major difference be-
tween this task and RC is that the argument types
refer to generic structural cues, textual patterns
etc., whereas our reason classes refer to the spe-
cific reasons an author may mention to support her
stance in a domain. For instance, in the case of
a scientific article, the argument types correspond
to general background, description of the paper?s
or some other papers? approach, objective, con-
trastive and/or comparative comments, etc. (Teufel
and Moens, 2000). The argument types for legal
documents refer to legal factors which are either
pro-plaintiff or pro-defendant (Bru?ninghaus and
Ashley, 2005). For instance, for trade secret law
cases, factors such as Waiver-of-Confidentiality
and Disclosure-in-Public-Forum refer to certain
facts strengthening the claim of one of the sides
participating in a case.
Sentiment analysis. RC resembles certain tasks
in sentiment analysis. One such task is pro and con
reason classification in reviews (Kim and Hovy,
2006), where sentences containing opinions as
well as reasons justifying the opinions are to be
extracted and classified as PRO, CON, or NONE.
Hence, this task focuses on categorizing sentences
into coarse-grained, high-level groups (e.g., PRO
vs. CON, POSITIVE vs. NEGATIVE), but does not
attempt to subcategorize the PRO and CON classes
into fine-grained reason classes, unlike RC. Some-
what similar to the PRO and CON sentence classifi-
cation task is the task of determining the relevance
of a sentence in a review for polarity classifica-
tion. Zaidan et al. (2007) coined the term ratio-
nale to refer to any subjective textual content that
contains evidence supporting the author?s opinion
or stance. These rationales, however, may not al-
ways contain reasons. For instance, a sentence that
mentions that the author likes a product is a ra-
tionale, but it does not contain any reason for her
liking it. Methods have been proposed for auto-
matically identifying rationales (e.g., Yessenalina
et al. (2010), Trivedi and Eisenstein (2013)) and
distinguishing subjective from objective materials
in a review (e.g., Pang and Lee (2004), Wiebe and
Riloff (2005), McDonald et al. (2007), Zhao et al.
(2008)). Note that in all these attempts, the end
goal is not to classify sentences, but to employ
the results of sentence classification to improve a
higher-level task, such as sentiment classification.
7 Conclusion
We examined the new task of reason classification.
We exploited stance information for reason classi-
fication, proposing systems of varying complexity
for modeling stances and reasons. Experiments on
our reason-annotated corpus of ideological debate
posts from four domains demonstrate that sophis-
ticated models of stances and reasons can indeed
yield more accurate reason and stance classifica-
tion results than their simpler counterparts. Nev-
ertheless, reason classification remains a challeng-
ing task: the best post-level F-scores are in the low
50s. By making our corpus publicly available, we
hope to stimulate further research on this task.
760
Acknowledgments
We thank the three anonymous reviewers for their
detailed and insightful comments on an earlier
draft of this paper. This work was supported in
part by NSFGrants IIS-1147644 and IIS-1219142.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views or of-
ficial policies, either expressed or implied, of NSF.
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, pages 1?9.
Kevin D. Ashley and Vern R. Walker. 2013. From in-
formation retrieval (IR) to argument retrieval (AR)
for legal cases: Report on a baseline study. In Pro-
ceedings of the 26th InternationalConference on Le-
gal Knowledge and Information System, pages 29?
38.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In COLING 2008: Companion volume:
Posters, pages 15?18.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247?1250.
Filip Boltuz?ic? and Jan
?
Snajder. 2014. Back up your
stance: Recognizing arguments in online discus-
sions. In Proceedings of the First Workshop on Ar-
gumentation Mining, pages 49?58.
Stefanie Bru?ninghaus and Kevin D. Ashley. 2005.
Reasoning with textual cases. In Proceedings of the
6th International Conference on Case-Based Rea-
soning, pages 137?151.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Elena Cabrio and Serena Villata. 2012. Natural lan-
guage arguments: A combined approach. In Pro-
ceedings of the 20th European Conference on Artifi-
cial Intelligence, pages 205?210.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The Kappa statistic. Computational
Linguistics, 22(2):249?254.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. SEMAFOR 1.0: A
probabilistic frame-semantic parser. Technical re-
port, Carnegie Mellon University Technical Report
CMU-LTI-10-001.
Vanessa Wei Feng and Graeme Hirst. 2011. Classi-
fying arguments by scheme. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 987?996.
Kazi Saidul Hasan and Vincent Ng. 2013a. Extra-
linguistic constraints on stance recognition in ide-
ological debates. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pages 816?821.
Kazi Saidul Hasan and Vincent Ng. 2013b. Frame se-
mantics for stance classification. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning, pages 124?132.
Kazi Saidul Hasan and Vincent Ng. 2013c. Stance
classification of ideological debates: Data, mod-
els, features, and constraints. In Proceedings of
the Sixth International Joint Conference on Natural
Language Processing, pages 1348?1356.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL Main
Conference Poster Sessions, pages 483?490.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmentation. In
Proceedings of the 17th International Conference on
Machine Learning, pages 591?598.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 432?439.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
Raquel Mochales Palau and Marie-Francine Moens.
2011. Argumentationmining. Artificial Intelligence
and Law, 19(1):1?22.
761
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Meeting of the Association for Computa-
tional Linguistics, pages 271?278.
Isaac Persing and Vincent Ng. 2009. Semi-supervised
cause identification from aviation safety reports. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 843?851.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on ComputationalNatural Language Learning,
pages 1?8.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing stances in online debates. In Proceed-
ings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 226?234.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697?706.
Simone Teufel and Marc Moens. 2000. What?s yours
and what?s mine: Determining intellectual attribu-
tion in scientific text. In Proceedings of the 2000
Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, pages 9?17.
Simone Teufel. 2001. Task-based evaluation of sum-
mary quality: Describing relationships between sci-
entific papers. In Proceedings of the NAACL Work-
shop on Automatic Summarization, pages 12?21.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Rakshit Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 808?813.
Maria Paz Garcia Villalba and Patrick Saint-Dizier.
2012. Some facets of argument mining for opinion
analysis. In Proceedings of the Fourth International
Conference on Computational Models of Argument,
pages 23?34.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decod-
ing algorithm. IEEE Transactions on Information
Theory, 13(2):260?269.
MarilynWalker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the 6th International
Conference on Computational Linguistics and Intel-
ligent Text Processing, pages 486?497.
AdamWyner, Raquel Mochales-Palau, Marie-Francine
Moens, and David Milward. 2010. Approaches to
text mining arguments from legal cases. In Enrico
Francesconi, Simonetta Montemagni, Wim Peters,
and Daniela Tiscornia, editors, Semantic Processing
of Legal Texts: Where the Language of Law Meets
the Law of Language, pages 60?79. Springer-Verlag.
Adam Wyner, Jodi Schneider, Katie Atkinson, and
Trevor J. M. Bench-Capon. 2012. Semi-automated
argumentative analysis of online product reviews. In
Proceedings of the Fourth International Conference
on Computational Models of Argument, pages 43?
50.
Ainur Yessenalina, Yejin Choi, and Claire Cardie.
2010. Automatically generating annotator rationales
to improve sentiment classification. In Proceedings
of the ACL 2010 Conference Short Papers, pages
336?341.
Omar Zaidan, Jason Eisner, and Christine Piatko.
2007. Using ?annotator rationales? to improve ma-
chine learning for text categorization. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 260?267.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding re-
dundant features for crfs-based sentence sentiment
classification. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 117?126.
762
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 816?821,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extra-Linguistic Constraints on Stance Recognition in Ideological Debates
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Determining the stance expressed by an
author from a post written for a two-
sided debate in an online debate forum
is a relatively new problem. We seek to
improve Anand et al?s (2011) approach
to debate stance classification by model-
ing two types of soft extra-linguistic con-
straints on the stance labels of debate
posts, user-interaction constraints and ide-
ology constraints. Experimental results on
four datasets demonstrate the effectiveness
of these inter-post constraints in improv-
ing debate stance classification.
1 Introduction
While a lot of work on document-level opinion
mining has involved determining the polarity ex-
pressed in a customer review (e.g., whether a re-
view is ?thumbs up? or ?thumbs down?) (see Pang
and Lee (2008) and Liu (2012) for an overview
of the field), researchers have begun exploring
new opinion mining tasks in recent years. One
such task is debate stance classification: given
a post written for a two-sided topic discussed in
an online debate forum (e.g., ?Should abortion be
banned??), determine which of the two sides (i.e.,
for and against) its author is taking.
Debate stance classification is potentially more
interesting and challenging than polarity classifi-
cation for at least two reasons. First, while in po-
larity classification sentiment-bearing words and
phrases have proven to be useful (e.g., ?excellent?
correlates strongly with the positive polarity), in
debate stance classification it is not uncommon to
find debate posts where stances are not expressed
in terms of sentiment words, as exemplified in Fig-
ure 1, where the author is for abortion.
Second, while customer reviews are typically
written independently of other reviews in an on-
line forum, the same is not true for debate posts. In
The fetus is simply a part of the mother?s body and she
can have an abortion because it is her human rights. Also
I take this view because every woman can face with sit-
uation when two lives are at stake and the moral obli-
gation is to save the one closest at hand ? namely, that
of the mother, whose life is always more immediate than
that of the unborn child within her body. Permission for
an abortion could then be based on psychiatric consider-
ations such as prepartum depression, especially if there
is responsible psychiatric opinion that a continued preg-
nancy raises the strong probability of suicide in a clini-
cally depressed patient.
Figure 1: A sample post on abortion.
a debate forum, debate posts form threads, where
later posts often support or oppose the viewpoints
raised in earlier posts in the same thread.
Previous approaches to debate stance classifica-
tion have focused on three debate settings, namely
congressional floor debates (Thomas et al, 2006;
Bansal et al, 2008; Balahur et al, 2009; Yesse-
nalina et al, 2010; Burfoot et al, 2011), company-
internal discussions (Murakami and Raymond,
2010), and online social, political, and ideologi-
cal debates in public forums (Agrawal et al, 2003;
Somasundaran and Wiebe, 2010; Wang and Rose?,
2010; Biran and Rambow, 2011; Hasan and Ng,
2012). As Walker et al (2012) point out, debates
in public forums differ from congressional debates
and company-internal discussions in terms of lan-
guage use. Specifically, online debaters use color-
ful and emotional language to express their points,
which may involve sarcasm, insults, and question-
ing another debater?s assumptions and evidence.
These properties can potentially make stance clas-
sification of online debates more challenging than
that of the other two types of debates.
Our goal in this paper is to improve the state-
of-the-art supervised learning approach to debate
stance classification of online debates proposed by
Anand et al (2011), focusing in particular on ideo-
logical debates. Specifically, we hypothesize that
there are two types of soft extra-linguistic con-
straints on the stance labels of debate posts that,
816
Number ?for? % of posts Average thread
Domain of posts posts (%) in a thread length
ABO 1741 54.9 75.1 4.1
GAY 1376 63.4 74.5 4.0
OBA 985 53.9 57.1 2.6
MAR 626 69.5 58.0 2.5
Table 1: Statistics of the four datasets.
if explicitly modeled, could improve a learning-
based stance classification system. We refer to
these two types of inter-post constraints as user-
interaction constraints and ideology constraints.
We show how they can be learned from stance-
annotated debate posts in Sections 4.1 and 4.2, re-
spectively.
2 Datasets
For our experiments, we collect debate posts
from four popular domains, Abortion (ABO),
Gay Rights (GAY), Obama (OBA), and Marijuana
(MAR), from an online debate forum1. All de-
bates are two-sided, so each post receives one of
two domain labels, for or against, depending on
whether the author of the post supports or opposes
abortion, gay rights, Obama, or the legalization of
marijuana.
We construct one dataset for each domain (see
Table 1 for statistics). The fourth column of the
table shows the percentage of posts in each domain
that appear in a thread. More precisely, a thread
is a tree with one or more nodes such that (1) each
node corresponds to a debate post, and (2) a post yi
is the parent of another post yj if yj is a reply to yi.
Given a thread, we can generate post sequences,
each of which is a path from the root of the thread
to one of its leaves.
3 Baseline Systems
We employ as baselines two stance classification
systems, Anand et al?s (2011) approach and an en-
hanced version of it, as described below.
Our first baseline, Anand et al?s approach is a
supervised method that trains a stance classifier
for determining whether the stance expressed in
a debate post is for or against the topic. Hence,
we create one training instance from each post in
the training set, using the stance it expresses as
its class label. Following Anand et al, we repre-
sent a training instance using three types of lexico-
syntactic features, which are briefly summarized
in Table 2. In our implementation, we train the
1http://www.createdebate.com/
Feature type Features
Basic Unigrams, bigrams, syntactic and POS-
generalized dependencies
Sentiment LIWC counts, opinion dependencies
Argument Cue words, repeated punctuation, context
Table 2: Anand et al?s features.
stance classifier using SVMlight (Joachims, 1999).
After training, we can apply the classifier to clas-
sify the test instances, which are generated in the
same way as the training instances.
Related work on stance classification of con-
gressional debates has found that enforcing au-
thor constraints (ACs) can improve classification
performance (e.g., Thomas et al (2006), Bansal et
al. (2008), Burfoot et al (2011), Lu et al (2012),
Walker et al (2012)). ACs are a type of inter-
post constraints that specify that two posts written
by the same author for the same debate domain
should have the same stance. We hypothesize that
ACs could similarly be used to improve stance
classification of ideological debates, and therefore
propose a second baseline where we enhance the
first baseline with ACs. Enforcing ACs is simple.
We first use the learned stance classifier to classify
the test posts as in the first baseline, and then post-
process the labels of the test posts. Specifically,
we sum up the confidence values2 assigned to the
set of test posts written by the same author for the
same debate domain. If the sum is positive, then
we label all the posts in this set as for; otherwise
we label them as against.
4 Extra-Linguistic Constraints
In this section, we introduce two types of inter-
post constraints on debate stance classification.
4.1 User-Interaction Constraints
We call the first type of constraints user-
interaction constraints (UCs). UCs are motivated
by the observation that the stance labels of the
posts in a post sequence are not independent of
each other. Consider the post sequence in Fig-
ure 2, where each post is a response to the preced-
ing post. It shows an opening anti-abortion post
(P1), followed by a pro-abortion comment (P2),
which is in turn followed by another anti-abortion
view (P3). While this sequence contains alternat-
ing posts from opposing stances, in general there
is no hard constraint on the stance of a post given
2We use as the confidence value the signed distance of the
associated test point from the SVM hyperplane.
817
[P1: Anti-abortion] There are thousands of people who
want to take these children because they cannot have their
own. If you do not want a child, have it and put it up for
adoption. At least you will be preserving a human life rather
than killing one.
[P2: Pro-abortion] I agree that if people don?t want
their babies, they should have the choice of putting it
up for adoption. But it should not be made compulsory,
which is essentially what happens if you ban abortion.
[P3: Anti-abortion] Why should it not be made
compulsory? Those children have as much right to
live as you and I. Besides, no one loses with adop-
tion, so why wouldn?t you utilize it?
Figure 2: A sample post sequence. P2 and P3 are
replies to P1 and P2, respectively.
the preceding sequence of posts. Nevertheless, we
found that in our training data, a for (against) post
is followed by a against (for) post 80% of the time.
UCs aim to model the regularities in how users
interact with each other in a post sequence as soft
constraints. These kinds of soft constraints can be
naturally encoded as factors over adjacent posts in
a post sequence (see Kschischang et al (2001)),
which can in turn be learned by recasting stance
classification as a sequence labeling task. In our
experiments, we seek to derive the best sequence
of stance labels for each post sequence of length ?
1 using a Conditional Random Field (CRF) (Laf-
ferty et al, 2001).
We train the CRF model using the CRF im-
plementation in Mallet (McCallum, 2002). Each
training sequence corresponds to a post sequence.
Each post in a sequence is represented using the
same set of features as in the baselines.
After training, the resulting CRF model can be
used to assign a stance sequence to each test post
sequence. There is a caveat, however. Since a
given test post may appear in more than one se-
quence, different occurrences of it may be as-
signed different stance labels by the CRF. To deter-
mine the final stance label for the post, we average
the probabilities assigned to the for stance over all
its occurrences; if the average is ? 0.5, then its
final label is for; otherwise, its label is against.
4.2 Ideology Constraints
Next, we introduce our second type of inter-post
constraints, ideology constraints (ICs). ICs are
cross-domain, author-based constraints: they are
only applicable to debate posts written by the same
author in different domains. ICs model the fact
that for some authors, their stances on various is-
sues are determined in part by their ideological
values, and in particular, their stances on different
issues may be correlated. For example, someone
who opposes abortion is likely to be a conserva-
tive and has a good chance of opposing gay rights.
ICs aim to capture this kind of inter-domain corre-
lation of stances. Below we describe how we im-
plement ICs and show how they can be integrated
with ACs.
4.2.1 Implementing Ideology Constraints
We first compute a set of conditional probabil-
ities, P (stance(dq )=sd|stance(dp)=sc), where (1)
dp, dq ? Domains (i.e., the set of four domains),
(2) sc, sd ? {for, against}, and (3) dp 6= dq .
To compute P (stance(dq )=sd|stance(dp)=sc), we
(1) determine for each author a in the train-
ing set and each domain dp the stance of a
in dp (denoted by author-stance(dp ,a)), where
author-stance(dp ,a) is computed as the majority
stance labels associated with the debate posts
in the training set that a wrote for dp; and
(2) compute P (stance(dq )=sd|stance(dp)=sc) as
the ratio of
?
a?A Count(author-stance(dp ,a)=sc,
author-stance(dq ,a)=sd) to
?
a?A Count(author-
stance(dp,a)=sc), where A is the set of authors in
the training set who posted in both dp and dq. It
should be fairly easy to see that these conditional
probabilities measure the degree of correlation be-
tween the stances in different domains.
4.2.2 Inference Using ILP
Recall that in our second baseline, we employ
ACs to postprocess the output of the stance clas-
sifier simply by summing up the confidence val-
ues assigned to the posts written by the same au-
thor for the same debate domain. However, since
we now want to enforce two types of inter-post
constraints (namely, ACs and ICs), we will have
to employ a more sophisticated inference mecha-
nism. Previous work has focused on employing
graph minimum cut (MinCut) as the inference al-
gorithm. However, since MinCut suffers from the
weakness of not being able to enforce negative
constraints (i.e., two posts cannot receive the same
label) (Bansal et al, 2008), we propose to use in-
teger linear programming (ILP) as the underlying
inference mechanism. Below we show how to im-
plement ACs and ICs within the ILP framework.
Owing to space limitations, we refer the reader
to Roth and Yih (2004) for details of the ILP
framework. Briefly, ILP seeks to optimize an
objective function subject to a set of linear con-
818
straints. Below we focus on describing the ILP
program and how the ACs and ICs can be encoded.
Let Y = y1, . . . , yn be the set of debate posts.
For each yi, we create one (binary-valued) indi-
cator variable xi, which will be used in the ILP
program. Let pi = P (for|yi) be the ?benefit? of
setting xi to 1, where P (for|yi) is provided by the
CRF. Consequently, after optimization, yi?s stance
is for if its xi is set to 1. We optimize the following
objective function:
max
?
i
pixi + (1? pi)(1? xi)
subject to a set of linear constraints, which encode
the ACs and the ICs, as described below.
Implementing author constraints. If yi and yj
are composed by the same author, we ensure that
xi and xj will be assigned the same value by em-
ploying the linear constraint |xi ? xj| = 0.
Implementing ideology constraints. For con-
venience, below we use the notation introduced in
Section 4.2.1, and assume that yi and yj are two
arbitrary posts written by the same author in do-
mains dp and dq, respectively.
Case 1: If P (stance(dq )=for|stance(dp)=for) ? t,
we want to ensure that xi=1 =? xj=1.3 This can
be achieved using the constraint (1?xj) ? (1?xi).
Case 2: If P (stance(dq )=against|stance(dp )=against)
? t, we want to ensure that xi=0 =? xj=0. This
can be achieved using the constraint xj ? xi.
Case 3: If P (stance(dq )=against|stance(dp )=for)
? t, we want to ensure that xi=1 =? xj=0. This
can be achieved using the constraint xj ? (1?xi).
Case 4: If P (stance(dq )=for|stance(dp)=against)
? t, we want to ensure that xi=0 =? xj=1. This
can be achieved using the constraint (1?xj) ? xi.
Two points deserve mention. First, cases 3 and
4 correspond to negative constraints, and unlike in
MinCut, they can be implemented easily in ILP.
Second, if ICs are used, one ILP program will be
created to perform inference over the debate posts
in all four domains.
5 Evaluation
5.1 Experimental Setup
Results are expressed in terms of accuracy ob-
tained via 5-fold cross validation, where accuracy
3Intuitively, if this condition is satisfied, it means that
there is sufficient evidence that the two nodes from differ-
ent domains should have the same stance, and so we convert
the soft ICs into (hard) linear constraints in ILP. Note that t is
a threshold to be tuned using development data.
System ABO GAY OBA MAR
Anand 61.4 62.6 58.1 66.9
Anand+AC 72.0 64.9 62.7 67.8
Anand+AC+UC 73.7 69.9 64.1 75.4
Anand+AC+UC+IC 74.9 70.9 72.7 75.4
Table 3: 5-fold cross-validation accuracies.
is the percentage of test instances correctly classi-
fied. Since all experiments require the use of de-
velopment data for parameter tuning, we use three
folds for model training, one fold for development,
and one fold for testing in each fold experiment.
5.2 Results
Results are shown in Table 3. Row 1 shows the
results of the Anand et al (2011) baseline (see
Section 3) on the four datasets, obtained by train-
ing a SVM stance classifier using the SVMlight
software.4 Row 2 shows the results of the sec-
ond baseline, Anand et al?s system enhanced with
ACs. As we can see, incorporating ACs into
Anand et al?s system improves its performance
significantly on all datasets and yields a system
that achieves an average improvement of 4.6 ac-
curacy points.5
Next, we incorporate our first type of con-
straints, UCs, into the better of the two baselines
(i.e., the second baseline). Results of applying the
CRF for modeling UCs to the test posts and post-
processing them using the ACs are shown in row 3
of Table 3. As we can see, incorporating UCs into
the second baseline significantly improves its per-
formance and yields a system that achieves an av-
erage improvement of 3.93 accuracy points.
Finally, we incorporate our second type of con-
straints, ICs, effectively performing inference over
the CRF output using ILP with ACs and ICs as the
inter-post constraints. Results of this experiment
are shown in row 4 of Table 3. As we can see, in-
corporating the ICs significantly improves the per-
formance of the system on all but MAR and yields
a system that achieves an average improvement of
2.7 accuracy points.
Overall, our inter-post constraints yield a stance
classification system that significantly outper-
forms the better baseline on all four datasets, with
an average improvement of 6.63 accuracy points.
4For all SVM experiments, the regularization parameter C
is tuned using development data, but the remaining learning
parameters are set to their default values.
5All significance tests are paired t-tests, with p < 0.05.
819
5.3 Discussion
Next, we make some observations on the results of
applying ICs to our datasets.
First, ICs do not improve the MAR dataset. An
examination of the domains reveals the reason. We
find three pairs of ICs involving the other three do-
mains ? ABO, GAY, and OBA ? in our training
data. More specifically, the stances of the posts
written by an author for these three domains are
all positively co-related. In other words, if an au-
thor supports abortion, it is likely that she supports
both gay rights and Obama as well. On the other
hand, we find no co-relation between MAR and
the remaining domains. This means that no ICs
can be established between the posts in MAR and
those in the remaining domains.
Second, the improvement resulting from the ap-
plication of ICs is much larger on the OBA dataset
than on ABO and GAY. The reason can be at-
tributed to the fact that ICs exist more frequently
between OBA and ABO and between OBA and
GAY than between ABO and GAY. Specifically,
ICs are seen in all five folds of the data in the
first two pairs of domains, whereas they are seen
in only two folds in the last pair of domains.
6 Related Work
Previous work has investigated the use of extra-
linguistic constraints to improve stance classifica-
tion. Introduced by Thomas et al (2006), ACs are
arguably the most commonly used extra-linguistic
constraints. Since then, they have been employed
and extended in different ways (see, for example,
Bansal et al (2008), Burfoot et al (2011), Lu et al
(2012), and Walker et al (2012)).
ICs are different from ACs in at least two re-
spects. First, ICs are softer than ACs, so accu-
rate modeling of ICs has to be based on stance-
annotated data. Although we employ ICs as hard
constraints (owing in part to our use of the ILP
framework), they can be used directly as soft con-
straints in other frameworks, such as MinCut. Sec-
ond, ICs are inter-domain constraints, whereas
ACs are intra-domain constraints. To our knowl-
edge, this is the first time inter-domain constraints
are employed for stance classification.
There has been work related to the modeling of
user interaction in a post sequence. Recall that be-
tween two adjacent posts in a post sequence that
have opposing stances, there exists a rebuttal link.
Walker et al (2012) employ manually identified
rebuttal links as hard inter-post constraints dur-
ing inference. However, since automatic discov-
ery of rebuttal links is a non-trivial problem, em-
ploying gold rebuttal links substantially simplifies
the stance classification task. Lu et al (2012), on
the other hand, predict whether a link is of type
agreement or disagreement using a bootstrapped
classifier. Anand et al (2011) do not predict links.
Instead, hypothesizing that the content of the pre-
ceding post in a post sequence would be useful
for predicting the stance of the current post, they
employ features computed based on the preceding
post when training a stance classifier. Hence, un-
like us, they classify each post independently of
the others, whereas we classify the posts in a se-
quence in dependent relation to each other.
The ILP framework has been applied to perform
joint inference for a variety of stance prediction
tasks. Lu et al (2012) address the task of discov-
ering opposing opinion networks, where the goal
is to partition the authors in a debate (e.g., gay
rights) based on whether they support or oppose
the given issue. To this end, they employ ILP
to coordinate different sources of information. In
our previous work on debate stance classification
(Hasan and Ng, 2012), we employ ILP to coor-
dinate the output of two classifiers: a post-stance
classifier, which determines the stance of a debate
post written for a domain (e.g., gay rights); and
a topic-stance classifier, which determines the au-
thor?s stance on each topic mentioned in her post
(e.g., gay marriage, gay adoption). In this work,
on the other hand, we train only one classifier,
but use ILP to coordinate two types of constraints,
ACs and ICs.
7 Conclusions
We examined the under-studied task of stance
classification of ideological debates. Employing
our two types of extra-linguistic constraints yields
a system that outperforms an improved version of
Anand et al?s approach by 2.9?10 accuracy points.
While the effectiveness of ideology constraints de-
pends to some extent on the ?relatedness? of the
underlying ideological domains, we believe that
the gains they offer will increase with the num-
ber of authors posting in different domains and the
number of related domains.6
6Only a small fraction of the authors posted in multiple
domains in our datasets: 12% and 5% of them posted in two
and three domains, respectively.
820
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In Pro-
ceedings of the 12th International Conference on
World Wide Web, WWW ?03, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2011), pages 1?9.
Alexandra Balahur, Zornitsa Kozareva, and Andre?s
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Proceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ?09, pages 468?480.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of the 22nd International
Conference on Computational Linguistics: Com-
panion volume: Posters, pages 15?18.
Or Biran and Owen Rambow. 2011. Identifying justi-
fications in written dialogs. In Proceedings of the
2011 IEEE Fifth International Conference on Se-
mantic Computing, ICSC ?11, pages 162?168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Kazi Saidul Hasan and Vincent Ng. 2012. Predict-
ing stance in ideological debate with rich linguistic
knowledge. In Proceedings of the 24th International
Conference on Computational Linguistics: Posters,
pages 451?460.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Advances in Kernel Methods -
Support Vector Learning, pages 44?56. MIT Press.
Frank Kschischang, Brendan J. Frey, and Hans-Andrea
Loeliger. 2001. Factor graphs and the sum-product
algorithm. IEEE Transactions on Information The-
ory, 47:498?519.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282?
289.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Morgan & Claypool Publishers.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM
?12, pages 1642?1646.
Andrew Kachites McCallum. 2002. Mallet: A ma-
chine learning for language toolkit. http://
mallet.cs.umass.edu.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose? Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1?2):1?135.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of the Eighth Confer-
ence on Computational Natural Language Learning,
pages 1?8.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 116?124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Making
conversational structure explicit: Identification of
initiation-response pairs within online discussions.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 673?676.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1046?1056.
821
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1262?1273,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic Keyphrase Extraction: A Survey of the State of the Art
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
While automatic keyphrase extraction has
been examined extensively, state-of-the-
art performance on this task is still much
lower than that on many core natural lan-
guage processing tasks. We present a sur-
vey of the state of the art in automatic
keyphrase extraction, examining the major
sources of errors made by existing systems
and discussing the challenges ahead.
1 Introduction
Automatic keyphrase extraction concerns ?the au-
tomatic selection of important and topical phrases
from the body of a document? (Turney, 2000). In
other words, its goal is to extract a set of phrases
that are related to the main topics discussed in a
given document (Tomokiyo and Hurst, 2003; Liu
et al, 2009b; Ding et al, 2011; Zhao et al, 2011).
Document keyphrases have enabled fast and ac-
curate searching for a given document from a large
text collection, and have exhibited their potential
in improving many natural language processing
(NLP) and information retrieval (IR) tasks, such
as text summarization (Zhang et al, 2004), text
categorization (Hulth and Megyesi, 2006), opin-
ion mining (Berend, 2011), and document index-
ing (Gutwin et al, 1999).
Owing to its importance, automatic keyphrase
extraction has received a lot of attention. However,
the task is far from being solved: state-of-the-art
performance on keyphrase extraction is still much
lower than that on many core NLP tasks (Liu et al,
2010). Our goal in this paper is to survey the state
of the art in keyphrase extraction, examining the
major sources of errors made by existing systems
and discussing the challenges ahead.
2 Corpora
Automatic keyphrase extraction systems have
been evaluated on corpora from a variety of
sources ranging from long scientific publications
to short paper abstracts and email messages. Ta-
ble 1 presents a listing of the corpora grouped by
their sources as well as their statistics.
1
There are
at least four corpus-related factors that affect the
difficulty of keyphrase extraction.
Length The difficulty of the task increases with
the length of the input document as longer doc-
uments yield more candidate keyphrases (i.e.,
phrases that are eligible to be keyphrases (see Sec-
tion 3.1)). For instance, each Inspec abstract has
on average 10 annotator-assigned keyphrases and
34 candidate keyphrases. In contrast, a scientific
paper typically has at least 10 keyphrases and hun-
dreds of candidate keyphrases, yielding a much
bigger search space (Hasan and Ng, 2010). Conse-
quently, it is harder to extract keyphrases from sci-
entific papers, technical reports, and meeting tran-
scripts than abstracts, emails, and news articles.
Structural consistency In a structured doc-
ument, there are certain locations where a
keyphrase is most likely to appear. For instance,
most of a scientific paper?s keyphrases should ap-
pear in the abstract and the introduction. While
structural information has been exploited to ex-
tract keyphrases from scientific papers (e.g., title,
section information) (Kim et al, 2013), web pages
(e.g., metadata) (Yih et al, 2006), and chats (e.g.,
dialogue acts) (Kim and Baldwin, 2012), it is most
useful when the documents from a source exhibit
structural similarity. For this reason, structural in-
formation is likely to facilitate keyphrase extrac-
tion from scientific papers and technical reports
because of their standard format (i.e., standard
sections such as abstract, introduction, conclusion,
etc.). In contrast, the lack of structural consistency
in other types of structured documents (e.g., web
pages, which can be blogs, forums, or reviews)
1
Many of the publicly available corpora can be found
in http://github.com/snkim/AutomaticKeyphraseExtraction/
and http://code.google.com/p/maui-indexer/downloads/list.
1262
Source Dataset/Contributor
Statistics
Documents Tokens/doc Keys/doc
Paper abstracts Inspec (Hulth, 2003)? 2,000 <200 10
Scientific papers
NUS corpus (Nguyen and Kan, 2007)? 211 ?8K 11
citeulike.org (Medelyan et al, 2009)? 180 - 5
SemEval-2010 (Kim et al, 2010b)? 284 >5K 15
Technical reports NZDL (Witten et al, 1999)? 1,800 - -
News articles
DUC-2001 (Wan and Xiao, 2008b)? 308 ?900 8
Reuters corpus (Hulth and Megyesi, 2006) 12,848 - 6
Web pages Yih et al (2006) 828 - -
Hammouda et al (2005)? 312 ?500 -
Blogs (Grineva et al, 2009) 252 ?1K 8
Meeting transcripts ICSI (Liu et al, 2009a) 161 ?1.6K 4
Emails Enron corpus (Dredze et al, 2008)? 14,659 - -
Live chats Library of Congress (Kim and Baldwin, 2012) 15 - 10
Table 1: Evaluation datasets. Publicly available datasets are marked with an asterisk (?).
may render structural information less useful.
Topic change An observation commonly ex-
ploited in keyphrase extraction from scientific ar-
ticles and news articles is that keyphrases typically
appear not only at the beginning (Witten et al,
1999) but also at the end (Medelyan et al, 2009)
of a document. This observation does not neces-
sarily hold for conversational text (e.g., meetings,
chats), however. The reason is simple: in a conver-
sation, the topics (i.e., its talking points) change as
the interaction moves forward in time, and so do
the keyphrases associated with a topic. One way
to address this complication is to detect a topic
change in conversational text (Kim and Baldwin,
2012). However, topic change detection is not al-
ways easy: while the topics listed in the form of an
agenda at the beginning of formal meeting tran-
scripts can be exploited, such clues are absent in
casual conversations (e.g., chats).
Topic correlation Another observation com-
monly exploited in keyphrase extraction from
scientific articles and news articles is that the
keyphrases in a document are typically related to
each other (Turney, 2003; Mihalcea and Tarau,
2004). However, this observation does not nec-
essarily hold for informal text (e.g., emails, chats,
informal meetings, personal blogs), where people
can talk about any number of potentially uncorre-
lated topics. The presence of uncorrelated topics
implies that it may no longer be possible to exploit
relatedness and therefore increases the difficulty of
keyphrase extraction.
3 Keyphrase Extraction Approaches
A keyphrase extraction system typically operates
in two steps: (1) extracting a list of words/phrases
that serve as candidate keyphrases using some
heuristics (Section 3.1); and (2) determining
which of these candidate keyphrases are correct
keyphrases using supervised (Section 3.2) or un-
supervised (Section 3.3) approaches.
3.1 Selecting Candidate Words and Phrases
As noted before, a set of phrases and words is
typically extracted as candidate keyphrases using
heuristic rules. These rules are designed to avoid
spurious instances and keep the number of candi-
dates to a minimum. Typical heuristics include (1)
using a stop word list to remove stop words (Liu et
al., 2009b), (2) allowing words with certain part-
of-speech tags (e.g., nouns, adjectives, verbs) to be
candidate keywords (Mihalcea and Tarau, 2004;
Wan and Xiao, 2008b; Liu et al, 2009a), (3) al-
lowing n-grams that appear in Wikipedia article
titles to be candidates (Grineva et al, 2009), and
(4) extracting n-grams (Witten et al, 1999; Hulth,
2003; Medelyan et al, 2009) or noun phrases
(Barker and Cornacchia, 2000; Wu et al, 2005)
that satisfy pre-defined lexico-syntactic pattern(s)
(Nguyen and Phan, 2009).
Many of these heuristics have proven effective
with their high recall in extracting gold keyphrases
from various sources. However, for a long docu-
ment, the resulting list of candidates can be long.
Consequently, different pruning heuristics have
been designed to prune candidates that are un-
likely to be keyphrases (Huang et al, 2006; Kumar
and Srinathan, 2008; El-Beltagy and Rafea, 2009;
You et al, 2009; Newman et al, 2012).
3.2 Supervised Approaches
Research on supervised approaches to keyphrase
extraction has focused on two issues: task refor-
mulation and feature design.
1263
3.2.1 Task Reformulation
Early supervised approaches to keyphrase extrac-
tion recast this task as a binary classification prob-
lem (Frank et al, 1999; Turney, 1999; Witten et
al., 1999; Turney, 2000). The goal is to train a
classifier on documents annotated with keyphrases
to determine whether a candidate phrase is a
keyphrase. Keyphrases and non-keyphrases are
used to generate positive and negative examples,
respectively. Different learning algorithms have
been used to train this classifier, including na??ve
Bayes (Frank et al, 1999; Witten et al, 1999),
decision trees (Turney, 1999; Turney, 2000), bag-
ging (Hulth, 2003), boosting (Hulth et al, 2001),
maximum entropy (Yih et al, 2006; Kim and Kan,
2009), multi-layer perceptron (Lopez and Romary,
2010), and support vector machines (Jiang et al,
2009; Lopez and Romary, 2010).
Recasting keyphrase extraction as a classifica-
tion problem has its weaknesses, however. Recall
that the goal of keyphrase extraction is to identify
the most representative phrases for a document.
In other words, if a candidate phrase c
1
is more
representative than another candidate phrase c
2
, c
1
should be preferred to c
2
. Note that a binary clas-
sifier classifies each candidate keyphrase indepen-
dently of the others, and consequently it does not
allow us to determine which candidates are better
than the others (Hulth, 2004; Wang and Li, 2011).
Motivated by this observation, Jiang et al
(2009) propose a ranking approach to keyphrase
extraction, where the goal is to learn a ranker
to rank two candidate keyphrases. This pairwise
ranking approach therefore introduces competi-
tion between candidate keyphrases, and has been
shown to significantly outperform KEA (Witten
et al, 1999; Frank et al, 1999), a popular su-
pervised baseline that adopts the traditional super-
vised classification approach (Song et al, 2003;
Kelleher and Luz, 2005).
3.2.2 Features
The features commonly used to represent an in-
stance for supervised keyphrase extraction can be
broadly divided into two categories.
3.2.2.1 Within-Collection Features
Within-collection features are computed based
solely on the training documents. These features
can be further divided into three types.
Statistical features are computed based on sta-
tistical information gathered from the training
documents. Three such features have been exten-
sively used in supervised approaches. The first
one, tf*idf (Salton and Buckley, 1988), is com-
puted based on candidate frequency in the given
text and inverse document frequency (i.e., number
of other documents where the candidate appears).
2
The second one, the distance of a phrase, is de-
fined as the number of words preceding its first
occurrence normalized by the number of words in
the document. Its usefulness stems from the fact
that keyphrases tend to appear early in a docu-
ment. The third one, supervised keyphraseness,
encodes the number of times a phrase appears as
a keyphrase in the training set. This feature is de-
signed based on the assumption that a phrase fre-
quently tagged as a keyphrase is more likely to be
a keyphrase in an unseen document. These three
features form the feature set of KEA (Witten et al,
1999; Frank et al, 1999), and have been shown to
perform consistently well on documents from var-
ious sources (Yih et al, 2006; Kim et al, 2013).
Other statistical features include phrase length and
spread (i.e., the number of words between the first
and last occurrences of a phrase in the document).
Structural features encode how different in-
stances of a candidate keyphrase are located in
different parts of a document. A phrase is more
likely to be a keyphrase if it appears in the ab-
stract or introduction of a paper or in the metadata
section of a web page. In fact, features that en-
code how frequently a candidate keyphrase occurs
in various sections of a scientific paper (e.g., in-
troduction, conclusion) (Nguyen and Kan, 2007)
and those that encode the location of a candidate
keyphrase in a web page (e.g., whether it appears
in the title) (Chen et al, 2005; Yih et al, 2006)
have been shown to be useful for the task.
Syntactic features encode the syntactic pat-
terns of a candidate keyphrase. For example, a
candidate keyphrase has been encoded as (1) a
PoS tag sequence, which denotes the sequence of
part-of-speech tag(s) assigned to its word(s); and
(2) a suffix sequence, which is the sequence of
morphological suffixes of its words (Yih et al,
2006; Nguyen and Kan, 2007; Kim and Kan,
2009). However, ablation studies conducted on
web pages (Yih et al, 2006) and scientific articles
2
A tf*idf-based baseline, where candidate keyphrases are
ranked and selected according to tf*idf, has been widely used
by both supervised and unsupervised approaches (Zhang et
al., 2005; Dredze et al, 2008; Paukkeri et al, 2008; Grineva
et al, 2009).
1264
(Kim and Kan, 2009) reveal that syntactic features
are not useful for keyphrase extraction in the pres-
ence of other feature types.
3.2.2.2 External Resource-Based Features
External resource-based features are computed
based on information gathered from resources
other than the training documents, such as lex-
ical knowledge bases (e.g., Wikipedia) or the
Web, with the goal of improving keyphrase extrac-
tion performance by exploiting external knowl-
edge. Below we give an overview of the exter-
nal resource-based features that have proven use-
ful for keyphrase extraction.
Wikipedia-based keyphraseness is computed as
a candidate?s document frequency multiplied by
the ratio of the number of Wikipedia articles where
the candidate appears as a link to the number of
articles where it appears (Medelyan et al, 2009).
This feature is motivated by the observation that
a candidate is likely to be a keyphrase if it occurs
frequently as a link in Wikipedia. Unlike super-
vised keyphraseness, Wikipedia-based keyphrase-
ness can be computed without using documents
annotated with keyphrases and can work even if
there is a mismatch between the training domain
and the test domain.
Yih et al (2006) employ a feature that en-
codes whether a candidate keyphrase appears in
the query log of a search engine, exploiting the ob-
servation that a candidate is potentially important
if it was used as a search query. Terminological
databases have been similarly exploited to encode
the salience of candidate keyphrases in scientific
papers (Lopez and Romary, 2010).
While the aforementioned external resource-
based features attempt to encode how salient a
candidate keyphrase is, Turney (2003) proposes
features that encode the semantic relatedness be-
tween two candidate keyphrases. Noting that can-
didate keyphrases that are not semantically re-
lated to the predicted keyphrases are unlikely to
be keyphrases in technical reports, Turney em-
ploys coherence features to identify such can-
didate keyphrases. Semantic relatedness is en-
coded in the coherence features as two candidate
keyphrases? pointwise mutual information, which
Turney computes by using the Web as a corpus.
3.3 Unsupervised Approaches
Existing unsupervised approaches to keyphrase
extraction can be categorized into four groups.
3.3.1 Graph-Based Ranking
Intuitively, keyphrase extraction is about finding
the important words and phrases from a docu-
ment. Traditionally, the importance of a candi-
date has often been defined in terms of how related
it is to other candidates in the document. Infor-
mally, a candidate is important if it is related to (1)
a large number of candidates and (2) candidates
that are important. Researchers have computed re-
latedness between candidates using co-occurrence
counts (Mihalcea and Tarau, 2004; Matsuo and
Ishizuka, 2004) and semantic relatedness (Grineva
et al, 2009), and represented the relatedness in-
formation collected from a document as a graph
(Mihalcea and Tarau, 2004; Wan and Xiao, 2008a;
Wan and Xiao, 2008b; Bougouin et al, 2013).
The basic idea behind a graph-based approach
is to build a graph from the input document and
rank its nodes according to their importance us-
ing a graph-based ranking method (e.g., Brin and
Page (1998)). Each node of the graph corresponds
to a candidate keyphrase from the document and
an edge connects two related candidates. The
edge weight is proportional to the syntactic and/or
semantic relevance between the connected candi-
dates. For each node, each of its edges is treated
as a ?vote? from the other node connected by the
edge. A node?s score in the graph is defined recur-
sively in terms of the edges it has and the scores of
the neighboring nodes. The top-ranked candidates
from the graph are then selected as keyphrases for
the input document. TextRank (Mihalcea and Ta-
rau, 2004) is one of the most well-known graph-
based approaches to keyphrase extraction.
This instantiation of a graph-based approach
overlooks an important aspect of keyphrase ex-
traction, however. A set of keyphrases for a doc-
ument should ideally cover the main topics dis-
cussed in it, but this instantiation does not guaran-
tee that all the main topics will be represented by
the extracted keyphrases. Despite this weakness, a
graph-based representation of text was adopted by
many approaches that propose different ways of
computing the similarity between two candidates.
3.3.2 Topic-Based Clustering
Another unsupervised approach to keyphrase
extraction involves grouping the candidate
keyphrases in a document into topics, such that
each topic is composed of all and only those
candidate keyphrases that are related to that topic
(Grineva et al, 2009; Liu et al, 2009b; Liu et
1265
al., 2010). There are several motivations behind
this topic-based clustering approach. First, a
keyphrase should ideally be relevant to one or
more main topic(s) discussed in a document
(Liu et al, 2010; Liu et al, 2012). Second, the
extracted keyphrases should be comprehensive
in the sense that they should cover all the main
topics in a document (Liu et al, 2009b; Liu et al,
2010; Liu et al, 2012). Below we examine three
representative systems that adopt this approach.
KeyCluster Liu et al (2009b) adopt a
clustering-based approach (henceforth KeyClus-
ter) that clusters semantically similar candidates
using Wikipedia and co-occurrence-based statis-
tics. The underlying hypothesis is that each of
these clusters corresponds to a topic covered in
the document, and selecting the candidates close
to the centroid of each cluster as keyphrases
ensures that the resulting set of keyphrases covers
all the topics of the document.
While empirical results show that KeyCluster
performs better than both TextRank and Hulth?s
(2003) supervised system, KeyCluster has a poten-
tial drawback: by extracting keyphrases from each
topic cluster, it essentially gives each topic equal
importance. In practice, however, there could
be topics that are not important and these topics
should not have keyphrase(s) representing them.
Topical PageRank (TPR) Liu et al (2010) pro-
pose TPR, an approach that overcomes the afore-
mentioned weakness of KeyCluster. It runs Tex-
tRank multiple times for a document, once for
each of its topics induced by a Latent Dirichlet Al-
location (Blei et al, 2003). By running TextRank
once for each topic, TPR ensures that the extracted
keyphrases cover the main topics of the document.
The final score of a candidate is computed as the
sum of its scores for each of the topics, weighted
by the probability of that topic in that document.
Hence, unlike KeyCluster, candidates belonging to
a less probable topic are given less importance.
TPR performs significantly better than both
tf*idf and TextRank on the DUC-2001 and Inspec
datasets. TPR?s superior performance strength-
ens the hypothesis of using topic clustering for
keyphrase extraction. However, though TPR is
conceptually better than KeyCluster, Liu et al did
not compare TPR against KeyCluster.
CommunityCluster Grineva et al (2009) pro-
pose CommunityCluster, a variant of the topic
clustering approach to keyphrase extraction. Like
TPR, CommunityCluster gives more weight to
more important topics, but unlike TPR, it extracts
all candidate keyphrases from an important topic,
assuming that a candidate that receives little focus
in the text should still be extracted as a keyphrase
as long as it is related to an important topic. Com-
munityCluster yields much better recall (without
losing precision) than extractors such as tf*idf,
TextRank, and the Yahoo! term extractor.
3.3.3 Simultaneous Learning
Since keyphrases represent a dense summary of a
document, researchers hypothesized that text sum-
marization and keyphrase extraction can poten-
tially benefit from each other if these tasks are per-
formed simultaneously. Zha (2002) proposes the
first graph-based approach for simultaneous sum-
marization and keyphrase extraction, motivated by
a key observation: a sentence is important if it con-
tains important words, and important words ap-
pear in important sentences. Wan et al (2007) ex-
tend Zha?s work by adding two assumptions: (1)
an important sentence is connected to other im-
portant sentences, and (2) an important word is
linked to other important words, a TextRank-like
assumption. Based on these assumptions, Wan et
al. (2007) build three graphs to capture the asso-
ciation between the sentences (S) and the words
(W) in an input document, namely, a S?S graph,
a bipartite S?W graph, and a W?W graph. The
weight of an edge connecting two sentence nodes
in a S?S graph corresponds to their content simi-
larity. An edge weight in a S?W graph denotes the
word?s importance in the sentence it appears. Fi-
nally, an edge weight in a W?W graph denotes the
co-occurrence or knowledge-based similarity be-
tween the two connected words. Once the graphs
are constructed for an input document, an itera-
tive reinforcement algorithm is applied to assign
scores to each sentence and word. The top-scored
words are used to form keyphrases.
The main advantage of this approach is that it
combines the strengths of both Zha?s approach
(i.e., bipartite S?W graphs) and TextRank (i.e., W?
W graphs) and performs better than both of them.
However, it has a weakness: like TextRank, it does
not ensure that the extracted keyphrases will cover
all the main topics. To address this problem, one
can employ a topic clustering algorithm on the W?
W graph to produce the topic clusters, and then en-
sure that keyphrases are chosen from every main
topic cluster.
1266
3.3.4 Language Modeling
Many existing approaches have a separate, heuris-
tic module for extracting candidate keyphrases
prior to keyphrase ranking/extraction. In contrast,
Tomokiyo and Hurst (2003) propose an approach
(henceforth LMA) that combines these two steps.
LMA scores a candidate keyphrase based on
two features, namely, phraseness (i.e., the ex-
tent to which a word sequence can be treated as
a phrase) and informativeness (i.e., the extent to
which a word sequence captures the central idea of
the document it appears in). Intuitively, a phrase
that has high scores for phraseness and informa-
tiveness is likely to be a keyphrase. These feature
values are estimated using language models (LMs)
trained on a foreground corpus and a background
corpus. The foreground corpus is composed of
the set of documents from which keyphrases are
to be extracted. The background corpus is a large
corpus that encodes general knowledge about the
world (e.g., the Web). A unigram LM and an n-
gram LM are constructed for each of these two
corpora. Phraseness, defined using the foreground
LM, is calculated as the loss of information in-
curred as a result of assuming a unigram LM (i.e.,
conditional independence among the words of the
phrase) instead of an n-gram LM (i.e., the phrase
is drawn from an n-gram LM). Informativeness is
computed as the loss that results because of the
assumption that the candidate is sampled from the
background LM rather than the foreground LM.
The loss values are computed using Kullback-
Leibler divergence. Candidates are ranked accord-
ing to the sum of these two feature values.
In sum, LMA uses a language model rather than
heuristics to identify phrases, and relies on the lan-
guage model trained on the background corpus to
determine how ?unique? a candidate keyphrase is
to the domain represented by the foreground cor-
pus. The more unique it is to the foreground?s do-
main, the more likely it is a keyphrase for that do-
main. While the use of language models to iden-
tify phrases cannot be considered a major strength
of this approach (because heuristics can identify
phrases fairly reliably), the use of a background
corpus to identify candidates that are unique to the
foreground?s domain is a unique aspect of this ap-
proach. We believe that this idea deserves further
investigation, as it would allow us to discover a
keyphrase that is unique to the foreground?s do-
main but may have a low tf*idf value.
4 Evaluation
In this section, we describe metrics for evaluating
keyphrase extraction systems as well as state-of-
the-art results on commonly-used datasets.
4.1 Evaluation Metrics
Designing evaluation metrics for keyphrase ex-
traction is by no means an easy task. To score
the output of a keyphrase extraction system, the
typical approach, which is also adopted by the
SemEval-2010 shared task on keyphrase extrac-
tion, is (1) to create a mapping between the
keyphrases in the gold standard and those in the
system output using exact match, and then (2)
score the output using evaluation metrics such as
precision (P), recall (R), and F-score (F).
Conceivably, exact match is an overly strict con-
dition, considering a predicted keyphrase incor-
rect even if it is a variant of a gold keyphrase.
For instance, given the gold keyphrase ?neural
network?, exact match will consider a predicted
phrase incorrect even if it is an expanded version
of the gold keyphrase (?artificial neural network?)
or one of its morphological (?neural networks?) or
lexical (?neural net?) variants. While morphologi-
cal variations can be handled using a stemmer (El-
Beltagy and Rafea, 2009), other variations may
not be handled easily and reliably.
Human evaluation has been suggested as a pos-
sibility (Matsuo and Ishizuka, 2004), but it is time-
consuming and expensive. For this reason, re-
searchers have experimented with two types of
automatic evaluation metrics. The first type of
metrics addresses the problem with exact match.
These metrics reward a partial match between a
predicted keyphrase and a gold keyphrase (i.e.,
overlapping n-grams) and are commonly used
in machine translation (MT) and summarization
evaluations. They include BLEU, METEOR, NIST,
and ROUGE. Nevertheless, experiments show that
these MT metrics only offer a partial solution to
problem with exact match: they can only detect a
subset of the near-misses (Kim et al, 2010a).
The second type of metrics focuses on how a
system ranks its predictions. Given that two sys-
tems A and B have the same number of correct
predictions, binary preference measure (Bpref)
and mean reciprocal rank (MRR) (Liu et al, 2010)
will award more credit to A than to B if the ranks
of the correct predictions in A?s output are higher
than those in B?s output. R-precision (R
p
) is an
1267
IR metric that focuses on ranking: given a docu-
ment with n gold keyphrases, it computes the pre-
cision of a system over its n highest-ranked can-
didates (Zesch and Gurevych, 2009). The motiva-
tion behind the design of R
p
is simple: a system
will achieve a perfect R
p
value if it ranks all the
keyphrases above the non-keyphrases.
4.2 The State of the Art
Table 2 lists the best scores on some popular evalu-
ation datasets and the corresponding systems. For
example, the best F-scores on the Inspec test set,
the DUC-2001 dataset, and the SemEval-2010 test
set are 45.7, 31.7, and 27.5, respectively.
3
Two points deserve mention. First, F-scores de-
crease as document length increases. These re-
sults are consistent with the observation we made
in Section 2 that it is more difficult to extract
keyphrases correctly from longer documents. Sec-
ond, recent unsupervised approaches have rivaled
their supervised counterparts in performance (Mi-
halcea and Tarau, 2004; El-Beltagy and Rafea,
2009; Liu et al, 2009b). For example, KP-Miner
(El-Beltagy and Rafea, 2010), an unsupervised
system, ranked third in the SemEval-2010 shared
task with an F-score of 25.2, which is comparable
to the best supervised system scoring 27.5.
5 Analysis
With the goal of providing directions for future
work, we identify the errors commonly made by
state-of-the-art keyphrase extractors below.
5.1 Error Analysis
Although a few researchers have presented a sam-
ple of their systems? output and the corresponding
gold keyphrases to show the differences between
them (Witten et al, 1999; Nguyen and Kan, 2007;
Medelyan et al, 2009), a systematic analysis of
the major types of errors made by state-of-the-art
keyphrase extraction systems is missing.
To fill this gap, we ran four keyphrase extrac-
tion systems on four commonly-used datasets of
varying sources, including Inspec abstracts (Hulth,
2003), DUC-2001 news articles (Over, 2001), sci-
entific papers (Kim et al, 2010b), and meeting
transcripts (Liu et al, 2009a). Specifically, we ran-
domly selected 25 documents from each of these
3
A more detailed analysis of the results of the SemEval-
2010 shared task and the approaches adopted by the partici-
pating systems can be found in Kim et al (2013).
Dataset
Approach and System
[Supervised?]
Score
P R F
Abstracts
(Inspec)
Topic clustering
(Liu et al, 2009b) [?]
35.0 66.0 45.7
Blogs
Topic community detection
(Grineva et al, 2009) [?]
35.1 61.5 44.7
News
(DUC
-2001)
Graph-based ranking
for extended neighborhood
(Wan and Xiao, 2008b) [?]
28.8 35.4 31.7
Papers
(SemEval
-2010)
Statistical, semantic, and
distributional features
(Lopez and Romary, 2010) [X]
27.2 27.8 27.5
Table 2: Best scores achieved on various datasets.
four datasets and manually analyzed the output of
the four systems, including tf*idf, the most fre-
quently used baseline, as well as three state-of-the-
art keyphrase extractors, of which two are unsu-
pervised (Wan and Xiao, 2008b; Liu et al, 2009b)
and one is supervised (Medelyan et al, 2009).
Our analysis reveals that the errors fall into four
major types, each of which contributes signifi-
cantly to the overall errors made by the four sys-
tems, despite the fact that the contribution of each
of these error types varies from system to system.
Moreover, we do not observe any significant dif-
ference between the types of errors made by the
four systems other than the fact that the super-
vised system has the expected tendency to predict
keyphrases seen in the training data. Below we
describe these four major types of errors.
Overgeneration errors are a major type of pre-
cision error, contributing to 28?37% of the overall
error. Overgeneration errors occur when a system
correctly predicts a candidate as a keyphrase be-
cause it contains a word that appears frequently in
the associated document, but at the same time er-
roneously outputs other candidates as keyphrases
because they contain the same word. Recall that
for many systems, it is not easy to reject a non-
keyphrase containing a word with a high term fre-
quency: many unsupervised systems score a can-
didate by summing the score of each of its compo-
nent words, and many supervised systems use un-
igrams as features to represent a candidate. To be
more concrete, consider the news article on athlete
Ben Johnson in Figure 1, where the keyphrases are
boldfaced. As we can see, the word Olympic(s)
has a significant presence in the document. Con-
sequently, many systems not only correctly predict
Olympics as a keyphrase, but also erroneously pre-
dict Olympic movement as a keyphrase, yielding
overgeneration errors.
Infrequency errors are a major type of re-
1268
Canadian Ben Johnson left the Olympics today ?in a
complete state of shock,? accused of cheating with drugs
in the world?s fastest 100-meter dash and stripped of
his gold medal. The prize went to American Carl
Lewis. Many athletes accepted the accusation that John-
son used a muscle-building but dangerous and illegal an-
abolic steroid called stanozolol as confirmation of what
they said they know has been going on in track and field.
Two tests of Johnson?s urine sample proved positive and
his denials of drug use were rejected today. ?This is
a blow for the Olympic Games and the Olympic move-
ment,? said International Olympic Committee President
Juan Antonio Samaranch.
Figure 1: A news article on Ben Johnson from the
DUC-2001 dataset. The keyphrases are boldfaced.
call error contributing to 24?27% of the overall
error. Infrequency errors occur when a system
fails to identify a keyphrase owing to its infre-
quent presence in the associated document (Liu
et al, 2011). Handling infrequency errors is a
challenge because state-of-the-art keyphrase ex-
tractors rarely predict candidates that appear only
once or twice in a document. In the Ben Johnson
example, many keyphrase extractors fail to iden-
tify 100-meter dash and gold medal as keyphrases,
resulting in infrequency errors.
Redundancy errors are a type of precision er-
ror contributing to 8?12% of the overall error. Re-
dundancy errors occur when a system correctly
identifies a candidate as a keyphrase, but at the
same time outputs a semantically equivalent can-
didate (e.g., its alias) as a keyphrase. This type
of error can be attributed to a system?s failure
to determine that two candidates are semantically
equivalent. Nevertheless, some researchers may
argue that a system should not be penalized for re-
dundancy errors because the extracted candidates
are in fact keyphrases. In our example, Olympics
and Olympic games refer to the same concept, so
a system that predicts both of them as keyphrases
commits a redundancy error.
Evaluation errors are a type of recall error con-
tributing to 7?10% of the overall error. An evalu-
ation error occurs when a system outputs a can-
didate that is semantically equivalent to a gold
keyphrase, but is considered erroneous by a scor-
ing program because of its failure to recognize
that the predicted phrase and the corresponding
gold keyphrase are semantically equivalent. In
other words, an evaluation error is not an error
made by a keyphrase extractor, but an error due
to the naivety of a scoring program. In our exam-
ple, while Olympics and Olympic games refer to
the same concept, only the former is annotated as
keyphrase. Hence, an evaluation error occurs if a
system predicts Olympic games but not Olympics
as a keyphrase and the scoring program fails to
identify them as semantically equivalent.
5.2 Recommendations
We recommend that background knowledge be
extracted from external lexical databases (e.g.,
YAGO2 (Suchanek et al, 2007), Freebase (Bol-
lacker et al, 2008), BabelNet (Navigli and
Ponzetto, 2012)) to address the four types of er-
rors discussed above.
First, we discuss how redundancy errors could
be addressed by using the background knowledge
extracted from external databases. Note that if we
can identify semantically equivalent candidates,
then we can reduce redundancy errors. The ques-
tion, then, is: can background knowledge be used
to help us identify semantically equivalent candi-
dates? To answer this question, note that Freebase,
for instance, has over 40 million topics (i.e., real-
world entities such as people, places, and things)
from over 70 domains (e.g., music, business, ed-
ucation). Hence, before a system outputs a set of
candidates as keyphrases, it can use Freebase to
determine whether any of them is mapped to the
same Freebase topic. Referring back to our run-
ning example, both Olympics and Olympic games
are mapped to a Freebase topic called Olympic
games. Based on this information, a keyphrase ex-
tractor can determine that the two candidates are
aliases and should output only one of them, thus
preventing a redundancy error.
Next, we discuss how infrequency errors
could be addressed using background knowledge.
A natural way to handle this problem would be
to make an infrequent keyphrase frequent. To ac-
complish this, we suggest exploiting an influen-
tial idea in the keyphrase extraction literature: the
importance of a candidate is defined in terms of
how related it is to other candidates in the text (see
Section 3.3.1). In other words, if we could relate
an infrequent keyphrase to other candidates in the
text, we could boost its importance.
We believe that this could be accomplished us-
ing background knowledge. The idea is to boost
the importance of infrequent keyphrases using
their frequent counterparts. Consider again our
running example. All four systems have managed
to identify Ben Johnson as a keyphrase due to its
1269
significant presence. Hence, we can boost the im-
portance of 100-meter dash and gold medal if we
can relate them to Ben Johnson.
To do so, note that Freebase maps a candi-
date to one or more pre-defined topics, each of
which is associated with one or more types. Types
are similar to entity classes. For instance, the
candidate Ben Johnson is mapped to a Freebase
topic with the same name, which is associated
with Freebase types such as Person, Athlete, and
Olympic athlete. Types are defined for a specific
domain in Freebase. For instance, Person, Ath-
lete, and Olympic athlete are defined in the People,
Sports, and Olympics domains, respectively. Next,
consider the two infrequent candidates, 100-meter
dash and gold medal. 100-meter dash is mapped
to the topic Sprint of type Sports in the Sports do-
main, whereas gold medal is mapped to a topic
with the same name of type Olympic medal in the
Olympics domain. Consequently, we can relate
100-meter dash to Ben Johnson via the Sports do-
main (i.e., they belong to different types under the
same domain). Additionally, gold medal can be
related to Ben Johnson via the Olympics domain.
As discussed before, the relationship between
two candidates is traditionally established using
co-occurrence information. However, using co-
occurrence windows has its shortcomings. First,
an ad-hoc window size cannot capture related can-
didates that are not inside the window. So it
is difficult to predict 100-meter dash and gold
medal as keyphrases: they are more than 10 tokens
away from frequent words such as Johnson and
Olympics. Second, the candidates inside a window
are all assumed to be related to each other, but it is
apparently an overly simplistic assumption. There
have been a few attempts to design Wikipedia-
based relatedness measures, with promising ini-
tial results (Grineva et al, 2009; Liu et al, 2009b;
Medelyan et al, 2009).
4
Overgeneration errors could similarly be ad-
dressed using background knowledge. Recall that
Olympic movement is not a keyphrase in our ex-
ample although it includes an important word (i.e.,
Olympic). Freebase maps Olympic movement to
a topic with the same name, which is associated
with a type called Musical Recording in the Mu-
sic domain. However, it does not map Olympic
4
Note that it may be difficult to employ our recommen-
dations to address infrequency errors in informal text with
uncorrelated topics because the keyphrases it contains may
not be related to each other (see Section 2).
movement to any topic in the Olympics domain.
The absence of such a mapping in the Olympics
domain could be used by a keyphrase extractor as
a supporting evidence against predicting Olympic
movement as a keyphrase.
Finally, as mentioned before, evaluation errors
should not be considered errors made by a sys-
tem. Nevertheless, they reveal a problem with the
way keyphrase extractors are currently evaluated.
To address this problem, one possibility is to con-
duct human evaluations. Cheaper alternatives in-
clude having human annotators identify semanti-
cally equivalent keyphrases during manual label-
ing, and designing scoring programs that can au-
tomatically identify such semantic equivalences.
6 Conclusion and Future Directions
We have presented a survey of the state of the art
in automatic keyphrase extraction. While unsu-
pervised approaches have started to rival their su-
pervised counterparts in performance, the task is
far from being solved, as reflected by the fairly
poor state-of-the-art results on various commonly-
used evaluation datasets. Our analysis revealed
that there are at least three major challenges ahead.
1. Incorporating background knowledge.
While much recent work has focused on algo-
rithmic development, keyphrase extractors need
to have a deeper ?understanding? of a document
in order to reach the next level of performance.
Such an understanding can be facilitated by the
incorporation of background knowledge.
2. Handling long documents. While it may be
possible to design better algorithms to handle the
large number of candidates in long documents, we
believe that employing sophisticated features, es-
pecially those that encode background knowledge,
will enable keyphrases and non-keyphrases to be
distinguished more easily even in the presence of
a large number of candidates.
3. Improving evaluation schemes. To more ac-
curately measure the performance of keyphrase
extractors, they should not be penalized for evalu-
ation errors. We have suggested several possibili-
ties as to how this problem can be addressed.
Acknowledgments
We thank the anonymous reviewers for their de-
tailed and insightful comments on earlier drafts of
this paper. This work was supported in part by
NSF Grants IIS-1147644 and IIS-1219142.
1270
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In
Proceedings of the 13th Biennial Conference of the
Canadian Society on Computational Studies of In-
telligence, pages 40?52.
G?abor Berend. 2011. Opinion expression mining by
exploiting keyphrase extraction. In Proceedings of
the 5th International Joint Conference on Natural
Language Processing, pages 1162?1170.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Ma-
chine Learning Research, 3:993?1022.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247?1250.
Adrien Bougouin, Florian Boudin, and B?eatrice Daille.
2013. Topicrank: Graph-based topic ranking for
keyphrase extraction. In Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing, pages 543?551.
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
Computer Networks, 30(1?7):107?117.
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase ex-
traction for web pages. In Proceedings of the 14th
ACM International Conference on Information and
Knowledge Management, pages 277?278.
Zhuoye Ding, Qi Zhang, and Xuanjing Huang. 2011.
Keyphrase extraction from online news using binary
integer programming. In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing, pages 165?173.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th International Conference on Intelligent User
Interfaces, pages 199?206.
Samhaa R. El-Beltagy and Ahmed A. Rafea. 2009.
KP-Miner: A keyphrase extraction system for En-
glish and Arabic documents. Information Systems,
34(1):132?144.
Samhaa R. El-Beltagy and Ahmed Rafea. 2010. KP-
Miner: Participation in SemEval-2. In Proceedings
of the 5th International Workshop on Semantic Eval-
uation, pages 190?193.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of 16th International Joint Conference on Arti-
ficial Intelligence, pages 668?673.
Maria Grineva, Maxim Grinev, and Dmitry Lizorkin.
2009. Extracting key terms from noisy and multi-
theme documents. In Proceedings of the 18th In-
ternational Conference on World Wide Web, pages
661?670.
Carl Gutwin, Gordon Paynter, Ian Witten, Craig Nevill-
Manning, and Eibe Frank. 1999. Improving brows-
ing in digital libraries with keyphrase indexes. De-
cision Support Systems, 27:81?104.
Khaled M. Hammouda, Diego N. Matute, and Mo-
hamed S. Kamel. 2005. CorePhrase: Keyphrase ex-
traction for document clustering. In Proceedings of
the 4th International Conference on Machine Learn-
ing and Data Mining in Pattern Recognition, pages
265?274.
Kazi Saidul Hasan and Vincent Ng. 2010. Conun-
drums in unsupervised keyphrase extraction: Mak-
ing sense of the state-of-the-art. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 365?373.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275?284.
Anette Hulth and Be?ata B. Megyesi. 2006. A study
on automatically extracted keywords in text catego-
rization. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 537?544.
Anette Hulth, Jussi Karlgren, Anna Jonsson, Henrik
Bostr?om, and Lars Asker. 2001. Automatic key-
word extraction using domain knowledge. In Pro-
ceedings of the 2nd International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, pages 472?482.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 Conference on Empirical Meth-
ods in Natural Language Processing, pages 216?
223.
Anette Hulth. 2004. Enhancing linguistically ori-
ented automatic keyword extraction. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: Short Papers, pages 17?
20.
Xin Jiang, Yunhua Hu, and Hang Li. 2009. A rank-
ing approach to keyphrase extraction. In Proceed-
ings of the 32nd International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 756?757.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence, pages 1608?1609.
1271
Su Nam Kim and Timothy Baldwin. 2012. Extracting
keywords from multi-party live chats. In Proceed-
ings of the 26th Pacific Asia Conference on Lan-
guage, Information, and Computation, pages 199?
208.
Su Nam Kim and Min-Yen Kan. 2009. Re-examining
automatic keyphrase extraction approaches in scien-
tific articles. In Proceedings of the ACL-IJCNLP
Workshop on Multiword Expressions, pages 9?16.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 572?580.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2010b. SemEval-2010 Task 5:
Automatic keyphrase extraction from scientific arti-
cles. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 21?26.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and
Timothy Baldwin. 2013. Automatic keyphrase
extraction from scientific articles. Language Re-
sources and Evaluation, 47(3):723?742.
Niraj Kumar and Kannan Srinathan. 2008. Automatic
keyphrase extraction from scientific documents us-
ing n-gram filtration technique. In Proceedings of
the 8th ACM Symposium on Document Engineering,
pages 199?208.
Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009a. Unsupervised approaches for automatic key-
word extraction using meeting transcripts. In Pro-
ceedings of Human Language Technologies: The
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 620?628.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong
Sun. 2009b. Clustering to find exemplar terms for
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 257?266.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 366?376.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning, pages 135?144.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012.
Topical word trigger model for keyphrase extraction.
In Proceedings of the 24th International Conference
on Computational Linguistics, pages 1715?1730.
Patrice Lopez and Laurent Romary. 2010. HUMB:
Automatic key term extraction from scientific arti-
cles in GROBID. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
248?251.
Yutaka Matsuo and Mitsuru Ishizuka. 2004. Key-
word extraction from a single document using word
co-occurrence statistical information. International
Journal on Artificial Intelligence Tools, 13.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1318?1327.
Rada Mihalcea and Paul Tarau. 2004. TextRank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing, pages 404?411.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
BabelNet: The automatic construction, evaluation
and application of a wide-coverage multilingual se-
mantic network. Artificial Intelligence, 193:217?
250.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmenta-
tion for index term identification and keyphrase ex-
traction. In Proceedings of the 24th International
Conference on Computational Linguistics, pages
2077?2092.
Thuy Dung Nguyen and Min-Yen Kan. 2007.
Keyphrase extraction in scientific publications. In
Proceedings of the International Conference on
Asian Digital Libraries, pages 317?326.
Chau Q. Nguyen and Tuoi T. Phan. 2009. An
ontology-based approach for key phrase extraction.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the Association for Computa-
tional Linguistics and the 4th International Joint
Conference on Natural Language Processing: Short
Papers, pages 181?184.
Paul Over. 2001. Introduction to DUC-2001: An in-
trinsic evaluation of generic news text summariza-
tion systems. In Proceedings of the 2001 Document
Understanding Conference.
Mari-Sanna Paukkeri, Ilari T. Nieminen, Matti P?oll?a,
and Timo Honkela. 2008. A language-independent
approach to keyphrase extraction and evaluation. In
Proceedings of the 22nd International Conference
on Computational Linguistics: Companion Volume:
Posters, pages 83?86.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation Processing and Management, 24(5):513?
523.
1272
Min Song, Il-Yeol Song, and Xiaohua Hu. 2003.
KPSpotter: A flexible information gain-based
keyphrase extraction system. In Proceedings of the
5th ACM International Workshop on Web Informa-
tion and Data Management, pages 50?53.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. In Proceedings of the 16th International
World Wide Web Conference, pages 697?706.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions, pages 33?40.
Peter Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2:303?336.
Peter Turney. 2003. Coherent keyphrase extraction
via web mining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelligence,
pages 434?439.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labRank: Towards a collaborative approach to
single-document keyphrase extraction. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics, pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 855?860.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559.
Chen Wang and Sujian Li. 2011. CoRankBayes:
Bayesian learning to rank under the co-training
framework and its application in keyphrase extrac-
tion. In Proceedings of the 20th ACM International
Conference on Information and Knowledge Man-
agement, pages 2241?2244.
Ian H. Witten, Gordon W. Paynter, Eibe Frank, Carl
Gutwin, and Craig G. Nevill-Manning. 1999. KEA:
Practical automatic keyphrase extraction. In Pro-
ceedings of the 4th ACM Conference on Digital Li-
braries, pages 254?255.
Yi-Fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,
and Xin Chen. 2005. Domain-specific keyphrase
extraction. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 283?284.
Wen-Tau Yih, Joshua Goodman, and Vitor R. Carvalho.
2006. Finding advertising keywords on web pages.
In Proceedings of the 15th International Conference
on World Wide Web, pages 213?222.
Wei You, Dominique Fontaine, and Jean-Paul Barth`es.
2009. Automatic keyphrase extraction with a
refined candidate set. In Proceedings of the
IEEE/WIC/ACM International Joint Conference on
Web Intelligence and Intelligent Agent Technology,
pages 576?579.
Torsten Zesch and Iryna Gurevych. 2009. Approxi-
mate matching for evaluating keyphrase extraction.
In Proceedings of the International Conference on
Recent Advances in Natural Language Processing
2009, pages 484?489.
Hongyuan Zha. 2002. Generic summarization and
keyphrase extraction using mutual reinforcement
principle and sentence clustering. In Proceedings
of 25th Annual International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, pages 113?120.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2004. World Wide Web site summariza-
tion. Web Intelligence and Agent Systems, 2:39?53.
Yongzheng Zhang, Nur Zincir-Heywood, and Evange-
los Milios. 2005. Narrative text classification for
automatic key phrase extraction in web document
corpora. In Proceedings of the 7th ACM Interna-
tional Workshop on Web Information and Data Man-
agement, pages 51?58.
Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn
Achanauparp, Ee-Peng Lim, and Xiaoming Li.
2011. Topical keyphrase extraction from Twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 379?388.
1273
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 124?132,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Frame Semantics for Stance Classification
Kazi Saidul Hasan and Vincent Ng
Human Language Technology Research Institute
University of Texas at Dallas
Richardson, TX 75083-0688
{saidul,vince}@hlt.utdallas.edu
Abstract
Determining the stance expressed by an
author from a post written for a two-sided
debate in an online debate forum is a
relatively new problem in opinion min-
ing. We extend a state-of-the-art learning-
based approach to debate stance classifica-
tion by (1) inducing lexico-syntactic pat-
terns based on syntactic dependencies and
semantic frames that aim to capture the
meaning of a sentence and provide a gen-
eralized representation of it; and (2) im-
proving the classification of a test post via
a novel way of exploiting the information
in other test posts with the same stance.
Empirical results on four datasets demon-
strate the effectiveness of our extensions.
1 Introduction
Given a post written for a two-sided topic in an
online debate forum (e.g., ?Should abortion be al-
lowed??), the task of debate stance classification
involves determining which of the two sides (i.e.,
for or against) its author is taking. For example, a
stance classification system should determine that
the author of the following post is anti-abortion.
Post 1: Abortion has been legal for decades and no
one seems to have a problem with it. That?s ridicu-
lous! There are millions of people in the world
who would love to have children but can?t.
Previous approaches to debate stance classifica-
tion have focused on three debate settings, namely
congressional floor debates (Thomas et al, 2006;
Bansal et al, 2008; Balahur et al, 2009; Yesse-
nalina et al, 2010; Burfoot et al, 2011), company-
internal discussions (Murakami and Raymond,
2010), and online social, political, and ideologi-
cal debates in public forums (Agrawal et al, 2003;
Somasundaran and Wiebe, 2010; Wang and Rose?,
2010; Biran and Rambow, 2011; Hasan and Ng,
2012). As Walker et al (2012) point out, debates
in public forums differ from congressional debates
and company-internal discussions in terms of lan-
guage use. Specifically, online debaters use color-
ful and emotional language to express their points,
which may involve sarcasm, insults, and question-
ing another debater?s assumptions and evidence.
These properties can potentially make stance clas-
sification of online debates more challenging than
that of the other two types of debates.
Our goal in this paper is to improve the state
of the art in stance classification of online de-
bates, focusing in particular on ideological de-
bates. Specifically, we present two extensions,
one linguistic and the other extra-linguistic, to
the state-of-the-art supervised learning approach
to this task proposed by Anand et al (2011). In our
linguistic extension, we induce patterns from each
sentence in the training set using syntactic depen-
dencies and semantic frames that aim to capture
the meaning of a sentence and provide a general-
ized representation of it. Note that while Anand et
al.?s lexico-syntactic approach aims to generalize
from a sentence using syntactic dependencies, we
aim to generalize using semantic frames. As we
will see in Section 4, not only is there no guaran-
tee that syntactic dependencies can retain or suf-
ficiently capture the meaning of a sentence during
the generalization process, it is in fact harder to
generalize from syntactic dependencies than from
semantic frames. In our extra-linguistic extension,
we improve the classification of a test post via a
novel way of exploiting the information in other
test posts with the same stance.
We evaluate our approach to stance classifica-
tion of ideological debates on datasets collected
for four domains from online debate forums. Ex-
perimental results demonstrate the effectiveness of
our approach: it outperforms an improved version
of Anand et al?s approach by 2.6?7.0 accuracy
points on the four domains.
124
Number of % of ?for?
Domain posts posts
ABO 1741 54.9
GAY 1376 63.4
OBA 985 53.9
MAR 626 69.5
Table 1: Statistics of the four datasets.
The rest of the paper is organized as follows.
We first present our datasets in Section 2. Sec-
tion 3 describes our two learning-based baseline
systems for stance classification. Sections 4 and 5
discuss our two extensions. Finally, we show eval-
uation results in Section 6 and present conclusions
in Section 7.
2 Datasets
For our experiments, we collect debate posts
from four popular domains, Abortion (ABO),
Gay Rights (GAY), Obama (OBA), and Marijuana
(MAR). Each post should receive one of two do-
main labels, for or against, depending on whether
the author of the post supports or opposes abor-
tion, gay rights, Obama, or the legalization of mar-
ijuana. To see how we obtain these domain labels,
let us first describe the data collection process in
more detail.
We collect our debate posts for the four domains
from an online debate forum1. In each domain,
there are several two-sided debates. Each debate
has a subject (e.g., ?Abortion should be banned?)
for which a number of posts were written by dif-
ferent authors. Each post is manually tagged with
its author?s stance (i.e., yes or no) on the debate
subject. Since the label of each post represents the
subject stance but not the domain stance, we need
to automatically convert the former to the latter.
For example, for the subject ?Abortion should be
banned?, the subject stance yes implies that the au-
thor opposes abortion, and hence the domain label
for the corresponding label should be against.
We construct one dataset for each domain.
Statistics of these datasets are shown in Table 1.
3 Baseline Systems
We employ as baselines two stance classification
systems, Anand et al?s (2011) approach and an en-
hanced version of it, as described below.
Our first baseline, Anand et al?s approach, is
a supervised method that trains a stance classifier
1http://www.createdebate.com/
for determining whether the stance expressed in
a debate post is for or against. Hence, we cre-
ate one training instance from each post in the
training set, using the stance it expresses as its
class label. Following Anand et al, we repre-
sent a training instance using five types of fea-
tures: n-grams, document statistics, punctuations,
syntactic dependencies, and, if applicable, the set
of features computed for the immediately preced-
ing post in its thread. Their n-gram features in-
clude both the unigrams and bigrams in a post,
as well as its first unigram, first bigram, and first
trigram. The features based on document statis-
tics include the post length, the number of words
per sentence, the percentage of words with more
than six letters, and the percentage of words as
pronouns and sentiment words. The punctuation
features are composed of the repeated punctuation
symbols in a post. The dependency-based features
have three variants. In the first variant, the pair
of arguments involved in each dependency rela-
tion extracted by a dependency parser is used as a
feature. The second variant is the same as the first
except that the head (i.e., the first argument in a re-
lation) is replaced by its part-of-speech (POS) tag.
The features in the third variant, the topic-opinion
features, are created by replacing each feature
from the first two types that contains a sentiment
word with the corresponding polarity label (i.e.,
+ or ?). For instance, given the sentence ?John
hates guns?, the topic-opinion features John? and
guns? are generated, since ?hate? has a negative
polarity and it is connected to ?John? and ?guns?
via the nsubj and dobj relations, respectively. In
our implementation, we train the stance classifier
using SVMlight (Joachims, 1999). After training,
we can apply the stance classifier to classify the
test instances, which are generated in the same
way as the training instances.
Related work on stance classification of con-
gressional debates has found that enforcing author
constraints (ACs) can improve classification per-
formance (e.g., Thomas et al (2006), Burfoot et al
(2011), Lu et al (2012)). ACs are a type of inter-
post constraints that specify that two posts written
by the same author for the same debate domain
should have the same stance. We hypothesize that
ACs could similarly be used to improve stance
classification of ideological debates, and therefore
propose a second baseline where we enhance the
first baseline with ACs. Enforcing ACs is simple.
125
We first use the learned stance classifier to classify
the test posts as in the first baseline, and then post-
process the labels of the test posts. Specifically,
we sum up the confidence values2 assigned to the
set of test posts written by the same author for the
same debate domain. If the sum is positive, then
we label all the posts in this set as for; otherwise
we label them as against.
4 Semantic Generalization
Our first extension to Anand et al?s (2011) ap-
proach involves semantic generalization.
To motivate this extension, let us take a closer
look at Anand et al?s attempt to generalize using
syntactic dependencies. Note that any approach
that aims to generalize using syntactic dependen-
cies suffers from several weaknesses. First, the
semantic relationship between the pair of lexical
items involved in each of these features is not en-
coded. This means that the resulting features do
not adequately capture the meaning of the under-
lying sentence. Second, replacing a word with
its POS tag is a syntactic, not semantic, gener-
alization, and doing so further abstracts the re-
sulting feature from the meaning of the under-
lying sentence. Above all, while the resulting
features are intended to improve generalizations,
they can provide very limited generalizations. To
see why, consider two semantically similar sen-
tences ?I hate arrogant people? and ?I dislike ar-
rogant people?. Ideally, any features that intend to
provide a generalized representation of these sen-
tences should be able to encode the fact that they
are semantically similar. However, Anand et al?s
features would fail to do so because they cannot
capture the fact that ?hate? and ?dislike? are se-
mantically similar.
In the rest of this section we describe how we
generate a semantic generalization of a sentence
to capture its meaning. Our approach to seman-
tic generalization involves (1) inducing from the
training data a set of patterns that aim to provide
a semantic generalization of the sentences in the
training posts and (2) using them in combination
with the baseline systems to classify a test post.
Below we describe these two steps in detail.
4.1 Step 1: Pattern Induction
This step is composed of two sub-steps.
2We use as the confidence value the signed distance of the
associated test point from the SVM hyperplane.
4.1.1 Sub-step 1: Topic Extraction
For each domain, we extract a list of topics. We
define a topic as a word sequence that (1) starts
with zero or more adjectives and ends with one or
more nouns and (2) appears in at least five posts
from the domain. Using this method, for example,
we can extract ?abortion?, ?partial-birth abortion?,
?birth control?, etc., as the topics for Abortion.
4.1.2 Sub-step 2: Pattern Creation
Given a sentence, we create patterns to capture its
information using syntactic dependencies and se-
mantic frames.3 These patterns can be divided into
three types, as described below. For ease of expo-
sition, we will use the two (semantically equiva-
lent) sentences below as our running examples and
see what patterns are created from them.
(1) Some people hate guns.
(2) Some people do not like guns.
Subject-Frame-Object (SFO) patterns. We
create a set of SFO patterns for a transitive verb
if (1) it is a frame target4; (2) its subject (respec-
tively object) is a topic; and (3) its object (respec-
tively subject) is a frame target. In sentence (1),
hate is the target of the frame Experiencer focus
(henceforth EF), its subject, people, is a topic, and
its object, guns is the target of the frame Weapon.
As a result, we create a set of SFO patterns, each
of which is represented as a 6-tuple. More specifi-
cally, we create the 8 SFO patterns shown in the
first column of Table 2. Pattern 1 says that (1)
this is an SFO pattern; (2) the subject is the word
people; (3) the frame name of the verb is EF; (4)
the frame name of the object is Weapon; (5) the
verb is not negated (POS); and (6) we don?t care
(DC) whether the verb is sentiment-bearing. If the
verb is sentiment-bearing (in this case, hate has a
negative sentiment), we create another pattern that
is the same as the first one, except that DC is re-
placed with its sentiment value (see Pattern 2).
Next, note that since the subject of hate is the
target of the frame People and its object is a topic,
we need to create patterns in a similar manner,
resulting in Patterns 3 and 4. Note that People
in these two patterns (with ?P? capitalized) is the
3We use the Stanford parser (de Marneffe and Manning,
2008) and SEMAFOR (Das et al, 2010) to obtain depen-
dency relations and semantic frames, respectively.
4A word w is the target of a frame f if f is assigned to
w to generalize its meaning. For example, assassination, kill,
and terminate are the targets of the frame Killing.
126
1 <SFO:people:EF:Weapon:POS:DC> 9 <SFO:people:EF:Weapon:NEG:DC> 17 <DF:dobj:EF:Weapon:POS:DC>
2 <SFO:people:EF:Weapon:POS:?> 10 <SFO:people:EF:Weapon:POS:?> 18 <DF:dobj:EF:Weapon:POS:?>
3 <SFO:People:EF:guns:POS:DC> 11 <SFO:People:EF:guns:NEG:DC> 19 <DF:dobj:EF:guns:POS:DC>
4 <SFO:People:EF:guns:POS:?> 12 <SFO:People:EF:guns:POS:?> 20 <DF:dobj:EF:guns:POS:?>
5 <SFO:people:EF:DC:POS:DC> 13 <SFO:people:EF:DC:NEG:DC> 21 <FET:people:Experiencer:EF:POS:DC>
6 <SFO:people:EF:DC:POS:?> 14 <SFO:people:EF:DC:POS:?> 22 <FET:people:Experiencer:EF:POS:?>
7 <SFO:DC:EF:guns:POS:DC> 15 <SFO:DC:EF:guns:NEG:DC> 23 <FET:guns:Content:EF:POS:DC>
8 <SFO:DC:EF:guns:POS:?> 16 <SFO:DC:EF:guns:POS:?> 24 <FET:guns:Content:EF:POS:?>
Table 2: Sample patterns created for sentences (1) and (2).
name of the frame People, not the word people ap-
pearing in the sentence.
To provide better generalization, we create a
simplified version of each SFO pattern by replac-
ing the frame name representing subject/object
with the value DC. This results in Patterns 5?8.
For sentence (2), we can generate patterns in a
similar manner, resulting in Patterns 9?16. For ex-
ample, Pattern 9 contains the element NEG, which
encodes the fact that the verb like is negated. Pat-
tern 10 deserves discussion. Since the positive
sentiment-bearing verb like is negated, the senti-
ment value of Pattern 10 is ?, which encodes the
fact that not like has a negative sentiment. The
negation value of Pattern 10 is POS rather than
NEG, reflecting the fact that not like does not ap-
pear in a negative context. In other words, the
sentiment value needs to be flipped if the verb
is negated, and so may the negation value. It is
worth noting that Patterns 2 and 10 are identical,
which provides suggestive evidence that sentences
(1) and (2) are semantically equivalent.
Dependency-Frame (DF) patterns. We create
a set of DF patterns for a dependency relation d
if (1) both arguments of d are frame targets or (2)
the head is a frame target and the dependent is a
topic. For example, in the dependency relation
dobj(hate,guns), both hate and guns are frame tar-
gets, as discussed above, and guns is a topic, so a
set of DF patterns (Patterns 17?20 in Table 2) will
be created from it. A DF pattern is represented as
a 6-tuple. For example, Pattern 17 says that (1)
this is a DF pattern; (2) the relation type is dobj;
(3) the frame name of the head is EF; (4) the frame
name of the dependent is Weapon; (5) the head is
not negated; and (6) we don?t care about the sen-
timent of the head. Pattern 18 is the same as Pat-
tern 17, except that it takes into account the senti-
ment value of the verb. Patterns 19 and 20 replaces
the frame name of the dependent with the topic
name, which is guns. The negation and sentiment
values are computed in the same way as those in
the SFO patterns.
Frame-Element-Topic (FET) patterns. We
create one FET pattern for every (v,fe) pair in
a sentence where v is a verb and a frame target,
and fe is a topic and a frame element of v?s
frame.5 In sentence (1), people is a topic and
it is assigned the role Experiencer, so two FET
patterns (Patterns 21 and 22) are created. Also,
since guns is a topic and it is assigned the role
Content, two additional FET patterns (Patterns 23
and 24) are created. The negation and sentiment
values are computed in the same way as those in
the SFO patterns.
4.2 Step 2: Classification
In this step, we will use the patterns learned in
Step 1 in combination with the baseline systems to
classify a test post. A simple way to combine the
learned patterns with the baseline systems would
be to augment the feature set they employ with the
learned patterns. One potential weakness of this
method is that the impact of these patterns could
be undermined by the fact that they are signifi-
cantly outnumbered by the baseline features, par-
ticularly the n-gram features.
For this reason, we decided to train another
stance classifier, which we will refer to as the
semantics-based classifier, cs. Like the base-
line stance classifier cb, (1) cs is trained using
SVMlight, (2) each training instance for cs corre-
sponds to a training post, and (3) its class label is
the stance the post expresses. Unlike cb, however,
the features employed by cs are created from the
learned patterns. Specifically, from each pattern
we create one binary feature whose value is 1 if
and only if the corresponding pattern is applicable
to the training post under consideration.
A natural question, then, is: how can we com-
bine the decisions made by cb and cs? To answer
this question, we applied both classifiers to the de-
5Note that since fe is a frame element of v?s frame, it is
assigned a semantic role.
127
System ABO GAY OBA MAR
cb 60.3 63.2 59.5 67.1
cs 56.1 58.7 56.0 65.2
Table 3: Development set accuracies.
System ABO GAY OBA MAR
cb 22.9 18.5 24.1 9.6
cs 17.6 14.3 19.4 7.2
Table 4: Percentage of posts predicted correctly
by one but not both classifiers on the development
set.
velopment set for each domain and obtained the
results in Table 3. As we can see, cs performs sig-
nificantly worse than cb for all domains.6
At first glance, we should just abandon cs
because of its consistently poorer performance.
However, since the two classifiers are trained on
disjoint feature sets (one is lexico-syntactic and
the other semantic), we hypothesize that the mis-
takes they made on the development set could be
complementary. To confirm this hypothesis, we
compute the percentage of posts in the develop-
ment set that are correctly classified by one but not
the other. Results of this experiment are shown in
Table 4. As we can see, these results are largely
consistent with our hypothesis. For instance, for
ABO, 22.9% of the posts are classified correctly
only by cb but not cs, whereas 17.6% of them are
classified correctly only by cs but not cb.
Given these results, we hypothesize that perfor-
mance could be improved by combining the pre-
dictions made by cb and cs. Since cb consistently
outperforms cs on all datasets, we use cs to make a
prediction if and only if (1) cb cannot predict con-
fidently and (2) cs can predict confidently. This
preference for cb is encoded in the following rule-
based strategy for classifying a test post p, where
the rules are applied in the order in which they are
listed.
Rule 1: if cb can classify p confidently, then use
cb?s prediction.
Rule 2: if cs can classify p confidently, use cs?s
prediction.
Rule 3: use cb?s prediction.
The next question is: how do we define con-
fidence? Since cb and cs are SVM-based clas-
sifiers, the data points that are closer to the hy-
perplane are those whose labels the SVM is less
6All significance tests are paired t-tests, with p < 0.05.
confident about. Hence, we define confidence for
classifier ci by the interval [conf il , conf iu], where
conf il < 0 and conf iu > 0 are signed distances
from the hyperplane defining ci. Specifically, we
say that a point p is confidently classified by ci if
and only if p lies outside the interval defined by
conf il and conf iu. Since we have two classifiers,
cb and cs, we need to define two intervals (i.e., four
numbers). Rather than defining these four num-
bers by hand, we tune them jointly so that the ac-
curacy of our combination strategy on the devel-
opment set is maximized.7
There is a caveat, however. Recall that when
applying this extension, we need to compute the
signed distances of every post p from cb and cs
to determine which classifier will be used to clas-
sify p. The question, then, is: when applying this
extension to the second baseline (the Anand et al
baseline extended with ACs) where all the posts
written by the same author for the same domain
should have the same stance, how should their
signed distances be computed? We adopt a sim-
ple solution: we take the average of the signed
distances of all such posts from the correspond-
ing hyperplane and set the signed distance of each
such post to the average value.
5 Exploiting Same-Stance Posts
To classify a debate post p in the test set, we have
so far exploited only the information extracted
from p itself. However, it is conceivable that we
can improve the classification of p by exploiting
the information extracted from other test posts that
have the same stance as p. This is the goal of our
second extension.
To see why doing so can improve the classifi-
cation of p, we make a simple observation: some
posts are easier to classify than the others. Typi-
cally, posts containing expressions that are strong
indicators of the stance label are easier to classify
than those that do not. As an example, consider
the following posts:
Post 2: I don?t think abortion should be illegal.
Post 3: What will you do if a woman?s life is in
danger while she?s pregnant? Do you still want to
sacrifice her life simply because the fetus is alive?
It should be fairly easy for a human to see that
the authors of both posts support abortion. How-
ever, Post 2 is arguably easier to classify than
7For parameter tuning, for each of the four numbers we
tried the values from ?0.5 to +0.5 with a step value of 0.001.
128
Post 3: Post 2 has an easy-to-determine stance,
whereas Post 3 has a couple of rhetorical questions
that may be difficult for a machine to understand.
Hence, we might be able to improve the classifica-
tion of Post 3 by exploiting information from other
posts that have the same stance as itself (which in
this case would be Post 2).
In practice, however, we are not given the infor-
mation of which posts have the same stance. In
the two subsections below, we discuss two sim-
ple methods of determining whether two posts are
likely to have the same stance.
5.1 Using Same-Author Information
The first method, which we will refer to as M1, is
fairly straightforward: we posit that two posts are
likely to have the same stance if they are written
by the same author. Given a test post p to be clas-
sified, we can use this method to identify a sub-
set of p?s same-stance posts. For convenience, we
denote this set as SameStancePosts(p). The ques-
tion, then, is: how can we exploit information in
SameStancePosts(p) to improve the classification
of p? One way would be to combine the con-
tent of the posts in SameStancePosts(p) with that
of p (i.e., by taking the union of all the binary-
valued feature vectors), and use the class value of
the combined post as the class value of p. How-
ever, rather than simply combining all the posts
to form one big post, we generalize this idea by
(1) generating all possible combinations of posts
in SameStancePosts(p); (2) for each such combi-
nation, combine it with p; (3) classify each combi-
nation obtained in (2) using the SVM classifier; (4)
sum the confidence values of all the combinations;
and (5) use the signed value as the class value of p.
Note that if SameStancePosts(p) contains n posts,
the number of possible combinations is
?n
i=0
(n
i
)
.
For efficiency reasons, we allow each combination
to contain at most 10 posts.
At first glance, it seems that the combination
method described in the previous paragraph is an
alternative implementation of ACs. (Recall that
ACs are inter-post constraints that ensure that two
posts written by the same author for the same do-
main should receive the same label.) Neverthe-
less, there are two major differences between our
combination method and ACs. First, in ACs, the
same-author posts can only interact via the confi-
dence values assigned to them. On the other hand,
in our proposal, the same-author posts interact via
Feature Definition
SameDebate whether authors posted in same debate
SameThread whether authors posted in same thread
Replied whether one author replied to the other
Table 5: Interaction features for the author-
agreement classifier.
feature sharing. In other words, in ACs, the same-
author posts interact after they are classified by
the stance classifier, whereas in our proposal, the
interaction occurs before the posts are classified.
Second, in ACs, all the same-author posts receive
the same stance label. On the other hand, this is
not necessarily the case in our proposal, because
two same-author posts can be classified using dif-
ferent combinations. In other words, ACs and our
combination method are not the same. In fact, they
can be used in conjunction with each other.
5.2 Finding Similar-Minded Authors
Using M1 to identify same-stance posts has a po-
tential weakness. If an author has composed a
small number of posts, then the number of com-
binations that can be generated will be small. In
the extreme case, if an author has composed just
one post p, then no combinations will be gener-
ated using M1.
To enable p to benefit from our idea of ex-
ploiting same-stance posts, we propose another
method to identify same-stance posts, M2, which
is a generalization of M1. In M2, we posit
that two posts are likely to have the same stance
if they are written by the same author or by
similar-minded authors. Given test post p, we
can compute SameStancePosts(p) using the defi-
nition of M2, and apply the same 5-step combina-
tion method described in the previous subsection
to SameStancePosts(p) to classify p.
The remaining question is: given an author,
a, in the test set, how do we compute his set of
similar-minded authors, Asimilar? To do this, we
train a binary author-agreement classifier on the
training set to generate Asimilar for a. Specifi-
cally, each training instance corresponds to a pair
of authors in the training set having one of two
class labels, agree (i.e., authors have the same
stance) and disagree (i.e., authors have opposing
stances). We represent each instance with two
types of features. Features of the first type are ob-
tained by taking the difference of the feature vec-
tors corresponding to the two authors under con-
sideration, where the feature vector of an author is
129
obtained by taking the union of the feature vectors
corresponding to all of the posts written by her.
Taking the difference would allow the learner to
focus on those features whose values differ in the
feature vectors. For the second type of features,
we use author interaction information encoded as
three binary features (see Table 5 for their defi-
nitions), which capture how authors interact with
each other in a debate thread. After training the
classifier, we apply it to classify the author-pairs
in the test set. Then, for each author a, we com-
pute her k-nearest authors based on the magnitude
of their agreement, where k is tuned to maximize
accuracy on the development data.8 Finally, we
take Asimilar to be the set of k-nearest authors.
6 Evaluation
6.1 Experimental Setup
Results are expressed in terms of accuracy ob-
tained via 5-fold cross validation, where accuracy
is the percentage of test instances correctly classi-
fied. Since all experiments require the use of de-
velopment data for parameter tuning, we use three
folds for model training, one fold for development,
and one fold for testing in each fold experiment.
6.2 Results
Results are shown in Table 6. Row 1 shows the
results of the Anand et al (2011) baseline on the
four datasets, obtained by training a stance classi-
fier using the SVMlight package.9 Row 2 shows
the results of the second baseline, Anand et al?s
system enhanced with ACs. As we can see, incor-
porating ACs into Anand et al?s system improves
its performance significantly on all datasets and
yields a system that achieves an average improve-
ment of 4.6 accuracy points.
Next, we incorporate our first extension, pattern
induction, into the better of the two baselines (i.e.,
the second baseline). Results of combining cb and
cs to classify the test posts (together with the ACs)
are shown in row 3 of Table 6. As we can see, in-
corporating pattern induction into the second base-
line significantly improves its performance on all
four datasets and yields a system that achieves an
average improvement of 2.48 accuracy points.
Before incorporating our second extension, let
8We tested values of k from 1 to 7.
9For all SVM experiments, the regularization parameter C
is tuned using development data, but the remaining learning
parameters are set to their default values.
System ABO GAY OBA MAR
cb 61.4 62.6 58.1 66.9
cb+AC 72.0 64.9 62.7 67.8
cb+cs+AC 73.2 68.0 64.2 71.9
cbs+AC 71.8 65.0 60.2 67.9
cb+cs+M1+AC 74.8 69.1 69.7 73.2
cb+cs+M2+AC 75.9 70.6 71.2 75.3
Table 6: 5-fold cross-validation accuracies.
us recall our earlier hypothesis that combining cb
and cs using our method would be better than
training just one classifier that combines the fea-
tures used by cb and cs. The reason behind our
hypothesis was that simply combining the feature
sets would undermine the impact of pattern-based
features because they would be significantly out-
numbered by the features in cb. To confirm this
hypothesis, we showed in row 4 of Table 6 the
results of this experiment, where we trained one
classifier on all the features used by cb and cs.
As we can see, this classifier (referred to as cbs in
the table) together with the ACs performs signif-
icantly worse than the cb+cs+AC system (row 3)
on all datasets. In fact, the cb+AC system (row 2)
outperforms the cbs+AC system on OBA, but they
are statistically indistinguishable on the remaining
datasets. These results suggest that combining the
pattern-based features with the baseline features
into one feature set renders the former ineffective.
Finally, we incorporate our second extension,
the one that involves generating combinations of
test posts written by the same author (M1) and by
both the same author and similar-minded authors
(M2). Results of these experiments are shown in
rows 5?6 of Table 6. The M1-based system sig-
nificantly outperforms cb+cs+AC on all four do-
mains, yielding an average improvement of 2.4 ac-
curacy points. The M2-based system further beats
the M1-based system by 1.5 accuracy points on
average, and their performance difference is sig-
nificant on all but the ABO domain.
Overall, our two extensions yield a stance clas-
sification system that significantly outperforms the
better baseline on all four datasets, with an average
improvement of 6.4 accuracy points.
Given the better performance of the
combination-based systems, a natural ques-
tion is: can we further improve performance
by applying our combination methods to gen-
erate artificial posts and use them as additional
training instances? To answer this question, we
apply both M1 and M2 to generate additional
130
training instances, using a random selection of
same-stance authors in place of M2?s k-nearest
neighbor method. However, neither method yields
an improvement in performance over the method
on which it is based. We speculate that since all
the posts in the training combinations are already
present in the training set as individual posts,
they are more likely to be farther away from the
hyperplane than the individual posts, meaning
that they are less likely to be support vectors. This
in turn implies that they are less likely to affect
classification performance.
6.3 Error Analysis
To gain additional insights into our approach, we
performed a qualitative analysis of the errors pro-
duced by our best-performing system below.
Failure to accumulate decisions from several
clues. Authors often express their stance using a
group of sentences where the latter sentence(s) in-
dicate the actual stance and the initial sentence(s)
may give a false impression about the author?s
stance. Consider Post 1 (see Section 1) and Post 4.
Post 4: I agree abortion creates stress and pain. I
agree it kills a potential life. That does not mean
it is right to ban abortion.
In Post 1, the author is anti-abortion, whereas
in Post 4, the author is pro-abortion. However,
the first sentence in Post 1 gives a misleading clue
about the author?s stance, and so do the first two
sentences in Post 4. Since all the systems dis-
cussed in the paper operate on one sentence at a
time, they are all prone to such errors. One way
to address this problem could be to determine how
adjacent sentences are related to each other via the
use of discourse relations.
Presence of materials irrelevant to stance. Be-
cause of the informal style of writing, we often
find long posts with one or two sentences indicat-
ing the actual stance of the author. The rest of such
posts often include descriptions of an author?s per-
sonal experience, comments or questions directed
to other authors etc. Such long posts are frequently
misclassified for all four domains. Consider the
following example.
Post 5: Marijuana should at least be decriminal-
ized. Driving stoned, however, is something totally
different and should definitely be a crime. Also,
weed can?t kill you, unlike cigarettes and alcohol.
In my opinion cigarettes should definitely be ille-
gal, but they?re so ingrained into our culture that I
doubt that is going to happen any time soon.
In this post, the author supports the legalization
of marijuana. However, the only useful hints about
her stance are ?marijuana should at least be de-
criminalized? and ?weed can?t kill you?. The rest
of the post is not helpful for stance classification.
Convoluted posts appearing later in long post
sequences. As a post sequence gets longer, au-
thors tend to focus on specific aspects of a de-
bate and consequently, it becomes more difficult to
classify their stances, even with the context-based
features (features taken from the immediately pre-
ceding post) proposed by Anand et al Consider
the following post sequence, where only the first
post (P1) and the nth post (Pn) are shown due to
space limitations.
[P1: Anti-Obama] Obama is a pro-abortionist. Killing ba-
bies is wrong so stop doing it. The new health reform bill
is not good. There are some good things but more worse
than good. You could have just passed some laws instead of
making a whole bill.
? ? ?
[Pn: Pro-Obama] Killing fetuses isn?t wrong. Be-
sides, we could use those fetuses for stem cell re-
search.
As we can see, the author of P1 does not sup-
port Obama because of his pro-abortion views. In
Pn, a pro-Obama author explains why she thinks
abortion is not wrong. However, without the con-
text from P1 that Obama is pro-abortion, it is not
easy for a machine to classify Pn correctly. This
problem is more serious in ABO and GAY than in
the other domains as the average length of a post
sequence in these two domains is larger.
7 Conclusions
We examined the under-studied task of stance
classification of ideological debates. Employing
our two extensions yields a system that outper-
forms an improved version of Anand et al?s ap-
proach by 2.6?7.0 accuracy points. In particular,
while existing approaches to debate stance classi-
fication have primarily employed lexico-syntactic
features, to our knowledge this is the first attempt
to employ FrameNet for this task to induce fea-
tures that aim to capture the meaning and pro-
vide semantic generalizations of a sentence. In
addition, our method for identifying and exploit-
ing same-stance posts during the inference proce-
dure provides further gains when used on top of
our FrameNet extension.
131
References
Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
Srikant, and Yirong Xu. 2003. Mining newsgroups
using networks arising from social behavior. In
Proceedings of the 12th international conference on
World Wide Web, WWW ?03, pages 529?535.
Pranav Anand, Marilyn Walker, Rob Abbott, Jean E.
Fox Tree, Robeson Bowmani, and Michael Minor.
2011. Cats rule and dogs drool!: Classifying stance
in online debate. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis (WASSA 2011), pages 1?9.
Alexandra Balahur, Zornitsa Kozareva, and Andre?s
Montoyo. 2009. Determining the polarity and
source of opinions expressed in political debates. In
Proceedings of the 10th International Conference on
Computational Linguistics and Intelligent Text Pro-
cessing, CICLing ?09, pages 468?480.
Mohit Bansal, Claire Cardie, and Lillian Lee. 2008.
The power of negative thinking: Exploiting label
disagreement in the min-cut classification frame-
work. In Proceedings of the 22nd International
Conference on Computational Linguistics: Com-
panion volume: Posters, pages 15?18.
Or Biran and Owen Rambow. 2011. Identifying justi-
fications in written dialogs. In Proceedings of the
2011 IEEE Fifth International Conference on Se-
mantic Computing, ICSC ?11, pages 162?168.
Clinton Burfoot, Steven Bird, and Timothy Baldwin.
2011. Collective classification of congressional
floor-debate transcripts. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1506?1515.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic frame-semantic
parsing. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 948?956.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies
representation. In Proceedings of the COLING
Workshop on Cross-Framework and Cross-Domain
Parser Evaluation, CrossParser ?08, pages 1?8.
Kazi Saidul Hasan and Vincent Ng. 2012. Predict-
ing stance in ideological debate with rich linguistic
knowledge. In Proceedings of the 24th International
Conference on Computational Linguistics: Posters,
pages 451?460.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scholkopf and
Alexander Smola, editors, Advances in Kernel Meth-
ods - Support Vector Learning, pages 44?56. MIT
Press.
Yue Lu, Hongning Wang, ChengXiang Zhai, and Dan
Roth. 2012. Unsupervised discovery of opposing
opinion networks from forum discussions. In Pro-
ceedings of the 21st ACM International Conference
on Information and Knowledge Management, CIKM
?12, pages 1642?1646.
Akiko Murakami and Rudy Raymond. 2010. Support
or oppose? Classifying positions in online debates
from reply activities and opinion expressions. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 869?875.
Swapna Somasundaran and Janyce Wiebe. 2010. Rec-
ognizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Gener-
ation of Emotion in Text, CAAGET ?10, pages 116?
124.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335.
Marilyn Walker, Pranav Anand, Rob Abbott, and Ricky
Grant. 2012. Stance classification using dialogic
properties of persuasion. In Proceedings of the 2012
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 592?596.
Yi-Chia Wang and Carolyn P. Rose?. 2010. Making
conversational structure explicit: Identification of
initiation-response pairs within online discussions.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 673?676.
Ainur Yessenalina, Yisong Yue, and Claire Cardie.
2010. Multi-level structured models for document-
level sentiment classification. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 1046?1056.
132
