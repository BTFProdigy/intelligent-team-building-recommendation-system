 Decomposition for ISO/IEC 10646 Ideographic Characters 
LU Qin, CHAN Shiu Tong, LI Yin, LI Ngai Ling 
Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong 
{csluqin, cstchan, csyinli, csnlli}@comp.polyu.edu.hk 
 
Abstract 
Ideograph characters are often formed by some 
smaller functional units, which we call character 
components. These character components can be 
ideograph radicals, ideographs proper, or some 
pure components which must be used with 
others to form characters. Decomposition of 
ideographs can be used in many applications. It 
is particularly important in the study of Chinese 
character formation, phonetics and semantics.  
However, the way a character is decomposed 
depends on the definition of components as well 
as the decomposition rules. The 12 Ideographic 
Description Characters (IDCs) introduced in ISO 
10646 are designed to describe characters using 
components. The Hong Kong SAR Government 
recently published two sets of glyph standards 
for ISO10646 characters. The standards, being 
the first of its kind, make use of character 
decomposition to specify a character glyph using 
its components. In this paper, we will first 
introduce the IDCs and how they can be used 
with components to describe two dimensional 
ideograph characters in a linear fashion. Next we 
will briefly discuss the basic references and 
character decomposition rules. We will then 
describe the data structure and algorithms to 
decompose Chinese characters into components 
and, vice versa. We have also implemented our 
database and algorithms as an internet 
application, called  the Chinese Character 
Search System, available at website 
http://www.iso10646hk.net/. With this tool, 
people can easily search characters and 
components in ISO 10646. 
  
Introduction 
ISO/IEC 10646 (ISO 10646) in its current 
version, contains more than 27,000 Han 
characters, or ideograph characters as it is called, 
in its basic multilingual plane and another 
40,000 in the second plane[1-2]. The complete 
set of ideograph repertoire includes Han 
characters in all national/regional standards as 
well as all characters from the Kang Xi 
Dictionary( ) and other major 
references. In almost all the current encoding 
systems including ISO 10646 and Unicode, each 
Han character is treated as a separate unique 
symbol and given a separate code point.  This 
single character encoding method has some 
serious drawbacks. Consider most of the 
alphabet-based languages, such as English, even 
though new words are created quite frequently, 
the alphabet itself is quite stable. Thus the newly 
adopted words do not have any impact on 
coding standards. When new Han characters are 
created, they must be assigned a new code point, 
thus all codesets supporting Han characters must 
leave space for extension. As there is no formal 
rule to limit the formation of new Han characters, 
the standardization process for code point 
assignment can be potentially endless. On the 
other hand, new Han characters are almost 
always be created using some existing character 
components which can be existing radicals, 
characters proper, or pure components which are 
not used alone as characters. If we can use coded 
components to describe a new character, we can 
potentially eliminate the standardization process. 
Han characters can be considered as a two 
dimensional encoding of components. The same 
set of components when used in different 
relative positions can form different characters. 
For example the two components  and  
can form two different characters:   
depending on the relative positions of the two 
components. However, the current internal code 
point assignments in no way can reveal the 
relationship of the these characters with respect 
to their component characters. Because of the 
limitation of the encoding system, people have 
to put a lot of efforts to develop different input 
methods. Searching for characters with similar 
shapes are also quite difficult. The 12 
 Ideographic Description Characters (IDCs) were 
introduced in ISO 10646 in the code range of 
2FF0 - 2FFB to describe the relative positions of 
components as shown in Table 1. Each IDC 
symbol shows a typical ideograph character 
composition structure. For example,  
(U+2FF0) indicates that a character is formed by 
two components, one on the left-hand side and 
one on the right-hand side. All IDCs except 
U+2FF2 and U+2FF3 have cardinality of two 
because the decomposition requires two 
components only.  Details of these symbols can 
be found in Annex F of ISO 10646 2nd Edition 
[1] and in John Jenkens' report [3]. 
 
 
Smbl 
 
Code 
point  
 
Name in ISO 10646 
 
Cardi- 
nality 
 
Label 
 2FF0 IDC LEFT TO RIGHT IDC2 A 
 2FF1 IDC ABOVE TO BELOW 
IDC2 B 
 2FF2 IDC LEFT TO MIDDLE AND RIGHT 
IDC3 K 
 2FF3 IDC ABOVE TO MIDDLE AND 
BELOW 
IDC3 L 
 2FF4 IDC FULL SURROUND 
IDC2 I 
 2FF5 IDC SURROUND FROM ABOVE 
IDC2 F 
 2FF6 IDC SURROUND FROM BELOW 
IDC2 G 
 2FF7 IDC SURROUND FROM LEFT 
IDC2 H 
 2FF8 IDC SURROUND FROM UPPER LEFT 
IDC2 D 
 2FF9 IDC SURROUND FROM UPPER RIGHT 
IDC2 C 
 2FFA IDC SURROUND FROM LOWER LEFT 
IDC2 E 
 2FFB IDC OVERLAID IDC2 J 
Table 1. The 12 Ideograph Description 
Characters 
 
The IDCs can be used to describe not only 
unencoded characters, but also coded characters 
to reveal their internal structures and 
relationships among components. Thus 
applications for using these structural symbols 
can be quite useful. In fact the most common 
applications are in electronic dictionaries and 
on-line education [4]. 
 
In this paper, however, we introduce a new 
application where the IDCs and components are 
used in the standardization of Han character 
glyphs. As we all know that ISO 10646 is a 
character standard, which allows different glyph 
styles for the same character and different 
regions can develop different glyph styles to suit 
their own needs. The ideographic repertoire in 
ISO 10646 has a so called Horizontal Extension, 
where each coded ideograph character is listed 
under the respective CJKV columns. The glyph 
of each character can be different under different 
columns because ISO 10646 is a character 
standard, not a glyph standard. We normally call 
these different glyphs as variants. For example, 
the character bone   can take three different 
forms(variants):  
 
   
HK Mainland Taiwan 
 
Even with the ISO 10646 horizontal extensions, 
people in Hong Kong still get confused as to 
which styles to use, as only some characters in 
the Hong Kong style deviate from both G 
column(mainland China) and T column(Taiwan). 
Consequently, the Hong Kong SAR Government 
has decided to  develop the Hong Kong glyph 
standards for ISO 10646 which can serve as a 
reference guide for font vendors when 
developing products for Hong Kong. The 
standards, being the first of its kind, makes uses 
of character decomposition to specify a 
character glyph using its components. 
 
The rest of the paper is organized as follows. 
Section 1 gives the rationale for the use of 
character components, the references and 
decomposition rules. Section 2 describes the 
data structure and algorithms to decompose 
Chinese characters into components and, vice 
versa. Section 3 discusses performance 
considerations and Section 4 is the conclusion. 
 
1. Character Decomposition Rules 
At the beginning of the glyph standardization, 
one important requirement was agreed by the 
working group, namely, extensibility. That is, the 
specifications should be easily extended by 
adding more characters into later versions of the 
ISO/IEC 10646, which we refer to as the new 
characters. The specifications should also not 
contain any internal inconsistency, or 
inconsistency in relation to the ISO/IEC 10646?s 
 source standards. In order to satisfy both 
consistency requirements, we have concluded 
that listing every character in ISO/IEC 10646 is 
not desirable. Instead, we decided to produce the 
specifications by giving the correct glyphs of 
character components based on a common 
assumption that if a component or a character is 
written in a certain way, all other characters 
using it as a component should also write it in 
the same way. For example if the character 
?bone?  (U+9AA8) is written in a certain 
way, all characters using ?bone? as a component, 
such as ? ? (U+6ED1) and ? ? (U+9ABC), 
should have the bone ? ? component follow 
the same style. In this way, the specification can 
be extended very easily for all new characters 
using bone ? ? as a component. In other words, 
we can assume that component glyphs are 
standardized for general usage. By using 
components to describe a character, we can also 
avoid inconsistency. That is, by avoid listing all 
characters with bone, ? ? as a component, we 
do not need to be concerned about producing 
inconsistent glyphs in the specifications. This is 
important because the working group does not 
have any font vendor as a member, because of 
an implicit rule that was specified by the 
Government of the HKSAR to avoid any 
potential conflict of interest. The glyph style is 
mostly based on the book published by the Hong 
Kong Institute of Education in 2000[5] 
 
In principle, for producing glyph specifications, 
we have to produce a concrete, minimal, and 
unique list of basic components. In order to 
achieve this, we need to have a set of rules to 
decompose the characters systematically. In our 
work, we have used the GF 3001-1997 [6] as our 
major component reference. The following is a 
brief description of the rules. (For a detailed 
description, please refer to the paper ?The Hong 
Kong Glyph Specifications for ISO 10646's 
Ideographic Characters?[7].) 
? Use GF 3001-1997 specifications as the 
basis to construct a set of primary 
components. Components for simplified 
Chinese are removed. The shapes are 
modified to match the glyph style for 
Hong Kong. 
? Characters are decomposed into 
components according to their structure 
and etymological origin. 
? In some cases, an ?ad-hoc? 
decomposition occurs if the 
etymological origin and its glyph shape 
are not consistent, or the etymological 
origin is not clear, or to avoid defining 
additional components. 
? Characters are not decomposed if it 
appears in GF 3001-1997 as a 
component. 
? Detached parts can be further 
decomposed. 
? Merely touched parts that do not have 
overlapping or crossing can be 
decomposed. 
? In some cases, we do not decompose 
some components to prevent the 
components from getting too small. 
? In some cases, a single component will 
be distinguished as two different 
components. This is the concept of 
variant or related component. 
This set of rules, together with 644 basic 
components and the set of intermediate 
components defined, enables us to decompose 
Chinese characters that appear in the first 
version ISO 10646 with 20,902 characters, Ext. 
A in the second version of ISO 10646[1] and 
Hong Kong Suplementary Character Set [8-9].  
The 644 basic components play a very important 
role because they form all the Chinese characters 
in our scope. 
 
In order to describe the position relationship 
amongst components in a character, we have 
used the 12 Ideographic Description Characters 
(IDC) in ISO/IEC 10646 Part1:2000 in the range 
from 2FF0 to 2FFB, and defined an extra IDC 
?M? (which indicates that a particular 
component is a basic component and will not be 
further decomposed), as shown in Table 1. Every 
character can be decomposed into up to three 
components depending on the cardinality of the 
IDC used.  
 
Each Character is decomposed according to the 
following definition: 
 
Character = IDC2 CC(1)  CC(2)  
| IDC3 CC(1) CC(2) CC(3) 
| M 
where 
IDC2 ? (2FF0 ? 2FFB)  
CC(i)  is a set of character components 
and i indicates its position in the 
sequence  
 M is a special symbol indicating 
Character will not be further 
decomposed 
 
By our definition, a CC can be formed by three 
subsets: (1) coded radicals, (2) coded 
components and ideographs proper, and (3) 
intermediate components that are not coded in 
ISO 10646. The intermediate components are 
maintained by our system only. The 
decomposition result is stored in the database. 
Conceptually, every entry in the database can be 
treated as a Chinese component, having a data 
structure described above. 
 
2. Decomposition/Formation Algorithms 
As mentioned above, the decomposition 
database only gives information on how a 
character is decomposed in a minimal way. 
However, some characters have nested 
components. For instance, the character ??? 
can be decomposed into two components: ??? 
and ???, but ??? being a character can be 
further decomposed into two components. In 
order to handle nesting and finding components 
to the most elementary form(no further 
decomposition), we have defined the 
decomposition and formation algorithms.  
There are mainly two algorithms, one for the 
decomposition of a character into a set of 
components(the algorithm is called 
Char-to-Compnt) , another one for the formation 
of a set of characters from a component ( the 
algorithm is called Compn-to-Charr). 
Let x be the seed (x = starting character for 
search); 
Stop = false 
WHILE NOT  stop DO 
      IF Struct_Symbol(CD[x]) = ?M? 
        Stop = True 
     ELSE 
LCmp ={ cc[x] ?  CC } 
ENDWHILE 
 
Figure 1. Pseudo-code of ?Char-to-Compnt? 
 
Both algorithms are very similar. They 
recursively retrieve all characters/components 
appearing in the decomposition database by 
using the characters/components themselves as a 
seed, but their directions of retrieval are opposite 
to each other. In the ?Char-to-Compnt?, the 
decomposition goes from its current level down, 
one level at a time, until no more decomposition 
can be done. Figure 1 the pseudo code of the 
algorithm for one level only and they can be 
done recursively to find all components of a 
character. Table 2 shows the entries related to 
the character ???. Notice that the number of 
components for ??? is not two, but 4 because 
one of the components ??? can be further 
decomposed into two more components. 
 
Character IDC Comp1 Comp2 Comp3 
? B ? ?  
? A ? ?  
? M    
? M    
? M    
Table 2. Component Entries of character 
??? 
 
On the other hand, the ?Compnt-to-Char? 
algorithm searches from its current level up until 
no more character can be found using the current 
component. Figure 2 shows the pseudo code of 
the upward search algorithm where x is 
considered the seed to start the search and the 
variable contains all characters formed using the 
current component x.  
 
Let x be the seed (x = starting component for 
search); 
Stop = false 
Char_List ={ x} 
WHILE NOT  stop DO 
      IF No Change to Char_List 
              Stop = True 
     ELSE 
FOR each x in Char_List 
Char_List = Char_List ?{ Char[x]} 
     ENDFOR 
 
ENDWHILE 
 
Figure 2. Pseudo-code of ?Compnt-to-Char? 
 
Character IDC Comp1 Comp2 Comp3 
? M    
? B ? ?  
? A ? ?  
? A ? ?  
? A ? ?  
?     
Table 3. Example character entries of  
component ???   
  
Table 3 shows some of the search results 
involving the component  ???.  Note that the  
result not only find the character ???, but also 
the characters using ??? as components as well.  
 
Further more, due to the fact that there are two 
IDCs with cardinality of three, the 
decomposition is not unique.  Based Han 
characters formation rules, some characters 
should be decomposed into two components first 
before considering further decomposition.  For 
instance, ??? should be decomposed into ??? 
and ??? whereas ??? should be decomposed 
into ??? and ???.  However, for upward 
search we certainly want the character ??? to 
be found if the search component is ?? ?. 
Therefore, in addition to using the most reason 
decomposition at the first level, we also 
maintain different decompositions for 
applications where character formation rule are 
less important. In other words, we also provided 
composition and decompositions independent of 
certain particular character formal rules. Again 
taking the character ??? as an examples, its 
components should not only be ??? and ???, 
but also  ???, ???, ??? as well as ??? and 
? ?.  In fact, in our system,  ?? ?  is 
decomposed into ???, ??? and ? ? as shown 
in Table 4. The ?Char-to-Compnt? algorithm 
will take the relative positions of the 
components into consideration based on the IDC 
defined in each entry to find other three possible 
components  ???, ??? and ???.  This can 
be done because the combination of ??? and 
??? will form ???; similarly ??? and ? ? 
will form ???;, and ??? and ? ?will form 
???. Note that in the first two cases of the OR 
clause, ??? and ??? will be identified. In the 
third case of the OR  clause, the character  
??? will be identified. You may argue the 
validity of the third case of the OR clause, but 
for the character ????, finding the component 
??? would be very important.    
Character IDC Comp1 Comp2 Comp3 
? K ? ?  
? A ? ?  
? A ?   
? A ?   
Table 4 An example of handling a character 
with three components 
 
The  basic principle of the algorithm, as shown 
in Figure 3,  is that if we see a character with 
an IDC {K} or {L}, or an IDC of a character 
that can be transformed to IDC {K} or {L}, we 
will try to use its components to form characters. 
 
Let x be a Chinese component (x = cc); 
Let LCsub be the list of sub-components c; 
IF x[structure] = IDC{K} THEN 
LCsub = c : c[structure] = IDC{A}AND 
c[component(1)] = x[component(1)] AND 
c[component(2)] = x[component(2)] or 
c[component(2)] = x[component(2)] AND 
c[component(3)] = x[component(3)] or 
c[component(1)] = x[component(1)] AND 
c[component(3)] = x[component(3)] 
END 
**the same algorithm works when x[structure] = 
IDC{L}, then the result c[structure] will become 
IDC{B} 
Figure 4 Pseudo-code for handling a 
character with three components 
 
Let s be the seed (s = cc); 
Let r be the result component; 
if s[structure] = IDC{A} 
if s[component(1)][ structure] = IDC{A} then 
r = IDC{K} + 
s[component(1)][component(1)] + 
s[component(1)][component(2)] + 
s[component(2)] 
else if s[component(2)][ structure] = IDC{A} 
then 
r = IDC{K} + s[component(1)] + 
s[component(2)][component(1)] + 
s[component(2)][component(2)] 
end 
end 
**this algorithm also works when s[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 4 Pseudo-code of For the Split Step 
 
In many cases, we still want to maintain the 
characters in the right decomposition, e.g, to 
decompose them into two components first and 
then further decompose if needed. Take another  
character ??? as an example. Suppose it is only 
decomposed into two components (??? and 
???). This makes the search more complex. In 
order to simplify the search, we need to go 
through an internal step which we call the Split 
Step to decompose the character into three 
 components before we allow for component to 
character search. The pseudo code for the Split 
Step is shown in Figure 4. The generated result 
is shown in Table 5.  
 
Character IDC Comp1 Comp2 Comp3 
? A ? ?  
? A ? ?  
 
? K ? ? ? 
Table 5. An example Output of the Split Step 
 
For some characters like ???, the Split Step 
must consider the component  ?? ? in the 
middle as an insertion into the character ???. 
We use similar handling to decompose  ??? 
into  ???, ??? and ???, with an IDC {K}. In 
order to find a character with the component 
??? such as ??? , we need additional algorithm 
to locate components that are potentially being 
split to the two sides with an inserted component. 
We try to decompose a component into two 
sub-components if their IDC is ?A? or ?B?. 
Once we get the two sub-components, we try to 
make different combinations to see if there are 
any characters with an IDC {K} or {L} that 
contain the two sub-components as shown in 
Figure 5. 
 
Let x be a Chinese character (x = cc); 
Let Clst be the list of results c; 
if x[structure] = IDC{A} then 
Clst = c : c[structure] = IDC{K} and 
((c[component(1)] = x[component(1)] and 
c[component(2)] = x[component(2)] ) or 
(c[component(2)] = x[component(1)] and 
c[component(3)] = x[component(2)]) or 
(c[component(1)] = x[component(1)] and  
c[component(3)] = x[component(2)])) 
end 
**this algorithm also works when x[structure] = 
IDC{B}, then the result structure will become 
IDC{L} 
Figure 5. Pseudo-code of finding inserted 
component 
 
3. Performance Evaluation 
Since the algorithms have to do excessive search 
for many combinations in many levels 
recursively, performance becomes a very 
important issue especially if we want to make 
this for public access through the internet. 
However, since the decomposition is static, it 
does not need to be done in real time. as the 
search doesn?t need to be done online, In other 
words, searching of the same data will always 
give the same result unless the decomposition 
rules or algorithms are changed. Consequently, 
we built two pre-searched tables to store the 
results of both ?Compnt-to-Char? algorithm and 
the ?Char-to-Compnt?algorithm. Once we have 
the pre-searched tables, we can totally avoid the 
recursive search. Instead, the search result can 
be directly retrieved in a single tuple. This 
results in much better performance both in terms 
of usage of CPU time and I/O usage. 
 
Character Pre-searched result 
? ? ? ? ? ? ?  
? ? ? ? ? ? 
??  
Table 6. Examples of pre-searched results of 
?Cha-to-Compnt?Algorithm 
 
Character Pre-searched result 
? ? ? ? ? ? ? ? ? (total 
5481 characters) 
? ? ? ? ? ? ? ? (total 44 
characters) 
??  
Table 7. Examples of pre-searched results of 
?Component to Character? 
Table 6 and table 7 shows some samples of the 
pre-searched tables for the downward search and 
the upward search, respectively. 
  
Although the advanced control algorithms can 
retrieve most Chinese characters correctly, they 
also return some components that do not make 
much sense. For example, the character ??? has 
a structure of IDC{B}, and components ??? 
and ?? ?. However, when it is eventually 
decomposed into  ???, ??? and ???. Using 
the algorithm ?Char-to-Compnt?, the component 
??? will also be returned, even though ??? has 
no cognate relationship with the character ???. 
We can take into consideration of only a subset 
of characters that can be split in character 
formation, such as ??? and ???. This way, the 
insertion components will only be considered for 
these characters.   
 
4. Conclusion 
In this paper, we focus on the algorithms of 
character decomposition and formation. The 
results can be used for the standardization of 
 computer fonts, glyphs, or relevant language 
resources. We have implemented a Chinese 
Character Search System based on the result of 
this standardization work. We can use this search 
system to look for character decomposition or 
formation results. The system comes with many 
handy and useful features. It provides a lot of 
useful information on Chinese characters, such 
as the code for various encodings, and 
pronunciations. A stand-alone version is also 
built. The actual implementation of these 
algorithms and of the database helps people to 
get information about Chinese characters very 
quickly. It further facilitates researchers? work in 
related areas. For more information on the 
system, please visit the website 
http://www.iso10646hk.net. 
 
Acknowledgement 
The project is partially supported by the Hong 
Kong SAR Government(Project code: ITF 
AF/212/99) and  the Hong Kong Polytechnic 
University(Project Code: Z044). 
 
References 
[1] ISO/IEC, ?ISO/IEC 10646-1 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2000 
[2] ISO/IEC, ?ISO/IEC 10646-2 Information 
Technology-Universal Multiple-Octet 
Coded Character Set - Part 1?, ISO/IEC, 
2001 
[3] John Jenkins, "New Ideographs in Unicode 
3.0 and Beyond", Proceedings of the 15th 
International Unicode Conference C15, 
San Jose, California, Sept. 1-2, 1999 
[4] Dept. of Education(Taiwan), ?Dictionary 
of Chinese Character Variants Version 2?, 
Dept. of Education, Taiwan, 2000 
[5] ???????, ????????, (?
???????), ???????
? , ????? ( LEE Hok-ming as 
Chief Editor, Common Character 
Glyph Table 2nd Edition, Hong Kong 
Institute of Education, 2000) 
[6] GF3001-1997???????????? 
?????? ?????GB13000.1?
?????????, ????????
???, 1997? 12?. 
 
[7] Lu Qin, The Hong Kong Glyph 
Specifications for ISO 10646?s Ideographic 
Characters. 21st International Unicode 
Conference, Dublin, Ireland, May 2002 
[8] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set?, HKSARG, September 28, 
1999 
[9] Hong Kong Special Administrative Region 
Government, ?Hong Kong Supplementary 
Character Set ? 2001 ? , HKSARG, 
December 31, 2001 
 
 
  
A Unicode based Adaptive Segmentor 
Q. Lu, S. T. Chan, R. F. Xu, T. S. Chiu 
Dept. Of Computing, 
The Hong Kong Polytechnic University, 
Hung Hom, Hong Kong 
{csluqin,csrfxu}@comp.polyu.edu.hk 
B. L. Li, S. W. Yu 
The Institute of Computational Linguistics, 
Peking University, 
Beijing, China 
{libi,yusw}@pku.edu.cn 
 
Abstract 
This paper presents a Unicode based 
Chinese word segmentor. It can handle 
Chinese text in Simplified, Traditional, or 
mixed mode. The system uses the strategy 
of divide-and-conquer to handle the 
recognition of personal names, numbers, 
time and numerical values, etc in the pre-
processing stage. The segmentor further 
uses tagging information to work on 
disambiguation. Adopting a modular 
design approach, different functional parts 
are separately implemented using 
different modules and each module 
tackles one problem at a time providing 
more flexibility and extensibility. Results 
show that with added pre-processing 
modules and accessorial modules, the 
accuracy of the segmentor is increased 
and the system is easily adaptive to 
different applications. 
1 Introduction 
The most difficult problem in Chinese word 
segmentation is due to overlapping ambiguities [1-
2]. The recognition of names, foreign names, and 
organizations are quite unique for Chinese. Some 
systems can already achieve very high accuracy [3], 
but they heavily rely on manual work in getting the 
system to be trained to work certain language 
environment. However, for many applications, we 
need to look at the cost to achieve high accuracy. 
In a competitive environment, we also need to 
have systems that are quickly adaptive to new 
requirements with limited resources available. 
In this paper, we report a Unicode based Chinese 
word segmentor. The segmentor can handle 
Chinese text in Simplified, Traditional, or mixed 
mode where internally only one dictionary is 
needed. The system uses the strategy of divide-
and-conquer to handle the recognition of personal 
names, numbers, time and numerical values. The 
system has a built-in new word extractor that can 
extract new words from running text, thus save 
time on training and getting the system quickly 
adaptive to new language environment. The 
Bakeoff results in the open text for our system in 
all categories have shown that it works reasonably 
good for all different corpora. 
The rest of the paper is organized as follows. 
Section 2 presents our system design objectives 
and components. Section 3 discusses more 
implementation details. Section 4 gives some 
performance evaluations. Section 5 is the 
conclusion. 
2 Design Objectives and Components 
With the wide use of Unicode based operating 
systems such as Window 2000 and Window XP, 
we now see more and more text data written in 
both the Simplified form and the Traditional form 
to co-exist on the same system. It is also likely that 
text written in mixed mode. Because of this reality, 
the first design objective of this system is its ability 
to handle the segmentation of Chinese text written 
in either Simplified Chinese, Traditional Chinese, 
or mixed mode.  As an example, we should be able 
to segment the same sentence in different forms 
such as the example given below:   
 
The second design objective is to adopt the 
modular design approach where different 
functional parts are separately implemented using 
independent modules and each module tackles one 
problem at a time. Using this modular approach, 
we can isolate problems and fine tune each module 
with minimal effect on other modules in the system. 
  
Special features like adding new rules or new 
dictionary can be easily done without affecting 
other modules. Consequently, the system is more 
flexible and can be easily extended.  
The third design objective of the system is to make 
the segmentor adaptive to different application 
domains. We consider it having more practical 
value if the segmentor can be easily trained using 
some semi-automatic process to work in different 
domains and work well for text with different 
regional variations. We consider it essential that 
the segmentor has tools to help it to obtain regional 
related information quickly even if annotated 
corpora are not available. For instance, when it 
runs text from Hong Kong, it must be able to 
recognize the personal names such as  if 
such a name(quadra-gram) appears in the text often. 
 
Figure 1. System components 
Figure 1 shows the two major components, the 
segmentor and data manager. The segmentor is the 
core component of the system. It has a pre-
processor, the kernel, and a post-processor. As the 
system has to maintain a number of tables such as 
the dictionaries, family name list, etc., a separate 
component called data manager is responsible in 
handling the maintenance of these data.  The pre-
processor has separate modules to handle 
paragraphs, ASCII code, numbers, time, and 
proper names including personal names, place and 
organizational names, and foreign names. The 
kernel supports different segmentation algorithms. 
It is the application or user?s choice to invoke the 
preferred segmentation algorithms that at current 
time include the basic maximum matching and 
minimum matching in both forward and backward 
mode. These can also be used to build more 
complicated algorithms later on. In addition, the 
system provides segmentation using part-of-speech 
tagging information to help resolve ambiguity. The 
post-processor applies morphological rules which 
cannot be easily applied using a dictionary.   
The data manager helps to maintain the knowledge 
base in the system. It also has an accessory 
software called the new word extractor which can 
collect statistical information based on character 
bi-grams, tri-grams and quadra-grams to semi-
automatically extract words and names so that they 
can be used by the segmentor to improve 
performance especially when switching to a new 
domain. Another characteristic of this segmentor is 
that it provides tagging information for segmented 
text. The tagging information can be optionally 
omitted if not needed by an application. 
3 Implementation Details 
The basic dictionary of this system was provided 
by Peking University [4] and we also used the 
tagging data from [4]. The data structure for our 
dictionaries are very similar to that discussed in [5]. 
As our program needs to handle both Simplified 
and Traditional Chinese characters, Unicode is the 
only solution for dealing with more than one script 
at the same time. 
Even though it is our design objective to support 
both Simplified and Traditional Chinese, we do not 
want to keep two different sets of dictionaries for 
Simplified and Traditional Chinese. Even if two 
versions are kept, it would not serve well for text 
in mixed mode. For example, Traditional Chinese 
word of ?the day after tomorrow? should be , 
and for Simplified Chinese, it should be . 
However sometimes we can see the word  
appears in a Traditional Chinese text. We cannot 
say that it is wrong because the sentence is still 
semantically correct especially in Unicode 
environment. Therefore the segmentor should be 
able to segment those words correctly such as in 
the examples: ? ?, and in ?  
?. We must also deal with dictionary 
maintenance related to Chinese variants. For 
example, characters  are variants, so are 
. 
Data manager Segmentor 
Pre-
Processor 
Kernel 
Post 
Processor 
New Word
Extractor
Knowledge-
base 
  
In order to keep the dictionary maintenance simple, 
our system uses a single dictionary which only 
keeps the so called canonical form of a word. In 
our system, the canonical form of a word is its 
?simplified form?.  We quoted the word 
?simplified? because only certain characters have 
simplified forms such as  to , but for  
, there is no simplified form. In the case of 
variants, we simply choose one of them as the 
canonical character.  The canonical characters are 
maintained in the traditional-simplified character 
conversion table as well as in a variant table.  
Whenever a new word, item, is added into the 
dictionary, it must be added using a function 
CanonicalConversion(), which takes item as an 
input. During segmentation, the corresponding 
dictionary look up function will first convert the 
token to its canonical form before looking up in the 
dictionary.  
The personal name recognizers (separate for 
Chinese names and foreign names) use the 
maximum-likelihood algorithm with consideration 
of commonly used Chinese family names, given 
names, and foreign name characters. It works for 
Chinese names of length up to 5 characters. In the 
following examples you can see that our system 
successfully recognized the name . This 
is done using our algorithm, not by putting her 
name in our dictionary: 
 
Organization names and place names are 
recognized mainly using special purpose 
dictionaries. The segmentor uses tagging 
information to help resolve ambiguity. The 
disambiguation is mostly based on rules such as  
p + (n + f) -> p + n + f 
which would word to correct 
  
For efficiency reasons, our system uses only about 
20 rules. The system is flexible enough for new 
rules to be added to improve performance.  
The new word extractor is an accessory program to 
extract new words from running text based on 
statistical data which can either be grabbed from 
the internet or collected from other sources. The 
basic statistical data include bi-gram frequency, tri-
gram frequency, and quadra-gram frequencies. In 
order to further example whether a bi-gram, say  
, is indeed a word, we further collect forward 
conditional frequency of  , and 
the back-ward conditional frequency of , 
. For an i-gram token, we also 
use the (i+1)-gram statistics to eliminate those i-
grams that are only a part of (i+1) ? gram word.  
For instance, if the frequency of bi-gram  is 
very close to the frequency of tri-gram , it 
is less likely that  is a word. Of course, 
whether  is a word depends on quadra-gram 
results.  Using the statistical result, a set of rules 
was applied to these i-grams to eliminate entries 
that are not considered new words. Minimal 
manual work is required to identify whether the 
remaining candidates are new words. Before words 
are added into the dictionary, part-of-speech 
information are added manually (although not 
necessary) before using the canonical function. 
The following table shows examples of bi-grams 
which are found by the new word extractor using 
one year Hong Kong Commercial Daily News data. 
 
 
 
4 Performance Evaluation 
The valuation metrics used in [6] were adopted 
here. 
1
3
N
N
recall =     (1) 
 
2
3
N
Npresicion =     (2) 
  
precisionrecall
precisionrecallrecallprecisionF +
??= 2),(1   (3) 
where N  1  denotes the number of words in the 
annotated corpus, N 2 denotes the number of words 
identified by the segmentation algorithm , and N 3 is 
the number of words correctly identified. 
We participated in the open tests for all four 
corpora. The results are shown in the following 
table. 
The worst performance in the 4 tests were for the 
CTB(UPenn) data. From the observation from the 
testing data, we found that the main problem with 
have with CTB data is the difference in word 
granularity. To confirm our observation, we have 
done an analysis of combining errors and 
overlapping errors. The results show that the ratios 
of combining errors in all the error types are 
0.8425(AS), 0.87684(CTB), 0.82085(HK), and 
0.77102(PK). The biggest problem we have with 
AS data, on the other hand is due to out of 
vocabulary mistakes. Even though our new word 
extractor can help us to reduce this problem, but 
we have not trained our system using data from 
Taiwan.  Our best performance was on PK data 
because we used a very similar dictionary. The 
additional training of data for HK was done using 
one year Commercial Daily( ). 
The following table summarizes the execution 
speed of our program for the 4 different 
sources: 
Data No. of 
chars 
Processi
ng Time 
(sec.) 
Processin
g Rate 
(char/sec) 
Segmentat
ion Rate 
(char/sec) 
AS 18,743 4.703 3,985 7,641 
CTB 62,332 10.110 6,165 7,930 
HK 57,432 10.329 5,560 7,109 
PK 28,458 4.829 5,893 10,970 
 
The program initialization needs around 2.25 
seconds mainly to load the dictionaries and other 
data into the memory before the segmentation can 
start. If we only count the segmentation time, the 
rate of segmentation on the average is around 
7,500 characters for the first three corpora. It 
seems that the processing speed for Peking U. data 
is faster. This may be because the dictionaries we 
used are closer to the PK system, thus it would 
take less time to work on disambiguation.  
5 Conclusion 
In this paper, design and algorithms of a general-
purposed Unicode based segmentor is proposed. It 
is able to process Simplified and Traditional 
Chinese appear in the same text. Sophisticated pre-
processing and other auxiliary modules help 
segmenting text more accurately. User interactions 
and modules can be easily added with the help of 
its modular design. A built-in new word extractor 
is also implemented for extracting new words from 
running text. It saves much time on training and 
thus it can be quickly adapted to new environments. 
Acknowledgement 
We thank the PI of ITF Grant by ITC of 
HKSARG (ITS/024/01) entitled: Towards Cost-
Effective E-business in the News Media & 
Publishing Industry for the use of HK Commercial 
Daily. 
References 
[1] Automatic Segmentation and Tagging for Chinese 
Text ( ) , K.Y. Liu, 
Commercial Press, 2000 
[2] Segmentation Issues in Chinese Information 
Processing,  (C.N. Huang Issue No. 
1, 1997) 
[3] The design and Implementation of a Modern General 
Purpose Segmentation System (B. Lou,  R. Song, W.L. 
Li, and Z.Y. Luo, Journal of Chinese Information 
Processing, Issue No. 5, 2001) 
[4] (Institute of 
Computational Linguistics, Peking Univ., 2002) 
[5] 
  Journal of Chinese information 
processing vol. 14, no. 1, 2001) 
[6] Chinese Word Segmentation and Information 
Retrieval, Palmer D., and Burger J., In AAAI 
Symposium Cross-Language Text and Speech 
Retrieval 1997 
