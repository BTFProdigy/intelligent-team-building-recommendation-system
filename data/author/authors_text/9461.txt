Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 591?599,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Seeded Discovery of Base Relations in Large Corpora
Nicholas Andrews
BBN Technologies?
noa@bbn.com
Naren Ramakrishnan
Virginia Tech
naren@cs.vt.edu
Abstract
Relationship discovery is the task of iden-
tifying salient relationships between named
entities in text. We propose novel approaches
for two sub-tasks of the problem: identifying
the entities of interest, and partitioning
and describing the relations based on their
semantics. In particular, we show that term
frequency patterns can be used effectively
instead of supervised NER, and that the p-
median clustering objective function naturally
uncovers relation exemplars appropriate for
describing the partitioning. Furthermore, we
introduce a novel application of relationship
discovery: the unsupervised identification of
protein-protein interaction phrases.
1 Introduction
Relationship extraction (RE) is the task of extracting
named relationships between entities in text given
some information about the relationships of interest.
Relationship discovery (RD), on the other hand, is
the task of finding which relations exist in a corpus
without any prior knowledge. The discovered rela-
tionships can then be used to bootstrap RE, which is
why RD has also been called unsupervised relation
extraction (Rosenfeld and Feldman, 2006). RD gen-
erally involves three sub-tasks: entities of interest
are either supplied or recognized in the corpus; sec-
ond, of all phrases in which entities co-occur, those
which express a relation are picked out; finally, these
relationship phrases are partitioned based on their
semantics and described. This work considers only
binary relations (those between exactly two entities).
Finding entities of interest has involved either
named entity recognition (NER) or general noun
?This work was conducted while author was at Virginia
Tech.
phrase (NP) chunking, to create the initial pool
of candidate entities. In Section 2, we describe a
corpus statistics approach, previously applied for
web mining (Davidov and Rappoport, 2006), which
we extend for relation discovery. Unlike supervised
machine learning methods, this algorithm does
not need training, is computationally efficient, and
requires as input only the raw corpus and a small set
of seed entities (as few as two). The result is a set
of entities likely to be related to the seeds.
An assumption commonly held in RD work is
that frequently co-occurring entity tuples are likely
to stand in some fixed relation (Hasegawa et al,
2004; Shinyama and Sekine, 2006; Rosenfeld and
Feldman, 2006; Rosenfeld and Feldman, 2007).
Tuples which share similar contexts (the exact
definition of context varies) are then grouped
together in clusters of relations using variants of hi-
erarchical agglomerate clustering (HAC). However,
to our knowledge, no prior work has satisfactorily
addressed the problem of describing the resulting
clusters. In Section 3, we propose an approach
which incorporates this requirement directly into
the clustering objective: to find relation clusters
which are well-described by a single exemplar.
In Section 4, we apply RD to recognize protein-
protein interaction (PPI) sentences, using proteins
as seeds for the entity discovery phase. We compare
our results against special-purpose methods in terms
of precision and recall on standard data sets.
The remainder of this paper is outlined below:
Section 2 describes how a small number of input
words (the entities of interest) are used as seeds
for unsupervised entity discovery. Section 3 de-
scribes how discovered entities are used to discover
relationships. Section 4 describes evaluation
methodology and results. Section 5 describes
related work. Section 6 concludes and discusses
591
directions for future work.
2 Entity discovery
For a corpus C, each sentence s ? C with words
s = (w1, w2, ..., wn), is mapped to the sequence
s
?
= f(s). The function f maps each word w ? s
to a symbol based on its frequency in C as follows:
f(w) =
?
?
?
S if w is a seed word
H otherwise if w is a frequent word
X otherwise
For example, the sentence:
A and B are usually mediated by an
overproduced C.
might be mapped to the sequence
(S,H,X,H,H,X,H,H,X,X), which we will
write as SHXHHXHHXX for brevity. In this
case, A is a seed term, while B and C are not. The
underlying assumption is that content words can
be distinguished from other words based on their
frequency in the corpus.
2.1 Pattern induction
In the example sentence, ?A and B are usually
mediated by an overproduced C?, ?and? is a good
indicator that A,B share some aspect of their
semantics; in this case, that they are both me-
diated by an overproduced C, and are therefore
also likely to belong to same family or type of
entities. The indicators ?and? and ?or? have together
been used to discover word categories in lexical
acquisition (Dorow et al, 2005). However, there
can be many other such indicators, many discourse
or corpus specific. To discover them, we use a
slightly modified version of the method presented
in (Davidov and Rappoport, 2006). In particular, in
this work we consider named entities of arbitrary
length (i.e., longer than a single token).
The corpus is searched for all instances of the
frequency pattern H1S1H2S2H3, for seed words
S1, S2, and pattern (H1, H2, H3). Of all these pat-
tern instances, we keep those which also appear as
H1S2H2S1H3. If seed words appear on either side
of the pattern, it is a good indication that the sym-
metric pattern expresses some sort of a conjunction,
often domain specific. This procedure is repeated for
variations of HSHSH with the goal of capturing
different forms of speech; for example, HSHSH
will capture ?; A , B and?, while HSHHSH will
capture ?; A but not B ,? and so on. We enforce that
frequent words appear before and after (i.e., sur-
round) the two seed words to ensure they are stand-
alone entities, and not part of a longer noun phrase.
For example, the phrase ?IFN-gamma mRNA and
IL-6 are? maps to XXHSH , and therefore ?mRNA?
would (correctly) not be added to the entity pool.
New entities are added to the initial set of seed
by matching symmetric patterns. If a seed word
S is found to occur with an infrequent word X in
any discovered symmetric pattern (as HSHXH or
HXHSH), then we add X to the pool of entities.
This process can be bootstrapped as needed.
2.2 Chunking
In Section 3.1, sentences in which entities co-occur
are clustered based on a measure of pairwise simi-
larity. The features used in this similarity calculation
are based on the surrounding or connecting words
in the sentence in which entities co-occur. To ensure
the context is not polluted with words which actually
belong the entity NP (such as ?IFN-gamma mRNA?)
rather than the context, we use frequency patterns
to search the corpus for common NP chunks.
In each sentence in which entities occur, we form
a candidate chunk by matching the regular expres-
sion HX?SX?H , which returns all content-words
X bracketing the entity S. Of all candidate chunks,
we keep those which occur frequently enough to
significantly affect the similarity calculations. The
remaining chunks are pruned based on the entropy
of the words appearing immediately before and after
the chunk in the corpus; if a given chunk appears
in a variety of contexts, it is more likely to express
a meaningful collocation (Shimohata et al, 1997).
Therefore, as an efficient filter on the candidate
chunks, we discard those which tend to occur in the
same contexts (where the context is H...H).
3 Identifying relation phrases
Once the pool of entities has been recognized in the
corpus, those which frequently co-occur are taken
as likely to stand in a relation. Order matters in that
S1..S2 is considered a different entity co-occurrence
(and therefore potential relation) than S2..S1.
The effect of the co-occurrence threshold on the
resulting relations is investigated in Section 4.
3.1 Clustering relation phrases
Partitioning the candidate relationships serves to
identify groups of differently expressed relation-
ships of similar semantics. The resulting clusters
should cover the most important relations in a cor-
pus between the entities of interest. The phrases in
592
each cluster are expected to capture most syntactic
variation in the expression of a given relationship.
Therefore, the largest clusters are well suited
as positive examples for training a relationship
extractor (Rosenfeld and Feldman, 2006).
We take the context of a co-occurring tuple to
be the terms connecting the two entities within
the sentence in which they appear, and call the
connecting terms a relation phrase (RP). Each RP is
treated separately in the similarity calculations and
the clustering. Relations are modeled using a vector
space model. Each relation is treated as a vector of
term frequencies (tf) weighted by tf ? idf. RPs are
preprocessed by filtering stopwords1. However, we
do not stem the remaining words, as suffixes can be
highly discriminative in determining the semantics
of a relation (e.g., ?production? vs ?produced?). Af-
ter normalizing vectors to unit length, we compute a
similarity matrix by computing the dot product be-
tween the vectors for each distinct RP pair. The sim-
ilarity matrix is then used as input for the clustering.
3.2 p-Median clustering
Prior approaches to relationship discovery have
used HAC to identify relation clusters. HAC is
attractive in unsupervised applications since the
number of clusters is not required a priori, but
can be determined from the resulting dendogram.
On the other hand, a typical HAC implementation
runs in ?(N2 log(N)), which can be prohibitive on
larger data sets2.
A further feature of HAC, and many other par-
titional clustering algorithms such as k-means and
spectral cuts, is that the resulting clusters are not
necessarily well-described by single instance. Re-
lations, however, typically have a base or root form
which would be desirable to uncover to describe the
relation clusters. For example, in the following RPs:
induced transient increases in
induced biphasic increases in
induced an increase in
induced an increase in both
induced a further increase in
the phrase ?induced an increase in? is well suited
as a base form of the relation and a descriptor for
the cluster. The p-median clustering objective is to
find p clusters which are well-described by a single
1We use the English stopword list from the Snowball
project, available at http://snowball.tartarus.
org/
2An optimization to ?(N2) is possible for single-linkage
HAC.
exemplar. Formally, given an N ? N similarity
matrix, the goal is to select p columns such that the
sum of the maximum values within each row of the
selected columns are maximized.
Note that an exemplar can also be chosen a
posteriori using some heuristic; for example, the
most frequently occurring instance in a cluster can
be taken as the exemplar. However, the p-median
clustering objective is robust, and ensures that only
those clusters which are well described by a single
exemplar appear in the resulting partition of the
relations. This means that the optimal number of
clusters for the p-median clustering objective in a
given data set will usually be quite different (usually
higher) than the optimal number of groups according
to the HAC, k-means, or normalized cut objectives.
Affinity propagation (AP) is the most efficient
approximation for the p-median problem that we are
aware of, which also has the property of not requir-
ing the number of clusters as an explicit input (Frey
and Dueck, 2007). Runtime is linear in the number
of similarities, which in the worst case is N2 (for
N relations), but in practice many relations share
no words in common, and therefore do not need to
have their similarity considered in the clustering.
AP is an iterative message-passing procedure
in which the objects being clustered compete to
serve as cluster exemplars by exchanging two types
of messages. The responsibility r(x,m), sent
from object x ? X (for set X of objects to be
clustered) to candidate exemplar m ? X , denotes
how well-suited m is of being the exemplar for x by
considering all other potential exemplars m? of x:
s(x,m)? max
m??X ,m? 6=m
a(x,m
?
) + s(x,m
?
)
where s(x,m) is the similarity between x,m. The
availability a(x,m) of each object x ? X is initially
set to zero. Availabilities, sent from candidate
exemplar m to object x, increase as evidence for m
to serve as the exemplar for x increases:
min
?
?
?
0, r(m,m) +
?
x??X ,x? 6?{x,m}
max{0, r(x?,m)}
?
?
?
Each object to be clustered is assigned an initial
preference of becoming a cluster exemplar. If there
are no a priori preferences for cluster exemplars, the
preferences are set to the median similarity (which
can be thought of as the ?knee? of the objective
function graph vs. number of clusters), and exem-
plars emerge from the message passing procedure.
However, shorter RP are more likely to contain base
593
forms of relations (because longer phrases likely
contain additional words specific to the sentence).
Therefore, we include a slight scaling factor in the
preferences, which assigns shorter RP higher initial
values (up to 1.5? the median similarity).
3.3 Pruning clusters
After clustering relation phrases with AP, we prune
the resulting partition by evaluating the number
of different relation instances appearing in each
cluster, as well as the entities involved. In our
experiments, we discard all clusters smaller than a
certain threshold, since we ultimately wish to use
the clustering to train RE, and small clusters do
not provide enough positive examples for training
(we investigate the effect of this threshold in Sec-
tion 4.2). We further assume that for a relationship
to be useful, a number of different entities should
stand in this relation. In particular, we inspect the
set of left and right arguments in the cluster, which
(in English) usually correspond to the subject and
object of the sentence. If a single entity constitutes
more than two thirds (23 ) of the left or right argu-
ments of a cluster, then this cluster is discarded from
the results. Our assumption is that these clusters
describe relations too specific to be useful.
4 Evaluation
RD systems are usually evaluated based on their re-
sults for a particular task such as RE (Rosenfeld and
Feldman, 2006), or by a manual inspection of their
results (Davidov et al, 2007; Rosenfeld and Feld-
man, 2007; Hasegawa et al, 2004), but we are not
aware of any which examines the effects of parame-
ters on performance exhaustively. In this section we
test several hypotheses of RD using data sets which
are already labeled for sentences which contain
entities of a particular type and in a fixed relation of
some kind. In particular, we adapt the output of the
discovery phase to identify phrases which express
PPIs. While this task is traditionally performed
using supervised algorithms such as support vector
machines (Erkan et al, 2007), we show that RD
is capable of achieving similar levels of precision
without any manually annotated training data.
4.1 Method
We construct a corpus of 87300 abstracts by query-
ing the PubMed database with the proteins shown in
Table 1. The 60 most frequent words are considered
definite non-entities; all remaining words are can-
didate entities. This corpus serves as input for the
Table 1: Proteins queried to create the evaluation corpus.
Seed entities (proteins)
c-cbl AmpC CD18 CD54 CD5
CD59 CK c-myc CNP DM
EBNA GSH IL-8 IL-1beta JNK1
p38 PABP PCNA PP1 PP2a
PPAR PSM TAT TNF-alpha TPO
relationship discovery. As seeds, we use the same
25 proteins used to query the database. Since all
seeds are proteins, we expect the entities discovered
to be proteins. The pattern induction found roughly
200 symmetric extraction patterns, which yield
4402 unique entities after 1 pass through the corpus.
Depending on the frequency of the seeds in the
corpus, more passes through the corpus might be
needed (bootstrapping with the discovered entities
after each pass). We retain all chunks that appear
at least 10 times in the corpus, yielding 3282
additional entities after entropy pruning.
A PPI denotes a broad class of bio-medical
relationships between two proteins. One example
of an interaction is where the two proteins bind
together to form a structural complex of cellular
machinery such as signal transduction machinery. A
second example is when one protein binds upstream
of the DNA sequence encoding a gene which en-
codes the second protein. A final example is when
proteins serve as enzymes catalyzing successive
steps of a biochemical reaction. More categories
of interactions are continually being catalogued
and hence unsupervised identification of PPIs is
important in biomedical text mining.
4.2 Experiment 1: PPI sentence identification
Method: To evaluate the performance of our sys-
tem, we measure how well the relationships discov-
ered compare with manually selected PPI sentences.
To do so, we follow the same procedure and data
sets used to evaluate semi-supervised classification
of PPI sentences (Erkan et al, 2007). The two data
sets are AIMED and CB, which have been marked
for protein entities and interaction phrases3.
For each sentence in which n proteins appear,
we build
(
n
2
)
phrases. Each phrase consists of
the words between each entity combination, and is
labeled as positive if it describes a PPI, or negative
otherwise. This results in 4026 phrases for the
3Available in preprocessed form at http://belabog.
si.umich.edu/biocreative
594
AIMED data set (951 positive, 3075 negative), and
4056 phrases for the CB data set (2202 positive,
1854 negative).
The output of the discovery phase is a clustering
of RPs. For purpose of this experiment, we ignore
the partition and treat the phrases in aggregate. A
phrase in the evaluation data set is classified as
positive (describing a PPI) if any substring of the
phrase matches an RP in our output. For example,
if the phrase is:
A significantly inhibited B
and the string ?inhibited? appears as a relation in
our output, then this phrase is marked positive.
Otherwise, the phrase is marked negative.
Performance is evaluated using standard metrics
of precision (P ), recall (R), and F-measure (F1),
defined as:
P =
TP
TP + FP
; R =
TP
TP + FN
where TP is the number of phrases correctly
identified as describing a PPI, FP is the number of
phrases incorrectly classified as describing a rela-
tion, and FN is the number of interaction phrases
(positives) marked negative. F1 is defined as:
F1 =
2PR
P + R
We calculate P , R, and F1 for three parameters
affecting which phrases are identified as expressing
a relation:
? the minimum co-occurrence threshold that con-
trols which entity tuples are kept as likely to stand
in some fixed relation
? the minimum cluster size that controls which
groups of relations are discarded
? the minimum RP length that controls the smallest
number of words appearing in relations
The threshold on the length of the relations can be
thought of as controlling the amount of contextual
information expressed. A single term relation
will be very general, while longer RPs express a
relation very specific to the context in which they
are written. The results are reported in Figures 1
through 6. Odd numbered figures use the AIMED
corpus; even numbered figures the CB corpus.
Results: Discarding clusters below a certain size
had no significant effect on precision. However, this
step is still necessary for bootstrapping RE, since
machine learning approaches require a sufficient
number of positive examples to train the extractor.
Table 2: Comparison with supervised methods?AIMED
corpus
Method P R F1
RD-F1 30.08 60.67 40.22
RD-P 55.17 5.04 9.25
(Yakushiji et al, 2005) 33.70 33.10 33.40
(Mitsumori et al, 2006) 54.20 42.60 47.70
(Erkan et al, 2007) 59.59 60.68 59.96
Table 3: Comparison with supervised methods?CB
corpus
Method P R F1
RD-F1 65.03 69.16 67.03
RD-P 86.27 2.00 3.91
(Erkan et al, 2007) 85.62 84.89 85.22
On the other hand, our results confirm the
observation that frequently co-occurring pairs of
entities are likely to stand in a fixed relation. On
the CB corpus, precision ranges from 0.63 to 0.86
for phrases between entities co-occurring at least
50 times. On the AIMED corpus, precision ranges
from 0.29 to 0.55 in the same threshold range.
The minimum phrase length had the most impact
on performance, which was particularly evident in
the CB corpus: this corpus reached perfect precision
discarding all RPs of fewer than 3 words. Lower
thresholds result in significantly more relations, at
the cost of precision.
The generally lower performance on the AIMED
corpus suggests that our training data (retrieved from
the seed proteins) provided less coverage for those
interactions than for the those in the CB corpus.
Table 2 and Table 3 compare our results at fixed
parameter settings with supervised approaches.
RD-F1 reports parameters which give highest recall
and RD-P highest precision. Specifically, both
RD-F1 and RD-P use a minimum RP length of
1, RD-F1 uses a co-occurrence threshold of 10,
and RD-P uses a co-occurrence threshold of 50.
As expected, RD alone does not match combined
precision and recall of state-of-the-art supervised
systems. However, we show better performance
than expected. RD-F1 outperforms the best results
of (Yakushiji et al, 2005). RD-P settings out-
perform or match the precision of top-performing
systems on both datasets.
595
AIMED corpus CB corpus
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
5 10 15 20 25
Cluster size threshold
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
5 10 15 20 25
Cluster size threshold
Precision
Recall
F-Measure
Figures 1 & 2: Performance as minimum cluster size is adjusted
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
10 20 30 40 50
Co-occurence threshold
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
10 20 30 40 50
Co-occurence threshold
Precision
Recall
F-Measure
Figures 3 & 4: Performance as co-occurrence threshold is adjusted
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
0 0.5 1 1.5 2 2.5 3
Minimum phrase length
Precision
Recall
F-Measure
0
0.2
0.4
0.6
0.8
1
R
a
t
i
o
R
a
t
i
o
0 0.5 1 1.5 2 2.5 3
Minimum phrase length
Precision
Recall
F-Measure
Figures 5 & 6: Performance as minimum phrase length is adjusted
596
4.3 Experiment 2: clustering relations
Method: We evaluate the appropriateness of the
p-median clustering as follows. For each cluster,
we take the cluster exemplar as defining the base
relation. If the base relation does not express
something meaningful, then we mark each mem-
ber of the cluster incorrect. Otherwise, we label
each member of the cluster either as semantically
similar to the exemplar (correct) or different than
the exemplar (incorrect). Thus, clusters with
inappropriate exemplars are heavily penalized.
These results are reported in Table 4. For purpose
of this experiment, we use the same parameters
as for RD-P , and evaluate the 20 largest clusters.
Results: In the 20 largest clusters, each cluster ex-
emplar expressed something meaningful. 3 of the
cluster exemplars were not representative of their
other members. We found that most error was due to
stopwords not being considered in our similarity cal-
culations. For example, ?detected by? and ?detected
in? express the same relationship in our similarity
calculations; however, they are clearly quite differ-
ent. Another source of error evident in Table 4 are
mistakes in the pattern and entropy based chunking.
The exemplar ?mrna expression in? includes the to-
ken ?mrna?, which belongs with the left protein NP
in the relation chosen as an exemplar.
5 Related work
RD is a relatively new area of research. Existing
methods differ primarily in the amount of super-
vision required and in how contextual features are
defined and used.
(Hasegawa et al, 2004) use NER to identify
frequently co-occurring entities as likely relation
phrases. As in this work, they use the vector model
and cosine similarity to define a measure of simi-
larity between relations, but build relation vectors
out of all instances of each frequently co-occurring
entity pair. Therefore, each mention of the same
co-occurring pair is assumed to express the same
relationship. These aggregate feature vectors are
clustered using complete-linkage HAC, and cluster
exemplars are determined by manual inspection
for evaluation purposes. (Shinyama and Sekine,
2006) rely further on supervised methods, defining
features over a full syntactic parse, and exploit
multiple descriptions of the same event in newswire
to identify useful relations.
(Rosenfeld and Feldman, 2006) consider the use
of RD for unsupervised relation extraction, and use
Table 4: Base relations identified using RP-P parameters
Exemplar Size P (%)
by activation of 33 87.9
was associated with 28 92.9
was induced by 24 83.3
was detected by 24 83.3
as compared with the 25 92.0
were measured with 23 87.0
mrna expression in 21 9.5
in response to 21 95.23
was determined by 21 90.4
with its effect in 19 10.5
was correlated with 18 100.0
by induction of 16 93.8
for binding to 16 75.0
is mediated by 16 93.8
was observed by 16 50.0
is an important 15 66.6
increased expression of 15 60.0
related to the 15 93.3
protein production as well as 15 33.3
dependent on 14 85.7
Median precision: 86.35
a more complex pattern-learning approach to define
feature vectors to cluster candidate relations, report-
ing gains in accuracy compared with the tf ? idf
weighed features used in (Hasegawa et al, 2004)
and in this work. They also use HAC, and do not
address the description of the relations. Arbitrary
noun phrases obtained through shallow parsing are
used as entities. (Rosenfeld and Feldman, 2007) use
a feature ranking scheme using separability-based
scores, and compare the performance of different
variants of HAC (finding single-linkage to perform
best). The complexity of the feature ranking-scheme
described can be greater than the clustering itself; in
contrast, while we use simple features, our approach
is much more efficient.
(Davidov et al, 2007) introduce the use of
term frequency patterns for relationship discovery.
However, they search for a specific type of relation-
ship; namely, attributes common to all entities of
a particular type (for example, all countries have
the attribute capital), and use a special purpose
set of filters rather than entity co-occurrence and
clustering. Our work can be seen as a generalization
of theirs to relationships of any kind, and we extend
the use of frequency patterns to finding general
n-gram entities rather than single word entities.
(Madkour et al, 2007) give an excellent overview
597
of biomedical NER and RE. They propose a statis-
tical system for RE, but rely on NER, POS tagging,
and the creation of a dictionary for each domain of
application. Also, they do not cluster relationships
into semantically related groups.
6 Conclusion
Our work makes a series of important improvements
to the state-of-the-art in relationship discovery.
First, by incorporating entity discovery into the rela-
tionship discovery pipeline, our method does not re-
quire distinct training phases to accommodate differ-
ent entity types, relations, or discourse types. Sec-
ond, p-median clustering effectively uncovers the
base form of relations present in the corpus, address-
ing an important limitation in usability. In terms of
specific hypotheses, we have tested and confirmed
that co-occurrence can be a good indicator of the
presence of a relationship but the size of a cluster
is not necessarily a good indicator of the importance
or strength of the discovered relationship. Further-
more, we have shown that longer RPs with more
context give higher precision (at the cost of reduced
coverage). Finally, the integration of ideas in our
approach?unsupervisedness, efficiency, flexibility
(in application), and specificity?is novel in itself.
In future work, we seek to expand upon our RD
methods in three directions. First, we would like
to generalize the scope of our discovery pipeline
beyond binary relations and with richer considera-
tions of context, even across sentences. Second, we
hope to achieve greater tunability of performance,
to account for additional discovery metrics besides
precision. Finally, we intend to induce entire con-
cept maps from text using the discovered relations
to bootstrap an RE phase, where the underlying
problem is not just of inferring multiple types of
relations, but to have sufficient co-ordination among
the discovered relations to ensure connectedness
among the resulting concepts.
While our method requires no supervision in the
form of manually annotated entities or relations,
the effectiveness of the system relies on the careful
tuning of a number of parameters. Nevertheless,
the results reported in Section 4.2 suggest that the
two parameters that most significantly affect perfor-
mance exhibit predictable precision/recall behavior.
Of the parameters not considered in Section 4.2,
we would like to further investigate the benefits of
chunking entities on the resulting base relations, ex-
perimenting with different measures of collocation.
Acknowledgements
We would like to thank our anonymous reviewers
for their thought-provoking questions. This work
was supported in part by the Institute for Critical
Technology and Applied Science (ICTAS), Virginia
Tech.
References
Dmitry Davidov and Ari Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In ACL
?06: Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the ACL, pages 297?304, Morristown, NJ,
USA. Association for Computational Linguistics.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 232?239, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, and Elisha Moses.
2005. Using curvature and markov clustering in
graphs for lexical acquisition and word sense discrim-
ination. In MEANING 05: 2nd workshop organized
by the MEANING Project, Trento, Italy, February.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting
protein interaction sentences using dependency pars-
ing. In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 228?237.
Brendan J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science,
315:972?976.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In ACL ?04: Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, page 415, Morristown, NJ, USA.
Association for Computational Linguistics.
Amgad Madkour, Kareem Darwish, Hany Hassan,
Ahmed Hassan, and Ossama Emam. 2007. Bionoc-
ulars: Extracting protein-protein interactions from
biomedical text. In Biological, translational, and
clinical language processing, pages 89?96, Prague,
Czech Republic, June. Association for Computational
Linguistics.
598
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with svm. IEICE Transac-
tions on Information and Systems, 89(8):2464?2466.
Benjamin Rosenfeld and Ronen Feldman. 2006. High-
performance unsupervised relation extraction from
large corpora. In ICDM ?06: Proceedings of the
Sixth International Conference on Data Mining, pages
1032?1037, Washington, DC, USA. IEEE Computer
Society.
Benjamin Rosenfeld and Ronen Feldman. 2007. Cluster-
ing for unsupervised relation identification. In CIKM
?07: Proceedings of the sixteenth ACM conference on
Conference on information and knowledge manage-
ment, pages 411?418, New York, NY, USA. ACM.
Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata.
1997. Retrieving collocations by co-occurrences and
word order constraints. In In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 476?481.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation
discovery. In Proceedings of the Human Language
Technology Conference of the NAACL, Main Con-
ference, pages 304?311, New York City, USA, June.
Association for Computational Linguistics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In Proceedings of the
eleventh annual meeting of the association for natural
language processing, pages 93?96.
599
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 344?355, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Name Phylogeny: A Generative Model of String Variation
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Many linguistic and textual processes involve transduc-
tion of strings. We show how to learn a stochastic trans-
ducer from an unorganized collection of strings (rather
than string pairs). The role of the transducer is to orga-
nize the collection. Our generative model explains simi-
larities among the strings by supposing that some strings
in the collection were not generated ab initio, but were in-
stead derived by transduction from other, ?similar? strings
in the collection. Our variational EM learning algorithm
alternately reestimates this phylogeny and the transducer
parameters. The final learned transducer can quickly link
any test name into the final phylogeny, thereby locating
variants of the test name. We find that our method can
effectively find name variants in a corpus of web strings
used to refer to persons inWikipedia, improving over stan-
dard untrained distances such as Jaro-Winkler and Leven-
shtein distance.
1 Introduction
Systematic relationships between pairs of strings
are at the core of problems such as transliteration
(Knight and Graehl, 1998), morphology (Dreyer and
Eisner, 2011), cross-document coreference resolu-
tion (Bagga and Baldwin, 1998), canonicalization
(Culotta et al2007), and paraphrasing (Barzilay and
Lee, 2003). Stochastic transducers such as proba-
bilistic finite-state transducers are often used to cap-
ture such relationships. They model a conditional
distribution p(y | x), and are ordinarily trained on
input-output pairs of strings (Dreyer et al2008).
In this paper, we are interested in learning from
an unorganized collection of strings, some of which
might have been derived from others by transforma-
tive linguistic processes such as abbreviation, mor-
phological derivation, historical sound or spelling
change, loanword formation, translation, transliter-
ation, editing, or transcription error. We assume that
each string was derived from at most one parent, but
may give rise to any number of children.
The difficulty is that most or all of these parent-
child relationships are unobserved. We must recon-
struct this evolutionary phylogeny. At the same time,
we must fit the parameters of a model of the relevant
linguistic process p(y | x), which says what sort of
children y might plausibly be derived from parent x.
Learning this model of p(y | x) helps us organize the
training collection by reconstructing its phylogeny,
and also permits us to generalize to new forms.
We will focus on the problem of name varia-
tion. We observe a collection of person names?full
names, nicknames, abbreviated or misspelled names,
etc. Some of these names can refer to the same per-
son; we hope to detect this. It would be an unlikely
coincidence if two mentions of John Jacob Jingle-
heimer Schmidt referred to different people, since
this is a long and unusual name. Similarly, John Ja-
cob Jingelhimer Smith andDr. J. J. Jingleheimermay
also be related names for this person. That is, these
names may be derived from one another, via unseen
relationships, although we cannot be sure.
Readers may be reminded of unsupervised clus-
tering, in which ?suspiciously similar? points can be
explained as having been generated by the same clus-
ter. Since each name is linked to at most one parent,
our setting resembles single-link clustering?with a
learned, asymmetric distance measure p(y | x).
We will propose a generative process that makes
explicit assumptions about how strings are copied
with mutation. It is assumed to have generated all the
names in the collection, in an unknown order. Given
learned parameters, we can ask the model whether a
name Dr. J. J. Jingelheimer in the collection is more
likely to have been generated from scratch, or derived
from some previous name.
1.1 Related Work
Several previous papers have also considered learn-
ing transducers or other models of word pairs when
344
the pairing between inputs and outputs is not given.
Most commonly, one observes parallel or compa-
rable corpora in two languages, and must recon-
struct a matching from one language?s words to the
other?s before training on the resulting pairs (Schafer,
2006b; Klementiev and Roth, 2006; Haghighi et al
2008; Snyder et al2010; Sajjad et al2011).
Hall and Klein (2010) extend this setting to more
than two languages, where the phylogenetic tree is
known. A given lexeme (abstract word) can be re-
alized in each language by at most one word (string
type), derived from the parent language?s realization
of the same lexeme. The system must match words
that share an underlying lexeme (i.e., cognates), cre-
ating a matching of each language?s vocabulary to its
parent language?s vocabulary. A further challenge is
that the parent words are unobserved ancestral forms.
Similarly, Dreyer and Eisner (2011) organize
words into morphological paradigms of a given
structure. Again words with the same underlying lex-
eme (i.e., morphemes) must be identified. A lexeme
can be realized in each grammatical inflection (such
as ?first person plural present?) by exactly one word
type, related to other inflected forms of the same lex-
eme, which as above may be unobserved. Their in-
ference setting is closer to ours because the input is
an unorganized collection of words?input words are
not tagged with their grammatical inflections. This
contrasts with the usual multilingual setting where
each word is tagged with its true language.
In one way, our problem differs significantly from
the above problems. We are interested in random
variation that may occur within a language as well
as across languages. A person name may have un-
boundedly many different variants. This is unlike
the above problems, in which a lexeme has at most
K realizations, where K is the (small) number of
languages or inflections.1 We cannot assign the ob-
served strings to positions in an existing structure
that is shared across all lexemes, such as a given phy-
logenetic tree whose K nodes represent languages,
or a given inflectional grid whose K cells represent
grammatical inflections. Rather, we must organize
1In the above problems, one learns a set ofO(K) orO(K2)
specialized transducers that relate Latin to Italian, singular to
plural, etc. We instead use one global mutation model that ap-
plies to all names?but see footnote 14 on incorporating special-
ized transductions (Latin to Italian) within our mutation model.
them into a idiosyncratic phylogenetic tree whose
nodes are the string types or tokens themselves.
Names and words are not the only non-biological
objects that are copied with mutation. Documents,
database records, bibliographic entries, code, and
images can evolve in the same way. Reconstructing
these relationships has been considered by a number
of papers on authorship attribution, near-duplicate
detection, deduplication, record linkage, and plagia-
rism detection. A few such papers reconstruct a phy-
logeny, as in the case of chain letters (Bennett et
al., 2003), malware (Karim et al2005), or images
(Dias et al2012). In fact, the last of these uses the
same minimum spanning tree method that we apply
in ?5.3. However, these papers do not train a similar-
ity measure as we do. To our knowledge, these two
techniques have not been combined outside biology.
In molecular evolutionary analysis, phylogenetic
techniques have often been combined with estima-
tion of some parametric model of mutation (Tamura
et al2011). However, names mutate differently
from biological sequences, and our mutation model
for names (?4, ?8) reflects that. We also posit a spe-
cific process (?3) that generates the name phylogeny.
2 An Example
A fragment of a phylogeny for person names is
shown in Figure 1. Our procedure learned this auto-
matically from a collection of name tokens, without
observing any input/output pairs. The nodes of the
phylogeny are the observed name types,2 each one
associated with a count of observed tokens.
Each arrow corresponds to a hypothesized mu-
tation. These mutations reflect linguistic processes
such as misspelling, initialism, nicknaming, translit-
eration, etc. As an exception, however, each ar-
row from the distinguished root node ? generates
an initial name for a new entity. The descendants of
this initial name are other names that subsequently
evolved for that entity. Thus, the child subtrees of ?
give a partition of the name types into entities.
Thanks to the phylogeny, the seemingly disparate
names Ghareeb Nawaz and Muinuddin Chishti are
seen to refer to the same entity. They may be traced
back to their common ancestor Khawaja Gharib-
2We cannot currently hypothesize unobserved intermediate
forms, e.g., common ancestors of similar strings. See ?6.2.
345
Khawaja Gharibnawaz Muinuddin Hasan Chisty
Khwaja Gharib NawazKhwaja Muin al-Din ChishtiGhareeb Nawaz Khwaja Moinuddin ChishtiKhwaja gharibnawaz Muinuddin Chishti
Thomas Ruggles Pynchon, Jr.Thomas Ruggles Pynchon Jr.Thomas R. Pynchon, Jr.Thomas R. Pynchon Jr.Thomas R. Pynchon Thomas Pynchon, Jr.Thomas Pynchon Jr.
Figure 1: A portion of a spanning tree found by our model.
nawaz Muinuddin Hasan Chisty, from which both
were derived via successive mutations.
Not shown in Figure 1 is our learned family p of
conditional probability distributions, which models
the likely mutations in this corpus. Our EM learn-
ing procedure found p jointly with the phylogeny.
Specifically, it alternated between improving p and
improving the distribution over phylogenies. At the
end, we extracted the single best phylogeny.
Together, the learned p and the phylogeny in Fig-
ure 1 form an explanation of the observed collection
of names. What makes it more probable than other
explanations? Informally, two properties:
? Each node in the tree is plausibly derived from
its parent. More precisely, the product of
the edge probabilities under p is comparatively
high. A different p would have reduced the
probability of the events in this phylogeny. A
different phylogeny would have involved a more
improbable collection of events, such as replac-
ing Chishti with Pynchon, or generating many
unrelated copies of Pynchon directly from ?.
? In the phylogeny, the parent names tend to be
used often enough that it is plausible for variants
of these names to have emerged. Our model
says that new tokens are derived from previ-
ously generated tokens. Thus?other things
equal?Barack Obama is more plausibly a vari-
ant of Barack Obama, Jr. than of Barack
Obama, Sr. (which has fewer tokens).
3 A Generative Model of Tokens
Our model should reflect the reasons that name vari-
ation exists. A named entity has the form y = (e, w)
where w is a string being used to refer to entity e. A
single entity e may be referred to on different occa-
sions by different name strings w. We suppose that
this is the result of copying the entity with occasional
mutation of its name (as in asexual reproduction).
Thus, we assume the following simple generative
process that produces an ordered sequence of tokens
y1, y2, . . ., where yi = (ei, wi).
? After the first k tokens y1, . . . yk have been gen-
erated, the author responsible for generating yk+1
must choose whom to talk about next. She is likely
to think of someone she has heard about often in the
past. So to make this choice, she selects one of the
previous tokens yi uniformly at random, each having
probability 1/(k + ?); or else she selects ?, with
probability ?/(k + ?).
? If the author selected a previous token yi, then
with probability 1 ? ? she copies it faithfully, so
yk+1 = yi. But with probability ?, she instead draws
a mutated token yk+1 = (ek+1, wk+1) from the mu-
tation model p(? | yi). This preserves the entity
(ek+1 = ei with probability 1), but the new name
wk+1 is a stochastic transduction of wi drawn from
p(? | wi).3 For example, in referring to ei, the author
may shorten and respellwi = Khwaja Gharib Nawaz
into wk+1 = Ghareeb Nawaz (Figure 1).
? If the author selected?, she must choose a fresh
entity yk+1 = (ek+1, wk+1) to talk about. So she
sets ek+1 to a newly created entity, sampling its name
wk+1 from the distribution p(? | ?). For example,
wk+1 = Thomas Ruggles Pynchon, Jr. (Figure 1).
Nothing prevents wk+1 from being a name that is al-
ready in use for another entity (i.e., wk+1 may equal
wj for some j ? k).
3Straightforward extensions are to allow a variable mutation
rate ?(yi) that depends on properties of yi, and to allow wk+1
to depend on known properties of ei. See footnote 14 for further
discussion of enriched tokens.
346
3.1 Relationship to other models
If we ignore the name strings, we can see that the
sequence of entities e1, e2, . . . eN is being generated
from a Chinese restaurant process (CRP) with con-
centration parameter ?. To the extent that ? is low
(so that  is rarely used), a few randomly chosen en-
tities will dominate the corpus.
The CRP is equivalent to sampling e1, e2, . . . IID
from an unknown distribution that was itself drawn
from a Dirichlet process with concentration ?. This
is indeed a standard model of a distribution over en-
tities. For example, Hall et al2008) use it to model
venues in bibliographic entries.
From this characterization of the CRP, one can see
that any permutation of this entity sequence would
have the same probability. That is, our distribution
over sequences of entities e is exchangeable.
However, our distribution over sequences of
named entities y = (e, w) is non-exchangeable.
It assigns different probabilities to different order-
ings of the same tokens. This is because our model
posits that later authors are influenced by earlier au-
thors, copying entity names from them with muta-
tion. So ordering is important. The mutation process
is not symmetric?for example, Figure 1 reflects a
tendency to shorten rather than lengthen names.
Non-exchangeability is one way that our present
model differs from (parametric) transformationmod-
els (Eisner, 2002) and (non-parametric) transforma-
tion processes (Andrews and Eisner, 2011). These
too are defined using mutation of strings or other
types. From a transformation process, one can draw
a distribution over types, from which the tokens are
then sampled IID. This results in an exchangeable
sequence of tokens, just as in the Dirichlet process.
We avoid transformation models here for three
reasons. (1) Inference is more expensive. (2) A
transformation process seems less realistic as a
model of authorship. It constructs a distribution over
derivational paths, similar to the paths in Figure 1.
It effectively says that each token is generated by re-
capitulating some previously used path from ?, but
with some chance of deviating at each step. For an
author to generate a name token this way, she would
have to know the whole derivational history of the
previous name she was adapting. Our present model
instead allows an author simply to select a name she
previously saw and copy or mutate its surface form.
(3) One should presumably prefer to explain a novel
name y as a mutation of a frequent name x, other
things equal (?2). But surprisingly, inference under
the transformation process does not prefer this.4
Another view of our present model comes from
the literature on random graphs (e.g., for modeling
social networks or the link structure of the web). In
a preferential attachment model, a graph?s vertices
are added one by one, and each vertex selects some
previous vertices as its neighbors. Our phylogeny
is a preferential attachment tree, a random directed
graph in which each vertex selects a single previous
vertex as its parent. Specifically, it is a random recur-
sive tree (Smythe and Mahmoud, 1995) whose ver-
tices are the tokens.5 To this simple random topol-
ogy we have added a random labeling process with
mutation. The first ? vertices are labeled with ?.
4 A Mutation Model for Strings
Our model in ?3 samples the next token y, when it is
not simply a faithful copy, from p(y | x) or p(y | ?).
The key step there is to sample the name string wy
from p(wy | wx) or p(wy | ?).
Our model of these distributions could easily in-
corporate detailed linguistic knowledge of the muta-
tion process (see ?8). Here we describe the specific
model that we use in our experiments. Like many
such models, it can be regarded as a stochastic finite-
state string-to-string transducer parameterized by ?.
There is much prior work on stochastic models of
edit distance (Ristad andYianilos, 1998; Bilenko and
Mooney, 2003; Oncina and Sebban, 2006; Schafer,
2006a; Bouchard-C?t? et al2008; Dreyer et al
2008, among others). For the present experiments,
we designed a moderately simple one that employs
(1) conditioning on one character of right context,
(2) latent ?edit? and ?no-edit? regions to capture the
fact that groups of edits are often made in close prox-
imity, and (3) some simple special handling for the
distribution conditioned on the root p(wy | ?).
We assume a stochastic mutation process which,
when given an input string wx, edits it from left to
4The very fact that x has been frequently observed demon-
strates that it has often chosen to stop mutating. This implies
that it is likely to choose stop again rather than mutate into y.
5This is not the tree shown in Figure 1, whose vertices are
types rather than tokens.
347
right into an output string wy. Then p(wy | wx) is
the total probability of all operation sequences onwx
that would produce wy. This total can be computed
in time O(|wx| ? |wy|) by dynamic programming.
Our process has four character-level edit opera-
tions: copy, substitute, insert, delete. It also has a
distinguished no-edit operation that behaves exactly
like copy. At each step, the process first randomly
chooses whether to edit or no-edit, conditioned only
on whether the previous operation was an edit. If it
chooses to edit, it chooses a random edit type with
some probability conditioned on the next input char-
acter. In the case of insert or substitute, it then ran-
domly chooses an output character, conditioned on
the type of edit and the next input character.
It is common to mutate a name by editing con-
tiguous substrings (e.g., words). Contiguous regions
of copying versus editing can be modeled by a low
probability of transitioning between no-edit and edit
regions.6 Note that an edit region may include some
copy edits (or substitute edits that replace a charac-
ter with itself) without leaving the edit region. This
is why we distinguish copy from no-edit.
Input and output strings are augmented with a
trailing eos (?end-of-string?) symbol that is seen by
the single-character lookahead. If the next character
is eos, the only available edit is insert. Alternatively,
if the process selects no-edit, then eos is copied to
the output string and the process terminates.
In the case of p(wy | ?), the input string is empty,
and both input and output are augmented with a trail-
ing eos? character that behaves like eos. Then wy
is generated by a sequence of insertions followed by
a copy. These are conditioned as usual on the next
character, here eos?, so the model can learn to insert
more or different characters when the input is ?.
The parameters ? determining the conditional
probabilities of the different operations and charac-
ters are estimated with backoff smoothing.
5 Inference
The input to inference is a collection of named entity
tokens y. Most are untagged tokens of the form y =
(?, w). In a semi-supervised setting, however, some
6This somewhat resembles the traditional affine gap penalty
in computational biology (Gusfield, 1997), which makes dele-
tions or insertions cheaper if they are consecutive. We instead
make consecutive edits cheaper regardless of the edit type.
of the tokens may be tagged tokens of the form y =
(e, w), whose true entity is known. The entity tags
place a constraint on the phylogeny, since each child
subtree of ? must correspond to exactly one entity.
5.1 An unrealistically supervised setting
Suppose we were lucky enough to fully observe the
sequence of named entity tokens yi = (ei, wi) pro-
duced by our generative model. That is, suppose all
tokens were tagged and we knew their ordering.
Yet there would still be something to infer: which
tokens were derived from which previous tokens.
This phylogeny is described by a spanning tree over
the tokens. Let us see how to infer it.
For each potential edge x ? y between named
entity tokens, define ?(y | x) to be the probability of
choosing x and copying it (possibly with mutation)
to obtain y. So
?(yj | ?) = ? p(yj | ?) (1)
?(yj | yi) = ? p(yj | yi) + (1? ?)1(yj = yi) (2)
except that if i ? j or if ei 6= ej , then ?(yj | yi) = 0
(since yj can only be derived from an earlier token
yi with the same entity).
Now the prior probability of generating y1, . . . yN
with a given phylogenetic tree is easily seen to be a
product over all tree edges,
?
j ?(yj | pa(yj)) where
pa(yj) is the parent of yj . As a result, it is known
that the following are efficient to compute from the
(N + 1)? (N + 1) matrix of ? values (see ?5.3):
(a) the max-probability spanning tree
(b) the total probability of all spanning trees
(c) the marginal probability of each edge, under the
posterior distribution on spanning trees
(a) is our single best guess of the phylogeny. We use
this during evaluation. (b) gives the model likeli-
hood, i.e., the total probability of the observed data
y1, . . . yN . To locally maximize the model likeli-
hood, (c) can serve as the E step of our EM algorithm
(?6) for tuning our mutation model. The M step then
retrains the mutation model?s parameters ? on input-
output pairs wi ? wj , weighting each pair by its
edge?s posterior marginal probability (c), since that
is the expected count of a wi ? wj mutation. This
computation is iterated.
348
5.2 The unsupervised setting
Now we turn to a real setting?fully unsupervised
data. Two issues will force us to use an approximate
inference algorithm. First, we have an untagged cor-
pus: a token?s entity tag e is never observed. Second,
the order of the tokens is not observed, so we do not
know which other tokens are candidate parents.
Our first approximation is to consider only phylo-
genies over types rather than tokens.7 The type phy-
logeny in Figure 1 represents a set of possible token
phylogenies. Each node of Figure 1 represents an
untagged name type y = (?, w). By grouping all ny
tokens of this type into a single node, we mean that
the first token of y was derived by mutation from the
parent node, while each later token of y was derived
by copying an (unspecified) earlier token of y.
A token phylogeny cannot be represented in this
way if two or more tokens of y were created by mu-
tations. In that case, their name strings are equal only
by coincidence. They may have different parents
(perhaps of different entities), whereas the y node in
a type phylogeny can have only one parent.
We argue, however, that these unrepresentable to-
ken phylogenies are comparatively unlikely a poste-
riori and can be reasonably ignored during inference.
The first token of y is necessarily amutation, but later
tokens are much more likely to be copies. The prob-
ability of generating a later token y by copying some
previous token is at least
(1? ?)/(N + ?),
while the probability of generating it in some other
way is at most
max(? p(y | ?), ? max
x?Y
p(y | x))
where Y is the set of observed types. The second
probability is typically much smaller: an author is
unlikely to invent exactly the observed string y, cer-
tainly from ? but even by mutating a similar string
x (especially when the mutation rate ? is small).
How do we evaluate a type phylogeny? Con-
sider the probability of generating untagged tokens
7Working over types improves the quality of our second ap-
proximation, and also speeds up the spanning tree algorithms.
?6 explains how to regard this approximation as variational EM.
y1, . . . yN in that order and respecting the phylogeny:
(
N?
k=1
1
k + ?
)
?
y?Y
g(y | pa(y))
?
?
ny?1?
i=1
i (1? ?)
?
?
(3)
where g(y | pa(y)) is a factor for generating the first
token of y from its parent pa(y), defined by
g(y | ?) = ? ? p(y | ?) (4)
g(y | x) = ? ? (# tokens of x preceding
first token of y) ? p(y | x) (5)
But we do not actually know the token order: by
assumption, our input corpus is only an unordered
bag of tokens. So we must treat the hidden order-
ing like any other hidden variable and maximize the
marginal likelihood, which sums (3) over all possi-
ble orderings (permutations). This sum can be re-
garded as the number of permutations N ! (which is
fixed given the corpus) times the expectation of (3)
for a permutation chosen uniformly at random.
This leads to our second approximation. We ap-
proximate this expectation of the product (3) with a
product of expectations of its individual factors.8 To
find the expectation of (5), observe that the expected
number of tokens of x that precede the first token of
y is nx/(ny+1), since each of the nx tokens of x has
a 1/(ny + 1) chance of falling before all ny tokens
of y. It follows that the approximated probability of
generating all tokens in some order, with our given
type parentage, is proportional to
?
y?Y
?(y | pa(y)) (6)
where
?(y | ?) = ? ? p(y | ?) (7)
?(y | x) = ? ? p(y | x) ? nx/(ny + 1) (8)
and the constant of proportionality depends on the
corpus.
The above equations are analogous to those in
?5.1. Again, the approximate posterior probability
of a given type parentage tree is edge-factored?it is
the product of individual edge weights defined by ?.
Thus, we are again eligible to use the spanning tree
algorithms in ?5.3 below.
8In general this is an overestimate for each phylogeny.
349
Notice that the ratio ?/? controls the preference
for an entity to descend from ? versus an existing
entity. Thus, by tuning this ratio, we can control
the number of entities inferred by our method, where
each entity corresponds to one of the child subtrees
of ?.
Also note that nx in the numerator of (8) means
that y?s parent is more likely to be frequent. Also,
ny +1 in the denominator means that a frequent y is
not as likely to have any parent x 6= ?, because its
first token probably falls early in the sequence where
there are fewer available parents x 6= ?.
5.3 Spanning tree algorithms
Define a complete directed graphG over the vertices
Y ? {?}. The weight of an edge x ? y is defined
by ?(y | x). The (approximate) posterior probability
of a given phylogeny given our evidence, is propor-
tional to the product of the ? values of its edges.
Formally, let T?(G) denote the set of spanning
trees of G rooted at ?, and define the weight of a
particular spanning tree T ? T?(G) to be the prod-
uct of the weights of its edges:
w(T ) =
?
(x?y)?T
?(y | x) (9)
Then the posterior probability of spanning tree T is
p?(T ) =
w(T )
Z(G)
(10)
where Z(G) =
?
T?T?(G)
w(T ) is the partition
function, i.e. the total probability of generating the
dataG via any spanning tree of the formwe consider.
This distribution is determined by the parameters ?
of the transducer p?, along with the ratio ?/?.
There exist several algorithms to find the sin-
glemaximum-probability spanning tree, notably Tar-
jan?s implementation of the Chu-Liu-Edmonds algo-
rithm, which runs in O(m log n) for a sparse graph
or O(n2) for a dense graph (Tarjan, 1977). Figure 1
shows a spanning tree found by our model using Tar-
jan?s algorithm. Here n is the number of vertices
(in our case, types and ), whilem is the number of
edges (which we can keep small by pruning, ?6.1).
6 Training the Transducer with EM
Our inference algorithm assumes that we know the
transducer parameters ?. We now explain how to op-
timize ? to maximize the marginal likelihood of the
training data. This marginal likelihood sums over all
the other latent variables in the model?the spanning
tree, the alignments between strings, and the hidden
token ordering.
The EMprocedure repeats the following until con-
vergence:
E-step: Given ?, compute the posterior marginal
probabilities cxy of all possible phylogeny
edges.
M-step Given all cxy, retrain ? to assign a high
conditional probability to the mutations on the
probable edges.
We actually use a variational EM algorithm: our
E step approximates the true distribution q over all
phylogenies with the closest distribution p that as-
signs positive probability only to type-based phylo-
genies. This distribution is given by (10) and min-
imizes KL(p || q). We argued in section ?5.2 that
it should be a good approximation. The posterior
marginal probability of a directed edge from vertex
x to vertex y, according to (10), is
cxy =
?
T?T?(G):(x?y)?T
p?(T ) (11)
The probability cxy is a ?pseudocount? for the ex-
pected number of mutations from x to y. This is at
most 1 under our assumptions.
Calculating cxy requires summing over all span-
ning trees of G, of which there are nn?2 for a fully
connected graph with n vertices. Fortunately, Tutte
(1984) shows how to compute this sum by the fol-
lowing method, which extends Kirchhoff?s classi-
cal matrix-tree theorem to weighted directed graphs.
This result has previously been employed in non-
projective dependency parsing (Koo et al2007;
Smith and Smith, 2007).
Let L ? Rn?n denote the Laplacian ofG, namely
L =
{ ?
x? ?(y | x
?) if x = y
??(y | x) if x 6= y
(12)
Tutte?s theorem relates the determinant of the Lapla-
cian to the spanning trees in graph G. In particular,
the cofactor L0,0 is equal to the sum of the weights
350
of all directed spanning trees rooted at 0, which (sup-
posing? is indexed at 0) yields the partition function
Z(G).
The edge marginals of interest are related to the
log partition function by
cxy =
?Z(G)
??(y | x)
(13)
which has the closed-form solution
cxy =
{
?(y | ?)L?1yy if x = y
?(y | x)(L?1xx ? L
?1
xy ) if x 6= y
(14)
Thus, the problem of computing edge marginals re-
duces to that of computing a matrix inverse, which
may be done in O(n3) time.
At the M step, we retrain the mutation model pa-
rameters ? to maximize
?
xy cxy log p(wy | wx).
This is tantamount to maximum conditional likeli-
hood training on a supervised collection of (wx, wy)
pairs that are respectively weighted by cxy.
The M step is nontrivial because the term p(wy |
wx) sums over a hidden alignment between two
strings. It may be performed by an inner loop of EM,
where the E step uses dynamic programming to ef-
ficiently consider all possible alignments, as in (Ris-
tad and Yianilos, 1996). In practice, we have found it
effective to take only a single step of this inner loop.
Such a Generalized EM procedure enjoys the same
convergence properties as EM, but may reach a local
optimum faster (Dempster et al1977).
6.1 Pruning the graph
For large graphs, it is essential to prune the number
of edges to avoid considering all n(n ? 1) input-
output pairs. To prune the graph, we eliminate all
edges between strings that do not share any common
trigrams (case- and diacritic-insensitive), by setting
their matrix entries to 0. As a result, the graph Lapla-
cian is a sparse matrix, which often allows faster
matrix inversion using preconditioned iterative algo-
rithms. Furthermore, pruned edges do not appear in
any spanning tree, so the E step will find that their
posterior marginal probabilities are 0. This means
that the input-output pairs corresponding to these
edges can be ignored when re-estimating the trans-
ducer parameters in the M step. We found that prun-
ing significantly improves training time with no ap-
preciable loss in performance.9
6.2 Training with unobserved tokens?
A deficiency of our method is that it assumes that
authors of our corpus have only been exposed to pre-
vious tokens in our corpus. In principle, one could
also train with U additional tokens (e, w) where we
observe neither e nor w, for very large U . This is the
?universe of discourse? in which our authors oper-
ate.10 In this case, we would need (expensive) new
algorithms to reconstruct the strings w. However,
this model could infer a more realistic phylogeny by
positing unobserved ancestral or intermediate forms
that relate the observed tokens, as in transformation
models (Eisner, 2002; Andrews and Eisner, 2011).
7 Experimental Evaluation
7.1 Data preparation
Scraping Wikipedia. Wikipedia documents many
variant names for entities. As a result, it has fre-
quently been used as a source for mining name vari-
ations, both within and across languages (Parton et
al., 2008; Cucerzan, 2007). We used Wikipedia to
create a list of name aliases for different entities.
Specifically, we mined English Wikipedia11 for all
redirects: page names that lead directly to another
page. Redirects are created by Wikipedia users for
resolving common name variants to the correct page.
For example, the pages titled Barack Obama Ju-
nior and Barack Hussein Obama automatically redi-
rect to the page titled Barack Obama. This redirec-
tion implies that the first two are name variants of
the third. Collecting all such links within English
Wikipedia yields a large number of aliases for each
page. However, many redirects are for topics other
than individual people, and these would be poor ex-
amples of name variation. In addition, some phrases
9For instance, on a dataset of approximately 6000 distinct
names, pruning reduced the number of outgoing edges at each
vertex to fewer than 100 per vertex.
10Notice that theN observed tokens would be approximately
exchangeable in this setting: they are unlikely to depend on one
another when N  U , and hence their order no longer matters
much. In effect, generating theU hidden tokens constructs a rich
distribution (analogous to a sample from the Dirichlet process)
from which the N observed tokens are then sampled IID.
11Using a Wikipedia dump from February 2, 2011.
351
Ho Chi Minh, Ho chi mihn, Ho-Chi Minh, Ho Chih-minh
Guy Fawkes, Guy fawkes, Guy faux, Guy Falks, Guy Faukes, Guy Fawks, Guy foxe, Guy Falkes
Nicholas II of Russia, Nikolai Aleksandrovich Romanov, Nicholas Alexandrovich of Russia, Nicolas II
Bill Gates, Lord Billy, Bill Gates, BillGates, Billy Gates, William Gates III, William H. Gates
William Shakespeare, William shekspere, William shakspeare, Bill Shakespear
Bill Clinton, Billll Clinton, William Jefferson Blythe IV, Bill J. Clinton, William J Clinton
Figure 2: Sample alias lists scraped from Wikipedia. Note that only partial alias lists are shown for space reasons.
that redirect to an entity are descriptions rather than
names. For example, 44th President of the United
States also links to Barack Obama, but it is not a
name variant.
Freebase filtering. To improve data quality we used
Freebase, a structured knowledge base that incorpo-
rates information from Wikipedia. Among its struc-
tured information are entity types, including the type
?person.? We filtered the Wikipedia redirect col-
lection to remove pairs where the target page was
not listed as a person in Freebase. Additionally, to
remove redirects that were not proper names (44th
President of the United States), we applied a series
of rule based filters to remove bad aliases: removing
numerical names, parentheticals after names, quota-
tion marks, and names longer than 5 tokens, since
we found that these long names were rarely person
names (e.g. United States Ambassador to the Eu-
ropean Union, Success Through a Positive Mental
Attitude which links to the author Napoleon Hill.)
While not perfect, these modifications dramatically
improved quality. The result was a list of 78,079 dif-
ferent person entities, each with one or more known
names or aliases. Some typical names are shown in
Figure 2.
Estimating empirical type counts. Our method is
really intended to be run on a corpus of string to-
kens. However, for experimental purposes, we in-
stead use the above dataset of string types because
this allows us to use the ?ground truth? given by
the Wikipedia redirects. To synthesize token counts,
empirical token frequencies for each type were esti-
mated from the LDC Gigaword corpus,12 which is
a corpus of newswire text spanning several years.
Wikipedia name types that did not appear in Giga-
word were assigned a ?backoff count? of one. Note
that by virtue of the domain, many misspellings will
12LDC Catalog No. LDC2003T05.
not appear; however, edges ?popular? names (which
may be canonical names) will be assigned higher
weight.
7.2 Experiments
We begin by evaluating the generalization ability of a
transducer trained using a transformation model. To
do so, we measure log-likelihood on held-out entity
title and alias pairs. We then verify that the general-
ization ability according to log-likelihood translates
into gains for a name matching task. For the experi-
ments in this section, we use ? = 0.9 and ? = 0.1.13
Held-out log-likelihood. We construct pairs of en-
tity title (input) and alias (output) names from the
Wikipedia data. For different amounts of supervised
data, we trained the transformation model on the
training set, and plotted the log-likelihood of held-
out test data for the transducer parameters at each it-
eration of EM. The held-out test set is constructed
from a disjoint set of Wikipedia entities, the same
number of entities as in the training set. We used
different corpora of 1000 and 1500 entities for train
and test.
Name matching. For each alias a in a test set (not
seen at training time), we produce a ranking of test
entity titles t according to transducer probabilities
p?(a | t). A good transducer should assign high
probability to transformations from the correct ti-
tle for the alias. Mean reciprocal rank (MRR) is a
commonly used metric to estimate the quality of a
ranking, which we report in Figure 4. The reported
mean is over all aliases in the test data. In addition to
evaluating the ranking for different initializations of
our transducer, we compare to two baselines: Lev-
enshtein distance and Jaro-Winkler similarity. Jaro-
Winkler is a measure on strings that was specifically
designed for record linkage (Winkler, 1999). The
13We did not find these parameters to be sensitive.
352
0 1 2 3 4 5 6 7 8 9EM iteration150000
140000
130000
120000
110000
100000
90000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(a) 1000 entities.
0 1 2 3 4 5 6 7 8 9EM iteration240000
230000
220000
210000
200000
190000
180000
170000
160000
150000
Held o
ut log
-likelih
ood
sup=0sup=5sup=25sup=100sup=250
(b) 1500 entities.
Figure 3: Learning curves for different initializations of the transducer parameters. Above, ?sup=100? (for instance)
means that 100 entities were used as training data to initialize the transducer parameters (constructing pairs between
all title-alias pairs for those Wikpedia entities).
15000.60
0.65
0.70
0.75
0.80
0.85
MRR
jwinklevsup10semi10unsupsup
Figure 4: Mean reciprocal rank (MRR) results for differ-
ent training conditions: ?sup10? means that 10 entities
(roughly 40 name pairs) were used as training data for
the transducer; ?semi10? means that the ?sup10? model
was used as initialization before re-estimating the param-
eters using our model; ?unsup? is the transducer trained
using our model without any initial supervision; ?sup? is
trained on all 1500 entities in the training set (an upper
bound on performance); ?jwink? and ?lev? correspond to
Jaro-Winkler and Levenshtein distance baselines.
matching experiments were performed on a corpus
of 1500 entities (with separate corpora of the same
size for training and test).
8 Conclusions and Future Work
We have presented a new unsupervised method for
learning string-to-string transducers. It learns from
a collection of related strings whose relationships are
unknown. The key idea is that some strings are mu-
tations of common strings that occurred earlier. We
compute a distribution over the unknown phyloge-
netic tree that relates these strings, and use it to rees-
timate the transducer parameters via EM.
One direction for future work would be more so-
phisticated transduction models than the one we de-
veloped in ?4. For names, this could include learn-
ing common nicknames (nonparametrically); explic-
itly modeling abbreviation processes such as initials;
conditioning on name components such as title and
middle name; and transliterating across languages.14
In other domains, one could model bibliographic en-
try propagation, derivational morphology, or histor-
ical sound change (again using language tags).
Another future direction would be to incorporate
the context of tokens in order to help reconstruct
which tokens are coreferent. For example, we might
extend the generative story to generate a context for
token (e, w) conditioned on e. Combining contex-
tual similarity with string similarity has previously
proved very useful for identifying cognates (Schafer
and Yarowsky, 2002; Schafer, 2006b; Bergsma and
Van Durme, 2011). In our setting it would help to
distinguish people with identical names, as well as
determining whether two people with similar names
are really the same.
14These last two points suggest that the mutation model
should operate not on simple (entity, string) pairs, but on richer
representations in which the name has been parsed into its com-
ponents (Eisenstein et al2011), labeled with a language ID,
and perhaps labeled with a phonological pronunciation. These
additional properties of a named entity may be either observed
or latent in training data. For example, if wy and `y denote the
string and language of name y, then define p(y | x) = p(`y |
`x) ? p(wy | `y, `x, wx). The second factor captures translitera-
tion from language `x to language `y , e.g., by using ?4?s model
with an (`x, `y)-specific parameter setting.
353
References
Nicholas Andrews and Jason Eisner. 2011. Transformation pro-
cess priors. In NIPS 2011 Workshop on Bayesian Nonpara-
metrics: Hope or Hype?, Sierra Nevada, Spain, December.
Extended abstract (3 pages).
A. Bagga and B. Baldwin. 1998. Algorithms for scoring coref-
erence chains. In LREC.
Regina Barzilay and Lillian Lee. 2003. Learning to para-
phrase: an unsupervised approach using multiple-sequence
alignment. In Proc. of NAACL-HLT, pages 16?23, Strouds-
burg, PA, USA.
C. H. Bennett, M. Li, , and B. Ma. 2003. Chain letters
and evolutionary histories. Scientific American, 288(3):76?
81, June. More mathematical version available at http:
//www.cs.uwaterloo.ca/~mli/chain.html.
Shane Bergsma and Benjamin Van Durme. 2011. Learning
bilingual lexicons using the visual similarity of labeled web
images. In Proc. of IJCAI, pages 1764?1769, Barcelona,
Spain.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive
duplicate detection using learnable string similarity mea-
sures. In Proc. of ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ?03, pages
39?48, New York, NY, USA. ACM.
Alexandre Bouchard-C?t?, Percy Liang, Thomas Griffiths, and
Dan Klein. 2008. A probabilistic approach to language
change. In Proc. of NIPS, pages 169?176.
S. Cucerzan. 2007. Large-scale named entity disambiguation
based on Wikipedia data. In Proc. of EMNLP.
Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli,
and Andrew McCallum. 2007. Canonicalization of database
records using adaptive similarity measures. In Proc. of ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining, KDD ?07, pages 201?209.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maxi-
mum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society. Series B (Method-
ological), 39(1):1?38.
Z. Dias, A. Rocha, and S. Goldenstein. 2012. Image phy-
logeny by minimal spanning trees. IEEE Trans. on Informa-
tion Forensics and Security, 7(2):774?788, April.
Markus Dreyer and Jason Eisner. 2011. Discovering morpho-
logical paradigms from plain text using a Dirichlet process
mixture model. In Proc. of EMNLP, pages 616?627. Sup-
plementary material (9 pages) also available.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008. Latent-
variable modeling of string transductions with finite-state
methods. In Proc. of EMNLP, pages 1080?1089, Honolulu,
Hawaii, October. Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, and
Eric Xing. 2011. Structured databases of named entities
fromBayesian nonparametrics. InProc. of the First workshop
on Unsupervised Learning in NLP, pages 2?12, Edinburgh,
Scotland, July. Association for Computational Linguistics.
Jason Eisner. 2002. Transformational priors over grammars.
In Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Philadelphia, July.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences?Computer Science and Computational Biology.
Cambridge University Press.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan
Klein. 2008. Learning bilingual lexicons from monolingual
corpora. In Proc. of ACL-08: HLT, pages 771?779.
David Hall and Dan Klein. 2010. Finding cognates using phylo-
genies. In Association for Computational Linguistics (ACL).
Rob Hall, Charles Sutton, and Andrew McCallum. 2008. Un-
supervised deduplication using cross-field dependencies. In
Proc. of the ACM SIGKDD International Conference On
Knowledge Discovery and Data Mining, KDD ?08, pages
310?317.
Md. Enamul. Karim, Andrew Walenstein, Arun Lakhotia, and
Laxmi Parida. 2005. Malware phylogeny generation using
permutations of code. Journal in Computer Virology, 1(1?
2):13?23.
Alexandre Klementiev and Dan Roth. 2006. Weakly supervised
named entity transliteration and discovery from multilingual
comparable corpora. In Proc. of COLING-ACL, pages 817?
824.
K. Knight and J. Graehl. 1998. Machine transliteration. Com-
putational Linguistics, 24:599?612.
Terry Koo, Amir Globerson, Xavier Carreras, and Michael
Collins. 2007. Structured prediction models via the matrix-
tree theorem. In Proc. of EMNLP-CoNLL, pages 141?150.
Jose Oncina and Marc Sebban. 2006. Using learned conditional
distributions as edit distance. In Proc. of the 2006 Joint IAPR
international Conference on Structural, Syntactic, and Statis-
tical Pattern Recognition, SSPR?06/SPR?06, pages 403?411.
Kristen Parton, Kathleen R. McKeown, James Allan, and En-
rique Henestroza. 2008. Simultaneous multilingual search
for translingual information retrieval. In Proceeding of the
ACM conference on Information and Knowledge Manage-
ment, CIKM ?08, pages 719?728.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning string
edit distance. Technical Report CS-TR-532-96, Princeton
University, Department of Computer Science.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recognition and
Machine Intelligence, 20(5):522?532, May.
Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2011.
An algorithm for unsupervised transliteration mining with an
application to word alignment. In Proc. of ACL, pages 430?
439.
Charles Schafer and David Yarowsky. 2002. Inducing transla-
tion lexicons via diverse similarity measures and bridge lan-
guages. In Proc. of CONLL, pages 146?152.
Charles Schafer. 2006a. Novel probabilistic finite-state transduc-
ers for cognate and transliteration modeling. In 7th Biennial
Conference of the Association for Machine Translation in the
Americas (AMTA).
Charles Schafer. 2006b. Translation Discovery Using Diverse
Smilarity Measures. Ph.D. thesis, Johns Hopkins University.
David A. Smith and Noah A. Smith. 2007. Probabilistic mod-
els of nonprojective dependency trees. In Proc. of EMNLP-
CoNLL, pages 132?140.
354
R. T. Smythe and H. M. Mahmoud. 1995. A survey of recur-
sive trees. Theory of Probability andMathematical Statistics,
51(1?27).
Benjamin Snyder, Regina Barzilay, and Kevin Knight. 2010. A
statistical model for lost language decipherment. In Proc. of
ACL, pages 1048?1057.
Koichiro Tamura, Daniel Peterson, Nicholas Peterson, Glen
Stecher, Masatoshi Nei, and Sudhir Kumar. 2011. Mega5:
Molecular evolutionary genetics analysis using maximum
likelihood, evolutionary distance, and maximum parsimony
methods. Molecular Biology and Evolution, 28(10):2731?
2739.
R E Tarjan. 1977. Finding optimum branchings. Networks,
7(1):25?35.
W. Tutte. 1984. Graph Theory. Addison-Wesley.
William E. Winkler. 1999. The state of record linkage and cur-
rent research problems. Technical report, Statistical Research
Division, U.S. Census Bureau.
355
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 60?69,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Entity Clustering Across Languages
Spence Green*, Nicholas Andrews?, Matthew R. Gormley?,
Mark Dredze?, and Christopher D. Manning*
*Computer Science Department, Stanford University
{spenceg,manning}@stanford.edu
?Human Language Technology Center of Excellence, Johns Hopkins University
{noa,mrg,mdredze}@cs.jhu.edu
Abstract
Standard entity clustering systems commonly
rely on mention (string) matching, syntactic
features, and linguistic resources like English
WordNet. When co-referent text mentions ap-
pear in different languages, these techniques
cannot be easily applied. Consequently, we
develop new methods for clustering text men-
tions across documents and languages simulta-
neously, producing cross-lingual entity clusters.
Our approach extends standard clustering algo-
rithms with cross-lingual mention and context
similarity measures. Crucially, we do not as-
sume a pre-existing entity list (knowledge base),
so entity characteristics are unknown. On an
Arabic-English corpus that contains seven dif-
ferent text genres, our best model yields a 24.3%
F1 gain over the baseline.
1 Introduction
This paper introduces techniques for clustering co-
referent text mentions across documents and lan-
guages. On the web today, a breaking news item
may instantly result in mentions to a real-world entity
in multiple text formats: news articles, blog posts,
tweets, etc. Much NLP work has focused on model
adaptation to these diverse text genres. However, the
diversity of languages in which the mentions appear
is a more significant challenge. This was particularly
evident during the 2011 popular uprisings in the Arab
world, in which electronic media played a prominent
role. A key issue for the outside world was the aggre-
gation of information that appeared simultaneously
in English, French, and various Arabic dialects.
To our knowledge, we are the first to consider clus-
tering entity mentions across languages without a pri-
ori knowledge of the quantity or types of real-world
entities (a knowledge base). The cross-lingual set-
ting introduces several challenges. First, we cannot
assume a prototypical name format. For example,
the Anglo-centric first/middle/last prototype used in
previous name modeling work (cf. (Charniak, 2001))
does not apply to Arabic names like Abdullah ibn
Abd Al-Aziz Al-Saud or Chinese names like Hu Jin-
tao (referred to as Mr. Hu, not Mr. Jintao). Sec-
ond, organization names often require both translit-
eration and translation. For example, the Arabic
	PP?

K?? ?Q
	
g.

??Q?? ?General Motors Corp? contains
transliterations of 	PP?K?? ?Q
	
g. ?General Motors?,
but a translation of

??Q?? ?Corporation?.
Our models are organized as a pipeline. First, for
each document, we perform standard mention detec-
tion and coreference resolution. Then, we use pair-
wise cross-lingual similarity models to measure both
mention and context similarity. Finally, we cluster
the mentions based on similarity.
Our work makes the following contributions: (1)
introduction of the task, (2) novel models for cross-
lingual entity clustering of person and organization en-
tities, (3) cross-lingual annotation of the NIST Auto-
matic Content Extraction (ACE) 2008 Arabic-English
evaluation set, and (4) experimental results using both
gold and automatic within-document processing. We
will release our software and annotations to support
future research.
1.1 Task Description via a Simple Example
Consider the toy corpus in Fig. 1. The English docu-
ments contain mentions of two people: Steven Paul
Jobs and Mark Elliot Zuckerberg. Of course, the sur-
face realization of Mr. Jobs? last name in English is
also an ordinary nominal, hence the ambiguous men-
tion string (absent context) in the second document.
The Arabic document introduces an organization en-
tity (Apple Inc.) along with proper and pronominal
references to Mr. Jobs. Finally, the French document
refers to Mr. Jobs by the honorific ?Monsieur,? and to
60
Jobs program details delayed
Steve Jobs admired Mark Zuckerberg
M. Jobs, le fondateur d'Apple, est mort
	
		
?
=
? E1
E2
E3
=
=
doc1:
doc2:
doc3:
doc4:
Figure 1: Clustering entity mentions across languages and documents. The toy corpus contains English (doc1 and
doc2), Arabic (doc3), and French (doc4). Together, the documents make reference to three real-world entities, the
identification of which is the primary objective of this work. We use a separately-trained system for within-document
mention detection and coreference (indicated by the text boxes and intra-document links, respectively). Our experimental
results are for Arabic-English only.
Apple without its corporate designation.
Our goal is to automatically produce the cross-
lingual entity clusters E1 (Mark Elliot Zuckerberg),
E2 (Apple Inc.), and E3 (Steven Paul Jobs). Both the
true number and characteristics of these entities are
unobserved. Our models require two pre-processing
steps: mention detection and within-document coref-
erence/anaphora resolution, shown in Fig. 1 by the
text boxes and intra-document links, respectively. For
example, in doc3, a within-document coreference sys-
tem would pre-link 	QK. ?k. joobz ?Jobs? with the mascu-
line pronoun ? h ?his?. In addition, the mention detec-
tor determines that the surface form ?Jobs? in doc2
is not an entity reference. For this within-document
pre-processing we use Serif (Ramshaw et al, 2011).1
Our models measure cross-lingual similarity of the
coreference chains to make clustering decisions (?
in Fig. 1). The similarity models (indicated by the
= and 6= operators in Fig. 1) consider both mention
string and context similarity (?2). We use the men-
tion similarities as hard constraints, and the context
similarities as soft constraints. In this work, we inves-
tigate two standard constrained clustering algorithms
(?3). Our methods can be used to extend existing sys-
tems for mono-lingual entity clustering (also known
as ?cross-document coreference resolution?) to the
cross-lingual setting.
1Serif is a commercial system that assumes each document
contains only one language. Currently, there are no publicly avail-
able within-document coreference systems for Arabic and many
other languages. To remedy this problem, the CoNNL-2012
shared task aims to develop multilingual coreference systems.
2 Mention and Context Similarity
Our goal is to create cross-lingual sets of co-referent
mentions to real-world entities (people, places, orga-
nizations, etc.). In this paper, we adopt the following
notation. LetM be a set of distinct text mentions in a
collection of documents;C is a partitioning ofM into
document-level sets of co-referent mentions (called
coreference chains); E is a partitioning of C into sets
of co-referent chains (called entities). Let i, j be non-
negative integers less than or equal to |M | and a, b be
non-negative integers less than or equal to |C|. Our
experiments use a separate within-document corefer-
ence system to createC, which is fixed. We will learn
E, which has size no greater than |C| since the set of
mono-lingual chains is the largest valid partitioning.
We define accessor functions to access properties
of mentions and chains. For any mentionmi, define
the following functions: lang(mi) is the language;
doc(mi) is the document containingmi; type(mi) is
the semantic type, which is assigned by the within-
document coreference system. We also extract a set
of mention contexts S, which are the sentences con-
taining each mention (i.e., |S| = |M |).
We learn the partition E by considering mention
and context similarity, which are measured with sep-
arate component models.
2.1 Mention Similarity
We use separate methods for within- and cross-
language mention similarity. The pairwise similarity
61
Arabic Rules
H. ? b
H? t H? th h. ? j
h? h p? kh X? d
	
X? th
P? r 	P? z ?? s ?? sh
?? s 	?? d ?? t 	?? th
?? a
	
?? g
	
?? f

?? q
?? k ?? l ?? m 	?? n
?? h @? a ?? w ?? a

?? ah ?


? ? Z? ?
English Rules
k? c p? b x? ks e,i,o,u? ?
Table 1: English-Arabic mapping rules to a common or-
thographic representation. ??? indicates a null mapping.
For English, we also lowercase and remove determiners
and punctuation. For Arabic, we remove the determiner
?@ Al ?the? and the elongation character tatwil ??.
of any two mentionsmi andmj is:
sim(mi,mj) =
{
jaro-winkler(mi,mj) if lang(mi) = lang(mj)
maxent(mi,mj) otherwise
Jaro-Winkler Distance (within-language) If
lang(mi) = lang(mj), we use the Jaro-Winkler edit
distance (Porter and Winkler, 1997). Jaro-Winkler
rewards matching prefixes, the empirical justification
being that less variation typically occurs at the
beginning of names.2 The metric produces a score in
the range [0,1], where 0 indicates equality.
Maxent model (cross-language) When lang(mi)
6= lang(mj), then the two mentions might be in dif-
ferent writing systems. Edit distance calculations
no longer apply directly. One solution would be
full-blown transliteration (Knight and Graehl, 1998),
followed by application of Jaro-Winkler. However,
transliteration systems are complex and require sig-
nificant training resources. We find that a simpler,
low-resource approach works well in practice.
First, we deterministically map both languages to a
common phonetic representation (Tbl. 1).3 Next, we
align the mention pairs with the Hungarian algorithm,
2For multi-token names, we sort the tokens prior to computing
the score, as suggested by Christen (2006).
3This idea is reminiscent of Soundex, which Freeman et al
(2006) used for cross-lingual name matching.
Overlap Active for each bigram in
cbigrams(mi,u)
?
cbigrams(mj,v)
Bigram-Diff-mi Active for each bigram in
cbigrams(mi)? cbigrams(mj)
Bigram-Diff-mj Active for each bigram in
cbigrams(mj)? cbigrams(mi)
Bigram-Len-Diff Value of abs(size(cbigrams(mi)?
cbigrams(mj)))
Big-Edit-Dist Count of token pairs with
Lev(mi,u,mj,v) > 3.0
Total-Edit-Dist Sum of aligned token edit distances
Length Active for one of:
len(mi) > len(mj) or
len(mi) < len(mj) or
len(mi) = len(mj)
Length-Diff abs(len(mi)? len(mj))
Singleton Active if len(mi) = 1
Singleton-Pair Active if len(mi) = len(mj) = 1
Table 2: Cross-language Maxent feature templates for a
whitespace-tokenized mention pair ?mi,mj? with align-
ment Ami,mj . Let (u, v) ? Ami,mj indicate aligned to-
ken indices. Define the following functions for strings:
cbigrams(?) returns the set of character bigrams; len(?) is
the token length; Lev(?, ?) is the Levenshtein edit distance
between two strings. Prior to feature extraction, we add
unique start and end symbols to the mention strings.
which produces a word-to-word alignment Ami,mj .
4
Finally, we build a simple binary Maxent classifier
p(y|mi,mj ;?) that extracts features from the aligned
mentions (Tbl. 2). We learn the parameters ? using a
quasi-Newton procedure with L1 (lasso) regulariza-
tion (Andrew and Gao, 2007).
2.2 Context Mapping and Similarity
Mention strings alone are not always sufficient for
disambiguation. Consider again the simple exam-
ple in Fig. 1. Both doc3 and doc4 reference ?Steve
Jobs? and ?Apple? in the same contexts. Context co-
occurence and/or similarity can thus disambiguate
these two entities from other entities with similar ref-
erences (e.g., ?Steve Jones? or ?Apple Corps?). As
with the mention strings, the contexts may originate
in different writing systems. We consider both high-
and low-resource approaches for mapping contexts to
a common representation.
4The Hungarian algorithm finds an optimal minimum-cost
alignment. For pairwise costs between tokens, we used the Lev-
enshtein edit distance
62
Machine Translation (MT) For the high-resource
setting, if lang(mi) 6=English, then we translate both
mi and its context si to English with an MT system.
We use Phrasal (Cer et al, 2010), a phrase-based
system which, like most public MT systems, lacks a
transliteration module. We believe that this approach
yields the most accurate context mapping for high-
resource language pairs (like English-Arabic).
Polylingual Topic Model (PLTM) The polylin-
gual topic model (PLTM) (Mimno et al, 2009) is
a generative process in which document tuples?
groups of topically-similar documents?share a topic
distribution. The tuples need not be sentence-aligned,
so training data is easier to obtain. For example, one
document tuple might be the set of Wikipedia articles
(in all languages) for Steve Jobs.
Let D be a set of document tuples, where
there is one document in each tuple for each
of L languages. Each language has vocabu-
lary Vl and each document dlt has N
l
t tokens.
We specify a fixed-size set of topics K. The
PLTM generates the document tuples as follows:
Polylingual Topic Model
?t ? Dir(?K) [cross-lingual tuple-topic prior]
?lk ? Dir(?
Vl) [word-topic prior]
for each token wlt,n with n = {1, . . . , N
l
t}:
zt,n ? Mult(?t)
wlt,n ? Mult(?
l
zt,n)
For cross-lingual context mapping, we infer the 1-
best topic assignments for each token in all S mention
contexts. This technique reduces Vl = k for all l.
Moreover, all languages have a common vocabulary:
the set of K topic indices. Since the PLTM is not
a contribution of this paper, we refer the interested
reader to (Mimno et al, 2009) for more details.
After mapping each mention context to a common
representation, we measure context similarity based
on the choice of clustering algorithm.
3 Clustering Algorithms
We incorporate the mention and context similarity
measures into a clustering framework. We consider
two algorithms. The first is hierarchical agglomera-
tive clustering (HAC), with which we assume basic
familiarity (Manning et al, 2008). A shortcoming of
HAC is that a stop threshold must be tuned. To avoid
this requirement, we also consider non-parametric
probabilistic clustering in the form of a Dirichlet pro-
cess mixture model (DPMM) (Antoniak, 1974) .
Both clustering algorithms can be modified to ac-
commodate pairwise constraints. We have observed
better results by encoding mention similarity as a
hard constraint. Context similarity is thus the cluster
distance measure.5
To turn the Jaro-Winkler distance into a hard
boolean constraint, we tuned a threshold ? on held-out
data, i.e., jaro-winkler(mi,mj) ? ? ? mi = mj .
Likewise, the Maxent model is a binary classifier, so
p(y = 1|mi,mj ;?) > 0.5? mi = mj .
In both clustering algorithms, any two chains Ca
and Cb cannot share the same cluster assignment if:
1. Document origin: doc(Ca) = doc(Cb)
2. Semantic type: type(Ca) 6= type(Cb)
3. Mention Match: sim(mi,mj) = false,
wheremi = repr(Ca) andmj = repr(Cb).
The deterministic accessor function repr(Ca) returns
the representative mention of a chain. The heuristic
we used was ?first mention?: the function returns the
earliest mention that appears in the associated docu-
ment. In many languages, the first mention is typi-
cally more complete than later mentions. This heuris-
tic also makes our system less sensitive to within-
document coreference errors.6 The representative
mention only has special status for mention similar-
ity: context similarity considers all mention contexts.
3.1 Constrained Hierarchical Clustering
HAC iteratively merges the ?nearest? clusters accord-
ing to context similarity. In our system, each cluster
context is a bag of wordsW formed from the contexts
of all coreference chains in that cluster. For each word
inW we estimate a unigram Entity Language Model
(ELM) (Raghavan et al, 2004):
P (w) =
countW (w) + ?PV (w)
?
w? countW (w
?) + ?
PV (w) is the unigram probability in all contexts in
the corpus7 and ? is a smoothing parameter. For any
5Specification of a combined similarity measure is an inter-
esting direction for future work.
6These constraints are similar to the pair-filters of Mayfield
et al (2009).
7Recall that after context mapping, all languages have a com-
mon vocabulary V .
63
two entity clusters Ea and Eb, the distance between
PEa and PEb is given by a metric based on the Jensen-
Shannon Divergence (JSD) (Endres and Schindelin,
2003):
dist(PEa , PEb) =
?
2 ? JSD(PEa ||PEb)
=
?
KL(PEa ||M) +KL(M ||PEb)
where KL(PEa ||M) is the Kullback-Leibler diver-
gence andM = 12(PEa + PEb).
We initialize HAC to E = C, i.e., the initial clus-
tering solution is just the set of all coreference chains.
Thenwe remove all links in the HAC proximitymatrix
that violate pairwise cannot-link constraints. During
clustering, we do not merge Ea and Eb if any pair of
chains violates a cannot-link constraint. This proce-
dure propagates the cannot-link constraints (Klein et
al., 2002). To output E, we stop clustering when the
minimum JSD exceeds a stop threshold ?, which is
tuned on a development set.
3.2 Constrained Dirichlet Process Mixture
Model (DPMM)
Instead of tuning a parameter like ?, it would be prefer-
able to let the data dictate the number of entity clus-
ters. We thus consider a non-parametric Bayesian
mixture model where the mixtures are multinomial
distributions over the entity contexts S. Specifically,
we consider a DPMM, which automatically infers
the number of mixtures. Each Ca has an associated
mixture ?a:
Ca|?a ? Mult(?a)
?a|G ? G
G|?,G0 ? DP(?,G0)
? ? Gamma(1, 1)
where ? is the concentration parameter of the DP
prior and G0 is the base distribution with support V .
For our experiments, we set G0 = Dir(pi1, . . . , piV ),
where pii = PV (wi).
For inference, we use the Gibbs sampler of Vla-
chos et al (2009), which can incorporate pairwise
constraints. The sampler is identical to a standard col-
lapsed, token-based sampler, except the conditional
probability p(Ea = E|E?a, Ca) = 0 if Ca cannot
be merged with the chains in clusterE. This property
makes the model non-exchangeable, but in practice
non-exchangeable models are sometimes useful (Blei
and Frazier, 2010). During sampling, we also learn ?
using the auxiliary variable procedure of West (1995),
so the only fixed parameters are those of the vague
Gamma prior. However, we found that these hyper-
parameters were not sensitive.
4 Training Data and Procedures
We trained our system for Arabic-English cross-
lingual entity clustering.8
Maxent Mention Similarity The Maxent mention
similarity model requires a parallel name list for train-
ing. Name pair lists can be obtained from the LDC
(e.g., LDC2005T34 contains nearly 450,000 parallel
Chinese-English names) or Wikipedia (Irvine et al,
2010). We extracted 12,860 name pairs from the par-
allel Arabic-English translation treebanks,9 although
our experiments show that the model achieves high
accuracy with significantly fewer training examples.
We generated a uniform distribution of training ex-
amples by running a Bernoulli trial for each aligned
name pair in the corpus. If the coin was heads, we
replaced the English name with another English name
chosen randomly from the corpus.
MT Context Mapping For the MT context map-
ping method, we trained Phrasal with all data permit-
ted under the NIST OpenMT Ar-En 2009 constrained
track evaluation. We built a 5-gram language model
from the Xinhua and AFP sections of the Gigaword
corpus (LDC2007T07), in addition to all of the target
side training data. In addition to the baseline Phrasal
feature set, we used the lexicalized re-ordering model
of Galley and Manning (2008).
PLTM Context Mapping For PLTM training, we
formed a corpus of 19,139 English-Arabic topically-
aligned Wikipedia articles. Cross-lingual links in
Wikipedia are abundant: as of February 2010, there
were 77.07M cross-lingual links among Wikipedia?s
272 language editions (de Melo and Weikum, 2010).
To increase vocabulary coverage for our ACE2008
evaluation corpus, we added 20,000 document sin-
gletons from the ACE2008 training corpus. The
8We tokenized all English documents with packages from
the Stanford parser (Klein and Manning, 2003). For Arabic
documents, we used Mada (Habash and Rambow, 2005) for
orthographic normalization and clitic segmentation.
9LDC Catalog numbers LDC2009E82 and LDC2009E88.
64
topically-aligned tuples served as ?glue? to share top-
ics between languages, while the ACE documents
distribute those topics over in-domain vocabulary.10
We used the PLTM implementation in Mallet (Mc-
Callum, 2002). We ran the sampler for 10,000 itera-
tions and set the number of topicsK = 512.
5 Task Evaluation Framework
Our experimental design is a cross-lingual extension
of the standard cross-document coreference resolu-
tion task, which appeared in ACE2008 (Strassel et
al., 2008; NIST, 2008). We evaluate name (NAM)
mentions for cross-lingual person (PER) and organi-
zation (ORG) entities. Neither the number nor the
attributes of the entities are known (i.e., the task does
not include a knowledge base). We report results for
both gold and automatic within-document mention
detection and coreference resolution.
Evaluation Metrics We use entity-level evaluation
metrics, i.e., we evaluate the E entity clusters rather
than the mentions. For the gold setting, we report:
? B3 (Bagga and Baldwin, 1998a): Precision and
recall are computed from the intersection of the
hypothesis and reference clusters.
? CEAF (Luo, 2005): Precision and recall are
computed from a maximum bipartite matching
between hypothesis and reference clusters.
? NVI (Reichart and Rappoport, 2009):
Information-theoretic measure that uti-
lizes the entropy of the clusters and their mutual
information. Unlike the commonly-used Varia-
tion of Information (VI) metric, normalized VI
(NVI) is not sensitive to the size of the data set.
For the automatic setting, we must apply a different
metric since the number of system chains may differ
from the reference. We use B3sys (Cai and Strube,
2010), a variant of B3 that was shown to penalize
both twinless reference chains and spurious system
chains more fairly.
Evaluation Corpus The automatic evaluation of
cross-lingual coreference systems requires annotated
10Mimno et al (2009) showed that so long as the proportion
of topically-aligned to non-aligned documents exceeded 0.25,
the topic distributions (as measured by mean Jensen-Shannon
Divergence between distributions) did not degrade significantly.
Docs Tokens Entities Chains Mentions
Arabic 412 178,269 2,594 4,216 9,222
English 414 246,309 2,278 3,950 9,140
Table 3: ACE2008 evaluation corpus PER and ORG entity
statistics. Singleton chains account for 51.4% of the Arabic
data and 46.2% of the English data. Just 216 entities appear
in both languages.
multilingual corpora. Cross-document annotation
is expensive (Strassel et al, 2008), so we chose the
ACE2008 Arabic-English evaluation corpus as a start-
ing point for cross-lingual annotation. The corpus
consists of seven genres sampled from independent
sources over the course of a decade (Tbl. 3). The
corpus provides gold mono-lingual cross-document
coreference annotations for both PER and ORG enti-
ties. Using these annotations as a starting point, we
found and annotated 216 cross-lingual entities.11
Because a similar corpus did not exist for develop-
ment, we split the evaluation corpus into development
and test sections. However, the usual method of split-
ting by document would not confine all mentions of
each entity to one side of the split. We thus split the
corpus by global entity id. We assigned one-third of
the entities to development, and the remaining two-
thirds to test.
6 Comparison to Related Tasks and Work
Our modeling techniques and task formulation can be
viewed as cross-lingual extensions to cross-document
coreference resolution. The classic work on this task
was by Bagga and Baldwin (1998b), who adapted
the Vector Space Model (VSM) (Salton et al, 1975).
Gooi and Allan (2004) found effective algorithmic
extensions like agglomerative clustering. Successful
feature extensions to the VSM for cross-document
coreference have included biographical information
(Mann and Yarowsky, 2003) and syntactic context
(Chen and Martin, 2007). However, neither of these
feature sets generalize easily to the cross-lingual set-
ting with multiple entity types. Fleischman and Hovy
(2004) added a discriminative pairwise mention clas-
sifier to a VSM-like model, much as we do. More
11The annotators were the first author and another fluent
speaker of Arabic. The annotations, corrections, and corpus
split are available at http://www.spencegreen.com/research/.
65
recent work has considered new models for web-scale
corpora (Rao et al, 2010; Singh et al, 2011).
Cross-document work on languages other than En-
glish is scarce. Wang (2005) used a combination of
the VSM and heuristic feature selection strategies to
cluster transliterated Chinese personal names. For
Arabic, Magdy et al (2007) started with the output of
the mention detection and within-document corefer-
ence system of Florian et al (2004). They clustered
the entities incrementally using a binary classifier.
Baron and Freedman (2008) used complete-link ag-
glomerative clustering, wheremerging decisions were
based on a variety of features such as document topic
and name uniqueness. Finally, Sayeed et al (2009)
translated Arabic name mentions to English and then
formed clusters greedily using pairwise matching.
To our knowledge, the cross-lingual entity cluster-
ing task is novel. However, there is significant prior
work on similar tasks:
? Multilingual coreference resolution: Adapt
English within-document coreference models to
other languages (Harabagiu andMaiorano, 2000;
Florian et al, 2004; Luo and Zitouni, 2005).
? Named entity translation: For a non-English
document, produce an inventory of entities in
English. An ACE2007 pilot task (Song and
Strassel, 2008).
? Named entity clustering: Assign semantic
types to text mentions (Collins and Singer, 1999;
Elsner et al, 2009).
? Cross-language name search / entity linking:
Match a single query name against a list of
known multilingual names (knowledge base). A
track in the 2011NIST Text Analysis Conference
(TAC-KBP) evaluation (Aktolga et al, 2008;
McCarley, 2009; Udupa and Khapra, 2010; Mc-
Namee et al, 2011).
Our work incorporates elements of the first three tasks.
Most importantly, we avoid the key element of entity
linking: a knowledge base.
7 Experiments
We performed intrinsic evaluations for both mention
and context similarity. For context similarity, we
analyzed mono-lingual entity clustering, which also
facilitated comparison to prior work on the ACE2008
Genre #Train #Test Accuracy(%)
wb 125 16 87.5
bn 2,720 340 95.6
nw 7,443 930 96.6
all 10,288 1,286 97.1 (+7.55)
Table 4: Cross-lingual mention matching accuracy [%].
The training data contains names from three genres: broad-
cast news (bn), newswire (nw), and weblog (wb). We used
the full training corpus (all) for the cross-lingual clustering
experiments, but the model achieved high accuracy with
significantly fewer training examples (e.g., bn).
CEAF? NVI? B3 ?
#hyp P R F1
Mono-lingual Arabic (#gold=1,721)
HAC 87.2 0.052 1,669 89.8 89.8 89.8
Mono-lingual English (#gold=1,529)
HAC 88.5 0.042 1,536 93.7 89.0 91.4
Table 5: Mono-lingual entity clustering evaluation (test
set, gold within-document processing). Higher scores (?)
are better for CEAF and B3, whereas lower (?) is better
for NVI. #gold indicates the number of reference entities,
whereas #hyp is the size of E.
evaluation set. Our main results are for the new task:
cross-lingual entity clustering.
7.1 Intrinsic Evaluations
Cross-lingual Mention Matching We created a
random 80/10/10 (train, development, test) split of
the Maxent training corpus and evaluated binary clas-
sification accuracy (Tbl. 4). Of the mis-classified
examples, we observed three major error types. First,
the model learns that high edit distance is predictive
of a mismatch. However, singleton strings that do not
match often have a lower edit distance than longer
strings that do match. As a result, singletons often
cause false positives. Second, names that originate in
a third language tend to violate the phonemic corre-
spondences. For example, the model gives a false neg-
ative for a German football team: 	?QK???P 	Q
? ?

??
	
?@
(phonetic mapping: af s kazrslawtrn) versus ?FC
Kaiserslautern.? Finally, names that require trans-
lation are problematic. For example, the classifier
produces a false negative for ?God, gd?
?
= ? ?

<?

@, allh?.
66
#gold = 3,057 CEAF? NVI? B3 ? B3target ? (#gold = 146)
#hyp P R F1 #hyp P R F1
Singleton 64.9 0.165 5,453 100.0 56.1 71.8 1,587 100.0 9.20 16.9
No-context 57.4 0.136 2,216 65.6 75.2 70.1 517 78.3 41.8 54.5
HAC+MT 79.8 0.070 2,783 84.4 86.4 85.4 310 91.7 69.1 78.8
DPMM+MT 74.3 0.122 3,649 89.3 64.1 74.6 634 93.3 24.3 38.6
HAC+PLTM 72.1 0.110 2,746 76.9 77.6 77.3 506 84.4 44.6 58.4
DPMM+PLTM 57.2 0.180 2,609 64.0 62.8 63.4 715 73.9 22.2 34.1
Table 6: Cross-lingual entity clustering (test set, gold within-document processing). B3target is the standard B
3 metric
applied to the subset of target cross-lingual entities in the test set. For CEAF and B3, Singleton is the stronger baseline
due to the high proportion of singleton entities in the corpus. Of course, cross-lingual entities have at least two chains,
so No-context is a better baseline for cross-lingual clustering.
Mono-lingual Entity Clustering For comparison,
we also evaluated our system on a standard mono-
lingual cross-document coreference task (Arabic and
English) (Tbl. 5). We configured the system with
HAC clustering and Jaro-Winkler (within-language)
mention similarity. We built mono-lingual ELMs for
context similarity.
We used two baselines:
? Singleton: E = C, i.e., the cross-lingual clus-
tering solution is just the set of mono-lingual
coreference chains. This is a common baseline
for mono-lingual entity clustering (Baron and
Freedman, 2008).
? No-context: We run HAC with ? =?. There-
fore, E is the set of fully-connected components
in C subject to the pairwise constraints.
For HAC, we manually tuned the stop threshold ?,
the Jaro-Winkler threshold ?, and the ELM smoothing
parameter ? on the development set. For the DPMM,
no development tuning was necessary, and we evalu-
ated a single sample of E taken after 3,000 iterations.
To our knowledge, Baron and Freedman (2008)
reported the only previous results on the ACE2008
data set. However, they only gave gold results for
English, and clustered the entire evaluation corpus
(test+development). To control for the effect of
within-document errors, we considered their gold in-
put (mention detection and within-document coref-
erence resolution) results. They reported B3 for the
two entity types separately: ORG (91.5% F1) and
PER (94.3% F1). The different experimental designs
preclude a precise comparison, but the accuracy of
#gold = 3,057 B3sys ?
#hyp P R F1
Singleton 7,655 100.0 57.1 72.7
No-context 2,918 63.3 71.1 67.0
HAC+MT 3,804 75.6 77.8 76.7
DPMM+MT 4,491 77.1 62.5 69.0
HAC+PLTM 6,353 94.1 62.8 75.3
DPMM+PLTM 3,522 64.6 62.0 63.3
Table 7: Cross-lingual entity clustering (test set, automatic
(Serif) within-document processing). For HAC, we used
the same parameters as the gold setting.
the two systems are at least in the same range.
7.2 Cross-lingual Entity Clustering
We evaluated four system configurations on the new
task: HAC+MT, HAC+PLTM, DPMM+MT, and
DPMM+PLTM. First, we established an upper bound
by assuming gold within-document mention detection
and coreference resolution (Tbl. 6). This setting iso-
lated the new cross-lingual clustering methods from
within-document processing errors. Then we evalu-
ated with Serif (automatic) within-document process-
ing (Tbl. 7). This second experiment replicated an
application setting. We used the same baselines and
tuning procedures as in the mono-lingual clustering
experiment.
Results In the gold setting, HAC+MTproduces the
best results, as expected. The dimensionality reduc-
tion of the vocabulary imposed by PLTM significantly
reduces accuracy, but HAC+PLTM still exceeds the
67
baseline. We tried increasing the number of PLTM
topics k, but did not observe an improvement in task
accuracy. For both context-mapping methods, the
DPMM suffers from low-recall. Upon inspection, the
clustering solution of DPMM+MT contains a high
proportion of singleton hypotheses, suggesting that
the model finds lower similarity in the presence of a
larger vocabulary. When the context vocabulary con-
sists of PLTM topics, larger clusters are discovered
(DPMM+PLTM).
The effect of dimensionality reduction is also appar-
ent in the clustering solutions of the PLTM models.
For example, for the Serif output, DPMM+PLTM
produces a cluster consisting of ?White House?, ?Sen-
ate?, ?House of Representatives?, and ?Parliament?.
Arabic mentions of the latter three entities pass the
pairwise mention similarity constraints due to the
word ??m.? ?council?, which appears in text mentions
for all three legislative bodies. A cross-language
matching error resulted in the linking of ?White
House?, and the reduced granularity of the contexts
precluded further disambiguation. Of course, these
entities probably appear in similar contexts.
The caveat with the Serif results in Tbl. 7 is that
3,251 of the 7,655 automatic coreference chains are
not in the reference. Consequently, the evaluation is
dominated by the penalty for spurious system coref-
erence chains. Nonetheless, all models except for
DPMM+PLTM exceed the baselines, and the rela-
tionships between models depicted in the gold exper-
iments hold for the this setting.
8 Conclusion
Cross-lingual entity clustering is a natural step to-
ward more robust natural language understanding.
We proposed pipeline models that make clustering
decisions based on cross-lingual similarity. We inves-
tigated two methods for mapping documents in differ-
ent languages to a common representation: MT and
the PLTM. Although MT may achieve more accurate
results for some language pairs, the PLTM training
resources (e.g., Wikipedia) are readily available for
many languages. As for the clustering algorithms,
HAC appears to perform better than the DPMM on
our dataset, but this may be due to the small corpus
size. The instance-level constraints represent tenden-
cies that could be learned from larger amounts of data.
With more data, we might be able to relax the con-
straints and use an exchangeable DPMM,whichmight
be more effective. Finally, we have shown that sig-
nificant quantities of within-document errors cascade
into the cross-lingual clustering phase. As a result,
we plan a model that clusters the mentions directly,
thus removing the dependence on within-document
coreference resolution.
In this paper, we have set baselines and proposed
models that significantly exceeded those baselines.
The best model improved upon the cross-lingual en-
tity baseline by 24.3% F1. This result was achieved
without a knowledge base, which is required by previ-
ous approaches to cross-lingual entity linking. More
importantly, our techniques can be used to extend
existing cross-document entity clustering systems for
the increasingly multilingual web.
AcknowledgmentsWe thank Jason Eisner, David Mimno,
Scott Miller, Jim Mayfield, and Paul McNamee for helpful
discussions. This work was started during the SCALE
2010 summer workshop at Johns Hopkins. The first author
is supported by a National Science Foundation Graduate
Fellowship.
References
E. Aktolga, M. Cartright, and J. Allan. 2008. Cross-document
cross-lingual coreference retrieval. In CIKM.
G. Andrew and J. Gao. 2007. Scalable training of L1-regularized
log-linear models. In ICML.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with
applications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152?1174.
A. Bagga and B. Baldwin. 1998a. Algorithms for scoring coref-
erence chains. In LREC.
A. Bagga and B. Baldwin. 1998b. Entity-based cross-document
coreferencing using the vector space model. In COLING-ACL.
A. Baron and M. Freedman. 2008. Who is Who and What
is What: Experiments in cross-document co-reference. In
EMNLP.
D. Blei and P. Frazier. 2010. Distance dependent Chinese restau-
rant processes. In ICML.
J. Cai and M. Strube. 2010. Evaluation metrics for end-to-
end coreference resolution systems. In Proceedings of the
SIGDIAL 2010 Conference.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
E. Charniak. 2001. Unsupervised learning of name structure
from coreference data. In NAACL.
Y. Chen and J. Martin. 2007. Towards robust unsupervised
personal name disambiguation. In EMNLP-CoNLL.
68
P. Christen. 2006. A comparison of personal name matching:
Techniques and practical issues. Technical Report TR-CS-06-
02, Australian National University.
M. Collins and Y. Singer. 1999. Unsupervised models for named
entity classification. In EMNLP.
G. de Melo and G. Weikum. 2010. Untangling the cross-lingual
link structure of Wikipedia. In ACL.
M. Elsner, E. Charniak, and M. Johnson. 2009. Structured
generative models for unsupervised named-entity clustering.
In HLT-NAACL.
D. M. Endres and J. E. Schindelin. 2003. A new metric for
probability distributions. IEEE Transactions on Information
Theory, 49(7):1858 ? 1860.
M. Fleischman and E. Hovy. 2004. Multi-document person name
resolution. In ACL Workshop on Reference Resolution and its
Applications.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, et al
2004. A statistical model for multilingual entity detection and
tracking. In HLT-NAACL.
A. T. Freeman, S. L. Condon, and C. M. Ackerman. 2006. Cross
linguistic name matching in English and Arabic: a one to
many mapping extension of the Levenshtein edit distance
algorithm. In HLT-NAACL.
M. Galley and C. D. Manning. 2008. A simple and effective
hierarchical phrase reordering model. In EMNLP.
C. H. Gooi and J. Allan. 2004. Cross-document coreference on
a large scale corpus. In HLT-NAACL.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
S. M. Harabagiu and S. J. Maiorano. 2000. Multilingual corefer-
ence resolution. In ANLP.
A. Irvine, C. Callison-Burch, and A. Klementiev. 2010. Translit-
erating from all languages. In AMTA.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. Klein, S. D. Kamvar, and C. D.Manning. 2002. From instance-
level constraints to space-level constraints: Making the most
of prior knowledge in data clustering. In ICML.
K. Knight and J. Graehl. 1998. Machine transliteration. Compu-
tational Linguistics, 24:599?612.
X. Luo and I. Zitouni. 2005. Multi-lingual coreference resolution
with syntactic features. In HLT-EMNLP.
X. Luo. 2005. On coreference resolution performance metrics.
In HLT-EMNLP.
W. Magdy, K. Darwish, O. Emam, and H. Hassan. 2007. Arabic
cross-document person name normalization. In Workshop on
Computational Approaches to Semitic Languages.
G. S. Mann and D. Yarowsky. 2003. Unsupervised personal
name disambiguation. In NAACL.
C. D. Manning, P. Raghavan, and H. Sch?tze. 2008. Introduction
to Information Retrieval. Cambridge University Press.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed, et al
2009. Cross-document coreference resolution: A key technol-
ogy for learning by reading. In AAAI Spring Symposium on
Learning by Reading and Learning to Read.
A. K. McCallum. 2002. MALLET: A machine learning for
language toolkit. http://mallet.cs.umass.edu.
J. S. McCarley. 2009. Cross language name matching. In SIGIR.
P. McNamee, J. Mayfield, D. Lawrie, D.W. Oard, and D. Doer-
mann. 2011. Cross-language entity linking. In IJCNLP.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A. Smith, and
A. McCallum. 2009. Polylingual topic models. In EMNLP.
NIST. 2008. Automatic Content Extraction 2008 evaluation
plan (ACE2008): Assessment of detection and recognition
of entities and relations within and across documents. Tech-
nical Report rev. 1.2d, National Institute of Standards and
Technology (NIST), 8 August.
E. H. Porter and W. E. Winkler, 1997. Approximate String Com-
parison and its Effect on an Advanced Record Linkage System,
chapter 6, pages 190?199. U.S. Bureau of the Census.
H. Raghavan, J. Allan, and A. McCallum. 2004. An explo-
ration of entity models, collective classification and relation
description. In KDD Workshop on Link Analysis and Group
Detection.
L. Ramshaw, E. Boschee, M. Freedman, J. MacBride,
R. Weischedel, and A. Zamanian. 2011. SERIF language
processing?effective trainable language understanding. In
J. Olive et al, editors,Handbook of Natural Language Process-
ing and Machine Translation: DARPA Global Autonomous
Language Exploitation, pages 636?644. Springer.
D. Rao, P. McNamee, and M. Dredze. 2010. Streaming cross
document entity coreference resolution. In COLING.
R. Reichart and A. Rappoport. 2009. The NVI clustering evalu-
ation measure. In CoNLL.
G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model
for automatic indexing. CACM, 18:613?620, November.
A. Sayeed, T. Elsayed, N. Garera, D. Alexander, T. Xu, et al
2009. Arabic cross-document coreference detection. In ACL-
IJCNLP, Short Papers.
S. Singh, A. Subramanya, F. Pereira, and A. McCallum. 2011.
Large-scale cross-document coreference using distributed in-
ference and hierarchical models. In ACL.
Z. Song and S. Strassel. 2008. Entity translation and alignment
in the ACE-07 ET task. In LREC.
S. Strassel, M. Przybocki, K. Peterson, Z. Song, and K. Maeda.
2008. Linguistic resources and evaluation techniques for
evaluation of cross-document automatic content extraction.
In LREC.
R. Udupa and M. M. Khapra. 2010. Improving the multilin-
gual user experience of Wikipedia using cross-language name
search. In HLT-NAACL.
A. Vlachos, A. Korhonen, and Z. Ghahramani. 2009. Unsuper-
vised and constrained Dirichlet process mixture models for
verb clustering. In Proc. of the Workshop on Geometrical
Models of Natural Language Semantics.
H. Wang. 2005. Cross-document transliterated personal name
coreference resolution. In L. Wang and Y. Jin, editors, Fuzzy
Systems and Knowledge Discovery, volume 3614 of Lecture
Notes in Computer Science, pages 11?20. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
69
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 63?68,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
PARMA: A Predicate Argument Aligner
Travis Wolfe, Benjamin Van Durme, Mark Dredze, Nicholas Andrews,
Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder,
Jonathan Weese, Tan Xu?, and Xuchen Yao
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland USA
?University of Maryland, College Park, Maryland USA
Abstract
We introduce PARMA, a system for cross-
document, semantic predicate and argu-
ment alignment. Our system combines a
number of linguistic resources familiar to
researchers in areas such as recognizing
textual entailment and question answering,
integrating them into a simple discrimina-
tive model. PARMA achieves state of the
art results on an existing and a new dataset.
We suggest that previous efforts have fo-
cussed on data that is biased and too easy,
and we provide a more difficult dataset
based on translation data with a low base-
line which we beat by 17% F1.
1 Introduction
A key step of the information extraction pipeline
is entity disambiguation, in which discovered en-
tities across many sentences and documents must
be organized to represent real world entities. The
NLP community has a long history of entity dis-
ambiguation both within and across documents.
While most information extraction work focuses
on entities and noun phrases, there have been a
few attempts at predicate, or event, disambigua-
tion. Commonly a situational predicate is taken to
correspond to either an event or a state, lexically
realized in verbs such as ?elect? or nominaliza-
tions such as ?election?. Similar to entity coref-
erence resolution, almost all of this work assumes
unanchored mentions: predicate argument tuples
are grouped together based on coreferent events.
The first work on event coreference dates back to
Bagga and Baldwin (1999). More recently, this
task has been considered by Bejan and Harabagiu
(2010) and Lee et al (2012). As with unanchored
entity disambiguation, these methods rely on clus-
tering methods and evaluation metrics.
Another view of predicate disambiguation seeks
to link or align predicate argument tuples to an ex-
isting anchored resource containing references to
events or actions, similar to anchored entity dis-
ambiguation (entity linking) (Dredze et al, 2010;
Han and Sun, 2011). The most relevant, and per-
haps only, work in this area is that of Roth and
Frank (2012) who linked predicates across docu-
ment pairs, measuring the F1 of aligned pairs.
Here we present PARMA, a new system for pred-
icate argument alignment. As opposed to Roth and
Frank, PARMA is designed as a a trainable plat-
form for the incorporation of the sort of lexical se-
mantic resources used in the related areas of Rec-
ognizing Textual Entailment (RTE) and Question
Answering (QA). We demonstrate the effective-
ness of this approach by achieving state of the art
performance on the data of Roth and Frank despite
having little relevant training data. We then show
that while the ?lemma match? heuristic provides a
strong baseline on this data, this appears to be an
artifact of their data creation process (which was
heavily reliant on word overlap). In response, we
evaluate on a new and more challenging dataset for
predicate argument alignment derived from multi-
ple translation data. We release PARMA as a new
framework for the incorporation and evaluation of
new resources for predicate argument alignment.1
2 PARMA
PARMA (Predicate ARguMent Aligner) is a
pipelined system with a wide variety of features
used to align predicates and arguments in two doc-
uments. Predicates are represented as mention
spans and arguments are represented as corefer-
ence chains (sets of mention spans) provided by
in-document coreference resolution systems such
as included in the Stanford NLP toolkit. Results
indicated that the chains are of sufficient quality
so as not to limit performance, though future work
1https://github.com/hltcoe/parma
63
RF
? Australian [police]1 have [arrested]2 a man in the western city of Perth over an alleged [plot]3 to [bomb]4 Israeli diplomatic
[buildings]5 in the country , police and the suspect s [lawyer]6 [said]7
? Federal [police]1 have [arrested]2 a man over an [alleged]5 [plan]3 to [bomb]4 Israeli diplomatic [posts]8 in Australia , the
suspect s [attorney]6 [said]7 Tuesday
LDC MTC
? As I [walked]1 to the [veranda]2 side , I [saw]2 that a [tent]3 is being decorated for [Mahfil-e-Naat]4 -LRB- A [get-together]5
in which the poetic lines in praise of Prophet Mohammad are recited -RRB-
? I [came]1 towards the [balcony]2 , and while walking over there I [saw]2 that a [camp]3 was set up outside for the [Naatia]4
[meeting]5 .
Figure 1: Example of gold-standard alignment pairs from Roth and Frank?s data set and our data set
created from the LDC?s Multiple Translation Corpora. The RF data set exhibits high lexical overlap,
where most of the alignments are between identical words like police-police and said-said. The LDC
MTC was constructed to increase lexical diversity, leading to more challenging alignments like veranda-
balcony and tent-camp
may relax this assumption.
We refer to a predicate or an argument as an
?item? with type predicate or argument. An align-
ment between two documents is a subset of all
pairs of items in either documents with the same
type.2 We call the two documents being aligned
the source document S and the target document
T . Items are referred to by their index, and ai,j is a
binary variable representing an alignment between
item i in S and item j in T . A full alignment is an
assignment ~a = {aij : i ? NS , j ? NT }, where
NS and NT are the set of item indices for S and T
respectively.
We train a logistic regression model on exam-
ple alignmentsand maximize the likelihood of a
document alignment under the assumption that the
item alignments are independent. Our objective
is to maximize the log-likelihood of all p(S, T )
with an L1 regularizer (with parameter ?). After
learning model parameters w by regularized max-
imum likelihood on training data, we introducing
a threshold ? on alignment probabilities to get a
classifier. We perform line search on ? and choose
the value that maximizes F1 on dev data. Train-
ing was done using the Mallet toolkit (McCallum,
2002).
2.1 Features
The focus of PARMA is the integration of a diverse
range of features based on existing lexical seman-
tic resources. We built PARMA on a supervised
framework to take advantage of this wide variety
of features since they can describe many different
correlated aspects of generation. The following
features cover the spectrum from high-precision
2Note that type is not the same thing as part of speech: we
allow nominal predicates like ?death?.
to high-recall. Each feature has access to the pro-
posed argument or predicate spans to be linked and
the containing sentences as context. While we use
supervised learning, some of the existing datasets
for this task are very small. For extra training data,
we pool material from different datasets and use
the multi-domain split feature space approach to
learn dataset specific behaviors (Daume?, 2007).
Features in general are defined over mention
spans or head tokens, but we split these features
to create separate feature-spaces for predicates and
arguments.3
For argument coref chains we heuristically
choose a canonical mention to represent each
chain, and some features only look at this canon-
ical mention. The canonical mention is cho-
sen based on length,4 information about the head
word,5 and position in the document.6 In most
cases, coref chains that are longer than one are
proper nouns and the canonical mention is the first
and longest mention (outranking pronominal ref-
erences and other name shortenings).
PPDB We use lexical features from the Para-
phrase Database (PPDB) (Ganitkevitch et al,
2013). PPDB is a large set of paraphrases ex-
tracted from bilingual corpora using pivoting tech-
niques. We make use of the English lexical portion
which contains over 7 million rules for rewriting
terms like ?planet? and ?earth?. PPDB offers a
variety of conditional probabilities for each (syn-
chronous context free grammar) rule, which we
3While conceptually cleaner, In practice we found this
splitting to have no impact on performance.
4in tokens, not counting some words like determiners and
auxiliary verbs
5like its part of speech tag and whether the it was tagged
as a named entity
6mentions that appear earlier in the document and earlier
in a given sentence are given preference
64
treat as independent experts. For each of these rule
probabilities (experts), we find all rules that match
the head tokens of a given alignment and have a
feature for the max and harmonic mean of the log
probabilities of the resulting rule set.
FrameNet FrameNet is a lexical database based
on Charles Fillmore?s Frame Semantics (Fill-
more, 1976; Baker et al, 1998). The database
(and the theory) is organized around seman-
tic frames that can be thought of as descrip-
tions of events. Frames crucially include spec-
ification of the participants, or Frame Elements,
in the event. The Destroying frame, for in-
stance, includes frame elements Destroyer or
Cause Undergoer. Frames are related to other
frames through inheritance and perspectivization.
For instance the frames Commerce buy and
Commerce sell (with respective lexical real-
izations ?buy? and ?sell?) are both perspectives of
Commerce goods-transfer (no lexical re-
alizations) which inherits from Transfer (with
lexical realization ?transfer?).
We compute a shortest path between headwords
given edges (hypernym, hyponym, perspectivized
parent and child) in FrameNet and bucket by dis-
tance to get features. We also have a binary feature
for whether two tokens evoke the same frame.
TED Alignments Given two predicates or argu-
ments in two sentences, we attempt to align the
two sentences they appear in using a Tree Edit
Distance (TED) model that aligns two dependency
trees, based on the work described by (Yao et al,
2013). We represent a node in a dependency tree
with three fields: lemma, POS tag and the type
of dependency relation to the node?s parent. The
TED model aligns one tree with the other using
the dynamic programming algorithm of Zhang and
Shasha (1989) with three predefined edits: dele-
tion, insertion and substitution, seeking a solution
yielding the minimum edit cost. Once we have
built a tree alignment, we extract features for 1)
whether the heads of the two phrases are aligned
and 2) the count of how many tokens are aligned
in both trees.
WordNet WordNet (Miller, 1995) is a database
of information (synonyms, hypernyms, etc.) per-
taining to words and short phrases. For each entry,
WordNet provides a set of synonyms, hypernyms,
etc. Given two spans, we use WordNet to deter-
mine semantic similarity by measuring how many
synonym (or other) edges are needed to link two
terms. Similar words will have a short distance.
For features, we find the shortest path linking the
head words of two mentions using synonym, hy-
pernym, hyponym, meronym, and holonym edges
and bucket the length.
String Transducer To represent similarity be-
tween arguments that are names, we use a stochas-
tic edit distance model. This stochastic string-to-
string transducer has latent ?edit? and ?no edit?
regions where the latent regions allow the model
to assign high probability to contiguous regions of
edits (or no edits), which are typical between vari-
ations of person names. In an edit region, param-
eters govern the relative probability of insertion,
deletion, substitution, and copy operations. We
use the transducer model of Andrews et al (2012).
Since in-domain name pairs were not available, we
picked 10,000 entities at random from Wikipedia
to estimate the transducer parameters. The entity
labels were used as weak supervision during EM,
as in Andrews et al (2012).
For a pair of mention spans, we compute the
conditional log-likelihood of the two mentions go-
ing both ways, take the max, and then bucket to get
binary features. We duplicate these features with
copies that only fire if both mentions are tagged as
PER, ORG or LOC.
3 Evaluation
We consider three datasets for evaluating PARMA.
For richer annotations that include lemmatiza-
tions, part of speech, NER, and in-doc corefer-
ence, we pre-processed each of the datasets using
tools7 similar to those used to create the Annotated
Gigaword corpus (Napoles et al, 2012).
Extended Event Coreference Bank Based on
the dataset of Bejan and Harabagiu (2010), Lee et
al. (2012) introduced the Extended Event Coref-
erence Bank (EECB) to evaluate cross-document
event coreference. EECB provides document clus-
ters, within which entities and events may corefer.
Our task is different from Lee et al but we can
modify the corpus setup to support our task. To
produce source and target document pairs, we se-
lect the first document within every cluster as the
source and each of the remaining documents as
target documents (i.e. N ? 1 pairs for a cluster
of size N ). This yielded 437 document pairs.
Roth and Frank The only existing dataset for
our task is from Roth and Frank (2012) (RF), who
7https://github.com/cnap/anno-pipeline
65
annotated documents from the English Gigaword
Fifth Edition corpus (Parker et al, 2011). The data
was generated by clustering similar news stories
from Gigaword using TF-IDF cosine similarity of
their headlines. This corpus is small, containing
only 10 document pairs in the development set and
60 in the test set. To increase the training size,
we train PARMA with 150 randomly selected doc-
ument pairs from both EECB and MTC, and the
entire dev set from Roth and Frank using multi-
domain feature splitting. We tuned the threshold
? on the Roth and Frank dev set, but choose the
regularizer ? based on a grid search on a 5-fold
version of the EECB dataset.
Multiple Translation Corpora We constructed
a new predicate argument alignment dataset
based on the LDC Multiple Translation Corpora
(MTC),8 which consist of multiple English trans-
lations for foreign news articles. Since these mul-
tiple translations are semantically equivalent, they
provide a good resource for aligned predicate ar-
gument pairs. However, finding good pairs is a
challenge: we want pairs with significant overlap
so that they have predicates and arguments that
align, but not documents that are trivial rewrites
of each other. Roth and Frank selected document
pairs based on clustering, meaning that the pairs
had high lexical overlap, often resulting in mini-
mal rewrites of each other. As a result, despite ig-
noring all context, their baseline method (lemma-
alignment) worked quite well.
To create a more challenging dataset, we se-
lected document pairs from the multiple transla-
tions that minimize the lexical overlap (in En-
glish). Because these are translations, we know
that there are equivalent predicates and arguments
in each pair, and that any lexical variation pre-
serves meaning. Therefore, we can select pairs
with minimal lexical overlap in order to create
a system that truly stresses lexically-based align-
ment systems.
Each document pair has a correspondence be-
tween sentences, and we run GIZA++ on these
sentences to produce token-level alignments. We
take all aligned nouns as arguments and all aligned
verbs (excluding be-verbs, light verbs, and report-
ing verbs) as predicates. We then add negative ex-
amples by randomly substituting half of the sen-
tences in one document with sentences from an-
8LDC2010T10, LDC2010T11, LDC2010T12,
LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01,
LDC2003T18, and LDC2005T05
0.3 0.4 0.5 0.6 0.7 0.8
0.0
0.2
0.4
0.6
0.8
1.0
Performance vs Lexical Overlap
Doc-pair Cosine Similarity
F1
Figure 2: We plotted the PARMA?s performance on
each of the document pairs. Red squares show the
F1 for individual document pairs drawn from Roth
and Frank?s data set, and black circles show F1 for
our Multiple Translation Corpora test set. The x-
axis represents the cosine similarity between the
document pairs. On the RF data set, performance
is correlated with lexical similarity. On our more
lexically diverse set, this is not the case. This
could be due to the fact that some of the docu-
ments in the RF sets are minor re-writes of the
same newswire story, making them easy to align.
other corpus, guaranteed to be unrelated. The
amount of substitutions we perform can vary the
?relatedness? of the two documents in terms of
the predicates and arguments that they talk about.
This reflects our expectation of real world data,
where we do not expect perfect overlap in predi-
cates and arguments between a source and target
document, as you would in translation data.
Lastly, we prune any document pairs that have
more than 80 predicates or arguments or have a
Jaccard index on bags of lemmas greater than 0.5,
to give us a dataset of 328 document pairs.
Metric We use precision, recall, and F1. For the
RF dataset, we follow Roth and Frank (2012) and
Cohn et al (2008) and evaluate on a version of F1
that considers SURE and POSSIBLE links, which
are available in the RF data. Given an alignment
to be scored A and a reference alignment B which
contains SURE and POSSIBLE links, Bs andBp re-
spectively, precision and recall are:
P = |A ?Bp||A| R =
|A ?Bs|
|Bs|
(1)
66
F1 P R
EECB lemma 63.5 84.8 50.8
PARMA 74.3 80.5 69.0
RF lemma 48.3 40.3 60.3
Roth and Frank 54.8 59.7 50.7
PARMA 57.6 52.4 64.0
MTC lemma 42.1 51.3 35.7
PARMA 59.2 73.4 49.6
Table 1: PARMA outperforms the baseline lemma
matching system on the three test sets, drawn from
the Extended Event Coreference Bank, Roth and
Frank?s data, and our set created from the Multiple
Translation Corpora. PARMA achieves a higher F1
and recall score than Roth and Frank?s reported
result.
and F1 as the harmonic mean of the two. Results
for EECB and MTC reflect 5-fold cross validation,
and RF uses the given dev/test split.
Lemma baseline Following Roth and Frank we
include a lemma baseline, in which two predicates
or arguments align if they have the same lemma.9
4 Results
On every dataset PARMA significantly improves
over the lemma baselines (Table 1). On RF,
compared to Roth and Frank, the best published
method for this task, we also improve, making
PARMA the state of the art system for this task.
Furthermore, we expect that the smallest improve-
ments over Roth and Frank would be on RF, since
there is little training data. We also note that com-
pared to Roth and Frank we obtain much higher
recall but lower precision.
We also observe that MTC was more challeng-
ing than the other datasets, with a lower lemma
baseline. Figure 2 shows the correlation between
document similarity and document F1 score for
RF and MTC. While for RF these two measures
are correlated, they are uncorrelated for MTC. Ad-
ditionally, there is more data in the MTC dataset
which has low cosine similarity than in RF.
5 Conclusion
PARMA achieves state of the art performance on
three datasets for predicate argument alignment.
It builds on the development of lexical semantic
resources and provides a platform for learning to
utilize these resources. Additionally, we show that
9We could not reproduce lemma from Roth and Frank
(shown in Table 1) due to a difference in lemmatizers. We ob-
tained 55.4; better than their system but worse than PARMA.
task difficulty can be strongly tied to lexical simi-
larity if the evaluation dataset is not chosen care-
fully, and this provides an artificially high baseline
in previous work. PARMA is robust to drops in lex-
ical similarity and shows large improvements in
those cases. PARMA will serve as a useful bench-
mark in determining the value of more sophis-
ticated models of predicate-argument alignment,
which we aim to address in future work.
While our system is fully supervised, and thus
dependent on manually annotated examples, we
observed here that this requirement may be rela-
tively modest, especially for in-domain data.
Acknowledgements
We thank JHU HLTCOE for hosting the winter
MiniSCALE workshop that led to this collabora-
tive work. This material is based on research spon-
sored by the NSF under grant IIS-1249516 and
DARPA under agreement number FA8750-13-2-
0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints
for Governmental purposes. The views and con-
clusions contained in this publication are those of
the authors and should not be interpreted as repre-
senting official policies or endorsements of NSF,
DARPA, or the U.S. Government.
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Empirical Methods in Natural
Language Processing (EMNLP).
Amit Bagga and Breck Baldwin. 1999. Cross-
document event coreference: Annotations, exper-
iments, and observations. In Proceedings of the
Workshop on Coreference and its Applications,
pages 1?8. Association for Computational Linguis-
tics.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of the 36th Annual Meeting of the Associa-
tion for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics -
Volume 1, ACL ?98, pages 86?90, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
67
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
put. Linguist., 34(4):597?614, December.
Hal Daume?. 2007. Frustratingly easy domain adap-
tation. In Annual meeting-association for computa-
tional linguistics, volume 45, page 256.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Conference on
Computational Linguistics (Coling).
Charles J. Fillmore. 1976. Frame semantics and
the nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin and
Development of Language and Speech, 280(1):20?
32.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL).
Xianpei Han and Le Sun. 2011. A generative entity-
mention model for linking entities with knowledge
base. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies-Volume 1, pages 945?
954. Association for Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity and
event coreference resolution across documents. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In AKBC-
WEKEX Workshop at NAACL 2012, June.
Robert Parker, David Graff, Jumbo Kong, Ke Chen,
and Kazuaki Maeda. 2011. English gigaword fifth
edition.
Michael Roth and Anette Frank. 2012. Aligning predi-
cate argument structures in monolingual comparable
texts: A new corpus for a new task. In *SEM 2012:
The First Joint Conference on Lexical and Compu-
tational Semantics ? Volume 1: Proceedings of the
main conference and the shared task, and Volume 2:
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 218?
227, Montre?al, Canada, 7-8 June. Association for
Computational Linguistics.
Xuchen Yao, Benjamin Van Durme, Peter Clark, and
Chris Callison-Burch. 2013. Answer extraction as
sequence tagging with tree edit distance. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262, De-
cember.
68
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775?785,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Robust Entity Clustering via Phylogenetic Inference
Nicholas Andrews and Jason Eisner and Mark Dredze
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
3400 N. Charles St., Baltimore, MD 21218 USA
{noa,eisner,mdredze}@jhu.edu
Abstract
Entity clustering must determine when two
named-entity mentions refer to the same
entity. Typical approaches use a pipeline ar-
chitecture that clusters the mentions using
fixed or learned measures of name and con-
text similarity. In this paper, we propose a
model for cross-document coreference res-
olution that achieves robustness by learn-
ing similarity from unlabeled data. The
generative process assumes that each entity
mention arises from copying and option-
ally mutating an earlier name from a sim-
ilar context. Clustering the mentions into
entities depends on recovering this copying
tree jointly with estimating models of the
mutation process and parent selection pro-
cess. We present a block Gibbs sampler for
posterior inference and an empirical evalu-
ation on several datasets.
1 Introduction
Variation poses a serious challenge for determin-
ing who or what a name refers to. For instance,
Wikipedia contains more than 100 variations of the
name Barack Obama as redirects to the U.S. Presi-
dent article, including:
President Obama Barack H. Obama, Jr.
Barak Obamba Barry Soetoro
To relate different names, one solution is to use
specifically tailored measures of name similarity
such as Jaro-Winkler similarity (Winkler, 1999; Co-
hen et al, 2003). This approach is brittle, however,
and fails to adapt to the test data. Another option is
to train a model like stochastic edit distance from
known pairs of similar names (Ristad and Yian-
ilos, 1998; Green et al, 2012), but this requires
supervised data in the test domain.
Even the best model of name similarity is not
enough by itself, since two names that are similar?
even identical?do not necessarily corefer. Docu-
ment context is needed to determine whether they
may be talking about two different people.
In this paper, we propose a method for jointly
(1) learning similarity between names and (2) clus-
tering name mentions into entities, the two major
components of cross-document coreference reso-
lution systems (Baron and Freedman, 2008; Finin
et al, 2009; Rao et al, 2010; Singh et al, 2011;
Lee et al, 2012; Green et al, 2012). Our model
is an evolutionary generative process based on the
name variation model of Andrews et al (2012),
which stipulates that names are often copied from
previously generated names, perhaps with mutation
(spelling edits). This can deduce that rather than
being names for different entities, Barak Obamba
and Barock obama more likely arose from the fre-
quent name Barack Obama as a common ancestor,
which accounts for most of their letters. This can
also relate seemingly dissimilar names via multiple
steps in the generative process:
Taylor Swift? T-Swift? T-Swizzle
Our model learns without supervision that these all
refer to the the same entity. Such creative spellings
are especially common on Twitter and other so-
cial media; we give more examples of coreferents
learned by our model in Section 8.4.
Our primary contributions are improvements on
Andrews et al (2012) for the entity clustering task.
Their inference procedure only clustered types (dis-
tinct names) rather than tokens (mentions in con-
text), and relied on expensive matrix inversions for
learning. Our novel approach features:
?4.1 A topical model of which entities from previ-
ously written text an author tends to mention
from previously written text.
?4.2 A name mutation model that is sensitive to
features of the input and output characters and
takes a reader?s comprehension into account.
?5 A scalable Markov chain Monte Carlo sam-
pler used in training and inference.
775
?7 A minimum Bayes risk decoding procedure
to pick an output clustering. The procedure is
applicable to any model capable of producing
a posterior over coreference decisions.
We evaluate our approach by comparing to sev-
eral baselines on datasets from three different gen-
res: Twitter, newswire, and blogs.
2 Overview and Related Work
Cross-document coreference resolution (CDCR)
was first introduced by Bagga and Baldwin (1998b).
Most approaches since then are based on the intu-
itions that coreferent names tend to have ?similar?
spellings and tend to appear in ?similar? contexts.
The distinguishing feature of our system is that both
notions of similarity are learned together without
supervision.
We adopt a ?phylogenetic? generative model of
coreference. The basic insight is that coreference is
created when an author thinks of an entity that was
mentioned earlier in a similar context, and men-
tions it again in a similar way. The author may
alter the name mention string when copying it, but
both names refer to the same entity. Either name
may later be copied further, leading to an evolution-
ary tree of mentions?a phylogeny. Phylogenetic
models are new to information extraction. In com-
putational historical linguistics, Bouchard-C?ot?e et
al. (2013) have also modeled the mutation of strings
along the edges of a phylogeny; but for them the
phylogeny is observed and most mentions are not,
while we observe the mentions only.
To apply our model to the CDCR task, we ob-
serve that the probability that two name mentions
are coreferent is the probability that they arose from
a common ancestor in the phylogeny. So we design
a Monte Carlo sampler to reconstruct likely phylo-
genies. A phylogeny must explain every observed
name. While our model is capable of generating
each name independently, a phylogeny will gener-
ally achieve higher probability if it explains similar
names as being similar by mutation (rather than
by coincidence). Thus, our sampled phylogenies
tend to make similar names coreferent?especially
long or unusual names that would be expensive to
generate repeatedly, and especially in contexts that
are topically similar and therefore have a higher
prior probability of coreference.
For learning, we iteratively adjust our model?s
parameters to better explain our samples. That is,
we do unsupervised training via Monte Carlo EM.
What is learned? An important component of
a CDCR system is its model of name similarity
(Winkler, 1999; Porter and Winkler, 1997), which
is often fixed up front. This role is played in our sys-
tem by the name mutation model, which we take to
be a variant of stochastic edit distance (Ristad and
Yianilos, 1996). Rather than fixing its parameters
before we begin CDCR, we learn them (without
supervision) as part of CDCR, by training from
samples of reconstructed phylogenies.
Name similarity is also an important component
of within-document coreference resolution, and ef-
forts in that area bear resemblance to our approach.
Haghighi and Klein (2010) describe an ?entity-
centered? model where a distance-dependent Chi-
nese restaurant process is used to pick previous
coreferent mentions within a document. Similarly,
Durrett and Klein (2013) learn a mention similarity
model based on labeled data. Our cross-document
setting has no observed mention ordering and no
observed entities: we must sum over all possibili-
ties, a challenging inference problem.
The second major component of CDCR is
context-based disambiguation of similar or iden-
tical names that refer to the same entity. Like
Kozareva and Ravi (2011) and Green et al (2012)
we use topics as the contexts, but learn mention
topics jointly with other model parameters.
3 Generative Model of Coreference
Let x = (x
1
, . . . , x
N
) denote an ordered sequence
of distinct named-entity mentions in documents
d = (d
1
, . . . , d
D
). We assume that each doc-
ument has a (single) known language, and that
its mentions and their types have been identified
by a named-entity recognizer. We use the object-
oriented notation x.v for attribute v of mention x.
Our model generates an ordered sequence x al-
though we do not observe its order. Thus each men-
tion x has latent position x.i (e.g., x
729
.i = 729).
The entire corpus, including these entities, is gen-
erated according to standard topic model assump-
tions; we first generate a topic distribution for a
document, then sample topics and words for the
document (Blei et al, 2003). However, any topic
may generate an entity type, e.g. PERSON, which is
then replaced by a specific name: when PERSON is
generated, the model chooses a previous mention
of any person and copies it, perhaps mutating its
name.
1
Alternatively, the model may manufacture
1
We make the closed-world assumption that the author is
776
a name for a new person, though the name itself
may not be new.
If all previous mentions were equally likely, this
would be a Chinese Restaurant Process (CRP) in
which frequently mentioned entities are more likely
to be mentioned again (?the rich get richer?). We
refine that idea by saying that the current topic, lan-
guage, and document influence the choice of which
previous mention to copy, similar to the distance-
dependent CRP (Blei and Frazier, 2011).
2
This will
help distinguish multiple John Smith entities if they
tend to appear in different contexts.
Formally, each mention x is derived from a par-
ent mention x.p where x.p.i < x.i (the parent
came first), x.e = x.p.e (same entity) and x.n is
a copy or mutation of x.p.n. In the special case
where x is a first mention of x.e, x.p is the special
symbol ?, x.e is a newly allocated entity of some
appropriate type, and the name x.n is generated
from scratch.
Our goal is to reconstruct mappings p, i, z that
specify the latent properties of the mentions x. The
mapping p : x 7? x.p forms a phylogenetic tree on
the mentions, with root?. Each entity corresponds
to a subtree that is rooted at some child of ?. The
mapping i : x 7? x.i gives an ordering consistent
with that tree in the sense that (?x)x.p.i < x.i.
Finally, the mapping z : x 7? x.z specifies, for
each mention, the topic that generated it. While i
and z are not necessary for creating coref clusters,
they are needed to produce p.
4 Detailed generative story
Given a few constants that are referenced in the
main text, we assume that the corpus d was gener-
ated as follows.
First, for each topic z = 1, . . .K and each lan-
guage `, choose a multinomial ?
z`
over the word
vocabulary, from a symmetric Dirichlet with con-
centration parameter ?. Then set m = 0 (entity
only aware of previous mentions from our corpus. This means
that two mentions cannot be derived from a common ancestor
outside our corpus. To mitigate this unrealistic assumption, we
allow any ordering x of the observed mentions, not respecting
document timestamps or forcing the mentions from a given
document to be generated as a contiguous subsequence of x.
2
Unlike the ddCRP, our generative story is careful to pro-
hibit derivational cycles: each mention is copied from a previ-
ous mention in the latent ordering. This is why our phylogeny
is a tree, and why our sampler is more complex. Also unlike
the ddCRP, we permit asymmetric ?distances?: if a certain
topic or language likes to copy mentions from another, the
compliment is not necessarily returned.
count), i = 0 (mention count), and for each docu-
ment index d = 1, . . . , D:
1. Choose the document?s length L and language
`. (The distributions used to choose these
are unimportant because these variables are
always observed.)
2. Choose its topic distribution ?
d
from an
asymmetric Dirichlet prior with parameters
m (Wallach et al, 2009).
3
3. For each token position k = 1, . . . , L:
(a) Choose a topic z
dk
? ?
d
.
(b) Choose a word conditioned on the topic
and language, w
dk
? ?
z
dk
`
.
(c) If w
dk
is a named entity type (PERSON,
PLACE, ORG, . . . ) rather than an ordinary
word, then increment i and:
i. create a new mention x with
x.e.t = w
dk
x.d = d x.` = `
x.i = i x.z = z
dk
x.k = k
ii. Choose the parent x.p from a distri-
bution conditioned on the attributes
just set (see ?4.1).
iii. If x.p = ?, increment m and set
x.e = a new entity e
m
. Else set
x.e = x.p.e.
iv. Choose x.n from a distribution con-
ditioned on x.p.n and x.` (see ?4.2).
Notice that the tokens w
dk
in document d are
exchangeable: by collapsing out ?
d
, we can re-
gard them as having been generated from a CRP.
Thus, for fixed values of the non-mention tokens
and their topics, the probability of generating the
mention sequence x is proportional to the prod-
uct of the probabilities of the choices in step 3 at
the positions dk where mentions were generated.
These choices generate a topic x.z (from the CRP
for document d), a type x.e.t (from ?
x.z
), a par-
ent mention (from the distribution over previous
mentions), and a name string (conditioned on the
parent?s name if any). ?5 uses this fact to construct
an MCMC sampler for the latent parts of x.
4.1 Sub-model for parent selection
To select a parent for a mention x of type t = x.e.t,
a simple model (as mentioned above) would be a
CRP: each previous mention of the same type is
selected with probability proportional to 1, and? is
3
Extension: This choice could depend on the language d.`.
777
selected with probability proportional to ?
t
> 0. A
larger choice of ?
t
results in smaller entity clusters,
because it prefers to create new entities of type t
rather than copying old ones.
We modify this story by re-weighting ? and
previous mentions according to their relative suit-
ability as the parent of x:
Pr?(x.p | x) =
exp (? ? f(x.p, x))
Z(x)
(1)
where x.p ranges over ? and all previous mentions
of the same type as x, that is, mentions p such that
p.i < x.i and p.e.t = x.e.t. The normalizing con-
stant Z(x)
def
=
?
p
exp (? ? f(x.p, x)) is chosen
so that the probabilities sum to 1.
This is a conditional log-linear model parameter-
ized by ?, where ?
k
? N (0, ?
2
k
). The features f
are extracted from the attributes of x and x.p. Our
most important feature tests whether x.p.z = x.z.
This binary feature has a high weight if authors
mainly choose mentions from the same topic. To
model which (other) topics tend to be selected, we
also have a binary feature for each parent topic
x.p.z and each topic pair (x.p.z, x.z).
4
4.2 Sub-model for name mutation
Let x denote a mention with parent p = x.p. As in
Andrews et al (2012), its name x.n is a stochastic
transduction of its parent?s name p.n. That is,
Pr?(x.n | p.n) (2)
is given by the probability that applying a random
sequence of edits to the characters of p.n would
yield x.n. The contextual probabilities of different
edits depend on learned parameters ?.
(2) is the total probability of all edit sequences
that derive x.n from p.n. It can be computed in
time O(|x.n| ? |p.n|) by dynamic programming.
The probability of a single edit sequence, which
corresponds to a monotonic alignment of x.n to
p.n, is a product of individual edit probabilities of
the form Pr?((
a
b
) | a?), which is conditioned on the
next input character a?. The edit (
a
b
) replaces input
a ? {, a?} with output b ? {} ? ? (where  is
4
Many other features could be added. In a multilingual
setting, one would similarly want to model whether English
authors select Arabic mentions. One could also imagine fea-
tures that reward proximity in the generative order (x.p.i ?
x.i), local linguistic relationships (when x.p.d = x.d and
x.p.k ? x.k), or social information flow (e.g., from main-
stream media to Twitter). One could also make more specific
versions of any feature by conjoining it with the entity type t.
the empty string and ? is the alphabet of language
x.`). Insertions and deletions are the cases where
respectively a =  or b = ?we do not allow both
at once. All other edits are substitutions. When
a? is the special end-of-string symbol #, the only
allowed edits are the insertion (

b
) and the substi-
tution (
#
#
). We define the edit probability using a
locally normalized log-linear model:
Pr?((
a
b
) | a?) =
exp(? ? f(a?, a, b))
?
a
?
,b
?
exp(? ? f(a?, a
?
, b
?
))
(3)
We use a small set of simple feature functions f ,
which consider conjunctions of the attributes of the
characters a? and b: character, character class (letter,
digit, etc.), and case (upper vs. lower).
More generally, the probability (2) may also be
conditioned on other variables such as on the lan-
guages p.` and x.`?this leaves room for a translit-
eration model when x.` 6= p.`?and on the entity
type x.t. The features in (3) may then depend on
these variables as well.
Notice that we use a locally normalized proba-
bility for each edit. This enables faster and sim-
pler training than the similar model of Dreyer et al
(2008), which uses a globally normalized probabil-
ity for the whole edit sequence.
When p = ?, we are generating a new name x.n.
We use the same model, taking?.n to be the empty
string (but with #
?
rather than # as the end-of-
string symbol). This yields a feature-based unigram
language model (whose character probabilities may
differ from usual insertion probabilities because
they see #
?
as the lookahead character).
Pragmatics. We can optionally make the model
more sophisticated. Authors tend to avoid names
x.n that readers would misinterpret (given the pre-
viously generated names). The edit model thinks
that Pr?(CIA | ?) is relatively high (because CIA is
a short string) and so is Pr?(CIA | Chuck?s Ice Art).
But in fact, if CIA has already been frequently used
to refer to the Central Intelligence Agency, then an
author is unlikely to use it for a different entity.
To model this pragmatic effect, we multiply
our definition of Pr?(x.n | p.n) by an extra fac-
tor Pr(x.e | x)
?
, where ? ? 0 is the effect
strength.
5
Here Pr(x.e | x) is the probability that
a reader correctly identifies the entity x.e. We
take this to be the probability that a reader who
knows our sub-models would guess some parent
5
Currently we omit the step of renormalizing this deficient
model. Our training procedure also ignores the extra factor.
778
having the correct entity (or ? if x is a first men-
tion):
?
p
?
:p
?
.e=x.e
w(p
?
, x)/
?
p
?
w(p
?
, x). Here p
?
ranges over mentions (including ?) that precede
x in the ordering i, and w(p
?
, x)?defined later in
sec. 5.3?is proportional to the posterior probabil-
ity that x.p = p
?
, given name x.n and topic x.z.
6
5 Inference by Block Gibbs Sampling
We use a block Gibbs sampler, which from an ini-
tial state (p
0
, i
0
, z
0
) repeats these steps:
1. Sample the ordering i from its conditional
distribution given all other variables.
2. Sample the topic vector z likewise.
3. Sample the phylogeny p likewise.
4. Output the current sample s
t
= (p, i, z).
It is difficult to draw exact samples at steps 1
and 2. Thus, we sample i or z from a simpler
proposal distribution, but correct the discrepancy
using the Independent Metropolis-Hastings (IMH)
strategy: with an appropriate probability, reject the
proposed new value and instead use another copy
of the current value (Tierney, 1994).
5.1 Resampling the ordering i
We resample the ordering i of the mentions x,
conditioned on the other variables. The current
phylogeny p already defines a partial order on x,
since each parent must precede its children. For
instance, phylogeny (a) below requires ? ? x and
? ? y. This partial order is compatible with 2
total orderings, ? ? x ? y and ? ? y ? x. By
contrast, phylogeny (b) requires the total ordering
? ? x ? y.
?
yx
(a)
?
x
y
(b)
We first sample an ordering i
?
(the ordering
of mentions with parent ?, i.e. all mentions) uni-
formly at random from the set of orderings compat-
ible with the current p. (We provide details about
this procedure in Appendix A.)
7
However, such or-
derings are not in fact equiprobable given the other
variables?some orderings better explain why that
phylogeny was chosen in the first place, according
6
Better, one could integrate over the reader?s guess of x.z.
7
The full version of this paper is available at
http://cs.jhu.edu/
?
noa/publications/
phylo-acl-14.pdf
to our competitive parent selection model (?4.1).
To correct for this bias using IMH, we accept the
proposed ordering i
?
with probability
a = min
(
1,
Pr(p, i?, z,x | ?,?)
Pr(p, i, z,x | ?,?)
)
(4)
where i is the current ordering. Otherwise we reject
i
?
and reuse i for the new sample.
5.2 Resampling the topics z
Each context word and each named entity is asso-
ciated with a latent topic. The topics of context
words are assumed exchangeable, and so we re-
sample them using Gibbs sampling (Griffiths and
Steyvers, 2004).
Unfortunately, this is prohibitively expensive for
the (non-exchangeable) topics of the named men-
tions x. A Gibbs sampler would have to choose
a new value for x.z with probability proportional
to the resulting joint probability of the full sample.
This probability is expensive to evaluate because
changing x.z will change the probability of many
edges in the current phylogeny p. (Equation (1)
puts x is in competition with other parents, so ev-
ery mention y that follows x must recompute how
happy it is with its current parent y.p.)
Rather than resampling one topic at a time, we re-
sample z as a block. We use a proposal distribution
for which block sampling is efficient, and use IMH
to correct the error in this proposal distribution.
Our proposal distribution is an undirected graph-
ical model whose random variables are the topics
z and whose graph structure is given by the current
phylogeny p:
Q(z) ?
?
x 6=?
?
x
(x.z)?
x.p,x
(x.p.z, x.z) (5)
Q(z) is an approximation to the posterior distri-
bution over z. As detailed below, a proposal can
be sampled from Q(z) in time O(|z|K
2
) where K
is the number of topics, because the only interac-
tions among topics are along the edges of the tree
p. The unary factor ?
x
gives a weight for each
possible value of x.z, and the binary factor ?
x.p,x
gives a weight for each possible value of the pair
(x.p.z, x.z).
The ?
x
(x.z) factors in (5) approximate the topic
model?s prior distribution over z. ?
x
(x.z) is pro-
portional to the probability that a Gibbs sampling
step for an ordinary topic model would choose this
value of x.z. This depends on whether?in the
779
current sample?x.z is currently common in x?s
document and x.t is commonly generated by x.z.
It ignores the fact that we will also be resampling
the topics of the other mentions.
The ?
x.p,x
factors in (5) approximate Pr(p |
z, i) (up to a constant factor), where p is the current
phylogeny. Specifically, ?
x.p,x
approximates the
probability of a single edge. It ought to be given
by (1), but we use only the numerator of (1), which
avoids modeling the competition among parents.
We sample from Q using standard methods, sim-
ilar to sampling from a linear-chain CRF by run-
ning the backward algorithm followed by forward
sampling. Specifically, we run the sum-product
algorithm from the leaves up to the root ?, at each
node x computing the following for each topic z:
?
x
(z)
def
= ?
x
(z) ?
?
y?children(x)
?
z
?
?
x,y
(z, z
?
) ? ?
y
(z
?
)
Then we sample from the root down to the leaves,
first sampling ?.z from ?
?
, then at each x 6= ?
sampling the topic x.z to be z with probability
proportional to ?
x.p,x
(x.p.z, z) ? ?
x
(z).
Again we use IMH to correct for the bias in Q:
we accept the resulting proposal
?
z with probability
min
(
1,
Pr(p, i,
?
z,x | ?,?)
Pr(p, i, z,x | ?,?)
?
Q(z)
Q(
?
z)
)
(6)
While Pr(p, i,
?
z,x | ?,?) might seem slow to
compute because it contains many factors (1) with
different denominators Z(x), one can share work
by visiting the mentions x in their order i. Most
summands in Z(x) were already included in Z(x
?
),
where x
?
is the latest previous mention having the
same attributes as x (e.g., same topic).
5.3 Resampling the phylogeny p
It is easy to resample the phylogeny. For each x, we
must choose a parent x.p from among the possible
parents p (having p.i < x.i and p.e.t = x.e.t).
Since the ordering i prevents cycles, the resulting
phylogeny p is indeed a tree.
Given the topics z, the ordering i, and the ob-
served names, we choose an x.p value according
to its posterior probability. This is proportional to
w(x.p, x)
def
= Pr?(x.p | x) ? Pr?(x.n | x.p.n),
independent of any other mention?s choice of par-
ent. The two factors here are given by (1) and (2)
respectively. As in the previous section, the de-
nominators Z(x) in the Pr(x.p | x) factors can be
computed efficiently with shared work.
With the pragmatic model (section 4.2), the par-
ent choices are no longer independent; then the
samples of p should be corrected by IMH as usual.
5.4 Initializing the sampler
The initial sampler state (z
0
,p
0
, i
0
) is obtained as
follows. (1) We fix topics z
0
via collapsed Gibbs
sampling (Griffiths and Steyvers, 2004). The sam-
pler is run for 1000 iterations, and the final sam-
pler state is taken to be z
0
. This process treats all
topics as exchangeable, including those associated
with named entities.(2) Given the topic assignment
z
0
, initialize p
0
to the phylogeny rooted at ? that
maximizes
?
x
logw(x.p, x). This is a maximum
rooted directed spanning tree problem that can be
solved in time O(n
2
) (Tarjan, 1977). The weight
w(x.p, x) is defined as in section 5.3?except that
since we do not yet have an ordering i, we do not
restrict the possible values of x.p to mentions p
with p.i < x.p.i. (3) Given p
0
, sample an ordering
i
0
using the procedure described in ?5.1.
6 Parameter Estimation
Evaluating the likelihood and its partial derivatives
with respect to the parameters of the model requires
marginalizing over our latent variables. As this
marginalization is intractable, we resort to Monte
Carlo EM procedure (Levine and Casella, 2001)
which iterates the following two steps:
E-step: Collect samples by MCMC simulation as
in ?5, given current model parameters ? and ?.
M-step: Improve ? and ? to increase
8
L
def
=
1
S
S
?
s=1
log Pr?,?(x,ps, is, zs) (7)
It is not necessary to locally maximize L at each
M-step, merely to improve it if it is not already
at a local maximum (Dempster et al, 1977). We
improve it by a single update: at the tth M-step, we
update our parameters to ?
t
= (?
t
,?
t
)
?
t
= ?
t?1
+ ??
t
?
?
L(x,?
t?1
) (8)
where ? is a fixed scaling term and ?
t
is an adap-
tive learning rate given by AdaGrad (Duchi et al,
2011).
We now describe how to compute the gradient
?
?
L. The gradient with respect to the parent se-
8
We actually do MAP-EM, which augments (7) by adding
the log-likelihoods of ? and ? under a Gaussian prior.
780
lection parameters ? is
?
1
S
?
?
f(p, x)?
?
p
?
Pr?(p
?
| x)f(p
?
, x)
?
?
(9)
The outer summation ranges over all edges in the
S samples. The other variables in (9) are associ-
ated with the edge being summed over. That edge
explains a mention x as a mutation of some parent
p in the context of a particular sample (p
s
, i
s
, z
s
).
The possible parents p
?
range over ? and the men-
tions that precede x according to the ordering i
s
,
while the features f and distribution Pr? depend
on the topics z
s
.
As for the mutation parameters, let c
p,x
be the
fraction of samples in which p is the parent of x.
This is the expected number of times that the string
p.n mutated into x.n. Given this weighted set of
string pairs, let c
a?,a,b
be the expected number of
times that edit (
a
b
) was chosen in context a?: this
can be computed using dynamic programming to
marginalize over the latent edit sequence that maps
p.n to x.n, for each (p, x). The gradient of L with
respect to ? is
?
a?,a,b
c
a?,a,b
(f(a?, a, b)?
?
a
?
,b
?
Pr?(a
?
, b
?
| a?)f(a?, a
?
, b
?
))
(10)
7 Consensus Clustering
From a single phylogeny p, we deterministically
obtain a clustering e by removing the root ?. Each
of the resulting connected components corresponds
to a cluster of mentions. Our model gives a distribu-
tion over phylogenies p (given observations x and
learned parameters ?)?and thus gives a posterior
distribution over clusterings e, which can be used
to answer various queries.
A traditional query is to request a single cluster-
ing e. We prefer the clustering e
?
that minimizes
Bayes risk (MBR) (Bickel and Doksum, 1977):
e
?
= argmin
e?
?
e
L(e
?
, e) Pr(e | x,?,?) (11)
This minimizes our expected loss, where L(e
?
, e)
denotes the loss associated with picking e
?
when
the true clustering is e. In practice, we again esti-
mate the expectation by sampling e values.
The Rand index (Rand, 1971)?unlike our actual
evaluation measure?is an efficient choice of loss
function L for use with (11):
R(e
?
, e)
def
=
TP + TN
TP + FP + TN + FN
=
TP + TN
(
N
2
)
where the true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN) use
the clustering e to evaluate how well e
?
classi-
fies the
(
N
2
)
mention pairs as coreferent or not.
More similar clusterings achieve larger R, with
R(e
?
, e) = 1 iff e
?
= e. In all cases, 0 ?
R(e
?
, e) = R(e, e
?
) ? 1.
The MBR decision rule for the (negated) Rand
index is easily seen to be equivalent to
e
?
= argmax
e?
E[TP] + E[TN] (12)
= argmax
e?
?
i,j: x
i
?x
j
s
ij
+
?
i,j: x
i
6?x
j
(1? s
ij
)
where ? denotes coreference according to e
?
. As
explained above, the s
ij
are coreference probabil-
ities s
ij
that can be estimated from a sample of
clusterings e.
This objective corresponds to min-max graph
cut (Ding et al, 2001), an NP-hard problem with
an approximate solution (Nie et al, 2010).
9
8 Experiments
In this section, we describe experiments on three
different datasets. Our main results are described
first: Twitter features many instances of name vari-
ation that we would like our model to be able to
learn. We also report the performance of different
ablations of our full approach, in order to see which
consistently helped across the different splits. We
report additional experiments on the ACE 2008 cor-
pus, and on a political blog corpus, to demonstrate
that our approach is applicable in different settings.
For Twitter and ACE 2008, we report the stan-
dard B
3
metric (Bagga and Baldwin, 1998a). For
the political blog dataset, the reference does not
consist of entity annotations, and so we follow the
evaluation procedure of Yogatama et al (2012).
8.1 Twitter
Data. We use a novel corpus of Twitter posts dis-
cussing the 2013 Grammy Award ceremony. This
is a challenging corpus, featuring many instances
9
In our experiments, we run the clustering algorithm five
times, initialized from samples chosen at random from the last
10% of the sampler run, and keep the clustering that achieved
highest expected Rand score.
781
of name variation. The dataset consists of five splits
(by entity), the smallest of which is 604 mentions
and the largest is 1374. We reserve the largest split
for development purposes, and report our results
on the remaining four. Appendix B provides more
detail about the dataset.
Baselines. We use the discriminative entity cluster-
ing algorithm of Green et al (2012) as our baseline;
their approach was found to outperform another
generative model which produced a flat cluster-
ing of mentions via a Dirichlet process mixture
model. Their method uses Jaro-Winkler string sim-
ilarity to match names, then clusters mentions with
matching names (for disambiguation) by compar-
ing their unigram context distributions using the
Jenson-Shannon metric. We also compare to the
EXACT-MATCH baseline, which assigns all strings
with the same name to the same entity.
Procedure. We run four test experiments in which
one split is used to pick model hyperparameters
and the remaining three are used for test. For the
discriminative baseline, we tune the string match
threshold, context threshold, and the weight of the
context model prior (all via grid search). For our
model, we tune only the fixed weight of the root
feature, which determines the precision/recall trade-
off (larger values of this feature result in more
attachments to ? and hence more entities). We
leave other hyperparameters fixed: 16 latent top-
ics, and Gaussian priors N (0, 1) on all log-linear
parameters. For PHYLO, the entity clustering is
the result of (1) training the model using EM, (2)
sampling from the posterior to obtain a distribu-
tion over clusterings, and (3) finding a consensus
clustering. We use 20 iterations of EM with 100
samples per E-step for training, and use 1000 sam-
ples after training to estimate the posterior. We
report results using three variations of our model:
PHYLO does not consider mention context (all men-
tions effectively have the same topic) and deter-
mines mention entities from a single sample of
p (the last); PHYLO+TOPIC adds context (?5.2);
PHYLO+TOPIC+MBR uses the full posterior and
consensus clustering to pick the output clustering
(?7). Our results are shown in Table 1.
10
10
Our single-threaded implementation took around 15 min-
utes per fold of the Twitter corpus on a personal laptop with
a 2.3 Ghz Intel Core i7 processor (including time required to
parse the data files). Typical acceptance rates for ordering and
topic proposals ranged from 0.03 to 0.08.
Mean Test B
3
P R F1
EXACT-MATCH 99.6 53.7 69.8
Green et al (2012) 92.1 69.8 79.3
PHYLO 85.3 91.4 88.7
PHYLO+TOPIC 92.8 90.8 91.8
PHYLO+TOPIC+MBR 92.9 90.9 91.9
Table 1: Results for the Twitter dataset. Higher B
3
scores
are better. Note that each number is averaged over four
different test splits. In three out of four experiments,
PHYLO+TOPIC+MBR achieved the highest F1 score; in one
case PHYLO+TOPIC won by a small margin.
Test B
3
P R F1
PER
EXACT-MATCH 98.0 81.2 88.8
Green et al (2012) 95.0 88.9 91.9
PHYLO+TOPIC+MBR 97.2 88.6 92.7
ORG
EXACT-MATCH 98.2 78.3 87.1
Green et al (2012) 92.1 88.5 90.3
PHYLO+TOPIC+MBR 95.5 80.9 87.6
Table 2: Results for the ACE 2008 newswire dataset.
8.2 Newswire
Data. We use the ACE 2008 dataset, which is
described in detail in Green et al (2012). It is
split into a development portion and a test portion.
The baseline system took the first mention from
each (gold) within-document coreference chain as
the canonical mention, ignoring other mentions in
the chain; we follow the same procedure in our
experiments.
11
Baselines & Procedure. We use the same base-
lines as in ?8.1. On development data, modeling
pragmatics as in ?4.2 gave large improvements for
organizations (8 points in F-measure), correcting
the tendency to assume that short names like CIA
were coincidental homonyms. Hence we allowed
? > 0 and tuned it on development data.
12
Results
are in Table 2.
8.3 Blogs
Data. The CMU political blogs dataset consists of
3000 documents about U.S. politics (Yano et al,
2009). Preprocessed as described in Yogatama et al
(2012), the data consists of 10647 entity mentions.
11
That is, each within-document coreference chain is
mapped to a single mention as a preprocessing step.
12
We used only a simplified version of the pragmatic model,
approximating w(p
?
, x) as 1 or 0 according to whether p
?
.n =
x.n. We also omitted the IMH step from section 5.3. The
other results we report do not use pragmatics at all, since we
found that it gave only a slight improvement on Twitter.
782
Unlike our other datasets, mentions are not anno-
tated with entities: the reference consists of a table
of 126 entities, where each row is the canonical
name of one entity.
Baselines. We compare to the system results
reported in Figure 2 of Yogatama et al (2012).
This includes a baseline hierarchical clustering ap-
proach, the ?EEA? name canonicalization system
of Eisenstein et al (2011), as well the model pro-
posed by Yogatama et al (2012). Like the output
of our model, the output of their hierarchical clus-
tering baseline is a mention clustering, and there-
fore must be mapped to a table of canonical entity
names to compare to the reference table.
Procedure & Results We tune our method as in
previous experiments, on the initialization data
used by Yogatama et al (2012) which consists of
a subset of 700 documents of the full dataset. The
tuned model then produced a mention clustering
on the full political blog corpus. As the mapping
from clusters to a table is not fully detailed in Yo-
gatama et al (2012), we used a simple heuristic:
the most frequent name in each cluster is taken as
the canonical name, augmented by any titles from
a predefined list appearing in any other name in
the cluster. The resulting table is then evaluated
against the reference, as described in Yogatama et
al. (2012). We achieved a response score of 0.17
and a reference score of 0.61. Though not state-of-
the-art, this result is close to the score of the ?EEA?
system of Eisenstein et al (2011), as reported in
Figure 2 of Yogatama et al (2012), which is specif-
ically designed for the task of canonicalization.
8.4 Discussion
On the Twitter dataset, we obtained a 12.6-point F1
improvement over the baseline. To understand our
model?s behavior, we looked at the sampled phy-
logenetic trees on development data. One reason
our model does well in this noisy domain is that
it is able to relate seemingly dissimilar names via
successive steps. For instance, our model learned
to relate many variations of LL Cool J:
Cool James LLCoJ El-El Cool John
LL LL COOL JAMES LLCOOLJ
In the sample we inspected, these mentions were
also assigned the same topic, further boosting the
probability of the configuration.
The ACE dataset, consisting of editorialized
newswire, naturally contains less name variation
than Twitter data. Nonetheless, we find that the
variation that does appear is often properly handled
by our model. For instance, we see several in-
stances of variation due to transliteration that were
all correctly grouped together, such as Megawati
Soekarnoputri and Megawati Sukarnoputri. The prag-
matic model was also effective in grouping com-
mon acronyms into the same entity.
We found that multiple samples tend to give dif-
ferent phylogenies (so the sampler is mobile), but
essentially the same clustering into entities (which
is why consensus clustering did not improve much
over simply using the last sample). Random restarts
of EM might create more variety by choosing dif-
ferent locally optimal parameter settings. It may
also be beneficial to explore other sampling tech-
niques (Bouchard-C?ot?e, 2014).
Our method assembles observed names into an
evolutionary tree. However, the true tree must in-
clude many names that fall outside our small ob-
served corpora, so our model would be a more
appropriate fit for a far larger corpus. Larger cor-
pora also offer stronger signals that might enable
our Monte Carlo methods to mix faster and detect
regularities more accurately.
A common error of our system is to connect
mentions that share long substrings, such as dif-
ferent PERSONs who share a last name, or differ-
ent ORGANIZATIONs that contain University of. A
more powerful name mutation than the one we use
here would recognize entire words, for example
inserting a common title or replacing a first name
with its common nickname. Modeling the internal
structure of names (Johnson, 2010; Eisenstein et
al., 2011; Yogatama et al, 2012) in the mutation
model is a promising future direction.
9 Conclusions
Our primary contribution consists of new model-
ing ideas, and associated inference techniques, for
the problem of cross-document coreference resolu-
tion. We have described how writers systematically
plunder (?) and then systematically modify (?) the
work of past writers. Inference under such models
could also play a role in tracking evolving memes
and social influence, not merely in establishing
strict coreference. Our model also provides an al-
ternative to the distance-dependent CRP.
2
Our implementation is available for re-
search use at: https://bitbucket.org/
noandrews/phyloinf.
783
References
Nicholas Andrews, Jason Eisner, and Mark Dredze.
2012. Name phylogeny: A generative model of
string variation. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 344?355, Jeju, Korea, July.
Amit Bagga and Breck Baldwin. 1998a. Algorithms
for scoring coreference chains. In In The First In-
ternational Conference on Language Resources and
Evaluation Workshop on Linguistics Coreference,
pages 563?566.
Amit Bagga and Breck Baldwin. 1998b. Entity-
based cross-document coreferencing using the vec-
tor space model. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 1, ACL ?98, pages
79?85, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Alex Baron and Marjorie Freedman. 2008. Who
is who and what is what: Experiments in cross-
document co-reference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 274?283, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Peter J. Bickel and Kjell A. Doksum. 1977. Mathe-
matical Statistics : Basic Ideas and Selected Topics.
Holden-Day, Inc.
David M. Blei and Peter I. Frazier. 2011. Distance
dependent chinese restaurant processes. J. Mach.
Learn. Res., 12:2461?2488, November.
D. M. Blei, A. Y. Ng, and M. I. Jordan. 2003. Latent
Dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022.
Alexandre Bouchard-C?ot?e, David Hall, Thomas L.
Griffiths, and Dan Klein. 2013. Automated re-
construction of ancient languages using probabilis-
tic models of sound change. Proceedings of the Na-
tional Academy of Sciences.
Alexandre Bouchard-C?ot?e. 2014. Sequential Monte
Carlo (SMC) for Bayesian phylogenetics. Bayesian
phylogenetics: methods, algorithms, and applica-
tions.
William W. Cohen, Pradeep Ravikumar, and Stephen E.
Fienberg. 2003. A comparison of string metrics for
matching names and records. In KDD Workshop on
data cleaning and object consolidation.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical Soci-
ety. Series B (Methodological), 39(1):1?38.
C.H.Q. Ding, Xiaofeng He, Hongyuan Zha, Ming Gu,
and H.D. Simon. 2001. A min-max cut algorithm
for graph partitioning and data clustering. In Data
Mining, 2001. ICDM 2001, Proceedings IEEE Inter-
national Conference on, pages 107 ?114.
Mark Dredze, Michael J Paul, Shane Bergsma, and
Hieu Tran. 2013. Carmen: A twitter geolocation
system with applications to public health. In AAAI
Workshop on Expanding the Boundaries of Health
Informatics Using AI (HIAI).
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In Proceedings of the
2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1080?1089, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971?1982.
Association for Computational Linguistics.
Jacob Eisenstein, Tae Yano, William W. Cohen,
Noah A. Smith, and Eric P. Xing. 2011. Structured
databases of named entities from bayesian nonpara-
metrics. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
2?12, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
T. Finin, Z. Syed, J. Mayfield, P. McNamee, and C. Pi-
atko. 2009. Using Wikitology for cross-document
entity coreference resolution. In AAAI Spring Sym-
posium on Learning by Reading and Learning to
Read.
Spence Green, Nicholas Andrews, Matthew R. Gorm-
ley, Mark Dredze, and Christopher D. Manning.
2012. Entity clustering across languages. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 60?69, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Stephen Guo, Ming-Wei Chang, and Emre K?c?man.
2013. To link or not to link? a study on end-to-end
tweet entity linking. In Proceedings of NAACL-HLT,
pages 1020?1030.
784
Aria Haghighi and Dan Klein. 2010. Coreference res-
olution in a modular, entity-centered model. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 385?393,
Los Angeles, California, June. Association for Com-
putational Linguistics.
Mark Johnson. 2010. Pcfgs, topic models, adaptor
grammars and learning topical collocations and the
structure of proper names. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 1148?1157,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsuper-
vised name ambiguity resolution using a generative
model. In Proceedings of the First Workshop on
Unsupervised Learning in NLP, EMNLP ?11, pages
105?112, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL).
Richard A. Levine and George Casella. 2001. Im-
plementations of the Monte Carlo EM Algorithm.
Journal of Computational and Graphical Statistics,
10(3):422?439.
Feiping Nie, Chris H. Q. Ding, Dijun Luo, and Heng
Huang. 2010. Improved minmax cut graph cluster-
ing with nonnegative relaxation. In Jos?e L. Balc?azar,
Francesco Bonchi, Aristides Gionis, and Mich`ele
Sebag, editors, ECML/PKDD (2), volume 6322 of
Lecture Notes in Computer Science, pages 451?466.
Springer.
E. H. Porter and W. E. Winkler, 1997. Approximate
String Comparison and its Effect on an Advanced
Record Linkage System, chapter 6, pages 190?199.
U.S. Bureau of the Census.
William M. Rand. 1971. Objective criteria for the eval-
uation of clustering methods. Journal of the Ameri-
can Statistical Association, 66(336):846?850.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 1050?1058, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learn-
ing string edit distance. Technical Report CS-TR-
532-96, Princeton University, Department of Com-
puter Science.
Eric Sven Ristad and Peter N. Yianilos. 1998.
Learning string edit distance. IEEE Transactions
on Pattern Recognition and Machine Intelligence,
20(5):522?532, May.
Alan Ritter, Sam Clark, Oren Etzioni, et al 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Sameer Singh, Amarnag Subramanya, Fernando
Pereira, and Andrew McCallum. 2011. Large-scale
cross-document coreference using distributed infer-
ence and hierarchical models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 793?803, Portland, Oregon, USA, June.
Association for Computational Linguistics.
R E Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25?35.
Luke Tierney. 1994. Markov Chains for Exploring
Posterior Distributions. The Annals of Statistics,
22(4):1701?1728.
Hanna Wallach, David Mimno, and Andrew McCal-
lum. 2009. Rethinking lda: Why priors matter. In
Advances in Neural Information Processing Systems,
pages 1973?1981.
Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A discriminative hierarchical model for fast
coreference at large scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers - Volume 1, ACL
?12, pages 379?388, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
William E. Winkler. 1999. The state of record link-
age and current research problems. Technical report,
Statistical Research Division, U.S. Census Bureau.
Tae Yano, William W. Cohen, and Noah A. Smith.
2009. Predicting response to political blog posts
with topic models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
477?485, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dani Yogatama, Yanchuan Sim, and Noah A. Smith.
2012. A probabilistic model for canonicalizing
named entity mentions. In Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers - Volume 1, ACL
?12, pages 685?693, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
785
