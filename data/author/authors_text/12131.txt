Committed Belief Annotation and Tagging
Mona T. Diab Lori Levin
CCLS LTI
Columbia U. CMU
mdiab@cs.columbia.edu lsl@cs.cmu.edu
Teruko Mitamura Owen Rambow
LTI CCLS
CMU Columbia U.
teruko+@cs.cmu.edu rambow@ccls.columbia.edu
Vinodkumar Prabhakaran Weiwei Guo
CS CS
Columbia U. Columbia U.
Abstract
We present a preliminary pilot study of
belief annotation and automatic tagging.
Our objective is to explore semantic mean-
ing beyond surface propositions. We aim
to model people?s cognitive states, namely
their beliefs as expressed through linguis-
tic means. We model the strength of their
beliefs and their (the human) degree of
commitment to their utterance. We ex-
plore only the perspective of the author of
a text. We classify predicates into one of
three possibilities: committed belief, non
committed belief, or not applicable. We
proceed to manually annotate data to that
end, then we build a supervised frame-
work to test the feasibility of automati-
cally predicting these belief states. Even
though the data is relatively small, we
show that automatic prediction of a belief
class is a feasible task. Using syntactic
features, we are able to obtain significant
improvements over a simple baseline of
23% F-measure absolute points. The best
performing automatic tagging condition is
where we use POS tag, word type fea-
ture AlphaNumeric, and shallow syntac-
tic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
1 Introduction
As access to large amounts of textual informa-
tion increases, there is a strong realization that
searches and processing purely based on surface
words is highly limiting. Researchers in infor-
mation retrieval and natural language processing
(NLP) have long used morphological and (in a
more limited way) syntactic analysis to improve
access and processing of text; recently, interest has
grown in relating text to more abstract representa-
tions of its propositional meaning, as witnessed by
work on semantic role labeling, word sense disam-
biguation, and textual entailment. However, there
are more levels to ?meaning? than just proposi-
tional content. Consider the following examples,
and suppose we find these sentences in the New
York Times:1
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
If we are searching text to find out whether GM
will lay off workers, all of the sentences in (1) con-
1In this paper, we concentrate on written communication,
and we use the terms reader and writer. However, nothing in
the approach precludes applying it to spoken communication.
tain the proposition LAYOFF(GM,WORKERS).
However, the six sentences clearly allow us very
different inferences about whether GM will lay off
workers or not. Supposing we consider the Times
a trustworthy news source, we would be fairly cer-
tain with (1a) and (1b). (1c) suggests the Times is
not certain about the layoffs, but considers them
possible. When reading (1d), we know that some-
one else thinks that GM will lay off workers, but
that the Times does not necessarily share this be-
lief. (1e), (1f), and (1g) do not tell us anything
about whether anyone believes whether GM will
lay off workers.
In order to tease apart what is happening, we
need to refine a simple IR-ish view of text as a
repository of propositions about the world. We use
two theories to aid us. The first theory is that in ad-
dition to facts about the world (GM will or will not
lay off workers), we have facts about people?s cog-
nitive states, and these cognitive states relate their
bearer to the facts in the world. (Though perhaps
there are only cognitive states, and no facts about
the world.) Following the literature in Artificial
Intelligence (Cohen and Levesque, 1990), we can
model cognitive state as beliefs, desires, and inten-
tions. In this paper, we are only interested in be-
liefs (and in distinguishing them from desires and
intentions). The second theory is that communi-
cation is intention-driven, and understanding text
actually means understanding the communicative
intention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader?s
cognitive state ? his or her beliefs, desires, and/or
intentions. This view has been worked out in the
text generation and dialog community more than
in the text understanding community (Mann and
Thompson, 1987; Hovy, 1993; Moore, 1994).
In this paper we are interested in exploring the
following: we would like to recognize what the
text wants to make us believe about various peo-
ple?s cognitive states, including the speaker?s. As
mentioned, we are only interested in people?s be-
lief. In this view, the result of text processing is
not a list of facts about the world, but a list of facts
about different people?s cognitive states.
This paper is part of an on-going research effort.
The goals of this paper are to summarize a pilot
annotation effort, and to present the results of ini-
tial experiments in automatically extracting facts
about people?s beliefs from open domain running
text.
2 Belief Annotation
We have developed a manual for annotating be-
lief, which we summarize here. For more de-
tailed information, we refer to the cited works. In
general, we are interested in the writer?s intention
as to making us believe that various people have
certain beliefs, desires, and intentions. We sim-
plify the annotation in two ways: we are only in-
teretsed in beliefs, and we are only interested in
the writer?s beliefs. This is not because we think
this is the only interesting information in text, but
we do this in order to obtain a manageable anno-
tation in our pilot study. Specifically, we annotate
whether the writer intends the reader to interpret
a stated proposition as the writer?s strongly held
belief, as a proposition which the writer does not
believe strongly (but could), or as a proposition
towards which the writer has an entirely differ-
ent cognitive attitude, such as desire or intention.
We do not annotate subjectivity (Janyce Wiebe and
Martin, 2004; Wilson and Wiebe, 2005), nor opin-
ion (for example: (Somasundaran et al, 2008)):
the nature of the proposition (opinion and type of
opinion, statement about interior world, external
world) is not of interest. Thus, this work is or-
thogonal to the extensive literature on opinion de-
tection. And we do not annotate truth: real-world
(encyclopedic) truth is not relevant.
We have three categories:
? Committed belief (CB): the writer indicates
in this utterance that he or she believes the
proposition. For example, GM has laid off
workers, or, even stronger, We know that GM
has laid off workers.
A subcase of committed belief concerns
propositions about the future, such as GM
will lay off workers. People can have equally
strong beliefs about the future as about the
past, though in practice probably we have
stronger beliefs about the past than about the
future.
? Non-committed belief (NCB): the writer
identifies the propositon as something which
he or she could believe, but he or she hap-
pens not to have a strong belief in. There are
two subcases. First, there are cases in which
the writer makes clear that the belief is not
strong, for example by using a modal auxil-
iary:2 GM may lay off workers. Second, in
reported speech, the writer is not signaling to
us what he or she believes about the reported
speech: The politician claimed that GM will
lay off workers. However, sometimes, we can
use the speech act verb to infer the writer?s
attitude,3 and we can use our own knowledge
2The annotators must distinguish epistemic and deontic
uses of modals.
3Some languages may also use grammatical devices; for
to infer the writer?s beliefs; for example, in
A GM spokesman said that GM will lay off
workers, we can assume that the writer be-
lieves that GM intends to lay off workers, not
just the spokesman. However, this is not part
of the annotation, and all reported speech is
annotated as NCB. Again, the issue of tense
is orthogonal.
? Not applicable (NA): for the writer, the
proposition is not of the type in which he or
she is expressing a belief, or could express a
belief. Usually, this is because the proposi-
tion does not have a truth value in this world
(be it in the past or in the future). This covers
expressions of desire (Some wish GM would
lay of workers), questions (Will GM lay off
workers? or Many wonder if GM will lay
off workers, and expressions of requirements
(GM is required to lay off workers or Lay off
workers!).
This sort of annotation is part of an annotation
of all ?modalities? that a text may express. We
only annotate belief. A further complication is
that these modalities can be nested: one can ex-
press a belief about someone else?s belief, and one
may be strong and the other weak (I believe John
may believe that GM will lay off workers). At this
phase, we only annotate from the perspective of
the writer, i.e. what the writer of the text that is
being annotated believes.
The annotation units (annotatables) are, con-
ceptually, propositions as defined by PropBank
(Kingsbury et al, 2002). In practice, annotators
are asked to identify full lexical verbs (whether
in main or embedded clauses, whether finite or
non-finite). In predicative constructions (John is a
doctor/in the kitchen/drunk), we ask them to iden-
tify the nominal, prepositional, or adjectival head
rather than the form of to be, in order to also han-
dle small clauses (I think [John an idiot]).
The interest of the annotation is clear: we want
to be able to determine automatically from a given
text what beliefs we can ascribe to the writer,
and with what strengths he or she holds them.
Across languages, many different linguistic means
are used to denote this attitude towards an uttered
proposition, including syntax, lexicon, and mor-
phology. To our knowledge, no systematic empir-
ical study exists for English, and this annotation is
a step towards that goal.
example, in German, the choice between indicative mood and
subjunctive mood in reported speech can signal the writer?s
attitude.
3 Related Work
The work of Roser et al (2006) is, in many re-
spects, very similar to ours. In particular, they are
concerned with extracting information about peo-
ple?s beliefs and the strength of these beliefs from
text. However, their annotation is very different
from ours. They extend the TimeML annotation
scheme to include annotation of markers of belief
and strength of belief. For example, in the sen-
tence The Human Rights Committee regretted that
discrimination against women persisted in prac-
tice, TimeML identifies the events associated with
the verbs regret and persist, and then the extension
to the annotation adds the mark that there is a ?fac-
tive? link between the regret event and the persist
event, i.e., if we regret something, then we assume
the truth of that something. In contrast, in our
annotation, we directly annotate events with their
level of belief. In this example, we would annotate
persist as being a committed belief of the Human
Rights Committee (though in this paper we only
report on beliefs attributed to the writer). This dif-
ference is important, as in the annotation of Roser
et al (2006), the annotator must analyze the situ-
ation and find evidence for the level of belief at-
tributed to an event. As a result, we cannot use
the annotation to discover how natural language
expresses level of belief. Our annotation is more
primitively semantic: we ask the annotators sim-
ply to annotate meaning (does X believe the event
takes place), as opposed to annotating the linguis-
tic structures which express meaning. As a conse-
quence of the difference in annotation, we cannot
compare our automatic prediction results to theirs.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Ralf Krestel and Bergler,
2007; Krestel et al, 2008), the authors explore
belief in the context of news media exploring re-
ported speech where they track newspaper text
looking for elements indicating evidentiality. The
notion of belief is more akin to finding statements
that support or negate specific events with differ-
ent degrees of support. This is different from our
notion of committed belief in this work, since we
seek to make explicit the intention of the author or
the speaker.
4 Our Approach
4.1 Data
We create a relatively small corpus of English
manually annotated for the three categories: CB,
NCB, NA. The data covers different domains and
genres from newswire, to blog data, to email cor-
respondence, to letter correspondence, to tran-
scribed dialogue data. The data comprises 10K
words of running text. 70% of the data was dou-
bly annotated comprising 6188 potentially anno-
tatable tokens. Hence we had a 4 way manual clas-
sification in essence between NONE, CB, NCB,
and NA. Most of the confusions between NONE
and CB from both annotators, for 103 tokens.
The next point of disagreement was on NCB and
NONE for 48 tokens.They disagreed on NCB and
CB for 32 of the tokens. In general the interanno-
tator agreements were high as they agreed 95.8%
of the time on the annotatable and the exact belief
classification.4 Here is an example of a disagree-
ment between the two annotators, The Iraqi gov-
ernment has agreed to let Rep Tony Hall visit the
country next week to assess a humanitarian cri-
sis that has festered since the Gulf War of 1991
Hall?s office said Monday. One annotator deemed
?agreed? a CB while the other considered it an
NCB.
4.2 Automatic approach
Once we had the data manually annotated and re-
vised, we wanted to explore the feasibility of au-
tomatically predicting belief states based on lin-
guistic features. We apply a supervised learning
framework to the problem of both identifying and
classifying a belief annotatable token in context.
This is a three way classification task where an
annotatable token is tagged as one of our three
classes: Committed Belief (CB), Non Committed
Belief (NCB), and Not Applicable (NA). We adopt
a chunking approach to the problem using an In-
side Outside Beginning (IOB) tagging framework
for performing the identification and classification
of belief tokens in context. For chunk tagging,
we use YamCha sequence labeling system.5 Yam-
Cha is based on SVM technology. We use the de-
fault parameter settings most importantly the ker-
nels are polynomial degree 2 with a c value of 0.5.
We label each sentence with standard IOB tags.
Since this is a ternary classification task, we have
7 different tags: B-CB (Beginning of a commit-
ted belief chunk), I-CB (Inside of a committed be-
lief chunk), B-NCB (Beginning of non commit-
ted belief chunk), I-NCB (Inside of a non com-
mitted belief chunk), B-NA (Beginning of a not
applicable chunk), I-NA (Inside a not applicable
chunk), and O (Outside a chunk) for the cases
that are not annotatable tokens. As an example
of the annotation, a sentence such as Hall said
he wanted to investigate reports from relief agen-
cies that a quarter of Iraqi children may be suffer-
4This interannotator agreement number includes the
NONE category.
5http://www.tado-chasen.com/yamcha
ing from chronic malnutrition. will be annotated
as follows: {Hall O said B-CB he O wanted B-
NCB to B-NA investigate I-NA reports O from O
relief O agencies O that O a O quarter O of O
Iraqi O children O may O be O suffering B-NCB
from O chronic O malnutrition O.}
We experiment with some basic features and
some more linguistically motivated ones.
CXT: Since we adopt a sequence labeling
paradigm, we experiment with different window
sizes for context ranging from ?/+2 tokens after
and before the token of interest to ?/+5.
NGRAM: This is a character n-gram feature,
explicity representing the first and last character
ngrams of a word. In this case we experiment with
up to ?/+4 characters of a token. This feature
allows us to capture implicitly the word inflection
morphology.
POS: An important feature is the Part-of-Speech
(POS) tag of the words. Most of the annotatables
are predicates but not all predicates in the text are
annotatables. We obtain the POS tags from the
TreeTagger POS tagger tool which is trained on
the Penn Treebank.6
ALPHANUM: This feature indicates whether
the word has a digit in it or not or if it is a non
alphanumeric token.
VerbType: We classify the verbs as to whether
they are modals (eg. may, might, shall, will,
should, can, etc.), auxilliaries (eg. do, be, have),7
or regular verbs. Many of our annotatables occur
in the vicinity of modals and auxilliaries. The list
of modals and auxilliaries is deterministic.
Syntactic Chunk (CHUNK): This feature ex-
plicitly models the syntactic phrases in which our
tokens occur. The possible phrases are shallow
syntactic representations that we obtain from the
TreeTagger chunker:8 ADJC (Adjective Chunk),
ADVC (Adverbial Chunk), CONJC (Conjunc-
tional Chunk), INTJ (Interjunctional Chunk), LST
(numbers 1, 2,3 etc), NC (Noun Chunk), PC
(Prepositional Chunk), PRT (off,out,up etc), VC
(Verb Chunk).
5 Experiments and Results
5.1 Conditions
Since the data is very small, we tested our au-
tomatic annotation using 5 fold cross validation
6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
7We realize in some of the grammar books auxilliaries
include modal verbs.
8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
where 10% of the data is set aside as development
data, then 70% is used for training and 20% for
testing. The reported results are averaged over the
5 folds for the Test data for each of our experimen-
tal conditions.
Our baseline condition is using the tokenized
words only with no other features (TOK). We em-
pirically establish that a context size of ?/+3
yields the best results in the baseline condition as
evaluated on the development data set. Hence all
the results are yielded from a CXT of size 3.
The next conditions present the impact of
adding a single feature at a time and then combin-
ing them. It is worth noting that the results reflect
the ability of the classifier to identify a token that
could be annotatable and also classify it correctly
as one of the possible classes.
5.2 Evaluation Metrics
We use F?=1 (F-measure) as the harmonic mean
between (P)recision and (R)ecall. All the pre-
sented results are the F-measure. We report the
results separately for the three classes CB, NCB,
and NA as well as the overall global F measure for
any one condition averaged over the 5 folds of the
TEST data set.
5.3 Results
In Table 1 we present the results yielded per con-
dition including the baseline TOK and presented
for the three different classes as well as the overall
F-measure.
All the results yielded by our experiments
outperform the baseline TOK. We highlight
the highest performing conditions in Ta-
ble 1: TOK+AlphaNum+POS +CHUNK,
TOK+AN+POS and TOK+POS. Even though
all the features independently outperform the
baseline TOK in isolation, POS is the single most
contributing feature. The least contributing factor
independently is the AlphaNumeric feature AN.
However combining AN with character Ngram
NG yields better results than using each of them
independently. We note that adding NG to any
other feature combination is not helpful, in fact
it seems to add noise rather than signal to the
learning process in the presence of more sophis-
ticated features such as POS or syntactic chunk
information. Adding the verbtype VT explicitly
as a feature is not helpful for all categories, it
seems most effective with CB. As mentioned
earlier we deterministically considered all modal
verbs to be modal. This might not be the case
for all modal auxilliaries since some of them
are used epistemically while others deontically,
hence our feature could be introducing an element
of noise. Adding syntactic chunk information
helps boost the results by a small margin from
53.5 to 53.97 F-measure. All the results seem to
suggest the domination of the POS feature and it?s
importance for such a tagging problem. In general
our performance on CB is the highest, followed
by NA then we note that NCB is the hardest
category to predict. Examining the data, NCB
has the lowest number of occurrence instances
in this data set across the board in the whole
data set and accordingly in the training data,
which might explain the very low performance.
Also in our annotation effort, it was the hardest
category to annotate since the annotation takes
more than the sentential context into account.
Hence a typical CB verb such as ?believe? in the
scope of a reporting predicate such as ?say? as
in the following example Mary said he believed
the suspect with no qualms. The verb believed
should be tagged NCB however in most cases it
is tagged as a CB. Our syntactic feature CHUNK
helps a little but it does not capture the overall
dependencies in the structure. We believe that
representing deeper syntactic structure should
help tremendously as it will model these relatively
longer dependencies.
We also calculated a confusion matrix for the
different classes. The majority of the errors are
identification errors where an annotatable is con-
sidered an O class as opposed to one of the 3 rel-
evant classes. This suggests that identifying the
annotatable words is a harder task than classifica-
tion into one of the three classes, which is consis-
tent with our observation from the interannotator
disagreements where most of their disagreements
were on the annotatable tokens, though a small
overall number of tokens, 103 tokens out of 6188,
it was the most significant disagreement category.
We find that for the TOK+POS condition, CBs are
mistagged as un-annotatable O 55% of the time.
We find most of the confusions between NA and
CB, and NCB and CB, both cases favoring a CB
tag.
6 Conclusion
We presented a preliminary pilot study of belief
annotation and automatic tagging. Even though
the data is relatively tiny, we show that automatic
prediction of a belief class is a feasible task. Us-
ing syntactic features, we are able to obtain signif-
icant improvements over a simple baseline of 23%
F-measure absolute points. The best performing
automatic tagging condition is where we use POS
tag, word type feature AlphaNumeric, and shallow
syntactic chunk information CHUNK. Our best
overall performance is 53.97% F-measure.
CB NA NCB Overall F
TOK 25.12 41.18 13.64 30.3
TOK+NG 33.18 42.29 5 34.25
TOK+AN 30.43 44.57 12.24 33.92
TOK+AN+NG 37.17 42.46 9.3 36.61
TOK+POS 54.8 59.23 13.95 53.5
TOK+NG+POS 43.15 50.5 22.73 44.35
TOK+AN+POS 54.79 58.97 22.64 53.54
TOK+NG+AN+POS 43.09 54.98 18.18 45.91
TOK+POS+CHUNK 55.45 57.5 15.38 52.77
TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43
TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97
TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89
Table 1: Final results averaged over 5 folds of test data using different features and their combinations:
NG is NGRAM, AN is AlphaNumeric, VT is verbtype
In the future we are looking at ways of adding
more sophisticated deep syntactic and semantic
features using lexical chains from discourse struc-
ture. We will also be exploring belief annotation in
Arabic and Urdu on a parallel data collection since
these languages express evidentiality in ways that
differ linguistically from English. Finally we will
explore ways of automatically augmenting the la-
beled data pool using active learning.
Acknowledgement
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the authors and do not necessarily reflect the views
of the sponsor.
References
Philip R. Cohen and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Jerry Morgan Philip Cohen and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Eduard H. Hovy. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Rebecca Bruce Matthew Bell Janyce Wiebe,
Theresa Wilson and Melanie Martin. 2004.
Learning subjective language. In Computational
Linguistics, Volume 30 (3).
Paul Kingsbury, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Tree-
Bank. In Proceedings of the Human Language Tech-
nology Conference, San Diego, CA.
Ralf Krestel, Sabine Bergler, and Rene? Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In European
Language Resources Association (ELRA), editor,
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC 2008), Marrakech,
Morocco, May 28?30.
William C. Mann and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
Johanna Moore. 1994. Participating in Explanatory
Dialogues. MIT Press.
Rene? Witte Ralf Krestel and Sabine Bergler. 2007.
Processing of Beliefs extracted from Reported
Speech in Newspaper Articles. In International
Conference on Recent Advances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bul-
garia, September 27?29.
Saur?? Roser, Marc Verhagen, and James Pustejovsky.
2006. Annotating and Recognizing Event Modality
in Text. In FLAIRS 2006, editor, In Proceedings
of the 19th International FLAIRS Conference, Mel-
bourne Beach, Florida, May 11-13.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Theresa Wilson and Janyce Wiebe. 2005. Annotat-
ing attributions and private states. In Proceedings of
the Workshop on Frontiers in Corpus Annotations II:
Pie in the Sky, pages 53?60, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Coling 2010: Poster Volume, pages 1014?1022,
Beijing, August 2010
Automatic Committed Belief Tagging
Vinodkumar Prabhakaran
Columbia University
vp2198@columbia.edu
Owen Rambow
Columbia University
rambow@ccls.columbia.edu
Mona Diab
Columbia University
mdiab@ccls.columbia.edu
Abstract
We go beyond simple propositional mean-
ing extraction and present experiments in
determining which propositions in text the
author believes. We show that deep syn-
tactic parsing helps for this task. Our
best feature combination achieves an F-
measure of 64%, a relative reduction in F-
measure error of 21% over not using syn-
tactic features.
1 Introduction
Recently, interest has grown in relating text to
more abstract representations of its propositional
meaning, as witnessed by work on semantic role
labeling, word sense disambiguation, and textual
entailment. However, there is more to ?meaning?
than just propositional content. Consider the fol-
lowing examples, and suppose we find these sen-
tences in the New York Times:
(1) a. GM will lay off workers.
b. A spokesman for GM said GM will lay off
workers.
c. GM may lay off workers.
d. The politician claimed that GM will lay
off workers.
e. Some wish GM would lay of workers.
f. Will GM lay off workers?
g. Many wonder if GM will lay off workers.
If we are searching text to find out whether
GM will lay off workers, all of the sen-
tences above contain the proposition LAY-
OFF(GM,WORKERS). However, they allow us
very different inferences about whether GM will
lay off workers or not. Supposing we consider
the Times a trustworthy news source, we would
be fairly certain if we read (1a) and (1b). (1c)
suggests the Times is not certain about the layoffs,
but considers them possible. When reading (1d),
we know that someone else thinks that GM will
lay off workers, but that the Times does not nec-
essarily share this belief. (1e), (1f), and (1g) do
not tell us anything about whether anyone believes
whether GM will lay off workers.
In order to tease apart what is happening, we
need to abandon a simple view of text as a repos-
itory of propositions about the world. We use two
assumptions to aid us. The first assumption is that
discourse participants model each other?s cogni-
tive state during discourse (we take the term to in-
clude the reading of monologic written text), and
that language provides cues for the discourse par-
ticipants to do the modeling. This assumption is
commonly made, for example by Grice (1975) in
his Maxim of Quantity. Following the literature
in Artificial Intelligence (Bratman, 1999; Cohen
and Levesque, 1990), we model cognitive state as
beliefs, desires, and intentions. Crucially, these
three dimensions are orthogonal; for example, we
can desire something but not believe it.
(2) I know John won?t be here, but I wouldn?t
mind if he were
However, we cannot both believe something
and not believe it:
(3) #John won?t be here, but nevertheless I think
he may be here
Note that (2) requires but in order to be felic-
itous, but sentence (3) cannot be ?saved? by any
discourse markers ? it is not interpretable. In this
paper, we are interested in beliefs (and in distin-
1014
guishing them from desires and intentions).
The second assumption is that communication
is intention-driven, and understanding text actu-
ally means understanding the communicative in-
tention of the writer. Furthermore, communica-
tive intentions are intentions to affect the reader?s
cognitive state ? his or her beliefs, desires, and/or
intentions. This view has been adopted in the text
generation and dialog community more than in
the information extraction and text understanding
communities (Mann and Thompson, 1987; Hovy,
1993; Moore, 1994; Bunt, 2000; Stone, 2004). In
this paper we explore the following: we would
like to recognize what the writer of the text intends
the reader to believe about various people?s beliefs
about the world (including the writer?s own). In
this view, the result of text processing is not a list
of facts about the world, but a list of facts about
different people?s cognitive states. In this paper,
we limit ourselves to the writer?s beliefs, but we
specifically want to determine which propositions
he or she intends us to believe he or she holds as
beliefs, and with what strength. The result of such
processing will be a much more fine-grained rep-
resentation of the information contained in written
text than has been available so far.
2 Belief Annotation and Data
We use a corpus of 10,000 words annotated for
speaker belief of stated propositions (Diab et al,
2009). The corpus is very diverse in terms of
genre, and it includes newswire text, email, in-
structions, and solicitations. The corpus annotates
each verbal proposition (clause or small clause),
by attaching one of the following tags to the head
of the proposition (verbs and heads of nominal,
adjectival, and prepositional predications).
? Committed belief (CB): the writer indicates
in this utterance that he or she believes the propo-
sition. For example, GM has laid off workers, or,
even stronger, We know that GM has laid off work-
ers. Committed belief can also include proposi-
tions about the future: people can have equally
strong beliefs about the future as about the past,
though in practice probably we have stronger be-
liefs about the past than about the future.
? Non-committed belief (NCB): the writer
identifies the proposition as something which he
or she could believe, but he or she happens not
to have a strong belief in. There are two sub-
cases. First, the writer makes clear that the be-
lief is not strong, for example by using a modal
auxiliary epistemically: GM may lay off workers.
Second, in reported speech, the writer is not sig-
naling to the reader what he or she believes about
the reported speech: The politician claimed that
GM will lay off workers. Again, the issue of tense
is orthogonal.
? Not applicable (NA): for the writer, the propo-
sition is not of the type in which he or she is ex-
pressing a belief, or could express a belief. Usu-
ally, this is because the proposition does not have a
truth value in this world (be it in the past or in the
future). This covers expressions of desire (Some
wish GM would lay of workers), questions (Will
GM lay off workers?), and expressions of require-
ments (GM is required to lay off workers or Lay
off workers!).
All propositional heads are classified as one of
the classes CB, NCB, or NA, and all other tokens
are classified as O. Note that in this corpus, event
nominals (such as the lay-offs by GM were unex-
pected) are, unfortunately, not annotated for be-
lief and are always marked ?O?. Note also that
the syntactic form does not determine the annota-
tion, but the perceived writer?s intention ? a ques-
tion will usually be an NA, but sometimes a ques-
tion can be used to convey a belief (for example,
a rhetorical question), in which case it would be
labeled CB.
3 Automatic Belief Tagging
3.1 Approach
We applied a supervised learning framework to
the problem of identifying committed belief in
context. Our task consists of two conceptual sub-
tasks: identifying the propositions, and classify-
ing each proposition as CB, NCB, or NA. For the
first subtask, we could use a system that cuts a
sentence into propositions, but we are not aware
of such a system that performs at an adequate
level. Instead, we tag the heads of the proposi-
tion, which amounts to the same in the sense that
there is a bijection between propositions and their
heads. Practically, we have the choice between
1015
No Feature Type Description
Features that performed well
1 isNumeric L Word is Alphabet or Numeric?
2 POS L Word?s POS tag
3 verbType L Modal/Aux/Reg ( = ?nil? if the word is not a verb)
4 whichModalAmI L If I am a modal, what am I? ( = ?nil? if I am not a modal)
3 amVBwithDaughterTo S Am I a VB with a daughter to?
4 haveDaughterPerfect S Do I have a daughter which is one of has, have, had?
5 haveDaughterShould S Do I have a daughter should?
6 haveDaughterWh S Do I have a daughter who is one of where, when, while, who, why?
7 haveReportingAncestor S Am I a verb/predicate with an ancestor whose lemma is one of tell, accuse,
insist, seem, believe, say, find, conclude, claim, trust, think, suspect, doubt,
suppose?
8 parentPOS S What is my parent?s POS tag?
9 whichAuxIsMyDaughter S If I have a daughter which is an auxiliary, what is it? ( = ?nil? if I do not have
an auxiliary daughter)
10 whichModalIsMyDaughter S If I have a daughter which is a modal, what is it? ( = ?nil? if I do not have a
modal daughter)
Features that were not useful
1 Lemma L Word?s Lemma
2 Stem L Word stem (Using Porter Stemmer)
3 Drole S Deep role (drole in MICA features)
4 isRoot S Is the word the root of the MICA Parse tree?
5 parentLemma S Parent word?s Lemma
6 parentStem S Parent word stem (Using Porter Stemmer)
7 parentSupertag S Parent word?s super tag (from Penn Treebank)
8 Pred S Is the word a predicate? (pred in MICA features)
9 wordSupertag S Word?s Super Tag (from Penn Treebank)
Table 1: All Features Used
a joint model, in which the heads are chosen and
classified simultaneously, and a pipeline model, in
which heads are chosen first and then classified.
In this paper, we consider the joint model in de-
tail and in Section 3.5.3, we present results of the
pipeline model; they support our choice.
In the joint model, we define a four-way clas-
sification task where each token is tagged as one
of four classes ? CB, NCB, NA, or O (nothing)
? as defined in Section 2. For tagging, we ex-
perimented with Support Vector Machines (SVM)
and Conditional Random Fields (CRF). For SVM,
we used the YAMCHA(Kudo and Matsumoto,
2000) sequence labeling system,1 which uses the
TinySVM package for classification.2 For CRF,
we used the linear chain CRF implementation of
1http://chasen.org/ taku/software/YAMCHA/
2http://chasen.org/ taku/software/TinySVM/
the MALLET(McCallum, 2002) toolkit.3
3.2 Features
We divided our features into two types - Lexi-
cal and Syntactic. Lexical features are at the to-
ken level and can be extracted without any pars-
ing with relatively high accuracy. We expect these
features to be useful for our task. For example,
isNumeric, which denotes whether the word is a
number or alphabetic, is a lexical feature. Syn-
tactic features of a token access its syntactic con-
text in the dependency tree. For example, par-
entPOS, the POS tag of the parent word in the
dependency parse tree, is a syntactic feature. We
used the MICA deep dependency parser (Banga-
lore et al, 2009) for parsing in order to derive
the syntactic features. We use MICA because
we assume that the relevant information is the
3http://MALLET.cs.umass.edu/
1016
predicate-argument structure of the verbs, which
is explicit in the MICA output. While it is clear
that having a perfect parse would yield useful fea-
tures, current parsers perform at levels of accuracy
lower than that of part-of-speech taggers, so that it
is not a foregone conclusion that using automatic
parser output helps in our task.
The list of features we used in our experiments
are summarized in Table 1. The column ?Type?
denotes the type of the feature. ?L? stands for lex-
ical features and ?S? stands for syntactic features.
The tree below shows the dependency parse
tree output by MICA for the sentence Republican
leader Bill Frist said the Senate was hijacked.
said
Frist
Republican leader Bill
hijacked
Senate
the
was
In the above sentence, said and hijacked are
the propositions that should be tagged. Let?s look
at hijacked in detail. The feature haveReportin-
gAncestor of hijacked is ?Y? because it is a verb
with a parent verb said. Similarly, the feature
haveDaughterAux would also be ?Y? because of
daughter was, whereas whichAuxIsMyDaughter
would get the value was.
We also considered several other features which
did not yield good results. For example, the to-
ken?s supertag (Bangalore and Joshi, 1999), the
parent token?s supertag, a binary feature isRoot
(Is the word the root of the parse tree?) were
deemed not useful. We list the features we exper-
imented with and decided to discard in Table 1.
For finding the best performing features, we did
an exhaustive search on the feature space, incre-
mentally pruning away features that are not use-
ful.
3.3 Experiments
This section describes different experiments we
conducted in detail. It explains the experimen-
tal setup for both learning frameworks we used
- YAMCHA and MALLET. We also explain the
pipeline model in detail.
Class Description
LC Lexical features with Context
LNSN Lexical and Syntactic features with No-
context
LCSN Lexical features with Context and Syntactic
features with No-context
LCSC Lexical and Syntactic features with Context
Table 2: YAMCHA Experiment Sets
3.3.1 YAMCHA Experiments
We categorized our YAMCHA experiments
into different experimental conditions as shown in
Table 2. For each class, we did experiments with
different feature sets and (linear) context widths.
Here, context width denotes the window of tokens
whose features are considered. For example, a
context width of 2 means that the feature vector
of any given token includes, in addition to its own
features, those of 2 tokens before and after it as
well as the tag prediction for 2 tokens before it.
For LNSN , the context width of all features was
set to 0. For LCSN , the context width of syntactic
features alone was set to 0. A context width of 0
for a feature means that the feature vector includes
that feature of the current token only. When con-
text width was non-zero, we varied it from 1 to 5,
and we report the results for the optimal context
width.
We tuned the SVM parameters, and the best
results were obtained using the One versus All
method for multiclass classification on a quadratic
kernel with a c value of 0.5. All results presented
for YAMCHA here use this setting.
3.3.2 MALLET Experiments
Class Description
L Lexical features only
LS Lexical and Syntactic features
Table 3: MALLET Experiment Sets
We categorized our MALLET experiments into
two classes as shown in Table 3. We computed
the features described in Section 3.2 at the to-
ken level and converted them to binary in order to
use them for CRF. We experimented with varying
orders and the best results were obtained for or-
1017
Class Feature Set Parm P R F
YAMCHA - Joint Model
LC POS, whichModalAmI, verbType, isNumeric CW=3 61.9 52.7 56.9
LNSN POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-
IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, amVBwith-
DaughterTo, haveDaughterWh, haveDaughterShould
CW=0 62.5 57.5 59.9
LCSN POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModalIs-
MyDaughter, whichAuxIsMyDaughter, haveDaughterShould
CW=2 67.4 58.1 62.4
LCSC POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-
IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-
terWh, haveDaughterShould
CW=2 68.5 60.0 64.0
MALLET - Joint Model
L POS, whichModalAmI, verbType GV=1 55.1 45.0 49.6
LS POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-
IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-
terWh, haveDaughterShould
GV=1 64.5 54.4 59.0
Pipeline Model
LCSC POS, whichModalAmI, parentPOS, haveReportingAncestor, whichModal-
IsMyDaughter, haveDaughterPerfect, whichAuxIsMyDaughter, haveDaugh-
terWh, haveDaughterShould
CW=2 49.8 42.9 46.1
Table 4: Overall Results. CW = Context Width, GV = Gaussian Variance, P = Precision, R = Recall, F
= F-Measure
der= ?0,1?, which makes the CRF similar to Hid-
den Markov Model. All results reported here use
the order= ?0,1?. We also conducted experiments
varying the Gaussian variance parameter from 1.0
to 10.0 using the same experimental setup (i.e.
we did not have a distinct tuning corpus) and ob-
served that best results were obtained with a low
value of 1 to 3, instead of MALLET?s default
value of 10.0.
3.3.3 Pipeline Model
We also did experiments to support our choice
of the joint model over the pipeline model. We
chose the best performing feature configuration
of the LCSC class (which is the overall best
performer as we present in Section 3.5), and
set up the pipeline model. We trained a se-
quence classifier using YAMCHA to identify the
head tokens, where tokens are tagged as just
propositional heads without distinguishing be-
tween CB/NA/NCB. The predicted head tokens
were then classified using a 3-Way SVM classi-
fier trained on gold data.
3.4 Evaluation
For evaluation, we used 4-fold cross validation on
the training data. The data was divided into 4 folds
of which 3 folds were used to train a model which
was tested on the 4th fold. We did this with all
four configurations and all the reported results in
this paper are averaged results across 4 folds. We
report Recall and Precision on word tokens in our
corpus for each of the three tags. It is worth noting
that the majority of the words in our data will not
be tagged with any of the three classes. (Recall
that most words have neither of the three tags).
We also report F?=1 (F)-measure as the harmonic
mean between (P)recision and (R)ecall.
3.5 Results
This section summarizes the results of various
experiments we conducted. The best perform-
ing feature configuration and corresponding Pre-
cision, Recall and F-measure for each experimen-
tal setup discussed in previous section is presented
in Table 4. The best F-measure for each category
under various experimental setups is presented in
Table 5.
We obtained the best performance using YAM-
1018
Setup Class CB NCB NA
Joint-YAMCHA LC 61.5 15.2 63.2
Joint-YAMCHA LNSN 67.0 28.3 59.9
Joint-YAMCHA LCSN 67.6 33.2 64.5
Joint-YAMCHA LCSC 69.6 34.1 64.5
Joint-MALLET L 53.9 7.5 54.1
Joint-MALLET LS 65.8 40.6 59.1
Pipeline LCSC 55.2 16.5 51.3
Table 5: Results per Category (F-Measure)
CHA in a joint model. So, we first analyze this
configuration in great detail in Section 3.5.1. We
discuss results obtained using MALLET in Sec-
tion 3.5.2 and the pipeline model in Section-3.5.3.
3.5.1 YAMCHA - Results
As described in Section 3.3.1, we divide our
experiments into 4 classes - LC , LNSN , LCSN
and LCSC . Table 4 presents the best perform-
ing feature sets and context width configuration
for each class. For all experiments with context,
the best result was obtained with a context width
of 2, except for LC , where a context width of 3
gave the best results. The results show that syn-
tactic features improve the classifier performance
considerably. The best model obtained for LC
has an F-measure of 56.9%. In LNSN it im-
proves marginally to 59.9%. Adding back context
to lexical features improves it to 62.4% in LCSN
whereas addition of context to syntactic features
further improves this to 64.0%. We observed that
the feature parentPOS has the most impact on in-
creased context widths, among syntactic features.
The improvement pattern of Precision and Re-
call across the classes is also interesting. Syntac-
tic features with no context improve Recall by 4.8
percentage points over only lexical features with
context, whereas Precision improves only by 0.6
points. However, adding back context to lexical
features further improves Precision by 4.9 points
while Recall just improves by 0.6 points. Finally,
adding context of syntactic features improves both
Precision and Recall moderately. We infer that
syntactic features (without context) help identify
more annotatable patterns thereby improving Re-
call, whereas linear context helps removing the
wrong ones, thereby improving Precision.
The per-category F-measure results presented
in Table 5 are also interesting. The CB F-measure
improves by 8.1 points and NCB improves 18.9
points from LC to LCSC . But, the improvement
in NA F-measure is only a marginal 1.3 points
between LC and LCSC . Furthermore, the F-
measure decreases by 3.3 points when syntactic
and lexical features with no context are used. On
analysis, we found that NAs often occur in syn-
tactic structures like want to find or should go (de-
ontic should), in which the relevant words occur
in a small linear window. In contrast, NCBs are
often signaled by deeper syntactic structures. For
example, in He said that his visit to the US will
mainly focus on the humanitarian issues, a simpli-
fied sentence from our training set, the verb focus
is an NCB because it is in the scope of the report-
ing verb said (specifically, it is its daughter). This
could not be captured using the context because
said and focus are far apart in the sentence. But
a correct parse tree gives focus as the daughter of
said. So, a feature like haveReportingAncestor
could easily capture this. It is also the case that the
root of a dependency parse tree would mostly be
a CB. This is captured by the feature parentPOS
having value ?nil?. This property also cannot be
captured by lexical features alone.
However, NCB performs much worse than the
other two categories. NCB is a class which occurs
rarely compared to CB and NA in our corpus. Out
of the 1, 357 propositions tagged, only 176 were
NCB. We assume that this could be a main factor
of its poor performance.
We analyzed the performance across the folds.
Fold-2 contains only 0.03% NCBs compared to
1.89% on the rest of the folds. Similarly, it con-
tains 6.43% NAs compared to 3.82% across other
folds. However, our best performing model gives
a Recall of 59.1% with a Precision of 69.7% (F-
measure 64.0%) for Fold-2, which is as good as
other folds. Hence, we observe that our learned
model is robust under distributional variations.
3.5.2 MALLET Results
As explained in Section 3.3.2, we explored
MALLET-CRF using two experimental condi-
tions L and LS. Table 4 presents the best per-
forming feature sets for both classes. These re-
1019
sults again show that syntactic features improve
the classifier performance considerably. The best
model obtained for L class has an F-measure of
49.6%, whereas addition of syntactic features im-
proves this to 59.0%. Both Precision and Recall
are improved by 9.4 percentage points as well.
However, MALLET-CRF?s performance was
comparatively worse than YAMCHA?s SVM. The
best model for MALLET (LS) obtained an F-
measure of 59.0% which is 5.0 percentage points
less than that of the best model for YAMCHA
(LCSC).
It is interesting to note that MALLET per-
formed well on predicting NCB. The highest NCB
F-measure of MALLET - 40.6% is 6.5 percent-
age points higher than the highest NCB F-measure
for YAMCHA. However, corresponding CB and
NA F-measures were 61.2% and 56.1% which
are much lower than YAMCHA?s performance for
these categories.
Also, MALLET was more time efficient than
YAMCHA. On an average, for our corpus size
and feature sets, MALLET ran 3 times as fast as
YAMCHA in a cross validation setup (i.e. training
and testing together).
3.5.3 Joint Model vs Pipeline Model
As discussed in Section 3.3.3, we set up a
pipeline model for the best performing configu-
ration of LCSC class of YAMCHA experiments.
The head prediction step of the pipeline obtained
an F-measure of 83.9% with Precision and Re-
call of 86.7% and 81.2%, respectively, across all
4 folds. The 3-way classification step to classify
the belief of the identified head obtained an ac-
curacy of 72.7% across all folds. In the pipeline
model, false positives and false negatives adds up
from step 1 and step 2, where as only the true
positives of step 2 is considered as the true pos-
itives overall. In this way, the overall Precision
was only 49.8% and Recall was 42.9% with an F-
measure of 46.1% as shown in Table 4. The results
for CB/NCB/NA separately are given in Table 5.
The per-category best F-measure was decreased
by 14.4, 17.6 and 13.2 percentage points from the
YAMCHA joint model for CB, NCB and NA, re-
spectively. The performance gap is big enough to
conclude that our choice of joint model was right.
4 Related Work
Our work falls in the rich tradition of modeling
agents in terms of their cognitive states (for ex-
ample, (Rao and Georgeff, 1991)) and relating
this modeling to language use through extensions
to speech act theory (for example, (Perrault and
Allen, 1980; Clark, 1996; Bunt, 2000)). These no-
tions have been particularly fruitful in the dialog
community, where dialog act tagging is a major
topic of research; to cite just one prominent ex-
ample: (Stolcke et al, 2000). A dialog act repre-
sents the communicative intention of the speaker,
and its recognition is crucial for the building of
dialog systems. The specific contribution of this
paper is to investigate exactly how discourse par-
ticipants signal their beliefs using language, and
the strength of their beliefs; this latter point is not
usually included in dialog act tagging.
This paper is not concerned with issues relating
to logics for belief representation or inferencing
that can be done on beliefs (for an overview, see
(McArthur, 1988)), nor theories of automatic be-
lief ascription (Wilks and Ballim, 1987). For ex-
ample, this paper is not concerned with determin-
ing whether a belief in the requirement of p entails
the belief in p; instead, we are only interested in
whether the writer wants the reader to understand
whether the writer holds a belief in the require-
ment that p or in p directly. This paper is also not
concerned with subjectivity (Wiebe et al, 2004),
the nature of the proposition p (statement about
interior world or external world) is not of interest,
only whether the writer wants the reader to believe
the writer believes p. This paper is also not con-
cerned with opinion and determining the polarity
(or strength) of opinion (for example: (Somasun-
daran et al, 2008)), which corresponds to the de-
sire dimension. Thus, this work is orthogonal to
the extensive literature on opinion classification.
The work of (Saur?? and Pustejovsky, 2007;
Saur?? and Pustejovsky, 2008) is, in many re-
spects, very similar to ours. They propose Fact-
bank, which represents the factual interpretation
as modality-polarity pairs, extracted from the ba-
sic structural elements denoting factuality en-
coded by Timebank. Also, they attribute the factu-
ality to specific sources within the text. Our work
1020
is more limited in several ways: we currently only
model the writer?s beliefs; we do not express po-
larity (we believe we can derive it from the syn-
tax and lexicon); Saur?? and Pustejovsky (2008)
ask their annotators to perform extensive linguis-
tic transformations on the text to obtain a ?nor-
malized? representation of propositional content
(we simply ask the annotators to make a judg-
ment about the writer?s strength of belief with
respect to a given proposition, and expect to be
able to extract representations of pure proposi-
tional meaning independently); and finally, Saur??
and Pustejovsky (2008) have a more fine-grained
representation of non-committed belief. While it
is plausible to distinguish between more or less
firm non-committed belief, we believe the crucial
distinction is between committed belief and non-
committed belief. Furthermore, Saur?? and Puste-
jovsky (2008) group reported speech with non-
belief statements (our NA), while we group them
with weak belief (our NCB). The reason for our
decision is that we wanted to keep NA as a cat-
egory which contains no-one?s beliefs, as we as-
sumed this is semantically more coherent. The
category NCB thus covers beliefs which the writer
does not hold firmly or has expressed no opinion
on ? which is different from propositions which
the writer has clearly attributed to other cognitive
states (such as desire). In principle, we believe
a 4-way distinction is the right approach, but our
NCB category is already the least frequent, and
splitting it would have resulted in two very rare
classes. Another difference include the use of the
word ?fact? in the FactBank manual, which we
avoid because we are interested in cognitive mod-
eling; however, this is merely a terminological is-
sue.
Other related works explored belief systems in
an inference scenario as opposed to an intentional-
ity scenario. In work by (Krestel et al, 2008), the
authors explore belief in the context of reported
speech in news media: they track newspaper text
looking for elements indicating evidentiality. This
is different from our work, since we seek to make
explicit the intention of the author or the speaker.
5 Future Work
We are exploring ways to utilize the FactBank an-
notated corpus for our purpose, with the goal of
automatically converting it to our annotation for-
mat. With the added data from FactBank, we
hope to be able to split the NCB category into
WB (weak belief) and RS (reported speech). We
will also explore learning embedded belief attri-
butions, as annotated in FactBank.
We found that the per-sentence F-measure
has a small positive correlation with the length-
normalized probability of the MICA derivation (a
measure of parse confidence). In case of a bad
parse, syntax features add noise which in turn re-
duces classifier performance. We are planning
to exploit this correlation in order to choose sen-
tences for selective self-training. Another direc-
tion we are looking to extend this work is to em-
ploy active learning to overcome the shortcom-
ings of a small training set. Also, we found fre-
quent use of epistemic and deontic modals in our
data. Both types of modals have identical syntac-
tic structure, but they receive very different anno-
tations. This is not easily captured in our system.
We are exploring ways to handle this.
We will release our Committed Belief Tagging
tool as a standalone black-box tool. We also in-
tend to release the annotated corpus.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank Bonnie Dorr,
Lori Levin and our other partners on the TTO8
project. We also thank several anonymous review-
ers for their constructive feedback.
References
Bangalore, Srinivas and Aravind Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2):237?266.
Bangalore, Srinivas, Pierre Boullier, Alexis Nasr,
Owen Rambow, and Beno??t Sagot. 2009. MICA:
1021
A probabilistic dependency parser based on tree in-
sertion grammars. In NAACL HLT 2009 (Short Pa-
pers).
Bratman, Michael E. 1999 [1987]. Intention, Plans,
and Practical Reason. CSLI Publications.
Bunt, Harry. 2000. Dialogue pragmatics and context
specification. In Bunt, Harry and William J. Black,
editors, Abduction, Belief and Context in Dialogue,
pages 81?150.
Clark, Herbert H. 1996. Using Language. cup, Cam-
bridge, England.
Cohen, Philip R. and Hector J. Levesque. 1990. Ratio-
nal interaction as the basis for communication. In
Philip Cohen, Jerry Morgan and James Allen, edi-
tors, Intentions in Communication. MIT Press.
Diab, Mona T., Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed belief annotation and tag-
ging. In ACL-IJCNLP ?09: Proceedings of the
Third Linguistic Annotation Workshop, pages 68?
73, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Grice, Herbert Paul. 1975. Logic and conversation. In
Cole, P. and J. Morgan, editors, Syntax and seman-
tics, vol 3. Academic Press, New York.
Hovy, Eduard H. 1993. Automated discourse gener-
ation using discourse structure relations. Artificial
Intelligence, 63:341?385.
Krestel, Ralf, Sabine Bergler, and Rene? Witte. 2008.
Minding the Source: Automatic Tagging of Re-
ported Speech in Newspaper Articles. In (ELRA),
European Language Resources Association, edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC 2008),
Marrakech, Morocco, May 28?30.
Kudo, Taku and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In
Proceedings of CoNLL-2000 and LLL-2000, pages
142?144.
Mann, William C. and Sandra A. Thompson. 1987.
Rhetorical Structure Theory: A theory of text orga-
nization. Technical Report ISI/RS-87-190, ISI.
McArthur, Gregory L. 1988. Reasoning about knowl-
edge and belief: a survey. Computational Intelli-
gence, 4:223?243.
McCallum, Andrew Kachites. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Moore, Johanna. 1994. Participating in Explanatory
Dialogues. MIT Press.
Perrault, C. Raymond and James F. Allen. 1980. A
plan-based analysis of indirect speech acts. Compu-
tational Linguistics, 6(3?4):167?182.
Rao, Anand S. and Michael P. Georgeff. 1991. Mod-
eling rational agents within a BDI-architecture. In
Allen, James, Richard Fikes, and Erik Sandewall,
editors, Proceedings of the 2nd International Con-
ference on Principles of Knowledge Representation
and Reasoning, pages 473?484. Morgan Kaufmann
publishers Inc.: San Mateo, CA, USA.
Saur??, Roser and James Pustejovsky. 2007. Determin-
ing Modality and Factuality for Textual Entailment.
In First IEEE International Conference on Semantic
Computing., Irvine, California.
Saur??, Roser and James Pustejovsky. 2008. From
Structure to Interpretation: A Double-layered An-
notation for Event Factuality. In Proceedings of the
2nd Linguistic Annotation Workshop. LREC 2008.
Somasundaran, Swapna, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpre-
tation. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 801?808, Manchester, UK, August.
Coling 2008 Organizing Committee.
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26:339?373.
Stone, Matthew. 2004. Intention, interpretation and
the computational structure of language. Cognitive
Science, 24:781?809.
Wiebe, Janyce, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learning
subjective language. In Computational Linguistics,
Volume 30 (3).
Wilks, Yorick and Afzal Ballim. 1987. Multiple
agents and the heuristic ascription of belief. In Pro-
ceedings of the 10th International Joint Conference
on Artificial Intelligence (IJCAI), pages 118?124.
1022
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1486,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Staying on Topic:
An Indicator of Power in Political Debates
Vinodkumar Prabhakaran
Dept. of Computer Science
Columbia University
New York, NY, USA
vinod@cs.columbia.edu
Ashima Arora
Dept. of Computer Science
Columbia University
New York, NY, USA
aa3470@columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We study the topic dynamics of interac-
tions in political debates using the 2012
Republican presidential primary debates
as data. We show that the tendency of
candidates to shift topics changes over the
course of the election campaign, and that it
is correlated with their relative power. We
also show that our topic shift features help
predict candidates? relative rankings.
1 Introduction
The field of computational social sciences has cre-
ated many interesting applications for natural lan-
guage processing in recent years. One of the areas
where NLP techniques have shown great promise
is in the analysis of political speech. For example,
researchers have applied NLP techniques to polit-
ical texts for a variety of tasks such as predicting
voting patterns (Thomas et al., 2006), identifying
markers of persuasion (Guerini et al., 2008), cap-
turing cues that signal charisma (Rosenberg and
Hirschberg, 2009), and detecting ideological po-
sitions (Sim et al., 2013). Our work also analyzes
political speech, more specifically, presidential de-
bates. The contribution of this paper is to show
that the topic shifting tendency of a presidential
candidate changes over the course of the election
campaign, and that it is correlated with his or her
relative power. We also show that this insight can
help computational systems that predict the candi-
dates? relative rankings based on their interactions
in the debates.
2 Motivation
The motivation for this paper stems from prior
work done by the first author in collaboration
with other researchers (Prabhakaran et al., 2013a;
Prabhakaran et al., 2013b). Prabhakaran et al.
(2013a) introduced the notion of power in the do-
main of presidential debates, and Prabhakaran et
al. (2013b) followed it up with an automatic power
ranker system based on interactions within the de-
bates. The power that a candidate had at a cer-
tain point in the election campaign was modeled
based on his or her recent poll standings: in elec-
tions, popularity is power. Those studies analyzed
the 2012 Republican presidential primary debates
and found that a candidate?s power at the time of
a debate correlates with the structure of interac-
tions within the debate (e.g., turn frequency and
interruption patterns). Another finding was that
the candidates? power correlates with the distribu-
tion of topics they speak about in the debates: can-
didates with more power spoke significantly more
about certain topics (e.g., economy) and less about
certain other topics (e.g., energy). However, these
findings relate to the specific election cycle that
was analyzed and will not carry over to political
debates in general.
A further dimension with relevance beyond a
specific election campaign is how topics evolve
during the course of an interaction (e.g., who at-
tempts to shift topics). In (Prabhakaran et al.,
2014), we explored this dimension and found that
candidates with higher power introduce signifi-
cantly more topics in the debates, but attempt to
shift topics significantly less often while respond-
ing to a moderator. We used the basic LDA topic
modeling method (with a filter for substantivity of
turns) to assign topics to turns, which were then
used to detect shifts in topics. However, segment-
ing interactions into coherent topic segments is an
active area of research and a variety of topic mod-
eling approaches have been proposed for that pur-
pose. In this paper, we explore the utility of one
such topic modeling approach to tackle this prob-
lem.
While most of the early approaches for topic
segmenting in interactions have focused on the
1481
content of the contribution, Nguyen et al. (2012)
introduced a system called Speaker Identity for
Topic Segmentation (SITS) which also takes into
account the topic shifting tendencies of the partic-
ipants of the conversation. In later work, Nguyen
et al. (2013) demonstrated the SITS system?s util-
ity in detecting influencers in Crossfire debates
and Wikipedia discussions. They also applied the
SITS system to the domain of political debates.
However they were able to perform only a qual-
itative analysis of its utility in the debates domain
since the debates data did not have influence an-
notations. In this paper, we use the SITS system
to assign topics to turns and perform a quantita-
tive analysis of how the topic shift features calcu-
lated using the SITS system relate to the notion of
power as captured by (Prabhakaran et al., 2013a).
The SITS system associates each debate partic-
ipant with a constant scalar value that captures his
or her tendency to shift topics. However, since
we want to investigate how each candidate?s topic
shifting tendency relates to his or her changing
power over the course of the campaign, we intro-
duce a variation of the SITS analysis in which we
represent a different ?persona? for each candidate
in each debate. Once equipped with this notion
of ?persona?, we find that the topic shifting ten-
dency of a candidate does indeed show a great deal
of fluctuation during the election campaign period.
We also find that this fluctuation in topic shifting
tendencies is significantly correlated with the can-
didates? power.
As an additional contribution of this paper, we
demonstrate the utility of our topic shift features
extracted using both types of SITS-based anal-
yses in improving the performance of the auto-
matic power ranker system presented in (Prab-
hakaran et al., 2013b). We also investigated the
utility of topic shifting features described in (Prab-
hakaran et al., 2014) extracted using LDA based
topic modeling. However, they did not improve
the performance of the ranker, and hence we do
not discuss them in detail in this paper.
3 Data
We use the presidential debates corpus released by
Prabhakaran et al. (2013a), which contains manual
transcripts of 20 debates held between May 2011
and February 2012 as part of the 2012 Republican
presidential primaries. The corpus also captures
each candidate?s power at the time of each debate,
computed based on their relative standing in re-
cent public polls. The poll numbers capture how
successful candidates are in convincing the elec-
torate of their candidature, which in turn affects
their confidence within the debates. These debates
serve as a rich domain to explore manifestations
of power since they are a medium through which
candidates pursue and maintain power over other
candidates. Prabhakaran et al. (2013b) offers a de-
tailed description of how the relative standings in
national and state-level polls from various sources
are aggregated to obtain candidates? power.
The transcripts are originally obtained from The
American Presidency Project, where each turn of
the conversation is manually demarcated and their
speakers identified. The turns in the corpus are
preprocessed using the Stanford CoreNLP pack-
age to perform basic NLP steps such as tokeniza-
tion, sentence segmentation, parts-of-speech tag-
ging and lemmatization.
4 Modeling Topic Shifts
Topic segmentation, the task of segmenting inter-
actions into coherent topic segments, is an impor-
tant step in analyzing interactions. In addition
to its primary purpose, topic segmentation also
identifies the speaker turn where the conversation
changed from one topic to another, i.e., where the
topic shifted, which may shed light on the char-
acteristics of the speaker who changed the topic.
We use the SITS approach proposed by (Nguyen
et al., 2012) to detect topic shifts. We also propose
a different way of using SITS to obtain an analysis
of our corpus, which we call SITS
var
. We discuss
both in turn, and then provide a discussion.
4.1 Segmentation using SITS
Most computational approaches towards auto-
matic topic segmentation have focused mainly on
the content of the contribution without taking into
account the social aspects or speaker character-
istics. Different discourse participants may have
different tendencies to introduce or shift topics in
interactions. In order to address this shortcom-
ing, Nguyen et al. (2012) proposed a new topic
segmentation model called Speaker Identity for
Topic Segmentation (SITS), in which they explic-
itly model the individual?s tendency to introduce
new topics.
Like traditional topic modeling approaches, the
SITS system also considers each turn to be a
1482
0	 ?
0.1	 ?
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
6/13
/11	 ?
8/11
/11	 ? 9/5/
11	 ?
9/7/
11	 ?
9/12
/11	 ?
9/22
/11	 ?
10/1
1/11
	 ?
10/1
8/11
	 ?
11/9
/11	 ?
11/1
2/11
	 ?
11/2
2/11
	 ?
12/1
0/11
	 ?
12/1
5/11
	 ?
1/7/
12	 ?
1/8/
12	 ?
1/16
/12	 ?
1/19
/12	 ?
1/23
/12	 ?
1/26
/12	 ?
2/22
/12	 ?
To
pic
	 ?Sh
i?	 ?
Ten
de
ncy
,	 ?P
I(x
_d
)	 ?
Date	 ?of	 ?Debate	 ?
BACHMANN	 ?
CAIN	 ?
GINGRICH	 ?
HUNTSMAN	 ?
PAUL	 ?
PERRY	 ?
ROMNEY	 ?
SANTORUM	 ?
Figure 1: SITS
var
Topic shift tendency values across debates
bag of words generated from a mixture of top-
ics. These topics themselves are multinomial dis-
tributions over terms. In order to account for the
topic shifts that happen during the course of an in-
teraction, they introduce a binary latent variable
l
d;t
called the topic shift to indicate whether the
speaker changed the topic or not in conversation
d at turn t. To capture the individual speaker?s
topic shifting tendency, they introduced another
latent variable called topic shift tendency (pi
x
) of
speaker x. The pi
x
value represents the propensity
of speaker x to perform a topic shift.
4.2 Segmentation using SITS
var
Within the SITS formulation, the topic shifting
tendency of an individual (pi
x
) is considered a con-
stant across conversations. While an individual
may have an inherent propensity to shift topics or
not, we argue that the topic shifting tendency he
or she displays can vary based on the social set-
tings in which he or she interacts and his or her
status within those settings. In other words, the
same discourse participant may behave differently
in different social situations and at different points
in time. This is especially relevant in the context
of our dataset, where the debates happen over a
period of 10 months, and the power and status
of each candidate in the election campaign vary
greatly within that time period.
We propose a variant of SITS which takes this
issue into account. We consider each candi-
date to have a different ?persona? in each debate.
To accomplish this, we create new identities for
each candidate x for each debate d, denoted by
x d. For example, ?ROMNEY 08-11-2011? de-
notes the persona of the candidate ROMNEY in
the debate held on 08-11-2011. Running the SITS
system using this formulation, we obtain different
pi
x d
values for candidate x for different debates,
capturing different topic shift tendencies of x.
4.3 Execution
We perform both the SITS and SITS
var
analyses
on the 20 debates in our corpus. We used the non-
parametric version of SITS for both runs, since it
systemically estimates the number of topics in the
data. We set the maximum number of iterations
at 5000, sample lag at 100 and initial number of
topics at 25. We refer the reader to (Nguyen et al.,
2013) for details on these parameters.
For each candidate, we calculate the mean and
standard deviation of the topic shift tendency
(pi
x d
) of his or her personas across all debates
he or she participated in. We then average these
means and standard deviations, and obtain an av-
erage mean of 0.14 and an average standard devia-
tion of 0.09. This shows that the topic shift tenden-
cies of candidates vary by a considerable amount
across debates. Figure 1 shows the pi
x d
value fluc-
tuating across different debates.
5 Analysis of Topic Shift Features
Nguyen et al. (2013) used the SITS analysis as a
means to model influence in multi party conver-
sations. They propose two features to detect in-
fluencers: Total Topic Shifts (TTS) and Weighted
Topic Shifts (WTS). TTS(x, d) captures the ex-
pected number of topic shifts the individual x
makes in conversation d. This expectation is cal-
culated through the empirical average of samples
1483
Feature Set Feature Correlation
TopSh
Total Topic Shifts (TTS) 0.12
Weighted Topic Shifts (WTS) 0.16
TopSh
var
Total Topic Shifts (TTS
var
) 0.12
Weighted Topic Shifts (WTS
var
) 0.15
Topic Shift Tendency (PI
var
) -0.27
Table 1: Pearson Correlations for Topical Features
boldface denotes statistical significance (p < 0.05)
from the Gibbs sampler, after a burn-in period. We
refer the reader to (Nguyen et al., 2013) for more
details on how this value is computed. WTS(x, d)
is the value of TTS(x, d) weighted by 1? pi
x
. The
intuition here is that a topic shift by a speaker with
low topic shift tendency must be weighted higher
than that by a speaker with a high topic shift ten-
dency. We use these two features as well, and de-
note the set of these two features as TopSh.
We also extract the TTS and WTS features us-
ing our SITS
var
variation of topic segmentation
analysis and denote them as TTS
var
and WTS
var
respectively. In addition, we also use a feature
PI
var
(x, d) which is the pi
x d
value obtained by the
SITS
var
for candidate x in debate d. It captures the
topic shifting tendency of candidate x in debate d.
(We do not include the SITS pi
x
value in our corre-
lation analysis since it is constant across debates.)
We denote the set of these three features obtained
from the SITS
var
run as TopSh
var
.
Table 1 shows the Pearson?s product correla-
tion between each topical feature and candidate?s
power. We obtain a highly significant (p = 0.002)
negative correlation between topic shift tendency
of a candidate (PI) and his/her power. In other
words, the variation in the topic shifting tenden-
cies is significantly correlated with the candidates?
recent poll standings. Candidates who are higher
up in the polls tend to stay on topic while the
candidates with less power attempt to shift top-
ics more often. This is in line with our previous
findings from (Prabhakaran et al., 2014) that can-
didates with higher power attempt to shift topics
less often than others when responding to moder-
ators. It is also in line with the findings by Prab-
hakaran et al. (2013a) that candidates with higher
power tend not to interrupt others. On the other
hand, we did not obtain any significant correlation
for the features proposed by Nguyen et al. (2013).
6 Topic Shift Features in Power Ranker
In this section, we investigate the utility of the
SITS and SITS
var
based topic shift features de-
scribed above in the problem of automatically
ranking the participants of debates based on their
power. Prabhakaran et al. (2013b) define the prob-
lem as follows: given a debate d with a set of par-
ticipants C
d
= {x
1
, x
2
, ...x
n
} and corresponding
power indices P (x
i
) for 1 < i < n, find a ranking
function r : C
d
? {1...n} such that for all 1 <
i, j < n, r(x
i
) > r(x
j
) ?? P (x
i
) > P (x
j
).
For our experiments, we use the SVM
rank
based
supervised learned power ranker presented in that
work to estimate this ranking function.
As we do in (Prabhakaran et al., 2013b), we
here report Kendall?s Tau and Normalized Dis-
counted Cumulative Gain values (NDCG and
NDCG@3) on 5-fold cross validation (at the de-
bate level). All three metrics are based on the
number of rank inversions between original and
predicted ranking. While Tau treats all rank in-
versions equal, NDCG and NDCG@3 penalize
the inversions happening in the top of the ranked
list more than those happening in the bottom.
NDCG@3 focuses only on the top 3 positions in
the ranked list.
We use the best performing feature set of (Prab-
hakaran et al., 2013b) as the baseline (BL), which
contains three features: Words Deviation (WD),
Question Deviation (QD) and Mention Percent-
age (MP). WD and QD capture the deviation of
percentage of words spoken by the candidate and
questions addressed to the candidate from the ex-
pected fair share of those measures in the particu-
lar debate. The fair share for debate d is 1/|C
d
|?
the percentage each candidate would have gotten
for each feature if it was equally distributed. This
deviation measure is used instead of the raw per-
1484
Kendall?s Tau NDCG NDCG@3
BL 0.55 0.962 0.932
TopSh 0.36 0.907 0.830
TopSh
var
0.39 0.919 0.847
BL + TopSh 0.59 0.967 0.929
BL + TopSh
var
0.60 0.970 0.937
BL + TopSh + TopSh
var
0.59 0.968 0.934
Table 2: Power Ranker results using topic shift features on 5-fold cross validation
BL: Baseline system (Prabhakaran et al., 2013b)
NDCG: Normalized Discounted Cumulative Gain
centage in order to handle the fact that the percent-
age values are dependent on the number of partic-
ipants in a debate, which varied from 9 to 4. MP
captures the percentage of mentions of the candi-
date within a debate.
Table 2 shows the results obtained using the
baseline features (BL) as well as combinations of
TopSh and TopSh
var
features. The baseline sys-
tem obtained a Kendall Tau of 0.55, NDCG of
0.962 and NDCG@3 of 0.932. The topic shift
features by themselves performed much worse,
with TopSh
var
posting marginally better results
than TopSh. Combining the topic shift and base-
line features increases performance considerably.
TopSh
var
obtained better performance than TopSh
across the board. BL + TopSh
var
posted the over-
all best system obtaining a Tau of 0.60, NDCG
of 0.970, and NDCG@3 of 0.937. These results
demonstrates the utility of topic shift features in
the power ranking problem, especially using the
SITS
var
formulation. We also experimented with
all subsets of TopSh and TopSh
var
; the best results
were obtained using all features in each set.
7 Related Work
Studies in sociolinguistics (e.g., (Ng et al., 1993;
Ng et al., 1995; Reid and Ng, 2000)) have long
established that dialog structure in interactions re-
lates to power and influence. Researchers in the
NLP community have studied power and influence
in various genres of interactions, such as organiza-
tional email threads (Bramsen et al., 2011; Gilbert,
2012; Prabhakaran and Rambow, 2013; Prab-
hakaran and Rambow, 2014), online discussion fo-
rums (Danescu-Niculescu-Mizil et al., 2012; Bi-
ran et al., 2012) and online chat dialogs (Strza-
lkowski et al., 2012). The correlates analyzed in
these studies range from word and phrase patterns,
to derivatives of such patterns such as linguistic
coordination, to deeper dialogic features such as
argumentation and dialog acts. Our work differs
from these studies in that we study the correlates
of power in topic dynamics. Furthermore, we an-
alyze spoken interactions.
8 Conclusion
In this paper, we studied how topic shift patterns
in the 2012 Republican presidential debates corre-
late with the power of candidates. We proposed an
alternate formulation of the SITS topic segmenta-
tion system that captures fluctuations in each can-
didate?s topic shifting tendencies, which we found
to be correlated with their power. We also showed
that features based on topic shift improve the pre-
diction of the relative rankings of candidates. In
future work, we will explore a model that cap-
tures individuals? inherent topic shift propensities,
while also capturing their fluctuations due to so-
cial factors.
Acknowledgments
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense or
the U.S. Government. We also thank the anony-
mous reviewers for their constructive feedback.
1485
References
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37?45, Montr?eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773?782, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
?12, New York, NY, USA. ACM.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
?12, pages 1037?1046, New York, NY, USA. ACM.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2008. Corps: A corpus of tagged political speeches
for persuasive communication processing. Journal
of Information Technology & Politics, 5(1):19?32.
Sik Hung Ng, Dean Bell, and Mark Brooke. 1993.
Gaining turns and achieving high in influence rank-
ing in small conversational groups. British Journal
of Social Psychology, pages 32, 265?275.
Sik Hung Ng, Mark Brooke, and Michael Dunne.
1995. Interruption and in influence in discussion
groups. Journal of Language and Social Psychol-
ogy, pages 14(4),369?381.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2012. Sits: A hierarchical nonparametric
model using speaker identity for topic segmentation
in multiparty conversations. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 78?87, Jeju Island, Korea, July. Association
for Computational Linguistics.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216?224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran and Owen Rambow. 2014.
Predicting power relations between participants in
written dialog from a single thread. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 339?344, Baltimore, Maryland, June.
Association for Computational Linguistics.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013a. Power dynamics in spoken in-
teractions: a case study on 2012 republican primary
debates. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
99?100. International World Wide Web Conferences
Steering Committee.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013b. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365?373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Vinodkumar Prabhakaran, Ashima Arora, and Owen
Rambow. 2014. Power of confidence: How poll
scores impact topic dynamics in political debates.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, page 49, Baltimore, MD, USA, June. Associa-
tion for Computational Linguistics.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as
a resource for in influence: evidence for prototypical
arguments and social identification processes. Euro-
pean Journal of Social Psych., pages 30, 83?100.
Andrew Rosenberg and Julia Hirschberg. 2009.
Charisma perception from text and speech. Speech
Communication, 51(7):640?655.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of the
2013 Conference on EMNLP, pages 91?101, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535?2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
1486
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1965?1976,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Gender and Power:
How Gender and Gender Environment Affect Manifestations of Power
Vinodkumar Prabhakaran
Dept. of Computer Science
Columbia University
New York, NY, USA
vinod@cs.columbia.edu
Emily E. Reid
Dept. of Computer Science
Columbia University
New York, NY, USA
eer2137@columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We investigate the interaction of power,
gender, and language use in the Enron
email corpus. We present a freely avail-
able extension to the Enron corpus, with
the gender of senders of 87% messages
reliably identified. Using this data, we
test two specific hypotheses drawn from
the sociolinguistic literature pertaining to
gender and power: women managers use
face-saving communicative strategies, and
women use language more explicitly than
men to create and maintain social rela-
tions. We introduce the notion of ?gender
environment? to the computational study
of written conversations; we interpret this
notion as the gender makeup of an email
thread, and show that some manifestations
of power differ significantly between gen-
der environments. Finally, we show the
utility of gender information in the prob-
lem of automatically predicting the direc-
tion of power between pairs of participants
in email interactions.
1 Introduction
It has long been observed that men and women
communicate differently in different contexts.
This phenomenon has been studied by sociolin-
guists, who typically rely on case studies or sur-
veys. The availability of large corpora of nat-
urally occurring social interactions has given us
the opportunity to study language use at a broader
level than before. In this paper, we use the Enron
Corpus of work-related emails to examine written
communication in a corporate setting. We inves-
tigate three factors that affect choices in commu-
nication: the writer?s gender, the gender of his or
her fellow discourse participants (what we call the
?gender environment?), and the relations of orga-
nizational power he or she has to the discourse par-
ticipants. We concentrate on modeling the writer?s
choices related to discourse structure, rather than
lexical choice. Specifically, our goal is to show
that gender, gender environment, and power all af-
fect individuals? choices in complex ways, result-
ing in patterns in the discourse that reveal the un-
derlying factors.
This paper makes three major contributions.
First, we introduce an extension to the well-known
Enron corpus of emails: we semi-automatically
identify the sender?s gender of 87% of email mes-
sages in the corpus. This extension will be made
publicly available. Second, we use this enriched
version of the corpus to investigate the interaction
of hierarchical power and gender. We formalize
the notion of ?gender environment?, which reflects
the gender makeup of the discourse participants
of a particular conversation. We study how gen-
der, power, and gender environment influence dis-
course participants? choices in dialog. We inves-
tigate two specific hypotheses from the sociolin-
guistic literature, relating to face-saving use of lan-
guage, and to the use of language to strengthen so-
cial relations. This contribution does not exhaust
the possibilities of our corpus, but it shows how
social science can benefit from advanced natural
language processing techniques in analyzing cor-
pora, allowing social scientists to tackle corpora
such as the Enron corpus which cannot be exam-
ined in its entirety by hand. Third, we show that
the gender information in the enriched corpus can
be useful for computational tasks, specifically for
training a system that predicts the direction of hier-
archical power between participants in an interac-
tion. Our use of the gender-based features boosts
the accuracy of predicting the direction of power
between pairs of email interactants from 68.9% to
70.2% on an unseen test set.
1965
The paper is structured as follows. We review
related work in Section 2. We present the Gender
Identified Enron Corpus (our first contribution) in
Section 3. Section 4 defines the problem of pre-
dicting power and the various dimensions of in-
teraction we analyze. We turn to our second con-
tribution, the analysis of the data, in Sections 5
and 6. Section 7 describes our third contribution,
the machine learning experiments using gender-
related features in the prediction of hierarchical
power. We then conclude and discuss future work.
2 Related Work
There is much sociolinguistic background related
to gender and language use, some of it specifically
related to language use in the work environment
(Kendall and Tannen, 1997; Holmes and Stubbe,
2003; Kendall, 2003; Herring, 2008). We do not
provide a full discussion of this work for lack of
space, but single out one paper which has partic-
ularly influenced our work. Holmes and Stubbe
(2003) provide two case studies that do not look
at the differences between male and female man-
agers? communication, but at the difference be-
tween female managers? communication in more
heavily female vs. more heavily male environ-
ments. They find that, while female managers tend
to break many stereotypes of ?feminine? commu-
nication, they have different strategies in connect-
ing with employees and exhibiting power in the
two gender environments. This work has inspired
us to look at this phenomenon by including ?Gen-
der Environment? in our study. By finding the ra-
tios of males to females on a thread, we can look at
whether indicators change within a more heavily
male or female thread. This notion of gender envi-
ronment is supported by an idea in recent Twitter-
based sociolinguistic research on gender identity
and lexical variation (Bamman et al., 2014). One
of the many insights from their work is that gen-
dered linguistic behavior is oriented by a number
of factors, one of which includes the speaker?s au-
dience. Their work looks at Twitter users whose
linguistic style fails to identify their gender in clas-
sification experiments, and finds that the linguis-
tic gender norms can be influenced by the style of
their interlocutors.
Within the NLP community, there has been
substantial research exploring language use and
power. A large number of these studies are per-
formed in the domain of organizational email
where the notion of power is well defined in terms
of organizational hierarchy. It is also aided by the
availability of the moderately large Enron email
corpus which captures email interactions in an or-
ganizational setting. Earlier approaches used sim-
ple lexical features alone (e.g. (Bramsen et al.,
2011; Gilbert, 2012)) as a means to predict power.
Later studies have used more complex linguistic
and structural features, such as formality (Peterson
et al., 2011), dialog acts (Prabhakaran and Ram-
bow, 2013), and thread structure (Prabhakaran and
Rambow, 2014). Our work is also on the Enron
email corpus, and our baseline features are derived
from some of this prior work. Researchers have
also studied power and influence in other genres
of interactions, such as online forums (Danescu-
Niculescu-Mizil et al., 2012; Biran et al., 2012),
multi-party chats (Strzalkowski et al., 2012) and
off-line interactions such as presidential debates
(Nguyen et al., 2013; Prabhakaran et al., 2013;
Prabhakaran et al., 2014).
There is also some work within the NLP field
on analyzing language use in relation to gender.
Mohammad and Yang (2011) analyzed the way
gender affects the expression of sentiments in text,
while we are interested in how gender relates to
manifestations of organizational power. For their
study, they assigned gender for the core employees
in the Enron email corpus based on whether the
first name of the person was easily gender iden-
tifiable or not. If the person had an unfamiliar
name or a name that could be of either gender,
they marked his/her gender as unknown and ex-
cluded them from their study.
1
For example, the
gender of the employee Kay Mann was marked as
unknown in their gender assignment. However, in
our work, we manually research and determine the
gender of every core employee.
Researchers have also attempted to automati-
cally predict the gender of email senders using su-
pervised learning techniques based on linguistic
features (Corney et al., 2002; Cheng et al., 2011;
Deitrick et al., 2012), a task we do not address in
this paper. These studies use datasets that are rel-
atively smaller in size. Corney et al. (2002) use
around 4K emails from 325 gender identified au-
thors. Cheng et al. (2011) use around 9K emails
from 108 gender identified authors. Deitrick et al.
(2012) use around 18K emails from 144 gender
1
http://www.saifmohammad.com/WebDocs/dir-email-
gender.txt
1966
identified authors. The dataset we offer is much
larger in size, with around 97K emails whose au-
thors are gender identified. We believe that our
resource will aid further research in this area.
3 Gender Identified Enron Corpus
3.1 Enron Corpus
In our work, we use the version of Enron email
corpus released by Yeh and Harnly (2006). The
corpus contains emails from the mailboxes of 145
core employees who held top managerial positions
within Enron at the time of bankruptcy. Yeh and
Harnly (2006) preprocessed the corpus to combine
multiple email addresses belonging to the same
entity and identify each entity in the corpus with
a unique identifier. The corpus contains a total of
111,933 messages. This version of the corpus has
been enriched later by Agarwal et al. (2012) with
gold organizational power relations, manually de-
termined using information from Enron organiza-
tional charts. It includes relations of 1,518 em-
ployees and captures dominance relations between
13,724 pairs of them. This information enables us
to study the manifestations of power in these inter-
actions, in relation to gender.
In this version of the corpus, the thread structure
of email messages is reconstructed, with the miss-
ing messages restored from other emails in which
they were quoted. This allows us to go beyond
isolated messages and study the dialog structure
within email threads. There were 34,156 unique
discourse participants across all the email threads
present in the corpus. Manually determining the
gender of all the discourse participants in the cor-
pus is not feasible. Hence, we adopt a two-step
approach through which we reliably identify the
gender of a large majority of entities in the email
threads within the corpus. We manually deter-
mine the gender of the 145 core employees who
have a bigger representation in the corpus, and we
systemically determine the gender of the rest of
the discourse participants using the Social Secu-
rity Administration?s baby names database. We
adopt a conservative approach so that we assign
a gender only when the name of the participant
meets a very low ambiguity threshold.
3.2 Manual Gender Assignment
We researched each of the 145 core employees us-
ing web search and found public records about
them or articles referring to them. In order to
make sure that the results are about the same per-
son we want, we added the word ?enron? to the
search queries. Within the public records returned
for each core employee, we looked for instances
in which they were being referred to either using a
gender revealing pronoun (he/him/his vs. she/her)
or using a gender revealing addressing form (Mr.
vs. Mrs./Ms./Miss). Since these employees held
top managerial positions within Enron at the time
of bankruptcy, it was fairly easy to find public
records or articles referring to them. For example,
the page we found for Kay Mann clearly identifies
her gender.
2
We were able to correctly determine
the gender of each of the 145 core employees in
this manner. A benefit of manually determining
the gender of these core employees is that it en-
sures a high coverage of 100% confident gender
assignments in the corpus.
3.3 Automatic Gender Assignment
As mentioned in Section 3.1, our corpus contains
a large number of discourse participants in addi-
tion to the 145 core employees for which we man-
ually identified the gender. To attempt to find
the gender of these other discourse participants,
we first determine their first names and then find
how ambiguous the names are by querying the So-
cial Security Administration?s (SSA) baby names
dataset. We first describe how we calculate an am-
biguity score for a name using the SSA dataset and
then describe how we use it to determine the gen-
der of discourse participants in our corpus.
3.3.1 SSA Names and Gender Dataset
The US Social Security Administration maintains
a dataset of baby names, gender, and name count
for each year starting with the 1880s, for names
with at least five counts.
3
We used this dataset
in order to determine the gender ambiguity of a
name. The Enron data set contains emails from
1998 to 2001. We estimate the common age range
for a large, corporate firm like Enron at 24-67,
4
so
we used the SSA data from 1931-1977 to calculate
ambiguity scores for our purposes.
For each name n in the database, let mp(n)
and fp(n) denote the percentages of males and fe-
males with the name n. Then, we calculate the
ambiguity score AS (n) as 100?|mp(n)? fp(n)|.
2
http://www.prnewswire.com/news-releases/kay-mann-
joins-noble-as-general-counsel-57073687.html
3
http://www.ssa.gov/oact/babynames/limits.html
4
http://www.bls.gov/cps/demographics.htm
1967
The value of AS (n) varies between 0 and 100. A
name that is ?perfectly unambiguous? would have
an ambiguity score of 0, while a ?perfectly am-
biguous? name (i.e., 50%/50% split between gen-
ders) would have an ambiguity score of 100. We
assign the likely gender of the name to be the one
with the higher percentage, if the ambiguity score
is below a threshold AS
T
.
G(n) =
{
M, if AS(n) ? AS
T
and mp(n) > fp(n)
F, if AS(n) ? AS
T
and mp(n) ? fp(n)
I, if AS(n) > AS
T
Around 88% of the names in the SSA dataset
have AS (n) = 0. We choose a very conserva-
tive threshold of AS
T
= 10 for our gender assign-
ments, which assigns gender to around 93% names
in the SSA dataset.
5
3.3.2 Identifying the First Name
Each discourse participant in our corpus has at
least one email address and zero or more names
associated with it. The name field is automatically
assembled by Yeh and Harnly (2006), where they
captured the different names from email headers,
which are populated from individual email clients
and do not follow a standard format. Not all dis-
course participants are human; some may refer to
organizational groups (e.g., HR Department) or
anonymous corporate email accounts (e.g., a web-
master account, do-not-reply address etc.). The
name field may sometimes be empty, contain mul-
tiple names, contain an email address, or show
other irregularities. Hence, it is nontrivial to deter-
mine the first name of our discourse participants.
We used the heuristics below to extract the most
likely first name for each discourse participant.
? If the name field contains two words, pick the
second or first word, depending on whether a
comma separates them or not.
? If the name field contains three words and a
comma, choose the second and third words
(a likely first and middle name, respectively).
If the name field contains three words but no
comma, choose the first and second words
(again, a likely first and middle name).
? If the name field contains an email address,
pick the portion from the beginning of the
string to a ?.?,? ? or ?-?; if the email address
is in camel case, take portion from the begin-
ning of the string to the first upper case letter.
5
In the corpus that will be released, we retain the AS(n)
of each name, so that the users of this resource can decide the
threshold that suit their needs.
? If the name field is empty, apply the above
rule to the email address field to pick a name.
The above heuristics create a list of candidate
names for each discourse participant which we
then query for an ambiguity score (Section 3.3.1)
and the likely gender. We find the candidate
name with the lowest ambiguity score that passes
the threshold and assign the associated gender to
the discourse participant. If none of the candi-
date names for a discourse participant passes the
threshold, we assign the gender to be ?I? (Indeter-
minate). We also assign the gender to be ?I?, if
none of the candidate names is present in the SSA
dataset. This will occur if the name is a first name
that is not in the database (an unusual or interna-
tional name; e.g., Vladi), or if no true first name
was found (e.g., the name field was empty and the
email address was only a pseudonym). This will
also include most of the cases where the discourse
participant is not a human.
3.3.3 Coverage and Accuracy
We evaluated the coverage and accuracy of our
gender assignment system on the manually as-
signed gender data of the 145 core people. We
obtained a coverage of 90.3%, i.e., for 14 of the
145 core people, the ambiguity score was higher
than the threshold. Of the 131 people the sys-
tem assigned a gender to, we obtained an accu-
racy of 89.3% in correctly identifying the gender.
We investigated the errors and found that all er-
rors were caused due to incorrectly identifying the
first name. These errors arise because the name
fields are automatically populated and sometimes
the core discourse participants? name fields in-
clude their secretaries. While this is common for
people in higher managerial positions, we expect
this not to happen in the middle management and
below, to which most of the automatically gender-
assigned discourse participants belong.
3.4 Corpus Statistics and Divisions
We apply the gender assignment system described
above to all discourse participants of all email
threads in the entire Enron corpus described in
Section 3.1. Table 1 shows the coverage of gen-
der assignment in our corpus at different lev-
els: unique discourse participants, messages and
threads. In Table 2, we show the male/female per-
centage split of all unique discourse participants,
as well as the split at the level of messages (i.e.,
messages sent by males vs. females).
1968
Count (%)
Total unique discourse participants 34,156
- gender identified 23,009 (67.3%)
Total messages 111,933
- senders gender identified 97,255 (86.9%)
Total threads 36,615
- all senders gender identified 26,015 (71.1%)
- all participants gender identified 18,030 (49.2%)
Table 1: Coverage of Gender Identification at various level:
unique discourse participants, messages and threads
Male Female
Unique Discourse Participants 66.1% 33.9%
Message Senders 58.2% 41.8%
Table 2: Male/Female split across a) all unique participants
who were gender identified, b) all messages whose senders
were gender identified
We divide the entire corpus into Train, Dev and
Test sets at the thread level, through random sam-
pling, with a distribution of 50%, 25% and 25%
each. The number of threads and messages in each
subdivision is shown in Table 3.
Total Train Dev Test
Threads 36,615 18,498 8,973 9,144
Messages 111,933 56,447 27,565 27,921
Table 3: Train/Test/Dev breakup of the entire corpus
We also create a sub-corpus of the threads called
All Participants Gender Identified (APGI), con-
taining the 18,030 threads for which the gender as-
signment system succeeded in assigning the gen-
ders of all participants, including senders and all
recipients (To and CC). For the analysis and ex-
periments presented in the rest of this paper, we
use 17,788 threads from this APGI subset, exclud-
ing the remaining 242 threads that were used for
previous manual annotation efforts.
4 Manifestations of Power
We use the gender information of the participants
to investigate how the gender of the sender and
recipients affect the manifestations of hierarchical
power in interactions. In order to do this, we use
the interaction analysis framework from our prior
work (Prabhakaran and Rambow, 2014). In this
section, we give a brief overview of the problem
formulation and the structural features we used.
4.1 Hierarchically Related Interacting Pairs
Let t denote an email thread and M
t
denote the
set of all messages in t . Also, let P
t
be the set
of all participants in t , i.e., the union of senders
and recipients (To and CC) of all messages in M
t
.
We are interested in analyzing the power relations
between pairs of participants who interact within
a given email thread. Not every pair of partic-
ipants (p
1
, p
2
) ? P
t
? P
t
interact with one an-
other within t . Let IM
t
(p
1
, p
2
) denote the set of
Interaction Messages ? non-empty messages in
t in which either p
1
is the sender and p
2
is one
of the recipients or vice versa. We call the set
of (p
1
, p
2
) such that |IM
t
(p
1
, p
2
)| > 0 the inter-
acting participant pairs of t (IPP
t
). For every
(p
1
, p
2
) ? IPP
t
, we query the set of dominance
relations in the gold hierarchy and assign their hi-
erarchical power relation (HP(p
1
, p
2
)) to be su-
perior if p
1
dominates p
2
, and subordinate if p
2
dominates p
1
. We exclude pairs that do not exist
in the gold hierarchy from our analysis and call
the remaining set related interacting participant
pairs (RIPP
t
). Table 4 shows the total number
of pairs in IPP
t
and RIPP
t
from all the threads
in the APGI subset of our corpus and across Train,
Dev and Test sets.
Description Total Train Dev Test
# of threads 17,788 8,911 4,328 4,549
?
t
|IPP
t
| 74,523 36,528 18,540 19,455
?
t
|RIPP
t
| 4,649 2,260 1,080 1,309
Table 4: Data Statistics
Row 1 presents the total number of threads in different
subsets of the corpus. Row 2 and 3 present the number of
interacting participant pairs (IPP ) and related interacting
participant pairs (RIPP ) in those subsets.
4.2 Structural Features
Now, we describe various features that capture
the structure of interaction between the pairs of
participants in a thread. Each feature f is ex-
tracted with respect to a person p over a refer-
ence set of messages M (denoted f
p
M
). For a pair
(p
1
, p
2
), we extract 4 versions of each feature f :
f
p
1
IM
t
(p
1
,p
2
)
, f
p
2
IM
t
(p
1
,p
2
)
, f
p
1
M
t
and f
p
2
M
t
. The first two
capture behavior of each person of the pair in in-
teractions between themselves, while the third and
fourth capture their overall behavior in the entire
thread. We group our features into three categories
? THR
STR
, THR
META
and DIA. THR
STR
cap-
tures the thread structure in terms of verbosity and
1969
positional features of messages (e.g., how many
emails did a person send). THR
META
contain
email header meta-data based features that cap-
ture the thread structure (e.g., how many recipients
were there). Both sets of features do not perform
any NLP analysis on the the content of the emails.
DIA captures the pragmatics of the dialog and re-
quires a deeper analysis of the email content (e.g.,
did they issue any requests).
THR
STR
: This feature set includes two kinds
of features ? positional and verbosity. The po-
sitional features are a boolean feature to denote
whether p sent the first message (Initiate), and
the relative positions of p?s first and last messages
(FirstMsgPos and LastMsgPos) in M . The ver-
bosity features are p?s message count (MsgCount),
message ratio (MsgRatio), token count (Token-
Count), token ratio (TokenRato) and tokens per
message (TokenPerMsg), all calculated over M .
THR
META
: This feature set includes the av-
erage number of recipients (AvgRecipients) and
To recipients (AvgToRecipients) in emails sent by
p, the percentage of emails p received in which
he/she was in the To list (InToList%), boolean fea-
tures denoting whether p added or removed peo-
ple when responding to a message (AddPerson
and RemovePerson), average number of replies re-
ceived per message sent by p (ReplyRate) and av-
erage number of replies received from the other
person of the pair to messages where he/she was
a To recipient (ReplyRateWithinPair). ReplyRate-
WithinPair applies only to IM
t
(p
1
, p
2
).
DIA: We use dialog acts (DA) and overt dis-
plays of power (ODP) tags to model the struc-
ture of interactions within the message content.
We obtain DA and ODP tags using automatic tag-
gers trained on manual annotations. The DA tag-
ger (Omuya et al., 2013) obtained an accuracy of
92%. The ODP tagger (Prabhakaran et al., 2012)
obtained an accuracy of 96% and F-measure of
54%. The DA tagger labels each sentence to be
one of the 4 dialog acts: Request Action, Request
Information, Inform, and Conventional. The ODP
Tagger identifies sentences (mostly requests) that
express additional constraints on their addressee,
beyond those introduced by the dialog act. For
example, the sentence ?Please come to my of-
fice right now? is considered as an ODP, while
?It would be great if you could come to my of-
fice now? is not, even though both issue the same
request. For more details on ODP, we refer the
Feature Name Mean(f
X
IM
t
)|X =
F
sub
F
sup
M
sub
M
sup
THR
META
AvgRecipients
???
4.76 5.74 5.58 4.98
AvgToRecipients
???
3.63 4.73 3.84 3.80
InToList%
.
0.83 0.86 0.84 0.83
ReplyRate
???
0.72 0.86 0.70 0.61
AddPerson 0.58 0.66 0.59 0.68
RemovePerson 0.55 0.60 0.54 0.65
THR
STR
Initiate 0.38 0.24 0.39 0.30
FirstMsgPos
?
0.18 0.25 0.19 0.22
LastMsgPos
??
0.34 0.33 0.34 0.39
MsgCount
???
0.92 0.61 0.93 0.91
MsgRatio
???
0.33 0.23 0.33 0.32
TokenCount 76.5 41.0 102.0 54.3
TokenRatio 0.38 0.23 0.40 0.27
TokenPerMsg
???
90.2 67.9 118.2 53.2
DIA
PR
Conventional 0.55 0.43 0.64 0.56
Inform 3.50 1.96 4.51 2.53
ReqAction
??
0.07 0.06 0.05 0.10
ReqInform 0.29 0.21 0.20 0.16
DanglingReq% 0.06 0.12 0.07 0.18
ODPCount
???
0.10 0.07 0.09 0.13
Table 5: ANOVA results and group means for Hierarchical
Power and Gender
F
sub
: Female subordinates; F
sup
: Female superiors;
M
sub
: Male subordinates; M
sup
: Male superiors;
* (p < .05 ); ** (p < .01 ); *** (p < .001 )
reader to (Prabhakaran et al., 2012). We use 5
features: ReqAction, ReqInform, Inform, Conven-
tional, and ODPCount to capture the number of
sentences in messages sent by p that have each of
these labels. We also use a feature to capture the
number of p?s messages with a request that did not
get a reply, i.e., dangling request percentage (Dan-
glingReq%), over all messages sent by p.
5 Gender and Power
In this subsection, we analyze the impact of gen-
der on the expression of power in email. We per-
form an ANOVA test on all features described in
Section 4.2 keeping both Hierarchical Power and
Gender as independent variables. We perform this
on the Train subset of the APGI subset of our cor-
pus. Table 5 shows the results for thread level ver-
sion of the features (we obtain similar significance
results at the interaction level as well). As can be
seen from the ANOVA results, the mean values of
many features differ significantly for the factorial
1970
0.091	 ?
0.114	 ?
0.086	 ?
0.113	 ?
0.096	 ?
0.072	 ?
0.086	 ?
0.135	 ?
0	 ?
0.04	 ?
0.08	 ?
0.12	 ?
0.16	 ?
Subordinates	 ? Superiors	 ? Female	 ? Male	 ? Female	 ?
Subordinates	 ?
Female	 ?
Superiors	 ?
Male	 ?
Subordinates	 ?
Male	 ?
Superiors	 ?
Figure 1: Mean values of ODPCounts in different groups: Subordinates vs. Superiors; Female vs. Male;
across all combinations of Hierarchical Power and Gender.
groups of Hierarchical Power and Gender. For ex-
ample, ReplyRate was highly significant; female
superiors obtain the highest reply rate.
It is crucial to note that ANOVA only deter-
mines that there is a significant difference between
groups, but does not tell which groups are signifi-
cantly different. In order to ascertain that, we must
use the Tukey?s HSD (Honest Significant Differ-
ence) Test. We do not describe the analysis of
all our features to that depth in this paper due to
space limitations. Instead, we investigate specific
hypotheses which we have derived from sociolin-
guistic literature. The first hypothesis we investi-
gate is:
? Hypothesis 1: Female superiors tend to use
?face-saving? strategies at work that include
conventionally polite requests and imperson-
alized directives, and that avoid imperatives
(Herring, 2008).
As a stand-in for a face-threatening communica-
tive strategy, we use our ?Overt Display of Power?
feature (ODP). An ODP limits the addressee?s
range of possible responses, and thus threatens his
or her (negative) face.
6
We thus reformulate our
hypothesis as follows: the use of ODP by superi-
ors changes when looking at the splits by gender,
with female superiors using fewer ODPs than male
superiors. We look further into the ANOVA anal-
ysis of the thread-level ODPCount treating Hierar-
chical Power and Gender as independent variables.
Figure 1 shows the mean values of ODP counts in
6
For a discussion of the notion of ?face?, see (Brown and
Levinson, 1987).
each group of participants. A summary of the re-
sults follows.
Hierarchical Power was significant. Subordi-
nates had an average of 0.091 ODP counts and Su-
periors had an average of 0.114 ODP counts. Gen-
der was also significant; Females had an average
of 0.086 ODP counts and Males had an average of
0.113 ODP counts. When looking at the factorial
groups of Hierarchical Power and Gender, how-
ever, several results were very highly significant.
The significantly different pairs of groups, as per
the Tukey?s HSD test, are Male Superiors/Male
Subordinates, Male Superiors/Female Superiors,
and Male Superiors/Female Subordinates. Male
Superiors used the most ODPs, with an average
of 0.135 counts. Somewhat surprisingly, Female
Superiors used the least of the entire group, with
an average of 0.072 counts. Among Subordinates,
Females actually used slightly more ODP, with an
average of 0.096 counts. Male Subordinates had
an average of 0.086 ODP counts. However, the
differences among these three groups (Female Su-
periors, Female Subordinates, and Male Subordi-
nates) are not significant.
The results confirm our hypothesis: female
superiors use fewer ODPs than male superiors.
However, we also see that among women, there
is no significant difference between superiors and
subordinates, and the difference between superi-
ors and subordinates in general (which is signif-
icant) is entirely due to men. This in fact shows
that a more specific (and more interesting) hypoth-
esis than our original hypothesis is validated: only
male superiors use more ODPs than subordinates.
1971
6 Gender Environment and Power
We now turn to gender environments and their re-
lation to the expression of power in written di-
alogs. We again start with a hypothesis based on
the sociolinguistic literature.
? Hypothesis 2: Women use language to cre-
ate and maintain social relations, for exam-
ple, they use more small talk (based on a re-
ported ?stereotype? in (Holmes and Stubbe,
2003)).
We first define more formally what we mean by
?gender environment? (Section 6.1), and then in-
vestigate our hypothesis (Section 6.2).
6.1 The Notion of ?Gender Environment?
The notion of ?gender environment? refers to the
gender composition of a group who are communi-
cating. In the sociolinguistic studies we have con-
sulted (Holmes and Stubbe, 2003; Herring, 2008),
the notion refers to a stable work group who in-
teract regularly. Since we are interested in study-
ing email conversations (threads), we adapt the
notion to refer to a single thread at a time. Fur-
thermore, we assume that a discourse participant
makes communicative decisions based on (among
other factors) his or her own gender, and based
on the genders of the people he or she is commu-
nicating with in a given conversation (i.e., email
thread). We therefore consider the ?gender envi-
ronment? to be specific to each discourse partic-
ipant and to describe the other participants from
his or her point of view. Put differently, we use the
notion of ?gender environment? to model a dis-
course participant?s (potential) audience in a con-
versation. For example, a conversation among five
women and one man looks like an all-female audi-
ence from the man?s point of view, but a majority-
female audience from the women?s points of view.
We define the gender environment of a dis-
course participant p in a thread t as follows. As
discussed, we assume that the gender environment
is a property of each discourse participant p in
thread t. We take the set of all discourse partic-
ipants of the thread t, P
t
(see Section 4.1), and
exclude p from it: P
t
\ {p}. We then calculate
the percentage of women in this set.
7
We obtain
7
We note that one could also define the notion of gender
environment at the level of individual emails: not all emails
in a thread involve the same set of participants. We leave this
to future work.
three groups by setting thresholds on these per-
centages. Finer-grained gender environments re-
sulted in partitions of the data with very few in-
stances, since most of our data involves fairly bal-
anced gender ratios. The three gender environ-
ments we use are the following:
? Female Environment: if the percentage of
women in P
t
\ {p} is above 66.7%.
? Mixed Environment: if the percentage of
women in P
t
\ {p} is between 33.3% and
66.7%.
? Male Environment: if the percentage of
women in P
t
\ {p} is below 33.3%
Across all threads and discourse participants in
the threads, we have 791 female, 2087 mixed and
1642 male gender environments.
6.2 Gender Environment and Conventional
Dialog Acts
We now turn to testing Hypothesis 2. We have at
present no way of testing for ?small talk? as op-
posed to work-related talk, so we instead test Hy-
pothesis 2 by asking how many conventional dia-
log acts a person performs. Conventional dialog
acts serve not to convey information or requests
(both of which would typically be work-related in
the Enron corpus), but to establish communication
(greetings) and to manage communication (sign-
offs); since communication is an important way of
creating and maintaining social relations, we can
say that conventional dialog acts serve the purpose
of easing conversations and thus of maintaining
social relations. Since this aspect of language is
specifically dependent on a group of people (it is
an inherently social function), we assume that the
relevant feature is not simply Gender, but Gender
Environment. Specifically, we make our Hypothe-
sis 2 more precise by saying that a higher number
of conventional dialog acts is used in Female En-
vironments. We use the thread level version of the
feature ConventionalCount.
Figure 2 shows the mean values of Conven-
tionalCount in each sub-group of participants.
Hierarchical Power was highly significant as
per ANOVA results. Subordinates use conven-
tional language more (0.60 counts) than Superiors
(0.52). Gender is a very highly significant vari-
able; Males use 0.60 counts on average, whereas
1972
0.60	 ?
0.52	 ?
0.61	 ?
0.54	 ? 0.56	 ?
0.79	 ?
0.48	 ?
0.57	 ? 0.51	 ? 0.56	 ? 0.55	 ?
0.00	 ?
0.25	 ?
0.50	 ?
0.75	 ?
1.00	 ?
Subordinates	 ? Superiors	 ? Female	 ?Env	 ? Mixed	 ?Env	 ? Male	 ?Env	 ? Subordinates	 ?
in	 ?Female	 ?Env	 ?
Superiors	 ?in	 ?
Female	 ?Env	 ?
Subordinates	 ?
in	 ?Mixed	 ?Env	 ?
Superiors	 ?in	 ?
Mixed	 ?Env	 ?
Subordinates	 ?
in	 ?Male	 ?Env	 ?
Superiors	 ?in	 ?
Male	 ?Env	 ?
Figure 2: Mean values of Conventional Counts: Subordinates vs. Superiors; across all Gender
Environments; across all combinations of Hierarchical Power and Gender Environments.
Females use 0.50. This result is somewhat sur-
prising, but does not invalidate our Hypothesis 2,
since our hypothesis is not formulated in terms of
Gender, but in terms of Gender Environment. The
analysis of Gender Environment at first appears to
be a negative result: while the averages by Gender
Environment differ, the differences are not signif-
icant. However, the groups defined by both Hi-
erarchical Power and Gender Environment have
highly significant differences. Subordinates in Fe-
male Environments use the most conventional lan-
guage of all six groups, with an average of 0.79.
Superiors in Female Environments use the least,
with an average of 0.48. Mixed Environments and
Male Environments differ, but are more similar to
each other than to Female Environments. In fact,
in the Tukey HSD test, the only significant pairs
are exactly the set of subordinates in Female En-
vironments paired with each other group (Supe-
riors in Female Environments, and Subordinates
and Superiors in Mixed Environments and Male
Environments). That is, Subordinates in Female
environments use significantly more conventional
language than any other group, but the remaining
groups do not differ significantly from each other.
Our hypothesis is thus only partially verified:
while gender environment is a crucial aspect of the
use of conventional DAs, we also need to look at
the power status of the writer. In fact only sub-
ordinates in female environments use more con-
ventional DAs than any other group (as defined by
power status and gender environment). While our
hypothesis is not fully verified, we interpret the
results to mean that subordinates are more com-
fortable in female environments to use a style of
communication which includes more conventional
DAs than outside the female environments.
7 Predicting Power in Participant Pairs
In this section, we use the formulation of
the power prediction problem presented in our
prior work (Prabhakaran and Rambow, 2014).
Given a thread t and a pair of participants
(p
1
, p
2
) ? RIPP
t
, we want to automatically de-
tect HP(p
1
, p
2
). We use the SVM-based su-
pervised learning system from (Prabhakaran and
Rambow, 2014) that can predict HP(p
1
, p
2
) to
be either superior or subordinate based on the in-
teraction within a thread t for any pair of partici-
pants (p
1
, p
2
) ? RIPP
t
. The order of participants
in (p
1
, p
2
) is fixed such that p
1
is the sender of
the first message in IM
t
(p
1
, p
2
). The power pre-
diction system is built using the ClearTK (Ogren
et al., 2008) wrapper for SVMLight (Joachims,
1999) package. It uses a quadratic kernel to cap-
ture feature-feature interactions, which is very im-
portant as we see in Section 5 and 6. We use the
Train, Dev and Test subsets of the APGI subset
of our corpus for our experiments. We use the re-
lated interacting participant pairs in threads from
the Train set to train our models and optimize our
performance on those from the Dev set. We report
results on both Dev and Test sets.
In addition to the features described in Sec-
tion 4.2, the power prediction system presented
in (Prabhakaran and Rambow, 2014) uses a lexi-
cal feature set (LEX) that captures word ngrams,
POS (part of speech) ngrams and mixed ngrams,
since lexical features have been established to be
very useful for power prediction. Mixed ngrams
are word ngrams where words belonging to open
classes are replaced with their POS tags. We add
two gender-based feature sets: GEN containing
the gender of both persons of the pair and ENV
containing the gender environment feature.
1973
Table 6 presents the results obtained using vari-
ous feature combinations. We experimented using
all subsets of {LEX, THR
STR
, THR
META
, DIA,
GEN, ENV } on the Dev set; we report the most
interesting results here. The majority baseline
(subordinate) obtains an accuracy of 55.8%. Us-
ing the gender-based features alone performs only
slightly better than the majority baseline. We use
the best performing feature subset from (Prab-
hakaran and Rambow, 2014) (LEX + THR
META
)
as another baseline, which obtains an accuracy
of 68.2%. Adding the GEN features improves
the performance to 70.6%. Further adding the
ENV features improves the performance, but only
marginally to 70.7% (our overall best result, an
improvment of 2.4% points). The best perform-
ing feature set without using LEX was the combi-
nation of DIA, THR
META
and GEN (67.3%). Re-
moving the gender features from this reduced the
performance to 64.6%. Similarly, the best per-
forming feature set which do not use the content
of emails at all was THR
STR
+ THR
META
+ GEN
(66.6). Removing the gender features decreases
the accuracy by a larger margin (5.4% accuracy
reduction to 63.0).
We interpret the differences in absolute im-
provement as follows: the gender-based features
on their own are not very useful, and gain predic-
tive value only when paired with other features.
This is because the other features in fact make
quite different predictions depending on gender
and/or gender environment. However, the content
features (and in particular the lexical features) are
so powerful on their own that the relative contribu-
tion of the gender-based features decreases again.
Nonetheless, we take these results as validation of
the claim that gender-based features enhance the
value of other features in the task of predicting
power relations.
We performed another experiment where we
partitioned the data into two subsets according to
the gender of the first person of the pair and trained
two separate models to predict power. At test time,
we chose the appropriate model based on the gen-
der of the first person of the pair. However, this
did not improve the performance.
On our blind test set, the majority baseline ob-
tains an accuracy of 57.9% and the (Prabhakaran
and Rambow, 2014) baseline obtains an accuracy
of 68.9%. On adding the gender-based features,
the accuracy of the system improves to 70.2%.
Description Accuracy
Majority (Always Subordinate) 55.83
GEN 57.59
GEN + ENV 57.59
Baseline (LEX + THR
META
) 68.24
Baseline (LEX + THR
META
) + GEN 70.56
Baseline (LEX + THR
META
) + GEN + ENV 70.74
DIA + THR
META
+ GEN 67.31
DIA + THR
META
64.63
THR
STR
+ THR
META
+ GEN 66.57
THR
STR
+ THR
META
62.96
Table 6: Accuracies on feature subsets (Dev set).
THR
META
: meta-data; THR
STR
: structural; DIA: dialog-act;
GEN: gender; ENV: gender environment; LEX: ngrams;
8 Conclusion
We presented a new, freely available resource: the
Gender Identified Enron Corpus, and explored the
relation between power, gender, and language us-
ing this resource. We also introduced the notion
of gender environment, and showed that the man-
ifestations of power differ significantly between
gender environments. We also showed that the
gender-related features helps in improving power
prediction. In future work, we will explore ma-
chine learning algorithms which capture the inter-
actions between features better than our SVM with
quadratic kernel.
We expect our corpus to be a rich resource for
social scientists interested in the effect of power
and gender on language use. We will investi-
gate several other sociolinguistic-inspired research
questions; for example, do the strategies managers
use for ?effectiveness? of communication differ
based on gender environments?
While our findings pertain to the Enron data
set, we believe that the insights and techniques
from this study can be extended to other genres
in which there is an independent notion of hierar-
chical power, such as moderated online forums.
Acknowledgments
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We thank several anony-
mous reviewers for their constructive feedback.
1974
References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A comprehensive gold stan-
dard for the enron organizational hierarchy. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 161?165, Jeju Island, Korea, July.
Association for Computational Linguistics.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical varia-
tion in social media. Journal of Sociolinguistics,
18(2):135?160.
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37?45, Montr?eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In ACL, pages
773?782. The Association for Computational Lin-
guistics.
Penelope Brown and Stephen C. Levinson. 1987.
Politeness : Some Universals in Language Usage
(Studies in Interactional Sociolinguistics). Cam-
bridge University Press, February.
Na Cheng, R. Chandramouli, and K. P. Subbalakshmi.
2011. Author gender identification from text. Digit.
Investig., 8(1):78?88, July.
Malcolm Corney, Olivier de Vel, Alison Anderson, and
George Mohay. 2002. Gender-preferential text min-
ing of e-mail discourse. In Computer Security Ap-
plications Conference, 2002. Proceedings. 18th An-
nual, pages 282?289. IEEE.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
?12, New York, NY, USA. ACM.
William Deitrick, Zachary Miller, Benjamin Valyou,
Brian Dickinson, Timothy Munson, and Wei Hu.
2012. Author gender prediction in an email stream
using neural networks. Journal of Intelligent Learn-
ing Systems & Applications, 4(3).
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
?12, pages 1037?1046, New York, NY, USA. ACM.
Susan C Herring. 2008. Gender and power in on-
line communication. The handbook of language and
gender, page 202.
Janet Holmes and Maria Stubbe. 2003. feminine work-
places: stereotype and reality. The handbook of lan-
guage and gender, pages 572?599.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Sch?olkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
Shari Kendall and Deborah Tannen. 1997. Gender
and language in the workplace. In Gender and Dis-
course, pages 81?105. Sage, London.
Shari Kendall. 2003. Creating gendered demeanors
of authority at work and at home. The handbook of
language and gender, page 600.
Saif Mohammad and Tony Yang. 2011. Tracking sen-
timent in mail: How genders differ on emotional
axes. In Proceedings of the 2nd Workshop on Com-
putational Approaches to Subjectivity and Sentiment
Analysis (WASSA 2.011), pages 70?79, Portland,
Oregon, June. Association for Computational Lin-
guistics.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Philip V. Ogren, Philipp G. Wetzler, and Steven
Bethard. 2008. ClearTK: A UIMA toolkit for sta-
tistical natural language processing. In Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP workshop at Language Resources
and Evaluation Conference (LREC).
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minor-
ity class identification in dialog act tagging. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
802?807, Atlanta, Georgia, June. Association for
Computational Linguistics.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study
on the enron corpus. In Proceedings of the Work-
shop on Language in Social Media (LSM 2011),
pages 86?95, Portland, Oregon, June. Association
for Computational Linguistics.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216?224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran and Owen Rambow. 2014.
Predicting power relations between participants in
written dialog from a single thread. In Proceed-
ings of the 52nd Annual Meeting of the Association
1975
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 339?344, Baltimore, Maryland, June.
Association for Computational Linguistics.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technolo-
gies: The 2012 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Montreal, Canada, June. Associ-
ation for Computational Linguistics.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365?373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Vinodkumar Prabhakaran, Ashima Arora, and Owen
Rambow. 2014. Power of confidence: How poll
scores impact topic dynamics in political debates.
In Proceedings of the ACL 2014 Workshop on Lan-
guage Technologies and Computational Social Sci-
ence, page 49, Baltimore, MD, USA, June. Associa-
tion for Computational Linguistics.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535?2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In CEAS
2006 - The Third Conference on Email and Anti-
Spam, July 27-28, 2006, Mountain View, California,
USA, Mountain View, California, USA, July.
1976
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 518?522,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Overt Display of Power in Written Dialogs
Vinodkumar Prabhakaran
Computer Science Dept.
Columbia University
New York, NY 10027, USA
vinod@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY 10027, USA
rambow@ccls.columbia.edu
Mona Diab
CCLS
Columbia University
New York, NY 10027, USA
mdiab@ccls.columbia.edu
Abstract
We analyze overt displays of power (ODPs)
in written dialogs. We present an email cor-
pus with utterances annotated for ODP and
present a supervised learning system to predict
it. We obtain a best cross validation F-measure
of 65.8 using gold dialog act features and 55.6
without using them.
1 Introduction
Analyzing written dialogs (such as email exchanges)
to extract social power relations has generated great
interest recently. This paper introduces a new task
within the general field of finding power relations
in written dialogs. In written dialog, an utterance
can represent an overt display of power (ODP) on
the part of the utterer if it constrains the addressee?s
actions beyond the constraints that the underlying
dialog act on its own imposes. For example, a re-
quest for action is the first part of an adjacency pair
and thus requires a response from the addressee, but
declining the request is a valid response. However,
the utterer may formulate her request for action in a
way that attempts to remove the option of declining
it (?Come to my office now!?). In so doing, she re-
stricts her addressee?s options for responding more
severely than a simple request for action would. Our
new task is to classify utterances in written dialog
as to whether they are ODPs or not. Such a classifi-
cation can be interesting in and of itself, and it can
also be used to study social relations among dialog
participants.
After reviewing related work (Section 2), we de-
fine ?overt display of power? (Section 3) and then
present manual annotations for ODP in a small sub-
set of Enron email corpus. In Section 5, we present a
supervised learning system using word and part-of-
speech features along with features indicating dialog
acts.
2 Related Work
Many studies in sociolinguistics have shown that
power relations are manifested in language use
(e.g., (O?Barr, 1982)). Locher (2004) recognizes
?restriction of an interactant?s action-environment?
(Wartenberg, 1990) as a key element by which ex-
ercise of power in interactions can be identified.
Through ODP we capture this action-restriction at
an utterance level. In the computational field, sev-
eral studies have used Social Network Analysis
(e.g., (Diesner and Carley, 2005)) for extracting so-
cial relations from online communication. Only re-
cently have researchers started using NLP to analyze
the content of messages to deduce social relations
(e.g., (Diehl et al, 2007)). Bramsen et al (2011) use
knowledge of the actual organizational structure to
create two sets of messages: messages sent from a
superior to a subordinate, and vice versa. Their task
is to determine the direction of power (since all their
data, by construction of the corpus, has a power re-
lationship). Their reported results cannot be directly
compared with ours since their results are on classi-
fying aggregations of messages as being to a supe-
rior or to a subordinate, whereas our results are on
predicting whether a single utterance has an ODP or
not.
518
3 Overt Display of Power (ODP)
Dialog is successful when all discourse participants
show cooperative dialog behavior. Certain types of
dialog acts, notably requests for actions and requests
for information (questions), ?set constraints on what
should be done in a next turn? (Sacks et al, 1974).
Suppose a boss sends an email to her subordinate:
?It would be great if you could come to my of-
fice right now?. He responds by politely declining
(?Would love to, but unfortunately I need to pick up
my kids?). He has met the expectation to respond
in one of the constrained ways that the request for
action allows (other acceptable responses include a
commitment to performing the action, or actually
performing the action, while unacceptable responses
include silence, or changing the topic). However, di-
alog acts only provide an initial description of these
constraints. Other sources of constraints include
the social relations between the utterer and the ad-
dressee, and the linguistic form of the utterance. As-
sume our email example had come, say, from the
CEO of the company. In this case, the addressee?s
response would not meet the constraints set by the
utterance, even though it is still analyzed as the same
dialog act (a request for action). Detecting such
power relations and determining their effect on di-
alog is a hard problem, and it is the ultimate goal of
our research. Therefore, we do not use knowledge
of power relations as features in performing a finer-
grained analysis of dialog acts. Instead, we turn to
the linguistic form of an utterance. Specifically, the
utterer can choose linguistic forms in her utterance
to signal that she is imposing further constraints on
the addressee?s choice of how to respond, constraints
which go beyond those defined by the standard set
of dialog acts. For example, if the boss?s email is
?Please come to my office right now?, and the ad-
dressee declines, he is clearly not adhering to the
constraints the boss has signaled, though he is ad-
hering to the general constraints of cooperative dia-
log by responding to the request for action. We are
interested in these additional constraints imposed on
utterances through choices in linguistic form. We
define an utterance to have Overt Display of Power
(ODP) if it is interpreted as creating additional con-
straints on the response beyond those imposed by
the general dialog act. Note that use of polite lan-
ID Sample utterance
s1 If there is any movement of these people between
groups can you please keep me in the loop.
s2 I need the answer ASAP, as ....
s3 Please give me your views ASAP.
s4* Enjoy the rest of your week!
s5 Would you work on that?
s6* ... would you agree that the same law firm advise on
that issue as well?
s7* can you BELIEVE this bloody election?
s8 ok call me on my cell later.
Table 1: Sample utterances from the corpus; * next to ID
denotes an utterance without an ODP
guage does not, on its own, determine the presence
or absence of an ODP. Furthermore, the presence of
an ODP does not presuppose that the utterer actually
possess social power: the utterer could be attempt-
ing to gain power.
Table 1 presents some sample utterances cho-
sen from our corpus (the * indicates those without
ODP). An utterance with ODP can be an explicit or-
der or command (s3, s8) or an implicit one (s2, s5).
It can be a simple sentence (s3) or a complex one
(s1). It can be an imperative (s3), an interrogative
(s5) or even a declarative (s2) sentence. But not all
imperatives (s4) or interrogatives (s6, s7) are ODPs.
s5, s6 and s7 are all syntactically questions. How-
ever, s5?s discourse function within an email is to
request/order to work on ?that? which makes it an
instance of ODP, while s6 is merely an inquiry and
s7 is a rhetorical question. This makes the problem
of finding ODP in utterances a non-trivial one.
4 Data and Annotations
For our study, we use a small corpus of Enron email
threads which has been previously annotated with
dialog acts (Hu et al, 2009). The corpus contains
122 email threads with 360 messages, 1734 utter-
ances and 20,740 word tokens. We trained an anno-
tator using the definition for ODP given in Section
3. She was given full email threads whose messages
were already segmented into utterances. She iden-
tified 86 utterances (about 5%) to have an ODP.1 In
1These annotations were done as part of a larger annotation
effort (Prabhakaran et al, 2012). The annotated corpus can be
obtained at http://www.cs.columbia.edu/?vinod/powerann/.
519
order to validate the annotations, we trained another
annotator using the same definitions and examples
and had him annotate 46 randomly selected threads
from the corpus, which contained a total of 595 ut-
terances (34.3% of whole corpus). We obtained a
reasonable inter annotator agreement, ? value, of
0.669, which validates the annotations while con-
firming that the task is not a trivial one.
5 Automatic ODP Tagging
In this section, we present a supervised learning
method to tag unseen utterances that contain an ODP
using a binary SVM classifier. We use the tokenizer,
POS tagger, lemmatizer and SVMLight (Joachims,
1999) wrapper that come with ClearTK (Ogren et
al., 2008). We use a linear kernel with C = 1 for
all experiments and present (P)recision, (R)ecall and
(F)-measure obtained on 5-fold cross validation on
the data. Our folds do not cross thread boundaries.
5.1 Handling Class Imbalance
In its basic formulation, SVMs learn a decision func-
tion f from a set of positive and negative training in-
stances such that an unlabeled instance x is labeled
as positive if f(x) > 0. Since SVMs optimize on
training set accuracy to learn f , it performs better
on balanced training sets. However, our dataset is
highly imbalanced (? 5% positive instances). We
explore two ways of handling this class imbalance
problem: an instance weighting method, InstWeight,
where training errors on negative instances are out-
weighed by errors on positive instances, and SigTh-
resh, a threshold adjusting method to find a better
threshold for f(x). For InstWeight, we used the j
option in SVMlight to set the outweighing factor
to be the ratio of negative to positive instances in
the training set for each cross validation fold. Inst-
Weight is roughly equivalent to oversampling by re-
peating positive instances. For SigThresh, we used
a threshold based on a posterior probabilistic score,
p = Pr(y = 1|x), calculated using the ClearTK im-
plementation of Lin et al (2007)?s algorithm. It uses
Platt (1999)?s approximation of p to a sigmoid func-
tion PA,B(f) = (1 + exp(Af + B))?1, where A
and B are estimated from the training set. Then, we
predict x as positive if p > 0.5 which in effect shifts
the threshold for f(x) to a value based on its distri-
Experiment
InstWeight SigThresh
P R F P R F
ALL-TRUE 5.0 100.0 9.5 5.0 100.0 9.5
RANDOM 5.7 58.1 10.4 5.7 58.1 10.4
WORD-UNG 43.1 29.1 34.7 63.0 39.5 48.6
PN,MN,FV,DA 66.7 48.8 56.4 72.3 54.7 62.3
PN,MN,DA 64.5 46.5 54.1 75.8 58.1 65.8
LN,PN,MN,FV 64.4 44.2 52.4 65.2 50.0 56.6
Table 2: Results
Class Imbalance Handling: InstWeight: Instance weighting and
SigThresh: Sigmoid thresholding
Features: WORD-UNG: Word unigrams, LN: Lemma ngrams, PN:
POS ngrams, MN: Mixed ngrams, FV: First verb, DA: Dialog acts
bution on positive and negative training instances.
5.2 Features
We present experiments using counts of three types
of ngrams: lemma ngrams (LN), POS ngrams (PN)
and mixed ngrams (MN).2 Mixed ngram is a re-
stricted formulation of lemma ngram where open-
class lemmas (nouns, verbs, adjectives and adverbs)
are replaced by POS tags. E.g., for the utterance
s2, LN would capture patterns {i, need, i need, . . .},
while PN would capture {PRP, VBP, PRP VBP, . . .}
and MN would capture {i VBP the NN, . . .}. We
also used a feature (FV) to denote the first verb
lemma in the utterance. Since ODPs, like dialog
acts, constrain how the addressee should react, we
also include Dialog Acts as features (DA). We use
the manual gold dialog act annotations present in
our corpus, which use a very small dialog act tag
set. An utterance has one of 5 dialog acts: Reques-
tAction, RequestInformation, Inform, Commit and
Conventional (see (Hu et al, 2009) for details). For
example, for utterance s2, FV would be ?need? and
DA would be ?Inform?.3
5.3 Results and Analysis
We present two simple baselines ? ALL-TRUE,
where an utterance is always predicted to have an
ODP, and RANDOM, where an utterance is pre-
dicted at random, with 50% chance to have an ODP.
We also present a strong baseline WORD-UNG,
2LN performed consistently better than word ngrams.
3We also explored other features including the number of
tokens, the previous or following dialect act, none of which im-
proved the results and. We omit a detailed discussion for rea-
sons of space.
520
which is trained using surface-form word unigrams
as features. ALL-TRUE and RANDOM obtained F
scores of 9.5 and 10.4 respectively, while WORD-
UNG obtained an F score of 34.7 under InstWeight,
and improved it to 48.6 under SigThresh.
For LN, PN and MN, we first found the best value
for n to be 1, 2 and 4, respectively. We then did
an exhaustive search in all combinations of LN, PN,
MN, FV and DA under both InstWeight and SigTh-
resh. Results obtained for best feature subset under
both configurations are presented in Table 2 in rows
3 and 4. SigThresh outweighed InstWeight in all our
experiments. (Combining these two techniques for
dealing with class imbalance performed worse than
using either one.) In both settings, we surpassed the
WORD-UNG baseline by a high margin. We found
MN and DA to be most useful: removing either from
the feature set dropped the F significantly in both
settings. We obtained a best F score of 65.8 using
PN, MN and DA under the SigThresh.
Following (Guyon et al, 2002), we inspected fea-
ture weights of the model created for the last fold of
our best performing feature configuration as a post-
hoc analysis. The binary feature DA:RequestAction
got the highest positive weight of 2.5. The top
ten positive weighted features included patterns
like you VB, * VB, MD PRP, VB VB and * MD,
where * denotes the utterance boundary. DA:Inform
got the most negative weight of -1.4, followed by
DA:Conventional with -1.0. The top ten negative
weighted features included patterns like MD VB,
VB you, what, VB VB me VB and WP. In both
cases, DA features got almost 2.5 times higher
weight than the highest weighted ngram pattern,
which reaffirms their importance in this task. Also,
mixed ngrams helped to capture long patterns like
?please let me know? by VB VB me VB without in-
creasing dimensionality as much as word ngrams;
they also distinguish VB you with a negative weight
of -0.51 from VB me with a positive weight of 0.32,
which pure POS ngrams couldn?t have captured.
5.4 Not Using Gold Dialog Acts
We also evaluate the performance of our ODP tagger
without using gold DA tags. We instead use the DA
tagger of Hu et al (2009), which we re-trained us-
ing the training sets for each of our cross validation
folds, applying it to the test set of that fold. We then
did cross validation for the ODP tagger using gold
dialog acts for training and automatically tagged di-
alog acts for testing. However, for our best perform-
ing feature set so far, this reduced the F score from
65.8 to 52.7. Our best result for ODP tagging with-
out using gold DAs is shown in row 5 in Table 2,
56.9 F score under SigThresh. The features used are
all of our features other than the DA tags. On fur-
ther analysis, we find that even though the dialog
act tagger has a high accuracy (85.8% in our cross
validation), it obtained a very low recall of 28.6%
and precision of 47.6% for the RequestAction dia-
log act. Since RequestAction is the most important
feature (weighted 1.7 times more than the next fea-
ture), the DA-tagger?s poor performance on Reques-
tAction hurt ODP tagging badly. The performance
reduction in this setting is probably partly due to us-
ing gold DAs in training and automatically tagged
DAs in testing; however, we feel that improving the
detection of minority classes in dialog act tagging
(RequestAction constitutes only 2.5% in the corpus)
is a necessary first step towards successfully using
automatically tagged DAs in ODP tagging.
6 Conclusion
We have introduced a new binary classification task
on utterances in dialogs, namely predicting Overt
Display of Power. An ODP adds constraints on the
possible responses by the addressee. We have in-
troduced a corpus annotated for ODP and we have
shown that using supervised machine learning with
gold dialog acts we can achieve an F-measure of
66% despite the fact that ODPs are very rare in the
corpus. We intend to develop a better dialog act tag-
ger which we can use to automatically obtain dialog
act labels for ODP classification.
7 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank several anony-
mous reviewers for their constructive feedback.
521
References
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In ACL, pages 773?
782. The Association for Computer Linguistics.
Christopher P. Diehl, Galileo Namata, and Lise Getoor.
2007. Relationship identification for social network
discovery. In AAAI, pages 546?552. AAAI Press.
Jana Diesner and Kathleen M. Carley. 2005. Exploration
of communication networks from the enron email cor-
pus. In In Proc. of Workshop on Link Analysis, Coun-
terterrorism and Security, SIAM International Confer-
ence on Data Mining 2005, pages 21?23.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Mach.
Learn., 46:389?422, March.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.
Contrasting the interaction structure of an email and a
telephone corpus: A machine learning approach to an-
notation of dialogue function units. In Proceedings of
the SIGDIAL 2009 Conference, London, UK, Septem-
ber. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Scho?lkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Mach. Learn., 68:267?276, October.
Miriam A. Locher. 2004. Power and politeness in ac-
tion: disagreements in oral communication. Lan-
guage, power, and social process. M. de Gruyter.
William M. O?Barr. 1982. Linguistic evidence: lan-
guage, power, and strategy in the courtroom. Studies
on law and social control. Academic Press.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC).
John C. Platt. 1999. Probabilistic outputs for support
vector machines and comparisons to regularized like-
lihood methods. In ADVANCES IN LARGE MARGIN
CLASSIFIERS, pages 61?74. MIT Press.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Annotations for power relations on
email threads. In Proceedings of the Eighth confer-
ence on International Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Sacks, E Schegloff, and G Jefferson. 1974. A simplest
systematics for the organization of turn-taking for con-
versation. Language, 50:696?735.
Thomas E. Wartenberg. 1990. The forms of power:
from domination to transformation. Temple Univer-
sity Press.
522
Proceedings of NAACL-HLT 2013, pages 802?807,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improving the Quality of Minority Class Identification in Dialog Act Tagging
Adinoyi Omuya
10gen
New York, NY, USA
wisdom@10gen.com
Vinodkumar Prabhakaran
CS, Columbia University
New York, NY, USA
vinod@cs.columbia.edu
Owen Rambow
CCLS, Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We present a method of improving the perfor-
mance of dialog act tagging in identifying mi-
nority classes by using per-class feature opti-
mization and a method of choosing the class
based not on confidence, but on a cascade of
classifiers. We show that it gives a minor-
ity class F-measure error reduction of 22.8%,
while also reducing the error for other classes
and the overall error by about 10%.
1 Introduction
In this paper, we discuss dialog act tagging, the
task of assigning a dialog act to an utterance, where
a dialog act (DA) is a high-level categorization of
the pragmatic meaning of the utterance. Our data is
email. Our starting point is the tagger described in
(Hu et al, 2009), which uses a standard multi-class
classifier based on support vector machines (SVMs).
While the performance of this system is pretty good
as measured by accuracy, it performs badly on the
DA REQUEST-ACTION, which is a rare class. Multi-
class SVMs are typically implemented as a set of
SVMs, one per class, with the overall choice of class
being determined by the SVM with the highest con-
fidence (?one-against-all?). Multi-class SVMs are
typically packaged as a single system, whose inner
workings are ignored by the NLP researcher. In this
paper we show that, for our problem of DA classi-
fication, we can boost the performance of the rare
classes (while maintaining the overall performance)
by performing feature optimization separately for
each individual classifier. But we also show that we
can achieve an all-around error reduction by alter-
ing the method by which the multi-class classifier
combines the individual SVMs. This new method
of combination is a simple cascade: we run the in-
dividual classifiers in ascending order of frequency
of the classes in the training corpus; the first classi-
fier to classify the data point positively determines
the choice of the overall classifier. If no classifier
classifies the data point positively, we use the usual
confidence-based method. This new method obtains
a 22.8% error reduction for the minority class, and
around 10% error reduction for the other classes and
for the overall classifier.
This paper is structured as follows. We start out
by discussing related work (Section 2). We then
present our data in Section 3, and in Section 4 we
present the experiments with our systems and the re-
sults. We report the results of an extrinsic evaluation
in Section 5, and conclude.
2 Related Work
Dialog act (DA) annotations and tagging, inspired
by the speech act theory of Austin (1975) and Searle
(1976), have been used in the NLP community to un-
derstand and model dialog. Initial work was done on
spoken interactions (see for example (Stolcke et al,
2000)). Recently, studies have explored dialog act
tagging in written interactions such as emails (Co-
hen et al, 2004), forums (Kim et al, 2006; Kim et
al., 2010b), instant messaging (Kim et al, 2010a)
and Twitter (Zhang et al, 2012). Most DA tagging
systems for written interactions use a message/post
level tagging scheme, and allow multiple tags for
each message/post. In such a tagging scheme, indi-
802
vidual binary classifiers for each tag are independent
of one another. However, recent studies have found
merit in segmenting each message into functional
units and assigning a single DA to each segment (Hu
et al, 2009). Our work falls in this paradigm (we
choose a single DA for smaller textual units). We
build on the work by (Hu et al, 2009); we improve
their dialog act predicting performance on minority
classes using per-class feature optimization.
3 Data
In this study, we use the email corpus presented in
(Hu et al, 2009), which is manually annotated for
DA tags. The corpus contains 122 email threads
with a total of 360 messages and 20,740 word to-
kens. This set of email threads is chosen from a ver-
sion of the Enron email corpus with some missing
messages restored from other emails in which they
were quoted (Yeh and Harnly, 2006; Agarwal et al,
2012). Most emails are concerned with exchanging
information, scheduling meetings, or solving prob-
lems, but there are also purely social emails.
Dialog Act Tag Count (%)
REQUEST-ACTION (R-A) 35 (2.5%)
REQUEST-INFORMATION (R-I) 151 (10.7%)
CONVENTIONAL (CONV) 357 (25.4%)
INFORM (INF) 853 (60.7%)
Total # of DFUs 1406
Table 1: Annotation statistics
Each message in the thread is segmented into Di-
alog Functional Units (DFUs). A DFU is a con-
tiguous span within an email message which has
a coherent communicative intention. Each DFU
is assigned a single DA label which is one of the
following: REQUEST-ACTION (R-A), REQUEST-
INFORMATION (R-I), CONVENTIONAL (CONV)
and INFORM (INF). There are three other DA labels
? INFORM-OFFLINE, COMMIT, and NODA for no
dialog act ? which occurred 5 or fewer times in the
corpus. We ignore these DA labels in this paper. The
corpus also contains links between the DFUs, but we
do not use those annotations in this study. Table 1
presents the distribution of DA labels in our corpus.
We now describe each of the DAs we consider in our
experiments.
In a REQUEST-ACTION, the writer signals
her desire that the reader perform some non-
communicative act, i.e., an act that cannot in itself
be part of the dialogue. For example, a writer can
ask the reader to write a report or make coffee.
In a REQUEST-INFORMATION, the writer signals
her desire that the reader perform a specific com-
municative act, namely that he provide information
(either facts or opinion).
In an INFORM, the writer conveys information, or
more precisely, the writer signals that her desire that
the reader adopt a certain belief. It covers many dif-
ferent types of information that can be conveyed in-
cluding answers to questions, beliefs (committed or
not), attitudes, and elaborations on prior DAs.
A CONVENTIONAL dialog act does not signal any
specific communicative intention on the part of the
writer, but rather it helps structure and thus facilitate
the communication. Examples include greetings, in-
troductions, expressions of gratitude, etc.
4 System
We developed four systems for our experiments: a
baseline (BAS) system which is close to the system
described in (Hu et al, 2009), and three variants of
our novel divide and conquer (DAC) system. Fea-
tures used in both systems are extracted as explained
in Section 4.2. Section 4.3 describes the baseline
system, the basic DAC system, and two variations
of the DAC system.
4.1 Experimental Framework
In all our experiments, we use linear kernel Sup-
port Vector Machines (SVM). However, across the
systems, there are differences in how we use them.
Our framework was built with the ClearTK toolkit
(Ogren et al, 2008) with its wrapper for SVMLight
(Joachims, 1999). The ClearTK wrapper internally
shifts the prediction threshold based on posterior
probabilistic scores calculated using the algorithm
of Lin et al (2007). We report results from 5-fold
cross validation performed on the entire corpus.
4.2 Feature Engineering
In developing our system, we classified our features
into three categories: lexical, verbal and message-
803
level. Lexical features consists of n-grams of words,
n-grams of POS tags, mixed n-grams of closed class
words and POS tags (Prabhakaran et al, 2012), as
well as a small set of specialized features ? Start-
POS/Lemma (POS tag and lemma of the first word),
LastPOS/Lemma (POS tag and lemma of the last
word), MDCount (number of modal verbs in the
DFU) and QuestionMark (is there a question mark
in the DFU). We used the POS tags produced by the
OpenNLP POS tagger. Verbal features capture the
position and identity of the first verb in the DFU. Fi-
nally, message-level features capture aspects of the
location of the DFU in the message and of the mes-
sage in the thread (relative position and size). In
optimizing each system, we first performed an ex-
haustive search across all combinations of features
within each category. For the lexical n-gram fea-
tures we varied the n-gram window from 1 to 5. This
step gave us the best performing feature combination
within each category. In a second step, we found the
best combination of categories, using the previously
determined features for each category. In this pa-
per, we do not report best performing feature sets
for each configuration, due to lack of space.
4.3 Experiments
Baseline (BAS) System This system uses the
ClearTK built-in one-versus-all multiclass SVM in
prediction. Internally, the multi-class SVM builds
a set of binary classifiers, one for each dialog act.
For a given test instance, the classifier that obtains
the highest probability score determines the overall
prediction. We performed feature optimization on
the whole multiclass classifier (as described in Sec-
tion 4.2), i.e., the same set of features was available
to all component classifiers. We optimized for sys-
tem accuracy. Table 2 shows results using this sys-
tem. In this and all tables, we give the performance
of the system on the four DAs, using precision, re-
call, and F-measure. The DAs are listed in ascend-
ing order of frequency in the corpus (least frequent
DA first). We also give an overall accuracy evalua-
tion. As we can see, detecting REQUEST-ACTION is
much harder than detecting the other DAs.
Basic Divide and Conquer (DAC) System Like
the BAS system, the DAC system also builds a bi-
nary classifier for each dialog act separately, and the
Prec. Rec. F-meas.
R-A 57.9 31.4 40.7
R-I 91.5 78.2 84.3
CONV 92.0 95.8 93.8
INF 91.6 95.1 93.3
Accuracy 91.3
Table 2: Results for baseline (BAS) system (standard
multiclass SVM)
component classifier with highest probability score
determines the overall prediction. The crucial dif-
ference in the DAC system is that the feature opti-
mization is performed for each component classifier
separately. Each component classifier is optimized
for F-measure. Table 3 shows results using this sys-
tem.
Prec. Recall F-meas. ER
R-A 66.7 40.0 50.0 15.6
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.4 96.1 93.7 5.7
Accuracy 91.7 4.9
Table 3: Results for basic DAC system (per-class feature
optimization followed by maximum confidence based
choice); ?ER? refers to error reduction in percent over
standard multiclass SVM (Table 2)
Minority Preference (DACMP) System This sys-
tem is exactly the same as the basic DAC system
except for one crucial difference: overall classifica-
tion is biased towards a specified minority class. If
the minority class binary classifier predicts true, this
system chooses the minority class as the predicted
class. In cases where the minority class classifier
predicts false, it backs off to the basic DAC system
after removing the minority class classifier from the
confidence tally. Table 4 shows our results using
REQUEST-ACTION as the minority class.
Cascading Minority Preference (DACCMP) System
This system is similar to the Minority Preference
System; however, instead of a single supplied mi-
nority class, the system accepts an ordered list of
classes. The classifier then works, in order, through
this list; whenever any classifier in the list predicts
804
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.5 78.2 84.3 0.0
CONV 93.9 94.1 94.0 2.6
INF 91.6 96.0 93.8 6.5
Accuracy 91.8 5.7
Table 4: Results for minority-preference DAC system ?
DACMP (first consult REQUEST-ACTION tagger, then de-
fault to choice by maximum confidence); ?ER? refers to
error reduction in percent over standard multiclass SVM
(Table 2)
true, for a given instance, it then assigns this class
as the predicted class. The subsequent classifiers in
the list are not run. If all classifiers predict false, we
back off to the basic DAC system, i.e., the compo-
nent classifier with highest probability score deter-
mines the overall prediction. We ordered the list of
classes in the ascending order of their frequencies in
the training data. This ordering is driven by the ob-
servation that the less frequent classes are also hard
to predict correctly. Table 5 shows our results using
the ordered list: (REQUEST-ACTION, REQUEST-
INFORMATION, CONVENTIONAL, INFORM).
Prec. Recall F-meas. ER
R-A 66.7 45.7 54.2 22.8
R-I 91.0 80.8 85.6 8.4
CONV 93.7 95.3 94.5 10.1
INF 92.4 95.8 94.0 10.0
Accuracy 92.2 10.6
Table 5: Results for cascading minority-preference DAC
system ? DACCMP (consult classifiers in reverse order
of frequency of class); ?ER? refers to error reduction in
percent over standard multiclass SVM (Table 2)
4.4 Discussion
As shown in Table 3, the basic DAC system obtained
a 15.6% F-measure error reduction for the minor-
ity class REQUEST-ACTION over the BAS system.
It also improves performance of two other classes
? CONVENTIONAL and INFORM, and obtaines a
4.9% error reduction on overall accuracy. Recall
here that the only difference between the DAC sys-
tem and the BAS system is the per-class feature op-
timization and therefore this must be the reason for
this boost in performance. When we turn to DACMP,
we see that the performance on the minority class
REQUEST-ACTION is further enhanced, with an F-
measure error reduction of 22.8%; the overall ac-
curacy improves slightly with an error reduction of
5.7%. Finally, DACCMP further improves the perfor-
mance. Since the method of choosing the minor-
ity class REQUEST-ACTION does not change over
DACMP, the F-measure error reduction remains the
same. However, now all three other classes also im-
prove their performance, and we obtain a 10.6% er-
ror reduction on overall accuracy over the baseline
system.
Following (Guyon et al, 2002), we performed a
post-hoc analysis by inspecting the feature weights
of the best performing models created for each in-
dividual classifier in the DAC system. Table 6 lists
some interesting features chosen during feature opti-
mization for the individual SVMs. We selected them
from the top 25 features in terms of absolute value
of feature weights.
Some features help distinguish different DA cat-
egories. For example, the feature QuestionMark
is the feature with the highest negative weight for
INFORM, but has the highest positive weight for
REQUEST-INFORMATION. Features like fyi and pe-
riod (.) have high positive weights for INFORM
and high negative weights for CONVENTIONAL.
Some other features are important only for certain
classes. For e.g., please and VB NN are important
for REQUEST-ACTION, but not so for other classes.
Overall, the most discriminating features for both
INFORM and CONVENTIONAL are mostly word
ngrams, while those for REQUEST-ACTION and
REQUEST-INFORMATION are mostly POS ngrams.
This shows why our approach of per-class feature
optimization is important to boost the classification
performance.
Another interesting observation is that the least
frequent category, REQUEST-ACTION, has the least
strong indicators (as measured by feature weights).
Presumably this is because there is much less train-
ing data for this class. This explains why our cascad-
ing classifiers approach giving priority to the least
frequent categories worked better than a simple con-
fidence based approach, since the simple approach
drowns out the less confident classifiers.
805
REQUEST-ACTION REQUEST-INFORMATION CONVENTIONAL INFORM
please (0.9) QuestionMark (6.6) StartPOS NNP (2.7) QuestionMark (-3.0)
VB NN (0.7) BOS PRP (-1.2) thanks (2.3) thanks (-2.2)
you VB (0.3) WRB (1.0) . (-2.0) . (2.2)
PRP (-0.3) PRP VBP (-0.9) fyi (-2.0) fyi (1.9)
MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0)
will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9)
Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with corresponding
feature weights in parentheses, for each individual tagger. (POS tags are capitalized; BOS stands for Beginning Of
Sentence)
5 Extrinsic Evaluation
In this section, we perform an extrinsic evaluation
for the dialog act tagger presented in Section 4 by
applying it to the task of identifying Overt Displays
of Power (ODP) in emails, proposed by Prabhakaran
et al (2012). The task is to identify utterances where
the linguistic form introduces additional constraints
on its responses, beyond those introduced by the
general dialog act. The dialog act features were
found to be useful and the best performing system
obtained an F-measure of 65.8 using gold dialog
act tags. For our extrinsic evaluation, we retrained
the ODP tagger using dialog act tags predicted by
our BAS and DACCMP systems instead of gold dia-
log acts. ODP tagger uses the same dataset as ours
for training. In the cross validation step, we made
sure that the test folds for ODP were excluded from
training the taggers to obtain DA tags. At each ODP
cross validation step, we trained a BAS or DACCMP
tagger using ODP?s training folds for that step and
used tags produced by that tagger for both training
and testing the ODP tagger for that step. Table 7 lists
the results obtained.
Prec. Rec. F-meas.
No-DA 55.7 45.4 50.0
Gold-DA 75.8 58.1 65.8
BAS-DA 60.6 46.5 52.6
DACCMP-DA 67.2 45.4 54.2
Table 7: Results for ODP system using various sources
of DA tags
Using BAS tagged DA, the F-measure of ODP
system reduced by 13.2 points to 52.6 from using
gold dialog acts (F=65.8). Using DACCMP, the F-
measure improved over BAS by 1.6 points to 54.2.
This constitutes an error reduction of 12.1%, tak-
ing the system using gold DA tags as the reference.
This improvement is noteworthy, given the fact that
the overall error reduction obtained by DACCMP over
BAS in the DA tagging was around 10.6%. Also, the
DACCMP-based ODP system obtained an error reduc-
tion of about 26.6% over a system that does not use
the DA features at all (F=50.0).
6 Conclusion
We presented a method of improving the perfor-
mance of dialog act tagging in identifying minority
classes by using per-class feature optimization and
choosing the class based on a cascade of classifiers.
We showed that it gives a minority class F-measure
error reduction of 22.8% while also reducing the er-
ror on other classes and the overall error by around
10%. We also presented an extrinsic evaluation of
this technique on detecting Overt Displays of Power
in dialog, where we achieve an error reduction of
12.1% over using the standard multiclass SVM to
generate dialog act tags.
Acknowledgements
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of the sponsor. While working on this project,
the first author Adinoyi Omuya was affiliated with
the Center for Computational Learning Systems at
Columbia University. We thank several anonymous
reviewers for their constructive feedback.
806
References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A Comprehensive Gold Stan-
dard for the Enron Organizational Hierarchy. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 161?165, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
J. L. Austin. 1975. How to Do Things with Words. Har-
vard University Press, Cambridge, Mass.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to Classify Email into
?Speech Acts? . In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 309?316,
Barcelona, Spain, July. Association for Computational
Linguistics.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene Selection for Cancer
Classification using Support Vector Machines. Mach.
Learn., 46:389?422, March.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.
Contrasting the Interaction Structure of an Email and
a Telephone Corpus: A Machine Learning Approach
to Annotation of Dialogue Function Units. In Pro-
ceedings of the SIGDIAL 2009 Conference, London,
UK, September. Association for Computational Lin-
guistics.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Scho?lkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
J. Kim, G. Chern, D. Feng, E. Shaw, and E. Hovy.
2006. Mining and Assessing Discussions on the Web
Through Speech Act Analysis. In Proceedings of the
Workshop on Web Content Mining with Human Lan-
guage Technologies at the 5th International Semantic
Web Conference.
S.N. Kim, L. Cavedon, and T. Baldwin. 2010a. Classify-
ing Dialogue Acts in One-on-one Live Chats. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 862?871.
Association for Computational Linguistics.
S.N. Kim, L. Wang, and T. Baldwin. 2010b. Tagging
and Linking Web Forum Posts. In Proceedings of
the Fourteenth Conference on Computational Natural
Language Learning, pages 192?202. Association for
Computational Linguistics.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A Note on Platt?s Probabilistic Outputs for Support
Vector Machines. Mach. Learn., 68:267?276, Octo-
ber.
Philip V. Ogren, Philipp G. Wetzler, and Steven Bethard.
2008. ClearTK: A UIMA toolkit for statistical natural
language processing. In Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP work-
shop at Language Resources and Evaluation Confer-
ence (LREC).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technologies:
The 2012 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Canada, June. Association for Compu-
tational Linguistics.
J.R. Searle. 1976. A Classification of Illocutionary Acts.
Language in society, 5(01):1?23.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C.V. Ess-Dykema,
and M. Meteer. 2000. Dialogue Act Modeling for
Automatic Tagging and Recognition of Conversational
Speech. Computational linguistics, 26(3):339?373.
J.Y. Yeh and A. Harnly. 2006. Email Thread Reassembly
Using Similarity Matching. In Third Conference on
Email and Anti-Spam (CEAS), pages 27?28.
R. Zhang, D. Gao, and W. Li. 2012. Towards Scalable
Speech Act Recognition in Twitter: Tackling Insuffi-
cient Training Data. EACL 2012, page 18.
807
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 339?344,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Predicting Power Relations between Participants
in Written Dialog from a Single Thread
Vinodkumar Prabhakaran
Dept. of Computer Science
Columbia University, New York, NY
vinod@cs.columbia.edu
Owen Rambow
Cntr. for Comp. Learning Systems
Columbia University, New York, NY
rambow@ccls.columbia.edu
Abstract
We introduce the problem of predicting
who has power over whom in pairs of peo-
ple based on a single written dialog. We
propose a new set of structural features.
We build a supervised learning system to
predict the direction of power; our new
features significantly improve the results
over using previously proposed features.
1 Introduction
Computationally analyzing the social context in
which language is used has gathered great interest
within the NLP community recently. One of the
areas that has generated substantial research is the
study of how social power relations between peo-
ple affect and/or are revealed in their interactions
with one another. Researchers have proposed sys-
tems to detect social power relations between par-
ticipants of organizational email threads (Bramsen
et al, 2011; Gilbert, 2012; Prabhakaran and Ram-
bow, 2013), online forums (Danescu-Niculescu-
Mizil et al, 2012; Biran et al, 2012; Danescu-
Niculescu-Mizil et al, 2013), chats (Strzalkowski
et al, 2012), and off-line interactions such as pres-
idential debates (Prabhakaran et al, 2013; Nguyen
et al, 2013). Automatically identifying power and
influence from interactions can have many prac-
tical applications ranging from law enforcement
and intelligence to online marketing.
A significant number of these studies are per-
formed in the domain of organizational email
where there is a well defined notion of power (or-
ganizational hierarchy). Bramsen et al (2011) and
Gilbert (2012) predict hierarchical power relations
between people in the Enron email corpus using
lexical features extracted from all the messages
exchanged between them. However, their ap-
proaches primarily apply to situations where large
collections of messages exchanged between pairs
of people are available. In (Prabhakaran and Ram-
bow, 2013), we introduced the problem of detect-
ing whether a participant of an email thread has
power over someone else in the thread and estab-
lished the importance of dialog structure in that
task. However, in that work we did not detect over
whom that person has power.
In this paper, we introduce a new problem for-
mulation. We predict the hierarchical power rela-
tion between pairs of participants in an email in-
teraction thread based solely on features extracted
from that thread. As a second major contribution,
we introduce a new set of features to capture as-
pects of participant behavior such as responsive-
ness, and we show that these features are signifi-
cantly correlated with the direction of power. We
present a fully automatic system for this task ob-
taining an accuracy of 73.0%, an improvement of
6.9% over 68.3% by a system using only lexical
features. This best-performing system uses our
new feature set.
2 Motivation
Early NLP-based approaches such as Bramsen et
al. (2011) and Gilbert (2012) built systems to pre-
dict hierarchical power relations between people
in the Enron email corpus using lexical features
from all the messages exchanged between them.
One limitation of this approach is that it relies
solely on lexical cues and hence works best when
large collections of messages exchanged between
the pairs of people are available. For example,
Bramsen et al (2011) excluded sender-recipient
pairs who exchanged fewer than 500 words from
their evaluation set, since they found smaller text
samples are harder to classify. By taking the mes-
sage out of the context of the interaction in which
it was exchanged, they fail to utilize cues from the
structure of interactions, which complements the
lexical cues in detecting power relations, as we
showed in (Prabhakaran and Rambow, 2013).
339
We modeled the problem of detecting power re-
lationships differently in (Prabhakaran and Ram-
bow, 2013): we predicted whether a participant
in an email thread has a certain type of power
or not. However, in that work we did not pre-
dict over whom he/she has that power. This
may result in noisy features; consider a thread in
which participant X has power over participant
Y , who has power over participant Z . By ag-
gregating features over all messages sent by Y ,
features salient to a subordinate-superior interac-
tion are incorrectly conflated with those salient to
superior-subordinate interaction. Another limita-
tion of (Prabhakaran and Rambow, 2013) is that
we used manual annotations for many of our fea-
tures such as dialog acts and overt displays of
power. Relying on manual annotations for features
limited our analysis to a small subset of the Enron
corpus, which has only 18 instances of hierarchi-
cal power. Consequently, our findings with respect
to hierarchical power were weak in terms of both
correlations of features and system performance.
In this paper, we introduce the problem of pre-
dicting who has power over whom in pairs of inter-
acting participants based on a single thread of in-
teractions. From (Bramsen et al, 2011) we retain
the idea that we want to predict the power relation
between pairs of people. But in contrast to their
formulation, we retain the goal from (Prabhakaran
and Rambow, 2013) that we want to study com-
munication in the context of an interaction, and
that we want to be able to make predictions us-
ing only the emails exchanged in a single thread.
Like (Prabhakaran and Rambow, 2013), we use
features to capture the dialog structure, but we use
automatic taggers to generate them and assume no
manual annotation at all at training or test time.
This allows us to use the entire Enron email cor-
pus for this study.
3 Data
In this work, we use the version of Enron email
corpus by Yeh and Harnly (2006) which captures
the thread structure of email exchanges. The cor-
pus contains 36,615 email threads. We excluded a
small subset of 419 threads that was used for pre-
vious manual annotation efforts, part of which was
also used to train the DA and ODP taggers (Sec-
tion 5) that generate features for our system. The
average number of email messages per thread was
around 3. We divided the remaining threads into
train (50%), dev (25%) and test (25%) sets by ran-
dom sampling. We then applied various basic NLP
preprocessing steps such as tokenization, POS tag-
ging and lemmatization to the body of email mes-
sages. We use the Enron gold organizational hier-
archy released by Agarwal et al (2012) to model
hierarchical power. Their corpus was manually
built using information from Enron organizational
charts. It includes relations of 1,518 employees
and captures dominance relations between 13,724
pairs of them. Theirs is the largest such data set
available to the best of our knowledge.
4 Problem Formulation
Let t denote an email thread and M
t
denote the
set of all messages in t . Also, let P
t
be the set
of all participants in t , i.e., the union of senders
and recipients (To and CC) of all messages in
M
t
. We are interested in detecting power rela-
tions between pairs of participants who interact
within a given email thread. Not every pair of par-
ticipants (p
1
, p
2
) ? P
t
? P
t
interact with one an-
other within t . Let IM
t
(p
1
, p
2
) denote the set of
Interaction Messages ? non-empty messages in
t in which either p
1
is the sender and p
2
is one
of the recipients or vice versa. We call the set of
(p
1
, p
2
) such that |IM
t
(p
1
, p
2
)| > 0 the interact-
ing participant pairs of t (IPP
t
).
We focus on the manifestations of power in in-
teractions between people across different levels
of hierarchy. For every (p
1
, p
2
) ? IPP
t
, we query
the set of dominance relations in the gold hierar-
chy to determine their hierarchical power relation
(HP(p
1
, p
2
)). We exclude pairs that do not exist
in the gold hierarchy from our analysis and denote
the remaining set of related interacting participant
pairs as RIPP
t
. We assign HP(p
1
, p
2
) to be su-
perior if p
1
dominates p
2
, and subordinate if p
2
dominates p
1
. Table 1 shows the total number of
pairs in IPP
t
and RIPP
t
from all the threads in
our corpus and across train, dev and test sets.
Description Total Train Dev Test
# of threads 36,196 18,079 8,973 9,144
?
t
|IPP
t
| 355,797 174,892 91,898 89,007
?
t
|RIPP
t
| 15,048 7,510 3,578 3,960
Table 1: Data Statistics
Row 1 presents the total number of threads in different
subsets of the corpus. Row 2 and 3 present the number of
interacting participant pairs (IPP ) and related interacting
participant pairs (RIPP ) in those subsets.
340
Given a thread t and a pair of participants
(p
1
, p
2
) ? RIPP
t
, we want to automatically de-
tect HP(p
1
, p
2
). This problem formulation is
similar to the ones in (Bramsen et al, 2011) and
(Gilbert, 2012). However, the difference is that for
us an instance is a pair of participants in a single
thread of interaction (which may or may not in-
clude other people), whereas for them an instance
constitutes all messages exchanged between a pair
of people in the entire corpus. Our formula-
tion also differs from (Prabhakaran and Rambow,
2013) in that we detect power relations between
pairs of participants, instead of just whether a par-
ticipant had power over anyone in the thread.
5 Structural Analysis
In this section we analyze various features that
capture the structure of interaction between the
pairs of participants in a thread. Each feature f
is extracted with respect to a person p over a ref-
erence set of messages M (denoted f
p
M
). For a
pair (p
1
, p
2
), we extract 4 versions of each fea-
ture f : f
p
1
IM
t
(p
1
,p
2
)
, f
p
2
IM
t
(p
1
,p
2
)
, f
p
1
M
t
and f
p
2
M
t
. The
first two capture behavior of the pair among them-
selves, while the third and fourth capture their
overall behavior in the entire thread. We group our
features into three categories ? THR
New
, THR
PR
and DIA
PR
. THR
New
is a set of new features we
propose, while THR
PR
and DIA
PR
incorporate fea-
tures we proposed in (Prabhakaran and Rambow,
2013). THR
New
and THR
PR
capture the structure
of message exchanges without looking at the con-
tent of the emails (e.g., how many emails did a per-
son send), while DIA
PR
captures the pragmatics of
the dialog and requires an analysis of the content
of the emails (e.g., did they issue any requests).
THR
New
: This is a new set of features we in-
troduce in this paper. It includes the average num-
ber of recipients (AvgRecipients) and To recipients
(AvgToRecipients) in emails sent by p, the per-
centage of emails p received in which he/she was
in the To list (InToList%), boolean features de-
noting whether p added or removed people when
responding to a message (AddPerson and Re-
movePerson), average number of replies received
per message sent by p (ReplyRate) and average
number of replies received from the other person
of the pair to messages where he/she was a To re-
cipient (ReplyRateWithinPair). ReplyRateWithin-
Pair applies only to IM
t
(p
1
, p
2
).
THR
PR
: This feature set includes two meta-
data based feature sets ? positional and verbosity.
Positional features include a boolean feature to de-
note whether p sent the first message (Initiate),
and relative positions of p?s first and last messages
(FirstMsgPos and LastMsgPos) in M . Verbosity
features include p?s message count (MsgCount),
message ratio (MsgRatio), token count (Token-
Count), token ratio (TokenRato) and tokens per
message (TokenPerMsg), all calculated over M .
DIA
PR
: In (Prabhakaran and Rambow, 2013),
we used dialog features derived from manual an-
notations ? dialog acts (DA) and overt displays
of power (ODP) ? to model the structure of inter-
actions within the message content. In this work,
we obtain DA and ODP tags on the entire cor-
pus using automatic taggers trained on those man-
ual annotations. The DA tagger (Omuya et al,
2013) obtained an accuracy of 92%. The ODP
tagger (Prabhakaran et al, 2012) obtained an ac-
curacy of 96% and F-measure of 54%. The DA
tagger labels each sentence to be one of the 4
dialog acts: Request Action, Request Informa-
tion, Inform, and Conventional. The ODP Tag-
ger identifies sentences (mostly requests) that ex-
press additional constraints on its response, be-
yond those introduced by the dialog act. We use
5 features: ReqAction%, ReqInform%, Inform%,
Conventional%, and ODP% to capture the per-
centage of sentences in messages sent by p that has
each of these labels. We also use a feature to cap-
ture the number of p?s messages with a request that
did not get a reply, i.e., dangling requests (Dan-
glingReq%), over all messages sent by p.
We perform an unpaired two-sample two-tailed
Student?s t-Test comparing mean values of each
feature for subordinates vs. superiors. For our
analysis, a data point is a related interacting pair,
and not a message. Hence, a message with mul-
tiple recipients who have a superior/subordinate
relation with the sender will contribute to features
for multiple data points. We limit our analysis to
the related interacting pairs from only our train
set. Table 2 presents mean values of features for
subordinates and superiors at the interaction level.
Thread level versions of these features also ob-
tained similar results overall in terms of direction
of difference and significance. We denote three
significance levels ? * (p < .05 ), ** (p < .01 ),
and *** (p < .001 ). To control false discovery
rates in multiple testing, we adjusted the p-values
(Benjamini and Hochberg, 1995). We summarize
341
Feature Name Mean(f
sub
IM
t
) Mean(f
sup
IM
t
)
THR
New
AvgRecipients
???
21.14 43.10
AvgToRecipients
???
18.19 38.94
InToList% 0.82 0.80
ReplyRate
???
0.86 1.23
ReplyRateWithinPair
???
0.16 0.10
AddPerson 0.48 0.47
RemovePerson
???
0.41 0.37
THR
PR
Initiate
???
0.45 0.56
FirstMsgPos 0.04 0.03
LastMsgPos
???
0.15 0.11
MsgCount
???
0.64 0.70
MsgRatio
???
0.44 0.56
TokenCount 91.22 83.26
TokenRatio
???
0.45 0.55
TokenPerMsg
?
140.60 120.87
DIA
PR
Conventional%
???
0.15 0.17
Inform%
???
0.78 0.72
ReqAction%
???
0.02 0.04
ReqInform%
???
0.05 0.06
DanglingReq%
???
0.12 0.15
ODP%
???
0.03 0.06
Table 2: Student?s t-Test Results of f
p
IM
t
.
THR
New
: new meta-data features; THR
PR
, DIA
PR
: meta-data
and dialog-act features from previous studies;
* (p < .05 ); ** (p < .01 ); *** (p < .001 )
the main findings on the significant features below.
1. Superiors send messages addressed to more
people (AvgRecipients and AvgToRecipi-
ents). Consequently, they get more replies to
their messages (ReplyRate). However, con-
sidering messages where the other person of
the pair is addressed in the To list (ReplyRate-
WithinPair), subordinates get more replies.
2. Superiors issue more requests (ReqAction%
and ReqInform%) and overt displays of
power (ODP%). Subordinates issue more
informs (Inform%) and, surprisingly, have
fewer unanswered requests (DanglingReq%).
3. Superiors initiate the interactions more often
than subordinates (Initiate). They also leave
interactions earlier (LastMsgPos).
4. Superiors send shorter messages (Token-
PerMsg). They also send more messages
(MsgCount & MsgRatio) and even contribute
a higher ratio of tokens in the thread (Token-
Ratio) despite sending shorter messages.
Finding 1 goes in line with findings from stud-
ies analyzing social networks that superiors have
higher connectivity in the networks that they are
part of (Rowe et al, 2007). Intuitively, those who
have higher connectivity also send emails to larger
number of people, and hence our result. Since su-
periors address more people in their emails, they
also have a higher chance of getting replies. Find-
ing 2 also aligns with the general intuition about
how superiors and subordinates behave within in-
teractions (e.g., superiors exhibit more overt dis-
plays of power than subordinates).
Findings 3 & 4 are interesting since they re-
veal special characteristics of threads involving hi-
erarchically related participants. In (Prabhakaran
and Rambow, 2013), we had found that persons
with hierarchical power rarely initiated threads
and contributed less within the threads. But that
problem formulation was different ? we were
identifying whether a person in a given thread had
hierarchical power over someone else or not. The
data points in that formulation included partici-
pants from threads that did not have any hierar-
chically related people, whereas our current for-
mulation do not. These findings suggest that if a
person starts an email thread, he?s likely not to be
the one who has power, but if a thread includes a
pair of people who are hierarchically related, then
it is likely to be initiated by the superior and he/she
tends to contribute more in such threads.
6 Predicting Direction of Power
We build an SVM-based supervised learning sys-
tem that can predict HP(p
1
, p
2
) to be either su-
perior or subordinate based on the interaction
within a thread t for any pair of participants
(p
1
, p
2
) ? RIPP
t
. We deterministically fix the
order of participants in (p
1
, p
2
) such that p
1
is the
sender of the first message in IM
t
(p
1
, p
2
). We
use the ClearTK (Ogren et al, 2008) wrapper for
SVMLight (Joachims, 1999) in our experiments.
We use the related interacting participant pairs in
threads from the train set to train our models and
optimize our performance on those from the dev
set. We report results obtained on dev and test sets.
In our formulation, values of many features are
undefined for some instances (e.g., Inform% is un-
defined when MsgCount = 0). Handling of unde-
fined values for features in SVM is not straight-
forward. Most SVM implementations assume the
value of 0 by default in such cases, conflating them
342
Description Accuracy
Baseline (Always Superior) 52.54
Baseline (Word Unigrams + Bigrams) 68.56
THR
New
55.90
THR
PR
54.30
DIA
PR
54.05
THR
PR
+ THR
New
61.49
DIA
PR
+ THR
PR
+ THR
New
62.47
LEX 70.74
LEX + DIA
PR
+ THR
PR
67.44
LEX + DIA
PR
+ THR
PR
+ THR
New
68.56
BEST (= LEX + THR
New
) 73.03
BEST (Using p
1
features only) 72.08
BEST (Using IM
t
features only) 72.11
BEST (Using M
t
only) 71.27
BEST (No Indicator Variables) 72.44
Table 3: Accuracies on feature subsets (dev set).
THR
New
: new meta-data features; THR
PR
, DIA
PR
: meta-data
and dialog-act features from previous studies; LEX: ngrams;
BEST: best subset; IM
t
stands for IM
t
(p
1
,p
2
)
with cases where Inform% is truly 0. In order to
mitigate this issue, we use an indicator feature for
each structural feature to denote whether or not it
is valid. Since we use a quadratic kernel, we ex-
pect the SVM to pick up the interaction between
each feature and its indicator feature.
Lexical features have already been shown to be
valuable in predicting power relations (Bramsen
et al, 2011; Gilbert, 2012). We use another fea-
ture set LEX to capture word ngrams, POS (part
of speech) ngrams and mixed ngrams. A mixed
ngram (Prabhakaran et al, 2012) is a special case
of word ngram where words belonging to open
classes are replaced with their POS tags. We found
the best setting to be using both unigrams and bi-
grams for all three types of ngrams, by tuning in
our dev set. We then performed experiments using
all subsets of {LEX, THR
New
, THR
PR
, DIA
PR
}.
Table 3 presents the results obtained using var-
ious feature subsets. We use a majority class
baseline assigning HP(p
1
, p
2
) to be always su-
perior, which obtains 52.5% accuracy. We also
use a stronger baseline using word unigrams and
bigrams as features, which obtained an accuracy
of 68.6%. The performance of the system using
each structural feature class on its own is very
low. Combining all three of them improves the
accuracy to 62.5%. The highest performance ob-
tained without using any message content is for
THR
PR
and THR
New
(61.5%). LEX features by
itself obtain a very high accuracy of 70.7%, con-
firming the importance of lexical patterns in this
task. Perplexingly, adding all structural features to
LEX reduces the accuracy by around 2.2 percent-
age points. The best performing system (BEST)
uses LEX and THR
New
features and obtains an
accuracy of 73.0%, a statistically significant im-
provement over the LEX-only system (McNemar).
We also performed an ablation study to under-
stand the importance of different slices of our fea-
ture sets. If we remove all feature versions with
respect to the second person, the accuracy drops
to 72.1%. This suggests that features about the
other person?s behavior also help the prediction
task. If we remove either the thread level versions
of features or interaction level versions of features,
the accuracy again drops, suggesting that both the
pair?s behavior among themselves, and their over-
all behavior in the thread add value to the predic-
tion task. Removing the indicator feature denot-
ing the structural features? validity also reduces
the performance of the system.
We now discuss evaluation on our blind test set.
The majority baseline (Always Superior) for ac-
curacy is 55.0%. The word unigrams and bigrams
baseline obtains an accuracy of 68.3%. The LEX
system (using other forms of ngrams as well) ob-
tains a slightly lower accuracy of 68.1%. Our
BEST system using LEX and THR
New
features
obtains an accuracy of 73.0% (coincidentally the
same as on the dev set), an improvement of 6.9%
over the system using only lexical features.
7 Conclusion
We introduced the problem of predicting who has
power over whom based on a single thread of writ-
ten interactions. We introduced a new set of fea-
tures which describe the structure of the dialog.
Using this feature set, we obtain an accuracy of
73.0% on a blind test. In future work, we will
tackle the problem of three-way classification of
pairs of participants, which will cover cases in
which they are not in a power relation at all.
Acknowledgments
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We also thank several
anonymous reviewers for their feedback.
343
References
Apoorv Agarwal, Adinoyi Omuya, Aaron Harnly, and
Owen Rambow. 2012. A Comprehensive Gold
Standard for the Enron Organizational Hierarchy. In
Proceedings of the 50th Annual Meeting of the ACL
(Short Papers), pages 161?165, Jeju Island, Korea,
July. Association for Computational Linguistics.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: a practical and pow-
erful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37?45, Montr?eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773?782, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
?12, New York, NY, USA. ACM.
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec, and Christopher Potts.
2013. A computational approach to politeness with
application to social factors. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
250?259, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
?12, pages 1037?1046, New York, NY, USA. ACM.
Thorsten Joachims. 1999. Making Large-Scale SVM
Learning Practical. In Bernhard Sch?olkopf, Christo-
pher J.C. Burges, and A. Smola, editors, Advances
in Kernel Methods - Support Vector Learning, Cam-
bridge, MA, USA. MIT Press.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Philip V. Ogren, Philipp G. Wetzler, and Steven
Bethard. 2008. ClearTK: A UIMA toolkit for sta-
tistical natural language processing. In Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP workshop at Language Resources
and Evaluation Conference (LREC).
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minor-
ity class identification in dialog act tagging. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
802?807, Atlanta, Georgia, June. Association for
Computational Linguistics.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216?224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012. Predicting Overt Display of Power in
Written Dialogs. In Human Language Technolo-
gies: The 2012 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Montreal, Canada, June. Associ-
ation for Computational Linguistics.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365?373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Ryan Rowe, German Creamer, Shlomo Hershkop, and
Salvatore J. Stolfo. 2007. Automated social hier-
archy detection through email network analysis. In
Proceedings of the 9th WebKDD and 1st SNA-KDD
2007 workshop on Web Mining and Social Network
Anal. ACM.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535?2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In CEAS
2006 - The Third Conference on Email and Anti-
Spam, July 27-28, 2006, Mountain View, California,
USA, Mountain View, California, USA, July.
344
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 132?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Uncertainty Learning Using SVMs and CRFs
Vinodkumar Prabhakaran
Computer Science Department
Columbia University, New York
vp2198@columbia.edu
Abstract
In this work, we explore the use of SVMs
and CRFs in the problem of predicting cer-
tainty in sentences. We consider this as a
task of tagging uncertainty cues in context,
for which we used lexical, wordlist-based
and deep-syntactic features. Results show
that the syntactic context of the tokens in
conjunction with the wordlist-based fea-
tures turned out to be useful in predicting
uncertainty cues.
1 Introduction
Extracting factual information from text is a crit-
ical NLP task which has important applications
in Information Extraction, Textual Entailment etc.
It is found that linguistic devices such as hedge
phrases help to distinguish facts from uncertain
information. Hedge phrases usually indicate that
authors do not or cannot back up their opin-
ions/statements with facts. As part of the CoNLL
shared task 2010 (Farkas et al, 2010), we explored
the applicability of different machine learning ap-
proaches and feature sets to learn to detect sen-
tences containing uncertainty.
In Section 2, we present the task formally and
describe the data used. Section 3 presents the
system description and explains the features used
in the task in detail. We investigated two differ-
ent machine learning frameworks in this task and
did experiments on various feature configurations.
Section 4 presents those experiments and analyzes
the results. Section 5 describes the system used
for the shared task final submission and presents
the results obtained in the evaluation. Section 6
concludes the paper and discusses a few future di-
rections to extend this work.
2 Task Description and Data
We attempt only the Task 1 of the CoNLL shared
task which was to identify sentences in texts which
contain unreliable or uncertain information. In
particular, the task is a binary classification prob-
lem, i.e. to distinguish factual versus uncertain
sentences.
As training data, we use only the corpus of
Wikipedia paragraphs with weasel cues manually
annotated (Ganter and Strube, 2009). The annota-
tion of weasel/hedge cues was carried out on the
phrase level, and sentences containing at least one
cue are considered as uncertain, while sentences
with no cues are considered as factual. The corpus
contained 11, 110 sentences out of which 2, 484
were tagged as uncertain. A sentence could have
more than one cue phrases. There were 3143 cue
phrases altogether.
3 System Description
3.1 Approach
We considered this task as a cue tagging task
where in phrases suggesting uncertainty will be
tagged in context. This is a 3-way classification
problem at token level - B-cue, I-cue and O denot-
ing beginning, inside and outside of a cue phrase.
We applied a supervised learning framework for
this task, for which We experimented with both
SVMs and CRFs. For SVM, we used the Yam-
cha1 system which is built on top of the tinySVM2
package. Yamcha has been shown useful in simi-
lar tasks before. It was the best performing system
in the CoNLL-2000 Shared task on chunking. In
this task, Yamcha obtained the best performance
for a quadratic kernel with a c value of 0.5. All
results presented here use this setting. For CRF,
we used the Mallet3 software package. Experi-
ments are done only with order-0 CRFs. CRFs
proved to marginally improve the prediction accu-
racy while substantially improving the speed. For
e.g, for a configuration of 10 features with context
width of 2, Yamcha took around 5-6 hrs for 9-fold
1http://chasen.org/ taku/software/YamCha/
2http://chasen.org/ taku/software/TinySVM/
3http://mallet.cs.umass.edu/
132
cross validation on the whole training set, where
as Mallet took only around 30-40 minutes only.
3.2 Features
Our approach was to explore the use of deep syn-
tactic features in this tagging task. Deep syntac-
tic features had been proven useful in many simi-
lar tagging tasks before. We used the dependency
parser MICA (Bangalore et al, 2009) based on
Tree Adjoining Grammar (Joshi et al, 1975) to ex-
tract these deep syntactic features.
We classified the features into three classes -
Lexical (L), Syntactic (S) and Wordlist-based (W).
Lexical features are those which could be found at
the token level without using any wordlists or dic-
tionaries and can be extracted without any parsing
with relatively high accuracy. For example, isNu-
meric, which denotes whether the word is a num-
ber or alphabetic, is a lexical feature. Under this
definition, POS tag will be considered as a lexical
feature.
Syntactic features of a token access its syntactic
context in the dependency tree. For example, par-
entPOS, the POS tag of the parent word in the
dependency parse tree, is a syntactic feature. The
tree below shows the dependency parse tree output
by MICA for the sentence Republican leader Bill
Frist said the Senate was hijacked.
said
Frist
Republican leader Bill
hijacked
Senate
the
was
In this case, the feature haveReportingAnces-
tor of the word hijacked is ?Y? because it is a verb
with a parent verb said. Similarly, the feature
haveDaughterAux would also be ?Y? because of
daughter was, whereas whichAuxIsMyDaughter
would get the value was.
Wordlist-based features utilized a list of words
which occurred frequently as a cue word in the
training corpus. We used two such lists ? one
which included adjectives like many, most, some
etc. The other list contained adverbs like proba-
bly, possibly etc. The complete list of words in
these wordlists are given in Table 1.
For finding the best performing feature set -
context width configuration, we did an exhaustive
search on the feature space, pruning away features
which were proven not useful by results at stages.
The list of features we used in our experiments
are summarized in Table 1 and Table 2. Ta-
ble 1 contains features which were useful and
are present in the results presented in section 4.
Out of the syntactic features, parentPOS and is-
MyNNSparentGeneric turned out to be the most
useful. It was noticed that in most cases in which
a generic adjective (i.e., a quantifier such as many,
several, ...) has a parent which is a plural noun,
and this noun has only adjectival daughters, then
it is part of a cue phrase. This distinction can be
made clear by the below example.
? ?ccue? Many people ?/ccue? enjoy having
professionally made ?family portraits?
? Many departments, especially those in which
students have research or teaching responsi-
bilities ...
In the first case, the noun people comes with the
adjective Many, but is not qualified further. This
makes it insufficiently defined and hence is tagged
as a cue phrase. However in the second case, the
clause which starts with especially is qualifying
the noun departments further and hence the phrase
is not tagged as a cue word despite the presence
of Many. This scenario occurred often with other
adjectives like most, some etc. This distinction
was caught to a good extent by the combination
of isMyNNSparentGeneric and isGenericAdj.
Hence, the best performing configuration used fea-
tures from both W and S categories.
The features which were found to be not useful
is listed in Table 2. We used only two wordlist
features, both of which were useful.
4 Experiments
To find the best configuration, we used 10% of the
training data as the development set to tune param-
eters. Since even the development set was fairly
large, we used 9-fold cross validation to evaluate
each models. The development set was divided
into 9 folds of which 8 folds were used to train a
model which was tested on the 9th fold. All the
reported results in this section are averaged over
the 9 folds. We report F?=1 (F)-measure as the
harmonic mean between (P)recision and (R)ecall.
We categorized the experiments into three dis-
tinct classes as shown in Table 3. For each class,
we did experiments with different feature sets and
133
No Feature Description
Lexical Features
1 verbType Modal/Aux/Reg ( = ?nil? if the word is not a verb)
2 lemma Lemma of the token
3 POS Word?s POS tag
4 whichModalAmI If I am a modal, what am I? ( = ?nil? if I am not a modal)
Word List Features
1 isGenericAdj Am I one of some, many, certain, several?
2 isUncertainAdv Am I one of generally, probably, usually, likely, typically, possibly, commonly, nearly,
perhaps, often?
3 levinClass If I am a verb, which levin class do I belong to?
Syntactic Features
1 parentPOS What is my parent?s POS tag?
2 leftSisPOS What is my left sister?s POS tag?
3 rightSisPOS What is my right sister?s POS tag?
4 whichModalIsMyDaughter If I have a daughter which is a modal, what is it? ( = ?nil? if I do not have a modal
daughter)
5 Voice Active/Passive (refer MICA documentation for details)
6 Mpos MICA?s mapping of POS tags (refer MICA documentation for details)
7 isMyNNSparentGeneric If I am an adjective and if my parent is NNS and does not have a child other than
adjectives
8 haveDaughterAux Do I have a daughter which is an auxiliary.
9 whichAuxIsMyDaughter If I have a daughter which is an auxiliary, what is it? ( = ?nil? if I do not have an
auxiliary daughter)
Table 1: Features used in the configurations listed in Table 4 and Table 6
Class Description
L Lexical features
LW Lexical and Wordlist features
LS Lexical and Syntactic features
LSW Lexical, Syntactic and Wordlist fea-
tures
Table 3: Experiment Sets
(linear) context widths. Here, context width de-
notes the window of tokens whose features are
considered. For example, a context width of 2
means that the feature vector of any given token
includes, in addition to its own features, those of
2 tokens before and after it as well as the predic-
tion for 2 tokens before it. We varied the context
widths from 1 to 5, and found that the best results
were obtained for context width of 1 and 2.
4.1 Experimental Results
In this section, we present the results of experi-
ments conducted on the development set as part
of this task. The results for the system using Yam-
cha and Mallet are given in Table 4. CW stands for
Context Width and P, R and F stands for Precision,
Recall and F-measure, respectively. These results
include the top performing 5 feature set - context
width configurations using all three classes of fea-
tures in both cases. It includes cue level predic-
tion performance as well as sentence level predic-
tion performance, where in a sentence is tagged
as uncertain if it contains at least one cue phrase.
In case of Mallet, it is observed that the best per-
forming top 5 feature sets were all from the LSW
category whereas in Yamcha, even configurations
of LS category worked well.
We also present cue level results across feature
categories for the Mallet experiments. Table 5
shows the best feature set - context width configu-
ration for each class of experiments.
Class Feature Set CW
L POS, verbType 2
LW lemma, POS, modalMe, isGenericAdj,
isUncertainAdj
2
LS POS, parentPOS, modalDaughter, left-
SisPOS, rightSisPOS, voice
2
LSW POS, parentPOS, modalMe, isDaughter-
Aux, leftSisPOS, mpos, isUncertainAdj,
isGenericAdj, myNNSparentIsGeneric
1
Table 5: Best Feature sets - Across feature classes
Table 6 shows the cue level results of the best
model for each class of experiments.
134
No Feature Description
Lexical Features
1 Stem Word stem (Using Porter Stemmer)
2 isNumeric Word is Alphabet or Numeric?
Syntactic Features
1 parentStem Parent word stem (Using Porter Stemmer)
2 parentLemma Parent word?s Lemma
3 wordSupertag Word?s Super Tag (from Penn Treebank)
4 parentSupertag Parent word?s super tag (from Penn Treebank)
5 isRoot Is the word the root of the MICA Parse tree?
6 pred Is the word a predicate? (pred in MICA features)
7 drole Deep role (drole in MICA features)
8 haveDaughterTo Do I have a daughter ?to??
9 haveDaughterPerfect Do I have a daughter which is one of has, have, had?
10 haveDaughterShould Do I have a daughter should?
11 haveDaughterWh Do I have a daughter who is one of where, when, while, who, why?
Table 2: Features which turned out to be not useful
Class Cue P Cue R Cue F
L 54.89 21.99 30.07
LW 51.14 20.70 28.81
LS 52.08 25.71 33.23
LSW 51.13 29.38 36.71
Table 6: Cue level Results - Across feature classes
4.2 Analysis
It is observed that the best results were observed
on LSW category. The main constituent of this
category was the combination of isMyNNSpar-
entGeneric and isGenericAdj. Also, it was
found that W features used without S features de-
creased the prediction performance. Out of the
syntactic features, parentPOS, leftSisPOS and
rightSisPOS proved to be the most useful in ad-
dition to isMyNNSparentGeneric.
Also, the highest cue level precision of 54.89%
was obtained for L class, whereas it was lowered
to 51.13% by the addition of S and W features.
However, the performance improvement is due to
the improved recall, which is as per the expec-
tation that syntactic features would help identify
new patterns, which lexical features alone cannot.
It is also worth noting that addition of W features
decreased the precision by 3.75 percentage points
whereas addition of S features decreased the pre-
cision by 2.81 percentage points. Addition of S
features improved the recall by 3.72 percentage
points where as addition of both S and W features
improved it by 7.39 percentage points. However,
addition of W features alone decreased the recall
by 1.29 percentage points. This suggests that the
words in the wordlists were useful only when pre-
sented with the syntactic context in which they oc-
curred.
Mallet proved to consistently over perform
Yamcha in this task in terms of prediction perfor-
mance as well as speed. For e.g, for a configura-
tion of 10 features with context width of 2, Yam-
cha took around 5-6 hrs to perform the 9-fold cross
validation on the entire training dataset, whereas
Mallet took only around 30-40 minutes.
5 System used for Evaluation
In this section, we explain in detail the system
which was used for the results submitted in the
shared task evaluation.
For predicting the cue phrases on evaluation
dataset for the shared task, we trained a model us-
ing the best performing configuration (feature set
and machinery) from the experiments described in
Section 4. The best configuration used the feature
set <POS, parentPOS, modalMe, isDaugh-
terAux, leftSisPOS, mpos, isUncertainAdj, is-
GenericAdj, myNNSparentIsGeneric> with a
context width of 1 and it was trained using Mal-
let?s CRF. The cross validation results of this con-
figuration is reported in Table 4 (First feature set in
the Mallet section). This model was trained on the
entire Wikipedia training set provided for Task 1.
We used this model to tag the evaluation dataset
with uncertainty cues and any sentence where a
cue phrase was tagged was classified as an uncer-
tain sentence.
135
Feature Set CW Cue SentP R F P R F
Yamcha - Top 5 Configurations
POS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,
levinClass, myNNSparentIsGeneric
2 51.59 26.96 34.10 65.27 38.33 48.30
POS, parentPOS, amIuncertain 1 43.13 29.41 33.79 55.37 41.77 47.62
POS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,
voice
2 52.08 25.71 33.23 66.52 37.10 47.63
POS, parentPOS, modalDaughter, leftSisPOS 2 54.25 25.16 33.20 69.38 35.63 47.08
POS, parentPOS, modalDaughter, leftSisPOS, rightSisPOS,
mpos
2 51.82 25.56 33.01 65.62 36.12 46.59
Mallet - Top 5 Configurations
POS, parentPOS, modalMe, isDaughterAux, leftSisPOS,
mpos, isUncertainAdj, isGenericAdj, myNNSparentIsGeneric
1 51.13 29.38 36.71 66.29 42.71 51.95
POS, parentPOS, modalMe, isDaughterAux, leftSisPOS,
mpos, voice, isUncertainAdj, isGenericAdj, myNNSparentIs-
Generic
1 49.81 29.07 36.04 65.64 42.24 51.40
POS, parentPOS, modalMe, isUncertainAdj, isGenericAdj,
myNNSparentIsGeneric
2 52.57 28.96 35.55 65.18 39.56 49.24
POS, parentPOS, modalMe, auxDaughter, leftSisPOS, mpos,
voice, isUncertainAdj, isGenericAdj, myNNSparentIsGeneric
1 48.22 28.67 35.40 65.25 42.80 51.69
POS, parentPOS, modalMe, leftSisPOS, mpos, voice,
isUncertainAdj, isGenericAdj, myNNSparentIsGeneric
1 52.26 28.12 35.34 65.99 40.05 49.85
Table 4: Overall Results
5.1 Evaluation Results
This section presents the results obtained on the
shared task evaluation in detail. The sentence level
results are given in Table 7. Our system obtained
a high precision of 87.95% with a low recall of
28.42% and F-measure of 42.96% on the task.
This was the 3rd best precision reported for the
Wikipedia task 1.
System Precision Recall F-Measure
Best System 72.04 51.66 60.17
... ... ... ...
This System 87.95 28.42 42.96
Last System 94.23 6.58 12.30
Table 7: Evaluation - Cue Level Results
Table 8 presents the cue level results for the
task. Our system had a cue level prediction pre-
cision of 67.14% with a low recall of 16.70% and
F-measure of 26.75%, which is the 3rd best F-
measure result among the published cue level re-
sults4.
We ran the best model trained onWikipedia cor-
pus on the biomedical evaluation dataset. As ex-
pected, the results were much lower. It obtained a
precision of 67.54% with a low recall of 19.49%
and F-measure of 30.26%.
4In the submitted result, cues were tagged in IOB format.
Hence, cue level statistics were not computed and published
in the CoNLL website.
System Precision Recall F-Measure
X 63.01 25.94 36.55
X 76.06 21.64 33.69
This System 67.14 16.70 26.75
X 28.95 14.70 19.50
X 24.57 7.35 11.32
Table 8: Evaluation - Cue Level Results
6 Conclusion and Future Work
A simple bag of words approach at the sentence
level could have given similar or even better per-
formance for the sentence level prediction task.
However, identifying cues in context is important
to extend this task to application where we need to
make semantic inferences or even identifying the
scope of uncertainty (which was the task 2 of the
shared task). Hence, we infer that this or a simi-
lar cue tagging approach with a more sophisticated
feature set and machinery should be explored fur-
ther.
Our experiments show that the addition of syn-
tactic features helps in improving recall. However,
the advantage given by syntactic features were sur-
prisingly marginal. In detailed error analysis, it
was found that the syntactic patterns that proved
helpful for this task were fairly local. So, proba-
bly exploring shallow syntactic features instead of
deep syntactic features might be helpful for this
task. Also, we assume that using more sophis-
136
ticated lexical features or custom made lexicons
could also improve performance.
Acknowledgements
This work was supported by grants from the Hu-
man Language Technology Center of Excellence.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the views
of the sponsor.
I would also like to extend my heartfelt grat-
itude to Prof. Kathy McKeown and Yves Petinot
for their vital encouragement and support through-
out this project. I would also like to thank my ad-
visors Dr. Mona Diab and Dr. Owen Rambow for
their valuable suggestions and support.
References
Srinivas Bangalore, Pierre Boullier, Alexis Nasr, Owen
Rambow, and Beno??t Sagot. 2009. MICA: A prob-
abilistic dependency parser based on tree insertion
grammars. In NAACL HLT 2009 (Short Papers).
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
Aravind K. Joshi, Leon Levy, and M Takahashi. 1975.
Tree adjunct grammars. Journal of the Computer
and System Sciences, 10:136?163.
137
Proceedings of the 2012 Student Research Workshop, pages 7?12,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Detecting Power Relations from Written Dialog
Vinodkumar Prabhakaran
Department of Computer Science
Columbia University
New York, NY 10027, USA
vinod@cs.columbia.edu
Abstract
In my thesis I propose a data-oriented study
on how social power relations between par-
ticipants manifest in the language and struc-
ture of online written dialogs. I propose that
there are different types of power relations
and they are different in the ways they are ex-
pressed and revealed in dialog and across dif-
ferent languages, genres and domains. So far,
I have defined four types of power and anno-
tated them in corporate email threads in En-
glish and found support that they in fact man-
ifest differently in the threads. Using dialog
and language features, I have built a system to
predict participants possessing these types of
power within email threads. I intend to extend
this system to other languages, genres and do-
mains and to improve it?s performance using
deeper linguistic analysis.
1 Introduction
Social relations like power and influence are difficult
concepts to define, but are easily recognizable when
expressed. Most classical definitions of power in
the sociology literature (e.g. (Bierstedt, 1950; Dahl,
1957)) include ?an element indicating that power is
the capability of one social actor to overcome re-
sistance in achieving a desired objective or result?
(Pfeffer, 1981). Influence closely resembles power,
although some consider it as one of the means by
which power is used (Handy, 1985). The five bases
of power ? Coercive, Reward, Legitimate (Posi-
tional), Referent, and Expert ? proposed by French
and Raven (1959) and its extensions are widely used
in sociology to study power. I find these definitions
and typologies helpful as general background, but
not specific enough for a data-oriented study on how
they are expressed in online written dialogs.
One of the primary ways power is manifested is
the manner in which people participate in dialog.
Power relations sometimes constrain how one be-
haves when engaging in dialog; in some other cases,
they enable one to constrain someone else?s behav-
ior. And in some cases, the dialog behavior becomes
a tool to express and even pursue power. By dialog
behavior, I mean the choices one makes while en-
gaging in dialog. It includes choices with respect
to the message content, like lexical choices, degree
of politeness or instances of overt display of power
such as orders or commands. It also includes choices
participants make in terms of dialog structure, like
the choice of when to participate with how much and
what sort of contribution, how many questions to ask
and which of those questions to answer and the time
between those questions and their answers.
The primary goal of my thesis is to show that
different social power relations manifest themselves
in written dialog in different, but predictable ways,
and to investigate how these manifestations differ
across languages, genres and domains. To achieve
this goal, I aim to introduce a new typology of power
that is relevant in online written interactions and can
be validated using data-oriented approaches. Then, I
aim to study how these different types of power dif-
fer in their manifestations in dialog. Specifically, I
aim to capture and compare these manifestations in
two dimensions of the dialog: content and structure.
In addition to using existing components like dialog
act taggers and linkers to capture the dialog structure
7
and lexical analyzers to capture content features, I
plan to identify and extract more structural and lin-
guistic indicators of power relations. Using these
features, I will build a system that can automati-
cally extract power relations between participants of
written dialogs across different languages (English
vs. Arabic), genres (discussion forums vs. emails)
and domains (political vs. scientific). Currently, I
have partially achieved this goal within the context
of English corporate email threads, which represent
a specific language-genre-domain combination. The
four types of power I have defined are: situational
power, hierarchical power, control of communica-
tion and influence. My future research directions in-
clude 1) broadening this work onto other languages,
genres and domains and 2) using deeper analysis to
identify more indicators of power and capture power
relations at finer granularity
2 Literature survey
It has long been established that there is a correla-
tion between dialog behavior of a discourse partic-
ipant and how influential she is perceived to be by
the other discourse participants (Bales et al, 1951;
Scherer, 1979; Ng et al, 1995). Specifically, fac-
tors such as frequency of contribution, proportion of
turns, and number of successful interruptions have
been identified as being important indicators of in-
fluence. Locher (2004) recognizes ?restriction of
an interactant?s action-environment? (Wartenberg,
1990) as a key element by which exercise of power
in interactions can be identified. I use a linguis-
tic indicator Overt Display of Power which cap-
tures action-restriction at an utterance level. Warten-
berg (1990) also makes the important distinction be-
tween two notions of power: power-over and power-
to. Power-over refers to hierarchical relationships
between interactants, while power-to refers to the
ability an interactant possesses (may be temporar-
ily) and can use within the interaction. My notions
of hierarchical power and situational power roughly
correspond to Wartenberg?s notions of power-over
and power-to, respectively. Both can be considered
special cases of French and Raven (1959)?s notion
of legitimate power. I consider influence as a type
of power which captures notions of expert power
and referent power described by French and Raven.
Finally, my notion of control of communication is
based on the concept of conversational control in-
troduced by Ng and Bradac (1993). It is a form of
power the participant has over the interaction; other
forms of power are modeled between participants.
In computational literature, several studies have
used Social Network Analysis (Diesner and Carley,
2005; Shetty and Adibi, 2005; Creamer et al, 2009)
to deduce social relations from online communica-
tion. These studies use only meta-data about mes-
sages: who sent a message to whom and when. For
example, Creamer et al (2009) find that the response
time is an indicator of hierarchical relations; how-
ever, they calculate the response time based only on
the meta-data, and do not have access to information
such as thread structure or message content, which
would actually verify that the second email is in fact
a response to the first.
Using NLP to analyze the content of messages to
deduce power relations from written dialog is a rela-
tively new area which has been studied only recently
(Strzalkowski et al, 2010; Bramsen et al, 2011;
Peterson et al, 2011). Using knowledge of the or-
ganizational structure, Bramsen et al (2011) create
two sets of messages: messages sent from a supe-
rior to a subordinate, and vice versa. Their task is
to determine the direction of power (since all their
data, by construction of the corpus, has a power re-
lationship). They approach the task as a text classi-
fication problem and build a classifier to determine
whether the set of all emails (regardless of thread)
between two participants is an instance of up-speak
or down-speak. In contrast, I plan to use a com-
plete communication thread as a data unit and cap-
ture instances where power is actually manifested. I
also plan to study power in a broader sense, look-
ing beyond power attributed by hierarchy to other
forms of power. Strzalkowski et al (2010) are also
interested in power in written dialog. However, their
work concentrates on lower-level constructs called
Language Uses, which might indicate higher level
social constructs such as leadership and power. This
said, one of their language uses is agenda control,
which is very close to our notion of conversational
control. They model it using notions of topic switch-
ing, using mainly complex lexical features. Peter-
son et al (2011) focuses on formality in Enron email
messages and relates it to social distance and power.
8
3 Work done so far: Power in Corporate
Emails
So far, I have worked on my primary goal ? study-
ing manifestations of social power relations ? within
the context of English corporate email threads. For
this purpose, I used a subset of email threads from a
version of the Enron email corpus (Yeh and Harnly,
2006) in which messages are organized as threaded
conversations. In the remainder of this section, I first
introduce the power typology and annotations and
then present the linguistic and structural features I
used. Then, I present the findings from a statistical
significance study conducted between these features
and different types of power. Finally, I present a sys-
tem built using these features to predict participants
with power within an email thread.
Power Typology and Annotations: After care-
ful analysis of a part of the email corpus, I defined a
power typology to capture different types of power
relevant in corporate emails. I propose four types of
power: situational power, hierarchical power, con-
trol of communication and influence.1 Person 1 is
said to have situational power (SP) over person 2
if person 1 has power or authority to direct and/or
approve person 2?s actions in the current situation
or while a particular task is being performed, as can
be deduced from the communication in the current
thread. Person 1 with situational power may or may
not be above person 2 in the organizational hierar-
chy (or there may be no organizational hierarchy at
all). Person 1 is said to have hierarchical power
(HP) over person 2 if person 1 appears to be above
person 2 in the organizational hierarchy, as can be
deduced from the communication in the given thread
(annotators did not have access to independent in-
formation about the organizational hierarchy). Pos-
sible clues to HP include (by way of example): 1)
characteristic of a part of a message as being an ap-
proval, or being a direct order; 2) a person?s behav-
ior such as asking for approval; 3) a person?s au-
thority to make the final decision. A person is said
to have control of the communication (CNTRL) if
she actively attempts to achieve the intended goals
of the communication. These are people who ask
questions, request others to take action, etc. and
1This typology is an extension of an initial typology formu-
lated through collaborative effort with another student.
not people who simply respond to questions or per-
form actions when directed to do so. A thread could
have multiple such participants. A person is said
to have influence (INFL) if she 1) has credibility
in the group, 2) persists in attempting to convince
others, even if some disagreement occurs, 3) intro-
duces topics/ideas that others pick up on or support,
and 4) is a group participant but not necessarily ac-
tive in the discussion(s) where others support/credit
her. In addition, the influencer?s ideas or language
may be adopted by others and others may explic-
itly recognize influencer?s authority.2 Prabhakaran
et al (2012a) presents more details on annotations
of these power relations in the email corpus.
Manifestations in Content and Stucture: I used
six sets of features to explore manifestations of
power: dialog act percentages (DAP), dialog link
counts (DLC), positional (PST), verbosity (VRB),
lexical (LEX) and overt display of power (ODP).
The first four sets of features relate to the whole di-
alog and its structure while the last two relate to the
form and content of individual messages. The email
corpus I used has been previously annotated with di-
alog acts and links by other researchers (Hu et al,
2009). I used these annotations to capture DAP and
DLC features. DAP captures percentages of each of
the dialog act labels (Request Action, Request In-
formation, Inform, Conventional, and Commit) ag-
gregated over all messages sent by the participant
within the thread. The dialog links include forward
links which denote utterances with requests for in-
formation or actions, backward links which denote
their responses and secondary forward links which
denote utterances without explicit requests that were
interpreted as requests and were linked back from
later utterances. DLC captures various features de-
rived from these links with respect to each partici-
pant such as counts of each type of link, counts of
forward links that are connected back and counts
and percentages of those which were not connected
back. PST includes features to indicate relative posi-
tions of first and last messages by a participant. VRB
includes features to denote how much and how often
a participant took part in the conversation. PST and
2I adopt this definition from the IARPA Socio-Cultural Con-
tent in Language (SCIL) program, where many researchers par-
ticipating in the SCIL program contributed to the scope and re-
finement of the definition of a person with influence.
9
VRB are readily derivable from the email threads. I
used simple word ngram features to capture LEX.
Overt display of power (ODP) is a linguistic indi-
cator of power I introduced. An utterer can choose
linguistic forms in her utterance to signal that she
is imposing constraints on the addressee?s choice
of how to respond, which go beyond those defined
by the standard set of dialog acts. For example, if
the boss?s email is ?Please come to my office right
now?, and the addressee declines, he is clearly not
adhering to the constraints the boss has signaled,
though he is adhering to the general constraints of
cooperative dialog by responding to the request for
action. I am interested in these additional constraints
imposed on utterances through choices in linguistic
form. I define an utterance to have ODP if it is in-
terpreted as creating additional constraints on the re-
sponse beyond those imposed by the general dialog
act. An ODP can be an order, command, question
or even a declarative sentence. The presence of an
ODP does not presuppose that the utterer actually
possess social power: the utterer could be attempt-
ing to gain power. In (Prabhakaran et al, 2012b),
I present a system to identify utterances with ODP
using lexical features like word and part of speech
ngrams along with dialog acts of the utterance.
Statistical significance study: For each type of
power, I considered two populations of people who
participated in the dialog ? Pp, those judged to have
that type of power and Pn, those not judged to have
that power. Then, for each feature, I performed a
two-sample, two-tailed t-test comparing means of
feature values of Pp and Pn. I found many fea-
tures which are statistically significant, which sug-
gests that power types are reflected in the email
threads. I also found that the significance of fea-
tures differ considerably from one type of power to
another, which suggests that these power types are
reflected differently in the threads, and that they are
thus indeed different types of power. For hierarchi-
cal power, the feature TokenRatio has a mean of 0.38
for Pp and 0.54 for Pn with a p-value of 0.07. This
suggests that bosses tend to talk less within a thread.
People with situational power or control request ac-
tions significantly more often than others and send
significantly more and longer messages than others.
People with influence never request actions and send
much longer messages than others. They also tend to
have more secondary forward links (with a p-value
of 0.07) which suggests that people often respond
to what people with influence say even if the influ-
encer?s contribution is not a request.
Predicting Persons with Power: I formally de-
fined the problem as: given a communication thread
T and an active participant X , predict whether X
has power of type P ? {SP, HP, INFL, CNTRL}
over some person Y in the thread. I built a binary
SVM classifier for each power type P predicting
whether or not X has power P based on features
with respect to X in the context of the given thread
T . I obtained good results for SP and CNTRL, but
HP and INFL were hard to predict since they oc-
curred rarely in my corpus. The combination of
DLC and OSP performed best for SP (F = 64.4) and
PST performed best for CNTRL (F = 90.0). For HP,
the combination of DLC and LEX performed best (F
= 34.8). For INFL, the best performer was DLC (F
= 22.6). All results except the ones for INFL were
statistically significant improvement over an always-
true baseline. I found dialog features to be signif-
icant in predicting power, though content features
also contribute to detecting some types of power.
4 Proposed Work
So far, I have defined four types of power and have
studied how they are expressed and revealed in En-
ron email threads. My future research directions in-
clude deepening this study by i) capturing more lin-
guistic indicators of social power in dialog, ii) build-
ing automatic taggers for all linguistic indicators, iii)
using deeper semantic analysis on the content and
iv) extending it to capture power relations at finer
granularity. I also intend to broaden this work into
different languages, genres and domains, adapting
work done in email threads when viable.
More power indicators : I will work on captur-
ing more linguistic indicators of power from dialog.
I currently have annotations at the utterance level
that capture attempts to exercise power and attempts
to influence. I will use these annotations to build
systems that can automatically detect them. In ad-
dition, I plan to capture linguistic expressions that
suggest lack of power such as asking for approvals,
permissions etc. or acting overly polite. For this,
I will have to add new annotations to the data. I
10
also plan to perform deeper analysis on the content
to capture subjectivity ? whether someone states
more facts than opinions, commitment ? whether
someone commits to what she says, and the pres-
ence of other modalities such as permissions, re-
quirements, desires etc. I plan to use existing work
in subjectivity analysis (Wilson, 2008) and commit-
ment analysis (Prabhakaran et al, 2010) for this pur-
pose. For modality analysis, I plan to use previous
unpublished work that I participated in.
Fully automated system: I plan to use automatic
taggers to extract dialog act and link features and
other linguistic indicators of power (like ODP), to
build a fully automated social power extraction sys-
tem. Hu et al (2009) presented a dialog act tagger
and link predictor which could be used to extract
DAP and DLC. However, I found their dialog act
tagger performs poorly on minority classes such as
requests for actions, which are more critical to pre-
dict power. Their link predictor obtained an F mea-
sure of 35% which makes it unfit to be used in its
current form. For ODP, I will use the SVM clas-
sifier I built, which obtained a best cross validation
F measure of 65.8. I plan to improve the perfor-
mance of the dialog act tagger, the link predictor and
the ODP tagger using new features and techniques.
I plan to use a threshold adjustment algorithm pro-
posed by Lin et al (2007) to handle the class imbal-
ance problem in dialog act tagger and link predictor
(ODP tagger already uses this). I will also build au-
tomatic taggers for all other linguistic indicators of
power discussed above.
Deeper Semantic Analysis I will explore new
features derived from deeper semantic analysis to
improve performance of the dialog act tagger, the
link predictor and the taggers for other indicators of
power like ODP. In particular, I plan to use seman-
tic information from VerbNet to provide useful ab-
straction of verbs into verb classes. This will reduce
data sparseness, thereby improving the performance
of the taggers. In an initial experiment, I found that
using VerbNet class name instead of verb lemma im-
proved the performance of ODP tagger by a small
margin. I did this only for those verbs that belong
to a single VerbNet class (hence needing no dis-
ambiguation). I will explore ways to disambiguate
verbs with multiple VerbNet class assignments and
employ this feature in other taggers as well.
Finer granularity of relations: I will enhance
the system to predict power relations between pairs
of participants. Aggregating features at the partic-
ipant level is prone to noise. For example, let X ,
Y , Z be active participants such that X has power
over Y , who has power over Z . When we aggregate
features with respect to Y , we are introducing noise
from the part of communication between X and Y .
Extending my work to the person pair level would
prevent this noise and provide us with a finer gran-
ularity of power relations. Formally, I want to pre-
dict if person X has power P over person Y , given
a communication thread T . My power annotations
already capture the recipient (person 2) of power re-
lations which I will use for this purpose.
Language, genre and domain adaptation: I will
extend my work in the English email threads to other
languages, genres and domains. Specifically, I plan
to work on existing data containing Wikipedia dis-
cussion threads and political forums in both English
and Arabic. Thus, my thesis would include the
analysis of power under 5 different language-genre-
domain settings. This step will need extensive an-
notation efforts. I expect that my proposed power
typology might need to be refined to capture types
of relations in the new genres. Also, I may have
to define new linguistic indicators relevant to the
new genres or refine the ones I identified for email
threads to adapt to the new genres. This would also
require me to adapt various subsystems/taggers to
capture features such as dialog acts, links, ODP etc.
to new genres or build new systems.
5 Conclusion
In my thesis, I propose to study how different power
relations are manifested in the structure and lan-
guage of online written dialogs and build a system
to automatically extract power relations from them.
I have already conducted this study in English email
threads and I plan to extend this to other languages,
genres and domains.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
11
those of the author and do not necessarily reflect the
views of the sponsor. I thank my advisors Dr. Owen
Rambow and Dr. Mona Diab for their valuable guid-
ance and support. I thank Daniel Bauer for useful
discussions and feedback on this proposal.
References
Robert F. Bales, Fred L. Strodtbeck, Theodore M. Mills,
and Mary E. Roseborough. 1951. Channels of com-
munication in small groups. American Sociological
Review, pages 16(4), 461?468.
Robert Bierstedt. 1950. An Analysis of Social Power.
American Sociological Review.
Philip Bramsen, Martha Escobar-Molano, Ami Patel, and
Rafael Alonso. 2011. Extracting social power rela-
tionships from natural language. In ACL, pages 773?
782. The Association for Computer Linguistics.
Germa?n Creamer, Ryan Rowe, Shlomo Hershkop, and
Salvatore J. Stolfo. 2009. Advances in web min-
ing and web usage analysis. chapter Segmentation
and Automated Social Hierarchy Detection through
Email Network Analysis, pages 40?58. Springer-
Verlag, Berlin, Heidelberg.
Robert A. Dahl. 1957. The concept of power. Syst. Res.,
2(3):201?215.
Jana Diesner and Kathleen M. Carley. 2005. Exploration
of communication networks from the enron email cor-
pus. In In Proc. of Workshop on Link Analysis, Coun-
terterrorism and Security, SIAM International Confer-
ence on Data Mining 2005, pages 21?23.
John R. French and Bertram Raven. 1959. The Bases of
Social Power. In Dorwin Cartwright, editor, Studies in
Social Power, pages 150?167+. University of Michi-
gan Press.
Charles B. Handy. 1985. Understanding Organisations.
Institute of Purchasing & Supply.
Jun Hu, Rebecca Passonneau, and Owen Rambow. 2009.
Contrasting the interaction structure of an email and a
telephone corpus: A machine learning approach to an-
notation of dialogue function units. In Proceedings of
the SIGDIAL 2009 Conference, London, UK, Septem-
ber. Association for Computational Linguistics.
Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng. 2007.
A note on platt?s probabilistic outputs for support vec-
tor machines. Mach. Learn., 68:267?276, October.
Miriam A. Locher. 2004. Power and politeness in ac-
tion: disagreements in oral communication. Lan-
guage, power, and social process. M. de Gruyter.
Sik Hung. Ng and James J. Bradac. 1993. Power in lan-
guage : verbal communication and social influence /
Sik Hung Ng, James J. Bradac. Sage Publications,
Newbury Park :.
Sik Hung Ng, Mark Brooke, , and Michael Dunne.
1995. Interruption and influence in discussion groups.
Journal of Language and Social Psychology, pages
14(4),369?381.
Kelly Peterson, Matt Hohensee, and Fei Xia. 2011.
Email formality in the workplace: A case study on
the enron corpus. In Proceedings of the Workshop
on Language in Social Media (LSM 2011), pages 86?
95, Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jeffrey Pfeffer. 1981. Power in organizations. Pitman,
Marshfield, MA.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014?1022, Beijing,
China, August. Coling 2010 Organizing Committee.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012a. Annotations for power relations on
email threads. In Proceedings of the Eighth confer-
ence on International Language Resources and Eval-
uation (LREC?12), Istanbul, Turkey, May. European
Language Resources Association (ELRA).
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2012b. Predicting overt display of power in
written dialogs. In Human Language Technologies:
The 2012 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Montreal, Canada, June. Association for Compu-
tational Linguistics.
K. R. Scherer. 1979. Voice and speech correlates of per-
ceived social influence in simulated juries. In H. Giles
and R. St Clair (Eds), Language and social psychol-
ogy, pages 88?120. Oxford: Blackwell.
Jitesh Shetty and Jafar Adibi. 2005. Discovering im-
portant nodes through graph entropy the case of en-
ron email database. In Proceedings of the 3rd inter-
national workshop on Link discovery, LinkKDD ?05,
pages 74?81, New York, NY, USA. ACM.
Tomek Strzalkowski, George Aaron Broadwell, Jennifer
Stromer-Galley, Samira Shaikh, Sarah Taylor, and
Nick Webb. 2010. Modeling socio-cultural phenom-
ena in discourse. In Proceedings of the 23rd Interna-
tional Conference on COLING 2010, Beijing, China,
August. Coling 2010 Organizing Committee.
Thomas E. Wartenberg. 1990. The forms of power:
from domination to transformation. Temple Univer-
sity Press.
Theresa Wilson. 2008. Annotating subjective content in
meetings. In Proceedings of the Language Resources
and Evaluation Conference. LREC-2008, Springer.
AMIDA-85.
Jen-yuan Yeh and Aaron Harnly. 2006. Email thread
reassembly using similarity matching. In In Proc. of
CEAS.
12
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 57?64, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Statistical Modality Tagging
from Rule-based Annotations and Crowdsourcing
Vinodkumar Prabhakaran Michael Bloodgood Mona Diab
CS CASL CCLS
Columbia University University of Maryland Columbia University
vinod@cs.columbia.edu meb@umd.edu mdiab@ccls.columbia.edu
Bonnie Dorr Lori Levin Christine D. Piatko
CS and UMIACS LTI APL
University of Maryland Carnegie Mellon University Johns Hopkins University
bonnie@umiacs.umd.edu lsl@cs.cmu.edu christine.piatko@jhuapl.edu
Owen Rambow Benjamin Van Durme
CCLS HLTCOE
Columbia University Johns Hopkins University
rambow@ccls.columbia.edu vandurme@cs.jhu.edu
Abstract
We explore training an automatic modality
tagger. Modality is the attitude that a speaker
might have toward an event or state. One of
the main hurdles for training a linguistic tag-
ger is gathering training data. This is par-
ticularly problematic for training a tagger for
modality because modality triggers are sparse
for the overwhelming majority of sentences.
We investigate an approach to automatically
training a modality tagger where we first gath-
ered sentences based on a high-recall simple
rule-based modality tagger and then provided
these sentences to Mechanical Turk annotators
for further annotation. We used the resulting
set of training data to train a precise modality
tagger using a multi-class SVM that delivers
good performance.
1 Introduction
Modality is an extra-propositional component of
meaning. In John may go to NY, the basic propo-
sition is John go to NY and the word may indi-
cates modality. Van Der Auwera and Ammann
(2005) define core cases of modality: John must
go to NY (epistemic necessity), John might go to
NY (epistemic possibility), John has to leave now
(deontic necessity) and John may leave now (de-
ontic possibility). Many semanticists (e.g. Kratzer
(1981), Kratzer (1991), Kaufmann et al (2006)) de-
fine modality as quantification over possible worlds.
John might go means that there exist some possi-
ble worlds in which John goes. Another view of
modality relates more to a speakers attitude toward
a proposition (e.g. McShane et al (2004)).
Modality might be construed broadly to include
several types of attitudes that a speaker wants to ex-
press towards an event, state or proposition. Modal-
ity might indicate factivity, evidentiality, or senti-
ment (McShane et al, 2004). Factivity is related to
whether the speaker wishes to convey his or her be-
lief that the propositional content is true or not, i.e.,
whether it actually obtains in this world or not. It
distinguishes things that (the speaker believes) hap-
pened from things that he or she desires, plans, or
considers merely probable. Evidentiality deals with
the source of information and may provide clues to
the reliability of the information. Did the speaker
57
have firsthand knowledge of what he or she is re-
porting, or was it hearsay or inferred from indirect
evidence? Sentiment deals with a speaker?s positive
or negative feelings toward an event, state, or propo-
sition.
In this paper, we focus on the following five
modalities; we have investigated the belief/factivity
modality previously (Diab et al, 2009b; Prab-
hakaran et al, 2010), and we leave other modalities
to future work.
? Ability: can H do P?
? Effort: does H try to do P?
? Intention: does H intend P?
? Success: does H succeed in P?
? Want: does H want P?
We investigate automatically training a modality
tagger by using multi-class Support Vector Ma-
chines (SVMs). One of the main hurdles for training
a linguistic tagger is gathering training data. This is
particularly problematic for training a modality tag-
ger because modality triggers are sparse for the over-
whelming majority of the sentences. Baker et al
(2010) created a modality tagger by using a semi-
automatic approach for creating rules for a rule-
based tagger. A pilot study revealed that it can boost
recall well above the naturally occurring proportion
of modality without annotated data but with only
60% precision. We investigated an approach where
we first gathered sentences based on a simple modal-
ity tagger and then provided these sentences to an-
notators for further annotation, The resulting anno-
tated data also preserved the level of inter-annotator
agreement for each example so that learning algo-
rithms could take that into account during training.
Finally, the resulting set of annotations was used for
training a modality tagger using SVMs, which gave
a high precision indicating the success of this ap-
proach.
Section 2 discusses related work. Section 3 dis-
cusses our procedure for gathering training data.
Section 4 discusses the machine learning setup
and features used to train our modality tagger and
presents experiments and results. Section 5 con-
cludes and discusses future work.
2 Related Work
Previous related work includes TimeML (Sauri et
al., 2006), which involves modality annotation on
events, and Factbank (Sauri and Pustejovsky, 2009),
where event mentions are marked with degree of fac-
tuality. Modality is also important in the detection of
uncertainty and hedging. The CoNLL shared task in
2010 (Farkas et al, 2010) deals with automatic de-
tection of uncertainty and hedging in Wikipedia and
biomedical sentences.
Baker et al (2010) and Baker et al (2012) ana-
lyze a set of eight modalities which include belief,
require and permit, in addition to the five modalities
we focus on in this paper. They built a rule-based
modality tagger using a semi-automatic approach to
create rules. This earlier work differs from the work
described in this paper in that the our emphasis is on
the creation of an automatic modality tagger using
machine learning techniques. Note that the anno-
tation and automatic tagging of the belief modality
(i.e., factivity) is described in more detail in (Diab et
al., 2009b; Prabhakaran et al, 2010).
There has been a considerable amount of inter-
est in modality in the biomedical domain. Negation,
uncertainty, and hedging are annotated in the Bio-
scope corpus (Vincze et al, 2008), along with infor-
mation about which words are in the scope of nega-
tion/uncertainty. The i2b2 NLP Shared Task in 2010
included a track for detecting assertion status (e.g.
present, absent, possible, conditional, hypothetical
etc.) of medical problems in clinical records.1 Apos-
tolova et al (2011) presents a rule-based system for
the detection of negation and speculation scopes us-
ing the Bioscope corpus. Other studies emphasize
the importance of detecting uncertainty in medical
text summarization (Morante and Daelemans, 2009;
Aramaki et al, 2009).
Modality has also received some attention in the
context of certain applications. Earlier work de-
scribing the difficulty of correctly translating modal-
ity using machine translation includes (Sigurd and
Gawro?nska, 1994) and (Murata et al, 2005). Sig-
urd et al (1994) write about rule based frameworks
and how using alternate grammatical constructions
such as the passive can improve the rendering of the
modal in the target language. Murata et al (2005)
1https://www.i2b2.org/NLP/Relations/
58
analyze the translation of Japanese into English
by several systems, showing they often render the
present incorrectly as the progressive. The authors
trained a support vector machine to specifically han-
dle modal constructions, while our modal annotation
approach is a part of a full translation system.
The textual entailment literature includes modal-
ity annotation schemes. Identifying modalities is
important to determine whether a text entails a hy-
pothesis. Bar-Haim et al (2007) include polarity
based rules and negation and modality annotation
rules. The polarity rules are based on an indepen-
dent polarity lexicon (Nairn et al, 2006). The an-
notation rules for negation and modality of predi-
cates are based on identifying modal verbs, as well
as conditional sentences and modal adverbials. The
authors read the modality off parse trees directly us-
ing simple structural rules for modifiers.
3 Constructing Modality Training Data
In this section, we will discuss the procedure we
followed to construct the training data for build-
ing the automatic modality tagger. In a pilot study,
we obtained and ran the modality tagger described
in (Baker et al, 2010) on the English side of the
Urdu-English LDC language pack.2 We randomly
selected 1997 sentences that the tagger had labeled
as not having the Want modality and posted them on
Amazon Mechanical Turk (MTurk). Three differ-
ent Turkers (MTurk annotators) marked, for each of
the sentences, whether it contained the Want modal-
ity. Using majority rules as the Turker judgment,
95 (i.e., 4.76%) of these sentences were marked as
having a Want modality. We also posted 1993 sen-
tences that the tagger had labeled as having a Want
modality and only 1238 of them were marked by the
Turkers as having a Want modality. Therefore, the
estimated precision of this type of approach is only
around 60%.
Hence, we will not be able to use the (Baker et
al., 2010) tagger to gather training data. Instead,
our approach was to apply a simple tagger as a first
pass, with positive examples subsequently hand-
annotated using MTurk. We made use of sentence
data from the Enron email corpus,3 derived from the
2LDC Catalog No.: LDC2006E110.
3http://www-2.cs.cmu.edu/?enron/
version owing to Fiore and Heer,4 further processed
as described by (Roark, 2009).5
To construct the simple tagger (the first pass), we
used a lexicon of modality trigger words (e.g., try,
plan, aim, wish, want) constructed by Baker et al
(2010). The tagger essentially tags each sentence
that has a word in the lexicon with the corresponding
modality. We wrote a few simple obvious filters for a
handful of exceptional cases that arise due to the fact
that our sentences are from e-mail. For example, we
filtered out best wishes expressions, which otherwise
would have been tagged as Want because of the word
wishes.
The words that trigger modality occur with very
different frequencies. If one is not careful, the
training data may be dominated by only the com-
monly occurring trigger words and the learned tag-
ger would then be biased towards these words. In
order to ensure that our training data had a diverse
set of examples containing many lexical triggers and
not just a lot of examples with the same lexical trig-
ger, for each modality we capped the number of sen-
tences from a single trigger to be at most 50. After
we had the set of sentences selected by the simple
tagger, we posted them on MTurk for annotation.
The Turkers were asked to check a box indicat-
ing that the modality was not present in the sentence
if the given modality was not expressed. If they did
not check that box, then they were asked to highlight
the target of the modality. Table 1 shows the number
of sentences we posted on MTurk for each modal-
ity.6 Three Turkers annotated each sentence. We
restricted the task to Turkers who were adults, had
greater than a 95% approval rating, and had com-
pleted at least 50 HITs (Human Intelligence Tasks)
on MTurk. We paid US$0.10 for each set of ten sen-
tences.
Since our data was annotated by three Turkers,
for training data we used only those examples for
which at least two Turkers agreed on the modality
and the target of the modality. This resulted in 1,008
examples. 674 examples had two Turkers agreeing
and 334 had unanimous agreement. We kept track
of the level of agreement for each example so that
4http://bailando.sims.berkeley.edu/enron/enron.sql.gz
5Data received through personal communication
6More detailed statistics on MTurk annotations are available
at http://hltcoe.jhu.edu/datasets/.
59
Modality Count
Ability 190
Effort 1350
Intention 1320
Success 1160
Want 1390
Table 1: For each modality, the number of sentences re-
turned by the simple tagger that we posted on MTurk.
our learner could weight the examples differently
depending on the level of inter-annotator agreement.
4 Multiclass SVM for Modality
In this section, we describe the automatic modal-
ity tagger we built using the MTurk annotations de-
scribed in Section 3 as the training data. Section 4.1
describes the training and evaluation data. In Sec-
tion 4.2, we present the machinery and Section 4.3
describes the features we used to train the tagger.
In Section 4.4, we present various experiments and
discuss results. Section 4.5, presents additional ex-
periments using annotator confidence.
4.1 Data
For training, we used the data presented in Section 3.
We refer to it as MTurk data in the rest of this paper.
For evaluation, we selected a part of the LU Corpus
(Diab et al, 2009a) (1228 sentences) and our expert
annotated it with modality tags. We first used the
high-recall simple modality tagger described in Sec-
tion 3 to select the sentences with modalities. Out
of the 235 sentences returned by the simple modal-
ity tagger, our expert removed the ones which did
not in fact have a modality. In the remaining sen-
tences (94 sentences), our expert annotated the tar-
get predicate. We refer to this as the Gold dataset
in this paper. The MTurk and Gold datasets differ in
terms of genres as well as annotators (Turker vs. Ex-
pert). The distribution of modalities in both MTurk
and Gold annotations are given in Table 2.
4.2 Approach
We applied a supervised learning framework us-
ing multi-class SVMs to automatically learn to tag
Modality MTurk Gold
Ability 6% 48%
Effort 25% 10%
Intention 30% 11%
Success 24% 9%
Want 15% 23%
Table 2: Frequency of Modalities
modalities in context. For tagging, we used the Yam-
cha (Kudo and Matsumoto, 2003) sequence labeling
system which uses the SVMlight (Joachims, 1999)
package for classification. We used One versus All
method for multi-class classification on a quadratic
kernel with a C value of 1. We report recall and pre-
cision on word tokens in our corpus for each modal-
ity. We also report F?=1 (F)-measure as the har-
monic mean between (P)recision and (R)ecall.
4.3 Features
We used lexical features at the token level which can
be extracted without any parsing with relatively high
accuracy. We use the term context width to denote
the window of tokens whose features are considered
for predicting the tag for a given token. For example,
a context width of 2 means that the feature vector
of any given token includes, in addition to its own
features, those of 2 tokens before and after it as well
as the tag prediction for 2 tokens before it. We did
experiments varying the context width from 1 to 5
and found that a context width of 2 gives the optimal
performance. All results reported in this paper are
obtained with a context width of 2. For each token,
we performed experiments using following lexical
features:
? wordStem - Word stem.
? wordLemma - Word lemma.
? POS - Word?s POS tag.
? isNumeric - Word is Numeric?
? verbType - Modal/Auxiliary/Regular/Nil
? whichModal - If the word is a modal verb,
which modal?
60
We used the Porter stemmer (Porter, 1997) to ob-
tain the stem of a word token. To determine the
word lemma, we used an in-house lemmatizer using
dictionary and morphological analysis to obtain the
dictionary form of a word. We obtained POS tags
from Stanford POS tagger and used those tags to
determine verbType and whichModal features. The
verbType feature is assigned a value ?Nil? if the word
is not a verb and whichModal feature is assigned a
value ?Nil? if the word is not a modal verb. The fea-
ture isNumeric is a binary feature denoting whether
the token contains only digits or not.
4.4 Experiments and Results
In this section, we present experiments performed
considering all the MTurk annotations where two
annotators agreed and all the MTurk annotations
where all three annotators agreed to be equally cor-
rect annotations. We present experiments applying
differential weights for these annotations in Section
4.5. We performed 4-fold cross validation (4FCV)
on MTurk data in order to select the best feature
set configuration ?. The best feature set obtained
waswordStem,POS,whichModal with a context
width of 2. For finding the best performing fea-
ture set - context width configuration, we did an ex-
haustive search on the feature space, pruning away
features which were proven not useful by results at
stages. Table 3 presents results obtained for each
modality on 4-fold cross validation.
Modality Precision Recall F Measure
Ability 82.4 55.5 65.5
Effort 95.1 82.8 88.5
Intention 84.3 61.3 70.7
Success 93.2 76.6 83.8
Want 88.4 64.3 74.3
Overall 90.1 70.6 79.1
Table 3: Per modality results for best feature set ? on
4-fold cross validation on MTurk data
We also trained a model on the entire MTurk data
using the best feature set ? and evaluated it against
the Gold data. The results obtained for each modal-
ity on gold evaluation are given in Table 4. We at-
tribute the lower performance on the Gold dataset to
its difference from MTurk data. MTurk data is en-
tirely from email threads, whereas Gold data con-
tained sentences from newswire, letters and blogs
in addition to emails. Furthermore, the annotation
is different (Turkers vs expert). Finally, the distri-
bution of modalities in both datasets is very differ-
ent. For example, Ability modality was merely 6%
of MTurk data compared to 48% in Gold data (see
Table 2).
Modality Precision Recall F Measure
Ability 78.6 22.0 34.4
Effort 85.7 60.0 70.6
Intention 66.7 16.7 26.7
Success NA 0.0 NA
Want 92.3 50.0 64.9
Overall 72.1 29.5 41.9
Table 4: Per modality results for best feature set ? evalu-
ated on Gold dataset
We obtained reasonable performances for Effort
and Want modalities while the performance for other
modalities was rather low. Also, the Gold dataset
contained only 8 instances of Success, none of which
was recognized by the tagger resulting in a recall
of 0%. Precision (and, accordingly, F Measure) for
Success was considered ?not applicable? (NA), as no
such tag was assigned.
4.5 Annotation Confidence Experiments
Our MTurk data contains sentence for which at least
two of the three Turkers agreed on the modality and
the target of the modality. In this section, we investi-
gate the role of annotation confidence in training an
automatic tagger. The annotation confidence is de-
noted by whether an annotation was agreed by only
two annotators or was unanimous. We denote the set
of sentences for which only two annotators agreed as
Agr2 and that for which all three annotators agreed
as Agr3.
We present four training setups. The first setup
is Tr23 where we train a model using both Agr2
and Agr3 with equal weights. This is the setup we
used for results presented in the Section 4.4. Then,
we have Tr2 and Tr3, where we train using only
Agr2 and Agr3 respectively. Then, for Tr23W , we
61
TrainingSetup
Tested on Agr2 and Agr3 Tested on Agr3 only
Precision Recall F Measure Precision Recall F Measure
Tr23 90.1 70.6 79.1 95.9 86.8 91.1
Tr2 91.0 66.1 76.5 95.6 81.8 88.2
Tr3 88.1 52.3 65.6 96.8 71.7 82.3
Tr23W 89.9 70.5 79.0 95.8 86.5 90.9
Table 5: Annotator Confidence Experiment Results; the best results per column are boldfaced
(4-fold cross validation on MTurk Data)
train a model giving different cost values for Agr2
and Agr3 examples. The SVMLight package al-
lows users to input cost values ci for each training
instance separately.7 We tuned this cost value for
Agr2 and Agr3 examples and found the best value
at 20 and 30 respectively.
For all four setups, we used feature set ?. We per-
formed 4-fold cross validation on MTurk data in two
ways ? we tested against a combination of Agr2
and Agr3, and we tested against only Agr3. Results
of these experiments are presented in Table 5. We
also present the results of evaluating a tagger trained
on the whole MTurk data for each setup against the
Gold annotation in Table 6. The Tr23 tested on both
Agr2 andAgr3 presented in Table 5 and Tr23 tested
on Gold data presented in Table 6 correspond to the
results presented in Table 3 and Table 4 respectively.
TrainingSetup Precision Recall F Measure
Tr23 72.1 29.5 41.9
Tr2 67.4 27.6 39.2
Tr3 74.1 19.1 30.3
Tr23W 73.3 31.4 44.0
Table 6: Annotator Confidence Experiment Results; the
best results per column are boldfaced
(Evaluation against Gold)
One main observation is that including annota-
tions of lower agreement, but still above a threshold
(in our case, 66.7%), is definitely helpful. Tr23 out-
performed both Tr2 and Tr3 in both recall and F-
7This can be done by specifying ?cost:<value>? after the
label in each training instance. This feature has not yet been
documented on the SVMlight website.
measure in all evaluations. Also, even when evaluat-
ing against only the high confident Agr3 cases, Tr2
gave a high gain in recall (10 .1 percentage points)
over Tr3, with only a 1.2 percentage point loss on
precision. We conjecture that this is because there
are far more training instances in Tr2 than in Tr3
(674 vs 334), and that quantity beats quality.
Another important observation is the increase in
performance by using varied costs for Agr2 and
Agr3 examples (the Tr23W condition). Although
it dropped the performance by 0.1 to 0.2 points
in cross-validation F measure on the Enron cor-
pora, it gained 2.1 points in Gold evaluation F mea-
sure. These results seem to indicate that differential
weighting based on annotator agreement might have
more beneficial impact when training a model that
will be applied to a wide range of genres than when
training a model with genre-specific data for appli-
cation to data from the same genre. Put differently,
using varied costs prevents genre over-fitting. We
don?t have a full explanation for this difference in
behavior yet. We plan to explore this in future work.
5 Conclusion
We have presented an innovative way of combining
a high-recall simple tagger with Mechanical Turk
annotations to produce training data for a modality
tagger. We show that we obtain good performance
on the same genre as this training corpus (annotated
in the same manner), and reasonable performance
across genres (annotated by an independent expert).
We also present experiments utilizing the number of
agreeing Turkers to choose cost values for training
examples for the SVM. As future work, we plan to
extend this approach to other modalities which are
62
not covered in this study.
6 Acknowledgments
This work is supported, in part, by the Johns Hop-
kins Human Language Technology Center of Ex-
cellence. Any opinions, findings, and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the views of the sponsor. We thank several anony-
mous reviewers for their constructive feedback.
References
Emilia Apostolova, Noriko Tomuro, and Dina Demner-
Fushman. 2011. Automatic extraction of lexico-
syntactic patterns for detection of negation and spec-
ulation scopes. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies: short papers -
Volume 2, HLT ?11, pages 283?287, Portland, Oregon.
Eiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,
Tomoko Ohkuma, Hiroshi Mashuichi, and Kazuhiko
Ohe. 2009. Text2table: Medical text summarization
system based on named entity recognition and modal-
ity identification. In Proceedings of the BioNLP 2009
Workshop, pages 185?192, Boulder, Colorado, June.
Association for Computational Linguistics.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Nathaniel W. Filardo, Lori S. Levin, and Christine D.
Piatko. 2010. A modality lexicon and its use in auto-
matic tagging. In LREC.
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr,
Chris Callison-Burch, Nathaniel W. Filardo, Christine
Piatko, Lori Levin, and Scott Miller. 2012. Use of
modality and negation in semantically-informed syn-
tactic mt. Computational Linguistics, 38(22).
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of the 22nd Na-
tional Conference on Artificial intelligence - Volume 1,
pages 871?876, Vancouver, British Columbia, Canada.
AAAI Press.
Mona Diab, Bonnie Dorr, Lori Levin, Teruko Mitamura,
Rebecca Passonneau, Owen Rambow, and Lance
Ramshaw. 2009a. Language Understanding Anno-
tation Corpus. Linguistic Data Consortium (LDC),
USA.
Mona Diab, Lori Levin, Teruko Mitamura, Owen Ram-
bow, Vinodkumar Prabhakaran, and Weiwei Guo.
2009b. Committed belief annotation and tagging. In
Proceedings of the Third Linguistic Annotation Work-
shop, pages 68?73, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Szarvas,
Gyo?rgy Mo?ra, and Ja?nos Csirik, editors. 2010. Pro-
ceedings of the Fourteenth Conference on Computa-
tional Natural Language Learning. Association for
Computational Linguistics, Uppsala, Sweden, July.
Thorsten Joachims, 1999. Making large-scale support
vector machine learning practical, pages 169?184.
MIT Press, Cambridge, MA, USA.
Stefan Kaufmann, Cleo Condoravdi, and Valentina
Harizanov, 2006. Formal Approaches to Modality,
pages 72?106. Mouton de Gruyter.
Angelika Kratzer. 1981. The Notional Category of
Modality. In H. J. Eikmeyer and H. Rieser, editors,
Words, Worlds, and Contexts, pages 38?74. de Gruyter,
Berlin.
Angelika Kratzer. 1991. Modality. In Arnim von Ste-
chow and Dieter Wunderlich, editors, Semantics: An
International Handbook of Contemporary Research.
de Gruyter.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In 41st Meeting of the
Association for Computational Linguistics (ACL?03),
Sapporo, Japan.
Marjorie McShane, Sergei Nirenburg, and Ron
Zacharsky. 2004. Mood and modality: Out of
the theory and into the fray. Natural Language
Engineering, 19(1):57?89.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages 28?
36, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Toshiyuki
Kanamaru, and Hitoshi Isahara. 2005. Analysis of
machine translation systems? errors in tense, aspect,
and modality. In Proceedings of the 19th Asia-Pacific
Conference on Language, Information and Computa-
tion (PACLIC), Tapei.
Rowan Nairn, Cleo Condorovdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of the International Workshop on
Inference in Computational Semantics, ICoS-5, pages
66?76, Buxton, England.
M. F. Porter, 1997. An algorithm for suffix stripping,
pages 313?316. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA.
Vinodkumar Prabhakaran, Owen Rambow, and Mona
Diab. 2010. Automatic committed belief tagging.
In Coling 2010: Posters, pages 1014?1022, Beijing,
China, August. Coling 2010 Organizing Committee.
63
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
report, Oregon Health and Science University.
Roser Sauri and James Pustejovsky. 2009. Factbank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Sauri, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In FLAIRS Conference, pages 333?339.
Bengt Sigurd and Barbara Gawro?nska. 1994. Modals
as a problem for MT. In Proceedings of the 15th In-
ternational Conference on Computational Linguistics
(COLING) Volume 1, COLING ?94, pages 120?124,
Kyoto, Japan.
Johan Van Der Auwera and Andreas Ammann, 2005.
Overlap between situational and epistemic modal
marking, chapter 76, pages 310?313. Oxford Univer-
sity Press.
Veronika Vincze, Gy orgy Szarvas, Richa?d Farkas,
Gy orgy Mora, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9(Suppl 11):S9+.
64
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, page 49,
Baltimore, Maryland, USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Power of Confidence:
How Poll Scores Impact Topic Dynamics in Political Debates
Vinodkumar Prabhakaran
Dept. of Computer Science
Columbia University
New York, NY
vinod@cs.columbia.edu
Ashima Arora
Dept. of Computer Science
Columbia University
New York, NY
aa3470@columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY
rambow@ccls.columbia.edu
Abstract
In this paper, we investigate how topic dy-
namics during the course of an interaction
correlate with the power differences be-
tween its participants. We perform this
study on the US presidential debates and
show that a candidate?s power, modeled
after their poll scores, affects how often
he/she attempts to shift topics and whether
he/she succeeds. We ensure the validity
of topic shifts by confirming, through a
simple but effective method, that the turns
that shift topics provide substantive topi-
cal content to the interaction. A paper de-
scribing this work is published in the ACL
2014 Joint Workshop on Social Dynamics
and Personal Attributes in Social Media.
49
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 77?82,
Baltimore, Maryland USA, 27 June 2014.
c?2014 Association for Computational Linguistics
Power of Confidence:
How Poll Scores Impact Topic Dynamics in Political Debates
Vinodkumar Prabhakaran
Dept. of Computer Science
Columbia University
New York, NY
vinod@cs.columbia.edu
Ashima Arora
Dept. of Computer Science
Columbia University
New York, NY
aa3470@columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY
rambow@ccls.columbia.edu
Abstract
In this paper, we investigate how topic dy-
namics during the course of an interaction
correlate with the power differences be-
tween its participants. We perform this
study on the US presidential debates and
show that a candidate?s power, modeled
after their poll scores, affects how often
he/she attempts to shift topics and whether
he/she succeeds. We ensure the validity
of topic shifts by confirming, through a
simple but effective method, that the turns
that shift topics provide substantive topical
content to the interaction.
1 Introduction
Analyzing political speech has gathered great in-
terest within the NLP community. Researchers
have analyzed political text to identify markers of
persuasion (Guerini et al., 2008), predict voting
patterns (Thomas et al., 2006; Gerrish and Blei,
2011), and detect ideological positions (Sim et al.,
2013). Studies have also looked into how per-
sonal attributes of political personalities such as
charisma, confidence and power affect how they
interact (Rosenberg and Hirschberg, 2009; Prab-
hakaran et al., 2013b). Our work belongs to this
genre of studies. We analyze how a presidential
candidate?s power, modeled after his/her relative
poll standings, affect the dynamics of topic shifts
during the course of a presidential debate.
2 Motivation
In early work on correlating personal attributes
to political speech, Rosenberg and Hirschberg
(2009) analyzed speech transcripts in the con-
text of 2004 Democratic presidential primary elec-
tions, to identify prosodic and lexico-syntactic
cues that signal charisma of political personalities.
More recently, Prabhakaran et al. (2013a) intro-
duced the notion of power an election candidate
has at a certain point in the election campaign,
modeled after the confidence that stems from their
recent poll standings. They analyzed the 2012 Re-
publican presidential primary debates and found
that the candidate?s power at the time of a de-
bate impacts the structure of interactions (e.g., fre-
quency of turns and interruption patterns). They
followed up their study with an automatic ranker
to identify leading candidates based on the inter-
action within a debate (Prabhakaran et al., 2013b).
One of the interesting findings by Prabhakaran
et al. (2013a) was that candidates? power corre-
lates with the distribution of topics they speak
about in the debates. They found that when can-
didates have more power, they speak significantly
more about certain topics (e.g., economy) and less
about certain other topics (e.g., energy). However,
these findings relate to the specific election cycle
they analyzed and will not carry over to all polit-
ical debates in general. A topical dimension with
broader relevance is how topics change during the
course of an interaction (e.g., who introduces more
topics, who attempts to shift topics etc.). For in-
stance, Nguyen et al. (2013) found that topic shifts
within an interaction are correlated with the role
a participant plays in it (e.g., being a moderator).
They also analyzed US presidential debates, but
with the objective of validating a topic segmenta-
tion method they proposed earlier (Nguyen et al.,
2012). They do not study the topic shifting ten-
dencies among the candidates in relation to their
power differences.
In this paper, we bring these two ideas together.
We analyze the 2012 Republican presidential de-
bates, modeling the power of a candidate based
on poll scores as proposed by Prabhakaran et al.
(2013a) and investigate various features that cap-
ture the topical dynamics in the debates. We show
that the power affects how often candidates at-
77
Turn # Speaker Turn Text Substantive?
223 PAWLENTY (C) I support a constitutional amendment to define marriage between a man and
woman. I was the co-author of the state ? a law in Minnesota to define it
and now we have courts jumping over this.
[S]
224 KING (M) OK. Let?s just go through this. [NS]
225 PAUL (C) The federal government shouldn?t be involved. I wouldn?t support an
amendment. [...] I don?t think government should give us a license to
get married. It should be in the church.
[S]
226 KING (M) Governor Romney, constitutional amendment or state decision? [NS]
227 ROMNEY (C) Constitutional. [NS]
228 KING (M) Mr. Speaker? [NS]
229 GINGRICH (C) Well, I helped author the Defense of Marriage Act which the Obama ad-
ministration should be frankly protecting in court. [...]
[S]
[...]
235 CAIN (C) If I had my druthers, I never would have overturned ?don?t ask/don?t tell?
in the first place. [...] Our men and women have too many other things to
be concerned about rather than have to deal with that as a distraction.
[S]
[...]
240 KING (M) Leave it in place, [...] or overturn it? [S]
241 ROMNEY (C) Well, one, we ought to be talking about the economy and jobs. But given
the fact you?re insistent, the ? the answer is, I believe that ?don?t ask/don?t
tell? should have been kept in place until conflict was over.
[S]
Table 1: Excerpt from Goffstown, NH debate (06/13/11), discussing marriage equality and the ?Don?t Ask/Don?t Tell? policy
[S]/ [NS] denote substantiveness of turns
tempt to shift topics and whether they succeed in
it or not. In order to correctly model topic shifts,
we ensure that the shifts happen in turns that con-
tribute substantial topical content to the interac-
tion. We introduce the notion of a ?non-substantial
turn?, and use a simple, but effective method to au-
tomatically identify non-substantial turns. This al-
lows us to identify different topic segments within
the interaction, while permitting (and capturing)
interruptions within those segments. We will com-
pare the segments that we obtain with those by
Nguyen et al. (2012) in future work.
3 Domain and Data
We use the same corpus as Prabhakaran et al.
(2013b). The corpus contains manual transcripts
of 20 debates held between May 2011 and Febru-
ary 2012 as part of the 2012 Republican pres-
idential primaries. The transcripts are obtained
from The American Presidency Project.
1
Each
turn is clearly demarcated in the transcripts and
their speakers are identified. The turns in the cor-
pus are preprocessed using the Stanford CoreNLP
package to perform basic NLP steps such as tok-
enization, sentence segmentation, parts-of-speech
tagging and lemmatization. We show an excerpt
1
http://www.presidency.ucsb.edu/debates.php
from one of the debates in Table 1. This segment
of the debate discusses marriage equality followed
by the overturning of the ?Don?t Ask/Don?t Tell?
policy prohibiting openly gay, lesbian, or bisexual
persons from US military service.
Prabhakaran et al. (2013b) added each candi-
date?s power at the time of each debate to the cor-
pus, computed based on their relative standing in
recent public polls. We refer the reader to (Prab-
hakaran et al., 2013b) for the detailed description
of how the relative standings in national and state-
level polls from various sources are aggregated to
obtain candidates? power. The poll numbers cap-
ture how successful candidates are in convincing
the electorate of their candidature, which in turn
affects their confidence within the debates. These
debates serve as a rich domain to explore manifes-
tations of power since they are a medium through
which candidates pursue and maintain power over
other candidates.
4 Modeling Topics
Prabhakaran et al. (2013a) model topics in the de-
bates using Latent Dirichlet Allocation (LDA), as-
signing topic probabilities to each turn. The num-
ber of topics was set to be 15 and the topic that was
assigned the highest probability for a turn was cho-
78
sen as its topic. Assigning topics to each turn in
this manner, however, is problematic. Not all turns
by themselves contribute to the conversational top-
ics in an interaction. A large number of turns,
especially by the moderator, manage the conver-
sation rather than contribute content to it. These
include turns redirecting questions to specific can-
didates (e.g., turns 224, 226 and 228 in Table 1) as
well as moderator interruptions (e.g., ?Quickly.?,
?We have to save time?). Furthermore, some other
turns address a topic only when considered to-
gether with preceding turns, but not when read in
isolation. These include turns that are short one-
word answers (e.g., turn 227) and turns that are
uninterpretable without resolving anaphora (e.g.,
?That?s right?). While these turns are substantive
to human readers, topic modeling approaches such
as LDA cannot assign them topics correctly be-
cause of their terseness.
We define the turns that do not, in isolation, con-
tribute substantially to the conversational topics as
non-substantive turns. In order to obtain a gold
standard for non-substantivity, two of the authors
manually annotated each turn in one entire debate
(dated 06/13/11) as either substantive (S) or non-
substantive (NS). The annotators were instructed
not to consider the identity of the speaker or the
context of the turn (preceding/following turns) in
making their assessment. We obtained a high
inter-annotator agreement (observed agreement =
89.3%; Kappa = .76). We took the assessments
by one of the annotators as the gold standard, in
which 108 (31.5%) of the 343 turns were identi-
fied as non-substantive. We show the S vs. NS
assessments for each turn in column 4 of Table 1.
Figure 1a shows the line graph of topic proba-
bilities assigned by LDA to the sequence of turns
in Table 1. As the graph shows, non-substantive
turns are assigned spurious topic probabilities by
LDA. For example, turn 224 by KING (?OK. Lets
just go through this.?) was assigned small prob-
abilities for all topics; the highest of which was
economy (probability of 0.12). This error is prob-
lematic when modeling topic shifts, since this turn
and the next one by PAUL would have been incor-
rectly identified as shifts in topic from their cor-
responding previous turns. Instead, if we assume
that the non-substantive turns follow the same
topic probabilities as the most recent substantive
turn, we obtain the line graph shown in Figure 1b.
This topic assignment captures the topic dynam-
(a) Topic Probabilities assigned by LDA
(b) Topic Probabilities after ignoring non-substantive turns
Figure 1: Line graphs of topic probabilities for turns in
Table 1 (legend shows only the top 5 topics in this segment)
ics in the segment more accurately. It identifies
Gay Rights as the predominant topic until turn 234
followed by a mix of Gay Rights and Military as
topics while discussing the ?Don?t Ask/Don?t Tell?
policy. It also captures the attempt by ROMNEY
in turn 242 to shift the topic to Economy.
4.1 Identifying Non-substantive Turns
In order to automatically detect non-substantive
turns, we investigate a few alternatives. A simple
observation is that many of the NS turns such as
redirections of questions or short responses have
only a few words. We tried a word count thresh-
old based method (WC Thresh) where we assign
a turn to be NS if the number of tokens (words) in
the turn is less than a threshold. Another intuition
is that for a non-substantive turn, it would be hard
for the LDA to assign topics and hence all topics
will get almost equal probabilities assigned. In or-
der to capture this, we used a method based on a
standard deviation threshold (SD Thresh), where
we assign a turn to be NS if the standard deviation
of that turn?s topic probabilities is below a thresh-
old. We also used a combination system where
we tag a turn to be NS if either system tags it to
be. We tuned for the value of the thresholds and
the best performances obtained for each case are
shown in Table 2. We obtained the best results
for the WC Thresh method with a threshold of 28
words, while for SD Thresh the optimal threshold
is .13 (almost twice the mean).
79
Method Accuracy (%) F-measure
WC Thresh 82.6 73.7
SD Thresh 76.2 64.7
WC Thresh + SD Thresh 76.8 70.4
Table 2: Accuracy and F-measure of different methods to
identify non-substantive turns
4.2 Topic Assignments
We first ran the LDA at a turn-level for all debates,
keeping the number of topics to be 15, and se-
lected the best model after 2000 iterations. Then,
we ran the WC Thresh method described above to
detect NS turns. For all NS turns, we replace the
topic probabilities assigned by LDA with the last
substantive turn?s topic probabilities. Note that an
S turn coming after one or more NS turns could
still be of the same topic as the last S turn, i.e.,
non-substantivity of a turn is agnostic to whether
the topic changes after that or not. A topic shift (or
attempt) happens only when LDA assigns a differ-
ent topic to a substantive turn.
5 Topical Dimensions
We now describe various features we use to cap-
ture the topical dynamics within each debate, with
respect to each candidate. When we compute a
feature value, we use the topic probabilities as-
signed to each turn as described in the previous
section. For some features we only use the topic
with the highest probability, while for some oth-
ers, we use the probabilities assigned to all topics.
We consider features along four dimensions which
we describe in detail below.
5.1 Topic Shift Patterns
We build various features to capture how of-
ten a candidate stays on the topic being dis-
cussed. We say a candidate attempted to shift
the topic in a turn if the topic assigned to that
turn differs from the topic of the previous (sub-
stantive) turn. We use a feature to count the
number of times a candidate attempts to shift
topics within a debate (TS Attempt#) and a
version of that feature normalized over the to-
tal number of turns (TS Attempt#
N
). We also
use a variation of these features which consid-
ers only the instances of topic shift attempts by
the candidates when responding to a question
from the moderator (TS AttemptAfterMod# and
TS AttemptAfterMod#
N
). We also compute a
softer notion of topic shift where we measure the
average Euclidean distance between topic proba-
bilities of each of the candidate turns and turns
prior to them (EuclideanDist). This feature in
essence captures whether the candidate stayed on
topic, even if he/she did not completely switch
topics in a turn.
5.2 Topic Shift Sustenance Patterns
We use a feature to capture the average number
of turns for which topic shifts by a candidate was
sustained (TS SustTurns). However, as discussed
in Section 4, the turns vary greatly in terms of
length. A more sensible measure is the time pe-
riod for which a topic shift was sustained. We
approximate the time by the number of word to-
kens and compute the average number of tokens
in the turns that topic shifts by a candidate were
sustained (TS SustTime).
5.3 Topic Shift Success Patterns
We define a topic shift to be successful if it was
sustained for at least three turns. We compute
three features ? total number of successful topic
shifts by a candidate (TS Success#), that number
normalized over the total number of turns by the
candidate (TS Success#
N
), and the success rate of
candidate?s topic shifts (TS SuccessRate)
5.4 Topic Introduction Patterns
We also looked at cases where a candidate intro-
duces a new topic, i.e., shifts to a topic which
is entirely new for the debate. We use the num-
ber of topics introduced by a candidate as a fea-
ture (TS Intro#). We also use features to cap-
ture how important those topics were, measured
in terms of the number of turns about those top-
ics in the en tire debate (TS IntroImpTurns) and
the time spent on those topics in the entire debate
(TS IntroImpTime).
6 Analysis and Results
We performed a correlation analysis on the fea-
tures described in the previous section with re-
spect to each candidate against the power he/she
had at the time of the debate (based on recent poll
scores). Figure 2 shows the Pearson?s product cor-
relation between each topical feature and candi-
date?s power. Dark bars denote statistically signif-
icant (p < 0.05) features.
80
Figure 2: Pearson Correlations for Topical Features
We obtained significant strong positive correla-
tion for TS Attempt# and TS AttemptAfterMod#.
However, the normalized measure TS Attempt#
N
did not have any significant correlation, suggest-
ing that the correlation obtained for TS Attempt#
is mostly due to the fact that candidates with
more power have more turns, a finding that is al-
ready established by Prabhakaran et al. (2013b).
However, interestingly, we obtained a weak,
but statistically significant, negative correlation
for TS AttemptAfterMod#
N
which suggests that
more powerful candidates tend to stay on topic
when responding to moderators. We did not ob-
tain any correlation for EuclideanDist.
We did not obtain any significant correlations
between candidate?s power and their topic shift
sustenance features. We obtained significant cor-
relation for topic shift success (TS Success#),
modeled based on the sustenance of topic shifts,
suggesting that powerful candidates have a higher
number of successful topic shifts. However,
TS SuccessRate or TS Success#
N
did not obtain
any significant correlation. We also found that
powerful candidates are more likely to introduce
new topics (TS Intro#) and that the topics they in-
troduce tend to be important (TS IntroImpTurns
and TS IntroImpTime).
7 Related Work
Studies in sociolinguistics (e.g., (Ng et al., 1993;
Ng et al., 1995)) have explored how dialog struc-
ture in interactions relates to power and influence.
Reid and Ng (2000) identified that factors such as
frequency of contribution, proportion of turns, and
number of successful interruptions are important
indicators of influence. Within the dialog commu-
nity, researchers have studied notions of control
and initiative in dialogs (Walker and Whittaker,
1990; Jordan and Di Eugenio, 1997). Walker and
Whittaker (1990) define ?control of communica-
tion? in terms of whether the discourse partici-
pants are providing new, unsolicited information
in their utterances. Their notion of control dif-
fers from our notion of power; however, the way
we model topic shifts is closely related to their
utterance level control assignment. Within the
NLP community, researchers have studied power
and influence in various genres of interactions,
such as organizational email threads (Bramsen et
al., 2011; Gilbert, 2012; Prabhakaran and Ram-
bow, 2013), online discussion forums (Danescu-
Niculescu-Mizil et al., 2012; Biran et al., 2012)
and online chat dialogs (Strzalkowski et al., 2012).
The correlates analyzed in these studies range
from word/phrase patterns, to derivatives of such
patterns such as linguistic coordination, to deeper
dialogic features such as argumentation and dialog
acts. Our work differs from these studies in that
we study the correlates of power in topic dynam-
ics. Furthermore, we analyze spoken interactions.
8 Conclusion
We studied the topical dynamics in the 2012 US
presidential debates and investigated their corre-
lation with the power differences between candi-
dates. We showed that a candidate?s power, mod-
eled after their poll scores, has significant correla-
tion with how often he/she introduces new topics,
attempts to shift topics, and whether they succeed
in doing so. In order to ensure the validity of our
topic shifts we devised a simple yet effective way
to eliminate turns which do not provide substan-
tial topical content to the interaction. Furthermore,
this allowed us to identify different topic segments
within the interaction. In future work, we will ex-
plore how our way of identifying segments com-
pares to other approaches on topic segmentation
in interactions (e.g., (Nguyen et al., 2012)).
Acknowledgments
This paper is based upon work supported by the
DARPA DEFT Program. The views expressed are
those of the authors and do not reflect the official
policy or position of the Department of Defense
or the U.S. Government. We also thank Debanjan
Ghosh and several anonymous reviewers for their
constructive feedback.
81
References
Or Biran, Sara Rosenthal, Jacob Andreas, Kathleen
McKeown, and Owen Rambow. 2012. Detecting
influencers in written online conversations. In Pro-
ceedings of the Second Workshop on Language in
Social Media, pages 37?45, Montr?eal, Canada, June.
Association for Computational Linguistics.
Philip Bramsen, Martha Escobar-Molano, Ami Patel,
and Rafael Alonso. 2011. Extracting social power
relationships from natural language. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 773?782, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee,
Bo Pang, and Jon Kleinberg. 2012. Echoes of
power: language effects and power differences in
social interaction. In Proceedings of the 21st in-
ternational conference on World Wide Web, WWW
?12, New York, NY, USA. ACM.
Sean Gerrish and David Blei. 2011. Predicting legisla-
tive roll calls from text. In Lise Getoor and Tobias
Scheffer, editors, Proceedings of the 28th Interna-
tional Conference on Machine Learning, ICML ?11,
pages 489?496, New York, NY, USA, June. ACM.
Eric Gilbert. 2012. Phrases that signal workplace hier-
archy. In Proceedings of the ACM 2012 conference
on Computer Supported Cooperative Work, CSCW
?12, pages 1037?1046, New York, NY, USA. ACM.
Marco Guerini, Carlo Strapparava, and Oliviero Stock.
2008. Corps: A corpus of tagged political speeches
for persuasive communication processing. Journal
of Information Technology & Politics, 5(1):19?32.
Pamela W. Jordan and Barbara Di Eugenio. 1997.
Control and initiative in collaborative problem solv-
ing dialogues. In Working Notes of the AAAI Spring
Symposium on Computational Models for Mixed Ini-
tiative, pages 81?84.
Sik Hung Ng, Dean Bell, and Mark Brooke. 1993.
Gaining turns and achieving high in influence rank-
ing in small conversational groups. British Journal
of Social Psychology, pages 32, 265?275.
Sik Hung Ng, Mark Brooke, and Michael Dunne.
1995. Interruption and in influence in discussion
groups. Journal of Language and Social Psychol-
ogy, pages 14(4),369?381.
Viet-An Nguyen, Jordan Boyd-Graber, and Philip
Resnik. 2012. Sits: A hierarchical nonparametric
model using speaker identity for topic segmentation
in multiparty conversations. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
pages 78?87, Jeju Island, Korea, July. Association
for Computational Linguistics.
Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,
Deborah A. Cai, Jennifer E. Midberry, and Yuanxin
Wang. 2013. Modeling topic control to detect in-
fluence in conversations using nonparametric topic
models. Machine Learning, pages 1?41.
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written dialog and social power: Manifestations of
different types of power in dialog behavior. In Pro-
ceedings of the IJCNLP, pages 216?224, Nagoya,
Japan, October. Asian Federation of Natural Lan-
guage Processing.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013a. Power dynamics in spoken in-
teractions: a case study on 2012 republican primary
debates. In Proceedings of the 22nd international
conference on World Wide Web companion, pages
99?100. International World Wide Web Conferences
Steering Committee.
Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.
Seligmann. 2013b. Who had the upper hand? rank-
ing participants of interactions based on their rela-
tive power. In Proceedings of the IJCNLP, pages
365?373, Nagoya, Japan, October. Asian Federation
of Natural Language Processing.
Scott A. Reid and Sik Hung Ng. 2000. Conversation as
a resource for in influence: evidence for prototypical
arguments and social identification processes. Euro-
pean Journal of Social Psych., pages 30, 83?100.
Andrew Rosenberg and Julia Hirschberg. 2009.
Charisma perception from text and speech. Speech
Communication, 51(7):640?655.
Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and
Noah A. Smith. 2013. Measuring ideological pro-
portions in political speeches. In Proceedings of the
2013 Conference on EMNLP, pages 91?101, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
Tomek Strzalkowski, Samira Shaikh, Ting Liu,
George Aaron Broadwell, Jenny Stromer-Galley,
Sarah Taylor, Umit Boz, Veena Ravishankar, and
Xiaoai Ren. 2012. Modeling leadership and influ-
ence in multi-party online discourse. In Proceedings
of COLING, pages 2535?2552, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from
congressional floor-debate transcripts. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing, pages 327?335,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse
segmentation. In Proceedings of the 28th annual
meeting on Association for Computational Linguis-
tics, pages 70?78. Association for Computational
Linguistics.
82
