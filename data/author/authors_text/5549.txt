Shallow language processing architecture for Bulgarian
Hristo Tanev
ITC-Irst,
Centro per la Ricerca Scientifica e Tecnologica
Povo,Trento, Italy 38050
tanev@itc.it
Ruslan Mitkov
School of Humanities,
Languages and Social Studies
Wolverhampton WV1 1SB UK
R.Mitkov@wlv.ac.uk
Abstract
This paper describes LINGUA - an architec-
ture for text processing in Bulgarian. First, the
pre-processing modules for tokenisation, sen-
tence splitting, paragraph segmentation, part-
of-speech tagging, clause chunking and noun
phrase extraction are outlined. Next, the pa-
per proceeds to describe in more detail the
anaphora resolution module. Evaluation results
are reported for each processing task.
1 Introduction
The state of the art of today?s full parsing and
knowledge-based automatic analysis still falls
short of providing a reliable processing frame-
work for robust, real-world applications such
as automatic abstracting or information ex-
traction. The problem is especially acute for
languages which do not benefit from a wide
range of processing programs such as Bulgar-
ian. There have been various projects which
address different aspects of the automatic anal-
ysis in Bulgarian such as morphological analy-
sis (Krushkov, 1997), (Simov et al, 1992), mor-
phological disambiguation (Simov et al, 1992)
and parsing (Avgustinova et al, 1989), but no
previous work has pursued the development of
a knowledge-poor, robust processing environ-
ment with a high level of component integrity.
This paper reports the development and im-
plementation of a robust architecture for lan-
guage processing in Bulgarian referred to as
LINGUA, which includes modules for POS tag-
ging, sentence splitting, clause segmentation,
parsing and anaphora resolution. Our text pro-
cessing framework builds on the basis of con-
siderably shallower linguistic analysis of the in-
put, thus trading off depth of interpretation for
breadth of coverage and workable, robust solu-
tion. LINGUA uses knowledge poor, heuristi-
cally based algorithms for language analysis, in
this way getting round the lack of resources for
Bulgarian.
2 LINGUA - an architecture for
language processing in Bulgarian
LINGUA is a text processing framework for
Bulgarian which automatically performs tokeni-
sation, sentence splitting, part-of-speech tag-
ging, parsing, clause segmentation, section-
heading identification and resolution for third
person personal pronouns (Figure 1). All mod-
ules of LINGUA are original and purpose-
built, except for the module for morphological
analysis which uses Krushkov?s morphological
analyser BULMORPH (Krushkov, 1997). The
anaphora resolver is an adaptation for Bulgar-
ian of Mitkovs knowledge-poor pronoun resolu-
tion approach (Mitkov, 1998).
LINGUA was used in a number of projects
covering automatic text abridging, word seman-
tic extraction (Totkov and Tanev, 1999) and
term extraction. The following sections outline
the basic language processing functions, pro-
vided by the language engine.
2.1 Text segmentation: tokenisation,
sentence splitting and paragraph
identification
The first stage of every text processing task is
the segmentation of text in terms of tokens, sen-
tences and paragraphs.
LINGUA performs text segmentation by op-
erating within an input window of 30 tokens, ap-
plying rule-based algorithm for token synthesis,
sentence splitting and paragraph identification.
2.1.1 Tokenisation and token stapling
Tokens identified from the input text serve as
input to the token stapler. The token stapler
forms more complex tokens on the basis of a
Figure 1: General architecture of LINGUA
token grammar. With a view to improving to-
kenisation, a list of abbreviations has been in-
corporated into LINGUA.
2.1.2 Sentence splitting
LINGUA?s sentence splitter operates to iden-
tify sentence boundaries on the basis of 9 main
end-of-sentence rules and makes use of a list of
abbreviations. Some of the rules consist of sev-
eral finer sub-rules. The evaluation of the per-
formance of the sentence splitter on a text of
190 sentences reports a precision of 92% and
a recall of 99%. Abbreviated names such as
J.S.Simpson are filtered by special constraints.
The sentence splitting and tokenising rules were
adapted for English. The resulting sentence
splitter was then employed for identifying sen-
tence boundaries in the Wolverhampton Corpus
of Business English project.
2.1.3 Paragraph identification
Paragraph identification is based on heuristics
such as cue words, orthography and typograph-
ical markers. The precision of the paragraph
splitter is about 94% and the recall is 98% (Ta-
ble 3).
2.2 Morphological analysis and
part-of-speech tagging
2.2.1 Morphological analysis
Bulgarian morphology is complex, for exam-
ple the paradigm of a verb has over 50
forms. Krushkov?s morphological analyser
BULMORPH (Krushkov, 1997) is integrated in
the language engine with a view to processing
Bulgarian texts at morphological level.
2.2.2 Morphological disambiguation
The level of morphological ambiguity for
Bulgarian is not so high as it is in other
languages. As a guide, we measured the ratio:
Number of all tags/Number of all words.
The results show that this ratio is compara-
tively low and for a corpus of technical texts
of 9000 words the ratio tags per word is 1,26,
whereas for a 13000-word corpus from the genre
of fiction this ratio is 1,32. For other languages
such as Turkish this ratio is about 1,9 and for
certain English corpora 2,0 1.
We used 33 hand-crafted rules for disam-
biguation. Since large tagged corpora in Bul-
garian are not widely available, the development
of a corpus-based probabilistic tagger was an
unrealistic goal for us. However, as some stud-
ies suggest (Voutilainen, 1995), the precision of
rule-based taggers may exceed that of the prob-
abilistic ones.
2.3 Parsing
Seeking a robust flexible solution for pars-
ing we implemented two alternative approaches
in LINGUA: a fast-working NP extractor and
more general parser, which works more slowly,
but delivers better results both in accuracy and
coverage. As no syntactically annotated Bulgar-
ian corpora were available to us, using statistical
data to implement probabilistic algorithm was
not an option.
The NP extraction algorithm is capable of
analysing nested NPs, NPs which contain left
1Kemal Oflazer, personal communication
modifiers, prepositional phrases and coordinat-
ing phrases. The NP extractor is based on a
simple unification grammar for NPs and APs.
The recall of NP extraction, measured against
352 NPs from software manuals, was 77% and
the precision - 63.5%.
A second, better coverage parser was im-
plemented which employs a feature grammar
based on recent formal models for Bulgarian,
(Penchev, 1993), (Barkalova, 1997). All basic
types of phrases such as NP, AP, PP, VP and
AdvP are described in this grammar. The
parser is supported by a grammar compiler,
working on grammar description language
for representation of non context unification
grammars. For example one of the rules for
synthesis of NP phrases has the form:
NP (def :Y full art :F ext :+ rex :? nam :?)
?AP (gender :X def :Y full art:F number: L )
NP (ext:? def:? number:L gender:X rex:?)
The features and values in the rules are not
fixed sets and can be changed dynamically. The
flexibility of this description allows the gram-
mar to be extended easily. The parser uses a
chart bottom-up strategy, which allows for par-
tial parsing in case full syntactic tree cannot be
built over the sentence.
There are currently about 1900 syntac-
tic rules in the grammar which are encoded
through 70 syntactic formulae.
Small corpus of 600 phrases was syntactically
annotated by hand. We used this corpus to
measure the precision of the parsing algorithm
(Table 3).
We found that the precision of NP extraction
performed by the chart parser is higher than
the precision of the standalone NP extraction -
74.8% vs. 63.5% while the recall improves by
only 0.9% - 77.9% vs. 77% .
The syntactic ambiguity is resolved using syn-
tactic verb frames and heuristics, similar to the
ones described in (Allen, 1995).
The parser reaches its best performance for
NPs (74.8% precision and 77.9% recall) and low-
est for VPs (33% precision, 26% recall) and
Ss (20% precision and 5.9% recall) (Table 3).
The overall (measured on all the 600 syntac-
tic phrases) precision and recall, are 64.9% and
60.5% respectively. This is about 20% lower,
compared with certain English parsers (Murat
and Charniak, 1995), which is due to the in-
sufficient grammar coverage, as well as the lack
of reliable disambiguation algorithm. However
the bracket crossing accuracy is 80%, which is
comparable to some probabilistic approaches. It
should be noted that in our experiments we re-
stricted the maximal number of arcs up to 35000
per sentence to speed up the parsing.
3 Anaphora resolution in Bulgarian
3.1 Adaptation of Mitkovs
knowledge-poor approach for
Bulgarian
The anaphora resolution module is imple-
mented as the last stage of the language pro-
cessing architecture (Figure 1). This module re-
solves third-person personal pronouns and is an
adaptation of Mitkov?s robust, knowledge-poor
multilingual approach (Mitkov, 1998) whose lat-
est implementation by R. Evans is referred to
as MARS 2 (Orasan et al, 2000). MARS does
not make use of parsing, syntactic or seman-
tic constraints; nor does it employ any form
of non-linguistic knowledge. Instead, the ap-
proach relies on the efficiency of sentence split-
ting, part-of-speech tagging, noun phrase iden-
tification and the high performance of the an-
tecedent indicators; knowledge is limited to a
small noun phrase grammar, a list of (indicat-
ing) verbs and a set of antecedent indicators.
The core of the approach lies in activating the
antecedent indicators after filtering candidates
(from the current and two preceding sentences)
on the basis of gender and number agreement
and the candidate with the highest composite
score is proposed as antecedent 3. Before that,
the text is pre-processed by a sentence split-
ter which determines the sentence boundaries, a
part-of-speech tagger which identifies the parts
of speech and a simple phrasal grammar which
detects the noun phrases. In the case of com-
plex sentences, heuristic ?clause identification?
rules track the clause boundaries.
LINGUA performs the pre-processing,
needed as an input to the anaphora resolution
algorithm: sentence, paragraph and clause
splitters, NP grammar, part-of-speech tagger,
2MARS stands for Mitkov?s Anaphora Resolution
System.
3For a detailed procedure how candidates are handled
in the event of a tie, see (Mitkov, 1998).
Text Pronouns Weight set
Standard Optimised Baseline
most recent
Software manuals Success rate 221 75.0% 78.8% 58.0%
Critical succ. rate 70.0% 73.0% 54.0%
Non trivial succ. rate 70.0% 78.8% 58.0%
Tourist guides Success rate 116 68.1% 69.8% 65.0%
Critical succ. rate 63.3% 64.4% 58.8%
Non trivial succ. rate 67.2% 69.0% 65.0%
All texts Success rate 337 72.6% 75.7% 60.4%
Critical succ. rate 67.7% 70.0% 55.7%
Non trivial succ. rate 72.3% 75.4% 60.4%
Table 1: Success rate of anaphora resolution
section heading identification heuristics. Since
one of the indicators that Mitkov?s approach
uses is term preference, we manually developed4
a small term bank containing 80 terms from
the domains of programming languages, word
processing, computer hardware and operating
systems 5. This bank additionally featured 240
phrases containing these terms.
The antecedent indicators employed in
MARS are classified as boosting (such indica-
tors when pointing to a candidate, reward it
with a bonus since there is a good probability
of it being the antecedent) or impeding (such in-
dicators penalise a candidate since it does not
appear to have high chances of being the an-
tecedent). The majority of indicators are genre-
independent and are related to coherence phe-
nomena (such as salience and distance) or to
structural matches, whereas others are genre-
specific (e.g. term preference, immediate refer-
ence, sequential instructions). Most of the indi-
cators have been adopted in LINGUA without
modification from the original English version
(see (Mitkov, 1998) for more details). How-
ever, we have added 3 new indicators for Bul-
garian: selectional restriction pattern, adjectival
NPs and name preference.
The boosting indicators are
First Noun Phrases: A score of +1 is assigned
to the first NP in a sentence, since it is deemed
4This was done for experimental purposes. In future
applications, we envisage the incorporation of automatic
term extraction techniques.
5Note that MARS obtains terms automatically using
TF.IDF.
to be a good candidate for the antecedent.
Indicating verbs: A score of +1 is assigned
to those NPs immediately following the verb
which is a member of a previously defined set
such as discuss, present, summarise etc.
Lexical reiteration: A score of +2 is assigned
those NPs repeated twice or more in the
paragraph in which the pronoun appears, a
score of +1 is assigned to those NP, repeated
once in the paragraph.
Section heading preference: A score of +1 is
assigned to those NPs that also appear in the
heading of the section.
Collocation match: A score of +2 is assigned
to those NPs that have an identical collocation
pattern to the pronoun.
Immediate reference: A score of +2 is as-
signed to those NPs appearing in constructions
of the form ? ...V1 NP < CB > V2 it ? , where
< CB > is a clause boundary.
Sequential instructions: A score of +2 is
applied to NPs in the NP1 position of con-
structions of the form: ?To V1 NP1 ... To V2 it
...?
Term preference: a score of +1 is applied to
those NPs identified as representing domain
terms.
Selectional restriction pattern: a score of
Text Pronouns Intrasentential: Average Average Average Average
Intersentential number of distance from distance from distance from
anaphors candidates the antecedent the antecedent the antecedent
per anaphor in clauses in sentences in NP
Sofware 221 106 : 115 3.29 1.10 0.62 3.30
manuals
Tourist 116 17 : 99 3.35 1.74 0.98 5.13
guides
Table 2: Complexity of the evaluation data
+2 is applied to noun phrases occurring in
collocation with the verb preceding or following
the anaphor. This preference is different
from the collocation match preference in that
it operates on a wider range of ?selectional
restriction patterns? associated with a specific
verb 6 and not on exact lexical matching. If
the verb preceding or following the anaphor
is identified to be in a legitimate collocation
with a certain candidate for antecedent, that
candidate is boosted accordingly. As an il-
lustration, assume that ?Delete file? has been
identified as a legitimate collocation being a
frequent expression in a domain specific corpus
and consider the example ?Make sure you save
the file in the new directory. You can now
delete it. ? Whereas the ?standard? collocation
match will not be activated here, the selectional
restriction pattern will identify ?delete file? as
an acceptable construction and will reward the
candidate ?the file?.
Adjectival NP: a score of +1 is applied to
NPs which contain adjectives modifying the
head. Empirical analysis shows that Bulgarian
constructions of that type are more salient
than NPs consisting simply of a noun. Recent
experiments show that the success rate of the
anaphora resolution is improved by 2.20%,
using this indicator. It would be interesting
to establish if this indicator is applicable for
English.
Name preference: a score +2 is applied to
names of entities (person, organisation, product
6At the moment these patterns are extracted from a
list of frequent expressions involving the verb and do-
main terms in a purpose-built term bank but in gener-
ally they are automatically collected from large domain-
specific corpora.
names).
The impeding indicator is Prepositional
Noun Phrases: NPs appearing in prepositional
phrases are assigned a score of -1.
Two indicators, Referential distance and
Indefiniteness may increase or decrease a
candidate?s score.
Referential distance gives scores of +2 and
+1 for the NPs in the same and in the previous
sentence respectively, and -1 for the NPs two
sentences back. This indicator has strong influ-
ence on the anaphora resolution performance,
especially in the genre of technical manuals.
Experiments show that its switching off can
decrease the success rate by 26% .
Indefiniteness assigns a score of -1 to indefi-
nite NPs, 0 to the definite (not full article) and
+1 to these which are definite, containing the
definite ?full? article in Bulgarian.
4 Evaluation of the anaphora
resolution module
The precision of anaphora resolution measured
on corpus of software manuals containing 221
anaphors, is 75.0%. Given that the anaphora
resolution system operates in a fully automatic
mode, this result could be considered very
satisfactory. It should be noted that some of
the errors arise from inaccuracy of the pre-
processing modules such as clause segmentation
and NP extraction (see Table 3).
We also evaluated the anaphora resolution
system in the genre of tourist texts. As ex-
pected, the success rate dropped to 68.1%
which, however, can still be regarded as a very
Language processing module Precision % Recall % Evaluation data
sentence splitter 92.00 99.00 190 sentences
paragraph splitter 94.00 98.00 268 paragraphs
clause chunker 93.50 93.10 232 clauses
POS tagger 95.00 95.00 303 POS tags
NP extractor 63.50 77.00 352 NPs
chart parsing
NP 74.84 77.89 294 NPs
AP 65.15 67.19 64 APs
AdvP 37.14 50.00 26 AdvPs
VP 33.33 26.39 72 VPs
PP 70.00 60.21 93 PPs
S 20.00 5.88 51 Ss
Total 64.93 60.50 600 phrases
Bracket crossing accuracy 80.33 - 600 phrases
Anaphora resolution 72.60 - 337 anaphors
Table 3: Summary of LINGUA performance
good result, given the fact that neither man-
ual pre-editing of the input text, nor any post-
editing of the output of the pre-processing tools
were undertaken. The main reason for the de-
cline of performance is that some of the origi-
nal indicators such as term preference, immedi-
ate reference and sequential instructions of the
knowledge-poor approach, are genre specific.
The software manuals corpus featured 221
anaphoric third person pronouns, whereas the
tourist text consisted of 116 such pronouns. For
our evaluation we used the measures success
rate, critical success rate and non-trivial suc-
cess rate (Mitkov, 2001). Success rate is the
ratio SR = AC/A, where AC is the number of
correctly resolved and A is the number of all
anaphors. Critical success rate is the success
rate for the anaphors which have more than one
candidates for antecedent after the gender and
number agreement filter is applied. Non-trivial
success rate is calculated for those anaphors
which have more than one candidates for an-
tecedent before the gender and number agree-
ment is applied. We also compared our ap-
proach with the typical baseline model Baseline
most recent which takes as antecedent the most
recent NP matching the anaphor in gender and
number. The results are shown in the Table 1.
These results show that the performance of
LINGUA in anaphora resolution is comparable
to that of MARS (Orasan et al, 2000). An opti-
mised version 7 of the indicator weights scored a
success rate of 69,8% on the tourist guide texts,
thus yielding an improvement of 6,1%.
Table 2 illustrates the complexity of the eval-
uation data by providing simple quantifying
measures such as average number of candi-
dates per anaphor, average distance from the
anaphor to the antecedent in terms of sentences,
clauses, intervening NPs, number of intrasen-
tential anaphors as opposed to intersentential
ones etc.
5 Conclusion
This paper outlines the development of the
first robust and shallow text processing frame-
work in Bulgarian LINGUA which includes
modules for tokenisation, sentence splitting,
paragraph segmentation, part-of-speech tag-
ging, clause chunking, noun phrases extraction
and anaphora resolution (Figure 1). Apart
from the module on pronoun resolution which
was adapted from Mitkov?s knowledge-poor ap-
proach for English and the incorporation of
BULMORPH in the part-of-speech tagger, all
modules were specially built for LINGUA. The
evaluation shows promising results for each of
the modules.
7The optimisation made use of genetic algorithms in
a manner similar to that described in (Orasan et al,
2000).
References
J. Allen. 1995. Natural Language Understand-
ing. The Benjamin/Cummings Publishing
Company, Inc.
T. Avgustinova, K. Oliva, and E. Paskaleva.
1989. An HPSG-based parser for bulgar-
ian. In International Seminar on Machine
Translation ?Computer and Translation 89?,
Moscow, Russia.
P. Barkalova. 1997. Bulgarian syntax - known
and unknown. Plovdiv University Press,
Plovdiv. in Bulgarian.
H. Krushkov. 1997. Modelling and building of
machine dictionaries and morphological pro-
cessors. Ph.D. thesis, University of Plovdiv.
in Bulgarian.
R. Mitkov. 1998. Robust pronoun reso-
lution with limited knowledge. In Pro-
ceedings of the 18.th International Confer-
ence on Computational Linguistics (COL-
ING?98)/ACL?98 Conference, pages 869?875,
Montreal,Canada.
R. Mitkov. 2001. Towards a more consistent
and comprehensive evaluation of anaphora
resolution algorithms and systems. Towards
a more consistent and comprehensive evalu-
ation of anaphora resolution algorithms and
systems, (15):253?276.
E. Murat and E. Charniak. 1995. A statistical
syntactic disambiguation program and what
it learns. CS, 29-95.
C. Orasan, R. Evans, and R. Mitkov. 2000.
Enhancing preference-based anaphora resolu-
tion with genetic algorithms. In Proceedings
of NLP?2000, Patras, Greece.
J. Penchev. 1993. Bulgarian Syntax - Govern-
ment and Binding. Plovdiv University Press,
Plovdiv. in Bulgarian.
K. Simov, E. Paskaleva, M. Damova, and
M. Slavcheva. 1992. Morpho-assistant - a
knowledge based system for bulgarian mor-
phology. In Proceedings of the Third Confer-
ence on Applied Natural Language Process-
ing, Trento, Italy.
G. Totkov and Ch. Tanev. 1999. Computerized
extraction of word semantics through con-
nected text analysis. In Proc. of the Interna-
tional Workshop DIALOGUE ?99, pages 360
? 365.
A. Voutilainen. 1995. A syntax-based part-of-
speech tagger. In Proceedings of the 7th con-
ference of the European Chapter of EACL,
Dublin, Ireland.
135
136
137
138
Introduction to the Special Issue on 
Computational Anaphora Resolution 
Ruslan Mitkov* 
University of Wolverhampton 
Shalom Lappin* 
King's College, London 
Branimir Boguraev* 
IBM T. J. Watson Research Center 
Anaphora accounts for cohesion in texts and is a phenomenon under active study 
in formal and computational linguistics alike. The correct interpretation of anaphora 
is vital for natural anguage processing (NLP). For example, anaphora resolution is 
a key task in natural anguage interfaces, machine translation, text summarization, 
information extraction, question answering, and a number of other NLP applications. 
After considerable initial research, followed by years of relative silence in the early 
1980s, anaphora resolution has attracted the attention of many researchers in the last 10 
years and a great deal of successful work on the topic has been carried out. Discourse- 
oriented theories and formalisms uch as Discourse Representation Theory and Cen- 
tering Theory inspired new research on the computational treatment of anaphora. The 
drive toward corpus-based robust NLP solutions further stimulated interest in alterna- 
tive and/or data-enriched approaches. Last, but not least, application-driven research 
in areas uch as automatic abstracting and information extraction i dependently high- 
lighted the importance of anaphora nd coreference r solution, boosting research in 
this area. 
Much of the earlier work in anaphora resolution heavily exploited omain and lin- 
guistic knowledge (Sidner 1979; Carter 1987; Rich and LuperFoy 1988; Carbonell and 
Brown 1988), which was difficult both to represent and to process, and which required 
considerable human input. However, the pressing need for the development of robust 
and inexpensive solutions to meet the demands of practical NLP systems encouraged 
many researchers tomove away from extensive domain and linguistic knowledge and 
to embark instead upon knowledge-poor anaphora resolution strategies. A number of 
proposals in the 1990s deliberately imited the extent o which they relied on domain 
and/or linguistic knowledge and reported promising results in knowledge-poor per- 
ational environments (Dagan and Itai 1990, 1991; Lappin and Leass 1994; Nasukawa 
1994; Kennedy and Boguraev 1996; Williams, Harvey, and Preston 1996; Baldwin 1997; 
Mitkov 1996, 1998b). 
The drive toward knowledge-poor and robust approaches was further motivated 
by the emergence of cheaper and more reliable corpus-based NLP tools such as part- 
of-speech taggers and shallow parsers, alongside the increasing availability of corpora 
and other NLP resources (e.g., ontologies). In fact, the availability of corpora, both raw 
and annotated with coreferential links, provided a strong impetus to anaphora resolu- 
* School of Humanities, Language and Social Sciences, Stafford Street, Wolverhampton WV1 1SB, UK. 
E-maih r.mitkov@wlv.ac.uk 
t 30 Saw Mill River Road, Hawthorne, NY 10532, USA. E-mail: bkb@watson.ibm.com 
~: Department of Computer Science, King's College, The Strand, London WC2R 2LS, UK. 
E-mail: lappin@dcs.kcl.ac.uk 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
tion with regard to both training and evaluation. Corpora (especially when annotated) 
are an invaluable source not only for empirical research but also for automated learning 
(e.g., machine learning) methods aiming to develop new rules and approaches; they 
also provide an important resource for evaluation of the implemented approaches. 
From simple co-occurrence rules (Dagan and Itai 1990) through training decision trees 
to identify anaphor-antecedent pairs (Aone and Bennett 1995) to genetic algorithms to 
optimize the resolution factors (Or~san, Evans, and Mitkov 2000), the successful per- 
formance of more and more modern approaches was made possible by the availability 
of suitable corpora. 
While the shift toward knowledge-poor strategies and the use of corpora repre- 
sented the main trends of anaphora resolution in the 1990s, there are other signifi- 
cant highlights in recent anaphora resolution research. The inclusion of the corefer- 
ence task in the Sixth and Seventh Message Understanding Conferences (MUC-6 and 
MUC-7) gave a considerable impetus to the development of coreference resolution 
algorithms and systems, such as those described in Baldwin et al (1995), Gaizauskas 
and Humphreys (1996), and Kameyama (1997). The last decade of the 20th century 
saw a number of anaphora resolution projects for languages other than English such as 
French, German, Japanese, Spanish, Portuguese, and Turkish. Against the background 
of a growing interest in multilingual NLP, multilingual anaphora/coreference reso- 
lution has gained considerable momentum in recent years (Aone and McKee 1993; 
Azzam, Humphreys, and Gaizauskas 1998; Harabagiu and Maiorano 2000; Mitkov 
and Barbu 2000; Mitkov 1999; Mitkov and Stys 1997; Mitkov, Belguith, and Stys 1998). 
Other milestones of recent research include the deployment of probabilistic and ma- 
chine learning techniques (Aone and Bennett 1995; Kehler 1997; Ge, Hale, and Char- 
niak 1998; Cardie and Wagstaff 1999; the continuing interest in centering, used either 
in original or in revised form (Abra~os and Lopes 1994; Strube and Hahn 1996; Hahn 
and Strube 1997; Tetreault 1999); and proposals related to the evaluation methodology 
in anaphora resolution (Mitkov 1998a, 2001b). For a more detailed survey of the state 
of the art in anaphora resolution, see Mitkov (forthcoming). 
The papers published in this issue reflect he major trends in anaphora resolution 
in recent years. Some of them describe approaches that do not exploit full syntactic 
knowledge (as in the case of Palomar et al's and Stuckardt's work) or that employ 
machine learning techniques (Soon, Ng, and Lira); others present centering-based pro- 
noun resolution (Tetreault) or discuss theoretical centering issues (Kibble). Almost all 
of the papers feature extensive valuation (including comparative valuation as in 
the case of Tetreault's and Palomar et al's work) or discuss general evaluation issues 
(Byron as well as Stuckardt). 
Palomar et al's paper describes an approach that works from the output of a 
partial parser and handles third person personal, demonstrative, reflexive, and zero 
pronouns, featuring among other things syntactic onditions on Spanish NP-pronoun 
noncoreference and an enhanced set of resolution preferences. The authors also im- 
plement several known methods and compare their performance with that of their 
own algorithm. An indirect conclusion from this work is that an algorithm requires 
semantic knowledge in order to hope for a success rate higher than 75%. 
Soon, Ng, and Lira describe a C5-based learning approach to coreference resolu- 
tion of noun phrases in unrestricted text. The approach learns from a small, annotated 
corpus and tackles pronouns, proper names, and definite descriptions. The coreference 
resolution module is part of a larger coreference resolution system that also includes 
sentence segmentation, tokenization, morphological analysis, part-of-speech tagging, 
noun phrase identification, named entity recognition, and semantic lass determina- 
tion (via WordNet). The evaluation is carried out on the MUC-6 and MUC-7 test 
474 
Mitkov, Boguraev, and Lappin Anaphora Resolution: Introduction 
corpora. The paper reports on experiments aimed at quantifying the contribution of 
each resolution factor and features error analysis. 
Stuckardt's work presents an anaphor esolution algorithm for systems where only 
partial syntactic information is available. Stuckardt applies Government and Bind- 
ing Theory principles A, B, and C to the task of coreference resolution on partially 
parsed texts. He also argues that evaluation of anaphora resolution systems hould 
take into account several factors beyond simple accuracy of resolution. In particular, 
both developer-oriented (e.g., related to the selection of optimal resolution factors) 
and application-oriented (e.g., related to the requirement of the application, as in the 
case of information extraction, where a proper name antecedent is needed) evaluation 
metrics should be considered. 
Tetreault's contribution features comparative valuation involving the author's 
own centering-based pronoun resolution algorithm called the Left-Right Centering 
algorithm (LRC) as well as three other pronoun resolution methods: Hobbs's naive 
algorithm (Hobbs 1978), BFP (Brennan, Friedman, and Pollard 1987), and Strube's S- 
list approach (Strube 1998). The LRC is an alternative to the original BFP algorithm in 
that it processes utterances incrementally. It works by first searching for an antecedent 
in the current sentence; if none can be found, it continues the search on the Cf-list of 
the previous and the other preceding utterances in a left-to-right fashion. 
In her squib, Byron maintains that additional kinds of information should be 
included in an evaluation in order to make the performance of algorithms on pronoun 
resolution more transparent. In particular, she suggests that the pronoun coverage be 
explicitly reported and proposes that the evaluation details be presented in a concise 
and compact tabular format called standard isclosure. Byron also proposes ameasure, 
the resolution rate, which is computed as the number of pronouns resolved correctly 
divided by the number of (only) referential pronouns. 
Finally, in his squib Kibble discusses a reformulation of the centering transitions 
(Continue, Retain, and Shift), which specify the center movement across sentences. 
Instead of defining a total preference ordering, Kibble argues that a partial ordering 
emerges from the interaction among cohesion (maintaining the same center), salience 
(realizing the center as subject), and cheapness (realizing the anticipated center of a 
following utterance as subject). 
The last years have seen considerable advances in the field of anaphora resolution, 
but a number of outstanding issues either remain unsolved or need more attention 
and, as a consequence, represent major challenges to the further development of the 
field (Mitkov 2001a). A fundamental question that needs further investigation is how 
far the performance of anaphora resolution algorithms can go and what the limitations 
of knowledge-poor methods are. In particular, more research should be carried out on 
the factors influencing the performance of these algorithms. One of the impediments 
to the evaluation or fuller utilization of machine learning techniques is the lack of 
widely available corpora annotated for anaphoric or coreferential links. More work 
toward the proposal of consistent and comprehensive evaluation is necessary; so too 
is work in multilingual contexts. Some of these challenges have been addressed in the 
papers published in this issue, but ongoing research will continue to address them in 
the near future. 
References 
Abra~os, Jose and Jos6 Lopes. 1994. 
Extending DRT with a focusing 
mechanism for pronominal anaphora nd 
ellipsis resolution. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING'94), pages 1128-1132, 
Kyoto, Japan. 
Aone, Chinatsu and Scott Bennett. 1995. 
Evaluating automated and manual 
475 
Computational Linguistics Volume 27, Number 4 
acquisition of anaphora resolution 
strategies. In Proceedings ofthe 33rd Annual 
Meeting of the Association for Computational 
Linguistics (ACU95), pages 122-129, Las 
Cruces, NM. 
Aone, Chinatsu and Douglas McKee. 1993. 
A language-independent anaphora 
resolution system for understanding 
multilingual texts. In Proceedings ofthe 31st 
Annual Meeting of the Association for 
Computational Linguistics (ACU93), 
pages 156-163, Columbus, OH. 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Coreference 
resolution in a multilingual information 
extraction. In Proceedings ofa Workshop on 
Linguistic Coreference, Granada, Spain. 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In 
Proceedings ofthe ACU97/EACU97 
Workshop on Operational Factors in Practical, 
Robust Anaphora Resolution for Unrestricted 
Texts, pages 38-45, Madrid, Spain. 
Baldwin, Breck, Jeff Reynar, Mike Collins, 
Jason Eisner, Adwait Ratnaparki, Joseph 
Rosenzweig, Anoop Sarkar, and Srivinas 
Bangalore. 1995. Description of the 
University of Pennsylvania system used 
for MUC-6. In Proceedings ofthe Sixth 
Message Understanding Conference 
(MUC-6), pages 177-191, Columbia, MD. 
Brennan, Susan, Marilyn Friedman, and 
Carl Pollard. 1987. A centering approach 
to pronouns. In Proceedings ofthe 25th 
Annual Meeting of the Association for 
Computational Linguistics (ACU87), 
pages 155-162, Stanford, CA. 
Carbonell, Jaime and Ralf Brown. 1988. 
Anaphora resolution: A multi-strategy 
approach. In Proceedings ofthe 12th 
International Conference on Computational 
Linguistics (COLING'88), volume 1, 
pages 96-101, Budapest, 
Hungary. 
Cardie, Claire and Kiri Wagstaff. 1999. 
Noun phrase coreference asclustering. In 
Proceedings ofthe 1999 Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, 
pages 82-89, College Park, MD. 
Carter, David M. 1987. Interpreting Anaphors 
in Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), volume 3, pages 1-3, 
Helsinki, Finland. 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artifi'cial Intelligence and Computer 
Vision. Elsevier Science Publishers B.V. 
(North-Holland), Amsterdam, pages 
125-135. 
Gaizauskas, Robert and Kevin Humphreys. 
1996. Quantitative evaluation of 
coreference algorithms in an information 
extraction system. Presented at Discourse 
Anaphora nd Anaphor Resolution 
Colloquium (DAARC-1), Lancaster, UK. 
Reprinted in Simon Botley and Tony 
McEnery, editors, Corpus-Based and 
Computational Approaches to Discourse 
Anaphora. John Benjamins, Amsterdam, 
2000, pages 143-167. 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Very Large Corpora, 
pages 161-170, Montreal, Canada. 
Hahn, Udo and Michael Strube. 1997. 
Centering-in-the-large: Computing 
referential discourse segments. In 
Proceedings ofthe 35th Annual Meeting of the 
Association for Computational Linguistics 
(ACU97/EACU97), pages 104-111, 
Madrid, Spain. 
Harabagiu, Sanda and Steven Maiorano. 
2000. Multilingual coreference r solution. 
In Proceedings ofConference on Applied 
Natural Language Processing~North American 
Chapter of the Association for Computational 
Linguistics (ANLP-NAACL2000), pages 
142-149, Seattle, WA. 
Hobbs, Jerry. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kameyama, Megumi. 1997. Recognizing 
referential links: An information 
extraction perspective. In Proceedings ofthe 
ACU97/EACL'97 Workshop on Operational 
Factors in Practical, Robust Anaphora 
Resolution for Unrestricted Texts, 
pages 46-53, Madrid, Spain. 
Kehler, Andrew. 1997. Probabilistic 
coreference in information extraction. In 
Proceedings ofthe 2nd Conference on 
Empirical Methods in Natural Language 
Processing (EMNLP-2), pages 163-173, 
Providence, RI. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen, Denmark. 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
476 
Mitkov, Boguraev, and Lappin Anaphora Resolution: Introduction 
resolution. Computational Linguistics, 
20(4):535-561. 
Mitkov, Ruslan. 1996. Pronoun resolution: 
The practical alternative. Presented at the 
Discourse Anaphora nd Anaphor 
Resolution Colloquium (DAARC-1), 
Lancaster, UK. Reprinted in Simon Botley 
and Tony McEnery, editors, Corpus-Based 
and Computational Approaches to Discourse 
Anaphora. John Benjamins, Amsterdam, 
2000, 189-212. 
Mitkov, Ruslan. 1998a. Evaluating anaphora 
resolution approaches. In Proceedings ofthe 
Discourse Anaphora nd Anaphora Resolution 
Colloquium (DAARC-2), Lancaster, UK. 
Mitkov, Ruslan. 1998b. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
the 17th International Conference on 
Computational Linguistics 
(COLING'98/ACU98), pages 869-875, 
Montreal, Canada. 
Mitkov, Ruslan. 1999. Multilingual anaphora 
resolution. Machine Translation, 
14(3-4):281-299. 
Mitkov, Ruslan. 2001a. Outstanding issues 
in anaphora resolution. In Alexander 
Gelbukh, editor, Computational Linguistics 
and Intelligent Text Processing. Springer, 
Berlin, pages 110-125. 
Mitkov, Ruslan. 2001b. Towards a more 
consistent and comprehensive evaluation 
of anaphora resolution algorithms and 
systems. Applied Artificial Intelligence: An 
International Journal, 15:253-276. 
Mitkov, Ruslan. Forthcoming. Anaphora 
Resolution. Longman, Harlow, UK. 
Mitkov, Ruslan, Lamia Belguith, and 
Malgorzata Stys. 1998. Multilingual robust 
anaphora resolution. In Proceedings ofthe 
Third International Conference on Empirical 
Methods in Natural Language Processing 
(EMNLP-3), pages 7-16, Granada, Spain. 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark, Bulgaria. 
Mitkov, Ruslan and Catalina Barbu. 2000. 
Improving pronoun resolution in two 
languages by means of bilingual corpora. 
In Proceedings ofthe Discourse, Anaphora nd 
Reference Resolution Conference (DAARC 
2000), pages 133-137, Lancaster, UK. 
Nasukawa, Tetsuya. 1994. Robust method of 
pronoun resolution using full-text 
information. In Proceedings ofthe 15th 
International Conference on Computational 
Linguistics (COLING'94), pages 1157-1163, 
Kyoto, Japan. 
Or~san, Constantin, Richard Evans, and 
Ruslan Mitkov. 2000. Enhancing 
preference-based anaphora resolution 
with genetic algorithms. In Proceedings of
NLP-2000, pages 185-195, Patras, Greece. 
Rich, Elaine and Susann LuperFoy. 1988. An 
architecture for anaphora resolution. In 
Proceedings ofthe Second Conference on 
Applied Natural Language Processing 
(ANLP-2), pages 18-24, Austin, TX. 
Sidner, Candace. 1979. Toward a 
computational theory of definite anaphora 
comprehension in English. Technical 
Report AI-TR-537, MIT, Cambridge, MA. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics (COLING'98/ACL'98), 
pages 1251-1257, Montreal, Canada. 
Strube, Michael and Udo Hahn. 1996. 
Functional centering. In Proceedings ofthe 
34th Annual Meeting of the Association for 
Computational Linguistics (ACL'96), 
pages 270-277, Santa Cruz, CA. 
Tetreault, Joel. 1999. Analysis of 
syntax-based pronoun resolution 
methods. In Proceedings ofthe 37th Annual 
Meeting of the Association for Computational 
Linguistics (ACL'99), pages 602-605, 
College Park, MD. 
Williams, Sandra, Mark Harvey, and Keith 
Preston. 1996. Rule-based reference 
resolution for unrestricted text using 
part-of-speech tagging and noun phrase 
parsing. In Proceedings ofthe Discourse 
Anaphora nd Anaphora Resolution 
Colloquium (DAARC-1), pages 441-456, 
Lancaster, UK. 
477 

Evaluation tool for rule-based anaphora resolution methods
Catalina Barbu
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
Stafford Street
Wolverhampton WV1 1SB
United Kingdom
c.barbu@wlv.ac.uk
Ruslan Mitkov
School of Humanities, Languages
and Social Sciences
University of Wolverhampton
Stafford Street
Wolverhampton WV1 1SB
United Kingdom
r.mitkov@wlv.ac.uk
Abstract
In this paper we argue that comparative
evaluation in anaphora resolution
has to be performed using the same
pre-processing tools and on the same
set of data. The paper proposes an
evaluation environment for comparing
anaphora resolution algorithms which
is illustrated by presenting the results
of the comparative evaluation of
three methods on the basis of several
evaluation measures.
1 Introduction
The evaluation of any NLP algorithm or system
should indicate not only its efficiency or
performance, but should also help us discover
what a new approach brings to the current state
of play in the field. To this end, a comparative
evaluation with other well-known or similar
approaches would be highly desirable.
We have already voiced concern (Mitkov,
1998a), (Mitkov, 2000b) that the evaluation of
anaphora resolution algorithms and systems is
bereft of any common ground for comparison due
not only to the difference of the evaluation data,
but also due to the diversity of pre-processing
tools employed by each anaphora resolution
system. The evaluation picture would not
be accurate even if we compared anaphora
resolution systems on the basis of the same data
since the pre-processing errors which would
be carried over to the systems? outputs might
vary. As a way forward we have proposed
the idea of the evaluation workbench (Mitkov,
2000b) - an open-ended architecture which
allows the incorporation of different algorithms
and their comparison on the basis of the same
pre-processing tools and the same data. Our
paper discusses a particular configuration of this
new evaluation environment incorporating three
approaches sharing a common ?knowledge-poor
philosophy?: Kennedy and Boguraev?s (1996)
parser-free algorithm, Baldwin?s (1997) CogNiac
and Mitkov?s (1998b) knowledge-poor approach.
2 The evaluation workbench for
anaphora resolution
In order to secure a ?fair?, consistent and
accurate evaluation environment, and to
address the problems identified above, we
have developed an evaluation workbench for
anaphora resolution which allows the comparison
of anaphora resolution approaches sharing
common principles (e.g. similar pre-processing
or resolution strategy). The workbench enables
the ?plugging in? and testing of anaphora
resolution algorithms on the basis of the same
pre-processing tools and data. This development
is a time-consuming task, given that we have to
re-implement most of the algorithms, but it is
expected to achieve a clearer assessment of the
advantages and disadvantages of the different
approaches. Developing our own evaluation
environment (and even reimplementing some
of the key algorithms) also alleviates the
impracticalities associated with obtaining the
codes of original programs.
Another advantage of the evaluation
workbench is that all approaches incorporated
can operate either in a fully automatic mode
or on human annotated corpora. We believe
that this is a consistent way forward because it
would not be fair to compare the success rate of
an approach which operates on texts which are
perfectly analysed by humans, with the success
rate of an anaphora resolution system which
has to process the text at different levels before
activating its anaphora resolution algorithm. In
fact, the evaluations of many anaphora resolution
approaches have focused on the accuracy of
resolution algorithms and have not taken into
consideration the possible errors which inevitably
occur in the pre-processing stage. In the real-
world, fully automatic resolution must deal
with a number of hard pre-processing problems
such as morphological analysis/POS tagging,
named entity recognition, unknown word
recognition, NP extraction, parsing, identification
of pleonastic pronouns, selectional constraints,
etc. Each one of these tasks introduces errors and
thus contributes to a drop in the performance of
the anaphora resolution system.1 As a result, the
vast majority of anaphora resolution approaches
rely on some kind of pre-editing of the text which
is fed to the resolution algorithm, and some of
the methods have only been manually simulated.
By way of illustration, Hobbs? naive approach
(1976; 1978) was not implemented in its original
version. In (Dagan and Itai, 1990; Dagan and
Itai, 1991; Aone and Bennett, 1995; Kennedy
and Boguraev, 1996) pleonastic pronouns are
removed manually2 , whereas in (Mitkov, 1998b;
Ferrandez et al, 1997) the outputs of the part-of-
speech tagger and the NP extractor/ partial parser
are post-edited similarly to Lappin and Leass
(1994) where the output of the Slot Unification
Grammar parser is corrected manually. Finally,
Ge at al?s (1998) and Tetrault?s systems (1999)
1For instance, the accuracy of tasks such as robust
parsing and identification of pleonastic pronouns is far below
100% See (Mitkov, 2001) for a detailed discussion.
2In addition, Dagan and Itai (1991) undertook additional
pre-editing such as the removal of sentences for which the
parser failed to produce a reasonable parse, cases where
the antecedent was not an NP etc.; Kennedy and Boguraev
(1996) manually removed 30 occurrences of pleonastic
pronouns (which could not be recognised by their pleonastic
recogniser) as well as 6 occurrences of it which referred to a
VP or prepositional constituent.
make use of annotated corpora and thus do not
perform any pre-processing. One of the very
few systems3 that is fully automatic is MARS,
the latest version of Mitkov?s knowledge-poor
approach implemented by Evans. Recent work
on this project has demonstrated that fully
automatic anaphora resolution is more difficult
than previous work has suggested (Ora?san et al,
2000).
2.1 Pre-processing tools
Parser
The current version of the evaluation
workbench employs one of the high performance
?super-taggers? for English - Conexor?s FDG
Parser (Tapanainen and Ja?rvinen, 1997). This
super-tagger gives morphological information
and the syntactic roles of words (in most of
the cases). It also performs a surface syntactic
parsing of the text using dependency links that
show the head-modifier relations between words.
This kind of information is used for extracting
complex NPs.
In the table below the output of the FDG parser
run over the sentence: ?This is an input file.? is
shown.
1 This this subj:>2 @SUBJ PRON SG
2 is be main:>0 @+FMAINV V
3 an an det:>5 @DN> DET SG
4 input input attr:>5 @A> N SG
5 file file comp:>2 @PCOMPL-S N SG
$.
$<s>
Example 1: FDG output for the text This is an
input file.
Noun phrase extractor
Although FDG does not identify the noun
phrases in the text, the dependencies established
between words have played an important role in
building a noun phrase extractor. In the example
above, the dependency relations help identifying
the sequence ?an input file?. Every noun phrase
is associated with some features as identified
by FDG (number, part of speech, grammatical
function) and also the linear position of the verb
that they are arguments of, and the number of
the sentence they appear in. The result of the NP
3Apart from MUC coreference resolution systems which
operated in a fully automatic mode.
extractor is an XML annotated file. We chose
this format for several reasons: it is easily read,
it allows a unified treatment of the files used for
training and of those used for evaluation (which
are already annotated in XML format) and it is
also useful if the file submitted for analysis to
FDG already contains an XML annotation; in
the latter case, keeping the FDG format together
with the previous XML annotation would lead
to a more difficult processing of the input file.
It also keeps the implementation of the actual
workbench independent of the pre-processing
tools, meaning that any shallow parser can be
used instead of FDG, as long as its output is
converted to an agreed XML format.
An example of the overall output of the pre-
processing tools is given below.
<P><S><w ID=0 SENT=0 PAR=1 LEMMA="this" DEP="2"
GFUN="SUBJ" POS="PRON" NR="SG">This</w><w ID=1
SENT=0 PAR=1 LEMMA="be" DEP="0" GFUN="+FMAINV"
POS="V"> is </w><COREF ID="ref1"><NP> <w ID=2
SENT=0 PAR=1 LEMMA="an" DEP="5" GFUN="DN" POS="DET"
NR="SG">an </w> <w ID=3 SENT=0 PAR=1 LEMMA="input"
DEP="5" GFUN="A" POS="N" NR="SG">input</w><w ID=4
SENT=0 PAR=1 LEMMA="file" DEP="2" GFUN="PCOMPL"
POS="N" NR="SG">file</w> </NP></COREF><w ID=5
SENT=0 PAR=1 LEMMA="." POS="PUNCT">.</w> </s>
<s><COREF ID="ref2" REF="ref1"><NP><w ID=0 SENT=1
PAR=1 LEMMA="it" DEP="2" GFUN="SUBJ" POS="PRON"> It
</w></NP></COREF> <w ID=1 SENT=1 PAR=1 LEMMA="be"
DEP="3" GFUN="+FAUXV" POS="V">is </w><w ID=2 SENT=1
PAR=1 LEMMA="use" DEP="0" GFUN="-FMAINV" POS="EN">
used</w><w ID=3 SENT=1 PAR=1 LEMMA="for" DEP="3"
GFUN="ADVL" POS="PREP">for</w> <NP><w ID=4 SENT=1
PAR=1 LEMMA="evaluation" DEP="4" GFUN="PCOMP"
POS="N"> evaluation</w></NP> <w ID=5 SENT=0 PAR=1
LEMMA="." POS="PUNCT">.</w></s></p>
Example 2: File obtained as result of the pre-
processing stage (includes previous coreference
an-notation) for the text This is an input file. It
is used for evaluation.
2.2 Shared resources
The three algorithms implemented receive as
input a representation of the input file. This
representation is generated by running an
XML parser over the file resulting from the
pre-processing phase. A list of noun phrases is
explicitly kept in the file representation. Each
entry in this list consists of a record containing:
? the word form
? the lemma of the word or of the head of the
noun phrase
? the starting position in the text
? the ending position in the text
? the part of speech
? the grammatical function
? the index of the sentence that contains the
referent
? the index of the verb whose argument this
referent is
Each of the algorithms implemented for
the workbench enriches this set of data with
information relevant to its particular needs.
Kennedy and Boguraev (1996), for example,
need additional information about whether a
certain discourse referent is embedded or not,
plus a pointer to the COREF class associated to
the referent, while Mitkov?s approach needs a
score associated to each noun phrase.
Apart from the pre-processing tools, the
implementation of the algorithms included in the
workbench is built upon a common program-
ming interface, which allows for some basic
processing functions to be shared as well. An
example is the morphological filter applied over
the set of possible antecedents of an anaphor.
2.3 Usability of the workbench
The evaluation workbench is easy to use. The
user is presented with a friendly graphical
interface that helps minimise the effort involved
in preparing the tests. The only information
she/he has to enter is the address (machine
and directory) of the FDG parser and the
file annotated with coreferential links to be
processed. The results can be either specific to
each method or specific to the file submitted
for processing, and are displayed separately
for each method. These include lists of the
pronouns and their identified antecedents in
the context they appear as well as information
as to whether they were correctly solved or
not. In addition, the values obtained for the
four evaluation measures (see section 3.2) and
several statistical results characteristic of each
method (e.g. average number of candidates
for antecedents per anaphor) are computed.
Separately, the statistical values related to the
annotated file are displayed in a table. We should
note that (even though this is not the intended
usage of the workbench) a user can also submit
unannotated files for processing. In this case,
the algorithms display the antecedent found for
each pronoun, but no automatic evaluation can be
carried out due to the lack of annotated testing
data.
2.4 Envisaged extensions
While the workbench is based on the FDG
shallow parser at the moment, we plan to update
the environment in such a way that two different
modes will be available: one making use of
a shallow parser (for approaches operating on
partial analysis) and one employing a full parser
(for algorithms making use of full analysis).
Future versions of the workbench will include
access to semantic information (WordNet) to
accommodate approaches incorporating such
types of knowledge.
3 Comparative evaluation of
knowledge-poor anaphora resolution
approaches
The first phase of our project included
comparison of knowledge-poorer approaches
which share a common pre-processing
philosophy. We selected for comparative
evaluation three approaches extensively cited in
the literature: Kennedy and Boguraev?s parser-
free version of Lappin and Leass? RAP (Kennedy
and Boguraev, 1996), Baldwin?s pronoun
resolution method (Baldwin, 1997) and Mitkov?s
knowledge-poor pronoun resolution approach
(Mitkov, 1998b). All three of these algorithms
share a similar pre-processing methodology: they
do not rely on a parser to process the input and
instead use POS taggers and NP extractors; nor
do any of the methods make use of semantic
or real-world knowledge. We re-implemented
all three algorithms based on their original
description and personal consultation with the
authors to avoid misinterpretations. Since the
original version of CogNiac is non-robust and
resolves only anaphors that obey certain rules, for
fairer and comparable results we implemented the
?resolve-all? version as described in (Baldwin,
1997). Although for the current experiments
we have only included three knowledge-poor
anaphora resolvers, it has to be emphasised that
the current implementation of the workbench
does not restrict in any way the number or
the type of the anaphora resolution methods
included. Its modularity allows any such method
to be added in the system, as long as the pre-
processing tools necessary for that method are
available.
3.1 Brief outline of the three approaches
All three approaches fall into the category of
factor-based algorithms which typically employ
a number of factors (preferences, in the case
of these three approaches) after morphological
agreement checks.
Kennedy and Boguraev
Kennedy and Boguraev (1996) describe an
algorithm for anaphora resolution based on
Lappin and Leass? (1994) approach but without
employing deep syntactic parsing. Their method
has been applied to personal pronouns, reflexives
and possessives. The general idea is to construct
coreference equivalence classes that have an
associated value based on a set of ten factors. An
attempt is then made to resolve every pronoun to
one of the previous introduced discourse referents
by taking into account the salience value of the
class to which each possible antecedent belongs.
Baldwin?s Cogniac
CogNiac (Baldwin, 1997) is a knowledge-
poor approach to anaphora resolution based
on a set of high confidence rules which are
successively applied over the pronoun under
consideration. The rules are ordered according
to their importance and relevance to anaphora
resolution. The processing of a pronoun stops
when one rule is satisfied. The original version
of the algorithm is non-robust, a pronoun being
resolved only if one of the rules is applied. The
author also describes a robust extension of the
algorithm, which employs two more weak rules
that have to be applied if all the others fail.
Mitkov?s approach
Mitkov?s approach (Mitkov, 1998b) is a
robust anaphora resolution method for technical
texts which is based on a set of boosting and
impeding indicators applied to each candidate
for antecedent. The boosting indicators assign
a positive score to an NP, reflecting a positive
likelihood that it is the antecedent of the current
pronoun. In contrast, the impeding ones apply
a negative score to an NP, reflecting a lack of
confidence that it is the antecedent of the current
pronoun. A score is calculated based on these
indicators and the discourse referent with the
highest aggregate value is selected as antecedent.
3.2 Evaluation measures used
The workbench incorporates an automatic scoring
system operating on an XML input file where the
correct antecedents for every anaphor have been
marked. The annotation scheme recognised by
the system at this moment is MUC, but support
for the MATE annotation scheme is currently
under developement as well.
We have implemented four measures for
evaluation: precision and recall as defined by
Aone and Bennett (1995)4 as well as success rate
and critical success rate as defined in (Mitkov,
2000a). These four measures are calculated as
follows:
? Precision = number of correctly resolved
anaphor / number of anaphors attempted to
be resolved
? Recall = number of correctly resolved
anaphors / number of all anaphors identified
by the system
? Success rate = number of correctly resolved
anaphors / number of all anaphors
? Critical success rate = number of correctly
resolved anaphors / number of anaphors
with more than one antecedent after a
morphological filter was applied
The last measure is an important criterion
for evaluating the efficiency of a factor-based
anaphora resolution algorithm in the ?critical
cases? where agreement constraints alone cannot
point to the antecedent. It is logical to assume
that good anaphora resolution approaches should
4This definition is slightly different from the one used in
(Baldwin, 1997) and (Gaizauskas and Humphreys, 2000).
For more discussion on this see (Mitkov, 2000a; Mitkov,
2000b).
have high critical success rates which are close
to the overall success rates. In fact, in most cases
it is really the critical success rate that matters:
high critical success rates naturally imply high
overall success rates.
Besides the evaluation system, the workbench
also incorporates a basic statistical calculator
which addresses (to a certain extent) the question
as to how reliable or realistic the obtained
performance figures are - the latter depending on
the nature of the data used for evaluation. Some
evaluation data may contain anaphors which are
more difficult to resolve, such as anaphors that
are (slightly) ambiguous and require real-world
knowledge for their resolution, or anaphors that
have a high number of competing candidates, or
that have their antecedents far away both in terms
of sentences/clauses and in terms of number of
?intervening? NPs etc. Therefore, we suggest that
in addition to the evaluation results, information
should be provided in the evaluation data as to
how difficult the anaphors are to resolve.5 To this
end, we are working towards the development of
suitable and practical measures for quantifying
the average ?resolution complexity? of the
anaphors in a certain text. For the time being, we
believe that simple statistics such as the number
of anaphors with more than one candidate,
and more generally, the average number of
candidates per anaphor, or statistics showing the
average distance between the anaphors and their
antecedents, could serve as initial quantifying
measures (see Table 2). We believe that these
statistics would be more indicative of how ?easy?
or ?difficult? the evaluation data is, and should
be provided in addition to the information on the
numbers or types of anaphors (e.g. intrasentential
vs. intersentential) occurring or coverage (e.g.
personal, possessive, reflexive pronouns in the
case of pronominal anaphora) in the evaluation
data.
3.3 Evaluation results
We have used a corpus of technical texts manually
annotated for coreference. We have decided on
5To a certain extent, the critical success rate defined
above addresses this issue in the evaluation of anaphora
resolution algorithms by providing the success rate for the
anaphors that are more difficult to resolve.
Success Rate Precision
File Number of
words
Number of
pronouns
Anaphoric
pronouns Mitkov Cogniac K&B Mitkov Cogniac K&B
ACC 9617 182 160 52.34% 45.0% 55.0% 42.85% 37.18% 48.35%
WIN 2773 51 47 55.31% 44.64% 63.82% 50.98% 41.17% 58.82%
BEO 6392 92 70 48.57% 42.85% 55.71% 36.95% 32.60% 42.39%
CDR 9490 97 85 71.76% 67.05% 74.11% 62.88% 58.76% 64.95%
Total 28272 422 362 56.9% 49.72% 61.6% 48.81% 42.65% 52.84%
Table 1: Evaluation results
Average referential distance
File Pronouns Personal Possesive Reflexive Intrasentential
anaphors Sentences NPs
Average no of
antecedents
ACC 182 161 18 3 90 1.2 4.2 9.4
WIN 51 40 11 0 41 1.1 4.1 11.9
BEO 92 74 18 0 56 1.4 5.1 12.9
CDR 97 85 10 2 54 1.4 3.7 9.2
Total 422 360 57 5 241 1.275 4.275 10.85
Table 2: Statistical results
this genre because both Kennedy&Boguraev and
Mitkov report results obtained on technical texts.
The corpus contains 28,272 words, with
19,305 noun phrases and 422 pronouns, out of
which 362 are anaphoric. The files that were
used are: ?Beowulf HOW TO? (referred in Table
1 as BEO), ?Linux CD-Rom HOW TO? (CDR),
?Access HOW TO? (ACC), ?Windows Help file?
(WIN). The evaluation files were pre-processed
to remove irrelevant information that might alter
the quality of the evaluation (tables, sequences
of code, tables of contents, tables of references).
The texts were annotated for full coreferential
chains using a slightly modified version of
the MUC annotation scheme. All instances of
identity-of-reference direct nominal anaphora
were annotated. The annotation was performed
by two people in order to minimize human errors
in the testing data (see (Mitkov et al, 2000) for
further details).
Table 1 describes the values obtained for the
success rate and precision6 of the three anaphora
resolvers on the evaluation corpus. The overall
success rate calculated for the 422 pronouns
found in the texts was 56.9% for Mitkov?s
method, 49.72% for Cogniac and 61.6% for
Kennedy and Boguraev?s method.
Table 2 presents statistical results on the
evaluation corpus, including distribution of
6Note that, since the three approaches are robust, recall
is equal to precision.
pronouns, referential distance, average number of
candidates for antecedent per pronoun and types
of anaphors.7
As expected, the results reported in Table 1
do not match the original results published by
Kennedy and Boguraev (1996), Baldwin (1997)
and Mitkov (1998b) where the algorithms were
tested on different data, employed different
pre-processing tools, resorted to different degrees
of manual intervention and thus provided no
common ground for any reliable comparison.
By contrast, the evaluation workbench enables
a uniform and balanced comparison of the
algorithms in that (i) the evaluation is done on
the same data and (ii) each algorithm employs
the same pre-processing tools and performs
the resolution in fully automatic fashion. Our
experiments also confirm the finding of Orasan,
Evans and Mitkov (2000) that fully automatic
resolution is more difficult than previously
thought with the performance of all the three
algorithms essentially lower than originally
reported.
4 Conclusion
We believe that the evaluation workbench for
anaphora resolution proposed in this paper
7In Tables 1 and 2, only pronouns that are treated
as anaphoric and hence tried to be resolved by the three
methods are included. Therefore, pronouns in first and
second person singular and plural and demonstratives do not
appear as part of the number of pronouns.
alleviates a long-standing weakness in the area
of anaphora resolution: the inability to fairly
and consistently compare anaphora resolution
algorithms due not only to the difference of
evaluation data used, but also to the diversity of
pre-processing tools employed by each system.
In addition to providing a common ground for
comparison, our evaluation environment ensures
that there is fairness in terms of comparing
approaches that operate at the same level of
automation: formerly it has not been possible
to establish a correct comparative picture due to
the fact that while some approaches have been
tested in a fully automatic mode, others have
benefited from post-edited input or from a pre- (or
manually) tagged corpus. Finally, the evaluation
workbench is very helpful in analysing the
data used for evaluation by providing insightful
statistics.
References
Chinatsu Aone and Scot W. Bennett. 1995.
Evaluating automated and manual acquisition of
anaphora resolution rules. In Proceedings of
the 33th Annual Meeting of the Association for
Computational Linguistics (ACL ?95), pages 122?
129.
Breck Baldwin. 1997. Cogniac: High precision
coreference with limited knowledge and linguistic
resources. In R. Mitkov and B. Boguraev, editors,
Operational factors in practical, robust anaphora
resolution for unrestricted texts, pages 38 ? 45.
Ido Dagan and Alon Itai. 1990. Automatic
processing of large corpora for the resolution
of anaphora references. In Proceedings of the
13th International Conference on Computational
Linguistics (COLING?90), volume III, pages 1?3.
Ido Dagan and Alon Itai. 1991. A statistical filter for
resolving pronoun references. In Y.A. Feldman and
A. Bruckstein, editors, Artificial Intelligence and
Computer Vision, pages 125 ? 135. Elsevier Science
Publishers B.V.
Antonio Ferrandez, Manolo Palomar, and L. Moreno.
1997. Slot unification grammar and anaphora
resolution. In Proceedings of the International
Conference on Recent Advances in Natural
Language Proceeding (RANLP?97), pages 294?
299.
Robert Gaizauskas and Kevin Humphreys. 2000.
Quantitative evaluation of coreference algorithms in
an information extraction system. In Simon Botley
and Antony Mark McEnery, editors, Corpus-
based and Computational Approaches to Discourse
Anaphora, Studies in Corpus Linguistics, chapter 8,
pages 145 ? 169. John Benjamins Publishing
Company.
Niyu Ge, J. Hale, and E. Charniak. 1998. A statistical
approach to anaphora resolution. In Proceedings
of the Sixth Workshop on Very Large Corpora,
COLING-ACL ?98, pages 161 ? 170, Montreal,
Canada.
Jerry Hobbs. 1976. Pronoun resolution. Research
report 76-1, City College, City University of New
York.
Jerry Hobbs. 1978. Pronoun resolution. Lingua,
44:339?352.
Christopher Kennedy and Branimir Boguraev. 1996.
Anaphora for everyone: pronominal anaphora
resolution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics (COLING?96), pages 113?118,
Copenhagen, Denmark.
Shalom Lappin and H.J. Leass. 1994. An
algorithm for pronominal anaphora resolution.
Computational Linguistics, 20(4):535 ? 562.
Ruslan Mitkov, R. Evans, C. Orasan, C. Barbu,
L. Jones, and V. Sotirova. 2000. Coreference
and anaphora: developing annotating tools,
annotated resources and annotation strategies.
In Proceedings of the Discourse, Anaphora and
Reference Resolution Conference (DAARC2000),
pages 49?58, Lancaster, UK.
Ruslan Mitkov. 1998a. Evaluating anaphora
resolution approaches. In Proceedings of the
Discourse Anaphora and Anaphora Resolution
Colloquium (DAARC?2), pages 164 ? 172,
Lancaster, UK.
Ruslan Mitkov. 1998b. Robust pronoun resolution
with limited knowledge. In Proceedings of the
18th International Conference on Computational
Linguistics (COLING?98/ACL?98, pages 867 ? 875.
Morgan Kaufmann.
Ruslan Mitkov. 2000a. Towards a more consistent and
comprehensive evaluation of anaphora resolution
algorithms and systems. In Proceedings of the
Discourse, Anaphora and Reference Resolution
Conference (DAARC2000), pages 96 ? 107,
Lancaster, UK.
Ruslan Mitkov. 2000b. Towards more comprehensive
evaluation in anaphora resolution. In Proceedings
of the Second International Conference on
Language Resources and Evaluation, volume III,
pages 1309 ? 1314, Athens, Greece.
Ruslan Mitkov. 2001. Outstanding issues in anaphora
resolution. In Al. Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing, pages
110?125. Springer.
Constantin Ora?san, Richard Evans, and Ruslan
Mitkov. 2000. Enhancing preference-based
anaphora resolution with genetic algorithms. In
Proceedings of Natural Language Processing -
NLP2000, pages 185 ? 195. Springer.
P. Tapanainen and T. Ja?rvinen. 1997. A non-
projective dependency parser. In Proceedings of
the 5th Conference of Applied Natural Language
Processing, pages 64 ? 71, Washington D.C., USA.
Joel R. Tetreault. 1999. Analysis of syntax-based
pronoun resolution methods. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics (ACL ?99), pages 602 ?
605, Maryland, USA.
  
Computer-Aided Generation of Multiple-Choice Tests 
 
Ruslan Mitkov, Le An Ha 
School of Humanities, Languages and Social Sciences 
University of Wolverhampton, WV1 1SB 
Email {r.mitkov, l.a.ha}@wlv.ac.uk 
 
Abstract  
This paper describes a novel computer-aided 
procedure for generating multiple-choice tests from 
electronic instructional documents. In addition to 
employing various NLP techniques including term 
extraction and shallow parsing, the program makes 
use of language resources such as a corpus and 
WordNet. The system generates test questions and 
distractors, offering the user the option to post-edit 
the test items. 
1. Introduction 
Multiple-choice tests have proved to be an efficient tool 
for measuring students' achievement.1 The manual 
construction of such tests, however, is a time-
consuming and labour-intensive task. 
In this paper we seek to provide an alternative to the 
lengthy and demanding activity of developing multiple-
choice tests and propose a new, NLP-based approach 
for generating tests from narrative texts (textbooks, 
encyclopaedias).  The approach uses a simple set of 
transformational rules, a shallow parser, automatic term 
extraction, word sense disambiguation, a corpus and 
WordNet. While in the current experiment we have used 
an electronic textbook in linguistics to automatically 
generate test items in this area, we should note that the 
methodology is general and can be extended to 
practically any other area. 
To the best of our knowledge, no related work has 
been reported addressing such a type of application.2 
                                                           
1 This work is not concerned with (and does not discuss) 
the issue of whether multiple-choice tests are better 
assessment methodology that other types of tests. What 
it focuses on is a new NLP methodology to generate 
multiple-choice tests about facts explicitly stated in a 
text. 
2 Fairon (1999) reports that their exercises ?can take the 
appearance of a multiple choice test? (if distractors are 
added), but does not explain exactly as to how this can 
be done. 
2. NLP-based methodology for generation 
of multiple-choice test items 
The proposed methodology for generating multiple-
choice test items is based on the premise that questions 
should focus on key concepts rather than addressing less 
central and even irrelevant concepts or ideas. Therefore 
the first stage of the procedure is to identify domain-
specific terms which serve as ?anchors? of each 
question. By way of example, syntax is a prime 
candidate for a domain-specific term in the sentence 
"Syntax is the branch of linguistics which studies the 
way words are put together into sentences". This 
sentence can be then transformed into questions asking 
about this term such as "Which branch of linguistics 
studies the way words are put together into sentences?" 
or "Which discipline studies the way words are put 
together into sentences?" both of which can act as stems 
in multiple-choice test items. 
Another important premise is that distractors3 
should be as semantically close to the correct answer as 
possible so that no additional clues are provided for the 
students. Semantically close distractors are more 
plausible and therefore better at distinguishing good, 
confident students from poor and uncertain ones. In the 
above example, the distractors for the correct answer 
syntax should preferably be semantics or pragmatics 
and not chemistry or football, for instance.  
In order to keep the test item comprehensible and 
avoid additional complexity, the test questions are 
generated from declarative sentences using simple 
transformational rules which, in turn, results in only 
minimal change of the original wording. 
Underpinned by the above principles, a system for 
computer-aided generation of multiple-choice test items 
from instructional documents in electronic form has 
been implemented. The system is built on separate 
components, which perform the following tasks: (i) term 
extraction, (ii) selection of distractors and (iii) question 
generation.  
                                                           
3 Known also as ?distracters? in the literature of classical 
test theory. 
  
2.1 Term extraction 
To retrieve terms, nouns and noun phrases are first 
identified, using the FDG shallow parser (Tapanainen 
and J?rvinen 1997).  Next, their frequency is counted 
and sorted, and nouns with a frequency over a certain 
threshold4 are considered as key terms. In addition, noun 
phrases having these key terms as heads, and satisfying 
the regular expression [AN]+N or [AN]*NP[AN]*N 
(Justeson and Katz 1996), are considered as terms. 
Although this method is very simple,5 the results show 
that, for this particular application, the performance is 
more than acceptable (only 3 questions did not address a 
domain-specific term). One of the main reasons not to 
employ more complicated methods for term extraction 
derives from the small size of the corpus used in the 
current experiment (10 000 words). 
It should be noted that, from a keyword, as in the 
case of the keyword "phrase", a list of semantically 
close terms including noun phrase, verb phrase, 
adjective phrase and adverb phrase can be obtained. In 
addition, a word sense disambiguation program is used 
to identify the correct sense of the alternatives given 
that WordNet frequently returns an unnecessarily high 
number of senses. The word sense disambiguation 
algorithm compares the definition of sense (as extracted 
from WordNet) and the context of the keyword (words 
around the keyword in the corpus). 
As an illustration, in the following extract (Kies 
2003) 
 
(1) A prepositional phrase at the beginning of a 
sentence constitutes an introductory modifier. 
 
one of the terms identified is introductory modifier 
which can serve as an ?anchor? for generating  the test 
question. 
2.2 Selection of distractors 
WordNet is consulted to compute concepts semantically 
close to the correct answer/concept which can then be 
selected as distractors. WordNet retrieves hypernyms, 
hyponyms, and coordinates of the term, if applicable. If 
WordNet returns too many concepts, those appearing in 
the corpus are given preference. If, as in (1), the term is 
                                                           
4 For this particular project the threshold has been 
determined through experiments. The value of the 
threshold of course depends on a number of parameters 
such as the size of the corpus, number of nouns etc.  
5 We experimented with the tf.idf method for key term 
extraction and noted that while precision is slightly 
higher, recall is much lower. As the time needed to 
validate a question is much less than the time needed to 
produce it, we believe that the recall rate is more 
important. 
a noun phrase and WordNet fails to return any 
semantically close concept, the corpus is searched for 
noun phrases with the same head which are then used as 
distractors.6 As an illustration, the electronic textbook 
contains the following noun phrases with modifier as the 
head, each one of which can act as a distractor: modifier 
that accompanies a noun, associated modifier, 
misplaced modifier. As a result, the program generates 
the following multiple-choice test item: 
 
(2) What does a prepositional phrase at the 
beginning of a sentence constitute? 
 
i. a modifier that accompanies a 
noun 
ii. an associated modifier 
iii. an introductory modifier 
iv. a misplaced modifier 
 
2.3 Generation of test questions 
Sentences eligible for question generation are those 
containing domain-specific terms. Another condition for 
a sentence to be eligible is that its structure is of SVO or 
SV type.7 Currently, a number of simple question 
generation rules have been implemented. Example rules 
include the transformation of an SVO sentence in which 
the subject is a term, into the question  "Which HVO" 
where H is a hypernym of the term. Such a rule would 
generate the question "Which part of speech is the most 
central element in a clause" from the sentence "The verb 
is the most central element in a clause". This rule 
operates in several variants, one being that if the 
hypernym is a key term, then a ?Which kind of? 
question may be generated  (e.g. ?Transitive verbs 
require objects? would trigger the question "Which kind 
of verbs require objects?"). Another rule often used 
transforms an SVO sentence with object representing a 
term into the question "What do/does/did the S V". By 
way of example, this rule would convert the sentence in 
example (1) into the question "What does a 
prepositional phrase at the beginning of a sentence 
constitute?" 
The system makes use of agreement rules which 
ensure the grammaticality of the question generated. 
These rules also check for agreement between concepts 
mentioned in the question and the distractors. As an 
illustration, in addition to the local agreement in the 
question "What kind of phrases can act as adjectives, 
                                                           
6 In the rare case of the program not being able to 
extract suitable distractors from WordNet or/and from 
the corpus, no test item is generated. 
7 Sentences of such types are identified by the FDG 
parser which returns syntax functions. 
  
?? 
29 of 36 
Which kind of pronoun will agree with the
subject in number, person, and gender? 
  relative pronoun    
  second person pronoun     
  indefinite pronoun    
  reflexive pronoun   
?? 
 
 
 
adverbs and nouns", the alternatives selected will be 
plural (e.g. infinitive phrases, prepositional phrases, 
adverbial phrases, noun phrases). On the other hand, 
the alternatives belonging to the test item featuring the 
question "What grammatical category does a 
prepositional phrase at the beginning of a sentence 
constitute?" will be singular. 
The generation strategy of multiple-choice items 
included additional genre-specific heuristics such as 
discounting examples for further processing, excluding 
sentences that refer to tables or previously mentioned 
entities, not splitting compound verbs, etc. 
3. In-class experiments and system interface 
We introduced a controlled set8 of the generated test 
items into a classroom environment in order to obtain 
sufficient evaluation data related to their 
acceptability/revision and quality. The controlled set 
currently consists of 24 test items generated with the 
help of the program and 12 items produced manually. 
A total of 45 undergraduate students in 
language/linguistics took the class test. The majority of 
students were from our university, but several students 
were studying in other UK or European Universities. 
Students were asked not to spend more than 2 minutes 
on a test question. 
Figure 1: A snapshot of the interface 
 
The system works through the Questionmark 
Perception web-based testing software which in addition 
to providing a user-friendly interface, computes diverse 
statistics related to the test questions answered.  Figure 
1 shows the interface of the system in a class test 
environment. The test item displayed is one of the 24 
                                                           
8 Only items approved by a linguistics lecturer were 
used in the experiment (e.g. it was made sure that the 
items addressed material covered by undergraduate 
students). 
items generated with the help of the system that are used 
in the experiment.9  
The current experimental setting does not look at the 
problem of delivering a balanced test of preset overall 
difficulty based on random (or constraint-driven) 
selection of test items. Instead, it focuses on exploring 
the feasibility of the computer-aided procedure and on 
the quality of the test items produced. 
4. Evaluation 
In order to validate the efficiency of the method, we 
evaluated the performance of the system in two different 
ways. Firstly, we investigated the efficiency of the 
procedure by measuring the average time needed to 
produce a test item with the help of the program as 
opposed to the average time needed to produce a test 
item manually.10 Secondly, we examined the quality of 
the items generated with the help of the program, and 
compared it with the quality of the items produced 
manually. The quality was assessed via standard test 
theory measures such as discriminating power and 
difficulty of each test item, and the usefulness of each 
alternative was applied. 
4.1 The procedure of generating test items with the 
help of the program and its efficiency 
The first step of the procedure consists of the automatic 
generation of test items. The items so generated were 
then either (i) declared as ?worthy? and accepted for 
direct use without any revision, or further post-edited 
before being put into use, or (ii) declared as ?unworthy? 
and discarded. ?Unworthy? items were those that did not 
focus on a central concept or required too much 
revision, and so they were rejected. 
The items selected for further post-editing required 
minor, fair or major revisions. ?Minor? revision 
describes minor syntactical post-editing of the test 
question, including minor operations such insertions of 
articles, correction of spelling and punctuation. ?Fair? 
revision refers to some grammatical post-editing of the 
test question, including re-ordering or deletion of words 
and replacement of one distractor at most. ?Major? 
revision applied to the generated test items involved 
more substantial grammatical revision of the test 
question and replacement of two or more of the 
                                                           
9 The position of the correct answer (in this case 
?reflexive pronoun?) is generated randomly. 
10 Two graduate students in linguistics acted as post-
editors. The same students were involved in the 
production of test items manually. The texts used were 
selected with care so that possible influence of 
potentially similar or familiar texts was minimised. See 
also the discussion in section 5 on the effect of 
familiarity. 
  
distractors. As an illustration, the automatically 
generated test item  
 
(3) Which kind of language unit seem to be the 
most obvious component of language, and any 
theory that fails to account for the contribution 
of words to the functioning of language is 
unworthy of our attention?  
 
(a) word  
(b) name  
(c) syllable  
(d) morpheme 
 
was not acceptable in this form and required the 
deletion of the text ?and any theory that fails to account 
for the contribution of words to the functioning of 
language is unworthy of our attention? which was 
classed as ?fair? revision. 
From a total of about 575 items automatically 
generated by the program, 57% were deemed to be 
?worthy? i.e. considered for further use. From the 
worthy items, 6% were approved for direct class test use 
without any post-editing and 94% were subjected to 
post-editing.  From the items selected for revision, 17% 
needed minor revision, 36% needed fair revision and 
47% needed major revision. 
The time needed to produce 300 test items with the 
help of the program, including the time necessary to 
reject items, accept items for further editing or approve 
for direct use, amounted to 9 hours. The time needed to 
manually produce 65 questions was 7 hours and 30 
minutes. This results in an average of 1 minute and 48 
seconds to produce a test item with the help of the 
program and an average of 6 minutes and 55 seconds to 
develop a test item manually (Table 1). 
 
  items produced Time 
average 
time  
per item 
computer-aided 300 540' 1' 48'' 
Manual 65 450' 6' 55'' 
Table 1: Effectiveness of the method. 
 
4.2 Analysis of the items generated with the help of 
the program 
Item analysis is an important procedure in classical test 
theory which provides information as to how well each 
item has functioned. The item analysis for multiple-
choice tests usually consists of the following 
information (Gronlund 1982): (i) the difficulty of the 
item, (ii) the discriminating power and (iii) the 
usefulness11 of each alternative. This information can 
tell us if a specific test item was too easy or too hard, 
how well it discriminated between high and low scorers 
on the test and whether all of the alternatives functioned 
as intended. Such types of analysis help improve test 
items or discard defective items. 
In order to conduct this type of analysis, we used a 
simplified procedure, described in  (Gronlund 1982). 
We arranged the test papers in order from the highest 
score to the lowest score. We selected one third of the 
papers and called this the upper group (15 papers). We 
also selected the same number of papers with the lowest 
scores and called this the lower group (15 papers). For 
each item, we counted the number of students in the 
upper group who selected each alternative; we made the 
same count for the lower group. 
 
(i) Item Difficulty 
 
We estimated the Item Difficulty (ID) by establishing 
the percentage of students from the two groups who 
answered the item correctly (ID = C/T x 100, where C is 
the number who answered the item correctly and T is 
the total number of students who attempted the item). 
From the 24 items subjected to analysis, there were 0 
too difficult and 3 too easy items.12 The average item 
difficulty was 0.75. 
 
(ii) Discriminating Power 
 
We estimated the item's Discriminating Power (DP) 
by comparing the number students in the upper and 
lower groups who answered the item correctly. It is 
desirable that the discrimination is positive which means 
that the item differentiates between students in the same 
way that the total test score does.13 The formula for 
computing the Discriminating Power is as follows: DP 
= (CU ? CL): T/2 where CU is the number of students in 
the upper group who answered the item correctly and  
CL - the number of the students in the lower group that 
                                                           
11 Originally called ?effectiveness?. We chose to term 
this type of analysis ?usefulness? to distinguish it from 
the (cost/time) ?effectiveness? of the (semi-) automatic 
procedure as opposed to the manual construction of 
tests. 
12 For experimental purposes, we consider an item to be 
?too difficult? if ID  0.15 and an item ?too easy? if ID  
0.85. 
13 Zero DP is obtained when an equal number of 
students in each group respond to the item correctly. On 
the other hand, negative DP is obtained when more 
students in the lower group than the upper group answer 
correctly. Items with zero or negative DP should be 
either discarded or improved. 
  
did so. Here again T is the total number of students 
included in the item analysis.14 The average DP for the 
set of items used in the class test was 0.40. From the 
analysed test items, there were was only one item that 
had a negative discrimination. 
 
(iii) Usefulness of the distractors 
  
The usefulness of the distractors is estimated by 
comparing the number of students in the upper and 
lower groups who selected each incorrect alternative. A 
good distractor should attract more students from the 
lower group than the upper group.  
The evaluation of the distractors estimated the 
average difference between students in the lower and 
upper groups to be 1.92. Distractors classed as poor are 
those that attract more students from the upper group 
than from the lower group, and there were 6 such 
distractors. On the other hand, we term distractors not 
useful if they are selected by no student. The evaluation 
showed that there were 3 distractors deemed not useful. 
4.3 Analysis of the items constructed manually 
An experiment worthwhile pursing was to conduct item 
analysis of the manually produced test items and 
compare the results obtained regarding the items 
produced with the help of the program. A set of 12 
manually produced items were subjected to the above 
three types of item analysis. There were 0 too difficult 
and 1 too easy items. The average item difficulty of the 
items was 0.59. The average discriminating power was 
assessed to be 0.25 and there were 2 items with negative 
discrimination. The evaluation of the usefulness of the 
distractors resulted in an average difference between 
students in the upper and lower groups of 1.18. There 
were 10 distractors that attracted more students from the 
                                                           
14 Maximum positive DP is obtained only when all 
students in the upper group answer correctly and no one 
in the lower group does. An item that has a maximum 
DP (1.0) would have an ID 0.5; therefore, test authors 
are advised to construct items at the 0.5 level of 
difficulty. 
upper group and were therefore, declared as poor and 2 
distractors not selected at all, and therefore deemed to 
be not useful. 
Table 2 summarises the item analysis results for 
both test items produced with the help of the program 
and those produced by hand. 
5. Discussion and plans for future work 
The evaluation results clearly show that the construction 
of multiple-choice test items with the help of the 
program is much more effective than purely manual 
construction. We believe that this is the main advantage 
of the proposed methodology. As an illustration, the 
development of a test databank of considerable size 
consisting of 1000 items would require 30 hours of 
human input when using the program, and 115 hours if 
done manually. This has direct financial implications as 
the time and cost in developing test items would be 
dramatically cut. 
At the same time, the test item analysis shows that 
the quality of test items produced with the help program 
is not compromised in exchange for time and labour 
savings. The test items produced with of the program 
were evaluated as being of very satisfactory quality. As 
a matter of fact, in many cases they scored even better 
than those manually produced. Whereas the item 
difficulty factor assessed for manual items emerges as 
better15, of those produced with the help of the program, 
there were only 3 too easy items and 0 too difficult ones. 
In addition, whilst the values obtained for the 
discriminating power are not as high as we would have 
desired, the items produced with the help of the program 
scored much better on that measure and what is also 
very important, is that there was only one item among 
them with negative discrimination (as opposed to 2 
from those manually constructed). Finally, the analysis 
of the distractors confirms that it is not possible to class 
the manually produced test items as better quality than 
the ones produced with the help of the program. The test 
items generated with the help of the program scored 
                                                           
15 Ideally, item difficulty should be around the mark of 
0.5 
item difficulty item discriminating power usefulness of distractors 
 
avg 
item 
difficulty 
too 
easy 
Too 
difficult 
average 
discriminating 
power 
negative 
discriminating 
power 
poor 
not 
useful 
Total 
avg 
difference 
computer-
aided 
0.75 3 0 0.4 1 6 3 65 1.92 
manual 0.59 1 0 0.25 2 10 2 33 1.18 
Table 2: Item analysis 
 
  
better on the number of distractors deemed as not useful, 
were assessed to contain fewer poor distractors and had 
a higher average difference between students in the 
lower and upper groups. 
In order to ensure a more objective assessment of the 
efficiency of the procedure, we plan to run the following 
experiment. At least 6 months after a specific set of 
items has been produced with the help of the program, 
the post-editors involved will be asked to produce 
another, based on the same material, manually. 
Similarly, after such a period items originally produced 
manually will be produced by the same post-editors 
with the help of the program. Such an experiment is 
expected to extinguish any effect of familiarity and to 
provide a more objective measure as to how computer-
aided construction of tests is more effective than manual 
production.  
It should be noted that the post-editors were not 
professional test developers. It would be interesting to 
investigate the impact of the program on professional 
test developers. This is an experiment envisaged as part 
of our future work. 
In addition to extending the set of test items to be 
evaluated and the samples of students taking the test, 
further work includes experimenting with more 
sophisticated term extraction techniques and with other 
more elaborate models for measuring semantic 
similarity of concepts. We would like to test the 
feasibility of using collocations from an appropriate 
domain corpus with a view to extending the choice of 
plausible distractors. We also envisage the development 
of a more comprehensive grammar for generating 
questions, which in turn will involve studying and 
experimenting with existing question generation 
theories. As our main objective has been to investigate 
the feasibility of the methodology, we have so far 
refrained from more advanced NLP processing of the 
original documents such as performing anaphora 
resolution and temporal or spatial reasoning which will 
certainly allow for more questions to be generated. 
Future work also envisages evaluation as to what extent 
the questions cover the course material. Finally, even 
though the agreement between post-editors appears to 
be a complex issue, we would like to investigate it in 
more depth. This agreement should be measured on 
semantic rather than syntactic principles, as the post-
editors may produce syntactically different test 
questions which are semantically equivalent. Similarly, 
different distractors may be equally good if they are 
equal in terms of semantic distance to the correct 
answer. 
 
6. Conclusion 
This paper describes a novel NLP-based and computer-
aided procedure for the construction of multiple-choice 
tests from instructional documents in electronic form. 
The results from the evaluation conducted suggest that 
the new procedure is very effective in terms of time and 
labour, and that the test items produced with the help of 
the program are not of inferior quality to those produced 
manually. 
 
References 
 
Fairon, C. (1999). ?A Web-based System for Automatic 
Language Skill Assessment: EVALING?. 
Proceedings of Computer Mediated Language 
Assessment and Evaluation in Natural Language 
Processing Workshop. 
Gronlund, N. (1982) Constructing achievement tests. 
New York: Prentice-Hall Inc. 
Justeson, J. S. and S. L. Katz (1996) ?Technical 
terminology: some linguistic properties and an 
algorithm for identification in text?. Natural 
Language Engineering, 3, (2), 259-289. 
Kies, D. (2003) Modern English Grammar. Online 
textbook. 
http://www.papyr.com/hypertextbooks/engl_126/b
ook126.htm 
Tapanainen, P. and J?rvinen, T. (1997) ?A non-
projective dependency parser?. Proceedings of the 
5th Conference of Applied Natural Language 
Processing (ANLP-5), 64-71. 
Proceedings of the Fourth International Natural Language Generation Conference, pages 111?113,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Generating Multiple-Choice Test Items from Medical Text:
A Pilot Study
Nikiforos Karamanis
Computer Laboratory
University of Cambridge
CB3 0FD, UK
nk304@cam.ac.uk
Le An Ha and Ruslan Mitkov
Computational Linguistics Research Group
University of Wolverhampton
WV1 1SB, UK
{L.A.Ha, R.Mitkov}@wlv.ac.uk
Abstract
We report the results of a pilot study on generating
Multiple-Choice Test Items from medical text and
discuss the main tasks involved in this process and
how our system was evaluated by domain experts.
1 Introduction
AlthoughMultiple-Choice Test Items (MCTIs) are
used daily for assessment, authoring them is a
laborious task. This gave rise to a relatively new
research area within the emerging field of Text-
to-Text Generation (TTG) called Multiple-Choice
Test Item Generation (MCTIG).1
Mitkov et al (2006) developed a system
which detects the important concepts in a
text automatically and produces MCTIs testing
explicitly conveyed factual knowledge.2 This
differs from most related work in MCTIG such as
Brown et al (2005) and the papers in BEAUNLP-
II (2005) which deploy various NLP techniques to
produce MCTIs for vocabulary assessment, often
using preselected words as the input (see Mitkov
et al for more extensive comparisons).
The approach of Mitkov et al is semi-automatic
since the MCTIs have to be reviewed by domain
experts to assess their usability. They report that
semi-automatic MCTIG can be more than 3 times
quicker than authoring of MCTIs without the aid
of their system.
1TTG, in which surface text is used as the input to
algorithms for text production, contrasts with Concept-
to-Text Generation (better known as Natural Language
Generation) which is concerned with the automatic
production of text from some underlying non-linguistic
representation of information (Reiter and Dale, 2000).
2Mitkov et al used an online textbook on Linguistics as
their source text. Clearly, their approach is not concerned
with concepts or facts derived through inferencing. Neither
does it address the problem of compiling a balanced test from
the generated MCTIs.
Moreover, analysis of MCTIs produced semi-
automatically and used in the classroom reveals
that their educational value is not compromised in
exchange for time and labour savings. In fact, the
semi-automatically produced MCTIs turn out to
fare better than MCTIs produced without the aid
of the system in certain aspects of item quality.
This paper reports the results of a pilot study on
generating MCTIs from medical text which builds
on the work of Mitkov et al
2 Multiple-Choice Test Item Generation
A MCTI such as the one in example (1) typically
consists of a question or stem, the correct answer
or anchor (in our example, ?chronic hepatitis?)
and a list of distractors (options b to d):
(1) Which disease or syndrome may progress to cirrhosis
if it is left untreated?
a) chronic hepatitis
b) hepatic failure
c) hepatic encephalopathy
d) hypersplenism
The MCTI in (1) is based on the following clause
from the source text (called the source clause; see
section 2.3 below):
(2) Chronic hepatitis may progress to cirrhosis if it is left
untreated.
We aim to automatically generate (1) from (2)
using our simple Rapid Item Generation (RIG)
system that combines several components
available off-the-shelf. Based on Mitkov et al, we
saw MCTIG as consisting of at least the following
tasks: a) Parsing b) Key-Term Identification c)
Source Clause Selection d) Transformation to
Stem e) Distractor Selection. These are discussed
in the following sections.
111
2.1 Sentence Parsing
Sentence Parsing is crucial for MCTIG since the
other tasks rely greatly on this information. RIG
employs Charniak?s (1997) parser which appeared
to be quite robust in the medical domain.
2.2 Key-Term Identification
One of our main premises is that an appropriate
MCTI should have a key-term as its anchor
rather than irrelevant concepts. For instance, the
concepts ?chronic hepatitis? and ?cirrhosis? are
quite prominent in the source text that example (2)
comes from, which in turn means that MCTIs
containing these terms should be generated using
appropriate sentences from that text.
RIG uses the UMLS thesaurus3 as a domain
specific resource to compute an initial set of
potential key terms such as ?hepatitis? from the
source text. Similarly to Mitkov et al, the initial
set is enlarged with NPs featuring potential key
terms as their heads and satisfying certain regular
expressions. This step adds terms such as ?acute
hepatitis? (which was not included in the version
of UMLS utilised by our system) to the set.
The tf.idf method (that Mitkov et al did
not find particularly effective) is used to promote
the 30 most prominent potential key terms within
the source text for subsequent processing, ruling
out generic terms such as ?patient? or ?therapy?
which are very frequent within a larger collection
of medical texts (our reference corpus).
2.3 Source Clause Selection
Mitkov et al treat a clause in the source text
as eligible for MCTIG if it contains at least one
key term and is finite as well as of the SV(O)
structure. They acknowledge, however, that this
strategy gives rise to a lot of inappropriate source
clauses, which was the case in our domain too.
To address this problem, we implemented a
module which filters out inappropriate structures
for MCTIG (see Table 1 for examples). This
explains why the number of key terms and MCTIs
varies among texts (Table 2).
A finite main clause which contains an NP
headed by a key term and functioning as a
subject or object with all the subordinate clauses
which depend on it is a source clause eligible
for MCTIG provided that it satisfies our filters.
Example (2) is such an eligible source clause.
3http://www.nlm.nih.gov/research/umls/
Structure Example (key term in italics)
Subordinate clause Although asthma is a lung disease, ...
Negated clause Autoimmune hepatitis should not
be treated with interferon.
Coordinated NP Excessive salt intake causes
hypertension and hypokalemia.
Initial pronoun It associates with hypertension instead.
Table 1: Inappropriate structures for MCTIG.
Experimentation during development showed that
our module improves source clause selection by
around 30% compared to the baseline approach of
Mitkov et al
2.4 Transformation to Stem
Once an appropriate source clause is identified,
it has to be turned to the stem of a MCTI. This
involves getting rid of discourse cues such as
?however? and substituting the NP headed by the
key term such as ?chronic hepatitis? in (1) with a
wh-phrase such as ?which disease or syndrome?.
The wh-phrase is headed by the semantic type of
the key-term derived from UMLS.
RIG utilises a simple transformational
component which produces a stem via minimal
changes in the ordering of the source clause. The
filtering module discussed in the previous section
disregards the clauses in which the key term
functions as a modifier or adjunct. Additionally,
most of the key terms in the eligible source clauses
appear in subject position which in turn means
that wh-fronting and inversion is performed in just
a handful of cases. The following example, again
based on the source clause in (2), is one such case:
(3) To which disease or syndrome may chronic hepatitis
progress if it is left untreated?
2.5 Selection of Appropriate Distractors
MCTIs aim to test the ability of the student
to identify the correct answer among several
distractors. An appropriate distractor is a concept
semantically close to the anchor which, however,
cannot serve as the right answer itself.
RIG computes a set of potential distractors
for a key term using the terms with the same
semantic type in UMLS (rather than WordNet
coordinates employed by Mitkov et al). Then, we
apply a simple measure of distributional similarity
derived from our reference corpus to select the
best scoring distractors. This strategy means that
MCTIs with the same answer feature very similar
distractors.
112
# of # of Usable Usable Items w/out Replaced distractors Total Average Time
Chapter Words Key-terms Items Items post-edited stems per term Time per Item
Asthma 8,843 9 66 42 (64%) 18 (27%) 2.0 140 mins 3 mins 20 secs
Hepatitis 10,259 17 92 49 (53%) 19 (21%) 0.9 150 mins 3 mins 04 secs
Hypertension 12,941 22 121 59 (49%) 15 (12%) 0.8 200 mins 3 mins 23 secs
Total 32,043 40 279 150 (54%) 52 (19%) ? 490 mins 3 mins 16 secs
Table 2: Usability and efficiency of Multiple-Choice Test Item Generation from medical text.
3 Evaluation
RIG is a simple system which often avoids
tough problems such as dealing with key-terms in
syntactic positions that might puzzle the parser or
might be too difficult to question upon. So how
does it actually perform?
Three experts in producing MCTIs for medical
assessment jointly reviewed 279 MCTIs (each
featuring four distractors) generated by the
system. Three chapters from a medical textbook
served as the source texts while a much larger
collection of MEDLINE texts was used as the
reference corpus.
The domain experts regarded a MCTI as
unusable if it could not be used in a test or required
too much revision to do so. The remaining items
were considered to be usable and could be post-
edited by the experts to improve their content and
readability or replace inappropriate distractors.
As Table 2 shows, more than half of the items in
total were judged to be usable. Additionally, about
one fifth of the usable items did not require any
editing. The Table also shows the total number of
key-terms identified in each chapter as well as the
average number of distractors replaced per term.
The last column of Table 2 reports on the
efficiency of MCTIG in our domain. This variable
is calculated by dividing the total time it took
the experts to review all MCTIs by the amount
of usable items which represent the actual end-
product. This is a bit longer than 3 minutes
per usable item across all chapters. Anecdotal
evidence and the experts? own estimations suggest
that it normally takes them at least 10 minutes to
produce an MCTI manually.
Given the distinct domains in which our system
and the one of Mitkov et al were deployed (as
well as the differences between them), a direct
comparison between them could be misleading.
We note, however, that our usability scores are
always higher than their worst score (30%) and
quite close to their best score (57%). The amount
of directly usable items in Mitkov et al was
between just 3.5% and 5%, much lower than
what we achieved. They also report an almost
3-fold improvement in efficiency for computer-
aided MCTIG, which is very similar to our
estimate. These results indicate what our work has
contributed to the state of the art in MCTIG.
In our future work, we aim to address the
following issues: (a) As in Mitkov et al, the
anchor of a MCTI produced by RIG always
corresponds to a key-term. However, the domain
experts pointed out several cases in which it is
better for the key-term to stay in the stem and
for another less prominent concept to serve as the
answer. (b) Students who simply memorise the
input chapter might be able to answer the MCTI if
its surface form is too close to the source clause so
another interesting suggestion was to paraphrase
the stem during MCTIG. (c) We also intend to
introduce greater variability in our process for
distractor selection by investigating several other
measures of semantic similarity.
Acknowledgments
We are grateful to Tony LaDuca, Manfred Straehle and
Robert Galbraith from the National Board of Medical
Examiners (NBME) for their expert-based feedback and to
three anonymous reviewers for their comments.
References
BEAUNLP-II. 2005. Papers on MCTIG by Hoshino and
Nakagawa, Liu et al, and Sumita et al In Proceedings of
the 2nd Workshop on Building Educational Applications
Using NLP.
Jonathan Brown, Gwen Frishkoff, and Maxine Eskenazi.
2005. Automatic question generation for vocabulary
assessment. In Proceedings of HLT-EMNLP 2005, pages
249?254.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of AAAI
1997, pages 598?603.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006.
A computer-aided environment for generating multiple-
choice test items. Natural Language Engineering,
12(2):177?194.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge University
Press.
113
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 49?56,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
Semantic similarity of distractors in multiple-choice tests:  extrinsic evaluation 
Ruslan Mitkov, Le An Ha, Andrea Varga and Luz Rello University of Wolverhampton Wolverhampton, UK {R.Miktov, L.A.Ha, Andrea.Varga, L.RelloSanchez}@wlv.ac.uk     Abstract Mitkov and Ha (2003) and Mitkov et al (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. One of the interesting research questions which emerged during these projects was how better quality distractors could automatically be chosen. This paper reports the results of a study seeking to establish which similarity measures generate better quality distractors of multiple-choice tests. Similarity measures employed in the procedure of selection of distractors are collocation patterns, four different methods of WordNet-based semantic similarity (extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s as well as Lin?s measures), distributional similarity, phonetic similarity as well as a mixed strategy combining the aforementioned measures. The evaluation results show that the methods based on Lin?s measure and on the mixed strategy outperform the rest, albeit not in a statistically significant fashion. 1 Introduction  Multiple-choice tests are sets of test items, the latter consisting of a question or stem (e.g. Who was voted the best international footballer for 2008?), the correct answer (e.g. 
Ronaldo) and distractors (e.g. Messi, Ronaldino, Torres). This type of test has proved to be an efficient tool for measuring students? achievement and is used on a daily basis both for assessment and diagnostics worldwide.1 According to Question Mark Computing Ltd (p.c.), who have licensed their Perception software to approximately three million users so far, 95% of their users employ this software to administrate multiple-choice tests.2  Despite their popularity, the manual construction of such tests remains a time-consuming and labour-intensive task. One of the main challenges in constructing a multiple-choice test item is the selection of plausible alternatives to the correct answer which will better distinguish confident students from unconfident ones. Mitkov and Ha (2003) and Mitkov et al (2006) offered an alternative to the lengthy and demanding activity of developing multiple-choice test items by proposing an NLP-based methodology for construction of test items from instructive texts such as textbook chapters and encyclopaedia entries. This methodology makes use of NLP techniques including shallow parsing, term extraction, sentence transformation and semantic distance computing and employs resources such as corpora and ontologies like WordNet. More specifically, the system identifies important terms in a textbook text,                                                            1 This paper is not concerned with the issue of whether multiple-choice tests are better assessment methodology that other types of tests. What it focuses is on improving our new NLP methodology to generate multiple-choice tests about facts explicitly stated in single declarative sentences by establishing which semantic similarity measures give rise to better distractors. 2 More information on the Perception software can be found at: www.questionmark.com/perception 
49
transforms declarative sentences into questions and mines for terms which are semantically close to the correct answer, to serve as distractors.  The system for generation of multiple-choice tests described in Mitkov and Ha (2003) and in Mitkov et al (2006) was evaluated in practical environment where the user was offered the option to post-edit and in general to accept, or reject the test items generated by the system3. The formal evaluation showed that even though a significant part of the generated test items had to be discarded, and that the majority of the items classed as ?usable? had to be revised and improved by humans, the quality of the items generated and proposed by the system was not inferior to the tests authored by humans, were more diverse in terms of topics and very importantly ? their production needed 4 times less time than the manually written items. The evaluation was conducted both in terms of measuring the time needed to develop test items and in terms of classical test analysis to assess the quality of test items.  The paper is structured as follows. Section 2 will outline the importance of distractors in multiple-choice testing as the different strategies for automatic selection of the distractors are the subject of this study. Section 3 will describe how test items are produced and will detail the different strategies (semantic similarity measures and phonetic similarity) used for the selection of distractors. Section 4 outlines the in-class experiments, presents the evaluation methodology, reports on the results and discusses these results. 2 The importance of quality distractors One of the interesting research questions which emerged during the above research was how better quality distractors could automatically be chosen. In fact user evaluation showed that from the three main tasks performed in the generation of multiple-choice tests (term identification, sentence transformation and distractor selection), it was distractor selection which needed further improvement with a view to putting it in practical use.                                                            3 A post-editor?s interface was developed to this end. 
Distractors play a vital role for the process of multiple-choice testing in that good quality distractors ensure that the outcome of the tests provides more credible and objective picture of the knowledge of the testees involved. On the other hand, poor distractors would not contribute much to the accuracy of the assessment as obvious or too easy distractors will pose no challenge to the students and as a result, will not be able to distinguish high performing from low performing learners. The principle according to which the distractors were chosen, was semantic similarity (Mitkov and Ha, 2003). The semantically closer were the distractors to the correct answer, the most ?plausible? they were deemed to be. The rationale behind this consists in the fact that distractors semantically distant from the correct answer could make guessing a ?straightforward task?. By way an example, if processing the sentence ?Syntax is the branch of linguistics which studies the way words are put together into sentences?, the multiple-choice generation system would identify syntax as an important term, would transform the sentence into the question ?Which branch of linguistics studies the way words are put together into sentences?? and would choose ?Pragmatics?, ?Morphology? and ?Semantics? as distractors to the correct answer ?Syntax?, being closer to it than ?Chemistry?, ?Football? or ?Beer? for instance (which if offered as distractors, would be easily dismissed by people who do not have even any knowledge of linguistics). While the semantic similarity premise appears as a logical way forward to automatically select distractors, there are different methods or measures which compute semantic similarity. Each of these methods could be evaluated individually but here we evaluate their suitability for the task of selection of distractors in multiple-choice tests. This type of evaluation could be regarded as extrinsic evaluation of each of the methods, where the benchmark for their performance would not be an annotated corpus or human judgement on accuracy, but to what extent a specific NLP application can benefit from employing a method. Another premise that this study seeks to verify is whether orthographically close distractors, in addition to being semantically related, could yield even better results.  
50
3 Production of test items and selection of distractors Test items were constructed by a program based on the methodology described in the previous section. We ran the program on an on-line course materials in linguistics (Vajda, 2001). A total of 144 items were initially generated. 31 out of these 144 items were kept for further considerations as they either did not need any or, only minor revision. The remaining 113 items were deemed to require major post-editing revision. The 31 items kept for consideration were further revised by a second linguist and finally, we narrowed down the selection to 20 questions for the experiments4. These 20 questions gave a rise to a total of eight different assessments. Each assessment had the same 20 questions but they differed in the sets of distractors as these were chosen using different similarity measures5 (sections 3.1-3.5). To generate a list of distractors for single-word terms the function coordinate terms in WordNet is employed. For multi-word terms, noun phrases with the same head as the correct answers appearing in the source text as well as entry terms from Wikipedia having the same head with the correct answers, are used to compile the list of distractors. This list of distractors is offered to the user from which he or she could choose his/her preferred distractors.  In this study we explore which is the best way to narrow down the distractors to the 4 most suitable ones. To this end, the following strategies for computing semantic (and in one case, phonetic) similarity were employed: (i) collocation patterns, (ii-v) four different methods of WordNet-based semantic similarity (Extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s and Lin?s measures), (vi) Distributional Similarity, and (vii) Phonetic similarity.  
                                                           4 The following is an example of an item generated of the program and then post-edited.  "Which type of clause might contain verb and dependent words? i) verb clause ii) adverb clause iii) adverbial clause    iv) multiple subordinate clause v) subordinate clause". 5 It should be noted that there were cases where the different selection/similarity strategies picked the same distractors. 
3.1 Collocation patterns The collocation extraction strategy used in this experiment is based on the method reported in (Mitkov and Ha, 2003). Distractors that appear in the source text are given preference. If there are not enough distractors, distractors are selected randomly from the list. For the other methods described below (sections 3.2-3.5), instead of giving preference to noun phrases appearing in the same text, and randomly pick the rest from the list, we ranked the distractors in the list based on the similarity scores between each distractor and the correct answer and chose the top 4 distractors. We compute similarity for words rather than multi-word terms. When the correct answers and distractors are multi-word terms, we calculate the similarities between their modifier words. By way of example, in the case of "verb clause" and "adverbial clause", the similarity score between "verb" and "adverbial" is computed. When the correct answer or distractor contains more than one modifiers we compute the similarity for each modifier pairs and we choose the maximum score. (e.g. for "verb clause" and "multiple subordinate clause", similarity scores of "verb" and "multiple" and of "verb" and "subordinate" are calculated, the higher one is considered to represent the similarity score). 3.2 Four different methods for WordNet-based similarity For computing WordNet-based semantic similarity we employed the package made available by Ted Pedersen6. Pedersen?s tool computes (i) extended gloss overlap measure (Banerjee and Pedersen, 2003), (ii) Leacock and Chodorow?s (1998) measure, (iii) Jiang and Conrath?s (1997) measure and (iv) Lin?s (1997) measure.  The extended gloss overlap measure calculates the overlaps between not only the definitions of the two concepts measured but also among those concepts to which they are related. The relatedness score is the sum of the squares of the overlap lengths.  Leacock and Chodorow?s measure uses the normalised path length between the two concepts c1 and c2 and is computed as follows: 
                                                           6 http://search.cpan.org/~tpederse/WordNet-Similarity 
51
 (1) 
where len is the number of edges on the shortest path in the taxonomy between the two concepts and MAX is the depth of the taxonomy. Jiang and Conrath?s measure compares the sum of the information content of the individual concepts with that of their lowest common subsumer: 
 (2) 
where IC(c) is the information content (Patwardhan et al, 2003) of the concept c, and lcs denotes the lowest common subsumer, which represents the most specific concept that the two concepts have in common. The Lin measure scales the information content of lowest common subsumer with the sum of information content of two concepts.  
 (3) 
 3.3 Distributional similarity For computing distributional similarity we made use of Viktor Pekar's implementation7 based on Information Radius, which according to a comparative study by Dagan et al (1997) performs consistently better than the other similar measures. Information Radius (or Jensen-Shannon divergence) is a variant of Kullback-Leiber divergence measuring similarity between two words as the amount of information contained in the difference between the two corresponding co-occurrence vectors. Every word wj is presented by the set of words wi1...n with which it co-occurs. The semantics of wj are modelled as a vector in an n-dimensional space where n is the number of words co-occurring with wj, and the features of the vector are the probabilities of the co-occurrences established from their observed frequencies, as in (4). In Pekar?s implementation, if one word is identified as dependent on another word by a dependency 
                                                           7 http://clg.wlv.ac.uk/demos/similarity/index.html 
parser, these two words are said to be ?co-occuring?8. The corpus used to collect the co-occurance vector was the BNC and the dependency parsed used the FDG parser (Tapanainen and J?rvinen, 1997). The Information Radius (JS) is calculated using (5).  (4) 
 (5) 
where  
3.4 Phonetic similarity For measuring phonetic similarity we use Soundex, phonetic algorithm for indexing words by sound. It operates on the principle of term based evaluation where each term is given a Soundex code. Each Soundex code itself consists of a letter and three numbers between 0 and 6. By way of example the Soundex code of verb is V610 (the first character in the code is always the first letter of the word encoded). Vowels are not used and digits are based on the consonants as illustrate by the following table:  1. B, P, F, V 2. C, S, K, G, J, Q, X, Z 3. D, T 4. L 5. M, N 6. R Table 1 Digits based on consonants First the Soundex code for each word is generated9. Then similarity is computed using the Difference method, returning an integer result ranging in value from 1 (least similar) to 4 (most similar). 3.5 Mixed Strategy After items have been generated by the above seven methods, we pick three items from each method, except from Soundex, where only two items have been picked, to compose an                                                            8 There are many other ways to construct the co-occurrence vectors. This paper does not intend to exploit these different ways. 9 We adopt the phonetic representation used in MS SQL Server. As illustrated above, each soundex code consists of a letter and three numbers, such as A252. 
52
assessment of 20 items. This assessment is called ?mixed?, and used to assess whether or not an assessment with distractors generated by combining different methods would produce a different result from an assessment featuring distractors generated by a single method. 4 In-class experiments, evaluation, results and discussion The tests (papers) generated with the help of our program with the distractors chosen according the different methods described above, were taken by a total of 243 students from different European universities: University of Wolverhampton (United Kingdom), University College Ghent (Belgium), University of Saarbr?cken (Germany), University of Cordoba (Spain), University of Sofia (Bulgaria). A prerequisite for the students taking the test was that they studied language and linguistics and that they had a good command of English. Each test paper consisted of 20 questions and the students had 30 minutes to reply to the questions. The tests were offered through the Questionmark Perception web-based testing software which in addition to providing a user-friendly interface, computes diverse statistics related to the test questions answered.  In order to evaluate the quality of the multiple-choice test items generated by the program (and subsequently post-edited by humans), we employed standard item analysis. Item analysis is an important procedure in classical test theory which provides information as to how well each item has functioned. The item analysis for multiple-choice tests usually consists of the following information (Gronlund, 1982): (i) the difficulty of the item, (ii) the discriminating power and (iii) the usefulness10 of each distractor. This information can tell us if a specific test item was too easy or too hard, how well it discriminated between high and low scorers on the test and whether all of the alternatives functioned as intended. Such types of analysis help improve test items or discard defective items.                                                            10 Originally called ?effectiveness?. We chose to term this type of analysis ?usefulness? to distinguish it from the (cost/time) ?effectiveness? of the semi-automatic procedure as opposed to the manual construction of tests. 
Whilst this study focuses on the quality of the distractors generated, we believe that the distractors are essential for the quality of the overall test and hence the difficulty of an item and its discriminating power are deemed appropriate to assess the quality of distractors, even though the quality of the test stem also pays in important part. On the other hand usefulness is a completely independent measure as it looks at distractors only and not only the combination of stems and distractors. In order to conduct this type of analysis, we used a simplified procedure, described in (Gronlund, 1982). We arranged the test papers in order from the highest score to the lowest score. We selected one third of the papers and called this the upper group. We also selected the same number of papers with the lowest scores and called this the lower group. For each item, we counted the number of students in the upper group who selected each alternative; we made the same count for the lower group. (i) Item Difficulty 
We estimated the Item Difficulty (ID) by establishing the ratio of students from the two groups who answered the item correctly (ID = C/T, where C is the number who answered the item correctly and T is the total number of students who attempted the item). As Table 2 shows, from the items featuring distractors generated using the collocation method11, there were 4 too easy and 0 too difficult items.12 The average Item Difficulty was 0.61. From the items with distractors generated using WordNet-based similarity13, the results were the following. When employing the extended gloss overlap measure there were 2 too easy and 0 too difficult items, showing an average ID of 0.58. Leacock and Chodorow?s measure produced 1 too easy and 3 too difficult items with item average difficulty of 0.54. The use of Jiang and Conrath?s measure resulted in 3 too easy and 1 too difficult items; the average item difficulty observed was 0.57. Lin?s measure delivered the best results from the                                                            11 Henceforth referred to as ?collocation items?; the distractors generated are referred to as ?collocation distractors?. 12 For experimental purposes, we consider an item to be ?too difficult? if ID ? 0.15 and an item ?too easy? if ID ? 0.85. 13 Henceforth referred to as ?WordNet items?; the distractors are referred to as  ?WordNet distractors?. 
53
point of item difficulty with an almost ideal average item difficulty of 0.51 (the recommended item difficult is 0.5; see also footnote 16); there were 2 too easy and 1 too difficult items. The items constructed on the basis of distractors selected via the distributional similarity metric14, scored an average ID of 0.64 with 6 items being too easy and 1 ? too difficult.  From the items with distractors produced using the phonetic similarity algorithm15, there were 4 too easy and 0 too difficult questions with overall average difficult of 0.60. Finally, a mixed strategy produced test items with average difficulty of 0.53, 1 of them being too easy and 0 ? too difficult. The results showed that almost all items produced after selecting distractors using the strategies described above, featured very reasonable ID values. In many cases the average values were close to the recommended ID value of 0.5 with Lin?s measure delivering the best ID of 0.51. Runners-up are the mixed strategy delivering items with average ID 0.53 Leacock and Chodorow?s measure contributing to the generation of items with average ID of 0.54. (ii) Discriminating Power 
We estimated the item's Discriminating Power (DP) by comparing the number students in the upper and lower groups who answered the item correctly. It is desirable that the discrimination is positive which means that the item differentiates between students in the same way that the total test score does.16 The formula for computing the Discriminating Power is as follows: DP = (CU ? CL) : T/2, where CU is the number of students in the upper group who answered the item correctly and  CL the number of the students in the lower group that did so. Here again T is the 
                                                           14 Henceforth referred to as ?distributional items?; the distractors are referred to as ?distributional distractors?. 15 Henceforth referred to as ?phonetic items?; the distractors are referred to as ?phonetic distractors?. 16 Zero DP is obtained when an equal number of students in each group respond to the item correctly. On the other hand, negative DP is obtained when more students in the lower group than the upper group answer correctly. Items with zero or negative DP should be either discarded or improved. 
total number of students included in the item analysis.17  The average Discriminating Power for the collocation items was 0.33 and there were no negative discriminating collocation test items.18 The figures associated to the WordNet items were as follows. The average DP for items produced with the extended gloss overlap measure was 0.32, and there were 2 items with negative discrimination. Leacock and Chodorow?s measure did not produce any items with negative discrimination and the average DP of these was 0.38. Jiang and Conrath?s measure gave rise to 2 negatively discriminating items and the average DP of the items based on this measure was 0.29. The selection of distractors with Lin?s measure resulted in items with average DP of 0.37; none of them had a negative discrimination. The average discrimination power for the distributional items was 0.29 (1 item with negative discrimination) and for phonetic items ? 0.34 (0 item with negative discrimination). The employment of mixed strategy when selecting distractors which resulted in items with average DP of 0.39 (0 items with negative discrimination). The figures related to the Discriminating Power of the items generated showed that whereas the DP was not of the desired high level, as a whole the proportion of items with negative discrimination was fairly low (Table 2). The items did not differ substantially in terms of the values of DP, the top performer being the items where the distractors were selected on the basis of the mixed strategy, followed by those selected by Leacock and Chodorow?s measure and phonetic similarity. (iii) Usefulness of the distractors  The usefulness of the distractors is estimated by comparing the number of students in the upper and lower groups who selected each incorrect alternative. A good distractor should attract more students from the lower group than the upper group.  The evaluation of the distractors estimated the average difference between students in the                                                            17 Maximum positive DP is obtained only when all students in the upper group answer correctly and no one in the lower group does. An item that has a maximum DP (1.0) would have an ID 0.5; therefore, test authors are advised to construct items at the 0.5 level of difficulty. 18 Obviously a negative discriminating test item is not regarded as a good one. 
54
lower and upper groups to be 0.74 for the sets of distractors generated using collocations.  For the WordNet distractors the results were as follows. The average distance between the students in the lower and upper groups was found to be 0.71 for the extended gloss overlap distractors, 0.76 for the Leacock and Chodorow distractors, 0.71 for the Jiang and Conrath distractors and 0.83 for the Lin distractors. For the distractors selected by way of distributional similarity the average difference between students in the lower and upper groups was 0.79, for the phonetic distractors ? 0.66 and for those selected by a mixed strategy ? 0.89. In our evaluation we also used the notions of poor distractors as well as not-useful distractors. Distractors are classed as poor if they attract more students from the upper group than from the lower group. There were 2 (2.5%) poor distractors from the collocation distractors. The WordNet distractors fared as follows with regard to the number of poor distractors. There were altogether 9 (11%) poor distractors from the extended gloss overlap distractors, 9 (11%) from the Leacock and Chodorow distractors, 10 (12%) from the Jiang and Conrath distractors and 10 (12%) from the Lin ones. There were 6 (7.5%) from the distributional similarity which were classed as poor, 5 (6%) from the phonetic similarity ones were classed as poor and 5 (6%) from the distractors selected through a mixed strategy were classed as such (Table 2).  
On the other hand, distractors are termed not useful if they are not selected by any students at all. The evaluation showed (see Table 2) that there were 24 (30%) distractors deemed not useful from the collocation distractors. The figures for not useful distractors for those selected by way of WordNet similarity were as follows: 17 (21%) for extended gloss overlap distractors, 20 (25%) for the Leacock and Chodorow distractors, 19 (24%) for the Jiang and Conrath distractors and 16 (20%) for the Lin ones. From the distributional distractors, 27 (34%) emerged as not useful, whereas 31 (39%) phonetic similarity and 14 (18%) mixed strategy distractors were found not useful.  The overall figures suggest that the ?most useful? distractors are those chosen with mixed strategy (highest average difference 0.89; lowest number of not useful distractors, second lowest number of poor distractors), followed by those chosen with Lin?s WordNet measure (second highest average distance of 0.83; second lowest number of not useful distractors).  Summarising the results of the item analysis, it is clear that there is not a method that outperforms the rest in terms of producing best quality items or distractors. At the same time it is also clear that in general the mixed strategy and Lin?s measure consistently perform better than the rest of methods/measures. Phonetic similarity did not deliver as expected. 
 Item Difficulty Item Discriminating Power Usefulness of distractors 
 average item difficulty too easy too difficult average discriminating power negative discriminating power poor not useful average difference Collocation  items 0.61 4 0 0.33 0 2 24 0.74 WordNet items -  Extended gloss overlap -  Leacock and Chodorow -  Jiang and Conrath -  Lin 
 0.58 0.54 0.57 0.51 
 2 1 3 2 
 0 3 1 1 
 0.32 0.38 0.29 0.37 
 2 0 2 0 
 9 9 10 10 
 17 20 19 16 
 0.71 0.76 0.71 0.83 Distributional items 0.64 6 1 0.29 1 6 27 0.79 Phonetic items 0.60 4 0 0.34 0 5 31 0.66 Mixed strategy items 0.53 1 0 0.39 0 5 14 0.89 Table 2: Item analysis 
55
Although the results indicate that the Lin items have the best average item difficulty, none of the difference (between item difficulty of Lin and other methods, or between any pair of methods) is statistically significant. From the DP point of view, only the difference between mixed strategy (0.39) and distributional items (0.29) is statistically significant (p<0.05). For the distractor usefulness measure, none of the difference is statistically significant (p<0.05). 5 Conclusion In this study we conducted extrinsic evaluation of several similarity methods (collocation patterns; four different methods of WordNet-based semantic similarity: extended gloss overlap measure, Leacock and Chodorow?s, Jiang and Conrath?s as well as Lin?s measures; distributional similarity; phonetic similarity; mixed strategy) by seeking to establish which one would be most suitable for the task of selection of distractors in multiple-choice tests. The evaluation results based on item analysis suggests that whereas there is not a method that clearly outperforms in terms of delivering better quality distractors, mixed strategy and Lin?s measure consistently perform better than the rest of methods/measures. However, these two methods do not offer any statistically significant improvement over their closest competitors. Acknowledgments We would like to express our gratitude to Kathelijne Denturck, Johann Haller, Veronique Hoste, Constantin Orasan, Miriam Seghiri, Andrea Stockero and Irina Temnikova for helping us in the organisation of the in-class experiments.  References Banerjee, S. and Perderson, T. 2003. Extended gloss overlaps as a measure of semantic relatedness. Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, 805-810. Dagan I., Lee L., and Pereira F. 1997. Similarity-based methods for word sense disambiguation. Proceedings of the 35th Annual Meeting of 
the Association for Computational Linguistics. Madrid, Spain, 56-63. Gronlund, N. 1982. Constructing achievement tests. New York: Prentice-Hall Inc. Jiang, J. and Conrath, D. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. Proceedings of the International Conference on Research in Computational Linguistics. Taiwan, 19-33. Lin, D. 1997. Using syntactic dependency as a local context to resolve word sense ambiguity. Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics. Madrid, Spain, 4-71. Leacock, C., Chodorow, M. 1998. Combining local context and WordNet similarity for word sense identification. In: Fellbaum, C., 1998, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, 265?283. Mitkov R. and Ha L.A. 2003. Computer-aided generation of multiple-choice tests. Proceedings of the HLT/NAACL 2003 Workshop on Building educational applications using Natural Language Processing. Edmonton, Canada, 17-22. Mitkov, R., An, L.A. and Karamanis, N. 2006. "A computer-aided environment for generating multiple-choice test items". Journal of Natural Language Engineering, 12 (2): 177-194. Patwardhan, S, Banerjee, S. and Pedersen, T. 2003. Using Measures of Semantic Relatedness for Word Sense Disambiguation. Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics. Mexico City, 241-257. Resnik, P. 1995. Using information content to evaluate semantic similarity in a taxonomy. Proceedings of the 14th International Joint Conference on Artificial Intelligence. Montreal, 448?453. Tapanainen, P. and J?rvinen, T. 1997. A non-projective dependency parser. Proceedings of the 5th Conference of Applied Natural Language Processing, Washington, 64-71.  Vajda, E.J. 2001 Course Materials from the module of Introduction to Linguistics. Professor Edward J. Vajda Homepage, Washington, Western Washington University, Modern and Classical Languages.http://pandora.cii.wwu.edu/vajda/ling201/ling201home.htm 
56
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 706?715,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Elliphant: Improved Automatic Detection of
Zero Subjects and Impersonal Constructions in Spanish
Luz Rello?
NLP and Web Research Groups
Univ. Pompeu Fabra
Barcelona, Spain
Ricardo Baeza-Yates
Yahoo! Research
Barcelona, Spain
Ruslan Mitkov
Research Group in
Computational Linguistics
Univ. of Wolverhampton, UK
Abstract
In pro-drop languages, the detection of
explicit subjects, zero subjects and non-
referential impersonal constructions is cru-
cial for anaphora and co-reference resolu-
tion. While the identification of explicit
and zero subjects has attracted the atten-
tion of researchers in the past, the auto-
matic identification of impersonal construc-
tions in Spanish has not been addressed yet
and this work is the first such study. In
this paper we present a corpus to under-
pin research on the automatic detection of
these linguistic phenomena in Spanish and
a novel machine learning-based methodol-
ogy for their computational treatment. This
study also provides an analysis of the fea-
tures, discusses performance across two
different genres and offers error analysis.
The evaluation results show that our system
performs better in detecting explicit sub-
jects than alternative systems.
1 Introduction
Subject ellipsis is the omission of the subject in
a sentence. We consider not only missing refer-
ential subject (zero subject) as manifestation of
ellipsis, but also non-referential impersonal con-
structions.
Various natural language processing (NLP)
tasks benefit from the identification of ellip-
tical subjects, primarily anaphora resolution
(Mitkov, 2002) and co-reference resolution (Ng
and Cardie, 2002). The difficulty in detect-
ing missing subjects and non-referential pronouns
has been acknowledged since the first studies on
? This work was partially funded by a ?La Caixa? grant
for master students.
the computational treatment of anaphora (Hobbs,
1977; Hirst, 1981). However, this task is of cru-
cial importance when processing pro-drop lan-
guages since subject ellipsis is a pervasive phe-
nomenon in these languages (Chomsky, 1981).
For instance, in our Spanish corpus, 29% of the
subjects are elided.
Our method is based on classification of all ex-
pressions in subject position, including the recog-
nition of Spanish non-referential impersonal con-
structions which, to the best of our knowledge,
has not yet been addressed. The necessity of iden-
tifying such kind of elliptical constructions has
been specifically highlighted in work about Span-
ish zero pronouns (Ferra?ndez and Peral, 2000)
and co-reference resolution (Recasens and Hovy,
2009).
The main contributions of this study are:
? A public annotated corpus in Spanish to
compare different strategies for detecting ex-
plicit subjects, zero subjects and impersonal
constructions.
? The first ML based approach to this problem
in Spanish and a thorough analysis regarding
features, learnability, genre and errors.
? The best performing algorithms to automati-
cally detect explicit subjects and impersonal
constructions in Spanish.
The remainder of the paper is organized as fol-
lows. Section 2 describes the classes of Spanish
subjects, while Section 3 provides a literature re-
view. Section 4 describes the creation and the an-
notation of the corpus and in Section 5 the ma-
chine learning (ML) method is presented. The
analysis of the features, the learning curves, the
706
genre impact and the error analysis are all detailed
in Section 6. Finally, in Section 7, conclusions
are drawn and plans for future work are discussed.
This work is an extension of the first author mas-
ter?s thesis (Rello, 2010) and a preliminary ver-
sion of the algorithm was presented in Rello et al
(2010).
2 Classes of Spanish Subjects
Literature related to ellipsis in NLP (Ferra?ndez
and Peral, 2000; Rello and Illisei, 2009a; Mitkov,
2010) and linguistic theory (Bosque, 1989; Bru-
cart, 1999; Real Academia Espan?ola, 2009) has
served as a basis for establishing the classes of
this work.
Explicit subjects are phonetically realized and
their syntactic position can be pre-verbal or post-
verbal. In the case of post-verbal subjects (a), the
syntactic position is restricted by some conditions
(Real Academia Espan?ola, 2009).
(a) Carecera?n de validez las disposiciones que con-
tradigan otra de rango superior.1
The dispositions which contradict higher range
ones will not be valid.
Zero subjects (b) appear as the result of a nomi-
nal ellipsis. That is, a lexical element ?the elliptic
subject?, which is needed for the interpretation of
the meaning and the structure of the sentence, is
elided; therefore, it can be retrieved from its con-
text. The elision of the subject can affect the en-
tire noun phrase and not just the noun head when
a definite article occurs (Brucart, 1999).
(b) ? Fue refrendada por el pueblo espan?ol.
(It) was countersigned by the people of Spain.
The class of impersonal constructions is
formed by impersonal clauses (c) and reflex-
ive impersonal clauses with particle se (d) (Real
Academia Espan?ola, 2009).
(c) No hay matrimonio sin consentimiento.
(There is) no marriage without consent.
(d) Se estara? a lo que establece el apartado siguiente.
(It) will be what is established in the next section.
1All the examples provided are taken from our corpus.
In the examples, explicit subjects are presented in italics.
Zero subjects are presented by the symbol ? and in the En-
glish translations the subjects which are elided in Spanish are
marked with parentheses. Impersonal constructions are not
explicitly indicated.
3 Related Work
Identification of non-referential pronouns, al-
though a crucial step in co-reference and anaphora
resolution systems (Mitkov, 2010),2 has been ap-
plied only to the pleonastic it in English (Evans,
2001; Boyd et al 2005; Bergsma et al 2008)
and expletive pronouns in French (Danlos, 2005).
Machine learning methods are known to perform
better than rule-based techniques for identifying
non-referential expressions (Boyd et al 2005).
However, there is some debate as to which ap-
proach may be optimal in anaphora resolution
systems (Mitkov and Hallett, 2007).
Both English and French texts use an ex-
plicit word, with some grammatical information
(a third person pronoun), which is non-referential
(Mitkov, 2010). By contrast, in Spanish, non-
referential expressions are not realized by exple-
tive or pleonastic pronouns but rather by a certain
kind of ellipsis. For this reason, it is easy to mis-
take them for zero pronouns, which are, in fact,
referential.
Previous work on detecting Spanish subject el-
lipsis focused on distinguishing verbs with ex-
plicit subjects and verbs with zero subjects (zero
pronouns), using rule-based methods (Ferra?ndez
and Peral, 2000; Rello and Illisei, 2009b). The
Ferra?ndez and Peral algorithm (2000) outper-
forms the (Rello and Illisei, 2009b) approach
with 57% accuracy in identifying zero subjects.
In (Ferra?ndez and Peral, 2000), the implementa-
tion of a zero subject identification and resolution
module forms part of an anaphora resolution sys-
tem.
ML based studies on the identification of
explicit non-referential constructions in English
present accuracies of 71% (Evans, 2001), 87.5%
(Bergsma et al 2008) and 88% (Boyd et al
2005), while 97.5% is achieved for French (Dan-
los, 2005). However, in these languages, non-
referential constructions are explicit and not omit-
ted which makes this task more challenging for
Spanish.
4 Corpus
We created and annotated a corpus composed
of legal texts (law) and health texts (psychiatric
2In zero anaphora resolution, the identification of zero
anaphors first requires that they be distinguished from non-
referential impersonal constructions (Mitkov, 2010).
707
papers) originally written in peninsular Spanish.
The corpus is named after its annotated content
?Explicit Subjects, Zero Subjects and Impersonal
Constructions? (ESZIC es Corpus).
To the best of our knowledge, the existing cor-
pora annotated with elliptical subjects belong to
other genres. The Blue Book (handbook) and
Lexesp (journalistic texts) used in (Ferra?ndez and
Peral, 2000) contain zero subjects but not imper-
sonal constructions. On the other hand, the Span-
ish AnCora corpus based on journalistic texts in-
cludes zero pronouns and impersonal construc-
tions (Recasens and Mart??, 2010) while the Z-
corpus (Rello and Illisei, 2009b) comprises legal,
instructional and encyclopedic texts but has no an-
notated impersonal constructions.
The ESZIC corpus contains a total of 6,827
verbs including 1,793 zero subjects. Except for
AnCora-ES, with 10,791 elliptic pronouns, our
corpus is larger than the ones used in previous ap-
proaches: about 1,830 verbs including zero and
explicit subjects in (Ferra?ndez and Peral, 2000)
(the exact number is not mentioned in the pa-
per) and 1,202 zero subjects in (Rello and Illisei,
2009b).
The corpus was parsed by Connexor?s Ma-
chinese Syntax (Connexor Oy, 2006), which re-
turns lexical and morphological information as
well as the dependency relations between words
by employing a functional dependency grammar
(Tapanainen and Ja?rvinen, 1997).
To annotate our corpus we created an annota-
tion tool that extracts the finite clauses and the
annotators assign to each example one of the de-
fined annotation tags. Two volunteer graduate stu-
dents of linguistics annotated the verbs after one
training session. The annotations of a third volun-
teer with the same profile were used to compute
the inter-annotator agreement. During the anno-
tation phase, we evaluated the adequacy and clar-
ity of the annotation guidelines and established a
typology of the rising borderline cases, which is
included in the annotation guidelines.
Table 1 shows the linguistic and formal criteria
used to identify the chosen categories that served
as the basis for the corpus annotation. For each
tag, in addition to the two criteria that are crucial
for identifying subject ellipsis ([? elliptic] and
[? referential]) a combination of syntactic, se-
mantic and discourse knowledge is also encoded
during the annotation. The linguistic motivation
for each of the three categories is shown against
the thirteen annotation tags to which they belong
(Table 1).
Afterwards, each of the tags are grouped in one
of the three main classes.
? Explicit subjects: [- elliptic, + referential].
? Zero subjects: [+ elliptic, + referential].
? Impersonal constructions: [+ elliptic, - refer-
ential].
Of these annotated verbs, 71% have an explicit
subject, 26% have a zero subject and 3% belong
to an impersonal construction (see Table 2).
Number of instances Legal Health All
Explicit subjects 2,739 2,116 4,855
Zero subjects 619 1,174 1,793
Impersonals 71 108 179
Total 3,429 3,398 6,827
Table 2: Instances per class in ESZIC Corpus.
To measure inter-annotator reliability we use
Fleiss? Kappa statistical measure (Fleiss, 1971).
We extracted 10% of the instances of each of the
texts of the corpus covering the two genres.
Fleiss? Kappa Legal Health All
Two Annotators 0.934 0.870 0.902
Three Annotators 0.925 0.857 0.891
Table 3: Inter-annotator Agreement.
In Table 3 we present the Fleiss kappa inter-
annotator agreement for two and three annota-
tors. These results suggest that the annotation
is reliable since it is common practice among re-
searchers in computational linguistics to consider
0.8 as a minimum value of acceptance (Artstein
and Poesio, 2008).
5 Machine Learning Approach
We opted for an ML approach given that our
previous rule-based methodology improved only
0.02 over the 0.55 F-measure of a simple base-
line (Rello and Illisei, 2009b). Besides, ML based
methods for the identification of explicit non-
referential constructions in English appear to per-
form better than than rule-based ones (Boyd et al
2005).
708
LINGUISTIC INFORMATION PHONETIC
REALIZATION
SYNTACTIC
CATEGORY
VERBAL
DIATHESIS
SEMANTIC
INTERPR.
DISCOURSE
Annotation
Categories
Annotation
Tags
Elliptic
noun
phrase
Ell. noun
phrase
head
Nominal
subject
Active Active
participant
Referential
subject
Explicit subject ? ? + + + +
Explicit
subject
Reflex passive
subject
? ? + + ? +
Passive subject ? ? + ? ? +
Omitted subject + ? + + + +
Omitted subject
head
? + + + + +
Non-nominal
subject
? ? ? + + +
Zero
subject
Reflex passive
omitted subject
+ ? + + ? +
Reflex pass. omit-
ted subject head
? + + + ? +
Reflex pass. non-
nominal subject
? ? ? + ? +
Passive omitted
subject
+ ? + ? ? +
Pass. non-nominal
subject
? ? ? ? ? +
Impersonal
construction
Reflex imp. clause
(with se)
? ? n/a ? n/a ?
Imp. construction
(without se)
? ? n/a + n/a ?
Table 1: ESZIC Corpus Annotation Tags.
5.1 Features
We built the training data from the annotated cor-
pus and defined fourteen features. The linguisti-
cally motivated features are inspired by previous
ML approaches in Chinese (Zhao and Ng, 2007)
and English (Evans, 2001). The values for the fea-
tures (see Table 4) were derived from information
provided both by Connexor?s Machinese Syntax
parser and a set of lists.
We can describe each of the features as broadly
belonging to one of ten classes, as follows:
1 PARSER: the presence or absence of a sub-
ject in the clause, as identified by the parser.
We are not aware of a formal evaluation of
Connexor?s accuracy. It presents an accu-
racy of 74.9% evaluated against our corpus
and we used it as a simple baseline.
2 CLAUSE: the clause types considered are:
main clauses, relative clauses starting with a
complex conjunction, clauses starting with a
simple conjunction, and clauses introduced
using punctuation marks (commas, semi-
colons, etc). We implemented a method
to identify these different types of clauses,
as the parser does not explicitly mark the
boundaries of clauses within sentences. The
method took into account the existence of a
finite verb, its dependencies, the existence of
conjunctions and punctuation marks.
3 LEMMA: lexical information extracted from
the parser, the lemma of the finite verb.
4-5 NUMBER, PERSON: morphological infor-
mation of the verb, its grammatical number
and its person.
6 AGREE: feature which encodes the tense,
mood, person, and number of the verb in the
clause, and its agreement in person, number,
709
Feature Definition Value
1 PARSER Parsed subject True, False
2 CLAUSE Clause type Main, Rel, Imp, Prop, Punct
3 LEMMA Verb lemma Parser?s lemma tag
4 NUMBER Verb morphological number SG, PL
5 PERSON Verb morphological person P1, P2, P3
6 AGREE Agreement in person, number, tense FTFF, TTTT, FFFF, TFTF, TTFF, FTFT, FTTF, TFTT,
and mood FFFT, TTTF, FFTF, TFFT, FFTT, FTTT, TFFF, TTFT
7 NHPREV Previous noun phrases Number of noun phrases previous to the verb
8 NHTOT Total noun phrases Number of noun phrases in the clause
9 INF Infinitive Number of infinitives in the clause
10 SE Spanish particle se True, False
11 A Spanish preposition a True, False
12 POSpre Four parts of the speech previous to 292 different values combining the parser?s
the verb POS tags
14 POSpos Four parts of the speech following 280 different values combining the parser?s
the verb POS tags
14 VERBtype Type of verb: copulative, impersonal CIPX, XIXX, XXXT, XXPX, XXXI, CIXX, XXPT, XIPX,
pronominal, transitive and intransitive XIPT, XXXX, XIXI, CXPI, XXPI, XIPI, CXPX
Table 4: Features, definitions and values.
tense, and mood with the preceding verb in
the sentence and also with the main verb of
the sentence.3
7-9 NHPREV, NHTOT, INF: the candidates for
the subject of the clause are represented by
the number of noun phrases in the clause that
precede the verb, the total number of noun
phrases in the clause, and the number of in-
finitive verbs in the clause.
10 SE: a binary feature encoding the presence
or absence of the Spanish particle se when it
occurs immediately before or after the verb
or with a maximum of one token lying be-
tween the verb and itself. Particle se occurs
in passive reflex clauses with zero subjects
and in some impersonal constructions.
11 A: a binary feature encoding the presence or
absence of the Spanish preposition a in the
clause. Since the distinction between passive
reflex clauses with zero subjects and imper-
sonal constructions sometimes relies on the
appearance of preposition a (to, for, etc.).
For instance, example (e) is a passive reflex
clause containing a zero subject while exam-
ple (s) is an impersonal construction.
3In Spanish, when a finite verb appears in a subordinate
clause, its tense and mood can assist in recognition of these
features in the verb of the main clause and help to enforce
some restrictions required by this verb, especially when both
verbs share the same referent as subject.
(e) Se admiten los alumnos que reu?nan los req-
uisitos.
? (They) accept the students who fulfill the
requirements.
(f) Se admite a los alumnos que reu?nan los req-
uisitos.
(It) is accepted for the students who fulfill
the requirements.
12-3 POSpre, POSpos: the part of the speech
(POS) of eight tokens, that is, the 4-grams
preceding and the 4-grams following the in-
stance.
14 VERBtype: the verb is classified as copula-
tive, pronominal, transitive, or with an im-
personal use.4 Verbs belonging to more than
one class are also accommodated with dif-
ferent feature values for each of the possible
combinations of verb type.
5.2 Evaluation
To determine the most accurate algorithm for our
classification task, two comparisons of learning
algorithms implemented in WEKA (Witten and
Frank, 2005) were carried out. Firstly, the classi-
fication was performed using 20% of the training
instances. Secondly, the seven highest perform-
ing classifiers were compared using 100% of the
4We used four lists provided by Molino de Ideas s.a. con-
taining 11,060 different verb lemmas belonging to the Royal
Spanish Academy Dictionary (Real Academia Espan?ola,
2001).
710
Class P R F Acc.
Explicit subj. 90.1% 92.3% 91.2% 87.3%
Zero subj. 77.2% 74.0% 75.5% 87.4%
Impersonals 85.6% 63.1% 72.7% 98.8%
Table 5: K* performance (87.6% accuracy for ten-fold
cross validation).
training data and ten-fold cross-validation. The
corpus was partitioned into training and tested
using ten-fold cross-validation for randomly or-
dered instances in both cases. The lazy learn-
ing classifier K* (Cleary and Trigg, 1995), us-
ing a blending parameter of 40%, was the best
performing one, with an accuracy of 87.6% for
ten-fold cross-validation. K* differs from other
instance-based learners in that it computes the dis-
tance between two instances using a method mo-
tivated by information theory, where a maximum
entropy-based distance function is used (Cleary
and Trigg, 1995). Table 5 shows the results
for each class using ten-fold cross-validation.
In contrast to previous work, the K* algorithm
(Cleary and Trigg, 1995) was found to provide the
most accurate classification in the current study.
Other approaches have employed various clas-
sification algorithms, including JRip in WEKA
(Mu?ller, 2006), with precision of 74% and recall
of 60%, and K-nearest neighbors in TiMBL: both
in (Evans, 2001) with precision of 73% and recall
of 69%, and in (Boyd et al 2005) with precision
of 82% and recall of 71%.
Since there is no previous ML approach for this
task in Spanish, our baselines for the explicit sub-
jects and the zero subjects are the parser output
and the previous rule-based work with the high-
est performance (Ferra?ndez and Peral, 2000). For
the impersonal constructions the baseline is a sim-
ple greedy algorithm that classifies as an imper-
sonal construction every verb whose lemma is cat-
egorized as a verb with impersonal use according
to the RAE dictionary (Real Academia Espan?ola,
2001).
Our method outperforms the Connexor parser
which identifies the explicit subjects but makes no
distinction between zero subjects and impersonal
constructions. Connexor yields 74.9% overall ac-
curacy and 80.2% and 65.6% F-measure for ex-
plicit and elliptic subjects, respectively.
To compare with Ferra?ndez and Peral
(Ferra?ndez and Peral, 2000) we do consider
Algorithm Explicit
subjects
Zero
subjects
Impersonals
RAE ? ? 70.4%
Connexor 71.7% 83.0%
Ferr./Peral 79.7% 98.4% ?
Elliphant 87.3% 87.4% 98.8%
Table 6: Summary of accuracy comparison with previ-
ous work.
it without impersonal constructions. We achieve
a precision of 87% for explicit subjects compared
to 80%, and a precision of 87% for zero subjects
compared to their 98%. The overall accuracy
is the same for both techniques, 87.5%, but our
results are more balanced. Nevertheless, the
approaches and corpora used in both studies are
different, and hence it is not possible to do a fair
comparison. For example, their corpus has 46%
of zero subjects while ours has only 26%.
For impersonal constructions our method out-
performs the RAE baseline (precision 6.5%,
recall 77.7%, F-measure 12.0% and accuracy
70.4%). Table 6 summarizes the comparison. The
low performance of the RAE baseline is due to the
fact that verbs with impersonal use are often am-
biguous. For these cases, we first tagged them as
ambiguous and then, we defined additional crite-
ria after analyzing then manually. The resulting
annotated criteria are stated in Table 1.
6 Analysis
Through these analyses we aim to extract the most
effective features and the information that would
complement the output of an standard parser to
achieve this task. We also examine the learning
process of the algorithm to find out how many in-
stances are needed to train it efficiently and de-
termine how much Elliphant is genre dependent.
The analyses indicate that our approach is robust:
it performs nearly as well with just six features,
has a steep learning curve, and seems to general-
ize well to other text collections.
6.1 Best Features
We carried out three different experiments to eval-
uate the most effective group of features, and
the features themselves considering the individ-
ual predictive ability of each one along with their
degree of redundancy.
Based on the following three feature selection
711
methods we can state that there is a complex and
balanced interaction between the features.
6.1.1 Grouping Features
In the first experiment we considered the 11
groups of relevant ordered features from the train-
ing data, which were selected using each WEKA
attribute selection algorithm and performed the
classifications over the complete training data, us-
ing only the different groups features selected.
The most effective group of six features (NH-
PREV, PARSER, NHTOT, POSpos, PERSON,
LEMMA) was the one selected by WEKA?s Sym-
metricalUncertAttribute technique, which gives
an accuracy of 83.5%. The most frequently
selected features by all methods are PARSER,
POSpos, and NHTOT, and they alone get an accu-
racy of 83.6% together. As expected, the two pairs
of features that perform best (both 74.8% accu-
racy) are PARSER with either POSpos or NHTOT.
Based on how frequent each feature is selected
by WEKA?s attribute selection algorithms, we can
rank the features as following: (1) PARSER,
(2) NHTOT, (3) POSpos, (4) NHPREV and (5)
LEMMA.
6.1.2 ?Complex? vs. ?Simple? Features
Second, a set of experiments was conducted
in which features were selected on the basis
of the degree of computational effort needed to
generate them. We propose two sets of fea-
tures. One group corresponds to ?simple? fea-
tures, whose values can be obtained by trivial
exploitation of the tags produced in the parser?s
output (PARSER, LEMMA, PERSON, POSpos,
POSpre). The second group of features, ?com-
plex? features (CLAUSE, AGREE, NHPREV,
NHTOT, VERBtype) have values that required the
implementation of more sophisticated modules to
identify the boundaries of syntactic constituents
such as clauses and noun phrases. The accuracy
obtained when the classifier exclusively exploits
?complex? features is 82.6% while for ?simple?
features is 79.9%. No impersonal constructions
are identified when only ?complex? features are
used.
6.1.3 One-left-out Feature
In the third experiment, to estimate the weight
of each feature, classifications were made in
which each feature was omitted from the train-
ing instances that were presented to the classifier.
Omission of all but one of the ?simple? features
led to a reduction in accuracy, justifying their in-
clusion in the training instances. Nevertheless, the
majority of features present low informativeness
except for feature A which does not make any
meaningful contribution to the classification. The
feature PARSER presents the greatest difference
in performance (86.3% total accuracy); however,
this is no big loss, considering it is the main fea-
ture. Hence, as most features do not bring a sig-
nificant loss in accuracy, the features need to be
combined to improve the performance.
6.2 Learning Analysis
The learning curve of Figure 1 (left) presents the
increase of the performance obtained by Elliphant
using the training data randomly ordered. The
performance reaches its plateau using 90% of the
training instances. Using different ordering of the
training set we obtain the same result.
Figure 1 (right) presents the precision for each
class and overall in relation to the number of train-
ing instances for each one of them. Recall grows
similarly to precision. Under all conditions, sub-
jects are classified with a high precision since the
information given by the parser (collected in the
features) achieves an accuracy of 74.9% for the
identification of explicit subjects.
The impersonal construction class has the
fastest learning curve. When utilizing a training
set of only 163 instances (90% of the training
data), it reaches a precision of 63.2%. The un-
stable behaviour for impersonal constructions can
be attributed to not having enough training data
for that class, since impersonals are not frequent
in Spanish. On the other hand, the zero subject
class is learned more gradually.
The learning curve for the explicit subject class
is almost flat due to the great variety of subjects
occurring in the training data. In addition, reach-
ing a precision of 92.0% for explicit subjects us-
ing just 20% of the training data is far more ex-
pensive in terms of the number of training in-
stances (978) as seen in Figure 1 (right). Actually,
with just 20% of the training data we can already
achieve a precision of 85.9%.
This demonstrates that Elliphant does not need
very large sets of expensive training data and
is able to reach adequate levels of performance
when exploiting far fewer training instances. In
fact, we see that we only need a modest set of
712
83.00
83.60
84.20
84.80
85.40
86.00
86.60
10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 
Precision Recall F-measure
85.6%
85.3%
85.8%
85.7%
85.2%
85.8%
86.3%
86.4%
85.9%
85.5%
86.0%
86.5% 86.6%
%
49.00
55.29
61.57
67.86
74.14
80.43
86.71
93.00
10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 
498
978
1461
1929
2433
2898
3400
3899
4386
4854
354
537
735
898 1094
1249
1416
1593
1793
167
17
32
49
66
82
129
146
179
Explicit subjects
Zero subjects
Impersonal
constructions
Overall
163
103
 
P
r
e
c
i
s
i
o
n
 
(
%
)
Figure 1: Learning curve for precision, recall and F-measure (left) and with respect to the number of instances
of each class (right) for a given percentage of training data.
annotated instances (fewer than 1,500) to achieve
good results.
6.3 Impact of Genre
To examine the influence of the different text gen-
res on this method, we divided our training data
into two subgroups belonging to different genres
(legal and health) and analyze the differences.
A comparative evaluation using ten-fold cross-
validation over the two subgroups shows that El-
liphant is more successful when classifying in-
stances of explicit subjects in legal texts (89.8%
accuracy) than health texts (85.4% accuracy).
This may be explained by the greater uniformity
of the sentences in the legal genre compared to
ones from the health genre, as well as the fact that
there are a larger number of explicit subjects in the
legal training data (2,739 compared with 2,116 in
the health texts). Further, texts from the health
genre present the additional complication of spe-
cialized named entities and acronyms, which are
used quite frequently. Similarly, better perfor-
mance in the detection of zero subjects and imper-
sonal sentences in the health texts may be due to
their more frequent occurrence and hence greater
learnability.
Training/Testing Legal Health All
Legal 90.0% 86.8% 89.3%
Health 86.8% 85.9% 88.7%
All 92.5% 93.7% 87.6%
Table 7: Accuracy of cross-genre training and testing
evaluation (ten-fold evaluation).
We have also studied the effect of training the
classifier on data derived from one genre and test-
ing on instances derived from a different genre.
Table 7 shows that instances from legal texts
are more homogeneous, as the classifier obtains
higher accuracy when testing and training only on
legal instances (90.0%). In addition, legal texts
are also more informative, because when both le-
gal and health genres are combined as training
data, only instances from the health genre show
a significant increased accuracy (93.7%). These
results reveal that the health texts are the most het-
erogeneous ones. In fact, we also found subsets of
the legal documents where our method achieves
an accuracy of 94.6%, implying more homoge-
neous texts.
6.4 Error Analysis
Since the features of the system are linguisti-
cally motivated, we performed a linguistic anal-
ysis of the erroneously classified instances to find
out which patterns are more difficult to classify
and which type of information would improve the
method (Rello et al 2011).
We extract the erroneously classified instances
of our training data and classify the errors. Ac-
cording to the distribution of the errors per class
(Table 8) we take into account the following four
classes of errors for the analysis: (a) impersonal
constructions classified as zero subjects, (b) im-
personal constructions classified as explicit sub-
jects, (c) zero subjects classified as explicit sub-
jects, and (d) explicit subjects classified as zero
subjects. The diagonal numbers are the true pre-
dicted cases. The classification of impersonal
constructions is less balanced than the ones for
explicit subjects and zero subjects. Most of the
wrongly identified instances are classified as ex-
plicit subject, given that this class is the largest
one. On the other hand, 25% of the zero subjects
are classified as explicit subject, while only 8% of
713
the explicit subjects are identified as zero subjects.
Class Zero Explicit Impers.
subjects subjects
Zero subj. 1327 453 (c) 13
Explicit subj. 368 (d) 4481 6
Impersonals 25 (a) 41 (b) 113
Table 8: Confusion Matrix (ten-fold validation).
For the analysis we first performed an explo-
ration of the feature values which allows us to
generate smaller samples of the groups of errors
for the further linguistic analyses. Then, we ex-
plore the linguistic characteristics of the instances
by examining the clause in which the instance ap-
pears in our corpus. A great variety of different
patterns are found. We mention only the linguistic
characteristics in the errors which at least double
the corpus general trends.
In all groups (a-d) there is a tendency of using
the following elements: post-verbal prepositions,
auxiliary verbs, future verbal tenses, subjunctive
verbal mode, negation, punctuation marks ap-
pearing before the verb and the preceding noun
phrases, concessive and adverbial subordinate
clauses. In groups (a) and (b) the lemma of the
verb may play a relevant role, for instance verb
haber (?there is/are?) appears in the errors seven
times more than in the training while verb tratar
(?to be about?, ?to deal with?) appears 12 times
more. Finally, in groups (c) and (d) we notice
the frequent occurrence of idioms which include
verbs with impersonal uses, such as es decir (?that
is to say?) and words which can be subject on their
own i.e. ambos (?both?) or todo (?all?).
7 Conclusions and Future Work
In this study we learn which is the most accurate
approach for identifying explicit subjects and im-
personal constructions in Spanish and which are
the linguistic characteristics and features that help
to perform this task. The corpus created is freely
available online.5 Our method complements pre-
vious work on Spanish anaphora resolution by ad-
dressing the identification of non-referential con-
structions. It outperforms current approaches in
explicit subject detection and impersonal con-
structions, doing better than the parser for every
5ESZIC es Corpus is available at: http:
//luzrello.com/Projects.html.
class.
A possible future avenue to explore could be
to combine our approach with Ferra?ndez and
Peral (Ferra?ndez and Peral, 2000) by employing
both algorithms in sequence: first Ferra?ndez and
Peral?s algorithm to detect all zero subjects and
then ours to identify explicit subjects and imper-
sonals. Assuming that the same accuracy could be
maintained, on our data set the combined perfor-
mance could potentially be in the range of 95%.
Future research goals are the extrinsic evalua-
tion of our system by integrating our system in
NLP tasks and its adaptation to other Romance
pro-drop languages. Finally, we believe that our
ML approach could be improved as it is the first
attempt of this kind.
Acknowledgements
We thank Richard Evans, Julio Gonzalo and the
anonymous reviewers for their wise comments.
References
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
S. Bergsma, D. Lin, and R. Goebel. 2008. Distri-
butional identification of non-referential pronouns.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies (ACL/HLT-08), pages 10?
18.
I. Bosque. 1989. Clases de sujetos ta?citos. In Julio
Borrego Nieto, editor, Philologica: homenaje a An-
tonio Llorente, volume 2, pages 91?112. Servicio
de Publicaciones, Universidad Pontificia de Sala-
manca, Salamanca.
A. Boyd, W. Gegg-Harrison, and D. Byron. 2005.
Identifying non-referential it: a machine learning
approach incorporating linguistically motivated pat-
terns. In Proceedings of the ACL Workshop on Fea-
ture Engineering for Machine Learning in Natural
Language Processing. 43rd Annual Meeting of the
Association for Computational Linguistics (ACL-
05), pages 40?47.
J. M. Brucart. 1999. La elipsis. In I. Bosque
and V. Demonte, editors, Grama?tica descriptiva de
la lengua espan?ola, volume 2, pages 2787?2863.
Espasa-Calpe, Madrid.
N. Chomsky. 1981. Lectures on Government and
Binding. Mouton de Gruyter, Berlin, New York.
J.G. Cleary and L.E. Trigg. 1995. K*: an instance-
based learner using an entropic distance measure.
In Proceedings of the 12th International Conference
on Machine Learning (ICML-95), pages 108?114.
714
Connexor Oy, 2006. Machinese language model.
L. Danlos. 2005. Automatic recognition of French
expletive pronoun occurrences. In Robert Dale,
Kam-Fai Wong, Jiang Su, and Oi Yee Kwong, ed-
itors, Natural language processing. Proceedings of
the 2nd International Joint Conference on Natural
Language Processing (IJCNLP-05), pages 73?78,
Berlin, Heidelberg, New York. Springer. Lecture
Notes in Computer Science, Vol. 3651.
R. Evans. 2001. Applying machine learning: toward
an automatic classification of it. Literary and Lin-
guistic Computing, 16(1):45?57.
A. Ferra?ndez and J. Peral. 2000. A computational ap-
proach to zero-pronouns in Spanish. In Proceedings
of the 38th Annual Meeting of the Association for
Computational Linguistics (ACL-2000), pages 166?
172.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
G. Hirst. 1981. Anaphora in natural language under-
standing: a survey. Springer-Verlag.
J. Hobbs. 1977. Resolving pronoun references. Lin-
gua, 44:311?338.
R. Mitkov and C. Hallett. 2007. Comparing pronoun
resolution algorithms. Computational Intelligence,
23(2):262?297.
R. Mitkov. 2002. Anaphora resolution. Longman,
London.
R. Mitkov. 2010. Discourse processing. In Alexander
Clark, Chris Fox, and Shalom Lappin, editors, The
handbook of computational linguistics and natural
language processing, pages 599?629. Wiley Black-
well, Oxford.
C. Mu?ller. 2006. Automatic detection of nonrefer-
ential it in spoken multi-party dialog. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-06), pages 49?56.
V. Ng and C. Cardie. 2002. Identifying anaphoric
and non-anaphoric noun phrases to improve coref-
erence resolution. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING-02), pages 1?7.
Real Academia Espan?ola. 2001. Diccionario de la
lengua espan?ola. Espasa-Calpe, Madrid, 22 edi-
tion.
Real Academia Espan?ola. 2009. Nueva grama?tica de
la lengua espan?ola. Espasa-Calpe, Madrid.
M. Recasens and E. Hovy. 2009. A deeper
look into features for coreference resolution. In
Lalitha Devi Sobha, Anto?nio Branco, and Ruslan
Mitkov, editors, Anaphora Processing and Applica-
tions. Proceedings of the 7th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC-09),
pages 29?42. Springer, Berlin, Heidelberg, New
York. Lecture Notes in Computer Science, Vol.
5847.
M. Recasens and M.A. Mart??. 2010. Ancora-
co: Coreferentially annotated corpora for Spanish
and Catalan. Language resources and evaluation,
44(4):315?345.
L. Rello and I. Illisei. 2009a. A comparative study
of Spanish zero pronoun distribution. In Proceed-
ings of the International Symposium on Data and
Sense Mining, Machine Translation and Controlled
Languages, and their application to emergencies
and safety critical domains (ISMTCL-09), pages
209?214. Presses Universitaires de Franche-Comte?,
Besanc?on.
L. Rello and I. Illisei. 2009b. A rule-based approach
to the identification of Spanish zero pronouns. In
Student Research Workshop. International Confer-
ence on Recent Advances in Natural Language Pro-
cessing (RANLP-09), pages 209?214.
L. Rello, P. Sua?rez, and R. Mitkov. 2010. A machine
learning method for identifying non-referential im-
personal sentences and zero pronouns in Spanish.
Procesamiento del Lenguaje Natural, 45:281?287.
L. Rello, G. Ferraro, and A. Burga. 2011. Error analy-
sis for the improvement of subject ellipsis detection.
Procesamiento de Lenguaje Natural, 47:223?230.
L. Rello. 2010. Elliphant: A machine learning method
for identifying subject ellipsis and impersonal con-
structions in Spanish. Master?s thesis, Erasmus
Mundus, University of Wolverhampton & Univer-
sitat Auto`noma de Barcelona.
P. Tapanainen and T. Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing
(ANLP-97), pages 64?71.
I. H. Witten and E. Frank. 2005. Data mining: practi-
cal machine learning tools and techniques. Morgan
Kaufmann, London, 2 edition.
S. Zhao and H.T. Ng. 2007. Identification and resolu-
tion of Chinese zero pronouns: a machine learning
approach. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP/CNLL-07), pages 541?550.
715
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 1?10,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
One Step Closer to Automatic Evaluation
of Text Simplification Systems
Sanja
?
Stajner
1
and Ruslan Mitkov
1
and Horacio Saggion
2
1
Research Group in Computational Linguistics, University of Wolverhampton, UK
2
TALN Research Group, Universitat Pompeu Fabra, Spain
S.Stajner@wlv.ac.uk, R.Mitkov@wlv.ac.uk, horacio.saggion@upf.edu
Abstract
This study explores the possibility of re-
placing the costly and time-consuming
human evaluation of the grammaticality
and meaning preservation of the output
of text simplification (TS) systems with
some automatic measures. The focus is on
six widely used machine translation (MT)
evaluation metrics and their correlation
with human judgements of grammatical-
ity and meaning preservation in text snip-
pets. As the results show a significant cor-
relation between them, we go further and
try to classify simplified sentences into:
(1) those which are acceptable; (2) those
which need minimal post-editing; and (3)
those which should be discarded. The pre-
liminary results, reported in this paper, are
promising.
1 Introduction
Lexically and syntactically complex sentences can
be difficult to understand for non-native speak-
ers (Petersen and Ostendorf, 2007; Alu??sio et
al., 2008b), and for people with language impair-
ments, e.g. people diagnosed with aphasia (Car-
roll et al., 1999; Devlin, 1999), autism spectrum
disorder (
?
Stajner et al., 2012; Martos et al., 2012),
dyslexia (Rello, 2012), congenital deafness (Inui
et al., 2003), and intellectual disability (Feng,
2009). At the same time, long and complex sen-
tences are also a stumbling block for many NLP
tasks and applications such as parsing, machine
translation, information retrieval, and summarisa-
tion (Chandrasekar et al., 1996). This justifies the
need for Text Simplification (TS) systems which
would convert such sentences into their simpler
and easier-to-read variants, while at the same time
preserving the original meaning.
So far, TS systems have been developed for En-
glish (Siddharthan, 2006; Zhu et al., 2010; Wood-
send and Lapata, 2011a; Coster and Kauchak,
2011; Wubben et al., 2012), Spanish (Saggion et
al., 2011), and Portuguese (Alu??sio et al., 2008a),
with recent attempts at Basque (Aranzabe et al.,
2012), Swedish (Rybing et al., 2010), Dutch
(Ruiter et al., 2010), and Italian (Barlacchi and
Tonelli, 2013).
Usually, TS systems are either evaluated for: (1)
the quality of the generated output, or (2) the effec-
tiveness/usefulness of such simplification on read-
ing speed and comprehension of the target popula-
tion. For the purpose of this study we focused only
on the former. The quality of the output generated
by TS systems is commonly evaluated by using
a combination of readability metrics (measuring
the degree of simplification) and human assess-
ment (measuring the grammaticality and meaning
preservation). Despite the noticeable similarity
between evaluation of the fluency and adequacy of
a machine translation (MT) output, and evaluation
of grammaticality and meaning preservation of a
TS system output, there have been no works ex-
ploring whether any of the MT evaluation metrics
are well correlated with the latter, and could thus
replace the time-consuming human assessment.
The contributions of the present work are the
following:
? It is the first study to explore the possibility of
replacing human assessment of the quality of
TS system output with automatic evaluation.
? It is the first study to investigate the correla-
tion of human assessment of TS system out-
put with MT evaluation metrics.
? It proposes a decision-making procedure for
the classification of simplified sentences into:
(1) those which are acceptable; (2) those
which need further post-editing; and (3) those
which should be discarded.
1
2 Related Work
The output of the TS system proposed by Sid-
dharthan (2006) was rated for grammaticality and
meaning preservation by three human evaluators.
Similarly, Drndarevic et al. (2013) evaluated the
grammaticality and the meaning preservation of
automatically simplified Spanish sentences on a
Likert scale with the help of twenty-five human
annotators. Additionally, the authors used seven
readability metrics to assess the degree of simplifi-
cation. Woodsend and Lapata (2011b), and Glava?s
and
?
Stajner (2013) used human annotators? rat-
ings for evaluating simplification, meaning preser-
vation, and grammaticality, while additionally ap-
plying several readability metrics for evaluating
complexity reduction in entire texts.
Another set of studies approached TS as an MT
task translating from ?original? to ?simplified?
language, e.g. (Specia, 2010; Woodsend and Lap-
ata, 2011a; Zhu et al., 2010). In this case, the qual-
ity of the output generated by the system was eval-
uated using several standard MT evaluation met-
rics: BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), and TERp (Snover et al., 2009).
3 Methodology
All experiments were conducted on a freely avail-
able sentence-level dataset
1
, fully described in
(Glava?s and
?
Stajner, 2013), and the two datasets
we derived from it. The original dataset and the
instructions for the human assessment are given in
the next two subsections. Section 3.3 explains how
we derived two additional datasets from the origi-
nal one, and to what end. Section 3.4 describes the
automatic MT evaluation metrics used as features
in correlation and classification experiments; Sec-
tion 3.5 presents the main goals of the study; and
Section 3.6 describes the conducted experiments.
3.1 Original dataset
The dataset contains 280 pairs of original sen-
tences and their corresponding simplified versions
annotated by humans for grammaticality, meaning
preservation, and simplicity of the simplified ver-
sion. We used all sentence pairs, focusing only on
four out of eight available features: (1) the original
text, (2) the simplified text, (3) the grammaticality
score, and (4) the score for meaning preservation.
2
1
http://takelab.fer.hr/data/evsimplify/
2
The other four features contain the pairID, groupID, the
method with which the simplification was obtained, and the
Category weighted ? Pearson MAE
Grammaticality 0.68 0.77 0.18
Meaning 0.53 0.67 0.37
Simplicity 0.54 0.60 0.28
Table 1: IAA from (Glava?s and
?
Stajner, 2013)
The simplified versions of original sentences
were obtained by using four different simplifi-
cation methods: baseline, sentence-wise, event-
wise, and pronominal anaphora. The baseline re-
tains only the main clause of a sentence, and dis-
cards all subordinate clauses, based on the out-
put of the Stanford constituency parser (Klein and
Manning, 2003). Sentence-wise simplification
eliminates all those tokens in the original sentence
that do not belong to any of the extracted factual
event mentions, while the event-wise simplifica-
tion transforms each factual event mention into a
separate sentence of the output. The last simplifi-
cation scheme (pronominal anaphora) additionally
employs pronominal anaphora resolution on top of
the event-wise simplification scheme.
3
3.2 Human Assessment
Human assessors were asked to score the given
sentence pairs (or text snippets in the case of split
sentences) on a 1?3 scale based on three crite-
ria: Grammaticality (1 ? ungrammatical, 2 ? mi-
nor problems with grammaticality, 3 ? grammati-
cal), Meaning (1 ? meaning is seriously changed
or most of the relevant information lost, 2 ? some
of the relevant information is lost but the meaning
of the remaining information is unchanged, 3 ? all
relevant information is kept without any change in
meaning), and Simplicity (1 ? a lot of irrelevant in-
formation is retained, 2 ? some of irrelevant infor-
mation is retained, 3 ? all irrelevant information is
eliminated). The inter-annotator agreement (IAA)
was calculated using weighted Kappa (weighted
?), Pearson?s correlation (Pearson), and mean av-
erage error (MAE), and the obtained results are
presented in Table 1. A few examples of assigned
scores are given in Table 2, where G, M, and S
denote human scores for grammaticality, meaning
preservation and simplicity respectively.
score for simplicity, which are not relevant here.
3
For more detailed explanation of simplification schemes
and the dataset see (Glava?s and
?
Stajner, 2013).
2
Ex. Original Simplified G M S SM
(a) ?It is understood the dead girl had
been living at her family home, in
a neighbouring housing estate, and
was visiting her older sister at the
time of the shooting.?
?The dead girl had been living at
her family home, in a neighbouring
housing estate and was visiting her
older sister.?
3 3 3 S
(b) ?On Facebook, more than 10,000
people signed up to a page an-
nouncing an opposition rally for
Saturday.?
?On Facebook, more than 10,000
people signed to a page announcing
an opposition rally for Saturday.?
2 3 3 S
(c) ?Joel Elliott, also 22, of North
Road, Brighton, was charged on
May 3 with murder. He appeared at
Lewes Crown Court on May 8 but
did not enter a plea.?
?Joel Elliott was charged on May 3
with murder. He appeared at Lewes
Crown Court on May 8.?
3 2 3 S
(d) ?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded capture and was one of the
world?s most wanted men, but his
time on the run finally ended last
year when he was arrested near
Belgrade.?
?For years the former Bosnia Serb
army commander Ratko Mladic had
evaded but his time the run ended
last year he was arrested near Bel-
grade.?
1 2 3 S
(e) ?Police have examined the scene at
a house at William Court in Bel-
laghy, near Magherafelt for clues to
the incident which has stunned the
community.?
?Police have examined the scene
at William Court near Magherafelt.
The incident has stunned the com-
munity.?
3 1 3 P
(f) ?Rastan, 25 km (15 miles) north
of Homs city, has slipped in and
out of government control several
times since the uprising against As-
sad erupted in March 2011.?
?Rastan has slipped government
control several times. The uprising
erupted in March 2011.?
2 1 3 P
(g) ?But opposition parties and inter-
national observers said the vote was
marred by vote-rigging, including
alleged ballot-box stuffing and false
voter rolls.?
?But opposition parties and inter-
national observers said .?
1 1 3 B
(h) ?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
?Foreign Affairs Secretary Albert
del Rosario was seeking a diplo-
matic solution with Chinese Ambas-
sador Ma Keqing, the TV network
said.?
3 3 1 B
(h) ? On Wednesday, two video jour-
nalists working for the state-owned
RIA Novosti news agency were
briefly detained outside the Election
Commission building where Putin
was handing in his application to
run.?
?On Wednesday two video journal-
ists were briefly detained outside
the Election Commission building.
Two video journalists worked for
the state-owned RIA Novosti news
agency. Putin was handing in his
application.?
3 2 2 E
Table 2: Human evaluation examples (G, M, and S correspond to the human scores for grammaticality,
meaning preservation and simplicity, and SM denotes the simplification method used: B ? baseline, S ?
sentence-wise, E ? event-wise, and P ? pronominal anaphora)
3
3.3 Derived Datasets
The original dataset (Original) contains separate
scores for grammaticality (G), meaning preserva-
tion (M), and simplicity (S), each of them on a 1?3
scale. From this dataset we derived two additional
ones: Total3 and Total2.
The Total3 dataset contains three marks (OK ?
use as it is, PE ? post-editing required, and Dis
? discard) derived from G and M in the Original
dataset. Those simplified sentences which scored
?3? for both meaning preservation (M) and gram-
maticality (G) are placed in the OK class as they
do not need any kind of post-editing. A closer
look at the remaining sentences suggests that any
simplified sentence which got a score ?2? or ?3?
for meaning preservation (M) could be easily post-
edited, i.e. it requires minimal changes which are
obvious from its comparison to the corresponding
original. For instance, in the sentence (b) in Ta-
ble 2 the only change that needs to be made is
adding the word ?up? after ?signed?. Those sen-
tences which scored ?2? for meaning need slightly
more, albeit simple modification. The simplified
text snippet (c) in Table 2 would need ?but did
not enter a plea? added at the end of the last
sentence. The next sentence (d) in the same ta-
ble needs a few more changes, but still very mi-
nor ones: adding the word ?capture? after ?had
evaded?, adding the preposition ?on? before ?the
run?, and adding ?when? after ?last year?. There-
fore, we grouped all those sentences into one class
? PE (sentences which require a minimal post-
editing effort). Those sentences which scored ?1?
for meaning need to either be left in their original
form or simplified from scratch. We thus classify
them as Dis. This newly created dataset (Total3)
allows us to investigate whether we could auto-
matically classify simplified sentences into those
three categories, taking into account both gram-
maticality and meaning preservation at the same
time.
The Total2 dataset contains only two marks (?0?
and ?1?) which correspond to the sentences which
should be discarded (?0?) and those which should
be retained (?1?), where ?0? corresponds to Dis in
Total3, and ?1? corresponds to the union of OK and
PE in Total3. The derivation procedure for both
datasets is presented in Table 3. We wanted to in-
vestigate whether the classification task would be
simpler (better performed) if there were only two
classes instead of three. In the case that such clas-
sification could be performed with satisfactory ac-
curacy, all sentences classified as ?0? would be left
in their original form or simplified with some dif-
ferent simplification strategy, while those classi-
fied as ?1? would be sent for a quick human post-
editing procedure.
Original
Total3 Total2
G M
3 3 OK 1
2 3 PE 1
1 3 PE 1
3 2 PE 1
2 2 PE 1
1 2 PE 1
3 1 Dis 0
2 1 Dis 0
1 1 Dis 0
Table 3: Datasets
Here it is important to mention that we decided
not to use human scores for simplicity (S) for sev-
eral reasons. First, simplicity was defined as the
amount of irrelevant information which was elim-
inated. Therefore, we cannot expect that any of
the six MT evaluation metrics would have a sig-
nificant correlation with this score (except maybe
TERp and, in particular, one of its parts ? ?number
of deletions?. However, none of the two demon-
strated any significant correlation with the sim-
plicity score, and those results are thus not re-
ported in this paper). Second, the output sentences
with a low simplicity score are not as detrimental
for the TS system as those with a low grammat-
icality or meaning preservation score. The sen-
tences with a low simplicity score would simply
not help the target user read faster or understand
better, but would not do any harm either. Alter-
natively, if the target ?user? is an MT or infor-
mation extraction (IE) system, or a parser for ex-
ample, such sentences would not lower the perfor-
mance of the system; they would just not improve
it. Low scores for G and M, however, would lead
to a worse performance for such NLP systems,
longer reading time, and a worse or erroneous un-
derstanding of the text. Third, the simplicity of
the output (or complexity reduction performed by
a TS system) could be evaluated separately, in a
fully automatic manner ? using some readability
measures or average sentence length as features
(as in (Drndarevi?c et al., 2013; Glava?s and
?
Stajner,
4
2013) for example).
3.4 Features: MT Evaluation Metrics
In all experiments, we focused on six commonly
used MT evaluation metrics. These are cosine
similarity (using the bag-of-words representation),
METEOR (Denkowski and Lavie, 2011), TERp
(Snover et al., 2009), TINE (Rios et al., 2011), and
two components of TINE: T-BLEU (which differs
from the standard BLEU (Papineni et al., 2002) by
using 3-grams, 2-grams, and 1-grams when there
are no 4-grams found, where the ?original? BLEU
would give score ?0?) and SRL (which is the com-
ponent of TINE based on semantic role labeling
using SENNA
4
). Although these two components
contribute equally to TINE (thus being linearly
correlated with TINE), we wanted to investigate
which one of them contributes more to the cor-
relation of TINE with human judgements. Given
their different natures, we expect T-BLEU to con-
tribute more to the correlation of TINE with hu-
man judgements of grammaticality, and SRL to
contribute more to the correlation of TINE with
human judgements of meaning preservation.
As we do not have the reference for the simpli-
fied sentence, all metrics are applied in a slightly
different way than in MT. Instead of evaluating the
translation hypothesis (output of the automatic TS
system in our case) with the corresponding ref-
erence translation (which would be a ?gold stan-
dard? simplified sentence), we apply the metrics
to the output of the automatic TS system com-
paring it with the corresponding original sentence.
Given that the simplified sentences in the used
dataset are usually shorter than the original ones
(due to the elimination of irrelevant content which
was the main focus of the TS system proposed by
Glava?s and
?
Stajner (2013)), we expect low scores
of T-BLEU and METEOR which apply a brevity
penalty. However, our dataset does not contain any
kind of lexical simplification, but rather copies all
relevant information from the original sentence
5
.
Therefore, we expect the exact matches of word
forms and semantic role labels (which are compo-
nents of the MT evaluation metrics) to have a good
correlation to human judgements of grammatical-
ity and meaning preservation.
4
http://ml.nec-labs.com/senna/
5
The exceptions being changes of gerundive forms into
past tense, and anaphoric pronoun resolution in some simpli-
fication schemes. See Section 3.1 and (Glava?s and
?
Stajner,
2013) for more details.
3.5 Goal
After we obtained the six automatic metrics (co-
sine, METEOR, TERp, TINE, T-BLEU, and
SRL), we performed two sets of experiments, try-
ing to answer two main questions:
1. Are the chosen MT evaluation metrics cor-
related with the human judgements of gram-
maticality and meaning preservation of the
TS system output?
2. Could we automatically classify the simpli-
fied sentences into those which are: (1) cor-
rect, (2) require a minimal post-editing, (3)
incorrect and need to be discarded?
A positive answer to the first question would
mean that there is a possibility of finding an au-
tomatic metric (or a combination of several au-
tomatic metrics) which could successfully replace
the time consuming human evaluation. The search
for that ?ideal? combination of automatic metrics
could be performed by using various classification
algorithms and carefully designed features. If we
manage to classify simplified sentences into the
three aforementioned categories with a satisfying
accuracy, the benefits would be two-fold. Firstly,
such a classification system could be used for an
automatic evaluation of TS systems and an easy
comparison of their performances. Secondly, it
could be used inside a TS system to mark those
sentences of low quality which need to be checked
further, or those sentences whose original mean-
ing changed significantly. The latter could then be
left in their original form or simplified using some
different technique.
3.6 Experiments
The six experiments conducted in this study are
presented in Table 4. The first two experiments
had the aim of answering the first question (Sec-
tion 3.5) as to whether the chosen MT metrics cor-
relate with the human judgements of grammatical-
ity (G) and meaning preservation (M) of the TS
system output. The results were obtained in terms
of Pearson?s, Kendall?s and Spearman?s correla-
tion coefficients. The third and the fourth exper-
iments (Table 4) could be seen as the intermediate
experiments exploring the possibility of automatic
classification of simplified sentences according to
their grammaticality, and meaning preservation.
The main experiment was the fifth experiment, try-
ing to answer the second question (Section 3.5)
5
Exp. Description
1. Correlation of the six automatic MT metrics with the human scores for Grammaticality
2. Correlation of the six automatic MT metrics with the human scores for Meaning preservation
3. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Grammaticality
4. Classification of the simplified sentences into 3 classes (?1? ? Bad, ?2? ? Medium, and ?3? ? Good) according to
their Meaning preservation
5. Classification of the simplified sentences into 3 classes (OK, PE, Dis) according to their Total3 score
6. Classification of the simplified sentences into 2 classes (?1? ? Retain, ?0? ? Discard) according to their Total2 score
Table 4: Experiments
as to whether we could automatically classify the
simplified sentences into those which are: (1) cor-
rect (OK), (2) require minimal post-editing (PE),
and (3) incorrect and need to be discarded (Dis).
The last experiment (Table 4) was conducted with
the aim of exploring whether the classification of
simplified sentences into only two classes ? Retain
(for further post-editing) and Discard ? would lead
to better results than the classification into three
classes (OK, PE, and Dis) in the fifth experiment.
All classification experiments were performed
in Weka workbench (Witten and Frank, 2005; Hall
et al., 2009), using seven classification algorithms
in a 10-fold cross-validation setup:
? NB ? NaiveBayes (John and Langley, 1995),
? SMO ? Weka implementation of Support
Vector Machines (Keerthi et al., 2001) with
normalisation (n) or with standardisation (s),
? Logistic (le Cessie and van Houwelingen,
1992),
? Lazy.IBk ? K-nearest neighbours (Aha and
Kibler, 1991),
? JRip ? a propositional rule learner (Cohen,
1995),
? J48 ? Weka implementation of C4.5 (Quin-
lan, 1993).
As a baseline we use the classifier which assigns
the most frequent (majority) class to all instances.
4 Results and Discussion
The results of the first two experiments (correla-
tion experiments in Table 4) are presented in Sec-
tion 4.1, while the results of the other four exper-
iments (classification experiments in Table 4) can
be found in Section 4.2. When interpreting the re-
sults of all experiments, it is important to keep in
mind that human agreements for meaning preser-
vation (M) and grammaticality (G) were accept-
able but far from perfect (Section 3.2), and thus
it would be unrealistic to expect the correlation
between the MT evaluation metrics and human
judgements or the agreement of the classification
system with human assessments to be higher than
the reported IAA agreement.
4.1 Correlation of Automatic Metrics with
Human Judgements
The correlations of automatic metrics with hu-
man judgements of grammaticality and meaning
preservation are given in Tables 5 and 6 respec-
tively. Statistically significant correlations (at a
0.01 level of significance) are presented in bold.
Metric Pearson Kendall Spearman
cosine 0.097 0.092 0.115
METEOR 0.176 0.141 0.178
T-BLEU 0.226 0.185 0.234
SRL 0.097 0.076 0.095
TINE 0.175 0.145 0.181
TERp -0.208 -0.158 -0.198
Table 5: Correlation between automatic evaluation
metrics and human scores for grammaticality
Metric Pearson Kendall Spearman
cosine 0.293 0.262 0.334
METEOR 0.386 0.322 0.405
T-BLEU 0.442 0.382 0.475
SRL 0.348 0.285 0.356
TINE 0.427 0.385 0.447
TERp -0.414 -0.336 -0.416
Table 6: Correlation between automatic evaluation
metrics and human scores for meaning preserva-
tion
It can be noted that human perception of gram-
maticality is positively correlated with three auto-
6
Algorithm
Grammaticality Meaning Total3 Total2
P R F P R F P R F P R F
NB 0.53 0.46 0.48 0.54 0.54 0.54 0.54 0.53 0.53 0.74 0.69 0.71
SMO(n) 0.39 0.63 0.48 0.52 0.49 0.45 0.43 0.53 0.44 0.55 0.74 0.63
SMO(s) 0.39 0.63 0.48 0.57 0.56 0.55 0.57 0.55 0.51 0.60 0.73 0.63
Logistic 0.45 0.61 0.49 0.57 0.57 0.56 0.61 0.60 0.59 0.75 0.77 0.74
Lazy.IBk 0.57 0.58 0.57 0.50 0.50 0.50 0.54 0.54 0.54 0.73 0.73 0.73
JRip 0.41 0.59 0.48 0.53 0.50 0.48 0.57 0.56 0.55 0.72 0.75 0.73
J48 0.45 0.61 0.49 0.48 0.47 0.47 0.59 0.57 0.54 0.68 0.71 0.69
baseline 0.39 0.63 0.48 0.17 0.41 0.24 0.21 0.46 0.29 0.55 0.74 0.63
Table 7: Classification results (the best performances are shown in bold; baseline uses the majority class)
Actual
Grammaticality Meaning
Good Med. Bad Good Med. Bad
Good 127 21 23 50 31 7
Med. 29 19 10 24 73 16
Bad 24 9 10 9 31 31
Table 8: Confusion matrices for the best classifications according to Grammaticality (Lazy.IBk) and
Meaning (Logistic). The number of ?severe? classification mistakes (classifying Good as Bad or vice
versa) are presented in bold.
matic measures ? METEOR, T-BLEU, and TINE,
while it is negatively correlated with TERp (TERp
measures the number of edits necessary to perform
on the simplified sentence to transform it into its
original one, i.e. the higher the value of TERp,
the less similar the original and its corresponding
simplified sentence are. The other five MT metrics
measure the similarity between the original and its
corresponding simplified version, i.e. the higher
their value is, the more similar are the sentences
are). All the MT metrics appear to be even bet-
ter correlated with the human scores for meaning
preservation (Table 6), demonstrating six positive
and one (TERp) negative statistically significant
correlation with M. The correlation is the highest
for T-BLEU, TINE, and TERp, though closely fol-
lowed by all others.
4.2 Sentence Classification
The results of the four classification experiments
(Section 3.6) are given in Table 7.
At first glance, the performance of the classifi-
cation algorithms seems similar for the first two
tasks (classification of the simplified sentences
according to their Grammaticality and Meaning
preservation). However, one needs to take into ac-
count that the baseline for the first task was much
much higher than for the second task (Table 7).
Furthermore, it can be noted that for the first task,
recall was significantly higher than precision for
most classification algorithms (all except NB and
Logistic), while for the second task they were very
similar in all cases. More importantly, a closer
look at the confusion matrices reveals that most of
the incorrectly classified sentences were assigned
to the nearest class (Medium into Bad or Good;
Bad into Medium; and Good into Medium
6
) in the
second task, while it was not the case in the first
task (Table 8).
Classification performed on the Total3 dataset
outperformed both previous classifications ? that
based on Grammaticality and that based on Mean-
ing ? on four different algorithms (NB, Logis-
tic, JRip, and J48). Classification conducted on
Total3 using Logistic outperformed all results of
classifications on either Grammaticality or Mean-
ing separately (Table 7). It reached a 0.61, 0.60,
and 0.59 score for the weighted precision (P), re-
call (R), and F-measure (F), respectively, thus out-
performing the baseline significantly. More im-
portantly, classification on the Total3 dataset led
to significantly fewer mis-classifications between
Good and Bad (Table 9) than the classification
based on Grammaticality, and slightly less than
6
Bad, Medium, and Good correspond to marks ?1?, ?2?,
and ?3? given by human evaluators.
7
Actual
Total3
OK PE Dis.
OK 41 32 4
PE 17 85 12
Dis. 6 31 28
Table 9: Confusion matrix for the best classifica-
tion according to Total3 (Logistic). The number of
?severe? classification mistakes (classifying Good
as Bad or vice versa) are presented in bold.
Actual
Total2
Retain Discard
Retain 21 50
Discard 12 189
Table 10: Confusion matrix for the best classifi-
cation according to Total2 (Logistic). The num-
ber of ?severe? classification mistakes (classifying
Retain as Discard or vice versa) are presented in
bold.
the classification based only on Meaning (Table 8).
Therefore, it seems that simplified sentences are
better classified into three classes giving a unique
score for both grammaticality and preservation of
meaning together.
The binary classification experiments based on
the Total2 led to results which significantly out-
performed the baseline in terms of precision and
F-measure (Table 7). However, they resulted in
a great number of sentences which should be re-
tained (Retain) being classified into those which
should be discarded (Discard) and vice versa (Ta-
ble 10). Therefore, it seems that it would be better
to opt for classification into three classes (Total3)
than for classification into two classes (Total2).
Additionally, we used CfsSubsetEval attribute
selection algorithm (Hall and Smith, 1998) in or-
der to identify the ?best? subset of features. The
?best? subsets of features for each of the four clas-
sification tasks returned by the algorithm are listed
in Table 11. However, the classification perfor-
mances achieved (P, R, and F) when using only
the ?best? features did not differ significantly from
those when using all initially selected features, and
thus are not presented in this paper.
5 Limitations
The used dataset does not contain any kind of
lexical simplification (Glava?s and
?
Stajner, 2013).
Classification ?Best? features
Meaning {TERp, T-BLEU, SRL, TINE}
Grammaticality {TERp, T-BLEU}
New3 {TERp, T-BLEU, SRL, TINE}
New2 {TERp, T-BLEU, SRL}
Table 11: The ?best? features (CfsSubsetEval)
Therefore, one should consider the limitation of
this TS system which performs only syntactic sim-
plification and content reduction. On the other
hand, the dataset used contains a significant con-
tent reduction in most of the sentences. If the same
experiments were conducted on a dataset which
performs only syntactic simplification, we would
expect much higher correlation of MT evaluation
metrics to human judgements, due to the lesser im-
pact of the brevity penalty in that case.
If we were to apply the same MT evaluation
metrics to a TS system which additionally per-
forms some kind of lexical simplification (either
a simple lexical substitution or paraphrasing), the
correlation results for T-BLEU and cosine similar-
ity would be lower (due to the lower number of
exact matches), but not for METEOR, TERp and
SRL (and thus TINE as well). As a similar prob-
lem is also present in the evaluation of MT sys-
tems where the obtained output could differ from
the reference translation (while still being equally
good), METEOR, TERp, and SRL in TINE ad-
ditionally use inexact matching. The first two use
the stem, synonym, and paraphrase matches, while
SRL uses ontologies and thesaurus.
6 Conclusions and Future Work
While the results reported are preliminary and
their universality needs to be validated on different
TS datasets, the experiments and results presented
can be regarded as a promising step towards an au-
tomatic assessment of grammaticality and mean-
ing preservation for the output of TS systems. In
addition and to the best of our knowledge, there
are no such datasets publicly available other than
the one used. Nevertheless, we hope that these re-
sults would initiate an interesting discussion in the
TS community and start a new direction of studies
towards automatic evaluation of text simplification
systems.
8
Acknowledgements
The research described in this paper was par-
tially funded by the European Commission un-
der the Seventh (FP7-2007-2013) Framework Pro-
gramme for Research and Technological Develop-
ment (FP7-ICT-2011.5.5 FIRST 287607).
References
D. Aha and D. Kibler. 1991. Instance-based learning
algorithms. Machine Learning, 6:37?66.
S. M. Alu??sio, L. Specia, T. A. S. Pardo, E. G. Maziero,
H. M. Caseli, and R. P. M. Fortes. 2008a. A cor-
pus analysis of simple account texts and the pro-
posal of simplification strategies: first steps towards
text simplification systems. In Proceedings of the
26th annual ACM international conference on De-
sign of communication, SIGDOC ?08, pages 15?22,
New York, NY, USA. ACM.
S. M. Alu??sio, L. Specia, T. A.S. Pardo, E. G. Maziero,
and R. P.M. Fortes. 2008b. Towards brazilian por-
tuguese automatic text simplification systems. In
Proceedings of the eighth ACM symposium on Doc-
ument engineering, DocEng ?08, pages 240?248,
New York, NY, USA. ACM.
M. J. Aranzabe, A. D??az De Ilarraza, and I. Gonz?alez.
2012. First Approach to Automatic Text Simplifica-
tion in Basque. In Proceedings of the first Natural
Language Processing for Improving Textual Acces-
sibility Workshop (NLP4ITA).
G. Barlacchi and S. Tonelli. 2013. ERNESTA: A sen-
tence simplification tool for childrens stories in ital-
ian. In Computational Linguistics and Intelligent
Text Processing.
J. Carroll, G. Minnen, D. Pearce, Y. Canning, S. De-
vlin, and J. Tait. 1999. Simplifying text for
language-impaired readers. In Proceedings of the
9th Conference of the European Chapter of the ACL
(EACL?99), pages 269?270.
R. Chandrasekar, Christine Doran, and B. Srinivas.
1996. Motivations and methods for text simplifi-
cation. In In Proceedings of the Sixteenth Inter-
national Conference on Computational Linguistics
(COLING ?96, pages 1041?1044.
W. Cohen. 1995. Fast Effective Rule Induction. In
Proceedings of the Twelfth International Conference
on Machine Learning, pages 115?123.
W. Coster and D. Kauchak. 2011. Learning to Sim-
plify Sentences Using Wikipedia. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 1?9.
M. Denkowski and A. Lavie. 2011. Meteor 1.3: Au-
tomatic Metric for Reliable Optimization and Evalu-
ation of Machine Translation Systems. In Proceed-
ings of the EMNLP Workshop on Statistical Machine
Translation.
S. Devlin. 1999. Simplifying natural language text for
aphasic readers. Ph.D. thesis, University of Sunder-
land, UK.
G. Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram coocurrence
statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Pub-
lishers Inc.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and
H. Saggion. 2013. Automatic Text Simplication
in Spanish: A Comparative Evaluation of Com-
plementing Components. In Proceedings of the
12th International Conference on Intelligent Text
Processing and Computational Linguistics. Lecture
Notes in Computer Science. Samos, Greece, 24-30
March, 2013., pages 488?500.
L. Feng. 2009. Automatic readability assessment for
people with intellectual disabilities. In SIGACCESS
Access. Comput., number 93, pages 84?91. ACM,
New York, NY, USA, jan.
G. Glava?s and S.
?
Stajner. 2013. Event-Centered Sim-
plication of News Stories. In Proceedings of the
Student Workshop held in conjunction with RANLP
2013, Hissar, Bulgaria, pages 71?78.
M. A. Hall and L. A. Smith. 1998. Practical feature
subset selection for machine learning. In C. Mc-
Donald, editor, Computer Science ?98 Proceedings
of the 21st Australasian Computer Science Confer-
ence ACSC?98, pages 181?191. Berlin: Springer.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The weka data min-
ing software: an update. SIGKDD Explor. Newsl.,
11:10?18, November.
K. Inui, A. Fujita, T. Takahashi, R. Iida, and T. Iwakura.
2003. Text simplification for reading assistance: a
project note. In Proceedings of the second inter-
national workshop on Paraphrasing - Volume 16,
PARAPHRASE ?03, pages 9?16, Stroudsburg, PA,
USA. Association for Computational Linguistics.
G. H. John and P. Langley. 1995. Estimating Contin-
uous Distributions in Bayesian Classifiers. In Pro-
ceedings of the Eleventh Conference on Uncertainty
in Artificial Intelligence, pages 338?345.
S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 2001. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 13(3):637?649.
9
D. Klein and C.D. Manning. 2003. Accurate unlex-
icalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, volume 1, pages 423?430. Association for
Computational Linguistics.
S. le Cessie and J.C. van Houwelingen. 1992. Ridge
Estimators in Logistic Regression. Applied Statis-
tics, 41(1):191?201.
J. Martos, S. Freire, A. Gonz?alez, D. Gil, and M. Se-
bastian. 2012. D2.1: Functional requirements spec-
ifications and user preference survey. Technical re-
port, FIRST technical report.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
S. E. Petersen and M. Ostendorf. 2007. Text Sim-
plification for Language Learners: A Corpus Anal-
ysis. In Proceedings of Workshop on Speech and
Language Technology for Education.
R. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers, San Ma-
teo, CA.
L. Rello. 2012. Dyswebxia: a model to improve ac-
cessibility of the textual web for dyslexic users. In
SIGACCESS Access. Comput., number 102, pages
41?44. ACM, New York, NY, USA, January.
M. Rios, W. Aziz, and L. Specia. 2011. TINE: A met-
ric to assess MT adequacy. In Proceedings of the
Sixth Workshop on Statistical Machine Translation
(WMT-2011), Edinburgh, UK.
M. B. Ruiter, T. C. M. Rietveld, Cucchiarini C., Krah-
mer E. J., and H. Strik. 2010. Human Language
Technology and communicative disabilities: Re-
quirements and possibilities for the future. In Pro-
ceedings of the the seventh international conference
on Language Resources and Evaluation (LREC).
J. Rybing, C. Smithr, and A. Silvervarg. 2010. To-
wards a Rule Based System for Automatic Simpli-
fication of Texts. In The Third Swedish Language
Technology Conference.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. An-
ula, and L. Bourg. 2011. Text Simplification in
Simplext: Making Text More Accessible. Revista
de la Sociedad Espa?nola para el Procesamiento del
Lenguaje Natural, 47:341?342.
A. Siddharthan. 2006. Syntactic simplification and
text cohesion. Research on Language & Computa-
tion, 4(1):77?109.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, Adequacy, or HTER? Exploring Dif-
ferent Human Judgments with a Tunable MT Metric.
In Proceedings of the Fourth Workshop on Statisti-
cal Machine Translation at the 12th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2009), Athens, Greece.
L. Specia. 2010. Translating from complex to simpli-
fied sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language, pages 30?39, Berlin, Hei-
delberg.
S.
?
Stajner, R. Evans, C. Orasan, and R. Mitkov. 2012.
What Can Readability Measures Really Tell Us
About Text Complexity? In Proceedings of the
LREC?12 Workshop: Natural Language Processing
for Improving Textual Accessibility (NLP4ITA), Is-
tanbul, Turkey.
I. H. Witten and E. Frank. 2005. Data mining: practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
K. Woodsend and M. Lapata. 2011a. Learning to Sim-
plify Sentences with Quasi-Synchronous Grammar
and Integer Programming. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing (EMNLP).
K. Woodsend and M. Lapata. 2011b. WikiSimple:
Automatic Simplification of Wikipedia Articles. In
Proceedings of the 25th AAI Coference on Artificial
Intelligence.
S. Wubben, A. van den Bosch, and E. Krahmer. 2012.
Sentence simplification by monolingual machine
translation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers - Volume 1, ACL ?12, pages 1015?
1024, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Z. Zhu, D. Berndard, and I. Gurevych. 2010. A Mono-
lingual Tree-based Translation Model for Sentence
Simplification. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 1353?1361.
10
Proceedings of the Workshop on Automatic Text Simplification: Methods and Applications in the Multilingual Society, pages 30?40,
Dublin, Ireland, August 24th 2014.
The Fewer, the Better? A Contrastive Study about Ways to Simplify
Ruslan Mitkov and Sanja
?
Stajner
Research Group in Computational Linguistics
Research Institute of Information and Language Processing
University of Wolverhampton, UK
{R.Mitkov, SanjaStajner}@wlv.ac.uk
Abstract
Simplified texts play an important role in providing accessible and easy-to-understand informa-
tion for a whole range of users who, due to linguistic, developmental or social barriers, would
have difficulty in understanding materials which are not adapted and/or simplified. However, the
production of simplified texts can be a time-consuming and labour-intensive task. In this paper
we show that the employment of a short list of simple simplification rules could result in texts
of comparable readability to those written as a result of applying a long list of more fine-grained
rules. We also prove that the simplification process based on the short list of simple rules is more
time efficient and consistent.
1 Rationale
Simplified texts play an important role in providing accessible and easy-to-understand information for a
whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in
understanding materials which are not adapted and/or simplified. Such users include but are not limited to
people with insufficient knowledge of the language in which the document is written, people with specific
language disorders and people with low literacy levels. However, while the production of simplified texts
is certainly an indispensable activity, it often proves to be a time-consuming and labour-intensive task.
Various methodologies and simplification strategies have been developed which are often employed by
authors to simplify original texts. Most methods involve a high number of rules which could result not
only in the simplification task being time-consuming but also in the authors getting confused as to which
rules to apply. We hypothesise that it is possible to achieve a comparable simplification effect by using a
small set of simple rules similar to the ones used in Controlled Languages which, in addition, enhances
the productivity and reliability of the simplification process.
In order to test our hypothesis we conduct the following experiments. First, we propose six Controlled
Language-inspired rules which we believe are simple and easy enough for writers of simplified texts to
understand and apply. We then ask two writers to apply these rules to a selection of newswire texts and
also to produce simplified versions of these texts using the 28 rules used in the Simplext project (Saggion
et al., 2011). Both sets of texts are compared in terms of readability. In both simplification tasks the time
efficiency is assessed and the inter-annotator agreement is evaluated. In an additional experiment, we
seek to investigate the possible effect of familiarisation in simplification. In this experiment a third
writer simplifies a sample of the texts used in the previous experiments by applying each set of rules in
a mixed sequence pattern which does not offer any familiarisation nor the advantage of one set of rules
over the other. Using these samples, three-way inter-annotator agreement is reported.
The rest of the paper is structured as follows. Section 2 outlines related work on simplification rules.
Section 3 introduces our proposal for a small set of easy-to-understand and easy-to-apply rules and
contrasts them with the longer and more elaborate rules employed in the Simplext proposal. Section
4 details the experiments conducted in order to validate or refute our hypothesis, and outlines the data
used for the experiments. Section 5 presents and discusses the results, while the last section of the paper
summarises the main conclusions of this study.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
30
2 Related work
Since the late 1990s, several initiatives which proposed guidelines for producing plain, easy-to-read and
more accessible documents have emerged. These include the ?Federal Plain Language Guidelines?,
?Make it Simple, European Guidelines for the Production of Easy-to-Read Information for people with
Learning Disability?, and ?Am I making myself clear? Mencap?s guidelines for accessible writing?.
The Plain Language Action and Information Network (PLAIN)
1
developed the first version of the
?Federal Plain Language Guidelines? (PlainLanguage, 2011) in the mid-90s and have revised it every
few years since then. Their original idea was to help writers of governmental documents (primarily
regulations) to write in a clear and and simple manner so that the users can: ?find what they need; under-
stand what they find; and use what they find to meet their needs.? (PlainLanguage, 2011). The ?Make it
Simple? European Guidelines for the Production of Easy-to-Read Information for people with Learning
Disability (Freyhoff et al., 1998) were produced by Inclusion Europe
2
in order to assist writers in devel-
oping texts, publications and videos that are more accessible to people with intellectual disabilities and
other people who cannot read complex texts, and thus enable those people to be better protected from
discrimination and social injustice. The ?Am I making myself clear?? Mencap?s guidelines for accessi-
ble writing (Mencap, 2002) were produced by the UK?s leading organisation working with people with
a learning disability.
3
Their goal is to help in editing and writing accessible material for that specific
target population. All of these guidelines are concerned with both verbal content of documents and their
layout. As we are interested in text simplification and not in text representation, we will concentrate only
on the former. All three guidelines share similar instructions for accessible writing, some of them more
detailed than others. Table 1 allows us to have a quick overview of intersecting rules suggested by these
guidelines which were intended for slightly different purposes and target audiences.. For example, they
all advise the writer to use active voice instead of passive, use short, simple words and omit unnecessary
words, write short sentences and cover only one main idea per sentence, etc. However, the ?Federal
Plain Language Guidelines? also instruct writers to use contractions where appropriate, avoid hidden
verbs (i.e. verbs converted into a noun), and place the main idea before exceptions and conditions, while
the other two guidelines do not go into many details. Some of the instructions, e.g. to use the simplest
form of a verb (present and not conditional or future), or to avoid double negatives and exceptions to
exceptions, are not present in the Mencap?s guidelines for accessible writing, while they are at the same
time implicitly present in the ?Make it Simple? guidelines, and explicitly present in the ?Federal Plain
Language Guidelines?.
Karreman et al. (2007) investigated whether the application of the ?Make it Simple? guidelines to the
website?s content would enhance its usability for users with intellectual disabilities. Additionally, they
investigated whether the application of these guidelines would have a negative effect on users without
disabilities, as Web Accessibility Initiative (WAI) guidelines
4
state that creation of multiple versions of
the same website should be avoided whenever possible. The authors prepared two versions of a website,
the original one and the one adapted according to the ?Make it Simple? guidelines. These two versions
were then tested for efficiency (searching and reading time) and effectiveness (comprehension) by 40
participants, 20 with diagnosed intellectual disabilities and 20 without. The results demonstrated that the
adaptation of the website according to the guidelines enhanced the efficiency and effectiveness for both
groups of participants.
There has been a body of work associated with the development and use of Controlled Languages
for simplification purposes. The original idea of developing a Controlled Language arose during the
1930s when influential scholars sought to establish a ?minimal? variety of English, a variety specifically
designed to make English accessible to and usable by the largest possible number of people worldwide
(Arnold et al., 1994). This variety was called Basic English and one of the central ideas was to use
a few hundred general-purpose words only. Operator verbs were to be used with a set of nouns and
1
http://www.plainlanguage.gov/
2
http://inclusion-europe.org/
3
http://november5th.net/resources/Mencap/Making-Myself-Clear.pdf
4
http://www.w3.org/WAI/
31
Rule Simple Clear Plain
Use active tense (instead of passive) yes yes yes
Use the simplest form of a verb* (yes) yes
Avoid hidden verbs (i.e. verbs converted into a noun) yes
Use ?must? to indicate requirements yes
Use contractions where appropriate yes
Don?t turn verbs into nouns yes
Use ?you? to speak directly to readers yes yes yes
Avoid abbreviations yes yes
Use short, simple words yes yes
Omit unnecessary words yes
Avoid definitions as much as possible yes
Use the same term consistently yes yes
Avoid legal, foreign and technical jargon yes yes yes
Don?t use slashes yes
Write short sentences yes yes yes
Keep subject, verb and object close together yes
Avoid double negatives and exceptions to exceptions (yes) yes
Place the main idea before exceptions and conditions yes
Cover only one main idea per sentence yes yes
Use examples (avoid abstract concepts) yes yes
Keep the punctuation simple yes yes
Be careful with figures of speech and metaphors yes
Use the number and not the word yes yes
Avoid cross references yes yes
*Use present tense and not conditional or future
Table 1: Rules for verbal content of documents (the three columns ?Simple?, ?Clear?, and ?Plain? contain
?yes? if this rule is present in the corresponding guidelines: ?Make it Simple?, ?Am I making myself
clear?? and ?Federal Plain Language Guidelines?, respectively; value ?(yes)? is used when the rule is not
explicitly present in the corresponding guidelines, only implicitly)
adjectives to replace most of the derived verbs. The Controlled Language writing rules included various
rules such as ?Keep it short and simple? (Keep sentences short, Omit redundant words, Order the parts of
the sentence logically, Don?t change constructions in mid-sentence, Take care with the logic of and and
or) and ?Make it explicit? (Avoid elliptical constructions, Don?t omit conjunctions or relatives, Adhere to
the PACE dictionary, Avoid strings of nouns, Do not use -ing unless the word appears thus in the PACE
dictionary) (Arnold et al., 1994). The concept of controlled languages evolved and developed further and
they have been regarded as a prerequisite part of successful Machine Translation. Controlled Languages
have been also employed in a number of critical situations where ambiguity could be a problem.
5
3 Simplification strategies: contrasting two sets of rules
The Simplext guidelines were written under the Simplext project, with the aim of helping the authors to
produce texts which would be accessible to people with Down syndrome. They follow the same main
ideas as those in ?Make it Simple, European Guidelines for People with Intellectual Disability? but they
adapt the rules to their specific target population and the Spanish language. The Simplext guidelines
contain 28 main rules
6
concerned with the verbal content of documents. Those rules cover the same
main ideas as our rules (see below), e.g. to keep sentences short, use only the most frequent words,
5
The reader is referred to (Kittredge, 2003), (Cardey, 2009) and (Temnikova, 2012) for more details.
6
The Simplext guidelines actually provide even more sub-rules for most of the main rules, but in this study we use only the
28 main rules.
32
remove redundant words, use a simpler paraphrase if applicable. However, the Simplext rules are more
fine-grained, thus providing several more specific rules instead of our more general rules. For example,
they explicitly instruct the writer to use frequent words, use non-ambiguous words, and not use words
with more than six syllables whenever it is possible.
On the other hand, the six simple rules selected for our study have been inspired from the rules in
Controlled Languages
7
. We conjecture that there is a small set of simple, easy-to-understand and easy-
to-apply rules which can be equally efficient in terms of simplicity (readability) and yet their employment
is less time-consuming and less contentious in practice. The rules which we propose are as follows
(examples are presented in Table 2):
1. Use simple sentences
We have selected this rule to ensure that the simplified version of the document features sufficiently
short and simple sentences only so that the reader does not have to process longer complex sen-
tences.
2. Remove anaphors
This rules caters for replacing the anaphors such as pronouns and one-anaphors with their antecedent
to minimise the risk of anaphoric ambiguity but also makes sure that the texts does not feature any
elliptical constructions which may be more difficult to understand.
3. Use active voice only
We have included this rule as active voice is generally easier to process.
4. Use the most frequent words only
Similarly to the practice recommended in Basic English, we recommend the use of the 1,000 most
frequent words in Spanish as documented by RAE (Real Academia Espa?nola)
8
. If this is not pos-
sible, then words from the list of the 5,000 most frequent Spanish words are resorted to
9
. We have
allowed the following exception for this rule. There are cases where a specific technical word occurs
in the text and which is unlikely to be on the list of 1,000 (or 5,000) basic / most frequent words in
Spanish. By way of example, in the sentence ?Ana Juan gan?o el Premio Nacional de Ilustraci?on de
2010? (Ana Juan won the national prize for illustration in 2010) the word Ilustraci?on is considered
as technical and is not replaced with a basic word.
5. Remove redundant words
Our rules recommend the removal of redundant words or phrases which do not really contribute to
the understanding of the text.
6. Use a simpler paraphrase, if applicable
There are cases where the sentence is difficult to read or understand due among other things, to
its syntax. Our rules recommend that in such cases the original sentence or part of the sentence is
paraphrased.
4 Experiments and data
In order to test our hypothesis we conducted several experiments. We selected 10 newswire texts in
Spanish and asked two writers who are native speakers of Spanish and who have a language/linguistics
background, to apply both our six rules and the 28 Simplext rules in order to simplify these newswire
texts. The writers familiarised themselves with the rules beforehand, had an induction with the authors
7
We shall often refer to these rules throughout the paper as ?our rules?
8
http://corpus.rae.es/frec/1000 formas.TXT
9
http://corpus.rae.es/frec/5000 formas.TXT
33
Rule Version Example
1 Original Desde hace ya 10 a?nos, La Casa Encendida ha propuesto y desarrollado, den-
tro del mundo profesional de las Artes Esc?enicas, el Ciclo Artes Esc?enicas y
Discapacidad.
[It is now 10 years ago that La Casa Encendida first proposed and carried
out, within the professional field of performing arts, the performing arts and
disabilities course.]
Simplified Desde hace ya 10 a?nos, La Casa Encendida ha organizado el Ciclo Artes
Esc?enicas y Discapacidad. El Ciclo Artes Esc?enicas y Discapacidad est?a den-
tro del mundo profesional de las Artes Esc?enicas.
[It is now 10 years ago that La Casa Encendida organised the performing arts
and disabilities course. The performing arts and disabilities course is part of
the professional field of performing arts.]
2 Original Sus solos en directo son acontecimientos imprevisibles que siempre sorpren-
den a la audiencia, en ellos interpreta temas de sus ?albumes en solitario con
partes de improvisaci?on.
[His live solos are unpredictable events which always surprise the audience;
during these, he performs songs from his albums on his own while improvising
some parts.]
Simplified Los solos en directo de Marc Ribot siempre sorprenden a la audiencia. En los
solos Marc Ribot toca canciones de sus ?albumes con partes de improvisaci?on.
[Marc Ribots live solos always surprise the audience. During solos, Marc
Ribot plays songs from his albums while improvising some parts.]
3 Original Los avisos recibidos por la Gerencia de Emergencias Sanitarias fueron
canalizados a trav?es de las unidades del Servicio Murciano de Salud.
[Calls received by medical emergency services were directed by the Depart-
ment of Health Services in Murcia.]
Simplified La Gerencia de Emergencias Sanitarias recibieron los avisos. Las unidades
del Servicio Murciano de Salud se encargaron de los avisos.
[The medical emergency services received the calls. The Department of
Health Services in Murcia took charge of the calls.]
4 Original Ratificaci
?
on Experimental
[Experimental ratification]
Simplified Confirmaci
?
on Experimental
[Experimental confirmation]
5 Original Un disolvente agresivo, muy vol
?
atil y que entra
?
na riesgos para la salud.
[An aggressive solvent, very volatile and which involves health risks.]
Simplified El disolvente Percloroetileno puede ser peligroso para la salud.
[The solvent perchloroethylene can be dangerous to your health.]
6 Original L?ogicamente, al ser menos agresivo, mejora sustancialmente el tacto de las
prendas y no deja el caracter??stico olor a tintorer??a.
[Logically, due to it being less aggressive, it considerably improves how
clothes feel and does not leave them with that characteristic dry cleaners
smell.]
Simplified Otros disolventes, al ser menos agresivos, dejan la ropa m
?
as suave y no dejan
el olor a tintorer??a.
[Other solvents, due to their being less aggressive, make clothes softer and
don?t leave them smelling of dry cleaner.]
Table 2: Examples of each of our rules (sentence parts altered by applying the corresponding rule are
shown in bold)
34
of this paper and were asked to have sessions no longer than 1 hour so that potential fatigue did not com-
promise the experiments. In order to minimise potential familiarity effect (texts which already have been
simplified are expected to be simplified faster and more efficiently as they are familiar to the writers),
we allowed a few days interval between each time a specific text was simplified using different rules.
We applied the Spauldings Spanish Readability index ? SSR (Spaulding, 1956) as well as the Lexical
Complexity index ? LC (Anula, 2007) to assess the readability of the simplified texts. Both metrics have
shown a good correlation with the possible reading obstacles for various target populations (
?
Stajner and
Saggion, 2013), and were used for the evaluation of the automatic TS system in Simplext (Drndarevi?c et
al., 2013). We also asked a third writer to simplify samples from the texts used by the first two writers
which were pre-assessed to be of comparable complexity, with a view to establishing whether familiari-
sation has an effect on the output. The results of these readability experiments are presented in Tables 4
and 5 of the following section. We also recorded the time needed to simplify each text as an indication
of, among other things, ease of use of (and clarity for) each set of rules and its productivity in general;
these results are reported in Tables 6 and 7 of the following section.
Several experiments were conducted to assess the inter-annotator agreement. We believe that the inter-
annotator agreement is another good indicator as to how straightforward it is to apply a specific set of
simplification rules and how reliable the simplification process is in general. We compute the inter-
annotator agreement in terms of the BLEU score (Papineni et al., 2002). BLEU score is widely used in
MT to compare the reference translation with the output of the system (translation hypothesis). Here we
use the BLEU score to compare the simple sentences produced by one annotator with the corresponding
sentences of another annotator. We measure the inter-annotator agreement for all three pairs of annotators
(Table 8). In addition, we examined how many times each of the rules was selected by each writer which
in our view would be not only a way of accounting for agreement and but also assessing the usefulness
of every rule and how balanced a set of rules is in general. Tables 9 and 10 report the results of this study
on the texts simplified by all three annotators.
While in the above experiments (which involved only two writers) we made sure that there was at
least a few days? span between applying the different sets of rules on the same text, we felt that the risk
of familiarity effect could not be removed completely. It is expected that a text which has already been
simplified would take less time to be simplified for a second time, even if different rules are applied.
Also, as Simplext rules were always applied after our simple rules, we felt that additional experiments
were needed where (i) there would be no risk of familiarisation effect and (ii) the rules were applied in a
mixed order so that any experience gained from simplification in general cannot serve as unfair advantage
to one of the sets of rules. In an experiment seeking to investigate the possible effect of familiarisation
in simplification, a third writer simplified a selection of the texts used in the previous experiments by
applying each set of rules in a mixed sequence pattern which does not offer any familiarisation nor any
advantage of one set of rules over the other. In other words, instead of this writer simplifying the same
text twice using different rules, different texts of comparable level of simplicity, informed by the input of
the first two writers, were selected and simplified. Based on the results of the time efficiency experiment
(Table 6, next section), we chose three pairs (Pair 1, Pair 2 and Pair 3) of texts where for each pair the
texts are deemed to be of comparable complexity. By way of example, in Pair 1 which consists of Text 1
and Text 2, Annotator 1 needed the same time for both texts with Simplext rules, and similar time with
our simple rules, Annotator 2 needed the same time with our rules, and similar time with Simplext rules.
Pair 2 consists of Text 3 and Text 4 and Pair 3 is made of Text 9 and Text 10 for the same reasons as
above. The simplification performed by a third writer makes it possible to report readability indices for
the text simplified by the third writer, as well as the time taken to simplify, and three-way agreement.
The 10 texts made available by the Spanish news agency Servimedia
10
belong to one of the four
following domains: international news (Texts 2, 6, and 10), national news (Texts 4 and 8), society (Texts
3 and 7), or culture (Texts 1, 5, and 9). The sizes of these samples (in sentences and words) are listed in
Table 3.
10
http://www.servimedia.es/
35
Size Text 1 Text 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 Text 10
Sentences 7 7 5 5 6 4 7 6 5 5
Words 166 183 172 193 176 167 197 180 156 169
Table 3: Size of the texts used for this study
5 Results and discussion
This section presents the results of a study on the readability of texts simplified with our rules as well as
with the Simplext rules. It also reports on a time efficiency experiment whose objective is to identify the
rules which are less time-consuming to apply. Next, interannotator agreement in terms of BLEU score
and selection of rules is discussed and finally, an interpretation of the results of an experiment seeking to
establish any familiarisation effect in simplification is provided.
5.1 Readability study
As can be observed from Table 4, simplification performed by our rules improves the readability of
texts in almost all cases (note the values in column ?original? with those in columns A-I and A-II for
both indices LC and SSR). This improvement was statistically significant in terms of both indices when
the texts were simplified by the second annotator, and in terms of the SSR index when the texts were
simplified by the first annotator (lower readability indices indicate text which is easier to read).
11
.
Text
LC SSR
original A - I A - II B - I B - II original A - I A - II B - I B - II
1 12.00 5.27 6.00 5.57 6.25 183.07 154.67 170.64 147.67 165.70
2 9.76 12.52 9.20 9.74 8.98 174.66 169.07 159.88 161.76 155.99
3 12.95 9.19 8.92 9.04 10.10 176.91 161.30 153.78 157.23 154.80
4 10.74 7.78 7.59 6.53 7.62 179.19 148.27 143.77 133.36 159.26
5 11.79 7.80 9.57 9.47 9.94 196.94 180.05 182.25 164.50 181.99
6 7.23 4.83 4.77 2.00 4.63 177.40 153.22 159.99 130.42 162.19
7 10.23 13.35 8.54 8.29 7.48 175.72 175.11 153.96 137.15 151.34
8 15.14 12.07 11.75 8.96 11.77 191.13 175.42 168.08 155.17 162.59
9 12.86 9.93 10.77 8.87 12.08 178.91 160.47 166.74 142.78 171.08
10 13.52 13.31 10.48 12.03 12.24 166.91 146.96 140.94 152.58 152.94
Table 4: Readability: two readability indices LC and SSR (lower readability indices indicate texts which
are easier to read; I and II refer to the two annotators who simplified all 10 texts; A and B refers to the
rules which are used: A ? ours, B ? Simplext)
Text
LC SSR
original A - III B - III original A - III B - III
1 12.00 4.92 183.07 170.64
2 9.76 8.00 174.66 172.58
3 12.95 6.38 176.91 153.78
4 10.74 7.82 179.19 175.80
9 12.86 10.57 178.91 166.74
10 13.52 12.15 166.91 154.12
Table 5: Readability of texts simplified by Annotator III (A and B refers to the rules which are used: A
? ours, B ? Simplext)
11
Statistical significance was measured by the paired t-test in SPSS at a 0.05 level of significance
36
The differences in readability between the texts written by employing our simplification rules (columns
A-I and A-II) and those written by following the Simplext rules (columns B-I and B-II), were not sta-
tistically significant when the simplification was performed by the second annotator, while they were
significant when the simplification was performed by the first annotator. When interpreting these results,
it is also important to bear in mind that the LC index measures only the lexical complexity of a text,
while the SSR index measures general complexity of a text, including both its lexical and its syntactic
complexity. We also benefited from the familiarity experiment in which a third annotator was involved,
to assess the readability of the simplified versions of the texts of comparable complexity, as produced by
the third additional annotator. The results, which are reported in Table 5, suggest that in fact the texts
simplified by the third annotator with our rules are easier to read. On the basis of these readability results,
it can be concluded that the application of Simplext rules does not necessarily result in a (significantly)
simpler version than the one produced by our rules and comparable results are likely to be achieved.
5.2 Time efficiency experiment
The results from the time efficiency experiment (Table 6) show that in all cases, the simplification with
our rules is done in shorter (or equal) time. This is also confirmed by the time needed by the third
annotator in the additional experiment seeking to establish any familiarity effect (Table 7), where texts
of comparable complexity simplified by our rules were simplified faster than the texts simplified with the
Simplext rules. In our view, the results of these experiments are indicative not only of the time and cost
savings when using our rules but also of our rules being simpler for writers and more straightforward to
employ.
Ann. Set Text 1 Text 2 Text 3 Text 4 Text 5 Text 6 Text 7 Text 8 Text 9 Text 10
I
A 48 41 30 39 55 29 32 43 24 24
B 60 60 40 44 44 18 29 19 15 16
II
A 15 15 10 12 30 30 20 15 10 10
B 30 20 20 15 15 10 10 10 10 10
Table 6: Time efficiency in simplification
Set Text 1 ? Text 2 Text 3 ? Text 4 Text 9 ? Text 10
A 12 15 11
B 16 16 14
Table 7: Time efficiency in simplification (Annotator III only)
5.3 Inter-annotator agreement and selection of rules
Table 8 presents the inter-annotator agreement in terms of BLEU score. This score accounts for the
agreement during the simplification process and the higher the value, the more similar the simplifications
performed by the annotators are. In both cases where the difference is significant our rules exhibited a
higher degree of agreement among the annotators than the Simplext rules.
Rules I ? II II ? III I ? III
A (Ours) 44.00 52.85 48.27
B (Simplext) 30.46 55.12 33.13
Table 8: Pair-wise inter-annotator agreement in terms of BLEU score
We also analysed how many times each rule was applied by each of the annotators (the annotators
were asked to write the numbers of all rules used during simplification of each sentence right after that
sentence). We regard the frequency of selection of rules as another indicator for the inter-annotator
37
agreement. Tables 9 and 10 report the frequency of selection of each of our simple rules as well as the
Simplext rules for all three annotators (measured only on the texts simplified by all three annotators).
Annotator Rule 1 Rule 2 Rule 3 Rule 4 Rule 5 Rule 6
I 12 12 5 33 13 9
II 17 14 6 31 10 4
III 15 22 5 16 7 8
Table 9: Frequency of selection of each of our rules (texts 1, 3, and 9)
Rule
Annotator
Rule
Annotator
Rule
Annotator
Rule
Annotator
I II III I II III I II III I II III
1 25 6 7 8 0 1 1 15 3 0 0 22 0 0 0
2 0 3 1 9 0 0 2 16 0 0 4 23 4 2 1
3 5 0 2 10 1 7 2 17 0 5 2 24 5 0 0
4 19 2 15 11 0 0 0 18 1 0 0 25 0 0 0
5 13 5 0 12 0 0 0 19 2 1 0 26 3 5 0
6 4 0 3 13 2 9 0 20 1 10 2 27 0 0 0
7 1 0 1 14 10 6 6 21 0 0 0 28 1 0 1
Table 10: Frequency of selection of each of the Simplext rules (texts 2, 4, and 10).
It can be seen that there is less difference/discrepancy in the selection of our rules as opposed to the
Simplext rules and hence the simplification process can be regarded as more consistent and reliable.
Here again, there is higher agreement on our rules as opposed to the Simplext ones. This phenomenon is
illustrated in the following example where the annotators used the Simplext rules:
Original: ?Esta reforma prev?e que todos los delitos relacionados con la seguridad vial (como
exceso de velocidad o conducir bajo los efectos del alcohol, las drogas, sin carn?e o sin puntos)
pueden conllevar el decomiso del veh??culo, si bien la decisi?on depender?a del juez.?
[This reform will envisage that all crimes related to road safety (such as speeding, driving
while under the effects of alcohol or drugs or driving without a licence or points) could result
in confiscation of the vehicle, although the decision to do so depends on the judge.]
Annotator 1: ?El cambio del C?odigo Penal dice que la decisi?on de embargar el coche o moto
depender?a del juez.? (rules used: 5,4,1,4,4)
[The change of the penal code says that the decision to confiscate the car or motorbike depends
on the judge.]
Annotator 2: ?Esta reforma prev?e que todos los delitos relacionados con la seguridad vial
como exceso de velocidad o conducir bajo los efectos del alcohol, las drogas, sin carn?e o sin
puntos. Los delitos pueden conllevar la retirada del veh??culo pero la decisi?on depender?a del
juez.? (rules used: 26,17,20,1,8)
[This reform will envisage that all crimes related to road safety such as speeding or driving
under the effects of alcohol, drugs, without a license or points. The crimes could result in
confiscation of the vehicle but the decision depends on the judge.]
Annotator 3: ?La reforma del C?odigo Penal prev?e que todos los delitos relacionados con la
seguridad vial pueden dar lugar a la p?erdida del veh??culo, aunque la decisi?on depender?a del
juez.? (rules used: 4,16,4,9)
[The penal code reform will envisage that all crimes related to road safety could result in loss
of the vehicle, although the decision depends on the judge.]
38
5.4 Familiarisation experiment
From the above results, it can be seen that the simplified texts written by the third annotator using a mixed
pattern indicate clearer preference to our simple rules in terms of better readability, time efficiency and
reliability as opposed to the simplified texts written by Annotator 1 and Annotator 2 where the Simplext
texts were applied only at the end. On the basis of this, we conjecture that this difference may be strongly
connected with the lingering familiarisation of the annotators when they simplify texts they have already
simplified.
6 Conclusions
Simplified texts play an important role in providing accessible and easy-to-understand information for a
whole range of users who, due to linguistic, developmental or social barriers, would have difficulty in
understanding materials which are not adapted and/or simplified. However, the production of simplified
texts can be a time-consuming and labour-intensive task. The results of this study show that a small set
of six simple rules, inspired by the concept of Controlled Languages, could produce simplified texts of
comparable readability to those produced using a long list of more fine-grained rules such as the ones
used in the Simplext project. In addition, the results of this study suggest that our simple rules could be
more time-efficient and reliable.
Acknowledgements
We would like to express our gratitude to Horacio Saggion for his help with the resources.
References
A. Anula. 2007. Tipos de textos, complejidad ling?u??stica y facilicitaci?on lectora. In Actas del Sexto Congreso de
Hispanistas de Asia, pages 45?61.
D. Arnold, L. Balkan, R. Lee Humphreys, S. Meijer, and L. Sadler, 1994. Machine Translation. An Introductory
guide., chapter 8, Input, pages 139?155. Blackwell publishers.
S. Cardey. 2009. Controlled Languages for More Reliable Human Communication in Safety Critical Domains. In
Proceedings of the 11th International Symposium on Social Communication, Santiago de Cuba, Cuba, 19-23
January 2009, pages 330?336.
B. Drndarevi?c, S.
?
Stajner, S. Bott, S. Bautista, and H. Saggion. 2013. Automatic Text Simplication in Spanish:
A Comparative Evaluation of Complementing Components. In Proceedings of the 12th International Confer-
ence on Intelligent Text Processing and Computational Linguistics. Lecture Notes in Computer Science. Samos,
Greece, 24-30 March, 2013., pages 488?500.
G. Freyhoff, G. Hess, L. Kerr, B. Tronbacke, and K. Van Der Veken, 1998. Make it Simple, European Guide-
lines for the Production of Easy-toRead Information for People with Learning Disability. ILSMH European
Association, Brussels.
J. Karreman, T. van der Geest, and E. Buursink. 2007. Accessible website content guidelines for users with
intellectual disabilities. Journal of Applied Research in Intellectual Disabilities, 20:510?518.
R. I. Kittredge, 2003. Oxford Handbook of Computational Linguistics, chapter 23, Sub-languages and controlled
languages.
Mencap, 2002. Am I making myself clear? Mencap?s guidelines for accessible writing.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
PlainLanguage. 2011. Federal plain language guidelines.
H. Saggion, E. G?omez Mart??nez, E. Etayo, A. Anula, and L. Bourg. 2011. Text Simplification in Simplext:
Making Text More Accessible. Revista de la Sociedad Espa?nola para el Procesamiento del Lenguaje Natural,
47:341?342.
39
S. Spaulding. 1956. A Spanish Readability Formula. Modern Language Journal, 40:433?441.
I. Temnikova. 2012. Text Complexity and Text Simplification in the Crisis Management domain. Ph.D. thesis,
University of Wolverhampton, UK.
S.
?
Stajner and H. Saggion. 2013. Readability Indices for Automatic Evaluation of Text Simplification Systems: A
Feasability Study for Spanish. In Proceedings of the 6th International Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan, 14-18 October 2013, pages 374?382.
40
