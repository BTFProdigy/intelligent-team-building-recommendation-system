Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2064?2074, Dublin, Ireland, August 23-29 2014.
Effective Incorporation of Source Syntax into
Hierarchical Phrase-based Translation
Tong Xiao??, Adri
`
a de Gispert?, Jingbo Zhu??, Bill Byrne?
? Northeastern University, Shenyang 110819, China
? Hangzhou YaTuo Company, Hangzhou 310012, China
? University of Cambridge, CB2 1PZ Cambridge, U.K.
{xiaotong,zhujingbo}@mail.neu.edu.cn
{ad465,wjb31}@eng.cam.ac.uk
Abstract
In this paper we explicitly consider source language syntactic information in both rule extraction
and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the
GHKM method and use them to complement Hiero-style rules. All these rules are then employed
to decode new sentences with source language parse trees. We experiment with our approach in
a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements
on the NIST newswire and web evaluation data of MT08 and MT12.
1 Introduction
Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT),
with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars
are easily extracted from word-aligned parallel corpora and can capture complex nested translation re-
lationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target
language syntax. This lack of constraint can lead to intractable decoding and bad performance due to
the over-generation of derivations in translation. To avoid these problems, the extraction and application
of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised;
and rules are limited to two non-terminals which are not allowed to be adjacent in the source language.
These constraints can yield good performing translation systems, although at a sacrifice in the ability to
model long-distance movement and complex reordering of multiple constituents.
By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on
either the source or target language side to guide SCFG extraction and translation. The parse tree provides
linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser
span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model
complex reordering multiple constituents. There are also modelling benefits as more meaningful features
can be used to encourage derivations with ?well-formed? syntactic tree structures. However, GHKM can
have robustness problems in that translation relies on the quality of the parse tree and the diversity of
rule types can lead to sparsity and limited coverage.
In this paper we describe a simple but effective approach to introducing source language syntax into
hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we
do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style
rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal,
2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the
baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not.
We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation
task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on
the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM
formalism and find, for example, that our approach works well with binarized trees.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2064
IP
NP
PN
?
VP
PP
P
?
NP
NN
??
VP
VV
??
NN
??
he
was
satisfied with the
answer
Hiero-style SCFG Rules
h
1
X? ??, he?
h
2
X? ??, with?
h
3
X? ???, the answer?
h
4
X? ?????, was satisfied?
h
5
X? ?X
1
????, was satisfied X
1
?
h
6
X? ?X
1
?? X
2
, was X
2
X
1
?
h
7
X? ?X
1
? X
2
????,
X
1
was satisfied with X
2
?
Tree-to-String Rules
r
1
NP(PN(?))? he
r
2
P(?)? with
r
3
NP(NN(??))? the answer
r
4
VP(VV(??) NN(??))? was satisfied
r
5
PP(x
1
:P x
2
:NP)? x
1
x
2
r
6
VP(x
1
:PP x
2
:VP)? x
2
x
1
r
7
IP(x
1
:NP x
2
:VP)? x
1
x
2
r
8
VP(PP(P(?) x
1
:NP) x
2
:VP)? x
2
with x
1
Figure 1: Hiero-syle and tree-to-string rules extracted from a pair of word-aligned Chinese-English
sentences with a source language (Chinese) parse tree.
2 Background
2.1 Hierarchical Phrase-based Translation
In the hierarchical phrase-based approach, translation is modelled using SCFGs. In general, probabilistic
SCFGs can be learned from word-aligned parallel data using heuristic methods (Chiang, 2007). We can
first extract initial phrase pairs and then obtain hierarchical phrase rules (i.e., rules with non-terminals
on the right hand side). Once the SCFG is obtained, new sentences can be decoded by finding the most
likely derivation of SCFG rules. See Figure 1 for example rules extracted from a sentence pair with word
alignments. A sequence of such rules covering the words of the source sentence is a SCFG derivation,
e.g., rules h
7
, h
1
and h
3
generate a derivation for the sentence pair.
The Hiero SCFG allows vast numbers of derivations which can make unconstrained decoding in-
tractable. In practice, several constraints are applied to control the model size and reduce ambiguity.
Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction,
set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set
to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the
glue rules).
2.2 Tree-to-String Translation
Instead of modelling the problem based on surface strings, tree-to-string systems model the translation
equivalency relations from source language syntactic trees to target language strings using derivations
of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A
tree-to-string rule is a tuple ?s
r
, t
r
,??, where s
r
is a source language tree-fragment with terminals and
non-terminals at leaves; t
r
is a string of target-language terminals and non-terminals; and ? is a 1-to-1
alignment between the non-terminals of s
r
and t
r
, for example, VP(VV(??) x
1
:NN)? increases x
1
is a tree-to-string rule, where the non-terminals labeled with the same index x
1
indicate the alignment.
To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al.,
2006) on the bilingual sentences with both word alignment and source (or target) language phrase-
structure tree annotations. In GHKM extraction, we first compute the set of the minimally-sized transla-
tion rules that can explain the mappings between source language tree and target-language string while
respecting the alignment and reordering between the two languages. More complex rules are then learned
by composing two or more minimal rules. See Figure 1 for rules extracted using GHKM.
One of the advantages of the above model is that non-terminals in tree-to-string rules are linguistically
2065
rule
match
decoding
input
string
Hiero
SCFG
ouput
string
(a) decoding with Hiero rules only
rule
match
decoding
input
string&tree
larger
SCFG
Hiero
SCFG
t-to-s
rules
ouput
string
(b) decoding with Hiero and tree-to-string rules
Figure 2: Overview of the Hiero baseline (a) and
our approach (b). ?means input or output of the
decoder. t-to-s is a short for tree-to-string.
VP
PP
P
?
x
1
:NP
x
2
:VP
x
2
with
x
1
X
?
?
?
X
1
X
2
, X
2
with
X
1
?
tree-to-string:
Hiero:
Figure 3: Converting the tree-to-string rule r
8
from Figure 1 to a Hiero-style rule.
motivated and can span word sequences with arbitrary length. Also, one can use rules with consecutive
(or more than two) source language non-terminals when the source language parse tree is available. For
example, r
8
in Figure 1 has a good Chinese syntactic structure indicating the reordered translations of NP
and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require
consecutive source language non-terminals (see Figure 3).
3 The Proposed Approach
Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and
weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while
hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences
1
and
free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance
hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is
simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and
then use tree-to-string rules as additional rules in decoding with the SCFG.
Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our
approach requires source language parse trees to be input in both rule extraction and decoding. In rule
extraction, we acquire tree-to-string rules using the GHKM method and Hiero-style rules using the Hiero-
style rule extraction method to form a larger SCFG. Then, we make use of both the input string and parse
tree to decode with the SCFG rules. We now describe our approach.
3.1 Transforming Tree-to-String Rules into SCFG Rules
As described in Section 2, tree-to-string rules have a different form from that of SCFG rules. We will use
tree-to-string rules in our hierarchical phrase-based systems by converting each tree-to-string rule into an
SCFG rule. The purpose of doing this is to make tree-to-string rules directly accessible to the Hiero-style
decoder which performs decoding with SCFG rules.
The rule mapping is straightforward: given a tree-to-string rule ?s
r
, t
r
,??, we take the frontier nodes
of s
r
as the source language part of the right hand side of the resulting SCFG rule, and keep t
r
and
? unchanged. Then we replace the non-terminal label with that used in the hierarchical phrase-based
system (e.g., X). See Figure 3 for rule mapping of rule r
8
of Figure 1.
In this way, every tree-to-string rule is associated with exactly one SCFG rule. Therefore we can
obtain a larger SCFG by combining the rules from the original Hiero-style SCFG and the transformed
tree-to-string rules. As explained next, to prevent computational problems we will apply these new rules
1
For example, the parser fails for 4% of the sentences in our training corpus, and 3% and 6% of the newswire and web
development/test sentences, indicating that the data is sometimes ill-formed.
2066
only on the spans that are consistent with the input parse trees. The main goal is to use the tree and the
adapted tree-to-string rules to provide the decoder with new linguistically-sensible translation hypotheses
that may be prevented by the usual Hiero constraints, and to do so without incurring a computational
explosion.
We categorize SCFG rules into two categories based on their availability in Hiero and GHKM extrac-
tion. If an SCFG rule is obtained from Hiero extraction, it is a type 1 rule; If not (i.e., this rule is only
available in GHKM extraction), it is a type 2 rule. E.g., the SCFG rule in Figure 3 is a type 2 rule because
it is not available in the original Hiero-style SCFG but can be generated from the tree-to-string rule.
Next we describe how each of these rule types are applied in decoding. We also describe which
features are used and how they are computed for each rule type.
3.2 Decoding
Both types of SCFG rules can be employed by usual Hiero decoders with a slight modification. Here
we follow the description of Hiero decoding by Iglesias et al. (2011). The source sentence is parsed
under the Hiero grammar using the CYK algorithm. Each cell in the CYK grid has associated with it a
list of rules that apply to its span; these rules are used to construct a recursive transition network (RTN)
which represents all translations of the source sentence under the grammar. The RTN is expanded to a
weighted finite state automaton for composition with n-gram language models (de Gispert et al., 2010).
Translations are produced via shortest path computation.
This procedure accommodates type 1 rules directly. For tree-to-string rules associated with type 2, we
attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching
tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to
a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process
is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar
modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing
(Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate
on the rule matching strategy.
Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception
that we impose no span limit on rule applications for source spans corresponding to constituents in the
Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is
based on string matching (see Figure 4(a)). For example, the type 1 rule h
9
in Figure 4(c) can be applied
to spans (1,13) and (2,13) since both of them agree with tree constituents (see Figure 4(b)). But h
9
is
not applied to span (3,13) because that span is longer than 10 words and agrees with no syntactic tree
constituent.
Type 2 Rules If the source side of a tree-to-string rule matches an input tree fragment: 1) that rule
is converted to a Hiero-style SCFG rule (Section 3.1); and 2) the Hiero-style rule is added to the rules
linked with the CYK grid cell associated with the span of the source syntactic tree fragment. Here, rules
are applied via tree matching. For example, rule h
11
in Figure 4(b) matches the tree fragment spanning
positions (2,13).
It is worth noting that some type 1 rules may be found via both Hiero-style and tree-to-string grammar
extraction. In this case we monitor whether a rule can be applied as a tree-to-string rule using tree-
matching so that features (Section 3.3) and weights can be set appropriately. As an example, rule h
10
in
Figure 4 is available in both extraction methods. For span (2,11), this rule can be matched via both string
matching and tree matching. We then note that we can apply h
10
as a tree-to-string rule for span (2, 11)
and activate the corresponding features defined in Section 3.3. For other spans (e.g., spans (2,3)-(2,10)),
no tree fragments can be matched and the baseline features are used for h
10
.
3.3 Features
The baseline feature set used in this work consists of 12 features (Pino et al., 2013), including a 4-gram
language model, a strong 5-gram language model, bidirectional translation probabilities, bidirectional
lexical weights, a word count, a phrase count, a glue rule count, a frequency-1 rule count, a frequency-2
2067
h9
: X? ?
X
1
?? , satisfied with X1 ?
???
1
?
2
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
??
12
??
13
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
span
(10,13)
matching
(a) matching a type 1 rule (h
9
) with the input string
IP
NP
NR
???
1
VP
PP
P
?
2
NP
??
3
?
4
?
5
?
6
?
7
??
8
?
9
??
10
??
11
VP
VV
??
12
NN
??
13
VP(PP(P(?) x
1
:NP) x
2
:VP)
? x
2
with x
1
h
11
: X? ?? X
1
X
2
,
X
2
with X
1
?
converting
. . .
.
.
.
.
.
.
.
.
.
Chart Used in Decoding
matching
span
(2,13)
(b) matching a type 2 rule (h
11
) with the input parse tree
ID Type Hiero-style Rule Tree-to-string Rule Applicable Spans
h
8
type 1 X? ?????, is satisfied ? N/A (12,13)
h
9
type 1 X? ? X
1
??, satisfied with X
1
? N/A (i,13), i = 1, 2 or 4 ? i ? 12
h
10
type 1 X? ?? X
1
, with X
1
? PP(P(?) x
1
NP)? with NPx
1
(2,j), 3 ? j ? 11 or j = 13
h
11
type 2 X? ?? X
1
X
2
, X
2
with X
1
? VP(PP(P(?) x
1
:NP) x
2
:VP) (2,13)
? x
2
with x
1
(c) example rules used in decoding
Figure 4: Decoding with both Hiero-style and tree-to-string grammars (span limit = 10). A span (i,j)
means spanning from position i to position j.
rule count, and a larger-than-frequency-2 rule count
2
. In addition, we introduce several features for
applying tree-to-string rules.
? Rule type indicators. We consider four indicator features, indicating tree-to-string rules, lexicalized
tree-to-string rules, rules with consecutive non-terminals, and non-lexicalized rules. Note that the tree-
to-string rule indicator feature is in principle a generalization of the soft syntactic features (Marton and
Resnik, 2008), in that a bonus (or penalty) is applied when a rule application is consistent with a source
tree constituent. The difference lies in that the tree-to-string rule indicator feature does not distinguish
between different syntactic labels, whereas soft syntactic features do.
? Features in syntactic MT. In general tree-to-string rules have their own features which are different
from those used in Hiero-style systems. For example, the features in syntactic MT systems can be
defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we
choose five popular features used in syntactic MT systems, including the bi-directional phrase-based
conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabil-
ities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates.
For example, the phrase-based features are the probabilities of translating between the frontier nodes
of s
r
and t
r
. The syntax-based features are the probabilities of generating r conditioned on its root,
2
We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline
system.
2068
source and target language sides, respectively. More formally, we use the following estimates for these
probabilities:
P
phr
(t
r
| s
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:?(s
r
?
)=?(s
r
)
c(r
?
)
P
phr
(s
r
| t
r
) =
?
r
??
:?(s
r
??
)=?(s
r
)?t
r
??
=t
r
c(r
??
)
?
r
?
:t
r
?
=t
r
c(r
?
)
P(r | root(r)) =
c(r)
?
r
?
:root(r
?
)=root(r)
c(r
?
)
P(r | s
r
) =
c(r)
?
r
?
:s
r
?
=s
r
c(r
?
)
P(r | t
r
) =
c(r)
?
r
?
:t
r
?
=t
r
c(r
?
)
where c(r) is the count of r, and root(?) and ?(?) are functions that return the source root symbol for
a tree-to-string rule and the sequence of leaf nodes for a tree-fragment respectively.
4 Evaluation
4.1 Experimental Setup
We report results in the NIST MT12 Chinese-English task, where our baseline system was among the top
academic systems. The parallel training corpus consists of 9.2 million sentence pairs which are provided
within the NIST Chinese-English MT12 track. Word alignments are obtained using MTTK (Deng and
Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links.
The data from newswire and web genres was used for tuning and test. The development sets contain
1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779
sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12
(mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test
sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language
model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English
side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST
MT12 and the Google counts corpus using the ?stupid? backoff method (Brants et al., 2007).
For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert
et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the
translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring
of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from
the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al.,
2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were
optimized using lattice-based minimum error rate training (Macherey et al., 2008).
For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and
extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we
restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded
lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules
with a Chinese-to-English translation probability of < 0.10.
4.2 Results
We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments
are organized as follows:
? Baseline and Span Limits (exp01 and exp02)
First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be
2069
Entry System Newswire Web
tune mt08 mt12 mt08.p all test tune mt08 mt12 mt08.p all test
(1755) (691) (400) (688) (1779) (2160) (666) (420) (682) (1768)
exp01 baseline 35.84 35.85 35.47 35.50 35.63* 29.98 25.15 23.07 27.19 25.33*
exp02 += no span limit 36.05 36.08 35.70 35.54 35.79* 30.11 25.28 23.08 27.17 25.37*
exp03 += t-to-s rules 36.63 36.51 36.08 36.09 36.25* 30.80 26.00 23.08 27.80 25.83*
exp04 += t-to-s features 36.82 36.49 36.53 36.16 36.38* 30.91 26.03 23.27 27.85 25.98*
exp05 t-to-s baseline 34.63 34.44 34.87 33.66 34.25* 28.30 23.40 21.38 25.30 23.56*
exp06 exp04 on spans > 10 36.17 36.11 35.71 35.86 35.92* 30.18 25.30 23.12 27.36 25.45*
exp07 exp04 with null trans. 36.10 36.03 35.35 34.86 35.42* 29.96 25.32 22.58 23.33 24.12*
exp08 exp04 + left binariz. 37.11 37.46 37.03 36.30 36.91* 31.18 26.15 23.54 27.98 26.13*
exp09 exp04 + right binariz. 36.58 36.56 36.41 35.70 36.20* 31.06 25.94 23.47 27.48 25.88*
exp10 exp04 + forest binariz. 37.03 37.27 37.09 36.62 36.98* 31.20 25.99 23.59 28.09 26.15*
Table 1: Case-insensitive BLEU[%] scores of various systems. += means incrementally adding method-
s/features to the previous system. * means that a system is significantly different than the exp01 baseline
at p < 0.01.
applied to any spans when they respect the tree constituents of the input tree. It can be regarded as
the simplest way of using source syntax in Hiero-style systems. Seen from Table 1, removing the
span limit shows modest BLEU improvements. It agrees with the previous result that loosening the
constraints on spans is helpful to systems based on the hard syntactic constraints (Li et al., 2013).
? GHKM+Hiero (exp03 and exp04)
The results of our proposed approach (w/o new features) are reported in exp03 and exp04. We see that
incorporating tree-to-string rules yields +0.6 and +0.5 improvements on the collected newswire and
web test sets (exp03 vs exp01). The new features (Section 3.3) give a further improvement (exp04 vs
exp03). This result confirms that the system can learn a preference for certain types of rules using the
new features.
? Impact of Search Space (exp05)
We also study the impact of search space on system performance. To do this, we force the improved
system (exp04) to respect source tree constituents and to discard any hypotheses which violate the
tree constituent constraints. Seen from exp05, this system has a lower BLEU score than both the
Hiero baseline (exp01) and GHKM+Hiero system (exp04), strongly suggesting that restricting MT
systems to a smaller space of hypotheses is harmful.
? GHKM+Hiero, Spans > 10 Only (exp06)
Another interesting question is whether tree-to-string rules and features are more helpful to larger
spans. We restricted our approach to spans > 10 only and conducted another experiment. As is shown
in exp06, applying tree-to-string rules and features for large spans is beneficial (exp06 vs. exp01). But
it underperforms the system with the full use of tree-to-string rules (exp06 vs. exp04). This interesting
observation implies that applying tree-to-string rules on smaller spans introduces good hypotheses that
can be selected with our additional features.
? Impact of Failed Parses (exp07)
As noted in Section 3, the parser fails to parse some of the sentences in our experiments. In this case
our approach generates the baseline result using the Hiero model (i.e., type 1 rules only). To investigate
the effect of failed parse trees on system performance, we also report the BLEU score including null
translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores
when null translations are included. It indicates that our approach is more robust than standard tree-
to-string systems which would generate an empty translation if the source language parser fails.
? Results on Binarization (exp08-10)
Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010).
exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina-
2070
Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ...
Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ...
GHKM+Hiero: After
North Korea again demanded that U.S. promised concessions before the new round of six-nation talks
, ...
a Hiero rule X? ?? X
1
?, after X
1
? is applied on span (1,15)
Input:
IP
PP
P
?
1
LCP
IP
??
2
??
3
??
4
??
5
?
6
?
7
??
8
?
9
?
10
??
11
?
12
??
13
??
14
LC
?
15
PU
,
VP
...
Reference: The Chinese star performance troupe presented a wonderful Peking opera as well as singing and dancing
Reference: performance to Hong Kong audience .
Baseline: Star troupe of China, highlights of Peking opera and dance show to the audience of Hong Kong .
GHKM+Hiero: Chinese star troupe presented a wonderful Peking opera singing and dancing
to
Hong Kong audience
.
Input:
A tree-to-string rule is applied:
(VP BA(?) x
1
:NP x
2
:VP PP(P(?) x
3
:NP))
? x
2
x
1
to x
3
IP
NP
??
1
??
2
???
3
VP
BA
?
4
NP
?
5
?
6
??
7
?
8
??
9
??
10
VP
VV
??
11
PP
P
?
12
NP
??
13
??
14
.
Figure 5: Comparison of translations generated by the baseline and improved systems.
rization
3
. We see that left-heavy binarization is very helpful and exp08 achieves overall improvements
of 1.2 and 0.8 BLEU points on the newsire and web data. In contrast, right-heavy binarization does
not yield promising performance. This agrees with the previous report (Wang et al., 2010) that MT
systems prefer to use certain ways of binarization in most cases. exp10 shows that the additional trees
introduced in our forest-based scheme are not sufficient to make a big impact on BLEU scores. Pos-
sibly larger gains can be obtained if taking a forest of parse trees from the source parser, but this is
outside the scope of this paper.
4.3 Analysis
We then analyse rule usage in the 1-best derivations for our improved system on the tuning set. We find
that type 2 rules represent 13.97% of the rules used in the 1-best derivations. Also, 44.45% of the applied
rules are available from the tree-to-string model (i.e., rules that use the features described in Section 3.3).
These numbers indicate that the tree-to-string rules are beneficial and our decoder likes to use them.
Finally, we discuss two real translation examples from our tuning set. See Figure 5 for translations
generated by different systems. In the first example, the Chinese input sentence contains? ...? which
is usually translated into after ... (i.e., a Hiero rule X? ?? X
1
?, after X
1
?). However, because the
?? ...?? pattern spans 15 words and that is beyond the span limit, our baseline is unable to apply this
desired rule and chooses a wrong translation in for the Chinese word ?. When the source parse tree
3
We found that the CTB-style parse trees usually have a very flat top-level IP (i.e., single clause) tree structure. As the IP
structure in Chinese is very complicated, the system might prefer a flexible binarization scheme. Thus we considered both left
and right-heavy binarization to form a binarization forest for IPs in Chinese parse trees, and binarized other tree constituents in
a left-heavy fashion.
2071
is available, our approach removes the span limit for spans that agree with the tree constituents. In this
case, the MT system successfully applies the rule on span (1, 15) and generates a much better translation.
In the second example, the translation of the input sentence requires complex reordering of adjacent
constituents. The baseline system cannot handle this case and generates a monotonic translation using
the glue rules. This results in a wrong order for the translation of Chinese verb?? (show). By contrast,
the improved system chooses a tree-to-string rule with three non-terminals (some of which are adjacent
in the source language) and perfectly performs a syntactic movement of the required tree constituents.
5 Related Work
Recently linguistically-motivated models have been intensively investigated in MT. In particular, source
tree-based models (Liu et al., 2006; Huang et al., 2006; Eisner, 2003; Zhang et al., 2008; Liu et al.,
2009a; Xie et al., 2011) have received growing interest due to their good abilities in modelling source
language syntax for better lexicon selection and reordering. Alternatively, the hierarchical phrase-based
approach (Chiang, 2005) considers the underlying hierarchical structures of sentences but does not re-
quire linguistically syntactic trees on either language side.
There are several lines of work for augmenting hierarchical phrase-based systems with the use of
source language phrase-structure trees. Liu et al. (2009b) describe novel approaches to translation under
multiple translation grammars. Their approach is very much motivated by system combination, and they
develop procedures for joint decoding and optimisation within a single system that give the benefit of
combining hypotheses from multiple systems. They demonstrate their approach by combining full tree-
to-string and Hiero systems. Our approach is much simpler and emphasises changes to the grammar
rather than the decoder or its parameter optimisation (MERT). Our aim is to augment the search space
of Hiero with linguistically-motivated hypotheses, and not to develop a new decoder that is capable of
translation under multiple grammars. Moreover, we consider Hiero as the backbone model and only
introduce tree-to-string rules where they can contribute; we show that extracting tree-to-string rules from
just 10% of the data suffices to get good gains. This results in a small number of tree-to-string rules and
does not slow down the decoder.
Another related line of work is to introduce syntactic constraints or annotations to hierarchical phrase-
based systems. Marton and Resnik (2008) and Li et al. (2013) proposed several soft or hard constraints to
model syntactic compatibility of Hiero derivations and input source language parse trees. We note that,
despite significant development effort, we were not able to improve our baseline through the use of these
soft syntactic constraints; it was this experience that led us to develop the hybrid approach described in
this paper.
Several research groups used syntactic labels as non-terminal symbols in their SCFG rules and develop
new features (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010; Hoang and
Koehn, 2010). However, all these methods still resort to rule extraction procedures similar to that of the
standard phrase/hierarchical rule extraction method. In contrast, we use the GHKM method which is a
mature technique to extract rules from tree-string pairs but does not impose those Hiero-style constraints
on rule extraction. More importantly, we consider the hierarchical syntactic tree structure to make use of
well-formed rules in decoding, while such information is not used in standard SCFG-based systems. We
also keep to the simpler non-terminals of Hiero, and do not ?decorate? any non-terminals with syntactic
or other information.
6 Conclusion
We have presented an approach to improving Hiero-style systems by augmenting the SCFG with tree-
to-string rules and syntax-based features. The input parse trees are used to introduce new linguistically-
sensible hypotheses into the translation search space while maintaining the Hiero robustness qualities
and avoiding computational explosion. We obtain significant improvements over a strong Hiero baseline
in Chinese-to-English. Further improvements are achieved when applying tree binarization.
2072
Acknowledgements
This work was done while the first author was visiting the speech group at University of Cambridge, and
was supported in part by the National Science Foundation of China (Grants 61272376 and 61300097),
and the China Postdoctoral Science Foundation (Grant 2013M530131). We would like to thank the
anonymous reviewers for their pertinent and insightful comments. We also would like to thank Juan
Pino, Rory Waite, Federico Flego and Gonzalo Iglesias for building parts of the baseline system.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in
Machine Translation. In Proceedings of EMNLP-CoNLL, pages 858?867, Prague, Czech Republic.
David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. In Proceedings of
ACL, pages 263?270, Ann Arbor, Michigan, USA.
David Chiang. 2007. Hierarchical Phrase-Based Translation. Computational Linguistics, 33:45?60.
David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proceedings of ACL, pages 1443?
1452, Uppsala, Sweden.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Blackwood, Eduardo R. Banga, and William Byrne. 2010. Hierarchi-
cal Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars. Computational
Linguistics, 36(3):505?533.
Yonggang Deng and William Byrne. 2008. HMM Word and Phrase Alignment for Statistical Machine Translation.
IEEE Transactions on Audio, Speech & Language Processing, 16(3):494?507.
Jason Eisner. 2003. Learning Non-Isomorphic Tree Mappings for Machine Translation. In Proceedings of ACL,
pages 205?208, Sapporo, Japan.
Yang Feng, Yang Liu, Qun Liu, and Trevor Cohn. 2012. Left-to-Right Tree-to-String Decoding with Prediction.
In Proceedings of EMNLP-CoNLL, pages 1191?1200, Jeju Island, Korea.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proceedings of
COLING-ACL, pages 961?968, Sydney, Australia.
Hieu Hoang and Philipp Koehn. 2010. Improved translation with source syntax labels. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 409?417, Uppsala, Sweden.
Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In
Proceedings of ACL, pages 144?151, Prague, Czech Republic.
Liang Huang and Haitao Mi. 2010. Efficient Incremental Decoding for Tree-to-String Translation. In Proceedings
of EMNLP, pages 273?283, Cambridge, MA, USA.
Liang Huang, Knight Kevin, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73, Cambridge, MA, USA.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga, and William Byrne. 2009. Rule Filtering by Pattern for
Efficient Hierarchical Translation. In Proceedings of EACL, pages 380?388, Athens, Greece.
Gonzalo Iglesias, Cyril Allauzen, William Byrne, Adri`a de Gispert, and Michael Riley. 2011. Hierarchical Phrase-
based Translation Representations. In Proceedings of EMNLP, pages 1373?1383, Edinburgh, Scotland, UK.
Junhui Li, Philip Resnik, and Hal Daum?e III. 2013. Modeling Syntactic and Semantic Structures in Hierarchical
Phrase-based Translation. In Proceedings of NAACL-HLT, pages 540?549, Atlanta, Georgia.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String Alignment Template for Statistical Machine Transla-
tion. In Proceedings of COLING-ACL, pages 609?616, Sydney, Australia.
Yang Liu, Yajuan L?u, and Qun Liu. 2009a. Improving Tree-to-Tree Translation with Packed Forests. In Proceed-
ings of ACL-IJCNLP, pages 558?566, Suntec, Singapore.
2073
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009b. Joint decoding with multiple translation models. In
Proceedings of ACL-IJCNLP, pages 576?584, Suntec, Singapore.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-based Minimum Error Rate
Training for Statistical Machine Translation. In Proceedings of EMNLP, pages 725?734, Honolulu, Hawaii.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight. 2006. SPMT: Statistical Machine Translation
with Syntactified Target Language Phrases. In Proceedings of EMNLP, pages 44?52, Sydney, Australia.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In
Proceedings of ACL-HLT, pages 1003?1011, Columbus, Ohio.
Haitao Mi and Liang Huang. 2008. Forest-based Translation Rule Extraction. In Proceedings of EMNLP, pages
206?214, Honolulu, Hawaii, USA.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-Based Translation. In Proceedings of ACL-HLT, pages
192?199, Columbus, Ohio.
Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. Bleu: a Method for Automatic Evaluation
of Machine Translation. In Proceedings of ACL, pages 311?318, Philadelphia, PA, USA.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of HLT-NAACL,
pages 404?411, Rochester, New York, USA.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gispert, Federico Flego, and William Byrne. 2013. The University
of Cambridge Russian-English system at WMT13. In Proceedings of WMT, pages 200?205, Sofia, Bulgaria.
David Vilar, Daniel Stein, Stephan Peitz, and Hermann Ney. 2010. If i only had a parser: poor man?s syntax for
hierarchical machine translation. In Proceedings of IWSLT, pages 345?352.
Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. 2010. Re-structuring, Re-labeling, and Re-aligning
for Syntax-Based Machine Translation. Computational Linguistics, 36(2):247?277.
Tong Xiao, Jingbo Zhu, Hao Zhang, and Qiang Li. 2012. NiuTrans: An Open Source Toolkit for Phrase-based
and Syntax-based Machine Translation. In Proceedings of ACL: System Demonstrations, pages 19?24, Jeju
Island, Korea.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of EMNLP, pages 216?226, Edinburgh, Scotland.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model. In Proceedings of ACL-HLT, pages 559?567, Columbus,
Ohio, USA.
Bing Zhao and Yaser Al-Onaizan. 2008. Generalizing Local and Non-Local Word-Reordering Patterns for Syntax-
Based Machine Translation. In Proceedings of EMNLP, pages 572?581, Honolulu, Hawaii.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In
Proceedings of WMT, pages 138?141, New York City.
2074
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 239?248,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Source-side Preordering for Translation using Logistic Regression and
Depth-first Branch-and-Bound Search
?
Laura Jehl
?
Adri
`
a de Gispert
?
Mark Hopkins
?
William Byrne
?
?
Dept. of Computational Linguistics, Heidelberg University. 69120 Heidelberg, Germany
jehl@cl.uni-heidelberg.de
?
SDL Research. East Road, Cambridge CB1 1BH, U.K.
{agispert,mhopkins,bbyrne}@sdl.com
Abstract
We present a simple preordering approach
for machine translation based on a feature-
rich logistic regression model to predict
whether two children of the same node
in the source-side parse tree should be
swapped or not. Given the pair-wise chil-
dren regression scores we conduct an effi-
cient depth-first branch-and-bound search
through the space of possible children per-
mutations, avoiding using a cascade of
classifiers or limiting the list of possi-
ble ordering outcomes. We report exper-
iments in translating English to Japanese
and Korean, demonstrating superior per-
formance as (a) the number of crossing
links drops by more than 10% absolute
with respect to other state-of-the-art pre-
ordering approaches, (b) BLEU scores im-
prove on 2.2 points over the baseline with
lexicalised reordering model, and (c) de-
coding can be carried out 80 times faster.
1 Introduction
Source-side preordering for translation is the task
of rearranging the order of a given source sen-
tence so that it best resembles the order of the tar-
get sentence. It is a divide-and-conquer strategy
aiming to decouple long-range word movement
from the core translation task. The main advan-
tage is that translation becomes computationally
cheaper as less word movement needs to be con-
sidered, which results in faster and better transla-
tions, if preordering is done well and efficiently.
Preordering also can facilitate better estimation
of alignment and translation models as the paral-
lel data becomes more monotonically-aligned, and
?
This work was done during an internship of the first au-
thor at SDL Research, Cambridge.
translation gains can be obtained for various sys-
tem architectures, e.g. phrase-based, hierarchical
phrase-based, etc.
For these reasons, preordering has a clear re-
search and commercial interest, as reflected by the
extensive previous work on the subject (see Sec-
tion 2). From these approaches, we are particu-
larly interested in those that (i) involve little or no
human intervention, (ii) require limited computa-
tional resources at runtime, and (iii) make use of
available linguistic analysis tools.
In this paper we propose a novel preordering
approach based on a logistic regression model
trained to predict whether to swap nodes in
the source-side dependency tree. For each pair
of sibling nodes in the tree, the model uses a
feature-rich representation that includes lexical
cues to make relative reordering predictions be-
tween them. Given these predictions, we conduct
a depth-first branch-and-bound search through
the space of possible permutations of all sibling
nodes, using the regression scores to guide the
search. This approach has multiple advantages.
First, the search for permutations is efficient and
does not require specific heuristics or hard limits
for nodes with many children. Second, the inclu-
sion of the regression prediction directly into the
search allows for finer-grained global decisions as
the predictions that the model is more confident
about are preferred. Finally, the use of a single
regression model to handle any number of child
nodes avoids incurring sparsity issues, while al-
lowing the integration of a vast number of features
into the preordering model.
We empirically contrast our proposed method
against another preordering approach based on
automatically-extracted rules when translating En-
glish into Japanese and Korean. We demonstrate
a significant reduction in number of crossing links
of more than 10% absolute, as well as translation
gains of over 2.2 BLEU points over the baseline.
239
We also show it outperforms a multi-class classifi-
cation approach and analyse why this is the case.
2 Related work
One useful way to organize previous preordering
techniques is by how they incorporate linguistic
knowledge.
On one end of the spectrum we find those ap-
proaches that rely on syntactic parsers and hu-
man knowledge, typically encoded via a set of
hand-crafted rules for parse tree rewriting or trans-
formation. Examples of these can be found
for French-English (Xia and McCord, 2004),
German-English (Collins et al., 2005), Chinese-
English (Wang et al., 2007), English-Arabic (Badr
et al., 2009), English-Hindi (Ramanathan et al.,
2009), English-Korean (Hong et al., 2009), and
English-Japanese (Lee et al., 2010; Isozaki et
al., 2010). A generic set of rules for transform-
ing SVO to SOV languages has also been de-
scribed (Xu et al., 2009). The main advantage of
these approaches is that a relatively small set of
good rules can yield significant improvements in
translation. The common criticism they receive is
that they are language-specific.
On the other end of the spectrum, there are pre-
ordering models that rely neither on human knowl-
edge nor on syntactic analysis, but only on word
alignments. One such approach is to form a cas-
cade of two translation systems, where the first
one translates the source to its preordered ver-
sion (Costa-juss`a and Fonollosa, 2006). Alterna-
tively, one can define models that assign a cost to
the relative position of each pair of words in the
sentence, and search for the sequence that opti-
mizes the global score as a linear ordering prob-
lem (Tromble and Eisner, 2009) or as a travel-
ing salesman problem (Visweswariah et al., 2011).
Yet another line of work attempts to automatically
induce a parse tree and a preordering model from
word alignments (DeNero and Uszkoreit, 2011;
Neubig et al., 2012). These approaches are at-
tractive due to their minimal reliance on linguistic
knowledge. However, their findings reveal that the
best performance is obtained when using human-
aligned data which is expensive to create.
Somewhere in the middle of the spectrum are
works that rely on automatic source-language syn-
tactic parses, but no direct human intervention.
Preordering rules can be automatically extracted
from word alignments and constituent trees (Li
et al., 2007; Habash, 2007; Visweswariah et
al., 2010), dependency trees (Genzel, 2010) or
predicate-argument structures (Wu et al., 2011),
or simply part-of-speech sequences (Crego and
Mari?no, 2006; Rottmann and Vogel, 2007). Rules
are assigned a cost based on Maximum En-
tropy (Li et al., 2007) or Maximum Likelihood es-
timation (Visweswariah et al., 2010), or directly
on their ability to make the training corpus more
monotonic (Genzel, 2010). The latter performs
very well in practice but comes at the cost of a
brute-force extraction heuristic that cannot incor-
porate lexical information. Recently, other ap-
proaches treat ordering the children of a node as
a learning to rank (Yang et al., 2012) or discrimi-
native multi-classification task (Lerner and Petrov,
2013). These are appealing for their use of finer-
grained lexical information, but they struggle to
adequately handle nodes with multiple children.
Our approach is closely related to this latter
work, as we are interested in feature-rich discrim-
inative approaches that automatically learn pre-
ordering rules from source-side dependency trees.
Similarly to Yang et al. (2012) we train a large
discriminative linear model, but rather than model
each child?s position in an ordered list of children,
we model a more natural pair-wise swap / no-swap
preference (like Tromble and Eisner (2009) did at
the word level). We then incorporate this model
into a global, efficient branch-and-bound search
through the space of permutations. In this way, we
avoid an error-prone cascade of classifiers or any
limit on the possible ordering outcomes (Lerner
and Petrov, 2013).
3 Preordering using logistic regression
and branch-and-bound search
Like Genzel (2010), our method starts with depen-
dency parses of source sentences (which we con-
vert to shallow constituent trees; see Figure 1 for
an example), and reorders the source text by per-
muting sibling nodes in the parse tree. For each
non-terminal node, we first apply a logistic regres-
sion model which predicts, for each pair of child
nodes, the probability that they should be swapped
or kept in their original order. We then apply
a depth-first branch-and-bound search to find the
global optimal reordering of children.
240
VB
he
NN
1
could
MD
2
stand
VB
3
NN
4
the
DT
smell
NN
nsubj
aux
HEAD
dobj
det HEAD
Figure 1: Shallow constituent tree generated from
the dependency tree. Non-terminal nodes inherit
the tag from the head.
3.1 Logistic regression
We build a regression model that assigns a prob-
ability of swapping any two sibling nodes, a and
b, in the source-side dependency tree. The proba-
bility of swapping them is denoted p(a, b) and the
probability of keeping them in their original order
is 1 ? p(a, b). We use LIBLINEAR (Fan et al.,
2008) for training an L1-regularised logistic re-
gression model based on positively and negatively
labelled samples.
3.1.1 Training data
We generate training examples for the logistic re-
gression from word-aligned parallel data which is
annotated with source-side dependency trees. For
each non-terminal node, we extract all possible
pairs of child nodes. For each pair, we obtain a
binary label y ? {?1, 1} by calculating whether
swapping the two nodes would reduce the number
of crossing alignment links. The crossing score of
having two nodes a and b in the given order is
cs(a, b) := |{(i, j) ? A
a
?A
b
: i > j}|
where A
a
and A
b
are the target-side positions to
which the words spanned by a and b are aligned.
The label is then given as
y(a, b) =
{
1 , cs(a, b) > cs(b, a)
?1 , cs(b, a) > cs(a, b)
Instances for which cs(a, b) = cs(b, a) are not
included in the training data. This usually happens
if either A
a
or A
b
is empty, and in this case the
alignments provide no indication of which order
is better. We also discard any samples from nodes
that have more than 16 children, as these are rare
cases that often result from parsing errors.

1
2 3 4
2 3
2
2
1
. . .
. . .
Figure 2: Branch-and-bound search: Partial search
space of permutations for a dependency tree node
with four children. The gray node marks a goal
node. For the root node of the tree in Figure 1, the
permutation corresponding to this path (1,4,3,2)
would produce ?he the smell stand could?.
3.1.2 Features
Using a machine learning setup allows us to in-
corporate fine-grained information in the form of
features. We use the following features to charac-
terise pairs of nodes:
l The dependency labels of each node
t The part-of-speech tags of each node.
hw The head words and classes of each node.
lm, rm The left-most and right-most words and classes
of a node.
dst The distances between each node and the head.
gap If there is a gap between nodes, the left-most
and right-most words and classes in the gap.
In order to keep the size of our feature space
manageable, we only consider features which oc-
cur at least 5 times
1
. For the lexical features, we
use the top 100 vocabulary items from our training
data, and 51 clusters generated by mkcls (Och,
1999). Similarly to previous work (Genzel, 2010;
Yang et al., 2012), we also explore feature con-
junctions. For the tag and label classes, we gen-
erate all possible combinations up to a given size.
For the lexical and distance features, we explicitly
specify conjunctions with the tag and label fea-
tures. Results for various feature configurations
are discussed in Section 4.3.1.
3.2 Search
For each non-terminal node in the source-side de-
pendency tree, we search for the best possible
1
Additional feature selection is achieved through L1-
regularisation.
241
permutation of its children. We define the score
of a permutation pi as the product of the proba-
bilities of its node pair orientations (swapped or
unswapped):
score(pi) =
?
1?i<j?k|pi[i]>pi[j]
p(i, j)
?
?
1?i<j?k|pi[i]<pi[j]
1? p(i, j)
Here, we represent a permutation pi of k nodes
as a k-length sequence containing each integer in
{1, ..., k} exactly once. Define a partial permu-
tation of k nodes as a k
?
< k length sequence
containing each integer in {1, ..., k} at most once.
We can construct a search space over partial per-
mutations in the natural way (see Figure 2). The
root node represents the empty sequence  and has
score 1. Then, given a search node representing
a k
?
-length partial permutation pi
?
, its successor
nodes are obtained by extending it by one element:
score(pi
?
? ?i?) = score(pi
?
)
?
?
j?V |i>j
p(i, j)
?
?
j?V |i<j
1? p(i, j)
where V = {1, ..., k}\(pi
?
? ?i?) is the set of source
child positions that have not yet been visited. Ob-
serve that the nodes at search depth k correspond
exactly to the set of complete permutations. To
search this space, we employ depth-first branch-
and-bound (Balas and Toth, 1983) as our search
algorithm. The idea of branch-and-bound is to
remember the best scoring goal node found thus
far, abandoning any partial paths that cannot lead
to a better scoring goal node. Algorithm 1 gives
pseudocode for the algorithm
2
. If the initial bound
(bound
0
) is set to 0, the search is guaranteed to
find the optimal solution. By raising the bound,
which acts as an under-estimate of the best scor-
ing permutation, search can be faster but possibly
fail to find any solution. All our experiments were
done with bound
0
= 0, i.e. exact search, but we
discuss search time in detail and pruning alterna-
tives in Section 4.3.2.
Since we use a logistic regression model and in-
corporate its predictions directly as swap probabil-
ities, our search prefers those permutations with
swaps which the model is more confident about.
2
See (Poole and Mackworth, 2010) for more details and a
worked example.
Algorithm 1 Depth-first branch-and-bound
Require: k: maximum sequence length, : empty sequence,
bound
0
: initial bound
procedure BNBSEARCH(, bound
0
, k)
best path? ?
bound? bound
0
SEARCH(??)
return best path
end procedure
procedure SEARCH(pi
?
)
if score(pi
?
) > bound then
if |pi
?
| = k then
best path? ?pi
?
?
bound? score(pi
?
)
return
else
for each i ? {1, ..., k}\pi
?
do
SEARCH(pi
?
? ?i?)
end for
end if
end if
end procedure
4 Experiments
4.1 Setup
We report translation results in English-to-
Japanese/Korean. Our corpora are comprised of
generic parallel data extracted from the web, with
some documents extracted manually and some au-
tomatically crawled. Both have about 6M sentence
pairs and roughly 100M words per language.
The dev and test sets are also generic. Source
sentences were extracted from the web and one
target reference was produced by a bilingual
speaker. These sentences were chosen to evenly
represent 10 domains, including world news,
chat/SMS, health, sport, science, business, and
others. The dev/test sets contain 602/903 sen-
tences and 14K/20K words each. We do English
part-of-speech tagging using SVMTool (Gim?enez
and M`arquez, 2004) and dependency parsing us-
ing MaltParser (Nivre et al., 2007).
For translation experiments, we use a phrase-
based decoder that incorporates a set of standard
features and a hierarchical reordering model (Gal-
ley and Manning, 2008) with weights tuned us-
ing MERT to optimize the character-based BLEU
score on the dev set. The Japanese and Korean lan-
guage models are 5-grams estimated on > 350M
words of generic web text.
For training the logistic regression model, we
automatically align the parallel training data and
intersect the source-to-target and target-to-source
alignments. We reserve a random 5K-sentence
242
approach EJ cs (%) EK cs (%)
rule-based (Genzel, 2010) 61.9 64.2
multi-class 65.2 -
df-bnb 51.4 51.8
Table 1: Percentage of the original crossing score
on the heldout set, obtained after applying each
preordering approach in English-Japanese (EJ,
left) and Korean (EK, right). Lower is better.
subset for intrinsic evaluation of preordering, and
use the remainder for model parameter estimation.
We evaluate our preordering approach with lo-
gistic regression and depth-first branch-and-bound
search (in short, ?df-bnb?) both in terms of reorder-
ing via crossing score reduction on the heldout set,
and in terms of translation quality as measured by
character-based BLEU on the test set.
4.2 Preordering baselines
We contrast our work against two data-driven pre-
ordering approaches. First, we implemented the
rule-based approach of Genzel (2010) and opti-
mised its multiple parameters for our task. We
report only the best results achieved, which corre-
spond to using ?100K training sentences for rule
extraction, applying a sliding window width of 3
children, and creating rule sequences of?60 rules.
This approach cannot incorporate lexical features
as that would make the brute-force rule extraction
algorithm unmanageable.
We also implemented a multi-class classifica-
tion setup where we directly predict complete per-
mutations of children nodes using multi-class clas-
sification (Lerner and Petrov, 2013). While this
is straightforward for small numbers of children,
it leads to a very large number of possible per-
mutations for larger sets of children nodes, mak-
ing classification too difficult. While Lerner and
Petrov (2013) use a cascade of classifiers and im-
pose a hard limit on the possible reordering out-
comes to solve this, we follow Genzel?s heuristic:
rather than looking at the complete set of children,
we apply a sliding window of size 3 starting from
the left, and make classification/reordering deci-
sions for each window separately. Since the win-
dows overlap, decisions made for the first window
affect the order of nodes in the second window,
etc. We address this by soliciting decisions from
the classifier on the fly as we preorder. One lim-
Figure 3: Crossing scores and classification accu-
racy improve with training data size.
itation of this approach is that it is able to move
children only within the window. We try to rem-
edy this by applying the method iteratively, each
time re-training the classifier on the preordered
data from the previous run.
4.3 Crossing score
We now report contrastive results in the intrin-
sic preordering task, as measured by the num-
ber of crossing links (Genzel, 2010; Yang et al.,
2012) on the 5K held-out set. Without preorder-
ing, there is an average of 22.2 crossing links in
English-Japanese and 20.2 in English-Korean. Ta-
ble 1 shows what percentage of these links re-
main after applying each preordering approach to
the data. We find that the ?df-bnb? method out-
performs the other approaches in both language
pairs, achieving more than 10 additional percent-
age points reduction over the rule-based approach.
Interestingly, the multi-class approach is not able
to match the rule-based approach despite using ad-
ditional lexical cues. We hypothesise that this is
due to the sliding window heuristic, which causes
a mismatch in train-test conditions: while samples
are not independent of each other at test time due
to window overlaps, they are considered to be so
when training the classifier.
4.3.1 Impact of training size and feature
configuration
We now report the effects of feature configura-
tion and training data size for the English-Japanese
case. We assess our ?df-bnb? approach in terms of
the classification accuracy of the trained logistic
243
features used acc (%) cs (%)
l,t,hw,lm,rm,dst,gap 82.43 51.3
l,t,hw,lm,rm,dst 82.44 51.4
l,t,hw,lm,rm 82.32 53.1
l,t,hw 82.02 55
l,t 81.07 58.4
Table 2: Ablation tests showing crossing scores
and classification accuracy as features are re-
moved. All models were trained on 8M samples.
regression model (using it to predict ?1 labels in
the held-out set) and by the percentage of crossing
alignment links reduced by preordering.
Figure 3 shows the performance of the logistic
regression model over different training set sizes,
extracted from the training corpus as described in
Section 3. We observe a constant increase in pre-
diction accuracy, mirrored by a steady decrease in
crossing score. However, gains are less for more
than 8M training examples. Note that a small vari-
ation in accuracy can produce a large variation in
crossing score if two nodes are swapped which
have a large number of crossing alignments.
Table 2 shows an ablation test for various fea-
ture configurations. We start with all features, in-
cluding head word and class (hw), left-most and
right-most word in each node?s span (lm, rm), each
node?s distance to the head (dst), and left-most
and right-most word of the gap between nodes
(gap). We then proceed by removing features to
end with only label and tag features (l,t), as in
Genzel (2010). For each configuration, we gener-
ated all tag- and label- combinations of size 2. We
then specified combinations between tag and label
and all other features. For the lexical features we
always used conjunctions of the word itself, and its
class. Class information is included for all words,
not just those in the top 100 vocabulary. Table 2
shows that lexical and distance feature groups con-
tribute to prediction accuracy and crossing score,
except for the gap features, which we omit from
further experiments.
4.3.2 Run time
We now demonstrate the efficiency of branch-and-
bound search for the problem of finding the opti-
mum permutation of n children at runtime. Even
though in the worst case the search could ex-
plore all n! permutations, making it prohibitive for
Figure 4: Average number of nodes explored in
branch-and-bound search by number of children.
nodes with many children, in practice this does
not happen. Many low-scoring paths are discarded
early by branch-and-bound search so that the opti-
mal solution can be found quickly. The top curve
in Figure 4 shows the average number of nodes
explored in searches run on our validation set (5K
sentences) as a function of the number of children.
All instances are far from the worst case
3
.
In our experiments, the time needed to conduct
exact search (bound
0
= 0) was not a problem ex-
cept for a few bad cases (nodes with more than 16
children), which we simply chose not to preorder;
in our data, 90% of the nodes have less than 6 chil-
dren, while only 0.9% have 10 children or more, so
this omission does not affect performance notice-
ably. We verified this on our held-out set, by car-
rying out exhaustive searches. We found that not
preordering nodes with 16 children did not worsen
the crossing score. In fact, setting a harsher limit
of 10 nodes would still produce a crossing score
of 51.9%, compared to the best score of 51.4%.
There are various ways to speed up the search,
if needed. First, one could impose a hard limit
on the number of explored nodes
4
. As shown
in Figure 4, a limit of 4K would still allow ex-
act search on average for permutations of up to
11 children, while stopping search early for more
children. We tested this for limits of 1K/4K nodes
and obtained crossing scores of 51.9/51.5%. Al-
ternatively, one could define a higher initial bound;
since the score of a path is a product of proba-
bilities, one would select a threshold probability
3
Note that 12!?479M nodes, whereas our search finds the
optimal permutation path after exploring <10K nodes.
4
As long as the limit exceeds the permutation length, a
solution will always be found as search is depth-first.
244
d approach ?LRM ? +LRM ?
baseline 25.39 - 26.62 -
rule-based 25.93 +0.54 27.65 +1.03
10
multi-class 25.60 +0.21 26.10 ?0.52
df-bnb 26.73 +1.34 28.09 +1.47
baseline 25.07 - 25.92 -
rule-based 26.35 +1.28 27.54 +1.62
4
multi-class 25.37 +0.30 26.31 +0.39
df-bnb 26.98 +1.91 28.13 +2.21
Table 3: English-Japanese BLEU scores with var-
ious preordering approaches (and improvement
over baseline) under two distortion limits d. Re-
sults reported both excluding and including lexi-
calised reordering model features (LRM).
p and calculate a bound depending on the size n
of the permutation as bound
0
= p
n?(n?1)
2
. Exam-
ples of this would be the lower curves of Figure 4.
The curve labels show the crossing score produced
with each threshold, and in parenthesis the per-
centage of searches that fail to find a solution with
a better score than bound
0
, in which case children
are left in their original order. As shown, this strat-
egy proves less effective than simply limiting the
number of explored nodes, because the more fre-
quent cases with less children remain unaffected.
4.4 Translation performance
Table 3 reports English-Japanese translation re-
sults for two different values of the distortion limit
d, i.e. the maximum number of source words that
the decoder is allowed to jump during search. We
draw the following conclusions. Firstly, all the
preordering approaches outperform the baseline
and the BLEU score gain they provide increases as
the distortion limit decreases. This is further anal-
ysed in Figure 5, where we report BLEU as a func-
tion of the distortion limit in decoding for both
English-Japanese and English-Korean. This re-
veals the power of preordering as a targeted strat-
egy to obtain high performance at fast decoding
times, since d can be drastically reduced with-
out performance degradation which leads to huge
decoding speed-ups; this is consistent with the
observations in (Xu et al., 2009; Genzel, 2010;
Visweswariah et al., 2011). We also find that with
preordering it is possible to apply harsher pruning
conditions in decoding while still maintaining the
Figure 5: BLEU scores as a function of distor-
tion limit in decoder (+LRM case). Top: English-
Japanese. Bottom: English-Korean.
exact same performance, achieving further speed-
ups. With preordering, our system is able to de-
code 80 times faster while producing translation
output of the same quality.
Secondly, we observe that the preordering
gains, which are correlated with the crossing score
reductions of Table 1, are largely orthogonal to
the gains obtained when incorporating a lexi-
calised reordering model (LRM). In fact, preorder-
ing gains are slightly larger with LRM, suggest-
ing that this reordering model can be better esti-
mated with preordered text. This echoes the notion
that reordering models are particularly sensitive
to alignment noise (DeNero and Uszkoreit, 2011;
Neubig et al., 2012; Visweswariah et al., 2013),
and that a ?more monotonic? training corpus leads
to better translation models.
Finally, ?df-bnb? outperforms all other preorder-
ing approaches, and achieves an extra 0.5?0.8
BLEU over the rule-based one even at zero distor-
tion limit. This is consistent with the substantial
crossing score reductions reported in Section 4.3.
We argue that these improvements are due to
the usage of lexical features to facilitate finer-
grained ordering decisions, and to our better
search through the children permutation space
which is not restricted by sliding windows, does
245
E
x
a
m
p
l
e
1
reference [
1
?????]
Barlow
[
2
???]
the smell
[
3
??]
endure
[
4
??????]
could
[
5
???]
hoped
[
6
?]
source [
1
Barlow] [
5
hoped] he [
4
could] [
3
stand] [
2
the smell] [
6
.]
preordered [
1
Barlow] he [
2
the smell] [
3
stand] [
4
could] [
5
hoped] [
6
.]
E
x
a
m
p
l
e
2
reference [
1
????]
my own
[
2
??]
experience
[
3
????]
in
, [
4
???????]
Rosa Parks
[
5
???]
called
[
6
???]
black
[
7
???]
woman
, [
8
???]
one day
[
9
????????]
somehow
[
10
???]
bus of
[
11
?????]
back seat in
[
12
??]
sit
??? [
13
????]
told being
[
14
???]
of
[
15
?????]
was fed up with
?
source [
3
In] [
1
my own] [
2
experience] , a [
6
black] [
7
woman] [
5
named] [
4
Rosa Parks] [
14
was just tired] [
8
one day]
[
14
of] [
13
being told] [
12
to sit] [
11
in the back] [
10
of the bus] .
rule-based [
1
my own] [
2
experience] [
3
In] [
14
was just tired] [
13
being told] [
10
the bus of] [
11
the back in] [
12
sit to] [
14
of]
[
8
one day] , [
6
a black] [
7
woman] [
4
Rosa Parks] [
5
named] .
df-bnb [
1
my own] [
2
experience] [
3
In] , [
5
named] [
6
a black] [
7
woman] [
4
Rosa Parks] [
10
the bus of] [
11
the back in]
[
12
sit to] [
13
told being] [
14
of] [
8
one day] [
14
was just tired] .
E
x
a
m
p
l
e
3
reference [
1
????]
we
?[
2
????]
quite
[
3
???]
Xi?an
[
4
??]
like
[
5
?]
to
[
6
?????]
come have
?
source [
1
we] [
6
have come] [
5
to] [
2
quite] [
4
like] [
1
xi?an] .
rule-based [
1
we] [
2
quite] [
4
like] [
3
xi?an] [
5
to] [
6
come have] .
df-bnb [
1
we] have [
2
quite] [
3
xi?an] [
4
like] [
5
to] [
6
come] .
baseline ????????????????
rule-based ??????????????????
df-bnb ????????????????
Table 4: Examples from our test data illustrating the differences between the preordering approaches.
not depend heavily on getting the right decision
in a multi-class scenario, and which incorporates
regression to carry out a score-driven search.
4.5 Analysis
Table 4 gives three English-Japanese examples
to illustrate the different preordering approaches.
The first, very short, example is preordered cor-
rectly by the rule-based and the df-bnb approach,
as the order of the brackets matches the order of
the Japanese reference.
For longer sentences we see more differences
between approaches, as illustrated by Example 2.
In this case, both approaches succeed at moving
prepositions to the back of the phrase (?my expe-
rience in?, ?the bus of?). However, while the df-
bnb approach correctly moves the predicate of the
second clause (?was just tired?) to the back, the
rule-based approach incorrectly moves the subject
(?a black woman named Rosa Parks?) to this posi-
tion - possibly because of the verb ?named? which
occurs in the phrase. This could be an indication
that the df-bnb is better suited for more compli-
cated constructions. With the exception of phrases
4 and 8, all other phrases are in the correct order
in the df-bnb reordering. None of the approaches
manage to reorder ?a black woman named Rosa
Parks? to the correct order.
Example 3 shows that the translations into
Japanese also reflect preordering quality. The
original source results in ?like? being translated
as the main verb (which is incorrectly interpreted
as ?to be like, to be equal to?). The rule-based
version correctly moves ?have come? to the end,
but fails to swap ?xi?an? and ?like?, resulting in
?come? being interpreted as a full verb, rather than
an auxiliary. Only the df-bnb version achieves al-
most perfect reordering, resulting in the correct
word choice of ?? (to get to, to become) for
?have come to?.
5
5 Conclusion
We have presented a novel preordering approach
that estimates a preference for swapping or not
swapping pairs of children nodes in the source-
side dependency tree by training a feature-rich
logistic regression model. Given the pair-wise
scores, we efficiently search through the space
of possible children permutations using depth-first
branch-and-bound search. The approach is able
to incorporate large numbers of features includ-
ing lexical cues, is efficient at runtime even with
a large number of children, and proves superior to
other state-of-the-art preordering approaches both
in terms of crossing score and translation perfor-
mance.
5
This translation is still not perfect, since it uses the wrong
level of politeness, an important distinction in Japanese.
246
References
Ibrahim Badr, Rabih Zbib, and James Glass. 2009.
Syntactic Phrase Reordering for English-to-Arabic
Statistical Machine Translation. In Proceedings of
EACL, pages 86?93, Athens, Greece.
Egon Balas and Paolo Toth. 1983. Branch and
Bound Methods for the Traveling Salesman Prob-
lem. Carnegie-Mellon Univ. Pittsburgh PA Manage-
ment Sciences Research Group.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. In Proceedings of ACL, pages 531?540,
Ann Arbor, Michigan.
Marta R. Costa-juss`a and Jos?e A. R. Fonollosa. 2006.
Statistical Machine Reordering. In Proceedings of
EMNLP, pages 70?76, Sydney, Australia.
Josep M. Crego and Jos?e B. Mari?no. 2006. Integra-
tion of POStag-based Source Reordering into SMT
Decoding by an Extended Search Graph. In Pro-
ceedings of AMTA, pages 29?36, Cambridge, Mas-
sachusetts.
John DeNero and Jakob Uszkoreit. 2011. Inducing
Sentence Structure from Parallel Corpora for Re-
ordering. In Proceedings of EMNLP, pages 193?
203, Edinburgh, Scotland, UK.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of EMNLP, pages 847?
855, Honolulu, Hawaii.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Jes?us Gim?enez and Llu??s M`arquez. 2004. SVMTool:
A general POS tagger generator based on Support
Vector Machines. In Proceedings of LREC, Lisbon,
Portugal.
Nizar Habash. 2007. Syntactic Preprocessing for Sta-
tistical Machine Translation. In Proceedings of MT-
Summit, pages 215?222, Copenhagen, Denmark.
Gumwon Hong, Seung-Wook Lee, and Hae-Chang
Rim. 2009. Bridging Morpho-Syntactic Gap be-
tween Source and Target Sentences for English-
Korean Statistical Machine Translation. In Proceed-
ings of ACL-IJCNLP, pages 233?236, Suntec, Sin-
gapore.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2010. Head Finalization: A Simple Re-
ordering Rule for SOV Languages. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 244?251, Up-
psala, Sweden.
Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.
2010. Constituent Reordering and Syntax Models
for English-to-Japanese Statistical Machine Trans-
lation. In Proceedings of COLING, pages 626?634,
Beijing, China.
Uri Lerner and Slav Petrov. 2013. Source-Side Clas-
sifier Preordering for Machine Translation. In Pro-
ceedings of EMNLP, Seattle, USA.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,
Ming Zhou, and Yi Guan. 2007. A Probabilistic
Approach to Syntax-based Reordering for Statistical
Machine Translation. In Proceedings of ACL, pages
720?727, Prague, Czech Republic.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a Discriminative Parser to Optimize
Machine Translation Reordering. In Proceedings of
EMNLP-CoNLL, pages 843?853, Jeju Island, Korea.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas
Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav
Marinov, and Erwin Marsi. 2007. Maltparser:
A language-independent system for data-driven de-
pendency parsing. Natural Language Engineering,
13(2):95?135.
Franz Josef Och. 1999. An efficient method for de-
termining bilingual word classes. In Proceedings of
EACL, pages 71?76, Bergen, Norway.
David L. Poole and Alan K. Mackworth. 2010. Ar-
tificial Intelligence: Foundations of Computational
Agents. Cambridge University Press. Full text on-
line at http://artint.info.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and Morphology: Addressing the crux
of the fluency problem in English-Hindi SMT. In
Proceedings of ACL-IJCNLP, pages 800?808, Sun-
tec, Singapore.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
TMI, pages 171?180, Sk?ovde, Sweden.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with auto-
matically derived rules for improved statistical ma-
chine translation. In Proceedings of COLING, pages
1119?1127, Beijing, China.
247
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for
improved machine translation. In Proceedings of
EMNLP, pages 486?496, Edinburgh, United King-
dom.
Karthik Visweswariah, Mitesh M. Khapra, and Anan-
thakrishnan Ramanathan. 2013. Cut the noise: Mu-
tually reinforcing reordering and alignments for im-
proved machine translation. In Proceedings of ACL,
pages 1275?1284, Sofia, Bulgaria.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese Syntactic Reordering for Statistical
Machine Translation. In Proceedings of EMNLP-
CoNLL, pages 737?745, Prague, Czech Republic.
Xianchao Wu, Katsuhito Sudoh, Kevin Duh, Hajime
Tsukada, and Masaaki Nagata. 2011. Extracting
Pre-ordering Rules from Predicate-Argument Struc-
tures. In Proceedings of IJCNLP, pages 29?37, Chi-
ang Mai, Thailand.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In Proceedings of COLING,
Geneva, Switzerland.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a Dependency Parser to Improve
SMT for Subject-Object-Verb Languages. In Pro-
ceedings of HTL-NAACL, pages 245?253, Boulder,
Colorado.
Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.
2012. A ranking-based approach to word reordering
for statistical machine translation. In Proceedings of
ACL, pages 912?920, Jeju Island, Korea.
248
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 259?268,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Word Ordering with Phrase-Based Grammars
Adri
`
a de Gispert, Marcus Tomalin, William Byrne
Department of Engineering, University of Cambridge, UK
ad465@cam.ac.uk, mt126@cam.ac.uk, wjb31@cam.ac.uk
Abstract
We describe an approach to word ordering
using modelling techniques from statisti-
cal machine translation. The system in-
corporates a phrase-based model of string
generation that aims to take unordered
bags of words and produce fluent, gram-
matical sentences. We describe the gen-
eration grammars and introduce parsing
procedures that address the computational
complexity of generation under permuta-
tion of phrases. Against the best previous
results reported on this task, obtained us-
ing syntax driven models, we report huge
quality improvements, with BLEU score
gains of 20+ which we confirm with hu-
man fluency judgements. Our system in-
corporates dependency language models,
large n-gram language models, and mini-
mum Bayes risk decoding.
1 Introduction
Word ordering is a fundamental problem in NLP
and has been shown to be NP-complete in dis-
course ordering (Althaus et al., 2004) and in SMT
with arbitrary word reordering (Knight, 1999).
Typical solutions involve constraints on the space
of permutations, as in multi-document summari-
sation (Barzilay and Elhadad, 2011) and preorder-
ing in SMT (Tromble and Eisner, 2009; Genzel,
2010).
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al.
(2009) use a dependency grammar to address word
ordering, while Zhang and Clark (2011; 2012)
use CCG and large-scale n-gram language models.
These techniques are applied to the unconstrained
problem of generating a sentence from a multi-set
of input words.
We describe GYRO (Get Your Order Right), a
phrase-based approach to word ordering. Given a
bag of words, the system first scans a large, trusted
text collection and extracts phrases consisting of
words from the bag. Strings are then generated
by concatenating these phrases in any order, sub-
ject to the constraint that every string is a valid
reordering of the words in the bag, and the re-
sults are scored under an n-gram language model
(LM). The motivation is that it is easier to make
fluent sentences from phrases (snippets of fluent
text) than from words in isolation.
GYRO builds on approaches developed for syn-
tactic SMT (Chiang, 2007; de Gispert et al., 2010;
Iglesias et al., 2011). The system generates strings
in the form of weighted automata which can be
rescored using higher-order n-gram LMs, depen-
dency LMs (Shen et al., 2010), and Minimum
Bayes Risk decoding, either using posterior prob-
abilities obtained from GYRO or SMT systems.
We report extensive experiments using BLEU
and conclude with human assessments. We
show that despite its relatively simple formulation,
GYRO gives BLEU scores over 20 points higher
than the best previously reported results, gener-
ated by a syntax-based ordering system. Human
fluency assessments confirm these substantial im-
provements.
2 Phrase-based Word Ordering
We take as input a bag of N words ? =
{w
1
, . . . , w
N
}. The words are sorted, e.g. alpha-
betically, so that it is possible to refer to the i
th
word in the bag, and repeated words are distinct
tokens. We also take a set of phrases, L(?) that
259
are extracted from large text collections, and con-
tain only words from ?. We refer to phrases as u,
i.e. u ? L(?). The goal is to generate all permu-
tations of ? that can be formed by concatenation
of phrases from L(?).
2.1 Word Order Generation Grammar
Consider a subset A ? ?. We can represent A by
an N-bit binary string I(A) = I
1
(A) . . . I
N
(A),
where I
i
(A) = 1 if w
i
? A, and I
i
(A) = 0 other-
wise. A Context-Free Grammar (CFG) for gener-
ation can then be defined by the following rules:
Phrase-based Rules: ?A ? ? and ?u ? L(A)
I(A)? u
Concatenation Rules: ?A ? ?, B ? A,C ? A
such that I(A) = I(B)+I(C) and I(B)?I(C) =
0
I(A)? I(B) I(C)
where ? is the bit-wise logical AND
Root: S ? I(?)
We use this grammar to ?parse? the list of the
words in the bag ?. The grammar has one non-
terminal per possible binary string, so potentially
2
N
distinct nonterminals might be needed to gen-
erate the language. Each nonterminal can produce
either a phrase u ? L(A), or the concatenation of
two binary strings that share no bits in common. A
derivation is sequence of rules that starts from the
bit string I(?). Rules are unweighted in this basic
formulation.
For example, assume the following bag
? = {a, b, c, d, e}, which we sort alphabet-
ically. Assume the phrases are L(?) =
{?a b?, ?b a?, ?d e c?}. The generation grammar
contains the following 6 rules:
R
1
: 11000? ab
R
2
: 11000? ba
R
3
: 00111? dec
R
4
: 11111? 11000 00111
R
5
: 11111? 00111 11000
R
6
: S? 11111
Figure 1 represents all the possible derivations
in a hypergraph, which generate four alternative
strings. For example, string ?d e c b a? is ob-
tained with derivation R
6
R
5
R
3
R
2
, whereas string
?a b d e c? is obtained via R
6
R
4
R
1
R
3
.
2.2 Parsing a Bag of Words
We now describe a general algorithm for parsing a
bag of words with phrase constraints. The search
a b c d e
11000 00111
3
1
2
1
2
2
1
11111
1
2
2
1
{"a b d e c", 
"b a d e c", 
"d e c a b", 
"d e c b a"}
{"d e c"}
{"a b", "b a"}
Figure 1: Hypergraph representing gen-
eration from {a, b, c, d, e} with phrases
{?a b?, ?b a?, ?d e c?}.
is organized along a two-dimensional gridM [x, y]
of 2
N
?1 cells, where each cell is associated with
a unique nonterminal in the grammar (a bit string
I with at least one bit set to 1). Each row x in
the grid has
(
N
x
)
cells, representing all the possible
ways of covering exactly x words from the bag.
There are N rows in total.
For a bit string I , X(I) is the length of I , i.e.
the number of 1?s in I . In this way X(I(A))
points to the row associated with set A. There
is no natural ordering of cells within a row, so
we introduce a second function Y (I) which indi-
cates which cell in row X(I) is associated with I .
Hence M [X(I), Y (I)] is the cell associated with
bit string I . In the inverse direction, we using the
notation I
x,y
to indicate a bit string associated with
the cell M [x, y].
The basic parsing algorithm is given in Figure 2.
We first initialize the grid by filling the cells linked
to phrase-based rules (lines 1-4 of Figure 2). Then
parsing proceeds as follows. For each row in in-
creasing order (line 5), and for each of the non-
empty cells in the row (line 6), try to combine its
bit string with any other bit strings (lines 7-8). If
combination is admitted, then form the resultant
bit string and add the concatenation rule to the as-
sociated cell in the grid (lines 9-10). The combi-
nation will always yield a bit string that resides in
a higher row of the grid, so search is exhaustive.
If a rule is found in cell M [N, 1], there is a parse
(line 11); otherwise none exists. The complexity
of the algorithm isO(2
N
?K). If back-pointers are
kept, traversing these from cell M [N, 1] yields all
the generated word sequences.
The number of cells will grow exponentially as
the bag grows in size. In practice, the number of
260
PARSE-BAG-OF-WORDS
Input: bag of words ? of size N
Input: list of phrases L(?)
Initialize - Add phrase-based rules:
1 M [x, y]? ?
2 for each subset A ? ?
3 for each phrase u ? L(A)
4 add rule I(A)? u to cell M [X(I(A)), Y (I(A))]
Parse:
5 for each row x = 1, . . . , N
6 for each y = 1, . . . ,
(
N
x
)
7 for each valid A ? ?
8 if I
x,y
? I(A) = 0, then
9 I
?
? I
x,y
+ I(A)
10 add rule I
?
? I
x,y
I(A) to cell M [X(I
?
), Y (I
?
)]
11 if |M [N, 1]| > 0, success.
Figure 2: Parsing algorithm for a bag of words.
cells actually used in parsing can be smaller than
2
N
? 1. This depends strongly on the number of
distinct phrase-based rules and the distinct subsets
of ? they cover. For example, if we consider 1-
word subsets of ?, then all cells are needed and
GYRO attempts all word permutation. However,
if only 10 distinct 5-word phrases and 20 distinct
4-word phrases are considered for a bag of N=9
words, then fewer than 431 cells will be used (20
+ 10 for the initial cells at rows 4 and 5; plus all
combinations of 4-word subsets into row 8, which
is less than 400; plus 1 for the last cell at row 9).
2.3 Generation from Exact Parsing
We are interested in producing the space of word
sequences generated by the grammar, and in scor-
ing each of the sequences according to a word-
based n-gram LM. Assuming that parsing the bag
of words suceeded, this is a very similar scenario
to that of syntax-based approaches to SMT: the
output is a large collection of word sequences,
which are built by putting together smaller units
and which can be found by a process of expansion,
i.e. by traversing the back-pointers from an initial
cell in a grid structure. A significant difference is
that in syntax-based approaches the parsing stage
tends to be computationally easier than the pars-
ing stage has only a quadratic dependency on the
length of the input sentence.
We borrow techniques from SMT to represent
and manipulate the space of generation hypothe-
ses. Here we follow the approach of expand-
ing this space onto a Finite-State Automata (FSA)
described in (de Gispert et al., 2010; Iglesias et
al., 2011). This means that in parsing, each cell
M [x, y] is associated with an FSA F
x,y
, which en-
codes all the sequences generated by the grammar
0
111000
2
00111 3
00111
11000
0
1a
2
b 3
b
a
0 1d 2e 3c
0
1a
2b
7
d
3
b
a
8e
4d 5e
6
c
9c
10a
11
b
b
a
11000
00111
11111
Expansion of RTN 11111
Figure 3: RTN representing generation from
{a, b, c, d, e} with phrases {?a b?, ?b a?, ?d e c?}
(top) and its expansion as an FSA (bottom).
when covering the words marked by the bit string
of that cell. When a rule is added to a cell, a new
path from the initial to the final state of F
x,y
is
created so that each FSA is the union of all paths
arising from the rules added to the cell. Impor-
tantly, when an instance of the concatenation rule
is added to a cell, the new path is built with only
two arcs. These point to other FSAs at lower rows
in the grid so that the result has the form of a
Recursive Transition Network with a finite depth
of recursion. Following the example from Sec-
tion 2.1, the top three FSAs in Figure 3 represent
the RTN for example from Figure 1.
The parsing algorithm is modified as follows:
4 add rule I(A)? u
as path to FSA F
X(I(A)),Y (I(A))
...
10 add rule I
?
? I
x,y
I(A)
as path to FSA F
X(I
?
),Y (I
?
)
11 if NumStates(F
N,1
) > 1, success.
At this point we specify two strategies:
Algorithm 1: Full expansion is described by the
pseudocode in Figure 4, excluding lines 2-3. A
recursive FSA replacement operation (Allauzen et
al., 2007) can be used to expand the FSA in the
top-most cell. In our running example, the result
261
is the FSA at the bottom of Figure 3. We then
apply a word-based LM to the resulting FSA via
standard FSA composition. This outputs the com-
plete (unpruned) language of interest, where each
word sequence generated from the bag according
to the phrasal constraints is scored by the LM.
Algorithm 2: Pruned expansion is described by
the pseudocode in Figure 4, now including lines
2-3. We introduce pruning because full, unpruned
expansion may not be feasible for large bags with
many phrasal rules. Once parsing is done, we in-
troduce the following bottom-up pruning strategy.
For each row starting at row r, we union all FSAs
of the row and expand the unioned FSA through
the recursive replacement operation. This yields
the space of all generation hypotheses of length
r. We then apply the language model to this lat-
tice and reduce it under likelihood-based pruning
at weight ?. We then update each cell in the row
with a new FSA obtained as the intersection of its
original FSA and the pruned FSA.
1
This intersec-
tion may yield an empty FSA for a particular cell
(meaning that all its hypotheses were pruned out
of the row), but it will always leave at least one
surviving FSA per row, guaranteeing that if pars-
ing succeeds, the top-most cell will expand into
a non-empty FSA. As we process higher rows,
the replacement operation will yield smaller FSAs
because some back-pointers will point to empty
FSAs. In this way memory usage can be con-
trolled through parameters r and ?. Of course,
when pruning in this way, the final output lattice
L will not contain the complete space of hypothe-
ses that could be generated by the grammar.
2.4 Algorithm 3: Pruned Parsing and
Generation
The two generation algorithms presented above
rely on a completed initial parsing step. However,
given that the complexity of the parsing stage is
O(2
N
? K), this may not be achievable in prac-
tice. Leaving aside time considerations, the mem-
ory required to store 2
N
FSAs will grow exponen-
tially in N , even if the FSAs contain only pointers
to other FSAs. Therefore we also describe an al-
gorithm to perform bottom-up pruning guided by
1
This step can be performed much more efficiently with
a single forward pass of the resultant lattice. This is possible
because the replace operation can yield a transducer where
the input symbols encode a pointer to the original FSA, so
in traversing the arcs of the pruned lattice, we know which
arcs will belong to which cell FSAs. However, for ease of
explanation we avoid this detail.
FULL-PARSE-EXPANSION
Input: bag of words ? of size N
Input: list phrases L(?)
Input: word-based LM G
Output: word lattice L of generated sequences
Generate:
1 PARSE-BAG-OF-WORDS(?)
2 for each row x = r, . . . , N ? 1
3 PRUNE-ROW(x)
4 F ? FSA-REPLACE(F
N,1
)
5 return L? F ?G
6 function PRUNE-ROW(x) :
7 F ?
?
y
F
x,y
8 F ? FSA-REPLACE(F )
9 F ? F ?G
10 F ? FSA-PRUNE(F, ?)
11 for each cell y = 1, . . . ,
(
N
x
)
12 F
x,y
? F
x,y
? F
13 return
Figure 4: Pseudocode for Algorithm 1 (excluding
lines 2-3) and Algorithm 2 (including all lines).
the LM during parsing. The pseudocode is identi-
cal to that of Algorithm 1 except for the following
changes: in parsing (Figure 2) we pass G as input
and we call the row pruning function of Figure 4
after line 5 if x ? r.
We note that there is a strong connection be-
tween GYRO and the IDL approach of Soricut
and Marcu (2005; 2006). Our bag of words parser
could be cast in the IDL-formalism, and the FSA
?Replace? operation would be expressed by an
IDL ?Unfold? operation. However, whereas their
work applies pruning in the creation of the IDL-
expression prior to LM application, GYRO uses
unweighted phrase constraints so the LM must be
considered for pruning while parsing.
3 Experimental Results
We now report various experiments evaluating the
performance of the generation approach described
above. The system is evaluated using the MT08-
nw, and MT09-nw testsets. These correspond to
the first English reference of the newswire por-
tion of the Arabic-to-English NIST MT evalua-
tion sets
2
. They contain 813 and 586 sentences
respectively (53,325 tokens in total; average sen-
tence length = 38.1 tokens after tokenization). In
order to reduce the computational complexity, all
sentences with more than 20 tokens were divided
into sub-sentences, with 20 tokens being the up-
per limit. Between 70-80% of the sentences in the
2
http://www.itl.nist.gov/iad/mig/tests/mt
262
6 8 10 12 14 16 18 201
10
100
1000
10000 2grams3grams4grams5grams
Size of the bag of words
Num
ber o
f n-g
rams
Figure 5: Average number of extracted phrases as
a function of the bag of word size.
testsets were divided in this way. For each of these
sentences we create a bag.
The GYRO system uses a n-gram LM estimated
over 1.3 billion words of English text, including
the AFP and Xinhua portions of the GigaWord
corpus version 4 (1.1 billion words) and the En-
glish side of various Arabic-English parallel cor-
pora typically used in MT evaluations (0.2 billion
words).
Phrases of up to length 5 are extracted for each
bag from a text collection containing 10.6 bil-
lion words of English news text. We use efficient
Hadoop-based look-up techniques to carry out this
extraction step and to retrieve rules for genera-
tion (Pino et al., 2012). The average number of
phrases extracted as a function of the size of the
bag is shown in Figure 5. These are the phrase-
based rules of our generation grammar.
3.1 Computational Analysis
We analyze here the computational requirements
of the three alternative GYRO algorithms pre-
sented in Sections 2.3 and 2.4. We carry out this
analysis on a subset of 200 random subsentences
from MT08-nw and MT09-nw chosen to have the
same sentence length distribution as the whole
data set. For a fixed generation grammar com-
prised of 3-gram, 4-gram and 5-gram rules only,
we run each algorithm with a memory limitation
of 20GB. If the process reaches this limit, then it
is killed. Figure 6 reports the worst-case memory
memory required by each algorithm as a function
of the size of the bag.
As shown, Full Expansion (Algorithm 1) is only
feasible for bags that contain at most 12 words.
By contrast, Pruned Expansion (Algorithm 2) with
? = 10 is feasible for bags of up to 18 words. For
4 6 8 10 12 14 16 18 20
0
2
4
6
8
10
12
14
16
18
20
bag of words size
me
mo
ry c
ons
ump
tion
 (in G
B)
 
 
Algorithm 1
Algorithm 2 ?=10
Algorithm 3 ?=10
Algorithm 3 ?=5
Figure 6: Worst-case memory required (GB) by
each GYRO algorithm relative to the size of the
bags.
bigger bags, the requirements of unpruned pars-
ing make generation intractable under the mem-
ory limit. Finally, Pruned Parsing and Generation
(Algorithm 3) is feasible at all bag sizes (up to 20
words), and its memory requirements can be con-
trolled via the beam-width pruning parameter ?.
Harsher pruning (i.e. lower ?) will incur more
coverage problems, so it is desirable to use the
highest feasible value of ?.
We emphasise that Algorithm 3, with suitable
pruning strategies, can scale up to larger problems
quite readily and generate output from much larger
input sets than reported here. We focus here on
generation quality for moderate sized problems.
3.2 Generation Performance
We now compare the GYRO system with the
Combinatory Categorial Grammar (CCG)-based
system described in (Zhang et al., 2012). By
means of extracted CCG rules, the CCG sys-
tem searches for an optimal parse guided by
large-margin training. Each partial hypothesis (or
?edge?) is scored using the syntax model and a 4-
gram LM trained similarly on one billion words of
English Gigaword data. Both systems are evalu-
ated using BLEU (Papineni et al., 2002; Espinosa
et al., 2010).
For GYRO, we use the pruned parsing algo-
rithm of Section 2.4 with r = 6 and ? = 10
and a memory usage limit of 20G. The phrase-
based rules of the grammar contain only 3-grams,
263
LM System MT08-nw MT09-nw
4g CCG 48.0 48.8
3g GYRO 59.0 58.4
GYRO +3g 63.0 64.1
4g GYRO +4g 65.5 65.9
100-best oracle 76.1 76.1
lattice oracle 80.4 80.2
Table 1: CCG and GYRO BLEU scores.
4-grams and 5-grams.
3
Under these conditions,
GYRO finds an output for 91.4% of the bags. For
the remainder, we obtain an output either by prun-
ing less or by adding bigram rules (in 7.2% of the
bags), or simply by adding all words as unigram
rules (1.4% of the bags).
Table 1 gives the results obtained by CCG and
GYRO under a 3-gram or a 4-gram LM. Because
GYRO outputs word lattices as opposed to a 1-
best hypothesis, we can reapply the same LM to
the concatenated lattices of any sentences longer
than 20 to take into account context in subsentence
boundaries. This is the result in the third row in
the Table, labeled ?GYRO +3g?. We can see that
GYRO benefits significantly from this rescoring,
beating the CCG system across both sets. This is
possibly explained by the CCG system?s depen-
dence upon in-domain data that have been explic-
itly marked-up using the CCG formalism. The fi-
nal row reports the positive impact of increasing
the LM order to 4.
Impact of generation grammar. To measure
the benefits of using high-order n-grams as con-
straints for generation, we also ran GYRO with
unigram rules only. This effectively does permu-
tation under the LM with the pruning mechanisms
described. The BLEU scores are 54.0 and 54.5 for
MT08-nw and MT09 respectively. This indicates
that a strong GYRO grammar is very much needed
for this type of parsing and generation.
Quality of generated lattices. We assess the
quality of the lattices output by GYRO under the
4-gram LM by computing the oracle BLEU score
of either the 100-best lists or the whole lattices
4
in the last two rows of Table 1. In order to com-
pute the latter, we use the linear approximation
to BLEU that allows an efficient FST-based im-
plementation of an Oracle search (Sokolov et al.,
2012). We draw two conclusions from these re-
sults: (a) that there is a significant potential for im-
3
Any word in the bag that does not occur in the large col-
lection of English material is added as a 1-gram rule.
4
Obtained by pruning at ? = 10 in generation.
provement from rescoring, in that even for small
100-best lists the improvement found by the Ora-
cle can exceed 10 BLEU points; and (b) that the
output lattices are not perfect in that the Oracle
score is not 100.
3.2.1 Rescoring GYRO output
We now report on rescoring procedures intended
to improve the first-pass lattices generated by
GYRO.
Higher-order language models. The first row
in Table 2 reports the result obtained when apply-
ing a 5-gram LM to the GYRO lattices generated
under a 4-gram. The 5-gram is estimated over the
complete 10.6 billion word collection using the
uniform backoff strategy of (Brants et al., 2007).
We find improvements of 3.0 and 1.9 BLEU with
respect to the 4-gram baseline.
Dependency language models. We now in-
vestigate the benefits of applying a dependency
LM (Shen et al., 2010) in a rescoring mode. We
run the MALT dependency parser
5
on the gener-
ation hypotheses and rescore them according to
log(p
LM
) + ?
d
log(p
depLM
), i.e. a weighted com-
bination of the word-based LM and the depen-
dency LM scores. Since it is not possible to run the
parser on the entire lattice, we carry out this exper-
iment using the 100-best lists generated from the
previous experiment (?+5g?). The dependency LM
is a 3-gram estimated on the entire GigaWord ver-
sion 5 collection (?5 billion words). Results are
shown in rows 2 and 3 in Table 2, where in each
row the performance over the set used to tune the
parameter ?
d
is marked with ?. In either case, we
observe modest but consistent gains across both
sets. We find this very promising considering that
the parser has been applied to noisy input sen-
tences.
Minimum Bayes Risk Decoding. We also use
Lattice-based Minimum Bayes Risk (LMBR) de-
coding (Tromble et al., 2008; Blackwood et al.,
2010a). Here, the posteriors over n-grams are
computed over the output lattices generated by the
GYRO system. The result is shown in row labeled
?+5g +LMBR?, where again we find modest but
consistent gains across the two sets with respect to
the 5-gram rescored lattices.
LMBR with MT posteriors. We investigate
LMBR decoding when applying to the generation
lattice a linear combination of the n-gram pos-
5
Available at www.maltparser.org
264
4g GYRO rescoring: MT08-nw MT09-nw
+5g 68.5 67.8
+5g +depLM ?
d
= 0.4 68.7
?
68.1
+5g +depLM ?
d
= 0.33 68.7 68.2
?
+5g +LMBR 68.6 68.3
+5g +LMBR-mt ? = 0.25 70.8
?
72.2
+5g +LMBR-mt ? = 0.25 70.8 72.2
?
Table 2: Results in BLEU when rescoring the lat-
tices generated by GYRO using various strategies.
Tuning conditions are marked by
?
.
terior probabilities extracted from (a) the same
generation lattice, and (b) from lattices produced
by an Arabic-to-English hierarchical-phrase based
MT system developed for the NIST 2012 OpenMT
Evaluation. As noted, LMBR relies on a posterior
distribution over n-grams as part of its computa-
tion or risk. Here, we use LMBR with a posterior
of the form ?p
GYRO
+ (1??) p
MT
. This is effec-
tively performing a system combination between
the GYRO generation system and the MT system
(de Gispert et al., 2009; DeNero et al., 2010) but
restricting the hypothesis space to be that of the
GYRO lattice (Blackwood et al., 2010b). Results
are reported in the last two rows of Table 2. Rel-
ative to 5-gram LM rescoring alone, we see gains
in BLEU of 2.3 and 4.4 in MT08-nw and MT09-
nw, suggesting that posterior distributions over n-
grams provided by SMT systems can give good
guidance in generation. These results also suggest
that if we knew what words to use, we could gen-
erate very good quality translation output.
3.3 Analysis and examples
Figure 7 gives GYRO generation examples. These
are often fairly fluent, and it is striking how the
output can be improved with guidance from the
SMT system. The examples also show the harsh-
ness of BLEU, e.g. ?german and turkish officials?
is penalised with respect to ? turkish and german
officials.? Metrics based on richer meaning rep-
resentations, such as HyTER, could be valuable
here (Dreyer and Marcu, 2012).
Figure 8 shows BLEU and Sentence Preci-
sion Rate (SPR), the percentage of exactly recon-
structed sentences. As expected, performance is
sensitive to length. For bags of up to 10, GYRO
reconstructs the reference perfectly in over 65%
of the cases. This is a harsh performance metric,
and performance falls to less than 10% for bags
of size 16-20. For bags of 6-10 words, we find
BLEU scores of greater than 85. Performance is
681 0862 66861 60842 46841 40g raams35?s12
11
02
01
?2
?1
?2
?1
?2
262
42?2
?212
02?2
?2?2Size of
 thbagwardbasNuagwamgn-?
Size
a??gn
b
 b?r
b??b
aonb?
t?tg?
afNrb
a? of
?
Figure 8: GYRO BLEU score and Sentence Pre-
cision Rate as a function of the bag of words size.
Computed on the concatenation of MT08-nw and
MT09-nw.
not as good for shorter segments, since these are
often headlines and bylines that can be ambiguous
in their ordering. The BLEU scores for bags of
size 21 and higher are an artefact of our sentence
splitting procedure. However, even for bag sizes
of 16-to-20 GYRO has BLEU scores above 55.
3.4 Human Assessments
Finally, the CCG and 4g-GYRO+5g systems were
compared using crowd-sourced fluency judge-
ments gathered on CrowdFlower. Judges were
asked ?Please read the reference sentence and
compare the fluency of items 1 & 2.? The test was
a selection of 75 fluent sentences of 20 words or
less taken from the MT dev sets. Each comparison
was made by at least 3 judges. With an average se-
lection confidence of 0.754, GYRO was preferred
in 45 cases, CCG was preferred in 14 cases, and
systems were tied 16 times. This is consistent with
the significant difference in BLEU between these
systems.
4 Related Work and Conclusion
Our work is related to surface realisation within
natural language generation (NLG). NLG typi-
cally assumes a relatively rich input representation
intended to provide syntactic, semantic, and other
relationships to guide generation. Example input
representations are Abstract Meaning Represen-
tations (Langkilde and Knight, 1998), attribute-
value pairs (Ratnaparkhi, 2000), lexical predicate-
argument structures (Bangalore and Rambow,
2000), Interleave-Disjunction-Lock (IDL) expres-
sions (Nederhof and Satta, 2004; Soricut and
Marcu, 2005; Soricut and Marcu, 2006), CCG-
bank derived grammars (White et al., 2007),
265
Hypothesis SBLEU
REF a third republican senator joins the list of critics of bush ?s policy in iraq .
(a) critics of bush ?s iraq policy in a third of republican senator joins the list . 47.2
(b) critics of bush ?s policy in iraq joins the list of a third republican senator . 69.8
(c) critics of bush ?s iraq policy in a list of republican senator joins the third . 39.1
(d) the list of critics of bush ?s policy in iraq a third republican senator joins . 82.9
REF it added that these messages were sent to president bashar al-asad through turkish and german officials .
(a-c) it added that president bashar al-asad through these messages were sent to german and turkish officials . 61.5
(d) it added that these messages were sent to president bashar al-asad through german and turkish officials . 80.8
REF a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , calling
for a new strategy just days before a new confrontation in congress
(a) a prominent republican senator george has joined the ranks of critics of bush ?s policy in iraq , just days
before a new strategy in congress calling for a new confrontation
66.7
(b) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just days
before congress calling for a new strategy in a new confrontation
77.8
(c) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , just days
before a new strategy in congress calling for a new confrontation
82.3
(d) a prominent republican senator has joined the ranks of critics of george bush ?s policy in iraq , calling
for a new strategy just days before a new confrontation in congress
100
Figure 7: 4g GYRO (Table 2) output examples, with sentence level BLEU: (a) GYRO+4g; (b)
GYRO+5g; (c) GYRO+5g+LMBR; (d) GYRO+5g+LMBR-mt. (a-c) indicates systems with identical
hypotheses.
meaning representation languages (Wong and
Mooney, 2007) and unordered syntactic depen-
dency trees (Guo et al., 2011; Bohnet et al., 2011;
Belz et al., 2011; Belz et al., 2012)
6
.
These input representations are suitable for ap-
plications such as dialog systems, where the sys-
tem maintains the information needed to gener-
ate the input representation for NLG (Lemon,
2011), or summarisation, where representations
can be automatically extracted from coherent,
well-formed text (Barzilay and Elhadad, 2011; Al-
thaus et al., 2004). However, there are other appli-
cations, such as automatic speech recognition and
SMT that could possibly benefit from NLG, but
which do not generate reliable linguistic annota-
tion in their output. For these problems it would
be useful to have systems, as described in this pa-
per, which do not require rich input representa-
tions. We plan to investigate these applications in
future work.
There is much opportunity for future develop-
ment. To improve coverage, the grammars of Sec-
tion 2.1 could perform generation with overlap-
ping, rather than concatenated, n-grams; and fea-
tures could be included to define tuneable log-
linear rule probabilities (Och and Ney, 2002; Chi-
ang, 2007). The GYRO grammar could be ex-
tended using techniques from string-to-tree SMT,
in particular by modifying the grammar so that
output derivations respect dependencies (Shen et
6
Surface Realisation Task, Generation Challenges 2011,
www.nltg.brighton.ac.uk/research/
genchal11
al., 2010); this will make it easier to integrate de-
pendency LMs into GYRO. Finally, it would be
interesting to couple the GYRO architecture with
automata-based models of poetry and rhythmic
text (Greene et al., 2010).
Acknowledgement
The research leading to these results has received
funding from the European Union Seventh
Framework Programme (FP7-ICT-2009-4)
under grant agreement number 247762, the
FAUST project faust-fp7.eu/faust/,
and the EPSRC (UK) Programme Grant
EP/I031022/1 (Natural Speech Technology)
natural-speech-technology.org .
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of CIAA, pages 11?23,
Prague, Czech Republic.
Ernst Althaus, Nikiforos Karamanis, and Alexander
Koller. 2004. Computing locally coherent dis-
courses. In Proceedings of the 42nd Annual Meeting
on Association for Computational Linguistics, page
399. Association for Computational Linguistics.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proceedings of the 18th conference on
Computational linguistics - Volume 1, COLING ?00,
pages 42?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
266
Regina Barzilay and Noemie Elhadad. 2011. In-
ferring strategies for sentence ordering in multi-
document news summarization. arXiv preprint
arXiv:1106.1820.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The first
surface realisation shared task: Overview and eval-
uation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 217?226,
Nancy, France.
Anja Belz, Bernd Bohnet, Simon Mille, Leo Wanner,
and Michael White. 2012. The surface realisation
task: Recent developments and future plans. In Pro-
ceedings of the 7th International Natural Language
Generation Conference, pages 136?140, Utica, IL,
USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010a. Efficient path counting transducers
for minimum Bayes-risk decoding of statistical ma-
chine translation lattices. In Proceedings of ACL:
Short Papers, pages 27?32, Uppsala, Sweden.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010b. Fluency constraints for minimum
Bayes-risk decoding of statistical machine transla-
tion lattices. In Proceedings of COLING, pages 71?
79, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <StuMaBa>: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
EMNLP-CoNLL, pages 858?867, Prague, Czech Re-
public.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
HLT-NAACL: Short Papers, pages 73?76, Boulder,
CO, USA.
Adri`a de Gispert, Gonzalo Iglesias, Graeme Black-
wood, Eduardo R. Banga, and William Byrne. 2010.
Hierarchical phrase-based translation with weighted
finite-state transducers and shallow-n grammars.
Computational Linguistics, 36(3):505?533.
John DeNero, Shankar Kumar, Ciprian Chelba, and
Franz Och. 2010. Model combination for machine
translation. In Proceedings of HTL-NAACL, pages
975?983, Los Angeles, CA, USA.
Markus Dreyer and Daniel Marcu. 2012. Hyter:
Meaning-equivalent semantics for translation eval-
uation. In Proceedings of NAACL-HLT, pages 162?
171, Montr?eal, Canada.
Dominic Espinosa, Rajakrishnan Rajkumar, Michael
White, and Shoshana Berleant. 2010. Further
meta-evaluation of broad-coverage surface realiza-
tion. In Proceedings of EMNLP, pages 564?574,
Cambridge, MA, USA.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine trans-
lation. In Proceedings of COLING, pages 376?384,
Beijing, China.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of EMNLP, pages 524?533, Cambridge,
MA, USA.
Yuqing Guo, Josef Van Genabith, and Haifeng Wang.
2011. Dependency-based n-gram models for gen-
eral purpose sentence realisation. Natural Language
Engineering, 17(04):455?483.
Gonzalo Iglesias, Cyril Allauzen, William Byrne,
Adri`a de Gispert, and Michael Riley. 2011. Hi-
erarchical phrase-based translation representations.
In Proceedings of EMNLP, pages 1373?1383, Edin-
burgh, Scotland, UK.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational
Linguistics, 25(4):607?615.
Irene Langkilde and Kevin Knight. 1998. Gener-
ation that exploits corpus-based statistical knowl-
edge. In Proceedings of ACL/COLING, pages 704?
710, Montreal, Quebec, Canada.
Oliver Lemon. 2011. Learning what to say and how to
say it: Joint optimisation of spoken dialogue man-
agement and natural language generation. Com-
puter Speech & Language, 25(2):210?221.
Mark-Jan Nederhof and Giorgio Satta. 2004. IDL-
expressions: A formalism for representing and pars-
ing finite languages in natural language processing.
Journal of Artificial Intelligence Research, 21:287?
317.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302, Philadelphia, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318, Philadelphia, PA, USA.
Juan Pino, Aurelien Waite, and William Byrne. 2012.
Simple and efficient model filtering in statistical ma-
chine translation. The Prague Bulletin of Mathemat-
ical Linguistics, 98:5?24.
267
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
NAACL, pages 194?201, Seattle, WA, USA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-dependency statistical machine transla-
tion. Computational Linguistics, 36(4):649?671.
Artem Sokolov, Guillaume Wisniewski, and Francois
Yvon. 2012. Computing lattice bleu oracle scores
for machine translation. In Proceedings of EACL,
pages 120?129, Avignon, France.
Radu Soricut and Daniel Marcu. 2005. Towards devel-
oping generation algorithms for text-to-text applica-
tions. In Proceedings of ACL, pages 66?74, Ann
Arbor, MI, USA.
Radu Soricut and Daniel Marcu. 2006. Stochastic
Language Generation Using WIDL-Expressions and
its Application in Machine Translation and Summa-
rization. In Proceedings of ACL, pages 1105?1112,
Sydney, Australia.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of EMNLP, pages 1007?1016, Singapore.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, pages 620?629, Honolulu,
Hawaii, USA.
Stephen Wan, Mark Dras, Robert Dale, and C?ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of EACL, pages 852?
860, Athens, Greece.
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface real-
ization with ccg. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Ma-
chine Translation (UCNLG+ MT).
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. Proceedings of Hu-
man Language Technologies: The Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT-07), pages
172?179.
Yue Zhang and Stephen Clark. 2011. Syntax-
based Grammaticality Improvement using CCG and
Guided Search. In Proceedings of EMNLP, pages
1147?1157, Edinburgh, Scotland, U.K.
Yue Zhang, Graeme Blackwood, and Stephen Clark.
2012. Syntax-based word ordering incorporating
a large-scale language model. In Proceedings of
EACL, pages 736?746, Avignon, France.
268
Pushdown Automata in Statistical
Machine Translation
Cyril Allauzen?
Google Research
Bill Byrne??
University of Cambridge
Adria` de Gispert??
University of Cambridge
Gonzalo Iglesias??
University of Cambridge
Michael Riley?
Google Research
This article describes the use of pushdown automata (PDA) in the context of statistical machine
translation and alignment under a synchronous context-free grammar. We use PDAs to com-
pactly represent the space of candidate translations generated by the grammar when applied to an
input sentence. General-purpose PDA algorithms for replacement, composition, shortest path,
and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the
PDA representation and these algorithms. We contrast the complexity of this decoder with a de-
coder based on a finite state automata representation, showing that PDAs provide a more suitable
framework to achieve exact decoding for larger synchronous context-free grammars and smaller
language models. We assess this experimentally on a large-scale Chinese-to-English alignment
and translation task. In translation, we propose a two-pass decoding strategy involving a weaker
language model in the first-pass to address the results of PDA complexity analysis. We study
in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art
performance for large-scale SMT.
? Google Research, 76 Ninth Avenue, New York, NY 10011. E-mail: {allauzen,riley}@google.com.
?? University of Cambridge, Department of Engineering. CB2 1PZ Cambridge, U.K. and SDL Research,
Cambridge U.K. E-mail: {wjb31,ad465,gi212}@eng.cam.ac.uk.
Submission received: 6 August 2012; revised version received: 20 February 2013; accepted for publication:
2 December 2013.
doi:10.1162/COLI a 00197
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 3
1. Introduction
Synchronous context-free grammars (SCFGs) are nowwidely used in statistical machine
translation, with Hiero as the preeminent example (Chiang 2007). Given an SCFG and
an n-gram language model, the challenge is to decode with them, that is, to apply them
to source text to generate a target translation.
Decoding is complex in practice, but it can be described simply and exactly in
terms of the formal languages and relations involved. We will use this description
to introduce and analyze pushdown automata (PDAs) for machine translation. This
formal description will allow close comparison of PDAs to existing decoders which are
based on other forms of automata. Decoding can be described in terms of the following
steps:
1. Translation: T = ?2({s}?G)
The first step is to compose the finite language {s}, which represents the
source sentence to be translated, with the algebraic relation G for the
translation grammar G. The result of this composition projected on the
output side is T , a weighted context-free grammar that contains all possible
translations of s under G. Following the usual definition of Hiero grammars,
we assume that G does not allow unbounded insertions so that T is a
regular language.
2. Language Model Application: L=T ?M
The next step is to compose the result of Step 1 with the weighted regular
grammarM defined by the n-gram language model, M. The result of this
composition is L, whose paths are weighted by the combined language
model and translation scores.
3. Search: l?=argmaxl?LL
The final step is to find the path through L that has the best combined
translation and language model score.
The composition {s} ? G in Step 1 that generates T can be performed by a modified
CYK algorithm (Chiang 2007). Our interest is in the different types of automata that can
be used to represent T as it is produced by this composition. We focus on three types
of representations: hypergraphs (Chiang 2007), weighted finite state automata (Iglesias
et al. 2009a; de Gispert et al. 2010), and PDAs. We will give a formal definition of PDAs
in Section 2, but we will first illustrate and compare these different representations by
a simple example.
Consider translating a source sentence ?s1 s2 s3? with a simple Hiero grammar G :
X??s1, t2 t3?
S??X s2 s3, t1 t2 X t4 t7?
S??X s2 s3, t1 t3 X t6 t7?
Step 1 yields the translations T = {?t1 t2 t2 t3 t4 t7? , ?t1 t3 t2 t3 t6 t7?}, and Figure 1 gives
examples of the different representations of these translations.We summarize the salient
features of these representations as they are used in decoding.
Hypergraphs. As described by Chiang (2007), a Hiero decoder can generate translations
in the form of a hypergraph, as in Figure 1a. As the figure shows, there is a
1:1 correspondence between each production in the CFG and each hyperedge in
the hypergraph.
688
Allauzen et al. Pushdown Automata in Statistical Machine Translation
(a) Hypergraph
0
1t1
6
t1
2t2
7t3
3X 4t4 5
t7
8X 9t6 10
t7 0 1t2 2t3
S X
(b) RTN
0
1t1
2
t1
3t2
4t3
5eps
6eps
7t2
8t2
9t3
10t3
11eps
12eps
13t4
14t6
15t7
16
t7
(c) FSA
0
1t1
6
t1
2t2
7t3
11
(
12t2
3 4t4 5
t7
[
8 9
t6
10t7
13t3
)
]
(d) PDA
Figure 1
Alternative representations of the regular language of possible translation candidates. Valid
paths through the PDA must have balanced parentheses.
Decoding proceeds by intersecting the translation hypergraph with a language
model, represented as a finite automaton, yielding L as a hypergraph. Step 3
yields a translation by finding the shortest path through the hypergraphL (Huang
2008).
Weighted Finite State Automata (WFSAs). Because T is a regular language and M is
represented by a finite automaton, it follows that T and L can themselves
be represented as finite automata. Consequently, Steps 2 and 3 can be solved
689
Computational Linguistics Volume 40, Number 3
using weighted finite-state intersection and single-source shortest path algo-
rithms, respectively (Mohri 2009). This is the general approach adopted in the
HiFST decoder (Iglesias et al. 2009a; de Gispert et al. 2010), which first represents
T as a Recursive Transition Network (RTN) and then performs expansion to
generate a WFSA.
Figure 1b shows the space of translations for this example represented as an RTN.
Like the hypergraph, it also has a 1:1 correspondence between each production
in the CFG and paths in the RTN components. The recursive RTN itself can be
expanded into a single WFSA, as shown in Figure 1c. Intersection and shortest
path algorithms are available for both of these WFSAs.
Pushdown Automata. Like WFSAs, PDAs are easily generated from RTNs, as will be
described later, and Figure 1d gives the PDA representation for this example. The
PDA represents the same language as the FSA, but with fewer states. Procedures
to carry out Steps 2 and 3 in decoding will be described in subsequent sections.
We will show that PDAs provide a general framework to describe key aspects
of several existing and novel translation algorithms. We note that PDAs have long
been used to describe parsing algorithms (Aho and Ullman 1972; Lang 1974), and it is
well known that pushdown transducers, the extended version of PDA with input and
output labels in each transition, do not have the expressive power needed to generate
synchronous context-free languages. For this reason, we do not use PDAs to implement
Step 1 in decoding: throughout this article a CYK-like parsing algorithm is always used
for Step 1. However, we do use PDAs to represent the regular languages produced in
Step 1 and in the intersection and shortest distance operations needed for Steps 2 and 3.
1.1 HiPDT: Hierarchical Phrase-Based Translation with PDAs
We introduce HiPDT, a hierarchical phrase-based decoder that uses a PDA representa-
tion for the target language. The architecture of the system is shown in Figure 2, where
CYK parse s with G
Build RTN
Expand RTN to FSA
Intersect FSA with LM
FSA 
Shortest 
Path
FSA 
Pruning
Lattice
1-Best
Hypothesis
RTN to PDA Replacement
Intersect PDA with LM
PDA 
(Pruned) 
Expansion
PDA 
Shortest 
Path
HiPDTHiFST
Figure 2
HiPDT versus HiFST: General flow and high-level operations.
690
Allauzen et al. Pushdown Automata in Statistical Machine Translation
we contrast it with HiFST (de Gispert et al. 2010). Both decoders parse the sentence with
a grammar G using a modified version of the CYK algorithm to generate the translation
search space as an RTN. Each decoder then follows a different path: HiFST expands
the RTN into an FSA, intersects it with the language model, and then prunes the result;
HiPDT performs the following steps:
1. Convert the RTN into PDA using the replacement algorithm. The PDA
representation for the example grammar in Section 1 is shown in Figure 1.
The algorithm will be described in Section 3.2.
2. Apply the language model scores to the PDA by composition. This operation
is described in Section 3.3.
3. Perform either one of the following operations:
(a) Shortest path through the PDA to get the exact best translation under
the model. Shortest distance/path algorithm is described in Section 3.4.
(b) Pruned expansion to an FSA. This expansion uses admissible pruning
and outputs a lattice. We do this for posterior rescoring steps. The
algorithm will be presented in detail in Sections 3.5 and 3.5.2.
The principal difference between the two decoders is the point at which finite-state
expansion is performed. In HiFST, the RTN representation is immediately expanded to
an FSA. In HiPDT, the PDA pruned expansion or shortest path computation is done
after the language model is applied, so that all computation is done with respect to both
the translation and language model scores.
The use of RTNs as an initial translation representation is somewhat influenced by
the development history of our FST and SMT systems. RTN algorithms were available
in OpenFST at the time HiFST was developed. HiPDT was developed as an extension to
HiFST using PDA algorithms, and these have subsequently been included in OpenFST.
A possible alternative approach could be to produce a PDA directly by traversing the
CYK grid. WFSAs could then be generated by PDA expansion, with a computational
complexity in speed and memory usage similar to the RTN-based approach.We present
RTNs as the initial translation representation because the generation of RTNs during
parsing is straightforward and has been previously presented (de Gispert et al. 2010).
We note, however, that RTN composition is algorithmically more complex than PDA
(and FSA) composition, so that RTNs themselves are not ideal representations of T if a
language model is to be applied. Composition of PDAs with FSAs will be discussed in
Section 3.3.
Figure 3 continues the simple translation example from earlier, showing how
HiPDT andHiFST both benefit from the compactness offeredbyWFSA epsilon removal,
determinization, andminimization operations. When applied to PDAs, these operations
treat parentheses as regular symbols. Compact representations of RTNs are shared by
both approaches. Figure 4 illustrates the PDA representation of the translation space
under a slightly more complex grammar that includes rules with alternative orderings
of nonterminals. The rule S??X1 s2 X2, t1 X1 X2? produces the sequence ?t1 t3 t4 t5 t6?,
and S??X1 s2 X2, t2 X2 X1? produces ?t2 t5 t6 t3 t4?. The PDA efficiently represents the
alternative orderings of the phrases ?t3 t4? and ?t5 t6? allowed under this grammar.
In addition to translation, this architecture can also be used directly to carry out
source-to-target alignment, or synchronous parsing, under the SCFG in a two-step
composition rather than one synchronous parsing stage. For example, by using M as the
automata that accepts ?t1 t2 t3 t6 t7?, Step 2 will yield all derivations that yield this string
691
Computational Linguistics Volume 40, Number 3
0 1t1
2t2
3
t3
4X
5X
6
t4
t6 7
t7
0 1t2 2t3
S X
(a) Optimized RTN
0 1t1
2t2
3
t3
4t2
5t2
6t3
7t3
8
t4
t6 9
t7
(b) Optimized FSA
0 1t1
2t2
3
t3 8
(
[ 9
t2
4
6
t4
7t7
5
t610t3
)
]
(c) Optimized PDA
Figure 3
Optimized representations of the regular language of possible translation candidates.
as a translation of the source string. This is the approach taken in Iglesias et al. (2009a)
and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b) for hypergraphs. In
Section 4 we analyze how PDAs can be used for alignment.
1.2 Goals
We summarize here the aims of this article.
We will show how PDAs can be used as compact representations of the space T
of candidate translations generated by a hierarchical phrase-based SCFG when
applied to an input sentence s and intersected with a language model M.
We have described the architecture of HiPDT, a hierarchical phrase-based de-
coder based on PDAs, and have identified the general-purpose algorithms needed
0
1
t1
2t2
3(
4
[
5
t3
6t57t4 8t69)
10]
)
11]
(
[
X??s1, t3 t4?
X??s3, t5 t6?
S??X1 s2 X2, t1 X1 X2?
S??X1 s2 X2, t2 X2 X1?
Figure 4
Example of translation grammar with reordered nonterminals and the PDA representing the
result of applying the grammar to input sentence s1 s2 s3.
692
Allauzen et al. Pushdown Automata in Statistical Machine Translation
to perform translation and alignment; in doing so we have highlighted the
similarities and differences relative to translation with FSAs (Section 1.1). We
will provide a formal description of PDAs (Section 2) and present in detail
the associated PDA algorithms required to carry out Steps 2 and 3, including
RTN replacement, composition, shortest path, expansion, and pruned expansion
(Section 3).
We will show both theoretically and experimentally that the PDA representation is
well suited for exact decoding under a large SCFG and a small languagemodel.
An analysis of decoder complexity in terms of the automata used in the repre-
sentation is presented (Section 3). One important aspect of the translation task
is whether the search for the best translation is admissible (or exact) under the
translation and language models. Stated differently, we wish to know whether a
decoder produces the actual shortest path found or whether some form of pruning
might have introduced search errors. In our formulation, we can exclude inadmis-
sible pruning from the shortest-path algorithms, and doing so makes it straight-
forward to compare the computational complexity of a full translation pipeline
using different representations of T (Section 4). We empirically demonstrate that
a PDA representation is superior to an FSA representation in the ability to perform
exact decoding both in an inversion transduction grammar?style word alignment
task and in a translation task with a small language model (Section 4). In these
experiments we take HiFST as a contrastive system for HiPDT, but we do not
present experimental results with hypergraph representations. Hypergraphs are
widely used by the SMT community, and discussions and contrastive experiments
between HiFST and cube pruning decoders are available in the literature (Iglesias
et al. 2009a; de Gispert et al. 2010).
We will propose a two-pass translation decoding strategy for HiPDT based on
entropy-pruned first-pass language models.
Our complexity analysis prompts us to investigate decoding strategies based on
large translation grammars and small language models. We describe, implement,
and evaluate a two-pass decoding strategy for a large-scale translation task using
HiPDT (Section 5). We show that entropy-pruned languagemodels can be used in
first-pass translation, followed by admissible beam pruning of the output lattice
and subsequent rescoring with a full language model. We analyze the search
errors that might be introduced by a two-pass translation approach and show
that these can be negligible if pruning thresholds are set appropriately (Sec-
tion 5.2). Finally, we detail the experimental conditions and speed/performance
tradeoffs that allow HiPDT to achieve state-of-the-art performance for large-
scale SMT under a large grammar (Section 5.3), including lattice rescoring steps
under a vast 5-gram language model and lattice minimum Bayes risk decoding
(Section 5.4).
With this translation strategyHiPDT can yield very good translation performance.
For comparison, the performance of this Chinese-to-English SMT described in
Section 5.4 is equivalent to that of the University of Cambridge submission to the
NIST OpenMT 2012 Evaluation.1
1 For details see http://www.nist.gov/itl/iad/mig/openmt12.cfm.
693
Computational Linguistics Volume 40, Number 3
2. Pushdown Automata
Informally, pushdown transducers are finite-state transducers that have been aug-
mented with a stack. Typically this is done by adding a stack alphabet and labeling
each transition with a stack operation (a stack symbol to be pushed onto, popped, or
read from the stack) in addition to the usual input and output labels (Aho and Ullman
1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petre and Salomaa 2009). Our
equivalent representation allows a transition to be labeled by a stack operation or a
regular input/output symbol, but not both. Stack operations are represented by pairs
of open and close parentheses (pushing a symbol on and popping it from the stack).
The advantage of this representation is that it is identical to the finite automaton repre-
sentation except that certain symbols (the parentheses) have special semantics. As such,
several finite-state algorithms either immediately generalize to this PDA representation
or do so with minimal changes. In this section we formally define pushdown automata
and transducers.
2.1 Definitions
A (restricted) Dyck language consist of ?well-formed? or ?balanced? strings over a
finite number of pairs of parentheses. Thus the string ( [ ( ) ( ) ] { } [ ] ) ( ) is in the
Dyck language over three pairs of parentheses (see Berstel 1979 for a more detailed
presentation).
More formally, let A and A be two finite alphabets such that there exists a bijection
f from A to A. Intuitively, f maps an open parenthesis to its corresponding close
parenthesis. Let a? denote f (a) if a?A and f?1(a) if a?A. The Dyck language DA
over the alphabet A?=A ? A is then the language defined by the following context-free
grammar: S? ?, S? SS and S? aSa? for all a?A. We define the mapping cA : A?? ? A??
as follows. cA(x) is the string obtained by iteratively deleting from x all factors of the
form aa? with a ? A. Observe that DA=c?1A (?). Finally, for a subset B ? A, we define the
mapping rB : A? ? B? by rB(x1 . . . xn)=y1 . . . yn with yi=xi if xi?B and yi=? otherwise.
A semiring (K,?,?, 0, 1) is a ring that may lack negation. It is specified by a set
of values K, two binary operations ? and ?, and two designated values 0 and 1.
The operation ? is associative, commutative, and has 0 as identity. The operation ?
is associative, has identity 1, distributes with respect to ?, and has 0 as annihilator:
for all a ? K, a? 0 = 0? a = 0. If ? is also commutative, we say that the semiring is
commutative.
The probability semiring (R+,+,?, 0, 1) is used when the weights represent prob-
abilities. The log semiring (R ? {?},?log,+,?, 0), isomorphic to the probability semi-
ring via the negative-log mapping, is often used in practice for numerical stability. The
tropical semiring (R ? {?}, min,+,?, 0) is derived from the log semiring using the
Viterbi approximation. These three semirings are commutative.
A weighted pushdown automaton (PDA) T over a semiring (K,?,?, 0, 1) is an
8-tuple (?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? and ? are the finite
open and close parenthesis alphabets, Q is a finite set of states, I?Q the initial state,
F ? Q the set of final states, E ? Q? (? ? ?? ? {?})?K?Q a finite set of transitions,
and ? : F? K the final weight function. Let e= (p[e], i[e],w[e], n[e]) denote a transition
in E; for simplicity, (p[e], i[e], n[e]) denotes an unweighted transition (i.e., a transition
with weight 1?).
694
Allauzen et al. Pushdown Automata in Statistical Machine Translation
A path ? is a sequence of transitions ?=e1 . . . en such that n[ei]=p[ei+1] for 1 ? i <
n. We then define p[?]=p[e1], n[?]=n[en], i[?]= i[e1] ? ? ? i[en], and w[?]=w[e1]? . . .?
w[en]. A path ? is accepting if p[?]= I and n[?]?F. A path ? is balanced if r??(i[?])?D?.
A balanced path ? accepts the string x??? if it is a balanced accepting path such that
r?(i[?])=x.
The weight associated by T to a string x??? is
T(x)=
?
??P(x)
w[?]??(n[?]) (1)
where P(x) denotes the set of balanced paths accepting x. A weighted language is
recognizable by a weighted pushdown automaton iff it is context-free. We define the
size of T as |T|= |Q|+|E|.
A PDA T has a bounded stack if there exists K ? N such that for any path ? from I
such that c?(r??(i[?])) ? ??:
|c?(r??(i[?]))| ? K (2)
In other words, the number of open parentheses that are not closed along ? is bounded.
If T has a bounded stack, then it represents a regular language. Figure 5 shows non-
regular, regular, and bounded-stack PDAs. A weighted finite automaton (FSA) can be
viewed as a PDA where the open and close parentheses alphabets are empty (see Mohri
2009 for a stand-alone definition).
Finally, a weighted pushdown transducer (PDT) T over a semiring (K,?,?, 0, 1)
is a 9-tuple (?,?,?,?,Q,E, I, F, ?) where ? is the finite input alphabet, ? is the finite
output alphabet, ? and ? are the finite open and close parenthesis alphabets, Q is a
finite set of states, I?Q the initial state, F ? Q the set of final states, E ? Q? (? ? ?? ?
0
1a
2
?
(
3)b
0
1
a
2
?
(
?
3
)
?
b
0
1
(
3
?
2
a
4(
)
5
b
)
(a) (b) (c)
0,?
1,(
?
3,?
?
2,(a
4,(?
?
5,(
b
?
0
1a:c/1
2
?:?
(:(/1
3):)b:c/1
2
0
?:?
1
a:c/1
3
S:  /1?
b:c/1
TS
(d) (e) (f)
Figure 5
PDA Examples: (a) Non-regular PDA accepting {anbn|n ? N}. (b) Regular (but not
bounded-stack) PDA accepting a?b?. (c) Bounded-stack PDA accepting a?b? and (d) its
expansion as an FSA. (e) Weighted PDT T1 over the tropical semiring representing the
weighted transduction (anbn, c2n) 7? 3n and (f) equivalent RTN ({S},{a, b}, {c}, {TS},S).
695
Computational Linguistics Volume 40, Number 3
{?})?K?Q a finite set of transitions, and ? : F? K the final weight function. Let
e= (p[e], i[e], o[e],w[e], n[e]) denote a transition in E. Note that a PDA can be seen as
a particular case of a PDT where i[e] = o[e] for all its transitions. For simplicity, our
following presentation focuses on acceptors, rather than the more general case of trans-
ducers. This is adequate for the translation applications we describe, with the exception
of the treatment of alignment in Section 4.3, for which the intersection algorithm for
PDTs and FSTs is given in Appendix A.
3. PDT Operations
In this section we describe in detail the following PDA algorithms: Replacement, Com-
position, Shortest Path, and (Pruned) Expansion. Although these are needed to implement
HiPDT, these are general purpose algorithms, and suitable for many other applications
outside the focus of this article. The algorithms described in this section have been
implemented in the PDT extension (Allauzen and Riley 2011) of the OpenFst library
(Allauzen et al. 2007). In this section, in order to simplify the presentation we will only
consider machines over the tropical semiring (R+ ? {?}, min,+,?, 0). However, for
each operation, we will specify in which semirings it can be applied.
3.1 Recursive Transition Networks
We briefly give formal definitions for RTNs that will be needed to present the RTN
expansion operation. Examples are shown earlier in Figures 1(b) and 3(a). Informally,
an RTN is an automaton where some labels, nonterminals, are recursively replaced
by other automata. We give the formal definition for acceptors; the extension to RTN
transducers is straightforward.
An RTN R over the tropical semiring (R+ ? {?}, min,+,?, 0) is a 4-tuple
(N,?, (T?)??N, S) where N is the alphabet of nonterminals, ? is the input alpha-
bet, (T?)??N is a family of FSTs with input alphabet ? ?N, and S ? N is the root
nonterminal.
A sequence x ? ?? is accepted by (R,?) if there exists an accepting path ? in T? such
that ? = ?1e1 . . . ?nen?n+1 with i[?k] ? ??, i[ek] ? N and such that there exists sequences
xk such that xk is accepted by (R, i[ek]) and x = i[?1]x1 . . . i[?n]xni[?n+1]. We say that x is
accepted by R when it is accepted by (R, S). The weight associated by (R,?) (and by R)
to x can be defined in the same recursive manner.
As an example of testing whether an RTN accepts a sequence, consider the RTN R
of Figure 6 and the sequence x = a a b. The path in the automata TS can be written as
? = ?1 e1 ?2, with i[?1] = a, i[e1] = X1, and i[?2] = b. In addition, the machine (R, i[e1])
accepts x1 = a. Because x = i[?1] x1 i[?2], it follows that x is accepted by (R, S).
3.2 Replacement
This algorithm converts an RTN into a PDA. As explained in Section 1.1, this PDT
operation is applied by the HiPDT decoder in Step 1, and examples are given in earlier
sections (e.g., in figures 1 and 3).
Replacement acts on every transition of the RTN that is associated with a non-
terminal. The source and destination states of these transitions are used to define the
matched opening and closing parentheses, respectively, in the new PDA. Each RTN
nonterminal transition is deleted and replaced by two new transitions that lead to and
696
Allauzen et al. Pushdown Automata in Statistical Machine Translation
from the automaton indicated by the nonterminal. These new transitions have matched
parentheses, taken from the source and destination states of the RTN transition they
replace. Figure 6 gives a simple example.
Formally, given an RTN R, defined as (N,?, (T?)??N, S), its replacement is the PDA
T equivalent to R defined by the 8-tuple (?,?,?,Q,E, I, F, ?) with Q = ? =
?
??N Q?,
I = IS, F = FS, ? = ?S, and E =
?
??N
?
e?E? E
e where Ee = {e} if i[e] 6? N and
Ee={(p[e], n[e], ?,w[e], I?), (f, n[e], ?, ??(f ), n[e])|f ?F?} (3)
with ? = i[e] ? N otherwise.
The complexity of the construction is in O(|T|). If |F?| = 1 for all ? ? N, then
|T| = O(???N |T?|) = O(|R|). Creating a superfinal state for each T? would lead to a T
whose size is always linear in the size of R. In this article, we assume this optimization
is always performed. We note here that RTNs can be defined and the replacement
operation can be applied in any semiring.
3.3 Composition
Once we have created the PDA with translation scores, Step 2 in Section 1.1 applies the
language model scores to the translation space. This is done by composition with an
FSA containing the relevant language model weights.
The class of weighted pushdown transducers is closed under composition with
weighted finite-state transducers (Bar-Hillel, Perles, and Shamir 1964; Nederhof and
Satta 2003). OpenFST supports composition between automata T1 and T2, where T1
is a weighted pushdown transducer and T2 is a weighted finite-state transducer. If
both T1 and T2 are acceptors, rather than transducers, the composition of a PDA and
an FSA produces a PDA containing their intersection, and so no separate intersection
algorithm is required for these automata. Given this, we describe only the simpler,
special case of intersection between a PDA and an FSA, as this is sufficient for most
of the translation applications described in this article. The alignment experiments of
RTN R
1 2 3 4
a X1 b
TS
5 6
X2
a 7 8
b
TX1 TX2
R accepts a a b and a b b.
PDT T
1 2
a
5 6
3 4
7 8
a
b
b
6
3 3?
6?
T accepts a 3 a 3? b and a 3 6 b 6? 3? b.
Figure 6
Conversion of an RTN R to a PDA T by the replacement operation of Section 3.2. Using the
notation of Section 2.1, in this example ? = {3, 5} and ? = {3?, 5?}, with f (3) = 3? and f (5) = 5?.
The unweighted transition (2,X1, 3) in R is deleted and replaced by two new transitions (2, 3, 5)
and (6, 3?, 3); similarly, (5,X2, 6) is replaced by (5, 6, 7) and (8, 6?, 6). After application of the r?
mapping, the strings accepted by R and by T are the same.
697
Computational Linguistics Volume 40, Number 3
0 1ab 2
a
b 3
a
b 4
a
b
T2
0
1a
2
?
(
3)b T1
0,0
1,1a
2,0
?
0,1(
3,0)
1,2a
2,1
?
b
0,2(
3,1)
1,3a
2,2
?
b
0,3(
3,2)
1,4a
2,3
?
b
0,4(
3,3)
2,4
?
b
T
Figure 7
Composition example: Composition of a PDA T1 accepting {an, bn} with an FSA T2 accepting
{a, b}4 to produce a PDA T = T1 ? T2 . T has only one balanced path, and this path accepts
a(a(?)b)b. Composition is performed by the PDA-FSA intersection described in Section 3.3.
Section 4.3 do require composition of transducers; the algorithm for composition of
transducers is given in Appendix A.
An example of composition by intersection is given in Figure 7. The states of T are
created as the product of all the states in T1 and T2. Transitions are added as illustrated
in Figure 8. These correspond to all paths through T1 and T2 that can be taken by
a synchronized reading of strings from {a, b}?. The algorithm is very similar to the
composition algorithm for finite-state transducers, the difference being the handling
of the parentheses. The parenthesis-labeled transitions are treated similarly to epsilon
transitions, but the parenthesis labels are preserved in the result. This adds many
unbalanced paths to T. In this example, T has five paths but only one balanced path,
so that T accepts the string a a b b.
Formally, given a PDA T1 = (?,?,?,Q1,E1, I1, F1, ?1) and an FSA T2 =
(?,Q2,E2, I2, F2, ?2), intersection constructs a new PDA T = (?,?,?,Q,E, I, F, ?),
where T = T1 ? T2 as follows:
1. The new state space is in the product of the input state spaces: Q ? Q1 ?Q2.
2. The new initial and final states are I = (I1, I2), and F = {(q1, q2) : q1 ? F1, q2 ? F2}.
3. Weights are assigned to final states (q1, q2) ? Q as ?(q1, q2) = ?(q1)+ ?(q2).
4. For pairs of transitions (q1, a1,w1, q?1) ? E1 and (q2, a2,w2, q?2) ? E2, a transition
is added between states (q1, q2) and (q?1, q?2) as specified in Figure 8.
PDT T1 FSA T2 PDT T = T1 ? T2 Input Symbols
q1 q?1
a1/w1
q2 q?2
a2/w2
q1, q2 q?1, q?2
a1/w1 + w2
a1 ? ? and a1 = a2
q1, q2 q?1, q2
a1/w1
a1 ? ? ?? or a1 = ?
Transitions are added to T if and only if the conditions on the input symbols are satisfied.
Figure 8
PDA?FSA intersection under the tropical semiring. The PDA T is created by the intersection of
the PDA T1 and the FSA T2, i.e., T = T1 ? T2.
698
Allauzen et al. Pushdown Automata in Statistical Machine Translation
The intersection algorithm given here assumes that T2 has no input-? transitions.
When T2 has input-? transitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and
Schalkwyk 2011) generalized to handle parentheses can be used. Note that Steps 1 and 2
do not require the construction of all possible pairs of states; only those states reachable
from the initial state and needed in Step 4 are actually generated. The complexity of
the algorithm is in O(|T1| |T2|) in the worst case, as will be discussed in Section 4.
Composition requires the semiring to be commutative.
3.4 Shortest Distance and Path Algorithms
With a PDA including both translation and language model weights, HiPDT can ex-
tract the best translation (Step 3a in Section 1.1). To this end, a general PDA shortest
distance/path algorithm is needed.
A shortest path in a PDA T is a balanced accepting path with minimal weight
and the shortest distance in T is the weight of such a path. We show that when
T has a bounded stack, shortest distance and shortest path can be computed in
O(|T|3 log |T|) time (assuming T has no negative weights) and O(|T|2) space. Figure 9
gives a pseudo-code description of the shortest-distance algorithm, which we now
discuss.
SHORTESTDISTANCE(T)
1 for each q ? Q and a ? ? do
2 B[q, a]? ?
3 for each q ? Q do
4 d[q, q]??
5 GETDISTANCE(T, I) ? I is the unique initial state
6 return d[I, f ] ? f is the unique final state
RELAX(s,q,w,S )
1 if d[s, q] > w then ? if w is a better estimate of the distance from s to q
2 d[s, q]? w ? update d[s, q]
3 if q 6? S then ? enqueue q in S if needed
4 ENQUEUE(S, q)
GETDISTANCE(T,s)
1 for each q ? Q do
2 d[s, q]??
3 d[s, s]? 0
4 Ss ? {s}
5 while Ss 6=? do
6 q? HEAD(Ss )
7 DEQUEUE(Ss )
8 for each e ? E[q] do ? E(q) is the set of transitions leaving state q
9 if i[e] ? ? ? {?} then ? i[e] is a regular symbol
10 RELAX(s,n[e], d[s, q]+w[e],Ss )
11 elseif i[e] ? ? then ? i[e] is a close parenthesis
12 B[s, i[e]]? B[s, i[e]] ? {e}
13 elseif i[e] ? ? then ? i[e] is an open parenthesis
14 if d[n[e], n[e]]=? then ? n[e] is the destination state of transition e
15 GETDISTANCE(T,n[e])
16 for each e? ? B[n[e], i[e]] do
17 w? d[s, q]+w[e]+ d[n[e], p[e?]]+ w[e?]
18 RELAX(s,n[e?],w,Ss )
Figure 9
PDT shortest distance algorithm.
699
Computational Linguistics Volume 40, Number 3
Given a PDA T = (?,?,?,Q,E, I, F, ?), the GETDISTANCE(T) algorithm computes
the shortest distance from the start state I to the final state2 f ? F. The algorithm
recursively calculates
d[q, q?] ? K ? the shortest distance from state q to state q? along a balanced path
At termination, the algorithm returns d[I, f ] as the cost of the shortest path through T.
The core of the shortest distance algorithm is the procedure GETDISTANCE(T, s)
which calculates the distances d[s, q] for all states q that can be reached from s. For an
FSA, this procedure is called once, as GETDISTANCE(T, I), to calculate d[I, q] ?q.
For a PDA, the situation is more complicated. Given a state s in T with at least
one incoming open parenthesis transition, we denote by Cs the set of states that can be
reached by a balanced path starting from s. If s has several incoming open parenthesis
transitions, a naive implementation might lead to the states in Cs to be visited exponen-
tially many times. This is avoided by memoizing the shortest distance from s to states in
Cs. To do this, GETDISTANCE(T, s) calculates d[s, s?] for all s? ? Cs, and it also constructs
sets of transitions
B[s, a] = {e ? E : p[e] ? Cs and i[e] = a} ?a ? ? (4)
These are the transitions with label a leaving states in Cs.
Consider any incoming transition to s, (q, a,w, s), with a ? ?. For every transition
e? = (s?, a,w?, q?), e? ? B[s, a] the following holds3
d[q, q?] = w+ d[s, s?]+ w? (5)
If d[s, s?] is available, the shortest distance from q to q? along any balanced path through
s can be computed trivially by Equation (5). For any state s with incoming open paren-
thesis transitions, only a single call to GETDISTANCE(T, s) is needed to precompute the
necessary values.
Figure 10 gives an example. When transition (2, (1, 0, 5) is processed,
GETDISTANCE(T, 5) is called. The distance d[5, 7] is computed, and following tran-
sitions are logged: B[5, (1]? {(7, )1, 0, 8)} and B[5, (2]? {(7, )2, 0, 9)}. Later, when the
transition (4, (2, 0, 5) is processed, its matching transition (7, )2, 0, 9) is extracted from
B[5, (2]. The distance d[4, 9] is then found by Equation (5) as d[5, 7]. This avoids redun-
dant re-calculation of distances along the shortest balanced path from state 4 to state 9.
We now briefly discuss the shortest distance pseudo-code given in Figure 9. The
description may be easier to follow after reading the worked example in Figure 10. Note
that the sets Cs are not computed explicitly by the algorithm.
The shortest distance calculation proceeds as follows. Self-distances, that is, d[q, q],
are set initially to ?; when GETDISTANCE(T, q) is called it sets d[q, q] = 0 to note that
q has been visited. GETDISTANCE(T, s) starts a new instance of the shortest-distance
algorithm from s using the queue Ss, initially containing s. While the queue is not empty,
a state is dequeued and its outgoing transitions examined (lines 7?11). Transitions
labeled by non-parenthesis are treated as in Mohri (2009) (lines 7?8). When a transition
e is labeled by a close parenthesis, e is added to B[s, i[e]] to indicate that this transition
2 For simplicity, we assume T has only one final state.
3 This assumes all paths from q to q? pass through s. The RELAX operation (Figure 9) handles the
general case.
700
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
GETDISTANCE(T) runs
1. Initialization: d[q, q]??, ?q ? Q
2. GETDISTANCE(T, 0) is called
GETDISTANCE(T, 0) runs
3. Distances are calculated from state 0:
d[0, 0]? 0; d[0, 1]? d[0, 0]+ w[0, 1]; d[0, 2]? d[0, 1]+ w[1, 2]
4. Transition e1 = (2, (1, 0, 5) is reached. e1 has symbol i[e1] = (1 and destination state n[e1] = 5
5. d[5, 5] =? so GETDISTANCE(T, 5) is called
GETDISTANCE(T, 5) runs
6. Distances are calculated from state 5:
d[5, 5]? 0; d[5, 6]? d[5, 5]+ w[5, 6]; d[5, 7]? d[5, 6]+ w[6, 7]
7. The transitions (7, )1, 0, 8) and (7, )2, 0, 9) are reached and memoized
B[5, (1]? {(7, )1, 0, 8)}
B[5, (2]? {(7, )2, 0, 9)}
GETDISTANCE(T, 5) ends
GETDISTANCE(T, 0) resumes
8. Transition e1 = (2, (1, 0, 5) is still being processed, with p[e1] = 2, n[e1] = 5, and i[e1] = (1
9. Transition e2 = (7, )1, 0, 8) matching (1 is extracted from B[n[e1], i[e1]], with p[e2] = 7
and n[e2] = 8
10. Distance d[0, 8] is calculated as d[0, n[e2]] :
d[0, n[e2]]? d[0, p[e1]]+ w[p[e1],n[e1]]+ d[n[e1], p[e2]]+ w[p[e2],n[e2]]
10. Processing of e1 finishes, and calculation of distances from 0 continues:
d[0, 10]? d[0, 8]+ w[8, 10]
10 is a final state. Processing continues with transition (0, t1, 20, 3)
d[0, 3]? d[0, 0]+ w[0, 3]; d[0, 4]? d[0, 3]+ w[3, 4]
13. Transition e3 = (4, (2, 0, 5) is reached
e3 has symbol i[e3] = (2, source state p[e3] = 4, and destination state n[e3] = 5
14. GETDISTANCE(T, 5) is not called, since d[5, 5] = 0 indicates state 5 has been previously
visited
15. Transition e4 = (7, )2, 0, 9) matching (2 is extracted from B[n[e3], i[e3]], with p[e4] = 7 and
n[e4] = 9
16. Distance d[0, 9] is calculated as d[0, n[e4]], using cached values:
d[0, n[e4]]? d[0, p[e3]]+ w[p[e3],n[e3]]+ d[n[e3], p[e4]]+ w[p[e4],n[e4]]
17. d[0, 10] is less than? :
d[0, 10]? min(d[0, 10], d[0, 9]+ w[9, 10])
18. GETDISTANCE(T, 0) ends and returns d[0, 10]
GETDISTANCE(T) ends
Figure 10
Step-by-step description of the shortest distance calculation for the given PDA by the algorithm
of Figure 9. For simplicity, w[q, q?] indicates the weight of the transition connecting q and q?.
balances all incoming open parentheses into s labeled by i[e] (lines 9?10). Finally, if e has
an open parenthesis, and if its destination has not already been visited, a new instance of
GETDISTANCE is started from n[e] (lines 12?13). The destination states of all transitions
balancing e are then relaxed (lines 14?16).
The space complexity of the algorithm is quadratic for two reasons. First, the
number of non-infinity d[q, s] is |Q|2. Second, the space required for storing B is at
most in O(|E|2) because for each open parenthesis transition e, the size of |B[n[e], i[e]]|
701
Computational Linguistics Volume 40, Number 3
is O(|E|) in the worst case. This last observation also implies that the accumulated
number of transitions examined at line 16 is in O(Z|Q| |E|2) in the worst case, where
Z denotes the maximal number of times a state is inserted in the queue for a given
call of GETDISTANCE. Assuming the cost of a queue operation is ?(n) for a queue
containing n elements, the worst-case time complexity of the algorithm can then be
expressed as O(Z|T|3 ?(|T|)). When T contains no negative weights, using a shortest-
first queue discipline leads to a time complexity in O(|T|3 log |T|). When all the
Cs?s are acyclic, using a topological order queue discipline leads to a O(|T|3) time
complexity.
As was shown in Section 3.2, when T has been obtained by converting an RTN
or a hypergraph into a PDA, the polynomial dependency in |T| becomes a linear
dependency both for the time and space complexities. Indeed, for each q in T, there
exists a unique s such that d[s, q] is non-infinity. Moreover, for each open parenthesis
transition e, there exists a unique close parenthesis transition e? such that e??B[n[e], i[e]].
When each component of the RTN is acyclic, the complexity of the algorithm is O(|T|)
in time and space.
The algorithm can be modified (without changing the complexity) to compute the
shortest path by keeping track of parent pointers. The notion of shortest path requires
the semiring (K,?,?, 0, 1) to have the path property: for all a, b in K, a? b ? {a, b}. The
shortest-distance operation as presented here and the shortest-path operation can be
applied in any semiring having the path property by using the natural order defined by
?: a ? b iff a? b = a. However, the shortest distance algorithm given in Figure 9 can be
extended to work for k-closed semirings using the same techniques that were used by
Mohri (2002).
The shortest distance in the intersection of a string s and a PDA T determines if T
recognizes s. PDA recognition is closely related to CFG parsing; a CFG can be repre-
sented as a PDT whose input recognizes the CFG and whose output identifies the parse
(Aho and Ullman 1972). Lang (1974) showed that the cubic tabular method of Earley
can be naturally applied to PDAs; others give the weighted generalizations (Stolcke
1995; Nederhof and Satta 2006). Earley?s algorithm has its analogs in the algorithm in
Figure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, the
predict step to taking an open parenthesis at lines 14?15, and the complete step to taking
the closed parentheses at lines 16?18.
Specialization to Translation. Following the formalism of Section 1, we are interested
in applying shortest distance and shortest path algorithms to automata created as
L = Tp ?M, where Tp, the translation representation, is a PDA derived from an RTN
(via replacement) and M, the language model, is a finite automaton.
For this particular case, the time complexity is O(|Tp||M|3) and the space complexity
is O(|Tp||M2|). The dependence on |Tp| is linear, rather than cubic or quadratic. The
reasoning is as follows. Given a state q in Tp, there exists a unique sq such that q belongs
to Csq. Given a state (q1, q2) in Tp?M, (q1, q2)?C(s1,s2 ) only if s1 = sq1 , and hence (q1, q2)
belongs to at most |M| components.
3.5 Expansion
As explained in Section 1.1, HiPDT can apply Step 3b to generate translation lattices.
This step is typically required for any posterior lattice rescoring strategies. We first
702
Allauzen et al. Pushdown Automata in Statistical Machine Translation
describe the unpruned expansion. However, in practice a pruning strategy of some sort
is required to avoid state explosion. Therefore, we also describe an implementation of
the PDA expansion that includes admissible pruning under a likelihood beam, thus
controlling on-the-fly the size of the output lattice.
3.5.1 Full Expansion. Given a bounded-stack PDA T, the expansion of T is the FSA T?
equivalent to T. A simple example is given in Figure 11.
Expansion starts from the PDA initial state. States and transitions are added to
the FSA as the expansion proceeds along paths through the PDA. In the new FSA,
parentheses are replaced by epsilons, and as open parentheses are encountered on
PDA transitions, they are ?pushed? into the FSA state labels; in this way the stack
depth is maintained along different paths through the PDA. Conversely, when a closing
parenthesis is encountered on a PDA path, a corresponding opening parenthesis is
?popped? from the FSA state label; if this is not possible, for example, as in state (5, ?)
in Figure 11, expansion along that path halts.
The resulting automata accept the same language. The FSA topology changes,
typically with more states and transitions than the original PDA, and the number of
added states is controlled only by the maximum stack depth of the PDA.
Formally, suppose the PDA T = (?,?,?,Q,E, I, F, ?) has a maximum stack depth
of K. The set of states in its FSA expansion T? are then
Q? = {(q, z) : q ? Q , z ? ?? and |z| ? K} (6)
and T? has initial state (I, ?) and final states F? = {(q, ?) : q ? F}. The condition that T
has a bounded stack ensures that Q? is finite. Transitions are added to T? as described in
Figure 12.
The full expansion operation can be applied to PDA over any semiring. The com-
plexity of the algorithm is linear in the size of T?. However, the size of T? can be
exponential in the size of T, which motivates the development of pruned expansion,
as discussed next.
0
1
2 3
4 5 6[ [
[
a
]
b ]
c
0,?
1, [ 2, [[ 3, [[ 4, [ 5, [ 6,??
? a ? b ?
2, [
3, [
?
a
c
4,? 5,?
?
b
Figure 11
Full expansion of a PDA to an equivalent FSA. The PDA maximum stack depth is 2; therefore
the FSA states belong to {0, .., 6} ? {?, [, [[}. Expansion can create incomplete paths in the FSA
(e.g., corresponding here to the unbalanced PDA path [ a ] b ]); however these are guaranteed to
be unconnected, namely, not to lead to a final state. Any unconnected states are removed after
expansion.
703
Computational Linguistics Volume 40, Number 3
Transition in PDA T New transition in FSA T? Conditions Explanation
q, z q?, z
a/w
a ? ? ? {?} a is not a parenthesis; stackdepth is unchanged
q q?
a/w
q, z q?, za
?
a ? ?
a is an open parenthesis; an
epsilon transition is added,
and a is ?pushed? into the
destination state, increas-
ing the stack depth
q, z?a q?, z?
?
a ? ?
a is a closing parenthe-
sis; an epsilon transition
is added, and the match-
ing open parenthesis a is
?popped? from the destina-
tion state, decreasing the
stack depth
Figure 12
PDA Expansion. A states (q, z) and (q?, z? ) in the FSA T? will be connected by a transition if and
only if the above conditions hold on the corresponding transition between q and q? in the PDA T.
3.5.2 Pruned Expansion. Given a bounded-stack PDA T, the pruned expansion of T with
threshold ? is an FST T?? obtained by deleting from T? all states and transitions that do
not belong to any accepting path ? in T? such that w[?]? ?[?] ? d+ ?, where d is the
shortest distance in T.
A naive implementation consisting of fully expanding T and then applying the
FST pruning algorithm would lead to a complexity in O(|T?| log |T?|)=O(e|T||T|).
Assuming that the reverse TR of T is also bounded-stack, an algorithm whose com-
plexity is in O(|T| |T??|+ |T|3 log |T|) can be obtained by first applying the shortest
distance algorithm from the previous section to TR and then using this to prune the
expansion as it is generated. To simplify the presentation, we assume that F={ f} and
?( f )=0.
The motivation for using reversed automaton in pruning is easily seen by looking
at FSAs. For an FSA, the cost of the shortest path through a transition (q, x,w, q?) can
be stated as d[I, q]+ w+ d[q?, f ]. Distances d[I, q] (i.e., distances from the start state) are
computed by the shortest distance algorithm, as discussed in Section 3.4. However,
distances of the form d[q?, f ] are not readily available. To compute these, a shortest
distance algorithm is run over the reversed automaton. Reversal preserves states and
transitions, but swaps the source and destination state (see Figure 13 for a PDA ex-
ample). The start state in the reversed machine is f , so that distances are computed
from f ; these are denoted dR[f, q] and correspond to d[q, f ] in the original FSA. The
cost of the shortest path through an FSA transition (q, x,w, q?) can then be computed as
d[I, q]+ w+ dR[f, q?].
Calculation for PDAs is more complex. Transitions with parentheses must be han-
dled such that distances through them are calculated over balanced paths. For example,
if T in Figure 13 was an FSA, the shortest cost of any path through the transition
e = (4, (2, 0, 5) could be calculated as d[0, 4]+ 0+ d[5, 10]. However, this is not correct,
because d[5, 10], the shortest distance from 5 to 10, is found via a path through the
transition (7, )1, 0, 8).
Correct calculation of the minimum cost of balanced paths through PDA transitions
can be done using quantities computed by the PDA shortest distance algorithm. For a
704
Allauzen et al. Pushdown Automata in Statistical Machine Translation
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
T
0
1 2
5 6 7
8
10
3 4 9t1/20
t3/200
(2
t1/10
t2/100
(1 t2/1 t3/1
)1 t4/1, 000
)2
t6/1, 000
TR
Figure 13
PDA T and its reverse TR. TR has start state 10, final state 0, ?R = {)1, )2}, and ?
R = {(1, (2}.
PDA transition e = (q, a,w, q?), a ? ?, the cost of the shortest balanced path through e
can be found as4
c(e) = d[I, q]+ w[e]+ min
e??B[q?,a]
d[q?, p[e?]]+ w[e?]+ dR[n[e?], f ] (7)
where B[q?, a] and d[p[e?], q?] are computed by the PDA shortest distance algorithm over
T, and dR[n[e?], f ] is computed by the PDA shortest distance algorithm over TR.
In Figure 13, the shortest cost of paths through the transition e = (4, (2, 0, 5) is found
as follows: the shortest distance algorithm over T calculates d[0, 4] = 220 , d[5, 7] = 2,
and B[5, (2] = {7, )2, 0, 9}; the shortest distance algorithm over TR calculates dR[10, 9] =
1, 000 (trivially, here); the cost of the shortest path through e is
d[0, 4]+ w[e]+ d[5, 7]+ w[e?]+ dR[10, 9] = 220+ 0+ 2+ 0+ 1, 000
Pruned expansion is therefore able to avoid expanding transitions that would not
contribute to any path that would survive pruning. Prior to expansion of a PDA T to an
FSA T?, the shortest distance d in T is calculated. Transitions e = (q, a,w, q?), a ? ?, are
expanded as transitions e = ((q, z), q,w, (q?, za)) in T? only if c(e) ? d+ ?, as calculated
by Equation (7).
The pruned expansion algorithm implemented in OpenFST is necessarily more
complicated than the simple description given here. Pseudo-code describing the Open-
FST implementation is given in Appendix B.
The pruned expansion operation can be applied in any semiring having the path
property.
4 Note that d[p[e?], q?] could be replaced by dR[q?, p[e?]].
705
Computational Linguistics Volume 40, Number 3
4. HiPDT Analysis and Experiments: Computational Complexity
We now address the following questions:
r What are the differences between the FSA and PDA representations as
observed in a translation/alignment task?
r How do their respective decoding algorithms perform in relation to the
complexity analysis described here?
r How many times is exact decoding achievable in each case?
We will discuss the complexity of both HiPDT and HiFST decoders as well as the
hypergraph representation, with an emphasis on Hiero-style SCFGs. We assess our
analysis for FSA and PDA representations by contrasting HiFST and HiPDT with large
grammars for translation and alignment. For convenience, we refer to the hypergraph
representation as Th, and to the FSA and PDA representations as Tf and Tp.
We first analyze the complexity of each MT step described in the introduction:
1. SCFG Translation: Assuming that the parsing of the input is performed by a
CYK parse, then the CFG, hypergraph, RTN, and PDA representations can
be generated in O(|s|3|G|) time and space (Aho and Ullman 1972). The FSA
representation can require an additional O(e|s|3|G|) time and space because
the RTN expansion to FSA can be exponential.
2. Intersection: The intersection of a CFG Th with a finite automaton M can be
performed by the classical Bar-Hillel algorithm (Bar-Hillel, Perles, and
Shamir 1964) with time and space complexity O(|Th||M|l+1), where l is the
maximum number of symbols on the right-hand side of a grammar rule in
Th. Dyer (2010a) presents a more practical intersection algorithm that avoids
creating rules that are inaccessible from the start symbol. With deterministic
M, the intersection complexity becomes O(|Th||M|lN+1), where lN is the
rank of the SCFG (i.e., lN is the maximum number of nonterminals on the
right-hand side of a grammar rule). With Hiero-styles rules, lN = 2 so the
complexity is O(|Th||M|3) in that case.5 The PDA intersection algorithm
from Section 3.3 has time and space complexity O(|Tp||M|). Finally, the FSA
intersection algorithm has time and space complexity O(|Tf ||M|) (Mohri 2009).
3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and
FSA representations requires linear time and space (given the underlying
acyclicity) (Huang 2008; Mohri 2009). As presented in Section 3.4, the PDA
representation can require time cubic and space quadratic in |M|.
Table 1 summarizes the complexity results for SCFGs of rank 2. The PDA represen-
tation is equivalent in time and superior in space complexity to the CFG/hypergraph
representation, in general, and it can be superior in both space and time to the FSA
representation depending on the relative SCFG and language model (LM) sizes. The
FSA representation favors smaller target translation grammars and larger language
models.
5 The modified Bar-Hillel construction described by Chiang (2007) has time and space complexity
O(|Th||M|4 ); the modifications were introduced presumably to benefit the subsequent pruning method
employed (but see Huang, Zhong, & Gildea 2005).
706
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 1
Translation complexity of target language representations for translation grammars of rank 2.
Representation Time Complexity Space Complexity
CFG/hypergraph O(|s|3 |G| |M|3) O(|s|3 |G| |M|3 )
PDA O(|s|3 |G| |M|3) O(|s|3 |G| |M|2 )
FSA O(e|s|3|G| |M|) O(e|s|3|G| |M|)
In practice, the PDA and FSA representations benefit greatly from the optimiza-
tions mentioned previously (Figure 3 and accompanying discussion). For the FSA
representation, these operations can offset the exponential dependencies in the worst-
case complexity analysis. For example, in a translation of a 15-word sentence taken
at random from the development sets described later, expansion of an RTN yields a
WFSA with 174? 106 states. By contrast, if the RTN is determinized and minimized
prior to expansion, the resulting WFSA has only 34? 103 states. Size reductions of this
magnitude are typical. In general, the original RTN, hypergraph, or CFG representation
can be exponentially larger than the RTN/PDT optimized as described.
Although our interest is primarily in Hiero-style translation grammars, which have
rank 2 and a relatively small number of nonterminals, this complexity analysis can be
extended to other grammars. For SCFGs of arbitrary rank lN, translation complexity in
time for hypergraphs becomes O(|G||s|lN+1|M|lN+1); with FSAs the time complexity be-
comes O(e|G||s|lN+1 |M|); and with PDAs the time complexity becomes O(|G||s|lN+1|M|3).
For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann
and Venugopal 2006) or GHKM (Galley et al. 2004), this suggests that PDA represen-
tations may offer computational advantages in the worst case relative to hypergraph
representations, although this must be balanced against other available strategies such
as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and
Langmead 2010). Of course, practical translation systems introduce various pruning
procedures to achieve much better decoding efficiency than the worst cases given here.
We will next describe the translation grammar and language model for our ex-
periments, which will be used throughout the remainder of this article (except when
stated otherwise). In the following sections we assess the complexity discussion with a
contrast between HiFST (FSA representation) and HiPDT (PDA representation) under
large grammars.
4.1 Translation Grammars and Language Models
Translation grammars are extracted from a subset of the GALE 2008 evaluation par-
allel text;6 this is 2.1M sentences and approximately 45M words per language. We
report translation results on a development set tune-nw (1,755 sentences) and a test set
test-nw (1,671 sentences). These contain translations produced by the GALE program
and portions of the newswire sections of the NIST evaluation setsMT02 throughMT06.7
6 See http://projects.ldc.upenn.edu/gale/data/catalog.html.We excluded the UN material and the
LDC2002E18, LDC2004T08, LDC2007E08, and CUDonga collections.
7 See http://www.itl.nist.gov/iad/mig/tests/mt/.
707
Computational Linguistics Volume 40, Number 3
Table 2
Number of n-grams with explicit conditional probability estimates assigned by the 4-gram
language models M?1 after entropy pruning of M1 at threshold values ?. Perplexities over the
(concatenated) tune-nw reference translations are also reported. The Kneser-Ney and Katz
4-gram LM have 416,190 unigrams, which are not removed by pruning.
? 0 7.5? 10?9 7.5? 10?8 7.5? 10?7 7.5? 10?6 7.5? 10?5 7.5? 10?4 7.5? 10?3
KN
2-grams 28M 10M 2.5M 442K 37K 1.3K 21 0
3-grams 61M 6M 969K 74K 2.7K 38 0 0
4-grams 117M 3M 219K 5K 44 0 0 0
perplexity 98.1 122.2 171.5 290.4 605.1 1270.2 1883.6 2200.0
KATZ
2-grams 28M 7M 2M 391K 52K 4K 117 1
3-grams 64M 10M 1.5M 148K 8.4K 197 1 0
4-grams 117M 4.6M 398K 19K 510 1 0 0
perplexity 106.7 120.4 146.9 210.5 336.6 596.5 905.0 1046.1
In tuning the systems, MERT (Och 2003) iterative parameter estimation under IBM
BLEU8 is performed on the development set.
The parallel corpus is aligned using MTTK (Deng and Byrne 2008) in both source-
to-target and target-to-source directions. We then follow published procedures (Chiang
2007; Iglesias et al. 2009b) to extract hierarchical phrases from the union of the
directional word alignments. We call a translation grammar (G) the set of rules
extracted from this process. For reference, the number of rules in G that can apply to the
tune-nw is 1.1M, of which 593K are standard non-hierarchical phrases and 511K are
strictly hierarchical rules.
We will use two English language models in these translation experiments. The
first language model, denoted M1, is a 4-gram estimated over 1.3B words taken
from the target side of the parallel text and the AFP and Xinhua portions of the
English Gigaword Fourth Edition (LDC2009T13). We use both Kneser-Ney (Kneser
and Ney 1995) and Katz (Katz 1987) smoothing in estimating M1. Where language
model reduction is required, we apply Stolcke entropy pruning (Stolcke 1998) to M1
under the relative perplexity threshold ?. The resulting language model is labeled
as M?1 .
The reduction in size in terms of component n-grams is summarized in Table 2.
For aggressive enough pruning, the original 4-gram model can be effectively reduced
to a trigram, bigram, or unigram model. For both the Katz and the Kneser-Ney 4-gram
language models: at ? = 7.5E? 05 the number of 4-grams in the LM is effectively
reduced to zero; at ? = 7.5E? 4 the number of 3-grams is effectively 0; and at
? = 7.5E? 3, only unigrams remain. Development set perplexities increase as entropy
pruning becomes more aggressive, with the Katz smoothed model performing better
under pruning (Chelba et al. 2010; Roark, Allauzen, and Riley 2013).
We will also use a larger language model, denoted M2, obtained by interpolat-
ing M1 with a zero-cutoff stupid-backoff 5-gram model (Brants et al. 2007) estimated
over 6.6B words of English newswire text; M2 is estimated as needed for the n-grams
required for the test sets.
8 See ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl.
708
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Table 3
Success in finding the 1-best translation under G with various M?1 under a memory size limit of
10GB as measured over tune-nw (1,755 sentences). We note which operations in translation
exceeded the memory limit: either Expansion and Intersection for HiFST, or Intersection and
Shortest Path operation for HiPDT.
Decoding with G + M?1 under a 10GB memory size limit
# ? HiFST HiPDT
Success Failure Success Failure
Expansion Intersection Intersection Shortest Path
2 7.5? 10?9 12% 51% 37% 40% 8% 52%
3 7.5? 10?8 16% 53% 31% 76% 1% 23%
4 7.5? 10?7 18% 53% 29% 99.8% 0% 0.2%
4.2 Exact Decoding with Large Grammars and Small LanguageModels
We now compare HiFST and HiPDT in translation with our large grammar G. In this
case we know that exact search is often not feasible for HiFST.
We run both decoders over tune-nw with a restriction on memory use of 10 GB.
If this limit is reached in decoding, the process is killed.9 Table 3 shows the number
of times each decoder succeeds in finding a hypothesis under the memory limit when
decoding with various entropy-pruned LMs M?1 . With ?=7.5? 10?9 (row 2), HiFST
can only decode 218 sentences, and HiPDT succeeds in 703 cases. The difference in
success rates between the decoders is more pronounced as the language model is more
aggressively pruned: for ?=7.5? 10?7 HiPDT succeeds for all but three sentences.
As Table 3 shows, HiFST fails most frequently in its initial expansion from RTN
to FSA; this operation depends only on the translation grammar and does not benefit
from any reduction in the language model size. Subsequent intersection of the FSA
with the language model can still pose a challenge, although as the language model
is reduced, this intersection fails less often. By contrast, HiPDT intersects the translation
grammar with the language model prior to expansion and this operation nearly always
finishes successfully. The subsequent shortest path (or pruned expansion) operation is
prone to failure, but the risk of this can be greatly reduced by using smaller language
models.
In the next section we contrast both HiPDT and HiFST for alignment.
4.3 Alignment with Inversion Transduction Grammars
We continue to explore applications characterized by large translation grammars G
and small language models M. As an extreme instance of a problem involving a large
translation grammar and a simple target language model, we consider parallel text
alignment under an Inversion Transduction Grammar (ITG) (Wu 1997). This task, or
something like it, is often done in translation grammar induction. The process should
yield the set of derivations, with scores, that generate the target sentence as a translation
9 We use the UNIX ulimit command. The experiment was carried out over machines with different
configurations and loads, so these numbers should be considered as approximate values.
709
Computational Linguistics Volume 40, Number 3
of the source sentence. In alignment the target language model is extremely simple:
It is simply an acceptor for the target language sentence so that |M| is linear in the
length of the target sentence. In contrast, the search space needs now to be represented
with pushdown transducers (instead of pushdown automata) keeping track of both
translations and derivations, that is, indices of the rules in the grammar (Iglesias et al.
2009a; de Gispert et al. 2010; Dyer 2010b).
We define a word-based translation grammar GITG for the alignment problem as
follows. First, we obtain word-to-word translation rules of the form X??s, t? based
on probabilities from IBM Model 1 translation tables estimated over the parallel text,
where s and t are one source and one target word, respectively (?16M rules). Then,
we allow monotonic and inversion transduction of two adjacent nonterminals in the
usual ITG style (i.e., add X??X1 X2, X1 X2? and X??X1 X2, X2 X1?). Additionally,
we allow unrestricted source word deletions (X??s, ??), and restricted target word
insertions (X??X1 X2, X1 t X2?). This restriction, which is solely motivated by ef-
ficiency reasons, disallows the insertion of two consecutive target words. We make
no claims about the suitability or appropriateness of this specific grammar for either
alignment or translation; we introduce this grammar only to define a challenging
alignment task.
A set of 2,500 sentence pairs of up to 50 source and 75 target words was chosen
for alignment. These sentences come from the same Chinese-to-English parallel data
described in Section 4.1. Hard limits on memory usage (10GB) and processing time
(10 minutes) were imposed for processing each sentence pair. If HiPDT or HiFST ex-
ceeded either limit in aligning any sentence pair, alignment was stopped and a ?mem-
ory/time failure? was noted. Even if the resource limits are not exceeded, alignment
may fail due to limitations in the grammar. This happens when either a particular word
pair rule that is not in our Model 1 table, or more than one consecutive target insertions
are needed to reach alignment. In such cases, we record a ?grammar failure,? as opposed
to a ?memory/time failure.?
Results are reported in Table 4. Of the 2,500 sentence pairs, HiFST successfully
aligns only 41% of the sentence pairs under these time and memory constraints. The
reason for this low success rate is that HiFST must generate and expand all possible
derivations under the ITG for a given sentence pair. Even if it is strictly enforced
that the FSA in every CYK cell contains only partial derivations which produce sub-
strings of the target sentence, expansion often exceeds the memory/time constraints.
In contrast, HiPDT succeeds in aligning all sentence pairs that can be aligned under
the grammar (89%), because it never fails due to memory or time constraints. In this
experiment, if alignment is at all possible, HiPDT will find the best derivation. Align-
ment success rate (or coverage) could trivially be improved by modifying the ITG to
allow more consecutive target insertions, or by increasing the number of word-to-word
Table 4
Percentages of success and failure in aligning 2,500 sentence pairs under GITG with HiFST and
HiPDT. HiPDT finds an alignment whenever it is possible under the translation grammar.
HiFST HiPDT
Success Failure Success Failure
memory/time grammar memory/time grammar
41% 53% 6% 89% 0% 11%
710
Allauzen et al. Pushdown Automata in Statistical Machine Translation
rules, but that would not change the conclusion in the contrast between HiFST and
HiPDT.
The computational analysis from the beginning of this section applies to alignment.
The language model M is replaced by an acceptor for the target sentence, and if we
assume that the target sentence length is proportional to the source sentence length, it
follows that |M| ? |s| and the worst-case complexity for HiPDT in alignment mode is
O(|s|6|G|). This is comparable to ITG alignment (Wu 1997) and the intersection algo-
rithm of Dyer (2010b).
Our experimental results support the complexity analysis summarized in Table 1.
HiPDT is more efficient in ITG alignment and this is consistent with its linear depen-
dence on the grammar size, whereas HiFST suffers from its exponential dependence.
This use of PDAs in alignment does not rely on properties specific either to Hiero
or to ITGs. We expect that the approach should be applicable with other types of
SCFGs, although we note that alignment under SCFGs with an arbitrary number of
nonterminals can be NP-hard (Satta and Peserico 2005).
5. HiPDT Two-Pass Translation Architecture and Experiments
The previous complexity analysis suggests that PDAs should excel when used with
large translation grammars and relatively small n-gram language models. In hierar-
chical phrase-based translation, this is a somewhat unusual scenario: It is far more
typical that translation tasks requiring a large translation grammar also require large
language models. To accommodate these requirements we have developed a two-
pass decoding strategy in which a weak version of a large language model is ap-
plied prior to the expansion of the PDA, after which the full language model is
applied to the resulting WFSA in a rescoring pass. An effective way of generating
weak language models is by means of entropy pruning under a threshold ?; these are
the language models M?1 of Section 4.1. Such a two-pass strategy is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999). The steps in two-pass
translation using entropy-pruned language models are given here, and depicted in
Figure 14.
Step 1. We translate with M?1 and G using the same parameters obtained by MERT
for the baseline system, with the exception that the word penalty parameter
is adjusted to produce hypotheses of roughly the correct length. This produces
translation lattices that contain hypotheses with exact scores under G and M?1 :
?2({s} ? G) ?M?1 .
Step 2. These translation lattices are pruned at beamwidth ?: [?2({s} ? G) ?M?1 ]?.
Step 3. We remove the M?1 scores from the pruned translation lattices, reapply the full
language model M1, and restore the word penalty parameter to the baseline
value obtained by MERT. This gives an approximation to ?2({s} ? G) ?M1:
scores are correctly assigned underG andM1, but only hypotheses that survived
pruning at Step 2 are included.
We can rescore the lattices produced by the baseline system or by the two-pass
system with the larger language model M2. If ?=? or if ?=0, the translation lattices
obtained in Step 3 should be identical to lattices produced by the baseline system (i.e.,
the rescoring step is no longer needed). The aim is to increase ? to shrink the language
model used at Step 1, but ? will then have to increase accordingly to avoid pruning
away desirable hypotheses in Step 2.
711
Computational Linguistics Volume 40, Number 3
CYK parse 
s with G Build RTN
RTN to PDA 
Replacement
Intersect PDA 
with WFSA M1
?
PDA to FSA 
Pruned Expansion,
threshold B
Intersect FSA with LM M1
FSA 
Shortest 
Path
FSA 
Pruning
Lattice1-Best Hypothesis
Remove 
LM scores 
Entropy Pruning, 
threshold ?
LM M1
(as WFSA) 
further rescoring
Figure 14
Two-pass HiPDT translation with an entropy pruned language model.
5.1 Efficient Removal of First-Pass Language Model Scores Using
Lexicographic Semirings
The two-pass translation procedure requires removal of the weak language model
scores used in the initial expansion of the translation search space; this is done so
that only the translation scores under G remain after pruning. In the tropical semiring,
the weak LM scores can be ?subtracted? at the path level from the lattice, but this
involves a determinization of an unweighted translation lattice, which can be very
inefficient.
As an alternative we can define a lexicographic semiring (Shafran et al. 2011;
Roark, Sproat, and Shafran 2011) ?w1,w2? over the tropical weights w1 and w2 with the
operations ? and ?:
?w1,w2? ? ?w3,w4? =
{
?w1,w2? if w1 < w3 or (w1 = w3 and w2 < w4)
?w3,w4? otherwise (8)
?w1,w2? ? ?w3,w4? = ?w1 + w3,w2 + w4? (9)
The PDA algorithms described in Section 3 are valid under this new semiring because
it is commutative and has the path property. In particular, the PDA representing {s} ? G
is constructed so that the translation grammar score appears in both w1 and w2 (i.e., it is
duplicated). In the first-pass language model, w1 has the n-gram language model scores
and the w2 are 0. After composition, the resulting automata have the combined trans-
lation grammar score and language model score in the first dimension, and the second
dimension contains the translation grammar scores alone. Pruning can be performed
under the lexicographic semiring with a threshold set so that only the combined scores
in the first dimension are considered. The resulting automata can easily be mapped back
into the regular tropical semiring such that only the translation scores in the second
712
Allauzen et al. Pushdown Automata in Statistical Machine Translation
dimension are retained (this is a linear operation done by the fstmap operation in the
OpenFST library).
5.2 Translation Quality and Modeling Errors in Two-Pass Decoding
We wish to analyze the degree to which the two-pass decoding strategy introduces
?modeling errors? into translation. A modeling error occurs in two-pass decoding
whenever the decoder produces a translation whose score is less than the best attainable
under the grammar and language model (i.e., whenever the best possible translation
is discarded by pruning at Step 2). We refer to these as modeling errors, rather than
search errors, because they are due to differences in scores assigned by the models
M1 and M?1 .
Ideally, we would compare the two-pass translation system against a baseline sys-
tem that performs exact translation, without pruning in search, under the grammar G
and language model M1. This would allow us to address the following questions:
r Is a two-pass decoding procedure that uses entropy-pruned language
models adequate for translation? How many modeling errors are
introduced? Does two-pass decoding impact on translation quality?
r Which smoothing/discounting technique is best suited for the first-pass
language model in two-pass translation, and which smoothing/
discounting technique is best at avoiding modeling errors?
Our grammar G is not suitable for these experiments, as we do not have a system
capable of exact decoding under both G and M1. To create a suitable baseline we there-
fore reduce G by excluding rules that have a forward translation probability p < 0.01,
and refer to this reduced grammar as Gsmall. This process reduces the number of strictly
hierarchical rules that apply to our tune-nw set from 511K to 189K, while the number of
standard phrases is unchanged.
Under Gsmall, both HiFST and HiPDT are able to exactly compose the entire space of
possible candidate hypotheses with the language model and to extract the shortest path
hypothesis. Because an exact decoding baseline is thus available, we can empirically
evaluate the proposed two-pass strategy. Any degradation in translation quality can
only be due to the modeling errors introduced by pruning under ? with respect to the
entropy-pruned M?1 .
Figure 15 shows translation performance under grammar Gsmall for different values
of entropy pruning threshold ?. Performance is reported after first-pass decoding with
M?1 (Step 1, Section 5), and after rescoring with M1 (Step 3, Section 5) the first-pass
lattices pruned at alternative ? beams. The first column reports the baseline for either
Kneser-Ney andKatz languagemodels, which are found by translation without entropy
pruning, that is, performed with M1. Both yield 34.5 on test-nw.
The first and main conclusion from this figure is that the two-pass strategy is ade-
quate because we are always able to recover the baseline performance. As expected, the
harsher the entropy-pruning ofM1 (as we lower ?) the greater?must be to recover from
the significant degradation in first-pass decoding. But even at a harsh ? = 7.5? 10?7,
when first-pass performance drops over 7 BLEU points, a relatively-low value of ? = 15
can recover the baseline performance.
Although this is true independently of the LM smoothing approach, a second
conclusion from the figure is that the choice of LM smoothing does impact first-pass
713
Computational Linguistics Volume 40, Number 3
Figure 15
Results (lower case IBM BLEU scores over test-nw) under Gsmall with various M?1 as obtained
with several values of ?. Performance in subsequent rescoring with M1 after likelihood-based
pruning of the resulting translation lattices for various ? is also reported. In the pipeline, M1
(and M?1 ) are estimated with either Katz or Kneser-Ney smoothing.
translation performance. For entropy pruning at ? = 7.5? 10?7, the Katz LMs perform
better for smaller beamwidths ?. These results are consistent with the test set
perplexities of the entropy pruned LMs (Table 2), and are also in line with other studies
of Kneser-Ney smoothing and entropy pruning (Chelba et al. 2010; Roark, Allauzen,
and Riley 2013).
Modeling errors are reported in Table 5 at the entropy pruning threshold ? = 7.5?
10?7. As expected, modeling errors decrease as the beamwidth ? increases, although
we find that the language model with Katz smoothing has fewer modeling errors.
However, modeling errors do not necessarily impact corpus level BLEU scores. For wide
beamwidths (e.g., ? = 15 here), there are still some modeling errors, but these are either
few enough or subtle enough that two-pass decoding under either smoothing method
yields the same corpus level BLEU score as the exact decoding baseline.
Table 5
Two-pass translation modeling errors as a function of RTN expansion pruning threshold ?. A
modeling error occurs whenever the score of a hypothesis produced by the two-pass translation
differs from the score found by the exact baseline system. Errors are tabulated over systems
reported in Figure 15, at ? = 7.5? 10?7.
? Kneser-Ney Katz
8 814 619
12 343 212
15 240 110
714
Allauzen et al. Pushdown Automata in Statistical Machine Translation
5.3 HIPDT Two-Pass Decoding Speed and Translation Performance
r What are the speed and quality tradeoffs for HiPDT as a function of
first-pass LM size and translation grammar complexity?
r How do these compare against the predicted computational complexity?
In this section we turn back to the original large grammar, for which HiFST cannot
perform exact decoding (see Table 3). In contrast, HiPDT is able to do exact decoding
so we study tradeoffs in speed and translation performance. The speed of two-pass
decoding can be increased by decreasing ? and/or increasing ?, but at the risk of
degradation in translation performance. For grammar G and language model M1 we
plot in Figure 16 the BLEU score against speed as a function of ? for a selection of ?
values. BLEU score is measured over the entire test set test-nw but speed is calculated
only on sentences of length up to 20 words (?500 sentences). In computing speed we
measure not only the PDA operations, but the entire HiPDT decoding process described
in Figure 14, including CYK parsing and the application of M1. We note in passing that
these unusually slow decoding speeds are a consequence of the large grammars, lan-
guage models, and broad pruning thresholds chosen for these experiments; in practice,
translation with either HiPDT or HiFST is much faster.
In these experiments we find that the language model entropy pruning threshold
? and the likelihood beamwidth ? work together to balance speed against translation
quality. For every entropy pruning threshold ? value considered, there is a value of ?
for which there is no degradation in translation quality. For example, suppose we want
to attain a translation quality of 34.5 BLEU: then ? should be set to 12 or greater. If the
goal is to find the fastest system at this level, then we choose ? = 7.5? 10?5.
The interaction between pruning in expansion and pruning of the language model
is explained by Figure 17, where decoding and rescoring times are shown for various
Figure 16
HiPDT translation quality versus speed (decoding with G, M?1 + rescoring with M1) under
different entropy pruning thresholds ? and for likelihood beamwidths ? = 15, 12, 9, 8, 7.
715
Computational Linguistics Volume 40, Number 3
Figure 17
Accumulated decoding+rescoring times for HiPDT under different entropy pruning thresholds,
reaching a performance of at least 34.5 BLEU, for which ? is set to 12.
values of ? and ? that achieve at least the translation quality target of 34.5. As ?
increases, decoding time decreases because a smaller language model is easier to apply;
however, rescoring times increase, because the larger values of ? lead to larger WFSAs
after expansion, and these are costly to rescore. The balance occurs at ? = 7.5? 10?5
and a translation rate of 3.0 words/sec. In this case, entropy pruning yields a severely
shrunken bigram language model, but this may vary depending on the translation
grammar and the original, unpruned LM.
5.4 Rescoring with 5-Gram Language Models and LMBR Decoding
r Does the HiPDT two-pass decoding generate lattices that can be useful in
rescoring?
We now report on rescoring experiments using WFSAs produced by the two-pass
HiPDT translation system under the large translation grammar G. We demonstrate that
HiPDT can be used to generate large, compact representations of the translation space
that are suitable for rescoring with large language models or by alternative decoding
procedures. We investigate translation performance by applying versions of the lan-
guage model M2 estimated with stupid backoff. We also investigate minimum Bayes
risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are
particularly interested in lattice MBR (LMBR) (Tromble et al. 2008), which is well suited
for the large WFSAs that the system can generate; we use the implementation described
by Blackwood, de Gispert, & Byrne (2010). There are two parameters to be tuned: a
scaling parameter to normalize the evidence scores and a word penalty applied to the
hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in
Figure 18.
We note first that rescoring with the large language model M2, which is effectively
interpolated with M1, gives consistent gains over initial results obtained with M1 alone.
After 5-gram rescoring there is already +0.5 BLEU improvement compared with Gsmall.
With a richer translation grammar we have generated a richer lattice that allows gains
to be gotten by our lattice rescoring techniques.
716
Allauzen et al. Pushdown Automata in Statistical Machine Translation
Figure 18
HiPDT decoding with G. Decoding language model M?1 and first pass rescoring language model
M1 are Katz. Results on test-nw are given for ML-Decoding under the 5-gram stupid backoff
language model (?5gML?) and for LMBR and for LMBR decoding. Parameter values are
? = 15, 12, 9, 8 and ? = 7.5? 10?7 , 7.5? 10?5, 7.5? 10?3.
We also find that BLEU scores degrade smoothly as ? decreases and the expansion
pruning beamwidth narrows, and at all values of ? LMBR gives improvement over
the MAP hypotheses. Because LMBR relies on posterior distributions over n-grams, we
conclude that HiPDT is able to generate compact representations of large search spaces
with posteriors that are robust to pruning conditions.
Finally, we find that increasing ? degrades performance quite smoothly for ? ? 9.
Again, with appropriate choices of ? and ? we can easily reach a compromise between
decoding speed and final performance of our HiPDT system. For instance, with ? =
7.5? 10?7 and? = 12, for whichwe decode at a rate of 3words/sec as seen in Figure 16,
we are losing only 0.5 BLEU after LMBR compared to ? = 7.5? 10?7 and ? = 15.
6. RelatedWork
There is extensive prior work on computational efficiency and algorithmic complexity
in hierarchical phrase-based translation. The challenge is to find algorithms that can be
made to work with large translation grammars and large language models.
Following the original algorithms and analysis of Chiang (2007), Huang and
Chiang (2007) developed the cube-growing algorithm, and more recently Huang and
Mi (2010) developed an incremental decoding approach that exploits the left-to-right
nature of n-gram language models.
Search errors in hierarchical translation, and in translation more generally, have
not been as extensively studied; this is undoubtedly due to the difficulties inherent in
finding exact translations for use in comparison. Using a relatively simple phrase-based
translation grammar, Iglesias et al. (2009b) compared search via cube-pruning to an
exact FST implementation (Kumar, Deng, and Byrne 2006) and found that cube-pruning
suffered significant search errors. For Hiero translation, an extensive comparison of
search errors between the cube pruning and FSA implementation was presented by
Iglesias et al. (2009a) and de Gispert et al. (2010). The effect of search errors has also been
717
Computational Linguistics Volume 40, Number 3
studied in phrase-based translation by Zens andNey (2008). Relaxation techniques have
also recently been shown to find exact solutions in parsing (Koo et al. 2010), phrase-
based SMT (Chang and Collins 2011), and in tree-to-string translation under trigram
language models (Rush and Collins 2011); this prior work involved much smaller
grammars and languages models than have been considered here.
Efficiency in synchronous parsing with Hiero grammars and hypergraphs has been
studied previously by Dyer (2010b), who showed that a single synchronous parsing al-
gorithm (Wu 1997) can be significantly improved upon in practice through hypergraph
compositions. We developed similar procedures for our HiFST decoder (Iglesias et al.
2009a; de Gispert et al. 2010) via a different route, after noting that with the space of
translations represented as WFSAs, alignment can be performed using operations over
WFSTs (Kumar and Byrne 2005).
Although entropy-pruned language models have been used to produce real-time
translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language
models in two-pass translation to be novel. This is an approach that is widely used in
automatic speech recognition (Ljolje, Pereira, and Riley 1999) and we note that it relies
on efficient representation of very large search spaces T for subsequent rescoring, as is
possible with FSAs and PDAs.
7. Conclusion
In this article, we have described a novel approach to hierarchical machine translation
using pushdown automata. We have presented fundamental PDA algorithms including
composition, shortest-path, (pruned) expansion, and replacement and have shown how
these can be used in PDA-based machine translation decoding and how this relates to
and compares with hypergraph and FSA-based decoding.
On the basis of the experimental results presented in the previous sections, we can
now address the questions laid out in Sections 4 and 5:
r A two-pass translation decoding procedure in which translation is first
performed with a weak entropy-pruned language model and followed by
admissible likelihood-based pruning and rescoring with a full language
model can yield good quality translations. Translation performance does
not degrade significantly unless the first-pass language model is very
heavily pruned.
r As predicted by the analysis of algorithmic complexity, intersection and
expansion algorithms based on the PDA representation are able to
perform exact decoding with large translation and weak language models.
By contrast, RTN to FSA expansion fails with large translation grammars,
regardless of the size of the language model. With large translation
grammars, language model composition prior to expansion may be more
attractive than expansion prior to language model composition.
r Our experimental results suggest that for a translation grammar and a
language model of a particular size, and given a value of language model
entropy pruning threshold ?, there is a value of the pruned expansion
parameter ? for which there is no degradation in translation quality with
HiPDT. This makes exact decoding under large translation grammars
possible. The values of ? and ? will be grammar- and task-dependent.
718
Allauzen et al. Pushdown Automata in Statistical Machine Translation
r Although there is some interaction between parameter tuning, pruning
thresholds, and language modeling strategies, the variation is not
significant enough to indicate that a particular language model or
smoothing technique is best. This is particularly true if minimum Bayes
risk decoding is applied to the output translation lattices.
Several questions naturally arise about the decoding strategies presented here. One
is whether inadmissible pruning methods can be applied to the PDA-based systems that
are analogous to those used in current hypergraph-based systems such as cube-pruning
(Chiang 2007). Another is whether a hybrid PDA?FSA system, where some parts of the
PDA are pre-expanded and some not, could provide benefits over full pre-expansion
(FSA) or none (PDA). We leave these questions for future work.
Appendix A. Composition of a Weighted PDT and a Weighted FST
Given a pair (T1,T2) where T1 is a weighted pushdown transducer and the T2 is a
weighted finite-state transducer, and such that T1 has input and output alphabets ?
and ? and T2 has input and output alphabets ? and ?, then there exists a weighted
pushdown transducer T1 ? T2, which is the composition of T1 and T2, such that for all
(x, y) ? ?? ? ??:
T = (T1 ? T2)(x, y) = minz???(T1(x, z)+ T2(z, y)) (A.10)
We also assume that T2 has no input-? transitions, noting that for T2 with input-? tran-
sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized
to handle parentheses could be used.
A state in T is a pair (q1, q2) where q1 is a state of T1 and q2 a state of T2. Given a
transition e1 = (q1, a, b,w1, q?1) in T1, transitions out of (q1, q2) in T are obtained using the
following rules. If b ? ?, then e1 can be matched with a transition (q2, b, c,w2, q?2) in T2
resulting in a transition ((q1, q2), a, c,w1 + w2, (q?1, q?2)) in T. If b = ?, then e1 is matched
with staying in q2 resulting in a transition ((q1, q2), a, ?,w1, (q?1, q2)). Finally, if b = a ? ??,
e1 is also matched with staying in q2, resulting in a transition ((q1, q2), a, a,w1, (q?1, q2)) in
T. The initial state is (I1, I2) and a state (q1, q2) in T is final when both q1 and q2 are both
final. Weight values are assigned as ?((q1, q2)) = ?1(q1)+ ?2(q2).
Appendix B. Pruned Expansion
Let dR and BR be the data structures computed by the shortest-distance algorithm
applied to TR. For a state q in T? (or equivalently T??), let d[q] denote the shortest distance
from the initial state to q, d[q] denote the shortest distance from q to the final state, and
s[q] denote the destination state of the last unbalanced open-parenthesis transition on a
shortest path from the initial state to q.
The algorithm is based on the following property: Letting e denote a transition in
T? such that p[e] = (q, z) and z = z?a, the weight of a shortest path through e can be
expressed as:
d[(q, z)]+ w[e]+ min
e??BR[qs ,a]
dR[n[e], p[e?]]+ w[e?]+ d[(n[e?], z?)] (B.11)
719
Computational Linguistics Volume 40, Number 3
PRUNEDEXPANSION(T, ?)
1 (dR,BR)? SHORTESTDISTANCE (TR )
2 ?? dR[I, f ]+? ? Compute the pruning threshold
3 B? REVERSE (BR ) ? Compute the balance information in T from the one in TR
4 (I?, f ? )? ((I,?), ( f,?)) ? I? and f ? are the initial and final states of the pruned expansion
5 (F?,??( f ?))? ({ f ?}, 0)
6 (d[I?], s[I?])? (0, I? )
7 (d[I?], d[ f ?])? (dR[I, f ], 0)
8 (zD,D[ f ])? (?, 0)
9 S? Q?? {I?}
10 while S 6=? do
11 (q, z)? HEAD(S)
12 DEQUEUE (S)
13 if s[(q, z)]= (q, z) then
14 if z 6= zD then ? If the stack has changed, D needs to be cleared and recomputed
15 CLEAR (D)
16 zD? z
17 for each e ? B[q, z|z|] do ? For each close paren. transition balancing the incoming z|z|-labeled open paren. transition in q
18 D[p[e]]? min(D[p[e]],w[e]+ d[(n[e], z1 ? ? ? z|z|?1 )])
19 for each e ? E[q] do
20 if i[e] ? ?? {?} then ? If i[e] is a regular symbol
21 if RETAINPATH (q, z,w[e],n[e]) then
22 E?? E? ? {((q, z), i[e], o[e],w[e], (n[e], z))}
23 elseif i[e] ?? then ? If i[e] is an open parenthesis
24 z?? zi[e]
25 r? false
26 for each e? ? B[n[e], i[e]] do ? For each close paren. transition e? that balances e
27 w? w[e]+ dR[n[e],p[e?]]+w[e?] ? w: weight of the shortest bal. path beginning by e and ending by e? in T
28 r? r? RETAINPATH (q, z,w,n[e?]) ? Does the expansion of that path belong to an accepting path below threshold?
29 wF?min(wF, dR[n[e], p[e?]]+w[e?]+ d[(n[e?], z)])
30 if r then ? If any of the paths considered above are below threshold
31 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
32 PROCESSSTATE ((n[e], z?))
33 s[(n[e], z?)]? (n[e], z? )
34 d[(n[e], z? )]?min(d[(n[e], z? )], d[(q, z)]+w[e])
35 d[(n[e], z? )]?min(d[(n[e], z? )],wF )
36 elseif i[e] ?? and c?(zi[e]) ? ?? then ? If i[e] is the close parenthesis matching the top of the stack
37 z?? c?(zi[e])
38 if d[(q, z)]+w[e]+ d[(n[e], z? )] ? ? then
39 E?? E? ? {((q, z),?,?,w[e], (n[e], z? ))}
40 return (?,?,?,?,Q?,E?, I?,F?,?? )
RETAINPATH(q, z,w, q? )
1 ? Returns true iff a path from (q, z) to (q?, z) with weight w belongs to an accepting path below threshold
2 wI? d[(q, z)]+w ? Shortest distance from I to (q?, z) when taking a path from (q, z) to (q?, z) of weight w
3 wF? min{dR[q?, t]+D[t]|D[t] 6=?}? Current estimate of s. d. from (q?, z) to f ?
4 if wI < d[(q? , z)] then ? If wI is a better estimate of s.-d. from I? to (q?, z), update d[(q?, z)] and s[(q?, z)]
5 d[(q? , z)]? wI
6 s[(q?, z)]? s[(q, z)]
7 if wF < d[(q?, z)] then ? If wF is a better estimate of s. d. from (q?, z) to f ? , update d[(q?, z)]
8 d[(q?, z)]? wF
9 if ? < wI +wF then ? wI +wF: min. weight of an accepting path taking a path of weight w from (q, z) to (q?, z)
10 return false
11 PROCESSSTATE ((q? , z))
12 return true
PROCESSSTATE((q, z))
1 if (q, z) 6? Q? then ? If state (q, z) does not exist yet, create it and add it to the queue
2 Q?? Q? ?{(q, z)}
3 ENQUEUE (S, (q, z))
Figure 19
PDT pruned expansion algorithm. We assume that F={ f} and ?( f )=0 to simplify the
presentation.
720
Allauzen et al. Pushdown Automata in Statistical Machine Translation
where (qs, z) = s[(q, z)]. This implies that assuming when (q, z) is visited, d[(n[e?], z?)] is
known; we then have all the required information for deciding whether e should be
pruned or retained. In order to ensure that each state is visited once, we need to ensure
that d[(q, z)] is known when (q, z) is visited so we can apply an A? queue discipline
among the states sharing the same stack.
Both conditions can be achieved by using a queue discipline defined by a partial
order? such that
z is a prefix of z? ? (q, z) ? (q?, z?) (B.12)
d[(q, z)]+ d[(q, z)] < d[(q?, z)]+ d[(q?, z)]? (q, z) ? (q?, z) (B.13)
We also assume that all states sharing the same stack will be dequeued consecutively
(z 6= z? ? for all (q, q?), (q, z) ? (q?, z?) or for all (q, q?), (q?, z?) ? (q, z)). This allows us to
cache some computations (the D data structure as described subsequently).
The pseudo code of the algorithm is given in Figure 19. First, the shortest distance
algorithm is applied to TR and the absolute pruning threshold is computed accordingly
(lines 1?2). The resulting balanced data information is then reversed (line 3). The initial
and final states are created (lines 4?5) and the d, d, and D data structures are initialized
accordingly (lines 6?8). The default value in these data structures is assumed to be?.
The queue is initialized containing the initial state (line 9).
The state (q, z) at the head of the queue is dequeued (lines 10?12). If (q, z) admits an
incoming open-parenthesis transition, B contains the balance information for that state
and D can be updated accordingly (lines 13?18).
If e is a regular transition, the resulting transition ((q, z), i[e], o[e],w[e], (n[e], z)) in T?
can be pruned using the criterion derived from Equation (B.11). If it is retained, the
transition is created as well as its destination state (n[e], z) if needed (lines 20?22).
If e is an open-parenthesis transition, each balanced path starting by the resulting
transition in T? and ending by a close-parenthesis transition is treated as a meta-
transition and pruned using the same criterion as regular transitions (lines 23?29). If any
of these meta-transitions is retained, the transition ((q, z), ?, ?,w[e], (n[e], zi[e])) resulting
from e is created as well as its destination state (n[e], zi[e]) if needed (lines 30?35).
If e is a closed-parenthesis transition, it is created if it belongs to a balanced path
below the threshold (lines 36?39).
Finally, the resulting transducer T?? is returned (line 40).
Acknowledgments
The research leading to these results has
received funding from the European Union
Seventh Framework Programme
(FP7-ICT-2009-4) under grant agreement
number 247762, and was supported in part
by the GALE program of the Defense
Advanced Research Projects Agency,
contract no. HR0011-06-C-0022, and a
May 2010 Google Faculty Research Award.
References
Aho, Alfred V. and Jeffrey D. Ullman. 1972.
The Theory of Parsing, Translation and
Compiling, volume 1-2. Prentice-Hall.
Allauzen, Cyril and Michael Riley, 2011.
Pushdown Transducers. http://pdt.
openfst.org.
Allauzen, Cyril, Michael Riley, and Johan
Schalkwyk. 2011. Filters for efficient
composition of weighted finite-state
transducers. In Proceedings of CIAA,
volume 6482 of LNCS, pages 28?38. Blois.
Allauzen, Cyril, Michael Riley, Johan
Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. OpenFst: A general and
efficient weighted finite-state transducer
library. In Proceedings of CIAA,
pages 11?23. http://www.openfst.org.
Bar-Hillel, Y., M. Perles, and E. Shamir. 1964.
On formal properties of simple phrase
721
Computational Linguistics Volume 40, Number 3
structure grammars. In Y. Bar-Hillel,
editor, Language and Information: Selected
Essays on their Theory and Application.
Addison-Wesley, pages 116?150.
Berstel, Jean. 1979. Transductions and
Context-Free Languages. Teubner.
Blackwood, Graeme, Adria` de Gispert,
and William Byrne. 2010. Efficient path
counting transducers for minimum
Bayes-risk decoding of statistical machine
translation lattices. In Proceedings of the
ACL: Short Papers, pages 27?32, Uppsala.
Brants, Thorsten, Ashok C. Popat, Peng Xu,
Franz J. Och, and Jeffrey Dean. 2007.
Large language models in machine
translation. In Proceedings of EMNLP-ACL,
pages 858?867, Prague.
Chang, Yin-Wen and Michael Collins. 2011.
Exact decoding of phrase-based translation
models through lagrangian relaxation.
In Proceedings of EMNLP, pages 26?37,
Edinburgh.
Chelba, Ciprian, Thorsten Brants, Will
Neveitt, and Peng Xu. 2010. Study
on interaction between entropy
pruning and Kneser-Ney smoothing.
In Proceedings of Interspeech,
pages 2,242?2,245, Makuhari.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
de Gispert, Adria`, Gonzalo Iglesias, Graeme
Blackwood, Eduardo R. Banga, and
William Byrne. 2010. Hierarchical
phrase-based translation with weighted
finite state transducers and shallow-n
grammars. Computational Linguistics,
36(3):201?228.
Deng, Yonggang and William Byrne. 2008.
HMM word and phrase alignment for
statistical machine translation. IEEE
Transactions on Audio, Speech, and Language
Processing, 16(3):494?507.
Dyer, Chris. 2010a. A Formal Model of
Ambiguity and its Applications in Machine
Translation. Ph.D. thesis, University of
Maryland.
Dyer, Chris. 2010b. Two monolingual parses
are better than one (synchronous parse). In
Proceedings of NAACL-HLT, pages 263?266,
Los Angeles, CA.
Galley, M., M. Hopkins, K. Knight, and
D. Marcu. 2004. What?s in a translation
rule. In Proceedings of HLT-NAACL,
pages 273?280, Boston, MA.
Hopkins, M. and G. Langmead. 2010. SCFG
decoding without binarization. In
Proceedings of EMNLP, pages 646?655,
Cambridge, MA.
Huang, Liang. 2008. Advanced dynamic
programming in semiring and hypergraph
frameworks. In Proceedings of COLING,
pages 1?18, Manchester.
Huang, Liang and David Chiang. 2007.
Forest rescoring: Faster decoding with
integrated language models. In Proceedings
of ACL, pages 144?151, Prague.
Huang, Liang and Haitao Mi. 2010. Efficient
incremental decoding for tree-to-string
translation. In Proceedings of EMNLP,
pages 273?283, Cambridge, MA.
Huang, Liang, Hao Zhang, and Daniel
Gildea. 2005. Machine translation as
lexicalized parsing with hooks. In
Proceedings of the Ninth International
Workshop on Parsing Technology,
Parsing ?05, pages 65?73, Vancouver.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009a. Hierarchical phrase-based
translation with weighted finite state
transducers. In Proceedings of NAACL-HLT,
pages 433?441, Boulder, CO.
Iglesias, Gonzalo, Adria` de Gispert,
Eduardo R. Banga, and William Byrne.
2009b. Rule filtering by pattern for efficient
hierarchical translation. In Proceedings of
EACL, pages 380?388, Athens.
Katz, Slava M. 1987. Estimation of
probabilities from sparse data for the
language model component of a speech
recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3):400?401.
Kneser, Reinhard and Herman Ney. 1995.
Improved backing-off for m-gram
language modeling. In Proceedings of
ICASSP, volume 1, pages 181?184,
Detroit, MI.
Koo, Terry, Alexander M. Rush, Michael
Collins, Tommi Jaakkola, and David
Sontag. 2010. Dual decomposition for
parsing with non-projective head
automata. In Proceedings of EMNLP,
pages 1,288?1,298, Cambridge, MA.
Kuich, Werner and Arto Salomaa. 1986.
Semirings, automata, languages. Springer.
Kumar, Shankar and William Byrne.
2004. Minimum Bayes-risk decoding
for statistical machine translation. In
Proceedings of HLT-NAACL, pages 169?176,
Boston, MA.
Kumar, Shankar and William Byrne.
2005. Local phrase reorderingmodels
for statistical machine translation.
In Proceedings of EMNLP-HLT,
pages 161?168, Rochester, NY.
Kumar, Shankar, Yonggang Deng, and
William Byrne. 2006. A weighted finite
722
Allauzen et al. Pushdown Automata in Statistical Machine Translation
state transducer translation template
model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
Lang, Bernard. 1974. Deterministic
techniques for efficient non-deterministic
parsers. In Proceedings of ICALP,
pages 255?269, Saarbru?cken.
Ljolje, Andrej, Fernando Pereira, and
Michael Riley. 1999. Efficient general lattice
generation and rescoring. In Proceedings of
Eurospeech, pages 1,251?1,254, Budapest.
Mohri, Mehryar. 2002. Semiring frameworks
and algorithms for shortest-distance
problems. Journal of Automata, Languages
and Combinatorics, 7:321?350.
Mohri, Mehryar. 2009. Weighted automata
algorithms. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 6,
pages 213?254.
Nederhof, Mark-Jan and Giorgio Satta. 2003.
Probabilistic parsing as intersection. In
Proceedings of 8th International Workshop on
Parsing Technologies, pages 137?148, Nancy.
Nederhof, Mark-Jan and Giorgio Satta. 2006.
Probabilistic parsing strategies. Journal of
the ACM, 53(3):406?436.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL, pages 160?167,
Sapporo.
Petre, Ion and Arto Salomaa. 2009.
Algebraic systems and pushdown
automata. In M. Drosde, W. Kuick,
and H. Vogler, editors, Handbook of
Weighted Automata. Springer, chapter 7,
pages 257?289.
Prasad, R., K. Krstovski, F. Choi, S. Saleem,
P. Natarajan, M. Decerbo, and D. Stallard.
2007. Real-time speech-to-speech
translation for PDAs. In Proceedings
of IEEE International Conference on
Portable Information Devices, pages 1?5,
Orlando, FL.
Roark, Brian, Cyril Allauzen, and
Michael Riley. 2013. Smoothed marginal
distribution constraints for language
modeling. In Proceedings of ACL,
pages 43?52, Sofia.
Roark, Brian, Richard Sproat, and Izhak
Shafran. 2011. Lexicographic semirings for
exact automata encoding of sequence
models. In Proceedings of ACL-HLT,
pages 1?5, Portland, OR.
Rush, Alexander M. and Michael Collins.
2011. Exact decoding of syntactic
translation models through lagrangian
relaxation. In Proceedings of ACL-HLT,
pages 72?82, Portland, OR.
Satta, Giorgio and Enoch Peserico. 2005.
Some computational complexity results
for synchronous context-free grammars.
In Proceedings of HLT-EMNLP,
pages 803?810, Vancouver.
Shafran, Izhak, Richard Sproat, Mahsa
Yarmohammadi, and Brian Roark.
2011. Efficient determinization of
tagged word lattices using categorial
and lexicographic semirings. In
Proceedings of ASRU, pages 283?288,
Honolulu, HI.
Stolcke, Andreas. 1995. An efficient
probabilistic context-free parsing
algorithm that computes prefix
probabilities. Computational Linguistics,
21(2):165?201.
Stolcke, Andreas. 1998. Entropy-based
pruning of backoff language models.
In Proceedings of DARPA Broadcast
News Transcription and Understanding
Workshop, pages 270?274, Landsdowne,
VA.
Tromble, Roy, Shankar Kumar, Franz J. Och,
and Wolfgang Macherey. 2008. Lattice
minimum Bayes-risk decoding for
statistical machine translation. In
Proceedings of EMNLP, pages 620?629,
Edinburgh.
Wu, Dekai. 1997. Stochastic inversion
transduction grammars and bilingual
parsing of parallel corpora. Computational
Linguistics, 23:377?403.
Xiao, Tong, Mu Li, Dongdong Zhang,
Jingbo Zhu, and Ming Zhou. 2009. Better
synchronous binarization for machine
translation. In Proceedings of EMNLP,
pages 362?370, Singapore.
Zens, Richard and Hermann Ney. 2008.
Improvements in dynamic programming
beam search for phrase-based statistical
machine translation. In Proceedings of
IWSLT, pages 195?205, Honolulu, HI.
Zhang, Hao, Liang Huang, Daniel Gildea,
and Kevin Knight. 2006. Synchronous
binarization for machine translation. In
Proceedings of HLT-NAACL, pages 256?263,
New York, NY.
Zollmann, Andreas and Ashish Venugopal.
2006. Syntax augmented machine
translation via chart parsing. In Proceedings
of NAACL Workshop on Statistical Machine
Translation, pages 138?141, New York, NY.
723

