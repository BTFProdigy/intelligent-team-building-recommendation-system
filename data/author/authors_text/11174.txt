Proceedings of ACL-08: HLT, pages 114?120,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Automatic Editing in a Back-End Speech-to-Text System
Maximilian Bisani Paul Vozila Olivier Divay Jeff Adams
Nuance Communications
One Wayside Road
Burlington, MA 01803, U.S.A.
{maximilian.bisani,paul.vozila,olivier.divay,jeff.adams}@nuance.com
Abstract
Written documents created through dictation
differ significantly from a true verbatim tran-
script of the recorded speech. This poses
an obstacle in automatic dictation systems as
speech recognition output needs to undergo
a fair amount of editing in order to turn it
into a document that complies with the cus-
tomary standards. We present an approach
that attempts to perform this edit from recog-
nized words to final document automatically
by learning the appropriate transformations
from example documents. This addresses a
number of problems in an integrated way,
which have so far been studied independently,
in particular automatic punctuation, text seg-
mentation, error correction and disfluency re-
pair. We study two different learning methods,
one based on rule induction and one based on
a probabilistic sequence model. Quantitative
evaluation shows that the probabilistic method
performs more accurately.
1 Introduction
Large vocabulary speech recognition today achieves
a level of accuracy that makes it useful in the produc-
tion of written documents. Especially in the medical
and legal domains large volumes of text are tradi-
tionally produced by means of dictation. Here docu-
ment creation is typically a ?back-end? process. The
author dictates all necessary information into a tele-
phone handset or a portable recording device and
is not concerned with the actual production of the
document any further. A transcriptionist will then
listen to the recorded dictation and produce a well-
formed document using a word processor. The goal
of introducing speech recognition in this process is
to create a draft document automatically, so that the
transcriptionist only has to verify the accuracy of the
document and to fix occasional recognition errors.
We observe that users try to spend as little time as
possible dictating. They usually focus only on the
content and rely on the transcriptionist to compose
a readable, syntactically correct, stylistically accept-
able and formally compliant document. For this rea-
son there is a considerable discrepancy between the
final document and what the speaker has said liter-
ally. In particular in medical reports we see differ-
ences of the following kinds:
? Punctuation marks are typically not verbalized.
? No instructions on the formatting of the report
are dictated. Section headings are not identified
as such.
? Frequently section headings are only implied.
(?vitals are? ? ?PHYSICAL EXAMINATION:
VITAL SIGNS:?)
? Enumerated lists. Typically speakers use
phrases like ?number one . . . next number . . . ?,
which need to be turned into ?1. . . . 2. . . . ?
? The dictation usually begins with a preamble
(e.g. ?This is doctor Xyz ...?) which does not
appear in the report. Similarly there are typ-
ical phrases at the end of the dictation which
should not be transcribed (e.g. ?End of dicta-
tion. Thank you.?)
114
? There are specific standards regarding the use
of medical terminology. Transcriptionists fre-
quently expand dictated abbreviations (e.g.
?CVA? ? ?cerebrovascular accident?) or oth-
erwise use equivalent terms (e.g. ?nonicteric
sclerae?? ?no scleral icterus?).
? The dictation typically has a more narrative
style (e.g. ?She has no allergies.?, ?I examined
him?). In contrast, the report is normally more
impersonal and structured (e.g. ?ALLERGIES:
None.?, ?he was examined?).
? For the sake of brevity, speakers frequently
omit function words. (?patient? ? ?the pa-
tient?, ?denies fever pain? ? ?he denies any
fever or pain?)
? As the dictation is spontaneous, disfluencies are
quite frequent, in particular false starts, correc-
tions and repetitions. (e.g. ?22-year-old fe-
male, sorry, male 22-year-old male? ? ?22-
year-old male?)
? Instruction to the transcriptionist and so-called
normal reports, pre-defined text templates in-
voked by a short phrase like ?This is a normal
chest x-ray.?
? In addition to the above, speech recognition
output has the usual share of recognition errors
some of which may occur systematically.
These phenomena pose a problem that goes beyond
the speech recognition task which has traditionally
focused on correctly identifying speech utterances.
Even with a perfectly accurate verbatim transcript of
the user?s utterances, the transcriptionist would need
to perform a significant amount of editing to obtain
a document conforming to the customary standards.
We need to look for what the user wants rather than
what he says.
Natural language processing research has ad-
dressed a number of these issues as individual prob-
lems: automatic punctuation (Liu et al, 2005),
text segmentation (Beeferman et al, 1999; Matusov
et al, 2003) disfluency repair (Heeman et al, 1996)
and error correction (Ringger and Allen, 1996;
Strzalkowski and Brandow, 1997; Peters and Drexel,
2004). The method we present in the following at-
tempts to address all this by a unified transforma-
tion model. The goal is simply stated as transform-
ing the recognition output into a text document. We
will first describe the general framework of learn-
ing transformations from example documents. In
the following two sections we will discuss a rule-
induction-based and a probabilistic transformation
method respectively. Finally we present experimen-
tal results in the context of medical transcription and
conclude with an assessment of both methods.
2 Text transformation
In dictation and transcription management systems
corresponding pairs of recognition output and edited
and corrected documents are readily available. The
idea of transformation modeling, outlined in fig-
ure 1, is to learn to emulate the transcriptionist. To
this end we first process archived dictations with the
speech recognizer to create approximate verbatim
transcriptions. For each document this yields the
spoken or source word sequence S = s1 . . . sM ,
which is supposed to be a word-by-word transcrip-
tion of the user?s utterances, but which may actu-
ally contain recognition errors. The corresponding
final reports are cleaned (removal of page headers
etc.), tagged (identification of section headings and
enumerated lists) and tokenized, yielding the text or
target token sequence T = t1...tN for each docu-
ment. Generally, the token sequence corresponds
to the spoken form. (E.g. ?25mg? is tokenized as
?twenty five milligrams?.) Tokens can be ordinary
words or special symbols representing line breaks,
section headings, etc. Specifically, we represent
each section heading by a single indivisible token,
even if the section name consists of multiple words.
Enumerations are represented by special tokens, too.
Different techniques can be applied to learn and ex-
ecute the actual transformation from S to T . Two
options are discussed in the following.
With the transformation model at hand, a draft
for a new document is created in three steps. First
the speech recognizer processes the audio recording
and produces the source word sequence S. Next,
the transformation step converts S into the target se-
quence T . Finally the transformation output T is
formatted into a text document. Formatting is the
115
archived
dictations
recognize

new
dictation
recognize

storeoo
transcripts
@A
train
//
transcript
transform

transformation
model
//
targets
GF //
tokens
format

archived
documents
tokenize
OO
draft
document
manual
correction

final
document
@A
store
OO
Figure 1: Illustration of how text transformation is inte-
grated into a speech-to-text system.
inverse of tokenization and includes conversion of
number words to digits, rendition of paragraphs and
section headings, etc.
Before we turn to concrete transformation tech-
niques, we can make two general statements about
this problem. Firstly, in the absence of observa-
tions to the contrary, it is reasonable to leave words
unchanged. So, a priori the mapping should be
the identity. Secondly, the transformation is mostly
monotonous. Out-of-order sections do occur but are
the exception rather than the rule.
3 Transformation based learning
Following Strzalkowski and Brandow (1997) and
Peters and Drexel (2004) we have implemented
a transformation-based learning (TBL) algorithm
(Brill, 1995). This method iteratively improves the
match (as measured by token error rate) of a col-
lection of corresponding source and target token se-
quences by positing and applying a sequence of sub-
stitution rules. In each iteration the source and tar-
get tokens are aligned using a minimum edit dis-
tance criterion. We refer to maximal contiguous
subsequences of non-matching tokens as error re-
gions. These consist of paired sequences of source
and target tokens, where either sequence may be
empty. Each error region serves as a candidate sub-
stitution rule. Additionally we consider refinements
of these rules with varying amounts of contiguous
context tokens on either side. Deviating from Peters
and Drexel (2004), in the special case of an empty
target sequence, i.e. a deletion rule, we consider
deleting all (non-empty) contiguous subsequences
of the source sequence as well. For each candi-
date rule we accumulate two counts: the number of
exactly matching error regions and the number of
false alarms, i.e. when its left-hand-side matches
a sequence of already correct tokens. Rules are
ranked by the difference in these counts scaled by
the number of errors corrected by a single rule ap-
plication, which is the length of the corresponding
error region. This is an approximation to the to-
tal number of errors corrected by a rule, ignoring
rule interactions and non-local changes in the mini-
mum edit distance alignment. A subset of the top-
ranked non-overlapping rules satisfying frequency
and minimum impact constraints are selected and
the source sequences are updated by applying the se-
lected rules. Again deviating from Peters and Drexel
(2004), we consider two rules as overlapping if the
left-hand-side of one is a contiguous subsequence
of the other. This procedure is iterated until no ad-
ditional rules can be selected. The initial rule set
is populated by a small sequence of hand-crafted
rules (e.g. ?impression colon?? ?IMPRESSION:?).
A user-independent baseline rule set is generated
by applying the algorithm to data from a collec-
tion of users. We construct speaker-dependent mod-
els by initializing the algorithm with the speaker-
independent rule set and applying it to data from the
given user.
4 Probabilistic model
The canonical approach to text transformation fol-
lowing statistical decision theory is to maximize the
text document posterior probability given the spoken
document.
T ? = argmax
T
p(T |S) (1)
Obviously, the global model p(T |S) must be con-
structed from smaller scale observations on the cor-
116
respondence between source and target words. We
use a 1-to-n alignment scheme. This means each
source word is assigned to a sequence of zero, one
or more target words. We denote the target words
assigned to source word si as ?i. Each replacement
?i is a possibly empty sequence of target words. A
source word together with its replacement sequence
will be called a segment. We constrain the set of pos-
sible transformations by selecting a relatively small
set of allowable replacements A(s) to each source
word. This means we require ?i ? A(si). We use
the usual m-gram approximation to model the joint
probability of a transformation:
p(S, T ) =
M?
i=1
p(si, ?i|si?m+1, ?i?m+1, . . . si?1, ?i?1)
(2)
The work of Ringger and Allen (1996) is similar
in spirit to this method, but uses a factored source-
channel model. Note that the decision rule (1) is
over whole documents. Therefore we processes
complete documents at a time without prior segmen-
tation into sentences.
To estimate this model we first align all training
documents. That is, for each document, the tar-
get word sequence is segmented into M segments
T = ?1^ . . .^?M . The criterion for this alignment
is to maximize the likelihood of a segment unigram
model. The alignment is performed by an expec-
tation maximization algorithm. Subsequent to the
alignment step, m-gram probabilities are estimated
by standard language modeling techniques. We cre-
ate speaker-specific models by linearly interpolating
an m-gram model based on data from the user with
a speaker-independent background m-gram model
trained on data pooled from a collection of users.
To select the allowable replacements for each
source word we count how often each particular tar-
get sequence is aligned to it in the training data. A
source target pair is selected if it occurs twice or
more times. Source words that were not observed
in training are immutable, i.e. the word itself is its
only allowable replacement A(s) = {(s)}. As an
example suppose ?patient? was deleted 10 times, left
unchanged 105 times, replaced by ?the patient? 113
times and once replaced by ?she?. The word patient
would then have three allowables: A(patient) =
{(), (patient), (the, patient)}.)
The decision rule (1) minimizes the document er-
ror rate. A more appropriate loss function is the
number of source words that are replaced incor-
rectly. Therefore we use the following minimum
word risk (MWR) decision strategy, which mini-
mizes source word loss.
T ? = (argmax
?1?A(si)
p(?1|S))^ . . .^( argmax
?M?A(sM )
p(?M |S))
(3)
This means for each source sequence position we
choose the replacement that has the highest poste-
rior probability p(?i|S) given the entire source se-
quence. To compute the posterior probabilities, first
a graph is created representing alternatives ?around?
the most probable transform using beam search.
Then the forward-backward algorithm is applied to
compute edge posterior probabilities. Finally edge
posterior probabilities for each source position are
accumulated.
5 Experimental evaluation
The methods presented were evaluated on a set of
real-life medical reports dictated by 51 doctors. For
each doctor we use 30 reports as a test set. Trans-
formation models are trained on a disjoint set of re-
ports that predated the evaluation reports. The typ-
ical document length is between one hundred and
one thousand words. All dictations were recorded
via telephone. The speech recognizer works with
acoustic models that are specifically adapted for
each user, not using the test data, of course. It
is hard to quote the verbatim word error rate of
the recognizer, because this would require a care-
ful and time-consuming manual transcription of the
test set. The recognition output is auto-punctuated
by a method similar in spirit to the one proposed by
Liu et al (2005) before being passed to the transfor-
mation model. This was done because we consid-
ered the auto-punctuation output as the status quo
ante which transformation modeling was to be com-
pared to. Neither of both transformation methods
actually relies on having auto-punctuated input. The
auto-punctuation step only inserts periods and com-
mas and the document is not explicitly segmented
into sentences. (The transformation step always ap-
plies to entire documents and the interpretation of a
period as a sentence boundary is left to the human
117
Table 1: Experimental evaluation of different text transformation techniques with different amounts of user-specific
data. Precision, recall, deletion, insertion and error rate values are given in percent and represent the average of 51
users, where the results for each user are the ratios of sums over 30 reports.
user sections punctuation all tokens
method docs precision recall precision recall deletions insertions errors
none (only auto-punct) 0.00 0.00 66.68 71.21 11.32 27.48 45.32
TBL SI 69.18 44.43 73.90 67.22 11.41 17.73 34.99
3-gram SI 65.19 44.41 73.79 62.26 18.15 12.27 36.09
TBL 25 75.38 53.39 75.59 69.11 10.97 15.97 32.62
3-gram 25 80.90 59.37 78.88 69.81 11.50 12.09 28.87
TBL 50 76.67 56.18 76.11 69.81 10.81 15.53 31.92
3-gram 50 81.10 62.69 79.39 70.94 11.31 11.46 27.76
TBL 100 77.92 58.03 76.41 70.52 10.67 15.19 31.29
3-gram 100 81.69 64.36 79.35 71.38 11.48 10.82 27.12
3-gram without MWR 100 81.39 64.23 79.01 71.52 11.55 10.92 27.29
reader of the document.) For each doctor a back-
ground transformation model was constructed using
100 reports from each of the other users. This is re-
ferred to as the speaker-independent (SI) model. In
the case of the probabilistic model, all models were
3-gram models. User-specific models were created
by augmenting the SI model with 25, 50 or 100 re-
ports. One report from the test set is shown as an
example in the appendix.
5.1 Evaluation metric
The output of the text transformation is aligned with
the corresponding tokenized report using a mini-
mum edit cost criterion. Alignments between sec-
tion headings and non-section headings are not per-
mitted. Likewise no alignment of punctuation and
non-punctuation tokens is allowed. Using the align-
ment we compute precision and recall for sections
headings and punctuation marks as well as the over-
all token error rate. It should be noted that the so de-
rived error rate is not comparable to word error rates
usually reported in speech recognition research. All
missing or erroneous section headings, punctuation
marks and line breaks are counted as errors. As
pointed out in the introduction the reference texts do
not represent a literal transcript of the dictation. Fur-
thermore the data were not cleaned manually. There
are, for example, instances of letter heads or page
numbers that were not correctly removed when the
text was extracted from the word processor?s file for-
mat. The example report shown in the appendix
features some of the typical differences between the
produced draft and the final report that may or may
not be judged as errors. (For example, the date of
the report was not given in the dictation, the sec-
tion names ?laboratory data? and ?laboratory evalu-
ation? are presumably equivalent and whether ?sta-
ble? is preceded by a hyphen or a period in the last
section might not be important.) Nevertheless, the
numbers reported do permit a quantitative compari-
son between different methods.
5.2 Results
Results are stated in table 1. In the baseline setup
no transformation is applied to the auto-punctuated
recognition output. Since many parts of the source
data do not need to be altered, this constitutes the
reference point for assessing the benefit of transfor-
mation modeling. For obvious reasons precision and
recall of section headings are zero. A high rate of
insertion errors is observed which can largely be at-
tributed to preambles. Both transformation methods
reduce the discrepancy between the draft document
and the final corrected document significantly. With
100 training documents per user the mean token er-
ror rate is reduced by up to 40% relative by the prob-
abilistic model. When user specific data is used, the
probabilistic approach performs consistently better
than TBL on all accounts. In particular it always
has much lower insertion rates reflecting its supe-
118
rior ability to remove utterances that are not typi-
cally part of the report. On the other hand the prob-
abilistic model suffers from a slightly higher dele-
tion rate due to being overzealous in this regard.
In speaker independent mode, however, the deletion
rate is excessively high and leads to inferior overall
performance. Interestingly the precision of the au-
tomatic punctuation is increased by the transforma-
tion step, without compromising on recall, at least
when enough user specific training data is available.
The minimum word risk criterion (3) yields slightly
better results than the simpler document risk crite-
rion (1).
6 Conclusions
Automatic text transformation brings speech recog-
nition output much closer to the end result desired
by the user of a back-end dictation system. It au-
tomatically punctuates, sections and rephrases the
document and thereby greatly enhances transcrip-
tionist productivity. The holistic approach followed
here is simpler and more comprehensive than a cas-
cade of more specialized methods. Whether or not
the holistic approach is also more accurate is not an
easy question to answer. Clearly the outcome would
depend on the specifics of the specialized methods
one would compare to, as well as the complexity
of the integrated transformation model one applies.
The simple models studied in this work admittedly
have little provisions for targeting specific transfor-
mation problems. For example the typical length of
a section is not taken into account. However, this is
not a limitation of the general approach. We have
observed that a simple probabilistic sequence model
performs consistently better than the transformation-
based learning approach. Even though neither of
both methods is novel, we deem this an important
finding since none of the previous publications we
know of in this domain allow this conclusion. While
the present experiments have used a separate auto-
punctuation step, future work will aim to eliminate
it by integrating the punctuation features into the
transformation step. In the future we plan to inte-
grate additional knowledge sources into our statis-
tical method in order to more specifically address
each of the various phenomena encountered in spon-
taneous dictation.
References
Beeferman, Doug, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Machine Learning, 34(1-3):177 ? 210.
Brill, Eric. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4):543 ? 565.
Heeman, Peter A., Kyung-ho Loken-Kim, and
James F. Allen. 1996. Combining the detec-
tion and correction of speech repairs. In Proc.
Int. Conf. Spoken Language Processing (ICSLP),
pages 362 ? 365. Philadelphia, PA, USA.
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and
Mary Harper. 2005. Using conditional random
fields for sentence boundary detection in speech.
In Proc. Annual Meeting of the ACL, pages 451 ?
458. Ann Arbor, MI, USA.
Matusov, Evgeny, Jochen Peters, Carsten Meyer,
and Hermann Ney. 2003. Topic segmentation
using markov models on section level. In Proc.
IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU), pages 471 ? 476.
IEEE, St. Thomas, U.S. Virgin Islands.
Peters, Jochen and Christina Drexel. 2004.
Transformation-based error correction for speech-
to-text systems. In Proc. Int. Conf. Spoken Lan-
guage Processing (ICSLP), pages 1449 ? 1452.
Jeju Island, Korea.
Ringger, Eric K. and James F. Allen. 1996. A fertil-
ity channel model for post-correction of continu-
ous speech recognition. In Proc. Int. Conf. Spoken
Language Processing (ICSLP), pages 897 ? 900.
Philadelphia, PA, USA.
Strzalkowski, Tomek and Ronald Brandow. 1997.
A natural language correction model for contin-
uous speech recognition. In Proc. 5th Workshop
on Very Large Corpora (WVVLC-5):, pages 168 ?
177. Beijing-Hong Kong.
119
A
pp
en
di
x
A
.E
xa
m
pl
e
of
a
m
ed
ic
al
re
po
rt
R
ec
og
ni
ti
on
ou
tp
ut
.
V
er
ti
ca
ls
pa
ce
w
as
ad
de
d
to
fa
ci
li
ta
te
vi
su
al
co
m
pa
ri
so
n.
do
ct
or
s
na
m
e
di
ct
at
in
g
a
pr
og
re
ss
no
te
on
fi
rs
tn
am
e
la
st
na
m
e
pa
ti
en
tw
it
ho
ut
co
m
pl
ai
nt
s
ha
s
be
en
am
bu
la
ti
ng
w
it
ho
ut
pr
ob
le
m
s
no
ch
es
tp
ai
n
ch
es
tp
re
ss
ur
e
st
il
l
ha
s
so
m
e
sh
or
tn
es
s
of
br
ea
th
bu
to
ve
ra
ll
ha
s
im
pr
ov
ed
si
gn
ifi
ca
nt
ly
vi
ta
ls
ig
ns
ar
e
st
ab
le
sh
e
is
af
eb
ri
le
lu
ng
s
sh
ow
de
cr
ea
se
d
br
ea
th
so
un
ds
at
th
e
ba
se
s
w
it
h
bi
la
te
ra
lr
al
es
an
d
rh
on
ch
ih
ea
rt
is
re
gu
la
r
ra
te
an
d
rh
yt
hm
tw
o
ov
er
si
x
cr
es
ce
nd
o
de
cr
es
ce
nd
o
m
ur
m
ur
at
th
e
ri
gh
t
st
er
na
lb
or
de
r
ab
do
m
en
so
ft
no
nt
en
de
r
no
nd
is
te
nd
ed
ex
tr
em
it
ie
s
sh
ow
on
e
pl
us
pe
da
le
de
m
a
bi
la
te
ra
ll
y
ne
ur
ol
og
ic
al
ex
am
is
no
nf
oc
al
w
hi
te
co
un
to
f
fiv
e
po
in
ts
ev
en
H
.a
nd
H
.
el
ev
en
po
in
ts
ix
an
d
th
ir
ty
fiv
e
po
in
tfi
ve
pl
at
el
et
co
un
to
f
on
e
fi
ft
y
fiv
e
so
di
um
on
e
th
ir
ty
se
ve
n
po
ta
ss
iu
m
th
re
e
po
in
tn
in
e
ch
lo
ri
de
on
e
hu
nd
re
d
ca
rb
on
di
ox
id
e
th
ir
ty
ni
ne
ca
lc
iu
m
ei
gh
tp
oi
nt
se
ve
n
gl
uc
os
e
ni
ne
ty
on
e
B
U
N
an
d
cr
ea
ti
ni
ne
th
ir
ty
se
ve
n
an
d
on
e
po
in
to
ne
im
pr
es
si
on
nu
m
be
r
on
e
C
O
P
D
ex
ac
er
ba
ti
on
co
nt
in
ue
br
ea
th
in
g
tr
ea
tm
en
ts
nu
m
be
r
tw
o
as
th
m
a
ex
ac
er
ba
ti
on
co
nt
in
ue
or
al
pr
ed
ni
so
ne
nu
m
be
r
th
re
e
br
on
ch
it
is
co
nt
in
ue
L
ev
aq
ui
n
nu
m
be
r
fo
ur
hy
pe
rt
en
si
on
st
ab
le
nu
m
be
r
fiv
e
un
co
nt
ro
ll
ed
di
ab
et
es
m
el
li
tu
s
im
pr
ov
ed
nu
m
be
r
si
x
ga
st
ro
es
op
ha
ge
al
re
fl
ux
di
se
as
e
st
ab
le
nu
m
be
r
se
ve
n
co
ng
es
tiv
e
he
ar
tf
ai
lu
re
st
ab
le
ne
w
pa
ra
gr
ap
h
pa
ti
en
ti
s
in
st
ab
le
co
nd
it
io
n
an
d
w
il
lb
e
di
sc
ha
rg
ed
to
na
m
e
nu
rs
in
g
ho
m
e
an
d
w
il
lb
e
m
on
it
or
ed
cl
os
el
y
on
an
ou
tp
at
ie
nt
ba
si
s
pr
og
re
ss
no
te
A
ut
om
at
ic
al
ly
ge
ne
ra
te
d
dr
af
t(
sp
ee
ch
re
co
gn
it
io
n
ou
tp
ut
af
te
r
tr
an
sf
or
m
at
io
n
an
d
fo
rm
at
ti
ng
)
P
ro
gr
es
s
no
te
S
U
B
JE
C
T
IV
E
:
T
he
pa
ti
en
ti
s
w
it
ho
ut
co
m
pl
ai
nt
s.
H
as
be
en
am
bu
la
ti
ng
w
it
ho
ut
pr
ob
le
m
s.
N
o
ch
es
tp
ai
n,
ch
es
tp
re
ss
ur
e,
st
il
lh
as
so
m
e
sh
or
tn
es
s
of
br
ea
th
,b
ut
ov
er
al
lh
as
im
pr
ov
ed
si
gn
ifi
ca
nt
ly
.
P
H
Y
S
IC
A
L
E
X
A
M
IN
A
T
IO
N
:
V
IT
A
L
S
IG
N
S
:
S
ta
bl
e.
S
he
is
af
eb
ri
le
.
L
U
N
G
S
:
S
ho
w
de
cr
ea
se
d
br
ea
th
so
un
ds
at
th
e
ba
se
s
w
it
h
bi
la
te
ra
lr
al
es
an
d
rh
on
ch
i.
H
E
A
R
T
:
R
eg
ul
ar
ra
te
an
d
rh
yt
hm
2/
6
cr
es
ce
nd
o
de
cr
es
ce
nd
o
m
ur
m
ur
at
th
e
ri
gh
ts
te
rn
al
bo
rd
er
.
A
B
D
O
M
E
N
:
S
of
t,
no
nt
en
de
r,
no
nd
is
te
nd
ed
.
E
X
T
R
E
M
IT
IE
S
:
S
ho
w
1+
pe
da
le
de
m
a
bi
la
te
ra
ll
y.
N
E
U
R
O
L
O
G
IC
A
L
:
N
on
fo
ca
l.
L
A
B
O
R
A
T
O
R
Y
D
A
T
A
:
W
hi
te
co
un
to
f
5.
7,
he
m
og
lo
bi
n
an
d
he
m
at
oc
ri
t1
1.
6
an
d
35
.5
,p
la
te
le
tc
ou
nt
of
15
5,
so
di
um
13
7,
po
ta
ss
iu
m
3.
9,
ch
lo
ri
de
10
0,
C
O
2
39
,c
al
ci
um
8.
7,
gl
uc
os
e
91
,
B
U
N
an
d
cr
ea
ti
ni
ne
37
an
d
1.
1.
IM
P
R
E
S
S
IO
N
:
1.
C
hr
on
ic
ob
st
ru
ct
iv
e
pu
lm
on
ar
y
di
se
as
e
ex
ac
er
ba
ti
on
.
C
on
ti
nu
e
br
ea
th
in
g
tr
ea
tm
en
ts
.
2.
A
st
hm
a
ex
ac
er
ba
ti
on
.
C
on
ti
nu
e
or
al
pr
ed
ni
so
ne
.
3.
B
ro
nc
hi
ti
s.
C
on
ti
nu
e
L
ev
aq
ui
n.
4.
H
yp
er
te
ns
io
n.
S
ta
bl
e.
5.
U
nc
on
tr
ol
le
d
di
ab
et
es
m
el
li
tu
s.
Im
pr
ov
ed
.
6.
G
as
tr
oe
so
ph
ag
ea
lr
efl
ux
di
se
as
e,
st
ab
le
.
7.
C
on
ge
st
iv
e
he
ar
tf
ai
lu
re
.
S
ta
bl
e.
P
L
A
N
:
T
he
pa
ti
en
ti
s
in
st
ab
le
co
nd
it
io
n
an
d
w
il
lb
e
di
sc
ha
rg
ed
to
na
m
e
nu
rs
in
g
ho
m
e
an
d
w
il
lb
e
m
on
it
or
ed
cl
os
el
y
on
an
ou
tp
at
ie
nt
ba
si
s.
F
in
al
re
po
rt
pr
od
uc
ed
by
a
hu
m
an
tr
an
sc
ri
pt
io
ni
st
w
it
ho
ut
re
fe
re
nc
e
to
th
e
au
to
m
at
ic
dr
af
t.
P
ro
gr
es
s
N
ot
e
D
A
T
E
:
Ju
ly
26
,2
00
5.
H
IS
T
O
R
Y
O
F
P
R
E
S
E
N
T
IL
L
N
E
S
S
:
T
he
pa
ti
en
th
as
no
co
m
pl
ai
nt
s.
S
he
is
am
bu
la
ti
ng
w
it
ho
ut
pr
ob
le
m
s.
N
o
ch
es
t
pa
in
or
ch
es
tp
re
ss
ur
e.
S
he
st
il
lh
as
so
m
e
sh
or
tn
es
s
of
br
ea
th
,
bu
to
ve
ra
ll
ha
s
im
pr
ov
ed
si
gn
ifi
ca
nt
ly
.
P
H
Y
S
IC
A
L
E
X
A
M
IN
A
T
IO
N
:
V
IT
A
L
S
IG
N
S
:
S
ta
bl
e.
S
he
?s
af
eb
ri
le
.
L
U
N
G
S
:
D
ec
re
as
ed
br
ea
th
so
un
ds
at
th
e
ba
se
s
w
it
h
bi
la
te
ra
l
ra
le
s
an
d
rh
on
ch
i.
H
E
A
R
T
:
R
eg
ul
ar
ra
te
an
d
rh
yt
hm
.
2/
6
cr
es
ce
nd
o,
de
cr
es
ce
nd
o
m
ur
m
ur
at
th
e
ri
gh
ts
te
rn
al
bo
rd
er
.
A
B
D
O
M
E
N
:
S
of
t,
no
nt
en
de
r
an
d
no
nd
is
te
nd
ed
.
E
X
T
R
E
M
IT
IE
S
:
1+
pe
da
le
de
m
a
bi
la
te
ra
ll
y.
N
E
U
R
O
L
O
G
IC
A
L
E
X
A
M
IN
A
T
IO
N
:
N
on
fo
ca
l.
L
A
B
O
R
A
T
O
R
Y
E
V
A
L
U
A
T
IO
N
:
W
hi
te
co
un
t5
.7
,H
&
H
11
.6
an
d
35
.5
,p
la
te
le
tc
ou
nt
of
15
5,
so
di
um
13
7,
po
ta
ss
iu
m
3.
9,
ch
lo
ri
de
10
0,
co
2
39
,c
al
ci
um
8.
7,
gl
uc
os
e
91
,B
U
N
an
d
cr
ea
ti
ni
ne
37
an
d
1.
1.
IM
P
R
E
S
S
IO
N
:
1.
C
hr
on
ic
ob
st
ru
ct
iv
e
pu
lm
on
ar
y
di
se
as
e
ex
ac
er
ba
ti
on
.
C
on
ti
nu
e
br
ea
th
in
g
tr
ea
tm
en
ts
.
2.
A
st
hm
a
ex
ac
er
ba
ti
on
.
C
on
ti
nu
e
or
al
pr
ed
ni
so
ne
.
3.
B
ro
nc
hi
ti
s.
C
on
ti
nu
e
L
ev
aq
ui
n.
4.
H
yp
er
te
ns
io
n-
st
ab
le
.
5.
U
nc
on
tr
ol
le
d
di
ab
et
es
m
el
li
tu
s-
im
pr
ov
ed
.
6.
G
as
tr
oe
so
ph
ag
ea
lr
efl
ux
di
se
as
e-
st
ab
le
.
7.
C
on
ge
st
iv
e
he
ar
tf
ai
lu
re
-s
ta
bl
e.
T
he
pa
ti
en
ti
s
in
st
ab
le
co
nd
it
io
n
an
d
w
il
lb
e
di
sc
ha
rg
ed
to
na
m
e
N
ur
si
ng
H
om
e,
an
d
w
il
lb
e
m
on
it
or
ed
on
an
ou
tp
at
ie
nt
ba
si
s.
120
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1191?1200,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
An Empirical Study Of Semi-Supervised Chinese Word Segmentation
Using Co-Training
Fan Yang
Nuance Communications, Inc.
fan.yang@nuance.com
Paul Vozila
Nuance Communications, Inc.
paul.vozila@nuance.com
Abstract
In this paper we report an empirical study
on semi-supervised Chinese word segmenta-
tion using co-training. We utilize two seg-
menters: 1) a word-based segmenter lever-
aging a word-level language model, and 2)
a character-based segmenter using character-
level features within a CRF-based sequence
labeler. These two segmenters are initially
trained with a small amount of segmented
data, and then iteratively improve each other
using the large amount of unlabelled data.
Our experimental results show that co-training
captures 20% and 31% of the performance
improvement achieved by supervised training
with an order of magnitude more data for the
SIGHAN Bakeoff 2005 PKU and CU corpora
respectively.
1 Introduction
In the literature there exist two general models for
supervised Chinese word segmentation, the word-
based approach and the character-based approach.
The word-based approach searches for all possible
segmentations, usually created using a dictionary,
for the optimal one that maximizes a certain util-
ity. The character-based approach treats segmenta-
tion as a character sequence labeling problem, indi-
cating whether a character is located at the bound-
ary of a word. Typically the word-based approach
uses word level features, such as word n-grams and
word length; while the character-based approach
uses character level information, such as character n-
grams. Both approaches have their own advantages
and disadvantages, and there has been some research
in combining the two approaches to improve the per-
formance of supervised word segmentation.
In this research we are trying to take advantage of
the word-based and the character-based approaches
in the semi-supervised setting for Chinese word seg-
mentation, where there is only a limited amount
of human-segmented data available, but there ex-
ists a relatively large amount of in-domain unseg-
mented data. The goal is to make use of the in-
domain unsegmented data to improve the ultimate
performance of word segmentation. According to
Sun et al (2009), ?the two approaches [word-based
and character-based approaches] are either based on
a particular view of segmentation.? This naturally
motivates the use of co-training, which utilizes two
models trained on different views of the input la-
beled data which then iteratively educate each other
with the unlabelled data. At the end of the co-
training iterations, the initially weak models achieve
improved performance. Co-training has been suc-
cessfully applied in many natural language process-
ing tasks. In this paper we describe an empiri-
cal study of applying co-training to semi-supervised
Chinese word segmentation. Our experimental re-
sults show that co-training captures 20% and 31%
of the performance improvement achieved by super-
vised training with an order of magnitude more data
for the SIGHANBakeoff 2005 PKU and CU corpora
respectively.
In section 2 we review the two supervised ap-
proaches and co-training algorithm in more detail.
In section 3 we describe our implementation of the
co-training word segmentation. In section 4 we de-
1191
Figure 1: A search space for word segmenter
scribe our co-training experiments. In section 5 we
conclude the paper.
2 Related Work
In this section, we first review the related research on
the word-based and the character-based approaches
for Chinese word segmentation, and comparatively
analyze these two supervised approaches. We then
review the related research on co-training.
2.1 Supervised Word Segmentation
2.1.1 Word-Based Segmenter
Given a character sequence c1c2...cn, the word-
based approach searches in all possible segmenta-
tions for one that maximizes a pre-defined utility
function, formally represented as in Equation 1. The
search space, GEN(c1c2...cn), can be represented
as a lattice, where each vertex represents a charac-
ter boundary index and each arc represents a word
candidate which is the sequence of characters within
the index range. A dictionary1 can be used to gener-
ate such a lattice. For example, given the character
sequence ??????? and a dictionary that con-
tains the words {????????} and all single
Chinese characters, the search space is illustrated in
Figure 1.
W? = arg max
W?GEN(c1c2...cn)
Util(W ) (1)
Dynamic programming such as Viterbi decoding
is usually used to search for the optimized segmen-
tation. The utility can be as simple as the negation
of number of words (i.e. Util(W ) = ? | W |),
1A dictionary is not a must to create the search space but
it could shrink the search space and also lead to improved seg-
mentation performance.
which gives a reasonable performance if the dictio-
nary used for generating the search space has a good
coverage. Alternatively one can search for the seg-
mentation that maximizes the word sequence prob-
ability P (W ) (i.e. Util(W ) = P (W )). With a
Markov assumption, P (W ) can be calculated using
a language model as in Equation 2.
P (W ) = P (w1w2...wm)
= P (w1).P (w2|w1)...P (wn|w1w2...wn1)
= P (w1)P (w2|w1)...P (wn|wn?1)
(2)
More generally, the utility can be formulated as
a semi-Markov linear model, defined as Equation 3,
in which ? is the feature function vector, and ? is
the parameter vector that can be learned from train-
ing data using different techniques: Liang (2005),
Gao et al (2005), and Zhang and Clark (2007) use
averaged perceptron; Nakagawa (2004) uses general
iterative scaling; Andrew (2006) uses semi-Markov
CRF; and Sun (2010) uses a passive-aggressive
learning algorithm.
Util(W ) = ?T?(c1c2...cn,W ) (3)
2.1.2 Character-Based Segmenter
The character-based approach treats word seg-
mentation as a character sequence labeling problem,
to label each character with its location in a word,
first proposed by Xue (2003).2 The basic label-
ing scheme is to use two tags: ?B? for the begin-
ning character of a word and ?O? for other charac-
ters (Peng et al, 2004). Xue (2003) use a four-tag
scheme based on some linguistic intuitions: ?B? for
the beginning character, ?I? for the internal charac-
ters, ?E? for the ending character, and ?S? for single-
character word. For example, the word sequence ??
?? ? ??? can be labelled as ?\B ?\I ?\E
?\S ?\B ?\E. Zhao et al (2010) further extend
this scheme by using six tags.
Training and decoding of the character labeling
problem is similar to part-of-speech tagging, which
2Teahan et al (2000) use a character language model to de-
termine whether a word boundary should be inserted after each
character, which can also be considered as a character-based
approach as well.
1192
is also generally formulated as a linear model. Many
machine learning techniques have been explored:
Xue (2003) use a maximum entropy model; Peng et
al. (2004) use linear-chain CRF; Liang (2005) uses
averaged perceptron; Sun et al (2009) use a discrim-
inative latent variable approach.
2.1.3 Comparison and Combination
It is more natural to use word-level informa-
tion, such as word n-grams and word length, in a
word-based segmenter; while it is more natural to
use character-level information, such as character n-
grams, in a character-based segmenter. Sun (2010)
gives a detailed comparison of the two approaches
from both the theoretical and empirical perspec-
tives. Word-level information has greater represen-
tational power in terms of contextual dependency,
while character-level information is better at mor-
phological analysis in terms of word internal struc-
tures.
On one hand, features in a character-based model
are usually defined in the neighboring n-character
window; and an order-K CRF can only look at the
labels of the previous K characters. Given that many
words contain more than one character, a word-
based model can examine a wider context. Thus
the contextual dependency information encoded in
a character-based model is generally weaker than in
a word-based model. Andrew (2006) also shows
that semi-Markov CRF makes strictly weaker in-
dependence assumptions than linear CRF and so
a word-based segmenter using an order-K semi-
Markov model is more expressive than a character-
based model using an order-K CRF.
On the other hand, Chinese words have internal
structures. Chinese characters can serve some mor-
phological functions in a word. For example, the
character? usually works as a suffix to signal plu-
ral; the character ? can also be a suffix meaning a
group of people; and ? generally works as a pre-
fix before a person?s nickname that has one charac-
ter. Such morphological information is extremely
useful for identifying unknown words. For exam-
ple, a character-based model can learn that ? is
usually tagged as ?B? and the next character is usu-
ally tagged as ?E?. Thus even when ?? is not an
existing word in the training data, a character-based
model might still be able to correctly label it as?\B
?\E.
Recent advanced Chinese word segmenters, either
word-based or character-based, have been trying to
make use of both word-level and character-level in-
formation. For example, Nakagawa (2004) inte-
grates the search space of a character-based model
into a word-based model; Andrew (2006) converts
CRF-type features into semi-CRF features in his
semi-Markov CRF segmenter; Sun et al (2009) add
word identify information into their character-based
model; and Sun (2010) combine the two approaches
at the system level using bootstrap aggregating.
2.2 Co-Training
The co-training approach was first introduced by
Blum and Mitchell (1998). Theoretical analysis of
its effectiveness is given in (Blum and Mitchell,
1998; Dasgupta et al, 2001; Abney, 2002). Co-
training works by partitioning the feature set into
two conditionally independent views (given the true
output). On each view a statistical model can be
trained. The presence of multiple distinct views of
the data can be used to train separate models, and
then each model?s predictions on the unlabeled data
are used to augment the training set of the other
model.
Figure 2 depicts a general co-training framework.
The inputs are two sets of data, a labelled set S and
an unlabelled set U. Generally S is small and U is
large. Two statistical models M1 and M2 are used,
which are built on two sets of data L1 and L2 initial-
ized as S but then incrementally increased in each
iteration. C is a cache holding a small subset of U
to be labelled by both models (Blum and Mitchell,
1998; Abney, 2002). In some applications, C is
not used and both models label the whole set of U
(i.e. C==U) (Collins and Singer, 1999; Nigam and
Ghani, 2000; Pierce and Cardie, 2001). The stop-
ping criteria can be, for example, when U is empty,
or when a certain number of iterations are executed.
In step 5 and 6 during each iteration, some data
labelled by M1 are selected and added to the train-
ing set L2, and vice versa. Several selection algo-
rithms have been proposed. Dasgupta et al (2001)
and Abney (2002) use a selection algorithm that tries
to maximize the agreement rate between the two
models. The more popular selection algorithm is to
choose the K examples that have the highest con-
1193
Input:
S is the labelled data
U is the unlabelled data
Variables:
L1 is the training data for View One
L2 is the training data for View Two
C is a cache holding a small subset of U
Initialization:
L1 <- S
L2 <- S
C <- randomly sample a subset of U
U <- U - C
REPEAT:
1. Train M1 using L1
2. Train M2 using L2
3. Use M1 to label C
4. Use M2 to label C
5. Select examples labelled by M1, add to L2
6. Select examples labelled by M2, add to L1
7. Randomly move samples from U to C
so that C maintains its size
UNTIL stopping criteria
Figure 2: A generic co-training framework
fidence score (Nigam and Ghani, 2000; Pierce and
Cardie, 2001). In order to balance the class distri-
butions in the training data L1 and L2, Blum and
Mitchell (1998) select P positive examples and Q
negative examples that have the highest confidence
scores respectively. Wang et al (2007) and Guz et
al. (2007) use disagreement-based selection, which
adds to L2, data that is labeled by M1 and M2 with
high and low confidence respectively, with the in-
tuition that such data are more useful and compen-
satory to M2. Finally, instead of adding the selected
data to the training data, Tur (2009) propose the co-
adaptation approach which linearly interpolates the
existing model with the new model built with the
new selected data.
3 Segmentation With Co-Training
3.1 Design of Two Segmenters
The use of co-training needs two statistical models
that satisfy the following three conditions. First, in
theory these two models need to be built on two con-
ditionally independent views. However this is a very
strong assumption and many large-scale NLP prob-
lems do not have a natural split of features to satisfy
this assumption. In practice it has been shown that
co-training can still achieve improved performance
when this assumption is violated, but conforming to
the conditionally independent assumption leads to
a bigger gain (Nigam and Ghani, 2000; Pierce and
Cardie, 2001). Thus we should strive to have the two
models less correlated. Second, the two models both
need to be effective for the task, that is, each of the
models itself can perform the task reasonably well.
Third, the decoding and training of the two models
need to be efficient, as in co-training we need to seg-
ment the unlabelled data and re-train the models in
each iteration. In the following we describe our de-
sign of the two segmenters.
Word-based segmenter In the word-based seg-
menter, we utilize a statistical n-gram lan-
guage model and try to optimize the language
modeling score together with a word insertion
penalty, as show in Equation 4. K is a per-
word penalty that is pre-determined with 10
fold cross-validation on the SIGhan PKU train-
ing set. We train a Kneser-Ney backoff lan-
guage model from the training data, and extract
a dictionary of words from the training data for
generating the search space. Our pilot study
suggested that a bigram language model is suf-
ficient for this task.
Util(W ) = ln(P (W )) ? |W | ? K (4)
Character-based segmenter We use an order-1
linear conditional random field to label a char-
acter sequence. Following Xue (2003), we use
the four-tag scheme ?BIES?. We use the tool
CRF++3. The features that we use are charac-
ter n-grams within the neighboring 5-character
window and tag bigrams. Given a character c0
in the character sequence c?2c?1c0c1c2, we ex-
tract the following features: character unigrams
c?2, c?1, c0, c1, c2, bigrams c?1c0 and c0c1. L2
regularization is applied in learning.
As can be seen, we build a word-based segmenter
that uses only word level features, and a character-
based segmenter that uses only character level fea-
tures. These two segmenters by no means satisfy
the conditionally independence assumption, but we
have the hope that they are not too correlated as
they use different levels of information and these
3http://crfpp.googlecode.com/svn/trunk/
doc/index.html
1194
different levels of information have been shown to
be complementary in literature. Also the effective-
ness of these two segmenters has been demonstrated
in literature and will be shown again in our results
in Section 4. Finally, both segmenters can decode
and be trained pretty quickly. In our implemen-
tation, running on a Xeon 2.93GHz CPU with 4G
of memory, it takes less than 30 seconds to build a
word-based segmenter and less than 1 hour to build
a character-based segmenter with the SIGhan PKU
training data, and it takes less than 20 seconds to ap-
ply the word-based segmenter or less than 5 seconds
to apply the character-based segmenter to the PKU
testing data.
3.2 Co-Training
We follow the framework in Figure 2 for the co-
training setup. We do not use the cache C, but di-
rectly label the whole unlabelled data set U, because
in our experiment setup (see Section 4) U is not
huge and computationally we can afford to label the
whole set. The stopping criteria we use is when U is
empty. Following Wang et al (2007) and Guz et al
(2007), we use disagreement-based data selection.
In every iteration, we pick some sentences that are
segmented by the character-based model with high
confidence but are segmented by the word-based
model with low confidence to add to the training
data of the word-based model, and vice versa. Con-
fidence score is normalized with regard to the length
of the sentence (i.e. number of characters) to avoid
biasing towards short sentences. Confidence scores
between the two segmenters, however, are not di-
rectly comparable. Thus we rank the sentences by
their confidence scores in each segmenter respec-
tively, and calculate the rank difference between the
two segmenters. This rank difference is used as the
indication of the gap of the confidence between the
two segmenters. The sentences of highest rank dif-
ference are assigned to the training data of the word-
based segmenter, with the segmentations from the
character-based model; and the sentences of lowest
rank difference are assigned to the training data of
the character-based model, with segmentations from
the word-based model.
4 Experiments
4.1 Data and Experiment Setup
We conduct a set of experiments to evaluate the per-
formance of our co-training on semi-supervised Chi-
nese word segmentation. Two corpora, the PKU cor-
pus and the CU corpus, from the SIGhan Bakeoff
2005 are used. The PKU corpus contains texts of
simplified Chinese characters, which include 19056
sentences in the training data and 1945 sentences in
the testing data. The CU corpus contains texts of
traditional Chinese characters, which include 53019
sentences in the training data and 1493 sentences in
the testing data. The training data in each corpus is
randomly split into 10 subsets. In each run one set
is used as the labelled data S, and the other nine sets
are combined and used as the unlabelled data U with
segmentations removed. That is, 10% of the training
data is used as segmented data, and 90% are used
as unsegmented data in our semi-supervised train-
ing. This setup resembles our semi-supervised ap-
plication, where there is only a small limited amount
of segmented data but a relatively large amount of
in-domain unsegmented data available. The final
trained character-based and word-based segmenters
from co-training are then evaluated on the testing
data. Results we report in this paper are the aver-
age of the 10 runs. F-measure is used as the per-
formance measurement. A 99% confidence interval
is calculated as ?2.56
?
p(1? F )/N for statistical
significance evaluation, where F is the F-measure
and N is the number of words. Subsequent asser-
tions in this paper about statistical significance indi-
cate whether or not the p-value in question exceeds
1%.
4.2 Co-Training Results
For comparison, we measure the baseline as the
performance of a model trained with the 10%
segmented data only (referred to as BASIC base-
lines). The BASIC baselines, both for the word-
based model and the character-based model, how-
ever, use only the segmented data but leave out the
large amount of available unsegmented data. We
thus measure another baseline (referred to as FOLD-
IN), which naively uses the unsegmented data. In the
FOLD-IN baseline, a model is first trained with the
10% segmented data, and then this model is used
1195
Table 1: Co-training results
PKU CU
char word char word
BASIC 90.4 84.2 89.2 78.4
FOLD-IN 90.5 84.2 89.3 78.5
CEILING 94.5 93.0 94.2 88.9
CO-TRAINING 91.2 90.3 90.2 86.2
Figure 3: Gap filling with different split ratio
to label the unsegmented data. The automatic seg-
mentation is then combined with the segmented data
to build a new model. We also measure the CEIL-
ING as the performance of a model trained with all
the training data available, i.e. we use the true seg-
mentations of the 90% unsegmented data together
with the 10% segmented data to train a model. The
CEILING tells us the oracle performance when we
have all segmented data for training, while the BA-
SIC shows how much performance is dropped when
we only have 10% of the segmented data. The per-
formance of co-training will tell us how much we
can fill the gap by taking advantage of the other 90%
as unsegmented data in the semi-supervised training.
The FOLD-IN baseline further verifies the effective-
ness of co-training, i.e. co-training should perform
better than naively folding in the unsegmented data.
Table 1 presents the results. First, we see that
both the word-based and character-based models
are doing a decent job under the CEILING condi-
tion. This confirms the effectiveness of each in-
dividual model, which is generally a requirement
for running co-training. The character-based seg-
menter, although simple and with character-level
features only, achieves the performance that is close
to the state-of-the-art technologies that are much
more complicated (The best performance is 95.2%
for the PKU corpus and 95.1% for the CU corpus,
see (Sun et al, 2009)). Second, we see that under
all four conditions, the character-based segmenter
performs better than the word-based model. This
is not too surprising as these results are consistent
with those reported in the literature. The word-based
segmenter implemented in this work is less power-
ful, and it needs a good dictionary to achieve good
performance. In our implementation, a dictionary
is extracted from the segmented training set. Thus
the word-based model suffers a lot when the train-
ing data is small. Third, we see that both the word-
based model and the character-based model are im-
proved by co-training, and the improvements are all
statistically significant. It is not surprising for the
word-based model to learn from the more accurate
character-based model, which can also identify new
words to add to the dictionary. More interestingly,
the character-based segmenter is able to benefit from
the less powerful word-based segmenter. For the
character-based model, about 20% of the gap be-
tween BASIC and CEILING is filled by co-training,
consistently in both the PKU and CU corpora. Fi-
nally, comparing FOLD-IN and BASIC, we see that
naively using the unsegmented data does not lead to
a significant improvement. This suggests that co-
training provides a process that effectively makes
use of the unsegmented data.
For completeness, in Figure 3 we also show the
relative gap filling with different splits of the seg-
mented vs unsegmented data. With more data mov-
ing to the segmented set, the absolute improvement
of co-training over BASIC gets smaller, while the
gap between the BASIC and CEILING also becomes
smaller. The relative gap filled, i.e. the improve-
ment relative to the difference between BASIC and
CEILING, as can be seen, consistently falls inside
the section of 15% and 25%.
1196
4.3 Further Analysis
It is not surprising that the word-based segmenter
benefits from co-training since it learns from the
more accurate character-based segmenter. Our fo-
cus, however, is to better understand what benefit
the character-based segmenter gains from the co-
training procedure. The character-based segmenter
treats word segmentation as a character sequence
labelling problem with four tags ?B I E S?. As-
suming that segmentation accuracy is proportional
to tag accuracy, we examine the tag accuracy of
the character-based segmenter before and after co-
training.
If a character is labelled with tag T0 initially be-
fore co-training and with tag T1 after co-training,
with the tag T1 different from T0, there can be one
of three cases: 1) T0 is correct; 2) T1 is correct; or
3) neither is correct. The absolute gain from co-
training of switching from tag T0 to T1 is defined
as the number of case 2 instances less case 1 in-
stances. Absolute gain indicates the gain of tag ac-
curacy where co-training learns to switch from T0 to
T1, and it contributes to the overall tag accuracy im-
provement. We also define relative gain of switch-
ing from tag T0 to T1 as the absolute gain divided
by the total number of cases switching from tag T0
to T1. Relative gain indicates how well co-training
learns to switch from T0 to T1.
Results are shown in Table 2. For both absolute
gain and relative gain, 12 ordered switching pairs
can be divided into two pools, a positive pool that
has higher gain includingB ? E, E ? B, S ? B,
S ? E, E ? I , B ? I , B ? S, and a neutral
pool that has lower or even negative gain including
I ? E, I ? S, I ? B, E ? S, S ? I . The
S ? B, S ? E, B ? I , E ? I in the positive
pool actually suggest that the character-based seg-
menter learns from co-training to combine a single-
character word with it?s neighbour to create a new
longer word; whereas the I ? E, I ? S, I ? B in
the neutral pool suggest that it does not really learn
how to separate a longer words into smaller units.
4.4 Feature Combination
We split the features into two sets, a character-level
feature set used by the character-based segmenter
and a word-level feature set used by the word-based
Table 2: Absolute Gain and Relative Gain
Absolute Gain Relative Gain
T0 T1 PKU CU PKU CU
B I 678 681 0.28 0.59
B E 2331 1727 0.41 0.46
B S 1025 686 0.07 0.08
I B 458 -283 0.07 -0.08
I E 61 -1117 0.01 -0.23
I S 323 -338 0.09 -0.34
E B 2163 1601 0.41 0.46
E I 963 819 0.36 0.62
E S 520 -13 0.03 0.00
S B 1847 892 0.27 0.30
S I 104 47 0.22 0.28
S E 1438 846 0.26 0.55
segmenter. We have shown that these two seg-
menters improve each other via co-training. How-
ever, as reviewed in Section 2.1, there is active re-
search in combining the character-level and word-
level features in a segmenter. When training with
the whole set of data (i.e. under the CEILING con-
dition), a segmenter with combined features tends to
perform better than only using one set of features.
Thus we need to address two problems. First, we
want to understand whether co-training, which splits
the features, can actually beat the BASIC and FOLD-
IN baselines of a segmenter with combined features.
Second, we want to explore whether we can further
improve the final co-training performance by feature
combination.
To address these two problems, we adopt Weiwei
Sun?s character-based segmenter4 in (Sun, 2010).
We use this segmenter because it is publicly avail-
able and it performs well on both the PKU corpus
and CU corpus. It models word segmentation as
a character labelling problem, and solves it with a
passive-aggressive optimization algorithm. It uses
the same feature set as in (Sun et al, 2009), in-
cluding both character-level features and word-level
features. Character-level features include character
uni-grams and bi-grams in the five character win-
dow, and whether the current character is the same
as the next or the one after the next character. Word-
4Available at http://www.coli.uni-
saarland.de/ wsun/ccws.tgz
1197
Table 3: Sun-Segmenter?s performance
PKU CU
BASIC 90.3 89.2
FOLD-IN 90.6 89.7
CEILING 94.8 95.0
Table 4: Results of feature combination
PKU CU
data combination 91.2 90.9
relabelling 91.2 91.0
level features include what word uni-grams or bi-
grams are anchored at the current character. Word
uni-grams and bi-grams are extracted from the la-
beled training data. For more details, please refer
to (Sun et al, 2009) and (Sun, 2010). For ease
of description, we will refer to Weiwei Sun?s seg-
menter with combined features as Sun-Segmenter,
and the character-based segmenter used in our co-
training which uses character-level features as Char-
Segmenter.
Table 3 shows the performance of the Sun-
Segmenter under the three conditions: BASIC,
FOLD-IN, and CEILING. We see that under
the CEILING condition, the Sun-Segmenter out-
performs the Char-Segmenter by 0.3% in the PKU
corpus and 0.8% in the CU corpus. However, un-
der the BASIC condition when there is only 10%
of training data available, the Sun-Segmenter gives
no gain. This probably is due to the fact that the
Sun-Segmenter uses a much larger feature set and
thus correspondingly a larger training set is needed
to avoid under-fitting. The Sun-Segmenter has more
gain when folding in the unsegmented data than the
Char-Segmenter, further suggesting that the Sun-
Segmenter is benefiting from the size of data. For
both corpora, however, the Char-Segmenter after co-
training beats the FOLD-IN baseline of the Sun-
Segmenter by at least 0.5%, and the improvement
is statistically significant. When there is only a
small amount of segmented data available, using a
more advanced segmenter with combined features
still under-performs compared to co-training. These
results justify the split of features for running co-
training.
Next we would like to explore whether we could
further improve the co-training performance, given
that we have a more advanced segmenter using com-
bined features. We try two approaches. In the first
approach, after all the iterations of co-training, the
data are split into two sets, one set for training the
word-based segmenter L1 and the other set for train-
ing the character-based segmenter L2. The segmen-
tations of these two sets of data are probably bet-
ter than the segmentations under the FOLD-IN con-
dition. We thus combine the two sets of data, and
use the combined data to train a new model with the
Sun-Segmenter. In the second approach, we use the
character-based segmenter after co-training, which
has an improved performance, to relabel the set of
unsegmented data U, and then combine it with the
segmented data set S.We then use the combined data
to train a new model with the Sun-Segmenter.
Results are shown in Table 4. In the PKU corpus,
we do not see a gain using either the data combina-
tion approach or the relabelling approach compared
to the performance of the Char-Segmenter after co-
training, probably because the Sun-Segmenter just
modestly improves over the Char-Segmenter under
the CEILING condition. However, in the CU cor-
pus, where under the CEILING condition the Sun-
Segmenter has a much bigger gain over the Char-
Segmenter, there is 0.7% improvement by using the
data combination approach and 0.8% by using the
relabelling approach, and the improvement is statis-
tically significant. Overall, using co-training with
feature combination we are able to cut the gap be-
tween the BASIC baseline and CEILING of the Sun-
Segmenter by 20% in the PKU corpus and 31% in
the CU corpus.
5 Discussion
There has been some research on semi-supervised
Chinese word segmentation. For example, Liang
(2005) derive word cluster features and mutual in-
formation features from unlabelled data, and add
them to supervised discriminative training; Li and
Sun (2009) use punctuation as implicit annotations
of a character starting a word (the character after a
punctation) or ending a word (the character before
a punctuation) in a large unlabelled data set to aug-
ment supervised data; Sun and Xu (2011) derive a
large set of features from unlabelled data, includ-
1198
ing mutual information, accessor variety and punc-
tuation variety to augment the character and word
features derived from labelled data. These research
works aim to use huge amount of unsegmented data
to further improve the performance of an already
well-trained supervised model.
In this paper, we assume a much limited amount
of segmented data available, and try to boost up the
performance by using in-domain unsegmented data.
Chinese word segmentation is domain-sensitive or
application sensitive. For example, a CRF seg-
menter trained on the SIGhan MSR training data,
which achieves an F-measure of 96.5% in the MSR
testing data, only has 83.8% when applied to the
PKU testing data; and the same CRF segmenter
trained on the PKU training data achieves 94.5% on
the PKU testing data. When one starts a new ap-
plication that requires word segmentation in a new
domain, it is likely that there is only a very small
amount of segmented data available.
We propose the approach of co-training for Chi-
nese word segmentation for the semi-supervised set-
ting where there is only a limited amount of human-
segmented data available, but there exists a relatively
large amount of in-domain unsegmented data. We
split the feature set into character-level features and
word-level features, and then build a character-based
segmenter with character-level features and a word-
based segmenter with word-level features, using the
limited amount of available segmented data. These
two segmenters then iteratively educate and improve
each other by making use of the large amount of
unsegmented data. Finally we combine the word-
level and character-level features with an advanced
segmenter to further improve the co-training perfor-
mance. Our experiments show that using 10% data
as segmented data and the other 90% data as unseg-
mented data, co-training reaches 20% performance
improvement achieved by supervised training with
all data in the SIGHAN 2005 PKU corpus and 31%
in the CU corpus.
Acknowledgments
The authors thank Weiwei Sun for helping with
data setup and technical consultation of the Sun-
Segmenter. The authors also thank Christian Mon-
son and Nicola Ueffing for helpful discussions.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 360?367.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 465?
472.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the eleventh annual conference on Computa-
tional learning theory, pages 92?100.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
100?110.
Sanjoy Dasgupta, Michael L. Littman, and David
Mcallester. 2001. Pac generalization bounds for co-
training. In Proceedings of Advances in Neural Infor-
mation Processing Systems, pages 375?382.
Jianfeng Gao, Mu Li, Andi Wu, and Chang-Ning Huang.
2005. Chinese word segmentation and named entity
recognition: A pragmatic approach. Computationl
Linguistics, 31(4):574.
Umit Guz, Sebastien Cuendet, Dilek Hakkani-Tur, and
Gokhan Tur. 2007. Co-training using prosodic and
lexical information for sentence segmentation. In pro-
ceedings of INTERSPEECH, pages 2597?2600.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for chinese word segmentation.
Comput. Linguist., 35:505?512, December.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, MASSACHUSETTS
INSTITUTE OF TECHNOLOGY, May.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In Pro-
ceedings of the ninth international conference on In-
formation and knowledge management, pages 86?93.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proceedings of the
20th international conference on Computational Lin-
guistics.
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In In Proceedings of the 2001 Conference on
1199
Empirical Methods in Natural Language Processing,
pages 1?9.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese word
segmentation using unlabeled data. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 970?979, Edinburgh,
Scotland, UK., July.
Xu Sun, Yaozhong Zhang, TakuyaMatsuzaki, Yoshimasa
Tsuruoka, and Jun?ichi Tsujii. 2009. A discrimi-
native latent variable chinese segmenter with hybrid
word/character information. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 56?64.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: comparison and combi-
nation. In Proceedings of the 23rd International Con-
ference on Computational Linguistics: Posters, pages
1211?1219.
W. J. Teahan, Rodger McNab, Yingying Wen, and Ian H.
Witten. 2000. A compression-based algorithm for
chinese word segmentation. Computational Linguis-
tics, 26(3):375?393, September.
Gokhan Tur. 2009. Co-adaptation: Adaptive co-
training for semi-supervised learning. In proceedings
of ICASSP, pages 3721?3724.
Wen Wang, Zhongqiang Huang, and Mary Harper. 2007.
Semi-supervised learning for part-of-speech tagging of
mandarin transcribed speech. In In ICASSP.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29?48.
Yue Zhang and Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 840?847, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing,
9(2):5:1?5:32, June.
1200
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 90?98,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Semi-Supervised Chinese Word Segmentation Using Partial-Label
Learning With Conditional Random Fields
Fan Yang
Nuance Communications Inc.
fan.yang@nuance.com
Paul Vozila
Nuance Communications Inc.
paul.vozila@nuance.com
Abstract
There is rich knowledge encoded in on-
line web data. For example, punctua-
tion and entity tags in Wikipedia data
define some word boundaries in a sen-
tence. In this paper we adopt partial-label
learning with conditional random fields to
make use of this valuable knowledge for
semi-supervised Chinese word segmenta-
tion. The basic idea of partial-label learn-
ing is to optimize a cost function that
marginalizes the probability mass in the
constrained space that encodes this knowl-
edge. By integrating some domain adap-
tation techniques, such as EasyAdapt, our
result reaches an F-measure of 95.98% on
the CTB-6 corpus, a significant improve-
ment from both the supervised baseline
and a previous proposed approach, namely
constrained decode.
1 Introduction
A general approach for supervised Chinese word
segmentation is to formulate it as a character se-
quence labeling problem, to label each charac-
ter with its location in a word. For example,
Xue (2003) proposes a four-label scheme based on
some linguistic intuitions: ?B? for the beginning
character of a word, ?I? for the internal characters,
?E? for the ending character, and ?S? for single-
character word. Thus the word sequence ????
???? can be turned into a character sequence
with labels as?\B?\I?\E?\S?\B?\E.
A machine learning algorithm for sequence label-
ing, such as conditional random fields (CRF) (Laf-
ferty et al., 2001), can be applied to the labelled
training data to learn a model.
Labelled data for supervised learning of Chi-
nese word segmentation, however, is usually ex-
pensive and tends to be of a limited amount. Re-
searchers are thus interested in semi-supervised
learning, which is to make use of unlabelled data
to further improve the performance of supervised
learning. There is a large amount of unlabelled
data available, for example, the Gigaword corpus
in the LDC catalog or the Chinese Wikipedia on
the web.
Faced with the large amount of unlabelled data,
an intuitive idea is to use self-training or EM, by
first training a baseline model (from the supervised
data) and then iteratively decoding the unlabelled
data and updating the baseline model. Jiao et al.
(2006) and Mann and McCallum (2007) further
propose to minimize the entropy of the predicted
label distribution on unlabeled data and use it as
a regularization term in CRF (i.e. entropy reg-
ularization). Beyond these ideas, Liang (2005)
and Sun and Xu (2011) experiment with deriv-
ing a large set of statistical features such as mu-
tual information and accessor variety from un-
labelled data, and add them to supervised dis-
criminative training. Zeng et al. (2013b) experi-
ment with graph propagation to extract informa-
tion from unlabelled data to regularize the CRF
training. Yang and Vozila (2013), Zhang et al.
(2013), and Zeng et al. (2013a) experiment with
co-training for semi-supervised Chinese word seg-
mentation. All these approaches only leverage
the distribution of the unlabelled data, yet do not
make use of the knowledge that the unlabelled data
might have integrated in.
There could be valuable information encoded
within the unlabelled data that researchers can take
advantage of. For example, punctuation creates
natural word boundaries (Li and Sun, 2009): the
character before a comma can only be labelled
as either ?S? or ?E?, while the character after a
comma can only be labelled as ?S? or ?B?. Fur-
thermore, entity tags (HTML tags or Wikipedia
tags) on the web, such as emphasis and cross refer-
ence, also provide rich information for word seg-
mentation: they might define a word or at least
90
Figure 1: Sausage constraint (partial labels) from natural annotations and punctuation
give word boundary information similar to punc-
tuation. Jiang et al. (2013) refer to such structural
information on the web as natural annotations, and
propose that they encode knowledge for NLP. For
Chinese word segmentation, natural annotations
and punctuation create a sausage
1
constraint for
the possible labels, as illustrated in Figure 1. In
the sentence ???????????????
?????, the first character ? can only be la-
belled with ?S? or ?B?; and the characters? before
the comma and ? before the Chinese period can
only be labelled as ?S? or ?E?. ?????? and ??
???? are two Wikipedia entities, and so they
define the word boundaries before the first char-
acter and after the last character of the entities as
well. The single character ? between these two
entities has only one label ?S?. This sausage con-
straint thus encodes rich information for word seg-
mentation.
To make use of the knowledge encoded in the
sausage constraint, Jiang et al. (2013) adopt a con-
strained decode approach. They first train a base-
line model with labelled data, and then run con-
strained decode on the unlabelled data by binding
the search space with the sausage; and so the de-
coded labels are consistent with the sausage con-
straint. The unlabelled data, together with the
labels from constrained decode, are then selec-
tively added to the labelled data for training the
final model. This approach, using constrained de-
code as a middle step, provides an indirect way
of leaning the knowledge. However, the middle
step, constrained decode, has the risk of reinforc-
ing the errors in the baseline model: the decoded
labels added to the training data for building the
final model might contain errors introduced from
the baseline model. The knowledge encoded in
1
Also referred to as confusion network.
the data carrying the information from punctuation
and natural annotations is thus polluted by the er-
rorful re-decoded labels.
A sentence where each character has exactly
one label is fully-labelled; and a sentence where
each character receives all possible labels is zero-
labelled. A sentence with sausage-constrained la-
bels can be viewed as partially-labelled. These
partial labels carry valuable information that re-
searchers would like to learn in a model, yet the
normal CRF training typically uses fully-labelled
sentences. Recently, T?ackstr?om et al. (2013) pro-
pose an approach to train a CRF model directly
from partial labels. The basic idea is to marginal-
ize the probability mass of the constrained sausage
in the cost function. The normal CRF training us-
ing fully-labelled sentences is a special case where
the sausage constraint is a linear line; while on
the other hand a zero-labelled sentence, where the
sausage constraint is the full lattice, makes no con-
tribution in the learning since the sum of proba-
bilities is deemed to be one. This new approach,
without the need of using constrained re-decoding
as a middle step, provides a direct means to learn
the knowledge in the partial labels.
In this research we explore using the partial-
label learning for semi-supervised Chinese word
segmentation. We use the CTB-6 corpus as the
labelled training, development and test data, and
use the Chinese Wikipedia as the unlabelled data.
We first train a baseline model with labelled data
only, and then selectively add Wikipedia data with
partial labels to build a second model. Because
the Wikipedia data is out of domain and has dis-
tribution bias, we also experiment with two do-
main adaptation techniques: model interpolation
and EasyAdapt (Daum?e III, 2007). Our result
reaches an F-measure of 95.98%, an absolute im-
provement of 0.72% over the very strong base-
91
line (corresponding to 15.19% relative error re-
duction), and 0.33% over the constrained decode
approach (corresponding to 7.59% relative error
reduction). We conduct a detailed error analy-
sis, illustrating how partial-label learning excels
constrained decode in learning the knowledge en-
coded in the Wikipedia data. As a note, our result
also out-performs (Wang et al., 2011) and (Sun
and Xu, 2011).
2 Partial-Label Learning with CRF
In this section, we review in more detail the
partial-label learning algorithm with CRF pro-
posed by (T?ackstr?om et al., 2013). CRF is an
exponential model that expresses the conditional
probability of the labels given a sequence, as
Equation 1, where y denotes the labels, x denotes
the sequence, ?(x, y) denotes the feature func-
tions, and ? is the parameter vector. Z(x) =
?
y
exp(?
T
?(x, y)) is the normalization term.
p
?
(y|x) =
exp(?
T
?(x, y))
Z(x)
(1)
In full-label training, where each item in the se-
quence is labelled with exactly one tag, maximum
likelihood is typically used as the optimization tar-
get. We simply sum up the log-likelihood of the n
labelled sequences in the training set, as shown in
Equation 2.
L(?) =
n
?
i=1
log p
?
(y|x)
=
n
?
i=1
(?
T
?(x
i
, y
i
)? log Z(x
i
))
(2)
The gradient is calculated as Equation 3, in
which the first term
1
n
?
n
i=1
?
j
is the empirical
expectation of feature function ?
j
, and the second
term E[?
j
] is the model expectation. Typically a
forward-backward process is adopted for calculat-
ing the latter.
?
??
j
L(?) =
1
n
n
?
i=1
?
j
? E[?
j
] (3)
In partial-label training, each item in the se-
quence receives multiple labels, and so for each
sequence we have a sausage constraint, denoted as
?
Y (x, y?). The marginal probability of the sausage
is defined as Equation 4.
p
?
(
?
Y (x, y?)|x) =
?
y?
?
Y (x,y?)
p
?
(y|x) (4)
The optimization target thus is to maximize the
probability mass of the sausage, as shown in Equa-
tion 5.
L(?) =
n
?
i=1
logp
?
(
?
Y (x
i
, y?
i
)|x
i
) (5)
A gradient-based approach such as L-BFGS
(Liu and Nocedal, 1989) can be employed to op-
timize Equation 5. The gradient is calculated as
Equation 6, where E
?
Y (x,y?)
[?
j
] is the empirical ex-
pectation of feature function ?
j
constrained by the
sausage, and E[?
j
] is the same model expectation
as in standard CRF. E
?
Y (x,y?)
[?
j
] can be calculated
via a forward-backward process in the constrained
sausage.
?
??
j
L(?) = E
?
Y (x,y?)
[?
j
]? E[?
j
] (6)
For fully-labelled sentences, E
?
Y (x,y?)
[?
j
] =
1
n
?
n
i=1
?
j
, and so the standard CRF is actually
a special case of the partial-label learning.
3 Experiment setup
In this section we describe the basic setup for
our experiments of semi-supervised Chinese word
segmentation.
3.1 Data
We use the CTB-6 corpus as the labelled data. We
follow the official CTB-6 guideline in splitting the
corpus into a training set, a development set, and a
test set. The training set has 23420 sentences; the
development set has 2079 sentences; and the test
set has 2796 sentences. These are fully-labelled
data.
For unlabelled data we use the Chinese
Wikipedia. The Wikipedia data is quite noisy
and asks for a lot of cleaning. We first filter out
references and lists etc., and sentences with ob-
viously bad segmentations, for example, where
every character is separated by a space. We
also remove sentences that contain mostly En-
glish words. We then convert all characters into
full-width. We also convert traditional Chinese
characters into simplified characters using the tool
92
mediawiki-zhconverter
2
. We then randomly select
7737 sentences and reserve them as the test set.
To create the partial labels in the Wikipedia
data, we use the information from cross-reference,
emphasis, and punctuation. In our pilot study we
found that it?s beneficial to force a cross-reference
or emphasis entity as a word when the item has
2 or 3 characters. That is, if an entity in the
Wikipedia has three characters it receives the la-
bels of ?BIE?; and if it has two characters it is la-
belled as ?BE?.
3
3.2 Supervised baseline model
We create the baseline supervised model by using
an order-1 linear CRF with L2 regularization, to
label a character sequence with the four candidate
labels ?BIES?. We use the tool wapiti (Lavergne
et al., 2010).
Following Sun et al. (2009), Sun (2010), and
Low et al. (2005), we extract two types of fea-
tures: character-level features and word-level fea-
tures. Given a character c
0
in the character se-
quence ...c
?2
c
?1
c
0
c
1
c
2
...:
Character-level features :
? Character unigrams: c
?2
, c
?1
, c
0
, c
1
, c
2
? Character bigrams: c
?2
c
?1
, c
?1
c
?0
,
c
0
c
1
, c
1
c
2
? Consecutive character equivalence:
?c
?2
= c
?1
, ?c
?1
= c
?0
, ?c
0
= c
1
,
?c
1
= c
2
? Separated character equivalence:
?c
?3
= c
?1
, ?c
?2
= c
0
, ?c
?1
= c
1
,
?c
0
= c
2
, ?c
1
= c
3
? Whether the current character is a punc-
tuation: ?Punct(c
0
)
? Character sequence pattern:
T (C
?2
)T (C
?1
)T (C
0
)T (C
1
)T (C
2
).
We classify all characters into four
types. Type one has three characters
??? (year) ??? (month) ??? (date).
Type two includes number characters.
Type three includes English characters.
All others are Type four characters.
Thus ?????S? would generate the
character sequence pattern ?41213?.
2
https://github.com/tszming/mediawiki-zhconverter
3
Another possibility is to label it as ?SS? but we find that
it?s very rare the case.
Word-level features :
? The identity of the string c[s : i] (i?6 <
s < i), if it matches a word from the
list of word unigrams; multiple features
could be generated.
? The identity of the string c[i : e] (i <
e < i+6), if it matches a word; multiple
features could be generated.
? The identity of the bi-gram c[s : i ?
1]c[i : e] (i ? 6 < s, e < i + 6), if
it matches a word bigram; multiple fea-
tures could be generated.
? The identity of the bi-gram c[s : i]c[i +
1 : e] (i?6 < s, e < i+6), if it matches
a word bigram; multiple features could
be generated.
? Idiom. We use the idiom list from (Sun
and Xu, 2011). If the current character
c
0
and its surrounding context compose
an idiom, we generate a feature for c
0
of
its position in the idiom. For example, if
c
?1
c
0
c
1
c
2
is an idiom, we generate fea-
ture ?Idiom-2? for c
0
.
The above features together with label bigrams
are fed to wapiti for training. The supervised base-
line model is created with the CTB-6 corpus with-
out the use of Wikipedia data.
3.3 Partial-label learning
The overall process of applying partial-label learn-
ing to Wikipedia data is shown in Algorithm 1.
Following (Jiang et al., 2013), we first train the
supervised baseline model, and use it to estimate
the potential contribution for each sentence in the
Wikipedia training data. We label the sentence
with the baseline model, and then compare the
labels with the constrained sausage. For each
character, a consistent label is defined as an ele-
ment in the constrained labels. For example, if
the constrained labels for a character are ?SB?,
the label ?S? or ?B? is consistent but ?I? or ?E? is
not. The number of inconsistent labels for each
sentence is then used as its potential contribution
to the partial-label learning: higher number indi-
cates that the partial-labels for the sentence con-
tain more knowledge that the baseline system does
not integrate, and so have higher potential contri-
bution. The Wikipedia training sentences are then
ranked by their potential contribution, and the top
93
Figure 2: Encoded knowledge: inconsistency ratio
and label reduction
K sentences together with their partial labels are
then added to the CTB-6 training data to build a
new model, using partial-label learning.
4
In our
experiments, we try six data points with K =
100k, 200k, 300k, 400k, 500k, 600k. Figure 2
gives a rough idea of the knowledge encoded in
Wikipedia for these data points with inconsistency
ratio and label reduction. Inconsistency ratio is the
percentage of characters that have inconsistent la-
bels; and label reduction is the percentage of the
labels reduced in the full lattice.
We modify wapiti to implement the partial-label
learning as described in Section 2. Same as base-
line, L2 regularization is adopted.
Algorithm 1 Partial-label learning
1. Train supervised baseline model M
0
2. For each sentence x in Wiki-Train:
3. y? Decode(x, M
0
)
4. diff? Inconsistent(y,
?
Y (x, ?y))
5. if diff > 0:
6. C? C ? (
?
Y (x, y?), diff)
7. Sort(C, diff, reverse)
8. Train model M
pl
with CTB-6 and top K sen-
tences in C using partial-label learning
3.4 Constrained decode
Jiang et al. (2013) implement the constrained de-
code algorithm with perceptron. However, CRF
is generally believed to out-perform perceptron,
yet the comparison of CRF vs perceptron is out
4
Knowledge is sparsely distributed in the Wikipedia data.
Using the Wikipedia data without the CTB-6 data for partial-
label learning does not necessarily guarantee convergence.
Also the CTB-6 training data helps to learn that certain la-
bel transitions, such as ?B B? or ?E E?, are not legal.
of the scope of this paper. Thus for fair compar-
ison, we re-implement the constrained decode al-
gorithm with CRF.
Algorithm 2 shows the constrained decode im-
plementation. We first train the baseline model
with the CTB-6 data. We then use this baseline
model to run normal decode and constrained de-
code for each sentence in the Wikipedia training
set. If the normal decode and constrained decode
have different labels, we add the constrained de-
code together with the number of different labels
to the filtered Wikipedia training corpus. The fil-
tered Wikipedia training corpus is then sorted us-
ing the number of different labels, and the top K
sentences with constrained decoded labels are then
added to the CTB-6 training data for building a
new model using normal CRF.
Algorithm 2 Constrained decode
1. Train supervised baseline model M
0
2. For each sentence x in Wiki-Train:
3. y? Decode(x, M
0
)
4. y?? ConstrainedDecode(x, M
0
)
5. diff? Difference(y, y?)
6. if diff > 0:
7. C? C ? (y?, diff)
8. Sort(C, diff, reverse)
9. Train model M
cd
with CTB-6 and top K sen-
tences in C using normal CRF
4 Evaluation on Wikipedia test set
In order to determine how well the models learn
the encoded knowledge (i.e. partial labels) from
the Wikipedia data, we first evaluate the mod-
els against the Wikipedia test set. The Wikipedia
test set, however, is only partially-labelled. Thus
the metric we use here is consistent label accu-
racy, similar to how we rank the sentences in Sec-
tion 3.3, defined as whether a predicted label for
a character is an element in the constrained la-
bels. Because partial labels are only sparsely dis-
tributed in the test data, a lot of characters receive
all four labels in the constrained sausage. Eval-
uating against characters with all four labels do
not really represent the models? difference as it is
deemed to be consistent. Thus beyond evaluating
against all characters in the Wikipedia test set (re-
ferred to as Full measurement), we also evaluate
against characters that are only constrained with
less than four labels (referred to as Label mea-
surement). The Label measurement focuses on en-
94
coded knowledge in the test set and so can better
represent the model?s capability of learning from
the partial labels.
Results are shown in Figure 3 with the Full
measurement and in Figure 4 with the Label mea-
surement. The x axes are the size of Wikipedia
training data, as explained in Section 3.3. As
can be seen, both constrained decode and partial-
label learning perform much better than the base-
line supervised model that is trained from CTB-6
data only, indicating that both of them are learning
the encoded knowledge from the Wikipedia train-
ing data. Also we see the trend that the perfor-
mance improves with more data in training, also
suggesting the learning of encoded knowledge.
Most importantly, we see that partial-label learn-
ing consistently out-performs constrained decode
in all data points. With the Label measurement,
partial-label learning gives 1.7% or higher abso-
lute improvement over constrained decode across
all data points. At the data point of 600k, con-
strained decode gives an accuracy of 97.14%,
while partial-label learning gives 98.93% (base-
line model gives 87.08%). The relative gain (from
learning the knowledge) of partial-label learning
over constrained decode is thus 18% ((98.93 ?
97.14)/(97.14 ? 87.08)). These results suggest
that partial-label learning is more effective in
learning the encoded knowledge in the Wikipedia
data than constrained decode.
5 CTB evaluation
5.1 Model adaptation
Our ultimate goal, however, is to determine
whether we can leverage the encoded knowledge
in the Wikipedia data to improve the word seg-
mentation in CTB-6. We run our models against
the CTB-6 test set, with results shown in Fig-
ure 5. Because we have fully-labelled sentences
in the CTB-6 data, we adopt the F-measure as
our evaluation metric here. The baseline model
achieves 95.26% in F-measure, providing a state-
of-the-art supervised performance. Constrained
decode is able to improve on this already very
strong baseline performance, and we see the nice
trend of higher performance with more unlabeled
data for training, indicating that constrained de-
code is making use of the encoded knowledge in
the Wikipedia data to help CTB-6 segmentation.
When we look at the partial-label model, how-
ever, the results tell a totally different story.
Figure 3: Wiki label evaluation results: Full
Figure 4: Wiki label evaluation results: Label
Figure 5: CTB evaluation results
95
First, it actually performs worse than the base-
line model, and the more data added to train-
ing, the worse the performance is. In the previ-
ous section we show that partial-label learning is
more effective in learning the encoded knowledge
in Wikipedia data than constrained decode. So,
what goes wrong? We hypothesize that there is
an out-of-domain distribution bias in the partial la-
bels, and so the more data we add, the worse the
in-domain performance is. Constrained decode
actually helps to smooth out the out-of-domain
distribution bias by using the re-decoded labels
with the in-domain supervised baseline model.
For example, both the baseline model and con-
strained decode correctly give the segmentation
???/?/??/?/???/?/??, while partial-
label learning gives incorrect segmentation ??
?/?/??/?/?/??/?/??. Looking at the
Wikipedia training data, ?? is tagged as an en-
tity 13 times; and ???, although occurs 13
times in the data, is never tagged as an entity.
Partial-label learning, which focuses on the tagged
entities, thus overrules the segmentation of ??
?. Constrained decode, on the other hand, by us-
ing the correctly re-decoded labels from the base-
line model, observes enough evidence to correctly
segment??? as a word.
To smooth out the out-of-domain distribution
bias, we experiment with two approaches: model
interpolation and EasyAdapt (Daum?e III, 2007).
5.1.1 Model interpolation
We linearly interpolate the model of partial-label
learningM
pl
with the baseline modelM
0
to create
the final model M
pl
+
, as shown in Equation 7. The
interpolation weight is optimized via a grid search
between 0.0 and 1.0 with a step of 0.1, tuned on
the CTB-6 development set. Again we modify
wapiti so that it takes two models and an interpo-
lation weight as input. For each model it creates
a search lattice with posteriors, and then linearly
combines the two lattices using the interpolation
weight to create the final search space for decod-
ing. As shown in Figure 5, model M
pl
+
consis-
tently out-performs constrained decode in all data
points. We also see the trend of better performance
with more training data.
M
pl
+
= ? ?M
0
+ (1? ?) ?M
pl
(7)
5.1.2 EasyAdapt
EasyAdapt is a straightforward technique but has
been shown effective in many domain adaptation
tasks (Daum?e III, 2007). We train the model
M
pl
ea
with feature augmentation. For each out-of-
domain training instance < x
o
, y
o
>, where x
o
is the input features and y
o
is the (partial) labels,
we copy the features and file them as an additional
feature set, and so the training instance becomes<
x
o
, x
o
, y
o
>. The in-domain training data remains
the same. Consistent with (Daum?e III, 2007),
EasyAdapt gives us the best performance, as show
in Figure 5. Furthermore, unlike in (Jiang et al.,
2013) where they find a plateau, our results show
no harm adding more training data for partial-label
learning when integrated with domain adaptation,
although the performance seems to saturate after
400k sentences.
Finally, we search for the parameter setting of
best performance on the CTB-6 development set,
which is to use EasyAdapt with K = 600k sen-
tences of Wikipedia data. With this setting, the
performance on the CTB-6 test set is 95.98%
in F-measure. This is 0.72% absolute improve-
ment over supervised baseline (corresponding to
15.19% relative error reduction), and 0.33% ab-
solute improvement over constrained decode (cor-
responding to 7.59% relative error reduction); the
differences are both statistically significant (p <
0.001).
5
As a note, this result out-performs (Sun
and Xu, 2011) (95.44%) and (Wang et al., 2011)
(95.79%), and the differences are also statistically
significant (p < 0.001).
5.2 Analysis with examples
To better understand why partial-label learning is
more effective in learning the encoded knowledge,
we look at cases where M
0
and M
cd
have the in-
correct segmentation while M
pl
(and its domain
adaptation variance M
pl
+
and M
pl
ea
) have the cor-
rect segmentation. We find that the majority is
due to the error in re-decoded labels outside of en-
coded knowledge. For example, M
0
and M
cd
give
the segmentation ???/?/?/?/6.9/??, yet the
correct segmentation given by partial-label learn-
ing is ???/?/??/6.9/ ??. Looking at the
Wikipedia training data, there are 38 tagged enti-
ties of??, but there are another 190 mentions of
5
Statistical significance is evaluated with z-test using the
standard deviation of
?
F ? (1 ? F )/N , where F is the F-
measure and N is the number of words.
96
?? that are not tagged as an entity. Thus for con-
strained decode it sees 38 cases of ??\B ?\E?
and 190 cases of ??\S ?\S? in the Wikipedia
training data. The former comes from the encoded
knowledge while the latter comes from re-decoded
labels by the baseline model. The much bigger
number of incorrect labels from the baseline re-
decoding badly pollute the encoded knowledge.
This example illustrates that constrained decode
reinforces the errors from the baseline. On the
other hand, the training materials for partial-label
learning are purely the encoded knowledge, which
is not impacted by the baseline model error. In this
example, partial-label learning focuses only on the
38 cases of ??\B ?\E? and so is able to learn
that?? is a word.
As a final remark, we want to make a point that,
although the re-decoded labels serve to smooth out
the distribution bias, the Wikipedia data is indeed
not the ideal data set for such a purpose, because
it itself is out of domain. The performance tends
to degrade when we apply the baseline model to
re-decode the out-of-domain Wikipedia data. The
errorful re-decoded labels, when being used to
train the model M
cd
, could lead to further er-
rors. For example, the baseline model M
0
is able
to give the correct segmentation ???/????
in the CTB-6 test set. However, when it is ap-
plied to the Wikipedia data for constrained de-
code, for the seven occurrences of???, three of
which are correctly labelled as ??\B?\I?\E?,
but the other four have incorrect labels. The fi-
nal model M
cd
trained from these labels then
gives incorrect segmentation ??/?/??/?/?
?/?/??/??/??/??/??? in the CTB-
6 test set. On the other hand, model interpolation
or EasyAdapt with partial-label learning, focusing
only on the encoded knowledge and not being im-
pacted by the errorful re-decoded labels, performs
correctly in this case. For a more fair comparison
between partial-label learning and constrained de-
code, we have also plotted the results of model in-
terpolation and EasyAdapt for constrained decode
in Figure 5. As can be seen, they improve on con-
strained decode a bit but still fall behind the cor-
respondent domain adaptation approach of partial-
label learning.
6 Conclusion and future work
There is rich information encoded in online web
data. For example, punctuation and entity tags de-
fine some word boundaries. In this paper we show
the effectiveness of partial-label learning in digest-
ing the encoded knowledge from Wikipedia data
for the task of Chinese word segmentation. Unlike
approaches such as constrained decode that use
the errorful re-decoded labels, partial-label learn-
ing provides a direct means to learn the encoded
knowledge. By integrating some domain adap-
tation techniques such as EasyAdapt, we achieve
an F-measure of 95.98% in the CTB-6 corpus, a
significant improvement from both the supervised
baseline and constrained decode. Our results also
beat (Wang et al., 2011) and (Sun and Xu, 2011).
In this research we employ a sausage constraint
to encode the knowledge for Chinese word seg-
mentation. However, a sausage constraint does
not reflect the legal label sequence. For exam-
ple, in Figure 1 the links between label ?B? and
label ?S?, between ?S? and ?E?, and between ?E?
and ?I? are illegal, and can confuse the machine
learning. In our current work we solve this issue
by adding some fully-labelled data into training.
Instead we can easily extend our work to use a lat-
tice constraint by removing the illegal transitions
from the sausage. The partial-label learning stands
the same, by executing the forward-backward pro-
cess in the constrained lattice. In future work we
will examine partial-label learning with this more
enforced lattice constraint in depth.
Acknowledgments
The authors would like to thank Wenbin Jiang, Xi-
aodong Zeng, and Weiwei Sun for helpful discus-
sions, and the anonymous reviewers for insightful
comments.
References
Hal Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Annual meetingassociation for computa-
tional linguistics, pages 256?263. Association for
Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan Lv, Yating Yang,
and Qun Liu. 2013. Discriminative learning with
natural annotations: Word segmentation as a case
study. In Proceedings of The 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 761?769.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
97
putational Linguistics and the 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, ACL-44, pages 209?216.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289, San Francisco, CA, USA.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35:505?512.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. Master?s thesis, MASSACHUSETTS
INSTITUTE OF TECHNOLOGY, May.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math-
ematical Programming, 45(3):503?528, December.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, pages
161?164, San Francisco, CA, USA.
Gideon S. Mann and Andrew McCallum. 2007. Ef-
ficient computation of entropy gradient for semi-
supervised conditional random fields. In Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association
for Computational Linguistics; Companion Volume,
Short Papers, NAACL-Short ?07, pages 109?112.
Weiwei Sun and Jia Xu. 2011. Enhancing chinese
word segmentation using unlabeled data. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 970?
979, Edinburgh, Scotland, UK., July.
Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-
masa Tsuruoka, and Jun?ichi Tsujii. 2009. A dis-
criminative latent variable Chinese segmenter with
hybrid word/character information. In Proceedings
of Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 56?64.
Weiwei Sun. 2010. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages 1211?1219.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics, 1:1?12.
Yiou Wang, Jun?ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Tori-
sawa. 2011. Improving chinese word segmentation
and POS tagging with semi-supervised methods us-
ing large auto-analyzed data. In Proceedings of the
5th International Joint Conference on Natural Lan-
guage Processing, pages 309?317.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, pages 29?48.
Fan Yang and Paul Vozila. 2013. An empirical study
of semi-supervised Chinese word segmentation us-
ing co-training. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1191?1200, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao, and
Isabel Trancoso. 2013a. Co-regularizing character-
based and word-based models for semi-supervised
chinese word segmentation. In Proceedings of The
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 171?176.
Xiaodong Zeng, Derek F. Wong, Lidia S. Chao,
and Isabel Trancoso. 2013b. Graph-based semi-
supervised model for joint chinese word segmen-
tation and part-of-speech tagging. In Proceedings
of The 51st Annual Meeting of the Association for
Computational Linguistics, pages 770?779.
Longkai Zhang, Houfeng Wang, Xu Sun, and Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for Chinese word seg-
mentation. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 311?321, Seattle, Washington, USA,
October. Association for Computational Linguistics.
98
