Improved Lexical Alignment by Combining Multiple Reified
Alignments
Dan Tufi?
 Institute for Artificial 
Intelligence
 13, ?13 Septembrie?, 
 050711, Bucharest 5, 
Romania 
tufis@racai.ro
Radu Ion 
Institute for Artificial
Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania 
radu@racai.ro
Alexandru Ceau?u
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5, 
Romania 
alceausu@racai.ro
Dan ?tef?nescu
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5,
 Romania 
danstef@racai.ro
Abstract
We describe a word alignment platform 
which ensures text pre-processing (to-
kenization, POS-tagging, lemmatization, 
chunking, sentence alignment) as re-
quired by an accurate word alignment. 
The platform combines two different 
methods, producing distinct alignments. 
The basic word aligners are described in 
some details and are individually evalu-
ated. The union of the individual align-
ments is subject to a filtering post-
processing phase. Two different filtering 
methods are also presented. The evalua-
tion shows that the combined word 
alignment contains 10.75% less errors 
than the best individual aligner. 
1 Introduction 
It is almost a truism that more decision makers, 
working together, are likely to find a better solu-
tion than when working alone. Dieterich (1998) 
discusses conditions under which different deci-
sions (in his case classifications) may be com-
bined for obtaining a better result. Essentially, a 
successful automatic combination method would 
require comparable performance for the decision 
makers and, additionally, that they should not 
make similar errors. This idea has been exploited 
by various NLP researchers in language model-
ling, statistical POS tagging, parsing, etc.
We developed two quite different word align-
ers, driven by two distinct objectives: the first 
one was motivated by a project aiming at the de-
velopment of an interlingually aligned set of 
wordnets while the other one was developed 
within an SMT ongoing project. The first one 
was used for validating, against a multilingual 
corpus, the interlingual synset equivalences and 
also for WSD experiments. Although, initially, it 
was concerned only with open class words re-
corded in a wordnet, turning it into an ?all 
words? aligner was not a difficult task. This 
word aligner, called YAWA is described in sec-
tion 3.
A quite different approach from the one used 
by YAWA, is implemented in our second word 
aligner, called MEBA, described in section 4. It 
is a multiple parameter and multiple step algo-
rithm using relevance thresholds specific to each 
parameter, but different from each step to the 
other. The implementation of MEBA was 
strongly influenced by the notorious five IBM 
models described in (Brown et al 1993). We 
used GIZA++ (Och and Ney 2000; Och and Ney, 
2003) to estimate different parameters of the 
MEBA aligner. 
The alignments produced by MEBA were 
compared to the ones produced by YAWA and 
evaluated against the Gold Standard (GS)1 anno-
tations used in the Word Alignment Shared 
Tasks (Romanian-English track) organized at 
HLT-NAACL2003 (Mihalcea and Pedersen 
2003).
Given that the two aligners are based on quite 
different models and that their F-measures are 
comparable, it was quite a natural idea to com-
bine their results and hope for an improved align-
ment. Moreover, by analyzing the alignment er-
rors done by each word aligner, we found that 
the number of common mistakes was small, so 
1 We noticed in the GS Alignment various errors (both sen-
tence and word alignment errors) that were corrected. The 
tokenization of the bitexts used in the GS Alignment was 
also modified, with the appropriate modification of the ref-
erence alignment. These reference data are available at 
http://www.racai.ro/res/WA-GS
153
the premises for a successful combination were 
very good (Dieterich, 1998). The Combined 
Word Aligner, COWAL-described in section 5, 
is a wrapper of the two aligners (YAWA and 
MEBA) merging the individual alignments and 
filtering the result. At the Shared Task on Word 
Alignment organized by the ACL2005 Work-
shop on ?Building and Using Parallel Corpora: 
Data-driven Machine Translation and Beyond? 
(Martin, et al 2005), we participated (on the 
Romanian-English track) with the two aligners 
and the combined one (COWAL). Out of 37 
competing systems, COWAL was rated the first, 
MEBA the 20th and TREQ-AL (Tufi? et al 
2003), the former version of YAWA, was rated 
the 21st. The usefulness of the aligner combina-
tion was convincingly demonstrated. 
Meanwhile, both the individual aligners and 
their combination were significantly improved. 
COWAL is now embedded into a larger platform 
that incorporates several tools for bitexts pre-
processing (briefly reviewed in section 2), a 
graphical interface that allows for comparing and 
editing different alignments, as well as a word 
sense disambiguation module.  
2 The bitext processing  
The two base aligners and their combination use 
the same format for the input data and provide 
the alignments in the same format. The input 
format is obtained from two raw texts that repre-
sent reciprocal translations. If not already sen-
tence aligned, the two texts are aligned by our 
sentence aligner that builds on Moore?s aligner 
(Moore, 2002) but which unlike it, is able to re-
cover the non-one-to-one sentence alignments. 
The texts in each language are then tokenized, 
tagged and lemmatized by the TTL module (Ion, 
2006). More often than not, the translation 
equivalents have the same part-of speech, but 
relying on such a restriction would seriously af-
fect the alignment recall. However, when the 
translation equivalents have different parts of 
speech, this difference is not arbitrary.  During 
the training phase, we estimated POS affinities: 
{p(POSmRO|POSnEN)} and p(POSnEN|POSmRO)}
and used them to filter out improbable translation 
equivalents candidates.
The next pre-processing step is represented by 
sentence chunking in both languages. The 
chunks are recognized by a set of regular expres-
sions defined over the tagsets and they corre-
spond to (non-recursive) noun phrases, adjectival 
phrases, prepositional phrases and verb com-
plexes (analytical realization of tense, aspect 
mood and diathesis and phrasal verbs). Finally, 
the bitext is assembled as an XML document 
(Tufi? and Ion, 2005), which is the standard input 
for most of our tools, including COWAL align-
ment platform. 
3 YAWA 
YAWA is a three stage lexical aligner that uses 
bilingual translation lexicons and phrase bounda-
ries detection to align words of a given bitext. 
The translation lexicons are generated by a dif-
ferent module, TREQ (Tufi?, 2002), which gen-
erates translation equivalence hypotheses for the 
pairs of words (one for each language in the par-
allel corpus) which have been observed occur-
ring in aligned sentences more than expected by 
chance. The hypotheses are filtered by a log-
likelihood score threshold. Several heuristics 
(string similarity-cognates, POS affinities and 
alignments locality2) are used in a competitive 
linking manner (Melamed, 2001) to extract the 
most likely translation equivalents. 
YAWA generates a bitext alignment by in-
crementally adding new links to those created at 
the end of the previous stage. The existing links 
act as contextual restrictors for the new added 
links. From one phase to the other new links are 
added without deleting anything. This monotonic 
process requires a very high precision (at the 
price of a modest recall) for the first step. The 
next two steps are responsible for significantly 
improving the recall and ensuring an increased 
F-measure.  
In the rest of this section we present the three 
stages of YAWA and evaluate the contribution 
of each of them to the final result. 
3.1 Phase 1: Content Words Alignment 
YAWA begins by taking into account only very 
probable links that represent the skeleton align-
ment used by the second phase. This alignment is 
done using outside resources such as translation 
lexicons and involves only the alignment of con-
tent words (nouns, verbs, adjective and adverbs). 
The translation equivalence pairs are ranked 
according to an association score (i.e. log-
likelihood, DICE, point-wise mutual informa-
2 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
requires that all alignment links starting from a chunk in the 
one language end in a chunk in the other language.
154
tion, etc.). We found that the best filtering of the 
translation equivalents was the one based on the
log-likelihood (LL) score with a threshold of 9.
Each translation unit (pair of aligned sen-
tences) of the target bitext is scanned for estab-
lishing the most likely links based on a competi-
tive linking strategy that takes into account the
LL association scores given by the TREQ trans-
lation lexicon. If a candidate pair of words is not 
found in the translation lexicon, we compute
their orthographic similarity (cognate score 
(Tufi?, 2002)). If this score is above a predeter-
mined threshold (for Romanian-English task we 
used the empirically found value of 0.43), the 
two words are treated as if they existed in the
translation lexicon with a high association score 
(in practice we have multiplied the cognate score
by 100 to yield association scores in the range 0
.. 100). The Figure 1 exemplifies the links cre-
ated between two tokens of a parallel sentence by
the end of the first phase.
Figure 1: Alignment after the first step 
3.2 Phase 2: Chunks Alignment 
The second phase requires that each part of the
bitext is chunked. In our Romanian-English ex-
periments, this requirement was fulfilled by us-
ing a set of regular expressions defined over the
tagsets used in the target bitext. These simple
chunkers recognize noun phrases, prepositional
phrases, verbal and adjectival or adverbial group-
ings of both languages.
In this second phase YAWA produces first
chunk-to-chunk matching and then aligns the 
words within the aligned chunks. Chunk align-
ment is done on the basis of the skeleton align-
ment produced in the first phase. The algorithm
is simple: align two chunks c(i) in source lan-
guage and c(j) in the target language if c(i) and 
c(j) have the same type (noun phrase, preposi-
tional phrase, verb phrase, adjectival/adverbial 
phrase) and if there exist a link ?w(s), w(t)? so 
that w(s) ? c(i) then w(t) ? c(j).
After alignment of the chunks, a language pair 
dependent module takes over to align the un-
aligned words belonging to the chunks. Our 
module for the Romanian-English pair of lan-
guages contains some very simple empirical
rules such as: if b is aligned to c and b is pre-
ceded by a, link a to c, unless there exist d in the 
same chunk with c and the POS category of d has 
a significant affinity with the category of a. The 
simplicity of these rules derives from the shallow
structures of the chunks. In the above example b
and c are content words while a is very likely a 
determiner or a modifier for b. The result of the 
second alignment phase, considering the same
sentence in Figure 1, is shown in Figure 2. The
new links are represented by the double lines. 
 Figure 2: Alignment after the second step 
3.3 Phase 3: Dealing with sequences of un-
aligned words
This phase identifies contiguous sequences of
words (blocks) in each part of the bitext which
remain unaligned and attempts to heuristically
match them. The main criteria used to this end
are the POS-affinities of the remaining unaligned 
words and their relative positions. Let us illus-
trate, using the same example and the result 
shown in Figure 2, how new links are added in
this last phase of the alignment. At the end of 
phase 2 the blocks of consecutive words that re-
main to be aligned are: English {en0 = (you), en1
= (that), en2 = (is, not), en3 = (and), en4 = (.)} and 
155
Romanian {ro0 = (), ro1 = (c?), ro2 = (nu, e), ro3 = 
(?i), ro4 = (.)}. The mapping of source and target
unaligned blocks depends on two conditions: that 
surrounding chunks are already aligned and that
pairs in candidate unaligned blocks have signifi-
cant POS-affinity. For instance in the figure
above, blocks en1 = (that) and ro1 = (c?) satisfy
the above conditions because they appear among
already aligned chunks (<?ll notice> ? <ve?i
observa> and <D?ncu ?s generosity> ? <gene- 
rozitatea lui D?ncu>) and they contain words 
with the same POS.
After block alignment3, given a pair of aligned
blocks, the algorithm links words with the same
POS and then the phase 2 is called again with 
these new links as the skeleton alignment. In 
Figure 3 is shown the result of phase 3 alignment
of the sentence we used as an example through-
out this section. The new links are shown (as
before) by double lines. 
Figure 3: Alignment after the third step 
The third phase is responsible for significant
improvement of the alignment recall, but it also 
generates several wrong links. The detection of
some of them is quite straightforward, and we
added an additional correction phase 3.f. By ana-
lysing the bilingual training data we noticed the trans-
lators? tendency to preserve the order of the 
phrasal groups. We used this finding (which 
might not be valid for any language pair) as a 
removal heuristics for the links that cross two or
more aligned phrase groups.  One should notice 
that the first word in the English side of the ex-
ample in Figure 3 (?you?) remained unaligned
(interpreted as not translated in the Romanian
side). According to the Gold Standard used for 
3 Only 1:1 links are generated between blocks. 
evaluation in the ACL2005 shared task, this in-
terpretation was correct, and therefore, for the
example in Figure 3, the F-measure for the 
YAWA alignment was 100%.
However, Romanian is a pro-drop language 
and although the translation of the English pro-
noun is not lexicalized in Romanian, one could 
argue that the auxiliary ?ve?i? should be aligned 
also to the pronoun ?you? as it incorporates the
grammatical information carried by the pronoun. 
Actually, MEBA (as exemplified in Figure 4)
produced this multiple token alignment (and was 
penalized for it!). 
3.4 Performance analysis
The table that follows presents the results of the
YAWA aligner at the end of each alignment
phase. Although the Precision decreases from
one phase to the next one, the Recall gains are 
significantly higher, so the F-measure is mono-
tonically increasing.
Precision Recall F-Measure
Phase 1 94.08% 34.99% 51.00%
Phase 1+2 89.90% 53.90% 67.40%
Phase 1+2+3 88.82% 73.44% 80.40%
Phase 1+2+3+3.f 88.80% 74.83% 81.22%
Table 1: YAWA evaluation 
4 MEBA 
MEBA uses an iterative algorithm that takes ad-
vantage of all pre-processing phases mentioned
in section 2. Similar to YAWA aligner, MEBA 
generates the links step by step, beginning with 
the most probable (anchor links). The links to be 
added at any later step are supported or restricted 
by the links created in the previous iterations. 
The aligner has different weights and different
significance thresholds on each feature and itera-
tion. Each of the iterations can be configured to
align different categories of tokens (named enti-
ties, dates and numbers, content words, func-
tional words, punctuation) in decreasing order of 
statistical evidence. 
The first iteration builds anchor links with a 
high level of certainty (that is cognates, numbers,
dates, pairs with high translation probability).
The next iteration tries to align content words
(open class categories) in the immediate vicinity
of the anchor links. In all steps, the candidates
are considered if and only if they meet the mini-
mal threshold restrictions.
A link between two tokens is characterized by
a set of features (with values in the [0,1] inter-
val). We differentiate between context independ-
156
ent features that refer only to the tokens of the
current link (translation equivalency, part-of-
speech affinity, cognates, etc.) and context de-
pendent features that refer to the properties of the 
current link with respect to the rest of links in a 
bi-text (locality, number of traversed links, to-
kens indexes displacement, collocation). Also, 
we distinguish between bi-directional features
(translation equivalence, part-of-speech affinity)
and non-directional features (cognates, locality,
number of traversed links, collocation, indexes 
displacement).
Precision Recall F-measure
?Anchor? links 98.50% 26.82% 42.16%
Words around 
?anchors? 96.78% 42.41% 58.97%
Funct. words 
and punctuation 94.74% 59.48% 73.08%
Probable links 92.05% 71.00% 80.17%
Table 2: MEBA evaluation 
The score of a candidate link (LS) between a 
source token i and a target token j is computed
by a linear function of several features scores
(Tiedemann, 2003).
?
 
 
n
i
ii ScoreFeatjiLS
1
*),( O ; 1
1
 ?
 
n
i
iO
Each feature has defined a specific signifi-
cance threshold, and if the feature?s value is be-
low this threshold, the contribution to the LS of 
the current link of the feature in case is nil. 
The thresholds of the features and lambdas are 
different from one iteration to the others and they
are set by the user during the training and system
fine-tuning phases. There is also a general 
threshold for the link scores and only the links 
that have the LS above this threshold are retained
in the bitext alignment. Given that this condition 
is not imposing unique source or target indexes,
the resulting alignment is inherently many-to-
many.
In the following subsections we briefly discuss
the main features we use in characterising a link.
4.1 Translation equivalence
This feature may be used for two types of pre-
processed data: lemmatized or non-lemmatized
input. Depending on the input format, MEBA
invokes GIZA++ to build translation probability
lists for either lemmas or the occurrence forms of 
the bitext4. Irrespective of the lemmatisation op-
tion, the considered token for the translation 
model build by GIZA++ is the respective lexical 
item (lemma or wordform) trailed by its POS tag 
(eg. plane_N, plane_V, plane_A). In this way we 
avoid data sparseness and filter noisy data. For 
instance, in case of highly inflectional languages 
(as Romanian is) the use of lemmas significantly
reduces the data sparseness. For languages with
weak inflectional character (as English is) the 
POS trailing contributes especially to the filter-
ing the search space. A further way of removing
the noise created by GIZA++ is to filter out all 
the translation pairs below a LL-threshold. We 
made various experiments and, based on the es-
timated ratio between the number of false nega-
tives and false positive, empirically set the value
of this threshold to 6. All the probability losses 
by this filtering were redistributed proportionally
to their initial probabilities to the surviving trans-
lation equivalence candidates. 
4.2 Translation equivalence entropy score 
The translation equivalence relation is a se-
mantic one and it directly addresses the notion of 
word sense. One of the Zipffian laws prescribes a 
skewed distribution of the senses of a word oc-
curring several times in a coherent text. We used
this conjecture as a highly informative informa-
tion source for the validity of a candidate link.
The translation equivalence entropy score is a 
favouring parameter for the words that have few 
high probability translations. Since this feature is
definitely sensitive to the order of the lexical 
items, we compute an average value for the link: 
DES(A)+EES(B). Currently we use D=E=0.5, but 
it might be interesting to see, depending on dif-
ferent language pairs, how the performance of 
the aligner would be affected by a different set-
tings of these parameters.
N
TRWpTRWp
N
i
ii
WES log
),(log*),(
11)(
?
 

 
4.3 Part-of-speech affinity
In faithful translations the translated words tend
to be translated by words of the same part-of-
speech. When this is not the case, the different 
POSes, are not arbitrary. The part of speech af-
finity, P(cat(A)|cat(B), can be easily computed
from a gold standard alignment. Obviously, this
4 Actually, this is a user-set parameter of the MEBA aligner;
if the input bitext contain lemmatization information, both 
translation probability tables may be requested. 
157
is a directional feature, so an averaging operation 
is necessary in order to ascribe this feature to a 
link: PA=DP(cat(A)|cat(B)) + EP(cat(B)|cat(A)).
Again, we used D=E=0.5 but different values of 
these weights might be worthwhile investigating. 
4.4 Cognates 
The similarity measure, COGN(TS, TT), is im-
plemented as a Levenstein metric. Using the
COGN test as a filtering device is a heuristic 
based on the cognate conjecture, which says that 
when the two tokens of a translation pair are 
orthographically similar, they are very likely to
have similar meanings (i.e. they are cognates). 
The threshold for the COGN(TS, TT) test was 
empirically set to 0.42. This value depends on 
the pair of languages in the bitext. The actual 
implementation of the COGN test includes a lan-
guage-dependent normalisation step, which strips 
some suffixes, discards the diacritics, reduces 
some consonant doubling, etc. This normalisa-
tion step was hand written, but, based on avail-
able lists of cognates, it could be automatically
induced.
4.5 Obliqueness 
Each token in both sides of a bi-text is character-
ized by a position index, computed as the ratio 
between the relative position in the sentence and 
the length of the sentence. The absolute value of 
the difference between tokens? position indexes,
subtracted from 15, gives the link?s ?oblique-
ness?.
)()(
1),(
TS
ji Sentlength
j
Sentlength
iTWSWOBL  
This feature is ?context free? as opposed to the 
locality feature described below.
4.6 Locality 
Locality is a feature that estimates the degree to 
which the links are sticking together. 
MEBA has three features to account for local-
ity: (i) weak locality, (ii) chunk-based locality
and (iii) dependency-based locality.
The value of the weak locality feature is de-
rived from the already existing alignments in a 
window of N tokens centred on the focused to-
ken. The window size is variable, proportional to 
the sentence length. If in the window there exist
k linked tokens and the relative positions of the 
5 This is to ensure that values close to 1 are ?good? ones and 
those near 0 are ?bad?. This definition takes into account the
relatively similar word order in English and Romanian.
tokens in these links are <i1 j1>, ?<ik jk> then 
the locality feature of the new link <ik+1, jk+1> is 
defined by the equation below: 
)
||
||1,1min(
1 1
1?
 


 
k
m mk
mk
jj
ii
k
LOC
If the new link starts from or ends in a token 
already linked, the index difference that would
be null in the formula above is set to 1. This way,
such candidate links would be given support by
the LOC feature (and avoid overflow error). In 
the case of chunk-based locality the window 
span is given by the indexes of the first and last 
tokens of the chunk. 
Dependency-based locality uses the set of the 
dependency links of the tokens in a candidate
link for the computation of the feature value. In
this case, the LOC feature of a candidate link
<ik+1, jk+1> is set to 1 or 0 according to the fol-
lowing rule: 
if between ik+1 and iD there is a (source lan-
guage) dependency and if between jk+1 and jE
there is also a (target language) dependency then 
LOC is 1 if iD and jE are aligned, and 0 otherwise. 
Please note that in case jk+1{ jE a trivial depend-
ency (identity) is considered and the LOC attrib-
ute of the link <ik+1, jk+1> is set to always to 1.
Figure 4: Chunk and dependency-based locality
4.7 Collocation 
Monolingual collocation is an important clue for 
word alignment. If a source collocation is trans-
lated by a multiword sequence, very often the
lexical cohesion of source words can also be
found in the corresponding translated words. In 
this case the aligner has strong evidence for 
158
many to many linking. When a source colloca-
tion is translated as a single word, this feature is
a strong indication for a many to 1 linking.
Bi-gram lists (only content words) were built 
from each monolingual part of the training cor-
pus, using the log-likelihood score (threshold of 
10) and minimal occurrence frequency (3) for
candidates filtering.
We used the bi-grams list to annotate the 
chains of lexical dependencies among the con-
tents words. Then, the value of the collocation 
feature is computed similar to the dependency-
based locality feature. The algorithm searches for
the links of the lexical dependencies around the 
candidate link. 
5 Combining the reified alignments 
From a given alignment one can compute a se-
ries of properties for each of its links (such as the 
parameters used by the MEBA aligner). A link
becomes this way a structured object that can be 
manipulated in various ways, independent of the
bitext (or even of the lexical tokens of the link)
from which it was extracted. We call this proce-
dure alignment reification. The properties of the 
links of two or more alignments are used for our 
methods of combining the alignments.
One simple, but very effective method of
alignment combination is a heuristic procedure, 
which merges the alignments produced by two or
more word aligners and filters out the links that 
are likely to be wrong. For the purpose of filter-
ing, a link is characterized by its type defined by
the pair of indexes (i,j) and the POS of the tokens
of the respective link. The likelihood of a link is 
proportional to the POS affinities of the tokens of
the link and inverse proportional to the bounded
relative positions (BRP) of the respective tokens:
  where avg is the average
displacement in a Gold Standard of the aligned 
tokens with the same POSes as the tokens of the 
current link. From the same gold standard we 
estimated a threshold below which a link is re-
moved from the final alignment.
||||1 avgjiBRP  
A more elaborated alignment combination
(with better results than the previous one) is 
modelled as a binary statistical classification 
problem (good / bad) and, as in the case of the 
previous method, the net result is the removal of 
the links which are likely to be wrong. We used
an ?off-the-shelf? solution for SVM training and 
classification - LIBSVM6 (Fan et al, 2005) with 
6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
the default parameters (C-SVC classification and
radial basis kernel function). Both context inde-
pendent and context dependent features charac-
terizing the links were used for training. The
classifier was trained with both positive and 
negative examples of links. A set of links ex-
tracted from the Gold Standard alignment was
used as positive examples set. The same number
of negative examples was extracted from the
alignments produced by COWAL and MEBA 
where they differ from the Gold Standard.
It is interesting to notice that for the example
discussed in Figures 1-4, the first combiner
didn?t eliminate the link <you ve?i> producing 
the result shown in Figure 4. This is because the 
relative positions of the two words are the same
and the POS-affinity of the English personal
pronouns and the Romanian auxiliaries is signifi-
cant. On the other hand, the SVM-based com-
biner deleted this link, producing the result
shown in Figure 3. The explanation is that, ac-
cording to the Gold Standard we used, the links 
between English pronouns and Romanian auxil-
iaries or main verbs in pro-drop constructions
were systematically dismissed (although we 
claim that they shouldn?t and that the alignment
in Figure 4 is better than the one in Figure 3).
The evaluation (according to the Gold Standard)
of the SVM-based combination (COWAL),
compared with the individual aligners, is shown
in Table 3. 
Aligner Precision Recall F-measure
YAWA 88.80% 74.83% 81.22%
MEBA 92.05% 71.00% 80.17%
COWAL 86.99% 79.91% 83.30%
Table 3: Combined alignment
6 Conclusions and further work
Neither YAWA nor MEBA needs an a priori bi-
lingual dictionary, as this will be automatically
extracted by TREQ or GIZA++. We made
evaluation of the individual alignments in both
experimental settings: without a start-up bilin-
gual lexicon and with an initial mid-sized bilin-
gual lexicon. Surprisingly enough, we found that
while the performance of YAWA increases a
little bit (approx. 1% increase of the F-measure)
MEBA is doing better without an additional lexi-
con. Therefore, in the evaluation presented in the 
previous section MEBA uses only the training
data vocabulary.
YAWA is very sensitive to the quality of the
bilingual lexicons it uses. We used automatically
translation lexicons (with or without a seed lexi-
159
con), and the noise inherently present might have 
had a bad influence on YAWA?s precision. Re-
placing the TREQ-generated bilingual lexicons 
with validated (reference bilingual lexicons) 
would further improve the overall performance 
of this aligner.  Yet, this might be a harder to 
meet condition for some pairs of languages than 
using parallel corpora. 
MEBA is more versatile as it does not require 
a-priori bilingual lexicons but, on the other hand, 
it is very sensitive to the values of the parameters 
that control its behaviour. Currently they are set 
according to the developers? intuition and after 
the analysis of the results from several trials. 
Since this activity is pretty time consuming (hu-
man analysis plus re-training might take a couple 
of hours) we plan to extend MEBA with a super-
vised learning module, which would automati-
cally determine the ?optimal? parameters 
(thresholds and weights) values. 
It is worth noticing that with the current ver-
sions of our basic aligners, significantly im-
proved since the ACL shared word alignment 
task in June 2005, YAWA is now doing better 
than MEBA, and the COWAL F-measure in-
creased with 9.4%. However, as mentioned be-
fore, these performances were measured on a 
different tokenization of the evaluation texts and 
on the partially corrected gold standard align-
ment (see footnote 1).  
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert J. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter 
estimation. Computational Linguistics, 19(2): 263?
311.
Thomas G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10 (7) 
1895-1924.
Rong-en Fan, Pai-Hsuen Chen, Chij-Jen Lin. 2005. 
Working set selection using the second order 
information for training SVM. Technical report, 
Department of Computer Science, National Taiwan 
University (www.csie.ntu.edu.tw/~cjlin/papers/
quadworkset.pdf).
William A. Gale, Kenneth W. Church. 1991. Identify-
ing word correspondences in parallel texts. In Pro-
ceedings of the Fourth DARPA Workshop on 
Speech and Natural Language. Asilomar, CA:152?
157.
Radu Ion. 2006. TTL: A portable framework for to-
kenization, tagging and lemmatization of large cor-
pora. PhD thesis progress report. Research Institute 
for Artificial Intelligence, Romanian Academy, 
Bucharest (in Romanian), 22p. 
Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. Cambridge, MA, MIT Press. 
Rada Mihalcea, Ted Pedersen. 2003. An Evaluation 
Exercise for Word Alignment. Proceedings of the 
HLT-NAACL 2003 Workshop: Building and Using 
Parallel Texts Data Driven Machine Translation 
and Beyond. Edmonton, Canada: 1?10. 
Joel Martin, Rada Mihalcea, Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proceeding of the ACL2005 Workshop 
on ?Building and Using Parallel Corpora: Data-
driven Machine Translation and Beyond?. June,
2005, Ann Arbor, Michigan, June, Association for 
Computational Linguistics, 65?74 
Robert Moore. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora in Machine Trans-
lation: From Research to Real Users. In Proceed-
ings of the 5th Conference of the Association for 
Machine Translation in the Americas, Tiburon, 
California), Springer-Verlag, Heidelberg, Ger-
many: 135-244. 
Franz J. Och, Herman Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, 29(1):19-51. 
Franz J. Och, Herman Ney. 2000. Improved Statistical 
Alignment Models. In Proceedings of the 38th Con-
ference of ACL, Hong Kong: 440-447. 
Joerg Tiedemann. 2003. Combining clues for word 
alignment. In Proceedings of the 10th EACL, Bu-
dapest, Hungary: 339?346. 
Dan Tufi?. 2002. A cheap and fast way to build useful 
translation lexicons. In Proceedings of COL-
ING2002, Taipei, China: 1030-1036. 
Dan Tufi?, Ana-Maria Barbu, Radu Ion. 2003. TREQ-
AL: A word-alignment system with limited lan-
guage resources. In Proceedings of the NAACL 
2003 Workshop on Building and Using Parallel 
Texts; Romanian-English Shared Task, Edmonton, 
Canada: 36-39. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan Ste-
f?nescu. 2005. Combined Aligners. In Proceeding
of the ACL2005 Workshop on ?Building and Using 
Parallel Corpora: Data-driven Machine Transla-
tion and Beyond?. June, 2005, Ann Arbor, Michi-
gan, June, Association for Computational Linguis-
tics, pp. 107-110. 
Dan Tufi?, Radu Ion. 2005. Multiple Sense Invento-
ries and Test-Bed Corpora. In C. Burileanu (ed.) 
Trends in Speech Technology, Publishing House of 
the Romanian Academy, Bucharest: 49-58.
160
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 107?110,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Combined word alignments  
 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan ?tef?nescu 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis, radu, alceusu, danstef}@racai.ro 
 
 
Abstract
 
We briefly describe a word alignment system 
that combines two different methods in bitext 
correspondences identification. The first one is 
a hypotheses testing approach (Gale and 
Church, 1991; Melamed, 2001; Tufi? 2002) 
while the second one is closer to a model 
estimating approach (Brown et al, 1993; Och 
and Ney, 2000). We show that combining the 
two aligners the results are significantly 
improved as compared to each individual 
aligner. 
 
Introduction 
In (Tufi?, 2002) we described a translation equivalence 
extraction program called TREQ the development of 
which was twofold motivated: to help enriching the 
synsets of the Romanian wordnet (Tufi? et al 2004a) 
with new literals based on bilingual corpora evidence 
and to check the interlingual alignment of our wordnet 
against the Princeton Wordnet. The translation 
equivalence extractor has been also incorporated into a 
WSD system (Tufi? et al, 2004b) part of a semantic 
web annotation platform. It also constituted the 
backbone of our TREQ-AL word aligner which 
successfully participated in the previous HLT-NAACL 
2003 Shared Task1 on word alignment for Romanian-
English parallel texts. A detailed description of 
TREQ&TREQ-AL is given in (Tufi? et al 2003b) and it 
will be very shortly overviewed. 
A quite different approach from our hypotheses 
testing implemented in the TREQ-AL aligner is taken 
by the model-estimating aligners, most of them relying 
on the IBM models (1 to 5) described in the (Brown et 
al. 1993) seminal paper. The first wide-spread and 
publicly available implementation of the IBM models 
was the GIZA program, which itself was part of the 
SMT toolkit EGYPT (Al-Onaizan et al, 1999). GIZA 
has been superseded by its recent extension GIZA++ 
(Och and Ney, 2000, 2003) publicly available2. We used 
the translation probabilities generated by GIZA++ for 
implementing a second aligner, MEBA, described in a 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared  
2 http://www.fjoch.com/GIZA++.2003-09-30.tar.gz 
little more details in a subsequent section. The 
alignments produced by MEBA were compared to the 
ones produced by TREQ-AL. We used for comparison 
the Gold Standard3 annotation from the HLT-NAACL 
2003 Shared Task. In order to combine the two aligners 
we had to check whether their accuracy was comparable 
and that when they are wrong the set of mistakes made 
by one aligner is not a proper set of the errors made by 
the second one. The first check was performed by using 
McNamer?s test  (Dieterich, 1998) and for the second 
we used Brill &Wu test (Brill, Wu, 1998). Both tests 
confirmed that the conditions for combining were 
ensured so, we built the combiner.  
The Combined Word Aligner, COWAL, is a 
wrapper of the two aligners (TREQ-AL and MEBA) 
ensuring the pre- and post-processing. It is 
complemented by a graphical user interface that allows 
for the visualisation of the alignments (intermediary and 
the final ones) as well as for their editing. We should 
note that the corrections made by the user are stored by 
COWAL as positive and negative examples for word 
dependencies (in the monolingual context) and 
translation equivalencies (in the bilingual context). In 
the current version the editorial logs are used by the 
human developers but we plan to further extend 
COWAL for automatic learning from this extremely 
valuable kind of data.    
 
The bitext processing  
The two base aligners and their combination use the 
same format for the input data and provide the 
alignments in the same format. The input format is 
obtained from two raw texts which represent reciprocal 
translations. If not already sentence aligned, the two 
texts are aligned. In the shared task this step was not 
necessary since both the training data and evaluation 
data were provided in the sentence aligned format.  
The texts in each language are then tokenized with 
the MULTEXT multilingual tokenizer4. The tokenizer is 
a finite state automaton using language specific 
                                                 
3 We noticed in the Gold Standard two sentences where 
alignments were wrongly shifted by one position (due to an 
unprintable character) and we corrected them.  
4 http://aune.lpl.univ-aix.fr:16080/projects/multext/MtSeg/  
107
 resources. It recognizes several compounds (phrasal 
verbs, idioms, dates) and split contrasted or cliticized 
constructions. This tokenization considerably differs 
from the one prescribed by the Shared Task where a 
token is any character string delimited by a blank or a 
punctuation sign (which itself is considered a token).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to original evaluation data (and consequently 
re-index) all the linking pairs. After tokenization, both 
texts are tagged and lemmatized.  We used in-house 
language models and lemmatizers and the Brants?s TnT 
tagger5. For both English and Romanian we used 
MULTEXT-EAST6 compliant tagsets. With different 
tags, a tagset mapping table becomes an obligatory 
external resource. Although, more often than not, the 
translation equivalents have the same part-of speech, 
relying on such a restriction would seriously affect the 
alignment recall. However, when the translation 
equivalents have different parts of speech, this 
difference is not arbitrary.  During the training phase we 
estimated bilingual POS affinities:{p(POSmRO| POSnEN)} 
and {p(POSnEN|POSmRO)}. POS affinities were used as 
one of the information sources in dealing with 
competitive alignments.  
The next preprocessing step is represented by a 
rather primitive form of sentence chunking in both 
languages. They roughly correspond to (non-recursive) 
noun phrases, adjectival phrases, prepositional phrases 
and verb complexes (analytical realization of tense, 
aspect mood and diathesis and phrasal verbs).  The 
?chunks? are recognized by a set of regular expressions 
defined over the tagsets. Finally, the bitext is assembled 
as an XML document (XCES-Align-ana format), as 
used in the MULTEXT-EAST corpus, which is the 
standard input for most of our tools, including COWAL 
alignment platform. 
 
The three aligners  
TREQ-AL generates translation equivalence hypotheses 
for the pairs of words (one for each language in the 
parallel corpus) which have been observed occurring in 
aligned sentences more than expected by chance. The 
hypotheses are filtered by a loglikelihood score 
threshold. Several heuristics (string similarity-cognates, 
POS affinities and alignments locality7) are used in a 
                                                 
5 http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf  
6 http://nl.ijs.si/ME/V2/  
7 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
competitive linking manner (Melamed, 2001) to make 
the final decision on the most likely translation 
equivalents. Given that, initially, this program was 
designed for extracting translation equivalents for the 
alignment of the Romanian wordnet to the Princeton 
wordnet, it deals only with one to one mappings. To 
cope with the many to many mappings (especially for 
functional words alignment), the earlier version of the 
translation equivalence extractor encoded some general 
rules assumed to be valid over a large set of natural 
languages such as: auxiliaries and verbal particles 
(infinitive, subjunctive, aspectual and temporal) are 
related to the closest main verb, determiners (articles, 
pronominal adjectives, quantifiers) are related to the 
closest nominal category (noun or pronoun). Currently 
this part of the TREQ-AL code became redundant 
because the chunking module mentioned before does 
the same job in a more general and flexible way.  
MEBA is an iterative algorithm which uses the 
translation probabilities, distorsions and POS-affinities 
generated by GIZA++ and takes advantage of all 
preprocessing phases mentioned in the previous section. 
In each step are aligned different categories of tokens 
(content words, named entities, functional words) in 
decreasing order of statistical evidence. The score of a 
link is computed by a linear function of 7 parameters? 
scores: translation probability, POS affinity, string 
similarity, alignments locality (both strict and weaker 
versions) distortions and the entropy of the translation 
equivalents. For all these parameters, in each processing 
step, we empirically set minimal thresholds and various 
weights. The tokens considered for the computing 
translation probabilities are the lemmas trailed by the 
grammatical categories (eg. plane_N, plane_V 
plane_A). This way we aimed at avoiding data 
sparseness and filtering noisy data. For highly 
inflectional languages (as Romanian is) the use of 
lemmas instead of word occurrences contributes 
significantly to the data sparseness reduction. For 
languages with weak inflectional character (as English 
is) the POS trailing contributes especially to the filtering 
the search space. Each processing step is controlled by 
above mentioned parameters, the weights and thresholds 
of which vary from step to step (even the order of the 
processing steps is one of the possible parameters). 
The first alignment step builds only links with a 
high level of certainty (that is cognates, pairs of high 
translation probability and high POS affinity). The 
grammatical categories which are considered in this step 
are user controlled (usually nouns, adjectives or non-
auxiliary verbs and which have the fewest competitive 
translations). The next processing steps try to align 
                                                                             
requires that all alignment links starting from a chunk, in the 
one language end in a chunk in the other language. This 
restricted form of locality is relevant for related languages.  
108
 content words (open class categories) as confidently as 
possible, following the alignments in previous steps as 
anchor points. In all steps the candidates are considered 
if and only if they meet the minimal threshold 
restrictions. If the input bitext is chunked, the strict 
alignment locality heuristics is very effective to 
determine the correct alignment even for unseen pairs of 
words (or for which the translation equivalence 
probability is below the considered threshold). When 
the pre-chunking of the parallel texts is not available, 
MEBA uses the weaker form of the locality heuristics 
by analyzing the alignments already existing in a 
window of N tokens centered on the focused token. The 
window size is variable, proportional to the sentence 
length. For all alignments in the window, an average 
displacement is computed and, among the competing 
alignments, preference will be given to the links with 
displacement values closer to the average one.  
The functional words and punctuation are processed 
in the last step and their alignments are guided by the 
POS-affinities and alignment locality heuristics. If none 
of the alignment clues or their combination (Tiedemann, 
2003) is strong enough, the functional words are 
automatically aligned with the word(s) their governor is 
aligned to. The governor is chunk-based defined: it is 
the content word of a chunk (if there are more content 
words in a chunk, then the governor is the grammatical 
head). If the chunking is not available, the closest 
content word is selected as the governor. Proximity is 
checked to the left or to the right according to the 
frequencies of the POS-ngram containing the current 
functional word.  
We should mention that the probabilities computed 
during the training phase are not re-estimated for each 
run-time processing step. At run-time only the weights 
and thresholds change from step to step.  
COWAL, the combined aligner takes advantage of the 
alignments independently provided by TREQ-AL and 
MEBA. The simplest combination method consists in 
computing either the union (high recall, low precision), 
or the intersection (lower recall, higher precision) of the 
independent alignments. We evaluated both these 
simple methods of combination and found that the best 
F-measure was provided by the union-based 
combination. Although for the shared task we submitted 
the union-based combined alignment (Baseline 
COWAL, see Table 1), there are various ways to 
improve it. We discuss three cases where improvement 
is possible (C1, C2 and C3, see below) and which were 
evaluated after the submission deadline. The results of 
this (unofficial) evaluation are summarized in Table 1 
by the f-COWAL line. These cases refer to competing 
links that appeared after the union of the independent 
alignments. The conflicts resolution is based on the 
(weak) locality and distortion heuristics discussed 
before. The currently identified competing links are 
only those for which the following conditions apply: 
C1) if one aligner found for a word W a non-null 
alignment and the other aligner generated for the 
same word W a null link, then the baseline alignment 
contains an impossible situation: the token W is 
recorded both as translated and not-translated in the 
other language. The translation probabilities, POS 
affinity and the relative displacement of the tokens in 
the non-null candidates were the strongest decision 
criteria. We found that in about 60% of the cases the 
null alignments were mistaken. So, for the time being, 
we simply eliminated the null competing alignments 
(this should be addressed in a more principled way by 
the future version of the combiner).  
C2) long distant competing links; this case appears 
when one aligner found for the word Ws the link to 
the target word Wtm, the other aligner found for Ws 
the target Wtn, and the distance between Wtm and 
Wtn, is more than 3 words (in a future version this 
maximum distance will be a dynamic parameter, 
depending on the sentence length and the POS of 
Ws). 
C3) competing links to the same target(s) of a word 
occurring several times in the same sentence; 
consider, for example, the Romanian fragment:  
     ??la1 Neptun, la2 Orastie si la3 Afumati, ?   
     which in English is translated by the next segment: 
     ??in Neptun, Orastie and Afumati? 
In spite of the gold standard considering that all three 
occurrences of the preposition ?la? in Romanian (la1, 
la2 ,la3) are aligned to the same word in English (?in?), 
the filtering, in this case, licensed only the alignment 
?la1 <-> in?. We consider that this filtered alignment 
is correct, since omitting ?la2? and ?la3? does not alter 
the syntactic correctness of the Romanian text, and 
also because the insertion in the English fragment of 
the preposition ?in? before ?Orastie? and before 
?Afumati? wouldn?t alter the grammaticality of the 
English fragment. Since both repetitions and 
omissions are optional, we consider that only the first 
occurrence of the preposition (?la1?) is translated in 
English, while the others are omitted. 
Another possible improvement (not implemented yet) 
was revealed by observing that the final result contained 
several incomplete n-m (phrasal) alignments. It is likely 
that even an elementary n-gram analysis (both sides of 
the bitext) would bring valuable evidence for improving 
the phrasal alignments.  
 
Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
109
 of the links. Some multi-word expressions recognized 
by the tokenizer as one token, such as dates (25 
ianuarie, 2001), compound prepositions (de la, p?n? 
la), conjunctions (pentru ca, de c?nd, p?n? c?nd) or 
adverbs (de jur ?mprejur, ?n fa?a) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
The same hold for the other way around. Therefore if 
two multiword expressions were initially found to be 
translation equivalents (one alignment link) after the 
post-processing number of  generated links became 
N*M, where N represented the number of words in the 
first language compound and M the number of words in 
the second language compound.   
Evaluation and conclusions 
Neither TREQ-AL nor MEBA needs an a priori 
bilingual dictionary, as this will be automatically 
extracted by the TREQ or GIZA++. We made 
evaluation of the individual alignments in both 
experimental settings: without a startup bilingual 
lexicon and with an initial mid-sized bilingual lexicon. 
Surprisingly enough, we found that while the 
performance of TREQ-AL increases a little bit (approx. 
1% increase of the F-measure) MEBA is doing better 
without an additional lexicon. So, in the evaluation 
below MEBA uses only the training data vocabulary.  
 
Aligner Precision Recall F-
meas. 
AER 
TREQ-AL 81.71 60.57 69.57 30.43 
MEBA 82.85 60.41 69.87 30.13 
Baseline 
(union)COWAL 
70.84 76.67 73.64 26.36 
f-COWAL 
(H1+H2+H3) 
87.17 70.25 77.80 22.20 
         Table 1. Evaluation results against the official GS 
After the release of the official Gold Standard we 
noticed and corrected some obvious errors and also 
removed the controversial links of the type c) discussed 
in the previous section. The evaluations against this new 
?Gold Standard? showed, on average, 3.5% better 
figures (precision, recall, F-measure and AER) for the 
individual aligners, while for the combined classifiers, 
the performance scores were about 4% better. 
MEBA is very sensitive to the values of the 
parameters which control its behavior. Currently they 
are set according to the developers? intuition and after 
the analysis of the results from several trials. Since this 
activity is pretty time consuming (human analysis plus 
re-training might take a couple of hours) we plan to 
extend MEBA with a supervised learning module, 
which would automatically determine the ?optimal? 
parameters (thresholds and weights) values. 
 
References 
Al-Onaizan, Y., Curin, J., Jahr, M., Knight K., Lafferty, J., 
Melamed, D., Och, F. J., Purdy, D., Smith, N.A., 
Yarowsky, D. (1999) : Statistical Machine 
Translation, Final Report, JHU Workshop, 42 pages 
Brill, E., and Wu, J. (1998). ?Classifier Combination for 
Improved Lexical Disambiguation? In Proceedings of 
COLING-ACL?98  Montreal, Canada, 191-195 
Brown, P. F., Della Pietra, S.A.,  Della Pietra, V. J., 
Mercer, R. L.(1993) ?The mathematics of statistical 
machine translation: Parameter estimation?. 
Computational Linguistics, 19(2) pp. 263?311. 
Dietterich, T. G., (1998). ?Approximate Statistical Tests 
for Comparing Supervised Classification Learning 
Algorithms?. Neural Computation, 10 (7) 1895-1924. 
Gale, W.A. and Church, K.W. (1991). ?Identifying word 
correspondences in parallel texts?. Proceedings of the 
Fourth DARPA Workshop on Speech and Natural 
Language. Asilomar, CA, pp. 152?157. 
Melamed, D. (2001). Empirical Methods for Exploiting 
Parallel Texts. Cambridge, MA: MIT Press. 
Och, F.J., Ney, H. (2003) "A Systematic Comparison of 
Various Statistical Alignment Models", Computa-
tional Linguistics, 29(1), pp. 19-51 
Och, F.J., Ney, H.(2000) "Improved Statistical Alignment 
Models". Proceedings of the 38th ACL, Hongkong,  
pp. 440-447 
Tiedemann, J. (2003) ?Combining clues for word 
alignment?. In Proceedings of the 10th EACL, 
Budapest, pp. 339?346 
Tufi?, D.(2002) ?A cheap and fast way to build useful 
translation lexicons?. Proceedings of COLING2002, 
Taipei, pp. 1030-1036. 
Tufi?, D., Barbu, A.M., Ion R (2003).: ?TREQ-AL: A 
word-alignment system with limited language 
resources?, Proceedings of the NAACL 2003 
Workshop on Building and Using Parallel Texts; 
Romanian-English Shared Task, Edmonton, pp. 36-39 
Tufi?, D., Ion, R., Ide, N.(2004a): Fine-Grained Word 
Sense Disambiguation Based on Parallel Corpora, 
Word Alignment, Word Clustering and Aligned 
Wordnets. Proceedings of COLING2004, Geneva, pp. 
1312-1318 
Tufis, D., Barbu, E., Mititelu, V., Ion, R., Bozianu, 
L.(2004b): ?The Romanian Wordnet?.  In Romanian 
Journal on Information Science and Technology, Dan 
Tufi? (ed.) Special Issue on BalkaNet, Romanian 
Academy, 7(2-3), pp. 105-122.  
110
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 91?96,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
ACCURAT Toolkit for Multi-Level Alignment and  
Information Extraction from Comparable Corpora 
 
M?rcis Pinnis1, Radu Ion2, Dan ?tef?nescu2, Fangzhong Su3, 
Inguna Skadi?a1, Andrejs Vasi?jevs1, Bogdan Babych3 
 1Tilde, Vien?bas gatve 75a, Riga, Latvia 
{marcis.pinnis,inguna.skadina,andrejs}@tilde.lv 
 
2Research Institute for Artificial Intelligence, Romanian Academy 
{radu,danstef}@racai.ro 
 
3Centre for Translation Studies, University of Leeds 
{f.su,b.babych}@leeds.ac.uk 
 
 
Abstract 
The lack of parallel corpora and linguistic 
resources for many languages and domains is 
one of the major obstacles for the further 
advancement of automated translation. A 
possible solution is to exploit comparable 
corpora (non-parallel bi- or multi-lingual text 
resources) which are much more widely 
available than parallel translation data. Our 
presented toolkit deals with parallel content 
extraction from comparable corpora. It consists 
of tools bundled in two workflows: (1) 
alignment of comparable documents and 
extraction of parallel sentences and (2) 
extraction and bilingual mapping of terms and 
named entities. The toolkit pairs similar 
bilingual comparable documents and extracts 
parallel sentences and bilingual terminological 
and named entity dictionaries from comparable 
corpora. This demonstration focuses on the 
English, Latvian, Lithuanian, and Romanian 
languages. 
Introduction 
In recent decades, data-driven approaches have 
significantly advanced the development of 
machine translation (MT). However, lack of 
sufficient bilingual linguistic resources for many 
languages and domains is still one of the major 
obstacles for further advancement of automated 
translation. At the same time, comparable corpora, 
i.e., non-parallel bi- or multilingual text resources 
such as daily news articles and large knowledge 
bases like Wikipedia, are much more widely 
available than parallel translation data.  
While methods for the use of parallel corpora in 
machine translation are well studied (Koehn, 
2010), similar techniques for comparable corpora 
have not been thoroughly worked out. Only the 
latest research has shown that language pairs and 
domains with little parallel data can benefit from 
the exploitation of comparable corpora (Munteanu 
and Marcu, 2005; Lu et al, 2010; Smith et al, 
2010; Abdul-Rauf and Schwenk, 2009 and 2011). 
In this paper we present the ACCURAT 
toolkit1 - a collection of tools that are capable of  
analysing comparable corpora and extracting 
parallel data which can be used to improve the 
performance of statistical and rule/example-based 
MT systems. 
Although the toolkit may be used for parallel 
data acquisition for open (broad) domain systems, 
it will be most beneficial for under-resourced 
languages or specific domains which are not 
covered by available parallel resources. 
The ACCURAT toolkit produces: 
? comparable document pairs with 
comparability scores, allowing to estimate 
the overall comparability of corpora; 
? parallel sentences which can be used as 
additional parallel data sources for 
statistical translation model learning; 
                                                          
1 http://www.accurat-project.eu/ 
91
? terminology dictionaries ? this type of 
data is expected to improve domain-
dependent translation; 
? named entity dictionaries. 
The demonstration showcases two general use 
case scenarios defined in the toolkit: ?parallel data 
mining from comparable corpora? and ?named 
entity/terminology extraction and mapping from 
comparable corpora?. 
The next section provides a general overview of 
workflows followed by descriptions of methods 
and tools integrated in the workflows. 
1 Overview of the Workflows 
The toolkit?s tools are integrated within two 
workflows (visualised in Figure 1). 
 
  
Figure 1. Workflows of the ACCURAT toolkit. 
 
The workflow for parallel data mining from 
comparable corpora aligns comparable corpora in 
the document level (section 2.1). This step is 
crucial as the further steps are computationally 
intensive. To minimise search space, documents 
are aligned with possible candidates that are likely 
to contain parallel data. Then parallel sentence 
pairs are extracted from the aligned comparable 
corpora (section 2.2). 
The workflow for named entity (NE) and 
terminology extraction and mapping from 
comparable corpora extracts data in a dictionary-
like format. Providing a list of document pairs, the 
workflow tags NEs or terms in all documents using 
language specific taggers (named entity 
recognisers (NER) or term extractors) and 
performs multi-lingual NE (section 2.3) or term 
mapping (section 2.4), thereby producing bilingual 
NE or term dictionaries. The workflow also 
accepts pre-processed documents, thus skipping 
the tagging process. 
Since all tools use command line interfaces, task 
automation and workflow specification can be 
done with simple console/terminal scripts. All 
tools can be run on the Windows operating system 
(some are also platform independent). 
2 Tools and Methods 
This section provides an overview of the main 
tools and methods in the toolkit. A full list of tools 
is described in ACCURAT D2.6. (2011). 
2.1 Comparability Metrics 
We define comparability by how useful a pair of 
documents is for parallel data extraction. The 
higher the comparability score, the more likely two 
documents contain more overlapping parallel data. 
The methods are developed to perform lightweight 
comparability estimation that minimises search 
space of relatively large corpora (e.g., 10,000 
documents in each language). There are two 
comparability metric tools in the toolkit: a 
translation based and a dictionary based metric. 
The Translation based metric (Su and Babych, 
2012a) uses MT APIs for document translation 
into English. Then four independent similarity 
feature functions are applied to a document pair: 
? Lexical feature ? both documents are pre-
processed (tokenised, lemmatised, and 
stop-words are filtered) and then 
vectorised. The lexical overlap score is 
calculated as a cosine similarity function 
over the vectors of two documents. 
? Structural feature ? the difference of 
sentence counts and content word counts 
(equally interpolated). 
? Keyword feature ? the cosine similarity 
of top 20 keywords. 
? NE feature ? the cosine similarity of NEs 
(extracted using Stanford NER). 
These similarity measures are linearly combined in 
a final comparability score. This is implemented by 
a simple weighted average strategy, in which each 
92
type of feature is associated with a weight 
indicating its relative confidence or importance. 
The comparability scores are normalised on a scale 
of 0 to 1, where a higher comparability score 
indicates a higher comparability level. 
The reliability of the proposed metric has been 
evaluated on a gold standard of comparable 
corpora for 11 language pairs (Skadi?a et al, 
2010). The gold standard consists of news articles, 
legal documents, knowledge-base articles, user 
manuals, and medical documents. Document pairs 
in the gold standard were rated by human judges as 
being parallel, strongly comparable, or weakly 
comparable. The evaluation results suggest that the 
comparability scores reliably reflect comparability 
levels. In addition, there is a strong correlation 
between human defined comparability levels and 
the confidence scores derived from the 
comparability metric, as the Pearson R correlation 
scores vary between 0.966 and 0.999, depending 
on the language pair.  
The Dictionary based metric (Su and Babych, 
2012b) is a lightweight approach, which uses 
bilingual dictionaries to lexically map documents 
from one language to another. The dictionaries are 
automatically generated via word alignment using 
GIZA++ (Och and Ney, 2000) on parallel corpora. 
For each word in the source language, the top two 
translation candidates (based on the word 
alignment probability in GIZA++) are retrieved as 
possible translations into the target language. This 
metric provides a much faster lexical translation 
process, although word-for-word lexical mapping 
produces less reliable translations than MT based 
translations. Moreover, the lower quality of text 
translation in the dictionary based metric does not 
necessarily degrade its performance in predicting 
comparability levels of comparable document 
pairs. The evaluation on the gold standard shows a 
strong correlation (between 0.883 and 0.999) 
between human defined comparability levels and 
the confidence scores of the metric. 
2.2 Parallel Sentence Extractor from 
Comparable Corpora 
Phrase-based statistical translation models are 
among the most successful translation models that 
currently exist (Callison-Burch et al, 2010). 
Usually, phrases are extracted from parallel 
corpora by means of symmetrical word alignment 
and/or by phrase generation (Koehn et al, 2003). 
Our toolkit exploits comparable corpora in order to 
find and extract comparable sentences for SMT 
training using a tool named LEXACC (?tef?nescu 
et al, 2012). 
LEXACC requires aligned document pairs (also 
m to n alignments) for sentence extraction. It also 
allows extraction from comparable corpora as a 
whole; however, precision may decrease due to 
larger search space. 
LEXACC scores sentence pairs according to five 
lexical overlap and structural matching feature 
functions. These functions are combined using 
linear interpolation with weights trained for each 
language pair and direction using logistic 
regression. The feature functions are: 
? a lexical (translation) overlap score for 
content words (nouns, verbs, adjectives, 
and adverbs) using GIZA++ (Gao and 
Vogel, 2008) format dictionaries; 
? a lexical (translation) overlap score for 
functional words (all except content 
words) constrained by the content word 
alignment from the previous feature; 
? the alignment obliqueness score, a measure 
that quantifies the degree to which the 
relative positions of source and target 
aligned words differ; 
? a score indicating whether strong content 
word translations are found at the 
beginning and the end of each sentence in 
the given pair; 
? a punctuation score which indicates 
whether the sentences have identical 
sentence ending punctuation. 
For different language pairs, the relevance of 
the individual feature functions differ. For 
instance, the locality feature is more important for 
the English-Romanian pair than for the English-
Greek pair. Therefore, the weights are trained on 
parallel corpora (in our case - 10,000 pairs). 
LEXACC does not score every sentence pair in 
the Cartesian product between source and target 
document sentences. It reduces the search space 
using two filtering steps (?tef?nescu et al, 2012). 
The first step makes use of the Cross-Language 
Information Retrieval framework and uses a search 
engine to find sentences in the target corpus that 
are the most probable translations of a given 
sentence. In the second step (which is optional), 
93
the resulting candidates are further filtered, and 
those that do not meet minimum requirements are 
eliminated.  
To work for a certain language pair, LEXACC 
needs additional resources: (i) a GIZA++-like 
translation dictionary, (ii) lists of stop-words in 
both languages, and (iii) lists of word suffixes in 
both languages (used for stemming). 
The performance of LEXACC, regarding 
precision and recall, can be controlled by a 
threshold applied to the overall interpolated 
parallelism score. The tool has been evaluated on 
news article comparable corpora. Table 1 shows 
results achieved by LEXACC with different 
parallelism thresholds on automatically crawled 
English-Latvian corpora, consisting of 41,914 
unique English sentences and 10,058 unique 
Latvian sentences. 
 
Threshold Aligned pairs Precision 
Useful 
pairs 
0.25 1036 39.19% 406 
0.3 813 48.22% 392 
0.4 553 63.47% 351 
0.5 395 76.96% 304 
0.6 272 84.19% 229 
0.7 151 88.74% 134 
0.8 27 88.89% 24 
0.9 0 - 0 
 
Table 1. English-Latvian parallel sentence extraction 
results on a comparable news corpus. 
 
Threshold Aligned pairs Precision Useful pairs
0.2 2324 10.32% 240 
0.3 1105 28.50% 315 
0.4 722 53.46% 386 
0.5 532 89.28% 475 
0.6 389 100% 389 
0.7 532 100% 532 
0.8 386 100% 386 
0.9 20 100% 20 
 
Table 2. English-Romanian parallel sentence extraction 
results on a comparable news corpus. 
Table 2 shows results for English-Romanian on 
corpora consisting of 310,740 unique English and 
81,433 unique Romanian sentences. 
Useful pairs denote the total number of parallel 
and strongly comparable sentence pairs (at least 
80% of the source sentence is a translation in the 
target sentence). The corpora size is given only as 
an indicative figure, as the amount of extracted 
parallel data greatly depends on the comparability 
of the corpora. 
2.3 Named Entity Extraction and Mapping 
The second workflow of the toolkit allows NE and 
terminology extraction and mapping. Starting with 
named entity recognition, the toolkit features the 
first NER systems for Latvian and Lithuanian 
(Pinnis, 2012). It also contains NER systems for 
English (through an OpenNLP NER2 wrapper) and 
Romanian (NERA). In order to map named entities, 
documents have to be tagged with NER systems 
that support MUC-7 format NE SGML tags.  
The toolkit contains the mapping tool NERA2. 
The mapper requires comparable corpora aligned 
in the document level as input. NERA2 compares 
each NE from the source language to each NE 
from the target language using cognate based 
methods. It also uses a GIZA++ format statistical 
dictionary to map NEs containing common nouns 
that are frequent in location names. This approach 
allows frequent NE mapping if the cognate based 
method fails, therefore, allowing increasing the 
recall of the mapper. Precision and recall can be 
tuned with a confidence score threshold. 
2.4 Terminology Mapping 
During recent years, automatic bilingual term 
mapping in comparable corpora has received 
greater attention in light of the scarcity of parallel 
data for under-resourced languages. Several 
methods have been applied to this task, e.g., 
contextual analysis (Rapp, 1995; Fung and 
McKeown, 1997) and compositional analysis 
(Daille and Morin, 2008). Symbolic, statistical, and 
hybrid techniques have been implemented for 
bilingual lexicon extraction (Morin and 
Prochasson, 2011). 
Our terminology mapper is designed to map 
terms extracted from comparable or parallel 
                                                          
2 Open NLP - http://incubator.apache.org/opennlp/. 
94
documents. The method is language independent 
and can be applied if a translation equivalents table 
exists for a language pair. As input, the application 
requires term-tagged bilingual corpora aligned in 
the document level. 
The toolkit includes term-tagging tools for 
English, Latvian, Lithuanian, and Romanian, but 
can be easily extended for other languages if a 
POS-tagger, a phrase pattern list, a stop-word list, 
and an inverse document frequency list (calculated 
on balanced corpora) are available. 
The aligner maps terms based on two criteria 
(Pinnis et al, 2012; ?tef?nescu, 2012): (i) a 
GIZA++-like translation equivalents table and (ii) 
string similarity in terms of Levenshtein distance 
between term candidates.  For evaluation, Eurovoc 
(Steinberger et al, 2002) was used. Tables 4 and 5 
show the performance figures of the mapper for 
English-Romanian and English-Latvian. 
 
Threshold P R F-measure
0.3 0.562 0.194 0.288 
0.4 0.759 0.295 0.425 
0.5 0.904 0.357 0.511 
0.6 0.964 0.298 0.456 
0.7 0.986 0.216 0.359 
0.8 0.996 0.151 0.263 
0.9 0.995 0.084 0.154 
 
Table 3. Term mapping performance for English-
Romanian. 
 
Threshold P R F-measure 
0.3 0.636 0.210 0.316 
0.4 0.833 0.285 0.425 
0.5 0.947 0.306 0.463 
0.6 0.981 0.235 0.379 
0.7 0.996 0.160 0.275 
0.8 0.996 0.099 0.181 
0.9 0.997 0.057 0.107 
 
Table 4. Term mapping performance for English-
Latvian. 
3 Conclusions and Related Information 
This demonstration paper describes the 
ACCURAT toolkit containing tools for multi-level 
alignment and information extraction from 
comparable corpora. These tools are integrated in 
predefined workflows that are ready for immediate 
use. The workflows provide functionality for the 
extraction of parallel sentences, bilingual NE 
dictionaries, and bilingual term dictionaries from 
comparable corpora. 
The methods, including comparability metrics, 
parallel sentence extraction and named entity/term 
mapping, are language independent. However, they 
may require language dependent resources, for 
instance, POS-taggers, Giza++ translation 
dictionaries, NERs, term taggers, etc.3 
 The ACCURAT toolkit is released under the 
Apache 2.0 licence and is freely available for 
download after completing a registration form4.  
Acknowledgements 
The research within the project ACCURAT 
leading to these results has received funding from 
the European Union Seventh Framework 
Programme (FP7/2007-2013), grant agreement no 
248347. 
References  
Sadaf Abdul-Rauf and Holger Schwenk. On the use of 
comparable corpora to improve SMT performance. 
EACL 2009: Proceedings of the 12th conference of 
the European Chapter of the Association for 
Computational Linguistics, Athens, Greece, 16-23. 
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Parallel 
sentence generation from comparable corpora for 
improved SMT. Machine Translation, 25(4): 341-
375. 
ACCURAT D2.6 2011. Toolkit for multi-level 
alignment and information extraction from 
comparable corpora (http://www.accurat-project.eu). 
Dan Gusfield. 1997. Algorithms on strings, trees and 
sequences. Cambridge University Press. 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Kay Peterson, Mark Przybocki and Omar Zaidan. 
2010. Findings of the 2010 Joint Workshop on 
Statistical Machine Translation and Metrics for 
Machine Translation. Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
MetricsMATR, 17-53. 
B?atrice Daille and Emmanuel Morin. 2008. Effective 
compositional model for lexical alignment. 
Proceedings of the 3rd International Joint Conference 
                                                          
3 Full requirements are defined in the documentation of each 
tool (ACCURAT D2.6, 2011). 
4 http://www.accurat-project.eu/index.php?p=toolkit 
95
on Natural Language Processing, Hyderabad, India, 
95-102. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. Proceedings of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, 440-447. 
Pascale Fung and Kathleen Mckeown. 1997. Finding 
terminology translations from non-parallel corpora. 
Proceedings of the 5th Annual Workshop on Very 
Large Corpora, 192-202. 
Qin Gao and Stephan Vogel. 2008. Parallel 
implementations of a word alignment tool. 
Proceedings of ACL-08 HLT: Software Engineering, 
Testing, and Quality Assurance for Natural Language 
Processing, June 20, 2008. The Ohio State 
University, Columbus, Ohio, USA, 49-57. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. 
Proceedings of the Human Language Technology and 
North American Association for Computational 
Linguistics Conference (HLT/NAACL), May 27-
June 1, Edmonton, Canada. 
Philip Koehn. 2010. Statistical machine translation, 
Cambridge University Press. 
Bin Lu, Tao Jiang, Kapo Chow and Benjamin K. Tsou. 
2010. Building a large English-Chinese parallel 
corpus from comparable patents and its experimental 
application to SMT. Proceedings of the 3rd workshop 
on building and using comparable corpora: from 
parallel to non-parallel corpora, Valletta, Malta, 42-
48. 
Drago? ?tefan Munteanu and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
nonparallel corpora. ACL-44: Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Morristown, NJ, USA, 81-88. 
Emmanuel Morin and Emmanuel Prochasson. 2011. 
Bilingual lexicon extraction from comparable 
corpora enhanced with parallel corpora. ACL HLT 
2011, 27-34. 
M?rcis Pinnis. 2012. Latvian and Lithuanian named 
entity recognition with TildeNER. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey. 
M?rcis Pinnis, Nikola Ljube?i?, Dan ?tef?nescu, Inguna 
Skadi?a, Marko Tadi?, Tatiana Gornostay. 2012. 
Term extraction, tagging, and mapping tools for 
under-resourced languages. Proceedings of the 10th 
Conference on Terminology and Knowledge 
Engineering (TKE 2012), June 20-21, Madrid, Spain. 
Reinhard Rapp. 1995. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd annual 
meeting on Association for Computational 
Linguistics, 320-322.  
Jason R. Smith, Chris Quirk, and Kristina Toutanova. 
2010.  Extracting parallel sentences from comparable 
corpora using document level alignment. Proceedings 
of NAACL 2010, Los Angeles, USA. 
Dan ?tef?nescu. 2012. Mining for term translations in 
comparable corpora. Proceedings of the 5th 
Workshop on Building and Using Comparable 
Corpora (BUCC 2012) to be held at the 8th edition of 
Language Resources and Evaluation Conference 
(LREC 2012), Istanbul, Turkey, May 23-25, 2012. 
Ralf Steinberger, Bruno Pouliquen and Johan Hagman. 
2002. Cross-lingual document similarity calculation 
using the multilingual thesaurus Eurovoc. 
Proceedings of the 3rd International Conference on 
Computational Linguistics and Intelligent Text 
Processing (CICLing '02), Springer-Verlag London, 
UK, ISBN:3-540-43219-1. 
Inguna Skadi?a, Ahmet Aker, Voula Giouli, Dan Tufis, 
Rob Gaizauskas, Madara Mieri?a and Nikos 
Mastropavlos. 2010. Collection of comparable 
corpora for under-resourced languages. In 
Proceedings of the Fourth International Conference 
Baltic HLT 2010, IOS Press, Frontiers in Artificial 
Intelligence and Applications, Vol. 219, pp. 161-168. 
Fangzhong Su and Bogdan Babych. 2012a. 
Development and application of a cross-language 
document comparability metric. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey.  
Fangzhong Su and Bogdan Babych.  2012b. Measuring 
comparability of documents in non-parallel corpora 
for efficient extraction of (semi-) parallel translation 
equivalents. Proceedings of  EACL'12 joint 
workshop on Exploiting Synergies between 
Information Retrieval and Machine Translation 
(ESIRMT) and Hybrid Approaches to Machine 
Translation (HyTra), Avignon, France.  
Dan ?tef?nescu, Radu Ion and Sabine Hunsicker. 2012. 
Hybrid parallel sentence mining from comparable 
corpora. Proceedings of the 16th Conference of the 
European Association for Machine Translation 
(EAMT 2012), Trento, Italy.  
96
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 163?168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
SEMILAR: The Semantic Similarity Toolkit 
 
Vasile Rus, Mihai Lintean, Rajendra Banjade, Nobal Niraula, and Dan Stefanescu 
Department of Computer Science 
The University of Memphis 
Memphis, TN 38152 
{vrus,rbanjade,mclinten,nbnraula,dstfnscu}@memphis.edu 
 
 
Abstract 
We present in this paper SEMILAR, the SE-
Mantic simILARity toolkit. SEMILAR im-
plements a number of algorithms for assessing 
the semantic similarity between two texts. It is 
available as a Java library and as a Java 
standalone ap-plication offering GUI-based 
access to the implemented semantic similarity 
methods. Furthermore, it offers facilities for 
manual se-mantic similarity annotation by ex-
perts through its component SEMILAT (a 
SEMantic simILarity Annotation Tool). 
1 Introduction 
We present in this paper the design and im-
plementation of SEMILAR, the SEMantic 
simILARity toolkit. SEMILAR 
(www.semanticsimilarity.org) includes im-
plementations of a number of algorithms pro-
posed over the last decade or so to address 
various instances of the general problem of 
text-to-text semantic similarity. Semantic sim-
ilarity is an approach to language understand-
ing that is widely used in real applications. It 
is a practical alternative to the true under-
standing approach, which is intractable as it 
requires world knowledge, a yet to-be-solved 
problem in Artificial Intelligence. 
Text A: York had no problem with MTA?s in-
sisting the decision to shift funds had been within 
its legal rights. 
Text B: York had no problem with MTA?s say-
ing the decision to shift funds was within its 
powers. 
 
Given such two texts, the paraphrase identifi-
cation task is about automatically assessing 
whether Text A is a paraphrase of, i.e. has the 
same meaning as, Text B. The example above is 
a positive instance, meaning that Text A is a par-
aphrase of Text B and vice versa. 
The importance of semantic similarity in Nat-
ural Language Processing (NLP) is highlighted 
by the diversity of datasets and shared task eval-
uation campaigns (STECs) that have been pro-
posed over the last decade (Dolan, Quirk, and 
Brockett, 2004; McCarthy & McNamara, 2008; 
Agirre et al, 2012). These datasets include in-
stances from various applications.  Indeed, there 
is a need to identify and quantify semantic rela-
tions between texts in many applications. For 
instance, paraphrase identification, an instance of 
the semantic similarity problem, is an important 
step in a number of applications including Natu-
ral Language Generation, Question Answering, 
and dialogue-based Intelligent Tutoring Systems. 
In Natural Language Generation, paraphrases are 
a method to increase diversity of generated text 
(Iordanskaja et al 1991). In Question Answer-
ing, multiple answers that are paraphrases of 
each other could be considered as evidence for 
the correctness of the answer (Ibrahim et al 
2003). In Intelligent Tutoring Sys-tems (Rus et 
al., 2009; Lintean et al, 2010; Lintean, 2011), 
paraphrase identification is useful to assess 
whether students? articulated answers to deep 
questions (e.g. conceptual physics questions) are 
similar-to/paraphrases-of ideal answers. 
Generally, the problem of semantic similarity 
between two texts, denoted text A and text B, is 
defined as quantifying and identifying the pres-
ence of semantic relations between the two texts, 
e.g. to what extent text A has the same meaning 
as or is a paraphrase of text B (paraphrase rela-
tion; Dolan, Quirk, and Brockett, 2004). Other 
semantic relations that have been investigated 
systematically in the recent past are entailment, 
i.e. to what extent text A entails or logically in-
fers text B (Dagan, Glickman, & Magnini, 2004), 
and elaboration, i.e. is text B is an elaboration of 
text A? (McCarthy & McNamara, 2008). 
163
Semantic similarity can be broadly construed 
between texts of any size. Depending on the 
granularity of the texts, we can talk about the 
following fundamental text-to-text similarity 
problems: word-to-word similarity, phrase-to-
phrase similarity, sentence-to-sentence similari-
ty, paragraph-to-paragraph similarity, or docu-
ment-to-document similarity. Mixed combina-
tions are also possible such as assessing the simi-
larity of a word to a sentence or a sentence to a 
paragraph. For instance, in summarization it 
might be useful to assess how well a sentence 
summarizes an entire paragraph. 
2 Motivation 
The problem of word-to-word similarity has been 
extensively studied over the past decades and a 
word-to-word similarity library (WordNet Simi-
larity) has been developed by Pedersen and col-
leagues (Pedersen, Patwardhan, & Michelizzi, 
2004). 
Methods to assess the semantic similarity of 
larger texts, in particular sentences, have been 
proposed over the last decade (Corley and 
Mihalcea, 2005; Fernando & Stevenson, 2008; 
Rus, Lintean, Graesser, & McNamara 2009). 
Androutsopoulos & Malakasiotis (2010) com-
piled a survey of methods for paraphrasing and 
entailment semantic relation identification at sen-
tence level. Despite all the proposed methods to 
assess semantic similarity between two texts, no 
semantic similarity library or toolkit, similar to 
the WordNet library for word-to-word similarity, 
exists for larger texts. Given the importance of 
semantic similarity, there is an acute need for 
such a library and toolkit. The developed SEMI-
LAR library and toolkit presented here fulfill this 
need. 
In particular, the development of the semantic 
similarity toolkit SEMILAR has been motivated 
by the need for an integrated environment that 
would provide:  
 
? easy access to implementations of various 
semantic similarity approaches from the 
same user-friendly interface and/or library. 
? easy access to semantic similarity methods 
that work at different levels of text granulari-
ty: word-to-word, sentence-to-sentence, par-
agraph-to-paragraph, document-to-
document, or a combination (SEMILAR in-
tegrates word-to-word similarity measures). 
? authoring methods for semantic similarity. 
? a common environment for that allows sys-
tematic and fair comparison of semantic sim-
ilarity methods. 
? facilities to manually annotate texts with se-
mantic similarity relations using a graphical 
user interface that make such annotations 
easier for experts (this component is called 
SEMILAT component - a SEMantic similari-
ty Annotation Tool). 
 
SEMILAR is thus a one-stop-shop for investi-
gating, annotating, and authoring methods for the 
semantic similarity of texts of any level of granu-
larity. 
3 SEMILAR: The Semantic Similarity 
Toolkit 
The authors of the SEMILAR toolkit (see Figure 
1) have been involved in assessing the semantic 
Figure 1. Snapshot of SEMILAR. The Data View tab is shown. 
164
similarity of texts for more than a decade. During 
this time, they have conducted a careful require-
ments analysis for an integrated software toolkit 
that would integrate various methods for seman-
tic similarity assessment. The result of this effort 
is the prototype presented here. We briefly pre-
sent the components of SEMILAR next and then 
describe in more detail the core component of 
SEMILAR, i.e. the set of semantic similarity 
methods that are currently available. It should be 
noted that we are continuously adding new se-
mantic similarity methods and features to SEMI-
LAR. 
The SEMILAR toolkit includes the following 
components: project management; data view-
browsing-visualization; preprocessing (e.g., col-
location identification, part-of-speech tagging, 
phrase or dependency parsing, etc.), semantic 
similarity methods (word-level and sentence-
level), classification components for qualitative 
decision making with respect to textual semantic 
relations (na?ve Bayes, Decision Trees, Support 
Vector Machines, and Neural Network), kernel-
based methods (sequence kernels, word sequence 
kernels, and tree kernels; as of this writing, we 
are still implementing several other tree kernel 
methods); debugging and testing facilities for 
model selection; and annotation components (al-
lows domain expert to manually annotate texts 
with semantic relations using GUI-based facili-
ties; Rus et al, 2012). For space reasons, we only 
detail next the main algorithms in the core com-
ponent, i.e. the major text-to-text similarity algo-
rithms currently available in SEMILAR. 
4 The Semantic Similarity Methods 
Available in SEMILAR 
The core component of SEMILAR is a set of 
text-to-text semantic similarity methods. We 
have implemented methods that handle both uni-
directional similarity measures as well as bidirec-
tional similarity measures. For instance, the se-
mantic relation of entailment between two texts 
is unidirectional (a text T logically entails a hy-
pothesis text H but H does not entail T) while the 
paraphrase relation is bidirectional (text A has 
same meaning as text B and vice versa). 
Lexical Overlap. Given two texts, the sim-
plest method to assess their semantic similarity is 
to compute lexical overlap, i.e. how many words 
they have in common. There are many lexical 
overlap variations. Indeed, a closer look at lexi-
cal overlap reveals a number of parameters that 
turns the simple lexical overlap problem into a 
large space of possibilities. The parameters in-
clude preprocessing options (collocation detec-
tion, punctuation, stopword removal, etc.), filter-
ing options (all words, content words, etc.), 
weighting schemes (global vs. local weighting, 
binary weighting, etc.), and normalization factors 
(largest text, weighted average, etc.). A total of 
3,456 variants of lexical overlap can be generat-
ed by different parameter settings in SEMILAR. 
Lintean (2011) has shown that performance on 
lexical overlap methods on the tasks of para-
phrase identification and textual entailment tasks 
can vary significantly depending on the selected 
parameters. Some lexical overlap variations lead 
to performance results rivaling more sophisticat-
ed, state-of-the-art methods. 
It should be noted that the overlap category of 
methods can be extended to include N-gram 
overlap methods (see the N-gram overlap meth-
ods proposed by the Machine Translation com-
munity such as BLEU and METEOR). SEMI-
LAR offers bigram and unigram overlap methods 
including the BLEU and METEOR scores. 
A natural approach to text-to-text similarity 
methods is to rely on word-to-word similarity 
measures. Many of the methods presented next 
compute the similarity of larger texts using indi-
vidual word similarities. 
Mihalcea, Corley, & Strappavara (2006; 
MCS) proposed a greedy method based on word-
to-word similarity measures. For each word in 
text A (or B) the maximum similarity score to 
any word in the other text B (or A) is used. An 
idf-weighted average is then computed as shown 
in the equation below. 
 
)
}
2
{
)(
}
2
{
)}(*)
1
,(max{
}
1
{
)(
}
1
{
)}(*)
2
,(max{
(
2
1
)2,1(
?
?
?
?
?
?
?
?
?
?
Tw
widf
Tw
widfTwSim
Tw
widf
Tw
widfTwSim
TTsim
 
 
The word-to-word similarity function sim(w, 
T) in the equation above can be instantiated to 
any word-to-word similarity measure (e.g. 
WordNet similarities or Latent Semantic Analy-
sis). The vast majority of word-to-word similari-
ty measures that rely on WordNet are concept-to-
concept measures and to be able to use them one 
must map words in the input texts onto concepts 
in WordNet, i.e. word sense disambiguation 
(WSD) is needed. As of this writing, SEMILAR 
addresses the issue in two simple ways: (1) se-
165
lecting the most frequent sense for each word, 
which is sense #1 in WordNet, and (2) using all 
the senses for each word and then take the max-
imum (or average) of the relatedness scores for 
each pair of word senses. We label the former 
method as ONE (sense one), whereas the latter is 
labeled as ALL-MAX or ALL-AVG (all senses 
maximum score or all senses average score, re-
spectively). Furthermore, most WordNet-based 
measures only work within a part-of-speech cat-
egory, e.g. only between nouns. 
Other types of word-to-word measures, such 
as those based on Latent Semantic Analysis or 
Latent Dirichlet Allocation, do not have a word-
sense disambiguation challenge.  
Rus and Lintean (2012; Rus-Lintean-
Optimal Matching or ROM) proposed an opti-
mal solution for text-to-text similarity based on 
word-to-word similarity measures. The optimal 
lexical matching is based on the optimal assign-
ment problem, a fundamental combinatorial op-
timization problem which consists of finding a 
maximum weight matching in a weighted bipar-
tite graph.  
Given a weighted complete bipartite graph 
, where edge  has weight 
, the optimal assignment problem is to 
find a matching M from X to Y with maximum 
weight. 
A typical application is about assigning a 
group of workers, e.g. words in text A in our 
case, to a set of jobs (words in text B in our case) 
based on the expertise level, measured by 
, of each worker at each job. By adding 
dummy workers or jobs we may assume that X 
and Y have the same size, n, and can be viewed 
as   and Y = . 
In the semantic similarity case, the weight  
is the word-to-word similarity between a word x 
in text A and a word y in text B.  
The assignment problem can also be stated as 
finding a permutation  of {1, 2, 3, ? , n} for 
which  is maximum. Such an 
assignment is called optimum assignment. The 
Kuhn-Munkres algorithm (Kuhn, 1955) can find 
a solution to the optimum assignment problem in 
polynomial time. 
Rus and colleagues (Rus et al, 2009; Rus & 
Graesser, 2006; Rus-Syntax-Negation or RSN) 
used a lexical overlap component combined with 
syntactic overlap and negation handling to com-
pute an unidirectional subsumption score be-
tween two sentences, T (Text) and H (Hypothe-
sis), in entailment recognition and student input 
assessment in Intelligent Tutoring Systems. Each 
text is regarded as a graph with words as 
nodes/vertices and syntactic dependencies as 
edges. The subsumption score reflects how much 
a text is subsumed or contained by another. The 
equation below provides the overall subsumption 
score, which can be averaged both ways to com-
pute a similarity score, as opposed to just the 
subsumption score, between the two texts.  
 
2
)
_#
)1(1(
)
||
),(max
||
),(max
(),(
relneg
h
E
eHh
E
tEh
Ematch
eTtE
h
V
vHh
V
tVh
Vmatch
vTtV
HTsubsump
??
?
?
?
?
??
?
?
?
??
?
?
 
The lexical component can be used by itself 
(given a weight of 1 with the syntactic compo-
nent given a weight of 0) in which case the simi-
larity between the two texts is just a composi-
tional extension of word-to-word similarity 
measures. The match function in the equation 
can be any word-to-word similarity measure in-
cluding simple word match, WordNet similarity 
measures, LSA, or LDA-based similarity 
measures. 
Fernando and Stevenson (FST; 2008) pro-
posed a method in which similarities among all 
pairs of words are taken into account for compu-
ting the similarity of two texts. Each text is rep-
resented as a binary vector (1 ? the word occurs 
in the text; 0 ? the word does not occur in the 
text). They use a similarity matrix operator W 
that contains word-to-word similarities between 
any two words. 
||||
),( ??
??
?
??
ba
TbWa
basim
 
Each element wij represents the word-level 
semantic similarity between word ai in text A 
and word bj in text B. Any word-to-word seman-
tic similarity measure can be used. 
Lintean and Rus (2010; weighted-LSA or 
wLSA) extensively studied methods for semantic 
similarity based on Latent Semantic Analysis 
(LSA; Landauer et al, 2006). LSA represents 
words as vectors in a 300-500 dimensional LSA 
space. An LSA vector for larger texts can be de-
rived by vector algebra, e.g. by summing up the 
individual words? vectors. The similarity of two 
texts A and B can be computed using the cosine 
(normalized dot product) of their LSA vectors. 
Alternatively, the individual word vectors can be 
166
combined through weighted sums. Lintean and 
Rus (2010) experimented with a combination of 
3 local weights and 3 global weights. All these 
versions of LSA-based text-to-text similarity 
measures are available in SEMILAR. 
SEMILAR also includes a set of similarity 
measures based on the unsupervised method La-
tent Dirichlet Allocation (LDA; Blei, Ng, & 
Jordnan, 2003; Rus, Banjade, & Niraula, 
2013). LDA is a probabilistic generative model 
in which documents are viewed as distributions 
over a set of topics (?d - text d?s distribution over 
topics) and topics are distributions over words (?t 
? topic t?s distribution over words). That is, each 
word in a document is generated from a distribu-
tion over words that is specific to each topic. 
A first LDA-based semantic similarity meas-
ure among words would then be defined as a dot-
product between the corresponding vectors rep-
resenting the contributions of each word to a top-
ic (?t(w) ? represents the probability of word w 
in topic t). It should be noted that the contribu-
tions of each word to the topics does not consti-
tute a distribution, i.e. the sum of contributions is 
not 1. Assuming the number of topics T, then a 
simple word-to-word measure is defined by the 
formula below. 
 
  
 
 
 
More global text-to-text similarity measures could 
be defined in several ways as detailed next.  
Because in LDA a document is a distribution 
over topics, the similarity of two texts needs to 
be computed in terms of similarity of distribu-
tions. The Kullback-Leibler (KL) divergence 
defines a distance, or how dissimilar, two distri-
butions p and q are as in the formula below. 
 
 
 
 
 
If we replace p with ?d (text/document d?s dis-
tribution over topics) and q with ?c 
(text/document c?s distribution over topics) we 
obtain the KL distance between two documents 
(documents d and c in our example). The KL 
distance has two major problems. In case qi is 
zero KL is not defined. Then, KL is not symmet-
ric. The Information Radius measure (IR) solves 
these problems by considering the average of pi 
and qi as below. Also, the IR can be transformed 
into a symmetric similarity measure as in the fol-
lowing (Dagan, Lee, & Pereira, 1997): 
 
 
 
The Hellinger and Manhattan distances be-
tween two distributions are two other options 
that avoid the shortcomings of the KL distance. 
Both are options are implemented in SEMILAR. 
LDA similarity measures between two docu-
ments or texts c and d can also include similarity 
of topics. That is, the text-to-text similarity is 
obtained multiplying the similarities between the 
distribution over topics (?d and ?c) and distribu-
tion over words (?t1 and ?t2). The similarity of 
topics can be computed using the same methods 
illustrated above as the topics are distributions 
over words (for all the details see Rus, Banjade, 
& Niraula, 2013). 
The last semantic similarity method presented 
in this paper is based on the Quadratic Assign-
ment Problem (QAP). The QAP method aims at 
finding an optimal assignment from words in text 
A to words in text B, based on individual word-
to-word similarity measures, while simultaneous-
ly maximizing the match between the syntactic 
dependencies of the words. 
The Koopmans-Beckmann (1957) formulation 
of the QAP problem best fits this purpose. The 
goal of the original QAP formulation, in the do-
main of economic activity, was to minimize the 
objective function QAP shown below where ma-
trix F describes the flow between any two facili-
ties, matrix D indicates the distances between 
locations, and matrix B provides the cost of lo-
cating facilities to specific locations. F, D, and B 
are symmetric and non-negative. 
 
?? ??????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,1 )()(,),,(min ???
 
 
The fi,j term denotes the flow between facili-
ties i and j which are placed at locations ?(i) and 
?(j), respectively. The distance between these 
locations is d?(i)?(j). In our case, F and D describe 
dependencies between words in one sentence 
while B captures the word-to-word similarity 
between words in opposite sentences. Also, we 
have weighted each term in the above formula-
tion and instead of minimizing the sum we are 
maximizing it resulting in the formulation below.  
 
?? ???????
n
i
n
i iib
n
j jidjifBDFQAP 1 1 )(,)1(1 )()(,),,(max ?????
 
 
?
?
?? T
t
tt vwvwwwLDA
1
)()(),(2 ??
),(10),( dcIRqpSIM ???
?
?
? T
i i
i
i q
ppqpKL
1
log),(
167
5 Discussion and Conclusions 
The above methods were experimented with on 
various datasets for paraphrase, entailment, and 
elaboration. For paraphrase identification, the 
QAP method provides best accuracy results 
(=77.6%) on the test subset of the Microsoft Re-
search Paraphrase corpus, one of the largest par-
aphrase datasets. 
 Due to space constraints, we have not de-
scribed all the features available in SEMILAR. 
For a complete list of features, latest news, refer-
ences, and updates of the SEMILAR toolkit 
along with downloadable resources including 
software and data files, the reader can visit this 
link: www.semanticsimilarity.org. 
 
Acknowledgments 
This research was supported in part by Institute 
for Education Sciences under award 
R305A100875. Any opinions, findings, and con-
clusions or recommendations expressed in this 
material are solely the authors? and do not neces-
sarily reflect the views of the sponsoring agency. 
References  
Androutsopoulos, I. & Malakasiotis, P. 2010. A sur-
vey of paraphrasing and textual entailment meth-
ods. Journal of Artificial Intelligence Research, 
38:135-187. 
Agirre, E., Cer, D., Diab, M., & Gonzalez-Agirre, A. 
(2012). SemEval-2012 Task 6: A Pilot on Semantic 
Textual Similarity, First Joint Conference on Lexi-
cal and Computational Semantics (*SEM), Mon-
treal, Canada, June 7-8, 2012. 
Blei, D.M., Ng, A.Y., & Jordan, M.I. 2003. Latent 
dirichlet alocation, The Journal of Machine Learn-
ing Research 3, 993-1022. 
Corley, C., & Mihalcea, R. (2005). Measuring the 
Semantic Similarity of Texts. In Proceedings of the 
ACL Workshop on Empirical Modeling of Seman-
tic Equivalence and Entailment. Ann Arbor, MI. 
Dagan, I., Glickman, O., & Magnini, B. (2004). The 
PASCAL Recognising textual entailment Chal-
lenge. In Quinorero-Candela, J.; Dagan, I.; Magni-
ni, B.; d'Alche-Buc, F. (Eds.), Machine Learning 
Challenges. Lecture Notes in Computer Science, 
Vol. 3944, pp. 177-190, Springer, 2006. 
Dolan, B., Quirk, C., & Brockett, C. (2004). Unsuper-
vised construction of large paraphrase corpora: Ex-
ploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on 
Computational Linguistics (COLING-2004), Gene-
va, Switzerland. 
Fernando, S. & Stevenson, M. (2008). A semantic 
similarity approach to paraphrase detec-
tion, Computational Linguistics UK (CLUK 2008) 
11th Annual Research Colloquium. 
Lintean, M., Moldovan, C., Rus, V., & McNamara D. 
(2010). The Role of Local and Global Weighting in 
Assessing the Semantic Similarity of Texts Using 
Latent Semantic Analysis. Proceedings of the 23rd 
International Florida Artificial Intelligence Re-
search Society Conference. Daytona Beach, FL. 
Lintean, M. (2011). Measuring Semantic Similarity: 
Representations and Methods, PhD Thesis, De-
partment of Computer Science, The University of 
Memphis, 2011.  
Ibrahim, A., Katz, B., & Lin, J. (2003). Extracting 
structural paraphrases from aligned monolingual 
corpora In Proceedings of the Second International 
Workshop on Paraphrasing, (ACL 2003). 
Iordanskaja, L., Kittredge, R., & Polgere, A. (1991). 
Natural Language Generation in Artificial Intelli-
gence and Computational Linguistics. Lexical se-
lection and paraphrase in a meaning-text genera-
tion model, Kluwer Academic. 
McCarthy, P.M. & McNamara, D.S. (2008). User-
Language Paraphrase Corpus Challenge 
https://umdrive.memphis.edu/pmmccrth/public/Par
aphraseCorpus/Paraphrase site.htm. Retrieved 
2/20/2010 online, 2009. 
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). 
WordNet::Similarity - Measuring the Relatedness 
of Concepts, In the Proceedings of the Nineteenth 
National Conference on Artificial Intelligence 
(AAAI-04), pp. 1024-1025, July 25-29, 2004, San 
Jose, CA (Intelligent Systems Demonstration). 
Rus, V., Lintean M., Graesser, A.C., & McNamara, 
D.S. (2009). Assessing Student Paraphrases Using 
Lexical Semantics and Word Weighting. In Pro-
ceedings of the 14th International Conference on 
Artificial Intelligence in Education, Brighton, UK. 
Rus, V., Lintean, M., Moldovan, C., Baggett, W., 
Niraula, N., Morgan, B. (2012). The SIMILAR 
Corpus: A Resource to Foster the Qualitative Un-
derstanding of Semantic Similarity of Texts, In 
Semantic Relations II: Enhancing Resources and 
Applications, The 8th Language Resources and 
Evaluation Conference (LREC 2012), May 23-25, 
Instanbul, Turkey. 
Rus, V., Banjade, R., & Niraula, N. (2013). Similarity 
Measures based on Latent Dirichlet Allocation, 
The 14th International Conference on Intelligent 
Text Procesing and Computational Linguistics, 
March 24-30, 2013, Samos, Greece. 
 
168
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 411?416,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
RACAI: Unsupervised WSD experiments @ SemEval-2, Task #17 
 
 
Radu Ion 
Institute for AI, Romanian Academy 
13, Calea 13 Septembrie, Bucharest 
050711, Romania 
radu@racai.ro 
Dan ?tef?nescu 
Institute for AI, Romanian Academy 
13, Calea 13 Septembrie, Bucharest 
050711, Romania 
danstef@racai.ro 
 
 
Abstract 
This paper documents the participation of the 
Research Institute for Artificial Intelligence of 
the Romanian Academy (RACAI) to the Task 
17 ? All-words Word Sense Disambiguation 
on a Specific Domain, of the SemEval-2 com-
petition. We describe three unsupervised WSD 
systems that make extensive use of the Prince-
ton WordNet (WN) structure and WordNet 
Domains in order to perform the disambigua-
tion. The best of them has been ranked the 12th 
by the task organizers out of 29 judged runs. 
1 Introduction 
Referring to the last SemEval (SemEval-1, 
(Agirre et al, 2007a)) and to our recent work 
(Ion and ?tef?nescu, 2009), unsupervised Word 
Sense Disambiguation (WSD) is still at the bot-
tom of WSD systems ranking with a significant 
loss in performance when compared to super-
vised approaches. With Task #17 @ SemEval-2, 
this observation is (probably 1 ) reinforced but 
another issue is re-brought to light: the difficulty 
of supervised WSD systems to adapt to a given 
domain (Agirre et al, 2009). With general scores 
lower with at least 3% than 3 years ago in Task 
#17 @ SemEval-1 which was a supposedly hard-
er task  (general, no particular domain WSD was 
required for all words), we observe that super-
vised WSD is certainly more difficult to imple-
ment in a real world application. 
Our unsupervised WSD approach benefited 
from the specification of this year?s Task #17 
which was a domain-limited WSD, meaning that 
the disambiguation would be applied to content 
words drawn from a specific domain: the sur-
rounding environment. We worked under the 
assumption that a term of the given domain 
                                                 
1 At the time of the writing we only know the systems rank-
ing without the supervised/unsupervised distinction. 
would have the same meaning with all its occur-
rences throughout the text. This hypothesis has 
been put forth by Yarowsky (1993) as the ?one 
sense per discourse? hypothesis (OSPD for 
short). 
The task organizers offered a set of back-
ground documents with no sense annotations to 
the competitors who want to train/tune their sys-
tems using data from the same domain as the 
official test set. Working with the OSPD hypo-
thesis, we set off to construct/test domain specif-
ic WSD models from/on this corpus using the 
WordNet Domains (Bentivogli et al, 2004). For 
testing purposes, we have constructed an in-
house gold standard from this corpus that com-
prises of 1601 occurrences of 204 terms of the 
?surrounding environment? domain that have 
been automatically extracted with the highest 
confidence. We have observed that our gold 
standard (which has been independently anno-
tated by 3 annotators but on non-overlapping 
sections which led to having no inter-annotator 
agreement scores) obeys the OSPD hypothesis 
which we think that is appropriate to domain-
limited WSD. 
In what follows, we will briefly acknowledge 
the usage of WordNet Domains in WSD, we will 
then describe the construction of the corpus of 
the background documents including here the 
creation of an in-house gold standard, we will 
then briefly describe our three WSD algorithms 
and finally we will conclude with a discussion on 
the ranking of our runs among the 29 evaluated 
by the task organizers. 
2 Related Work 
WordNet Domains is a hierarchy of labels that 
have been assigned to WN synsets in a one to 
(possible) many relationship (but the frequent 
case is a single WN domain for a synset). A do-
main is the name of an area of knowledge that is 
recognized as unitary (Bentivogli et al, 2004). 
411
Thus labels such as ?architecture?, ?sport? or 
?medicine? are mapped onto synsets like 
?arch(4)-noun?, ?playing(2)-noun? or ?chron-
ic(1)-adjective? because of the fact that the re-
spective concept evokes the domain. 
WordNet Domains have been used in various 
ways to perform WSD. The main usage of this 
mapping is that the domains naturally create a 
clustering of the WN senses of a literal thus of-
fering a sense inventory that is much coarser than 
the fine sense distinctions of WN. For instance, 
senses 1 (?a flat-bottomed motor vehicle that can 
travel on land or water?) and 2 (?an airplane 
designed to take off and land on water?) of the 
noun ?amphibian? are both mapped to the do-
main ?transport? but the 3rd sense of the same 
noun is mapped onto the domains ?ani-
mals/biology? being the ?cold-blooded verte-
brate typically living on land but breeding in 
water; aquatic larvae undergo metamorphosis 
into adult form? (definitions from version 2.0 of 
the WN). 
V?zquez et al (2004) use WordNet Domains 
to derive a new resource they call the Relevant 
Domains in which, using WordNet glosses, they 
extract the most representative words for a given 
domain. Thus, for a word w and a domain d, the 
Association Ratio formula between w and d is 
)(P
)|(Plog)|(P),(AR 2 w
dwdwdw ?=  
in which, for each synset its gloss has been POS 
tagged and lemmatized. The probabilities are 
computed counting pairs dw, in glosses (each 
gloss has an associated d domain via its synset). 
Using the Relevant Domains, the WSD proce-
dure for a given word w in its context C (a 100 
words window centered in w), computes a simi-
larity measure between two vectors of AR 
scores: the first vector is the vector of AR scores 
of the sentence in which w appears and the other 
is the vector of domain scores computed for the 
gloss of a sense of w (both vectors are norma-
lized such that they contain the same domains). 
The highest similarity gives the sense of w that is 
closest to the domain vector of C. With this me-
thod, V?zquez et al obtain a precision of 0.54 
and a recall of 0.43 at the SensEval-2, English 
All-Words Task placing them in the 10th position 
out of 22 systems where the best one (a super-
vised system) achieved a 0.69 precision and an 
equal recall. 
Another approach to WSD using the WordNet 
Domains is that of Magnini et al (2002). The 
method is remarkably similar to the previous one 
in that the description of the vectors and the se-
lection of the assigned sense is the same. What 
differs, is the weights that are assigned to each 
domain in the vector. Magnini et al distinguish 
between text vectors (C vectors in the previous 
presentation) and sense vectors. Text (or context) 
vector weights are computed comparing domain 
frequency in the context with the domain fre-
quency over the entire corpus (see Magnini et al 
(2002) for details). Sense vectors are derived 
from sense-annotated data which qualifies this 
method as a supervised one. The results that have 
been reported at the same task the previous algo-
rithm participated (SensEval-2, English All-
Words Task), are: precision 0.748 and recall 
0.357 (12th place). 
Both the methods presented here are very sim-
ple and easy to adapt to different domains. One 
of our methods (RACAI-1, see below) is even 
simpler (because it makes the OSPD simplifying 
assumption) and performs with approximately 
the same accuracy as any of these methods judg-
ing by the rank of the system and the total num-
ber of participants.  
3 Using the Background Documents 
collection  
Task #17 organizers have offered a set of back-
ground documents for training/tuning/testing 
purposes. The corpus consists of 124 files from 
the ?surrounding environment? domain that have 
been collected in the framework of the Kyoto 
Project (http://www.kyoto-project.eu/). 
First, we have assembled the files into a single 
corpus in order to be able to apply some cleaning 
procedures. These procedures involved the re-
moval of the paragraphs in which the proportion 
of letters (Perl character class ?[A-Za-z_-]?) 
was less than 0.8 because the text contained a lot 
of noise in form of lines of numbers and other 
symbols which probably belonged to tables. The 
next stage was to have the corpus POS-tagged, 
lemmatized and chunked using the TTL web ser-
vice (Tufi? et al, 2008). The resulting file is an 
XML encoded corpus which contains 136456 
sentences with 2654446 tokens out of which 
348896 are punctuation tokens. 
In order to test our domain constrained WSD 
algorithms, we decided to construct a test set 
with the same dimension as the official test set of 
about 2000 occurrences of content words specific 
to the ?surrounding environment? domain. In 
doing this, we have employed a simple term ex-
412
traction algorithm which considers that terms, as 
opposed to words that are not domain specific, 
are not evenly distributed throughout the corpus. 
To formalize this, the corpus is a vector of lem-
mas [ ]Nlll ,,,C 21 K=  and for each unique lem-
ma Njl j ??1, , we compute the mean of the 
absolute differences of its indexes in C as 
mjkj
j
Nkj llkmjmll
lf
kj
?<<??=?
?
=
?
?<? ,,,
1)(
1?
where )( jlf  is the frequency of jl  in C. We 
also compute the standard deviation of these dif-
ferences from the mean as 
2)(
)(
1
2
?
??
=
?
?<?
j
Nkj
lf
kj ?
?  
in the same conditions as above. 
With the mean and standard deviation of in-
dexes differences of a content word lemma com-
puted, we construct a list of all content word 
lemmas that is sorted in descending order by the 
quantity ?? / which we take as a measure of the 
evenness of a content word lemma distribution. 
Thus, lemmas that are in the top of this list are 
likely to be terms of the domain of the corpus (in 
our case, the ?surrounding environment? do-
main). Table 1 contains the first 20 automatically 
extracted terms along with their term score. 
Having the list of terms of our domain, we 
have selected the first ambiguous 210 (which 
have more than 1 sense in WN) and constructed 
a test set in which each term has (at least) 10 oc-
currences in order to obtain a test corpus with at 
least 2000 occurrences of the terms of the ?sur-
rounding environment? domain. A large part of 
these occurrences have been independently 
sense-annotated by 3 annotators which worked 
on disjoint sets of terms (70 terms each) in order 
to finish as soon as possible. In the end we ma-
naged to annotate 1601 occurrences correspond-
ing to 204 terms. 
When the gold standard for the test set was 
ready, we checked to see if the OSPD hypothesis 
holds. In order to determine if it does, we com-
puted the average number of annotated different 
senses per term which is 1.36. In addition, consi-
dering the fact that out of 204 annotated terms, 
145 are annotated with a single sense, we may 
state that in this case, the OSPD hypothesis 
holds. 
Term Score Term Score
gibbon 15.89 Oceanica 9.41
fleet 13.91 orangutan 9.19
sub-region 13.01 laurel 9.08
Amazon 12.41 coral 9.06
roundwood 12.26 polar 9.05
biocapacity 12.23 wrasse 8.80
footprint 11.68 reef 8.78
deen 11.45 snapper 8.67
dune 10.57 biofuel 8.53
grouper 9.67 vessel 8.35
 
Table 1: The first 20 automatically extracted terms of 
the ?surrounding environment? domain 
4 The Description of the Systems 
Since we are committed to assign a unique sense 
per word in the test set, we might as well try to 
automatically induce a WSD model from the 
background corpus in which, for each lemma 
along with its POS tag that also exists in WN, a 
single sense is listed that is derived from the cor-
pus. Then, for any test set of the same domain, 
the algorithm would give the sense from the 
WSD model to any of the occurrences of the 
lemma. 
What we actually did, was to find a list of 
most frequent 2 WN domains (frequency count 
extracted from the whole corpus) for each lemma 
with its POS tag, and using these, to list all 
senses of the lemma that are mapped onto these 2 
domains (thus obtaining a reduction of the aver-
age number of senses per word). The steps of the 
algorithm for the creation of the WSD model are: 
1. in the given corpus, for each lemma l 
and its POS-tag p normalized to WN 
POS notation (?n? for nouns, ?v? for 
verbs, ?a? for adjectives and ?b? for ad-
verbs), for each of its senses from WN, 
increase by 1 each frequency of each 
mapped domain; 
2. for each lemma l with its POS-tag p, re-
tain only those senses that map onto the 
most frequent 2 domains as determined 
by the frequency list from the first step. 
Using our 2.65M words background corpus to 
build such a model (Table 2 contains a sample), 
we have obtained a decrease in average ambigui-
ty degree (the average number of senses per con-
tent word lemma) from 2.43 to 2.14. If we set a 
threshold of at least 1 for the term score of the 
lemmas to be included into the WSD model 
(which selects 12062 lemmas, meaning about 1/3 
of all unique lemmas in the corpus), we obtain 
413
the same reduction thus contradicting our hypo-
thesis that the average ambiguity degree of terms 
would be reduced more than the average ambigu-
ity degree of all words in the corpus. This result 
might be due to the fact that the ?factotum? do-
main is very frequent (much more frequent than 
any of the other domains). 
 
Lemma POS:Total no. 
of WN senses 
First 2 selected 
domains 
Selected 
senses 
fish n:2 animals,biology
  
1 
Arctic n:1 geography 1 
coral n:4 chemistry,animals 2,3,4 
 
Table 2: A sample of the WSD model built from the 
background corpus 
 
In what follows, we will present our 3 systems 
that use WSD models derived from the test sets 
(both the in-house and the official ones). In the 
Results section we will explain this choice. 
4.1 RACAI-1: WordNet Domains-driven, 
Most Frequent Sense 
The first system, as its name suggests, is very 
simple: using the WSD model, it chooses the 
most frequent sense (MFS) of the lemma l with 
POS p according to WN (that is, the lowest num-
bered sense from the list of senses the lemma has 
in the WSD model).  
Trying this method on our in-house developed 
test set, we obtain encouraging results: the over-
all accuracy (precision is equal with the recall 
because all test set occurrences are tried) is at 
least 4% over the general MFS baseline (sense 
no. 1 in all cases). The Results section gives de-
tails. 
4.2 RACAI-2: The Lexical Chains Selection 
With this system, we have tried to select only 
one sense (not necessarily the most frequent one) 
of lemma l with POS p from the WSD model. 
The selection procedure is based on lexical 
chains computation between senses of the target 
word (the word to be disambiguated) and the 
content words in its sentence in a manner that 
will be explained below. 
We have used the lexical chains description 
and computation method described in (Ion and 
?tef?nescu, 2009). To reiterate, a lexical chain is 
not simply a set of topically related words but 
becomes a path of synsets in the WordNet hie-
rarchy. The lexical chain procedure is a function 
of two WN synsets, LXC(s1, s2), that returns a 
semantic relation path that one can follow to 
reach s2 from s1. On the path from s2 to s1 there 
are k synsets (k ? 0) and between 2 adjacent syn-
sets there is a WN semantic relation. Each lexical 
chain can be assigned a certain score that we in-
terpret as a measure of the semantic similarity 
(SS) between s1 and s2 (see (Ion and ?tef?nescu, 
2009) and (Moldovan and Novischi, 2002) for 
more details). Thus, the higher the value of 
SS(s1, s2), the higher the semantic similarity be-
tween s1 and s2. 
We have observed that using RACAI-1 on our 
in-house test set but allowing it to select the most 
frequent 2 senses of lemma l with POS p from 
the WSD model, we obtain a whopping 82% 
accuracy. With this observation, we tried to pro-
gram RACAI-2 to make a binary selection from 
the first 2 most frequent senses of lemma l with 
POS p from the WSD model in order to approach 
the 82% percent accuracy limit which would 
have been a very good result. The algorithm is as 
follows: for a lemma l with POS p and a lemma 
lc with POS pc from the context (sentence) of l, 
compute the best lexical chain between any of 
the first 2 senses of l and any of the first 2 senses 
of lc according to the WSD model. If the first 2 
senses of l are a and b and the first 2 senses of lc 
are x and y and the best lexical chain score has 
been found between a and y for instance, then 
credit sense a of l with SS(a, y). Sum over all lc 
from the context of l and select that sense of l 
which has a maximum semantic similarity with 
the context. 
4.3 RACAI-3: Interpretation-based Sense 
Assignment 
This system tries to generate all the possible 
sense assignments (called interpretations) to the 
lemmas in a sentence. Thus, in principle, for 
each content word lemma, all its WN senses are 
considered thus generating an exponential explo-
sion of the sense assignments that can be attri-
buted to a sentence. If we have N content word 
lemmas which have k senses on average, we ob-
tain a search space of kN interpretations which 
have to be scored. 
Using the observation mentioned above that 
the first 2 senses of a lemma according to the 
WSD model yields a performance of 82%, brings 
the search space to 2N but for a large N, it is still 
too big. 
The solution we adopted (besides considering 
the first 2 senses from the WSD model) consists 
in segmenting the input sentence in M indepen-
dent segments of 10 content word lemmas each, 
which will be processed independently, yielding 
414
a search space of at most 102?M of smaller in-
terpretations. The best interpretation per each 
segment would thus be a part of the best interpre-
tation of the sentence. Next, we describe how we 
score an interpretation. 
For each sense s of a lemma l with POS p 
(from the first 2 senses of l listed in the WSD 
model) we compute an associated set of content 
words (lemmas) from the following sources: 
? all content word lemmas extracted from 
the sense s corresponding gloss (disre-
garding the auxiliary verbs); 
? all literals of the synset in which lemma l 
with sense s exists; 
? all literals of the synsets that are linked 
with the synset l(s) by a relation of the fol-
lowing type: hypernym, near_antonym, 
eng_derivative, hyponym, meronym, ho-
lonym, similar_to, derived; 
? all content word lemmas extracted from 
the glosses corresponding to synsets that 
are linked with the l(s) synset by a relation 
of the following type: hypernym, 
eng_derivative, similar_to, derived; 
With this feature set V of a sense s belonging to 
lemma l with POS p, for a given interpretation (a 
specific assignment of senses to each lemma in a 
segment), its score S (initially 0) is computed 
iteratively (for two adjacent position i and i + 1 
in the segment) as 
111 VVV,VVSS +++ ???+? iiiii  
where the |X| function is the cardinality function 
on the set X and ?  is the assignment operator. 
5 Results 
In order to run our WSD algorithms, we had to 
extract WSD models. We tested the accuracy of 
the disambiguation (onto the in-house developed 
gold standard) with RACAI-1 and RACAI-2 sys-
tems (RACAI-3 was not ready at that time) with 
models extracted a) from the whole background 
corpus and b) from the in-house developed test 
set (named here the RACAI test set, see section 
3). The results are reported in Table 3 along with 
RACAI-1 system returning the first 2 senses of a 
lemma from the WSD model and the general 
MFS baseline. 
As we can see, the results with the WSD mod-
el extracted from the test set are marginally bet-
ter than the other results. This was the reason for 
which we chose to extract the WSD model from 
the official test set as opposed to using the WSD 
model extracted from the background corpus. 
 
 RACAI 
Test Set 
Background 
Corpus 
RACAI-1 0.647 0.644 
RACAI-1 (2 senses) 0.825 0.811 
RACAI-2 0.591 0.582 
MFS (sense no. 1) 0.602 0.602 
 
Table 3: RACAI systems results (accuracy) on the 
RACAI test set 
 
However, we did not research the possibility of 
adding the official test set to either the RACAI 
test set or the background corpus and extract 
WSD models from there. 
The official test set (named the SEMEVAL 
test set here) contains 1398 occurrences of con-
tent words for disambiguation, out of which 366 
are occurrences of verbs and 1032 are occur-
rences of nouns. These occurrences correspond 
to 428 lemmas. Inspecting these lemmas, we 
have found that there are many of them which 
are not domain specific (in our case, specific to 
the ?surrounding environment? domain). For 
instance, the verb to ?be? is at the top of the list 
with 99 occurrences. It is followed by the noun 
?index? with 32 occurrences and by the noun 
?network? with 22 occurrences. With fewer oc-
currences follow ?use?, ?include?, ?show?, ?pro-
vide?, ?part? and so on. Of course, the SEMEV-
AL test set includes proper terms of the designat-
ed domain such as ?area? (61 occurrences), 
?species? (58 occurrences), ?nature? (31 occur-
rences), ?ocean?, ?sea?, ?water?, ?planet?, etc. 
Table 4 lists our official results on the SE-
MEVAL test set. 
 
 Precision Recall Rank 
RACAI-1 0.461 0.46 #12 
RACAI-2 0.351 0.35 #25 
RACAI-3 0.433 0.431 #18 
MFS 0.505 0.505 #6  
 
Table 4: RACAI systems results (accuracy) on the 
SEMEVAL test set 
 
Precision is not equal to recall because of the fact 
that our POS tagger found two occurrences of the 
verb to ?be? as auxiliaries which were ignored. 
The column Rank indicates the place our systems 
have in a 29 run ranking of all systems that parti-
cipated in Task 17 ? All-words Word Sense Dis-
ambiguation on a Specific Domain, of the Se-
415
mEval-2 competition which was won by a sys-
tem that achieved a precision of 0.57 and a recall 
of 0.555.  
The differences with the runs on the RACAI 
test set are significant but this can be explained 
by the fact that our WordNet Domains WSD me-
thod cannot cope with general (domain indepen-
dent) WSD requirements in which the ?one sense 
per discourse? hypothesis does not necessarily 
hold. 
6 Conclusions 
Regarding the 3 systems that we entered in the 
Task #17 @ SemEval-2, we think that the lexical 
chains algorithm (RACAI-2) is the most promis-
ing even if it scored the lowest of the three. We 
attribute its poor performances to the lexical 
chains computation, especially to the weights of 
the WN semantic relations that make up a chain. 
Also, we will extend our research regarding the 
correctness of lexical chains (the degree to which 
a human judge will appreciate as correct or evoc-
ative or as common knowledge a semantic path 
between two synsets). 
We also want to check if our three systems 
make the same mistakes or not in order to devise 
a way in which we can combine their outputs.  
RACAI is at the second participation in the 
SemEval series of WSD competitions. We are 
committed to improving the unsupervised WSD 
technology which, we think, is more easily 
adaptable and usable in real world applications. 
We hope that SemEval-3 will reveal significant 
improvements in this direction. 
 
Acknowledgments 
The work reported here was supported by the 
Romanian Ministry of Education and Research 
through the STAR project (no. 742/19.01.2009). 
References  
Eneko Agirre, Llu?s M?rquez and Richard Wicen-
towski, Eds., 2007. Proceedings of Semeval-2007 
Workshop. Prague, Czech Republic: Association 
for Computational Linguistics, 2007. 
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, Piek Vos-
sen. 2009. SemEval-2010 Task 17: All-words Word 
Sense Disambiguation on a Specific Domain. In 
Proceedings of NAACL workshop on Semantic 
Evaluations (SEW-2009). Boulder,Colorado, 2009. 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini 
and Emanuele Pianta. 2004. Revising WordNet 
Domains Hierarchy: Semantics, Coverage, and 
Balancing. In COLING 2004 Workshop on "Multi-
lingual Linguistic Resources", Geneva, Switzer-
land, August 28, 2004, pp. 101-108. 
Radu Ion and Dan ?tef?nescu. 2009. Unsupervised 
Word Sense Disambiguation with Lexical Chains 
and Graph-based Context Formalization. In Zyg-
munt Vetulani, editor, Proceedings of the 4th Lan-
guage and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer 
Science and Linguistics, pages 190?194, Pozna?, 
Poland, November 6?8 2009. Wydawnictwo 
Pozna?skie Sp. 
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo, Alfio Gliozzo. 2002. The role of domain 
information in Word Sense Disambiguation. Natu-
ral Language Engineering, 8(4), 359?373, De-
cember 2002. 
Dan Moldovan and Adrian Novischi. 2002. Lexical 
chains for question answering. In Proceedings of 
the 19th International Conference on Computation-
al Linguistics, August 24 ? September 01, 2002, 
Taipei, Taiwan, pp. 1?7. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u and Dan 
?tef?nescu. 2008. RACAI's Linguistic Web Servic-
es. In Proceedings of the 6th Language Resources 
and Evaluation Conference ? LREC 2008, Marra-
kech, Morocco, May 2008. ELRA ? European 
Language Ressources Association. ISBN 2-
9517408-4-0. 
Sonia V?zquez, Andr?s Montoyo and German Ri-
gau. 2004. Using Relevant Domains Resource for 
Word Sense Disambiguation. In Proceedings of the 
International Conference on Artificial Intelligence 
(IC-AI'04), Las Vegas, Nevada, 2004. 
David Yarowsky. 1993. One sense per collocation. In 
ARPA Human Language Technology Workshop, 
pp. 266?271, Princeton, NJ, 1993. 
 
416
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 19?27,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Experiments with a Differential Semantics Annotation for WordNet 3.0 
 
 
Dan Tufi? Dan  ?tef?nescu 
Research Institute for Artificial Intelligence  
Romanian Academy 
Research Institute for Artificial Intelligence  
Romanian Academy 
Calea ?13 Septembrie?, no.13 Calea ?13 Septembrie?, no.13 
Bucharest 5, 050711, Romania Bucharest 5, 050711, Romania 
tufis@racai.ro danstef@racai.ro 
  
 
Abstract 
This article reports on the methodology and 
the development of a complementary 
information source for the meaning of the 
synsets of Princeton WordNet 3.0. This 
encoded information was built following 
the principles of the Osgoodian differential 
semantics theory and consists of numerical 
values which represent the scaling of the 
connotative meanings along the multiple 
dimensions defined by pairs of antonyms 
(factors). Depending on the selected 
factors, various facets of connotative 
meanings come under scrutiny and 
different types of textual subjective 
analysis may be conducted (opinion 
mining, sentiment analysis). 
1 Introduction 
According to ?Semantic Differential? theory 
(Osgood et al, 1957), the connotative meaning of 
most adjectives can be, both qualitatively and 
quantitatively, differentiated along a scale, the ends 
of which are antonymic adjectives. Such a pair of 
antonymic adjectives is called a factor. The 
intensive experiments Osgood and his colleagues 
made with their students1 outlined that most of the 
variance in the text judgment was explained by 
only three major factors: the evaluative factor (e.g., 
good-bad), the potency factor (e.g., strong-weak), 
and the activity factor (e.g., active-passive).   
                                                           
1
 The students were asked to rate the meaning of words, 
phrases, or texts on different scales defined in terms of pairs 
of bipolar adjectives such as good-bad, active-passive, 
strong-weak, optimistic-pessimistic, beautiful-ugly, etc.) 
Kamps and Marx (2002) implemented a 
WordNet-based method in the spirit of the theory 
of semantic differentials and proposed a method to 
assess the?attitude? of arbitrary texts. In their 
approach, a text unit is regarded as a bag of words 
and the overall scoring of the sentence is obtained 
by combining the scores for the individual words 
of the text. Depending on the selected factor, 
various facets of subjective meanings come under 
scrutiny.  
The inspiring work of Kamps and Marx still has 
several limitations. The majority of researchers 
working on subjectivity agree that the subjectivity 
load of a given word is dependent on the senses of 
the respective word (Andreevskaia and Bergler, 
2006), (Bentivogli et al, 2004), (Mihalcea et al, 
2007), (Valiutti et al, 2004) and many others.; yet, 
in Kamps and Marx?s model (KMM, henceforth), 
because they work with words and not word-
senses, the sense distinctions are lost, making it 
impossible to assign different scores to different 
senses of the word in case. Going up from the level 
of word to the level of sentence, paragraph or 
entire text, the bag of words approach can easily be 
fooled in the presence of valence shifters (Polanyi 
and Zaenen, 2006). In order to cope with this 
problem, the text under investigation needs a 
minimal level of sentence processing, required for 
the identification of the structures that could get 
under the scope of a valence shifter (Tufi?, 2008). 
For dealing with irony or sarcasm, processing 
requirements go beyond sentence level, and 
discourse structure of the text might be necessary. 
On the other hand, although the adjectives make 
up the obvious class of subjectivity words, the 
other open class categories have significant 
potential for expressing subjective meanings.  
19
In our models, unlike KMM, the building block 
is the word sense, thus making possible to assign 
different connotation values to different senses of a 
word. This was possible by using an additional 
source of information besides the WordNet itself, 
namely the SUMO/MILO ontology. Moreover, we 
considered all the word classes contained in 
WordNet, not only adjectives. 
From this point of view, our work, although 
through a different approach, shares objectives 
with other wordnet-based methods such as 
SentiWordNet (Esuli and Sebastiani, 2006) 
(Baccianella et al, 2010) and WordNet Affect 
(Valiuttti et al 2004). 
2 Base Definitions 
Let us begin with some definitions, slightly 
modified, from KMM. We will progressively 
introduce new definitions to serve our extended 
approach. 
Definition 1: Two words w? and w? are related 
if there exists a sequence of words (w? w1 
w2?wi? w?) so that each pair of adjacent words 
in the sequence belong to the same synset. If the 
length of such a sequence is n+1 one says that w? 
and w? are n-related. 
Two words may not be related at all or may be 
related by many different sequences, of various 
lengths. In the latter case, one would be interested 
in their minimal path-length. 
Definition 2: Let MPL(wi, wj) be the partial 
function: 
     
 otherwise    
related-n  are    wand  n when  wsmallest        the),( ji
?
?
?
=
undefined
n
wwMPL ji
 
Kamps and Marx (2002) showed that MPL is a 
distance measure that can be used as a metric for 
the semantic relatedness of two words. Observing 
the properties of the MPL partial function, one can 
quantify the relatedness of an arbitrary word wi to 
one or the other word of a bipolar pair. To this end, 
KMM introduced another partial function as in 
Definition 3. 
Definition 3: Let TRI (wi, w?, w?), with w? ? w? 
be: 
??
??
?
=
 otherwise                              
  defined   MPLs  if  
,(
,(- ,(
),,(
undefined
)wwMPL
)wwMPL)wwMPL
wwwTRI
ii
i ??
??
??
 
When defined, TRI(wi, w?, w?) is a real number 
in the interval [-1, 1]. The words w? and w? are the 
antonymic words of a factor, while wi is the word 
of interest for which TRI is computed. If one takes 
the negative values returned by the partial function 
TRI (wi, w?, w?) as an indication of wi being more 
similar to w? than to w? and the positive values as 
an indication of wi being more similar to w? than to 
w?, then a zero value could be interpreted as wi 
being neutrally related with respect to w? and w?. 
This is different from being unrelated. 
Definition 4: If ?-? is a factor used for the 
computation of relatedness of wi to ? and ?, the 
proper function TRI*?-? (wi) returns a value outside 
the interval [-1, 1] when wi is unrelated to the ?-? 
factor: 
??
?
=
?
 otherwise                      2
defined  ),,(w TRI  iff   ),,(w TRI)( ii* ?????? iwTRI
 
Given a factor ?-?, for each word wi in 
WordNet that can be reached on a path from ? to 
?, the function TRI*?-? (wi)  computes a score 
number, which is a proportional to the distances 
from wi to ? and to ?. The set of these words 
defines the coverage of the factor ? COV(?, ?).  
Our experiments show that the coverage of the 
vast majority of the factors, corresponding to the 
same POS category, is the same. From now on, we 
will use LUC (Literal Unrestricted2 Coverage) to 
designate this common coverage. The table below 
gives coverage figures for each of the POS 
categories in Princeton WordNet 3.0 (PWN 3.0). 
 
Class Factors LUC 
Adjectives  199  4,402 (20.43%) 
Nouns  106 11,964 (10,05%) 
Verbs  223 6,534 (56,66%) 
Adverbs 199 1,291 (28,81%) 
Table 1: LUC Statistics According to the POS of 
the Literals in PWN 3.0  
The PWN structuring does not allow us to 
compute TRI* scores for adverbs using this 
approach, but, more than half of the total number 
of adverbs (63.11%) are derived from adjectives. 
For those adverbs, we transferred the score values 
from their correspondent adjectives in the LUC set 
and we used the adjectival factors. 
                                                           
2
 In the following we will gradually introduce several 
restrictions, thus justifying the acronym used here. 
20
The results reported for adjectives by Kamps 
and Marx3  are consistent with our findings. The 
difference in numbers might be explained by the 
fact that the two compared experiments used 
different versions of the Princeton WordNet. 
3 Introducing Word-Sense Distinctions  
KMM defines a factor as a pair of words with 
antonymic senses. We generalize the notion of a 
factor to a pair of synsets. In the following, we will 
use the colon notation to specify the sense number 
of a literal that licenses the synonymy relation 
within a synset. Synonymy is a lexical relation that 
holds not between a pair of words but between 
specific senses of those words. That is, the notation 
{literal1:n1 literal2:n2 ? literalk:nk} will mean that 
the meaning given by the sense number n1 of the 
literal1, the meaning given by sense number n2 of 
the literal2 and so on are all pair-wise synonymous. 
The term literal is used to denote the dictionary 
entry form of a word (lemma).  
The antonymy is also a lexical relation that 
holds between specific senses of a pair of words. 
The synonyms of the antonymic senses, taken 
pairwise, definitely express a semantic opposition. 
Take for instance the antonymic pair <rise:1 
fall:2>. These two words belong to the synsets 
{rise:1, lift:4, arise:5, move up:2, go up:1, come 
up:6, uprise:6} and {descend:1, fall:2, go down:1, 
come down:1}. The pair <rise:1 fall:2> is 
explicitly encoded as antonymic. However, there is 
a conceptual opposition between the synsets to 
which the two word senses belong, that is between 
any pair of the Cartesian product: {rise:1, lift:4, 
arise:5, move up:2, go up:1, come up:6, 
uprise:6}?{descend:1, fall:2, go down:1, come 
down:1}. This conceptual opposition is even more 
obvious in this example, as the pairs <go up:1 go 
down:1> and <come up:1 come down:1> are also 
explicitly marked as antonymic. 
Definition 5: An S-factor is a pair of synsets 
(S?, S?) for which there exist ???: ??? ? ?? and 
???: ??? ? ?? so that ???: ???  and ???: ???  are 
antonyms and ??????? , ???? is defined. S? and S? 
                                                           
3
 They found 5410 adjectives that were in the coverage of the 
factors they investigated (WordNet 1.7). For PWN 2.0, the 
total number of covered adjectives is 5307. 
have opposite meanings, and we consider 
that ?????? , ??? = ??? ???? , ????.  
The previous example shows that the semantic 
opposition of two synsets may be reinforced by 
multiple antonymic pairs. Because of how MPL is 
defined, choosing different antonymic pairs might 
produce different values for ?????? , ???. That is 
why, wherever is the case, we need to specify the 
antonymic pair which defines the S-factor. 
Based on the definition of the coverage of a 
factor < ?iw , ?iw >, one may naturally introduce the 
notion of coverage of a S-factor - <S?,S?>: the set 
of synsets containing the words in COV< ?iw , ?iw >. 
The coverage of an S-factor <S?,S?> will be 
onward denoted by SCOV<S?, S?>.  
Since the word-relatedness and MPL definitions 
ignore the word senses, it might happen that the 
meaning of some synsets in the coverage of an S- 
factor have little (if anything) in common with the 
semantic field defined by the respective S-factor. 
More often than not, these outliers must be filtered 
out and, to this end, we further introduce the 
notions of semantic type of a synset, typed S-factor, 
and scoped synset with respect to a typed S-factor, 
which represent major deviations from KMM. 
 
Figure 1. Different levels of coverage (marked 
with cross hatching) for the S-factor <S?-S?> 
Before that, we need to introduce the mapping 
between the WordNet synsets and the SUMO/ 
MILO concepts. The Suggested Upper Merged 
Ontology (SUMO), Mid-Level Ontology (MILO) 
and its domain ontologies form the largest formal 
public 4  ontology in existence today, containing 
roughly 20,000 terms and 70,000 axioms (when 
                                                           
4
 http://www.ontologyportal.org/ 
21
SUMO, MILO, and domain ontologies are 
combined). One of the major attractions of this 
ontology (Niles and Pease, 2003) is that it has been 
mapped to the WordNet lexicon. Using this 
mapping, synsets are labeled with a SUMO/MILO 
concept which we will refer to as the synset?s 
semantic type. The hierarchical structure of 
SUMO/MILO induces a partial ordering of the S-
factors. 
Definition 6: An S-factor <S?, S?> is said to be 
a typed S-factor if the types of the synsets S? and 
S? are identical or they have a common ancestor. If 
this ancestor is the lowest common ancestor, it is 
called the 0-semantic type of the S-factor. The 
direct parent of the n-semantic type of an S-factor 
is the n+1-semantic type of the S-factor (Fig. 1). 
A typed S-factor is represented by indexing the 
S-factor with its type as in the examples below: 
<{unfairness:2?}, { fairness:1?}>NormativeAttribute  
<{discomfort:1?}, {comfort:1?}>StateOfMind  
<{distrust:2?}, {trust:3?}>TraitAttribute  
<{decrease:2? }, {increase:3?}>QuantityChange  
In the following, if not otherwise specified, by 
S-factors we mean typed S-factors. Unless there is 
ambiguity, the type of an S-factor will be omitted. 
Definition 7: A synset Si with the type L is n-
scoped relative to a typed S-factor <S?, S?> if L is 
a node in a sub-tree of the SUMO/MILO hierarchy 
having as root the n-semantic type of the S-factor 
<S?, S?>. We say that n defines the level of the 
scope coverage of the S-factor <S?, S?> and that 
every synset in this coverage is n-scoped. 
We use the notation SCOVn<S?, S?> for the 
scope coverage of level n of an S-factor <S?, S?>. 
If the root of the tree has the semantic type ?, we 
will use also use the notation SCOVn<S?, S?>? or 
simply SCOV<S?, S?>?. In other words, 
SCOV<S?, S?>? is the set of synsets the semantic 
types of which are subsumed by ?. For the example 
in Fig. 1, only the synsets S?1, S?2 and S?1 are in the 
SCOV0<S?, S?>. All depicted synsets are in 
SCOV1<S?, S?>.  
It is easy to see that when the value of the scope 
coverage level is increased so as to reach the top of 
the ontology, SCOVn<S?, S?>? will be equal to the 
set of synsets containing the literals in LUC (see 
Table 1).  Let us call this set SUC (Synset 
Unrestricted Coverage). 
 
 
Class S-Factors SUC 
Adjectives  264 4,240 (23.35%) 
Nouns  118 11,704 (14.25%) 
Verbs  246  8,640 (62.75%) 
Adverbs 264 1,284 (35.45%) 
Table 2: SUC Statistics According to the POS of 
the Synsets in PWN 3.0  
From the differential semantics point of view, 
the S-factor <S?, S?> quantitatively characterizes 
each synset in SCOVn<S?, S?> by a TRI*-like 
score (Definition 4). The synsets in SCOV0<S?, 
S?> are best discriminated, meaning that their 
scores for the <S?, S?> factor are the highest. For 
the synsets in SCOVn<S?, S?> but not in SCOVn-
1<S?, S?>, the scores are smaller and we say that 
the characterization of these synsets in terms of the 
<S?, S?> factor is weaker. Our model captures this 
through a slight modification of the TRI function 
in Definition 3, where w? and w? are the antonyms 
belonging to S? and S? respectively, and wi is a 
literal of a synset Sj in SCOVn<S?, S?> but not in 
SCOVn-1<S?, S?>: 
Definition 8: The differential score for a literal 
wi occurring in a synset Sj in SCOVn<S?, S?> but 
not in SCOVn-1<S?, S?> is computed by the 
function TRI+: 
    
,(
,(- ,(),,(
n)wwMPL
)wwMPL)wwMPL
SSwTRI iii +
=
+
??
??
??
 
Since we imposed the requirement that Sj be in 
SCOVn<S?, S?>, ),,( ?? SSwTRI i+  is defined for 
all literals in Sj, thus for any ji Sw ? the value of 
),,( ?? SSwTRI i+ is in the [-1,1] interval. The 
scores computed for the synsets in SCOVn<S?, S?> 
remained unchanged in SCOVn+k<S?, S?> for any  
k?0. The above modification of the TRI function 
insures that the score of a synset gets closer to zero 
(neutrality) with the increase of n.  
It is worth mentioning that using different 
antonymic literal pairs from the same opposed 
synsets does not have any impact on the sign of 
TRI+ scores, but their absolute values may differ.  
If one associates a semantic field with ?, the 
type of an S-factor <S?, S?>, then all the synsets in 
SCOVn<S?, S?>? are supposed to belong to the 
semantic field associated with ?. This observation 
should clarify why different senses of a given word 
22
may belong to different semantic coverages and 
thus, may have different scores for the S-factor in 
case. 
Definition 9: The differential score of a synset 
Si in SCOVn<S?, S?> with respect to the S-factor 
<S?, S?> is given by the function TRIS (Si, S?, S?), 
defined as the average of the TRI+ values 
associated with the m literals in the synset Si. 
m
SSwTRI
SSSTRIS
m
j
j
i
?
=
+
=
1
),,(
),,(
??
??
 
4 Computing the S-Factors and the 
Differential Scores for Synsets 
In accordance with the equations in the previous 
definitions, we associated each synset Sk of 
WordNet 3.0 with an ordered vector <F1, F2? Fn> 
where Fi is a pair (score; level) with score and 
level representing the value of the ith S-factor and, 
respectively, the minimal S-factor coverage level 
in which Sk was found.  
For instance, let us assume that the first S-factor 
in the description of the adjectival synsets is:  
<{nice:3},{nasty:2 ?}>SubjectiveAssesmentAtttribute 
then for the synset {fussy:1, crabby:1, grumpy:1, 
cross:2, grouchy:1, crabbed:1, bad-tempered:1, 
ill-tempered:1}SubjectiveAssesmentAtttribute the vector 
<F1,?> is <(0,66;0) ...> while for the synset 
{unplayful:1 serious:5 sober:4}SubjectiveAssesmentAtttribute 
the vector <F1,?> is    <(-0,166 ; 0) ...>. 
The values signify that the synset {fussy:1, 
crabby:1, grumpy:1, cross:2?}SubjectiveAssesment 
Atttribute is 0-scoped with respect to the S-factor 
<{nice:3}, {nasty:2 ?}> and its connotative 
meaning is significantly closer to the meaning of 
nasty:2 (0,66). Similarly, the synset {unplayful:1 
serious:5 sober:4} is 0-scoped with respect to the 
considered  S-factor and its connotative meaning is 
closer to the meaning of nice:3 (-0,166) 
Our experiments showed that in order to ensure 
the same sets of synsets for all factors of a given 
part-of-speech we had to set the level of the 
semantic coverages to 7 (corresponding to the 
SUC). For each of the typed S-factors <S?, S?> and 
for each synset Si in their respective semantic 
coverage SCOV<S?, S?>? we computed the 
TRIS??? , ??, ???  score. Each synset from the 
coverage of each POS category was associated 
with a vector of scores, as described above. Since 
the number of S-factors depends on the POS 
category the lengths of each of the four type 
vectors is different. The cell values in a synset 
vector have uneven values, showing that factors 
have different discriminative power for a given 
meaning. Because we considered SUC, all S-
factors are relevant and the cells in any synset 
vector are filled with pairs (score; level).  
For the noun part of the PW3.0 we identified 
118 typed S-factors, all of them covering the same 
set of 11,898 noun literals (9.99%) with their 
senses clustered into 11,704 synsets (14.25%).  
For the verb part of the PWN 3.0, we identified 
246 typed S-factors, all of them covering the same 
set of 6,524 verb literals (56.57%) with their senses 
clustered into 8,640 synsets (62.75%).  
For the adjective part of the PWN 3.0, we 
identified 264 typed S-factors, all of them covering 
the same set of 4,383 literals (20.35%) with their 
senses clustered into 4,240 synsets (23.35%)5. As 
previously mentioned, the same factors were used 
for the adverbs derived from adjectives. In this 
way, a total of 1,287 adverbs (28.72%) clustered 
into 1,284 synsets (35.45%) were successfully 
annotated (see Table 2). 
Apparently, the cardinals of the factor sets in 
Table 2 should be identical with those in Table 1. 
The differences are due to the fact that a pair of 
opposed synsets may contain more than a single 
pair of antonymic senses each of them specifying a 
distinct S-factor. 
In case the user restricted the coverages to lower 
levels, the original maximal semantic coverages 
are split into different subsets for which several S-
factors become irrelevant. The cell values 
corresponding to these factors are filled in with a 
conventional value outside the interval [-1, 1].  
Thus, we have the following annotation cases: 
A synset of a certain POS is not in the 
corresponding SUC. This case signifies that the 
synset cannot be characterized in terms of the 
differential semantics methodology and we 
conventionally say that such a synset is ?objective? 
(insensitive to any S-factor). Since this situation 
would require a factor vector with each cell having 
the same value (outside the [-1, 1] interval) and as 
                                                           
5
 In PWN 2.0 the number of covered literals (and synsets) is 
with almost 20% higher (Tufi? and ?tef?nescu, 2010). This 
difference is explained by the fact that 1081 adjectives (5%), 
mostly participial, from PWN 2.0 are not any more listed as 
adjectives in PWN 3.0.   
23
such a vector would be completely uninformative, 
we decided to leave the ?objective? synsets un-
annotated. As one can deduce from Table 2, the 
majority of the synsets in PWN3.0 are in this 
category (89,556 synsets, i.e. 77.58%). 
 Any synset of a certain POS in the 
corresponding SUC will have an associated factor 
vector. There are 25,868 such synsets. The ith cell 
of such a vector will correspond to the ith S-factor 
<S?, S?>. We may have the following sub-cases: 
(a) All cell scores are in the [-1,1] interval, and 
in this case all S-factors are relevant, that is, from 
any word in the synset one could construct a path 
to both words prompting an S-factor, irrespective 
of the S-factor itself. A negative score in the ith cell 
of the S-factor vector signifies that the current 
synset is more semantically related to S? than to S?, 
while a positive score in the ith cell of the factor 
vector signifies that the synset is more 
semantically related to S? than to S?. A zero score 
in the ith cell of the factor vector signifies that the 
synset is neutral with respect to the <S?, S?> S-
factor. 
(b) Several cell scores are not in the interval [-1, 
1], say FV[i1]=FV[i2] ? =FV[ik]=2. This signifies 
that the S-factors corresponding to those cells 
(<S?1,S?1>,<S?2,S?2>,?,<S?3,S?3>) are irrelevant 
for the respective synset and that the current synset 
is not included in the scope of the above-
mentioned S-factors, owing to the selected scope 
level of the coverage6. We say that, at the given 
scope level, the synset became ?objective? with 
respect to the S-factors FV[i1], FV[i2] ?FV[ik]. 
There are various ways to select, for a given 
POS coverage, those S-factors which are most 
informative or more interesting from a specific 
point of view. The simplest criterion is based on 
the coverage level: for a specified coverage level, 
select only those S-factors the coverage of which 
contains the analyzed synsets. In general, the most 
restrictive condition is choosing the 0-level 
coverage. This condition is equivalent to saying 
that the S-factors and the analyzed synsets should 
be in the same semantic class as defined by the 
SUMO/MILO labeling.  For instance, assume that 
the synset under investigation is {good:1} with the 
                                                           
6
 Remember that for the highest level (7) that corresponds to 
SUC, all factors are relevant. When the user selects coverages 
of lower levels some factors might become irrelevant for 
various synsets. 
definition ?having desirable or positive qualities 
especially those suitable for a thing specified? and 
the semantic type SubjectiveAssessmentAttribute.  
Imposing the restriction that the semantic type of 
the selected factors should be the same with the 
semantic type of good:1, some relevant factors for 
estimating the various connotations of ?good? from 
different perspectives are given below. In the 
shown factors, the words in bold face are those the 
meaning of which is closer to ?good?. 
 
good 01123148-a (SubjectiveAssessmentAttribute) 
-------------------------------------------------------------- 
effective ineffective#00834198-a_00835609-a 
(SubjectiveAssessmentAttribute) -0,78 
reasonable unreasonable#01943406-a_01944660-a 
(SubjectiveAssessmentAttribute) -0,71 
rich lean#02026785-a_02027003-a 
(SubjectiveAssessmentAttribute) -0,63 
ample meager#00105746-a_00106456-a 
(SubjectiveAssessmentAttribute) -0,5 
safe dangerous#02057829-a_02058794-a 
(SubjectiveAssessmentAttribute) -0,33 
brave cowardly#00262792-a_00264776-a 
(SubjectiveAssessmentAttribute) -0,14 
distant close#00450606-a_00451510-a 
(SubjectiveAssessmentAttribute) 0,64 
busy idle#00292937-a_00294175-a 
(SubjectiveAssessmentAttribute) 0,63 
cursed blessed#00669478-a_00670741-a 
(SubjectiveAssessmentAttribute) 0,5 
old new#01638438-a_01640850-a 
(SubjectiveAssessmentAttribute) 0,45 
formal informal#01041916-a_01044240-a 
(SubjectiveAssessmentAttribute) 0,38 
 
These factors? values should be clearer in the 
context of adequate examples: 
A good tool is an effective tool; 
A good excuse is a reasonable excuse; 
A good vein of copper is a reach vein of copper; 
A good resource is an ample resource; 
A good position is a safe position; 
A good attitude is a close attitude; 
A good soldier is a brave soldier 
A good time is an idle time; 
A good life is a blessed life; 
A good car is a new car; 
A good party is an informal party.  
 
From the definitions in the previous sections, 
one can easily see that the sign of a S-factor score 
depends on the order in which the semantically 
opposite pairs are considered. If one wishes to 
have a consistent interpretation of the factor scores 
(e.g. negative scores are ?bad? and positive scores 
are ?good?) the synset ordering in the S-factors is 
24
significant. We used a default ordering of 
antonyms in all factors, yet a text analyst could 
modify this ordering. For each POS, we selected a 
representative factor for which the synset order, 
from a subjective point of view, was very intuitive. 
For instance, for the adjective factors we selected 
the factor <good:1, bad:1>, for noun factors we 
selected the factor <order:5, disorder:2>, and for 
verb factors we selected the factor <succeed:1, 
fail:2>, the first word sense in each of the 
representative factors having a clear positive 
connotation. Then for each POS factor <S?, S?> we 
computed the distance of its constituents to the 
synsets of the representative factor of the same 
POS. The one that was closer to the ?positive? side 
of the reference factor was also considered 
?positive? and the order of the synsets was 
established accordingly. This empirical approach 
proved to be successful for most of the factors, 
except for a couple of them, which were manually 
ordered. 
We developed an application that allows text 
analysts to choose the S-factors they would like to 
work with. The interface allows the user to both 
select/deselect factors and to switch the order of 
the poles in any given factor.  Once the user 
decided on the relevant S-factors, the synsets are 
marked up according to the selected S-factors. This 
version of the WordNet can be saved and used as 
needed in the planned application. 
5 Extending the LUCs and SUCs  
Although the maximum semantic coverage of the 
S-factors for the adjectives contains more than 
28% of the PWN3.0 adjectival synsets, many 
adjectives with connotative potential are not in this 
coverage. This happens because the definition of 
the relatedness (Definition 1) implicitly assumes 
the existence of synonyms for one or more senses 
of a given word. Therefore from mono-semantic 
words in mono-literal synsets a path towards other 
synsets cannot be constructed anymore. Because of 
this, there are isolated ?bubbles? of related synsets 
that are not connected with synsets in maximum 
semantic coverage. In order to assign values to at 
least a part of these synsets, we experimented with 
various strategies out of which the one described 
herein was considered the easiest to implement 
and, to some extent motivated, from a conceptual 
point of view. The approach is similar for all the 
synsets which are not in the SUCs, but the 
algorithms for extending these coverages slightly 
differ depending on the part of speech under 
consideration.  
Class E-LUCs E-SUCs 
Adjectives  7,124 (33.07%) 6,216 (34.23%) 
Nouns  27,614 (23.19%) 22,897 (27.88%) 
Verbs  8,910 (77.26%) 10,798 (78.43%) 
Adverbs 1,838 (41.01%) 1,787 (49.35%) 
Table 3: Extended LUCs and SUCs 
The basic idea is to transfer the vectors of the 
synsets in SUC to those in the complementary set 
SUC , provided they have ?similar meanings?. We 
say that POS
POS
i SUCS ?  and POSPOSj SUCS ?  
have ?similar meanings? if ????/????(?????) =????/????(?????)  and ?????  and ?????  are 
directly linked by a semantic WordNet relation of a 
certain type. For adjectival synsets we consider the 
relations similar_to and also_see, for verbal 
synsets we consider the relations hyponym and 
also_see, and for the nominal synsets we take into 
account only the hyponymy. Consequently, the S-
factors coverage increased as shown in Table 3. 
6 A Preliminary Comparison with 
SentiWordnet 3.0  
SentiWordNet 3.0 (Baccianella, et al 2010) is the 
only public resource we are aware of, which 
considers sense distinctions and covers all synsets 
in Princeton WordNet 3.0. Although in 
SentiWordNet (henceforth SWN3.0) only the 
Subjective-Objective dichotomy is marked-up, 
with a further distinction between Positive-
Subjectivity and Negative-Subjectivity, using it for 
the comparison with our annotations is meaningful 
and relevant for both approaches. First, the 
connotative meanings are subjective meanings.  
Then, while the SWN3.0 mark-up is based on ML 
techniques and various heuristics exploiting the 
structure of PWN3.0 and some other external 
resources, the differential semantics approach, as 
implemented here, is a deterministic one, 
considering only the content and structural 
information in PWN3.0 + SUMO/MILO. 
Identifying contradictions in the two annotations 
might reveal limitations in the ML techniques and 
heuristics used by SWN3.0 on one hand, and, on 
25
the other hand, flaws in our method, possible 
incompleteness or inconsistencies in PWN3.0+ 
SUMO/MILO. It has to be noted that the possible 
incompleteness or inconsistencies in PWN3.0 
would also affect the accuracy of the SWN3.0 
values. 
 
Synset SWN DSA Definition 
dangerous, 
grave 
grievous, 
serious, severe 
? 
-0,63 0,42 
causing fear or 
anxiety by threatening 
great harm 
live 0,5 -0,5 exerting force or 
containing energy 
bastardly, 
mean 
-0,5 0,5 of no value or worth 
dangerous, 
unsafe -0,75 0,5 
involving or causing 
danger or risk; liable 
to hurt or harm 
delirious, 
excited,  
unrestrained,  
mad, 
frantic  
0,5 -0,5 
marked by un-
controlled excitement 
or emotion 
haunted 0,5 -0,43 showing emotional 
affliction or disquiet 
impeccable -0,63 0,8 not capable of sin 
evil, vicious 0,5 -0,75 
having the nature of 
vice 
delectable, 
sexually 
attractive 
0,63 -0,5 
capable of arousing 
desire 
ordinary 
 
-0,5 0,75 
not exceptional in any 
way especially in 
quality or ability or 
size or degree 
serious 
 
-0,75 0,75 
requiring effort or 
concentration; 
complex and not easy 
to answer or solve 
excusable 
 
0,63 -0,4 capable of being 
overlooked 
Table 4: Examples of divergent scores among the 
SWN3.0 and DSA 
For the partial comparison we selected the 
adjectives in SWN3.0 with Positive-Subjectivity or 
Negative-Subjectivity greater than or equal to 0.5. 
From our differential semantic (DSA) annotation 
we extracted all the adjectives which along the 
good-bad differential dimension had an absolute 
value greater than 0.4. Those adjectives closer to 
good were considered to be Subjective-Positive 
while the others were considered to be Subjective-
Negative. The threshold value was empirically 
selected, by observing that beyond the 0.4 and ?0.4 
values the factorial annotation was closer to our 
intuition concerning the connotative load of the 
analyzed words. We computed the intersection of 
the two adjectival synsets extracted this way and 
retained only the synsets contradictorily annotated. 
We found only 150 differences, which by itself is a 
small difference, showing that, at least with respect 
to the good-bad factor, SWN3.0 and DSA 
annotations are to a large extent consistent. 
We manually checked-out the 150 synsets 
marked-up with contradictory scores and the 
authors and 6 MSc students negotiated the scores 
towards reaching the consensus. For 142 of these 
synsets the consensus was easily reached with 76 
considered to be correct in the DSA annotation and 
65 correct in the SWN3.0 annotation. Table 4 
shows some examples of synsets, the scores of 
which were correctly judged (in bold) either by 
SWN3.0 or DSA as well as some examples of non-
consensual annotations (in underlined italics). 
7 Conclusions  
Differential semantics annotation addresses the 
connotative meanings of the lexical stock, the 
denotative meanings of which are recorded in 
WordNet 3.0. We revised and improved our 
previous method (Tufi? and ?tef?nescu, 2010). It 
generalizes the SWN3.0 subjectivity mark-up, 
according to a user-based multi-criteria differential 
semantics model.  
The partial comparison with SWN3.0 revealed 
specific limitations of our approach. The major one 
is due to the definitions of n-relatedness and the 
TRI relation. The problem resides in indiscriminate 
treatment of literals which have senses with 
different polarities with respect to a factor. If one 
of these senses is significantly closer to one of the 
poles of the factor, that sense might impose the 
sign for the rest of the senses. This risk is 
amplified when literals with high degrees of 
polysemy and/or high degrees of synonymy are 
reached on the way from the synset of interest to 
the synsets defining the S-factor (higher the 
polysemy/synonymy, higher the number of paths 
to the constituents of the S-factor). Most of the 
erroneous scores we noticed were explained by this 
drawback. We say that the words affected by this 
limitation of the current algorithm have a 
significant connotation shift potential (Tufi?, 
2009), (?tef?nescu, 2010). As such words could 
generate undesired implicatures, they should be 
26
avoided in formal texts and replaced by synonyms 
with less connotation shift potential.  
We also observed some inconsistencies 
regarding the association of SUMO/MILO (and the 
additional domain ontologies) concepts to PWN 
3.0 synsets. The semantic types of two opposable 
synsets (in the same semantic field) should be 
closely related, if not the same. However, for some 
S-factors, such as <agreement:3, disagreement:1> 
this does not happen. The semantic type of the 
synset {agreement:3?} is ?Cooperation?, while 
the semantic type of {disagreement:1?} is 
?SubjectiveAssessmentAttribute?. ?Cooperation? 
is a ?Process? (subsumed by ?Physical?) but, 
?SubjectiveAssessmentAttribute? is an ?Attribute? 
(subsumed by ?Abstract?). There are 9 such cases 
for nouns, 30 for verbs and 16 for adjectives. 
The current multi-factored annotation vectors 
for nominal, verbal, and adjectival synsets for 
PWN3.0, as well as an application to manage these 
annotations, can be freely downloaded from 
http://www.racai.ro/differentialsemantics/. 
Acknowledgments 
This research has been supported by the grant no. 
ID_1443, awarded by the Romanian Ministry for 
Education, Research, Youth and Sport. We thank 
also to SentiWordNet authors for making it public. 
References  
Andreevskaia Alina and Sabine Bergler. 2006. Mining 
WordNet for a fuzzy sentiment: Sentiment tag 
extraction from WordNet glosses. In Proceedings of 
the 11th Conference of the European Chapter of the 
Association for Computational Linguistics (EACL-
2006), Trento, Italy, pages 209?216.  
Stefano Baccianella, Andrea Esuli, and Fabrizio 
Sebastiani. 2010. SENTIWORDNET 3.0: An 
Enhanced Lexical Resource for Sentiment Analysis 
and Opinion Mining, in Proceedings of LREC2010, 
Malta, pp.2200-2204. 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, 
and Emanuele Pianta. 2004. Revising WordNet 
domains hierarchy: Semantics, coverage, and 
balancing. In Proceedings of COLING 2004 
Workshop on "Multilingual Linguistic Resources", 
Geneva, Switzerland, pages 101?108. 
Andrea Esuli, and Fabrizio Sebastiani. 2006. 
SENTIWORDNET: A publicly available lexical 
resource for opinion mining. In Proceedings of the 
5th Conference on Language Resources and 
Evaluation LREC-06, Genoa, Italy, pages 417?422. 
See also: http://sentiwordnet.isti.cnr.it/  
Christiane Fellbaum. 1998. WordNet: An Electronic 
Lexical Database. Academic Press, Cambridge, MA. 
Jaap Kamps and Maarten Marx. 2002. Words with 
attitude. In Proceedings of the 1st International 
WordNet Conference, Mysore, India, pages 332?341. 
Rada Mihalcea, Carmen Banea, and Janice Wiebe. 
2007. Learning multilingual subjective language via 
cross-lingual projections. In Proceedings of the 45th 
Annual Meeting of the Association of Computational 
Linguistics, Prague, Czech Republic, pages 976?983. 
Ian Niles and Adam Pease. 2003. Linking Lexicons and 
Ontologies: Mapping WordNet to the Suggested 
Upper Merged Ontology. In Proceedings of the 2003 
International Conference on Information and 
Knowledge Engineering (IKE 03), Las Vegas, pages 
23?26. 
Charles E. Osgood, George Suci and Percy 
Tannenbaum. 1957. The measurement of meaning, 
University of Illinois Press, Urbana IL. 
Bo Pang and Lillian Lee, 2008. Opinion mining and 
sentiment analysis. Foundations and Trends in 
Information Retrieval, 2(1?2): 1?135. 
Livia  Polanyi, and Annie Zaenen. 2006. Contextual 
valence shifters. In J. G. Shanahan, Y. Qu and J. 
Wiebe, editors, Computing Attitude and Affect in 
Text: Theory and Applications, The Information 
Retrieval Series, Vol. 20, Springer Verlag, 
Dordrecht, Netherlands, pages 1-10. 
Dan ?tef?nescu. 2010. Intelligent Information Mining 
from Multilingual Corpora (in Romanian). PhD 
Thesis, Romanian Academy, Bucharest. 
Dan Tufi?. 2008. Mind your words! You might convey 
what you wouldn?t like to. Int. J. of Computers, 
Communications & Control, III, pages 139?143. 
Dan Tufi?. 2009. Playing with word meanings,.In Lotfi 
A. Zadeh, Dan Tufi?, Florin Gh. Filip and Ioan 
Dzi?ac, (editors) From Natural Language to Soft 
Computing: New Paradigms in Artificial 
Intelligence. Publishing House of the Romanian 
Academy, Bucharest, pages 211?223. 
Dan Tufi?, Dan ?tef?nescu. 2010. A Differential 
Semantics Approach to the Annotation of the Synsets 
in WordNet. In Proceedings of LREC 2010, Malta, 
pages 3173-3180 
Alessandro Valitutti, Carlo Strapparava, and Oliviero 
Stock. 2004. Developing affective lexical resources, 
Psychology Journal, 2(1), pages 61?83. 
27
