Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 198?207,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Making Grammar-Based Generation Easier to Deploy in Dialogue Systems
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a development pipeline and asso-
ciated algorithms designed to make grammar-
based generation easier to deploy in imple-
mented dialogue systems. Our approach real-
izes a practical trade-off between the capabili-
ties of a system?s generation component and
the authoring and maintenance burdens im-
posed on the generation content author for a
deployed system. To evaluate our approach,
we performed a human rating study with sys-
tem builders who work on a common large-
scale spoken dialogue system. Our results
demonstrate the viability of our approach and
illustrate authoring/performance trade-offs be-
tween hand-authored text, our grammar-based
approach, and a competing shallow statistical
NLG technique.
1 Introduction
This paper gives an overview of a new example-
based generation technique that is designed to make
grammar-based generation easier to deploy in dia-
logue systems. Dialogue systems present several
specific requirements for a practical generation com-
ponent. First, the generator needs to be fast enough
to support real-time interaction with a human user.
Second, the generator must provide adequate cover-
age for the meanings the dialogue system needs to
express. What counts as ?adequate? can vary be-
tween systems, since the high-level purpose of a di-
alogue system can affect priorities regarding output
fluency, fidelity to the requested meaning, variety
of alternative outputs, and tolerance for generation
failures. Third, developing the necessary resources
for the generation component should be relatively
straightforward in terms of time and expertise re-
quired. This is especially important since dialogue
systems are complex systems with significant devel-
opment costs. Finally, it should be relatively easy
for the dialogue manager to formulate a generation
request in the format required by the generator.
Together, these requirements can reduce the at-
tractiveness of grammar-based generation when
compared to simpler template-based or canned text
output solutions. In terms of speed, off-the-
shelf, wide-coverage grammar-based realizers such
as FUF/SURGE (Elhadad, 1991) can be too slow for
real-time interaction (Callaway, 2003).
In terms of adequacy of coverage, in principle,
grammar-based generation offers significant advan-
tages over template-based or canned text output by
providing productive coverage and greater variety.
However, realizing these advantages can require sig-
nificant development costs. Specifying the neces-
sary connections between lexico-syntactic resources
and the flat, domain-specific semantic representa-
tions that are typically available in implemented sys-
tems is a subtle, labor-intensive, and knowledge-
intensive process for which attractive methodologies
do not yet exist (Reiter et al, 2003).
One strategy is to hand-build an application-
specific grammar. However, in our experience,
this process requires a painstaking, time-consuming
effort by a developer who has detailed linguistic
knowledge as well as detailed domain knowledge,
and the resulting coverage is inevitably limited.
Wide-coverage generators that aim for applicabil-
198
ity across application domains (White et al, 2007;
Zhong and Stent, 2005; Langkilde-Geary, 2002;
Langkilde and Knight, 1998; Elhadad, 1991) pro-
vide a grammar (or language model) for free. How-
ever, it is harder to tailor output to the desired word-
ing and style for a specific dialogue system, and
these generators demand a specific input format that
is otherwise foreign to an existing dialogue system.
Unfortunately, in our experience, the development
burden of implementing the translation between the
system?s available meaning representations and the
generator?s required input format is quite substan-
tial. Indeed, implementing the translation might re-
quire as much effort as would be required to build a
simple custom generator; cf. (Callaway, 2003; Buse-
mann and Horacek, 1998). This development cost is
exacerbated when a dialogue system?s native mean-
ing representation scheme is under revision.
In this paper, we survey a new example-based ap-
proach (DeVault et al, 2008) that we have devel-
oped in order to mitigate these difficulties, so that
grammar-based generation can be deployed more
widely in implemented dialogue systems. Our de-
velopment pipeline requires a system developer to
create a set of training examples which directly
connect desired output texts to available applica-
tion semantic forms. This is achieved through a
streamlined authoring task that does not require de-
tailed linguistic knowledge. Our approach then
processes these training examples to automatically
construct all the resources needed for a fast, high-
quality, run-time grammar-based generation compo-
nent. We evaluate this approach using a pre-existing
spoken dialogue system. Our results demonstrate
the viability of the approach and illustrate author-
ing/performance trade-offs between hand-authored
text, our grammar-based approach, and a competing
shallow statistical NLG technique.
2 Background and Motivation
The generation approach set out in this paper has
been developed in the context of a research pro-
gram aimed at creating interactive virtual humans
for social training purposes (Swartout et al, 2006).
Virtual humans are embodied conversational agents
that play the role of people in simulations or games.
They interact with human users and other virtual hu-
Figure 1: Doctor Perez.
mans using spoken language and non-verbal behav-
ior such as eye gaze, gesture, and facial displays.
The case study we present here is the genera-
tion of output utterances for a particular virtual hu-
man, Doctor Perez (see Figure 1), who is designed
to teach negotiation skills in a multi-modal, multi-
party, non-team dialogue setting (Traum et al, 2005;
Traum et al, 2008). The human trainee who talks
to the doctor plays the role of a U.S. Army captain
named Captain Kirk. We summarize Doctor Perez?s
generation requirements as follows.
In order to support compelling real-time conver-
sation and effective training, the generator must be
able to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Doctor Perez has a relatively rich internal men-
tal state including beliefs, goals, plans, and emo-
tions. As Doctor Perez attempts to achieve his con-
versational goals, his utterances need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Doctor Perez cur-
rently uses about 200 distinct output utterances in
the course of his dialogues.
Doctor Perez is designed to simulate a non-native
English speaker, so highly fluent output is not a ne-
cessity; indeed, a small degree of disfluency is even
desirable in order to increase the realism of talking
to a non-native speaker.
Finally, in reasoning about user utterances, dia-
logue management, and generation, Doctor Perez
199
26
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
addressee captain-kirk
dialogue-act
2
6
4
addressee captain-kirk
type assign-turn
actor doctor-perez
3
7
5
speech-act
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
actor doctor-perez
addressee captain-kirk
action assert
content
2
6
6
6
6
6
6
6
6
4
type state
polarity negative
time present
attribute resourceAttribute
value medical-supplies
object-id market
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
addressee captain-kirk
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
dialogue-act.actor doctor-perez
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.action assert
speech-act.content.type state
speech-act.content.polarity negative
speech-act.content.time present
speech-act.content.attribute resourceAttribute
speech-act.content.value medical-supplies
speech-act.content.object-id market
(a) Attribute-value matrix (b) Corresponding frame
Figure 2: An example of Doctor Perez?s representations for utterance semantics: Doctor Perez tells the captain that
there are no medical supplies at the market.
exploits an existing semantic representation scheme
that has been utilized in a family of virtual humans.
This scheme uses an attribute-value matrix (AVM)
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain, as well as other features such as po-
larity and modality. See (Traum, 2003) for some
more details and examples of this representation.
For ease of interprocess communication, and certain
kinds of statistical processing, this AVM structure is
linearized so that each non-recursive terminal value
is paired with a path from the root to the final at-
tribute. Thus, the AVM in Figure 2(a) is represented
as the ?frame? in Figure 2(b).
Because the internal representations that make up
Doctor Perez?s mental state are under constant de-
velopment, the exact frames that are sent to the gen-
eration component change frequently as new rea-
soning capabilities are added and existing capabil-
ities are reorganized. Additionally, while only hun-
dreds of frames currently arise in actual dialogues,
the number of potential frames is orders of magni-
tude larger, and it is difficult to predict in advance
which frames might occur.
In this setting, over a period of years, a number
of different approaches to natural language gener-
ation have been implemented and tested, including
hand-authored canned text, domain specific hand-
built grammar-based generators (e.g., (Traum et al,
2003)), shallow statistical generation techniques,
and the grammar-based approach presented in this
paper. We now turn to the details of our approach.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component.
The approach involves three primary steps: spec-
ification of training examples, grammar induction,
and search optimization. In this section, we present
the format that training examples take and then sum-
marize the subsequent automatic processing steps.
Due to space limitations, we omit the full details
of these automatic processing steps, and refer the
reader to (DeVault et al, 2008) for additional details.
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 3.
In this example, the generation content author
200
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
Semantics
we do n?t . . . . . . . . .
{
speech-act.action = assert
speech-act.content.polarity = negative
have . . . . . . . . . . . . . speech-act.content.attribute = resourceAttribute
medical supplies . . speech-act.content.value = medical-supplies
here . . . . . . . . . . . . . speech-act.content.object-id = market
captain . . . . . . . . . .
?
?
?
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 3: A generation training example for Doctor Perez.
suggests the output utterance u = we don?t have
medical supplies here captain. Each utterance u is
accompanied by syntax(u), a syntactic analysis in
Penn Treebank format (Marcus et al, 1994). In this
example, the syntax is a hand-corrected version of
the output of the Charniak parser (Charniak, 2001;
Charniak, 2005) on this sentence; we discuss this
hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
201
3.2 Automatic Grammar Induction and Search
Optimization
The first processing step is to induce a productive
grammar from the training examples. We adopt the
probabilistic tree-adjoining grammar (PTAG) for-
malism and grammar induction technique of (Chi-
ang, 2003). We induce our grammar from training
examples such as Figure 3 using heuristic rules to
assign derivations to the examples, as in (Chiang,
2003). Once derivations have been assigned, sub-
trees within the training example syntax are incre-
mentally detached. This process yields the reusable
linguistic resources in the grammar, as well as the
statistical model needed to compute operation prob-
abilities when the grammar is later used in genera-
tion. Figure 5 in the Appendix illustrates this pro-
cess by presenting the linguistic resources inferred
from the training example of Figure 3.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (200ms for Doctor Perez) and returning a
list of alternative outputs ranked by their derivation
probabilities.
The search space created by a grammar induced
in this way is too large to be searched exhaustively
in most applications. The second step of automated
processing, then, uses the training examples to learn
an effective search policy so that good output sen-
tences can be found in a reasonable time frame. The
solution we have developed employs a beam search
strategy that uses weighted features to rank alterna-
tive grammatical expansions at each step. Our al-
gorithm for selecting features and weights is based
on the search optimization algorithm of (Daum?
and Marcu, 2005), which decides to update feature
weights when mistakes are made during search on
training examples. We use the boosting approach of
(Collins and Koo, 2005) to perform feature selection
and identify good weight values.
4 Empirical Evaluation
In the introduction, we identified run-time speed, ad-
equacy of coverage, authoring burdens, and NLG re-
quest specification as important factors in the selec-
tion of a technology for a dialogue system?s NLG
component. In this section, we evaluate our tech-
nique along these four dimensions.
Hand-authored utterances. We collected a sam-
ple of 220 instances of frames that Doctor Perez?s
dialogue manager had requested of the generation
component in previous dialogues with users. Some
frames occurred more than once in this sample.
Each frame was associated with a single hand-
authored utterance. Some of these utterances arose
in human role plays for Doctor Perez; some were
written by a script writer; others were authored
by system builders to provide coverage for specific
frames. All were reviewed by a system builder for
appropriateness to the corresponding frame.
Training. We used these 220 (frame, utterance)
examples to evaluate both our approach and a shal-
low statistical method called sentence retriever (dis-
cussed below). We randomly split the examples
into 198 training and 22 test examples; we used the
same train/test split for our approach and sentence
retriever.
To train our approach, we constructed training ex-
amples in the format specified in Section 3.1. Syntax
posed an interesting problem, because the Charniak
parser frequently produces erroneous syntactic anal-
yses for utterances in Doctor Perez?s domain, but it
was not obvious how detrimental these errors would
be to overall generated output. We therefore con-
structed two alternative sets of training examples ?
one where the syntax of each utterance was the un-
corrected output of the Charniak parser, and another
where the parser output was corrected by hand (the
syntax in Figure 3 above is the corrected version).
Hand correction of parser output requires consider-
able linguistic expertise, so uncorrected output rep-
resents a substantial reduction in authoring burden.
The connections between surface expressions and
frame key-value pairs were identical in both uncor-
rected and corrected training sets, since they are in-
dependent of the syntax. For each training set, we
trained our generator on the 198 training examples.
We then generated a single (highest-ranked) utter-
ance for each example in both the test and training
sets. The generator sometimes failed to find a suc-
cessful utterance within the 200ms timeout; the suc-
cess rate of our generator was 95% for training ex-
202
amples and 80% for test examples. The successful
utterances were rated by our judges.
Sentence retriever is based on the cross-
language information retrieval techniques described
in (Leuski et al, 2006), and is currently in use for
Doctor Perez?s NLG problem. Sentence retriever
does not exploit any hierarchical syntactic analy-
sis of utterances. Instead, sentence retriever views
NLG as an information retrieval task in which a set
of training utterances are the ?documents? to be re-
trieved, and the frame to be expressed is the query.
At run-time, the algorithm functions essentially as a
classifier: it uses a relative entropy metric to select
the highest ranking training utterance for the frame
that Doctor Perez wishes to express. This approach
has been used because it is to some extent robust
against changes in internal semantic representations,
and against minor deficiencies in the training corpus,
but as with a canned text approach, it requires each
utterance to be hand-authored before it can be used
in dialogue. We trained sentence retriever on the 198
training examples, and used it to generate a single
(highest-ranked) utterance for each example in both
the test and training sets. Sentence retriever?s suc-
cess rate was 96% for training examples and 90%
for test examples. The successful utterances were
rated by our judges.
Figure 7 in the Appendix illustrates the alternative
utterances that were produced for a frame present in
the test data but not in the training data.
Run-time speed. Both our approach and sentence
retriever run within the available 200ms window.
Adequacy of Coverage. To assess output quality,
we conducted a study in which 5 human judges gave
overall quality ratings for various utterances Doctor
Perez might use to express specific semantic frames.
In total, judges rated 494 different utterances which
were produced in several conditions: hand-authored
(for the relevant frame), generated by our approach,
and sentence retriever.
We asked our 5 judges to rate each of the 494 ut-
terances, in relation to the specific frame for which
it was produced, on a single 1 (?very bad?) to 5
(?very good?) scale. Since ratings need to incorpo-
rate accuracy with respect to the frame, our judges
had to be able to read the raw system semantic rep-
resentations. This meant we could only use judges
who were deeply familiar with the dialogue system;
however, the main developer of the new generation
algorithms (the first author) did not participate as
a judge. Judges were blind to the conditions un-
der which utterances were produced. The judges
rated the utterances using a custom-built application
which presented a single frame together with 1 to 6
candidate utterances for that frame. The rating inter-
face is shown in Figure 6 in the Appendix. The order
of candidate utterances for each frame was random-
ized, and the order in which frames appeared was
randomized for each judge.
The judges were instructed to incorporate both
fluency and accuracy with respect to the frame into
a single overall rating for each utterance. While it
is possible to have human judges rate fluency and
accuracy independently, ratings of fluency alone are
not particularly helpful in evaluating Doctor Perez?s
generation component, since for Doctor Perez, a cer-
tain degree of disfluency can contribute to believ-
ability (as noted in Section 2). We therefore asked
judges to make an overall assessment of output qual-
ity for the Doctor Perez character.
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. Agreement between subsets of judges
ranged from ? = 0.802 for the most concordant pair
of judges to ? = 0.593 for the most discordant pair.
We also performed an ANOVA comparing three
conditions (generated, retrieved and hand-authored
utterances) across the five judges; we found sig-
nificant main effects of condition (F (2, 3107) =
55, p < 0.001) and judge (F (4, 3107) = 17, p <
0.001), but no significant interaction (F (8, 3107) =
0.55, p > 0.8). We therefore conclude that the indi-
vidual differences among the judges do not affect the
comparison of utterances across the different condi-
tions, so we will report the rest of the evaluation on
the mean ratings per utterance.
Due to the large number of factors and the dif-
ferences in the number of utterances correspond-
ing to each condition, we ran a small number
of planned comparisons. The distribution of rat-
ings across utterances is not normal; to validate
our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and signifi-
cance always fell in the same general range. We
found a significant difference between generated
203
Generated (N = 90)
Sentence retriever (N = 100)
Rating
Fr
eq
u
en
cy
(%
)
0
10
20
30
40
1 2 3 4 5
Figure 4: Observed ratings of generated (uncorrected
syntax) vs. retrieved sentences for test examples.
output for all examples, retrieved output for all ex-
amples, and hand-authored utterances (F (2, 622) =
16, p < 0.001); however, subsequent t-tests show
that all of this difference is due to the fact that hand-
authored utterances (mean rating 4.4) are better than
retrieved (t(376) = 3.7, p < 0.001) and gener-
ated (t(388) = 5.9, p < 0.001) utterances, whereas
the difference between generated (mean rating 3.8)
and retrieved (mean rating 4.0) is non-significant
(t(385) = 1.6, p > 0.1).
Figure 4 shows the observed rating frequencies
of sentence retriever (mean 3.0) and our approach
(mean 3.6) on the test examples. While this data
does not show a significant difference, it suggests
that retriever?s selected sentences are most fre-
quently either very bad or very good; this reflects
the fact that the classification algorithm retrieves
highly fluent hand-authored text which is sometimes
semantically very incorrect. (Figure 7 in the Ap-
pendix provides such an example, in which a re-
trieved sentence has the wrong polarity.) The qual-
ity of our generated output, by comparison, appears
more graded, with very good quality the most fre-
quent outcome and lower qualities less frequent. In
a system where there is a low tolerance for very
bad quality output, generated output would likely be
considered preferable to retrieved output.
In terms of generation failures, our approach had
poorer coverage of test examples than sentence re-
triever (80% vs. 90%). Note however that in this
study, our approach only delivered an output if it
could completely cover the requested frame. In the
future, we believe coverage could be improved, with
perhaps some reduction in quality, by allowing out-
puts that only partially cover requested frames.
In terms of output variety, in this initial study our
judges rated only the highest ranked output gener-
ated or retrieved for each frame. However, we ob-
served that our generator frequently finds several al-
ternative utterances of relatively high quality (see
Figure 7); thus our approach offers another poten-
tial advantage in output variety.
Authoring burdens. Both canned text and sen-
tence retriever require only frames and correspond-
ing output sentences as input. In our approach, syn-
tax and semantic links are additionally needed. We
compared the use of corrected vs. uncorrected syn-
tax in training. Surprisingly, we found no significant
difference between generated output trained on cor-
rected and uncorrected syntax (t(29) = 0.056, p >
0.9 on test items, t(498) = ?1.1, p > 0.2 on all
items). This is a substantial win in terms of reduced
authoring burden for our approach.
If uncorrected syntax is used, the additional bur-
den of our approach lies only in specifying the se-
mantic links. For the 220 examples in this study,
one system builder specified these links in about 6
hours. We present a detailed cost/benefit analysis of
this effort in (DeVault et al, 2008).
NLG request specification. Both our approach
and sentence retriever accept the dialogue manager?s
native semantic representation for NLG as input.
Summary. In exchange for a slightly increased
authoring burden, our approach yields a generation
component that generalizes to unseen test problems
relatively gracefully, and does not suffer from the
frequent very bad output or the necessity to author
every utterance that comes with canned text or a
competing statistical classification technique.
5 Conclusion and Future Work
In this paper we have presented an approach to spec-
ifying domain-specific, grammar-based generation
by example. The method reduces the authoring bur-
den associated with developing a grammar-based
NLG component for an existing dialogue system.
We have argued that the method delivers relatively
high-quality, domain-specific output without requir-
ing that content authors possess detailed linguistic
knowledge. In future work, we will study the perfor-
204
mance of our approach as the size of the training set
grows, and assess what specific weaknesses or prob-
lematic disfluencies, if any, our human rating study
identifies in output generated by our technique. Fi-
nally, we intend to evaluate the performance of our
generation approach within the context of the com-
plete, running Doctor Perez agent.
Acknowledgments
Thanks to Arno Hartholt, Susan Robinson, Thomas
Russ, Chung-chieh Shan, and Matthew Stone. This
work was sponsored by the U.S. Army Research,
Development, and Engineering Command (RDE-
COM), and the content does not necessarily reflect
the position or the policy of the Government, and no
official endorsement should be inferred.
References
Stephen Busemann and Helmut Horacek. 1998. A flex-
ible shallow approach to text generation. In Proceed-
ings of INLG, pages 238?247.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. Proceedings of the
International Joint Conferences on Artificial Intelli-
gence.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In ACL ?01: Proceedings of the
39th Annual Meeting on Association for Computa-
tional Linguistics, pages 124?131, Morristown, NJ,
USA. Association for Computational Linguistics.
Eugene Charniak. 2005.
ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
David Chiang. 2003. Statistical parsing with an auto-
matically extracted tree adjoining grammar. In Rens
Bod, Remko Scha, and Khalil Sima?an, editors, Data
Oriented Parsing, pages 299?316. CSLI Publications,
Stanford.
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?70.
Hal Daum?, III and Daniel Marcu. 2005. Learning as
search optimization: approximate large margin meth-
ods for structured prediction. In ICML ?05: Proceed-
ings of the 22nd international conference on Machine
learning, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Practical grammar-based NLG from examples. In
Fifth International Natural Language Generation
Conference (INLG).
Michael Elhadad. 1991. FUF: the universal unifier user
manual version 5.0. Technical Report CUCS-038-91.
Klaus Krippendorff, 1980. Content Analysis: An Intro-
duction to Its Methodology, chapter 12, pages 129?
154. Sage, Beverly Hills, CA.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In The 7th SIGdial Workshop
on Discourse and Dialogue.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
E. Reiter, S. Sripada, and R. Robertson. 2003. Acquir-
ing correct knowledge for natural language generation.
Journal of Artificial Intelligence Research, 18:491?
516.
William Swartout, Jonathan Gratch, Randall W. Hill, Ed-
uard Hovy, Stacy Marsella, Jeff Rickel, and David
Traum. 2006. Toward virtual humans. AI Mag.,
27(2):96?108.
David Traum, Michael Fleischman, and Eduard Hovy.
2003. Nl generation for virtual humans in a complex
social environment. In Working Notes AAAI Spring
Symposium on Natural Language Generation in Spo-
ken and Written Dialogue, March.
David Traum, William Swartout, Jonathan Gratch,
Stacy Marsella, Patrick Kenny, Eduard Hovy, Shri
Narayanan, Ed Fast, Bilyana Martinovski, Rahul
Baghat, Susan Robinson, Andrew Marshall, Dagen
Wang, Sudeep Gandhe, and Anton Leuski. 2005.
Dealing with doctors: A virtual human for non-team
interaction. In SIGdial.
D. R. Traum, W. Swartout, J Gratch, and S Marsella.
2008. A virtual human dialogue model for non-team
interaction. In Laila Dybkjaer and Wolfgang Minker,
editors, Recent Trends in Discourse and Dialogue.
Springer.
David Traum. 2003. Semantics and pragmatics of ques-
tions and answers for dialogue agents. In proceedings
of the International Workshop on Computational Se-
mantics, pages 380?394, January.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Huayan Zhong and Amanda Stent. 2005. Building
surface realizers automatically from corpora using
general-purpose tools. In Proc. Corpus Linguistics ?05
Workshop on Using Corpora for Natural Language
Generation.
205
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee =
captain-kirk
speech-act.addressee =
captain-kirk
Figure 5: The linguistic resources automatically inferred from the training example in Figure 3.
Figure 6: Human rating interface.
206
Input semantic form
addressee captain-kirk
dialogue-act.actor doctor-perez
dialogue-act.addressee captain-kirk
dialogue-act.type assign-turn
speech-act.action assert
speech-act.actor doctor-perez
speech-act.addressee captain-kirk
speech-act.content.attribute acceptableAttribute
speech-act.content.object-id clinic
speech-act.content.time present
speech-act.content.type state
speech-act.content.value yes
Outputs
Hand-authored
the clinic is acceptable captain
Generated (uncorrected syntax)
Rank Time (ms)
1 16 the clinic is up to standard captain
2 94 the clinic is acceptable captain
3 78 the clinic should be in acceptable condition captain
4 16 the clinic downtown is currently acceptable captain
5 78 the clinic should agree in an acceptable condition captain
Generated (corrected syntax)
Rank Time (ms)
1 47 it is necessary that the clinic be in good condition captain
2 31 i think that the clinic be in good condition captain
3 62 captain this wont work unless the clinic be in good condition
Sentence retriever
the clinic downtown is not in an acceptable condition captain
Figure 7: The utterances generated for a single test example by different evaluation conditions. Generated outputs
whose rank (determined by derivation probability) was higher than 1 were not rated in the evaluation reported in this
paper, but are included here to suggest the potential of our approach to provide a variety of alternative outputs for the
same requested semantic form. Note how the output of sentence retriever has the opposite meaning to that of the input
frame.
207
Survey Article
Inter-Coder Agreement for
Computational Linguistics
Ron Artstein?
University of Essex
Massimo Poesio??
University of Essex/Universita` di Trento
This article is a survey of methods for measuring agreement among corpus annotators. It exposes
the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff?s
alpha as well as Scott?s pi and Cohen?s kappa; discusses the use of coefficients in several annota-
tion tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-
like measures in computational linguistics, may be more appropriate for many corpus annotation
tasks?but that their use makes the interpretation of the value of the coefficient even harder.
1. Introduction and Motivations
Since the mid 1990s, increasing effort has gone into putting semantics and discourse
research on the same empirical footing as other areas of computational linguistics (CL).
This soon led to worries about the subjectivity of the judgments required to create
annotated resources, much greater for semantics and pragmatics than for the aspects of
language interpretation of concern in the creation of early resources such as the Brown
corpus (Francis and Kucera 1982), the British National Corpus (Leech, Garside, and
Bryant 1994), or the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). Prob-
lems with early proposals for assessing coders? agreement on discourse segmentation
tasks (such as Passonneau and Litman 1993) led Carletta (1996) to suggest the adoption
of the K coefficient of agreement, a variant of Cohen?s ? (Cohen 1960), as this had already
been used for similar purposes in content analysis for a long time.1 Carletta?s proposals
? Now at the Institute for Creative Technologies, University of Southern California, 13274 Fiji Way, Marina
Del Rey, CA 90292.
?? At the University of Essex: Department of Computing and Electronic Systems, University of Essex,
Wivenhoe Park, Colchester, CO4 3SQ, UK. E-mail: poesio@essex.ac.uk. At the University of Trento:
CIMeC, Universita` degli Studi di Trento, Palazzo Fedrigotti, Corso Bettini, 31, 38068 Rovereto (TN), Italy.
E-mail: massimo.poesio@unitn.it.
1 The literature is full of terminological inconsistencies. Carletta calls the coefficient of agreement she
argues for ?kappa,? referring to Krippendorff (1980) and Siegel and Castellan (1988), and using Siegel
and Castellan?s terminology and definitions. However, Siegel and Castellan?s statistic, which they call K,
is actually Fleiss?s generalization to more than two coders of Scott?s ?, not of the original Cohen?s ?; to
confuse matters further, Siegel and Castellan use the Greek letter ? to indicate the parameter which is
estimated by K. In what follows, we use ? to indicate Cohen?s original coefficient and its generalization
to more than two coders, and K for the coefficient discussed by Siegel and Castellan.
Submission received: 26 August 2005; revised submission received: 21 December 2007; accepted for
publication: 28 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
were enormously influential, and K quickly became the de facto standard for measuring
agreement in computational linguistics not only in work on discourse (Carletta et al
1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke
et al 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks
(e.g., Ve?ronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and
McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number
of questions have also been raised about K and similar coefficients?some already in
Carletta?s own work (Carletta et al 1997)?ranging from simple questions about the
way the coefficient is computed (e.g., whether it is really applicable when more than
two coders are used), to debates about which levels of agreement can be considered
?acceptable? (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K
is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera,
and Amorrortu 1999; Di Eugenio 2000; Stevenson and Gaizauskas 2000). Di Eugenio
raised the issue of the effect of skewed distributions on the value of K and pointed out
that the original ? developed by Cohen is based on very different assumptions about
coder bias from the K of Siegel and Castellan (1988), which is typically used in CL. This
issue of annotator bias was further debated in Di Eugenio and Glass (2004) and Craggs
andMcGeeWood (2005). Di Eugenio andGlass pointed out that the choice of calculating
chance agreement by using individual coder marginals (?) or pooled distributions (K)
can lead to reliability values falling on different sides of the accepted 0.67 threshold,
and recommended reporting both values. Craggs and McGee Wood argued, following
Krippendorff (2004a,b), that measures like Cohen?s ? are inappropriate for measur-
ing agreement. Finally, Passonneau has been advocating the use of Krippendorff?s ?
(Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and
disjoint categories, including anaphoric annotation, wordsense tagging, and summa-
rization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash,
and Rambow 2006).
Now that more than ten years have passed since Carletta?s original presentation
at the workshop on Empirical Methods in Discourse, it is time to reconsider the use
of coefficients of agreement in CL in a systematic way. In this article, a survey of
coefficients of agreement and their use in CL, we have threemain goals. First, we discuss
in some detail the mathematics and underlying assumptions of the coefficients used or
mentioned in the CL and content analysis literatures. Second, we also cover in some
detail Krippendorff?s ?, often mentioned but never really discussed in detail in previous
CL literature other than in the papers by Passonneau just mentioned. Third, we review
the past ten years of experience with coefficients of agreement in CL, reconsidering the
issues that have been raised also from a mathematical perspective.2
2. Coefficients of Agreement
2.1 Agreement, Reliability, and Validity
We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff
(2004a, Section 11.1). Researchers who wish to use hand-coded data?that is, data in
which items are labeled with categories, whether to support an empirical claim or to
develop and test a computational model?need to show that such data are reliable.
2 Only part of our material could fit in this article. An extended version of the survey is available from
http://cswww.essex.ac.uk/Research/nle/arrau/.
556
Artstein and Poesio Inter-Coder Agreement for CL
The fundamental assumption behind the methodologies discussed in this article is that
data are reliable if coders can be shown to agree on the categories assigned to units to
an extent determined by the purposes of the study (Krippendorff 2004a; Craggs and
McGeeWood 2005). If different coders produce consistently similar results, then we can
infer that they have internalized a similar understanding of the annotation guidelines,
and we can expect them to perform consistently under this understanding.
Reliability is thus a prerequisite for demonstrating the validity of the coding
scheme?that is, to show that the coding scheme captures the ?truth? of the phenom-
enon being studied, in case this matters: If the annotators are not consistent then either
some of them are wrong or else the annotation scheme is inappropriate for the data.
(Just as in real life, the fact that witnesses to an event disagree with each other makes it
difficult for third parties to know what actually happened.) However, it is important to
keep in mind that achieving good agreement cannot ensure validity: Two observers of
the same event may well share the same prejudice while still being objectively wrong.
2.2 A Common Notation
It is useful to think of a reliability study as involving a set of items (markables), a
set of categories, and a set of coders (annotators) who assign to each item a unique
category label. The discussions of reliability in the literature often use different notations
to express these concepts. We introduce a uniform notation, which we hope will make
the relations between the different coefficients of agreement clearer.
? The set of items is { i | i ? I } and is of cardinality i.
? The set of categories is { k | k ? K } and is of cardinality k.
? The set of coders is { c | c ? C } and is of cardinality c.
Confusion also arises from the use of the letter P, which is used in the literature with at
least three distinct interpretations, namely ?proportion,? ?percent,? and ?probability.?
We will use the following notation uniformly throughout the article.
? Ao is observed agreement and Do is observed disagreement.
? Ae and De are expected agreement and expected disagreement,
respectively. The relevant coefficient will be indicated with a superscript
when an ambiguity may arise (for example, A?e is the expected agreement
used for calculating ?, and A?e is the expected agreement used for
calculating ?).
? P(?) is reserved for the probability of a variable, and P?(?) is an estimate of
such probability from observed data.
Finally, we use n with a subscript to indicate the number of judgments of a given type:
? nik is the number of coders who assigned item i to category k;
? nck is the number of items assigned by coder c to category k;
? nk is the total number of items assigned by all coders to category k.
557
Computational Linguistics Volume 34, Number 4
2.3 Agreement Without Chance Correction
The simplest measure of agreement between two coders is percentage of agreement or
observed agreement, defined for example by Scott (1955, page 323) as ?the percentage of
judgments on which the two analysts agree when coding the same data independently.?
This is the number of items on which the coders agree divided by the total number
of items. More precisely, and looking ahead to the following discussion, observed
agreement is the arithmetic mean of the agreement value agri for all items i ? I, defined
as follows:
agri =
{
1 if the two coders assign i to the same category
0 if the two coders assign i to different categories
Observed agreement over the values agri for all items i ? I is then:
Ao =
1
i
?
i?I
agri
For example, let us assume a very simple annotation scheme for dialogue acts in
information-seeking dialogues which makes a binary distinction between the categories
statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core
1997). Two coders classify 100 utterances according to this scheme as shown in Table 1.
Percentage agreement for this data set is obtained by summing up the cells on the
diagonal and dividing by the total number of items: Ao = (20+ 50)/100 = 0.7.
Observed agreement enters in the computation of all the measures of agreement we
consider, but on its own it does not yield values that can be compared across studies,
because some agreement is due to chance, and the amount of chance agreement is
affected by two factors that vary from one study to the other. First of all, as Scott (1955,
page 322) points out, ?[percentage agreement] is biased in favor of dimensions with a
small number of categories.? In other words, given two coding schemes for the same
phenomenon, the one with fewer categories will result in higher percentage agreement
just by chance. If two coders randomly classify utterances in a uniform manner using
the scheme of Table 1, we would expect an equal number of items to fall in each of the
four cells in the table, and therefore pure chance will cause the coders to agree on half of
the items (the two cells on the diagonal: 14 +
1
4 ). But suppose wewant to refine the simple
binary coding scheme by introducing a new category, check, as in the MapTask coding
scheme (Carletta et al 1997). If two coders randomly classify utterances in a uniform
manner using the three categories in the second scheme, they would only agree on a
third of the items (19 +
1
9 +
1
9 ).
Table 1
A simple example of agreement on dialogue act tagging.
CODER A
STAT IREQ TOTAL
STAT 20 20 40
CODER B IREQ 10 50 60
TOTAL 30 70 100
558
Artstein and Poesio Inter-Coder Agreement for CL
The second reason percentage agreement cannot be trusted is that it does not
correct for the distribution of items among categories: We expect a higher percentage
agreement when one category is much more common than the other. This problem,
already raised by Hsu and Field (2003, page 207) among others, can be illustrated
using the following example (Di Eugenio and Glass 2004, example 3, pages 98?99).
Suppose 95% of utterances in a particular domain are statement, and only 5% are info-
request. We would then expect by chance that 0.95? 0.95 = 0.9025 of the utterances
would be classified as statement by both coders, and 0.05 ? 0.05 = 0.0025 as info-
request, so the coders would agree on 90.5% of the utterances. Under such circum-
stances, a seemingly high observed agreement of 90% is actually worse than expected by
chance.
The conclusion reached in the literature is that in order to get figures that are compa-
rable across studies, observed agreement has to be adjusted for chance agreement. These
are the measures we will review in the remainder of this article. We will not look at the
variants of percentage agreement used in CL work on discourse before the introduction
of kappa, such as percentage agreement with an expert and percentage agreement with
the majority; see Carletta (1996) for discussion and criticism.3
2.4 Chance-Corrected Coefficients for Measuring Agreement between Two Coders
All of the coefficients of agreement discussed in this article correct for chance on the
basis of the same idea. First we find how much agreement is expected by chance: Let us
call this value Ae. The value 1?Ae will then measure how much agreement over and
above chance is attainable; the value Ao ?Ae will tell us how much agreement beyond
chance was actually found. The ratio between Ao?Ae and 1?Ae will then tell us which
proportion of the possible agreement beyond chance was actually observed. This idea
is expressed by the following formula.
S,?, ? =
Ao ?Ae
1?Ae
The three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), ? (Scott
1955), and ? (Cohen 1960), and their generalizations, all use this formula; whereas
Krippendorff?s ? is based on a related formula expressed in terms of disagreement
(see Section 2.6). All three coefficients therefore yield values of agreement between
?Ae/1?Ae (no observed agreement) and 1 (observed agreement = 1), with the value 0
signifying chance agreement (observed agreement = expected agreement). Note also
that whenever agreement is less than perfect (Ao < 1), chance-corrected agreement
will be strictly lower than observed agreement, because some amount of agreement
is always expected by chance.
Observed agreement Ao is easy to compute, and is the same for all three
coefficients?the proportion of items on which the two coders agree. But the notion
of chance agreement, or the probability that two coders will classify an arbitrary item
as belonging to the same category by chance, requires a model of what would happen
if coders? behavior was only by chance. All three coefficients assume independence of
the two coders?that is, that the chance of c1 and c2 agreeing on any given category k
3 The extended version of the article also includes a discussion of why ?2 and correlation coefficients are
not appropriate for this task.
559
Computational Linguistics Volume 34, Number 4
Table 2
The value of different coefficients applied to the data from Table 1.
Coefficient Expected agreement Chance-corrected agreement
S 2? ( 12 )
2 = 0.5 (0.7? 0.5)/(1? 0.5) = 0.4
? 0.352 + 0.652 = 0.545 (0.7? 0.545)/(1? 0.545) ? 0.341
? 0.3? 0.4+ 0.6? 0.7 = 0.54 (0.7? 0.54)/(1? 0.54) ? 0.348
Observed agreement for all the coefficients is 0.7.
is the product of the chance of each of them assigning an item to that category:
P(k|c1) ? P(k|c2).4 Expected agreement is then the probability of c1 and c2 agreeing on
any category, that is, the sum of the product over all categories:
ASe = A
?
e = A
?
e = ?
k?K
P(k|c1) ? P(k|c2)
The difference between S, ?, and ? lies in the assumptions leading to the calculation of
P(k|ci), the chance that coder ci will assign an arbitrary item to category k (Zwick 1988;
Hsu and Field 2003).
S: This coefficient is based on the assumption that if coders were operating
by chance alone, we would get a uniform distribution: That is, for any two
coders cm, cn and any two categories kj, kl , P(kj|cm) = P(kl |cn).
?: If coders were operating by chance alone, we would get the same
distribution for each coder: For any two coders cm, cn and any category k,
P(k|cm) = P(k|cn).
?: If coders were operating by chance alone, we would get a separate
distribution for each coder.
Additionally, the lack of independent prior knowledge of the distribution of items
among categories means that the distribution of categories (for ?) and the priors for the
individual coders (for ?) have to be estimated from the observed data. Table 2 demon-
strates the effect of the different chance models on the coefficient values. The remainder
of this section explains how the three coefficients are calculated when the reliability data
come from two coders; we will discuss a variety of proposed generalizations starting in
Section 2.5.
2.4.1 All Categories Are Equally Likely: S. The simplest way of discounting for chance
is the one adopted to compute the coefficient S (Bennett, Alpert, and Goldstein 1954),
also known in the literature as C, ?n, G, and RE (see Zwick 1988; Hsu and Field 2003).
As noted previously, the computation of S is based on an interpretation of chance as
a random choice of category from a uniform distribution?that is, all categories are
equally likely. If coders classify the items into k categories, then the chance P(k|ci) of
4 The independence assumption has been the subject of much criticism, for example by John S. Uebersax.
http://ourworld.compuserve.com/homepages/jsuebersax/agree.htm.
560
Artstein and Poesio Inter-Coder Agreement for CL
any coder assigning an item to category k under the uniformity assumption is 1k ; hence
the total agreement expected by chance is
ASe = ?
k?K
1
k
?
1
k
= k ?
(
1
k
)2
=
1
k
The calculation of the value of S for the figures in Table 1 is shown in Table 2.
The coefficient S is problematic in many respects. The value of the coefficient can
be artificially increased simply by adding spurious categories which the coders would
never use (Scott 1955, pages 322?323). In the case of CL, for example, S would reward
designing extremely fine-grained tagsets, provided that most tags are never actually
encountered in real data. Additional limitations are noted byHsu and Field (2003). It has
been argued that uniformity is the best model for a chance distribution of items among
categories if we have no independent prior knowledge of the distribution (Brennan and
Prediger 1981). However, a lack of prior knowledge does not mean that the distribution
cannot be estimated post hoc, and this is what the other coefficients do.
2.4.2 A Single Distribution: ?. All of the other methods for discounting chance agreement
we discuss in this article attempt to overcome the limitations of S?s strong uniformity
assumption using an idea first proposed by Scott (1955): Use the actual behavior of the
coders to estimate the prior distribution of the categories. As noted earlier, Scott based
his characterization of ? on the assumption that random assignment of categories to
items, by any coder, is governed by the distribution of items among categories in the
actual world. The best estimate of this distribution is P?(k), the observed proportion of
items assigned to category k by both coders.
P(k|c1) = P(k|c2) = P?(k)
P?(k), the observed proportion of items assigned to category k by both coders, is the
total number of assignments to k by both coders nk, divided by the overall number of
assignments, which for the two-coder case is twice the number of items i:
P?(k) =
nk
2i
Given the assumption that coders act independently, expected agreement is computed
as follows.
A?e = ?
k?K
P?(k) ? P?(k) =
?
k?K
(nk
2i
)2
=
1
4i2
?
k?K
n2k
It is easy to show that for any set of coding data, A?e ? A
S
e and therefore ? ? S, with
the limiting case (equality) obtaining when the observed distribution of items among
categories is uniform.
2.4.3 Individual Coder Distributions: ?. The method proposed by Cohen (1960) to calcu-
late expected agreement Ae in his ? coefficient assumes that random assignment of
categories to items is governed by prior distributions that are unique to each coder,
and which reflect individual annotator bias. An individual coder?s prior distribution is
561
Computational Linguistics Volume 34, Number 4
estimated by looking at her actual distribution: P(k|ci), the probability that coder ci will
classify an arbitrary item into category k, is estimated by using P?(k|ci), the proportion
of items actually assigned by coder ci to category k; this is the number of assignments
to k by ci, ncik, divided by the number of items i.
P(k|ci) = P?(k|ci) =
ncik
i
As in the case of S and ?, the probability that the two coders c1 and c2 assign an item to
a particular category k ? K is the joint probability of each coder making this assignment
independently. For ? this joint probability is P?(k|c1) ? P?(k|c2); expected agreement is then
the sum of this joint probability over all the categories k ? K.
A?e = ?
k?K
P?(k|c1) ? P?(k|c2) =
?
k?K
nc1k
i
?
nc2k
i
=
1
i2
?
k?K
nc1knc2k
It is easy to show that for any set of coding data, A?e ? A
?
e and therefore ? ? ?, with the
limiting case (equality) obtaining when the observed distributions of the two coders are
identical. The relationship between ? and S is not fixed.
2.5 More Than Two Coders
In corpus annotation practice, measuring reliability with only two coders is seldom
considered enough, except for small-scale studies. Sometimes researchers run reliability
studies with more than two coders, measure agreement separately for each pair of
coders, and report the average. However, a better practice is to use generalized versions
of the coefficients. A generalization of Scott?s ? is proposed in Fleiss (1971), and a
generalization of Cohen?s ? is given in Davies and Fleiss (1982). We will call these
coefficients multi-? and multi-?, respectively, dropping the multi-prefixes when no
confusion is expected to arise.5
2.5.1 Fleiss?s Multi-?. With more than two coders, the observed agreement Ao can no
longer be defined as the percentage of items on which there is agreement, because
inevitably there will be items on which some coders agree and others disagree. The
solution proposed in the literature is to measure pairwise agreement (Fleiss 1971):
Define the amount of agreement on a particular item as the proportion of agreeing
judgment pairs out of the total number of judgment pairs for that item.
Multiple coders also pose a problem for the visualization of the data. When the
number of coders c is greater than two, judgments cannot be shown in a contingency
table like Table 1, because each coder has to be represented in a separate dimension.
5 Due to historical accident, the terminology in the literature is confusing. Fleiss (1971) proposed a
coefficient of agreement for multiple coders and called it ?, even though it calculates expected agreement
based on the cumulative distribution of judgments by all coders and is thus better thought of as a
generalization of Scott?s ?. This unfortunate choice of name was the cause of much confusion in
subsequent literature: Often, studies which claim to give a generalization of ? to more than two coders
actually report Fleiss?s coefficient (e.g., Bartko and Carpenter 1976; Siegel and Castellan 1988; Di Eugenio
and Glass 2004). Since Carletta (1996) introduced reliability to the CL community based on the definitions
of Siegel and Castellan (1988), the term ?kappa? has been usually associated in this community with
Siegel and Castellan?s K, which is in effect Fleiss?s coefficient, that is, a generalization of Scott?s ?.
562
Artstein and Poesio Inter-Coder Agreement for CL
Fleiss (1971) therefore uses a different type of table which lists each item with the num-
ber of judgments it received for each category; Siegel and Castellan (1988) use a similar
table, which Di Eugenio and Glass (2004) call an agreement table. Table 3 is an example
of an agreement table, in which the same 100 utterances from Table 1 are labeled by
three coders instead of two. Di Eugenio and Glass (page 97) note that compared to
contingency tables like Table 1, agreement tables like Table 3 lose information because
they do not say which coder gave each judgment. This information is not used in the
calculation of ?, but is necessary for determining the individual coders? distributions in
the calculation of ?. (Agreement tables also add information compared to contingency
tables, namely, the identity of the items that make up each contingency class, but this
information is not used in the calculation of either ? or ?.)
Let nik stand for the number of times an item i is classified in category k (i.e., the
number of coders that make such a judgment): For example, given the distribution in
Table 3, nUtt1Stat = 2 and nUtt1IReq = 1. Each category k contributes (
nik
2 ) pairs of agreeing
judgments for item i; the amount of agreement agri for item i is the sum of (
nik
2 ) over all
categories k ? K, divided by (c2), the total number of judgment pairs per item.
agri =
1
(c2)
?
k?K
(
nik
2
)
=
1
c(c? 1) ?
k?K
nik(nik ? 1)
For example, given the results in Table 3, we find the agreement value for Utterance 1
as follows.
agr1 =
1
(32)
((
nUtt1Stat
2
)
+
(
nUtt1IReq
2
))
=
1
3
(1+ 0) ? 0.33
The overall observed agreement is the mean of agri for all items i ? I.
Ao =
1
i
?
i?I
agri =
1
ic(c? 1) ?
i?I
?
k?K
nik(nik ? 1)
(Notice that this definition of observed agreement is equivalent to the mean of the
two-coder observed agreement values from Section 2.4 for all coder pairs.)
If observed agreement is measured on the basis of pairwise agreement (the pro-
portion of agreeing judgment pairs), it makes sense to measure expected agreement in
terms of pairwise comparisons as well, that is, as the probability that any pair of judg-
ments for an item would be in agreement?or, said otherwise, the probability that two
Table 3
Agreement table with three coders.
STAT IREQ
Utt1 2 1
Utt2 0 3
...
Utt100 1 2
TOTAL 90 (0.3) 210 (0.7)
563
Computational Linguistics Volume 34, Number 4
arbitrary coders would make the same judgment for a particular item by chance. This is
the approach taken by Fleiss (1971). Like Scott, Fleiss interprets ?chance agreement? as
the agreement expected on the basis of a single distribution which reflects the combined
judgments of all coders, meaning that expected agreement is calculated using P?(k), the
overall proportion of items assigned to category k, which is the total number of such
assignments by all coders nk divided by the overall number of assignments. The latter,
in turn, is the number of items imultiplied by the number of coders c.
P?(k) =
1
ic
nk
As in the two-coder case, the probability that two arbitrary coders assign an item to a
particular category k ? K is assumed to be the joint probability of each coder making
this assignment independently, that is (P?(k))2. The expected agreement is the sum of
this joint probability over all the categories k ? K.
A?e = ?
k?K
(
P?(k)
)2
=
?
k?K
(
1
ic
nk
)2
=
1
(ic)2
?
k?K
n2k
Multi-? is the coefficient that Siegel and Castellan (1988) call K.
2.5.2 Multi-?. It is fairly straightforward to adapt Fleiss?s proposal to generalize
Cohen?s ? proper to more than two coders, calculating expected agreement based on
individual coder marginals. A detailed proposal can be found in Davies and Fleiss
(1982), or in the extended version of this article.
2.6 Krippendorff?s ? and Other Weighted Agreement Coefficients
A serious limitation of both ? and ? is that all disagreements are treated equally. But
especially for semantic and pragmatic features, disagreements are not all alike. Even for
the relatively simple case of dialogue act tagging, a disagreement between an accept
and a reject interpretation of an utterance is clearly more serious than a disagreement
between an info-request and a check. For tasks such as anaphora resolution, where
reliability is determined by measuring agreement on sets (coreference chains), allowing
for degrees of disagreement becomes essential (see Section 4.4). Under such circum-
stances, ? and ? are not very useful.
In this section we discuss two coefficients that make it possible to differentiate
between types of disagreements: ? (Krippendorff 1980, 2004a), which is a coefficient
defined in a general way that is appropriate for use with multiple coders, different
magnitudes of disagreement, and missing values, and is based on assumptions similar
to those of ?; and weighted kappa ?w (Cohen 1968), a generalization of ?.
2.6.1 Krippendorff?s ?. The coefficient ? (Krippendorff 1980, 2004a) is an extremely ver-
satile agreement coefficient based on assumptions similar to ?, namely, that expected
agreement is calculated by looking at the overall distribution of judgments without
regard to which coders produced these judgments. It applies to multiple coders, and
it allows for different magnitudes of disagreement. When all disagreements are con-
sidered equal it is nearly identical to multi-?, correcting for small sample sizes by
using an unbiased estimator for expected agreement. In this section we will present
564
Artstein and Poesio Inter-Coder Agreement for CL
Krippendorff?s ? and relate it to the other coefficients discussed in this article, but we
will start with ??s origins as a measure of variance, following a long tradition of using
variance to measure reliability (see citations in Rajaratnam 1960; Krippendorff 1970).
A sample?s variance s2 is defined as the sum of square differences from the mean
SS =
?
(x ? x?)2 divided by the degrees of freedom df . Variance is a useful way of
looking at agreement if coders assign numerical values to the items, as in magnitude
estimation tasks. Each item in a reliability study can be considered a separate level
in a single-factor analysis of variance: The smaller the variance around each level, the
higher the reliability. When agreement is perfect, the variance within the levels (s2within)
is zero; when agreement is at chance, the variance within the levels is equal to the
variance between the levels, in which case it is also equal to the overall variance of the
data: s2within = s
2
between = s
2
total. The ratios s
2
within/s
2
between (that is, 1/F) and s
2
within/s
2
total are
therefore 0 when agreement is perfect and 1 when agreement is at chance. Additionally,
the latter ratio is bounded at 2: SSwithin ? SStotal by definition, and df total < 2 df within
because each item has at least two judgments. Subtracting the ratio s2within/s
2
total from 1
yields a coefficient which ranges between?1 and 1, where 1 signifies perfect agreement
and 0 signifies chance agreement.
? = 1?
s2within
s2
total
= 1?
SSwithin/df within
SStotal/df total
We can unpack the formula for ? to bring it to a form which is similar to the other
coefficients we have looked at, and which will allow generalizing ? beyond simple
numerical values. The first step is to get rid of the notion of arithmetic meanwhich lies at
the heart of the measure of variance. We observe that for any set of numbers x1, . . . , xN
with a mean x? = 1N ?
N
n=1 xn, the sum of square differences from the mean SS can be
expressed as the sum of square of differences between all the (ordered) pairs of numbers,
scaled by a factor of 1/2N.
SS =
N
?
n=1
(xn ? x?)2 =
1
2N
N
?
n=1
N
?
m=1
(xn ? xm)2
For calculating ? we considered each item to be a separate level in an analysis of
variance; the number of levels is thus the number of items i, and because each coder
marks each item, the number of observations for each item is the number of coders c.
Within-level variance is the sum of the square differences from the mean of each item,
SSwithin = ?i ?c(xic ? x?i)2, divided by the degrees of freedom df within = i(c ? 1). We
can express this as the sum of the squares of the differences between all of the judgment
pairs for each item, summed over all items and scaled by the appropriate factor. We use
the notation xic for the value given by coder c to item i, and x?i for the mean of all the
values given to item i.
s2within =
SSwithin
df within
=
1
i(c? 1) ?
i?I
?
c?C
(xic ? x?i)
2 =
1
2ic(c? 1) ?
i?I
c
?
m=1
c
?
n=1
(xicm ? xicn)
2
The total variance is the sum of the square differences of all judgments from the grand
mean, SStotal = ?i ?c(xic ? x?)2, divided by the degrees of freedom df total = ic? 1. This
565
Computational Linguistics Volume 34, Number 4
can be expressed as the sum of the squares of the differences between all of the judg-
ments pairs without regard to items, again scaled by the appropriate factor. The notation
x? is the overall mean of all the judgments in the data.
s2total =
SStotal
df total
=
1
ic? 1?
i?I
?
c?C
(xic ? x?)
2 =
1
2ic(ic? 1)
i
?
j=1
c
?
m=1
i
?
l=1
c
?
n=1
(xijcm ? xil cn)
2
Now that we have removed references tomeans from our formulas, we can abstract over
the measure of variance. We define a distance function d which takes two numbers and
returns the square of their difference.
dab = (a ? b)
2
We also simplify the computation by counting all the identical value assignments
together. Each unique value used by the coders will be considered a category k ? K.
We use nik for the number of times item i is given the value k, that is, the number of
coders that make such a judgment. For every (ordered) pair of distinct values ka, kb ? K
there are nikanikb pairs of judgments of item i, whereas for non-distinct values there
are nika(nika ? 1) pairs. We use this notation to rewrite the formula for the within-level
variance. D?o, the observed disagreement for ?, is defined as twice the variance within
the levels in order to get rid of the factor 2 in the denominator; we also simplify the
formula by using the multiplier nikanika for identical categories?this is allowed because
dkk = 0 for all k.
D?o = 2 s
2
within =
1
ic(c? 1) ?
i?I
k
?
j=1
k
?
l=1
nikjnikldkjkl
We perform the same simplification for the total variance, where nk stands for the
total number of times the value k is assigned to any item by any coder. The expected
disagreement for ?, D?e , is twice the total variance.
D?e = 2 s
2
total =
1
ic(ic? 1)
k
?
j=1
k
?
l=1
nkjnkldkjkl
Because both expected and observed disagreement are twice the respective vari-
ances, the coefficient ? retains the same form when expressed with the disagreement
values.
? = 1?
Do
De
Now that ? has been expressed without explicit reference to means, differences, and
squares, it can be generalized to a variety of coding schemes in which the labels cannot
be interpreted as numerical values: All one has to do is to replace the square difference
function d with a different distance function. Krippendorff (1980, 2004a) offers distance
metrics suitable for nominal, interval, ordinal, and ratio scales. Of particular interest is
566
Artstein and Poesio Inter-Coder Agreement for CL
the function for nominal categories, that is, a function which considers all distinct labels
equally distant from one another.
dab =
{
0 if a = b
1 if a = b
It turns out that with this distance function, the observed disagreement D?o is exactly the
complement of the observed agreement of Fleiss?s multi-?, 1? A?o , and the expected
disagreement D?e differs from 1 ? A
?
e by a factor of (ic ? 1)/ic; the difference is due
to the fact that ? uses a biased estimator of the expected agreement in the population
whereas ? uses an unbiased estimator. The following equation shows that given the
correspondence between observed and expected agreement and disagreement, the co-
efficients themselves are nearly equivalent.
? = 1?
D?o
D?e
? 1?
1?A?o
1?A?e
=
1?A?e ? (1?A
?
o )
1?A?e
=
A?o ?A
?
e
1?A?e
= ?
For nominal data, the coefficients ? and ? approach each other as either the number of
items or the number of coders approaches infinity.
Krippendorff?s ? will work with any distance metric, provided that identical cat-
egories always have a distance of zero (dkk = 0 for all k). Another useful constraint is
symmetry (dab = dba for all a, b). This flexibility affords new possibilities for analysis,
which we will illustrate in Section 4. We should also note, however, that the flexibility
also creates new pitfalls, especially in cases where it is not clear what the natural dis-
tance metric is. For example, there are different ways to measure dissimilarity between
sets, and any of these measures can be justifiably used when the category labels are
sets of items (as in the annotation of anaphoric relations). The different distance metrics
yield different values of ? for the same annotation data, making it difficult to interpret
the resulting values. We will return to this problem in Section 4.4.
2.6.2 Cohen?s ?w. A weighted variant of Cohen?s ? is presented in Cohen (1968). The
implementation of weights is similar to that of Krippendorff?s ??each pair of cate-
gories ka, kb ? K is associated with a weight dkakb , where a larger weight indicates more
disagreement (Cohen uses the notation v; he does not place any general constraints on
the weights?not even a requirement that a pair of identical categories have a weight of
zero, or that the weights be symmetric across the diagonal). The coefficient is defined
for two coders: The disagreement for a particular item i is the weight of the pair of
categories assigned to it by the two coders, and the overall observed disagreement is
the (normalized) mean disagreement of all the items. Let k(cn, i) denote the category
assigned by coder cn to item i; then the disagreement for item i is disagri = dk(c1,i)k(c2,i).
The observed disagreement Do is the mean of disagri for all items i, normalized to the
interval [0, 1] through division by the maximal weight dmax.
D?wo =
1
dmax
1
i
?
i?I
disagri =
1
dmax
1
i
?
i?I
dk(c1,i)k(c2,i)
If we take all disagreements to be of equal weight, that is dkaka = 0 for all categories ka
and dkakb = 1 for all ka = kb, then the observed disagreement is exactly the complement
of the observed agreement as calculated in Section 2.4: D?wo = 1?A?o.
567
Computational Linguistics Volume 34, Number 4
Like ?, the coefficient ?w interprets expected disagreement as the amount expected
by chance from a distinct probability distribution for each coder. These individual
distributions are estimated by P?(k|c), the proportion of items assigned by coder c to
category k, that is the number of such assignments nck divided by the number of items i.
P?(k|c) =
1
i
nck
The probability that coder c1 assigns an item to category ka and coder c2 assigns it to
category kb is the joint probability of each coder making this assignment independently,
namely, P?(ka|c1)P?(kb|c2). The expected disagreement is the mean of the weights for
all (ordered) category pairs, weighted by the probabilities of the category pairs and
normalized to the interval [0, 1] through division by the maximal weight.
D?we =
1
dmax
k
?
j=1
k
?
l=1
P?(kj|c1)P?(kl |c2)dkjkl =
1
dmax
1
i2
k
?
j=1
k
?
l=1
nc1kjnc2kldkjkl
If we take all disagreements to be of equal weight then the expected disagreement is
exactly the complement of the expected agreement for ? as calculated in Section 2.4:
D?we = 1?A?e.
Finally, the coefficient ?w itself is the ratio of observed disagreement to expected
disagreement, subtracted from 1 in order to yield a final value in terms of agreement.
?w = 1?
Do
De
2.7 An Integrated Example
We end this section with an example illustrating how all of the agreement coefficients
just discussed are computed. To facilitate comparisons, all computations will be based
on the annotation statistics in Table 4. This confusion matrix reports the results of an
experiment where two coders classify a set of utterances into three categories.
2.7.1 The Unweighted Coefficients. Observed agreement for all of the unweighted coeffi-
cients (S, ?, and ?) is calculated by counting the items on which the coders agree (the
Table 4
An integrated coding example.
CODER A
STAT IREQ CHCK TOTAL
STAT 46 6 0 52
IREQ 0 32 0 32
CODER B CHCK 0 6 10 16
TOTAL 46 44 10 100
568
Artstein and Poesio Inter-Coder Agreement for CL
figures on the diagonal of the confusion matrix in Table 4) and dividing by the total
number of items.
Ao =
46+ 32+ 10
100
= 0.88
The expected agreement values and the resulting values for the coefficients are shown in
Table 5. The values of ? and ? are very similar, which is to be expected when agreement
is high, because this implies similar marginals. Notice that A?e < A
?
e , hence ? > ?; this
reflects a general property of ? and ?, already mentioned in Section 2.4, which will be
elaborated in Section 3.1.
2.7.2 Weighted Coefficients. Suppose we notice that whereas Statement and Info-
Request are clearly distinct classifications, Check is somewhere between the two. We
therefore opt to weigh the distances between the categories as follows (recall that
1 denotes maximal disagreement, and identical categories are in full agreement and
thus have a distance of 0).
Statement Info-Request Check
Statement 0 1 0.5
Info-Request 1 0 0.5
Check 0.5 0.5 0
The observed disagreement is calculated by summing up all the cells in the contingency
table, multiplying each cell by its respective weight, and dividing the total by the
number of items (in the following calculation we ignore cells with zero items).
Do =
46? 0+ 6? 1+ 32? 0+ 6? 0.5+ 10? 0
100
=
6+ 3
100
= 0.09
The only sources of disagreement in the coding example of Table 4 are the six utterances
marked as Info-Requests by coder A and Statements by coder B, which receive the
maximal weight of 1, and the six utterances marked as Info-Requests by coder A and
Checks by coder B, which are given a weight of 0.5.
The calculation of expected disagreement for the weighted coefficients is shown in
Table 6, and is the sum of the expected disagreement for each category pair multiplied
Table 5
Unweighted coefficients for the data from Table 4.
Expected agreement Chance-corrected agreement
S 3? ( 13 )
2 = 13 (0.88?
1
3 )/(1?
1
3 ) = 0.82
?
0.46+0.52
2 +
0.44+0.32
2 +
0.10+0.16
2 = 0.4014 (0.88? 0.4014)/(1? 0.4014) ? 0.7995
? .46? .52+ .44? .32+ .1? .16 = 0.396 (0.88? 0.396)/(1? 0.396) ? 0.8013
569
Computational Linguistics Volume 34, Number 4
Table 6
Expected disagreement of the weighted coefficients for the data from Table 4.
D?e
(46+52)?(46+52)
2?100?(2?100?1) ? 0+
(44+32)?(46+52)
2?100?(2?100?1) ? 1 +
(10+16)?(46+52)
2?100?(2?100?1) ?
1
2
+
(46+52)?(44+32)
2?100?(2?100?1) ? 1 +
(44+32)?(44+32)
2?100?(2?100?1) ? 0 +
(10+16)?(44+32)
2?100?(2?100?1) ?
1
2
+
(46+52)?(10+16)
2?100?(2?100?1) ?
1
2 +
(44+32)?(10+16)
2?100?(2?100?1) ?
1
2 +
(10+16)?(10+16)
2?100?(2?100?1) ? 0
0.4879
D?we
46?52
100?100 ? 0+
44?52
100?100 ? 1 +
10?52
100?100 ?
1
2
+ 46?32100?100 ? 1 +
44?32
100?100 ? 0 +
10?32
100?100 ?
1
2
+ 46?16100?100 ?
1
2 +
44?16
100?100 ?
1
2 +
10?16
100?100 ? 0
0.49
by its weight. The value of the weighted coefficients is given by the formula 1? DoDe , so
? ? 1? 0.090.4879 ? 0.8156, and ?w = 1?
0.09
0.49 ? 0.8163.
3. Bias and Prevalence
Two issues recently raised by Di Eugenio and Glass (2004) concern the behavior of
agreement coefficients when the annotation data are severely skewed. One issue, which
Di Eugenio and Glass call the bias problem, is that ? and ? yield quite different
numerical values when the annotators? marginal distributions are widely divergent;
the other issue, the prevalence problem, is the exceeding difficulty in getting high
agreement values when most of the items fall under one category. Looking at these two
problems in detail is useful for understanding the differences between the coefficients.
3.1 Annotator Bias
The difference between ? and ? on the one hand and ? on the other hand lies in the
interpretation of the notion of chance agreement, whether it is the amount expected
from the the actual distribution of items among categories (?) or from individual coder
priors (?). As mentioned in Section 2.4, this difference has been the subject of much
debate (Fleiss 1975; Krippendorff 1978, 2004b; Byrt, Bishop, andCarlin 1993; Zwick 1988;
Hsu and Field 2003; Di Eugenio and Glass 2004; Craggs and McGee Wood 2005).
A claim often repeated in the literature is that single-distribution coefficients like
? and ? assume that different coders produce similar distributions of items among
categories, with the implication that these coefficients are inapplicable when the anno-
tators show substantially different distributions. Recommendations vary: Zwick (1988)
suggests testing the individual coders? distributions using the modified ?2 test of Stuart
(1955), and discarding the annotation as unreliable if significant systematic discrepan-
cies are observed. In contrast, Hsu and Field (2003, page 214) recommend reporting
the value of ? even when the coders produce different distributions, because it is ?the
only [index] . . . that could legitimately be applied in the presence of marginal hetero-
geneity?; likewise, Di Eugenio andGlass (2004, page 96) recommend using ? in ?the vast
majority . . . of discourse- and dialogue-tagging efforts? where the individual coders?
distributions tend to vary. All of these proposals are based on a misconception: that
570
Artstein and Poesio Inter-Coder Agreement for CL
single-distribution coefficients require similar distributions by the individual annota-
tors in order to work properly. This is not the case. The difference between the coeffi-
cients is only in the interpretation of ?chance agreement?: ?-style coefficients calculate
the chance of agreement among arbitrary coders, whereas ?-style coefficients calcu-
late the chance of agreement among the coders who produced the reliability data. There-
fore, the choice of coefficient should not depend on the magnitude of the divergence
between the coders, but rather on the desired interpretation of chance agreement.
Another common claim is that individual-distribution coefficients like ? ?reward?
annotators for disagreeing on the marginal distributions. For example, Di Eugenio and
Glass (2004, page 99) say that ? suffers from what they call the bias problem, described
as ?the paradox that ?Co [our ?] increases as the coders become less similar.? Similar
reservations about the use of ? have been noted by Brennan and Prediger (1981) and
Zwick (1988). However, the bias problem is less paradoxical than it sounds. Although
it is true that for a fixed observed agreement, a higher difference in coder marginals
implies a lower expected agreement and therefore a higher ? value, the conclusion that
? penalizes coders for having similar distributions is unwarranted. This is because Ao
and Ae are not independent: Both are drawn from the same set of observations. What
? does is discount some of the disagreement resulting from different coder marginals by
incorporating it into Ae. Whether this is desirable depends on the application for which
the coefficient is used.
Themost common application of agreement measures in CL is to infer the reliability
of a large-scale annotation, where typically each piece of data will be marked by just
one coder, by measuring agreement on a small subset of the data which is annotated
by multiple coders. In order to make this generalization, the measure must reflect the
reliability of the annotation procedure, which is independent of the actual annotators
used. Reliability, or reproducibility of the coding, is reduced by all disagreements?both
random and systematic. The most appropriate measures of reliability for this purpose
are therefore single-distribution coefficients like ? and ?, which generalize over the
individual coders and exclude marginal disagreements from the expected agreement.
This argument has been presented recently in much detail by Krippendorff (2004b) and
reiterated by Craggs and McGee Wood (2005).
At the same time, individual-distribution coefficients like ? provide important in-
formation regarding the trustworthiness (validity) of the data on which the annotators
agree. As an intuitive example, think of a person who consults two analysts when
deciding whether to buy or sell certain stocks. If one analyst is an optimist and tends to
recommend buying whereas the other is a pessimist and tends to recommend selling,
they are likely to agree with each other less than two more neutral analysts, so overall
their recommendations are likely to be less reliable?less reproducible?than those that
come from a population of like-minded analysts. This reproducibility is measured by ?.
But whenever the optimistic and pessimistic analysts agree on a recommendation for
a particular stock, whether it is ?buy? or ?sell,? the confidence that this is indeed the
right decision is higher than the same advice from two like-minded analysts. This is
why ? ?rewards? biased annotators: it is not a matter of reproducibility (reliability) but
rather of trustworthiness (validity).
Having said this, we should point out that, first, in practice the difference between
? and ? doesn?t often amount to much (see discussion in Section 4). Moreover, the
difference becomes smaller as agreement increases, because all the points of agreement
contribute toward making the coder marginals similar (it took a lot of experimentation
to create data for Table 4 so that the values of ? and ? would straddle the conventional
cutoff point of 0.80, and even so the difference is very small). Finally, one would expect
571
Computational Linguistics Volume 34, Number 4
the difference between ? and ? to diminish as the number of coders grows; this is shown
subsequently.6
We define B, the overall annotator bias in a particular set of coding data, as the
difference between the expected agreement according to (multi)-? and the expected
agreement according to (multi)-?. Annotator bias is a measure of variance: If we take c to
be a random variable with equal probabilities for all coders, then the annotator bias B
is the sum of the variances of P(k|c) for all categories k ? K, divided by the number of
coders c less one (see Artstein and Poesio [2005] for a proof).
B = A?e ?A
?
e =
1
c? 1 ?
k?K
?
2
P?(k|c)
Annotator bias can be used to express the difference between ? and ?.
? ? ? =
Ao ? (A?e ? B)
1? (A?e ? B)
?
Ao ?A?e
1?A?e
= B ?
(1?Ao)
(1?A?e)(1?A?e )
This allows us to make the following observations about the relationship between
? and ?.
Observation 1. The difference between ? and ? grows as the annotator bias grows: For a
constant Ao and A
?
e , a greater B implies a greater value for ? ? ?.
Observation 2. The greater the number of coders, the lower the annotator bias B, and hence
the lower the difference between ? and ?, because the variance of P?(k|c) does not increase in
proportion to the number of coders.
In other words, provided enough coders are used, it should not matter whether a
single-distribution or individual-distribution coefficient is used. This is not to imply that
multiple coders increase reliability: The variance of the individual coders? distributions
can be just as large with many coders as with few coders, but its effect on the value
of ? decreases as the number of coders grows, and becomes more similar to random
noise.
The same holds for weighted measures too; see the extended version of this article
for definitions and proof. In an annotation study with 18 subjects, we compared ? with
a variant which uses individual coder distributions to calculate expected agreement,
and found that the values never differed beyond the third decimal point (Poesio and
Artstein 2005).
We conclude with a summary of our views concerning the difference between ?-
style and ?-style coefficients. First of all, keep in mind that empirically the difference
is small, and gets smaller as the number of annotators increases. Then instead of
reporting two coefficients, as suggested by Di Eugenio and Glass (2004), the appropriate
coefficient should be chosen based on the task (not on the observed differences between
coder marginals). When the coefficient is used to assess reliability, a single-distribution
coefficient like ? or ? should be used; this is indeed already the practice in CL, because
Siegel and Castellan?s K is identical with (multi-)?. It is also good practice to test
6 Craggs and McGee Wood (2005) also suggest increasing the number of coders in order to overcome
individual annotator bias, but do not provide a mathematical justification.
572
Artstein and Poesio Inter-Coder Agreement for CL
reliability withmore than two coders, in order to reduce the likelihood of coders sharing
a deviant reading of the annotation guidelines.
3.2 Prevalence
We touched upon the matter of skewed data in Section 2.3 when we motivated the need
for chance correction: If a disproportionate amount of the data falls under one category,
then the expected agreement is very high, so in order to demonstrate high reliability
an even higher observed agreement is needed. This leads to the so-called paradox that
chance-corrected agreement may be low even though Ao is high (Cicchetti and Feinstein
1990; Feinstein and Cicchetti 1990; Di Eugenio and Glass 2004). Moreover, when the
data are highly skewed in favor of one category, the high agreement also corresponds
to high accuracy: If, say, 95% of the data fall under one category label, then random
coding would cause two coders to jointly assign this category label to 90.25% of the
items, and on average 95% of these labels would be correct, for an overall accuracy of at
least 85.7%. This leads to the surprising result that when data are highly skewed, coders
may agree on a high proportion of items while producing annotations that are indeed
correct to a high degree, yet the reliability coefficients remain low. (For an illustration,
see the discussion of agreement results on coding discourse segments in Section 4.3.1.)
This surprising result is, however, justified. Reliability implies the ability to dis-
tinguish between categories, but when one category is very common, high accuracy
and high agreement can also result from indiscriminate coding. The test for reliabil-
ity in such cases is the ability to agree on the rare categories (regardless of whether
these are the categories of interest). Indeed, chance-corrected coefficients are sensitive
to agreement on rare categories. This is easiest to see with a simple example of two
coders and two categories, one common and the other one rare; to further simplify the
calculation we also assume that the coder marginals are identical, so that ? and ? yield
the same values. We can thus represent the judgments in a contingency table with just
two parameters:  is half the proportion of items on which there is disagreement, and
? is the proportion of agreement on the Rare category. Both of these proportions are
assumed to be small, so the bulk of the items (a proportion of 1? (? + 2)) are labeled
with the Common category by both coders (Table 7). From this table we can calculate
Ao = 1? 2 and Ae = 1? 2(? + ) + 2(? + )2, as well as ? and ?.
?, ? =
1? 2? (1? 2(? + ) + 2(? + )2)
1? (1? 2(? + ) + 2(? + )2)
=
?
? + 
?

1? (? + )
When  and ? are both small, the fraction after the minus sign is small as well, so ? and ?
are approximately ?/(? + ): the value we get if we take all the items marked by one
Table 7
A simple example of agreement on dialogue act tagging.
CODER A
COMMON RARE TOTAL
COMMON 1? (? + 2)  1? (? + )
CODER B RARE  ? ? + 
TOTAL 1? (? + ) ? +  1
573
Computational Linguistics Volume 34, Number 4
particular coder asRare, and calculate what proportion of those itemswere labeledRare
by the other coder. This is a measure of the coders? ability to agree on the rare category.
4. Using Agreement Measures for CL Annotation Tasks
In this section we review the use of intercoder agreement measures in CL since
Carletta?s original paper in light of the discussion in the previous sections. We begin
with a summary of Krippendorff?s recommendations about measuring reliability
(Krippendorff 2004a, Chapter 11), then discuss how coefficients of agreement have
been used in CL to measure the reliability of annotation schemes, focusing in particular
on the types of annotation where there has been some debate concerning the most
appropriate measures of agreement.
4.1 Methodology and Interpretation of the Results: General Issues
Krippendorff (2004a, Chapter 11) notes with regret the fact that reliability is discussed in
only around 69% of studies in content analysis. In CL as well, not all annotation projects
include a formal test of intercoder agreement. Some of the best known annotation
efforts, such as the creation of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini
1993) and the British National Corpus (Leech, Garside, and Bryant 1994), do not report
reliability results as they predate the Carletta paper; but even among the more recent
efforts, many only report percentage agreement, as for the creation of the PropBank
(Palmer, Dang, and Fellbaum 2007) or the ongoing OntoNotes annotation (Hovy et al
2006). Even more importantly, very few studies apply a methodology as rigorous as
that envisaged by Krippendorff and other content analysts. We therefore begin this
discussion of CL practice with a summary of the main recommendations found in
Chapter 11 of Krippendorff (2004a), even though, as we will see, we think that some
of these recommendations may not be appropriate for CL.
4.1.1 Generating Data to Measure Reproducibility. Krippendorff?s recommendations were
developed for the field of content analysis, where coding is used to draw conclusions
from the texts. A coded corpus is thus akin to the result of a scientific experiment, and
it can only be considered valid if it is reproducible?that is, if the same coded results
can be replicated in an independent coding exercise. Krippendorff therefore argues that
any study using observed agreement as a measure of reproducibility must satisfy the
following requirements:
? It must employ an exhaustively formulated, clear, and usable coding
scheme together with step-by-step instructions on how to use it.
? It must use clearly specified criteria concerning the choice of coders
(so that others may use such criteria to reproduce the data).
? It must ensure that the coders that generate the data used to measure
reproducibility work independently of each other.
Some practices that are common in CL do not satisfy these requirements. The first
requirement is violated by the practice of expanding the written coding instructions
and including new rules as the data are generated. The second requirement is often
574
Artstein and Poesio Inter-Coder Agreement for CL
violated by using experts as coders, particularly long-term collaborators, as such coders
may agree not because they are carefully following written instructions, but because
they know the purpose of the research very well?which makes it virtually impossible
for others to reproduce the results on the basis of the same coding scheme (the prob-
lems arising when using experts were already discussed at length in Carletta [1996]).
Practices which violate the third requirement (independence) include asking coders to
discuss their judgments with each other and reach their decisions by majority vote, or
to consult with each other when problems not foreseen in the coding instructions arise.
Any of these practices make the resulting data unusable for measuring reproducibility.
Krippendorff?s own summary of his recommendations is that to obtain usable
data for measuring reproducibility a researcher must use data generated by three or
more coders, chosen according to some clearly specified criteria, and working indepen-
dently according to a written coding scheme and coding instructions fixed in advance.
Krippendorff also discusses the criteria to be used in the selection of the sample, from
the minimum number of units (obtained using a formula from Bloch and Kraemer
[1989], reported in Krippendorff [2004a, page 239]), to how to make the sample rep-
resentative of the data population (each category should occur in the sample often
enough to yield at least five chance agreements), to how to ensure the reliability of the
instructions (the sample should contain examples of all the values for the categories).
These recommendations are particularly relevant in light of the comments of Craggs
and McGee Wood (2005, page 290), which discourage researchers from testing their
coding instructions on data from more than one domain. Given that the reliability of
the coding instructions depends to a great extent on how complications are dealt with,
and that every domain displays different complications, the sample should contain
sufficient examples from all domains which have to be annotated according to the
instructions.
4.1.2 Establishing Significance. In hypothesis testing, it is common to test for the sig-
nificance of a result against a null hypothesis of chance behavior; for an agreement
coefficient this would mean rejecting the possibility that a positive value of agreement
is nevertheless due to random coding. We can rely on the statement by Siegel and
Castellan (1988, Section 9.8.2) that when sample sizes are large, the sampling distribu-
tion of K (Fleiss?s multi-?) is approximately normal and centered around zero?this
allows testing the obtained value of K against the null hypothesis of chance agreement
by using the z statistic. It is also easy to test Krippendorff?s ? with the interval distance
metric against the null hypothesis of chance agreement, because the hypothesis ? = 0 is
identical to the hypothesis F = 1 in an analysis of variance.
However, a null hypothesis of chance agreement is not very interesting, and demon-
strating that agreement is significantly better than chance is not enough to establish
reliability. This has already been pointed out by Cohen (1960, page 44): ?to knowmerely
that ? is beyond chance is trivial since one usually expects much more than this in the
way of reliability in psychological measurement.? The same point has been repeated
and stressed in many subsequent works (e.g., Posner et al 1990; Di Eugenio 2000;
Krippendorff 2004a): The reason for measuring reliability is not to test whether coders
perform better than chance, but to ensure that the coders do not deviate too much from
perfect agreement (Krippendorff 2004a, page 237).
The relevant notion of significance for agreement coefficients is therefore a confi-
dence interval. Cohen (1960, pages 43?44) implies that when sample sizes are large,
the sampling distribution of ? is approximately normal for any true population value
of ?, and therefore confidence intervals for the observed value of ? can be determined
575
Computational Linguistics Volume 34, Number 4
using the usual multiples of the standard error. Donner and Eliasziw (1987) propose
a more general form of significance test for arbitrary levels of agreement. In contrast,
Krippendorff (2004a, Section 11.4.2) states that the distribution of ? is unknown, so
confidence intervals must be obtained by bootstrapping; a software package for doing
this is described in Hayes and Krippendorff (2007).
4.1.3 Interpreting the Value of Kappa-Like Coefficients. Even after testing significance and
establishing confidence intervals for agreement coefficients, we are still faced with the
problem of interpreting the meaning of the resulting values. Suppose, for example, we
establish that for a particular task, K = 0.78? 0.05. Is this good or bad? Unfortunately,
deciding what counts as an adequate level of agreement for a specific purpose is still
little more than a black art: As we will see, different levels of agreement may be
appropriate for resource building and for more linguistic purposes.
The problem is not unlike that of interpreting the values of correlation coefficients,
and in the area of medical diagnosis, the best known conventions concerning the value
of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in
Figure 1, are indeed similar to those used for correlation coefficients, where values
above 0.4 are also generally considered adequate (Marion 2004). Many medical re-
searchers feel that these conventions are appropriate, and in language studies, a similar
interpretation of the values has been proposed by Rietveld and van Hout (1993). In
CL, however, most researchers follow the more stringent conventions from content
analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996,
page 252): ?content analysis researchers generally think of K > .8 as good reliability,
with .67 < K < .8 allowing tentative conclusions to be drawn? (Krippendorff was dis-
cussing values of ? rather than K, but the coefficients are nearly equivalent for cate-
gorical labels). As a result, ever since Carletta?s influential paper, CL researchers have
attempted to achieve a value of K (more seldom, of ?) above the 0.8 threshold, or, failing
that, the 0.67 level allowing for ?tentative conclusions.? However, the description of
the 0.67 boundary in Krippendorff (1980) was actually ?highly tentative and cautious,?
and in later work Krippendorff clearly considers 0.8 the absolute minimum value of
? to accept for any serious purpose: ?Even a cutoff point of ? = .800 . . . is a pretty
low standard? (Krippendorff 2004a, page 242). Recent content analysis practice seems
to have settled for even more stringent requirements: A recent textbook, Neuendorf
(2002, page 3), analyzing several proposals concerning ?acceptable? reliability, con-
cludes that ?reliability coefficients of .90 or greater would be acceptable to all, .80
or greater would be acceptable in most situations, and below that, there exists great
disagreement.?
This is clearly a fundamental issue. Ideally we would want to establish thresholds
which are appropriate for the field of CL, but as we will see in the rest of this section, a
decade of practical experience hasn?t helped in settling the matter. In fact, weighted
coefficients, while arguably more appropriate for many annotation tasks, make the
issue of deciding when the value of a coefficient indicates sufficient agreement even
K = 0.0 0.2 0.4 0.6 0.8 1.0
Poor Slight Fair Moderate Substantial Perfect
Figure 1
Kappa values and strength of agreement according to Landis and Koch (1977).
576
Artstein and Poesio Inter-Coder Agreement for CL
more complicated because of the problem of determining appropriate weights (see
Section 4.4). We will return to the issue of interpreting the value of the coefficients at
the end of this article.
4.1.4 Agreement and Machine Learning. In a recent article, Reidsma and Carletta (2008)
point out that the goals of annotation in CL differ from those of content analysis, where
agreement coefficients originate. A common use of an annotated corpus in CL is not
to confirm or reject a hypothesis, but to generalize the patterns using machine-learning
algorithms. Through a series of simulations, Reidsma and Carletta demonstrate that
agreement coefficients are poor predictors of machine-learning success: Even highly
reproducible annotations are difficult to generalize when the disagreements contain pat-
terns that can be learned, whereas highly noisy and unreliable data can be generalized
successfully when the disagreements do not contain learnable patterns. These results
show that agreement coefficients should not be used as indicators of the suitability of
annotated data for machine learning.
However, the purpose of reliability studies is not to find out whether annotations
can be generalized, but whether they capture some kind of observable reality. Even if
the pattern of disagreement allows generalization, we need evidence that this general-
ization would be meaningful. The decision whether a set of annotation guidelines are
appropriate or meaningful is ultimately a qualitative one, but a baseline requirement is
an acceptable level of agreement among the annotators, who serve as the instruments
of measurement. Reliability studies test the soundness of an annotation scheme and
guidelines, which is not to be equated with the machine-learnability of data produced
by such guidelines.
4.2 Labeling Units with a Common and Predefined Set of Categories: The Case
of Dialogue Act Tagging
The simplest and most common coding in CL involves labeling segments of text with
a limited number of linguistic categories: Examples include part-of-speech tagging,
dialogue act tagging, and named entity tagging. The practices used to test reliability
for this type of annotation tend to be based on the assumption that the categories used
in the annotation are mutually exclusive and equally distinct from one another; this
assumption seems to have worked out well in practice, but questions about it have been
raised even for the annotation of parts of speech (Babarczy, Carroll, and Sampson 2006),
let alne for discourse coding tasks such as dialogue act coding. We concentrate here on
this latter type of coding, but a discussion of issues raised for POS, named entity, and
prosodic coding can be found in the extended version of the article.
Dialogue act tagging is a type of linguistic annotation with which by now the CL
community has had extensive experience: Several dialogue-act-annotated spoken lan-
guage corpora now exist, such as MapTask (Carletta et al 1997), Switchboard (Stolcke
et al 2000), Verbmobil (Jekat et al 1995), and Communicator (e.g., Doran et al 2001),
among others. Historically, dialogue act annotation was also one of the types of annota-
tion that motivated the introduction in CL of chance-corrected coefficients of agreement
(Carletta et al 1997) and, as we will see, it has been the type of annotation that has
generated the most discussion concerning annotation methodology and measuring
agreement.
A number of coding schemes for dialogue acts have achieved values of K over
0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the
577
Computational Linguistics Volume 34, Number 4
13-tagMapTask coding scheme (Carletta et al 1997), K = 0.8 for the 42-tag Switchboard-
DAMSL scheme (Stolcke et al 2000), K = 0.90 for the smaller 20-tag subset of the CSTAR
scheme used by Doran et al (2001). All of these tests were based on the same two
assumptions: that every unit (utterance) is assigned to exactly one category (dialogue
act), and that these categories are distinct. Therefore, again, unweighted measures, and
in particular K, tend to be used for measuring inter-coder agreement.
However, these assumptions have been challenged based on the observation that
utterances tend to have more than one function at the dialogue act level (Traum and
Hinkelman 1992; Allen and Core 1997; Bunt 2000); for a useful survey, see Popescu-Belis
(2005). An assertion performed in answer to a question, for instance, typically performs
at least two functions at different levels: asserting some information?the dialogue act
that we called Statement in Section 2.3, operating at what Traum and Hinkelman called
the ?core speech act? level?and confirming that the question has been understood, a di-
alogue act operating at the ?grounding? level and usually known as Acknowledgment
(Ack). In older dialogue act tagsets, acknowledgments and statements were treated as
alternative labels at the same ?level?, forcing coders to choose one or the other when an
utterance performed a dual function, according to a well-specified set of instructions. By
contrast, in the annotation schemes inspired from these newer theories such as DAMSL
(Allen and Core 1997), coders are allowed to assign tags along distinct ?dimensions? or
?levels?.
Two annotation experiments testing this solution to the ?multi-tag? problem with
the DAMSL scheme were reported in Core and Allen (1997) and Di Eugenio et al
(1998). In both studies, coders were allowed to mark each communicative function
independently: That is, they were allowed to choose for each utterance one of the
Statement tags (or possibly none), one of the Influencing-Addressee-Future-Action
tags, and so forth?and agreement was evaluated separately for each dimension using
(unweighted) K. Core and Allen found values of K ranging from 0.76 for answer
to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action. Using differ-
ent coding instructions and on a different corpus, Di Eugenio et al observed higher
agreement, ranging from K = 0.93 (for other-forward-function) to 0.54 (for the tag
agreement).
These relatively low levels of agreement led many researchers to return to ?flat?
tagsets for dialogue acts, incorporating however in their schemes some of the in-
sights motivating the work on schemes such as DAMSL. The best known example
of this type of approach is the development of the SWITCHBOARD-DAMSL tagset
by Jurafsky, Shriberg, and Biasca (1997), which incorporates many ideas from the
?multi-dimensional? theories of dialogue acts, but does not allow marking an utterance
as both an acknowledgment and a statement; a choice has to be made. This tagset
results in overall agreement of K = 0.80. Interestingly, subsequent developments of
SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the
ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus
reintroduces some of the DAMSL ideas, in that annotators are allowed to assign multi-
ple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al 2004). Shriberg et al
achieved a comparable reliability to that obtained with SWITCHBOARD-DAMSL, but
only when using a tagset of just five ?class-maps?.
Shriberg et al (2004) also introduced a hierarchical organization of tags to improve
reliability. The dimensions of the DAMSL scheme can be viewed as ?superclasses? of
dialogue acts which share some aspect of their meaning. For instance, the dimension
of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts
Open-option (used to mark suggestions) and Directive, both of which bring into
578
Artstein and Poesio Inter-Coder Agreement for CL
consideration a future action to be performed by the addressee. At least in principle,
an organization of this type opens up the possibility for coders to mark an utterance
with the superclass (IAFA) in case they do not feel confident that the utterance satisfies
the additional requirements for Open-option or Directive. This, in turn, would do
away with the need to make a choice between these two options. This possibility
wasn?t pursued in the studies using the original DAMSL that we are aware of (Core
and Allen 1997; Di Eugenio 2000; Stent 2001), but was tested by Shriberg et al (2004)
and subsequent work, in particular Geertzen and Bunt (2006), who were specifically
interested in the idea of using hierarchical schemes to measure partial agreement, and
in addition experimented with weighted coefficients of agreement for their hierarchical
tagging scheme, specifically ?w.
Geertzen and Bunt tested intercoder agreement with Bunt?s DIT++ (Bunt 2005),
a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic
Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For
example, the class information-seeking is viewed as consisting of two classes, yes-
no question (ynq) and wh-question (whq). The hierarchy is explicitly introduced in order
to allow coders to leave some aspects of the coding undecided. For example, check is
treated as a subclass of ynq in which, in addition, the speaker has a weak belief that the
proposition that forms the belief is true. A coder who is not certain about the dialogue
act performed using an utterance may simply choose to tag it as ynq.
The distance metric d proposed by Geertzen and Bunt is based on the crite-
rion that two communicative functions are related (d(c1, c2) < 1) if they stand in an
ancestor?offspring relation within a hierarchy. Furthermore, they argue, the magnitude
of d(c1, c2) should be proportional to the distance between the functions in the hierar-
chy. A level-dependent correction factor is also proposed so as to leave open the option
tomake disagreements at higher levels of the hierarchymatter more than disagreements
at the deeper level (for example, the distance between information-seeking and ynq
might be considered greater than the distance between check and positive-check).
The results of an agreement test with two annotators run by Geertzen and Bunt
show that taking into account partial agreement leads to values of ?w that are higher
than the values of ? for the same categories, particularly for feedback, a class for which
Core andAllen (1997) got low agreement. Of course, even assuming that the values of ?w
and ? were directly comparable?we remark on the difficulty of interpreting the values
of weighted coefficients of agreement in Section 4.4?it remains to be seenwhether these
higher values are a better indication of the extent of agreement between coders than the
values of unweighted ?.
This discussion of coding schemes for dialogue acts introduced issues to which
we will return for other CL annotation tasks as well. There are a number of well-
established schemes for large-scale dialogue act annotation based on the assumption
of mutual exclusivity between dialogue act tags, whose reliability is also well known; if
one of these schemes is appropriate for modeling the communicative intentions found
in a task, we recommend to our readers to use it. They should also realize, however,
that the mutual exclusivity assumption is somewhat dubious. If a multi-dimensional or
hierarchical tagset is used, readers should also be aware that weighted coefficients do
capture partial agreement, and need not automatically result in lower reliability or in
an explosion in the number of labels. However, a hierarchical scheme may not reflect
genuine annotation difficulties: For example, in the case of DIT++, one might argue that
it is more difficult to confuse yes-no questions with wh-questions than with statements.
We will also see in a moment that interpreting the results with weighted coefficients is
difficult. We will return to both of these problems in what follows.
579
Computational Linguistics Volume 34, Number 4
4.3 Marking Boundaries and Unitizing
Before labeling can take place, the units of annotation, or markables, need to be
identified?a process Krippendorff (1995, 2004a) calls unitizing. The practice in CL for
the forms of annotation discussed in the previous section is to assume that the units are
linguistic constituents which can be easily identified, such as words, utterances, or noun
phrases, and therefore there is no need to check the reliability of this process. We are
aware of few exceptions to this assumption, such as Carletta et al (1997) on unitization
for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases
such as text segmentation, however, the identification of units is as important as their
labeling, if not more important, and therefore checking agreement on unit identification
is essential. In this section we discuss current CL practice with reliability testing of these
types of annotation, before briefly summarizing Krippendorff?s proposals concerning
measuring reliability for unitizing.
4.3.1 Segmentation and Topic Marking. Discourse segments are portions of text that con-
stitute a unit either because they are about the same ?topic? (Hearst 1997; Reynar
1998) or because they have to do with achieving the same intention (Grosz and Sidner
1986) or performing the same ?dialogue game? (Carletta et al 1997).7 The analysis
of discourse structure?and especially the identification of discourse segments?is the
type of annotation that, more than any other, led CL researchers to look for ways of
measuring reliability and agreement, as it made them aware of the extent of disagree-
ment on even quite simple judgments (Kowtko, Isard, and Doherty 1992; Passonneau
and Litman 1993; Carletta et al 1997; Hearst 1997). Subsequent research identified a
number of issues with discourse structure annotation, above all the fact that segmen-
tation, though problematic, is still much easier than marking more complex aspects of
discourse structure, such as identifying the most important segments or the ?rhetorical?
relations between segments of different granularity. As a result, many efforts to annotate
discourse structure concentrate only on segmentation.
The agreement results for segment coding tend to be on the lower end of the
scale proposed by Krippendorff and recommended by Carletta. Hearst (1997), for
instance, found K = 0.647 for the boundary/not boundary distinction; Reynar (1998),
measuring agreement between his own annotation and the TREC segmentation of
broadcast news, reports K = 0.764 for the same task; Ries (2002) reports even lower
agreement of K = 0.36. Teufel, Carletta, and Moens (1999), who studied agreement on
the identification of argumentative zones, found high reliability (K = 0.81) for their
three main zones (own, other, background), although lower for the whole scheme
(K = 0.71). For intention-based segmentation, Passonneau and Litman (1993) in the
pre-K days reported an overall percentage agreement with majority opinion of 89%, but
the agreement on boundaries was only 70%. For conversational games segmentation,
Carletta et al (1997) reported ?promising but not entirely reassuring agreement on
where games began (70%),? whereas the agreement on transaction boundaries was
K = 0.59. Exceptions are two segmentation efforts carried out as part of annotations
of rhetorical structure. Moser, Moore, and Glendening (1996) achieved an agreement
7 The notion of ?topic? is notoriously difficult to define and many competing theoretical proposals exist
(Reinhart 1981; Vallduv?? 1993). As it is often the case with annotation, fairly simple definitions tend to
be used in discourse annotation work: For example, in TDT topic is defined for annotation purposes
as ?an event or activity, along with all directly related events and activities? (TDT-2 Annotation Guide,
http://projects.ldc.upenn.edu/TDT2/Guide/label-instr.html).
580
Artstein and Poesio Inter-Coder Agreement for CL
of K = 0.9 for the highest level of segmentation of their RDA annotation (Poesio,
Patel, and Di Eugenio 2006). Carlson, Marcu, and Okurowski (2003) reported very
high agreement over the identification of the boundaries of discourse units, the build-
ing blocks of their annotation of rhetorical structure. (Agreement was measured sev-
eral times; initially, they obtained K = 0.87, and in the final analysis K = 0.97.) This,
however, was achieved by employing experienced annotators, and with considerable
training.
One important reason why most agreement results on segmentation are on the
lower end of the reliability scale is the fact, known to researchers in discourse analysis
from as early as Levin and Moore (1978), that although analysts generally agree on the
?bulk? of segments, they tend to disagree on their exact boundaries. This phenomenon
was also observed in more recent studies: See for example the discussion in Passonneau
and Litman (1997), the comparison of the annotations produced by seven coders of
the same text in Figure 5 of Hearst (1997, page 55), or the discussion by Carlson,
Marcu, and Okurowski (2003), who point out that the boundaries between elementary
discourse units tend to be ?very blurry.? See also Pevzner and Hearst (2002) for similar
comments made in the context of topic segmentation algorithms, and Klavans, Popper,
and Passonneau (2003) for selecting definition phrases.
This ?blurriness? of boundaries, combined with the prevalence effects discussed
in Section 3.2, also explains the fact that topic annotation efforts which were only
concerned with roughly dividing a text into segments (Passonneau and Litman 1993;
Carletta et al 1997; Hearst 1997; Reynar 1998; Ries 2002) generally report lower agree-
ment than the studies whose goal is to identify smaller discourse units. When disagree-
ment is mostly concentrated in one class (?boundary? in this case), if the total number of
units to annotate remains the same, then expected agreement on this class is lower when
a greater proportion of the units to annotate belongs to this class. When in addition this
class is much less numerous than the other classes, overall agreement tends to depend
mostly on agreement on this class.
For instance, suppose we are testing the reliability of two different segmentation
schemes?into broad ?discourse segments? and into finer ?discourse units??on a text
of 50 utterances, and that we obtain the results in Table 8. Case 1 would be a situation
in which Coder A and Coder B agree that the text consists of two segments, obviously
agree on its initial and final boundaries, but disagree by one position on the intermediate
boundary?say, one of them places it at utterance 25, the other at utterance 26. Never-
theless, because expected agreement is so high?the coders agree on the classification
of 98% of the utterances?the value of K is fairly low. In case 2, the coders disagree on
three times as many utterances, but K is higher than in the first case because expected
agreement is substantially lower (Ae = 0.53).
The fact that coders mostly agree on the ?bulk? of discourse segments, but tend
to disagree on their boundaries, also makes it likely that an all-or-nothing coefficient
like K calculated on individual boundaries would underestimate the degree of agree-
ment, suggesting low agreement even among coders whose segmentations are mostly
similar. A weighted coefficient of agreement like ? might produce values more in
keeping with intuition, but we are not aware of any attempts at measuring agreement
on segmentation using weighted coefficients. We see two main options. We suspect that
the methods proposed by Krippendorff (1995) for measuring agreement on unitizing
(see Section 4.3.2, subsequently) may be appropriate for the purpose of measuring
agreement on discourse segmentation. A second optionwould be tomeasure agreement
not on individual boundaries but on windows spanning several units, as done in the
methods proposed to evaluate the performance of topic detection algorithms such as
581
Computational Linguistics Volume 34, Number 4
Table 8
Fewer boundaries, higher expected agreement.
Case 1: Broad segments
Ao = 0.96,Ae = 0.89, K = 0.65
CODER A
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 2 1 3
CODER B NO BOUNDARY 1 46 47
TOTAL 3 47 50
Case 2: Fine discourse units
Ao = 0.88,Ae = 0.53, K = 0.75
CODER A
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 16 3 19
CODER B NO BOUNDARY 3 28 31
TOTAL 19 31 50
Pk (Beeferman, Berger, and Lafferty 1999) or WINDOWDIFF (Pevzner and Hearst 2002)
(which are, however, raw agreement scores not corrected for chance).
4.3.2 Unitizing (Or, Agreement on Markable Identification). It is often assumed in CL anno-
tation practice that the units of analysis are ?natural? linguistic objects, and therefore
there is no need to check agreement on their identification. As a result, agreement is
usually measured on the labeling of units rather than on the process of identifying them
(unitizing, Krippendorff 1995). We have just seen, however, two coding tasks for which
the reliability of unit identification is a crucial part of the overall reliability, and the
problem of markable identification is more pervasive than is generally acknowledged.
For example, when the units to be labeled are syntactic constituents, it is common
practice to use a parser or chunker to identify themarkables and then to allow the coders
to correct the parser?s output. In such cases one would want to know how reliable the
coders? corrections are. We thus need a general method of testing relibility on markable
identification.
The one proposal for measuring agreement onmarkable identification we are aware
of is the ?U coefficient, a non-trivial variant of ? proposed by Krippendorff (1995). A
full presentation of the proposal would require too much space, so we will just present
the core idea. Unitizing is conceived of as consisting of two separate steps: identifying
boundaries between units, and selecting the units of interest. If a unit identified by one
coder overlaps a unit identified by the other coder, the amount of disagreement is the
square of the lengths of the non-overlapping segments (see Figure 2); if a unit identified
by one coder does not overlap any unit of interest identified by the other coder, the
amount of disagreement is the square of the length of the whole unit. This distance
metric is used in calculating observed and expected disagreement, and ?U itself. We
refer the reader to Krippendorff (1995) for details.
Krippendorff?s ?U is not applicable to all CL tasks. For example, it assumes that
units may not overlap in a single coder?s output, yet in practice there are many
582
Artstein and Poesio Inter-Coder Agreement for CL
Coder A
Coder B
s?? ? s? ? s+? ?
Figure 2
The difference between overlapping units is d(A, B) = s2? + s
2
+ (adapted from Krippendorff
1995, Figure 4, page 61).
annotation schemes which require coders to label nested syntactic constituents. For
continuous segmentation tasks, ?U may be inappropriate because when a segment
identified by one annotator overlaps with two segments identified by another annotator,
the distance is smallest when the one segment is centered over the two rather than
aligned with one of them. Nevertheless, we feel that when the non-overlap assumption
holds, and the units do not cover the text exhaustively, testing the reliabilty of unit
identification may prove beneficial. To our knowledge, this has never been tested in CL.
4.4 Anaphora
The annotation tasks discussed so far involve assigning a specific label to each category,
which allows the various agreement measures to be applied in a straightforward way.
Anaphoric annotation differs from the previous tasks because annotators do not assign
labels, but rather create links between anaphors and their antecedents. It is therefore
not clear what the ?labels? should be for the purpose of calculating agreement. One
possibility would be to consider the intended referent (real-world object) as the label,
as in named entity tagging, but it wouldn?t make sense to predefine a set of ?labels?
applicable to all texts, because different objects are mentioned in different texts. An
alternative is to use the marked antecedents as ?labels?. However, we do not want to
count as a disagreement every time two coders agree on the discourse entity realized
by a particular noun phrase but just happen to mark different words as antecedents.
Consider the reference of the underlined pronoun it in the following dialogue excerpt
(TRAINS 1991 [Gross, Allen, and Traum 1993], dialogue d91-3.2).8
1.1 M: ....
1.4 first thing I?d like you to do
1.5 is send engine E2 off with a boxcar to Corning to
pick up oranges
1.6 as soon as possible
2.1 S: okay
3.1 M: and while it?s there it should pick up the tanker
Some of the coders in a study we carried out (Poesio and Artstein 2005) indicated the
noun phrase engine E2 as antecedent for the second it in utterance 3.1, whereas others
indicated the immediately preceding pronoun, which they had previously marked as
having engine E2 as antecedent. Clearly, we do not want to consider these coders to be in
disagreement. A solution to this dilemma has been proposed by Passonneau (2004): Use
the emerging coreference sets as the ?labels? for the purpose of calculating agreement.
This requires using weighted measures for calculating agreement on such sets, and
8 ftp://ftp.cs.rochester.edu/pub/papers/ai/92.tn1.trains 91 dialogues.txt.
583
Computational Linguistics Volume 34, Number 4
consequently it raises serious questions about weighted measures?in particular, about
the interpretability of the results, as we will see shortly.
4.4.1 Passonneau?s Proposal. Passonneau (2004) recommends measuring agreement on
anaphoric annotation by using sets of mentions of discourse entities as labels, that is,
the emerging anaphoric/coreference chains. This proposal is in line with the meth-
ods developed to evaluate anaphora resolution systems (Vilain et al 1995). But using
anaphoric chains as labels would not make unweighted measures such as K a good
measure for agreement. Practical experience suggests that, except when a text is very
short, few annotators will catch all mentions of a discourse entity: Most will forget to
mark a few, with the result that the chains (that is, category labels) differ from coder
to coder and agreement as measured with K is always very low. What is needed is
a coefficient that also allows for partial disagreement between judgments, when two
annotators agree on part of the coreference chain but not on all of it.
Passonneau (2004) suggests solving the problem by using ? with a distance metric
that allows for partial agreement among anaphoric chains. Passonneau proposes a dis-
tance metric based on the following rationale: Two sets are minimally distant when they
are identical andmaximally distant when they are disjoint; between these extremes, sets
that stand in a subset relation are closer (less distant) than ones that merely intersect.
This leads to the following distance metric between two sets A and B.
dP =
?
?
?
?
?
?
?
0 if A = B
1/3 if A ? B or B ? A
2/3 if A ? B = ?, but A ? B and B ? A
1 if A ? B = ?
Alternative distance metrics take the size of the anaphoric chain into account, based
on measures used to compare sets in Information Retrieval, such as the coefficient of
community of Jaccard (1912) and the coincidence index of Dice (1945) (Manning and
Schu?tze 1999).
Jaccard: dJ = 1?
|A ? B|
|A ? B|
Dice: dD = 1?
2 |A ? B|
|A|+ |B|
In later work, Passonneau (2006) offers a refined distance metric which she called MASI
(Measuring Agreement on Set-valued Items), obtained by multiplying Passonneau?s
original metric dP by the metric derived from Jaccard dJ .
dM = dP ? dJ
4.4.2 Experience with ? for Anaphoric Annotation. In the experiment mentioned previously
(Poesio and Artstein 2005) we used 18 coders to test ? and K under a variety of condi-
tions.We found that even though our coders by and large agreed on the interpretation of
anaphoric expressions, virtually no coder ever identified all the mentions of a discourse
entity. As a result, even though the values of ? and K obtained by using the ID of
the antecedent as label were pretty similar, the values obtained when using anaphoric
chains as labels were drastically different. The value of ? increased, because examples
where coders linked a markable to different antecedents in the same chain were no
584
Artstein and Poesio Inter-Coder Agreement for CL
longer considered as disagreements. However, the value of K was drastically reduced,
because hardly any coder identified all the mentions of discourse entities (Figure 3).
The study also looked at the matter of individual annotator bias, and as mentioned
in Section 3.1, we did not find differences between ? and a ?-style version of ? beyond
the third decimal point. This similarity is what one would expect, given the result about
annotator bias from Section 3.1 and given that in this experiment we used 18 annotators.
These very small differences should be contrasted with the differences resulting from
the choice of distance metrics, where values for the full-chain condition ranged from
? = 0.642 using Jaccard as distance metric, to ? = 0.654 using Passonneau?s metric, to
the value for Dice reported in Figure 3, ? = 0.691. These differences raise an important
issue concerning the application of ?-like measures for CL tasks: Using ? makes it diffi-
cult to compare the results of different annotation experiments, in that a ?poor? value or
a ?high? valuemight result from ?too strict? or ?too generous? distancemetrics, making
it even more important to develop a methodology to identify appropriate values for
these coefficients. This issue is further emphasized by the study reported next.
4.4.3 Discourse Deixis. A second annotation study we carried out (Artstein and Poesio
2006) shows even more clearly the possible side effects of using weighted coefficients.
This study was concerned with the annotation of the antecedents of references to
abstract objects, such as the example of the pronoun that in utterance 7.6 (TRAINS 1991,
dialogue d91-2.2).
7.3 : so we ship one
7.4 : boxcar
7.5 : of oranges to Elmira
7.6 : and that takes another 2 hours
Previous studies of discourse deixis annotation showed that these are extremely diffi-
cult judgments to make (Eckert and Strube 2000; Navarretta 2000; Byron 2002), except
perhaps for identifying the type of object (Poesio and Modjeska 2005), so we simplified
the task by only requiring our participants to identify the boundaries of the area of
text in which the antecedent was introduced. Even so, we found a great variety in
how these boundaries were marked: Exactly as in the case of discourse segmentation
discussed earlier, our participants broadly agreed on the area of text, but disagreed on
Chain K ?
None 0.628 0.656
Partial 0.563 0.677
Full 0.480 0.691
0.4
0.5
0.6
0.7
?
?
?
K
K
K
no partial full
chain chain chain
Figure 3
A comparison of the values of ? and K for anaphoric annotation (Poesio and Artstein 2005).
585
Computational Linguistics Volume 34, Number 4
its exact boundary. For instance, in this example, nine out of ten annotators marked the
antecedent of that as a text segment ending with the word Elmira, but some started with
the word so, some started with we, some with ship, and some with one.
We tested a number of ways tomeasure partial agreement on this task, and obtained
widely different results. First of all, we tested three set-based distance metrics inspired
by the Passonneau proposals that we just discussed: We considered discourse segments
to be sets of words, and computed the distance between them using Passonneau?s
metric, Jaccard, and Dice. Using these three metrics, we obtained ? values of 0.55 (with
Passonneau?s metric), 0.45 (with Jaccard), and 0.55 (with Dice). We should note that
because antecedents of different expressions rarely overlapped, the expected disagree-
ment was close to 1 (maximal), so the value of ? turned out to be very close to the com-
plement of the observed disagreement as calculated by the different distance metrics.
Next, we considered methods based on the position of words in the text. The first
method computed differences between absolute boundary positions: Each antecedent
was associated with the position of its first or last word in the dialogue, and agreement
was calculated using ? with the interval distance metric. This gave us ? values of
0.998 for the beginnings of the antecedent-evoking area and 0.999 for the ends. This is
because expected disagreement is exceptionally low: Coders tend to mark discourse an-
tecedents close to the referring expression, so the average distance between antecedents
of the same expression is smaller than the size of the dialogue by a few orders of
magnitude. The second method associated each antecedent with the position of its
first or last word relative to the beginning of the anaphoric expression. This time we found
extremely low values of ? = 0.167 for beginnings of antecedents and 0.122 for ends?
barely in the positive side. This shows that agreement among coders is not dramatically
better than what would be expected if they just marked discourse antecedents at a fixed
distance from the referring expression.
The three ranges of ? that we observed (middle, high, and low) show agreement on
the identity of discourse antecedents, their position in the dialogue, and their position
relative to referring expressions, respectively. The middle range shows variability of up
to 10 percentage points, depending on the distance metric chosen. The lesson is that
once we start using weighted measures we cannot anymore interpret the value of ?
using traditional rules of thumb such as those proposed by Krippendorff or by Landis
and Koch. This is because depending on the way we measure agreement, we can report
? values ranging from 0.122 to 0.998 for the very same experiment! New interpretation
methods have to be developed, which will be task- and distance-metric specific. We?ll
return to this issue in the conclusions.
4.5 Word Senses
Word sense tagging is one of the hardest annotation tasks. Whereas in the case of part-
of-speech and dialogue act tagging the same categories are used to classify all units, in
the case of word sense tagging different categories must be used for each word, which
makes writing a single codingmanual specifying examples for all categories impossible:
The only option is to rely on a dictionary. Unfortunately, different dictionaries make
different distinctions, and often coders can?t make the fine-grained distinctions that
trained lexicographers can make. The problem is particularly serious for verbs, which
tend to be polysemous rather than homonymous (Palmer, Dang, and Fellbaum 2007).
These difficulties, and in particular the difficulty of tagging senses with a fine-
grained repertoire of senses such as that provided by dictionaries or by WordNet
(Fellbaum 1998), have been highlighted by the three SENSEVAL initiatives. Already
586
Artstein and Poesio Inter-Coder Agreement for CL
during the first SENSEVAL, Ve?ronis (1998) carried out two studies of intercoder
agreement on word sense tagging in the so-called ROMANSEVAL task. One study was
concerned with agreement on polysemy?that is, the extent to which coders agreed
that a word was polysemous in a given context. Six naive coders were asked to make
this judgment about 600 French words (200 nouns, 200 verbs, 200 adjectives) using the
repertoire of senses in the Petit Larousse. On this task, a (pairwise) percentage agreement
of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding
to K values of 0.36, 0.37, and 0.67, respectively. The 20 words from each category
perceived by the coders in this first experiment to be most polysemous were then used
in a second study, of intercoder agreement on the sense tagging task, which involved
six different naive coders. Interestingly, the coders in this second experiment were
allowed to assign multiple tags to words, although they did not make much use of this
possibility; so ?w was used to measure agreement. In this experiment, Ve?ronis observed
(weighted) pairwise agreement of 0.63 for verbs, 0.71 for adjectives, and 0.73 for nouns,
corresponding to ?w values of 0.41, 0.41, and 0.46, but with a wide variety of values
when measured per word?ranging from 0.007 for the adjective correct to 0.92 for the
noun de?tention. Similarly mediocre results for intercoder agreement between naive
coders were reported in the subsequent editions of SENSEVAL. Agreement studies
for SENSEVAL-2, where WordNet senses were used as tags, reported a percentage
agreement for verb senses of around 70%, whereas for SENSEVAL-3 (English Lexical
Sample Task), Mihalcea, Chklovski, and Kilgarriff (2004) report a percentage agreement
of 67.3% and average K of 0.58.
Two types of solutions have been proposed for the problem of low agreement on
sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicog-
raphers and arbitration. The study carried out by Kilgarriff does not therefore qualify
as a true study of replicability in the sense of the terms used by Krippendorff, but it did
show that this approach makes it possible to achieve percentage agreement of around
95.5%. An alternative approach has been to address the problem of the inability of naive
coders to make fine-grained distinctions by introducing coarser-grained classification
schemes which group together dictionary senses (Bruce and Wiebe, 1998; Buitelaar
1998; Ve?ronis 1998; Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also
developed, such as HECTOR (Atkins 1992) or, indeed, WordNet itself (where senses are
related by hyponymy links). In the case of Buitelaar and Palmer, Dang, and Fellbaum,
the ?supersenses? were identified by hand, whereas Bruce andWiebe and Ve?ronis used
clustering methods such as those from Bruce and Wiebe (1999) to collapse some of the
initial sense distinctions.9 Palmer, Dang, and Fellbaum (2007) illustrate this practice
with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7:
They conflate these senses into a small number of groups using various criteria?for
example, four senses can be grouped in a group they call Group 1 on the basis of
subcategorization frame similarities (Table 9).
Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task
of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as
opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that
collapsing the senses of their test word (interest) on the basis of their use by coders and
merging the two classes found to be harder to distinguish resulted in an increase of
9 The methodology proposed in Bruce and Wiebe (1999) is in our view the most advanced technique to
?make sense? of the results of agreement studies available in the literature. The extended version of this
article contains a fuller introduction to these methods.
587
Computational Linguistics Volume 34, Number 4
Table 9
Group 1 of senses of call in Palmer, Dang, and Fellbaum (2007, page 149).
SENSE DESCRIPTION EXAMPLE HYPERNYM
WN1 name, call ?They nameda their son David? LABEL
WN3 call, give a quality ?She called her children lazy LABEL
and ungrateful?
WN19 call, consider ?I would not call her beautiful? SEE
WN22 address, call ?Call me mister? ADDRESS
aThe verb named appears in the original WordNet example for the verb call.
the value of K from 0.874 to 0.898. Using a related technique, Ve?ronis (1998) found that
agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86.
We should note, however, that the post hoc merging of categories is not equivalent to
running a study with fewer categories to begin with.
Attempts were also made to develop techniques to measure partial agreement with
hierarchical tagsets. A first proposal in this direction was advanced by Melamed and
Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used
in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and
Resnik proposed to ?normalize? the computation of observed and expected agreement
by taking each label which is not a leaf in the tag hierarchy and distributing it down
to the leaves in a uniform way, and then only computing agreement on the leaves. For
example, with a tagset like the one in Table 9, the cases in which the coders used the
label ?Group 1? would be uniformly ?distributed down? and added in equal measure
to the number of cases in which the coders assigned each of the four WordNet labels.
The method proposed in the paper has, however, problematic properties when used
to measure intercoder agreement. For example, suppose tag A dominates two sub-tags
A1 and A2, and that two coders mark a particular item as A. Intuitively, we would want
to consider this a case of perfect agreement, but this is not what the method proposed
by Melamed and Resnik yields. The annotators? marks are distributed over the two
sub-tags, each with probability 0.5, and then the agreement is computed by summing
the joint probabilities over the two subtags (Equation (4) of Melamed and Resnik 2000),
with the result that the agreement over the item turns out to be 0.52 + 0.52 = 0.5 instead
of 1. To correct this, Dan Melamed (personal communication) suggested replacing the
product in Equation (4) with a minimum operator. However, the calculation of expected
agreement (Equation (5) of Melamed and Resnik 2000) still gives the amount of agree-
ment which is expected if coders are forced to choose among leaf nodes, which makes
this method inappropriate for coding schemes that do not force coders to do this.
One way to use Melamed and Resnik?s proposal while avoiding the discrepancy
between observed and expected agreement is to treat the proposal not as a new co-
efficient, but rather as a distance metric to be plugged into a weighted coefficient
like ?. Let A and B be two nodes in a hierarchical tagset, let L be the set of all leaf
nodes in the tagset, and let P(l|T) be the probability of selecting a leaf node l given
an arbitrary node T when the probability mass of T is distributed uniformly to all the
nodes dominated by T. We can reinterpret Melamed?s modification of Equation (4) in
Melamed and Resnik (2000) as a metric measuring the distance between nodes A and B.
dM+R = 1?
?
l?L
min(P(l|A), P(l|B))
588
Artstein and Poesio Inter-Coder Agreement for CL
This metric has the desirable properties?it is 0 when tags A and B are identical,
1 when the tags do not overlap, and somewhere in between in all other cases. If we
use this metric for Krippendorff?s ? we find that observed agreement is exactly the
same as inMelamed and Resnik (2000) with the product operator replaced byminimum
(Melamed?s modification).
We can also use other distance metrics with ?. For example, we could associate
with each sense an extended sense?a set es(s) including the sense itself and its
grouped sense?and then use set-based distance metrics from Section 4.4, for ex-
ample Passonneau?s dP. To illustrate how this approach could be used to measure
(dis)agreement on word sense annotation, suppose that two coders have to annotate the
use of call in the following sentence (from theWSJ part of the Penn Treebank, section 02,
text w0209):
This gene, called ?gametocide,? is carried into the plant by a virus that
remains active for a few days.
The standard guidelines (in SENSEVAL, say) require coders to assign a WN sense to
words. Under such guidelines, if coder A classifies the use of called in the above example
as an instance of WN1, whereas coder B annotates it as an instance of WN3, we would
find total disagreement (dkakb = 1) which seems excessively harsh as the two senses are
clearly related. However, by using the broader senses proposed by Palmer, Dang, and
Fellbaum (2007) in combination with a distance metric such as the one just proposed,
it is possible to get more flexible and, we believe, more realistic assessments of the
degree of agreement in situations such as this. For instance, in case the reliability study
had already been carried out under the standard SENSEVAL guidelines, the distance
metric proposed above could be used to identify post hoc cases of partial agreement
by adding to each WN sense its hypernyms according to the groupings proposed by
Palmer, Dang, and Fellbaum. For example, A?s annotation could be turned into a new
set label {WN1,LABEL} and B?s mark into the set table {WN3,LABEL}, which would
give a distance d = 2/3, indicating a degree of overlap. The method for computing
agreement proposed here could could also be used to allow coders to choose either a
more specific label or one of Palmer, Dang, and Fellbaum?s superlabels. For example,
suppose A sticks to WN1, but B decides to mark the use above using Palmer, Dang, and
Fellbaum?s LABEL category, then we would still find a distance d = 1/3.
An alternative way of using ? for word sense annotation was developed and tested
by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to
assign multiple labels (WordNet synsets) for wordsenses, as done by Ve?ronis (1998) and
more recently by Rosenberg and Binkowski (2004) for text classification labels and by
Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared
using the MASI distance metric for ? (Passonneau 2006).
5. Conclusions
The purpose of this article has been to expose the reader to the mathematics of chance-
corrected coefficients of agreement as well as the current state of the art of using these
coefficients in CL. Our hope is that readers come to view agreement studies not as an
additional chore or hurdle for publication, but as a tool for analysis which offers new
insights into the annotation process. We conclude by summarizing what in our view are
the main recommendations emerging from ten years of experience with coefficients of
agreement. These can be grouped under three main headings: methodology, choice of
coefficients, and interpretation of coefficients.
589
Computational Linguistics Volume 34, Number 4
5.1 Methodology
Our first recommendation is that annotation efforts should perform and report rigorous
reliability testing. The last decade has already seen considerable improvement, from
the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini
1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central
role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al 2004)
and OntoNotes (Hovy et al 2006). But even the latter efforts only measure and report
percent agreement. We believe that part of the reluctance to report chance-corrected
measures is the difficulty in interpreting them. However, our experience is that chance-
corrected coefficients of agreement do provide a better indication of the quality of the
resulting annotation than simple percent agreement, and moreover, the detailed calcu-
lations leading to the coefficients can be very revealing as to where the disagreements
are located and what their sources may be.
A rigorous methodology for reliability testing does not, in our opinion, exclude the
use of expert coders, and here we feel there may be a motivated difference between the
fields of content analysis and CL. There is a clear tradeoff between the complexity of
the judgments that coders are required to make and the reliability of such judgments,
and we should strive to devise annotation schemes that are not only reliable enough
to be replicated, but also sophisticated enough to be useful (cf. Krippendorff 2004a,
pages 213?214). In content analysis, conclusions are drawn directly from annotated
corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a
resource which is used by other processes, so the emphasis is more towards usefulness.
There is also a tradeoff between the sophistication of judgments and the availability of
coders who can make such judgments. Consequently, annotation by experts is often
the only practical way to get useful corpora for CL. Current practice achieves high
reliability either by using professionals (Kilgarriff 1999) or through intensive training
(Hovy et al 2006; Carlson, Marcu, and Okurowski 2003); this means that results are not
replicable across sites, and are therefore less reliable than annotation by naive coders
adhering to written instructions. We feel that inter-annotator agreement studies should
still be carried out, as they serve as an assurance that the results are replicable when
the annotators are chosen from the same population as the original annotators. An
important additional assurance should be provided in the form of an independent
evaluation of the task for which the corpus is used (cf. Passonneau 2006).
5.2 Choosing a Coefficient
One of the goals of this article is to help authors make an informed choice regarding
the coefficients they use for measuring agreement. While coefficients other than K,
specifically Cohen?s ? and Krippendorff?s ?, have appeared in the CL literature as early
as Carletta (1996) and Passonneau and Litman (1996), they hadn?t sprung into general
awareness until the publication of Di Eugenio and Glass (2004) and Passonneau (2004).
Regarding the question of annotator bias, there is an overwhelming consensus in CL
practice: K and ? are used in the vast majority of the studies we reported. We agree with
the view that K and ? are more appropriate, as they abstract away from the bias of spe-
cific coders. But we also believe that ultimately this issue of annotator bias is of little con-
sequence because the differences get smaller and smaller as the number of annotators
grows (Artstein and Poesio 2005). We believe that increasing the number of annotators
is the best strategy, because it reduces the chances of accidental personal biases.
590
Artstein and Poesio Inter-Coder Agreement for CL
However, Krippendorff?s ? is indispensable when the category labels are not
equally distinct from one another. We think there are at least two types of coding
schemes in which this is the case: (i) hierarchical tagsets and (ii) set-valued interpre-
tations such as those proposed for anaphora. At least in the second case, weighted
coefficients are almost unavoidable. We therefore recommend using ?, noting however
that the specific choice of weights will affect the overall numerical result.
5.3 Interpreting the Values
We view the lack of consensus on how to interpret the values of agreement coefficients
as a serious problem with current practice in reliability testing, and as one of the
main reasons for the reluctance of many in CL to embark on reliability studies. Unlike
significance values which report a probability (that an observed effect is due to chance),
agreement coefficients report a magnitude, and it is less clear how to interpret such
magnitudes. Our own experience is consistent with that of Krippendorff: Both in our
earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts
(Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation
of reasonable quality (Poesio 2004a). We therefore feel that if a threshold needs to be set,
0.8 is a good value.
That said, we doubt that a single cutoff point is appropriate for all purposes.
For some CL studies, particularly on discourse, useful corpora have been obtained
while attaining reliability only at the 0.7 level. We agree therefore with Craggs and
McGee Wood (2005) that setting a specific agreement threshold should not be a pre-
requisite for publication. Instead, as recommended by Di Eugenio and Glass (2004) and
others, researchers should report in detail on the methodology that was followed in
collecting the reliability data (number of coders, whether they coded independently,
whether they relied exclusively on an annotation manual), whether agreement was sta-
tistically significant, and provide a confusion matrix or agreement table so that readers
can find out whether overall figures of agreement hide disagreements on less common
categories. For an example of good practice in this respect, see Teufel and Moens (2002).
The decision whether a corpus is good enough for publication should be based on more
than the agreement score?specifically, an important consideration is an independent
evaluation of the results that are based on the corpus.
Acknowledgments
This work was supported in part by EPSRC
grant GR/S76434/01, ARRAU. We wish to
thank four anonymous reviewers and Jean
Carletta, Mark Core, Barbara Di Eugenio,
Ruth Filik, Michael Glass, George Hripcsak,
Adam Kilgarriff, Dan Melamed, Becky
Passonneau, Phil Resnik, Tony Sanford,
Patrick Sturt, and David Traum for helpful
comments and discussion. Special thanks to
Klaus Krippendorff for an extremely detailed
review of an earlier version of this article. We
are also extremely grateful to the British
Library in London, which made accessible to
us virtually every paper we needed for this
research.
References
Allen, James and Mark Core. 1997. DAMSL:
Dialogue act markup in several layers.
Draft contribution for the Discourse
Resource Initiative, University of
Rochester. Available at
http://www.cs.rochester.edu/
research/cisd/resources/damsl/.
Artstein, Ron and Massimo Poesio. 2005.
Bias decreases in proportion to the number
of annotators. In Proceedings of FG-MoL
2005, pages 141?150, Edinburgh.
Artstein, Ron and Massimo Poesio. 2006.
Identifying reference to abstract objects
in dialogue. In brandial 2006: Proceedings
of the 10th Workshop on the Semantics and
591
Computational Linguistics Volume 34, Number 4
Pragmatics of Dialogue, pages 56?63,
Potsdam.
Atkins, Sue. 1992. Tools for computer-aided
corpus lexicography: The Hector project.
Acta Linguistica Hungarica, 41:5?71.
Babarczy, Anna, John Carroll, and Geoffrey
Sampson. 2006. Definitional, personal,
and mechanical constraints on part
of speech annotation performance.
Natural Language Engineering, 12(1):77?90.
Bartko, John J. and William T. Carpenter, Jr.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Beeferman, Doug, Adam Berger, and
John Lafferty. 1999. Statistical models
for text segmentation. Machine Learning,
34(1?3):177?210.
Bennett, E. M., R. Alpert, and A. C.
Goldstein. 1954. Communications
through limited questioning. Public
Opinion Quarterly, 18(3):303?308.
Bloch, Daniel A. and Helena Chmura
Kraemer. 1989. 2 ? 2 kappa coefficients:
Measures of agreement or association.
Biometrics, 45(1):269?287.
Brennan, Robert L. and Dale J. Prediger.
1981. Coefficient kappa: Some uses,
misuses, and alternatives. Educational
and Psychological Measurement,
41(3):687?699.
Bruce, Rebecca and Janyce Wiebe. 1998.
Word-sense distinguishability and
inter-coder agreement. In Proceedings
of EMNLP, pages 53?60, Granada.
Bruce, Rebecca F. and Janyce M. Wiebe. 1999.
Recognizing subjectivity: A case study in
manual tagging. Natural Language
Engineering, 5(2):187?205.
Buitelaar, Paul. 1998. CoreLex : Systematic
Polysemy and Underspecification. Ph.D.
thesis, Brandeis University, Waltham, MA.
Bunt, Harry C. 2000. Dynamic interpretation
and dialogue theory. In Martin M. Taylor,
Franc?oise Ne?el, and Don G. Bouwhuis,
editors, The Structure of Multimodal
Dialogue II. John Benjamins, Amsterdam,
pages 139?166.
Bunt, Harry C. 2005. A framework for
dialogue act specification. In Proceedings
of the Joint ISO-ACL Workshop on the
Representation and Annotation of Semantic
Information, Tilburg. Available at:
http://let.uvt.nl/research/ti/
sigsem/wg/discussionnotes4.htm.
Byron, Donna K. 2002. Resolving
pronominal reference to abstract entities.
In Proceedings of the 40th Annual Meeting of
the ACL, pages 80?87, Philadelphia, PA.
Byrt, Ted, Janet Bishop, and John B. Carlin.
1993. Bias, prevalence and kappa. Journal
of Clinical Epidemiology, 46(5):423?429.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Linguistics, 23(1):13?32.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2003. Building a
discourse-tagged corpus in the framework
of rhetorical structure theory. In Jan C. J.
van Kuppevelt and Ronnie W. Smith,
editors, Current and New Directions in
Discourse and Dialogue. Kluwer, Dordrecht,
pages 85?112.
Cicchetti, Domenic V. and Alvan R.
Feinstein. 1990. High agreement but low
kappa: II. Resolving the paradoxes. Journal
of Clinical Epidemiology, 43(6):551?558.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Cohen, Jacob. 1968. Weighted kappa:
Nominal scale agreement with provision
for scaled disagreement or partial credit.
Psychological Bulletin, 70(4):213?220.
Core, Mark G. and James F. Allen. 1997.
Coding dialogs with the DAMSL
annotation scheme. In Working Notes of the
AAAI Fall Symposium on Communicative
Action in Humans and Machines, AAAI,
Cambridge, MA. Available at: http://www.
cs.umd.edu/?traum/CA/fpapers.html.
Craggs, Richard and Mary McGee Wood.
2004. A two-dimensional annotation
scheme for emotion in dialogue. In Papers
from the 2004 AAAI Spring Symposium on
Exploring Attitude and Affect in Text:
Theories and Applications, Stanford,
pages 44?49.
Craggs, Richard and Mary McGee Wood.
2005. Evaluating discourse and dialogue
coding schemes. Computational Linguistics,
31(3):289?295.
Davies, Mark and Joseph L. Fleiss. 1982.
Measuring agreement for multinomial
data. Biometrics, 38(4):1047?1051.
Di Eugenio, Barbara. 2000. On the usage of
Kappa to evaluate agreement on coding
tasks. In Proceedings of LREC, volume 1,
pages 441?444, Athens.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
592
Artstein and Poesio Inter-Coder Agreement for CL
Di Eugenio, Barbara, Pamela W. Jordan,
Johanna D. Moore, and Richmond H.
Thomason. 1998. An empirical
investigation of proposals in collaborative
dialogues. In Proceedings of 36th Annual
Meeting of the ACL, pages 325?329,
Montreal.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Ecology, 26(3):297?302.
Donner, Allan and Michael Eliasziw.
1987. Sample size requirements for
reliability studies. Statistics in Medicine,
6:441?448.
Doran, Christine, John Aberdeen, Laurie
Damianos, and Lynette Hirschman.
2001. Comparing several aspects of
human-computer and human-human
dialogues. In Proceedings of the 2nd
SIGdial Workshop on Discourse and
Dialogue, Aalborg, Denmark. Available at:
http://www.sigdial.org/workshops/
workshop2/proceedings.
Eckert, Miriam and Michael Strube. 2000.
Dialogue acts, synchronizing units,
and anaphora resolution. Journal of
Semantics, 17(1):51?89.
Feinstein, Alvan R. and Domenic V. Cicchetti.
1990. High agreement but low kappa:
I. The problems of two paradoxes. Journal
of Clinical Epidemiology, 43(6):543?549.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Fleiss, Joseph L. 1975. Measuring agreement
between two judges on the presence or
absence of a trait. Biometrics, 31(3):651?659.
Francis, W. Nelson and Henry Kucera.
1982. Frequency Analysis of English Usage:
lexicon and grammar. Houghton Mifflin,
Boston, MA.
Geertzen, Jeroen and Harry Bunt. 2006.
Measuring annotator agreement in a
complex hierarchical dialogue act
annotation scheme. In Proceedings of the
7th SIGdial Workshop on Discourse and
Dialogue, pages 126?133, Sydney.
Gross, Derek, James F. Allen, and David R.
Traum. 1993. The Trains 91 dialogues.
TRAINS Technical Note 92-1, University
of Rochester Computer Science
Department, Rochester, NY.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Hayes, Andrew F. and Klaus Krippendorff.
2007. Answering the call for a standard
reliability measure for coding data.
Communication Methods and Measures,
1(1):77?89.
Hearst, Marti A. 1997. TextTiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Hovy, Eduard, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. OntoNotes: The 90%
solution. In Proceedings of HLT?NAACL,
Companion Volume: Short Papers,
pages 57?60, New York.
Hsu, Louis M. and Ronald Field. 2003.
Interrater agreement measures: Comments
on kappan, Cohen?s kappa, Scott?s ?,
and Aickin?s ?. Understanding Statistics,
2(3):205?219.
Jaccard, Paul. 1912. The distribution of the
flora in the Alpine zone. New Phytologist,
11(2):37?50.
Jekat, Susanne, Alexandra Klein, Elisabeth
Maier, Ilona Maleck, Marion Mast, and
J. Joachim Quantz. 1995. Dialogue acts in
VERBMOBIL. VM-Report 65, Universita?t
Hamburg, DFKI GmbH, Universita?t
Erlangen, and TU Berlin.
Jurafsky, Daniel, Elizabeth Shriberg, and
Debra Biasca. 1997. Switchboard
SWBD-DAMSL shallow-discourse-
function annotation coders manual,
draft 13. Technical Report 97-02,
University of Colorado at Boulder,
Institute for Cognitive Science.
Kilgarriff, Adam. 1999. 95% replicability
for manual word sense tagging. In
Proceedings of the Ninth Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 277?278, Bergen, Norway.
Klavans, Judith L., Samuel Popper, and
Rebecca Passonneau. 2003. Tackling
the internet glossary glut: Automatic
extraction and evaluation of genus
phrases. In Proceedings of the
SIGIR-2003 Workshop on the Semantic Web,
Toronto.
Kowtko, Jacqueline C., Stephen D. Isard,
and Gwyneth M. Doherty. 1992.
Conversational games within dialogue.
Research Paper HCRC/RP-31, Human
Communication Research Centre,
University of Edinburgh.
Krippendorff, Klaus. 1970. Estimating the
reliability, systematic error and random
error of interval data. Educational and
Psychological Measurement, 30(1):61?70.
593
Computational Linguistics Volume 34, Number 4
Krippendorff, Klaus. 1978. Reliability of
binary attribute data. Biometrics,
34(1):142?144. Letter to the editor,
with a reply by Joseph L. Fleiss.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology,
chapter 12. Sage, Beverly Hills, CA.
Krippendorff, Klaus. 1995. On the reliability
of unitizing contiguous data. Sociological
Methodology, 25:47?76.
Krippendorff, Klaus. 2004a. Content Analysis:
An Introduction to Its Methodology,
second edition, chapter 11. Sage,
Thousand Oaks, CA.
Krippendorff, Klaus. 2004b. Reliability
in content analysis: Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?433.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics,
33(1):159?174.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. CLAWS4: The tagging of the
British National Corpus. In Proceedings of
COLING 1994: The 15th International
Conference on Computational Linguistics,
Volume 1, pages 622?628, Kyoto.
Levin, James A. and James A. Moore. 1978.
Dialogue-games: Metacommunication
structures for natural language
interaction. Cognitive Science, 1(4):395?420.
Manning, Christopher D. and Hinrich
Schuetze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcu, Daniel, Magdalena Romera, and
Estibaliz Amorrortu. 1999. Experiments in
constructing a corpus of discourse trees:
Problems, annotation choices, issues. In
Workshop on Levels of Representation in
Discourse, pages 71?78, University of
Edinburgh.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Marion, Rodger. 2004. The whole art of
deduction. Unpublished manuscript.
Melamed, I. Dan and Philip Resnik. 2000.
Tagger evaluation given hierarchical
tagsets. Computers and the Humanities,
34(1?2):79?84. Available at:
http://www.sahs/utmb.edu/PELLINORE/
Intro to research/wad/wad/ home.htm.
Mieskes, Margot and Michael Strube. 2006.
Part-of-speech tagging of transcribed
speech. In Proceedings of LREC,
pages 935?938, Genoa.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The SENSEVAL-3
English lexical sample task. In Proceedings
of SENSEVAL-3, pages 25?28, Barcelona.
Miltsakaki, Eleni, Rashmi Prasad, Aravind
Joshi, and Bonnie Webber. 2004.
Annotating discourse connectives
and their arguments. In Proceedings
of the HLT-NAACL Workshop on
Frontiers in Corpus Annotation, pages 9?16,
Boston, MA.
Moser, Megan G., Johanna D. Moore, and
Erin Glendening. 1996. Instructions
for Coding Explanations: Identifying
Segments, Relations and Minimal Units.
Technical Report 96-17, University of
Pittsburgh, Department of Computer
Science.
Navarretta, Costanza. 2000. Abstract
anaphora resolution in Danish. In
Proceedings of the 1st SIGdial Workshop on
Discourse and Dialogue, Hong Kong,
pages 56?65.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of HLT-NAACL 2004,
pages 145?152, Boston, MA.
Neuendorf, Kimberly A. 2002. The
Content Analysis Guidebook. Sage,
Thousand Oaks, CA.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(2):137?163.
Passonneau, Rebecca J. 2004. Computing
reliability for coreference annotation.
In Proceedings of LREC, volume 4,
pages 1503?1506, Lisbon.
Passonneau, Rebecca J. 2006. Measuring
agreement on set-valued items (MASI)
for semantic and pragmatic annotation.
In Proceedings of LREC, Genoa,
pages 831?836.
Passonneau, Rebecca J., Nizar Habash, and
Owen Rambow. 2006. Inter-annotator
agreement on a multilingual semantic
annotation task. In Proceedings of LREC,
Genoa, pages 1951?1956.
Passonneau, Rebecca J. and Diane J. Litman.
1993. Intention-based segmentation:
Human reliability and correlation with
linguistic cues. In Proceedings of 31st
Annual Meeting of the ACL, pages 148?155,
Columbus, OH.
594
Artstein and Poesio Inter-Coder Agreement for CL
Passonneau, Rebecca J. and Diane J. Litman.
1996. Empirical analysis of three
dimensions of spoken discourse:
Segmentation, coherence and linguistic
devices. In Eduard H. Hovy and Donia
R. Scott, editors, Computational and
Conversational Discourse: Burning Issues ?
An Interdisciplinary Account, volume 151
of NATO ASI Series F: Computer and
Systems Sciences. Springer, Berlin,
chapter 7, pages 161?194.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics, 23(1):103?139.
Pevzner, Lev and Marti A. Hearst. 2002.
A critique and improvement of an
evaluation metric for text segmentation.
Computational Linguistics, 28(1):19?36.
Poesio, Massimo. 2004a. Discourse
annotation and semantic annotation in the
GNOME corpus. In Proceedings of the 2004
ACL Workshop on Discourse Annotation,
pages 72?79, Barcelona.
Poesio, Massimo. 2004b. The
MATE/GNOME proposals for anaphoric
annotation, revisited. In Proceedings of the
5th SIGdial Workshop on Discourse and
Dialogue, pages 154?162, Cambridge, MA.
Poesio, Massimo and Ron Artstein. 2005.
The reliability of anaphoric annotation,
reconsidered: Taking ambiguity into
account. In Proceedings of the Workshop on
Frontiers in Corpus Annotation II: Pie in the
Sky, pages 76?83, Ann Arbor, MI.
Poesio, Massimo and Natalia N. Modjeska.
2005. Focus, activation, and this-noun
phrases: An empirical study. In
Anto?nio Branco, Tony McEnery, and
Ruslan Mitkov, editors, Anaphora
Processing, volume 263 of Current Issues
in Linguistic Theory. John Benjamins,
pages 429?442, Amsterdam and
Philadelphia.
Poesio, Massimo, A. Patel, and Barbara
Di Eugenio. 2006. Discourse structure
and anaphora in tutorial dialogues: An
empirical analysis of two theories of the
global focus. Research in Language and
Computation, 4(2?3):229?257.
Poesio, Massimo and Renata Vieira. 1998.
A corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Popescu-Belis, Andrei. 2005. Dialogue acts:
One or more dimensions? Working
Paper 62, ISSCO, University of Geneva.
Posner, Karen L., Paul D. Sampson, Robert A.
Caplan, Richard J. Ward, and Frederick W.
Cheney. 1990. Measuring interrater
reliability among multiple raters: An
example of methods for nominal data.
Statistics in Medicine, 9:1103?1115.
Rajaratnam, Nageswari. 1960. Reliability
formulas for independent decision data
when reliability data are matched.
Psychometrika, 25(3):261?271.
Reidsma, Dennis and Jean Carletta. 2008.
Reliability measurement without limits.
Computational Linguistics, 34(3):319?326.
Reinhart, T. 1981. Pragmatics and linguistics:
An analysis of sentence topics.
Philosophica, 27(1):53?93.
Reynar, Jeffrey C. 1998. Topic Segmentation:
Algorithms and Applications. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Ries, Klaus. 2002. Segmenting conversations
by topic, initiative and style. In Anni R.
Coden, Eric W. Brown, and Savitha
Srinivasan, editors, Information Retrieval
Techniques for Speech Applications,
volume 2273 of Lecture Notes in Computer
Science. Springer, Berlin, pages 51?66.
Rietveld, Toni and Roeland van Hout.
1993. Statistical Techniques for the Study
of Language and Language Behaviour.
Mouton de Gruyter, Berlin.
Rosenberg, Andrew and Ed Binkowski.
2004. Augmenting the kappa statistic
to determine interannotator reliability
for multiply labeled data points. In
Proceedings of HLT-NAACL 2004: Short
Papers, pages 77?80, Boston, MA.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale
coding. Public Opinion Quarterly,
19(3):321?325.
Shriberg, Elizabeth, Raj Dhillon, Sonali
Bhagat, Jeremy Ang, and Hannah Carvey.
2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, MA.
Siegel, Sidney and N. John Castellan, Jr. 1988.
Nonparametric Statistics for the Behavioral
Sciences, 2nd edition, chapter 9.8.
McGraw-Hill, New York.
Stent, Amanda J. 2001. Dialogue Systems
as Conversational Partners: Applying
Conversation Acts Theory to Natural
Language Generation for Task-Oriented
Mixed-Initiative Spoken Dialogue. Ph.D.
thesis, Department of Computer Science,
University of Rochester.
Stevenson, Mark and Robert Gaizauskas.
2000. Experiments on sentence boundary
detection. In Proceedings of 6th ANLP,
pages 84?89, Seattle, WA.
595
Computational Linguistics Volume 34, Number 4
Stolcke, Andreas, Noah Coccaro, Rebecca
Bates, Paul Taylor, Carol Van Ess-Dykema,
Klaus Ries, Elizabeth Shriberg, Daniel
Jurafsky, Rachel Martin, and Marie Meteer.
2000. Dialogue act modeling for automatic
tagging and recognition of conversational
speech. Computational Linguistics,
26(3):339?373.
Stuart, Alan. 1955. A test for homogeneity of
the marginal distributions in a two-way
classification. Biometrika, 42(3/4):412?416.
Teufel, Simone, Jean Carletta, and Marc
Moens. 1999. An annotation scheme
for discourse-level argumentation in
research articles. In Proceedings of Ninth
Conference of the EACL, pages 110?117,
Bergen.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and
rhetorical status. Computational
Linguistics, 28(4):409?445.
Traum, David R. and Elizabeth A.
Hinkelman. 1992. Conversation
acts in task-oriented spoken dialogue.
Computational Intelligence, 8(3):575?599.
Vallduv??, Enric. 1993. Information
packaging: A survey. Research Paper
RP-44, University of Edinburgh, HCRC.
Ve?ronis, Jean. 1998. A study of polysemy
judgments and inter-annotator agreement.
In Proceedings of SENSEVAL-1,
Herstmonceux Castle, England. Available
at: http://www.itri.brighton.ac.uk/
events/senseval/ARCHIVE/PROCEEDINGS/.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth
Message Understanding Conference,
pages 45?52, Columbia, MD.
Zwick, Rebecca. 1988. Another look at
interrater agreement. Psychological
Bulletin, 103(3):374?378.
596
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 76?83,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguity
into Account
Massimo Poesio and Ron Artstein
University of Essex,
Language and Computation Group / Department of Computer Science
United Kingdom
Abstract
We report the results of a study of the
reliability of anaphoric annotation which
(i) involved a substantial number of naive
subjects, (ii) used Krippendorff?s ? in-
stead of K to measure agreement, as re-
cently proposed by Passonneau, and (iii)
allowed annotators to mark anaphoric ex-
pressions as ambiguous.
1 INTRODUCTION
We tackle three limitations with the current state of
the art in the annotation of anaphoric relations. The
first problem is the lack of a truly systematic study of
agreement on anaphoric annotation in the literature:
none of the studies we are aware of (Hirschman,
1998; Poesio and Vieira, 1998; Byron, 2003; Poe-
sio, 2004) is completely satisfactory, either because
only a small number of coders was involved, or
because agreement beyond chance couldn?t be as-
sessed for lack of an appropriate statistic, a situation
recently corrected by Passonneau (2004). The sec-
ond limitation, which is particularly serious when
working on dialogue, is our still limited understand-
ing of the degree of agreement on references to ab-
stract objects, as in discourse deixis (Webber, 1991;
Eckert and Strube, 2001).
The third shortcoming is a problem that affects all
types of semantic annotation. In all annotation stud-
ies we are aware of,1 the fact that an expression may
not have a unique interpretation in the context of its
1The one exception is Rosenberg and Binkowski (2004).
occurrence is viewed as a problem with the anno-
tation scheme, to be fixed by, e.g., developing suit-
ably underspecified representations, as done partic-
ularly in work on wordsense annotation (Buitelaar,
1998; Palmer et al, 2005), but also on dialogue act
tagging. Unfortunately, the underspecification solu-
tion only genuinely applies to cases of polysemy, not
homonymy (Poesio, 1996), and anaphoric ambigu-
ity is not a case of polysemy. Consider the dialogue
excerpt in (1):2 it?s not clear to us (nor was to our
annotators, as we?ll see below) whether the demon-
strative that in utterance unit 18.1 refers to the ?bad
wheel? or ?the boxcar?; as a result, annotators? judg-
ments may disagree ? but this doesn?t mean that the
annotation scheme is faulty; only that what is being
said is genuinely ambiguous.
(1) 18.1 S: ....
18.6 it turns out that the boxcar
at Elmira
18.7 has a bad wheel
18.8 and they?re .. gonna start
fixing that at midnight
18.9 but it won?t be ready until 8
19.1 M: oh what a pain in the butt
This problem is encountered with all types of anno-
tation; the view that all types of disagreement indi-
cate a problem with the annotation scheme?i.e., that
somehow the problem would disappear if only we
could find the right annotation scheme, or concen-
trate on the ?right? types of linguistic judgments?
is, in our opinion, misguided. A better approach
2This example, like most of those in the rest of the paper, is
taken from the first edition of the TRAINS corpus collected at
the University of Rochester (Gross et al, 1993). The dialogues
are available at ftp://ftp.cs.rochester.edu/pub/
papers/ai/92.tn1.trains_91_dialogues.txt.
76
is to find when annotators disagree because of in-
trinsic problems with the text, or, even better, to
develop methods to identify genuinely ambiguous
expressions?the ultimate goal of this work.
The paper is organized as follows. We first briefly
review previous work on anaphoric annotation and
on reliability indices. We then discuss our experi-
ment with anaphoric annotation, and its results. Fi-
nally, we discuss the implications of this work.
2 ANNOTATING ANAPHORA
It is not our goal at this stage to propose a new
scheme for annotating anaphora. For this study we
simply developed a coding manual for the purposes
of our experiment, broadly based on the approach
adopted in MATE (Poesio et al, 1999) and GNOME
(Poesio, 2004), but introducing new types of annota-
tion (ambiguous anaphora, and a simple form of dis-
course deixis) while simplifying other aspects (e.g.,
by not annotating bridging references).
The task of ?anaphoric annotation? discussed here
is related, although different from, the task of an-
notating ?coreference? in the sense of the so-called
MUCSS scheme for the MUC-7 initiative (Hirschman,
1998). This scheme, while often criticized, is still
widely used, and has been the basis of coreference
annotation for the ACE initiative in the past two
years. It suffers however from a number of prob-
lems (van Deemter and Kibble, 2000), chief among
which is the fact that the one semantic relation ex-
pressed by the scheme, ident, conflates a number
of relations that semanticists view as distinct: be-
sides COREFERENCE proper, there are IDENTITY
ANAPHORA, BOUND ANAPHORA, and even PRED-
ICATION. (Space prevents a fuller discussion and
exemplification of these relations here.)
The goal of the MATE and GNOME schemes (as
well of other schemes developed by Passonneau
(1997), and Byron (2003)) was to devise instructions
appropriate for the creation of resources suitable for
the theoretical study of anaphora from a linguis-
tic / psychological perspective, and, from a compu-
tational perspective, for the evaluation of anaphora
resolution and referring expressions generation. The
goal is to annotate the discourse model resulting
from the interpretation of a text, in the sense both of
(Webber, 1979) and of dynamic theories of anaphora
(Kamp and Reyle, 1993). In order to do this, annota-
tors must first of all identify the noun phrases which
either introduce new discourse entities (discourse-
new (Prince, 1992)) or are mentions of previously
introduced ones (discourse-old), ignoring those that
are used predicatively. Secondly, annotators have
to specify which discourse entities have the same
interpretation. Given that the characterization of
such discourse models is usually considered part
of the area of the semantics of anaphora, and that
the relations to be annotated include relations other
than Sidner?s (1979) COSPECIFICATION, we will use
the term ANNOTATION OF ANAPHORA for this task
(Poesio, 2004), but the reader should keep in mind
that we are not concerned only with nominal expres-
sions which are lexically anaphoric.
3 MEASURING AGREEMENT ON
ANAPHORIC ANNOTATION
The agreement coefficient which is most widely
used in NLP is the one called K by Siegel and Castel-
lan (1988). Howewer, most authors who attempted
anaphora annotation pointed out that K is not appro-
priate for anaphoric annotation. The only sensible
choice of ?label? in the case of (identity) anaphora
are anaphoric chains (Passonneau, 2004); but ex-
cept when a text is very short, few annotators will
catch all mentions of the same discourse entity?most
forget to mark a few, which means that agreement
as measured with K is always very low. Follow-
ing Passonneau (2004), we used the coefficient ? of
Krippendorff (1980) for this purpose, which allows
for partial agreement among anaphoric chains.3
3.1 Krippendorf?s alpha
The ? coefficient measures agreement among a set
of coders C who assign each of a set of items I to
one of a set of distinct and mutually exclusive cat-
egories K; for anaphora annotation the coders are
the annotators, the items are the markables in the
text, and the categories are the emerging anaphoric
chains. The coefficient measures the observed dis-
agreement between the coders Do, and corrects for
3We also tried a few variants of ?, but these differed from ?
only in the third to fifth significant digit, well below any of the
other variables that affected agreement. In the interest of space
we only report here the results obtained with ?.
77
chance by removing the amount of disagreement ex-
pected by chance De. The result is subtracted from 1
to yield a final value of agreement.
? = 1?
Do
De
As in the case of K, the higher the value of ?,
the more agreement there is between the annotators.
? = 1 means that agreement is complete, and ? = 0
means that agreement is at chance level.
What makes ? particularly appropriate for
anaphora annotation is that the categories are not
required to be disjoint; instead, they must be or-
dered according to a DISTANCE METRIC?a func-
tion d from category pairs to real numbers that spec-
ifies the amount of dissimilarity between the cate-
gories. The distance between a category and itself is
always zero, and the less similar two categories are,
the larger the distance between them. Table 1 gives
the formulas for calculating the observed and ex-
pected disagreement for ?. The amount of disagree-
ment for each item i ? I is the arithmetic mean of the
distances between the pairs of judgments pertaining
to it, and the observed agreement is the mean of all
the item disagreements. The expected disagreement
is the mean of the distances between all the judg-
ment pairs in the data, without regard to items.
Do =
1
ic(c?1) ?i?I ?k?K ?k??K niknik?dkk?
De =
1
ic(ic?1) ?k?K ?k??K nknk?dkk?
c number of coders
i number of items
nik number of times item i is classified in category k
nk number of times any item is classified in category k
dkk? distance between categories k and k?
Table 1: Observed and expected disagreement for ?
3.2 Distance measures
The distance metric is not part of the general defini-
tion of ?, because different metrics are appropriate
for different types of categories. For anaphora anno-
tation, the categories are the ANAPHORIC CHAINS:
the sets of markables which are mentions of the
same discourse entity. Passonneau (2004) proposes
a distance metric between anaphoric chains based on
the following rationale: two sets are minimally dis-
tant when they are identical and maximally distant
when they are disjoint; between these extremes, sets
that stand in a subset relation are closer (less distant)
than ones that merely intersect. This leads to the fol-
lowing distance metric between two sets A and B.
dAB =
?
???
???
0 if A = B
1/3 if A ? B or B ? A
2/3 if A?B 6= /0, but A 6? B and B 6? A
1 if A?B = /0
We also tested distance metrics commonly used
in Information Retrieval that take the size of the
anaphoric chain into account, such as Jaccard and
Dice (Manning and Schuetze, 1999), the ratio-
nale being that the larger the overlap between two
anaphoric chains, the better the agreement. Jac-
card and Dice?s set comparison metrics were sub-
tracted from 1 in order to get measures of distance
that range between zero (minimal distance, identity)
and one (maximal distance, disjointness).
dAB = 1?
|A?B|
|A?B|
(Jaccard)
dAB = 1?
2 |A?B|
|A|+ |B|
(Dice)
The Dice measure always gives a smaller distance
than the Jaccard measure, hence Dice always yields
a higher agreement coefficient than Jaccard when
the other conditions remain constant. The difference
between Dice and Jaccard grows with the size of the
compared sets. Obviously, the Passonneau measure
is not sensitive to the size of these sets.
3.3 Computing the anaphoric chains
Another factor that affects the value of the agree-
ment coefficient?in fact, arguably the most impor-
tant factor?is the method used for constructing from
the raw annotation data the ?labels? used for agree-
ment computation, i.e., the anaphoric chains. We
experimented with a number of methods. How-
ever, since the raw data are highly dependent on
the annotation scheme, we will postpone discussing
our chain construction methods until after we have
described our experimental setup and annotation
scheme. We will also discuss there how compar-
isons are made when an ambiguity is marked.
78
4 THE ANNOTATION STUDY
4.1 The Experimental Setup
Materials. The text annotated in the experiment
was dialogue 3.2 from the TRAINS 91 corpus. Sub-
jects were trained on dialogue 3.1.
Tools. The subjects performed their annotations
on Viglen Genie workstations with LG Flatron mon-
itors running Windows XP, using the MMAX 2 anno-
tation tool (Mu?ller and Strube, 2003).4
Subjects. Eighteen paid subjects participated in
the experiment, all students at the University of Es-
sex, mostly undergraduates from the Departments of
Psychology and Language and Linguistics.
Procedure. The subjects performed the experi-
ment together in one lab, each working on a separate
computer. The experiment was run in two sessions,
each consisting of two hour-long parts separated by
a 30 minute break. The first part of the first session
was devoted to training: subjects were given the an-
notation manual and taught how to use the software,
and then annotated the training text together. After
the break, the subjects annotated the first half of the
dialogue (up to utterance 19.6). The second session
took place five days later. In the first part we quickly
pointed out some problems in the first session (for
instance reminding the subjects to be careful during
the annotation), and then immediately the subjects
annotated the second half of the dialogue, and wrote
up a summary. The second part of the second session
was used for a separate experiment with a different
dialogue and a slightly different annotation scheme.
4.2 The Annotation Scheme
MMAX 2 allows for multiple types of markables;
markables at the phrase, utterance, and turn lev-
els were defined before the experiment. All noun
phrases except temporal ones were treated as phrase
markables (Poesio, 2004). Subjects were instructed
to go through the phrase markables in order (us-
ing MMAX 2?s markable browser) and mark each
of them with one of four attributes: ?phrase? if it
referred to an object which was mentioned earlier
in the dialogue; ?segment? if it referred to a plan,
4Available from http://mmax.eml-research.de/
event, action, or fact discussed earlier in the dia-
logue; ?place? if it was one of the five railway sta-
tions Avon, Bath, Corning, Dansville, and Elmira,
explicitly mentioned by name; or ?none? if it did
not fit any of the above criteria, for instance if it re-
ferred to a novel object or was not a referential noun
phrase. (We included the attribute ?place? in order
to avoid having our subjects mark pointers from ex-
plicit place names. These occur frequently in the
dialogue?49 of the 151 markables?but are rather un-
interesting as far as anaphora goes.) For markables
designated as ?phrase? or ?segment? subjects were
instructed to set a pointer to the antecedent, a mark-
able at the phrase or turn level. Subjects were in-
structed to set more than one pointer in case of am-
biguous reference. Markables which were not given
an attribute or which were marked as ?phrase? or
?segment? but did not have an antecedent specified
were considered to be data errors; data errors oc-
curred in 3 out of the 151 markables in the dialogue,
and these items were excluded from the analysis.
We chose to mark antecedents using MMAX 2?s
pointers, rather than its sets, because pointers allow
us to annotate ambiguity: an ambiguous phrase can
point to two antecedents without creating an asso-
ciation between them. In addition, MMAX 2 makes
it possible to restrict pointers to a particular level.
In our scheme, markables marked as ?phrase? could
only point to phrase-level antecedents while mark-
ables marked as ?segment? could only point to turn-
level antecedents, thus simplifying the annotation.
As in previous studies (Eckert and Strube, 2001;
Byron, 2003), we only allowed a constrained form
of reference to discourse segments: our subjects
could only indicate turn-level markables as an-
tecedents. This resulted in rather coarse-grained
markings, especially when a single turn was long
and included discussion of a number of topics. In
a separate experiment we tested a more compli-
cated annotation scheme which allowed a more fine-
grained marking of reference to discourse segments.
4.3 Computing anaphoric chains
The raw annotation data were processed using
custom-written Perl scripts to generate coreference
chains and calculate reliability statistics.
The core of Passonneau?s proposal (Passonneau,
2004) is her method for generating the set of dis-
79
tinct and mutually exclusive categories required by
? out of the raw data of anaphoric annotation. Con-
sidering as categories the immediate antecedents
would mean a disagreement every time two anno-
tators mark different members of an anaphoric chain
as antecedents, while agreeing that these different
antecedents are part of the same chain. Passonneau
proposes the better solution to view the emerging
anaphoric chains themselves as the categories. And
in a scheme where anaphoric reference is unambigu-
ous, these chains are equivalence classes of mark-
ables. But we have a problem: since our annotation
scheme allows for multiple pointers, these chains
take on various shapes and forms.
Our solution is to associate each markable m with
the set of markables obtained by following the chain
of pointers from m, and then following the pointers
backwards from the resulting set. The rationale for
this method is as follows. Two pointers to a single
markable never signify ambiguity: if B points to A
and C points to A then B and C are cospecificational;
we thus have to follow the links up and then back
down. However, two pointers from a single mark-
able may signify ambiguity, so we should not follow
an up-link from a markable that we arrived at via a
down-link. The net result is that an unambiguous
markable is associated with the set of all markables
that are cospecificational with it on one of their read-
ings; an ambiguous markable is associated with the
set of all markables that are cospecificational with at
least one of its readings. (See figure 1.)
Unambiguous
A
B C
 
 
@
@I
A 7? {A,B,C}
B 7? {A,B,C}
C 7? {A,B,C}
Ambiguous
D E
F
@
@I
 
 
D 7? {D,F}
E 7? {E,F}
F 7? {D,E,F}
Figure 1: Anaphoric chains
This method of chain construction also allows to
resolve apparent discrepancies between reference to
phrase-level and turn-level markables. Take for ex-
ample the snippet below: many annotators marked
a pointer from the demonstrative that in utterance
unit 4.2 to turn 3; as for that in utterance unit 4.3,
some marked a pointer to the previous that, while
others marked a pointer directly to turn 3.
(2) 3.1 M: and while it?s there it
should pick up the tanker
4.1 S: okay
4.2 and that can get
4.3 we can get that done by
three
In this case, not only do the annotators mark differ-
ent direct antecedents for the second that; they even
use different attributes??phrase? when pointing to a
phrase antecedent and ?segment? when pointing to
a turn. Our method of chain construction associates
both of these markings with the same set of three
markables ? the two that phrases and turn 3 ? captur-
ing the fact that the two markings are in agreement.5
4.4 Taking ambiguity into account
The cleanest way to deal with ambiguity would be
to consider each item for which more than one an-
tecedent is marked as denoting a set of interpreta-
tions, i.e., a set of anaphoric chains (Poesio, 1996),
and to develop methods for comparing such sets
of sets of markables. However, while our instruc-
tions to the annotators were to use multiple point-
ers for ambiguity, they only followed these instruc-
tions for phrase references; when indicating the ref-
erents of discourse deixis, they often used multi-
ple pointers to indicate that more than one turn had
contributed to the development of a plan. So, for
this experiment, we simply used as the interpreta-
tion of markables marked as ambiguous the union
of the constituent interpretations. E.g., a markable E
marked as pointing both to antecedent A, belonging
to anaphoric chain {A,B}, and to antecedent C, be-
longing to anaphoric chain {C,D}, would be treated
by our scripts as being interpreted as referring to
anaphoric chain {A,B,C,D}.
5 RESULTS
5.1 Agreement on category labels
The following table reports for each of the four cate-
gories the number of cases (in the first half) in which
5It would be preferable, of course, to get the annotators to
mark such configurations in a uniform way; this however would
require much more extensive training of the subjects, as well as
support which is currently unavailable from the annotation tool
for tracking chains of pointers.
80
a good number (18, 17, 16) annotators agreed on a
particular label?phrase, segment, place, or none?or
no annotators assigned a particular label to a mark-
able. (The figures for the second half are similar.)
Number of judgments 18 17 16 0
phrase 10 3 1 30
segment 1 52
place 16 1 1 54
none 10 5 1 29
Table 2: Cases of good agreement on categories
In other words, in 49 cases out of 72 at least 16
annotators agreed on a label.
5.2 Explicitly annotated ambiguity, and its
impact on agreement
Next, we attempted to get an idea of the amount
of explicit ambiguity?i.e., the cases in which coders
marked multiple antecedents?and the impact on re-
liability resulting by allowing them to do this. In
the first half, 15 markables out of 72 (20.8%) were
marked as explicitly ambiguous by at least one an-
notator, for a total of 55 explicit ambiguity mark-
ings (45 phrase references, 10 segment references);
in the second, 8/76, 10.5% (21 judgments of ambi-
guity in total). The impact of these cases on agree-
ment can be estimated by comparing the values of
K and ? on the antecedents only, before the con-
struction of cospecification chains. Recall that the
difference between the coefficients is that K does
not allow for partial disagreement while ? gives it
some credit. Thus if one subject marks markable A
as antecedent of an expression, while a second sub-
ject marks markables A and B, K will register a dis-
agreement while ? will register partial agreement.
Table 3 compares the values of K and ?, computed
separately for each half of the dialogue, first with
all the markables, then by excluding ?place? mark-
ables (agreement on marking place names was al-
most perfect, contributing substantially to overall
agreement). The value of ? is somewhat higher than
that of K, across all conditions.
5.3 Agreement on anaphora
Finally, we come to the agreement values obtained
by using ? to compare anaphoric chains computed
With place Without place
First Half K 0.62773 0.50066
? 0.65615 0.53875
Second Half K 0.66201 0.44997
? 0.67736 0.47490
The coefficient reported here as K is the one called K by Siegel
and Castellan (1988).
The value of ? is calculated using Passonneau?s distance metric;
for other distance metrics, see table 4.
Table 3: Comparing K and ?
as discussed above. Table 4 gives the value of ? for
the first half (the figures for the second half are sim-
ilar). The calculation of ? was manipulated under
the following three conditions.
Place markables. We calculated the value of ? on
the entire set of markables (with the exception of
three which had data errors), and also on a subset of
markables ? those that were not place names. Agree-
ment on marking place names was almost perfect:
45 of the 48 place name markables were marked cor-
rectly as ?place? by all 18 subjects, two were marked
correctly by all but one subject, and one was marked
correctly by all but two subjects. Place names thus
contributed substantially to the agreement among
the subjects. Dropping these markables from the
analysis resulted in a substantial drop in the value
of ? across all conditions.
Distance measure. We used the three measures
discussed earlier to calculate distance between sets:
Passonneau, Jaccard, and Dice.6
Chain construction. Substantial variation in the
agreement values can be obtained by making
changes to the way we construct anaphoric chains.
We tested the following methods.
NO CHAIN: only the immediate antecedents of an
anaphoric expression were considered, instead
of building an anaphoric chain.
PARTIAL CHAIN: a markable?s chain included only
phrase markables which occurred in the dia-
6For the nominal categories ?place? and ?none? we assign
a distance of zero between the category and itself, and of one
between a nominal category and any other category.
81
With place markables Without place markables
Pass Jacc Dice Pass Jacc Dice
No chain 0.65615 0.64854 0.65558 0.53875 0.52866 0.53808
Partial 0.67164 0.65052 0.67667 0.55747 0.53017 0.56477
Inclusive [?top] 0.65380 0.64194 0.69115 0.53134 0.51693 0.58237
Exclusive [?top] 0.62987 0.60374 0.64450 0.49839 0.46479 0.51830
Inclusive [+top] 0.60193 0.58483 0.64294 0.49907 0.47894 0.55336
Exclusive [+top] 0.57440 0.53838 0.58662 0.46225 0.41766 0.47839
Table 4: Values of ? for the first half of dialogue 3.2
logue before the markable in question (as well
as all discourse markables).
FULL CHAIN: chains were constructed by looking
upward and then back down, including all
phrase markables which occurred in the dia-
logue either before or after the markable in
question (as well as the markable itself, and all
discourse markables).
We used two separate versions of the full chain con-
dition: in the [+top] version we associate the top of
a chain with the chain itself, whereas in the [?top]
version we associate the top of a chain with its orig-
inal category label, ?place? or ?none?.
Passonneau (2004) observed that in the calcula-
tion of observed agreement, two full chains always
intersect because they include the current item. Pas-
sonneau suggests to prevent this by excluding the
current item from the chain for the purpose of cal-
culating the observed agreement. We performed the
calculation both ways ? the inclusive condition in-
cludes the current item, while the exclusive condi-
tion excludes it.
The four ways of calculating ? for full chains,
plus the no chain and partial chain condition, yield
the six chain conditions in Table 4. Other things be-
ing equal, Dice yields a higher agreement than Jac-
card; considering both halves of the dialogue, the
Passonneau measure always yielded a higher agree-
ment that Jaccard, while being higher than Dice in
10 of the 24 conditions, and lower in the remaining
14 conditions.
The exclusive chain conditions always give lower
agreement values than the corresponding inclusive
chain conditions, because excluding the current item
reduces observed agreement without affecting ex-
pected agreement (there is no ?current item? in the
calculation of expected agreement).
The [?top] conditions tended to result in a higher
agreement value than the corresponding [+top] con-
ditions because the tops of the chains retained their
?place? and ?none? labels; not surprisingly, the ef-
fect was less pronounced when place markables
were excluded from the analysis. Inclusive [?top]
was the only full chain condition which gave ? val-
ues comparable to the partial chain and no chain
conditions. For each of the four selections of mark-
ables, the highest ? value was given by the Inclusive
[?top] chain with Dice measure.
5.4 Qualitative Analysis
The difference between annotation of (identity!)
anaphoric relations and other semantic annotation
tasks such as dialogue act or wordsense annotation
is that apart from the occasional example of care-
lessness, such as marking Elmira as antecedent for
the boxcar at Elmira,7 all other cases of disagree-
ment reflect a genuine ambiguity, as opposed to dif-
ferences in the application of subjective categories.8
Lack of space prevents a full discussion of the
data, but some of the main points can already be
made with reference to the part of the dialogue in
(2), repeated with additional context in (3).
7According to our (subjective) calculations, at least one an-
notator made one obvious mistake of this type for 20 items out
of 72 in the first half of the dialogue?for a total of 35 careless
or mistaken judgment out of 1296 total judgments, or 2.7%.
8Things are different for associative anaphora, see (Poesio
and Vieira, 1998).
82
(3) 1.4 M: first thing I?d like you to do
1.5 is send engine E2 off with a boxcar
to Corning to pick up oranges
1.6 uh as soon as possible
2.1 S: okay [6 sec]
3.1 M: and while it?s there it
should pick up the tanker
The two it pronouns in utterance unit 3.1 are exam-
ples of the type of ambiguity already seen in (1).
All of our subjects considered the first pronoun a
?phrase? reference. 9 coders marked the pronoun
as ambiguous between engine E2 and the boxcar, 6
marked it as unambiguous and referring to engine
E2, and 3 as unambiguous and referring to the box-
car. This example shows that when trying to de-
velop methods to identify ambiguous cases it is im-
portant to consider not only the cases of explicit am-
biguity, but also so-called implicit ambiguity?cases
in which subjects do not provide evidence of being
consciously aware of the ambiguity, but the presence
of ambiguity is revealed by the existence of two or
more annotators in disagreement (Poesio, 1996).
6 DISCUSSION
In summary, the main contributions of this work so
far has been (i) to further develop the methodology
for annotating anaphoric relations and measuring the
reliability of this type of annotation, adopting ideas
from Passonneau and taking ambiguity into account;
and (ii) to run the most extensive study of reliabil-
ity on anaphoric annotation todate, showing the im-
pact of such choices. Our future work includes fur-
ther developments of the methodology for measur-
ing agreement with ambiguous annotations and for
annotating discourse deictic references.
ACKNOWLEDGMENTS
This work was in part supported by EPSRC project
GR/S76434/01, ARRAU. We wish to thank Tony
Sanford, Patrick Sturt, Ruth Filik, Harald Clahsen,
Sonja Eisenbeiss, and Claudia Felser.
References
P. Buitelaar. 1998. CoreLex : Systematic Polysemy and
Underspecification. Ph.D. thesis, Brandeis University.
D. Byron. 2003. Annotation of pronouns and their an-
tecedents: A comparison of two domains. Technical
Report 703, University of Rochester.
M. Eckert and M. Strube. 2001. Dialogue acts, synchro-
nising units and anaphora resolution. Journal of Se-
mantics.
D. Gross, J. Allen, and D. Traum. 1993. The TRAINS 91
dialogues. TRAINS Technical Note 92-1, Computer
Science Dept. University of Rochester, June.
L. Hirschman. 1998. MUC-7 coreference task definition,
version 3.0. In N. Chinchor, editor, In Proc. of the 7th
Message Understanding Conference.
H. Kamp and U. Reyle. 1993. From Discourse to Logic.
D. Reidel, Dordrecht.
K. Krippendorff. 1980. Content Analysis: An introduc-
tion to its Methodology. Sage Publications.
C. D. Manning and H. Schuetze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proc. of the 4th SIGDIAL.
M. Palmer, H. Dang, and C. Fellbaum. 2005. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering. To appear.
R. J. Passonneau. 1997. Instructions for applying dis-
course reference annotation for multiple applications
(DRAMA). Unpublished manuscript., December.
R. J. Passonneau. 2004. Computing reliability for coref-
erence annotation. In Proc. of LREC, Lisbon.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of the
ACL Workshop on Standards and Tools for Discourse
Tagging, pages 65?74.
M. Poesio. 1996. Semantic ambiguity and perceived am-
biguity. In K. van Deemter and S. Peters, editors, Se-
mantic Ambiguity and Underspecification, chapter 8,
pages 159?201. CSLI, Stanford, CA.
M. Poesio. 2004. The MATE/GNOME scheme for
anaphoric annotation, revisited. In Proc. of SIGDIAL,
Boston, May.
E. F. Prince. 1992. The ZPG letter: subjects, def-
initeness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: diverse
analyses of a fund-raising text, pages 295?325. John
Benjamins.
A. Rosenberg and E. Binkowski. 2004. Augmenting the
kappa statistic to determine interannotator reliability
for multiply labeled data points. In Proc. of NAACL.
C. L. Sidner. 1979. Towards a computational theory
of definite anaphora comprehension in English dis-
course. Ph.D. thesis, MIT.
S. Siegel and N. J. Castellan. 1988. Nonparametric
statistics for the Behavioral Sciences. McGraw-Hill.
K. van Deemter and R. Kibble. 2000. On coreferring:
Coreference in MUC and related annotation schemes.
Computational Linguistics, 26(4):629?637. Squib.
B. L. Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, New York.
B. L. Webber. 1991. Structure and ostension in the inter-
pretation of discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
83
Practical Grammar-Based NLG from Examples
David DeVault and David Traum and Ron Artstein
USC Institute for Creative Technologies
13274 Fiji Way
Marina del Rey, CA 90292
{devault,traum,artstein}@ict.usc.edu
Abstract
We present a technique that opens up
grammar-based generation to a wider range
of practical applications by dramatically re-
ducing the development costs and linguis-
tic expertise that are required. Our method
infers the grammatical resources needed for
generation from a set of declarative exam-
ples that link surface expressions directly to
the application?s available semantic represen-
tations. The same examples further serve to
optimize a run-time search strategy that gener-
ates the best output that can be found within an
application-specific time frame. Our method
offers substantially lower development costs
than hand-crafted grammars for application-
specific NLG, while maintaining high output
quality and diversity.
1 Introduction
This paper presents a new example-based genera-
tion technique designed to reduce the development
costs and linguistic expertise needed to integrate a
grammar-based generation component into an ex-
isting application. We believe this approach will
broaden the class of applications in which grammar-
based generation may feasibly be deployed.
In principle, grammar-based generation offers
significant advantages for many applications, when
compared with simpler template-based or canned
text output solutions, by providing productive cov-
erage and greater output variety. However, realiz-
ing these advantages can require significant devel-
opment costs (Busemann and Horacek, 1998).
One possible strategy is to exploit a wide-
coverage realizer that aims for applicability in mul-
tiple application domains (White et al, 2007; Cahill
and van Genabith, 2006; Zhong and Stent, 2005;
Langkilde-Geary, 2002; Langkilde and Knight,
1998; Elhadad, 1991). These realizers provide
a sound wide-coverage grammar (or robust wide-
coverage language model) for free, but demand a
specific input format that is otherwise foreign to
an existing application. Unfortunately, the devel-
opment burden of implementing the translation be-
tween the system?s available semantic representa-
tions and the required input format can be quite sub-
stantial (Busemann and Horacek, 1998). Indeed, im-
plementing the translation might require as much ef-
fort as would be required to build a simple custom
generator; cf. (Callaway, 2003). Thus, there cur-
rently are many applications where using a wide-
coverage generator remains impractical.
Another strategy is for system builders to hand
craft an application-specific grammar for genera-
tion. This approach can be initially attractive to
system builders because it allows syntactic cover-
age and semantic modeling to be tailored directly
to application needs. However, writing grammati-
cal rules by hand ultimately requires a painstaking,
time-consuming effort by a developer who has de-
tailed linguistic knowledge as well as detailed appli-
cation knowledge. Further, the resulting coverage is
inevitably limited to the set of linguistic construc-
tions that have been selected for careful modeling.
A third strategy is to use an example-based ap-
proach (Wong and Mooney, 2007; Stone, 2003;
Varges and Mellish, 2001) in which the connection
77
between available application semantic representa-
tions and desired output utterances is specified by
example. Example-based approaches aim to allow
system builders to specify a productive generation
capacity while leaving the representations and rea-
soning that underlie that productive capacity mostly
implicit in a set of training examples. This method-
ology insulates system builders from the detailed ex-
pertise and technical infrastructure needed to imple-
ment the productive capacity directly, and has made
example-based approaches attractive not only in text
generation but also in related areas such as concate-
native speech synthesis and motion capture based
animation; see, e.g., (Stone et al, 2004).
The technique we present in this paper is a new
example-based approach to specifying application-
specific text generation. As in other hand-crafted
and example-based approaches, our technique al-
lows syntactic coverage and semantic modeling to
follow the needs and available semantic representa-
tions in an application. One contribution of our tech-
nique is to relieve the generation content author of
the burden of manual syntactic modeling by lever-
aging an off-the-shelf parser; defects in the syntax
provided by the parser are effectively overcome us-
ing a machine learning technique. Additionally, our
technique organizes the authoring task in a way that
relieves the generation author of carefully modeling
the connections between particular syntactic con-
structions and available semantic representations.
Together, we argue, these features dramatically
reduce the linguistic expertise and other develop-
ment costs that are required to integrate a grammar-
based generation component into an existing system.
In a case study application, we show that our ap-
proach allows an application developer who lacks
detailed linguistic knowledge to extend grammatical
coverage at an expense of less than one minute per
additional lexical entry.
2 Case Study: Doctor Perez
Our approach has been tested as a replacement for
the generation component of interactive virtual hu-
mans used for social training purposes (Swartout et
al., 2006). Virtual humans are embodied conversa-
tional agents that play the role of people in simula-
tions or games. The case study we present in this
paper is the generation of output utterances for a
particular virtual human, Doctor Perez, who is de-
signed to teach negotiation skills in a multi-modal,
multi-party, non-team dialogue setting (Traum et al,
2008). The human trainee who talks to the doctor
plays the role of a U.S. Army captain named Cap-
tain Kirk. The design goals for Doctor Perez create
a number of requirements for a practical NLG com-
ponent. We briefly summarize these requirements
here; see (DeVault et al, 2008) for more details.
Doctor Perez has a relatively rich internal mental
state including beliefs, goals, plans, and emotions.
He uses an attribute-value matrix (AVM) semantic
representation to describe an utterance as a set of
core speech acts and other dialogue acts. Speech
acts generally have semantic contents that describe
propositions and questions about states and actions
in the domain. To facilitate interprocess communi-
cation, and statistical processing, this AVM structure
is linearized into a ?frame? of key values in which
each non-recursive terminal value is paired with a
path from the root to the final attribute. Figure 1
shows a typical frame. See (Traum, 2003) for addi-
tional details and examples of this representation.
While only hundreds of frames currently arise in
actual dialogues, the number of potential frames is
orders of magnitude larger, and it is difficult to pre-
dict in advance which frames might occur. The ut-
terances that realize these frames need to take a va-
riety of syntactic forms, including simple declar-
ative sentences, various modal constructions relat-
ing to hypothetical actions or plans, yes/no and wh-
questions, and abbreviated dialogue forms such as
elliptical clarification and repair requests, ground-
ing, and turn-taking utterances. Highly fluent out-
put is not a necessity for this character, since Doc-
tor Perez is designed to simulate a non-native En-
glish speaker. However, in order to support com-
pelling real-time conversational interaction and ef-
fective training, the generation module must be able
to identify an utterance for Doctor Perez to use
within approximately 200ms on modern hardware.
Finally, the development team for Doctor Perez?s
language capabilities includes approximately 10
programmers, testers, linguists, and computational
linguists. Wherever possible, it is better if any de-
veloper can improve any aspect of Doctor Perez?s
language processing; e.g., if a programmer discov-
78
ers a bug or disfluency in the NLG output, it is better
if she can fix it directly rather than requiring a (com-
putational) linguist to do so.
3 Technical Approach
Our approach builds on recently developed tech-
niques in statistical parsing, lexicalized syntax mod-
eling, generation with lexicalized grammars, and
search optimization to automatically construct all
the resources needed for a high-quality run-time
generation component. In particular, we leverage the
increasing availability of off-the-shelf parsers such
as (Charniak, 2001; Charniak, 2005) to automati-
cally (or semi-automatically) assign syntactic anal-
yses to a set of suggested output sentences. We
then draw on lexicalization techniques for statistical
language models (Magerman, 1995; Collins, 1999;
Chiang, 2000; Chiang, 2003) to induce a probabilis-
tic, lexicalized tree-adjoining grammar that supports
the derivation of all the suggested output sentences,
and many others besides.
The final step is to use the training examples to
learn an effective search policy so that our run-time
generation component can find good output sen-
tences in a reasonable time frame. In particular, we
use variants of existing search optimization (Daum?
and Marcu, 2005) and ranking algorithms (Collins
and Koo, 2005) to train our run-time component to
find good outputs within a specified time window;
see also (Stent et al, 2004; Walker et al, 2001). The
result is a run-time component that treats generation
as an anytime search problem, and is thus suitable
for applications in which a time/performance trade-
off is necessary (such as real-time dialogue).
3.1 Specification of Training Examples
Each training example in our approach speci-
fies a target output utterance (string), its syn-
tax, and a set of links between substrings within
the utterance and system semantic representa-
tions. Formally, a training example takes the form
(u, syntax(u), semantics(u)). We will illustrate
this format using the training example in Figure 1. In
this example, the generation content author suggests
the output utterance u = we don?t have medical
supplies here captain. Each utterance u is accom-
panied by syntax(u), a syntactic analysis in Penn
Treebank format (Marcus et al, 1994). In the fig-
ure, we show two alternative syntactic analyses that
might be specified: one is the uncorrected output of
the Charniak parser on this sentence, and the other
a hand-corrected version of that parse; we evaluate
the utility of this hand correction in Section 4.
To represent the meaning of utterances, our ap-
proach assumes that the system provides some set
M = {m1, ...,mj} of semantic representations.
The meaning of any individual utterance is then
identified with some subset of M . For Doctor Perez,
M comprises the 232 distinct key-value pairs that
appear in the system?s various generation frames. In
this example, the utterance?s meaning is captured by
the 8 key-value pairs indicated in the figure.
Our approach requires the generation content
author to link these 8 key-value pairs to con-
tiguous surface expressions within the utterance.
The technique is flexible about which surface ex-
pressions are chosen (e.g. they need not corre-
spond to constituent boundaries); however, they do
need to be compatible with the way the syntactic
analysis tokenizes the utterance, as follows. Let
t(u) = ?t1, ..., tn? be the terminals in the syn-
tactic analysis, in left-to-right order. Formally,
semantics(u) = {(s1,M1), ..., (sk,Mk)}, where
t(u) = s1@ ? ? ?@sk (with @ denoting concatena-
tion), and where Mi ? M for all i ? 1..k. In this
example, the surface expression we don?t, which to-
kenizes as ?we,do,n?t?, is connected to key-values
that indicate a negative polarity assertion.
This training example format has two features that
are crucial to our approach. First, the semantics of
an utterance is specified independently of its syntax.
This greatly reduces the amount of linguistic exper-
tise a generation content author needs to have. It
also allows making changes to the underlying syn-
tax without having to re-author the semantic links.
Second, the assignment of semantic representa-
tions to surface expressions must span the entire ut-
terance. No words or expressions can be viewed as
?meaningless?. This is essential because, otherwise,
the semantically motivated search algorithm used in
generation has no basis on which to include those
particular expressions when it constructs its output
utterance. Many systems, including Doctor Perez,
lack some of the internal representations that would
be necessary to specify semantics down to the lex-
79
Utterance we don?t have medical supplies here captain
Syntax
cat: SA??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP??
pos: RB??
here
cat: NP??
pos: NN??
captain
cat: SA??
cat: S??
cat: S??
cat: NP??
pos: PRP??
we
cat: VP??
pos: AUX??
do
pos: RB??
n?t
cat: VP??
pos: AUX??
have
cat: NP??
pos: JJ??
medical
pos: NNS??
supplies
cat: VP??
cat: ADVP??
pos: RB??
here
pos: VBP??
captain
(corrected Charniak parse) or (uncorrected Charniak parse)
Semantics
we do n?t . . . . . . .
{
have . . . . . . . . . . . . .
medical supplies . .
here . . . . . . . . . . . . .
captain . . . . . . . .
?
?
?
semantic frame
speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
speech-act.content.value = medical-supplies
speech-act.content.object-id = market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 1: A generation training example for Doctor Perez. If uncorrected syntax is used, the generation content author
only writes the utterance and the links to the semantic frame.
ical level. An important feature of our approach is
that it allows an arbitrary semantic granularity to be
employed, by mapping the representations available
in the system to appropriate multi-word chunks.
3.2 Automatic Grammar Induction
We adopt essentially the probabilistic tree-adjoining
grammar (PTAG) formalism and grammar induc-
tion technique of (Chiang, 2003). Our approach
makes three modifications, however. First, while
Chiang?s model includes both full adjunction and
sister adjunction operations, our grammar has only
sister adjunction (left and right), exactly as in the
TAGLET grammar formalism of (Stone, 2002). Sec-
ond, to support lexicalization at an arbitrary gran-
ularity, we allow Chiang?s tree templates to be as-
sociated with more than one lexical anchor. Third,
to unify syntactic and semantic reasoning in search,
we augment lexical anchors with semantic informa-
tion. Formally, wherever Chiang?s model has a lex-
ical anchor w, ours has a pair (?w1, ..., wn?,M ?),
where M ? ? M is connected to lexical anchors
?w1, ..., wn? by the generation content author, as in
Figure 1. The result is that the derivation probabili-
ties the grammar assigns depend not only on the im-
plicated syntactic structures and lexical anchors but
also on the senses of those lexical anchors in appli-
cation terms.
We induce our grammar from training exam-
ples such as Figure 1 using heuristics to assign
derivations to the examples, exactly as in (Chiang,
2003). The process proceeds in two stages. In
the first stage, a collection of rules is used to au-
tomatically ?decorate? the training syntax with a
number of features. These include deciding the
lexical anchor(s) for each non-terminal constituent
and assigning complement/adjunct status for non-
terminals which are not on their parent?s lexical-
ization path; see (Magerman, 1995; Chiang, 2003;
Collins, 1999). In addition, we deterministically add
features to improve several grammatical aspects, in-
cluding (1) enforcing verb inflectional agreement in
derived trees, (2) enforcing consistency in the finite-
ness of VP and S complements, and (3) restricting
subject/direct object/indirect object complements to
play the same grammatical role in derived trees.
In the second stage, the complements and ad-
juncts in the decorated trees are incrementally re-
80
syntax:
cat: SA??
fin: other,?? cat: S
cat: NP,?? apr: VBP,
apn: other??
pos: PRP??
we
fin: yes,?? cat: VP
apn: other,?? pos: VBP
do
pos: RB??
n?t
fin: yes,?? cat: VP,
gra: obj1??
fin: yes,?? cat: VP,
gra: obj1??
pos: VBP??
have
cat: NP,?? gra: obj1
operations: initial tree comp
semantics: speech-act.action = assert
speech-act.content.polarity = negative
speech-act.content.attribute = resourceAttribute
syntax:
cat: NP,?? apr: VBP,
gra: obj1,?? apn: other
pos: JJ??
medical
pos: NNS??
supplies
cat: ADVP,?? gra: adj
pos: RB??
here
cat: NP,?? apr: VBZ,
gra: adj,?? apn: 3ps
pos: NN??
captain
operations: comp left/right adjunction left/right adjunction
semantics: speech-act.content.value =
medical-supplies
speech-act.content.object-id =
market
addressee = captain-kirk
dialogue-act.addressee = captain-kirk
speech-act.addressee = captain-kirk
Figure 2: The linguistic resources inferred from the training example in Figure 1.
moved, yielding the reusable linguistic resources in
the grammar, as illustrated in Figure 2, as well as
the maximum likelihood estimates needed to com-
pute operation probabilities.
Our approach uses this induced grammar to treat
generation as a search problem: given a desired se-
mantic representation M ? ? M , use the grammar
to incrementally construct an output utterance u that
expressesM ?. We treat generation as anytime search
by accruing multiple goal states up until a specified
timeout (for Doctor Perez: 200ms) and returning a
list of alternative outputs ranked by their derivation
probabilities.
3.3 Optimizing the Search Strategy
The search space created by a grammar induced in
this way is too large to be searched exhaustively in
most applications. The solution we have developed
is a beam search strategy that uses weighted features
to rank alternative grammatical expansions at each
step. In particular, the beam size and structure is op-
timized so that, with high probability, the beam can
be searched exhaustively before the timeout.1 The
second step of automated processing, then, is a train-
ing problem of finding weighted features such that
1For Doctor Perez, we use a wider beam for initial trees,
since the Doctor?s semantic representation is particularly im-
poverished at the level of main verbs. At search depths > 1, we
use beam size 1 (i.e. greedy search).
for every training problem, nodes that lead to good
generation output are ranked highly enough by those
features to make it into the beam.
We use domain-independent rules to automati-
cally define a set of features that could be heuris-
tically useful for a given induced grammar. These
include features for various syntactic structures and
operations, numbers of undesired and desired mean-
ings of different types added by an expansion,
derivation probabilities, etc. (For Doctor Perez,
this yields about 600 features.) Our training algo-
rithm is based on the search optimization algorithm
of (Daum? and Marcu, 2005), which updates fea-
ture weights when mistakes are made during search
on training examples. For the weight update step,
we use the boosting approach of (Collins and Koo,
2005), which performs feature selection and iden-
tifies weight values that improve the ranking of al-
ternative derivation steps when mistakes are made
during search. We discuss the resulting success rate
and quality in the next section.
4 Cost/Benefit Analysis
The motivation that underlies our technical approach
is to reduce the development costs and linguistic ex-
pertise needed to develop a grammar-based genera-
tion component for an existing system. In this sec-
tion, we assess the progress we have made by ana-
81
lyzing the use of our approach for Doctor Perez.
Method. We began with a sample of 220 in-
stances of frames that Doctor Perez?s dialogue man-
ager had requested of the generation component in
previous dialogues with users. Each frame was as-
sociated with a hand-authored target output utter-
ance. We then constructed two alternative training
examples, in the format specified in Section 3.1, for
each frame. One example had uncorrected output
of the Charniak parser for the syntax, and another
had hand-corrected parser output (see Figure 1). The
connections between surface expressions and frame
key-value pairs were identical in both uncorrected
and corrected training sets.
We then built two generators using the two sets
of training examples. We used 90% of the data for
training and held out 10% for testing. The genera-
tors sometimes failed to find a successful utterance
within the 200ms timeout. For example, the success
rate of the version of our generator trained on uncor-
rected syntax was 96.0% for training examples and
81.8% for test examples.
Quality of generated output. To assess output
quality, 5 system developers rated each of 494 utter-
ances, in relation to the specific frame for which it
was produced, on a single 1 (?very bad?) to 5 (?very
good?) scale. The 494 utterances included all of the
hand-authored (suggested) utterances in the training
examples. They also included all the top-ranked ut-
terances that were successfully generated by the two
generators. We asked our judges to make an over-
all assessment of output quality, incorporating both
accuracy and fluency, for the Doctor Perez charac-
ter. Judges were blind to the conditions under which
utterances were produced. We discuss additional de-
tails of this rating task in (DeVault et al, 2008).
The judges achieved a reliability of ? = 0.708
(Krippendorff, 1980); this value shows that agree-
ment is well above chance, and allows for tentative
conclusions. We ran a small number of planned
comparisons on these ratings. Surprisingly, we
found no significant difference between generated
output trained on corrected and uncorrected syntax
(t(29) = 0.056, p > 0.9 on test items, t(498) =
?1.1, p > 0.2 on all items).2 However, we did
2The distribution of ratings across utterances is not normal;
to validate our results we accompanied each t-test by a non-
parametric Wilcoxon rank sum test, and significance always fell
Hand-authored (N = 1099)
Generated:
Training input (N = 949)
Test input (N = 90)
Rating
Fr
eq
ue
nc
y
(%
)
0
10
20
30
40
50
60
1 2 3 4 5
Figure 3: Observed rating frequencies for hand-authored
vs. generated utterances (uncorrected syntax).
find that hand-authored utterances (mean rating 4.4)
are significantly better (t(388) = 5.9, p < 0.001)
than generated utterances (mean rating 3.8 for un-
corrected syntax). These ratings are depicted in Fig-
ure 3. While the figure suggests a slight reduction in
quality for generated output for test frames vs. train-
ing frames, we did not find a significant difference
between the two (t(19) = 1.4228, p > 0.15).
Variety of generated output. In general our any-
time algorithm delivers a ranked list of alternative
outputs. While in this initial study our judges rated
only the highest ranked output generated for each
frame, we observed that many of the lower ranked
outputs are of relatively high quality. For example,
Figure 4 shows a variety of alternative outputs that
were generated for two of Doctor Perez?s training
examples. Many of these outputs are not present as
hand-authored utterances (for any frame); this illus-
trates the potential of our approach to provide a va-
riety of alternative outputs or paraphrases, which in
some applications may be useful even for meanings
for which an example utterance is hand-authored.
Figure 5 shows the overall distribution in the number
of outputs returned for Doctor Perez.
Development costs. The development costs in-
cluded implementation of the approach and specifi-
cation of Doctor Perez?s training set. Implementa-
in the same general range.
82
Rank Time (ms) Novel?
1 16 no the clinic is up to standard captain
2 94 no the clinic is acceptable captain (hand-authored for this input)
3 78 yes the clinic should be in acceptable condition captain
4 16 yes the clinic downtown is currently acceptable captain
5 78 yes the clinic should agree in an acceptable condition captain
1 94 no there are no medical supplies downtown captain
2 172 no we don?t have medical supplies downtown captain
3 125 yes well captain i do not think downtown there are medical supplies
4 16 yes i do not think there are medical supplies downtown captain
Figure 4: All the utterances generated (uncorrected syntax) for two examples. Rank is determined by derivation
probability. Outputs marked as novel are different from any suggested output for any training example.
Number of successful outputs
Fr
eq
ue
nc
y
(%
)
0
10
20
30
0 1 2 3 4 5 6 7 8 9
Figure 5: Variety of outputs for each input.
tion required an effort of approximately six person
months. The developer who carried out the imple-
mentation initially had no familiarity with the Doc-
tor Perez domain, so part of this time was spent un-
derstanding Doctor Perez and his available seman-
tic representations. The bulk of the development
time was spent implementing the grammar induction
and training processes. Grammar induction included
implementing the probabilistic grammar model and
writing about 40 rules that are used to extract gram-
matical entries from the training examples. Of these
40 rules, only 3 are specific to Doctor Perez.3 The
remainder are broadly applicable to syntactic anal-
yses in Penn Treebank format, and thus we expect
they would transfer to applications of our approach
in other domains. Similarly, the training algorithms
are entirely domain neutral and could be expected to
transfer well to additional domains.
Specification of Doctor Perez?s training data took
3These 3 rules compensate for frequent errors in Charniak
parser output for the words captain, elder, and imam, which are
often used to signal the addressee of Doctor Perez?s utterances.
about 6 hours, or about 1.6 minutes per training ex-
ample. This time included hand correction of syn-
tactic analyses generated by the Charniak parser and
definition of semantic links between surface expres-
sions and frame key-value pairs. Since we found
that hand-correcting syntax does not improve out-
put quality, this 1.6 minutes/example figure over-
estimates the authoring time required by our ap-
proach. The remaining work lies in defining the se-
mantic links. For Doctor Perez, approximately half
of the semantic links were automatically assigned
with simple ad hoc scripts.4 The semantic linking
process might be further sped up through a stream-
lined authoring interface offering additional automa-
tion, or even using a machine learning approach to
suggest appropriate links.
Linguistic expertise required. Since we found
that hand-correcting syntax does not improve output
quality, a developer who wishes to exploit our ap-
proach may use the Charniak parser to supply the
syntactic model for the domain. Thus, while one
developer with linguistic expertise is required to im-
plement the approach, anybody on the application
team can contribute by hand authoring additional ut-
terances and defining semantic links. The benefit of
this authoring effort is the ability to generate high
quality output for many novel semantic inputs.
Cost/benefit. The grammar induced from the 198
training examples (with uncorrected syntax) con-
tains 426 lexical entries of the type depicted in Fig-
ure 2. These 426 lexical entries were produced auto-
matically from about 6 hours worth of authoring ef-
4Time to compose these scripts is included in the 1.6 min-
utes/example.
83
fort together with domain-neutral algorithms. This
translates to a rate of grammar expansion of less
than 1 minute per lexical entry, on average, for this
small application-specific grammar. This constitutes
a dramatic improvement over our previous experi-
ence hand-crafting grammars. It would be challeng-
ing for an expert to specify a lexical entry such as
those in Figure 2 in under one minute (and probably
impossible for someone lacking detailed linguistic
knowledge). In our experience, however, the bulk
of development lies in additional time spent con-
sidering and investigating possible interactions be-
tween lexical entries in generation. Our technique
helps with both problems: the grammar induction
streamlines the specification of lexical entries, and
the training removes the need for a developer to
manually trace through the various complex inter-
actions between lexical entries during generation.
5 Limitations
Currently, we do not support semantic links from
non-contiguous expressions, which means a desired
output like ?we rely heavily on medical supplies?
would be difficult to annotate if rely...on corresponds
to a single semantic representation. This is not an in-
trinsic limitation to our general approach, but rather
a simplification in our initial implementation.
As discussed in Section 3.2, our grammar induc-
tion process adds syntactic features related to verb
inflection, finiteness, and grammatical role to the in-
ferred lexical entries. Such features improve the flu-
ency and accuracy of output derived with the gram-
mar. While we believe such features can always be
assigned using domain-independent rules, develop-
ing these rules requires linguistic expertise, and it
is likely that additional rules and features (not yet
implemented) would improve coverage of linguistic
phenomena such as control verbs, various kinds of
coordination, and relative clauses, inter alia.
A more entrenched limitation of our approach
is its assumption that the generator does not need
context as a separate input. This means, for ex-
ample, that our approach cannot generate referring
expressions (by selecting disambiguating semantic
properties); rather, all semantic properties must be
pre-selected and included in the generation request.
Generation of anaphoric expressions is also limited,
since contextual ambiguities are not considered.
6 Related Work
To our knowledge, this is the first implemented
generation technique that does all three of the fol-
lowing: directly interfaces to existing application
semantic representations, infers a phrase structure
grammar from examples, and does not require hand-
authored syntax as input. (Varges and Mellish,
2001) also aims to reduce the authoring burden of
domain-specific generation; however, they seem to
use a special purpose semantic annotation rather
than pre-existing application semantics, and their
task is defined in terms of the Penn Treebank, so
hand-authored syntax is used as input. (Wong and
Mooney, 2007) also interfaces to existing applica-
tion semantics, and does not require hand-authored
syntax as input. Their technique infers a syn-
chronous grammar in which the hierarchical linguis-
tic analysis is isomorphic to the hierarchy in the ap-
plication semantics, and differs from phrase struc-
ture. It would be interesting to compare their out-
put quality with ours; their automated alignment of
words to semantics might also provide a way to fur-
ther reduce the authoring burden of our approach.
7 Conclusion and Future Work
We have presented a new example-based approach
to specifying text generation for an existing appli-
cation. We have used a cost/benefit analysis to ar-
gue that our approach offers productive coverage
and high-quality output with less linguistic expertise
and lower development costs than building a hand-
crafted grammar. In future work, we will evaluate
our approach in additional application settings, and
study the performance of our approach as the size
and scope of the training set grows.
Acknowledgments
Thanks to our anonymous reviewers, Arno Hartholt,
Susan Robinson, Thomas Russ, Chung-chieh Shan,
andMatthew Stone. This work was sponsored by the
U.S. Army Research, Development, and Engineer-
ing Command (RDECOM), and the content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should be
inferred.
84
References
S. Busemann and H. Horacek. 1998. A flexible shallow
approach to text generation. In Proceedings of INLG,
pages 238?247.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In ACL, pages 1033?1040.
C. B. Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. Proceedings of IJCAI.
E. Charniak. 2001. Immediate-head parsing for lan-
guage models. In ACL, pages 124?131, Morristown,
NJ, USA. Association for Computational Linguistics.
E. Charniak. 2005. ftp://ftp.cs.brown.edu/pub/nlparser/
parser05Aug16.tar.gz.
D. Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
456?463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
D. Chiang. 2003. Statistical parsing with an automat-
ically extracted tree adjoining grammar. In R. Bod,
R. Scha, and K. Sima?an, editors, Data Oriented Pars-
ing, pages 299?316. CSLI Publications, Stanford.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. dissertation, Uni-
versity of Pennsylvania.
H. Daum? and D. Marcu. 2005. Learning as search
optimization: approximate large margin methods for
structured prediction. In ICML ?05: Proceedings of
the 22nd international conference on Machine learn-
ing, pages 169?176, New York, NY, USA. ACM.
David DeVault, David Traum, and Ron Artstein. 2008.
Making grammar-based generation easier to deploy in
dialogue systems. In Ninth SIGdial Workshop on Dis-
course and Dialogue (SIGdial).
M. Elhadad. 1991. FUF: the universal unifier user man-
ual version 5.0. Technical Report CUCS-038-91.
K. Krippendorff, 1980. Content Analysis: An Introduc-
tion to Its Methodology, chapter 12, pages 129?154.
Sage, Beverly Hills, CA.
I. Langkilde and K. Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
COLING-ACL, pages 704?710.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator.
D. M. Magerman. 1995. Statistical decision-tree mod-
els for parsing. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics,
pages 276?283, Morristown, NJ, USA. Association for
Computational Linguistics.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguistics,
19(2):313?330.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In ACL.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Ro-
driguez, Adrian Stere, Alyssa Lees, and Chris Bregler.
2004. Speaking with hands: creating animated con-
versational characters from recordings of human per-
formance. ACM Trans. Graph., 23(3):506?513.
M. Stone. 2002. Lexicalized grammar 101. In ACL
Workshop on Tools and Methodologies for Teaching
Natural Language Processing.
Matthew Stone. 2003. Specifying generation of referring
expressions by example. In AAAI Spring Symposium
on Natural Language Generation in Spoken and Writ-
ten Dialogue, pages 133?140.
W. Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,
J. Rickel, and D. Traum. 2006. Toward virtual hu-
mans. AI Mag., 27(2):96?108.
D. R. Traum, W. Swartout, J. Gratch, and S. Marsella.
2008. A virtual human dialogue model for non-team
interaction. In L. Dybkjaer and W. Minker, editors,
Recent Trends in Discourse and Dialogue. Springer.
D. Traum. 2003. Semantics and pragmatics of questions
and answers for dialogue agents. In proceedings of the
International Workshop on Computational Semantics,
pages 380?394, January.
Sebastian Varges and Chris Mellish. 2001. Instance-
based natural language generation. In NAACL, pages
1?8.
M. Walker, O. Rambow, and M. Rogati. 2001. Spot:
A trainable sentence planner. In Proceedings of the
North American Meeting of the Association for Com-
putational Linguistics.
M. White, R. Rajkumar, and S. Martin. 2007. To-
wards broad coverage surface realization with CCG.
In Proc. of the Workshop on Using Corpora for NLG:
Language Generation and Machine Translation (UC-
NLG+MT).
Yuk Wah Wong and Raymond Mooney. 2007. Genera-
tion by inverting a semantic parser that uses statistical
machine translation. In Proceedings of NAACL-HLT,
pages 172?179.
H. Zhong and A. Stent. 2005. Building surface realiz-
ers automatically from corpora using general-purpose
tools. In Proc. Corpus Linguistics ?05 Workshop on
Using Corpora for Natural Language Generation.
85
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 193?200,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Don?t tell anyone! Two Experiments on Gossip Conversations Jenny Brusk School of informatics and humanities University of Sk?vde P.O. Box 408 541 28 Sk?vde, Sweden jenny.brusk@his.se Ron Artstein, David Traum USC Institute for Creative Technologies 13274 Fiji Way Marina del Rey, CA 90292 {artstein, traum}@ict.usc.edu     Abstract The purpose of this study is to get a working definition that matches people?s intuitive notion of gossip and is suffi-ciently precise for computational imple-mentation. We conducted two experi-ments investigating what type of conver-sations people intuitively understand and interpret as gossip, and whether they could identify three proposed constitu-ents of gossip conversations: third per-son focus, pejorative evaluation and sub-stantiating behavior. The results show that (1) conversations are very likely to be considered gossip if all elements are present, no intimate relationships exist between the participants, and the person in focus is unambiguous. (2) Conversa-tions that have at most one gossip ele-ment are not considered gossip. (3) Con-versations that lack one or two elements or have an ambiguous element lead to inconsistent judgments.  1 Introduction We are interested in creating believable charac-ters, i.e. ?characters that provide the illusion of life? (Bates, 1994). Since people engage exten-sively in gossip, such characters also need to be able to understand and engage in gossip in or-der to be believable in some situations. To en-able characters to engage in gossip, we need a computational model of gossip that can be ap-plied in the authoring of such characters and/or by the characters themselves. Unfortunately, such a model does not yet exist.  Moreover, there is not yet a clear consensus on how gossip should be defined, and most of the definitions are too vague or too general to 
be useful. Merriam-Webster online dictionary, for example, defines gossip as ?rumor or report of an intimate nature? and ?chatty talk?, neither of which is specific enough. What we need is a working definition that (a) matches people?s intuitive notion of gossip to the extent possible, given that the notion itself is somewhat vague, and (b) is sufficiently precise to provide a basis for computational implementation.  More recent definitions (e.g. Eder and Enke, 1991; Eggins and Slade, 1997; Hallett et al, 2009) have been derived from analyzing tran-scriptions of real gossip conversations. These definitions have only minor individual differ-ences and can in essence be formulated as ?evaluative talk about an absent third person?. We have chosen to use this definition as a start-ing point since it currently is the most specific one and since it is based on the observed struc-ture of naturally occurring gossip conversa-tions. This paper reports the results from two ex-periments on gossip conversations. The first experiment aimed at investigating what type of conversations people intuitively perceive as gossip. In the second study we also wanted to find out whether the subjects would accept a given definition and could apply it by identify-ing three specified gossip elements.   The paper is structured as follows. In sec-tion 2 we give a background to gossip with re-spect to both its social function as well as its conversational structure. Section 3 introduces the experimental method. In sections 4 and 5 we present the two experiments and discuss the results. In section 6, finally, we give some final remarks and suggestions for future work. 2 Background Gossip has been described as a mechanism for social control (e.g. Gluckman, 1963; Fine and Rosnow, 1978; Bergmann, 1993; Eggins and 
193
Slade, 1997) that maintains ?the unity, morals and values of social groups? (Gluckman, 1963). It has furthermore been suggested that gossip is a form of ?information-management?, primar-ily to improve one?s self-image and ?protect individual interests? (Paine, 1967), but also to influence others (Szwed, 1966; Fine and Ros-now, 1978). Gossip can furthermore be viewed as a form of entertainment (Abrahams, 1970) ? ?a satisfying diversion from the tedium of rou-tine activities? (Fine and Rosnow, 1978:164).  Recent studies have used a sociological ap-proach focusing on analyzing the structure of gossip conversations (e.g. Bergmann, 1993; Eder and Enke, 1991; Eggins and Slade, 1997; Hallett et al, 2009). Rather than observing and interviewing people in a certain community about their gossip behavior, they have analyzed transcripts of naturally occurring gossip con-versations. Their studies show that gossipers collaborate in creating the gossip, making it a highly interactive genre. They also identified two key elements of gossip:  ? Third person focus ? the identification of an absent third person that is ac-quainted with, but emotionally disjoint from the other participants (Bergmann (1993) refers to this as being ?virtually? absent, while Goodwin (1980) labels it ?symbolically? absent).  ? An evaluation of the person in focus or of his or her behavior. Eggins and Slade (1997) propose that the evaluation necessarily is pejorative to separate gos-sip from other types of chat.  Hallett et al (2009) found that the gossipers often use implicit evaluations to conceal the critique, suggesting that the gossipers either speak in general terms about something that implicitly is understood to be about a certain person, or that the gossipers avoid evaluating the behavior under the assumption that the evaluation is implicit in the behavior itself. In-stead of specifying the evaluation as being pe-jorative, they say it is ?unsanctioned?.  In addition to the two elements described above, Eggins and Slade (1997) propose a third obligatory element: ? Substantiating behavior ? An elabora-tion of the deviant behavior that can ei-ther be used as a motivation for the nega-tive evaluation, or as a way to introduce gossip in the conversation. Eder and 
Enke (1991) use a different model, but the substantiating behavior component corresponds roughly to their optional Explanation act.  There seems to be a consensus that gossip conversations have third person focus. The question is whether a gossip conversation nec-essarily has both a substantiating behavior component as well as a pejorative evaluation component, and if they do, can they be identi-fied? In the experiments presented later in this paper, we hope to shed light on whether these components are necessary or not. 3 Method  During the fall 2009, we conducted two ex-periments about gossip conversations. The aim of the experiments was to verify to what extent the definition of gossip accords with intuitive recognition of gossip episodes, and secondly whether people could reliably identify constitu-ent elements. The data was collected using online ques-tionnaires1 that were distributed through differ-ent email-lists mainly targeting researchers and students within game design, language technol-ogy, and related fields, located primarily in North America and Europe. The questionnaires had the following structure: The first page con-sisted of an introduction, including instructions, and each page thereafter had a dialogue excerpt retrieved from a screenplay followed by the question and/or task.  3.1 Hypotheses  Based on the previous studies presented earlier (in particular Bergmann, 1993; Eder and Enke, 1991; and Eggins and Slade, 1997) we had the following hypotheses: ? The more gossip elements present in the text, the more likely the conversation will be considered gossip. ? Third person focus is a necessary (but not sufficient) element of gossip.  ? Conversations in which the participants (including the target) are intimately re-lated will be rated lower than those in which all participants are emotionally separated.                                                  1 Created using http://www.surveygizmo.com/ 
194
4 Experiment I: Identifying gossip text The aim of the first experiment was to investi-gate how people intuitively understand and in-terpret gossip conversations.   4.1 Material and procedure  The questionnaire contained 16 different dia-logue excerpts retrieved from transcripts of the famous sitcoms Desperate Housewives 2  and Seinfeld3. The excerpts were selected to cover different combinations of the elements pre-sented in the previous section (third person fo-cus, an evaluation, and a motivation for the evaluation), as in the following dialogue4: B:  Tisha. Tisha. Oh, I can tell by that look on your face you?ve got something good. Now, come on, don?t be selfish.  T:  Well, first off, you?re not friends with Maisy Gibbons, are you? B:  No. T:  Thank god, because this is too good. Maisy was arrested. While Harold was at work, she was having sex with men in her house for money. Can you imagine?  B:  No, I can?t. T:  And that?s not even the best part. Word is, she had a little black book with all her clients? names.  R:  So, uh ? you think that?ll get out? T:  Of course. These things always do. Nancy, wait up. I can?t wait to tell you this. Wait, wait. A preliminary analysis to determine whether the elements were present or not, was made by the first author. The instructions contained no information about the elements and no defini-tion was given. To each excerpt we provided some contextual information, such as the inter-personal relationship between the speakers and other people mentioned in the dialogue, e.g.: The married couple, Bree (B) and Rex (R) Van de Kamp, is having lunch at the club. Some women laughing at the next table cause the two of them to turn and look. One of their acquaintances, Tisha (T), walks away from that table and heads to another one. Maisy Gibbons is another woman in their neigh-borhood, known to be very dominant and judgmental towards the other women.                                                   2 Touchstone Television (season 1 & 2) 3 Castle Rock Entertainment 4 From Desperate Housewives, Touchstone Television. 
The subjects were asked to read and rank the excerpts using the following scale: ? Absolutely not gossip  ? Could be considered gossip in some contexts ? Would be considered gossip in most contexts ? Absolutely gossip For the purpose of analysis we converted the above responses to integers from 0 to 3. 4.2 Results  A total of 52 participants completed the ex-periment. The following table shows the distri-bution of ratings for each of the 16 excerpts (the table is sorted by the mean rating).  Rating distribution ID5 0 1 2 3 Mean rating 11 50 1 1 0 0.058 6 46 5 0 1 0.154 15 33 15 4 0 0.442 2 28 20 4 0 0.538 5 30 15 6 1 0.577 10 17 24 10 1 0.904 9 10 26 13 3 1.173 16 11 17 16 8 1.404 4 8 18 18 8 1.500 14 11 13 11 17 1.654 3 6 20 11 15 1.673 1 1 17 25 9 1.808 13 3 18 17 14 1.808 12 5 9 15 23 2.077 8 3 0 11 38 2.615 7 1 2 4 45 2.788  Table 1: Gossip ratings of all 16 questions sorted by their mean value.   It is apparent from the table that a few ex-cerpts are clearly gossip or clearly not gossip, but there is much disagreement on other ex-cerpts. Inter-rater reliability is ? = 0.437: well above chance, but not particularly high6.  Only 7 of the 16 excerpts (ID #2, 5, 6, 7, 8, 11, 15) were clearly rated as gossip or not gossip by more than half of the subjects, and only 5 of those have a mean rating below 0.5 or above 2.5.                                                   5 Presentation was ordered by ID, same for all subjects. 6 The reported value is Krippendorff?s ? with the interval distance metric (Krippendorff 1980). Interval ? is defined as 1 ? Do/De, where Do (observed disagreement) is twice the mean variance of the individual item ratings, and De (expected disagreement) is twice the variance of all the ratings. For the above table, Do = 1.327 and De = 2.585. 
195
Despite the apparently low agreement, the results correspond fairly well with our expecta-tions. The 3 excerpts with a mean value below 0.5 had no gossip elements at all and the other two excerpts with a median value of 0 had only one gossip element. Similarly, the two excerpts rated highest clearly had all gossip elements. The rest of the excerpts, however, either lacked one element or had one element that was un-clear in some regard (see discussion, below). Conversations between family members or partners also caused higher disagreements, which seem to support Bergmann?s (1993) re-mark: ?[?] we can ask whether we should call gossip the conversations between spouses [?] alone. This surely is a borderline case for which there is no single answer? (p. 68).  4.3 Discussion  Among the nine excerpts with a mean value approximately between 1 and 2 (ID #1, 3, 4, 9, 10, 12, 13, 14, and 16), we made the following observations: 3 excerpts lacked one element; in 2 of them, the gossipers were family members or partners; 3 excerpts had an ambiguous focus, among which one also possibly was perceived as a warning.  By ?ambiguous focus? we mean that it is un-clear whether the person in focus is the speaker, the addressee or the absent third person. In-stead, the absent third person seems to play a sub-ordinate role rather than focused role, for instance as part of a self-disclosure or a con-frontation. If the conversation is the least bit confrontational, the addressee tends to go into defense rather than choosing a more typical gossip response, such as support, expansion, or challenge (Eder and Enke, 1991) in order to protect the face. Hence, no ?gossip fuel? is added to the conversation.  The result of the remaining excerpt7 is how-ever more difficult to explain. One possible explanation is that the initiator was unac-quainted with the target, but perhaps more likely is that some of the subjects interpreted the conversation as mocking rather than gossip:  E: Who?s that? D:  That?s Sam, the new girl in accounting. W: What?s with her arms? They just hang like salamis. D:  She walks like orangutan. E:  Better call the zoo.                                                  7 ID #14. From Seinfeld, Castle Rock Entertainment. 
5 Experiment II: Identifying gossip elements in a text The aim of the second experiment was to inves-tigate whether the subjects could accept and apply a given definition by identifying the three obligatory elements of gossip according to Eggins and Slade (1997) (see section 2); third person focus, pejorative evaluation, and sub-stantiating behavior. In addition to the ele-ments, we provided the more general definition presented in section 1 (?evaluative talk about an absent third person?). The results from the first experiment indi-cated that conversations that seemingly had all the elements but in which the person in focus was ambiguous, received a lower gossip rating than those having an unambiguous third person focus. So an additional goal was to investigate whether changing the relationship between the participants would affect the gossip rating.  5.1 Material  We used excerpts from Seinfeld8, Desperate Housewives 9 , Legally blonde 10 , and Mean girls11. In total we selected 21 excerpts, of which 8 also occurred in the first experiment. Two of the recurring excerpts were used both in their original versions as well as in modified versions, in which we had removed the emo-tional connections between the participants. The purpose of this was to find out whether changing the interpersonal relationship would change the gossip rating.  5.2 Procedure The subjects were instructed to read the ex-cerpts and then identify the gossip elements according to the following description: ? The person being talked about (third per-son focus) ? the ?target?, e.g. ?Maisy Gibbons was arrested?  ? Pejorative evaluation. A judgment of the target him-/herself or of the target?s be-havior. This evaluation is in most cases negative, e.g. ?She?s a slut?, ?He?s weird?                                                  8 Touchstone Television. 9 Castle Rock Entertainment. 10 Directed by Robert Luketic. Metro Goldwyn Mayer (2001). 11 Directed by Mark Waters. Paramount Pictures (2004). 
196
? The deviant behavior that motivates the gossip and provides evidence for the judgment (also called the substantiating behavior stage), e.g. ?Maisy Gibbons was arrested? For each element they found, they were asked to specify the corresponding line refer-ence as given in the text. They were also in-structed to say whether they considered the conversation to be gossip or not gossip. If their rating disagreed with the definition, i.e. if they had found all the elements but still rated the conversation as not gossip, or if one or more elements were lacking but the conversation was considered gossip anyway, they were asked to specify why.  5.3 Results We analyzed the results from the 19 subjects who completed ratings for all 21 excerpts. This gave a total of 399 yes/no judgments on 4 at-tributes. Inter-coder reliability 12  is shown in Table 2.  The easiest attribute to interpret is third person focus. All but three of the subjects marked either 4 or 5 excerpts as not having third person focus, with the remaining subjects not deviating by much (marking 3, 6, and 7 excerpts). Moreover, the subjects agree on which excerpts have third person focus: only one excerpt gets a substantial number of con-flicting ratings (see the analysis given below in section 5.4), while the remaining 20 excerpts get consistent ratings from all subjects with only occasional deviation by one or two of the deviant subjects. This accounts for the high observed agreement on this feature (94.9%). Expected agreement is high because the corpus is not balanced (16 of 21 excerpts display third person focus), but even so, chance-corrected agreement is high (85.1%), showing that third person focus is an attribute that participants can readily and reliably identify. The remaining attributes, including gossip, are less clear. Agreement on all of them is clearly above chance, but is not particularly high, showing that these notions are either not fully defined, or that the excerpts are ambigu-ous. Gossip itself is identified somewhat more reliably than either substantiating behavior or pejorative evaluation; this casts doubt about the ability to use the latter two as defining features                                                  12 We used Krippendorff's alpha with the nominal dis-tance metric. Observed agreement is defined as Ao = 1 ? Do, while expected agreement is: Ae = 1 ? De. 
of gossip, given that they are more difficult to identify.   Alpha Observed agreement Expected agreement Gossip 0.466 0.744 0.520 Third person focus 0.851 0.949 0.661 Substantiat-ing behavior 0.376 0.709 0.533 Pejorative evaluation 0.384 0.733 0.567  Table 2: Inter-coder reliability.  To test the relationship between the various features, we looked for co-occurrences among the individual judgments. We have a total of 399 ratings (21 excerpts times 19 judges), each with 4 attributes; these are distributed as shown in Table 313. We can see that third person focus is an almost necessary condition for classifying a screenplay conversation as gossip, though it is by no means sufficient. Tables 4?6 show the co-occurrences of individual features to gossip; the association is strongest between gossip and third person focus and weakest between gossip and pejorative evaluation.    3rd person 3rd person    Subst Subst Subst Subst Pejor 168 24  2 Gossip Pejor 33 14   Pejor 25 20 17 17 Gossip Pejor 6 23 3 47  Table 3: Relationship between the different elements and gossip.   3rd person 3rd person Gossip 239 2 Gossip 74 84  Table 4: Gossip ? third person focus.   Substantiating behavior Substantiating behavior Gossip 201 40 Gossip 51 107  Table 5: Gossip ? substantiating behavior                                                   13 Strike-through marks the absence of a feature. 
197
 Pejorative Pejorative Gossip 194 47 Gossip 79 79  Table 6: Gossip ? pejorative evaluation   In addition to the co-occurrences of features on the individual judgments, we can look at these co-occurrences grouped by screenplay. Table 7 shows, for each of the 21 excerpts, how many subjects identified each of the four fea-tures (the table is sorted by the gossip score). It is apparent from the table that all the features are correlated to some extent.   ID14 Gossip Third person Subst.  behavior Pejorative evaluation   2   0   0   1   3 11   0   0   9   9 19   0   1   6   8 14   1   0   2 12   5   7 19   5   1 15   7 19 18 17 21   8 17   6 16 12   9 17 10 14 20 13 13 10 10 16 14 18 14   7   8 14 19   7 19   7 14 19   9   9 17 14 19 17 18 18 15 19 19 19   4 17 19 12   9 10 17 19 16 19   6 17 19 19 19   9 18 19 17   8   1 18 19 19 19   3 19 19 18 18 13 19 19 18 19  Table 7: Co-occurrences grouped by excerpts.  Table 8 shows the correlation between gos-sip and each of the other three features. The first column calculates correlation based on the individual judgments (399 items, each score is either 0 or 1); the second column calculates correlation based on the rated excerpts (21 items, each score is an integer between 0 and 19, as in table 7); and the third column groups the judgments by subject (19 items, each score is an integer between 0 and 21, indicating the number of dialogues in which the subject iden-tified the particular feature; the full data are not shown).                                                  14 Presentation was ordered by ID, same for all subjects. 
Pearson?s r Correlation with gossip Individual Excerpt Subject Third person 0.622*** 0.849*** 0.503* Substantiating 0.518*** 0.765*** 0.625** Pejorative 0.321*** 0.518* 0.459* * p < 0.05 ** p < 0.01 *** p < 0.001  Table 8: Correlation between gossip and each of the three features.  All the correlations are significantly different from 0 at the p ? 0.05 level or greater. The dif-ferences between the columns are not signifi-cant, except for the difference between the third person correlation by individuals and that by excerpt, which is significant at p ? 0.05. The correlations between the features on the indi-vidual judgments show that subjects tend to identify the different features together; this may be partly a reflection of awareness on their part that the features are expected to go together, given the task definition. The correlations be-tween the excerpt scores show that the excerpts themselves differ along the four dimensions, and these differences go hand in hand. Finally, we see that the subjects themselves differ in how often they identify the different features, though the correlations are likely to be just a reflection of the first tendency identified above, to mark the features together. 5.4 Discussion We wanted to find out whether the subjects would accept, understand and be able to apply a given definition. The results from the experi-ment showed that the subjects accepted the given definition to some extent and managed to apply it. When the subjects disagreed they were asked to say why. One of the subjects, for ex-ample, explicitly disagreed with the definition given in the introduction and provided a counter definition: ?Gossip is idle talk or ru-mor, especially about the personal or private affairs of others?. Yet another subject was un-certain about which definition to use: ?Depends what you mean by gossip. It can either mean malicious, behind the back talk of other people or idle chat.  If you mean ?idle chat? with gos-sip then this is also gossip?. A possible expla-nation could be that the subjects refer to differ-ent forms of gossip (see e.g. Gilmore, 1978) and therefore apply different definitions (such as the lexical definition presented earlier) than the one that was given in the experiment.  
198
Several subjects stated that they judged the conversation as gossip even if they did not identify any pejorative evaluation, and they also questioned whether the evaluation had to be negative or even present at all, or as one of the subjects put it: ?Although there is no pejo-rative evaluation (at least not clearly) I believe this is gossip?. These subjects thus explicitly reject Eggins and Slade?s (1997) requirement that the evaluation has to be pejorative.  The examples above show that people have variable intuitions of gossip and consequently the concept of gossip is somewhat vague. Even so, the experiment also showed that people to a large degree are in agreement when the exam-ples according to the given definition clearly are gossip or not gossip. Meaning that even though the definition does not capture all types of (potential) gossip conversations, it captures those episodes that most people agree to be gossip, which for our purpose is sufficient.  5.5 Effect of interpersonal relations In some particular cases, the subjects did not choose gossip even if all elements had been found. The results from the first experiment indicated that this deviation either was related to the interpersonal relationship between the gossip participants or that the focus was am-biguous. In order to test whether changing the inter-personal relationship between the partici-pants would change the gossip rating, we com-pared the results from the conversations we had modified with their original counterparts. In one of the original excerpts, the addressee was romantically involved with the man that the speaker was talking about. The speaker formu-lated the negative assessment and deviant be-havior in a way that for most people would be interpreted as a warning, which probably ex-plains why only 7 of the 19 subjects rated the original conversation as gossip. The modified version on the other hand, was rated as gossip by all subjects.   In the second dialogue, the speaker questions the addressee?s choice of person to date, and does this by both evaluating the person nega-tively as well as providing evidence for the evaluation. It turns out, however, that the ad-dressee thinks she is going out for a date with someone else, so a large part of the conversa-tion deals with trying to identify the target. 15 of 19 subjects rated the original conversation as gossip, while all subjects rated it as gossip in the modified version. These comparisons indi-
cate that the status of the relationship between the gossipers and the gossip target affects whether the dialogue is considered gossip or not. In the original version of both these exam-ples, the focus was ambiguous, i.e. the focus was as much on the addressee as on the absent third person. We have shown that third person focus is a key element of gossip. The correlation was fur-thermore confirmed by the subjects themselves in their comments, where the lack of third per-son often was listed as a reason for not choos-ing gossip. In one example, the respondent re-garded the conversation as gossip even if it really was an insult directed towards the ad-dressee, but explained it as its ??almost like he?s forgotten he?s talking to the person he?s giving this opinion/gossip about?. The highest disagreement concerning third person focus was found in the following ex-cerpt15: Karen: Okay, what is it? Gretchen: Regina says everyone hates you because you?re such a slut. Karen: She said that? Gretchen: You didn?t hear it from me. The dialogue contains an ambiguous focus in that it both includes a quote as well as a con-frontational insult. By using the third person reference, Gretchen avoids taking responsibility for the insult. In some sense both Karen and Regina are in focus, where Karen is the target of the pejorative evaluation and Regina can be interpreted as being the focus of the substantiat-ing behavior component. How Regina?s role is interpreted is determined by the respondents? personal attitude towards gossiping in general (i.e. whether they interpret Gretchen?s utterance as containing an implicit evaluation of Regina?s behavior or not), and how they perceive the interpersonal relationship between Karen and Gretchen. Gossip has an inherent contradiction in that it both has a function of negotiating the accepted way to behave while it at the same time often is considered an inappropriate activ-ity that can have serious negative consequences for both the gossipers as well as the gossip tar-get (see e.g. Gilmore, 1978; Bergmann, 1993; Eggins and Slade, 1997; Hallett et al, 2009).                                                   15 From Mean Girls, Paramount Pictures, 2004. 
199
6 Final remarks and future work The aim of these studies has been to get a workable definition of gossip that people can agree upon and that is sufficiently precise to provide a basis for computational implementa-tion.  We conducted two experiments to investi-gate people?s intuitive notion of gossip and the results show that (1) conversations in which all elements are present, where no intimate rela-tionships exist between the participants, and in which the person in focus is unambiguous, are very likely to be considered gossip. (2) Conver-sations that have at most one gossip element are not considered gossip. (3) Inconsistencies are mainly found in conversations that lack one or two elements or have at least one element that is ambiguous, or are taking place between gossipers that have an intimate relationship.  We have suggested that third person focus is a necessary, but not sufficient, element of gos-sip, but the other elements are less clear even if their co-occurrence in a conversation clearly affects the gossip score.  In the second experi-ment this might be due to the instructions, but it does not explain the unbiased results from the first experiment. So on the one hand we can clearly see that all three elements are important for the understanding of gossip, but on the other hand, the subjects? had trouble in identi-fying them. This suggests that we need to fur-ther investigate these elements to see how they can be specified more clearly.  We have taken a first step toward a computa-tional account of gossip, by empirically verify-ing the extent to which the given definition can be applied and the components recognized by people. Some of our next steps to further this program include authoring content for believ-able characters that follow this definition, as well as attempting to automatically recognize these elements. Among the possible applications of gossip we can think of game characters and virtual humans that are capable of engaging in gossip conversations to share information and create social bonds with a human user or its avatar. This involves being able to both generate gos-sip on basis of the interpersonal relationship and selecting content that could be regarded as gossip, as well as to automatically detect gossip occurring in a conversation. The latter use could also be used for characters that actively 
want to avoid taking part in gossip conversa-tions.  References  Roger D. Abrahams. 1970. A Performance-Centred Approach to Gossip. Man, New Series, Vol. 5, No. 2 (Jun.), pp. 290-301. Joseph Bates. (1994. The Role of Emotion in Belie-vable agents. Communications of the ACM, Vol. 37, No. 7 (Jul.), pp. 122-125. J?rg R. Bergmann. 1993. Discreet Indiscretions: The Social Organization of Gossip. New York: Aldine. Suzanne Eggins and Diana Slade (1997) Analysing Casual Conversation. Equinox Publishing Ltd.  Donna Eder and Janet Lynne Enke (1991) The Structure of Gossip: Opportunities and Con-straints on Collective Expression among Adoles-cents. American sociological Review, Vol. 56, No. 4 (Aug.), pp. 494?508. Gary Alan Fine and Ralph L. Rosnow. 1978. Gos-sip, Gossipers, Gossiping. Personality and So-cial Psychology Bulletin, Vol. 4, No. 1, pp 161-168. David Gilmore. 1978. Varieties of Gossip in a Spanish Rural Community. Ethnology, Vol. 17, No. 1 (Jan.), pp. 89-99. Max Gluckman. 1963. Papers in Honor of Melville J. Herskovits: Gossip and Scandal. Current An-thropology, Vol. 4, No. 3 (Jun), pp. 307?316.  Marjorie Harness Goodwin. 1980. He-Said-She-Said: Formal Cultural Procedures for the Con-struction of a Gossip Dispute Activity. American Ethnologist, Vol. 7, No. 4 (Nov.), pp. 674-695.  Tim Hallett., Brent Harget and Donna Eder (2009) Gossip at Work: Unsanctioned Evaluative Talk in Formal School Meetings. Journal of Contem-porary Ethnography, Vol. 38, No. 5, pp 584?618. Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology, chapter 12. Sage Beverly Hills, CA. Robert Paine.1967. What is gossip about? An alter-native hypothesis. Man, New Series, Vol. 2, No. 2 (Jun.), pp. 278-285. John F. Szwed. 1966. Gossip, Drinking, and Social Control: Consensus and Communication in a Newfoundland Parish. Ethnology, Vol. 5, No. 4 (Oct.), pp. 434-441.   
200
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 272?278,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
An Annotation Scheme for Cross-Cultural Argumentation
and Persuasion Dialogues
Kallirroi Georgila?, Ron Artstein?, Angela Nazarian?
Michael Rushforth??, David Traum?, Katia Sycara?
?Institute for Creative Technologies, University of Southern California
?Robotics Institute, Carnegie Mellon University
kgeorgila@ict.usc.edu
Abstract
We present a novel annotation scheme for
cross-cultural argumentation and persuasion
dialogues. This scheme is an adaptation of
existing coding schemes on negotiation, fol-
lowing a review of literature on cross-cultural
differences in negotiation styles. The scheme
has been refined through application to cod-
ing both two-party and multi-party negotia-
tion dialogues in three different domains, and
is general enough to be applicable to differ-
ent domains with few if any extensions. Di-
alogues annotated with the scheme have been
used to successfully learn culture-specific di-
alogue policies for argumentation and persua-
sion.
1 Introduction
In both cooperative and non-cooperative negotiation
the nature of the arguments used can be crucial for
the outcome of the negotiation. Argumentation and
persuasion are basic elements of negotiation. More-
over, different cultures favor different types of argu-
ments (Koch, 1983; Han and Shavitt, 1994; Zaharna,
1995; Brett and Gelfand, 2006). For example, it is
claimed that Western individualistic cultures favor
arguments based on logic over arguments that appeal
to emotions. On the other hand, people from East-
ern collectivistic cultures are more likely to use ar-
guments in which the beneficiary is not themselves.
Furthermore, Arab cultures tend to favor more indi-
rect ways of argumentation and expression (Koch,
1983; Zaharna, 1995).
?Now at the University of Texas at San Antonio.
In order to analyze negotiation in detail, including
aspects such as persuasion, negotiation, and cross-
cultural differences, we have developed a novel
annotation scheme. General purpose annotation
schemes such as DAMSL (Core and Allen, 1997)
and DIT++ (Bunt, 2006) represent moves in the dia-
logue but do not capture enough details of the inter-
action to distinguish between different styles of per-
suasion and argumentation, especially cross-cultural
differences.
Our goal for developing this coding scheme is
two-fold. First, we aim to fill the gap in the litera-
ture of cross-cultural argumentation and persuasion.
To our knowledge this is the first annotation scheme
designed specifically for coding cross-cultural argu-
mentation and persuasion strategies. Previous work
on cross-cultural negotiation, e.g. Brett and Gelfand
(2006), has not focused on argumentation or per-
suasion in particular. Also, previous work on argu-
mentation, e.g. Prakken (2008), has not attempted to
capture cross-cultural differences in argumentation
and persuasion strategies. Second, we use this cod-
ing scheme to annotate negotiation dialogues to au-
tomatically learn argumentation and persuasion di-
alogue policies for different cultures (Georgila and
Traum, 2011).
2 Related Work
2.1 Non-Culture Related Argumentation and
Persuasion
The topic of negotiation has widely been studied
across various fields including social and behavioral
science (Kern et al, 2005), and computer science
(Sidner, 1994; Rose? and Torrey, 2004). Our spe-
cific focus is on the role of argumentation and per-
272
suasion. Sycara (1990) studied the role of argumen-
tation in negotiation with regard to the role of ar-
guments in changing the decision process of the in-
terlocutor. Most attempts have focused on study-
ing the structure of argumentation and persuasion,
often using formal logic (Cohen, 1987; Prakken,
2008). Dung (1995) showed that argumentation can
be viewed as a special form of logic programming
with negation as failure. An argumentation scheme
is defined as a structure or template for forming an
argument. Schemes are necessary for identifying
arguments, finding missing premises, analyzing ar-
guments, and evaluating arguments (Pollock, 1995;
Katzav and Reed, 2004; Walton et al, 2008).
Recently, there has been some work on using ma-
chine learning techniques for automatically inter-
preting (George et al, 2007) and generating argu-
ments (Zukerman, 2001). Note also the work of Pi-
wek (2008) who performed a study on how argu-
ments can be presented as fictive dialogues. Finally,
there are a few persuasive dialogue systems, e.g.
Daphne (Grasso et al, 2000) and BIAS (Bayesian In-
teractive Argumentation System) (Zukerman, 2001).
2.2 Cross-Cultural Argumentation and
Persuasion
There is a vast amount of research on cultural ef-
fects on negotiation. Brett and Gelfand (2006) iden-
tify three aspects in cross-cultural negotiation: indi-
vidualism vs. collectivism, egalitarianism vs. hierar-
chy, and low context vs. high context communica-
tion. Typically Western individuals are individualis-
tic, egalitarian, and use low context communication
while Eastern individuals are collectivistic, hierar-
chical, and use high context communication.1
Although there has been a considerable amount of
work on building agents that can negotiate (Traum
et al, 2003; Rose? and Torrey, 2004), little has been
done towards building agents that can take into ac-
count culture aspects of negotiation (Cassell, 2009;
Paruchuri et al, 2009; Traum, 2009).
Our literature review on cross-cultural argumen-
tation and persuasion showed that there are com-
paratively few papers related to cross-cultural argu-
mentation and persuasion in dialogue. Most work
on cross-cultural studies is based on survey experi-
1In high-context cultures the listener must understand the
contextual cues in order to grasp the full meaning of the mes-
sage. In low-context cultures communication tends to be spe-
cific, explicit, and analytical.
ments rather than dialogue analysis. Below we sum-
marize the works that we were influenced by the
most.
Peng and Nisbett (1999) studied the way Chinese
vs. European-American people reason about con-
tradiction. By contradiction, here, we mean op-
posing pieces of information. Chinese individuals
adopt a dialectical or compromise approach by re-
taining basic elements of the opposing perspectives.
European-American people select one of the per-
spectives as correct and dismiss the opposing ones.
Koch (1983) linguistically analyzed several per-
suasive texts in contemporary Arabic in which there
was both repetition of form and repetition of con-
tent. She found that Arabs use repetition as a means
for persuasion. This strategy is called ?presentation
as proof? or ?argumentation by presentation?. Thus
in Arabic argumentation it is the presentation of an
idea that is persuasive, not the logical structure of
proof which Westerners see behind the words. Za-
harna (1995) examined how the Arab and American
cultures have two distinct perspectives for viewing
the role of language, for structuring persuasive mes-
sages, and for communicating effectively with their
audiences. For Arabs emphasis is on form over func-
tion, affect over accuracy, and image over meaning,
which is in line with the work of Koch (1983).
Finally, Cialdini?s work (1998) identified six prin-
ciples of persuasion: reciprocation (tendency to re-
turn favors), scarcity (associated with high value),
authority (tendency to follow authority figures), so-
cial proof (one is looking to the behavior of other in-
dividuals to determine her own actions), liking (one
tends to do things for people that she likes), and
commitment and consistency (one has difficulty to
reverse her commitments).
3 Our Annotation Scheme
We have developed a novel scheme for coding cross-
cultural argumentation and persuasion strategies.
This scheme is based on the literature review pre-
sented in section 2.2, as well as our own analysis of
three very different kinds of negotiation (section 4).
To develop this annotation scheme, we started by
adapting existing coding schemes on negotiation de-
veloped by Pruitt and Lewis (1975), Carnevale et al
(1981), and Sidner (1994). We were also influenced
by the work of Prakken on argumentation and di-
alogue (2008), and the work of Cialdini (1998) on
persuasion (see section 2.2). Our annotation scheme
273
was further refined by iteratively applying it to three
different negotiation domains.
In our coding scheme, we use three dimensions
for annotating an utterance: speech act, topic, and
response or reference to a previous utterance. We
have divided our codes for speech acts in categories.
Below we can see each category and the codes that
are included in it with explanatory examples, mostly
drawn from the florist-grocer dialogues described in
section 4.1.
3.1 Topic Tracking
start topic Let?s talk about the design.
end topic We are done with the design.
redirect topic We need to get back to the task.
3.2 Information Exchange
This category includes providing and requesting in-
formation, broken down into three kinds of informa-
tion that are about the negotiation (priority, value,
preference) as well as a fourth category (fact) which
can be further subdivided, depending on the issue
being negotiated (e.g. for the toy domain in sec-
tion 4.3, there are specializations for origin, func-
tion, and utility of the toy).
request info.priority Which issue is the most impor-
tant to you?
request info.value How much money will I get if I
give you this?
request info.preference What do you think about
the blue color?
request info.fact What will happen to the flowers if
the temperature gets higher?
provide info.priority I care most about tempera-
ture.
provide info.value You get $50 more if you agree to
lower the temperature by one degree.
provide info.preference I like design A.
provide info.fact (just a simple fact, neither prefer-
ence nor priority nor value) So one of them will
be yours and one mine.
3.3 Information Comparison
note similarities We both need the temperature to
be relatively low.
note differences It seems that you want design A
and I prefer design C.
project othersposition So you want an equal distri-
bution of rent.
3.4 Clarifications/Confirmations
request clarification I am not getting any more
money with more customers coming in?
provide clarification Not necessarily.
request confirmation Did you say 68 degrees?
self clarification (when the speaker tries to expand
on her ideas) Because when I thought temper-
ature, I was thinking temperature for the prod-
ucts, not temperature for the atmosphere.
3.5 Offer
We use the following format for an offer:
offer.?type?.?beneficiary?.?directness?. For a ?re-
quest offer?, generally only the directness field is
used.
Type can take the following values: ?standard?,
?tradeoff?, ?compromise?, ?concession?, and ?re-
traction?. The difference between ?compromise?
and ?concession? is subtle. ?Concession? means
that ?I don?t really want to do this but I?ll do it be-
cause there is no other way?. ?Compromise? is like
splitting the difference and it does not imply that the
speaker does not like the option.
Beneficiary can be ?me?, ?you?, ?both?, ?else?,
or ?null?. By beneficiary we mean who the offer or
argument would be good for (see also section 3.7).
So for example, if one?s argument is ?it will be too
cold for the customers? then ?beneficiary=else?.
Directness can be ?direct? or ?indirect?. An of-
fer or argument is ?indirect? when it needs to be in-
ferred. For example, when the grocer says ?well let?s
say there are lots of other local florists competing for
your prices?, she means that this is why advertising
is important, but this needs some kind of inference,
so the argument is indirect.
Below we can see examples of various types of
offers (the beneficiary and directness dimensions are
omitted for brevity).
offer.standard How about 62 degrees?
offer.tradeoff (between different issues) I?ll agree
on 64 degrees if you agree on design A.
offer.compromise Well should we just say 50/50?
offer.concession There is no other way so I agree
on 64 degrees.
offer.retraction I changed my mind, I don?t want de-
sign A.
request offer What temperature do you suggest?
274
3.6 General Reaction
accept Okay, 62 degrees is fine. or Yes, I said 62
degrees.
reject 62 degrees is too low for me. or No, I didn?t
say that.
acknowledge I see.
Note that ?accept? is used for accepting offers and
confirmation requests but also for agreement, for ex-
ample, when one interlocutor agrees with the argu-
ment of the other interlocutor. ?Reject? is used for
rejecting offers and confirmation requests but also
for disagreement.
3.7 Argumentation
An argument follows the following format:
?role?.?type?.?beneficiary?.?directness?. The role
can be ?provide argument?, ?attack argument?,
?rebut argument?, ?undercut argument?, and ?ac-
cept defeat?. Beneficiary and directness are defined
as in section 3.5. Below we can see examples of dif-
ferent argument roles.
provide argument The temperature must be low for
my flowers to stay fresh.
attack argument (without necessarily providing a
counter-argument) What you say does not make
sense.
rebut argument (provide a counter-argument) Yes,
but my customers wouldn?t want to shop in such
a low temperature.
undercut argument (invalidate an argument) You
don?t need a low temperature in the shop. Your
flowers can be refrigerated to stay fresh.
accept defeat You are right, I could use a refriger-
ator.
We have identified the following argument types:
ideology (what is ?right?), logic, fairness, prece-
dent, God?s will, promise for the future, honor, duty,
identity, authority, refer to relationship, appeal to
feelings, social responsibility, assurance (abstract
promises), stories/metaphors, ordinance, design
(aesthetics and functionality), effect/consequence,
cost/means. These types are mostly inspired by our
literature review (see section 2.2), as well as our ob-
servations in the domains that we used for develop-
ing the annotation scheme.
An example logical argument is ?my flowers need
low temperatures to stay fresh?. An example argu-
ment that appeals to fairness is ?I helped you last
time so it?s fair to help me now?. Arguments that
appeal to logic are more likely to appear in indi-
vidualistic cultures. Arguments that appeal to duty,
honor, social responsibility, ideology, and fairness
are more common in collectivistic cultures. Sto-
ries/metaphors are very common in Arab cultures
(Koch, 1983; Zaharna, 1995).
3.8 Other Speech Acts
repetition I prefer design A. I said design A.
heavy commitment $50 is all I can give, not a cent
more.
weak commitment Let?s assume that we agree on
this and continue.
meta task discussion (try to figure out the task) You
are the grocer and I am the florist.
self contradiction Speaker A: I like design C.
Speaker A (later): Design C is terrible.
show concern I understand that this solution would
not be good for you.
putdown You are stubborn.
show frustration I?m really sick and tired of this.
threat If you don?t accept my offer I won?t do busi-
ness with you again.
miscellaneous Yes, flowers are beautiful.
4 Applications of the Annotation Scheme
on Various Corpora
In order to prove its generality we applied this cod-
ing scheme to three different negotiation domains.
4.1 Florist-Grocer Domain
The first domain was dialogues between American
undergraduates playing the role of a florist and a gro-
cer who share a retail space. The dialogues were
collected by Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern at Northwestern University. The
florist and the grocer negotiate on four issues: the
design of the space, the temperature, the rent, and
their advertising policy. Using the above coding
scheme we annotated 21 dialogues. Example anno-
tations of speech acts are given in Figure 1, as well
as the examples in section 3, above.
The final scheme was the result of several cy-
cles of dialogue annotations and revisions of the
coding manual. We used the florist-grocer annota-
tions to measure inter-annotator reliability between
four annotators. In three cycles of annotation, we
275
measured agreement on speech acts only and com-
plex speech acts were unified, for example, all the
?provide argument? are treated as a single category.
Krippendorff?s ? (Krippendorff, 1980) rose from
0.375 to 0.463 to 0.565.2
After analyzing these results we noticed that the
main problems in terms of inter-annotator relia-
bility were the confusion between ?accept? and
?acknowledge? (e.g. the utterance ?yeah? could
be either, depending on the context), and the
confusion between ?provide argument.logic?, ?pro-
vide argument.effect?, and ?provide info?. So we
revised the manual as follows: in order for some-
thing to be annotated as ?accept? vs. ?acknowledge?
we need to look forward in the dialogue; if an ar-
gument?s type is both ?logic? and ?effect? then ?ef-
fect? supersedes; ?provide info? is just provision of
a piece of information with no argumentative role.
4.2 SASO Domain
In this second domain (Traum et al, 2008), we an-
notated role-play dialogues in English between a US
Army captain and a Spanish doctor in Iraq. We have
annotated five dialogues so far. An example is given
in Figure 2.
4.3 Toy-Naming Domain
Finally, in the third domain groups of four people
negotiate in English, Spanish, and Arabic about how
to name a toy. The dialogues were part of the UTEP-
ICT Cross-Cultural dialogue corpus (Herrera et al,
2010). We have annotated five dialogues in English
and three in Arabic so far, and are currently work-
ing on Spanish. An example is given in Figure 3.
The ?redirect topic? act was added based on this do-
main (to cover cases where one person consciously
redirects the group?s attention to the task when they
drift off-topic for an extended period of time). Also,
we added three domain-specific specializations of
?provide info.fact? and ?request info.fact?: ?pro-
vide info.fact.function? (discussion about what one
can do with the toy or things that it does or has, e.g.
a secret compartment); ?provide info.fact.origin?
(where the toy was manufactured or bought); ?re-
quest info.fact.utility? (a person prompts the others
for ideas or examples of how the toy could be used
and marketed).
2Krippendorff?s ? is 0.460 in the first cycle if we exclude
one of the annotators who annotated only 72% of the items.
5 Discussion
We believe that this annotation scheme can be used
for analyzing and modeling the fine differences of
argumentation and negotiation styles, cross-task,
and cross-culture, as well as providing a basis for
artificial agents to engage in differentiated negotia-
tion behavior.
Our first use of the annotated florist-grocer di-
alogues was for learning dialogue policies using
simulated users and Reinforcement Learning (RL)
(Georgila and Traum, 2011). To facilitate RL we
had to make a few simplifications, for example, fo-
cus only on the temperature issue. In particular, we
built policies for individualistic vs. altruistic florists
(and grocers). Our results in simulation were consis-
tent with our reward functions, i.e. the florist individ-
ualist agreed on low temperatures while interacting
with the grocer altruist, the florist altruist agreed on
high temperatures vs. the grocer individualist, etc.
Details are given in (Georgila and Traum, 2011).
6 Conclusion
We presented a novel annotation scheme for cross-
cultural argumentation and persuasion dialogues.
This scheme is based on a review of literature on
cross-cultural argumentation and persuasion, and
adaptation of existing coding schemes on negotia-
tion. Our annotation scheme is also based on our ob-
servations from its application to coding both two-
party and multi-party negotiation dialogues in three
different domains, and is general enough to be ap-
plicable to different domains with minor or no mod-
ifications at all. Furthermore, dialogues annotated
with the scheme have been used to successfully learn
culture-specific dialogue policies for argumentation
and persuasion.
Acknowledgments
This research was funded by a MURI award through
ARO grant number W911NF-08-1-0301. We are
grateful to Laurie R. Weingart, Jeanne M. Brett,
and Mary C. Kern who provided us with the florist-
grocer dialogues, and to David A. Herrera, David G.
Novick, and Dusan Jan who developed the UTEP-
ICT corpus, as well as Hussein Sadek for transcrip-
tions and translations of the Arabic dialogues.
276
References
J.M. Brett and M.J. Gelfand. 2006. A cultural analysis of
the underlying assumptions of negotiation theory. In
Frontiers of Negotiation Research, L. Thompson (Ed),
pages 173?201. Psychology Press.
H. Bunt. 2006. Dimensions in dialogue act annotation.
In Proc. of LREC.
P.J. Carnevale, D.G. Pruitt, and S.D. Seilheimer. 1981.
Looking and competing: Accountability and visual ac-
cess in integrative bargaining. Journal of Personality
and Social Psychology, 40(1):111?120.
J. Cassell. 2009. Culture as social culture: Being en-
culturated in human-computer interaction. In Proc. of
HCI International.
R.B. Cialdini. 1998. Influence: The psychology of per-
suasion, Revised. Collins.
R. Cohen. 1987. Analyzing the structure of argumen-
tative discourse. Computational Linguistics, 13(1-
2):11?24.
M.G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In Proc. of the AAAI Fall
Symposium on Communicative Actions in Humans and
Machines.
P.M. Dung. 1995. On the acceptability of arguments and
its fundamental role in nonmonotonic reasoning, logic
programming and n-person games. Artificial Intelli-
gence, 77(2):321?357.
S. George, I. Zukerman, and M. Niemann. 2007. In-
ferences, suppositions and explanatory extensions in
argument interpretation. User Modeling and User-
Adapted Interaction, 17(5):439?474.
K. Georgila and D. Traum. 2011. Learning culture-
specific dialogue models from non culture-specific
data. In Proc. of HCI International.
F. Grasso, A. Cawsey, and R. Jones. 2000. Dialectical ar-
gumentation to solve conflicts in advice giving: A case
study in the promotion of healthy nutrition. Interna-
tional Journal of Human-Computer Studies, 53:1077?
1115.
S. Han and S. Shavitt. 1994. Persuasion and culture:
Advertising appeals in individualistic and collectivistic
societies. Journal of Experimental Social Psychology,
30:326?350.
D. Herrera, D. Novick, D. Jan, and D. Traum. 2010. The
UTEP-ICT cross-cultural multiparty multimodal dia-
log corpus. In Proc. of the LREC Multimodal Corpora
Workshop: Advances in Capturing, Coding and Ana-
lyzing Multimodality (MMC).
J. Katzav and C. Reed. 2004. On argumentation schemes
and the natural classification of arguments. Argumen-
tation, 18(2):239?259.
M.C. Kern, J.M. Brett, and L.R. Weingart. 2005. Get-
ting the floor: Motive-consistent strategy and individ-
ual outcomes in multi-party negotiations. Group De-
cision and Negotiation, 14:21?41.
B. Johnstone Koch. 1983. Presentation as proof: The
language of Arabic rhetoric. Anthropological Linguis-
tics, 25(1):47?60.
K. Krippendorff. 1980. Content analysis: An introduc-
tion to its methodology, chapter 12. Sage, Beverly
Hills, CA.
P. Paruchuri, N. Chakraborty, R. Zivan, K. Sycara,
M. Dudik, and G. Gordon. 2009. POMDP based ne-
gotiation modeling. In Proc. of the IJCAI Workshop on
Modeling Intercultural Collaboration and Negotiation
(MICON).
K. Peng and R.E. Nisbett. 1999. Culture, dialectics,
and reasoning about contradiction. American Psychol-
ogist, 54(9):741?754.
P. Piwek. 2008. Presenting arguments as fictive dia-
logue. In Proc. of the ECAI Workshop on Computa-
tional Models of Natural Argument (CMNA).
J.L. Pollock. 1995. Cognitive Carpentry: A blueprint for
how to build a person. Bradford Books, MIT Press.
H. Prakken. 2008. A formal model of adjudication dia-
logues. Artificial Intelligence and Law, 16:305?328.
D.G. Pruitt and S.A. Lewis. 1975. Development of in-
tegrative solutions in bilateral negotiation. Journal of
Personality and Social Psychology, 31(4):621?633.
C. Rose? and C. Torrey. 2004. DReSDeN: Towards a
trainable tutorial dialogue manager to support negoti-
ation dialogues for learning and reflection. In Proc. of
ITS.
C.L. Sidner. 1994. An artificial discourse language for
collaborative negotiation. In Proc. of the National
Conference on Artificial Intelligence.
K. Sycara. 1990. Persuasive argumentation in negotia-
tion. Theory and Decision, 28(3):203?242.
D. Traum, J. Rickel, S. Marsella, and J. Gratch. 2003.
Negotiation over tasks in hybrid human-agent teams
for simulation-based training. In Proc. of AAMAS.
D. Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.
2008. Multi-party, multi-issue, multi-strategy negotia-
tion for multi-modal virtual agents. In Proc. of IVA.
D. Traum. 2009. Models of culture for virtual human
conversation. In Proc. of HCI International.
D. Walton, C. Reed, and F. Macagno. 2008. Argumenta-
tion Schemes. Cambridge University Press.
R.S. Zaharna. 1995. Understanding cultural preferences
of Arab communication partners. Public Relations Re-
view, 21(3):241?255.
I. Zukerman. 2001. An integrated approach for generat-
ing arguments and understanding rejoinders. In Proc.
of the International Conference on User Modeling.
277
Appendix
Florist: How does that work for you? (request info.preference)
Grocer: Well, personally for the grocery I think it is better to have a higher temperature. (pro-
vide argument.logic.me.indirect)
Grocer: Just because I want the customers to feel comfortable. (elaborate)
Florist: Okay. (acknowledge)
Grocer: And also if it is warm, people are more apt to buy cold drinks to keep themselves comfortable and
cool. (elaborate)
Florist: That?s true. (accept)
Florist: But what about your products staying fresh? Don?t they have to stay fresh or otherwise? (re-
but argument.logic.you.direct)
Figure 1: Example annotated dialogue with speech acts in the florist-grocer domain.
Captain: I think if you just made the compromise, we could provide so much for you if you just agreed to
let us move the clinic. (offer.standard.you.direct)
Doctor: Look I need to get back to my patients. They?re dying now. They?re dying. (show frustration)
Captain: They wouldn?t be dying if you let us move the clinic to the US Army base with the additional
medical support. (provide argument.logic.else.direct)
Doctor: Well they wouldn?t be dying if I was there. (rebut argument.logic.else.direct)
Doctor: Why don?t you provide us with additional medical support and get out of our lives? (re-
quest offer.direct)
Figure 2: Example annotated dialogue with speech acts in the SASO domain.
Speaker 3: Blue pal. (offer.standard.null.direct)
Speaker 4: Blue pal. (acknowledge)
Speaker 2: Blue pal. (acknowledge)
Speaker 4: That sounds pretty good. I actually like the idea. (accept)
Speaker 1: What if it?s a different color? (provide argument.logic.null.direct)
Speaker 2: Yeah, what if it?s like pink and purple. . . (elaborate)
Speaker 4: Uh I like blue pal. I think that one?s pretty cool. . . (provide info.preference)
Speaker 2: Something pal like your pal. (offer.standard.null.direct)
Speaker 4: Blue pal the singing singing pal the singing pal the singing and dancing buddy. The beast you
don?t want to get angry. (offer.standard.null.direct)
Speaker 2: That?s too long. (reject)
Speaker 2: It has to be short. (provide argument.logic.null.direct)
Speaker 1: Furball. (offer.standard.null.direct)
Speaker 4: A short name... Actually a good really long name might work because everything out there is
short... (rebut argument.logic.null.direct)
Figure 3: Example annotated dialogue with speech acts in the toy-naming domain.
278
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 319?324,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Error Return Plots
Ron Artstein
Institute for Creative Technologies, University of Southern California
12015 Waterfront Drive, Playa Vista, CA 90094-2536, USA
<lastname>@ict.usc.edu
Abstract
Error-return plots show the rate of error
(misunderstanding) against the rate of non-
return (non-understanding) for Natural Lan-
guage Processing systems. They are a use-
ful visual tool for judging system performance
when other measures such as recall/precision
and detection-error tradeoff are less informa-
tive, specifically when a system is judged on
the correctness of its responses, but may elect
to not return a response.
1 Introduction
Many Natural Language Processing systems make
a distinction between misunderstanding, where the
system interprets an input incorrectly, and non-
understanding, where the system is aware that it is
not able to interpret an input (Bohus and Rudnicky,
2005). This distinction is common in dialogue sys-
tems, where it pertains to Natural Language Under-
standing components which pass their output to a
dialogue manager: a dialogue manager will act on
the contents of misunderstood input, but if it knows
that the input is not understood then it can engage in
a variety of recovery techniques, such as asking for
clarification, moving on, or changing the topic. For
this reason non-understanding is usually preferred to
misunderstanding. While common to dialogue sys-
tems, the concept of non-understanding is useful for
other tasks as well, whenever a system can bene-
fit from the knowledge that its best interpretation is
likely to be incorrect (see below for an example in
question answering).
Detecting non-understanding is a tradeoff: a sys-
tem that is prone to non-understanding will in-
evitably miss some inputs that it would have under-
stood correctly under a forced interpretation. This
is similar but not identical to the familiar trade-
offs between recall and precision (van Rijsbergen,
1979) and between detection and error (Martin et al,
1997). Recall and precision are measures taken from
information retrieval, where there are typically mul-
tiple documents relevant to a query, and ideal per-
formance is defined as retrieving all and only the
relevant documents: recall measures the ?all? part
while precision measures the ?only? part, and tun-
ing a system to increase one measure typically im-
plies decreasing its counterpart. Detection and er-
ror apply to forced choice tasks: each input must be
classified as either positive or negative, and decreas-
ing false positives typically implies increasing false
negatives and vice versa. The tradeoff between mis-
understanding and non-understanding is similar to
recall-precision in that a response need not be given
to each input, and is similar to detection-error in that
when a response is given, we only care about its cor-
rectness and not about its exhaustiveness.
There is presently no accepted measure for
the tradeoff between misunderstanding and non-
understanding. A recent example illustrating the
confusion, and need for a standard measure, comes
from the QALD-1 Open Challenge (Question An-
swering over Linked Data).1 The task is defined
as giving a complete and correct answer to a nat-
ural language question, but systems are allowed to
not return an answer. The evaluation metric uses
recall and precision, but they are defined in a non-
standard way. Precision is defined as the number
1http://www.sc.cit-ec.uni-bielefeld.de/sites/www.sc.cit-ec.
uni-bielefeld.de/files/sharedtask.pdf (dated 2011-03-28)
319
of correctly answered questions divided by the to-
tal number of answered questions; given that each
question receives at most one answer, this is equiv-
alent to the standard definition of correct answers
divided by the total number of answers provided by
the system ? it penalizes misunderstanding and gives
credit to non-understanding. Recall is also defined
in a non-standard way.
number of correctly answered questions
number of questions
This would normally be considered the definition
of accuracy, and it penalizes misunderstanding and
non-understanding equally; the standard definition
of recall is the number of correct answers divided by
the number of available correct answers, and it does
not normally penalize incorrect answers. The reason
for the confusion between recall and accuracy is that
in a task where each question has a unique correct
answer, failure to provide a correct answer to a ques-
tion implies that an available answer has not been
retrieved. What the QALD-1 evaluation does, in
effect, is penalize non-understanding through accu-
racy, and penalize misunderstanding more, through
both accuracy and precision.
To properly evaluate the tradeoff between mis-
understanding and non-understanding we need to
look at each type of error separately. If each in-
put receives a response, then accuracy is the com-
plement of error; if some responses are not re-
turned, then accuracy is the complement of the sum
of errors (misunderstandings) and non-returns (non-
understandings). The relative severity of misunder-
standing and non-understanding will vary based on
the application: a question-answering system that
is required to provide accurate information might
have a low tolerance for misunderstanding, while a
story-driven dialogue system might have a low tol-
erance for asking clarification questions as a result
of non-understanding. The relation between misun-
derstanding and non-understanding is not fixed ? a
system with lower error rates under a forced inter-
pretation may turn out to have higher error rates than
a competitor after allowing for non-understanding.
It is therefore useful to look at the entire range of
return rates when evaluating systems. The remain-
der of this paper introduces the error-return plot as
a graphical representation for comparing error rates
across different return rates, and presents examples
for its use from recent experiments.
2 Characteristics of the tradeoff
A Natural Language Processing component that is
capable of indicating non-understanding consists of
two distinct processes: figuring out the best (or most
likely) response to an input, and deciding whether
the best response is likely to be appropriate. These
two processes may be implemented as distinct soft-
ware components, as in the system used for the
experiments in section 4, NPCEditor (Leuski and
Traum, 2010) ? a classification-based system for
Natural Language Understanding that chooses the
best interpretation from a fixed set. NPCEditor
first calculates the appropriateness of each avail-
able interpretation, and then compares the score of
the best interpretation to a predetermined threshold;
if the best interpretation falls below the threshold,
NPCEditor indicates non-understanding. Other im-
plementations are, of course, possible ? for example,
Patel et al (2006) describe an architecture where the
system first decides if it can understand the input,
and then tries to determine the interpretation only
if the answer is positive. The two processes may
also be linked more intimately together, but in order
to determine the tradeoff between misunderstand-
ing and non-understanding, there must be some way
to isolate the decision of whether or not the input
has been understood. By varying the sensitivity of
this decision, we can compare the rates of misunder-
standing across different rates of non-understanding.
Decomposing Natural Language Understand-
ing into two distinct processes helps illustrate
the inapplicability of the popular measures of
ROC curves (relative operating characteristic,
Swets, 1973) and DET curves (detection error trade-
off, Martin et al, 1997). These measures only look
at the decision of whether an interpretation is good
enough, while abstracting away the decision about
the actual interpretation. ROC and DET curves were
developed for detection and verification tasks, where
performance is determined by the rate of errors ?
misses and false alarms ? irrespective of the com-
position of the input. They plot the false alarm rate
against the hit rate (ROC) or miss rate (DET) ? that
is, the returned errors as a proportion of all errors
320
against the returned (ROC) or missed (DET) correct
responses as a proportion of all correct responses.
Consequently, ROC and DET curves say nothing
about the actual error rate. A system with an er-
ror rate of 10%, where errors are uniformly spread
among correct responses when ranked by the sys-
tem?s confidence, will have identical ROC and DET
curves to a system with an error rate of 40%, 50% or
90% with the errors spread uniformly.
For investigating the tradeoff between misunder-
standing and non-understanding, we want to look
not only at the system?s decision about whether or
not to return an interpretation, but also at the correct-
ness of the chosen interpretation. We therefore need
a plot that reflects the actual error rate as a function
of the return rate.
3 Definition
An error-return plot is a graphical representation of
the tradeoff between errors (misunderstandings) and
failures to return a response (non-understandings).
It applies to systems that react to each input in one
of three possible ways ? a correct response, an in-
correct response, or a failure to respond to the input.
The error rate and non-return rate are defined as fol-
lows.
Error rate =
incorrect responses
number of inputs
Non-return rate =
failures to respond
number of inputs
In order to plot the entire range of the tradeoff, the
system is set to make a forced-choice response to
each input. The responses are then ranked according
to the system?s confidence (or whatever other mea-
sure is used to decide when to issue a non-return),
and at each possible cutoff, the non-return rate is
plotted on the horizontal axis against the error rate
on the vertical axis. As the number of non-returns
grows, the number of errors can only go down, so
the plot is monotonically decreasing; at the extreme
right, where no responses are returned, error rates
are necessarily zero, while at the extreme left, the
error rate is equivalent to accuracy under a forced
choice. Lower curves indicate better performance.
Figure 1: Comparing tokenizers, SGT Star data
(Wang et al, 2011, black = baseline)
4 Examples
An example error-return plot is shown in Figure 1.
The plot is taken from Wang et al (2011), an experi-
ment which tested the effect of using phonetic infor-
mation in a Natural Language Understanding com-
ponent in order to recover from speech recognition
errors. The base system is NPCEditor (Leuski and
Traum, 2010), trained for SGT Star, a virtual charac-
ter who provides information about the U.S. Army to
potential recruits (Artstein et al, 2009). For each in-
put utterance, NPCEditor selects one output out of a
fixed set, based on a learned mapping between input
and output training examples; it also has the capabil-
ity of not returning a response if the classifier?s con-
fidence in the appropriateness of the best choice falls
below a certain threshold. The specific experiment
in Figure 1 tested alternative methods to tokenize the
input: the base tokenizer is represented by the thick
black curve, and uses words as tokens; alternative
tokenizers are shown in thinner lines or in shades of
gray, and they use tokens with various mixtures of
phonetic and word information (phone unigrams, bi-
grams etc.). The test data consisted of utterances for
which the correct interpretation is known, but which
NPCEditor would occasionally fail to classify due to
speech recognition errors.
Figure 1 shows several properties at a glance. The
base tokenizer has a fairly high error rate (over 30%)
321
Figure 2: Comparing tokenizers, Twins data
(Wang et al, 2011, black = baseline)
under forced choice, but the error rate decreases
rapidly when non-understanding is allowed (on the
left-hand side of the plot the slope is close to ?1,
which is the steepest possible decline). When tol-
erance for non-understanding is low, all the alter-
native tokenizers produce lower error rates than the
baseline; however, increasing the non-understanding
does not affect all tokenizers equally, and the er-
ror rate of the baseline tokenizer improves more
rapidly than others, so that at 30% non-return rate it
is better than most of the alternative tokenizers. Fi-
nally, one alternative tokenizer ? the thin black line ?
shows best or almost-best performance at all return
rates, supporting the hypothesis of the original ex-
periment, that adding phonetic information to a Nat-
ural Language Understanding component can help
in recovery from speech recognition errors.
Figure 2 is from the same experiment but using
a different data set ? the one developed for the the
twins Ada and Grace, two virtual guides at the Mu-
seum of Science in Boston who answer questions
about their neighboring exhibits and about science
in general (Swartout et al, 2010). The overall error
rate is much lower than in Figure 1. Otherwise, the
pattern is similar, though we see that the thin gray to-
kenizer has shifted from a close second-best to being
the worst performer. Once again, the thin black tok-
enizer beats all the others across most return rates.
Figure 3: Augmented classifiers (black = baseline)
Figure 3 shows a different experiment, also using
NPCEditor. This experiment tested the effect of tak-
ing an existing virtual character ? the twins Ada and
Grace ? and expanding the character?s understand-
ing by adding training input-output pairs extracted
automatically from text (the method for extracting
training data is described in Chen et al, 2011; the
present experiment is currently under review for
publication). The baseline classifier is the thick
black line, trained on the Twins? original question-
answer links; the alternative classifiers add automat-
ically extracted questions-answer training links from
successive orthogonal domains. All classifiers were
evaluated using the same test set of questions from
the original domain, in order to test how the addition
of orthogonal training data affects performance on
inputs from the original domain. The plot shows that
the effect is quite noticeable: the original classifier
has a 10% absolute error rate, which drops to virtu-
ally zero at a non-return rate of 20% and above; the
augmented classifiers display a higher initial error
rate, and moreover this higher error rate is not easily
mitigated by accepting higher non-return rates. The
augmented classifiers have the advantage of being
able to understand inputs from the added domains,
but the cost is some confusion on the original do-
main, both in terms of understanding the input, and
in the ability to identify non-understanding.
322
5 Discussion
The error-return plot is a graphical representation
for looking at the tradeoff between misunderstand-
ing and non-understanding. Evaluating systems ca-
pable of indicating non-understanding is somewhat
tricky, and error-return plots can show information
that is useful when comparing such systems. If
the curve of one system completely dominates the
other, then we can say with confidence that the first
system has better performance. If the curves in-
tersect, then we need to compare the parts of the
curve where we expect actual system performance
to fall, and this will vary by application. The sys-
tems described above all use the same strategy for
dealing with non-understanding: they issue an ?off-
topic? response which asks for clarification, stalls,
or changes the conversation topic. The systems are
intended for fairly short question-answer dialogues,
for which an off-topic response rate of about 1 in 5
is usually acceptable, so the critical region is around
20% non-understanding. In applications where it is
possible to judge the relative severity of misunder-
standing and non-understanding, a weighted aver-
age could identify the optimal setting for the non-
understanding threshold. Such an average should
give non-understanding a lower weight than misun-
derstanding, since treating them as equal would ob-
viate the need for identifying non-understanding.
A counterpart to the error rate would be the
?missed chance rate? ? the proportion of responses
that would have been correct under forced choice
but were not returned. Curves for missed chances
start at zero (when all responses are returned) and in-
crease with the non-return rate to a maximum of one
minus the absolute error rate. The relation between
the missed chance curve and the error return plot
is straightforward: wherever the error return curve
goes down, the missed chance curve stays level,
and wherever the error return plot stays level, the
missed chance curve goes up. The curves intersect
at the point where the number of misunderstandings
is identical to the number of non-understandings that
would have been correct under forced choice; it is
not clear, however, whether this point has any prac-
tical significance.
Error-return plots suffer from the usual problem
of evaluating single components in a dialogue sys-
tem: since subsequent input is to a certain extent
contingent on system actions, it is conceivable that
a system prone to misunderstanding would trigger
different user utterances than a system prone to non-
understanding. Determining the full consequences
of non-understanding would require running a full
dialogue system with real users under varying set-
tings; error-return plots show the performance of
Natural Language Understanding under the assump-
tion of fixed input.
Overall, error return plots provide useful in-
formation about the tradeoff between misunder-
standing and non-understanding in cases where re-
call/precision, ROC and DET curves are less infor-
mative. They have been used in several recent ex-
periments, and hopefully may gain acceptance as a
standard tool for system evaluation.
Acknowledgments
The project or effort described here has been spon-
sored by the U.S. Army Research, Development,
and Engineering Command (RDECOM). State-
ments and opinions expressed do not necessarily re-
flect the position or the policy of the United States
Government, and no official endorsement should be
inferred.
References
Ron Artstein, Sudeep Gandhe, Jillian Gerten, Anton
Leuski, and David Traum. 2009. Semi-formal evalu-
ation of conversational characters. In Orna Grumberg,
Michael Kaminski, Shmuel Katz, and Shuly Wintner,
editors, Languages: From Formal to Natural. Essays
Dedicated to Nissim Francez on the Occasion of His
65th Birthday, volume 5533 of LNCS, pages 22?35.
Springer, May.
Dan Bohus and Alexander I. Rudnicky. 2005. Sorry,
I didn?t catch that! ? An investigation of non-under-
standing errors and recovery strategies. In Proceed-
ings of the 6th SIGdial Workshop on Discourse and
Dialogue, pages 128?143, Lisbon, Portugal, Septem-
ber.
Grace Chen, Emma Tosch, Ron Artstein, Anton Leuski,
and David Traum. 2011. Evaluating conversa-
tional characters created through question generation.
In Proceedings of the Twenty-Fourth International
Florida Artificial Intelligence Research Society Con-
ference, pages 343?344, Palm Beach, Florida, May.
323
Anton Leuski and David Traum. 2010. Practical lan-
guage processing for virtual humans. In Proceedings
of the Twenty-Second Innovative Applications of Arti-
ficial Intelligence Conference (IAAI-10), pages 1740?
1747, Atlanta, Georgia, July.
Alvin Martin, George Doddington, Terri Kamm, Mark
Ordowski, and Mark Przybocki. 1997. The DET
curve in assessment of detection task performance. In
Eurospeech 1997, pages 1895?1898, Rhodes, Greece,
September.
Ronakkumar Patel, Anton Leuski, and David Traum.
2006. Dealing with out of domain questions in vir-
tual characters. In Jonathan Gratch, Michael Young,
Ruth Aylett, Daniel Ballin, and Patrick Olivier, editors,
Intelligent Virtual Agents: 6th International Confer-
ence, IVA 2006, Marina Del Rey, CA, USA, August 21?
23, 2006 Proceedings, volume 4133 of Lecture Notes
in Artificial Intelligence, pages 121?131, Heidelberg,
August. Springer.
William Swartout, David Traum, Ron Artstein, Dan
Noren, et al 2010. Ada and Grace: Toward realis-
tic and engaging virtual museum guides. In Jan All-
beck, Norman Badler, Timothy Bickmore, and Alla
Pelachaud, Catherine Safonova, editors, Intelligent
Virtual Agents, volume 6356 of LNAI, pages 286?300.
Springer, September.
John A. Swets. 1973. The relative operating characteris-
tic in psychology. Science, 182(4116):990?1000.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London, 2nd edition.
William Yang Wang, Ron Artstein, Anton Leuski, and
David Traum. 2011. Improving spoken dialogue un-
derstanding using phonetic mixture models. In Pro-
ceedings of the Twenty-Fourth International Florida
Artificial Intelligence Research Society Conference,
pages 329?334, Palm Beach, Florida, May.
324
Proceedings of the SIGDIAL 2013 Conference, pages 193?202,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Verbal indicators of psychological distress in interactive dialogue with a
virtual human
David DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,
Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe Morency
University of Southern California, Institute for Creative Technologies
Playa Vista, CA
devault@ict.usc.edu
Abstract
We explore the presence of indicators of
psychological distress in the linguistic be-
havior of subjects in a corpus of semi-
structured virtual human interviews. At
the level of aggregate dialogue-level fea-
tures, we identify several significant dif-
ferences between subjects with depres-
sion and PTSD when compared to non-
distressed subjects. At a more fine-grained
level, we show that significant differences
can also be found among features that
represent subject behavior during specific
moments in the dialogues. Finally, we
present statistical classification results that
suggest the potential for automatic assess-
ment of psychological distress in individ-
ual interactions with a virtual human dia-
logue system.
1 Introduction
One of the first steps toward dealing with psy-
chological disorders such as depression and PTSD
is diagnosing the problem. However, there is of-
ten a shortage of trained health care professionals,
or of access to those professionals, especially for
certain segments of the population such as mili-
tary personnel and veterans (Johnson et al, 2007).
One possible partial remedy is to use virtual hu-
man characters to do a preliminary triage screen-
ing, so that mental healthcare providers can focus
their attention on those who are most likely to need
help. The virtual human would engage an indi-
vidual in an interview and analyze some of their
behavioral characteristics. In addition to serving
a triage function, this automated interview could
produce valuable information to help the health-
care provider make their expert diagnosis.
In this paper, we investigate whether features
in the linguistic behavior of participants in a con-
versation with a virtual human could be used
for recognizing psychological distress. We focus
specifically on indicators of depression and post-
traumatic stress disorder (PTSD) in the verbal be-
havior of participants in a Wizard-of-Oz corpus.
The results and analysis presented here are part
of a broader effort to create an automated, interac-
tive virtual human dialogue system that can detect
indicators of psychological distress in the multi-
modal communicative behavior of its users. Re-
alizing this vision requires a careful and strate-
gic design of the virtual human?s dialogue behav-
ior, and in concert with the system?s behavior, the
identification of robust ?indicator? features in the
verbal and nonverbal responses of human intervie-
wees. These indicators should be specific behavior
patterns that are empirically correlated with spe-
cific psychological disorders, and that can inform
a triage screening process or facilitate the diagno-
sis or treatment performed by a clinician.
In this paper, we report on several kinds of such
indicators we have observed in a corpus of 43
Wizard-of-Oz interactions collected with our pro-
totype virtual human, Ellie, pictured in Figure 1.
We begin in Section 2 with a brief discussion of
background and related work on the communica-
tive behavior associated with psychological dis-
tress. In Section 3, we describe our Wizard-of-Oz
data set. Section 4 presents an analysis of indicator
features we have explored in this data set, identi-
fying several significant differences between sub-
jects with depression and PTSD when compared
to non-distressed subjects. In Section 5 we present
statistical classification results that suggest the po-
tential for automatic assessment of psychological
distress based on individual interactions with a vir-
tual human dialogue system. We conclude in Sec-
tion 6.
2 Background and Related Work
There has been a range of psychological and clin-
ical research that has identified differences in the
193
Figure 1: Ellie.
communicative behavior of patients with specific
psychological disorders such as depression. In this
section, we briefly summarize some closely re-
lated work.
Most work has observed the behavior of patients
in human-human interactions, such as clinical in-
terviews and doctor-patient interactions. PTSD is
generally less well studied than depression.
Examples of the kinds of differences that have
been observed in non-verbal behavior include dif-
ferences in rates of mutual gaze and other gaze
patterns, downward angling of the head, mouth
movements, frowns, amount of gesturing, fidget-
ing, emotional expressivity, and voice quality; see
Scherer et al (2013) for a recent review.
In terms of verbal behavior, our exploration of
features here is guided by several previous obser-
vations in the literature. Cohn and colleagues have
identified increased speaker-switch durations and
decreased variability of vocal fundamental fre-
quency as indicators of depression, and have ex-
plored the use of these features for classification
(Cohn et al, 2009). That work studied these fea-
tures in human-human clinical interviews, rather
than in virtual human interactions as reported here.
In clinical studies, acute depression has been as-
sociated with decreased speech, slow speech, de-
lays in delivery, and long silent pauses (Hall et al,
1995). Aggregate differences in lexical frequen-
cies have also been observed. For example, in
written essays, Rude et al (2004) observed that
depressed participants used more negatively va-
lenced words and used the first-person pronoun ?I?
more frequently than never-depressed individuals.
Heeman et al (2010) observed differences in chil-
dren with autism in how long they pause before
speaking and in their use of fillers, acknowledg-
ments, and discourse markers. Some of these fea-
tures are similar to those studied here, but looked
at children communicating with clinicians rather
than a virtual human dialogue system.
Recent work on machine classification has
demonstrated the ability to discriminate between
schizophrenic patients and healthy controls based
on transcriptions of spoken narratives (Hong et al,
2012), and to predict patient adherence to med-
ical treatment from word-level features of dia-
logue transcripts (Howes et al, 2012). Automatic
speech recognition and word alignment has also
been shown to give good results in scoring narra-
tive recall tests for identification of cognitive im-
pairment (Prud?hommeaux and Roark, 2011; Lehr
et al, 2012).
3 Data Set
In this section, we introduce the Wizard-of-Oz
data set that forms the basis for this paper. In
this virtual human dialogue system, the charac-
ter Ellie depicted in Figure 1 carries out a semi-
structured interview with a single user. The sys-
tem was designed after a careful analysis of a
set of face-to-face interviews in the same do-
main. The face-to-face interviews make up the
large human-human Distress Assessment Inter-
view Corpus (DAIC) that is described in Scherer
et al (2013). Drawing on observations of inter-
viewer behavior in the face-to-face dialogues, El-
lie was designed to serve as an interviewer who
is also a good listener, providing empathetic re-
sponses, backchannels, and continuation prompts
to elicit more extended replies to specific ques-
tions. The data set used in this paper is the re-
sult of a set of 43 Wizard-of-Oz interactions where
the virtual human interacts verbally and nonver-
bally in a semi-structured manner with a partici-
pant. Excerpts from the transcripts of two interac-
tions in this Wizard-of-Oz data set are provided in
the appendix in Figure 5.1
3.1 Procedure
The participants were recruited via Craigslist and
were recorded at the USC Institute for Creative
1A sample demonstration video of an interaction be-
tween the virtual agent and a human actor can be seen here:
http://www.youtube.com/watch?v=ejczMs6b1Q4
194
Technologies. In total 64 participants interacted
with the virtual human. All participants who met
requirements (i.e. age greater than 18, and ad-
equate eyesight) were accepted. In this paper,
we focus on a subset of 43 of these participants
who were told that they would be interacting with
an automated system. (The other participants,
which we exclude from our analysis, were aware
that they were interacting with a human-controlled
system.) The mean age of the 43 participants in
our data set was 36.6 years, with 23 males and 20
females.
We adhered to the following procedure for data
collection: After a short explanation of the study
and giving consent, participants completed a series
of questionnaires. These questionnaires included
the PTSD Checklist-Civilian version (PCL-C) and
the Patient Health Questionnaire, depression mod-
ule (PHQ-9) (Scherer et al, 2013) along with other
questions. Then participants engage in an inter-
view with the virtual human, Ellie. After the di-
alogue concludes, participants are then debriefed
(i.e. the wizard control is revealed), paid $25 to
$35, and escorted out.
The interaction between the participants and El-
lie was designed as follows: Ellie explains the pur-
pose of the interaction and that she will ask a series
of questions. She then tries to build rapport with
the participant in the beginning of the interaction
with a series of casual questions about Los Ange-
les. Then the main interview begins, including a
range of questions such as:
What would you say are some of your
best qualities?
What are some things that usually put
you in a good mood?
Do you have disturbing thoughts?
What are some things that make you re-
ally mad?
How old were you when you enlisted?
What did you study at school?
Ellie?s behavior was controlled by two human
?wizards? in a separate room, who used a graph-
ical user interface to select Ellie?s nonverbal be-
havior (e.g. head nods, smiles, back-channels)
and verbal utterances (including the interview
questions, verbal back-channels, and empathy re-
sponses). This Wizard-of-Oz setup allows us to
prove the utility of the protocol and collect training
data for the eventual fully automatic interaction.
The speech for each question was pre-recorded us-
ing an amateur voice actress (who was also one of
the wizards). The virtual human?s performance of
these utterances is animated using the SmartBody
animation system (Thiebaux et al, 2008).
3.2 Condition Assessment
The PHQ-9 and PCL-C scales provide researchers
with guidelines on how to assess the participants?
conditions based on the responses. Among the
43 participants, 13 scored above 10 on the PHQ-
9, which corresponds to moderate depression and
above (Kroenke et al, 2001). We consider these
13 participants as positive for depression in this
study. 20 participants scored positive for PTSD,
following the PCL-C classification. The two pos-
itive conditions overlap strongly, as the evalu-
ated measurements PHQ-9 and PCL-C correlate
strongly (Pearson?s r > 0.8, as reported in Scherer
et al (2013)).
4 Feature Analysis
4.1 Transcription and timing of speech
We have a set D = {d1, ..., d43} of 43 dialogues.
The user utterances in each dialogue were tran-
scribed using ELAN (Wittenburg et al, 2006),
with start and end timestamps for each utterance.2
At each pause of 300ms or longer in the user?s
speech, a new transcription segment was started.
The resulting speech segments were subsequently
reviewed and corrected for accuracy.
For each dialogue di ? D, this process resulted
in a sequence of user speech segments. We repre-
sent each segment as a tuple ?s, e, t?, where s and e
are the starting and ending timestamps in seconds,
and t is the manual text transcription of the corre-
sponding audio segment. The system speech seg-
ments, including their starting and ending times-
tamps and verbatim transcripts of system utter-
ances, were recovered from the system log files.
To explore aggregate statistical features based
on user turn-taking behavior in the dialogues, we
employ a simple approach to identifying turns
within the dialogues. First, all user and system
speech segments are sorted in increasing order of
2ELAN is a tool that supports annotation of
video and audio, from the Max Planck Insti-
tute for Psycholinguistics, The Language Archive,
Nijmegen, The Netherlands. It is available at
http://tla.mpi.nl/tools/tla-tools/elan/.
195
Segment level features
(a) mean speaking rate of each user segment
(b) mean onset time of first segment in each user turn
(c) mean onset time of non-first segments in user turns
(d) mean length of user segments
(e) mean minimum valence in user segments
(f) mean mean valence in user segments
(g) mean maximum valence in user segments
(h) mean number of filled pauses in user segments
(i) mean filled pause rate in user segments
Dialogue level features
(j) total number of user segments
(k) total length of all user segments
Figure 2: List of context-independent features.
their starting timestamps. All consecutive seg-
ments with the same speaker are then designated
as constituting a single turn. While this simple
scheme does not provide a detailed treatment of
relevant phenomena such as overlapping speech,
backchannels, and the interactive process of ne-
gotiating the turn in dialogue (Yang and Heeman,
2010), it provides a conceptually simple model for
the definition of features for aggregate statistical
analysis.
4.2 Context-independent feature analysis
We begin by analyzing a set of shallow features
which we describe as context-independent, as they
apply to user speech segments independently of
what the system has recently said. Most of these
are features that apply to many or all user speech
segments. We describe our context-independent
features in Section 4.2.1, and present our results
for these features in Section 4.2.2.
4.2.1 Context-independent features
We summarize our context-independent features
in Figure 2.
Speaking rate and onset times Based on previ-
ous clinical observations related to slowed speech
and increased onset time for depressed individuals
(Section 2), we defined features for speaking rate
and onset time of user speech segments.
We quantify the speaking rate of a user speech
segment ?s, e, t?, where t = ?w1, ..., wN ?, as
N/(e ? s). Feature (a) is the mean value of
this feature across all user speech segments within
each dialogue.
Onset time is calculated using the notion of user
turns. For each user turn, we extracted the first
user speech segment in the turn fu = ?su, eu, tu?,
and the most recent system speech segment ls =
?ss, es, ts?. We define the onset time of such a first
user segment as su ? es, and for each dialogue,
feature (b) is the intra-dialogue mean of these on-
set times.
In order to also quantify pause length between
user speech segments within a turn, we define fea-
ture (c), a similar feature that measures the mean
onset time between non-first user speech segments
within a user turn in relation to the preceding user
speech segment.
Length of user segments As one way to quan-
tify the amount of speech, feature (d) reports the
mean length of all user speech segments within a
dialogue (measured in words).
Valence features for user speech Features (e)-
(g) are meant to explore the idea that distressed
users might use more negative or less positive vo-
cabulary than non-distressed subjects. As an ex-
ploratory approach to this topic, we used Senti-
WordNet 3.0 (Baccianella and Sebastiani, 2010),
a lexical sentiment dictionary, to assign valence
to individual words spoken by users in our study.
The dictionary contains approximately 117,000
entries. In general, each word w may appear in
multiple entries, corresponding to different parts
of speech and word senses. To assign a single va-
lence score v(w) to each word in the dictionary, in
our features we compute the average score across
all parts of speech and word senses:
v(w) =
?
e?E(w) PosScoree(w)?NegScoree(w)
|E(w)|
where E(w) is the set of entries for the word w,
PosScoree(w) is the positive score for w in entry
e, and NegScoree(w) is the negative score for w
in entry e. This is similar to the ?averaging across
senses? method described in Taboada et al (2011).
We use several different measures of the va-
lence of each speech segment with transcript t =
?w1, ..., wn?. We compute the min, mean, and max
valence of each transcript:
minimum valence of t = minwi?t v(wi)
mean valence of t = 1n
?
wi?t v(wi)
maximum valence of t = maxwi?t v(wi)
Features (e)-(f) then are intra-dialogue mean
196
values for these three segment-level valence mea-
sures.
Filled pauses Another feature that we explored
is the presence of filled pauses in user speech seg-
ments. To do so, we counted the number of times
any of the tokens uh, um, uhh, umm, mm, or mmm
appeared in each speech segment. For each dia-
logue, feature (h) is the mean number of these to-
kens per user speech segment. In order to account
for the varying length of speech segments, we also
normalize the raw token counts in each segment
by dividing them by the length of the segment, to
produce a filled pause rate for the segment. Fea-
ture (i) is the mean value of the filled pause rate
for all speech segments in the dialogue.
Dialogue level features We also included two
dialogue level measures of how ?talkative? the
user is. Feature (j) is the total number of user
speech segments throughout the dialogue. Feature
(k) is the total length (in words) of all speech seg-
ments throughout the dialogue.
Standard deviation features For the classifica-
tion experiments reported in Section 5, we also in-
cluded a standard deviation variant of each of the
features (a)-(i) in Figure 2. These variants are de-
fined as the intra-dialogue standard deviation of
the underlying value, rather than the mean. We
discuss examples of standard deviation features
further in Section 5.
4.2.2 Results for context-independent
features
We summarize the observed significant effects in
our Wizard-of-Oz corpus in Table 1.
Onset time We report our findings for individu-
als with and without depression and PTSD for fea-
ture (b) in Table 1 and in Figure 3. The units are
seconds. While an increase in onset time for in-
dividuals with depression has previously been ob-
served in human-human interaction (Cohn et al,
2009; Hall et al, 1995), here we show that this
effect transfers to interactions between individuals
with depression and virtual humans. We find that
mean onset time is significantly increased for indi-
viduals with depression in their interactions with
our virtual human Ellie (p = 0.018, Wilcoxon
rank sum test).
Additionally, while to our knowledge onset time
for individuals with PTSD has not been reported,
we also found a significant increase in onset time
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
No depr.
??
Depr.
?
Me
an
on
se
td
ela
yo
ffi
rst
pa
rtic
ipa
nt
se
gm
en
t(s
ec
on
ds
)
0
1
2
3
4
?PTSD
?
PTSD
?
Figure 3: Onset time.
for individuals with PTSD (p = 0.019, Wilcoxon
rank sum test).
Filled pauses We report our findings for individ-
uals with and without depression and PTSD under
feature (h) in Table 1 and in Figure 4. We observed
a significant reduction in this feature for both in-
dividuals with depression (p = 0.012, Wilcoxon
rank sum test) and PTSD (p = 0.014, Wilcoxon
rank sum test). We believe this may be related
to the trend we observed toward shorter speech
segments from distressed individuals (though this
trend did not reach significance). There is a pos-
itive correlation, ? = 0.55 (p = 0.0001), be-
tween mean segment length (d) and mean number
of filled pauses in segments (h).
Other features We did not observe significant
differences in the values of the other context-
independent features in Figure 2.
4.3 Context-dependent features
Our data set alows us to zoom in and look at
specific contextual moments in the dialogues, and
observe how users respond to specific Ellie ques-
tions. As an example, one of Ellie?s utterances,
which has system ID happy lasttime, is:
happy lasttime = Tell me about the last
time you felt really happy.
In our data set of 43 dialogues, this question was
asked in 42 dialogues, including 12 users positive
for depression and 19 users positive for PTSD.
197
Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)
(b) mean onset time of first
segment in each user turn
?
Depr.: 1.72 (0.89)
No Depr.: 1.08 (0.56)
p = 0.018
?
PTSD.: 1.56 (0.80)
No PTSD.: 1.03 (0.57)
p = 0.019
(h) mean number of filled pauses
in user segments
?
Depr.: 0.32 (0.19)
No Depr.: 0.48 (0.23)
p = 0.012
?
PTSD: 0.36 (0.24)
No PTSD: 0.49 (0.21)
p = 0.014
Table 1: Results for context-independent features. For each feature and condition, we provide the mean
(standard deviation) for individuals with and without the condition. P-values for individual Wilcoxon
rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased feature values
for positive individuals. A down arrow (?) indicates a significant trend toward decreased feature values
for positive individuals.
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
No depr.
?
Depr.
?
Me
an
fille
dp
au
se
si
np
art
icip
an
ts
eg
me
nt
(to
ke
ns
)
0
0.2
0.4
0.6
0.8
1.0
1.2
?PTSD PTSD
?
Figure 4: Number of filled pauses per speech seg-
ment.
This question is one of 95 topic setting utter-
ances in Ellie?s repertoire. (Ellie has additional
utterances that serve as continuation prompts,
backchannels, and empathy responses, which can
be used as a topic is discussed.)
To define context-dependent features, we asso-
ciate with each user segment the most recent of
Ellie?s topic setting utterances that has occurred in
the dialogue. We then focus our analysis on those
user segments and turns that follow specific topic
setting utterances. In Table 2, we present some ex-
amples of our findings for context-dependent fea-
tures for happy lasttime.3
3While we provide significance test results here at the p <
0.05 level, it should be noted that because of the large number
of context-dependent features that may be defined in a small
corpus such as ours, we report these merely as observations in
our corpus. We do not claim that these results transfer beyond
The contextual feature labeled (g?) in Table 2 is
the mean of the maximum valence feature across
all segments for which happy lasttime is the most
recent topic setting system utterance. We provide
a full example of this feature calculation in Fig-
ure 5 in the appendix.
As the figure shows, we find that users with
both PTSD and depression show a sharp reduc-
tion in the mean maximum valence in their speech
segments that respond to this question. This sug-
gests that in these virtual human interactions, this
question plays a useful role in eliciting differen-
tial responses from subjects with these psycholog-
ical disorders. We observed three additional ques-
tions which showed a significant difference in the
mean maximum valence feature. One example is
the question, How would your best friend describe
you?.
With feature (b?) in Table 2, we find an in-
creased onset time in responses to this question for
subjects with depression.4 Feature (d?) shows that
subjects with PTSD exhibit shorter speech seg-
ments in their responses to this question.
We observed a range of findings of this sort for
various combinations of Ellie?s topic setting utter-
ances and specific context-dependent features. In
future work, we would like to study the optimal
combinations of context-dependent questions that
yield the most information about the user?s distress
status.
this data set.
4In comparing Table 2 with Table 1, this question seems
to induce a higher mean onset time for distressed users than
the average system utterance does. This does not seem to be
the case for non-distressed users.
198
Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)
(g?) mean maximum valence
in user segments following
happy lasttime
?
Depr.: 0.15 (0.07)
No Depr.: 0.26 (0.12)
p = 0.003
?
PTSD: 0.16 (0.08)
No PTSD: 0.28 (0.11)
p = 0.0003
(b?) mean onset time of first
segments in user turns
following happy lasttime
?
Depr.: 2.64 (2.70)
No Depr.: 0.94 (1.80)
p = 0.030
n.s.
PTSD: 2.18 (2.48)
No PTSD: 0.80 (1.76)
p = 0.077
(d?) mean length of user
segments following
happy lasttime
n.s.
Depr.: 5.95 (1.80)
No Depr.: 10.03 (6.99)
p = 0.077
?
PTSD: 6.82 (5.12)
No PTSD: 10.55 (6.68)
p = 0.012
Table 2: Example results for context-dependent features. For each feature and condition, we provide
the mean (standard deviation) for individuals with and without the condition. P-values for individual
Wilcoxon rank sum tests are provided. An up arrow (?) indicates a significant trend toward increased
feature values for positive individuals. A down arrow (?) indicates a significant trend toward decreased
feature values for positive individuals.
5 Classification Results
In this section, we present some suggestive clas-
sification results for our data set. We construct
three binary classifiers that use the kinds of fea-
tures described in Section 4 to predict the pres-
ence of three conditions: PTSD, depression, and
distress. For the third condition, we define dis-
tress to be present if and only if PTSD, depres-
sion, or both are present. Such a notion of distress
that collapses distinctions between disorders may
be the most appropriate type of classification for a
potential application in which distressed users of
any type are prioritized for access to health care
professionals (who will make a more informed as-
sessment of specific conditions).
For each individual dialogue, each of the three
classifiers emits a single binary label. We train
and evaluate the classifiers in a 10-fold cross-
validation using Weka (Hall et al, 2009).
While our data set of 43 dialogues is perhaps
of a typical size for a study of a research proto-
type dialogue system, it remains very small from
a machine learning perspective. We report here
two kinds of results that help provide perspective
on the prospects for classification of these condi-
tions. The first kind looks at classification based
on all the context-independent features described
in Section 4.2.1. The second looks at classifica-
tion based on individual features from this set.
In the first set of experiments, we trained a
Na??ve Bayes classifier for each condition using
all the context-independent features. We present
our results in Table 3, comparing each classifier to
a baseline that always predicts the majority class
(i.e. no condition for PTSD, no condition for de-
pression, and with condition for distress).
We note first that the trained classifiers all out-
perform the baseline in terms of weighted F-score,
weighted precision, weighted recall, and accuracy.
The accuracy improvement over baseline is sub-
stantial for PTSD (20.9% absolute improvement)
and distress (23.2% absolute improvement). The
accuracy improvement over baseline is more mod-
est for depression (2.3% absolute). We believe
one factor in the relative difficulty of classifying
depression more accurately is the relatively small
number of depressed participants in our study
(13).
While it has been shown in prior work (Cohn et
al., 2009) that depression can be classified above
baseline performance using features observed in
clinical human-human interactions, here we have
shown that classification above baseline perfor-
mance is possible in interactions between human
participants and a virtual human dialogue system.
Further, we have shown classification results for
PTSD and distress as well as depression.
We tried incorporating context-dependent fea-
tures, and also unigram features, but found that
neither improved performance. We believe our
data set is too small for effective training with
these very large extended feature sets.
199
Disorder Model Weighted F-score Weighted Precision Weighted Recall Accuracy
PTSD Na??ve Bayes 0.738 0.754 0.744 74.4%
Majority Baseline 0.373 0.286 0.535 53.5%
Depression Na??ve Bayes 0.721 0.721 0.721 72.1%
Majority Baseline 0.574 0.487 0.698 69.8%
Distress Na??ve Bayes 0.743 0.750 0.744 74.4%
Majority Baseline 0.347 0.262 0.512 51.2%
Table 3: Classification results.
In our second set of experiments, we sought to
gain understanding of which features were pro-
viding the greatest value to classification perfor-
mance. We therefore retrained Na??ve Bayes classi-
fiers using only one feature at a time. We summa-
rize here some of the highest performing features.
For depression, we found that the feature stan-
dard deviation in onset time of first segment in
each user turn yielded very strong performance
by itself. In our corpus, we observed that de-
pressed individuals show a greater standard devia-
tion in the onset time of their responses to Ellie?s
questions (p = 0.024, Wilcoxon rank sum test).
The value of this feature in classification comple-
ments the clinical finding that depressed individu-
als show greater onset times in their responses to
interview questions (Cohn et al, 2009).
For distress, we found that the feature mean
maximum valence in user segments was the most
valuable. We discussed findings for a context-
dependent version of this feature in Section 4.3.
This finding for distress can be related to previ-
ous observations that individuals with depression
use more negatively valenced words (Rude et al,
2004).
For PTSD, we found that the feature mean num-
ber of filled pauses in user segments was among
the most informative.
Based on our observation of classification per-
formance using individual features, we believe
there remains much room for improvement in fea-
ture selection and training. A larger data set would
enable feature selection approaches that use held
out data, and would likely result in both increased
performance and deeper insights into the most
valuable combination of features for classification.
6 Conclusion
In this paper, we have explored the presence of in-
dicators of psychological distress in the linguistic
behavior of subjects in a corpus of semi-structured
virtual human interviews. In our data set, we
have identified several significant differences be-
tween subjects with depression and PTSD when
compared to non-distressed subjects. Drawing on
these features, we have presented statistical classi-
fication results that suggest the potential for auto-
matic assessment of psychological distress in indi-
vidual interactions with a virtual human dialogue
system.
Acknowledgments
This work is supported by DARPA under con-
tract (W911NF-04-D-0005) and the U.S. Army
Research, Development, and Engineering Com-
mand. The content does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
References
Andrea Esuli Stefano Baccianella and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,
Ying Yang, Minh Hoai Nguyen, Margara Tejera
Padilla, Feng Zhou, and Fernando De la Torre.
2009. Detecting depression from facial actions and
vocal prosody. In Affective Computing and Intelli-
gent Interaction (ACII), September.
Judith A. Hall, Jinni A. Harrigan, and Robert Rosen-
thal. 1995. Nonverbal behavior in clinician-patient
interaction. Applied and Preventive Psychology,
4(1):21 ? 37.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18, November.
Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,
Lois Black, and Jan Van Santen. 2010. Autism and
200
interactional aspects of dialogue. In Proceedings
of the 11th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, pages 249?252.
Association for Computational Linguistics.
Kai Hong, Christian G. Kohler, Mary E. March, Am-
ber A. Parker, and Ani Nenkova. 2012. Lexi-
cal differences in autobiographical narratives from
schizophrenic patients and healthy controls. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 37?
47, Jeju Island, Korea, July. Association for Compu-
tational Linguistics.
Christine Howes, Matthew Purver, Rose McCabe,
Patrick G. T. Healey, and Mary Lavelle. 2012.
Predicting adherence to treatment for schizophrenia
from dialogue transcripts. In Proceedings of the
13th Annual Meeting of the Special Interest Group
on Discourse and Dialogue, pages 79?83, Seoul,
South Korea, July. Association for Computational
Linguistics.
Shannon J Johnson, Michelle D Sherman, Jeanne S
Hoffman, Larry C James, Patti L Johnson, John E
Lochman, Thomas N Magee, David Riggs, Jes-
sica Henderson Daniel, Ronald S Palomares, et al
2007. The psychological needs of US military ser-
vice members and their families: A preliminary re-
port. American Psychological Association Presi-
dential Task Force on Military Deployment Services
for Youth, Families and Service Members.
Kurt Kroenke, Robert L. Spitzer, and Janet B. W.
Williams. 2001. The phq-9. Journal of General
Internal Medicine, 16(9):606?613.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,
and Brian Roark. 2012. Fully automated neuropsy-
chological assessment for detecting mild cognitive
impairment. In Interspeech 2012: 13th Annual Con-
ference of the International Speech Communication
Association, Portland, Oregon, September.
Emily Prud?hommeaux and Brian Roark. 2011. Ex-
traction of narrative recall patterns for neuropsycho-
logical assessment. In Interspeech 2011: 12th An-
nual Conference of the International Speech Com-
munication Association, pages 3021?3024, Flo-
rence, Italy, August.
Stephanie Rude, Eva-Maria Gortner, and James Pen-
nebaker. 2004. Language use of depressed and
depression-vulnerable college students. Cognition
& Emotion, 18(8):1121?1133.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In IEEE Conference on Automatic
Face and Gesture Recognition.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional Linguistics, 37(2):267?307, June.
Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-
shall, and Marcelo Kallmann. 2008. Smartbody:
behavior realization for embodied conversational
agents. In Proceedings of the 7th international
joint conference on Autonomous agents and multi-
agent systems - Volume 1, AAMAS ?08, pages 151?
158, Richland, SC. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Peter Wittenburg, Hennie Brugman, Albert Russel,
Alex Klassmann, and Han Sloetjes. 2006. Elan: a
professional framework for multimodality research.
In Proceedings of LREC 2006, Fifth International
Conference on Language Resources and Evaluation.
Fan Yang and Peter A. Heeman. 2010. Initiative con-
flicts in task-oriented dialogue. Computer Speech &
Language, 24(2):175 ? 189.
201
Appendix A. Wizard-of-Oz Dialogue Excerpts
Example user with PTSD and depression Example non-distressed user
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.014 when i play with my dog
Ellie when was the last time
that happened?
User 0.000 this morning
Ellie aw
Ellie tell me more about that
User 0.312 i don?t know i just
User 0.010 whenever i get
User 0.312 super bored or i just
need some time to
User 0.312 for myself i just play
with the dog
Ellie that?s good
User 0.000 yeah
0.1371 = mean
maximum valence in
user segments following
happy lasttime
Ellie (ideal weekendC) tell
me how you spend your
ideal weekend
max
valence
transcript
Ellie (happy lasttime) tell me
about the last time you
felt really happy
User 0.562 um the last time i felt
really happy was
User 0.000 hm
User 0.000 today
Ellie tell me more about that
User 0.688 uh just from the moment
i woke up it was a
beautiful sunny day
User -0.062 i
User 0.565 went to see some friends
we had a good time
went to school
User 0.565 had some good grades
on some papers um
wrote a good essay
User 0.292 feel pretty accomplished
and
User 0.312 i feel like my day is just
User 0.565 a good day
Ellie that?s so good to hear
0.3487 = mean
maximum valence in
user segments following
happy lasttime
Ellie (BF describe) how
would your best friend
describe you?
Figure 5: Examples of maximum valence feature.
202
Proceedings of the SIGDIAL 2013 Conference, pages 394?403,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Which ASR should I choose for my dialogue system?
Fabrizio Morbini, Kartik Audhkhasi, Kenji Sagae, Ron Artstein,
Dog?an Can, Panayiotis Georgiou, Shri Narayanan, Anton Leuski and David Traum
University of Southern California
Los Angeles, California, USA
{morbini,sagae,artstein,leuski,traum}@ict.usc.edu
{audhkhas,dogancan}@usc.edu {georgiou,shri}@sipi.usc.edu
Abstract
We present an analysis of several pub-
licly available automatic speech recogniz-
ers (ASRs) in terms of their suitability for
use in different types of dialogue systems.
We focus in particular on cloud based
ASRs that recently have become available
to the community. We include features
of ASR systems and desiderata and re-
quirements for different dialogue systems,
taking into account the dialogue genre,
type of user, and other features. We then
present speech recognition results for six
different dialogue systems. The most in-
teresting result is that different ASR sys-
tems perform best on the data sets. We
also show that there is an improvement
over a previous generation of recognizers
on some of these data sets. We also inves-
tigate language understanding (NLU) on
the ASR output, and explore the relation-
ship between ASR and NLU performance.
1 Introduction
Dialogue system developers who are not also
speech recognition experts are in a better posi-
tion than ever before in terms of the ease of in-
tegrating existing speech recognizers in their sys-
tems. While there have been commercial solutions
and toolkits for a number of years, there were a
number of problems in getting these systems to
work. For example, early toolkits relied on spe-
cific machine hardware, software, and firmware
to function properly, often had a difficult instal-
lation process, and moreover often didn?t work
well for complex dialogue domains, or challeng-
ing acoustic environments. Fortunately the situ-
ation has greatly improved in recent years. Now
there are a number of easy to use solutions, in-
cluding open-source systems (like PocketSphinx),
as well as cloud-based approaches.
While this increased choice of quality recogniz-
ers is of great benefit to dialogue system develop-
ers, it also creates a dilemma ? which recognizer
to use? Unfortunately, the answer is not simple ?
it depends on a number of issues, including the
type of dialogue domain, availability and amount
of training data, availability of internet connectiv-
ity for the runtime system, and speed of response
needed. In this paper we assess several freely
available speech recognition engines, and exam-
ine their suitability and performance in several di-
alogue systems. Here we extend the work done in
Yao et al (2010) focusing in particular on cloud
based freely available ASR systems. We include
2 local ASRs for reference, one of which was also
used in the earlier work for easy comparison.
2 Speech Recognizer Features and
Engines
The following are some of the major criteria for
selection of a speech recognizer.
Customization Some of the available speech
recognizers allow the users to tune the recognizer
to the environment it will operate in, by providing
a specialized lexicon, trained language models or
acoustic models. Customization is especially im-
portant for dialogue systems whose input contains
specialized vocabulary (see section 4).
Output options A basic recognizer will output
a string of text, representing its best hypothesis
about the transcription of the speech input. Some
recognizers offer additional outputs which are use-
ful for dialogue systems: ranked n-best hypothe-
ses allow later processing to use context for dis-
ambiguation, and incremental results allow the
system to react while the user is still speaking.
Performance characteristics Dialogue systems
differ in their requirements for response speed; a
394
System Customization Output options Open Source PerformanceN-best Incremental Speed Installation
Pocketsphinx Full Yes Yes Yes realtime Local
Apple No Noa No No network Cloud
Google No Yes Yesb No network Cloud
AT&T Partialc Yes No No network Cloud
Otosense-Kaldi Full Yes No Yesd variablee Local
aSingle output annotated with alternative hypotheses. bOnly for web-delivered applications in a Google Chrome browser.
cCustom language models. dRelease scheduled for Fall 2013. eUser controls trade-off between speed and output quality.
Table 1: Speech recognizer features important for use in dialogue systems
speech recognizer that runs locally can help by
avoiding network latencies.
Output quality Typically, a dialogue system
would want the best recognition accuracy pos-
sible given the constraints. Ultimately, dialogue
systems want the output that would yield the best
performance for Natural Language Understand-
ing and other downstream processes. As a rule,
better speech recognition leads to better language
understanding, though this is not necessarily the
case for specific applications (see section 5).
We evaluated 5 freely available speech recog-
nizers. Their features are summarized in Table 1.
We did not include the MIT WAMI toolkit1 as we
are focused on speech services that can directly
be used by stand alone applications as opposed to
web delivered ones. We did not include commer-
cial recognizers such as Nuance, because licensing
terms can be difficult for research institutions, and
in particular, disallow publishing benchmarks.
Pocketsphinx is a version of the CMU Sphinx
ASR system optimized to run also on embedded
systems (Huggins-Daines et al, 2006). Pocket-
sphinx is fast, runs locally, and requires relatively
modest computational resources. It provides n-
best lists and lattices, and supports incremental
output. It also provides a voice activity detec-
tion functionality for continuous ASR. This ASR
is fully customizable and trainable, but users are
expected to provide language models suitable for
their applications. A few acoustic models are pro-
vided, and can be adapted using the CMUSphinx
tools.2
1http://wami.csail.mit.edu/
2http://cmusphinx.sourceforge.net/wiki/tutorialadapt
Apple Dictation is the OS level feature in both
MacOSX and iOS.3 It is integrated into the text in-
put system pipeline so a user can replace her key-
board with a microphone for entering text in any
application. Dictation is often associated with the
Siri personal assistant feature of iOS. While it is
likely that Dictation and Siri share the same ASR
technology, Dictation only does speech recogni-
tion. Apple states that Dictation learns the charac-
teristics of the user?s voice and adapts to her accent
(Apple Inc, 2012). Dictation requires an internet
connection to send recorded user speech to Ap-
ple?s servers and receive ASR results. Processing
starts as soon as the user starts speaking so the de-
lay of getting the recognition results after the user
finishes speaking is minimal.
To integrate Dictation into a dialogue system,
a system designer needs to include any system de-
fined text input control into her application and use
the control APIs to observe text changes. The user
would need to press a key when starting to speak
and push the key again once she is done speak-
ing. The ASR result is a text string annotated with
alternative interpretations of individual words or
phrases in the text. There is an API for extract-
ing those interpretations from the result. While the
Dictation feature is reasonably fast and easy to in-
tegrate, dialogue system developers have no con-
trol over the ASR process, which must be treated
as a black box. Apple dictation is limited in that
no customization is possible, no partial recogni-
tion results are provided, and there is an unspeci-
fied limit on the number of utterances dictated for
a period of time, which is not a problem for inter-
action between a single user and a dialogue sys-
tem, but may be an issue in dialogue systems that
support multiple concurrent users.
3Dictation was introduced in iOS 5.0 and MacOSX 10.8.
395
Google Speech API provides support for the
HTML 5 speech input feature.4 It is a cloud based
service in which a user submits audio data using
an HTML POST request and receives as reply the
ASR output in the form of an n-best list. The au-
dio data is limited to roughly 10 seconds in length,
longer clips are rejected and return no ASR results.
The user can (1) customize the number of hy-
potheses returned by the ASR, (2) specify which
language the audio file contains and (3) enable a
filter to remove profanities from the output text.
As is the case with Apple Dictation, ASR must be
treated as a black box, and no task customization
is possible for dialogue system developers. Users
cannot specify or provide custom language models
or acoustic models. The service returns only the fi-
nal hypothesis, there is no incremental output.5 In
addition, results for the same inputs may change
unpredictably, since Google may update or other-
wise change its service and models, and models
may be adapted using specific audio data supplied
by users. In our experiments, we observed accu-
racy improvements when submitting the same au-
dio files over repeated trials over two weeks.
AT&T Watson is the ASR engine available
through the AT&T Speech Mashup service.6 It is
a cloud based service that can be accessed through
HTML POST requests, like the Google Speech
API. AT&T Watson is designed to support the de-
mands of online spoken dialogue systems, and can
be customized with data specific to a dialogue sys-
tem. Additionally, in our tests we did not observe
any limitation in the maximum length of the in-
put audio data. However, AT&T does not provide
a default general-purpose language model, and
application-specific models must be built within
the Speech Mashup service using user-provided
text data. The acoustic model must be selected
from a list provided by the AT&T service, and
acoustic models can be further customized within
the Speech Mashup service. The ASR returns an
n-best list of hypotheses but does not provide in-
cremental output.
Otosense-Kaldi Another ASR we employed
was the Kaldi-based OtoSense-Kaldi engine de-
4https://www.google.com/speech-api/v1/recognize
5The demo page shows continuous speech understanding
with incremental results but requires Google Chrome to run
and is specific to web delivered applications:
http://www.google.com/intl/en/chrome/demos/speech.html
6https://service.research.att.com/smm
veloped at SAIL.7 OtoSense-Kaldi8 is an on-line,
multi-threaded architecture based on the Kaldi
toolkit (Povey et al, 2011) that allows for dynam-
ically configurable and distributed ASR.
3 Dialogue Systems, Users, and Data
All spoken dialogue systems are similar in some
respects, in that there is speech by a user (or users)
that needs to be recognized, and this speech is
punctuated by speech from the system. More-
over, the speech is not fully independent, but ut-
terances are connected to other utterances, e.g. an-
swers to questions, or clarifications. There are,
however many ways in which systems can differ,
that have implications for which speech recogniz-
ers are most appropriate. Some of the dimensions
to consider are:
Type of microphone(s) One of the biggest im-
pacts on ASR is the acoustic environment. Will
the audio be clean, coming from a close-talking
head or lapel-mounted microphone, or will it need
to be picked up from a broader directional micro-
phone or microphone array?
Number of speakers/microphones Will there
be one designated microphone per person, or will
speaker identification need to be performed? Will
audio from the system confuse the ASR?
Push to talk or continuous speech Will the
user clearly identify the start and end of speech,
or will the system need to detect speech acousti-
cally?
Type of Users Will there be designated long-
term users, where user-training or system model
adaptation is feasible, or will there be many un-
known users, where training is not feasible? See
also section 3.1 for more on user types.
Genre What kinds of things will people be say-
ing to the system? Is it mostly commands or short
answers to questions, or more open-ended conver-
sation? See section 3.2 for more on genre issues.
Training Data Is within-domain training data
available, and if so how much?
3.1 Types of Users
The type of user is important for the overall
design of the system and has implications for
7http://sail.usc.edu
8OtoSense-Kaldi will be released (BSD license) in 2013.
396
ASR performance as well. One important as-
pect is the broad physical differences among
speakers, such as male vs female, adult vs child
(e.g. Bell and Gustafson, 2003), or language pro-
ficiency/accent, that will have implications for the
acoustics of what is said, and ASR results. Other
aspects of users have implications for what will
be said, and how successful the interface may
be, overall. Many (e.g. Hassel and Hagen, 2006;
Jokinen and Kanto, 2004) have looked at the dif-
ferences between novice and expert users. Ai et
al. (2007a) also points out a difference between
real users and recruited subjects. Real users also
come in many different flavors, depending on their
purposes. E.g. are they interacting with the system
for fun, to do a specific task that they need to get
done, to learn something (specific or general), or
with some other purpose in mind?
We considered the following classes of users,
ordered from easiest to hardest to get to acceptable
performance and robustness levels:
Demonstrators are generally the easiest for a sys-
tem to understand ? a demonstrator is trained in
use of the system, knows what can and can?t be
said, is motivated toward success, and is gener-
ally interested in showing off the most impres-
sive/successful aspects of the system to an audi-
ence rather than using it for its own sake.
Trained/Expert Users are similar to demonstra-
tors, but use the system to achieve specific results
rather than just to show off its capabilities. This
means that users may be forced down lines that
are not ideal for the system, if these are necessary
to accomplish the task.
Motivated Users do not have the training of ex-
pert users, and may say many things that the sys-
tem can not handle as opposed to equivalent ex-
pressions that could be handled. However moti-
vated users do want the system to succeed, and in
general are willing to do whatever they think is
necessary to improve system performance. Unlike
expert users, motivated users might be incorrect
about what will help the system (e.g. hyperarticu-
lation in response to system misunderstanding).
Casual Users are interested in finding out what
the system can do, but do not have particular moti-
vations to help or hinder the system. Casual Users
may also leave in the middle of an interaction, if it
is not engaging enough.
Red Teams are out to test or ?break? the system,
or show it as not-competent, and may try to do
things the system can?t understand or react well
to, even when an alternative formulation is known
to work.
3.2 Types of Dialogue System Genres
Dialogue Genres can be distinguished along many
lines, e.g. the number and relationship of partic-
ipants, specific conversational rules, purposes of
the participants, etc. We distinguish here four gen-
res of dialogue system that have been in use at
the Institute for Creative Technologies and that we
have available corpora for (there are many other
types of dialogue genres, including tutoring, ca-
sual conversation, interviewing,. . . ). Each genre
has implications for the internal representations
and system architectures needed to engage in that
genre of dialogue.
Simple Question-answering This genre in-
volves strong user-initiative and weak global di-
alogue coherence. The user can ask any ques-
tion to the system at any time, and the system
should respond, with an appropriate answer if
able, or with some other reply indicating either
inability or unwillingness to provide the answer.
This genre allows modeling dialogue at a surface-
text level (Gandhe, 2013), without internal se-
mantic representations of the input, and where
the result of ?understanding? input is the system?s
expected output. The NCPEditor9 (Leuski and
Traum, 2011) is a toolkit that provides an author-
ing environment, classification, and dialogue ca-
pability for simple question-answering characters.
The SGT Blackwell, SGT Star, and Twins systems
described below are all systems in this genre.
Advanced Question-answering This genre is
similar to the simple question-answering charac-
ters, in that the main task of the user is to elicit
information from the system character. The differ-
ence is that there is more long-range and interme-
diate dialogue coherence, in that questions can be
answered several utterances after they have been
asked, there can be intervening sub-dialogues, and
characters sometimes take the initiative to pursue
their own goals rather than just responding to the
user. Because of the requirements for somewhat
deeper understanding, and relation of input to con-
9Available free for academic research purposes from
https://confluence.ict.usc.edu/display/VHTK/Home
397
text and character goals and policies, there is a
need of at least a shallow semantic representa-
tion and representation of the dialogue informa-
tion state, and the character must distinguish un-
derstanding of the input from the character out-
put (since the latter will depend on the dialogue
policy and information state, not just the under-
standing of input). The tactical questioning archi-
tecture (Gandhe et al, 2009)10 provides author-
ing and run-time support for advanced question-
answering characters, and has been used to build
over a dozen characters for purposes such as train-
ing tactical questioning, training culture, and psy-
chology experiments (Gandhe et al, 2011). The
Amani character described below is in this genre.
Slot-filling Probably the most common type of
dialogue system (at least in the research commu-
nity) is slot-filling. Here the dialogue is fairly
structured, with an initial greeting phase, then one
or more tasks, which all start with the user se-
lecting the task, and the system taking over ini-
tiative to ?fill? and possibly confirm the needed
slots, before retrieving some information from a
database, or performing a simple service.11 This
genre also requires a semantic representation, at
least of the slots and acceptable values. Gener-
ally, the set of possible values is large enough, that
some form of NLG is needed (at least template
filling), rather than authoring of all full sentences.
There are a number of toolkits and development
frameworks that are well suited to slot-filling sys-
tems, e.g. Ravenclaw (Bohus and Rudnicky, 2003)
or Trindikit (Larsson and Traum, 2000). The Ra-
diobots system, described below is in this genre.
Negotiation and Planning In this genre, the
system is more of an equal partner with the user,
than a servant, as in the slot-filling systems. The
system must not merely understand user requests,
but must also evaluate whether they meet the sys-
tem goals, what the consequences and precondi-
tions of requests are, and whether there are better
alternatives. For this kind of inference, a more de-
tailed semantic representation is required than just
filling in slots. While we are not aware of publicly
available software that makes this kind of system
easy to construct, there have been several built us-
ing an information-state approach, or the soar cog-
10Soon to be released as part of the virtual human toolkit.
11Mixed-initiative versions of this genre exist, where the
user can also provide unsolicited information, which reduces
the number of system queries needed.
nitive architecture. The TRIPS system (Allen et
al., 2001) also has many similarities.
3.3 ICT Dialogue Systems Tested
We tested the recognizers described in section 2
on data sets collected from six different dialogue
domains. Five are the same ones tested in Yao et
al. (2010), to which we added the Twins set. De-
tails on the size of the training and development
sets may be found in Yao et al (2010), here we
report only the numbers relevant to the Twins do-
main and to the NLU analysis, which are not in
Yao et al (2010).
SGT Blackwell was created as a virtual human
technology demonstration for the 2004 Army Sci-
ence Conference. This is a question-answering
character, with no internal semantic representation
and the primary NLU task merged with Dialogue
management as selecting the best response.
The original users were ICT demonstrators.
However, there were also some experiments with
recruited participants (Leuski et al, 2006a; Leuski
et al, 2006b). Later SGT Blackwell became a part
of the ?best design in America? triennial at the
Cooper-Hewitt Museum in New York City, and
the data set here is from visitors to the museum,
who are mostly casual users, but range from expert
to red-team. Users spoke into a mounted direc-
tional microphone (see Robinson et al, 2008 for
more details).
SGT STAR (Artstein et al, 2009a) is a question-
answering character similar to SGT Blackwell, al-
though designed to talk about Army careers rather
than general knowledge. The users are Army per-
sonnel who went to job fairs and visited schools in
the mobile Army adventure vans, speaking using
headset microphones, and performing for an audi-
ence. The users are somewhere between demon-
strators and expert users. They are speaking to
SGT STAR for the benefit of an audience, but their
primary purpose is to convey information to the
audience in a memorable way (through dialogue
with SGT STAR) rather than to show off the high-
lights of the character.
The Twins are two life-size virtual characters
who serve as guides at the Museum of Science
in Boston (Swartout et al, 2010). The charac-
ters promote interest in Science, Technology, En-
gineering and Mathematics (STEM) in children
between the ages of 7 and 14. They are question-
398
answering characters, but unlike SGTs Blackwell
and Star, the response is a whole dialogue se-
quence, potentially involving interchange from
both characters, rather than a single character turn.
There are two types of users for the Twins:
demonstrators, who are museum staff members,
using head-mounted microphones, and museum
visitors, who use a Shure 522 table-top mounted
microphone (Traum et al, 2012). More on analy-
sis of the museum data can be found in (Aggarwal
et al, 2012). We also investigated speech recog-
nition and NLU performance in this domain in
Morbini et al (2012).
This dataset contains 14K audio files each an-
notated with one of the 168 possible response se-
quences. The division in training development and
test is the same used in Morbini et al (2012) (10K
for training, the rest equally divided between de-
velopment and test).
Amani (Artstein et al, 2009b; Artstein et al,
2011) is an advanced question-answering char-
acter used as a prototype for systems meant to
train soldiers to perform tactical questioning. The
users are in between real users and test subjects:
they were cadets at the U.S. Military Academy in
April 2009, who interacted with Amani as a uni-
versity course exercise on negotiation techniques.
They used head-mounted microphones to talk with
Amani.
This dataset comprises of 1.8K audio files each
annotated with one of the 105 possible NLU se-
mantic classes.
Radiobots (Roque et al, 2006) is a training pro-
totype that responds to military calls for artillery
fire in a virtual reality urban combat environment.
This is a domain in the slot-filling genre, where
there is a preferred protocol for the order in which
information is provided and confirmed. Users are
generally trainees, learning how to do calls for fire,
they are motivated users with some training. The
semantic processing involved tagging each word
with the dialogue act and parameter that it was as-
sociated with (Ai et al, 2007b).
This data set was collected during the develop-
ment of the system in 2006 at Fort Sill, Oklahoma,
during two evaluation sessions from recruited vol-
unteer trainees who performed calls for specific
missions (Robinson et al, 2006). These subjects
used head-mounted microphones rather than the
ASTI simulated radios from later data collection.
SASO-EN (Traum et al, 2008) is a negotiation
training prototype in which two virtual characters
negotiate with a human ?trainee? about moving a
medical clinic. The genre is negotiation and plan-
ning, where the human participant must try to form
a coalition, and the characters reason about utili-
ties of different proposals, as well as causes and
effects. The output of NLU is a frame represen-
tation including both semantic elements, like the-
matic argument structure, and pragmatic elements,
such as addressee and referring expressions. Fur-
ther contextual interpretation is performed by each
of the virtual characters to match the (possibly par-
tial) representation to actions and states in their
task model, resolve other referring expressions,
and determine a full set of dialogue acts (Traum,
2003). Speech was collected at the USC Insti-
tute for Creative Technologies (ICT) during 2006?
2009, mostly from visitors and new hires, who
acted as test subjects.
This dataset has 4K audio files each anno-
tated with one of the 117 different NLU semantic
classes.
4 ASR Performance
We tested each of the Datasets described in Sec-
tion 3.3 with some of the recognizers described
in Section 2. All recognizers were tested on the
Amani, SASO-EN, and Twins domains, and we
also tested a natural language understanding com-
ponent on these data sets (Section 5). For SGT
Blackwell, SGT STAR, and Radiobots, we report
the performance on the same development set used
in Yao et al (2010). For Amani and SASO-EN
(where we also report the NLU performance), we
run a 10-fold cross-validation in which 9 folds
where used to train the NLU and ASR language
model and the 10th was used for testing. For the
Twins dialogue system, we used the same partition
into training, development and testing reported in
Morbini et al (2012) and the results reported here
are from the development set. Due to differences
in training/testing regimens, performance of sys-
tems are only comparable within each domain.
Table 2 summarizes the performance of the var-
ious ASR engines on the evaluation data sets. Per-
formance is measured as Word Error Rate and was
obtained using the NIST SCLITE tool.12
Note that only Otosense-Kaldi in the Twins do-
main had adapted acoustic models. In the remain-
12http://www.itl.nist.gov/iad/mig/tools/
399
Speech recognizer Evaluation data setAmani Radiobots SASO-EN SGT Blackwell SGT Star Twins
Pocketsphinx 39.7 11.8 28.4 51 28.6 81
Apple 28 ? 30.9 ? ? 29
AT&T 29 12.1 16.3 27.3 21.7 28.8
Google 23.8 36.3 20 18 26 20.6
Otosense-Kaldi 33.7 ? 22.1 ? ? 18.7
Table 2: Word Error Rates (%) for the various dialogue systems and ASR systems tested.
ing cases only the language model was adapted.
Looking at the results on the development set re-
ported in Yao et al (2010), we have improvements
in 3 out of 5 domains: Amani (?11.8% Google),
SASO-EN (?11.7% AT&T) and SGT Blackwell
(?13% Google). In Radiobots and SGT Star the
performance achieved with just language model
adaptation, when permitted, is worse: +4.8% and
+1.7% respectively.
We find that there is no single best performing
speech recognizer: results vary greatly between
the evaluation test sets. In 4 of the 6 datasets over-
all, and 2 of the 3 datatests tested with Otosense-
Kaldi, the best performer is a cloud-based ser-
vice (Google or AT&T). There are two datasets
for which a local, fully customizable recognizer
performs better than the cloud-based services. Ra-
diobots, consisting of military calls for artillery
fire, has a fairly limited and very specialized vo-
cabulary, and indeed the two recognizers with cus-
tom language models (Pocketsphinx and AT&T)
perform much better than the non-customizable
recognizer (Google).
The Twins dataset is unique in that for the
Otosense-Kaldi system we custom-trained acous-
tic and language models, while standard WSJ
acoustic models and adapted language models
were used for the other dialogue systems. In
both cases the models were triphone based with
a Linear Discriminant Analysis (LDA) front end,
and Maximum Likelihood Linear Transforma-
tion (MLLT) and Maximum Mutual Information
(MMI) training. This reflects on the very good
performance in the Twins domain, decent perfor-
mance on the SASO-EN domain (reasonable mis-
match of WSJ and SASO-EN) and very degraded
performance in Amani (highly mismatched Amani
and WSJ domains). The observed degradation in
performance is accentuated by the MMI discrim-
inative training on the mismatched-WSJ data. As
with PocketSphinx and Watson, and unlike with
Apple Dictation and Google Speech API, with
Kaldi we fully control experimental conditions
and can guarantee no contamination of the train-
test data.
In summary, our evaluation shows that cus-
tomizable recognizers are useful when the ex-
pected speech is highly specialized, or when sub-
stantial resources are available for tuning the rec-
ognizer.
5 NLU Accuracy & Relation between
ASR and NLU
While the different genres of system have different
types of output for NLU: response text, dialogue
act and parameter tags, speech acts, or semantic
frames, many of them can be coerced into a se-
lection task, in which the NLU selects the right
output from a set of possible outputs. This allows
any multiclass classification algorithm to be used
for NLU. A possible drawback is that for some
inputs, the right output might not be available in
the set considered by the training data, even if it
might easily be constructed from known parts us-
ing a generative approach.
A second issue is that even though we can cast
the problem as multi-class classification, classifi-
cation accuracy is not always the most appropriate
metric of NLU quality. For question-answering
characters, getting an appropriate and relevant re-
ply is more important than picking the exact re-
ply selected by a human domain designer or an-
notator: there might be multiple good answers, or
even the best available answer might not be very
good. For that reason, the question-answering
characters allow an ?off-topic? answer and Error-
return plots (Artstein, 2011) might be necessary
to choose an optimal threshold. For the SASO-EN
system, slot-filler metrics such as precision, recall,
and f-score are more appropriate than frame accu-
400
racy, because some frames may have many slots
in common and few that are different (e.g. just a
different addressee). Nonetheless, we begin our
analysis within this common framework. For sim-
plicity, we start with just three domains: Twins,
Amani, and SASO-EN. SGT STAR and Blackwell
are very similar to Twins in terms of NLU. Ra-
diobots is more challenging to coerce to multiclass
classification.
Conventional wisdom in the speech and lan-
guage processing community is that performance
of ASR and NLU are closely tied: improved
speech recognition leads to better language under-
standing, while deficiencies in speech recognition
cause difficulty in understanding. This conven-
tional wisdom is borne out by decades of experi-
ence with speech and dialogue systems, though we
are not aware of attempts to systematically demon-
strate it. The present study shows that the expected
relation between speech recognition and language
understanding holds for the systems we tested.
Accepted assumptions about the relation be-
tween speech recognition and language under-
standing have been repeatedly challenged. Direct
challenges are typically limited to specific appli-
cations. Wang et al (2003) show that for a slot-
filling NLU, ASR can be specifically tuned to rec-
ognize those words that are relevant to the slot-
filling task, resulting in improved understanding
despite a decrease in performance on overall word
recognition. However, Boros et al (1996) found
that when not optimizing the ASR for the specific
slot filling task there is a nearly linear correlation
between word accuracy and NLU accuracy. Al-
shawi (2003) and Huang and Cox (2006) show that
in call-routing applications the word level can be
dispensed with altogether and calls routed based
on phonetic information alone without noticeable
loss in performance. These challenges suggest that
the speech-language divide is not as clean as the
theory suggests.
To investigate the relation between ASR and
NLU, we ran each ASR output from each of
the 5 recognizers through an understanding com-
ponent to obtain an NLU output (each dataset
had a separate NLU component, which was held
constant for all speech recognizers). ASR and
NLU performance are conventionally measured on
scales of opposite polarity: better performance
shows up as lower word error rates but higher
NLU accuracies. For the correlations we invert the
conventional ASR scale and use word accuracy, so
that higher numbers signify better performance on
both scales.13
Figure 1 shows the results obtained in the 3 di-
alogue systems by the various ASR systems. The
figures plot ASR performance against NLU per-
formance; NLU results on manual transcriptions
are included for comparison. There are too few
data points for the correlations between ASR and
NLU performance to be significant, but the trends
are positive, as expected.
Our experiments lend supporting evidence to
the claim that in general, ASR performance is pos-
itively linked to NLU performance (special cases
notwithstanding). The 3 datasets exhibit posi-
tive correlations between speech recognition and
language understanding performance. Thus, we
claim that the basis of the conventional wisdom
is sound: speech recognition directly affects lan-
guage understanding. This conclusion holds when
the speech recognizer has been optimized to pro-
duce the most accurate transcript, rather than for a
specific NLU.
6 Conclusion and Future Work
We have extended here the ASR system evaluation
published in Yao et al (2010) including some new
cloud based ASR services that achieve very good
performance showing an improvement of around
12%. We also showed that ASR and NLU perfor-
mance are correlated.
One possible avenue of future work is to ex-
tract importance weights for each word from the
learnt NLU models and use these weights to try
to explain those cases that diverge from the corre-
lation between ASR and NLU performance. This
may also give us a better measure than WER for
assessing ASR performance in dialogue systems.
Another avenue of future work involves examin-
ing different types of NLU engines, and different
metrics for the different dialogue system genres,
which, again, may lead to a more relevant assess-
ment of ASR performance.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
13We define ?accuracy? as 1 minus WER, so this number
can in principle dip below zero if there are more errors than
words.
401
Amani
r = 0.54, df = 3, p = 0.345
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
50 60 70 80 90 100
51
54
57
60
63
66
69
?
?
?
? ?
?
SASO-EN
r = 0.77, df = 3, p = 0.130
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
60 70 80 90 100
30
40
50
60
70
80
90
?
???
?
?
Twins
r = 0.99, df = 3, p = 0.002
Word accuracy (%)
N
LU
ac
cu
ra
cy
(%
)
0 20 40 60 80 100
40
50
60
70
80
90
100
?
?
?
?
?
Figure 1: Relation between ASR and NLU performance (red dots are manual transcriptions)
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Priti Aggarwal, Ron Artstein, Jillian Gerten, Athana-
sios Katsamanis, Shrikanth Narayanan, Angela
Nazarian, and David Traum. 2012. The Twins cor-
pus of museum visitor questions. In LREC-2012,
Istanbul, Turkey, May.
Hua Ai, Antoine Raux, Dan Bohus, Maxine Eskenazi,
and Diane Litman. 2007a. Comparing spoken dia-
log corpora collected with recruited subjects versus
real users. In SIGdial 2007.
Hua Ai, Antonio Roque, Anton Leuski, and David
Traum. 2007b. Using information state to improve
dialogue move identification in a spoken dialogue
system. In Proceedings of the 10th Interspeech Con-
ference, Antwerp, Belgium, August.
James F. Allen, George Ferguson, and Amanda Stent.
2001. An architecture for more realistic conversa-
tional systems. In IUI, pages 1?8.
Hiyan Alshawi. 2003. Effective utterance classifica-
tion with unsupervised phonotactic models. In HLT-
NAACL 2003, pages 1?7, Edmonton, Alberta, May.
Apple Inc. 2012. Mac basics: Dictation (Technote
HT5449), November.
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and
D. Traum. 2009a. Semi-formal evaluation of con-
versational characters. In O. Grumberg, M. Kamin-
ski, S. Katz, and S. Wintner, editors, Languages:
From Formal to Natural. Essays Dedicated to Nis-
sim Francez on the Occasion of His 65th Birthday,
volume 5533 of Lecture Notes in Computer Science,
pages 22?35. Springer, Berlin.
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David R. Traum. 2009b. Viability of a simple dia-
logue act scheme for a tactical questioning dialogue
system. In DiaHolmia 2009: Proceedings of the
13th Workshop on the Semantics and Pragmatics of
Dialogue, page 43?50, Stockholm, Sweden, June.
Ron Artstein, Michael Rushforth, Sudeep Gandhe,
David Traum, and MAJ Aram Donigian. 2011.
Limits of simple dialogue acts for tactical question-
ing dialogues. In 7th IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
Barcelona, Spain, July.
Ron Artstein. 2011. Error return plots. In 12th SIG-
dial Workshop on Discourse and Dialogue, Port-
land, OR, June.
Linda Bell and Joakim Gustafson. 2003. Child and
adult speaker adaptation during error resolution in a
publicly available spoken dialogue system. In IN-
TERSPEECH 2003.
Dan Bohus and Alexander I. Rudnicky. 2003. Raven-
claw: dialog management using hierarchical task de-
composition and an expectation agenda. In INTER-
SPEECH 2003.
M. Boros, W. Eckert, F. Gallwitz, G. Grz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In In Proceedings of (ICSLP 96), pages
1009?1012.
Sudeep Gandhe, Nicolle Whitman, David R. Traum,
and Ron Artstein. 2009. An integrated authoring
tool for tactical questioning dialogue systems. In 6th
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Pasadena, California, July.
Sudeep Gandhe, Michael Rushforth, Priti Aggarwal,
and David R. Traum. 2011. Evaluation of
an integrated authoring tool for building advanced
question-answering characters. In Proceedings of
Interspeech-11, Florence, Italy, 08/2011.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
402
Liza Hassel and Eli Hagen. 2006. Adaptation of an
automotive dialogue system to users? expertise and
evaluation of the system. Language Resources and
Evaluation, 40(1):67?85.
Quiang Huang and Stephen Cox. 2006. Task-
independent call-routing. Speech Communication,
48(3?4):374?389.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Acoustics,
Speech and Signal Processing, 2006. ICASSP 2006
Proceedings. 2006 IEEE International Conference
on, volume 1, pages I?I.
Kristiina Jokinen and Kari Kanto. 2004. User ex-
pertise modeling and adaptivity in a speech-based
e-mail system. In Donia Scott, Walter Daelemans,
and Marilyn A. Walker, editors, ACL, pages 87?94.
ACL.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language En-
gineering, 6:323?340, September. Special Issue on
Spoken Language Dialogue System Engineering.
Anton Leuski and David R. Traum. 2011. NPCEditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32:42?56.
Anton Leuski, Brandon Kennedy, Ronakkumar Patel,
and David Traum. 2006a. Asking questions to
limited domain virtual characters: How good does
speech recognition have to be? In 25th Army Sci-
ence Conference.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006b. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27.
Fabrizio Morbini, Kartik Audhkhasi, Ron Artstein,
Maarten Van Segbroeck, Kenji Sagae, Panayio-
tis S. Georgiou, David R. Traum, and Shrikanth S.
Narayanan. 2012. A reranking approach for recog-
nition and classification of speech input in conversa-
tional dialogue systems. In SLT, pages 49?54. IEEE.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Luka?s?
Burget, Ondr?ej Glembek, Nagendra Goel, Mirko
Hannemann, Petr Motl??c?ek, Yanmin Qian, Petr
Schwarz, Jan Silovsky?, Georg Stemmer, and Karel
Vesely?. 2011. The Kaldi speech recognition
toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding, December.
S.M. Robinson, A. Roque, A. Vaswani, D. Traum,
C. Hernandez, and B. Millspaugh. 2006. Evalua-
tion of a spoken dialogue system for virtual reality
call for fire training. In 25th Army Science Confer-
ence, Orlando, Florida, USA.
S. Robinson, D. Traum, M. Ittycheriah, and J. Hen-
derer. 2008. What would you ask a conversational
agent? Observations of human-agent dialogues in
a museum setting. In Proc. of Sixth International
Conference on Language Resources and Evaluation
(LREC), Marrakech, Morocco.
A. Roque, A. Leuski, V. Rangarajan, S. Robinson,
A. Vaswani, S. Narayanan, and D. Traum. 2006.
Radiobot-CFF: A spoken dialogue system for mil-
itary training. In Proc. of Interspeech, Pittsburgh,
Pennsylvania, USA.
W. Swartout, D. Traum, R. Artstein, D. Noren, P. De-
bevec, K. Bronnenkant, J. Williams, A. Leuski,
S. Narayanan, D. Piepol, C. Lane, J. Morie, P. Ag-
garwal, M. Liewer, J. Chiang, J. Gerten, S. Chu,
and K. White. 2010. Ada and Grace: Toward
realistic and engaging virtual museum guides. In
J. Allbeck, N. Badler, T. Bickmore, C. Pelachaud,
and A. Safonova, editors, Intelligent Virtual Agents:
10th International Conference, IVA 2010, Philadel-
phia, PA, USA, September 20?22, 2010 Proceed-
ings, volume 6356 of Lecture Notes in Artificial In-
telligence, pages 286?300. Springer, Heidelberg.
David R. Traum, Stacy Marsella, Jonathan Gratch, Jina
Lee, and Arno Hartholt. 2008. Multi-party, multi-
issue, multi-strategy negotiation for multi-modal
virtual agents. In IVA, pages 117?130.
David Traum, Priti Aggarwal, Ron Artstein, Susan
Foutz, Jillian Gerten, Athanasios Katsamanis, Anton
Leuski, Dan Noren, and William Swartout. 2012.
Ada and grace: Direct interaction with museum
visitors. In The 12th International Conference on
Intelligent Virtual Agents (IVA), Santa Cruz, CA,
September.
David Traum. 2003. Semantics and pragmatics of
questions and answers for dialogue agents. In pro-
ceedings of the International Workshop on Compu-
tational Semantics, pages 380?394.
Ye-Yi Wang, A. Acero, and C. Chelba. 2003. Is
word error rate a good indicator for spoken lan-
guage understanding accuracy. In IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU ?03), pages 577?582.
Xuchen Yao, Pravin Bhutada, Kallirroi Georgila, Kenji
Sagae, Ron Artstein, and David R. Traum. 2010.
Practical evaluation of speech recognizers for vir-
tual human dialogue systems. In Nicoletta Calzo-
lari, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, LREC. European Language
Resources Association.
403
Proceedings of the SIGDIAL 2014 Conference, pages 254?256,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
A Demonstration of Dialogue Processing in SimSensei Kiosk
Fabrizio Morbini, David DeVault, Kallirroi Georgila,
Ron Artstein, David Traum, Louis-Philippe Morency
USC Institute for Creative Technologies
12015 Waterfront Dr., Playa Vista, CA 90094
{morbini,devault,kgeorgila,artstein,traum,morency}@ict.usc.edu
Abstract
This demonstration highlights the dia-
logue processing in SimSensei Kiosk, a
virtual human dialogue system that con-
ducts interviews related to psychologi-
cal distress conditions such as depression,
anxiety, and post-traumatic stress disorder
(PTSD). The dialogue processing in Sim-
Sensei Kiosk allows the system to con-
duct coherent spoken interviews of human
users that are 15-25 minutes in length,
and in which users feel comfortable talk-
ing and openly sharing information. We
present the design of the individual dia-
logue components, and show examples of
natural conversation flow between the sys-
tem and users, including expressions of
empathy, follow-up responses and contin-
uation prompts, and turn-taking.
1 Introduction
This demonstration highlights the dialogue pro-
cessing in SimSensei Kiosk, a virtual human di-
alogue system that conducts interviews related to
psychological distress conditions such as depres-
sion, anxiety, and post-traumatic stress disorder
(PTSD) (DeVault et al., 2014). SimSensei Kiosk
has two main functions ? a virtual human called
Ellie (pictured in Figure 1), who converses with a
user in a spoken, semi-structured interview, and a
multimodal perception system which analyzes the
user?s behavior in real time to identify indicators
of psychological distress.
The system has been designed and devel-
oped over two years using a series of face-to-
face, Wizard-of-Oz, and automated system stud-
ies involving more than 350 human participants
(Scherer et al., 2013; DeVault et al., 2013; DeVault
et al., 2014). Agent design has been guided by
two overarching goals: (1) the agent should make
Figure 1: Ellie, the virtual human interviewer in
SimSensei Kiosk.
the user feel comfortable talking and openly shar-
ing information, and at the same time (2) the agent
should create interactional situations that support
the automatic assessment of verbal and nonver-
bal behaviors correlated with psychological dis-
tress. During an interview, the agent presents a
set of questions which have been shown in user
testing to support these goals. Since the main in-
terview questions and their order are mostly fixed,
dialogue management concentrates on providing
appropriate verbal feedback behaviors to keep the
user engaged, maintain a natural and comfort-
able conversation flow, and elicit continuations
and elaborations from the user.
The agent is implemented using a modular ar-
chitecture (Hartholt et al., 2013). Dialogue pro-
cessing, which is the focus of this demonstration,
is supported by individual modules for speech
recognition, language understanding and dialogue
management (see Section 2). The agent?s lan-
guage and speech are executed by selecting from
pre-recorded audio clips. Additional agent mod-
ules include nonverbal behavior generation, which
matches appropriately timed body movements to
the agent?s speech; character animation in a vir-
tual 3D environment; and rendering in a game en-
254
gine. The perception system analyzes audio and
video in real time to identify features such as head
position, gaze direction, smile intensity, and voice
quality. DeVault et al. (2014) provides details on
all the agent?s modules.
2 Overview of Dialogue Processing
2.1 ASR and NLU components
Unlike many task-oriented dialogue domains, in-
terview dialogues between SimSensei Kiosk and
participants are naturally open-ended. People tend
to respond to interview stimuli such as ?what?s
one of your most memorable experiences?? with
idiosyncratic stories and events from their lives.
The variability in the vocabulary and content of
participants? answers to such questions is so large
that it makes the ASR task very challenging. Fur-
thermore, continuous ASR is employed to ensure
that participants feel comfortable interacting with
the system without being distracted by having to
use a push-to-talk microphone. The use of con-
tinuous ASR necessitates the development of spe-
cific policies for turn-taking (see Section 2.2). In
this demonstration, voice activity detection and
speech recognition are performed using Pocket-
Sphinx (Huggins-Daines et al., 2006).
Because of the open-ended participants? re-
sponses, for NLU, we cannot simply construct a
small semantic ontology and expect to cover the
majority of meanings that will be expressed by
users. Thus, this is an application in which the
dialogue policy needs to be able to create a sense
of engagement, continuation, and empathy despite
relatively shallow and limited understanding of
user speech. SimSensei Kiosk currently uses 4
statistically trained utterance classifiers to capture
different aspects of user utterance meaning.
The first NLU classifier identifies generic di-
alogue act types, including statements, yes-no
questions, wh-questions, yes and no answers, and
several others. This classifier is trained using
the Switchboard DAMSL corpus (Jurafsky et al.,
1997) using a maximum entropy model.
The second NLU classifier assigns positive,
negative, or neutral valence to utterances, in or-
der to guide Ellie?s expression of empathy. We
use SentiWordNet 3.0 (Baccianella et al., 2010), a
lexical sentiment dictionary, to assign valence to
individual words spoken by users (as recognized
by the ASR); the valence assigned to an utterance
is based primarily on the mean valence scores of
Opening Rapport Building Phase
Ellie What are some things you really like about LA?
(top level question)
User I love the weather, I love the palm trees, I love the
beaches, there?s a lot to do here.
Diagnostic Phase
Ellie Have you noticed any changes in your behavior or
thoughts lately? (top level question)
User Yes.
Ellie Can you tell me about that? (continuation prompt)
User I?m having a lot more nightmares now uh can?t
sleep have haven?t really been eating uh trying to
eat. . . I have to force down food um just feeling
like an emotional wreck.
Ellie I?m sorry to hear that. (empathy response)
Ellie What are you like when you don?t sleep well?
(follow-up question)
User Irritable, emotional, it just adds to my overall
stress um [long pause]
Ellie What. . . (Ellie speaks after the participant?s long
pause)
User Can?t concentrate uh I uh. . . (the participant starts
speaking while Ellie is speaking)
Ellie I?m sorry please continue. (Ellie realizes that she
has interrupted the participant and apologizes)
Positive Closing Phase
Ellie How would your best friend describe you? (top
level question)
User As caring, as fun because most of the time when
I?m around my best friends I?m happy and I?m fun
loving. I joke around with them a lot and uh I do
better when I?m around my friends. . .
Figure 2: Examples of Ellie?s interview phases.
the individual words in the utterance.
The third NLU classifier supports domain-
specific small talk by recognizing a handful of
specific anticipated responses to Ellie?s rapport-
building questions. For example, when Ellie asks
users where they are from, this classifier detects
the names of commonly mentioned cities and re-
gions using keyphrase spotting.
The fourth NLU classifier identifies domain-
specific dialogue acts, and supports Ellie?s follow-
up responses to specific questions, such as ?how
close are you to your family??. This maximum
entropy classifier is trained using face-to-face and
Wizard-of-Oz data to detect specific responses
such as assertions of closeness.
2.2 Dialogue Management
Ellie currently uses about 100 fixed utterances in
total in the automated system. She employs 60 top
level interview questions (e.g., ?do you travel a
255
lot??), plus some follow-up questions (e.g., ?what
do you enjoy about traveling??) and a range of
backchannels (e.g., ?uh huh?), empathy responses
(e.g., ?that?s great?, ?I?m sorry?), and continua-
tion prompts (e.g., ?tell me more about that?).
The dialogue policy is implemented using the
FLoReS dialogue manager (Morbini et al., 2012).
The policy groups interview questions into three
phases (opening rapport building, diagnostic, pos-
itive closing ? ensuring that the participant leaves
with positive feelings). Questions are generally
asked in a fixed order, with some branching based
on responses to specific questions.
Rule-based subpolicies determine what Ellie?s
follow-up responses will be for each of her top-
level interview questions. The rules for follow-ups
are defined in relation to the four NLU classifiers
and the duration of user speech (measured in sec-
onds). These rules trigger continuation prompts
and empathy responses under specific conditions.
The turn-taking policy supports our design goal
to encourage users to openly share information
and to speak at length in response to Ellie?s open-
ended questions. In this domain, users often pause
before or during their responses to think about
their answers to Ellie?s personal questions. The
turn-taking policy is designed to provide ample
time for users to consider their responses, and to
let users take and keep the initiative as much as
possible. Ellie?s turn-taking decisions are based
on thresholds for user pause duration, i.e., how
much time the system should wait after the user
has stopped speaking before Ellie starts speaking.
These thresholds are tuned to the face-to-face and
Wizard-of-Oz data to minimize Ellie?s interrup-
tion rate, and are extended dynamically when El-
lie detects that she has interrupted the participant.
This is to take into account the fact that some peo-
ple tend to use longer pauses than others.
Examples of the three interview phases and of
Ellie?s subdialogue policies (top level and follow-
up questions, continuation prompts, empathy re-
sponses, and turn-taking) are given in Figure 2.
3 Demonstration Summary
The demonstration will feature a live interac-
tion between Ellie and a participant, showing El-
lie?s real-time understanding and consequent pol-
icy actions. Live dialogues will highlight Ellie?s
strategies for questioning, follow-up continuation
prompts, displays of empathy, and turn-taking,
similar to the example in Figure 2. The demon-
stration will illustrate how these elements work to-
gether to enable Ellie to carry out extended inter-
views that also provide information relevant to the
automatic assessment of indicators of distress.
Acknowledgments
The effort described here is supported by DARPA
under contract W911NF-04-D-0005 and the U.S.
Army. Any opinion, content or information pre-
sented does not necessarily reflect the position or
the policy of the United States Government, and
no official endorsement should be inferred.
References
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of LREC.
D. DeVault, K. Georgila, R. Artstein, F. Morbini, D.
Traum, S. Scherer, A. Rizzo, and L.-P. Morency.
2013. Verbal indicators of psychological distress in
interactive dialogue with a virtual human. In Pro-
ceedings of SIGDIAL.
D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast,
A. Gainer, K. Georgila, J. Gratch, A. Hartholt, M.
Lhommet, G. Lucas, S. Marsella, F. Morbini, A.
Nazarian, S. Scherer, G. Stratou, A. Suri, D. Traum,
R. Wood, Y. Xu, A. Rizzo, and L.-P. Morency. 2014.
SimSensei Kiosk: A virtual human interviewer for
healthcare decision support. In Proceedings of AA-
MAS.
A. Hartholt, D. Traum, S. Marsella, A. Shapiro, G.
Stratou, A. Leuski, L.-P. Morency, and J. Gratch.
2013. All together now, introducing the virtual hu-
man toolkit. In Proceedings of IVA.
D. Huggins-Daines, M. Kumar, A. Chan, A.W. Black,
M. Ravishankar, and A.I. Rudnicky. 2006. Pocket-
Sphinx: A free, real-time continuous speech recog-
nition system for hand-held devices. In Proceedings
of ICASSP.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL Shallow-Discourse-Function
Annotation Coders Manual, Draft 13.
F. Morbini, D. DeVault, K. Sagae, J. Gerten, A. Nazar-
ian, and D. Traum. 2012. FLoReS: A forward look-
ing reward seeking dialogue manager. In Proceed-
ings of IWSDS.
S. Scherer, G. Stratou, M. Mahmoud, J. Boberg,
J. Gratch, A. Rizzo, and L.-P. Morency. 2013. Au-
tomatic behavior descriptors for psychological dis-
order analysis. In Proceedings of IEEE Conference
on Automatic Face and Gesture Recognition.
256
