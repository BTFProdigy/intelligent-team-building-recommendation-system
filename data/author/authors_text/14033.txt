NAACL HLT Demonstration Program, pages 11?12,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Cedit ? Semantic Networks Manual Annotation Tool
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University
Malostranske? na?m. 25, 11800 Praha, Czech Republic
novak@ufal.mff.cuni.cz
Abstract
We present a demonstration of an annota-
tion tool designed to annotate texts into a
semantic network formalism called Multi-
Net. The tool is based on a Java Swing
GUI and allows the annotators to edit
nodes and relations in the network, as well
as links between the nodes in the network
and the nodes from the previous layer of
annotation. The data processed by the tool
in this presentation are from the English
version of the Wall Street Journal.
1 Introduction
Cedit is a part of a project to create a rich resource
of manually annotated semantic structures (Nova?k,
2007) as a new layer of the Prague Dependency
Treebank (Sgall et al, 2004). The new layer is
based on the MultiNet paradigm described in (Hel-
big, 2006).
1.1 Prague Dependency Treebank
The Prague Dependency Treebank is a language
resource containing a deep manual analysis of
text (Sgall et al, 2004). PDT contains three lay-
ers of annotation, namely morphological, analytical
(shallow dependency syntax) and tectogrammatical
(deep dependency syntax). The units of each annota-
tion level are linked to corresponding units from the
shallower level. The morphological units are linked
directly to the original text.
The theoretical basis of the treebank is described
by the Functional Generative Description of lan-
guage system (Sgall et al, 1986).
1.2 MultiNet
Multilayered Extended Semantic Networks (Multi-
Net), described in (Helbig, 2006), provide a univer-
sally applicable formalism for treatment of semantic
phenomena of natural language. They offer distinct
advantages over classical predicate calculus and its
derivatives. Moreover, semantic networks are con-
venient for manual annotation because they are more
intuitive.
MultiNet?s semantic representation of natural lan-
guage is independent of the language being anno-
tated. However, syntax obviously varies across lan-
guages. To bridge the gap between different lan-
guages we can the deep syntactico-semantic repre-
sentation available in the Functional Generative De-
scription framework.
2 Project Goals
The main goals of the project are:
? Test the completeness and intuitiveness of
MultiNet specification
? Measure differences in semantic networks of
parallel texts
? Enrich the Prague Dependency Treebank with
a new layer of annotation
? Provide data for supervised training of text-to-
semantic-network transformation
11
? Test the extensibility of MultiNet to other lan-
guages than German
3 Cedit
The presented tool has two key components de-
scribed in this section.
3.1 Input/Output processing
The input module of the tool loads XML files in
Prague Markup Language (PML) and creates an in-
ternal representation of the semantic network, tec-
togrammatical layer, analytical layer, and the surface
text (Pajas and S?te?pa?nek, 2005). There is also an op-
tion to use files with named entity annotations. The
sentences in this demo are all annotated with named
entities.
The XML schema for the semantic network is an
application of the Prague Markup Language.
3.2 Network GUI
The annotation GUI is implemented using Java
Swing (Elliott et al, 2002). The key features of the
tool presented in the demonstration are:
? Editing links between the semantic network
and the tectogrammatical layer
? Adding and removing nodes
? Connecting nodes with directed edges
? Connecting edges with directed edges (i.e., cre-
ating relations on the metalevel)
? Editing attributes of both nodes and edges
? Undoing and redoing operations
? Reusing concepts from previous sentences
4 Related Work
There are various tools for annotation of the Prague
Dependency Treebank. The Tred tool (Hajic? et
al., 2001), for example, allows users to edit many
PML applications, even those that have never been
seen before. This functionality is enabled by roles
in PML specification (Pajas and S?te?pa?nek, 2005).
MultiNet structures can be edited using MWR
tool (Gnrlich, 2000), but this tool is not primarily
intended for annotation; it serves more as an in-
terface to tools automatically transforming German
sentences into MultiNet.
Acknowledgement
This work is supported by Czech Academy of Sci-
ence grant 1ET201120505 and Czech Ministry of
Education, Youth and Sports project LC536. The
views expressed are not necessarily endorsed by the
sponsors.
References
James Elliott, Robert Eckstein, Marc Loy, David Wood,
and Brian Cole. 2002. Java Swing. O?Reilly.
Carsten Gnrlich. 2000. MultiNet/WR: A Knowledge En-
gineering Toolkit for Natural Language Information.
Technical Report 278, University Hagen, Hagen, Ger-
many.
Jan Hajic?, Barbora Vidova?-Hladka?, and Petr Pajas. 2001.
The Prague Dependency Treebank: Annotation Struc-
ture and Support. In Proceedings of the IRCS
Workshop on Linguistic Databases, pages 105?114,
Philadelphia, USA. University of Pennsylvania.
Hermann Helbig. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer-Verlag,
Berlin Heidelberg.
Va?clav Nova?k. 2007. Large Semantic Network Man-
ual Annotation. In Proceedings of 7th International
Workshop on Computational Semantics, pages 355?
358, Tilburg, Netherlands.
Petr Pajas and Jan S?te?pa?nek. 2005. A Generic XML-
Based Format for Structured Linguistic Annotation
and Its Application to Prague Dependency Treebank
2.0. Technical Report 29, UFAL MFF UK, Praha.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel Publishing company, Do-
drecht, Boston, London.
Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?. 2004.
Deep Syntactic Annotation: Tectogrammatical Repre-
sentation and Beyond. In A. Meyers, editor, Proceed-
ings of the HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 32?38, Boston, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
12
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 42?52,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Corrective Modeling for Non-Projective Dependency Parsing
Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
keith hall@jhu.edu
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University
Prague, Czech Republic
novak@ufal.mff.cuni.cz
Abstract
We present a corrective model for recov-
ering non-projective dependency struc-
tures from trees generated by state-of-the-
art constituency-based parsers. The con-
tinuity constraint of these constituency-
based parsers makes it impossible for
them to posit non-projective dependency
trees. Analysis of the types of depen-
dency errors made by these parsers on a
Czech corpus show that the correct gov-
ernor is likely to be found within a local
neighborhood of the governor proposed
by the parser. Our model, based on a
MaxEnt classifier, improves overall de-
pendency accuracy by .7% (a 4.5% reduc-
tion in error) with over 50% accuracy for
non-projective structures.
1 Introduction
Statistical parsing models have been shown to
be successful in recovering labeled constituencies
(Collins, 2003; Charniak and Johnson, 2005; Roark
and Collins, 2004) and have also been shown to
be adequate in recovering dependency relationships
(Collins et al, 1999; Levy and Manning, 2004;
Dubey and Keller, 2003). The most successful mod-
els are based on lexicalized probabilistic context
free grammars (PCFGs) induced from constituency-
based treebanks. The linear-precedence constraint
of these grammars restricts the types of dependency
structures that can be encoded in such trees.1 A
shortcoming of the constituency-based paradigm for
parsing is that it is inherently incapable of repre-
senting non-projective dependencies trees (we de-
fine non-projectivity in the following section). This
is particularly problematic when parsing free word-
order languages, such as Czech, due to the frequency
of sentences with non-projective constructions.
In this work, we explore a corrective model which
recovers non-projective dependency structures by
training a classifier to select correct dependency
pairs from a set of candidates based on parses gen-
erated by a constituency-based parser. We chose to
use this model due to the observations that the de-
pendency errors made by the parsers are generally
local errors. For the nodes with incorrect depen-
dency links in the parser output, the correct gov-
ernor of a node is often found within a local con-
text of the proposed governor. By considering al-
ternative dependencies based on local deviations of
the parser output we constrain the set of candidate
governors for each node during the corrective proce-
dure. We examine two state-of-the-art constituency-
based parsers in this work: the Collins Czech parser
(1999) and a version of the Charniak parser (2001)
that was modified to parse Czech.
Alternative efforts to recover dependency struc-
ture from English are based on reconstructing the
movement traces encoded in constituency trees
(Collins, 2003; Levy and Manning, 2004; Johnson,
2002; Dubey and Keller, 2003). In fact, the fea-
1In order to correctly capture the dependency structure, co-
indexed movement traces are used in a form similar to govern-
ment and Binding theory, GPSG, etc.
42
wc
wa
wb
b ca
wc
b ca
wa wb
wc
wa
wb
b ca
Figure 1: Examples of projective and non-projective trees. The trees on the left and center are both projec-
tive. The tree on the right is non-projective.
tures we use in the current model are similar to those
proposed by Levy and Manning (2004). However,
the approach we propose discards the constituency
structure prior to the modeling phase; we model cor-
rective transformations of dependency trees.
The technique proposed in this paper is similar to
that of recent parser reranking approaches (Collins,
2000; Charniak and Johnson, 2005); however, while
reranking approaches allow a parser to generate a
likely candidate set according to a generative model,
we consider a set of candidates based on local per-
turbations of the single most likely tree generated.
The primary reason for such an approach is that we
allow dependency structures which would never be
hypothesized by the parser. Specifically, we allow
for non-projective dependencies.
The corrective algorithm proposed in this paper
shares the motivation of the transformation-based
learning work (Brill, 1995). We do consider local
transformations of the dependency trees; however,
the technique presented here is based on a generative
model that maximizes the likelihood of good depen-
dents. We consider a finite set of local perturbations
of the tree and use a fixed model to select the best
tree by independently choosing optimal dependency
links.
In the remainder of the paper we provide a defini-
tion of a dependency tree and the motivation for us-
ing such trees as well as a description of the particu-
lar dataset that we use in our experiments, the Prague
Dependency Treebank (PDT). In Section 3 we de-
scribe the techniques used to adapt constituency-
based parsers to train from and generate dependency
trees. Section 4 describes corrective modeling as
used in this work and Section 4.2 describes the par-
ticular features with which we have experimented.
Section 5 presents the results of a set of experiments
we performed on data from the PDT.
2 Syntactic Dependency Trees and the
Prague Dependency Treebank
A dependency tree is a set of nodes ? =
{w
0
, w
1
, . . . , wk} where w0 is the imaginary root
node2 and a set of dependency links G =
{g
1
, . . . , gk} where gi is an index into ? represent-
ing the governor of wi. In other words g3 = 1 in-
dicates that the governor of w
3
is w
1
. Finally, every
node has exactly one governor except for w
0
, which
has no governor (the tree constraints).3 The index of
the nodes represents the surface order of the nodes
in the sequence (i.e., wi precedes wj in the sentence
if i < j).
A tree is projective if for every three nodes: wa,
wb, and wc where a < b < c; if wa is governed by
wc then wb is transitively governed by wc or if wc
is governed by wa then wb is transitively governed
by wa.4 Figure 1 shows examples of projective and
non-projective trees. The rightmost tree, which is
non-projective, contains a subtree consisting of wa
and wc but not wb; however, wb occurs between wa
and wc in the linear ordering of the nodes. Projec-
tivity in a dependency tree is akin to the continuity
constraint in a constituency tree; such a constraint is
2The imaginary root node simplifies notation.
3The dependency structures here are very similar to those
described by Mel?c?uk (1988); however the nodes of the depen-
dency trees discussed in this paper are limited to the words of
the sentence and are always ordered according to the surface
word-order.
4Node w
a
is said to transitively govern node w
b
if w
b
is a
descendant of w
a
in the dependency tree.
43
implicitly imposed by trees generated from context
free grammars (CFGs).
Strict word-order languages, such as English, ex-
hibit non-projective dependency structures in a rel-
atively constrained set of syntactic configurations
(e.g., right-node raising). Traditionally, these move-
ments are encoded in syntactic analyses as traces.
In languages with free word-order, such as Czech,
constituency-based representations are overly con-
strained (Sgall et al, 1986). Syntactic dependency
trees encode syntactic subordination relationships
allowing the structure to be non-specific about the
underlying deep representation. The relationship
between a node and its subordinates expresses a
sense of syntactic (functional) entailment.
In this work we explore the dependency struc-
tures encoded in the Prague Dependency Treebank
(Hajic?, 1998; Bo?hmova? et al, 2002). The PDT 1.0
analytical layer is a set of Czech syntactic depen-
dency trees; the nodes of which contain the word
forms, morphological features, and syntactic anno-
tations. These trees were annotated by hand and
are intended as an intermediate stage in the annota-
tion of the Tectogrammatical Representation (TR),
a deep-syntactic or syntacto-semantic theory of lan-
guage (Sgall et al, 1986). All current automatic
techniques for generating TR structures are based on
syntactic dependency parsing.
When evaluating the correctness of dependency
trees, we only consider the structural relationships
between the words of the sentence (unlabeled depen-
dencies). However, the model we propose contains
features that are considered part of the dependency
rather than the nodes in isolation (e.g., agreement
features). We do not propose a model for correctly
labeling dependency structures in this work.
3 Constituency Parsing for Dependency
Trees
A pragmatic justification for using constituency-
based parsers in order to predict dependency struc-
tures is that currently the best Czech dependency-
tree parser is a constituency-based parser (Collins et
al., 1999; Zeman, 2004). In fact both Charniak?s
and Collins? generative probabilistic models con-
tain lexical dependency features.5 From a gener-
ative modeling perspective, we use the constraints
imposed by constituents (i.e., projectivity) to enable
the encapsulation of syntactic substructures. This di-
rectly leads to efficient parsing algorithms such as
the CKY algorithm and related agenda-based pars-
ing algorithms (Manning and Schu?tze, 1999). Addi-
tionally, this allows for the efficient computation of
the scores for the dynamic-programming state vari-
ables (i.e., the inside and outside probabilities) that
are used in efficient statistical parsers. The computa-
tional complexity advantages of dynamic program-
ming techniques along with efficient search tech-
niques (Caraballo and Charniak, 1998; Klein and
Manning, 2003) allow for richer predictive models
which include local contextual information.
In an attempt to extend a constituency-based pars-
ing model to train on dependency trees, Collins
transforms the PDT dependency trees into con-
stituency trees (Collins et al, 1999). In order to
accomplish this task, he first normalizes the trees
to remove non-projectivities. Then, he creates ar-
tificial constituents based on the parts-of-speech of
the words associated with each dependency node.
The mapping from dependency tree to constituency
tree is not one-to-one. Collins describes a heuristic
for choosing trees that work well with his parsing
model.
3.1 Training a Constituency-based Parser
We consider two approaches to creating projec-
tive trees from dependency trees exhibiting non-
projectivities. The first is based on word-reordering
and is the model that was used with the Collins
parser. This algorithm identifies non-projective
structures and deterministically reorders the words
of the sentence to create projective trees. An alter-
native method, used by Charniak in the adaptation
of his parser for Czech6 and used by Nivre and Nils-
son (2005), alters the dependency links by raising
the governor to a higher node in the tree whenever
5Bilexical dependencies are components of both the Collins
and Charniak parsers and effectively model the types of syntac-
tic subordination that we wish to extract in a dependency tree.
(Bilexical models were also proposed by Eisner (Eisner, 1996)).
In the absence of lexicalization, both parsers have dependency
features that are encoded as head-constituent to sibling features.
6This information was provided by Eugene Charniak in a
personal communication.
44
D
en
si
ty
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
0.005 0.006 0.02
0.843
0.084
0.023 0.009 0.005 0.004
less ?2 ?1 1 2 3 4 5 more
D
en
si
ty
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
0.005 0.006 0.022
0.824
0.092
0.029 0.012 0.005 0.005
less ?2 ?1 1 2 3 4 5 more
(a)Charniak (b)Collins
Figure 2: Statistical distribution of correct governor positions in the Charniak (left) and Collins (right) parser output of parsed
PDT development data.
a non-projectivity is observed. The trees are then
transformed into Penn Treebank style constituen-
cies using the technique described in (Collins et al,
1999).
Both of these techniques have advantages and dis-
advantages which we briefly outline here:
Reordering The dependency structure is preserved,
but the training procedure will learn statistics
for structures over word-strings that may not be
part of the language. The parser, however, may
be capable of constructing parses for any string
of words if a smoothed grammar is being used.
Governor?Raising The dependency structure is
corrupted leading the parser to incorporate ar-
bitrary dependency statistics into the model.
However, the parser is trained on true sen-
tences, the words of which are in the correct
linear order. We expect the parser to predict
similar incorrect dependencies when sentences
similar to the training data are observed.
Although the results presented in (Collins et al,
1999) used the reordering technique, we have exper-
imented with his parser using the governor?raising
technique and observe an increase in dependency ac-
curacy. For the remainder of the paper, we assume
the governor?raising technique.
The process of generating dependency trees from
parsed constituency trees is relatively straight-
forward. Both the Collins and Charniak parsers pro-
vide head-word annotation on each constituent. This
is precisely the information that we encode in an un-
labeled dependency tree, so the dependency struc-
ture can simply be extracted from the parsed con-
stituency trees. Furthermore, the constituency labels
can be used to identify the dependency labels; how-
ever, we do not attempt to identify correct depen-
dency labels in this work.
3.2 Constituency-based errors
We now discuss a quantitative measure for the types
of dependency errors made by constituency-based
parsing techniques. For node wi and the correct gov-
ernor wg?
i
the distance between the two nodes in the
hypothesized dependency tree is:
dist(wi, wg?
i
)
=
?
?
?
?
?
d(wi, wg?
i
) iff wg?
i
is ancestor of wi
d(wi, wg?
i
) iff wg?
i
is sibling/cousin of wi
?d(wi, wg?
i
) iff wg?
i
is descendant of wi
Ancestor, sibling, cousin, and descendant have the
standard interpretation in the context of a tree. The
dependency distance d(wi, wg?
i
) is the minimum
number of dependency links traversed on the undi-
rected path from wi to wg?
i
in the hypothesized de-
pendency tree. The definition of the dist function
makes a distinction between paths through the par-
ent of wi (positive values) and paths through chil-
45
CORRECT(W )
1 Parse sentence W using the constituency-based parser
2 Generate a dependency structure from the constituency tree
3 for wi ? W
4 do for wc ? N (wgh
i
) // Local neighborhood of proposed governor
5 do l(c) ? P (g?i = c|wi,N (wgh
i
))
6 g?i ? arg maxc l(c) // Pick the governor in which we are most confident
Table 1: Corrective Modeling Procedure
dren of wi (negative values). We found that a vast
majority of the correct governors were actually hy-
pothesized as siblings or grandparents (a dist values
of 2) ? an extreme local error.
Figure 2 shows a histogram of the fraction of
nodes whose correct governor was within a particu-
lar dist in the hypothesized tree. A dist of 1 indicates
the correct governor was selected by the parser; in
these graphs, the density at dist = 1 (on the x axis)
shows the baseline dependency accuracy of each
parser. Note that if we repaired only the nodes that
are within a dist of 2 (grandparents and siblings),
we can recover more than 50% of the incorrect de-
pendency links (a raw accuracy improvement of up
to 9%). We believe this distribution to be indirectly
caused by the governor raising projectivization rou-
tine. In the cases where non-projective structures
can be repaired by raising the node?s governor to its
parent, the correct governor becomes a sibling of the
node.
4 Corrective Modeling
The error analysis of the previous section suggests
that by looking only at a local neighborhood of the
proposed governor in the hypothesized trees, we can
correct many of the incorrect dependencies. This
fact motivates the corrective modeling procedure
employed here.
Table 1 presents the pseudo-code for the correc-
tive procedure. The set gh contains the indices of
governors as predicted by the parser. The set of gov-
ernors predicted by the corrective procedure is de-
noted as g? . The procedure independently corrects
each node of the parsed trees meaning that there
is potential for inconsistent governor relationships
to exist in the proposed set; specifically, the result-
ing dependency graph may have cycles. We em-
ploy a greedy search to remove cycles when they are
present in the output graph.
The final line of the algorithm picks the governor
in which we are most confident. We use the correct-
governor classification likelihood,
P (g?i = j|wi,N (wgh
i
)), as a measure of the confi-
dence that wc is the correct governor of wi where
the parser had proposed wgh
i
as the governor. In ef-
fect, we create a decision list using the most likely
decision if we can (i.e., there are no cycles). If the
dependency graph resulting from the most likely de-
cisions does not result in a tree, we use the decision
lists to greedily select the tree for which the product
of the independent decisions is maximal.
Training the corrective model requires pairs of
dependency trees; each pair contains a manually-
annotated tree (i.e., the gold standard tree) and a tree
generated by the parser. This data is trivially trans-
formed into per-node samples. For each node wi in
the tree, there are |N (wgh
i
)| samples; one for each
governor candidate in the local neighborhood.
One advantage to the type of corrective algorithm
presented here is that it is completely disconnected
from the parser used to generate the tree hypotheses.
This means that the original parser need not be sta-
tistical or even constituency based. What is critical
for this technique to work is that the distribution of
dependency errors be relatively local as is the case
with the errors made by the Charniak and Collins
parsers. This can be determined via data analysis
using the dist metric. Determining the size of the lo-
cal neighborhood is data dependent. If subordinate
nodes are considered as candidate governors, then a
more robust cycle removal technique is be required.
46
4.1 MaxEnt Estimation
We have chosen a MaxEnt model to estimate the
governor distributions, P (g?i = j|wi,N (wgh
i
)). In
the next section we outline the feature set with which
we have experimented, noting that the features are
selected based on linguistic intuition (specifically
for Czech). We choose not to factor the feature vec-
tor as it is not clear what constitutes a reasonable
factorization of these features. For this reason we
use the MaxEnt estimator which provides us with
the flexibility to incorporate interdependent features
independently while still optimizing for likelihood.
The maximum entropy principle states that we
wish to find an estimate of p(y|x) ? C that maxi-
mizes the entropy over a sample set X for some set
of observations Y , where x ? X is an observation
and y ? Y is a outcome label assigned to that obser-
vation,
H(p) ? ?
?
x?X,y?Y
p?(x)p(y|x) log p(y|x)
The set C is the candidate set of distributions from
which we wish to select p(y|x). We define this set
as the p(y|x) that meets a feature-based expectation
constraint. Specifically, we want the expected count
of a feature, f(x, y), to be equivalent under the dis-
tribution p(y|x) and under the observed distribution
p?(y|x).
?
x?X,y?Y
p?(x)p(y|x)fi(x, y)
=
?
x?X,y?Y
p?(x)p?(y|x)fi(x, y)
fi(x, y) is a feature of our model with which we
capture correlations between observations and out-
comes. In the following section, we describe a set of
features with which we have experimented to deter-
mine when a word is likely to be the correct governor
of another word.
We incorporate the expected feature-count con-
straints into the maximum entropy objective using
Lagrange multipliers (additionally, constraints are
added to ensure the distributions p(y|x) are consis-
tent probability distributions):
H(p)
+
?
i
?i
?
x?X,y?Y
(
p?(x)p(y|x)fi(x, y)
?p?(x)p?(y|x)fi(x, y)
)
+ ?
?
y?Y
p(y|x) ? 1
Holding the ?i?s constant, we compute the uncon-
strained maximum of the above Lagrangian form:
p?(y|x) =
1
Z?(x)
exp(
?
i
?ifi(x, y))
Z?(x) =
?
y?Y
exp(
?
i
?ifi(x, y))
giving us the log-linear form of the distributions
p(y|x) in C (Z is a normalization constant). Finally,
we compute the ?i?s that maximize the objective
function:
?
?
x?X
p?(x) log Z?(x) +
?
i
?ip?(x, y)fi(x, y)
A number of algorithms have been proposed to ef-
ficiently compute the optimization described in this
derivation. For a more detailed introduction to max-
imum entropy estimation see (Berger et al, 1996).
4.2 Proposed Model
Given the above formulation of the MaxEnt estima-
tion procedure, we define features over pairs of ob-
servations and outcomes. In our case, the observa-
tions are simply wi, wc, and N (wgh
i
) and the out-
come is a binary variable indicating whether c = g?i
(i.e., wc is the correct governor). In order to limit
the dimensionality of the feature space, we consider
feature functions over the outcome, the current node
wi, the candidate governor node wc and the node
proposed as the governor by the parser wgh
i
.
Table 2 describes the general classes of features
used. We write Fi to indicate the form of the current
child node, Fc for the form of the candidate, and Fg
as the form of the governor proposed by the parser.
A combined feature is denoted as LiTc and indicates
we observed a particular lemma for the current node
with a particular tag of the candidate.
47
Feature Type Id Description
Form F the fully inflected word form as it appears in the data
Lemma L the morphologically reduced lemma
MTag T a subset of the morphological tag as described in (Collins et al, 1999)
POS P major part-of-speech tag (first field of the morphological tag)
ParserGov G true if candidate was proposed as governor by parser
ChildCount C the number of children
Agreement A(x, y) check for case/number agreement between word x and y
Table 2: Description of the classes of features used
In all models, we include features containing the
form, the lemma, the morphological tag, and the
ParserGov feature. We have experimented with dif-
ferent sets of feature combinations. Each combina-
tion set is intended to capture some intuitive linguis-
tic correlation. For example, the feature component
LiTc will fire if a particular child?s lemma Li is ob-
served with a particular candidate?s morphological
tag Tc. This feature is intended to capture phenom-
ena surrounding particles; for example, in Czech,
the governor of the reflexive particle se will likely
be a verb.
4.3 Related Work
Recent work by Nivre and Nilsson introduces a tech-
nique where the projectivization transformation is
encoded in the non-terminals of constituents dur-
ing parsing (Nivre and Nilsson, 2005). This al-
lows for a deterministic procedure that undoes the
projectivization in the generated parse trees, creat-
ing non-projective structures. This technique could
be incorporated into a statistical parsing frame-
work, however we believe the sparsity of such non-
projective configurations may be problematic when
using smoothed backed-off grammars. We suspect
that the deterministic procedure employed by Nivre
and Nilsson enables their parser to greedily consider
non-projective constructions when possible. This
may also explain the relatively low overall perfor-
mance of their parser.
A primary difference between the Nivre and Nils-
son approach and what we propose in this paper is
that of determining the projectivization procedure.
While we exploit particular side-effects of the pro-
jectivization procedure, we do not assume any par-
ticular algorithm. Additionally, we consider trans-
formations for all dependency errors where their
technique explicitly addresses non-projectivity er-
rors.
We mentioned above that our approach appears to
be similar to that of reranking for statistical parsing
(Collins, 2000; Charniak and Johnson, 2005). While
it is true that we are improving upon the output of the
automatic parser, we are not considering multiple al-
ternate parses. Instead, we consider a complete set
of alternate trees that are minimal perturbations of
the best tree generated by the parser. In the context
of dependency parsing, we do this in order to gen-
erate structures that constituency-based parsers are
incapable of generating (i.e., non-projectivities).
Recent work by Smith and Eisner (2005) on con-
trastive estimation suggests similar techniques to
generate local neighborhoods of a parse; however,
the purpose in their work is to define an approxi-
mation to the partition function for log-linear esti-
mation (i.e., the normalization factor in a MaxEnt
model).
5 Empirical Results
In this section we report results from experiments on
the PDT Czech dataset. Approximately 1.9% of the
words? dependencies are non-projective in version
1.0 of this corpus and these occur in 23.2% of the
sentences (Hajic?ova? et al, 2004). We used the stan-
dard training, development, and evaluation datasets
defined in the PDT documentation for all experi-
ments.7 We use Zhang Lee?s implementation of the
7We have used PDT 1.0 (2002) data for the Charniak experi-
ments and PDT 2.0 (2005) data for the Collins experiments. We
use the most recent version of each parser; however we do not
have a training program for the Charniak parser and have used
the pretrained parser provided by Charniak; this was trained on
the training section of the PDT 1.0. We train our model on the
48
Model Features Description
Count ChildCount count of children for the three nodes
MTagL TiTc, LiLc, LiTc, TiLc, TiPg conjunctions of MTag and Lemmas
MTagF TiTc, FiFc, FiTc, TiFc, TiPg conjunctions of MTag and Forms
POSL Pi, Pc, Pg, PiPcPg, PiPg, PcLc conjunctions of POS and Lemma
TTT TiTcTg conjunction of tags for each of the three nodes
Agr A(Ti, Tc), A(Ti, Tg) binary feature if case/number agree
Trig LiLgTc, TiLgTc, LiLgLc trigrams of Lemma/Tag
Table 3: Model feature descriptions.
Model Charniak Parse Trees Collins Parse Trees
Devel. Accuracy NonP Accuracy Devel. Accuracy NonP Accuracy
Baseline 84.3% 15.9% 82.4% 12.0%
Simple 84.3% 16.0% 82.5% 12.2%
Simple + Count 84.3% 16.7% 82.5% 13.8%
Simple + MtagL 84.8% 43.5% 83.2% 44.1%
Simple + MtagF 84.8% 42.2% 83.2% 43.2%
Simple + POS 84.3% 16.0% 82.4% 12.1%
Simple + TTT 84.3% 16.0% 82.5% 12.2%
Simple + Agr 84.3% 16.2% 82.5% 12.2%
Simple + Trig 84.9% 47.9% 83.1% 47.7%
All Features 85.0% 51.9% 83.5% 57.5%
Table 4: Comparative results for different versions of our model on the Charniak and Collins parse trees for
the PDT development data.
MaxEnt estimator using the L-BFGS optimization
algorithms and Gaussian smoothing.8
Table 4 presents results on development data for
the correction model with different feature sets. The
features of the Simple model are the form (F),
lemma (L), and morphological tag (M) for the each
node, the parser-proposed governor node, and the
candidate node; this model also contains the Parser-
Gov feature. In the table?s following rows, we show
the results for the simple model augmented with fea-
ture sets of the categories described in Table 2. Ta-
ble 3 provides a short description of each of the mod-
els. As we believe the Simple model provides the
minimum information needed to perform this task,
Collins trees via a 20-fold Jackknife training procedure.
8Using held-out development data, we determined a Gaus-
sian prior parameter setting of 4 worked best. The optimal num-
ber of training iterations was chosen on held-out data for each
experiment. This was generally in the order of a couple hun-
dred iterations of L-BFGS. The MaxEnt modeling implemen-
tation can be found at http://homepages.inf.ed.ac.
uk/s0450736/maxent_toolkit.html.
we experimented with the feature-classes as addi-
tions to it. The final row of Table 4 contains results
for the model which includes all features from all
other models.
We define NonP Accuracy as the accuracy for
the nodes which were non-projective in the original
trees. Although both the Charniak and the Collins
parser can never produce non-projective trees, the
baseline NonP accuracy is greater than zero. This
is due to the parser making mistakes in the tree such
that the originally non-projective node?s dependency
is projective.
Alternatively, we report the Non-Projective Preci-
sion and Recall for our experiment suite in Table 5.
Here the numerator of the precision is the number
of nodes that are non-projective in the correct tree
and end up in a non-projective configuration; how-
ever, this new configuration may be based on incor-
rect dependencies. Recall is the obvious counterpart
to precision. These values correspond to the NonP
49
Model Charniak Parse Trees Collins Parse Trees
Precision Recall F-measure Precision Recall F-measure
Baseline N/A 0.0% 0.000 N/A 0.0% 0.000
Simple 22.6% 0.3% 0.592 5.0% 0.2% 0.385
Simple + Count 37.3% 1.1% 2.137 16.8% 2.0% 3.574
Simple + MtagL 78.0% 29.7% 43.020 62.4% 35.0% 44.846
Simple + MtagF 78.7% 28.6% 41.953 62.0% 34.3% 44.166
Simple + POS 23.3% 0.3% 0.592 2.5% 0.1% 0.192
Simple + TTT 20.7% 0.3% 0.591 6.1% 0.2% 0.387
Simple + Agr 40.0% 0.5% 0.988 5.7% 0.2% 0.386
Simple + Trig 74.6% 35.0% 47.646 52.3% 40.2% 45.459
All Features 75.7% 39.0% 51.479 48.1% 51.6% 49.789
Table 5: Alternative non-projectivity scores for different versions of our model on the Charniak and Collins
parse trees.
accuracy results reported in Table 4. From these ta-
bles, we see that the most effective features (when
used in isolation) are the conjunctive MTag/Lemma,
MTag/Form, and Trigram MTag/Lemma features.
Model Dependency NonP
Accuracy Accuracy
Collins 81.6% N/A
Collins + Corrective 82.8% 53.1%
Charniak 84.4% N/A
Charniak + Corrective 85.1% 53.9%
Table 6: Final results on PDT evaluation datasets
for Collins? and Charniak?s trees with and without
the corrective model
Finally, Table 6 shows the results of the full model
run on the evaluation data for the Collins and Char-
niak parse trees. It appears that the Charniak parser
fares better on the evaluation data than does the
Collins parser. However, the corrective model is
still successful at recovering non-projective struc-
tures. Overall, we see a significant improvement in
the dependency accuracy.
We have performed a review of the errors that
the corrective process makes and observed that the
model does a poor job dealing with punctuation.
This is shown in Table 7 along with other types of
nodes on which we performed well and poorly, re-
spectively. Collins (1999) explicitly added features
to his parser to improve punctuation dependency
parsing accuracy. The PARSEVAL evaluation met-
Top Five Good/Bad Repairs
Well repaired child se i si az? jen
Well repaired false governor v vs?ak li na o
Well repaired real governor a je sta?t ba ,
Poorly repaired child , se na z?e -
Poorly repaired false governor a , vs?ak mus?? li
Poorly repaired real governor root sklo , je -
Table 7: Categorization of corrections and errors
made by our model on trees from the Charniak
parser. root is the artificial root node of the PDT
tree. For each node position (child, proposed parent,
and correct parent), the top five words are reported
(based on absolute count of occurrences). The par-
ticle ?se? occurs frequently explaining why it occurs
in the top five good and top five bad repairs.
Charniak Collins
Correct to incorrect 13.0% 20.0%
Incorrect to incorrect 21.6% 25.8%
Incorrect to correct 65.5% 54.1%
Table 8: Categorization of corrections made by our
model on Charniak and Collins trees.
ric for constituency-based parsing explicitly ignores
punctuation in determining the correct boundaries of
constituents (Harrison et al, 1991) and so should the
dependency evaluation. However, the reported re-
sults include punctuation for comparative purposes.
Finally, we show in Table 8 a coarse analysis of the
corrective performance of our model. We are repair-
50
ing more dependencies than we are corrupting.
6 Conclusion
We have presented a Maximum Entropy-based cor-
rective model for dependency parsing. The goal is
to recover non-projective dependency structures that
are lost when using state-of-the-art constituency-
based parsers; we show that our technique recovers
over 50% of these dependencies. Our algorithm pro-
vides a simple framework for corrective modeling
of dependency trees, making no prior assumptions
about the trees. However, in the current model, we
focus on trees with local errors. Overall, our tech-
nique improves dependency parsing and provides
the necessary mechanism to recover non-projective
structures.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and
Barbora Vidova? Hladka?. 2002. The prague depen-
dency treebank: Three-level annotation scenario. In
Anne Abeille, editor, In Treebanks: Building and
Using Syntactically Annotated Corpora. Dordrecht,
Kluwer Academic Publishers, The Neterlands.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational Lin-
guistics, 21(4):543?565, December.
Sharon Caraballo and Eugene Charniak. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
Computational Linguistics, 24(2):275?298, June.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the Association for Computational Linguis-
tics.
Michael Collins, Lance Ramshaw, Jan Hajic?, and
Christoph Tillmann. 1999. A statistical parser for
czech. In Proceedings of the 37th annual meeting of
the Association for Computational Linguistics, pages
505?512.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Proceedings of the 17th In-
ternational Conference on Machine Learning 2000.
Michael Collins. 2003. Head-driven statistical models
for natural language processing. Computational Lin-
guistics, 29(4):589?637.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Sapporo.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Compu-
tational Linguistics (COLING), pages 340?345.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova? Hladka?.
2005. The prague dependency treebank 2.0.
http://ufal.mff.cuni.cz/pdt2.0.
51
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The prague dependency treebank. In Issues
of Valency and Meaning, pages 106?132. Karolinum,
Praha.
Eva Hajic?ova?, Jir??? Havelka, Petr Sgall, Kater?ina Vesela?,
and Daniel Zeman. 2004. Issues of projectivity in
the prague dependency treebank. Prague Bulletin of
Mathematical Linguistics, 81:5?22.
P. Harrison, S. Abney, D. Fleckenger, C. Gdaniec, R. Gr-
ishman, D. Hindle, B. Ingria, M. Marcus, B. Santorini,
, and T. Strzalkowski. 1991. Evaluating syntax perfor-
mance of parser/grammars of english. In Proceedings
of the Workshop on Evaluating Natural Language Pro-
cessing Systems, ACL.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Factored
A* search for models over sequences and trees. In
Proceedings of IJCAI 2003.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Cor-
recting the surface dependency approximation. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 327?334,
Barcelona, Spain.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of statistical natural language process-
ing. MIT Press.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany, NY.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 99?106, Ann Arbor.
Brian Roark and Michael Collins. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting of the Association for
Computational Linguistics, Barcelona.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. Kluwer Academic, Boston.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the Association for Computational
Linguistics (ACL 2005), Ann Arbor, Michigan.
Daniel Zeman. 2004. Parsing with a statistical de-
pendency model. Ph.D. thesis, Charles University in
Prague.
52
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 78?85,
Sydney, July 2006. c?2006 Association for Computational Linguistics
On Distance between Deep Syntax and Semantic Representation
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University
Praha, Czech Republic
novak@ufal.mff.cuni.cz
Abstract
We present a comparison of two for-
malisms for representing natural language
utterances, namely deep syntactical Tec-
togrammatical Layer of Functional Gen-
erative Description (FGD) and a seman-
tic formalism, MultiNet. We discuss the
possible position of MultiNet in the FGD
framework and present a preliminary map-
ping of representational means of these
two formalisms.
1 Introduction
The Prague Dependency Treebank 2.0 (PDT 2.0)
described in Sgall et al (2004) contains a large
amount of Czech texts with complex and inter-
linked morphological (2 million words), syntactic
(1.5M words), and complex semantic (tectogram-
matical) annotation (0.8M words); in addition,
certain properties of sentence information struc-
ture and coreference relations are annotated at the
semantic level.
The theoretical basis of the treebank lies in the
Functional Generative Description (FGD) of lan-
guage system by Sgall et al (1986).
PDT 2.0 is based on the long-standing Praguian
linguistic tradition, adapted for the current
computational-linguistics research needs. The
corpus itself is embedded into the latest annotation
technology. Software tools for corpus search, an-
notation, and language analysis are included. Ex-
tensive documentation (in English) is provided as
well.
An example of a tectogrammatical tree from
PDT 2.0 is given in figure 1. Function words are
removed, their function is preserved in node at-
tributes (grammatemes), information structure is
annotated in terms of topic-focus articulation, and
every node receives detailed semantic label corre-
sponding to its function in the utterance (e.g., ad-
dressee, from where, how often, . . . ). The square
node indicates an obligatory but missing valent.
The tree represents the following sentence:
Letos
 !!
CC
C se
<
<<
snaz???


o

na?vrat

do

politiky.
This year he tries to return to politics.
(1)
_ .
 
          
_
 
. . .
.
_
 . .
. . .
_
 
.
_ .
 
.
t-ln94200-123-p12s3
root
letos
t TWHEN basic
adv.denot.ngrad.nneg
#PersPron
t ACT
n.pron.def.pers
anim sg 3 basic
sna?it_se enunc
f PRED
v decl disp0 ind
proc it0 res0 sim
n?vrat
f PAT
n.denot
inan sg
politika
f DIR3 basic
n.denot
fem sg
Figure 1: Tectogrammatical tree of sentence (1)
1.1 MultiNet
The representational means of Multilayered Ex-
tended Semantic Networks (MultiNet), which are
78
described in Helbig (2006), provide a universally
applicable formalism for treatment of semantic
phenomena of natural language. To this end, they
offer distinct advantages over the use of the clas-
sical predicate calculus and its derivatives. The
knowledge representation paradigm and semantic
formalism MultiNet is used as a common back-
bone for all aspects of natural language process-
ing (be they theoretical or practical ones). It is
continually used for the development of intelligent
information and communication systems and for
natural language interfaces to the Internet. Within
this framework, it is subject to permanent practical
evaluation and further development.
The semantic representation of natural language
expressions by means of MultiNet is mainly in-
dependent of the considered language. In con-
trast, the syntactic constructs used in different
languages to describe the same content are ob-
viously not identical. To bridge the gap be-
tween different languages we can employ the deep
syntactico-semantic representation available in the
FGD framework.
An example of a MultiNet structure is given in
figure 2. The figure represents the following dis-
course:
Max gave his brother several apples.
This was a generous gift.
Four of them were rotten. (2)
MultiNet is not explicitly model-theoretical and
the extensional level is created only in those situ-
ations where the natural language expressions re-
quire it. It can be seen that the overall structure
of the representation is not a tree unlike in Tec-
togrammatical representation (TR). The layer in-
formation is hidden except for the most important
QUANT and CARD values. These attributes con-
vey information that is important with respect to
the content of the sentence. TR lacks attributes
distinguishing intensional and extensional infor-
mation and there are no relations like SUBM de-
noting relation between a set and its subset.
Note that the MultiNet representation crosses
the sentence boundaries. First, the structure repre-
senting a sentence is created and then this structure
is assimilated into the existing representation.
In contrast to CLASSIC (Brachman et al, 1991)
and other KL-ONE networks, MultiNet contains a
predefined final set of relation types, encapsula-
tion of concepts, and attribute layers concerning
cardinality of objects mentioned in discourse.
In Section 2, we describe our motivation for ex-
tending the annotation in FGD to an even deeper
level. Section 3 lists the MultiNet structural coun-
terparts of tectogrammatical means. We discuss
the related work in Section 4. Section 5 deals with
various evaluation techniques and we conclude in
Section 6.
2 FGD layers
PDT 2.0 contains three layers of information about
the text (as described in Hajic? (1998)):
Morphosyntactic Tagging. This layer represents
the text in the original linear word order with
a tag assigned unambiguously to each word
form occurence, much like the Brown corpus
does.
Syntactic Dependency Annotation. It contains
the (unambiguous) dependency representa-
tion of every sentence, with features describ-
ing the morphosyntactic properties, the syn-
tactic function, and the lexical unit itself. All
words from the sentence appear in its repre-
sentation.
Tectogrammatical Representation (TR). At
this level of description, we annotate every
(autosemantic non-auxiliary) lexical unit
with its tectogrammatical function, position
in the scale of the communicative dynamism
and its grammatemes (similar to the mor-
phosyntactic tag, but only for categories
which cannot be derived from the word?s
function, like number for nouns, but not its
case).
There are several reasons why TR may not be
sufficient in a question answering system or MT:
1. The syntactic functors Actor and Patient dis-
allow creating inference rules for cognitive
roles like Affected object or State carrier. For
example, the axiom stating that an affected
object is changed by the event ((v AFF o) ?
(v SUBS change.2.1)) can not be used
in the TR framework.
2. There is no information about sorts of con-
cepts represented by TR nodes. Sorts (the
upper conceptual ontology) are an important
source of constraints for MultiNet relations.
Every relation has its signature which in turn
79
Figure 2: MultiNet representation of example discourse (2)
reduces ambiguity in the process of text anal-
ysis and inferencing.
3. Lexemes of TR have no hierarchy which lim-
its especially the search for an answer in a
question answering system. In TR there is
no counterpart of SUB, SUBR, and SUBS
MultiNet relations which connect subordi-
nate concepts to superordinate ones and indi-
vidual object representatves to corresponding
generic concepts.
4. In TR, each sentence is isolated from the
rest of the text, except for coreference arrows
heading to preceding sentences. This, in ef-
fect, disallows inferences combining knowl-
edge from multiple sentences in one infer-
ence rule.
5. Nodes in TR always correspond to a word
or a group of words in the surface form of
sentence or to a deleted obligatory valency
of another node. There are no means for
representing knowledge generated during the
inference process, if the knowledge doesn?t
have a form of TR. For example, consider ax-
iom of temporal precedence transitivity (3):
(a ANTE b) ? (b ANTE c) ? (a ANTE c)
(3)
In TR, we can not add an edge denoting
(a ANTE c). We would have to include a
proposition like ?a precedes c? as a whole
new clause.
For all these reasons we need to extend our text
annotation to a form suitable to more advanced
tasks. It is shown in Helbig (2006) that MultiNet
is capable to solve all the above mentioned issues.
Helbig (1986) describes a procedure for auto-
matic translation of natural language utterances
into MultiNet structures used in WOCADI tool for
German. WOCADI uses no theoretical intermedi-
ate structures and relies heavily on semantically
annotated dictionary (HagenLex, see Hartrumpf et
al. (2003)).
In our approach, we want to take advantage of
existing tools for conversions between layers in
FGD. By combining several simpler procedures
for translation between adjacent layers, we can im-
prove the robustness of the whole procedure and
the modularity of the software tools. Moreover,
the process is divided to logical steps correspond-
ing to theoretically sound and well defined struc-
tures. On the other hand, such a multistage pro-
cessing is susceptible to accumulation of errors
made by individual components.
3 Structural Similarities
3.1 Nodes and Concepts
If we look at examples of TR and MultiNet struc-
tures, at first sight we can see that the nodes of
TR mostly correspond to concepts in MultiNet.
However, there is a major difference: TR does not
include the concept encapsulation. The encapsu-
lation in MultiNet serves for distinguishing def-
initional knowledge from assertional knowledge
about given node, e.g., in the sentence ?The old
man is sleeping?, the connection to old will be in
the definitional part of man, while the connection
to the state is sleeping belongs to the assertional
80
part of the concept representing the man. In TR,
these differences in content are represented by dif-
ferences in Topic-Focus Articulation (TFA) of cor-
responding words.
There are also TR nodes that correspond to no
MultiNet concept (typically, the node representing
the verb ?be?) and TR nodes corresponding to a
whole subnetwork, e.g., Fred in the sentence ?Fred
is going home.?, where the TR node representing
Fred corresponds to the subnetwork1 in figure 3.
SUB
human
ATTR
SUB
first name
VAL
fred
G01
Figure 3: The MultiNet subnetwork correspond-
ing to TR node representing Fred
3.2 Edges, relations and functions
An edge of TR between nodes that have their
conceptual counterparts in MultiNet alays corre-
sponds to one or more relations and possibly also
some functions. In general, it can be said that
MultiNet representation of a text contains signif-
icantly more connections (either as relations, or as
functions) than TR, and some of them correspond
to TR edges.
3.3 Functors and types of relations and
functions
There are 67 functor types in TR (see Hajic?ova?
et al (2000) for description), which correspond to
94 relation types and 19 function types in Multi-
Net (Helbig, 2006). The mapping of TR functions
to MultiNet is given in table 1:
TR functor MultiNet counterpart
ACMP ASSOC
ACT AFF, AGT, BENF, CSTR, EXP,
MEXP, SCAR
ADDR ORNT
ADVS SUBST, OPPOS
AIM PURP
APP ASSOC, ATTCH
continued . . .
1In fact the concept representing the man is the concept
G01, i.e. only one vertex. However, the whole network cor-
responds to the TR node representing Fred.
TR functor MultiNet counterpart
APPS EQU, NAME
ATT MODL
AUTH AGT, ORIG
BEN BENF
CAUS CAUS, JUST
CNCS CONC
CM *ITMS, MODL
COMPL PROP except for sentential com-
plements
COND COND
CONFR OPPOS
CONJ *IMTS-I, *TUPL
CONTRA OPPOS
CONTRD CONC
CPR *COMP
CRIT METH, JUST, CIRC, CONF
CSQ CAUS, JUST, GOAL
DIFF *MODP, *OP
DIR1 ORIGL, ORIG
DIR2 VIA
DIR3 DIRCL, ELMT
DISJ *ALTN2, *VEL2
EFF MCONT, PROP, RSLT
EXT QMOD
HER AVRT
ID NAME
INTT PURP
LOC LOC, LEXT
MANN MANNR, METH
MAT ORIGM
MEANS MODE, INSTR
MOD MODL
OPER *OP, TEMP
ORIG AVRT, INIT, ORIGM, ORIGL,
ORIG
PARTL MODL
PAT AFF, ATTR, BENF, ELMT,
GOAL, OBJ, PARS, PROP,
SSPE, VAL
PREC REAS, OPPOS
REAS CAUS, GOAL
REG CONF
RESL CAUS, GOAL
RESTR *DIFF
RHEM MODL
RSTR PROP, ATTR
SUBS SUBST
continued . . .
81
TR functor MultiNet counterpart
TFHL DUR
TFRWH TEMP
THL DUR
THO QUANT layer
TOWH SUBST, TEMP
TPAR TEMP, DUR
TSIN STRT
TTILL FIN
TWHEN TEMP
Table 1: Mapping of TR functors to MultiNet
There are also TR functors with no appropriate
MultiNet counterpart: CPHR, DENOM, DPHR,
FPHR, GRAD, INTF, PAR, PRED and VOCAT
Table 2 shows the mapping from MultiNet rela-
tions to TR functors:
MultiNet TR counterpart
Relations:
AFF PAT, DIR1
AGT ACT
ANTE TWHEN
ARG1/2/3 ACT, PAT, . . .
ASSOC ACMP, APP
ATTCH APP
ATTR RSTR
AVRT ORIG, ADDR, DIR1
BENF BEN
CAUS CAUS, RESL, REAS, GOAL
CIRC CRIT
CONC CNCS
COND COND
CONF REG, CRIT
CSTR ACT
CTXT REG
DIRCL DIR3
DUR TFHL, PAR, THL
ELMT DIR3, DIR1
EXP ACT
FIN TTILL
GOAL see RSLT, DIRCL and PURP
IMPL CAUS
INIT ORIG
INSTR MEANS
JUST CAUS
LEXT LOC
LOC LOC
MANNR MANN
continued . . .
MultiNet TR counterpart
MCONT PAT, EFF
MERO see PARS, ORIGM, *ELMT,
*SUBM and TEMP
METH MANN, CRIT
MEXP ACT
MODE see INSTR, METH and
MANNR
MODL MOD, ATT, PARTL, RHEM
NAME ID, APPS
OBJ PAT
OPPOS CONTRA
ORIG ORIG, DIR1, AUTH
ORIGL DIR1
ORIGM ORIG
ORNT ADDR
PROP COMPL, RSTR
PROPR COMPL, RSTR
PURP AIM
QMOD RSTR
REAS see CAUS, JUST and IMPL
RPRS LOC, MANN
RSLT PAT, EFF
SCAR ACT
SITU see CIRC and CTXT
SOURC see INIT, ORIG, ORIGL,
ORIGM and AVRT
SSPE PAT
STRT TSIN
SUBST SUBS
SUPPL PAT
TEMP TWHEN
VAL RSTR, PAT
VIA DIR2
Functions:
?ALTN1 CONJ
?ALTN1 DISJ
?COMP CPR, grammateme DEGCMP
?DIFF RESTR
?INTSC CONJ
?ITMS CONJ
?MODP MANN
?MODQ RHEM
?MODS MANNR
?NON grammateme NEGATION
?ORD grammateme NUMERTYPE
?PMOD RSTR
?QUANT MAT, RSTR
continued . . .
82
MultiNet TR counterpart
?SUPL grammateme DEGCMP
?TUPL CONJ
?UNION CONJ
?VEL1 CONJ
?VEL2 DISJ
Table 2: Mapping of MultiNet relations to TR
There are also MultiNet relations and functions
with no counterpart in TR (stars at the begin-
ning denote a function): ANLG, ANTO, CHEA,
CHPA, CHPE, CHPS, CHSA CHSP, CNVRS,
COMPL, CONTR, CORR, DISTG, DPND, EQU,
EXT, HSIT, MAJ, MIN, PARS, POSS, PRED0,
PRED, PREDR, PREDS, SETOF, SUB, SYNO,
VALR, *FLPJ and *OP.
From the tables 1 and 2, we can conclude that
although the mapping is not one to one, the prepro-
cessing of the input text to TR highly reduces the
problem of the appropriate text to MultiNet trans-
formation. However, it is not clear how to solve
the remaining ambiguity.
3.4 Grammatemes and layer information
TR has at its disposal 15 grammatemes, which
can be conceived as node attributes. Note that
not all grammatemes are applicable to all nodes.
The grammatemes in TR roughly correspond to
layer information in MultiNet, but also to specific
MultiNet relations.
1. NUMBER. This TR grammateme is trans-
formed to QUANT, CARD, and ETYPE at-
tributes in MultiNet.
2. GENDER. This syntactical information is not
transformed to the semantic representation
with the exception of occurences where the
grammateme distinguishes the gender of an
animal or a person and where MultiNet uses
SUB relation with appropriate concepts.
3. PERSON. This verbal grammateme is re-
flected in cognitive roles connected to the
event or state and is semantically superfluous.
4. POLITENESS has no structural counterpart
in MultiNet. It can be represented in the con-
ceptual hierarchy of SUB relation.
5. NUMERTYPE distinguishing e.g. ?three?
from ?third? and ?one third? is transformed to
corresponding number and also to the manner
this number is connected to the network.
6. INDEFTYPE corresponds to QUANT and
VARIA layer attributes.
7. NEGATION is transformed to both FACT
layer attribute and *NON function combined
with modality relation.
8. DEGCMP corresponds to *COMP and
*SUPL functions.
9. VERBMOD: imp value is represented by
MODL relation to imperative, cdn value is
ambiguous not only with respect to facticity
of the condition but also with regard to other
criteria distinguishing CAUS, IMPL, JUST
and COND relatinos which can all result in
a sentence with cdn verb. Also the FACT
layer attribute of several concepts is affected
by this value.
10. DEONTMOD corresponds to MODL rela-
tion.
11. DISPMOD is semantically superfluous.
12. ASPECT has no direct counterpart in Multi-
Net. It can be represented by the interplay
of temporal specification and RSLT relation
connecting an action to its result.
13. TENSE is represented by relations ANTE,
TEMP, DUR, STRT, and FIN.
14. RESULTATIVE has no direct counterpart
and must be expressed using the RSLT rela-
tion.
15. ITERATIVENESS should be represented by
a combination of DUR and TEMP rela-
tions where some of temporal concepts have
QUANT layer information set to several.
3.5 TFA, quantifiers, and encapsulation
In TR, the information structure of every utterance
is annotated in terms of Topic-Focus Articulation
(TFA):
1. Every autosemantic word is marked c, t, or
f for contrastive topic, topic, or focus, re-
spectively. The values can distinguish which
part of the sentence belongs to topic and
which part to focus.
2. There is an ordering of all nodes according to
communicative dynamism (CD). Nodes with
lower values of CD belong to topic and nodes
83
with greater values to focus. In this way, the
degree of ?aboutness? is distinguished even
inside topic and focus of sentences.
MultiNet, on the other hand, doesn?t contain
any representational means devoted directly to
representation of information structure. Neverthe-
less, the differences in the content of sentences dif-
fering only in TFA can be represented in MultiNet
by other means. The TFA differences can be re-
flected in these categories:
? Relations connecting the topic of sentence
with the remaining concepts in the sentence
are usually a part of definitional knowledge
about the concepts in the topic, while the re-
lations going to the focus belong to the asser-
tional part of knowledge about the concepts
in focus. In other words, TFA can be reflected
in different values of K TYPE attribute.
? TFA has an effect on the identification of
presuppositions (Peregrin, 1995a) and allega-
tions (Hajic?ova?, 1984). In case of presuppo-
sition, we need to know about them in the
process of assimilation of new information
into the existing network in order to detect
presupposition failures. In case of allegation,
there is a difference in FACT attribute of the
allegation.
? The TFA has an influence on the scope of
quantifiers (Peregrin, 1995b; Hajic?ova? et al,
1998). This information is fully transformed
into the quantifier scopes in MultiNet.
4 Related Work
There are various approaches trying to analyze
text to a semantic representation. Some of them
use layered approach and others use only a sin-
gle tool to directly produce the target struc-
ture. For German, there is the above mentioned
WOCADI parser to MultiNet, for English, there
is a Discourse Representation Theory (DRT) ana-
lyzer (Bos, 2005), and for Czech there is a Trans-
parent Intensional Logic analyzer (Hora?k, 2001).
The layered approaches: DeepThought
project (Callmeier et al, 2004) can combine
output of various tools into one representation.
It would be even possible to incorporate TR and
MultiNet into this framework. Meaning-Text
Theory (Bolshakov and Gelbukh, 2000) uses
an approach similar to Functional Generative
Description (Z?abokrtsky?, 2005) but it also has no
layer corresponding to MultiNet.
There were attempts to analyze the seman-
tics of TR, namely in question answering system
TIBAQ (Jirku? and Hajic?, 1982), which used TR di-
rectly as the semantic representation, and Kruijff-
Korbayova? (1998), who tried to transform the TFA
information in TR into the DRT framework.
5 Evaluation
It is a still open question how to evaluate systems
for semantic representation. Basically, three ap-
proaches are used in similar projects:
First, the coverage of the system may serve as a
basis for evaluation. This criterion is used in sev-
eral systems (Bos, 2005; Hora?k, 2001; Callmeier
et al, 2004). However, this criterion is far from
ideal, because it?s not applicable to robust systems
and can not tell anything about the quality of re-
sulting representation.
Second, the consistency of the semantic repre-
sentation serves as an evaluation criterion in Bos
(2005). It is a desired state to have a consistent
representation of texts, but there is no guarantee
that a consistent semantic representation is in any
sense also a good one.
Third, the performance in an application
(e.g., question answering system) is another cri-
terion used for evaluating a semantic representa-
tion (Hartrumpf, 2005). A problem in this kind
of evaluation is that we can not separate the eval-
uation of the formalism itself from the evaluation
of the automatic processing tools. This problem
becomes even bigger in a multilayered approach
like FGD or MTT, where the overall performance
depends on all participating transducers as well as
on the quality of the theoretical description. How-
ever, from the user point of view, this is so far
the most reliable form of semantic representation
evaluation.
6 Conclusion
We have presented an outline of a procedure that
enables us to transform syntactical (tectogrammat-
ical) structures into a fully equipped knowledge
representation framework. We have compared
the structural properties of TR and MultiNet and
found both similarities and differences suggest-
ing which parts of such a task are more difficult
and which are rather technical. The comparison
shows that for applications requiring understand-
84
ing of texts (e.g., question answering system) it is
desirable to further analyze TR into another layer
of knowledge representation.
Acknowledgement
This work was supported by Czech Academy
of Science grant 1ET201120505 and by Czech
Ministry of Education, Youth and Sports project
LC536. The views expressed are not necessarily
endorsed by the sponsors. We also thank anony-
mous reviewers for improvements in the final ver-
sion.
References
Igor Bolshakov and Alexander Gelbukh. 2000. TheMeaning-Text Model: Thirty Years After. Interna-
tional Forum on Information and Documentation,1:10?16.
Johan Bos. 2005. Towards Wide-Coverage Se-
mantic Interpretation. In Proceedings of Sixth In-
ternational Workshop on Computational Semantics
IWCS-6, pages 42?53.
Ronald J. Brachman, Deborah L. McGuinness, Pe-ter F. Patel-Schneider, Lori Alperin Resnick, and
Alex Borgida. 1991. Living with CLASSIC: Whenand How to Use a KL-ONE-like Language. In JohnSowa, editor, Principles of Semantic Networks: Ex-
plorations in the representation of knowledge, pages401?456. Morgan-Kaufmann, San Mateo, Califor-nia.
Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, andMelanie Siegel. 2004. The DeepThought Core Ar-
chitecture Framework. In Proceedings of LREC,May.
Jan Hajic?. 1998. Building a Syntactically Anno-tated Corpus: The Prague Dependency Treebank. In
E. Hajic?ova?, editor, Issues of Valency and Meaning.
Studies in Honour of Jarmila Panevova?, pages 106?132. Karolinum, Charles University Press, Prague,
Czech Republic.
Eva Hajic?ova?, Jarmila Panevova?, and Petr Sgall.
2000. A Manual for Tectogrammatic Tagging ofthe Prague Dependency Treebank. Technical Re-
port TR-2000-09, U?FAL MFF UK, Prague, CzechRepublic. in Czech.
Eva Hajic?ova?, Petr Sgall, and Barbara Partee. 1998.
Topic-Focus Articulation, Tripartite Structures, and
Semantic Content. Kluwer, Dordrecht.
Eva Hajic?ova?. 1984. Presupposition and AllegationRevisited. Journal of Pragmatics, 8:155?167.
Sven Hartrumpf, Hermann Helbig, and Rainer Oss-wald. 2003. The Semantically Based Computer
Lexicon HaGenLex ? Structure and TechnologicalEnvironment. Traitement automatique des langues,44(2):81?105.
Sven Hartrumpf. 2005. University of hagen at qa@clef
2005: Extending knowledge and deepening linguis-tic processing for question answering. In CarolPeters, editor, Results of the CLEF 2005 Cross-
Language System Evaluation Campaign, Work-
ing Notes for the CLEF 2005 Workshop, Wien,O?sterreich. Centromedia.
Hermann Helbig. 1986. Syntactic-Semantic Analy-sis of Natural Language by a New Word-Class Con-
trolled Functional Analysis. Computers and Artifi-
cial Inteligence, 5(1):53?59.
Hermann Helbig. 2006. Knowledge Representation
and the Semantics of Natural Language. Springer-
Verlag, Berlin Heidelberg.
Ales? Hora?k. 2001. The Normal Translation Algorithm
in Transparent Intensional Logic for Czech. Ph.D.thesis, Faculty of Informatics, Masaryk University,Brno, Czech Republic.
Petr Jirku? and Jan Hajic?. 1982. Inferencing and search
for an answer in TIBAQ. In Proceedings of the 9th
conference on Computational linguistics ? Volume
2, pages 139?141, Prague, Czechoslovakia.
Ivana Kruijff-Korbayova?. 1998. The Dynamic Po-
tential of Topic and Focus: A Praguian Approach
to Discourse Representation Theory. Ph.D. thesis,
U?FAL, MFF UK, Prague, Czech Republic.
Jaroslav Peregrin. 1995a. Topic, Focus and the Logicof Language. In Sprachtheoretische Grundlagen fu?r
die Computerlinguistik (Proceedings of the Goettin-
gen Focus Workshop, 17. DGfS), Heidelberg. IBMDeutschland.
Jaroslav Peregrin. 1995b. Topic-Focus Articulationas Generalized Quantification. In P. Bosch and
R. van der Sandt, editors, Proceedings of ?Focus and
natural language processing?, pages 49?57, Heidel-berg. IBM Deutschland.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing company,Dodrecht, Boston, London.
Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?. 2004.Deep Syntactic Annotation: Tectogrammatical Rep-
resentation and Beyond. In A. Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32?38, Boston,
Massachusetts, USA. Association for Computa-tional Linguistics.
Zdene?k Z?abokrtsky?. 2005. Resemblances between
Meaning-Text Theory and Functional GenerativeDescription. In Proceedings of the 2nd Interna-
tional Conference of Meaning-Text Theory, pages
549?557.
85
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125?129,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
English-Czech MT in 2008 ?
Ondr?ej Bojar, David Marec?ek, Va?clav Nova?k, Martin Popel,
Jan Pta?c?ek, Jan Rous?, Zdene?k ?Zabokrtsky?
Charles University, Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{bojar,marecek,novak,ptacek,zabokrtsky}@ufal.mff.cuni.cz
{popel,jan.rous}@matfyz.cz
Abstract
We describe two systems for English-to-
Czech machine translation that took part
in the WMT09 translation task. One of
the systems is a tuned phrase-based system
and the other one is based on a linguisti-
cally motivated analysis-transfer-synthesis
approach.
1 Introduction
We participated in WMT09 with two very dif-
ferent systems: (1) a phrase-based MT based
on Moses (Koehn et al, 2007) and tuned for
English?Czech translation, and (2) a complex
system in the TectoMT platform ( ?Zabokrtsky? et
al., 2008).
2 Data
2.1 Monolingual Data
Our Czech monolingual data consist of (1)
the Czech National Corpus (CNC, versions
SYN200[056], 72.6%, Kocek et al (2000)), (2)
a collection of web pages downloaded by Pavel
Pecina (Web, 17.1%), and (3) the Czech mono-
lingual data provided by WMT09 organizers
(10.3%). Table 1 lists sentence and token counts
(see Section 2.3 for the explanation of a- and t-
layer).
Sentences 52 M
with nonempty t-layer 51 M
a-nodes (i.e. tokens) 0.9 G
t-nodes 0.6 G
Table 1: Czech monolingual training data.
? The work on this project was supported by the grants
MSM0021620838, 1ET201120505, 1ET101120503, GAUK
52408/2008, M?SMT ?CR LC536 and FP6-IST-5-034291-STP
(EuroMatrix).
2.2 Parallel Data
As the source of parallel data we use an internal
release of Czech-English parallel corpus CzEng
(Bojar et al, 2008) extended with some additional
texts. One of the added sections was gathered
from two major websites containing Czech sub-
titles to movies and TV series1. The matching of
the Czech and English movies is rather straight-
forward thanks to the naming conventions. How-
ever, we were unable to reliably determine the se-
ries number and the episode number from the file
names. We employed a two-step procedure to au-
tomatically pair the TV series subtitle files. For
every TV series:
1. We clustered the files on both sides to remove
duplicates
2. We found the best matching using a provi-
sional translation dictionary. This proved to
be a successful technique on a small sample
of manually paired test data. The process was
facilitated by the fact that the correct pairs of
episodes usually share some named entities
which the human translator chose to keep in
the original English form.
Table 2 lists parallel corpus sizes and the distri-
bution of text domains.
English Czech
Sentences 6.91 M
with nonempty t-layer 6.89 M
a-nodes (i.e. tokens) 61 M 50 M
t-nodes 41 M 33 M
Distribution: [%] [%]
Subtitles 68.2 Novels 3.3
Software Docs 17.0 Commentaries/News 1.5
EU (Legal) Texts 9.5 Volunteer-supplied 0.4
Table 2: Czech-English data sizes and sources.
1www.opensubtitles.org and titulky.com
125
2.3 Data Preprocessing using TectoMT
platform: Analysis and Alignment
As we believe that various kinds of linguistically
relevant information might be helpful in MT, we
performed automatic analysis of the data. The
data were analyzed using the layered annotation
scheme of the Prague Dependency Treebank 2.0
(PDT 2.0, Hajic? and others (2006)), i.e. we used
three layers of sentence representation: morpho-
logical layer, surface-syntax layer (called analyti-
cal (a-) layer), and deep-syntax layer (called tec-
togrammatical (t-) layer).
The analysis was implemented using TectoMT,
( ?Zabokrtsky? et al, 2008). TectoMT is a highly
modular software framework aimed at creating
MT systems (focused, but by far not limited to
translation using tectogrammatical transfer) and
other NLP applications. Numerous existing NLP
tools such as taggers, parsers, and named entity
recognizers are already integrated in TectoMT, es-
pecially for (but again, not limited to) English and
Czech.
During the analysis of the large Czech mono-
lingual data, we used Jan Hajic??s Czech tagger
shipped with PDT 2.0, Maximum Spanning Tree
parser (McDonald et al, 2005) with optimized set
of features as described in Nova?k and ?Zabokrtsky?
(2007), and a tool for assigning functors (seman-
tic roles) from Klimes? (2006), and numerous other
components of our own (e.g. for conversion of an-
alytical trees into tectogrammatical ones).
In the parallel data, we analyzed the Czech side
using more or less the same scenario as used for
the monolingual data. English sentences were an-
alyzed using (among other tools) Morce tagger
Spoustova? et al (2007) and Maximum Spanning
Tree parser.2
The resulting deep syntactic (tectogrammatical)
Czech and English trees are then aligned using T-
aligner?a feature based greedy algorithm imple-
mented for this purpose (Marec?ek et al, 2008). T-
aligner finds corresponding nodes between the two
given trees and links them. For deciding whether
to link two nodes or not, T-aligner makes use of
a bilingual lexicon of tectogrammatical lemmas,
morphosyntactic similarities between the two can-
didate nodes, their positions in the trees and other
similarities between their parent/child nodes. It
2In some previous experiments (e.g. ?Zabokrtsky? et al
(2008)), we used phrase-structure parser Collins (1999) with
subsequent constituency-dependency conversion.
also uses word alignment generated from surface
shapes of sentences by GIZA++ tool, Och and Ney
(2003). We use acquired aligned tectogrammatical
trees for training some models for the transfer.
As analysis of such amounts of data is obvi-
ously computationally very demanding, we run it
in parallel using Sun Grid Engine3 cluster of 40
4-CPU computers. For this purpose, we imple-
mented a rather generic tool that submits any Tec-
toMT pipeline to the cluster.
3 Factored Phrase-Based MT
We essentially repeat our experiments from last
year (Bojar and Hajic?, 2008): GIZA++ align-
ments4 on a-layer lemmas (a-layer nodes corre-
spond 1-1 to surface tokens), symmetrized using
grow-diag-final (no -and) heuristic5 .
Probably due to the domain difference (the test
set is news), including Subtitles in the parallel data
and Web in the monolingual data did not bring any
improvement that would justify the additional per-
formance costs. For most of the phrase-based ex-
periments, we thus used only 2.2M parallel sen-
tences (27M Czech and 32M English tokens) and
43M Czech sentences (694 M tokens).
In Table 3 below, we report the scores for the
following setups selected from about 50 experi-
ments we ran in total:
Moses T is a simple phrase-based translation (T)
with no additional factors. The translation is
performed on truecased word forms (i.e. sen-
tence capitalization removed unless the first
word seems to be a name). The 4-gram lan-
guage model is based on the 43M sentences.
Moses T+C is a factored setup with form-to-form
translation (T) and target-side morphological
coherence check following Bojar and Hajic?
(2008). The setup uses two language mod-
els: 4-grams of word forms and 7-grams of
morphological tags.
Moses T+C+C&T+T+G 84k is a setup desirable
from the linguistic point of view. Two in-
dependent translation paths are used: (1)
form?form translation with two target-side
checks (lemma and tag generated from the
target-side form) as a fine-grained baseline
3http://gridengine.sunsource.net/
4Default settings, IBM models and iterations: 153343.
5Later, we found out that the grow-diag-final-and heuris-
tic provides insignificantly superior results.
126
with the option to resort to (2) an independent
translation of lemma?lemma and tag?tag
finished by a generation step that combines
target-side lemma and tag to produce the fi-
nal target-side form.
We use three language models in this setup
(3-grams of forms, 3-grams of lemmas, and
10-grams of tags).
Due to the increased complexity of the setup,
we were able to train this model on 84k par-
allel sentences only (the Commentaries sec-
tion) and we use the target-side of this small
training data for language models, too.
For all the setups we perform standard MERT
training on the provided development set.6
4 Translation Setup Based on
Tectogrammatical Transfer
In this translation experiment, we follow the tradi-
tional analysis-transfer-synthesis approach, using
the set of PDT 2.0 layers: we analyze the input
English sentence up to the tectogrammatical layer
(through the morphological and analytical ones),
then perform the tectogrammatical transfer, and
then synthesize the target Czech sentence from its
tectogrammatical representation. The whole pro-
cedure consists of about 80 steps, so the following
description is necessarily very high level.
4.1 Analysis
Each sentence is tokenized (roughly according to
the Penn Treebank conventions), tagged by the En-
glish version of the Morce tagger Spoustova? et al
(2007), and lemmatized by our lemmatizer. Then
the dependency parser (McDonald et al, 2005) is
applied. Then the analytical trees resulting from
the parser are converted to the tectogrammatical
ones (i.e. functional words are removed, only
morphologically indispensable categories are left
with the nodes using a sequence of heuristic proce-
dures). Unlike in PDT 2.0, the information about
the original syntactic form is stored with each t-
node (values such as v:inf for an infinitive verb
form, v:since+fin for the head of a subor-
dinate clause of a certain type, adj:attr for
an adjective in attribute position, n:for+X for a
given prepositional group are distinguished).
6We used the full development set of 2k sentences for
?Moses T? and a subset of 1k sentences for the other two
setups due to time constraints.
One of the steps in the analysis of English is
named entity recognition using Stanford Named
Entity Recognizer (Finkel et al, 2005). The nodes
in the English t-layer are grouped according to the
detected named entities and they are assigned the
type of entity (location, person, or organization).
This information is preserved in the transfer of the
deep English trees to the deep Czech trees to al-
low for the appropriate capitalization of the Czech
translation.
4.2 Transfer
The transfer phase consists of the following steps:
? Initiate the target-side (Czech) t-trees sim-
ply by ?cloning? the source-side (English) t-
trees. Subsequent steps usually iterate over
all t-nodes. In the following, we denote a
source-side t-node as S and the correspond-
ing target-side node as T.
? Translate formemes using
two probabilistic dictionaries
(p(T.formeme|S.formeme, S.parent.lemma)
and p(T.formeme|S.formeme)) and a few
manual rules. The formeme translation
probability estimates were extracted from a
part of the parallel data mentioned above.
? Translate lemmas using a probabilistic dictio-
nary (p(T.lemma|S.lemma)) and a few rules
that ensure compatibility with the previously
chosen formeme. Again, this probabilistic
dictionary was obtained using the aligned
tectogrammatical trees from the parallel cor-
pus.
? Fill the grammatemes (deep-syntactic equiv-
alent of morphological categories) gender
(for denotative nouns) and aspect (for verbs)
according to the chosen lemma. We also
fix grammateme values where the English-
Czech grammateme correspondence is non-
trivial (e.g. if an English gerund expression is
translated to Czech as a subordinating clause,
the tense grammateme has to be filled). How-
ever, the transfer of grammatemes is defi-
nitely much easier task than the transfer of
formemes and lemmas.
4.3 Synthesis
The transfer step yields an abstract deep
syntactico-semantical tree structure. Firstly,
127
we derive surface morphological categories
from their deep counterparts taking care of their
agreement where appropriate and we also remove
personal pronouns in subject positions (because
Czech is a pro-drop language).
To arrive at the surface tree structure, auxil-
iary nodes of several types are added, including
(1) reflexive particles, (2) prepositions, (3) subor-
dinating conjunctions, (4) modal verbs, (5) ver-
bal auxiliaries, and (6) punctuation nodes. Also,
grammar-based node ordering changes (imple-
mented by rules) are performed: e.g. if an English
possessive attribute is translated using Czech gen-
itive, it is shifted into post-modification position.
After finishing the inflection of nouns, verbs,
adjectives and adverbs (according to the values of
morphological categories derived from agreement
etc.), prepositions may need to be vocalized: the
vowel -e or -u is attached to the preposition if the
pronunciation of prepositional group would be dif-
ficult otherwise.
After the capitalization of the beginning of each
sentence (and each named entity instance), we ob-
tain the final translation by flattening the surface
tree.
4.4 Preliminary Error Analysis
According to our observations most errors happen
during the transfer of lemmas and formemes.
Usually, there are acceptable translations of
lemma and formeme in respective n-best lists
but we fail to choose the best one. The sce-
nario described in Section 4.2 uses quite a
primitive transfer algorithm where formemes
and lemmas are translated separately in two
steps. We hope that big improvements could
be achieved with more sophisticated algo-
rithms (optimizing the probability of the whole
tree) and smoothed probabilistic models (such
as p(T.lemma|S.lemma, T.parent.lemma) and
p(T.formeme|S.formeme, T.lemma, T.parent.lemma)).
Other common errors include:
? Analysis: parsing (especially coordinations
are problematic with McDonald?s parser).
? Transfer: the translation of idioms and col-
locations, including named entities. In these
cases, the classical transfer at the t-layer
is not appropriate and utilization of some
phrase-based MT would help.
? Synthesis: reflexive particles, word order.
5 Experimental Results and Discussion
Table 3 reports lowercase BLEU and NIST scores
and preliminary manual ranks of our submissions
in contrast with other systems participating in
English?Czech translation, as evaluated on the
official WMT09 unseen test set. Note that auto-
matic metrics are known to correlate quite poorly
with human judgements, see the best ranking but
?lower scoring? PC Translator this year and also
in Callison-Burch et al (2008).
System BLEU NIST Rank
Moses T 14.24 5.175 -3.02 (4)
Moses T+C 13.86 5.110 ?
Google 13.59 4.964 -2.82 (3)
U. of Edinburgh 13.55 5.039 -3.24 (5)
Moses T+C+C&T+T+G 84k 10.01 4.360 -
Eurotran XP 09.51 4.381 -2.81 (2)
PC Translator 09.42 4.335 -2.77 (1)
TectoMT 07.29 4.173 -3.35 (6)
Table 3: Automatic scores and preliminary human
rank for English?Czech translation. Systems in
italics are provided for comparison only. Best re-
sults in bold.
Unfortunately, this preliminary evaluation sug-
gests that simpler models perform better, partly
because it is easier to tune them properly both
from computational point of view (e.g. MERT
not stable and prone to overfitting with more fea-
tures7), as well as from software engineering point
of view (debugging of complex pipelines of tools
is demanding). Moreover, simpler models run
faster: ?Moses T? with 12 sents/minute is 4.6
times faster than ?Moses T+C?. (Note that we have
not tuned either of the models for speed.)
While ?Moses T? is probably nearly identical
setup as Google and Univ. of Edinburgh use,
the knowledge of correct language-dependent to-
kenization and the use of relatively high quality
large language model data seems to bring moder-
ate improvements.
6 Conclusion
We described our experiments with a complex lin-
guistically motivated translation system and vari-
ous (again linguistically-motivated) setups of fac-
tored phrase-based translation. An automatic eval-
uation seems to suggest that simpler is better, but
we are well aware that a reliable judgement comes
only from human annotators.
7For ?Moses T+C+C&T+T+G?, we observed BLEU
scores on the test set varying by up to five points absolute
for various weight settings yielding nearly identical dev set
scores.
128
References
Ondr?ej Bojar and Jan Hajic?. 2008. Phrase-Based and
Deep Syntactic English-to-Czech Statistical Ma-
chine Translation. In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
143?146, Columbus, Ohio, June. Association for
Computational Linguistics.
Ondr?ej Bojar, Miroslav Jan??c?ek, Zdene?k ?Zabokrtsky?,
Pavel ?Ces?ka, and Peter Ben?a. 2008. CzEng 0.7:
Parallel Corpus with Community-Supplied Transla-
tions. In Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, May. ELRA.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statisti-
cal Machine Translation, pages 70?106, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Michael Collins. 1999. Head-driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Jan Hajic? et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T0 1, Philadelphia.
Va?clav Klimes?. 2006. Analytical and Tectogrammat-
ical Analysis of a Natural Language. Ph.D. thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity, Prague, Czech Rep.
Jan Kocek, Marie Kopr?ivova?, and Karel Kuc?era, edi-
tors. 2000. ?Cesky? na?rodn?? korpus - u?vod a pr???ruc?ka
uz?ivatele. FF UK - ?U ?CNK, Praha.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
David Marec?ek, Zdene?k ?Zabokrtsky?, and Va?clav
Nova?k. 2008. Automatic Alignment of Czech and
English Deep Syntactic Dependency Trees. In Pro-
ceedings of European Machine Translation Confer-
ence (EAMT 08), pages 102?111, Hamburg, Ger-
many.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency
parsing using spanning tree algorithms. In HLT
?05: Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, pages 523?530, Vancou-
ver, British Columbia, Canada.
Va?clav Nova?k and Zdene?k ?Zabokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, ed-
itors, Lecture Notes in Artificial Intelligence, Pro-
ceedings of the 10th I nternational Conference on
Text, Speech and Dialogue, Lecture Notes in Com-
puter Science, pages 92?98, Pilsen, Czech Repub-
lic. Springer Science+Business Media Deutschland
GmbH.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Zdene?k ?Zabokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular Hybrid MT System
with Tectogrammatics Used as Transfer Layer. In
Proc. of the ACL Workshop on Statistical Machine
Translation, pages 167?170, Columbus, Ohio, USA.
129
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Large-scale Semantic Networks: Annotation and Evaluation
Va?clav Nova?k
Institute of Formal and Applied Linguistics
Charles University in Prague, Czech Republic
novak@ufal.mff.cuni.cz
Sven Hartrumpf
Computer Science Department
University of Hagen, Germany
Sven.Hartrumpf@FernUni-Hagen.de
Keith Hall?
Google Research
Zu?rich, Switzerland
kbhall@google.com
Abstract
We introduce a large-scale semantic-network
annotation effort based on the MutliNet for-
malism. Annotation is achieved via a pro-
cess which incorporates several independent
tools including a MultiNet graph editing tool,
a semantic concept lexicon, a user-editable
knowledge-base for semantic concepts, and a
MultiNet parser. We present an evaluation
metric for these semantic networks, allowing
us to determine the quality of annotations in
terms of inter-annotator agreement. We use
this metric to report the agreement rates for a
pilot annotation effort involving three annota-
tors.
1 Introduction
In this paper we propose an annotation frame-
work which integrates the MultiNet semantic net-
work formalism (Helbig, 2006) and the syntactico-
semantic formalism of the Prague Dependency Tree-
bank (Hajic? et al, 2006) (PDT). The primary goal of
this task is to increase the interoperability of these
two frameworks in order to facilitate efforts to an-
notate at the semantic level while preserving intra-
sentential semantic and syntactic annotations as are
found in the PDT.
The task of annotating text with global semantic
interactions (e.g., semantic interactions within some
discourse) presents a cognitively demanding prob-
lem. As with many other annotation formalisms,
?Part of this work was completed while at the Johns Hop-
kins University Center for Language and Speech Processing in
Baltimore, MD USA.
we propose a technique that builds from cognitively
simpler tasks such as syntactic and semantic anno-
tations at the sentence level including rich morpho-
logical analysis. Rather than constraining the se-
mantic representations to those compatible with the
sentential annotations, our procedure provides the
syntacitco-semantic tree as a reference; the annota-
tors are free to select nodes from this tree to create
nodes in the network. We do not attempt to measure
the influence this procedure has on the types of se-
mantic networks generated. We believe that using a
soft-constraint such as the syntactico-semantic tree,
allows us to better generate human labeled seman-
tic networks with links to the interpretations of the
individual sentence analyses.
In this paper, we present a procedure for com-
puting the annotator agreement rate for MultiNet
graphs. Note that a MultiNet graph does not rep-
resent the same semantics as a syntactico-semantic
dependency tree. The nodes of the MultiNet graph
are connected based on a corpus-wide interpretation
of the entities referred to in the corpus. These global
connections are determined by the intra-sentential
interpretation but are not restricted to that inter-
pretation. Therefore, the procedure for computing
annotator agreement differs from the standard ap-
proaches to evaluating syntactic and semantic de-
pendency treebanks (e.g., dependency link agree-
ment, label agreement, predicate-argument structure
agreement).
As noted in (Bos, 2008), ?Even though the de-
sign of annotation schemes has been initiated for
single semantic phenomena, there exists no anno-
tation scheme (as far as I know) that aims to inte-
37
grate a wide range of semantic phenomena all at
once. It would be welcome to have such a resource
at ones disposal, and ideally a semantic annotation
scheme should be multi-layered, where certain se-
mantic phenomena can be properly analysed or left
simply unanalysed.?
In Section 1 we introduce the theoretical back-
ground of the frameworks on which our annotation
tool is based: MultiNet and the Tectogrammatical
Representation (TR) of the PDT. Section 2 describes
the annotation process in detail, including an intro-
duction to the encyclopedic tools available to the an-
notators. In Section 3 we present an evaluation met-
ric for MultiNet/TR labeled data. We also present an
evaluation of the data we have had annotated using
the proposed procedure. Finally, we conclude with
a short discussion of the problems observed during
the annotation process and suggest improvements as
future work.
1.1 MultiNet
The representation of the Multilayered Extended
Semantic Networks (MultiNet), which is described
in (Helbig, 2006), provides a universal formalism
for the treatment of semantic phenomena of natu-
ral language. To this end, they offer distinct ad-
vantages over the use of the classical predicate
calculus and its derivatives. For example, Multi-
Net provides a rich ontology of semantic-concept
types. This ontology has been constructed to be
language independent. Due to the graphical inter-
pretation of MultiNets, we believe manual anno-
tation and interpretation is simpler and thus more
cognitively compatible. Figure 1 shows the Multi-
Net annotation of a sentence from the WSJ corpus:
?Stephen Akerfeldt, currently vice president fi-
nance, will succeed Mr. McAlpine.?
In this example, there are a few relationships that il-
lustrate the representational power of MultiNet. The
main predicate succeed is a ANTE dependent of the
node now, which indicates that the outcome of the
event described by the predicate occurs at some time
later than the time of the statement (i.e., the succes-
sion is taking place after the current time as captured
by the future tense in the sentence). Intra-sentential
coreference is indicated by the EQU relationship.
From the previous context, we know that the vice
president is related to a particular company, Magna
International Inc. The pragmatically defined rela-
tionship between Magna International Inc. and vice
president finance is captured by the ATTCH (con-
ceptual attachment) relationship. This indicates that
there is some relationship between these entities for
which one is a member of the other (as indicated by
the directed edge). Stephen Akerfeldt is the agent of
the predicate described by this sub-network.
The semantic representation of natural language
expressions by means of MultiNet is generally in-
dependent of the considered language. In contrast,
the syntactic constructs used in different languages
to express the same content are obviously not iden-
tical. To bridge the gap between different languages
we employ the deep syntactico-semantic representa-
tion available in the Functional Generative Descrip-
tion framework (Sgall et al, 1986).
1.2 Prague Dependency Treebank
The Prague Dependency Treebank (PDT) presents a
language resource containing a deep manual analy-
sis of texts(Sgall et al, 2004). The PDT contains
annotations on three layers:
Morphological A rich morphological annotation is
provided when such information is available in
the language. This includes lemmatization and
detailed morphological tagging.
Analytical The analytical layer is a dependency
analysis based purely on the syntactic interpre-
tation.
Tectogrammatical The tectogrammatical annota-
tion provides a deep-syntactic (syntactico-
semantic) analysis of the text. The formal-
ism abstracts away from word-order, function
words (syn-semantic words), and morphologi-
cal variation.
The units of each annotation level are linked with
corresponding units on the preceding level. The
morphological units are linked directly with the
original tokenized text. Linking is possible as most
of these interpretations are directly tied to the words
in the original sentence. In MultiNet graphs, addi-
tional nodes are added and nodes are removed.
The PDT 2.0 is based on the long-standing
Praguian linguistic tradition, adapted for the current
38
Figure 1: MultiNet annotation of sentence ?Stephen Akerfeldt, currently vice president finance, will succeed Mr.
McAlpine.? Nodes C4 and C8 are re-used from previous sentences. Node C2 is an unexpressed (not explicitly stated
in the text) annotator-created node used in previous annotations.
computational-linguistics research needs. The theo-
retical basis of the tectogrammatical representation
lies in the Functional Generative Description of lan-
guage systems (Sgall et al, 1986). Software tools
for corpus search, lexicon retrieval, annotation, and
language analysis are included. Extensive documen-
tation in English is provided as well.
2 Integrated Annotation Process
We propose an integrated annotation procedure
aimed at acquiring high-quality MultiNet semantic
annotations. The procedure is based on a combi-
nation of annotation tools and annotation resources.
We present these components in the this section.
2.1 Annotation Tool
The core annotation is facilitated by the cedit
tool1, which uses PML (Pajas and S?te?pa?nek, 2005),
an XML file format, as its internal representa-
tion (Nova?k, 2007). The annotation tool is an
application with a graphical user interface imple-
mented in Java (Sun Microsystems, Inc., 2007). The
1The cedit annotation tool can be downloaded from
http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
cedit tool is platform independent and directly con-
nected to the annotators? wiki (see Section 2.4),
where annotators can access the definitions of indi-
vidual MultiNet semantic relations, functions and at-
tributes; as well as examples, counterexamples, and
discussion concerning the entity in question. If the
wiki page does not contain the required information,
the annotator is encouraged to edit the page with
his/her questions and comments.
2.2 Online Lexicon
The annotators in the semantic annotation project
have the option to look up examples of MultiNet
structures in an online version of the semantically
oriented computer lexicon HaGenLex (Hartrumpf et
al., 2003). The annotators can use lemmata (instead
of reading IDs formed of the lemma and a numer-
ical suffix) for the query, thus increasing the recall
of related structures. English and German input is
supported with outputs in English and/or German;
there are approximately 3,000 and 25,000 seman-
tic networks, respectively, in the lexicon. An exam-
ple sentence for the German verb ?borgen.1.1? (?to
borrow?) plus its automatically generated and val-
39
Figure 2: HaGenLex entry showing an example sentence
for the German verb ?borgen.1.1? (?to borrow?). The
sentence is literally ?The man borrows himself money
from the friend.?
idated semantic representation is displayed in Fig-
ure 2. The quality of example parses is assured by
comparing the marked-up complements in the ex-
ample to the ones in the semantic network. In the
rare case that the parse is not optimal, it will not be
visible to annotators.
2.3 Online Parser
Sometimes the annotator needs to look up a phrase
or something more general than a particular noun
or verb. In this case, the annotator can use
the workbench for (MultiNet) knowledge bases
(MWR (Gno?rlich, 2000)), which provides conve-
nient and quick access to the parser that translates
German sentences or phrases into MultiNets.
2.4 Wiki Knowledge Base
Awiki (Leuf and Cunningham, 2001) is used collab-
oratively to create and maintain the knowledge base
used by all the annotators. In this project we use
Dokuwiki (Badger, 2007). The entries of individ-
ual annotators in the wiki are logged and a feed of
changes can be observed using an RSS reader. The
cedit annotation tool allows users to display appro-
priate wiki pages of individual relation types, func-
tion types and attributes directly from the tool using
their preferred web browser.
3 Network Evaluation
We present an evaluation which has been carried
out on an initial set of annotations of English arti-
cles from The Wall Street Journal (covering those
annotated at the syntactic level in the Penn Tree-
bank (Marcus et al, 1993)). We use the annotation
from the Prague Czech-English Dependency Tree-
bank (Cur???n et al, 2004), which contains a large por-
tion of the WSJ Treebank annotated according to the
PDT annotation scheme (including all layers of the
FGD formalism).
We reserved a small set of data to be used to train
our annotators and have excluded these articles from
the evaluation. Three native English-speaking anno-
tators were trained and then asked to annotate sen-
tences from the corpus. We have a sample of 67
sentences (1793 words) annotated by two of the an-
notators; of those, 46 sentences (1236 words) were
annotated by three annotators.2 Agreement is mea-
sured for each individual sentences in two steps.
First, the best match between the two annotators?
graphs is found and then the F-measure is computed.
In order to determine the optimal graph match be-
tween two graphs, we make use of the fact that
the annotators have the tectogrammatical tree from
which they can select nodes as concepts in theMulti-
Net graph. Many of the nodes in the annotated
graphs remain linked to the tectogrammatical tree,
therefore we have a unique identifier for these nodes.
When matching the nodes of two different annota-
tions, we assume a node represents an identical con-
cept if both annotators linked the node to the same
tectogrammatical node. For the remaining nodes,
we consider all possible one-to-one mappings and
construct the optimal mapping with respect to the F-
measure.
Formally, we start with a set of tectogrammatical
trees containing a set of nodes N . The annotation is
a tuple G = (V,E, T,A), where V are the vertices,
E ? V ? V ?P are the directed edges and their la-
bels (e.g., agent of an action: AGT ? P ), T ? V ?N
is the mapping from vertices to the tectogrammati-
cal nodes, and finally A are attributes of the nodes,
which we ignore in this initial evaluation.3 Analo-
gously, G? = (V ?, E?, T ?, A?) is another annotation
2The data associated with this experiment can be down-
loaded from http://ufal.mff.cuni.cz/?novak/files/data.zip. The
data is in cedit format and can be viewed using the cedit editor
at http://ufal.mff.cuni.cz/?novak/files/cedit.zip.
3We simplified the problem also by ignoring the mapping
from edges to tectogrammatical nodes and the MultiNet edge
attribute knowledge type.
40
of the same sentence and our goal is to measure the
similarity s(G,G?) ? [0, 1] of G and G?.
To measure the similarity we need a set ? of ad-
missible one-to-one mappings between vertices in
the two annotations. A mapping is admissible if
it connects vertices which are indicated by the an-
notators as representing the same tectogrammatical
node:
? =
{
? ? V ? V ?
??? (1)
?
n?N
v?V
v??V ?
((
(v,n)?T?(v?,n)?T ?
)
?(v,v?)??
)
? ?v?V
v?,w??V ?
((
(v,v?)???(v,w?)??
)
?(v?=w?)
)
? ?v,w?V
v??V ?
((
(v,v?)???(w,v?)??
)
?(v=w)
)}
In Equation 1, the first condition ensures that ? is
constrained by the mapping induced by the links to
the tectogrammatical layer. The remaining two con-
ditions guarantee that ? is a one-to-one mapping.
We define the annotation agreement s as:
sF (G,G?) = max??? (F (G,G
?, ?))
where F is the F1-measure:
Fm(G,G?, ?) = 2 ?m(?)|E|+ |E?|
wherem(?) is the number of edges that match given
the mapping ?.
We use four versions of m, which gives us four
versions of F and consequently four scores s for ev-
ery sentence:
Directed unlabeled: mdu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
((
v?, w?, ??
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected unlabeled: muu(?) =?????
{
(v,w,?)?E
????v?,w??V ?,???P
(
((v?, w?, ??) ? E? ? (w?, v?, ??) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Directed labeled: mdl(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
((
v?, w?, ?
)
? E?
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
Undirected labeled: mul(?) =
?????
{
(v,w,?)?E
????v?,w??V ?
(
((v?, w?, ?) ? E? ? (w?, v?, ?) ? E?)
? (v, v?) ? ? ? (w,w?) ? ?
)}?????
These four m(?) functions give us four possible
Fm measures, which allows us to have four scores
for every sentence: sdu, suu, sdl and sul.
Figure 3 shows that the inter-annotator agreement
is not significantly correlated with the position of the
sentence in the annotation process. This suggests
that the annotations for each annotator had achieved
a stable point (primarily due to the annotator training
process).
10 20 30 40 50
0.2
0.4
0.6
0.8
1.0
Sentence length
Inte
r?a
nno
tato
r F?
mea
sure
 ? U
ndir
ecte
d U
nlab
eled
Annotators
CB?CWSM?CWSM?CB
Figure 4: Inter-annotator agreement depending on the
sentence length. Each point represents a sentence.
Figure 4 shows that the agreement is not corre-
lated with the sentence length. It means that longer
41
0 10 20 30 40
0.2
0.4
0.6
0.8
1.0
Index
Und
irec
ted
 Un
labe
led 
F?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
0 10 20 30 40
0.0
0.2
0.4
0.6
Index
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 3: Inter-annotator agreement over time. Left: unlabeled, right: labeled. Each point represents a sentence; CB,
CW, and SM are the annotators? IDs.
sentences are not more difficult than short sentences.
The variance decreases with the sentence length as
expected.
In Figure 5 we show the comparison of directed
and labeled evaluations with the undirected unla-
beled case. By definition the undirected unlabeled
score is the upper bound for all the other scores.
The directed score is well correlated and not very
different from the undirected score, indicating that
the annotators did not have much trouble with de-
termining the correct direction of the edges. This
might be, in part, due to support from the formal-
ism and its tool cedit: each relation type is speci-
fied by a semantic-concept type signature; a relation
that violates its signature is reported immediately to
the annotator. On the other hand, labeled score is
significantly lower than the unlabeled score, which
suggests that the annotators have difficulties in as-
signing the correct relation types. The correlation
coefficient between suu and sul (approx. 0.75) is
also much lower than than the correlation coefficient
between suu and sdu (approx. 0.95).
Figure 6 compares individual annotator pairs. The
scores are similar to each other and also have a sim-
ilar distribution shape.
Undirected Unlabeled F?measure
Den
sity
0.0
0.5
1.0
1.5
2.0
2.5
0.2 0.4 0.6 0.8 1.0
CB ? CW 0.0
0.5
1.0
1.5
2.0
2.5
SM ? CB0.0
0.5
1.0
1.5
2.0
2.5
SM ? CW
Figure 6: Comparison of individual annotator pairs.
A more detailed comparison of individual anno-
tator pairs is depicted in Figure 7. The graph shows
that there is a significant positive correlation be-
tween scores, i.e. if two annotators can agree on the
42
0.2 0.4 0.6 0.8 1.0
0.2
0.4
0.6
0.8
Undirected Unlabeled F?measure
Dire
cte
d U
nlab
eled
 F?
me
asu
re
Annotators
CB?CWSM?CWSM?CB
0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
Undirected Unlabeled F?measure
Und
irec
ted
 La
bele
d F
?m
eas
ure
Annotators
CB?CWSM?CWSM?CB
Figure 5: Left: Directed vs. undirected inter-annotator agreement. Right: Labeled vs. unlabeled inter-annotator agree-
ment. Each point represents a sentence.
annotation, the third is likely to also agree, but this
correlation is not a very strong one. The actual cor-
relation coefficients are shown under the main diag-
onal of the matrix.
Sample Annotators Agreement F-measure
suu sdu sul sdl
Smaller CB-CW 61.0 56.3 37.1 35.0
Smaller SM-CB 54.9 48.5 27.1 25.7
Smaller SM-CW 58.5 50.7 31.3 30.2
Smaller average 58.1 51.8 31.8 30.3
Larger CB-CW 64.6 59.8 40.1 38.5
Table 1: Inter-annotator agreement in percents. The re-
sults come from the two samples described in the first
paragraph of Section 3.
Finally, we summarize the raw result in Table 1.
Note that we report simple annotator agreement
here.
4 Conclusion and Future Work
We have presented a novel framework for the anno-
tation of semantic network for natural language dis-
course. Additionally we present a technique to eval-
uate the agreement between the semantic networks
annotated by different annotators.
Our evaluation of an initial dataset reveals that
given the current tools and annotation guidelines, the
annotators are able to construct the structure of the
semantic network (i.e., they are good at building the
directed graph). They are not, however, able to con-
sistently label the semantic relations between the se-
mantic nodes. In our future work, we will investigate
the difficulty in labeling semantic annotations. We
would like to determine whether this is a product of
the annotation guidelines, the tool, or the formalism.
Our ongoing research include the annotation of
inter-sentential coreference relationships between
the semantic concepts within the sentence-based
graphs. These relationships link the local structures,
allowing for a complete semantic interpretation of
the discourse. Given the current level of consistency
in structural annotation, we believe the data will be
useful in this analysis.
43
CB_CW
0.2 0.4 0.6 0.8
ll
l
l
l
l
l
ll
l
l
l
l
l l
l l
l
l
l
l
l
l
l l
l
l l
l
l
l l
l
l
l l
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
1.0
ll
l
l
l
l
l
ll
l
l
l
l
ll
l l
l
l
l
l
l
l
l l
l
ll
l
l
ll
l
l
ll
l
l
l
l
l
l
l
l
l
l
0.2
0.4
0.6
0.8
0.34 SM_CW
ll
l
l
l
l l
l
l l
l
l
l
l
l
l
l
l
l
l
ll
l
l
l
l l
ll
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
l
0.2 0.4 0.6 0.8 1.0
0.55 0.56
0.2 0.4 0.6 0.8
0.2
0.4
0.6
0.8
SM_CB
Undirected Unlabeled F?measure with Correlation Coefficients
Figure 7: Undirected, unlabeled F-measure correlation of annotator pairs. Each cell represents two different pairs of
annotators; cells with graphs show scatter-plots of F-scores for the annotator pairs along with the optimal linear fit;
cells with values show the correlation coefficient (each point in the plot corresponds to a sentence). For example,
the top row, right-most column, we are comparing the F-score agreement of annotators CB and CW with that of the
F-score agreement of annotators SM and CB. This should help identify an outlier in the consistency of the annotations.
Acknowledgment
This work was partially supported by Czech
Academy of Science grants 1ET201120505 and
1ET101120503; by Czech Ministry of Educa-
tion, Youth and Sports projects LC536 and
MSM0021620838; and by the US National Science
Foundation under grant OISE?0530118. The views
expressed are not necessarily endorsed by the spon-
sors.
References
Mike Badger. 2007. Dokuwiki ? A Practical Open
Source Knowledge Base Solution. Enterprise Open
Source Magazine.
Johan Bos. 2008. Let?s not Argue about Semantics. In
European Language Resources Association (ELRA),
editor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08), Mar-
rakech, Morocco, may.
Jan Cur???n, Martin C?mejrek, Jir??? Havelka, and Vladislav
Kubon?. 2004. Building parallel bilingual syntacti-
cally annotated corpus. In Proceedings of The First
International Joint Conference on Natural Language
Processing, pages 141?146, Hainan Island, China.
Carsten Gno?rlich. 2000. MultiNet/WR: A Knowledge
Engineering Toolkit for Natural Language Informa-
tion. Technical Report 278, University Hagen, Hagen,
Germany.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr Sgall,
Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka, and Marie
Mikulova?. 2006. Prague Dependency Treebank 2.0.
44
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T01, Philadelphia, Pennsylvania.
Sven Hartrumpf, Hermann Helbig, and Rainer Osswald.
2003. The Semantically Based Computer Lexicon Ha-
GenLex ? Structure and Technological Environment.
Traitement Automatique des Langues, 44(2):81?105.
Hermann Helbig. 2006. Knowledge Representation and
the Semantics of Natural Language. Springer, Berlin,
Germany.
Bo Leuf and Ward Cunningham. 2001. The Wiki Way.
Quick Collaboration on the Web. Addison-Wesley,
Reading, Massachusetts.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn treebank. Computational
Linguistics, 19(2):313?330.
Va?clav Nova?k. 2007. Cedit ? semantic networks man-
ual annotation tool. In Proceedings of Human Lan-
guage Technologies: The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL-HLT), pages 11?12,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Petr Pajas and Jan S?te?pa?nek. 2005. A Generic XML-
Based Format for Structured Linguistic Annotation
and Its Application to Prague Dependency Treebank
2.0. Technical Report 29, UFAL MFF UK, Praha,
Czech Republic.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and Prag-
matic Aspects. D. Reidel, Dordrecht, The Netherlands.
Petr Sgall, Jarmila Panevova?, and Eva Hajic?ova?. 2004.
Deep syntactic annotation: Tectogrammatical repre-
sentation and beyond. In Adam Meyers, editor, Pro-
ceedings of the HLT-NAACL 2004 Workshop: Fron-
tiers in Corpus Annotation, pages 32?38, Boston,
Massachusetts, May. Association for Computational
Linguistics.
Sun Microsystems, Inc. 2007. Java Platform, Standard
Edition 6. http://java.sun.com/javase/6/webnotes/
README.html.
45
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 138?141,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Detection of Annotation Inconsistencies
Using Apriori Algorithm
Va?clav Nova?k Magda Raz??mova?
Institute of Formal and Applied Linguistics
Charles University in Prague
Czech Republic
{novak,razimova}@ufal.mff.cuni.cz
Abstract
We present a new method for automated
discovery of inconsistencies in a complex
manually annotated corpora. The pro-
posed technique is based on Apriori al-
gorithm for mining association rules from
datasets. By setting appropriate parame-
ters to the algorithm, we were able to au-
tomatically infer highly reliable rules of
annotation and subsequently we searched
for records for which the inferred rules
were violated. We show that the viola-
tions found by this simple technique are
often caused by an annotation error. We
present an evaluation of this technique on
a hand-annotated corpus PDT 2.0, present
the error analysis and show that in the first
100 detected nodes 20 of them contained
an annotation error.
1 Introduction
Complex annotation schemes pose a serious chal-
lenge to annotators caused by the number of at-
tributes they are asked to fill. The annotation
tool can help them in ensuring that the values of
all attributes are from the appropriate domain but
the interplay of individual values and their mutual
compatibility are at best described in annotation
instructions and often implicit. Another source of
errors are idiomatic expressions where it is diffi-
cult for the annotator to think about the categories
of a word which often exists only as a part of the
idiom at hand.
In our approach, detection of annotation in-
consistencies is an instance of anomaly detection,
which is mainly used in the field of intrusion de-
tection. Traditionally, the anomaly detection is
based on distances between feature vectors of indi-
vidual instances. These methods are described in
Section 2. Our new method presented in Section 3
uses the data-mining technique Apriori (Borgelt
and Kruse, 2002) for inferring high-quality rules,
whose violation indicates a possible annotator?s
mistake or another source of inconsistency. We
tested the proposed method on a manually anno-
tated corpus and described both the data and the
experimental results in Section 4. We conclude by
Section 5.
2 Related Work
Unsupervised anomaly detection has been shown
to be viable for intrusion detection (Eskin et al,
2002). The unsupervised techniques rely on fea-
ture vectors generated by individual instances and
try to find outliers in the vector space. This
can be done using clustering (Chimphlee et al,
2005), Principle Component Analysis (Hawkins,
1974), geometric methods (Eskin et al, 2002) and
more (Lazarevic et al, 2003).
The difference between our method and previ-
ous work lies mainly in the fact that instead us-
ing vector space of features, we directly infer an-
notation rules. The manual annotation is always
based on some rules, some of which are contained
in the annotation manual but many others are more
or less implied. These rules will have their confi-
dence measured in the annotated corpus equal to
1 or at least very close (see Section 3 for defi-
nition of confidence). In our approach we learn
such rules and detect exceptions to the most cred-
ible rules. The rules are learned using the com-
mon Apriori algorithm (Borgelt and Kruse, 2002).
Previously, rules have been also mined by GUHA
algorithm (Ha?jek and Havra?nek, 1978), but not in
the anomaly detection context.
3 Method Description
Our process of anomaly detection comprises two
steps: rules mining and anomaly search.
138
3.1 Rules Mining
The association rules mining was originally de-
signed for market basket analysis to automatically
derive rules such as ?if the customer buys a tooth-
paste and a soap, he is also likely to buy a tooth-
brush?. Every check-out x = (x1, x2, . . . , xN )
is modeled as a draw from an unknown probabil-
ity distribution ?, where N is the total number of
items available at the store and xi is the number
of items of type i contained in the shopping cart.
Further, we define event Ej = {x|xj > 0}, i.e.,
the event that the shopping cart contains the item
j.
In this model, we define a rule A = (L,R)
as a tuple where the left side L and the right
side R are sets of events Ej . For instance sup-
pose that the toothpaste, toothbrush and soap have
indices 1, 2 and 3, respectively. Then the ex-
ample rule mentioned above can be written as
Aexample = ({E1, E3}, {E2}), or alternatively
{E1, E3} ? {E2}. For every rule A = (L,R)
we define two important measures: the support
s(A) and the confidence c(A):
s ((L,R)) = P
?
??
l?L
(l) ? ?
r?R
(r)
?
? (1)
c ((L,R)) = P
?
??
r?R
(r)
???
?
l?L
(l)
?
? (2)
In our example the support is the probability
that a cart contains a toothpaste, a toothbrush and a
soap. The confidence is the probability that a cart
contains a toothbrush given the cart contains both
a toothpaste and a soap.
The input of the Apriori algorithm (Borgelt and
Kruse, 2002) consists of a sample from the proba-
bility distribution?, the threshold of the estimated
confidence, the threshold of the estimated support
and the maximum size of rules. Using this data
the Apriori algorithm lists all rules satisfying the
required constraints.
In the context of market basket analysis the con-
fidence is rarely anywhere close to one, but in the
case of linguistic annotation, there are rules that
are always or almost always followed. The confi-
dence of these rules is very close or equal to one.
The Apriori algorithm allows us to gather rules
that have the confidence close to one and a suf-
ficient support.
3.2 Anomaly Search
After extracting the highly confident rules we se-
lect the rules with the highest support and find the
annotations where these rules are violated. This
provides us with the list of anomalies. The search
is linear with the size of the data set and the size
of the list of extracted rules.
4 Experiments
4.1 Data and Tools
The experiments were carried out using the R sta-
tistical analysis software (R Development Core
Team, 2006) using the arules library (Borgelt and
Kruse, 2002). The dataset used was full manu-
ally annotated data of Prague Dependency Tree-
bank 2.0 (PDT 2.0). PDT 2.0 data were annotated
at three layers, namely morphological, analyti-
cal (shallow dependency syntax) and tectogram-
matical (deep dependency syntax; (Hajic? et al,
2006)). The units of each annotation layer were
linked with corresponding units of the preceding
layer. The morphological units were linked di-
rectly with the original text. The annotation at
the tectogrammatical layer was checked automat-
ically for consistency with the annotation instruc-
tions (S?te?pa?nek, 2006), however, using our tech-
nique, we were still able to automatically find er-
rors. The experimental dataset (full PDT 2.0 data
annotated at all three layers) contained 49,431 sen-
tences or 833,195 tokens.
4.2 Experimental Setup and Error Analysis
In our experimental setup, every check-out (i.e.,
every draw from the probability distribution ?)
contains all attributes of one tectogrammatical
node and its governor. The attributes extracted
from the nodes are listed in Table 1. Thus every
check-out has exactly 52 items, 26 coming from
the node in question and 26 coming from its gov-
ernor.
This being input to the Apriori algorithm, we
set the maximal size of rules to 3, minimal support
to 0.001 and minimal confidence to 0.995. When
the rules were extracted, we sorted them accord-
ing to the descending confidence and stripped all
rules with confidence equal to 1. Using the re-
maining rules, we searched the corpus for the vio-
lations of the rules (starting from the top one) until
we found first 100 suspicious nodes. We manually
analyzed these 100 positions and found out that 20
139
Attribute Description
functor semantic values of deep-syntactic dependency relations
is dsp root root node of the sub-tree representing direct speech
tfa contextual boundness
is generated element not expressed in the surface form of the sentence
is member member of a coordination or an apposition
is name of person proper name of a person
is parenthesis node is part of a parenthesis
is state modification with the meaning of a state
sentmod sentential modality
subfunctor semantic variation within a particular functor
aspect aspect of verbs
degcmp degree of comparison
deontmod an event is necessary, possible, permitted etc.
dispmod relation (attitude) of the agent to the event
gender masculine animate, masculine inanimate, feminine or neuter
indeftype types of pronouns (indefinite, negative etc.)
iterativeness multiple/iterated events
negation a negated or an affirmative form
number singular or plural
numertype types of numerals (cardinal, ordinal etc.)
person reference to the speaker/hearer/something else
politeness polite form
resultative event is presented as the resulting state
sempos semantic part of speech
tense verbal tense (simultaneous, preceding or subsequent events)
verbmod verbal mood (indicative, conditional or imperative)
Table 1: Attributes of tectogrammatical nodes used as the input to the rule mining algorithm. Their
complex interplay can hardly be fully prescribed in an annotation manual.
of them constitute an annotation error. Examples
of extracted rules follow.
is parenthesis:1
& governor:functor:PAR
? governor:is parenthesis:1
(3)
Rule 3 states that if a tectogrammatical node has
the attribute is parenthesis set to 1 (i.e., the node
is part of a parenthesis) and at the same time the
governor of this node in the tectogrammatical tree
has its functor set to PAR (it is the root node of
nodes which are parenthesis in a sentence), the
governor?s is parenthesis attribute is also set to 1.
Using this rule we detected 6 nodes in the corpus
where the annotator forgot to fill the value 1 in the
is parenthesis attribute. There were no false posi-
tives and this automatically extracted rule is likely
to be added to the consistency checking routines
in the future.
functor:RSTR
& gender:nr
? number:nr
(4)
Rule 4 states that RSTR nodes (mostly attributes
of nouns) with nr gender (indeterminable gender)
also have indeterminable number. Our procedure
located a node where the annotator correctly de-
termined the number as sg but failed to recognize
the gender (namely, masculine inanimate) of the
node.
is member:1
& dispmod:nil
? tense:nil
(5)
Rule 5, stating that for nodes with is member set
to 1 the nil value (which means that none of the
defined basic values is suitable) of the dispmod
attribute implicates the nil value of the tense, is
an example of a rule producing false positives.
140
Due to the data sparsity problem, there are not so
many nodes satisfying the premises and in most
of them the nil value were simply filled in their
tense attribute. However, there are (rather rare)
transgressive verb forms in the corpus for which
the correct annotation violates this rule. Many of
them were found by this procedure but they are
more anomalies in the underlying text rather than
anomalies in the annotation. An interesting point
to note is that there were several rules exhibiting
this behavior with different first premises (e.g.,
gender:anim & governor:dispmod:nil ? gover-
nor:tense:nil ). The more general rule (dispmod:nil
? tense:nil ) would not get enough confidence, but
by combining it with other unrelated attributes, the
procedure was able to find rules with enough con-
fidence, although not very useful ones.
resultative:res0
& governor:degcmp:pos
? governor:sempos:adj.denot
(6)
Rule 6 is an example of a successful rule. It
states that nodes that govern a non-resultative node
and have the positive degree of comparison are al-
ways denominating semantic adjectives (i.e., com-
mon adjectives such as black or good ). Using
this rule we detected a node where the annotators
correctly determined the semantic part of speech
as adj.quant.grad (quantificational semantic adjec-
tive) but failed to indicate degcmp:comp.
5 Conclusion and Future Work
We have described a fast method for automatic de-
tection of inconsistencies in a hand-annotated cor-
pus using easily available software tools and eval-
uated it showing that in top 100 suspicious nodes
there were an error in 20 cases. This method seem
to work best for high-quality annotation where the
errors are rare: in our experiments the rules had to
achieve at least 99.5% confidence to be included
in the search for violations. However, it can also
point out inconsistencies in the annotation instruc-
tions by revealing the suspicious data points. We
have shown the typical rules and errors revealed
by our procedure.
The method can be generalized for any manu-
ally entered categorical datasets. The rules can
take values from multiple data entries (nodes,
words, etc.) into account to capture the de-
pendency in the annotation. Other rule-mining
techniques such as GUHA (Ha?jek and Havra?nek,
1978) can be used instead of Apriori.
Acknowledgement
This work was supported by Czech Academy
of Science grants 1ET201120505 and
1ET101120503; by Ministry of Education, Youth
and Sports projects LC536 and MSM0021620838.
References
Christian Borgelt and Rudolf Kruse. 2002. Induction
of Association Rules: Apriori Implementation. In
Proceedings of 15th Conference on Computational
Statistics (Compstat), pages 395?400, Heidelberg,
Germany. Physica Verlag.
W. Chimphlee, Abdul Hanan Abdullah, Mohd
Noor Md Sap, S. Chimphlee, and S. Srinoy. 2005.
Unsupervised Clustering methods for Identifying
Rare Events in Anomaly Detection. In Proceed-
ings of the 6th International Enformatika Confer-
ence (IEC2005), Budapest, Hungary, October 26-28.
E. Eskin, A. Arnold, M. Prerau, L. Portnoy, and
S. Stolfo. 2002. A geometric framework for un-
supervised anomaly detection: Detecting intrusions
in unlabeled data. In Data Mining for Security Ap-
plications. Kluwer.
Petr Ha?jek and Toma?s? Havra?nek. 1978. Mechaniz-
ing Hypothesis Formation; Mathematical Founda-
tions for a General Theory. Springer-Verlag, Berlin,
Heidelberg, New York.
Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, Petr
Sgall, Petr Pajas, Jan S?te?pa?nek, Jir??? Havelka,
Marie Mikulova?, Zdene?k Z?abokrtsky?, and Magda
S?evc???kova?-Raz??mova?. 2006. Prague Dependency
Treebank 2.0. CD-ROM, Linguistic Data Consor-
tium, LDC Catalog No.: LDC2006T01, Philadel-
phia, Pennsylvania.
D. M. Hawkins. 1974. The Detection of Errors
in Multivariate Data Using Principal Components.
Journal of the American Statistical Association,
69(346):340?344.
A. Lazarevic, A. Ozgur, L. Ertoz, J. Srivastava, and
V. Kumar. 2003. A comparative study of anomaly
detection schemes in network intrusion detection. In
Proceedings of SIAM International Conference on
Data Mining.
R Development Core Team, 2006. R: A Language and
Environment for Statistical Computing. R Foun-
dation for Statistical Computing, Vienna, Austria.
ISBN 3-900051-07-0.
Jan S?te?pa?nek. 2006. Post-annotation Checking of
Prague Dependency Treebank 2.0 Data. In Proceed-
ings of the 9th International Conference, TSD 2006,
number 4188 in Lecture Notes in Computer Science,
pages 277?284. Springer-Verlag Berlin Heidelberg.
141
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 276?285,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Comparison of Classification and Ranking Approaches
to Pronominal Anaphora Resolution in Czech?
Ngu. y Giang Linh, Va?clav Nova?k, Zdene?k Z?abokrtsky?
Charles University in Prague
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, CZ-11800
{linh,novak,zabokrtsky}.ufal.mff.cuni.cz
Abstract
In this paper we compare two Ma-
chine Learning approaches to the task
of pronominal anaphora resolution: a
conventional classification system based
on C5.0 decision trees, and a novel
perceptron-based ranker. We use coref-
erence links annotated in the Prague De-
pendency Treebank 2.0 for training and
evaluation purposes. The perceptron sys-
tem achieves f-score 79.43% on recogniz-
ing coreference of personal and possessive
pronouns, which clearly outperforms the
classifier and which is the best result re-
ported on this data set so far.
1 Introduction
Anaphora Resolution (AR) is a well established
task in Natural Language Processing (Mitkov,
2002). Classification techniques (e.g., single can-
didate model aimed at answering: ?Is there a
coreference link between the anaphor and this
antecedent candidate, or not??) are very often
used for the task, e.g. in Mccarthy and Lehnert
(1995) and Soon et al (2001). However, as ar-
gued already in Yang et al (2003), better results
are achieved when the candidates can compete in
a pairwise fashion. It can be explained by the
fact that in this approach (called twin-candidate
model), more information is available for the de-
cision making. If we proceed further along this
direction, we come to the ranking approach de-
scribed in Denis and Baldridge (2007), in which
the entire candidate set is considered at once and
?The work on this project was supported by the
grants MSM 0021620838, GAAV C?R 1ET101120503 and
1ET201120505, MS?MT C?R LC536, and GAUK 4383/2009
which leads to further significant shift in perfor-
mance, more recently documented in Denis and
Baldridge (2008).
In this paper we deal with supervised ap-
proaches to pronominal anaphora in Czech.1 For
training and evaluation purposes, we use corefer-
ences links annotated in the Prague Dependency
Treebank, (Jan Hajic?, et al, 2006). We limit our-
selves only to textual coreference (see Section 2)
and to personal and possessive pronouns. We
make use of a rich set of features available thanks
to the complex annotation scenario of the tree-
bank.
We experiment with two of the above men-
tioned techniques for AR: a classifier and a ranker.
The former is based on a top-down induction of
decision trees (Quinlan, 1993). The latter uses
a simple scoring function whose optimal weight
vector is estimated using perceptron learning in-
spired by Collins (2002). We try to provide both
implementations with as similar input information
as possible in order to be able to compare their
performance for the given task.
Performance of the presented systems can be
compared with several already published works,
namely with a rule-based system described in
Kuc?ova? and Z?abokrtsky? (2005), some of the ?clas-
sical? algorithms implemented in Ne?mc???k (2006),
a system based on decision trees (Ngu. y, 2006),
and a rule-based system evaluated in Ngu. y and
Z?abokrtsky? (2007). To illustrate the real complex-
ity of the task, we also provide performance eval-
uation of a baseline solution.
1Currently one can see a growing interest in unsupervised
techniques, e.g. Charniak and Elsner (2009) and Ng (2008).
However, we make only a very tiny step in this direction:
we use a probabilistic feature based on collocation counts in
large unannotated data (namely in the Czech National Cor-
pus).
276
The most important result claimed in this pa-
per is that, to the best of our knowledge, the pre-
sented ranker system outperforms all the previ-
ously published systems evaluated on the PDT
data. Moreover, the performance of our ranker (f-
score 79.43%) for Czech data is not far from the
performance of the state-of-the-art system for En-
glish described in Denis and Baldridge (2008) (f-
score for 3rd person pronouns 82.2 %).2
A side product of this work lies in bringing
empirical evidence ? for a different language and
different data set ? for the claim of Denis and
Baldridge (2007) that the ranking approach is
more appropriate for the task of AR than the clas-
sification approach.
The paper is structured as follows. The data
with manually annotated links we use are de-
scribed in Section 2. Section 3 outlines prepro-
cessing the data for training and evaluation pur-
poses. The classifier-based and ranker-based sys-
tems are described in Section 4 and Section 5 re-
spectively. Section 6 summarizes the achieved re-
sults by evaluating both approaches using the test
data. Conclusions and final remarks follow in Sec-
tion 7.
2 Coreference links in the Prague
Dependency Treebank 2.0
The Prague Dependency Treebank 2.03 (PDT 2.0,
Jan Hajic?, et al (2006)) is a large collection of
linguistically annotated data and documentation,
based on the theoretical framework of Functional
Generative Description (FGD; introduced by Sgall
(1967) and later elaborated, e.g. in by Sgall et al
(1986)). The PDT 2.0 data are Czech newspaper
texts selected from the Czech National Corpus4
(CNC).
The PDT 2.0 has a three-level structure. On the
lowest morphological level, a lemma and a posi-
tional morphological tag are added to each token.
The middle analytical level represents each sen-
tence as a surface-syntactic dependency tree. On
the highest tectogrammatical level, each sentence
is represented as a complex deep-syntactic depen-
2However, it should be noted that exact comparison is not
possible here, since the tasks are slightly different for the
two languages, especially because of typological differences
between Czech and English (frequent pro-drop in Czech)
and different information available in the underlying data re-
source on the other hand (manually annotated morphological
and syntactical information available for Czech).
3http://ufal.mff.cuni.cz/pdt2.0/
4http://ucnk.ff.cuni.cz/
dency tree, see Mikulova? and others (2005) for de-
tails. This level includes also annotation of coref-
erential links.
The PDT 2.0 contains 3,168 newspaper texts
(49,431 sentences) annotated on the tectogram-
matical level. Coreference has been annotated
manually in all this data. Following the FGD,
there are two types of coreference distinguished:
grammatical coreference and textual coreference
(Panevova?, 1991). The main difference between
the two coreference types is that the antecedent in
grammatical coreference can be identified using
grammatical rules and sentence syntactic struc-
ture, whereas the antecedent in textual coreference
can not.
The further division of grammatical and textual
coreference is based on types of anaphors:
Grammatical anaphors: relative pronouns, re-
flexive pronouns, reciprocity pronouns, re-
stored (surface-unexpressed) ?subjects? of
infinitive verbs below verbs of control,
Textual anaphors: personal and possessive pro-
nouns, demonstrative pronouns.
The data in the PDT 2.0 are divided into three
groups: training set (80%), development test set
(10%), and evaluation test set (10%). The training
and development test set can be freely exploited,
while the evaluation test data should serve only for
the very final evaluation of developed tools.
Table 1 shows the distribution of each anaphor
type. The total number of coreference links in the
PDT 2.0 data is 45,174.5 Personal pronouns in-
cluding those zero ones and possessive pronouns
form 37.4% of all anaphors in the entire corpus
(16,888 links).
An example tectogrammatical tree with de-
picted coreference links (arrows) is presented in
Figure 1. For the sake of simplicity, only three
node attributes are displayed below the nodes: tec-
togrammatical lemma, functor, and semantic part
of speech (tectogrammatical nodes themselves are
complex data structures and around twenty at-
tributes might be stored with them).
Tectogrammatical lemma is a canonical word
form or an artificial value of a newly created node
5In terms of the number of coreference links, PDT 2.0
is one of the largest existing manually annotated resources.
Another comparably large resource is BBN Pronoun Coref-
erence and Entity Type Corpus (Weischedel and Brunstein,
2005), which contains a stand-off annotation of coreference
links in the Penn Treebank texts.
277
Type/Count train dtest etest
Personal pron. 12,913 1,945 2,030
Relative pron. 6,957 948 1,034
Under-control pron. 6,598 874 907
Reflexive pron. 3,381 452 571
Demonstrative pron. 2,582 332 344
Reciprocity pron. 882 110 122
Other 320 35 42
Total 34,983 4,909 5,282
Table 1: Distribution of the different anaphor
types in the PDT 2.0.
on the tectogrammatical level. E.g. the (artifi-
cial) tectogrammatical lemma #PersPron stands
for personal (and possessive) pronouns, be they
expressed on the surface (i.e., present in the orig-
inal sentence) or restored during the annotation
of the tectogrammatical tree structure (zero pro-
nouns).
Functor captures the deep-syntactic dependency
relation between a node and its governor in the
tectogrammatical tree. According to FGD, func-
tors are divided into actants (ACT ? actor, PAT ?
patient, ADDR ? addressee, etc.) and free modi-
fiers (LOC ? location, BEN ? benefactor, RHEM
? rhematizer, TWHEN ? temporal modifier, APP
? appurtenance, etc.).
Semantic parts of speech correspond to ba-
sic onomasiological categories (substance, fea-
ture, factor, event). The main semantic POS dis-
tinguished in PDT 2.0 are: semantic nouns, se-
mantic adjectives, semantic adverbs and semantic
verbs (for example, personal and possessive pro-
nouns belong to semantic nouns).
3 Training data preparation
The training phase of both presented AR systems
can be outlined as follows:
1. detect nodes which are anaphors (Sec-
tion 3.1),
2. for each anaphor ai, collect the set of an-
tecedent candidates Cand(ai) (Section 3.2),
3. for each anaphor ai, divide the set of
candidates into positive instances (true an-
tecedents) and negative instances (Sec-
tion 3.3),
4. for each pair of an anaphor ai and an an-
tecedent candidate cj ? Cand(ai), compute
the feature vector ?(c, ai) (Section 3.4),
5. given the anaphors, their sets of antecedent
candidates (with related feature vectors), and
the division into positive and negative candi-
dates, train the system for identifying the true
antecedents among the candidates.
Steps 1-4 can be seen as training data prepro-
cessing, and are very similar for both systems.
System-specific details are described in Section 4
and Section 5 respectively.
3.1 Anaphor selection
In the presented work, only third person per-
sonal (and possessive) pronouns are considered,6
be they expressed on the surface or reconstructed.
We treat as anaphors all tectogrammatical nodes
with lemma #PersPron and third person stored in
the gram/person grammateme. More than 98 %
of such nodes have their antecedents (in the sense
of textual coreference) marked in the training data.
Therefore we decided to rely only on this highly
precise rule when detecting anaphors.7
In our example tree, the node #PersPron rep-
resenting his on the surface and the node #Per-
sPron representing the zero personal pronoun he
will be recognized as anaphors.
3.2 Candidate selection
In both systems, the predicted antecedent of a
given anaphor ai is selected from an easy-to-
compute set of antecedent candidates denoted as
Cand(ai). We limit the set of candidates to se-
mantic nouns which are located either in the same
sentence before the anaphor, or in the preced-
ing sentence. Table 2 shows that if we disregard
cataphoric and longer anaphoric links, we loose
a chance for correct answer with only 6 % of
anaphors.
6The reason is that antecedents of most other types of
anaphors annotated in PDT 2.0 can be detected ? given
the tree topology and basic node attributes ? with precision
higher than 90 %, as it was shown already in Kuc?ova? and
Z?abokrtsky? (2005). For instance, antecedents of reflexive
pronouns are tree-nearest clause subjects in most cases, while
antecedents of relative pronouns are typically parents of the
relative clause heads.
7It is not surprising that no discourse status model (as used
e.g. in Denis and Baldridge (2008)) is practically needed
here, since we limit ourselves to personal pronouns, which
are almost always ?discourse-old?.
278
Antecedent location Percnt.
Previous sentence 37 %
Same sentence, preceding the anaphor 57 %
Same sentence, following the anaphor 5 %
Other 1 %
Table 2: Location of antecedents with respect to
anaphors in the training section of PDT 2.0.
3.3 Generating positive and negative
instances
If the true antecedent of ai is not present in
Cand(ai), no training instance is generated. If it is
present, the sets of negative and positive instances
are generated based on the anaphor. This prepro-
cessing step differs for the two systems, because
the classifier can be easily provided with more
than one positive instance per anaphor, whereas
the ranker can not.
In the classification-based system, all candi-
dates belonging to the coreferential chain are
marked as positive instances in the training data.
The remaining candidates are marked as negative
instances.
In the ranking-based system, the coreferential
chain is followed from the anaphor to the nearest
antecedent which itself is not an anaphor in gram-
matical coreference.8 The first such node is put on
the top of the training rank list, as it should be pre-
dicted as the winner (E.g., the nearest antecedent
of the zero personal pronoun he in the example
tree is the relative pronoun who, however, it is a
grammatical anaphor, so its antecedent Brien is
chosen as the winner instead). All remaining (neg-
ative) candidates are added to the list, without any
special ordering.
3.4 Feature extraction
Our model makes use of a wide range of features
that are obtained not only from all three levels of
the PDT 2.0 but also from the Czech National Cor-
pus and the EuroWordNet. Each training or test-
ing instance is represented by a feature vector. The
features describe the anaphor, its antecedent can-
didate and their relationship, as well as their con-
8Grammatical anaphors are skipped because they usually
do not provide sufficient information (e.g., reflexive pronouns
provide almost no cues at all). The classification approach
does not require such adaptation ? it is more robust against
such lack of information as it treats the whole chain as posi-
tive instances.
texts. All features are listed in Table 4 in the Ap-
pendix.
When designing the feature set on personal pro-
nouns, we take into account the fact that Czech
personal pronouns stand for persons, animals and
things, therefore they agree with their antecedents
in many attributes and functions. Further we use
the knowledge from the Lappin and Leass?s al-
gorithm (Lappin and Leass, 1994), the Mitkov?s
robust, knowledge-poor approach (Mitkov, 2002),
and the theory of topic-focus articulation (Kuc?ova?
et al, 2005). We want to take utmost advantage of
information from the antecedent?s and anaphor?s
node on all three levels as well.
Distance: Numeric features capturing the dis-
tance between the anaphor and the candidate, mea-
sured by the number of sentences, clauses, tree
nodes and candidates between them.
Morphological agreement: Categorial features
created from the values of tectogrammatical gen-
der and number9 and from selected morphological
categories from the positional tag10 of the anaphor
and of the candidate. In addition, there are features
indicating the strict agreement between these pairs
and features formed by concatenating the pair of
values of the given attribute in the two nodes (e.g.,
masc neut).
Agreement in dependency functions: Catego-
rial features created from the values of tec-
togrammatical functor and analytical functor (with
surface-syntactic values such as Sb, Pred, Obj) of
the anaphor and of the candidate, their agreement
and joint feature. There are two more features in-
dicating whether the candidate/anaphor is an ac-
tant and whether the candidate/anaphor is a sub-
ject on the tectogrammatical level.11
Context: Categorial features describing the con-
text of the anaphor and of the candidate:
? parent ? tectogrammatical functor and the se-
mantic POS of the effective parent12 of the
9Sometimes gender and number are unknown, but we can
identify the gender and number of e.g. relative or reflexive
pronouns on the tectogrammatical level thanks to their an-
tecedent.
10A positional tag from the morphological level is a string
of 15 characters. Every positions encodes one morphological
category using one character.
11A subject on the tectogrammatical level can be a node
with the analytical functor Sb or with the tectogrammatical
functor Actor in a clause without a subject.
12The ?true governor? in terms of dependency relations.
279
anaphor and the candidate, their agreement
and joint feature; a feature indicating the
agreement of both parents? tectogrammatical
lemma and their joint feature; a joint feature
of the pair of the tectogrammatical lemma
of the candidate and the effective parent?s
lemma of the anaphor; and a feature indicat-
ing whether the candidate and the anaphor are
siblings.13
? coordination ? a feature that indicates
whether the candidate is a member of a coor-
dination and a feature indicating whether the
anaphor is a possessive pronoun and is in the
coordination with the candidate
? collocation ? a feature indicating whether the
candidate has appeared in the same colloca-
tion as the anaphor within the text14 and a
feature that indicates the collocation assumed
from the Czech National Corpus.15
? boundness ? features assigned on the ba-
sis of contextual boundness (available in the
tectogrammatical trees) {contextually bound,
contrastively contextually bound, or contex-
tually non-bound}16 for the anaphor and the
candidate; their agreement and joint feature.
? frequency ? 1 if the candidate is a denotative
semantic noun and occurs more than once
within the text; otherwise 0.
Semantics: Semantically oriented feature that
indicates whether the candidate is a person name
for the present and a set of 63 binary ontologi-
cal attributes obtained from the EuroWordNet.17
These attributes determine the positive or negative
13Both have the same effective parent.
14If the anaphor?s effective parent is a verb and the can-
didate is a denotative semantic noun and has appeared as a
child of the same verb and has had the same functor as the
anaphor.
15The probability of the candidate being a subject preced-
ing the verb, which is the effective parent of the anaphor.
16Contextual boundness is a property of an expression (be
it expressed or absent in the surface structure of the sentence)
which determines whether the speaker (author) uses the ex-
pression as given (for the recipient), i.e. uniquely determined
by the context.
17The Top Ontology used in EuroWordNet (EWN) con-
tains the (structured) set of 63 basic semantic concepts like
Place, Time, Human, Group, Living, etc. For the majority of
English synsets (set of synonyms, the basic unit of EWN), the
appropriate subset of these concepts are listed. Using the In-
ter Lingual Index that links the synsets of different languages,
the set of relevant concepts can be found also for Czech lem-
mas.
relation between the candidate?s lemma and the se-
mantic concepts.
4 Classifier-based system
Our classification approach uses C5.0, a succes-
sor of C4.5 (Quinlan, 1993), which is probably the
most widely used program for inducing decision
trees. Decision trees are used in many AR sys-
tems such as Aone and Bennett (1995), Mccarthy
and Lehnert (1995), Soon et al (2001), and Ng
and Cardie (2002).18
Our classifier-based system takes as input a set
of feature vectors as described in Section 3.4 and
their classifications (1 ? true antecedent, 0 ? non-
antecedent) and produces a decision tree that is
further used for classifying new pairs of candidate
and anaphor.
The classifier antecedent selection algorithm
works as follows. For each anaphor ai, feature
vectors ?(c, ai) are computed for all candidates
c ? Cand(ai) and passed to the trained decision
tree. The candidate classified as positive is re-
turned as the predicted antecedent. If there are
more candidates classified as positive, the nearest
one is chosen.
If no candidate is classified as positive, a sys-
tem of handwritten fallback rules can be used. The
fallback rules are the same rules as those used in
the baseline system in Section 6.2.
5 Ranker-based system
In the ranker-based AR system, every training ex-
ample is a pair (ai, yi), where ai is the anaphoric
expression and yi is the true antecedent. Using
the candidate extraction function Cand, we aim
to rank the candidates so that the true antecedent
would always be the first candidate on the list. The
ranking is modeled by a linear model of the fea-
tures described in Section 3.4. According to the
model, the antecedent y?i for an anaphoric expres-
sion ai is found as:
y?i = argmax
c?Cand(ai)
?(c, ai) ?
??w
The weights ??w of the linear model are trained
using a modification of the averaged perceptron al-
18Besides C5.0, we plan to use also other classifiers in the
future (especially Support Vector Machine, which is often
employed in AR experiments, e.g. by Ng (2005) and Yang
et al (2006)) in order to study how the classifier choice in-
fluences the AR system performance on our data and feature
sets.
280
gorithm (Collins, 2002). This is averaged percep-
tron learning with a modified loss function adapted
to the ranking scenario. The loss function is tai-
lored to the task of correctly ranking the true an-
tecedent, the ranking of other candidates is irrel-
evant. The algorithm (without averaging the pa-
rameters) is listed as Algorithm 1. Note that the
training instances where yi /? Cand(ai) were ex-
cluded from the training.
input : N training examples (ai, yi),
number of iterations T
init : ??w ?
??
0 ;
for t? 1 to T , i? 1 to N do
y?i ? argmaxc?Cand(ai) ?(c, ai) ?
??w ;
if y?i 6= yi then
??w = ??w + ?(yi, ai)? ?(y?i, ai);
end
end
output: weights ??w
Algorithm 1: Modified perceptron algorithm
for ranking. ? is the feature extraction func-
tion, ai is the anaphoric expression, yi is the
true antecedent.
Antecedent selection algorithm using a ranker:
For each third person pronoun create a feature vec-
tor from the pronoun and the semantic noun pre-
ceding the pronoun and is in the same sentence or
in the previous sentence. Use the trained ranking
features weight model to get out the candidate?s
total weight. The candidate with the highest fea-
tures weight is identified as the antecedent.
6 Experiments and evaluation
6.1 Evaluation metrics
For the evaluation we use the standard metrics:19
Precision = number of correctly predicted anaphoric third person pronounsnumber of all predicted third person pronouns
Recall = number of correctly predicted anaphoric third person pronounsnumber of all anaphoric third person pronouns
F-measure = 2?Precision?RecallPrecision+Recall
We consider an anaphoric third person pronoun
to be correctly predicted when we can success-
19Using simple accuracy would not be adequate, as there
can be no link (or more than one) leading from an anaphor
in the annotated data. In other words, finding whether a pro-
noun has an antecedent or not is a part of the task. A deeper
discussion about coreference resolution metrics can be found
in Luo (2005).
fully indicate its antecedent, which can be any an-
tecedent from the same coreferential chain as the
anaphor.
Both the AR systems were developed and tested
on PDT 2.0 training and development test data. Fi-
nally they were tested on evaluation test data for
the final scoring, summarized in Section 6.3.
6.2 Baseline system
We have made some baseline rules for the task of
AR and tested them on the PDT 2.0 evaluation test
data. Their results are reported in Table 3. Base-
line rules are following: For each third person pro-
noun, consider all semantic nouns which precede
the pronoun and are not further than the previous
sentence, and:
? select the nearest one as its antecedent
(BASE 1),
? select the nearest one which is a clause sub-
ject (BASE 2),
? select the nearest one which agrees in gender
and number (BASE 3),
? select the nearest one which agrees in gen-
der and number; if there is no such noun,
choose the nearest clause subject; if no clause
subject was found, choose the nearest noun
(BASE 3+2+1).
6.3 Experimental results and discussion
Scores for all three systems (baseline, clasifier
with and without fallback, ranker) are given in Ta-
ble 3. Our baseline system based on the combina-
tion of three rules (BASE 3+2+1) reports results
superior to the ones of the rule-based system de-
scribed in Kuc?ova? and Z?abokrtsky? (2005). Kuc?ova?
and Z?abokrtsky? proposed a set of filters for per-
sonal pronominal anaphora resolution. The list of
candidates was built from the preceding and the
same sentence as the personal pronoun. After ap-
plying each filter, improbable candidates were cut
off. If there was more than one candidate left at
the end, the nearest one to the anaphor was cho-
sen as its antecedent. The reported final success
rate was 60.4 % (counted simply as the number of
correctly predicted links divided by the number of
pronoun anaphors in the test data section).
An interesting point of the classifier-based sys-
tem lies in the comparison with the rule-based
281
Rule P R F
BASE 1 17.82% 18.00% 17.90%
BASE 2 41.69% 42.06% 41.88%
BASE 3 59.00% 59.50% 59.24%
BASE 3+2+1 62.55% 63.03% 62.79%
CLASS 69.9% 70.44% 70.17%
CLASS+3+2+1 76.02% 76.60% 76.30%
RANK 79.13% 79.74% 79.43%
Table 3: Precision (P), Recall (R) and F-measure
(F) results for the presented AR systems.
system of Ngu. y and Z?abokrtsky? (2007). With-
out the rule-based fallback (CLASS), the clas-
sifier falls behind the Ngu. y and Z?abokrtsky??s
system (74.2%), while including the fallback
(CLASS+3+2+1) it gives better results.
Overall, the ranker-based system (RANK) sig-
nificantly outperforms all other AR systems for
Czech with the f-score of 79.43%. Comparing
with the model for third person pronouns of Denis
and Baldridge (2008), which reports the f-score of
82.2%, our ranker is not so far behind. It is im-
portant to say that our system relies on manually
annotated information20 and we solve the task of
anaphora resolution for third person pronouns on
the tectogrammatical level of the PDT 2.0. That
means these pronouns are not only those expressed
on the surface, but also artificially added (recon-
structed) into the structure according to the princi-
ples of FGD.
7 Conclusions
In this paper we report two systems for AR in
Czech: the classifier-based system and the ranker-
based system. The latter system reaches f-score
79.43% on the Prague Dependency Treebank test
data and significantly outperforms all previously
published results. Our results support the hypoth-
esis that ranking approaches are more appropriate
for the AR task than classification approaches.
References
Chinatsu Aone and Scott William Bennett. 1995.
Evaluating automated and manual acquisition of
20In the near future, we plan to re-run the experiments us-
ing sentence analyses created by automatic tools (all needed
tools are available in the TectoMT software framework
(Z?abokrtsky? et al, 2008)) instead of manually created analy-
ses, in order to examine the sensitivity of the AR system to
annotation quality.
anaphora resolution strategies. In Proceedings of the
33rd annual meeting on Association for Computa-
tional Linguistics, pages 122?129, Morristown, NJ,
USA. Association for Computational Linguistics.
Anto?nio Branco, Tony McEnery, Ruslan Mitkov, and
Fa?tima Silva, editors. 2007. Proceedings of the 6th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2007), Lagos (Algarve), Portugal.
CLUP-Center for Linguistics of the University of
Oporto.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 148?156, Athens, Greece,
March. Association for Computational Linguistics.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of EMNLP, volume 10, pages 1?8.
Pascal Denis and Jason Baldridge. 2007. A ranking
approach to pronoun resolution. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence (IJCAI2007), pages 1588?1593, Hyder-
abad, India, January 6?12.
Pascal Denis and Jason Baldridge. 2008. Special-
ized models and ranking for coreference resolu-
tion. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP2008), pages 660?669, Honolulu, Hawaii,
USA, October 25?27.
Jan Hajic?, et al 2006. Prague Dependency Treebank
2.0. CD-ROM, Linguistic Data Consortium, LDC
Catalog No.: LDC2006T01, Philadelphia.
Lucie Kuc?ova? and Zdene?k Z?abokrtsky?. 2005.
Anaphora in Czech: Large Data and Experiments
with Automatic Anaphora. LNCS/Lecture Notes in
Artificial Intelligence/Proceedings of Text, Speech
and Dialogue, 3658:93?98.
Lucie Kuc?ova?, Kater?ina Vesela?, Eva Hajic?ova?, and
Jir??? Havelka. 2005. Topic-focus articulation and
anaphoric relations: A corpus based probe. In Klaus
Heusinger and Carla Umbach, editors, Proceedings
of Discourse Domains and Information Structure
workshop, pages 37?46, Edinburgh, Scotland, UK,
Aug. 8-12.
Shalom Lappin and Herbert J. Leass. 1994. ?an algo-
rithm for pronominal anaphora resolution?. Compu-
tational Linguistics, 20(4):535?561.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In HLT ?05: Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 25?32, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
282
J Mccarthy and Wendy G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In In Pro-
ceedings of the Fourteenth International Joint Con-
ference on Artificial Intelligence, pages 1050?1055.
Marie Mikulova? et al 2005. Anotace na tektogra-
maticke? rovine? Praz?ske?ho za?vislostn??ho korpusu.
Anota?torska? pr???ruc?ka (t-layer annotation guide-
lines). Technical Report TR-2005-28, U?FAL MFF
UK, Prague, Prague.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
Va?clav Ne?mc???k. 2006. Anaphora Resolution. Mas-
ter?s thesis, Faculty of Informatics, Masaryk Univer-
sity.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111, Morristown, NJ, USA. Association
for Computational Linguistics.
Vincent Ng. 2005. Supervised ranking for pro-
noun resolution: Some recent improvements. In
Manuela M. Veloso and Subbarao Kambhampati,
editors, AAAI, pages 1081?1086. AAAI Press / The
MIT Press.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP2008), pages 640?649, Hon-
olulu, Hawaii, USA.
Giang Linh Ngu. y and Zdene?k Z?abokrtsky?. 2007.
Rule-based approach to pronominal anaphora reso-
lution applied on the prague dependency treebank
2.0 data. In Branco et al (Branco et al, 2007), pages
77?81.
Giang Linh Ngu. y. 2006. Proposal of a Set of Rules
for Anaphora Resolution in Czech. Master?s thesis,
Faculty of Mathematics and Physics, Charles Uni-
versity.
Jarmila Panevova?. 1991. Koreference gramaticka? nebo
textova?? In Etudes de linguistique romane et slave.
Krakow.
J. Ross Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Semantic and
Pragmatic Aspects. D. Reidel Publishing Company,
Dordrecht.
Petr Sgall. 1967. Generativn?? popis jazyka a c?eska?
deklinace. Academia, Prague, Czech Republic.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Comput. Linguist., 27(4):521?544.
Zdene?k Z?abokrtsky?, Jan Pta?c?ek, and Petr Pajas. 2008.
TectoMT: Highly Modular MT System with Tec-
togrammatics Used as Transfer Layer. In Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation, ACL.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus. CD-
ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2005T33, Philadelphia.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 176?
183, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006.
Kernel-based pronoun resolution with structured
syntactic knowledge. In Proceedings of the 21st
International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics (COLING-
ACL2006), pages 41?48, Sydney, Australia, July
17?21.
283
A Appendix
t-ln95049-047-p3s1
root
O - O
RSTR
n.denot
Brien - BRIEN
ACT
n.denot
kter? - WHO
ACT
n.pron.indef
Louganis - LOUGANIS
PAT
n.denot
tr novat - TO TRAIN
RSTR
v
rok - YEAR
THL
n.denot
deset - TEN
RSTR
adj.quant.def
#PersPron - HIS
ACT
n.pron.def.pers
onemocn?n - INJURY
PAT
n.denot.neg
vdt - TO KNOW
PRED
v
ale - BUT enunc
ADVS
coap
zavzat_se - TO TIE SOMEONE'S SELF
PRED
v
#PersPron - (HE)
ACT
n.pron.def.pers
ml?en - SECRECY
PAT
n.denot.neg
.
Figure 1: Simplified tectogrammatical tree representing the sentence O?Brien, ktery? Louganise tre?noval
deset let, o jeho onemocne?n?? ve?de?l, ale zava?zal se mlc?en??m. (Lit.: O?Brien, who Louganis trained for
ten years, about his injury knew, but (he) tied himself to secrecy.) Note two coreferential chains {Brien,
who, (he)} and {Louganis, his}.
284
Distance
sent dist sentence distance between c and ai
clause dist clause distance between c and ai
node dist tree node distance between c and ai
cand ord mention distance between c and ai
Morphological Agreement
gender t-gender of c and ai, agreement, joint
number t-number of c and ai, agreement, joint
apos m-POS of c and ai, agreement, joint
asubpos detailed POS of c and ai, agreement, joint
agen m-gender of c and ai, agreement, joint
anum m-number of c and ai, agreement, joint
acase m-case of c and ai, agreement, joint
apossgen m-possessor?s gender of c and ai, agreement, joint
apossnum m-possessor?s number of c and ai, agreement, joint
apers m-person of c and ai, agreement, joint
Functional Agreement
afun a-functor of c and ai, agreement, joint
fun t-functor of c and ai, agreement, joint
act c/ai is an actant, agreement
subj c/ai is a subject, agreement
Context
par fun t-functor of the parent of c and ai, agreement, joint
par pos t-POS of the parent of c and ai, agreement, joint
par lemma agreement between the parent?s lemma of c and ai, joint
clem aparlem joint between the lemma of c and the parent?s lemma of ai
c coord c is a member of a coordination
app coord c and ai are in coordination & ai is a possessive pronoun
sibl c and ai are siblings
coll c and ai have the same collocation
cnk coll c and ai have the same CNC collocation
tfa contextual boundness of c and ai, agreement, joint
c freq c is a frequent word
Semantics
cand pers c is a person name
cand ewn semantic position of c?s lemma within the EuroWordNet Top Ontology
Table 4: Features used by the perceptron-based model
285
