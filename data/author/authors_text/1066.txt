Learning to Classify Email into "Speech Acts"  
 
William W. Cohen1 Vitor R. Carvalho2 Tom M. Mitchell1,2 
1Center for Automated Learning & Discovery 
 Carnegie Mellon University 
  Pittsburgh, PA 15213 
2Language Technology Institute  
Carnegie Mellon University  
   Pittsburgh, PA 15213 
Abstract 
It is often useful to classify email accord-
ing to the intent of the sender (e.g., "pro-
pose a meeting", "deliver information"). 
We present experimental results in learn-
ing to classify email in this fashion, 
where each class corresponds to a verb-
noun pair taken from a predefined ontol-
ogy describing typical ?email speech 
acts?.   We demonstrate that, although 
this categorization problem is quite dif-
ferent from ?topical? text classification, 
certain categories of messages can none-
theless be detected with high precision 
(above 80%) and reasonable recall (above 
50%) using existing text-classification 
learning methods. This result suggests 
that useful task-tracking tools could be 
constructed based on automatic classifi-
cation into this taxonomy.  
1 Introduction 
In this paper we discuss using machine learn-
ing methods to classify email according to the 
intent of the sender.  In particular, we classify 
emails according to an ontology of verbs (e.g., 
propose, commit, deliver) and nouns (e.g., infor-
mation, meeting, task), which jointly describe the 
?email speech act? intended by the email sender.   
A method for accurate classification of email 
into such categories would have many potential 
benefits. For instance, it could be used to help an 
email user track the status of ongoing joint activi-
ties.  Delegation and coordination of joint tasks is 
a time-consuming and error-prone activity, and 
the cost of errors is high: it is not uncommon that 
commitments are forgotten, deadlines are missed, 
and opportunities are wasted because of a failure 
to properly track, delegate, and prioritize sub-
tasks. The classification methods we consider  
 
methods which could be used to partially auto-
mate this sort of activity tracking. A hypothetical 
example of an email assistant that works along 
these lines is shown in Figure 1. 
Bill,
Do you have any sample 
scheduling-related email we 
could use as data?  -Steve
Assistant announces:  ?new 
email request, priority 
unknown.?
Sure, I?ll put some together 
shortly. -Bill
Assistant:  ?should I add this 
new commitment to your to-
do list??
Fred, can you collect the msgs
from the CSPACE corpora 
tagged w/ the  ?meeting?
noun, ASAP? -Bill
Assistant:  notices outgoing
request, may take action if no 
answer is received promptly.
Yes, I can get to that in the 
next few days.  Is next 
Monday ok? -Fred
Assistant:  notices incoming 
commitment. ?Should I send 
Fred a reminder on Monday??
 
Figure 1 - Dialog with a hypothetical email assistant 
that automatically detects email speech acts.  Dashed 
boxes indicate outgoing messages.  (Messages have 
been edited for space and anonymity.) 
2 Related Work 
Our research builds on earlier work defining il-
locutionary points of speech acts (Searle, 1975), 
and relating such speech acts to email and work-
flow tracking (Winograd, 1987, Flores & Lud-
low, 1980, Weigant et al 2003). Winograd 
suggested that research explicating the speech-act 
based ?language-action perspective? on human 
communication could be used to build more use-
ful tools for coordinating joint activities.  The 
Coordinator (Winograd, 1987) was one such sys-
tem, in which users augmented email messages 
with additional annotations indicating intent. 
While such systems have been useful in lim-
ited contexts, they have also been criticized as 
cumbersome: by forcing users to conform to a 
particular formal system, they constrain commu-
nication and make it less natural (Schoop, 2001); 
in short, users often prefer unstructured email 
interactions (Camino et al 1998). We note that 
these difficulties are avoided if messages can be 
automatically annotated by intent, rather than 
soliciting a statement of intent from the user. 
Murakoshi et al (1999) proposed an email an-
notation scheme broadly similar to ours, called a 
?deliberation tree?, and an algorithm for con-
structing deliberation trees automatically, but 
their approach was not quantitatively evaluated. 
The approach is based on recognizing a set of 
hand-coded linguistic ?clues?.  A limitation of 
their approach is that these hand-coded linguistic 
?clues? are language-specific (and in fact limited 
to Japanese text.) 
Prior research on machine learning for text 
classification has primarily considered classifica-
tion of documents by topic (Lewis, 1992; Yang, 
1999), but also has addressed sentiment detection 
(Pang et al, 2002;  Weibe et al, 2001) and au-
thorship attribution (e.g., Argamon et al 2003).   
There has been some previous use of machine 
learning to classify email messages (Cohen 1996; 
Sahami et al, 1998; Rennie, 2000; Segal & 
Kephart, 2000).  However, to our knowledge, 
none of these systems has investigated learning 
methods for assigning email speech acts. Instead, 
email is generally classified into folders (i.e., ac-
cording to topic) or according to whether or not it 
is ?spam?. Learning systems have been previ-
ously used to automatically detect acts in 
conversational speech (e.g. Finke et al, 1998). 
3 An Ontology of Email Acts 
Our ontology of nouns and verbs covering some 
of the possible speech acts associated with emails 
is summarized in Figure 2.  We assume that a 
single email message may contain multiple acts, 
and that each act is described by a verb-noun pair 
drawn from this ontology (e.g., "deliver data").   
The underlined nodes in the figure indicate the 
nouns and verbs for which we have trained clas-
sifiers (as discussed in subsequent sections). 
To define the noun and verb ontology of 
Figure 2, we first examined email from several 
corpora (including our own inboxes) to find regu-
larities, and then performed a more detailed 
analysis of one corpus. The ontology was further 
refined in the process of labeling the corpora de-
scribed below. 
In refining this ontology, we adopted several 
principles. First, we believe that it is more impor-
tant for the ontology to reflect observed linguistic 
behavior than to reflect any abstract view of the 
space of possible speech acts. As a consequence, 
the taxonomy of verbs contains concepts that are 
atomic linguistically, but combine several illocu-
tionary points. (For example, the linguistic unit 
"let's do lunch" is both directive, as it requests the 
receiver, and commissive, as it implicitly com-
mits the sender. In our taxonomy this is a single 
'propose' act.) Also, acts which are abstractly 
possible but not observed in our data are not rep-
resented (for instance, declarations). 
 
Noun 
Activity Information 
Meeting 
Logistics 
Data 
Opinion Ongoing 
Activity 
Data Single 
Event 
Meeting Other   
Short Term 
Task 
Other 
Data Committee 
<Verb><Noun> 
Verb 
Remind 
Propose 
Deliver 
Commit 
Request 
Amend 
Refuse 
Greet 
Other Negotiate 
Initiate Conclude 
 
Figure 2 ? Taxonomy  
Second, we believe that the taxonomy must re-
flect common non-linguistic uses of email, such 
as the use of email as a mechanism to deliver 
files. We have grouped this with the linguistically 
similar speech act of delivering information. 
The verbs in Figure 1 are defined as follows.  
A request asks (or orders) the recipient to per-
form some activity. A question is also considered 
a request (for delivery of information).  
A propose message proposes a joint activity, 
i.e., asks the recipient to perform some activity 
and commits the sender as well, provided the re-
cipient agrees to the request.  A typical example 
is an email suggesting a joint meeting.  
An amend message amends an earlier proposal. 
Like a proposal, the message involves both a 
commitment and a request.  However, while a 
proposal is associated with a new task, an 
amendment is a suggested modification of an 
already-proposed task. 
A commit message commits the sender to 
some future course of action, or confirms the 
senders' intent to comply with some previously 
described course of action.   
A deliver message delivers something, e.g., 
some information, a PowerPoint presentation,  
the URL of a website, the answer to a question, a 
message sent "FYI?, or an opinion. 
The refuse, greet, and remind verbs occurred 
very infrequently in our data, and hence we did 
not attempt to learn classifiers for them (in this 
initial study). The primary reason for restricting 
ourselves in this way was our expectation that 
human annotators would be slower and less reli-
able if given a more complex taxonomy.  
The nouns in Figure 2 constitute possible ob-
jects for the email speech act verbs. The nouns 
fall into two broad categories. 
Information nouns are associated with email 
speech acts described by the verbs Deliver, Re-
mind and Amend, in which the email explicitly 
contains information. We also associate informa-
tion nouns with the verb Request, where the 
email contains instead a description of the needed 
information (e.g., "Please send your birthdate." 
versus "My birthdate is ?".  The request act is 
actually for a 'deliver information' activity). In-
formation includes data believed to be fact as 
well as opinions, and also attached data files. 
Activity nouns are generally associated with 
email speech acts described by the verbs Pro-
pose, Request, Commit, and Refuse.  Activities 
include meetings, as well as longer term activities 
such as committee memberships.   
Notice every email speech act is itself an ac-
tivity.  The <verb><noun> node in Figure 1 indi-
cates that any email speech act can also serve as 
the noun associated with some other email 
speech act.  For example, just as (deliver infor-
mation) is a legitimate speech act, so is (commit 
(deliver information)). Automatically construct-
ing such nested speech acts is an interesting and 
difficult topic; however, in the current paper we 
consider only the problem of determining top-
level the verb for such compositional speech acts. 
For instance, for a message containing a (commit 
(deliver information)) our goal would be to 
automatically detect the commit verb but not the 
inner (deliver information) compound noun. 
4 Categorization Results 
4.1 Corpora 
Although email is ubiquitous, large and realis-
tic email corpora are rarely available for research 
purposes.  The limited availability is largely due 
to privacy issues: for instance, in most US aca-
demic institutions, a users? email can only be dis-
tributed to researchers if all senders of the email 
also provided explicit written consent. 
The email corpora used in our experiments 
consist of four different email datasets collected 
from working groups who signed agreements to 
make their email accessible to researchers. The 
first three datasets, N01F3, N02F2, and N03F2 
are annotated subsets of a larger corpus, the 
CSpace email corpus, which contains approxi-
mately 15,000 email messages collected from a 
management course at Carnegie Mellon Univer-
sity. In this course, 277 MBA students, organized 
in approximately 50 teams of four to six mem-
bers, ran simulated companies in different market 
scenarios over a 14-week period (Kraut et al). 
N02F2, N01F3 and N03F2 are collections of all 
email messages written by participants from three 
different teams, and contain 351, 341 and 443 
different email messages respectively.  
The fourth dataset, the PW CALO corpus, was 
generated during a four-day exercise conducted 
at SRI specifically to generate an email corpus. 
During this time a group of six people assumed 
different work roles (e.g. project leader, finance 
manager, researcher, administrative assistant, etc) 
and performed a number of group activities.  
There are 222 email messages in this corpus. 
These email corpora are all task-related, and 
associated with a small working group, so it is 
not surprising that they contain many instances of 
the email acts described above?for instance, the 
CSpace corpora contain an average of about 1.3 
email verbs per message. Informal analysis of 
other personal inboxes suggests that this sort of 
email is common for many university users. We 
believe that negotiation of shared tasks is a cen-
tral use of email in many work environments.  
All messages were preprocessed by removing 
quoted material, attachments, and non-subject 
header information.  This preprocessing was per-
formed manually, but was limited to operations 
which can be reliably automated. The most diffi-
cult step is removal of quoted material, which we 
address elsewhere (Carvalho & Cohen, 2004). 
4.2 Inter-Annotator Agreement  
Each message may be annotated with several 
labels, as it may contain several speech acts.   To 
evaluate inter-annotator agreement, we double-
labeled N03F2 for the verbs Deliver, Commit, 
Request, Amend, and Propose, and the noun, 
Meeting, and computed the kappa statistic (Car-
letta, 1996) for each of these, defined as 
R
RA
?
?
=
1
?
 
where A is the empirical probability of agreement 
on a category, and R is the probability of agree-
ment for two annotators that label documents at 
random (with the empirically observed frequency 
of each label). Hence kappa ranges from -1 to +1. 
The results in Table 1 show that agreement is 
good, but not perfect. 
 
Email Act Kappa 
Meeting 0.82 
Deliver 0.75 
Commit 0.72 
Request 0.81 
Amend 0.83 
Propose 0.72 
Table 1 - Inter-Annotator Agreement on N03F2. 
We also took doubly-annotated messages 
which had only a single verb label and con-
structed the 5-class confusion matrix for the two 
annotators shown in Table 2. Note kappa values 
are somewhat higher for the shorter one-act mes-
sages. 
 
            Req Prop Amd Cmt Dlv kappa 
Req 55 0 0 0 0 0.97 
Prop 1 11 0 0 1 0.77 
Amd 0 1 15 0 0 0.87 
Cmt 1 3 1 24 4 0.78 
Dlv 1 0 2 3 135 0.91 
Table 2 - Inter-annotator agreement on documents 
with only one category. 
4.3 Learnability of Categories 
Representation of documents. To assess the 
types of message features that are most important 
for prediction, we adopted Support Vector Ma-
chines (Joachims, 2001) as our baseline learning 
method, and a TFIDF-weighted bag-of-words as 
a baseline representation for messages.  We then 
conducted a series of experiments with the 
N03F2 corpus only to explore the effect of dif-
ferent representations.   
NF032 Cmt Dlv Directive 
Baseline SVM 25.0 49.8 75.2 
no tfidf  47.3 58.4 74.6 
+bigrams 46.1 66.1 76.0 
+times 43.6 60.1 73.2 
+POSTags 48.6 61.8 75.4 
+personPhrases 41.2 61.1 73.4 
 
NF02F2 and NF01F3 Cmt Dlv Directive 
Baseline SVM 10.1 56.3 66.1 
All ?useful? features 42.0 64.0 73.3 
Table 3 ? F1 for different feature sets. 
 
We noted that the most discriminating words 
for most of these categories were common words, 
not the low-to-intermediate frequency words that 
are most discriminative in topical classification. 
This suggested that the TFIDF weighting was 
inappropriate, but that a bigram representation 
might be more informative. Experiments showed 
that adding bigrams to an unweighted bag of 
words representation slightly improved perform-
ance, especially on Deliver. These results are 
shown in Table 4 on the rows marked ?no tfidf? 
and ?bigrams?. (The TFIDF-weighted SVM is 
shown in the row marked ?baseline?, and the ma-
jority classifier in the row marked ?default?; all 
numbers are F1 measures on 10-fold cross-
validation.) Examination of messages suggested 
other possible improvements. Since much nego-
tiation involves timing, we ran a hand-coded ex-
tractor for time and date expressions on the data, 
and extracted as features the number of time ex-
pressions in a message, and the words that oc-
curred near a time (for instance, one such feature 
is ?the word ?before? appears near a time?). 
These results appear in the row marked ?times?.  
Similarly, we ran a part of speech (POS) tagger 
and added features for words appearing near a 
pronoun or proper noun (?personPhrases? in the 
table), and also added POS counts. 
To derive a final representation for each cate-
gory, we pooled all features that improved per-
formance over ?no tfidf? for that category.  We 
then compared performance of these document 
representations to the original TFIDF bag of 
words baseline on the (unexamined) N02F2 and 
N01F3 corpora.  As Table 3 shows, substantial 
improvement with respect to F1 and kappa was 
obtained by adding these additional features over 
the baseline representation. This result contrasts 
with previous experiments with bigrams for topi-
cal text classification (Scott & Matwin, 1999)  
and sentiment detection (Pang et al, 2002).  The 
difference is probably that in this task, more in-
formative words are potentially ambiguous: for 
instance, ?will you? and ?I will? are correlated 
with requests and commitments, respectively, but 
the individual words in these bigrams are less 
predictive. 
Learning methods.  In another experiment, 
we fixed the document representation to be un-
weighted word frequency counts and varied the 
learning algorithm. In these experiments, we 
pooled all the data from the four corpora, a total 
of 9602 features in the 1357 messages, and since 
the nouns and verbs are not mutually exclusive, 
we formulated the task as a set of several binary 
classification problems, one for each verb. 
The following learners were used from the 
Based on the MinorThird toolkit (Cohen, 2004). 
VP is an implementation of the voted perceptron 
algorithm (Freund & Schapire, 1999). DT is a 
simple decision tree learning system, which 
learns trees of depth at most five, and chooses 
splits to maximize the function ( )00112
?+?+ + WWWW  suggested by Schapire and 
Singer (1999) as an appropriate objective for 
?weak learners?. AB is an implementation of the 
confidence-rated boosting method described by 
Singer and Schapire (1999), used to boost the DT 
algorithm 10 times.  SVM is a support vector ma-
chine with a linear kernel (as used above). 
 
Act 
 VP AB SVM  DT 
Request 
(450/907) 
Error 
F1 
0.25 
0.58 
0.22 
0.65 
0.23 
0.64 
0.20 
0.69 
Proposal 
(140/1217) 
Error 
F1 
0.11 
0.19 
0.12 
0.26 
0.12 
0.44 
0.10 
0.13 
Delivery 
(873/484) 
Error 
F1 
0.26 
0.80 
0.28 
0.78 
0.27 
0.78 
0.30 
0.76 
Commit-
ment 
(208/1149) 
Error 
F1 
0.15 
0.21 
0.14 
0.44 
0.17 
0.47 
0.15 
0.11 
Directive 
(605/752) 
Error 
F1 
0.25 
0.72 
0.23 
0.73 
0.23 
0.73 
0.19 
0.78 
Commis-
sive 
(993/364) 
Error 
F1 
0.23 
0.84 
0.23 
0.84 
0.24 
0.83 
0.22 
0.85 
Meet 
(345/1012) 
Error 
F1 
0.187 
0.573 
0.17 
0.62 
0.14 
0.72 
0.18
0.60 
Table 4 ? Learning on the entire corpus. 
Table 4 reports the results on the most common 
verbs, using 5-fold cross-validation to assess ac-
curacy. One surprise was that DT (which we had 
intended merely as a base learner for AB) works 
surprisingly well for several verbs, while AB sel-
dom improves much over DT.  We conjecture 
that the bias towards large-margin classifiers that 
is followed by SVM, AB, and VP (and which has 
been so successful in topic-oriented text classifi-
cation) may be less appropriate for this task, per-
haps because positive and negative classes are 
not clearly separated (as suggested by substantial 
inter-annotator disagreement). 
Class:
 Commisive
 (Total: 1357 msgs)
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n Voted Perceptron
AdaBoost
SVM
Decision Tree
 
Figure 3 - Precision/Recall for Commissive act 
Further results are shown in Figure 3-5, which 
provide precision-recall curves for many of these 
classes. The lowest recall level in these graphs 
corresponds to the precision of random guessing. 
These graphs indicate that high-precision predic-
tions can be made for the top-level of the verb 
hierarchy, as well as verbs Request and Deliver, 
if one is willing to slightly reduce recall.  
Class:  Directive
(Total: 1357 msgs)
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
ci
s
io
n VotedPerceptron
AdaBoost
SVM
DecisionTree
 
Figure 4 - Precision/Recall for Directive act 
 
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Recall
Pr
e
c
is
io
n
Meet
Dlv
Req
AdaBoost Learner
(Total: 1357
 
messages)
 
Figure 5 - Precision/Recall of 3 different classes 
using AdaBoost 
 
 
Transferability. One important question in-
volves the generality of these classifiers: to what 
range of corpora can they be accurately applied?  
Is it possible to train a single set of email-act 
classifiers that work for many users, or is it nec-
essary to train individual classifiers for each 
user? To explore this issue we trained a DT clas-
sifier for Directive emails on the NF01F3 corpus, 
and tested it on the NF02F2 corpus; trained the 
same classifier on NF02F2 and tested it on 
NF01F3; and also performed a 5-fold cross-
validation experiment within each corpus.   
(NF02F2 and NF01F3 are for disjoint sets of us-
ers, but are approximately the same size.)  We 
then performed the same experiment with VP for 
Deliver verbs and SVM for Commit verbs (in 
each case picking the top-performing learner with 
respect to F1).  The results are shown in Table 5. 
  
 Test Data 
DT/Directive 1f3 2f2 
Train Data Error F1 Error F1 
1f3 25.1 71.6 16.4 72.8 
2f2 20.1 68.8 18.8 71.2 
VP/Deliver  
1f3 30.1 55.1 21.1 56.1 
2f2 35.0 25.4 21.1 35.7 
SVM/Commit  
1f3 23.4 39.7 15.2 31.6 
2f2 31.9 27.3 16.4 15.1 
Table 5 - Transferability of classifiers 
 
If learned classifiers were highly specific to a 
particular set of users, one would expect that the 
diagonal entries of these tables (the ones based 
on cross-validation within a corpus) would ex-
hibit much better performance than the off-
diagonal entries.  In fact, no such pattern is 
shown. For Directive verbs, performance is simi-
lar across all table entries, and for Deliver and 
Commit, it seems to be somewhat better to train 
on NF01F3 regardless of the test set. 
4.4 Future Directions 
None of the algorithms or representations dis-
cussed above take into account the context of an 
email message, which intuitively is important in 
detecting implicit speech acts.  A plausible notion 
of context is simply the preceding message in an 
email thread. 
Exploiting this context is non-trivial for sev-
eral reasons.  Detecting threads is difficult; al-
though email headers contain a ?reply-to? field, 
users often use the ?reply? mechanism to start 
what is intuitively a new thread.  Also, since 
email is asynchronous, two or more users may 
reply simultaneously to a message, leading to a 
thread structure which is a tree, rather than a se-
quence.  Finally, most sequential learning models 
assume a single category is assigned to each in-
stance?e.g., (Ratnaparkhi, 1999)?whereas our 
scheme allows multiple categories. 
Classification of emails according to our verb-
noun ontology constitutes a special case of a gen-
eral family of learning problems we might call 
factored classification problems, as the classes 
(email speech acts) are factored into two features 
(verbs and nouns) which jointly determine this 
class. A variety of real-world text classification 
problems can be naturally expressed as factored 
problems, and from a theoretical viewpoint, the 
additional structure may allow construction of 
new, more effective algorithms.   
For example, the factored classes provide a 
more elaborate structure for generative probabil-
istic models, such as those assumed by Na?ve 
Bayes. For instance, in learning email acts, one 
might assume words were drawn from a mixture 
distribution with one mixture component pro-
duces words conditioned on the verb class factor, 
and a second mixture component generates words 
conditioned on the noun (see Blei et al(2003) for 
a related mixture model).  Alternatively, models 
of the dependencies between the different factors 
(nouns and verbs) might also be used to improve 
classification accuracy, for instance by building 
into a classifier the knowledge that some nouns 
and verbs are incompatible.  
The fact that an email can contain multiple 
email speech acts almost certainly makes learn-
ing more difficult: in fact, disagreement between 
human annotators is generally higher for longer 
messages.  This problem could be addressed by 
more detailed annotation: rather than annotating 
each message with all the acts it contains, human 
annotators could label smaller message segments 
(say, sentences or paragraphs). An alternative to 
more detailed (and expensive) annotation would 
be to use learning algorithms that implicitly seg-
ment a message. As an example, another mixture 
model formulation might be used, in which each 
mixture component corresponds to a single verb 
category.    
5 Concluding Remarks 
We have presented an ontology of ?email 
speech acts? that is designed to capture some im-
portant properties of a central use of email: nego-
tiating and coordinating joint activities. Unlike 
previous attempts to combine speech act theory 
with email (Winograd, 1987; Flores and Ludlow, 
1980), we propose a system which passively ob-
serves email and automatically classifies it by 
intention. This reduces the burden on the users of 
the system, and avoids sacrificing the flexibility 
and socially desirable aspects of informal, natural 
language communication. 
This problem also raises a number of interest-
ing research issues. We showed that entity ex-
traction and part of speech tagging improves 
classifier performance, but leave open the ques-
tion of whether other types of linguistic analysis 
would be useful. Predicting speech acts requires 
context, which makes classification an inherently 
sequential task, and the labels assigned to mes-
sages have non-trivial structure; we also leave 
open the question of whether these properties can 
be effectively exploited. 
  Our experiments show that many categories 
of messages can be detected, with high precision 
and moderate recall, using existing text-
classification learning methods. This suggests 
that useful task-tracking tools could be con-
structed based on automatic classifiers?a poten-
tially important practical application. 
References 
S. Argamon, M. ?ari? and S. S. Stein. (2003). Style 
mining of electronic messages for multiple authorship 
discrimination: first results. Proceedings of the 9th 
ACM SIGKDD, Washington, D.C. 
 
V. Bellotti, N. Ducheneaut, M. Howard and I. Smith. 
(2003). Taking email to task: the design and evalua-
tion of a task management centered email tool. Pro-
ceedings of the Conference on Human Factors in 
Computing Systems, Ft. Lauderdale, Florida.  
 
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum. 
(2003).  Hierarchical topic models and the nested Chi-
nese restaurant process.  Advances in Neural Informa-
tion Processing Systems, 16, MIT Press. 
 
B. M. Camino, A. E. Millewski, D. R. Millen and T. 
M. Smith. (1998). Replying to email with structured 
responses. International Journal of Human-Computer 
Studies. Vol. 48, Issue 6, pp 763 ? 776. 
 
J. Carletta. (1996). Assessing Agreement on Classifi-
cation Tasks: The Kappa Statistic. Computational 
Linguistics, Vol. 22, No. 2, pp 249-254. 
 
V. R. Carvalho & W. W. Cohen (2004). Learning to 
Extract Signature and Reply Lines from Email.  To 
appear in Proc. of the 2004 Conference on Email and 
Anti-Spam. Mountain View, California. 
 
W. W. Cohen. (1996). Learning Rules that Classify E-
Mail. Proceedings of the 1996 AAAI Spring Sympo-
sium on Machine Learning and Information Access, 
Palo Alto, California. 
 
W. W. Cohen. (2004). Minorthird: Methods for Identi-
fying Names and Ontological Relations in Text using 
Heuristics for Inducing Regularities from Data, 
http://minorthird.sourceforge.net. 
 
M. Finke, M. Lapata, A. Lavie, L. Levin, L. May-
fieldTomokiyo, T. Polzin, K. Ries, A. Waibel and K. 
Zechner. (1998). CLARITY: Inferring Discourse 
Structure from Speech. In Applying Machine Learn-
ing to Discourse Processing, AAAI'98. 
 
F. Flores, and J.J. Ludlow. (1980). Doing and Speak-
ing in the Office. In: G. Fick, H. Spraque Jr. (Eds.). 
Decision Support Systems: Issues and Challenges, 
Pergamon Press, New York, pp. 95-118. 
 
Y. Freund and R. Schapire. (1999). Large Margin 
Classification using the Perceptron Algorithm.  Ma-
chine Learning 37(3), 277?296. 
 
T. Joachims. (2001). A Statistical Learning Model of 
Text Classification with Support Vector Machines. 
Proc. of the Conference on Research and Develop-
ment in Information Retrieval (SIGIR), ACM, 2001. 
 
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. 
Espinosa. (under review). Coordination in teams: evi-
dence from a simulated management game. To appear 
in the Journal of Organizational Behavior. 
 
D. D. Lewis. (1992). Representation and Learning in 
Information Retrieval. PhD Thesis, No. 91-93, Com-
puter Science Dept., Univ of Mass at Amherst 
 
A. McCallum, D. Freitag and F. Pereira. (2000). 
Maximum Entropy Markov Models for Information 
Extraction and Segmentation. Proc. of the 17th Int?l 
Conf. on Machine Learning, Nashville, TN. 
 
B. Pang, L. Lee and S. Vaithyanathan. (2002). 
Thumbs up? Sentiment Classification using Machine 
Learning Techniques. Proc. of the 2002 Conference 
on Empirical Methods in Natural Language Process-
ing (EMNLP), pp 79-86. 
 
A. E. Milewski and T. M. Smith. (1997). An Experi-
mental System For Transactional Messaging. Proc. of 
the international ACM  SIGGROUP conference on 
Supporting group work: the integration challenge, pp. 
325-330. 
 
H. Murakoshi, A. Shimazu and K. Ochimizu. (1999). 
Construction of Deliberation Structure in Email 
Communication,. Pacific Association for Computa-
tional Linguistics, pp. 16-28, Waterloo, Canada. 
 
A. Ratnaparkhi. (1999). Learning to Parse Natural 
Language with Maximum Entropy Models. Machine 
Learning, Vol. 34, pp. 151-175. 
  
J. D. M. Rennie. (2000). Ifile: An Application of Ma-
chine Learning to Mail Filtering. Proc. of the KDD-
2000 Workshop on Text Mining, Boston, MA. 
 
M. Sahami, S. Dumais, D. Heckerman and E. Horvitz. 
(1998). A Bayesian Approach to Filtering Junk E-
Mail. AAAI'98 Workshop on Learning for Text Cate-
gorization. Madison, WI. 
 
M. Schoop. (2001). An introduction to the language-
action perspective. SIGGROUP Bulletin, Vol. 22, No. 
2, pp 3-8. 
 
S. Scott and S. Matwin. (1999). Feature engineering 
for text classification. Proc. of 16th International Con-
ference on Machine Learning, Bled, Slovenia. 
 
J. R. Searle. (1975). A taxonomy of illocutionary acts.  
In K. Gunderson (Ed.), Language, Mind and Knowl-
edge, pp. 344-369.  Minneapolis: University of Min-
nesota Press. 
 
R. B. Segal and J. O. Kephart. (2000). Swiftfile: An 
intelligent assistant for organizing e-mail. In AAAI 
2000 Spring Symposium on Adaptive User Interfaces, 
Stanford, CA. 
 
Y. Yang. (1999). An Evaluation of Statistical Ap-
proaches to Text Categorization. Information Re-
trieval, Vol. 1, Numbers 1-2, pp 69-90. 
 
S. Wermter and M. L?chel. (1996). Learning dialog 
act processing. Proc. of the International Conference 
on Computational Linguistics, Kopenhagen, Denmark. 
 
R. E. Schapire and Y. Singer. (1998). Improved boost-
ing algorithms using confidence-rated predictions. The 
11th Annual Conference on Computational Learning 
Theory, Madison, WI. 
 
H. Wiegend, G. Goldkuhl, and A. de Moor. (2003). 
Proc. of the Eighth Annual Working Conference on 
Language-Action Perspective on Communication 
Modelling (LAP 2003), Tilburg, The Netherlands. 
 
J. Wiebe, R. Bruce, M. Bell, M. Martin and T. Wilson. 
(2001). A Corpus Study of Evaluative and Speculative 
Language.  Proceedings of the 2nd ACL SIGdial 
Workshop on Discourse and Dialogue. Aalborg, 
Denmark. 
 
T. Winograd. 1987. A Language/Action Perspective 
on the Design of Cooperative Work. Human-
Computer Interactions, 3:1, pp. 3-30. 
 
S. Whittaker, Q. Jones, B. Nardi, M. Creech, L. 
Terveen, E. Isaacs and J. Hainsworth. (in press). Us-
ing Personal Social Networks to Organize Communi-
cation in a Social desktop. To appear in Transactions 
on Human Computer Interaction. 
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 35?41,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Improving ?Email Speech Acts? Analysis via N-gram Selection
Vitor R. Carvalho
Language Technologies Institute
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA
vitor@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh PA
wcohen@cs.cmu.edu
Abstract
In email conversational analysis, it is of-
ten useful to trace the the intents behind
each message exchange. In this paper,
we consider classification of email mes-
sages as to whether or not they contain
certain intents or email-acts, such as ?pro-
pose a meeting? or ?commit to a task?.
We demonstrate that exploiting the con-
textual information in the messages can
noticeably improve email-act classifica-
tion. More specifically, we describe a
combination of n-gram sequence features
with careful message preprocessing that is
highly effective for this task. Compared
to a previous study (Cohen et al, 2004),
this representation reduces the classifica-
tion error rates by 26.4% on average. Fi-
nally, we introduce Ciranda: a new open
source toolkit for email speech act predic-
tion.
1 Introduction
One important use of work-related email is negoti-
ating and delegating shared tasks and subtasks. To
provide intelligent email automated assistance, it is
desirable to be able to automatically detect the intent
of an email message?for example, to determine if
the email contains a request, a commitment by the
sender to perform some task, or an amendment to an
earlier proposal. Successfully adding such a seman-
tic layer to email communication is still a challenge
to current email clients.
In a previous work, Cohen et al (2004) used text
classification methods to detect ?email speech acts?.
Based on the ideas from Speech Act Theory (Searle,
1975) and guided by analysis of several email cor-
pora, they defined a set of ?email acts? (e.g., Re-
quest, Deliver, Propose, Commit) and then classified
emails as containing or not a specific act. Cohen et
al. (2004) showed that machine learning algorithms
can learn the proposed email-act categories reason-
ably well. It was also shown that there is an accept-
able level of human agreement over the categories.
A method for accurate classification of email into
such categories would have many potential appli-
cations. For instance, it could be used to help
users track the status of ongoing joint activities, im-
proving task delegation and coordination. Email
speech acts could also be used to iteratively learn
user?s tasks in a desktop environment (Khoussainov
and Kushmerick, 2005). Email acts classification
could also be applied to predict hierarchy positions
in structured organizations or email-centered teams
(Leusky, 2004); predicting leadership positions can
be useful to analyze behavior in teams without an
explicitly assigned leader.
By using only single words as features, Cohen et
al. (2004) disregarded a very important linguistic as-
pect of the speech act inference task: the textual
context. For instance, the specific sequence of to-
kens ?Can you give me? can be more informative to
detect a Request act than the words ?can?, ?you?,
?give? and ?me? separately. Similarly, the word se-
quence ?I will call you? may be a much stronger in-
dication of a Commit act than the four words sep-
arately. More generally, because so many specific
35
sequence of words (or n-grams) are inherently as-
sociated with the intent of an email message, one
would expect that exploiting this linguistic aspect
of the messages would improve email-act classifi-
cation.
In the current work we exploit the linguistic as-
pects of the problem by a careful combination of n-
gram feature extraction and message preprocessing.
After preprocessing the messages to detect entities,
punctuation, pronouns, dates and times, we gener-
ate a new feature set by extracting all possible term
sequences with a length of 1, 2, 3, 4 or 5 tokens.
Using this n-gram based representation in classi-
fication experiments, we obtained a relative average
drop of 26.4% in error rate when compared to the
original Cohen et al (2004) paper. Also, ranking
the most ?meaningful? n-grams based on Informa-
tion Gain score (Yang and Pedersen, 1997) revealed
an impressive agreement with the linguistic intuition
behind the email speech acts.
We finalize this work introducing Ciranda: an
open source package for Email Speech Act predic-
tion. Among other features, Ciranda provides an
easy interface for feature extraction and feature se-
lection, outputs the prediction confidence, and al-
lows retraining using several learning algorithms.
2 ?Email-Acts? Taxonomy and
Applications
A taxonomy of speech acts applied to email com-
munication (email-acts) is described and motivated
in (Cohen et al, 2004). The taxonomy was divided
into verbs and nouns, and each email message is rep-
resented by one or more verb-noun pairs. For exam-
ple, an email proposing a meeting and also request-
ing a project report would have the labels Propose-
Meeting and Request-Data.
The relevant part of the taxonomy is shown in Fig-
ure 1. Very briefly, a Request asks the recipient to
perform some activity; a Propose message proposes
a joint activity (i.e., asks the recipient to perform
some activity and commits the sender); a Commit
message commits the sender to some future course
of action; Data is information, or a pointer to infor-
mation, delivered to the recipient; and a Meeting is a
joint activity that is constrained in time and (usually)
space.
Several possible verbs/nouns were not considered
here (such as Refuse, Greet, and Remind), either be-
cause they occurred very infrequently in the corpus,
or because they did not appear to be important for
task-tracking. The most common verbs found in the
labeled datasets were Deliver, Request, Commit, and
Propose, and the most common nouns were Meet-
ing and deliveredData (abbreviated as dData hence-
forth).
In our modeling, a single email message may have
multiple verbs-nouns pairs.
Figure 1: Taxonomy of email-acts used in experi-
ments. Shaded nodes are the ones for which a clas-
sifier was constructed.
Cohen et al (2004) showed that machine learn-
ing algorithms can learn the proposed email-act cat-
egories reasonably well. It was also shown that
there is an acceptable level of human agreement
over the categories. In experiments using different
human annotators, Kappa values between 0.72 and
0.85 were obtained. The Kappa statistic (Carletta,
1996) is typically used to measure the human inter-
rater agreement. Its values ranges from -1 (com-
plete disagreement) to +1 (perfect agreement) and
it is defined as (A-R)/(1-R), where A is the empiri-
cal probability of agreement on a category, and R is
the probability of agreement for two annotators that
36
label documents at random (with the empirically ob-
served frequency of each label).
3 The Corpus
The CSpace email corpus used in this paper con-
tains approximately 15,000 email messages col-
lected from a management course at Carnegie Mel-
lon University. This corpus originated from work-
ing groups who signed agreements to make certain
parts of their email accessible to researchers. In this
course, 277 MBA students, organized in approxi-
mately 50 teams of four to six members, ran sim-
ulated companies in different market scenarios over
a 14-week period (Kraut et al, ). The email tends to
be very task-oriented, with many instances of task
delegation and negotiation.
Messages were mostly exchanged with members
of the same team. Accordingly, we partitioned the
corpus into subsets according to the teams. The 1F3
team dataset has 351 messages total, while the 2F2,
3F2, 4F4 and 11F1 teams have, respectively, 341,
443, 403 and 176 messages. All 1716 messages
were labeled according to the taxonomy in Figure
1.
4 N-gram Features
In this section we detail the preprocessing step and
the feature selection applied to all email acts.
4.1 Preprocessing
Before extracting the n-grams features, a sequence
of preprocessing steps was applied to all email mes-
sages in order to emphasize the linguistic aspects of
the problem. Unless otherwise mentioned, all pre-
processing procedures were applied to all acts.
Initially, forwarded messages quoted inside email
messages were deleted. Also, signature files and
quoted text from previous messages were removed
from all messages using a technique described else-
where (Carvalho and Cohen, 2004). A similar clean-
ing procedure was executed by Cohen et al (2004).
Some types of punctuation marks (?,;:.)(][?) were
removed, as were extra spaces and extra page
breaks. We then perform some basic substitutions
such as: from ??m? to ? am?, from ??re? to ? are?,
from ??ll? to ? will?, from ?won?t? to ?will not?,
from ?doesn?t? to ?does not? and from ??d? to ?
would?.
Any sequence of one or more numbers was re-
placed by the symbol ?[number]?. The pattern
?[number]:[number]? was replaced with ?[hour]?.
The expressions ?pm or am? were replaced by
?[pm]?. ?[wwhh]? denoted the words ?why, where,
who, what or when?. The words ?I, we, you, he,
she or they? were replaced by ?[person]?. Days
of the week (?Monday, Tuesday, ..., Sunday?) and
their short versions (i.e., ?Mon, Tue, Wed, ..., Sun?)
were replaced by ?[day]?. The words ?after, before
or during? were replaced by ?[aaafter]?. The pro-
nouns ?me, her, him, us or them? were substituted by
?[me]?. The typical filename types ?.doc, .xls, .txt,
.pdf, .rtf and .ppt? were replaced by ?.[filetype]?. A
list with some of these substitutions is illustrated in
Table 1.
Symbol Pattern
[number] any sequence of numbers
[hour] [number]:[number]
[wwhh] ?why, where, who, what, or when?
[day] the strings ?Monday, Tuesday, ..., or Sunday?
[day] the strings ?Mon, Tue, Wed, ..., or Sun?
[pm] the strings ?P.M., PM, A.M. or AM?
[me] the pronouns ?me, her, him, us or them?
[person] the pronouns ?I, we, you, he, she or they?
[aaafter] the strings ?after, before or during?
[filetype] the strings ?.doc, .pdf, .ppt, .txt, or .xls?
Table 1: Some PreProcessing Substitution Patterns
For the Commit act only, references to the first
person were removed from the symbol [person] ?
i.e., [person] was used to replace ?he, she or they?.
The rationale is that n-grams containing the pronoun
?I? are typically among the most meaningful for this
act (as shall be detailed in Section 4.2).
4.2 Most Meaningful N-grams
After preprocessing the 1716 email messages, n-
gram sequence features were extracted. In this pa-
per, n-gram features are all possible sequences of
length 1 (unigrams or 1-gram), 2 (bigram or 2-
gram), 3 (trigram or 3-gram), 4 (4-gram) and 5 (5-
gram) terms. After extracting all n-grams, the new
dataset had more than 347500 different features. It
would be interesting to know which of these n-grams
are the ?most meaningful? for each one of email
speech acts.
37
1-gram 2-gram 3-gram 4-gram 5-gram
? do [person] [person] need to [wwhh] do [person] think [wwhh] do [person] think ?
please ? [person] [wwhh] do [person] do [person] need to let [me] know [wwhh] [person]
[wwhh] could [person] let [me] know and let [me] know a call [number]-[number]
could [person] please would [person] call [number]-[number] give [me] a call [number]
do ? thanks do [person] think would be able to please give give [me] a call
can are [person] are [person] meeting [person] think [person] need [person] would be able to
of can [person] could [person] please let [me] know [wwhh] take a look at it
[me] need to do [person] need do [person] think ? [person] think [person] need to
Table 2: Request Act:Top eight N-grams Selected by Information Gain.
One possible way to accomplish this is using
some feature selection method. By computing the
Information Gain score (Forman, 2003; Yang and
Pedersen, 1997) of each feature, we were able to
rank the most ?meaningful? n-gram sequence for
each speech act. The final rankings are illustrated
in Tables 2 and 3.
Table 2 shows the most meaningful n-grams for
the Request act. The top features clearly agree with
the linguistic intuition behind the idea of a Request
email act. This agreement is present not only in
the frequent 1g features, but also in the 2-grams,
3-grams, 4-grams and 5-grams. For instance, sen-
tences such as ?What do you think ?? or ?let me
know what you ...? can be instantiations of the top
two 5-grams, and are typically used indicating a re-
quest in email communication.
Table 3 illustrates the top fifteen 4-grams for all
email speech acts selected by Information Gain. The
Commit act reflects the general idea of agreeing to
do some task, or to participate in some meeting. As
we can see, the list with the top 4-grams reflects the
intuition of commitment very well. When accepting
or committing to a task, it is usual to write emails
using ?Tomorrow is good for me? or ?I will put the
document under your door? or ?I think I can finish
this task by 7? or even ?I will try to bring this to-
morrow?. The list even has some other interesting
4-grams that can be easily associated to very specific
commitment situations, such as ?I will bring copies?
and ?I will be there?.
Another act in Table 3 that visibly agrees with
its linguistic intuition is Meeting. The 4-grams
listed are usual constructions associated with ei-
ther negotiating a meeting time/location (?[day] at
[hour][pm]?), agreeing to meet (?is good for [me]?)
or describing the goals of the meeting (?to go over
the?).
The top features associated with the dData act in
Table 3 are also closely related to its general intu-
ition. Here the idea is delivering or requesting some
data: a table inside the message, an attachment, a
document, a report, a link to a file, a url, etc. And
indeed, it seems to be exactly the case in Table 3:
some of the top 4-grams indicate the presence of an
attachment (e.g., ?forwarded message begins here?),
some features suggest the address or link where a file
can be found (e.g., ?in my public directory? or ?in
the etc directory?), some features request an action
to access/read the data (e.g., ?please take a look?)
and some features indicate the presence of data in-
side the email message, possibly formatted as a table
(e.g., ?[date] [hour] [number] [number]? or ?[date]
[day] [number] [day]?).
From Table 3, the Propose act seems closely re-
lated to the Meeting act. In fact, by checking the
labeled dataset, most of the Proposals were associ-
ated with Meetings. Some of the features that are not
necessarily associated with Meeting are ? [person]
would like to?, ?please let me know? and ?was hop-
ing [person] could?.
The Deliver email speech act is associated with
two large sets of actions: delivery of data and deliv-
ery of information in general. Because of this gener-
ality, is not straightforward to list the most meaning-
ful n-grams associated with this act. Table 3 shows
a variety of features that can be associated with a
Deliver act. As we shall see in Section 5, the De-
liver act has the highest error rate in the classifica-
tion task.
In summary, selecting the top n-gram features
via Information Gain revealed an impressive agree-
ment with the linguistic intuition behind the differ-
ent email speech acts.
38
Request Commit Meeting
[wwhh] do [person] think is good for [me] [day] at [hour] [pm]
do [person] need to is fine with [me] on [day] at [hour]
and let [me] know i will see [person] [person] can meet at
call [number]-[number] i think i can [person] meet at [hour]
would be able to i will put the will be in the
[person] think [person] need i will try to is good for [me]
let [me] know [wwhh] i will be there to meet at [hour]
do [person] think ? will look for [person] at [hour] in the
[person] need to get $[number] per person [person] will see [person]
? [person] need to am done with the meet at [hour] in
a copy of our at [hour] i will [number] at [hour] [pm]
do [person] have any [day] is fine with to go over the
[person] get a chance each of us will [person] will be in
[me] know [wwhh] i will bring copies let?s plan to meet
that would be great i will do the meet at [hour] [pm]
dData Propose Deliver
? forwarded message begins [person] would like to forwarded message begins here
forwarded message begins here would like to meet [number] [number] [number] [number]
is in my public please let [me] know is good for [me]
in my public directory to meet with [person] if [person] have any
[person] have placed the [person] meet at [hour] if fine with me
please take a look would [person] like to in my public directory
[day] [hour] [number] [number] [person] can meet tomorrow [person] will try to
[number] [day] [number] [hour] an hour or so is in my public
[date] [day] [number] [day] meet at [hour] in will be able to
in our game directory like to get together just wanted to let
in the etc directory [hour] [pm] in the [pm] in the lobby
the file name is [after] [hour] or [after] [person] will be able
is in our game [person] will be available please take a look
fyi ? forwarded message think [person] can meet can meet in the
just put the file was hoping [person] could [day] at [hour] is
my public directory under do [person] want to in the commons at
Table 3: Top 4-grams Selected by Information Gain
5 Experiments
Here we describe how the classification experiments
on the email speech acts dataset were carried out.
Using all n-gram features, we performed 5-fold
crossvalidation tests over the 1716 email messages.
Linear SVM1 was used as classifier. Results are il-
lustrated in Figure 2.
Figure 2 shows the test error rate of four dif-
ferent experiments (bars) for all email acts. The
first bar denotes the error rate obtained by Cohen
et al (2004) in a 5-fold crossvalidation experiment,
also using linear SVM. Their dataset had 1354 email
messages, and only 1-gram features were extracted.
The second bar illustrates the error rate obtained
using only 1-gram features with additional data. In
this case, we used 1716 email messages. The third
bar represents the the same as the second bar (1-
1We used the LIBSVM implementation (Chang and Lin,
2001) with default parameters.
gram features with 1716 messages), with the differ-
ence that the emails went through the preprocessing
procedure previously described.
The fourth bar shows the error rate when all 1-
gram, 2-gram and 3-gram features are used and the
1716 messages go through the preprocessing proce-
dure. The last bar illustrates the error rate when all
n-gram features (i.e., 1g+2g+3g+4g+5g) are used in
addition to preprocessing in all 1716 messages.
In all acts, a consistent improvement in 1-gram
performance is observed when more data is added,
i.e., a drop in error rate from the first to the sec-
ond bar. Therefore, we can conclude that Cohen et
al. (2004) could have obtained better results if they
had used more labeled data.
A comparison between the second and third bars
reveals the extent to which preprocessing seems to
help classification based on 1-grams only. As we
can see, no significant performance difference can
be observed: for most acts the relative difference is
39
Figure 2: Error Rate 5-fold Crossvalidation Experiment
very small, and in one or maybe two acts some small
improvement can be noticed.
A much larger performance improvement can be
seen between the fourth and third bars. This reflects
the power of the contextual features: using all 1-
grams, 2-grams and 3-grams is considerably more
powerful than using only 1-gram features. This
significant difference can be observed in all acts.
Compared to the original values from (Cohen et
al., 2004), we observed a relative error rate drop of
24.7% in the Request act, 33.3% in the Commit act,
23.7% for the Deliver act, 38.3% for the Propose
act, 9.2% for Meeting and 29.1% in the dData act.
In average, a relative improvement of 26.4% in error
rate.
We also considered adding the 4-gram and 5-gram
features to the best system. As pictured in the last
bar of Figure 2, this addition did not seem to im-
prove the performance and, in some cases, even a
small increase in error rate was observed. We be-
lieve this was caused by the insufficient amount of
labeled data in these tests; and the 4-gram and 5-
gram features are likely to improve the performance
of this system if more labeled data becomes avail-
able.
Precision versus recall curves of the Request act
classification task are illustrated in Figure 3. The
curve on the top shows the Request act performance
when the preprocessing step cues and n-grams pro-
posed in Section 4 are applied. For the bottom curve,
only 1g features were used. These two curves corre-
spond to the second bar (bottom curve) and forth bar
(top curve) in Figure 2. Figure 3 clearly shows that
both recall and precision are improved by using the
contextual features.
To summarize, these results confirm the intuition
that contextual information (n-grams) can be very
effective in the task of email speech act classifica-
tion.
40
Figure 3: Precision versus Recall of the Request Act
Classification
6 The Ciranda Package
Ciranda is an open source package for Email Speech
Act prediction built on the top of the Minorthird
package (Cohen, 2004). Among other features,
Ciranda allows customized feature engineering, ex-
traction and selection. Email Speech Act classi-
fiers can be easily retrained using any learning al-
gorithm from the Minorthird package. Ciranda is
currently available from http://www.cs.cmu.
edu/?vitor.
7 Conclusions
In this work we considered the problem of automat-
ically detecting the intents behind email messages
using a shallow semantic taxonomy called ?email
speech acts? (Cohen et al, 2004). We were in-
terested in the task of classifying whether or not
an email message contains acts such as ?propose a
meeting? or ?deliver data?.
By exploiting contextual information in emails
such as n-gram sequences, we were able to notice-
ably improve the classification performance on this
task. Compared to the original study (Cohen et al,
2004), this representation reduced the classification
error rates by 26.4% on average. Improvements of
more than 30% were observed for some acts (Pro-
pose and Commit).
We also showed that the selection of the top n-
gram features via Information Gain revealed an im-
pressive agreement with the linguistic intuition be-
hind the different email speech acts.
References
[Carletta1996] Jean Carletta. 1996. Assessing agreement
on classification tasks: The kappa statistic. Computa-
tional Linguistics, 22(2):249?254.
[Carvalho and Cohen2004] Vitor R. Carvalho and
William W. Cohen. 2004. Learning to extract signa-
ture and reply lines from email. In Proceedings of the
Conference on Email and Anti-Spam, Palo Alto, CA.
[Chang and Lin2001] Chih-Chung Chang and Chih-
Jen Lin, 2001. LIBSVM: a library for sup-
port vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Cohen et al2004] William W. Cohen, Vitor R. Carvalho,
and Tom M. Mitchell. 2004. Learning to classify
email into ?speech acts?. In Proceedings of Empiri-
cal Methods in Natural Language Processing, pages
309?316, Barcelona, Spain, July.
[Cohen2004] William W. Cohen, 2004. Minorthird:
Methods for Identifying Names and Ontological Re-
lations in Text using Heuristics for Inducing Reg-
ularities from Data. http://minorthird.
sourceforge.net.
[Forman2003] George Forman. 2003. An extensive em-
pirical study of feature selection metrics for text classi-
fication. The Journal of Machine Learning Research,
3:1289?1305.
[Khoussainov and Kushmerick2005] Rinat Khoussainov
and Nicholas Kushmerick. 2005. Email task man-
agement: An iterative relational learning approach. In
Conference on Email and Anti-Spam (CEAS?2005).
[Kraut et al] R.E. Kraut, S.R. Fussell, F.J. Lerch, and
A. Espinosa. Coordination in teams: Evidence from
a simulated management game. To appear in the Jour-
nal of Organizational Behavior.
[Leusky2004] Anton Leusky. 2004. Email is a stage:
Discovering people roles from email archives. In ACM
Conference on Research and Development in Informa-
tion Retrieval (SIGIR).
[Searle1975] J. R. Searle. 1975. A taxonomy of illo-
cutionary acts. In In K. Gunderson (Ed.), Language,
Mind and Knowledge., pages 344?369, Minneapolis,
MN. University of Minnesota Press.
[Yang and Pedersen1997] Yiming Yang and Jan O. Peder-
sen. 1997. A comparative study on feature selection in
text categorization. In Proceedings of ICML-97, 14th
International Conference on Machine Learning, pages
412?420.
41
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 607?610,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
The Intelius Nickname Collection:
Quantitative Analyses from Billions of Public Records
Vitor R. Carvalho, Yigit Kiran and Andrew Borthwick
Intelius Data Research
500 108th Avenue NE, Bellevue, WA 98004
{vcarvalho,ykiran,aborthwick}@intelius.com
Abstract
Although first names and nicknames in the
United States have been well documented,
there has been almost no quantitative analysis
on the usage and association of these names
amongst themselves. In this paper we in-
troduce the Intelius Nickname Collection, a
quantitative compilation of millions of name-
nickname associations based on information
gathered from billions of public records. To
the best of our knowledge, this is the largest
collection of its kind, making it a natural re-
source for tasks such as coreference resolu-
tion, record linkage, named entity recogni-
tion, people and expert search, information ex-
traction, demographic and sociological stud-
ies, etc. The collection will be made freely
available.
1 Introduction
Nicknames are descriptive, invented person names
that are frequently used in addition or instead of the
person?s official name. Very often nicknames are
truncated forms of the original name that can be used
for convenience ? for instance, ?Betsy? instead of
?Elizabeth?.
Previous studies on nicknames have mostly fo-
cused on their origins or common descriptions. The
Oxford Dictionary of First Names (Hanks et al,
2007), for instance, presents a comprehensive de-
scription of origins and common uses of most nick-
names in modern English. More quantitative explo-
rations of the subject, such as the one provided by
Alias Conditional Probability
Betty 4.51%
Beth 3.83%
Liz 3.34%
Elisabeth 0.95%
Betsy 0.92%
Table 1: Nickname Distribution Sample for ?Elizabeth?
the US Social Security Office1 tend to focus on baby
name selection and on the relative popularity of most
common first names.
In this paper we present a quantitative study on
nickname usage in the United States. Using bil-
lions of personal public records and a state-of-the-
art large-scale record linkage system, we were able
to generate a comprehensive dataset with millions
of name-nickname associations and their relative
strength. A small sample of this collection can
be seen in Table 1, where the most frequent nick-
names associated with the first name ?Elizabeth?
and their Conditional Alias Probabilities. We ex-
plain the derivation of these probabilities in detail
in Section 3.3. This collection can provide valu-
able features and insights for applications as diverse
as entity extraction, coreference resolution, people
search, language modeling, and machine translation.
It will be made freely available for download from
the Linguistic Data Consortium.
1Popular Baby Names from Social Security Online:
http://www.ssa.gov/OACT/babynames/
607
2 Prior Work
To the best of our knowledge, there are no com-
prehensive, empirically derived nickname databases
currently made freely available for research pur-
poses. (Bollacker, 2008) contains an extensive
database of names and nicknames2, with listings
on over 13,000 given names, containing multi-
ple ?variations? for each name. However, this
database makes no attempt to distinguish between
common and less common variants and skips some
very common nicknames. For instance, the en-
try for ?William? lists ?Wilmot? and ?Wilton?
as variants of William but does not list ?Bill?
or ?Billy?. (Meranda, 1998) provides a more
useful database which appears to also be manu-
ally constructed. The database is in the form of
Name1|Name2|?substitution likelihood?, but the au-
thor states in the comments that the substitution like-
lihood is ?mostly guesswork? and the data contains
numerous coverage gaps. For instance, common
nicknames such as ?Jack?, ?Willy?, and ?Sally? are
all missing.
3 Generating the Nickname Distribution
The nickname collection was derived from billions
of public, commercial and web records that power a
major commercial People Search Engine. The pro-
cess described below associates all records belong-
ing to a particular person into clusters, and from
these clusters it constructs a final person profile that
is used to derive name-alias associations. The entire
process is briefly described below.
3.1 Data Collection and Cleaning
The process starts by collecting billions of personal
records from three different sources of U.S. per-
sonal records. The first source is derived from US
government records, such as marriage, divorce and
death records. The second is derived from publicly
available web profiles, such as professional and so-
cial network public profiles. The third type is de-
rived from commercial sources, such as financial
and property reports (e.g., information made public
after buying a house).
After collection and categorization, all records go
through a cleaning process that starts with the re-
2http://www.freebase.com/view/base/givennames/given name
moval of bogus, junk and spam records. Then all
records are normalized to an approximately com-
mon representation. Then finally, all major noise
types and inconsistencies are addressed, such as
empty/bogus fields, field duplication, outlier values
and encoding issues. At this point, all records are
ready for the Record Linkage process.
3.2 Record Linkage Process
The Record Linkage process should link together
all records belonging to the same real-world per-
son. That is, this process should turn billions of in-
put records into a few hundred million clusters of
records (or profiles), where each cluster is uniquely
associated with a real-world unique individual.
Our system follows the standard high-level struc-
ture of a record linkage pipeline (Elmagarmid et al,
2007) by being divided into four major components:
1) data cleaning 2) blocking 3) pair-wise linkage and
4) clustering. The data cleaning step was described
above. The blocking step uses a new algorithm im-
plemented in MapReduce (Dean et al, 2004) which
groups records by shared properties to determine
which pairs of records should be examined by the
pairwise linker as potential duplicates. The linkage
step assigns a score to pairs of records using a super-
vised pairwise-based machine learning model whose
implementation is described in detail in (Sheng et
al., 2011) and achieves precision in excess of 99.5%
with recall in excess of 80%, as measured on a ran-
dom set with tens of thousands of human labels.
If a pair scores above a user-defined threshold, the
records are presumed to represent the same person.
The clustering step first combines record pairs into
connected components and then further partitions
each connected component to remove inconsistent
pair-wise links. Hence at the end of the entire record
linkage process, the system has partitioned the input
records into disjoint sets called profiles, where each
profile corresponds to a single person. While the
task is very challeging (e.g., many people share com-
mon names such as ?John Smith?) and this process
is far from perfect, it is working sufficiently well to
power multiple products at Intelius, including a ma-
jor people search engine.
608
3.3 Algorithm
We used the MapReduce framework (Dean et al,
2004) to accomodate operations over very large
datasets. The main goal of this task is to preserve
the relationship amongst different names inside a
profile. The algorithm?s pseudocode is illustrated in
Figure 1.
Many different names can be listed under a pro-
file, including the real name (e.g., the ?official? or
?legal? name), nicknames, diminutives, typos, etc.
In the first phase of the algorithm, a mapper visits all
profiles to reveal these names and outputs a <key,
value>pair for each name token. The keys are the
names, and the values are a list with all other names
found in the profile. This is a safe approach since we
do not attempt to determine whether a given token is
an original name, a diminutive or a typo. Hence-
forth, we refer to the key as Name and the values as
Aliases.
The reducer will merge all alias lists of a given
name, and count, aggregate and filter them. Since
the mapper function produces intermediate pairs
with all different names seen inside a profile, re-
ducing them will create a bi-directional relation be-
tween names and aliases, where one can search for
all aliases of a name as well as the reverse. The re-
ducer also estimates conditional probabilities of the
aliases. The Conditional Alias Probability (CAP)
of an alias defines the probability of an alias being
used to denote a person with a given name. Specifi-
cally, It can be expressed as CAP (aliasi|namej) =
count(aliasi?namej)
count(namej)
, where the count() operator re-
turns the number of profiles satisfying its criteria.
Processing a large number of profiles creates a
huge alias lists for each name. Even worse, most
of the aliases in that list are typos or very unique
nicknames that would not be considered a typical
alias for the name. In order to help control this
noise, we used the following parameters in the al-
gorithm. Alias Count Minimum sets the minimum
number of profiles that should have an alias for
the alias to be included. Total Count Minimum
determines whether we output the whole set of
name and aliases. It is determined by comput-
ing the total number of occurrences of the name.
CAP Threshold forces the reducer to filter out
aliases whose probability is below a threshold.
MAP(profile)
1 names := ?
2 for name ? profile
3 names := names ? name
4 for current name ? names
5 aliases := ?
6 for other name ? names
7 if current name 6= other name
8 aliases := aliases ? other name
9 EMIT(current name, aliases)
REDUCE(key , values)
1 aliaslist := ?
2 for record ? values
3 if aliaslist .contains(record)
4 INCREMENT(aliaslist [record ])
5 else
6 aliaslist [record ] := 1;
7 SORT-BY-COUNT(aliaslist)
8 COMPUTE-FREQUENCIES(aliaslist)
9 FILTER(aliaslist)
10 EMIT(key , aliaslist)
Figure 1: MapReduce Nickname Extractor algorithm
3.4 Analysis
The number of generated name-alias associations
depends largely on the specific parameter set used
in by the algorithm. While different applications
may benefit from different parameters, many of our
internal applications had success using the follow-
ing set of parameters: Total Count Minimum =
100, Alias Count Minimum = 10, and
CAP Threshold = 0.1%. Using this parameter
set, the process generated 331,237 name-alias pairs.
Table 2 shows CAP values for various name-
alias pairs. As expected, notice that values
of CAP (X|Y ) can be completely different from
CAP (Y |X), as in the case of ?Monica? and
?Monic?. The collection also shows that completely
unrelated names can be associated to a short alias,
such as ?Al?. Notice also that very frequent ty-
pos, such as?Jefffrey?, are also part of the collection.
Finally, very common name abbreviations such as
?Jas? for ?James? are also part of the set as long as
they are statistically relevant.
609
Figure 2: Conditional Probability of ?William??s Aliases over the Decades in the US.
X Y CAP (Y |X)
Monica Monika 1.00%
Monica Monic 0.26%
Monic Monica 38.76%
Al Albert 14.83%
Al Alfred 8.28%
Al Alan 4.96%
Jas James 71.94%
Jas Jim 7.54%
James Jas 2.09%
Jefffrey Jeffrey 40.04%
Jefffrey Jeff 25.69%
Table 2: Sample CAPs For Multiple Aliases.
3.5 Limitations and Future Explorations
It is important to keep in mind that the collection
is only valid for adults in the USA. Also, despite the
noise reduction obtained by the algorithm thresholds
in Section 3.3, some cases of frequent typos, for-
eign spellings/transliterations, and abbreviations are
still statistically indistinguishable from actual nick-
names. For instance, ?WM? (a common abbreviation
of William) is as frequent as many of its nicknames.
While we could have used a human-edited list to fil-
ter out these cases, we decided to keep it in the col-
lection because some applications may benefit from
this information. A coreference application, for in-
stance, could infer that ?Wm Jones? and ?William
Jones? have a high probability of being the same per-
son.
Looking forward, there are multiple directions
to explore. Besides names, the final record clus-
ters generally contain other information such as ad-
dresses, date of birth (DOB), professional titles, etc.
As an example, Figure 2 illustrates the probability of
the most frequent nicknames of ?William? for people
born over different decades in the US. It is interest-
ing to notice that, while ?Bill? was the most likely
nickname for people born between the 1940s and
1980s, ?Will? has become significantly more popu-
lar since the 80s - to the point that it has become
the most likely nickname in the 90s. We believe our
next steps will include investigating various migra-
tion, economic, sociological and demographic pat-
terns while also leveraging this information in record
linkage and coreference resolution modules.
References
K Bollacker, C. Evans, P. Paritosh, et al 2008. Freebase: A
collaboratively created graph database for structuring hu-
man knowledge. ACM SIGMOD.
Ahmed Elmagarmid, Panagiotis Ipeirotis and Vassilios
Verykios 2007. Duplicate Record Detection: A Survey.
IEEE TKDE 19 (1)
Patrick Hanks, Hardcastle Kate and Flavia Hodges 2007 Ox-
ford Dictionary of First Names. Oxford University Press,
USA, 2nd edition, ISBN 978-0-19-861060-1.
Deron Meranda 1998 Most Common Nicknames for First
Names http://deron.meranda.us/data.
Jean-Baptiste Michel et al 2011 Quantitative Analysis of Cul-
ture Using Millions of Digitized Books. Science, Vol. 331
no. 6014 pp. 176-182
Sheng Chen, Andrew Borthwick and Vitor R. Carvalho 2011.
The Case for Cost-Sensitive and Easy-To-Interpret Models in
Industrial Record Linkage. International Workshop on Qual-
ity in Databases VLDB-2011
Jeff Dean and Sanjay Ghemawat 2004. MapReduce: Simplified
Data Processing on Large Clusters Symposium on Operat-
ing System Design and Implementation OSDI-2004
610
