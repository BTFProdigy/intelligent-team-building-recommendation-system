Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 365?372,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Quality Estimation for Machine Translation Using the Joint Method 
of Evaluation Criteria and Statistical Modeling 
 
 
Aaron Li-Feng Han 
hanlifengaaron@gmail.com 
Yi Lu 
mb25435@umac.mo 
Derek F. Wong 
derekfw@umac.mo 
   
Lidia S. Chao 
lidiasc@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Junwen Xing 
mb15470@umac.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to introduce our participation in 
the WMT13 shared tasks on Quality Estima-
tion for machine translation without using ref-
erence translations. We submitted the results 
for Task 1.1 (sentence-level quality estima-
tion), Task 1.2 (system selection) and Task 2 
(word-level quality estimation). In Task 1.1, 
we used an enhanced version of BLEU metric 
without using reference translations to evalu-
ate the translation quality. In Task 1.2, we uti-
lized a probability model Na?ve Bayes (NB) as 
a classification algorithm with the features 
borrowed from the traditional evaluation met-
rics. In Task 2, to take the contextual infor-
mation into account, we employed a discrimi-
native undirected probabilistic graphical mod-
el Conditional random field (CRF), in addition 
to the NB algorithm. The training experiments 
on the past WMT corpora showed that the de-
signed methods of this paper yielded promis-
ing results especially the statistical models of 
CRF and NB. The official results show that 
our CRF model achieved the highest F-score 
0.8297 in binary classification of Task 2. 
 
1 Introduction 
Due to the fast development of Machine transla-
tion, different automatic evaluation methods for 
the translation quality have been proposed in re-
cent years. One of the categories is the lexical 
similarity based metric. This kind of metrics in-
cludes the edit distance based method, such as 
WER (Su et al, 1992), Multi-reference WER 
(Nie?en et al, 2000), PER (Tillmann et al, 
1997), the works of (Akiba, et al, 2001), 
(Leusch et al, 2006) and (Wang and Manning, 
2012); the precision based method, such as 
BLEU (Papineni et al, 2002), NIST (Doddington, 
2002), and SIA (Liu and Gildea, 2006); recall 
based method, such as ROUGE (Lin and Hovy 
2003); and the combination of precision and re-
call, such as GTM (Turian et al, 2003), METE-
OR (Lavie and Agarwal, 2007), BLANC (Lita et 
al., 2005), AMBER (Chen and Kuhn, 2011), 
PORT (Chen et al, 2012b), and LEPOR (Han et 
al., 2012). 
Another category is the using of linguistic fea-
tures. This kind of metrics includes the syntactic 
similarity, such as the POS information used by 
TESLA (Dahlmeier et al, 2011), (Liu et al, 
2010) and (Han et al, 2013), phrase information 
used by (Povlsen, et al, 1998) and (Echizen-ya 
and Araki, 2010), sentence structure used by 
(Owczarzak et al, 2007); the semantic similarity, 
such as textual entailment used by (Mirkin et al, 
2009) and (Castillo and Estrella, 2012), Syno-
nyms used by METEOR (Lavie and Agarwal, 
2007), (Wong and Kit, 2012), (Chan and Ng, 
2008); paraphrase used by (Snover et al, 2009). 
The traditional evaluation metrics tend to 
evaluate the hypothesis translation as compared 
to the reference translations that are usually of-
fered by human efforts. However, in the practice, 
there is usually no golden reference for the trans-
lated documents, especially on the internet works. 
How to evaluate the quality of automatically 
translated documents or sentences without using 
the reference translations becomes a new chal-
lenge in front of the NLP researchers. 
365
ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB X . 
ADJ PREP, 
PREP/DEL 
ADV, 
NEG 
CC, 
CCAD, 
CCNEG, 
CQUE, 
CSUBF, 
CSUBI, 
CSUBX 
ART NC, 
NMEA, 
NMON, 
NP, 
PERCT,  
UMMX 
CARD, 
CODE, 
QU 
DM, 
INT, 
PPC, 
PPO, 
PPX, 
REL 
SE VCLIger, 
VCLIinf, 
VCLIfin, 
VEadj, 
VEfin, 
VEger, 
VEinf, 
VHadj, 
VHfin, 
VHger, 
VHinf, 
VLadj, 
VLfin, 
VLger, 
VLinf, 
VMadj, 
VMfin, 
VMger, 
VMinf, 
VSadj, 
VSfin, 
VSger, 
VSinf 
ACRNM, 
ALFP, 
ALFS, 
FO, ITJN, 
ORD, 
PAL, 
PDEL, 
PE, PNC, 
SYM 
BACKSLASH, 
CM, COLON, 
DASH, DOTS, 
FS, LP, QT, 
RP, SEMICO-
LON, SLASH 
Table 1: Developed POS mapping for Spanish and universal tagset 
 
2 Related Works 
Gamon et al (2005) perform a research about 
reference-free SMT evaluation method on sen-
tence level. This work uses both linear and non-
linear combinations of language model and SVM 
classifier to find the badly translated sentences. 
Albrecht and Hwa (2007) conduct the sentence-
level MT evaluation utilizing the regression 
learning and based on a set of weaker indicators 
of fluency and adequacy as pseudo references. 
Specia and Gimenez (2010) use the Confidence 
Estimation features and a learning mechanism 
trained on human annotations. They show that 
the developed models are highly biased by diffi-
culty level of the input segment, therefore they 
are not appropriate for comparing multiple sys-
tems that translate the same input segments. Spe-
cia et al (2010) discussed the issues between the 
traditional machine translation evaluation and the 
quality estimation tasks recently proposed. The 
traditional MT evaluation metrics require refer-
ence translations in order to measure a score re-
flecting some aspects of its quality, e.g. the 
BLEU and NIST. The quality estimation ad-
dresses this problem by evaluating the quality of 
translations as a prediction task and the features 
are usually extracted from the source sentences 
and target (translated) sentences. They also show 
that the developed methods correlate better with 
human judgments at segment level as compared 
to traditional metrics. Popovi? et al (2011) per-
form the MT evaluation using the IBM model 
one with the information of morphemes, 4-gram 
POS and lexicon probabilities. Mehdad et al 
(2012) use the cross-lingual textual entailment to 
push semantics into the MT evaluation without 
using reference translations. This evaluation 
work mainly focuses on the adequacy estimation. 
Avramidis (2012) performs an automatic sen-
tence-level ranking of multiple machine transla-
tions using the features of verbs, nouns, sentenc-
es, subordinate clauses and punctuation occur-
rences to derive the adequacy information. Other 
descriptions of the MT Quality Estimation tasks 
can be gained in the works of (Callison-Burch et 
al., 2012) and (Felice and Specia, 2012). 
3 Tasks Information  
This section introduces the different sub-tasks we 
participated in the Quality Estimation task of 
WMT 13 and the methods we used.  
3.1 Task 1-1 Sentence-level QE 
Task 1.1 is to score and rank the post-editing 
effort of the automatically translated English-
Spanish sentences without offering the reference 
translation. 
Firstly, we develop the English and Spanish 
POS tagset mapping as shown in Table 1. The 75 
Spanish POS tags yielded by the Treetagger 
(Schmid, 1994) are mapped to the 12 universal 
tags developed in (Petrov et al, 2012). The Eng-
lish POS tags are extracted from the parsed sen-
tences using the Berkeley parser (Petrov et al, 
2006). 
Secondly, the enhanced version of BLEU 
(EBLEU) formula is designed with the factors of 
modified length penalty (   ), precision, and 
recall, the   and   representing the lengths of 
hypothesis (target) sentence and source sentence 
respectively. We use the harmonic mean of pre-
cision and recall, i.e.  (       ). We assign 
the weight values     and    , i.e. higher 
weight value is assigned to precision, which is 
different with METEOR (the inverse values). 
 
       
          (?      ( (       ))) (1) 
 
     {
   
 
            
   
 
            
 (2) 
 
    
                    
                                
 (3) 
 
    
                    
                                
 (4) 
366
Lastly, the scoring for the post-editing effort 
of the automatically translated sentences is per-
formed on the extracted POS sequences of the 
source and target languages. The evaluated per-
formance of EBLEU on WMT 12 corpus is 
shown in Table 2 using the Mean-Average-Error 
(MAE), Root-Mean-Squared-Error (RMSE).  
 
 Precision Recall MLP EBLEU 
MAE 0.17 0.19 0.25 0.16 
RMSE 0.22 0.24 0.30 0.21 
Table 2: Performance on the WMT12 corpus 
The official evaluation scores of the testing re-
sults on WMT 13 corpus are shown in Table 3. 
The testing results show similar scores as com-
pared to the training scores (the MAE score is 
around 0.16 and the RMSE score is around 0.22), 
which shows a stable performance of the devel-
oped model EBLEU. However, the performance 
of EBLEU is not satisfactory currently as shown 
in the Table 2 and Table 3. This is due to the fact 
that we only used the POS information as lin-
guistic feature. This could be further improved 
by the combination of lexical information and 
other linguistic features such as the sentence 
structure, phrase similarity, and text entailment. 
 
 MAE RMSE DeltaAvg 
Spearman 
Corr 
EBLEU 16.97 21.94 2.74 0.11 
Baseline 
SVM 
14.81 18.22 8.52 0.46 
Table 3: Performance on the WMT13 corpus 
3.2 Task 1-2 System Selection 
Task 1.2 is the system selection task on EN-ES 
and DE-EN language pairs. Participants are re-
quired to rank up to five alternative translations 
for the same source sentence produced by multi-
ple translation systems.  
Firstly, we describe the two variants of 
EBLEU method for this task. We score the five 
alternative translation sentences as compared to 
the source sentence according to the closeness of 
their POS sequences. The German POS is also 
extracted using Berkeley parser (Petrov et al, 
2006). The mapping of German POS to universal 
POS tagset is using the developed one in the 
work of (Petrov et al, 2012). When we convert 
the absolute scores into the corresponding rank 
values, the variant EBLEU-I means that we use 
five fixed intervals (with the span from 0 to 1) to 
achieve the alignment as shown in Table 4. 
[1,0.4) [0.4, 0.3) [0.3, 0.25) [0.25, 0.2) [0.2, 0] 
5 4 3 2 1 
Table 4: Convert absolute scores into ranks 
 
The alignment work from absolute scores to 
rank values shown in Table 4 is empirically de-
termined. We have made a statistical work on the 
absolute scores yielded by our metrics, and each 
of the intervals shown in Table 4 covers the simi-
lar number of sentence scores. 
On the other hand, in the metric EBLEU-A, 
?A? means average. The absolute sentence edit 
scores are converted into the five rank values 
with the same number (average number). For 
instance, if there are 1000 sentence scores in to-
tal then each rank level (from 1 to 5) will gain 
200 scores from the best to the worst. 
Secondly, we introduce the NB-LPR model 
used in this task. NB-LPR means the Na?ve 
Bayes classification algorithm using the features 
of Length penalty (introduced in previous sec-
tion), Precision, Recall and Rank values. NB-
LPR considers each of its features independently. 
Let?s see the conditional probability that is also 
known as Bayes? rule. If the  ( | )  is given, 
then the  ( | ) can be calculated as follows: 
 
  ( | )  
 ( | ) ( )
 ( )
 (5) 
 
Given a data point identified as 
 (          ) and the classifications 
 (          ), Bayes? rule can be applied to 
this statement: 
 
  (  |          )  
 (         |  ) (  )
 (         )
 (6) 
 
As in many practical applications, parameter 
estimation for NB-LPR model uses the method 
of maximum likelihood. For details of Na?ve 
Bayes algorithm, see the works of (Zhang, 2004) 
and (Harrington, 2012). 
Thirdly, the SVM-LPR model means the sup-
port vector machine classification algorithm us-
ing the features of Length penalty, Precision, 
Recall and Rank values, i.e. the same features as 
in NB-LPR. SVM solves the nonlinear classifica-
tion problem by mapping the data from a low 
dimensional space to a high dimensional space 
using the Kernel methods. In the projected high 
dimensional space, the problem usually becomes 
a linear one, which is easier to solve. SVM is 
also called maximum interval classifier because 
it tries to find the optimized hyper plane that 
367
separates different classes with the largest mar-
gin, which is usually a quadratic optimization 
problem. Let?s see the formula below, we should 
find the points with the smallest margin to the 
hyper plane and then maximize this margin. 
 
          {    (      ( 
    ))  
 
? ?
}
 (7) 
 
where   is normal to the hyper plane, || || is 
the Euclidean norm of  , and | | || ||  is the 
perpendicular distance from the hyper plane to 
the origin. For details of SVM, see the works of 
(Cortes and Vapnik, 1995) and (Burges, 1998). 
 
EN-ES 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.315 .399 .40s .304 .551 60.67s 
DE-EN 
NB-LPR SVM-LPR 
MAE RMSE Time MAE RMSE Time 
.318 .401 .79s .312 .559 111.7s 
Table 5: NB-LPR and SVM-LPR training 
In the training stage, we used all the officially 
released data of WMT 09, 10, 11 and 12 for the 
EN-ES and DE-EN language pairs. We used the 
WEKA (Hall et al, 2009) data mining software 
to implement the NB and SVM algorithms. The 
training scores are shown in Table 5. The NB-
LPR performs lower scores than the SVM-LPR 
but faster than SVM-LPR. 
 
 DE-EN EN-ES 
Methods 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
Tau(ties 
penalized) 
|Tau|(ties 
ignored) 
EBLEU-I -0.38 -0.03 -0.35 0.02 
EBLEU-A N/A N/A -0.27 N/A 
NB-LPR -0.49 0.01 N/A 0.07 
Baseline  -0.12 0.08 -0.23 0.03 
Table 6: QE Task 1.2 testing scores 
The official testing scores are shown in Table 
6. Each task is allowed to submit up to two sys-
tems and we submitted the results using the 
methods of EBLEU and NB-LPR. The perfor-
mance of NB-LPR on EN-ES language pair 
shows higher Tau score (0.07) than the baseline 
system score (0.03) when the ties are ignored. 
Because of the number limitation of submitted 
systems for each task, we did not submit the 
SVM-LPR results. However, the training exper-
iments prove that the SVM-LPR model performs 
better than the NB-LPR model though SVM-
LPR takes more time to run. 
3.3 Task 2 Word-level QE 
Task 2 is the word-level quality estimation of 
automatically translated news sentences from 
English to Spanish without given reference trans-
lations. Participants are required to judge each 
translated word by assigning a two- or multi-
class labels. In the binary classification, a good 
or a bad label should be judged, where ?bad? 
indicates the need for editing the token. In the 
multi-class classification, the labels include 
?keep?, ?delete? and ?substitute?. In addition to 
the NB method, in this task, we utilized a dis-
criminative undirected probabilistic graphical 
model, i.e. Conditional Random Field (CRF). 
CRF is early employed by Lefferty (Lefferty 
et al, 2001) to deal with the labeling problems of 
sequence data, and is widely used later by other 
researchers. As the preparation for CRF defini-
tion, we assume that   is a variable representing 
the input sequence, and   is another variable rep-
resenting the corresponding labels to be attached 
to  . The two variables interact as conditional 
probability  ( | )  mathematically. Then the 
definition of CRF: Let a graph model   (   ) 
comprise a set   of vertices or nodes together 
with a set   of edges or lines and      |  
  , such that   is indexed by the vertices of  , 
then (   ) shapes a CRF model. This set meets 
the following form:  
 
   ( | )      
(?     (   |   )       ?     (   |   )     )
 (8) 
 
where   and   represent the data sequence and 
label sequence respectively;    and    are the 
features to be defined;    and    are the parame-
ters trained from the datasets. We used the tool 
CRF++1 to implement the CRF algorithm. The 
features we used for the NB and CRF are shown 
in Table 7. We firstly trained the CRF and NB 
models on the officially released training corpus 
(produced by Moses and annotated by computing 
TER with some tweaks). Then we removed the 
truth labels in the training corpus (we call it 
pseudo test corpus) and labeled each word using 
the derived training models. The test results on 
the pseudo test corpus are shown in Table 8, 
                                                 
1 https://code.google.com/p/crfpp/ 
368
which specifies CRF performs better than NB 
algorithm. 
 
     (    ) 
Unigram, from antecedent 4th 
to subsequent 3rd token 
       
 (    ) 
Bigram, from antecedent 2nd 
to subsequent 2nd token 
      
Jump bigram, antecedent and 
subsequent token 
          
 (    ) 
Trigram, from antecedent 2nd 
to subsequent 2nd token 
Table 7: Developed features 
 
Binary 
CRF NB 
Training Accuracy Training Accuracy 
Itera=108 
Time=2.48s 
0.944 Time=0.59s 0.941 
Multi-classes 
CRF NB 
Training Accuracy Training Accuracy 
Itera=106 
Time=3.67s 
0.933 Time=0.55s 0.929 
Table 8: Performance on pseudo test corpus 
The official testing scores of Task 2 are shown 
in Table 9. We include also the results of other 
participants (CNGL and LIG) and their ap-
proaches. 
 
 Binary Multiclass 
Methods Pre Recall F1 Acc 
CNGL-
dMEMM 
0.7392 0.9261 0.8222 0.7162 
CNGL-
MEMM 
0.7554 0.8581 0.8035 0.7116 
LIG-All N/A N/A N/A 0.7192 
LIG-FS 0.7885 0.8644 0.8247 0.7207 
LIG-
BOOSTING 
0.7779 0.8843 0.8276 N/A 
NB 0.8181 0.4937 0.6158 0.5174 
CRF 0.7169 0.9846 0.8297 0.7114 
Table 9: QE Task 2 official testing scores 
The results show that our method CRF yields 
a higher recall score than other systems in binary 
judgments task, and this leads to the highest F1 
score (harmonic mean of precision and recall). 
The recall value reflects the loyalty to the truth 
data. The augmented feature set designed in this 
paper allows the CRF to take the contextual in-
formation into account, and this contributes 
much to the recall score. On the other hand, the 
accuracy score of CRF in multiclass evaluation is 
lower than LIG-FS method. 
4 Conclusions 
This paper describes the algorithms and features 
we used in the WMT 13 Quality Estimation tasks. 
In the sentence-level QE task (Task 1.1), we de-
velop an enhanced version of BLEU metric, and 
this shows a potential usage for the traditional 
evaluation criteria. In the newly proposed system 
selection task (Task 1.2) and word-level QE task 
(Task 2), we explore the performances of several 
statistical models including NB, SVM, and CRF, 
of which the CRF performs best, the NB per-
forms lower than SVM but much faster than 
SVM. The official results show that the CRF 
model yields the highest F-score 0.8297 in binary 
classification judgment of word-level QE task. 
Acknowledgments 
The authors are grateful to the Science and 
Technology Development Fund of Macau and 
the Research Committee of the University of 
Macau for the funding support for our research, 
under the reference No. 017/2009/A and 
RG060/09-10S/CS/FST. The authors also wish to 
thank the anonymous reviewers for many helpful 
comments. 
References  
Akiba, Yasuhiro, Kenji Imamura, and Eiichiro Sumita. 
2001. Using Multiple Edit Distances to Automati-
cally Rank Machine Translation Output. In Pro-
ceedings of the MT Summit VIII, Santiago de 
Compostela, Spain. 
Albrecht, Joshua, and Rebecca Hwa. 2007. Regres-
sion for sentence-level MT evaluation with pseudo 
references. ACL. Vol. 45. No. 1. 
Avramidis, Eleftherios. 2012. Comparative quality 
estimation: Automatic sentence-level ranking of 
multiple machine translation outputs. In Proceed-
ings of 24th International Conference on 
Computational Linguistics (COLING), pages 
115?132, Mumbai, India. 
Burges, Christopher J. C. 1998. A Tutorial on Support 
Vector Machines for Pattern Recognition. J. Data 
Min. Knowl. Discov. Volume 2 Issue 2, June 
1998, 121-167. Kluwer Academic Publishers 
Hingham, MA, USA. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh 
369
Workshop on Statistical Machine Translation, 
pages 10?51, Montr?al, Canada, June. 
Castillo, Julio and Paula Estrella. 2012. Semantic 
Textual Similarity for MT evaluation, Proceed-
ings of the 7th Workshop on Statistical Ma-
chine Translation (WMT2012), pages 52?58, 
Montre a?l, Canada, June 7-8. Association for 
Computational Linguistics. 
Chan, Yee Seng and Hwee Tou Ng. 2008. MAXSIM: 
A maximum similarity metric for machine transla-
tion evaluation. In Proceedings of ACL 2008: 
HLT, pages 55?62. Association for Computational 
Linguistics. 
Chen, Boxing and Roland Kuhn. 2011. Amber: A 
modified bleu, enhanced ranking metric. In Pro-
ceedings of the Sixth Workshop on Statistical 
Machine translation of the Association for 
Computational Linguistics(ACL-WMT), pages 
71-77, Edinburgh, Scotland, UK. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July. 
Cortes, Corinna and Vladimir Vapnik. 1995. Support-
Vector Networks, J. Machine Learning, Volume 
20, issue 3, pp 273-297. Kluwer Academic Pub-
lishers, Boston. Manufactured in The Netherlands. 
Dahlmeier, Daniel, Chang Liu, and Hwee Tou Ng. 
2011. TESLA at WMT2011: Translation evalua-
tion and tunable metric. In Proceedings of the 
Sixth Workshop on Statistical Machine Trans-
lation, Association for Computational Linguis-
tics (ACL-WMT), pages 78-84, Edinburgh, Scot-
land, UK. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Echizen-ya, Hiroshi and Kenji Araki. 2010. Automat-
ic evaluation method for machine translation using 
noun-phrase chunking. In Proceedings of ACL 
2010, pages 108?117. Association for Computa-
tional Linguistics. 
Gamon, Michael, Anthony Aue, and Martine Smets. 
2005. Sentence-level MT evaluation without refer-
ence translations: Beyond language modeling. 
Proceedings of EAMT. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann, Ian H. Witten. 2009. 
The WEKA data mining software: An update. 
SIGKDD Explorations, 11. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Han, Aaron Li-Feng, Derek F. Wong, Lidia S. Chao, 
Liangye He, Yi Lu, Junwen Xing and Xiaodong 
Zeng. 2013. Language-independent Model for Ma-
chine Translation Evaluation with Reinforced Fac-
tors. Proceedings of the 14th International 
Conference of Machine Translation Summit 
(MT Summit 2013), Nice, France. 
Harrington, Peter. 2012. Classifying with probability 
theory: na?ve bayes. Machine Learning in Ac-
tion, Part 1 Classification. Page 61-82. Publisher: 
Manning Publications. April. 
Lafferty, John, McCallum Andrew, and Pereira C.N. 
Ferando. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In Proceeding of 18th Internation-
al Conference on Machine Learning. 282-289. 
Lavie, Alon and Abhaya Agarwal. 2007. METEOR: 
An Automatic Metric for MT Evaluation with High 
Levels of Correlation with Human Judgments, 
Proceedings of the ACL Second Workshop on 
Statistical Machine Translation, pages 228-231, 
Prague, June. 
Leusch, Gregor, Nicola Ueffing, and Hermann Ney. 
2006. CDer: Efficient MT Evaluation Using Block 
Movements. In Proceedings of the 13th Confer-
ence of the European Chapter of the Associa-
tion for Computational Linguistics (EACL-06), 
241-248. 
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of 2003 
Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May 27 - June 
1. 
Lita, Lucian Vlad, Monica Rogati and Alon Lavie. 
2005. BLANC: Learning Evaluation Metrics for 
MT, Proceedings of Human Language Tech-
nology Conference and Conference on Empir-
ical Methods in Natural Language Processing 
(HLT/EMNLP), pages 740?747, Vancouver, Oc-
tober. Association for Computational Linguistics. 
Liu, Chang, Daniel Dahlmeier and Hwee Tou Ng. 
2010. TESLA: Translation evaluation of sentences 
370
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR. 
Liu, Ding and Daniel Gildea. 2006. Stochastic itera-
tive alignment for machine translation evaluation. 
Sydney. ACL06. 
Mariano, Felice and Lucia Specia. 2012. Linguistic 
Features for Quality Estimation. Proceedings of 
the 7th Workshop on Statistical Machine 
Translation, pages 96?103. 
Mehdad, Yashar, Matteo Negri, and Marcello Federi-
co. 2012. Match without a referee: evaluating MT 
adequacy without reference translations. Proceed-
ings of the Seventh Workshop on Statistical 
Machine Translation. Association for Compu-
tational Linguistics. 
Mirkin, Shachar, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, and Idan Szpektor. 2009. 
Source-Language Entailment Modeling for Trans-
lating Unknown Terms, Proceedings of the Joint 
Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the 
AFNLP, pages 791?799, Suntec, Singapore, 2-7. 
ACL and AFNLP. 
Nie?en, Sonja, Franz Josef Och, Gregor Leusch, and 
Hermann Ney. 2000. A Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. In Proceedings of the 2nd International 
Conference on Language Resources and Eval-
uation (LREC-2000). 
Owczarzak, Karolina, Josef van Genabith and Andy 
Way. 2007. Labelled Dependencies in Machine 
Translation Evaluation, Proceedings of the ACL 
Second Workshop on Statistical Machine 
Translation, pages 104-111, Prague. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja, David Vilar, Eleftherios Avramidis, 
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In 
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, Association for 
Computational Linguistics (ACL-WMT), pages 
99-103, Edinburgh, Scotland, UK. 
Povlsen, Claus, Nancy Underwood, Bradley Music, 
and Anne Neville. 1998. Evaluating Text-Type 
Suitability for Machine Translation a Case Study 
on an English-Danish System. Proceedings of the 
First Language Resources and Evaluation 
Conference, LREC-98, Volume I. 27-31. Grana-
da, Spain. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew G., Nitin Madnani, Bonnie Dorr, 
and Richard Schwartz. 2009. TER-Plus: paraphrase, 
semantic, and alignment enhancements to Transla-
tion Edit Rate. J. Machine Tranlslation, 23: 117-
127. 
Specia, Lucia and Gimenez, J. 2010. Combining Con-
fidence Estimation and Reference-based Metrics 
for Segment-level MT Evaluation. The Ninth 
Conference of the Association for Machine 
Translation in the Americas (AMTA). 
Specia, Lucia, Dhwaj Raj, and Marco Turchi. 2010. 
Machine Translation Evaluation Versus Quality 
Estimation. Machine Translation, 24:39?50. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France, July. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Turian, Joseph P., Luke Shen, and I. Dan Melamed. 
2003. Evaluation of Machine Translation and its 
Evaluation. In Machine Translation Summit IX, 
pages 386?393. International Association for Ma-
chine Translation. 
Wang, Mengqiu and Christopher D. Manning. 2012. 
SPEDE: Probabilistic Edit Distance Metrics for 
MT Evaluation, WMT2012, 76-83. 
Wong, Billy T. M. and Chunyu Kit. 2012. Extending 
Machine Translation Evaluation Metrics with Lex-
ical Cohesion to Document Level. Proceedings of 
the 2012 Joint Conference on Empirical 
371
Methods in Natural Language Processing and 
Computational Natural Language Learning, 
pages 1060?1068, Jeju Island, Korea, 12?14 July. 
Association for Computational Linguistics. 
Zhang, Harry. 2004. The Optimality of Naive Bayes. 
Proceedings of the Seventeenth International 
Florida Artificial Intelligence Research Socie-
ty Conference, Miami Beach, Florida, USA. 
AAAI Press. 
372
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414?421,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
A Description of Tunable Machine Translation Evaluation Systems in 
WMT13 Metrics Task 
 
 
Aaron L.-F. Han 
hanlifengaaron@gmail.com 
Derek F. Wong 
derekfw@umac.mo 
Lidia S. Chao 
lidiasc@umac.mo 
   
Yi Lu 
mb25435@umac.mo 
Liangye He 
wutianshui0515@gmail.com 
Yiming Wang 
mb25433@umac.mo 
Jiaji Zhou 
mb25473@uamc.mo 
    
Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory 
Department of Computer and Information Science 
University of Macau, Macau S.A.R. China 
 
  
 
Abstract 
This paper is to describe our machine transla-
tion evaluation systems used for participation 
in the WMT13 shared Metrics Task. In the 
Metrics task, we submitted two automatic MT 
evaluation systems nLEPOR_baseline and 
LEPOR_v3.1. nLEPOR_baseline is an n-gram 
based language independent MT evaluation 
metric employing the factors of modified sen-
tence length penalty, position difference penal-
ty, n-gram precision and n-gram recall. 
nLEPOR_baseline measures the similarity of 
the system output translations and the refer-
ence translations only on word sequences. 
LEPOR_v3.1 is a new version of LEPOR met-
ric using the mathematical harmonic mean to 
group the factors and employing some linguis-
tic features, such as the part-of-speech infor-
mation. The evaluation results of WMT13 
show LEPOR_v3.1 yields the highest average-
score 0.86 with human judgments at system-
level using Pearson correlation criterion on 
English-to-other (FR, DE, ES, CS, RU) lan-
guage pairs. 
1 Introduction 
Machine translation has a long history since the 
1950s (Weaver, 1955) and gains a fast develop-
ment in the recent years because of the higher 
level of computer technology. For instances, Och 
(2003) presents Minimum Error Rate Training 
(MERT) method for log-linear statistical ma-
chine translation models to achieve better trans-
lation quality; Menezes et al (2006) introduce a 
syntactically informed phrasal SMT system for 
English-to-Spanish translation using a phrase 
translation model, which is based on global reor-
dering and the dependency tree; Su et al (2009) 
use the Thematic Role Templates model to im-
prove the translation; Costa-juss? et al (2012) 
develop the phrase-based SMT system for Chi-
nese-Spanish translation using a pivot language. 
With the rapid development of Machine Transla-
tion (MT), the evaluation of MT has become a 
challenge in front of researchers. However, the 
MT evaluation is not an easy task due to the fact 
of the diversity of the languages, especially for 
the evaluation between distant languages (Eng-
lish, Russia, Japanese, etc.). 
2 Related works 
The earliest human assessment methods for ma-
chine translation include the intelligibility and 
fidelity used around 1960s (Carroll, 1966), and 
the adequacy (similar as fidelity), fluency and 
comprehension (improved intelligibility) (White 
et al, 1994). Because of the expensive cost of 
manual evaluations, the automatic evaluation 
metrics and systems appear recently. 
The early automatic evaluation metrics in-
clude the word error rate WER (Su et al, 1992) 
and position independent word error rate PER 
(Tillmann et al, 1997) that are based on the Le-
venshtein distance. Several promotions for the 
MT and MT evaluation literatures include the 
ACL?s annual workshop on statistical machine 
translation WMT (Koehn and Monz, 2006; Calli-
son-Burch et al, 2012), NIST open machine 
translation (OpenMT) Evaluation series (Li, 
2005) and the international workshop of spoken 
language translation IWSLT, which is also orga-
nized annually from 2004 (Eck and Hori, 2005; 
414
Paul, 2008, 2009; Paul, et al, 2010; Federico et 
al., 2011). 
BLEU (Papineni et al, 2002) is one of the 
commonly used evaluation metrics that is de-
signed to calculate the document level precisions. 
NIST (Doddington, 2002) metric is proposed 
based on BLEU but with the information weights 
added to the n-gram approaches. TER (Snover et 
al., 2006) is another well-known MT evaluation 
metric that is designed to calculate the amount of 
work needed to correct the hypothesis translation 
according to the reference translations. TER in-
cludes the edit categories such as insertion, dele-
tion, substitution of single words and the shifts of 
word chunks. Other related works include the 
METEOR (Banerjee and Lavie, 2005) that uses 
semantic matching (word stem, synonym, and 
paraphrase), and (Wong and Kit, 2008), (Popovic, 
2012), and (Chen et al, 2012) that introduces the 
word order factors, etc. The traditional evalua-
tion metrics tend to perform well on the language 
pairs with English as the target language. This 
paper will introduce the evaluation models that 
can also perform well on the language pairs that 
with English as source language. 
3 Description of Systems 
3.1 Sub Factors 
Firstly, we introduce the sub factor of modified 
length penalty inspired by BLEU metric. 
 
    {
   
 
            
                  
   
 
            
 (1) 
 
In the formula,    means sentence length 
penalty that is designed for both the shorter or 
longer translated sentence (hypothesis translation) 
as compared to the reference sentence. Parame-
ters   and   represent the length of candidate 
sentence and reference sentence respectively. 
Secondly, let?s see the factors of n-gram pre-
cision and n-gram recall. 
 
    
              
                           
 (2) 
 
    
              
                          
  (3) 
 
The variable                represents the 
number of matched n-gram chunks between hy-
pothesis sentence and reference sentence. The n-
gram precision and n-gram recall are firstly cal-
culated on sentence-level instead of corpus-level 
that is used in BLEU (  ). Then we define the 
weighted n-gram harmonic mean of precision 
and recall (WNHPR). 
 
          (?       (        
 
    (4) 
 
Thirdly, it is the n-gram based position differ-
ence penalty (NPosPenal). This factor is de-
signed to achieve the penalty for the different 
order of successfully matched words in reference 
sentence and hypothesis sentence. The alignment 
direction is from the hypothesis sentence to the 
reference sentence. It employs the  -gram meth-
od into the matching period, which means that 
the potential matched word will be assigned 
higher priority if it also has nearby matching. 
The nearest matching will be accepted as a back-
up choice if there are both nearby matching or 
there is no other matched word around the poten-
tial pairs. 
 
                 (5) 
 
     
 
          
?      
         
    (6) 
 
                             (7) 
 
The variable           means the length of 
the hypothesis sentence; the variables 
          and           represent the posi-
tion number of matched words in hypothesis sen-
tence and reference sentence respectively.  
3.2 Linguistic Features 
The linguistic features could be easily employed 
into our evaluation models. In the submitted ex-
periment results of WMT Metrics Task, we used 
the part of speech information of the words in 
question. In grammar, a part of speech, which is 
also called a word class, a lexical class, or a lexi-
cal category, is a linguistic category of lexical 
items. It is generally defined by the syntactic or 
morphological behavior of the lexical item in 
question. The POS information utilized in our 
metric LEPOR_v3.1, an enhanced version of 
LEPOR (Han et al, 2012), is extracted using the 
Berkeley parser (Petrov et al, 2006) for English, 
German, and French languages, using COM-
POST Czech morphology tagger (Collins, 2002) 
for Czech language, and using TreeTagger 
(Schmid, 1994) for Spanish and Russian lan-
guages respectively. 
415
Ratio 
other-to-English English-to-other 
CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
HPR:LP:NPP(word) 7:2:1 3:2:1 7:2:1 3:2:1 7:2:1 1:3:7 3:2:1 3:2:1 
HPR:LP:NPP(POS) NA 3:2:1 NA 3:2:1 7:2:1 7:2:1 NA 3:2:1 
    (      1:9 9:1 1:9 9:1 9:1 9:1 9:1 9:1 
    (     NA 9:1 NA 9:1 9:1 9:1 NA 9:1 
        NA 1:9 NA 9:1 1:9 1:9 NA 9:1 
Table 1. The tuned weight values in LEPOR_v3.1 system 
 
System 
Correlation Score with Human Judgment 
other-to-English English-to-other Mean 
score CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FR 
LEPOR_v3.1 0.93 0.86 0.88 0.92 0.83 0.82 0.85 0.83 0.87 
nLEPOR_baseline 0.95 0.61 0.96 0.88 0.68 0.35 0.89 0.83 0.77 
METEOR 0.91 0.71 0.88 0.93 0.65 0.30 0.74 0.85 0.75 
BLEU 0.88 0.48 0.90 0.85 0.65 0.44 0.87 0.86 0.74 
TER 0.83 0.33 0.89 0.77 0.50 0.12 0.81 0.84 0.64 
Table 2. The performances of nLEPOR_baseline and LEPOR_v3.1 systems on WMT11 corpora 
 
3.3 The nLEPOR_baseline System 
The nLEPOR_baseline system utilizes the simple 
product value of the factors: modified length 
penalty, n-gram position difference penalty, and 
weighted n-gram harmonic mean of precision 
and recall. 
 
                           (8) 
 
The system level score is the arithmetical 
mean of the sentence level evaluation scores. In 
the experiments of Metrics Task using the 
nLEPOR_baseline system, we assign N=1 in the 
factor WNHPR, i.e. weighted unigram harmonic 
mean of precision and recall. 
3.4 The LEPOR_v3.1 System 
The system of LEPOR_v3.1 (also called as 
hLEPOR) combines the sub factors using 
weighted mathematical harmonic mean instead 
of the simple product value. 
 
        
                   
   
  
 
          
         
 
    
   
 (9) 
 
Furthermore, this system takes into account 
the linguistic features, such as the POS of the 
words. Firstly, we calculate the hLEPOR score 
on surface words            (the closeness of 
the hypothesis translation and the reference 
translation). Then, we calculate the hLEPOR 
score on the extracted POS sequences 
          (the closeness of the corresponding 
POS tags between hypothesis sentence and refer-
ence sentence). The final score             is 
the combination of the two sub-scores 
           and          . 
 
             
 
       
(              
               (10) 
 
4 Evaluation Method 
In the MT evaluation task, the Spearman rank 
correlation coefficient method is usually used by 
the authoritative ACL WMT to evaluate the cor-
relation of different MT evaluation metrics. So 
we use the Spearman rank correlation coefficient 
  to evaluate the performances of 
nLEPOR_baseline and LEPOR_v3.1 in system 
level correlation with human judgments. When 
there are no ties,   is calculated using: 
 
     
 ?  
 
 (     
  (11) 
 
The variable    is the difference value be-
tween the ranks for         and   is the number 
of systems. We also offer the Pearson correlation 
coefficient information as below. Given a sample 
of paired data (X, Y) as (      ,         , the 
Pearson correlation coefficient is: 
 
     
? (      (      
 
   
?? (       
 
   
?? (     )
 
     
 (12) 
416
where    and    specify the mean of discrete 
random variable X and Y respectively. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
LEPOR_v3.1 .91 .94 .91 .76 .77 .86 
nLEPOR_baseline .92 .92 .90 .82 .68 .85 
SIMP-
BLEU_RECALL 
.95 .93 .90 .82 .63 .84 
SIMP-
BLEU_PREC 
.94 .90 .89 .82 .65 .84 
NIST-mteval-
inter 
.91 .83 .84 .79 .68 .81 
Meteor .91 .88 .88 .82 .55 .81 
BLEU-mteval-
inter 
.89 .84 .88 .81 .61 .80 
BLEU-moses .90 .82 .88 .80 .62 .80 
BLEU-mteval .90 .82 .87 .80 .62 .80 
CDER-moses .91 .82 .88 .74 .63 .80 
NIST-mteval .91 .79 .83 .78 .68 .79 
PER-moses .88 .65 .88 .76 .62 .76 
TER-moses .91 .73 .78 .70 .61 .75 
WER-moses .92 .69 .77 .70 .61 .74 
TerrorCat .94 .96 .95 na na .95 
SEMPOS na na na .72 na .72 
ACTa .81 -.47 na na na .17 
ACTa5+6 .81 -.47 na na na .17 
Table 3. System-level Pearson correlation scores 
on WMT13 English-to-other language pairs 
5 Experiments 
5.1 Training 
In the training stage, we used the officially re-
leased data of past WMT series. There is no Rus-
sian language in the past WMT shared tasks. So 
we trained our systems on the other eight lan-
guage pairs including English to other (French, 
German, Spanish, Czech) and the inverse transla-
tion direction. In order to avoid the overfitting 
problem, we used the WMT11 corpora as train-
ing data to train the parameter weights in order to 
achieve a higher correlation with human judg-
ments at system-level evaluations. For the 
nLEPOR_baseline system, the tuned values of   
and   are 9 and 1 respectively for all language 
pairs except for (   ,    ) for CS-EN lan-
guage pair. For the LEPOR_v3.1 system, the 
tuned values of weights are shown in Table 1. 
The evaluation scores of the training results on 
WMT11 corpora are shown in Table 2. The de-
signed methods have shown promising correla-
tion scores with human judgments at system lev-
el, 0.87 and 0.77 respectively for 
nLEPOR_baseline and LEPOR_v3.1 of the mean 
score on eight language pairs. As compared to 
METEOR, BLEU and TER, we have achieved 
higher correlation scores with human judgments.  
5.2 Testing 
In the WMT13 shared Metrics Task, we also 
submitted our system performances on English-
to-Russian and Russian-to-English language 
pairs. However, since the Russian language did 
not appear in the past WMT shared tasks, we 
assigned the default parameter weights to Rus-
sian language for the submitted two systems. The 
officially released results on WMT13 corpora are 
shown in Table 3, Table 4 and Table 5 respec-
tively for system-level and segment-level per-
formance on English-to-other language pairs. 
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU 
Av 
SIMP-
BLEU_RECALL 
.92 .93 .83 .87 .71 .85 
LEPOR_v3.1 .90 .9 .84 .75 .85 .85 
NIST-mteval-
inter 
.93 .85 .80 .90 .77 .85 
CDER-moses .92 .87 .86 .89 .70 .85 
nLEPOR_baseline .92 .90 .85 .82 .73 .84 
NIST-mteval .91 .83 .78 .92 .72 .83 
SIMP-
BLEU_PREC 
.91 .88 .78 .88 .70 .83 
Meteor .92 .88 .78 .94 .57 .82 
BLEU-mteval-
inter 
.92 .83 .76 .90 .66 .81 
BLEU-mteval .89 .79 .76 .90 .63 .79 
TER-moses .91 .85 .75 .86 .54 .78 
BLEU-moses .90 .79 .76 .90 .57 .78 
WER-moses .91 .83 .71 .86 .55 .77 
PER-moses .87 .69 .77 .80 .59 .74 
TerrorCat .93 .95 .91 na na .93 
SEMPOS na na na .70 na .70 
ACTa5+6 .81 -.53 na na na .14 
ACTa .81 -.53 na na na .14 
Table 4. System-level Spearman rank correlation 
scores on WMT13 English-to-other language 
pairs 
 
Table 3 shows LEPOR_v3.1 and 
nLEPOR_baseline yield the highest and the sec-
ond highest average Pearson correlation score 
0.86 and 0.85 respectively with human judg-
ments at system-level on five English-to-other 
language pairs. LEPOR_v3.1 and 
417
nLEPOR_baseline also yield the highest Pearson 
correlation score on English-to-Russian (0.77) 
and English-to-Czech (0.82) language pairs re-
spectively. The testing results of LEPOR_v3.1 
and nLEPOR_baseline show better correlation 
scores as compared to METEOR (0.81), BLEU 
(0.80) and TER-moses (0.75) on English-to-other 
language pairs, which is similar with the training 
results.  
On the other hand, using the Spearman rank 
correlation coefficient, SIMPBLEU_RECALL 
yields the highest correlation score 0.85 with 
human judgments. Our metric LEPOR_v3.1 also 
yields the highest Spearman correlation score on 
English-to-Russian (0.85) language pair, which 
is similar with the result using Pearson correla-
tion and shows its robust performance on this 
language pair.  
 
Directions 
EN-
FR 
EN-
DE 
EN-
ES 
EN-
CS 
EN-
RU Av 
SIMP-
BLEU_RECALL 
.16 .09 .23 .06 .12 .13 
Meteor .15 .05 .18 .06 .11 .11 
SIMP-
BLEU_PREC 
.14 .07 .19 .06 .09 .11 
sentBLEU-moses .13 .05 .17 .05 .09 .10 
LEPOR_v3.1 .13 .06 .18 .02 .11 .10 
nLEPOR_baseline .12 .05 .16 .05 .10 .10 
dfki_logregNorm-
411 
na na .14 na na .14 
TerrorCat .12 .07 .19 na na .13 
dfki_logregNormS
oft-431 
na na .03 na na .03 
Table 5. Segment-level Kendall?s tau correlation 
scores on WMT13 English-to-other language 
pairs 
 
However, we find a problem in the Spearman 
rank correlation method. For instance, let two 
evaluation metrics MA and MB with their evalu-
ation scores   ??????                   and  
  ???? ??                   respectively reflecting 
three MT systems  
 ??            . Before the calculation of cor-
relation with human judgments, they will be 
converted into   ??????  ?          and   ???? ??  ?          
with the same rank sequence using Spearman 
rank method; thus, the two evaluation systems 
will get the same correlation with human judg-
ments whatever are the values of human judg-
ments. But the two metrics reflect different re-
sults indeed: MA gives the outstanding score 
(0.95) to M1 system and puts very low scores 
(0.50 and 0.45) on the other two systems M2 and 
M3 while MB thinks the three MT systems have 
similar performances (scores from 0.74 to 0.77). 
This information is lost using the Spearman rank 
correlation methodology. 
The segment-level performance of 
LEPOR_v3.1 is moderate with the average Ken-
dall?s tau correlation score 0.10 on five English-
to-other language pairs, which is due to the fact 
that we trained our metrics at system-level in this 
shared metrics task. Lastly, the officially released 
results on WMT13 other-to-English language 
pairs are shown in Table 6, Table 7 and Table 8 
respectively for system-level and segment-level 
performance.  
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .97 .99 .84 .95 
SEMPOS .95 .95 .96 .99 .82 .93 
Depref-align .97 .97 .97 .98 .74 .93 
Depref-exact .97 .97 .96 .98 .73 .92 
SIMP-
BLEU_RECALL 
.97 .97 .96 .94 .78 .92 
UMEANT .96 .97 .99 .97 .66 .91 
MEANT .96 .96 .99 .96 .63 .90 
CDER-moses .96 .91 .95 .90 .66 .88 
SIMP-
BLEU_PREC 
.95 .92 .95 .91 .61 .87 
LEPOR_v3.1 .96 .96 .90 .81 .71 .87 
nLEPOR_baseline .96 .94 .94 .80 .69 .87 
BLEU-mteval-
inter 
.95 .92 .94 .90 .61 .86 
NIST-mteval-inter .94 .91 .93 .84 .66 .86 
BLEU-moses .94 .91 .94 .89 .60 .86 
BLEU-mteval .95 .90 .94 .88 .60 .85 
NIST-mteval .94 .90 .93 .84 .65 .85 
TER-moses .93 .87 .91 .77 .52 .80 
WER-moses .93 .84 .89 .76 .50 .78 
PER-moses .84 .88 .87 .74 .45 .76 
TerrorCat .98 .98 .97 na na .98 
Table 6. System-level Pearson correlation scores 
on WMT13 other-to-English language pairs 
 
METEOR yields the highest average correla-
tion scores 0.95 and 0.94 respectively using 
Pearson and Spearman rank correlation methods 
on other-to-English language pairs. The average 
performance of nLEPOR_baseline is a little bet-
ter than LEPOR_v3.1 on the five language pairs 
of other-to-English even though it is also moder-
ate as compared to other metrics. However, using 
418
the Pearson correlation method, 
nLEPOR_baseline yields the average correlation 
score 0.87 which already wins the BLEU (0.86) 
and TER (0.80) as shown in Table 6. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
Meteor .98 .96 .98 .96 .81 .94 
Depref-align .99 .97 .97 .96 .79 .94 
UMEANT .99 .95 .96 .97 .79 .93 
MEANT .97 .93 .94 .97 .78 .92 
Depref-exact .98 .96 .94 .94 .76 .92 
SEMPOS .94 .92 .93 .95 .83 .91 
SIMP-
BLEU_RECALL 
.98 .94 .92 .91 .81 .91 
BLEU-mteval-
inter 
.99 .90 .90 .94 .72 .89 
BLEU-mteval .99 .89 .89 .94 .69 .88 
BLEU-moses .99 .90 .88 .94 .67 .88 
CDER-moses .99 .88 .89 .93 .69 .87 
SIMP-
BLEU_PREC 
.99 .85 .83 .92 .72 .86 
nLEPOR_baseline .95 .95 .83 .85 .72 .86 
LEPOR_v3.1 .95 .93 .75 0.8 .79 .84 
NIST-mteval .95 .88 .77 .89 .66 .83 
NIST-mteval-inter .95 .88 .76 .88 .68 .83 
TER-moses .95 .83 .83 0.8 0.6 
0.8
0 
WER-moses .95 .67 .80 .75 .61 .76 
PER-moses .85 .86 .36 .70 .67 .69 
TerrorCat .98 .96 .97 na na .97 
Table 7. System-level Spearman rank correlation 
scores on WMT13 other-to-English language 
pairs 
 
Once again, our metrics perform moderate at 
segment-level on other-to-English language pairs 
due to the fact that they are trained at system-
level. We notice that some of the evaluation met-
rics do not submit the results on all the language 
pairs; however, their performance on submitted 
language pair is sometimes very good, such as 
the dfki_logregFSS-33 metric with a segment-
level correlation score 0.27 on German-to-
English language pair. 
6 Conclusion 
This paper describes our participation in the 
WMT13 Metrics Task. We submitted two sys-
tems nLEPOR_baseline and LEPOR_v3.1. Both 
of the two systems are trained and tested using 
the officially released data. LEPOR_v3.1 yields 
the highest Pearson correlation average-score 
0.86 with human judgments on five English-to-
other language pairs, and nLEPOR_baseline 
yields better performance than LEPOR_v3.1 on 
other-to-English language pairs. Furthermore, 
LEPOR_v3.1 shows robust system-level perfor-
mance on English-to-Russian language pair, and 
nLEPOR_baseline shows best system-level per-
formance on English-to-Czech language pair us-
ing Pearson correlation criterion. As compared to 
nLEPOR_baseline, the experiment results of 
LEPOR_v3.1 also show that the proper use of 
linguistic information can increase the perfor-
mance of the evaluation systems. 
 
Directions 
FR-
EN 
DE-
EN 
ES-
EN 
CS-
EN 
RU-
EN Av 
SIMP-
BLEU_RECALL 
.19 .32 .28 .26 .23 .26 
Meteor .18 .29 .24 .27 .24 .24 
Depref-align .16 .27 .23 .23 .20 .22 
Depref-exact .17 .26 .23 .23 .19 .22 
SIMP-
BLEU_PREC 
.15 .24 .21 .21 .17 .20 
nLEPOR_baseline .15 .24 .20 .18 .17 .19 
sentBLEU-moses .15 .22 .20 .20 .17 .19 
LEPOR_v3.1 .15 .22 .16 .19 .18 .18 
UMEANT .10 .17 .14 .16 .11 .14 
MEANT .10 .16 .14 .16 .11 .14 
dfki_logregFSS-
33 
na .27 na na na .27 
dfki_logregFSS-
24 
na .27 na na na .27 
TerrorCat .16 .30 .23 na na .23 
Table 8. Segment-level Kendall?s tau correlation 
scores on WMT13 other-to-English language 
pairs 
Acknowledgments 
The authors wish to thank the anonymous re-
viewers for many helpful comments. The authors 
are grateful to the Science and Technology De-
velopment Fund of Macau and the Research 
Committee of the University of Macau for the 
funding support for our research, under the refer-
ence No. 017/2009/A and RG060/09-
10S/CS/FST.  
References  
Banerjee, Satanjeev and Alon Lavie. 2005. METEOR: 
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
Proceedings of the 43th Annual Meeting of the 
419
Association of Computational Linguistics 
(ACL- 05), pages 65?72, Ann Arbor, US, June. 
Association of Computational Linguistics. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
and Omar F. Zaidan. 2011. Findings of the 2011 
Workshop on Statistical Machine Translation. 
In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation (WMT '11). Asso-
ciation for Computational Linguistics, Stroudsburg, 
PA, USA, 22-64. 
Callison-Burch, Chris, Philipp Koehn, Christof Monz, 
Matt Post, Radu Soricut, and Lucia Specia. 2012. 
Findings of the 2012Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh 
Workshop on Statistical Machine Translation, 
pages 10?51, Montreal, Canada. Association for 
Computational Linguistics. 
Carroll, John B. 1966. An Experiment in Evaluating 
the Quality of Translations, Mechanical Transla-
tion and Computational Linguistics, vol.9, 
nos.3 and 4, September and December 1966, page 
55-66, Graduate School of Education, Harvard 
University. 
Chen, Boxing, Roland Kuhn and Samuel Larkin. 2012. 
PORT: a Precision-Order-Recall MT Evaluation 
Metric for Tuning, Proceedings of the 50th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 930?939, Jeju, Republic 
of Korea, 8-14 July 2012. 
Collins, Michael. 2002. Discriminative Training 
Methods for Hidden Markov Models: Theory and 
Experiments with Perceptron Algorithms. In Pro-
ceedings of the ACL-02 conference on Empiri-
cal methods in natural language processing, 
Volume 10 (EMNLP 02), pages 1-8. Association 
for Computational Linguistics, Stroudsburg, PA, 
USA. 
Costa-juss?, Marta R., Carlos A. Henr?quez and Ra-
fael E. Banchs. 2012. Evaluating Indirect Strategies 
for Chinese-Spanish Statistical Machine Transla-
tion. Journal of artificial intelligence research, 
Volume 45, pages 761-780. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research (HLT '02). Morgan 
Kaufmann Publishers Inc., San Francisco, CA, 
USA, 138-145. 
Eck, Matthias and Chiori Hori. 2005. Overview of the 
IWSLT 2005 Evaluation Campaign. Proceedings 
of IWSLT 2005. 
Federico, Marcello, Luisa Bentivogli, Michael Paul, 
and Sebastian Stiiker. 2011. Overview of the 
IWSLT 2011 Evaluation Campaign. In Proceed-
ings of IWSLT 2011, 11-27. 
Han, Aaron Li-Feng, Derek F. Wong and Lidia S. 
Chao. 2012. LEPOR: A Robust Evaluation Metric 
for Machine Translation with Augmented Factors. 
Proceedings of the 24th International Confer-
ence on Computational Linguistics (COLING 
2012: Posters), Mumbai, India. 
Koehn, Philipp and Christof Monz. 2006. Manual and 
Automatic Evaluation of Machine Translation be-
tween European Languages. Proceedings of the 
ACLWorkshop on Statistical Machine Trans-
lation, pages 102?121, New York City. 
Li, A. (2005). Results of the 2005 NIST machine 
translation evaluation. In Machine Translation 
Workshop. 
Menezes, Arul, Kristina Toutanova and Chris Quirk. 
2006. Microsoft Research Treelet Translation Sys-
tem: NAACL 2006 Europarl Evaluation, Proceed-
ings of the ACLWorkshop on Statistical Ma-
chine Translation, pages 158-161, New York 
City. 
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation.  In Pro-
ceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics (ACL-
2003). pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. 
In Proceedings of the 40th Annual Meeting on 
Association for Computational Linguis-
tics (ACL '02). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 311-318. 
Paul, Michael. 2008. Overview of the IWSLT 2008 
Evaluation Campaign. Proceeding of IWLST 
2008, Hawaii, USA. 
Paul, Michael. 2009. Overview of the IWSLT 2009 
Evaluation Campaign. In Proc. of IWSLT 2009, 
Tokyo, Japan, pp. 1?18. 
Paul, Michael, Marcello Federico and Sebastian 
Stiiker. 2010. Overview of the IWSLT 2010 Eval-
uation Campaign, Proceedings of the 7th Inter-
national Workshop on Spoken Language 
Translation, Paris, December 2nd and 3rd, 2010, 
page 1-25. 
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan 
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the 
21st International Conference on Computa-
tional Linguistics and the 44th annual meeting 
of the Association for Computational Linguis-
420
tics (ACL-44). Association for Computational 
Linguistics, Stroudsburg, PA, USA, 433-440. 
Popovic, Maja. 2012. Class error rates for evaluation 
of machine translation output. Proceedings of the 
7th Workshop on Statistical Machine Transla-
tion, pages 71?75, Canada. 
Schmid, Helmut. 1994. Probabilistic Part-of-Speech 
Tagging Using Decision Trees. In Proceedings of 
International Conference on New Methods in 
Language Processing, Manchester, UK. 
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
Study of Translation Edit Rate with Targeted Hu-
man Annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Trans-
lation in the Americas (AMTA-06), pages 223?
231, USA. Association for Machine Translation in 
the Americas. 
Su, Hung-Yu and Chung-Hsien Wu. 2009. Improving 
Structural Statistical Machine Translation for Sign 
Language With Small Corpus Using Thematic 
Role Templates as Translation Memory, IEEE 
TRANSACTIONS ON AUDIO, SPEECH, AND 
LANGUAGE PROCESSING, VOL. 17, NO. 7. 
Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. In Proceedings of 
the 14th International Conference on Compu-
tational Linguistics, pages 433?439, Nantes, 
France. 
Tillmann, Christoph, Stephan Vogel, Hermann Ney, 
Arkaitz Zubiaga, and Hassan Sawaf. 1997. Accel-
erated DP Based Search For Statistical Translation. 
In Proceedings of the 5th European Confer-
ence on Speech Communication and Technol-
ogy (EUROSPEECH-97). 
Weaver, Warren. 1955. Translation. In William Locke 
and A. Donald Booth, editors, Machine Transla-
tion of Languages: Fourteen Essays. John 
Wiley & Sons, New York, pages 15?23. 
White, John S., Theresa O?Connell, and Francis 
O?Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and future approaches. 
In Proceedings of the Conference of the Asso-
ciation for Machine Translation in the Ameri-
cas (AMTA 1994). pp193-205. 
Wong, Billy and Chunyu Kit. 2008. Word choice and 
word position for automatic MT evaluation. In 
Workshop: MetricsMATR of the Association for 
Machine Translation in the Americas (AMTA), 
short paper, Waikiki, Hawai?I, USA. Association 
for Machine Translation in the Americas. 
 
421
