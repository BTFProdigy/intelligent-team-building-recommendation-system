Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721?1731,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Generating Coherent Event Schemas at Scale
Niranjan Balasubramanian, Stephen Soderland, Mausam, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{niranjan,ssoderlan, mausam, etzioni}@cs.washington.edu
Abstract
Chambers and Jurafsky (2009) demonstrated
that event schemas can be automatically in-
duced from text corpora. However, our analy-
sis of their schemas identifies several weak-
nesses, e.g., some schemas lack a common
topic and distinct roles are incorrectly mixed
into a single actor. It is due in part to their
pair-wise representation that treats subject-
verb independently from verb-object. This of-
ten leads to subject-verb-object triples that are
not meaningful in the real-world.
We present a novel approach to inducing
open-domain event schemas that overcomes
these limitations. Our approach uses co-
occurrence statistics of semantically typed re-
lational triples, which we call Rel-grams (re-
lational n-grams). In a human evaluation, our
schemas outperform Chambers?s schemas by
wide margins on several evaluation criteria.
Both Rel-grams and event schemas are freely
available to the research community.
1 Introduction
Event schemas (also known as templates or frames)
have been widely used in information extraction.
An event schema is a set of actors (also known as
slots) that play different roles in an event, such as
the perpetrator, victim, and instrument in a bomb-
ing event. They provide essential guidance in ex-
tracting information related to events from free text
(Patwardhan and Riloff, 2009), and can also aid in
other NLP tasks, such as coreference (Irwin et al,
2011), summarization (Owczarzak and Dang, 2010),
and inference about temporal ordering and causality.
Actor Rel Actor
A1:<person> failed A2:test
A1:<person> was suspended for A3:<time period>
A1:<person> used A4:<substance, drug>
A1:<person> was suspended for A5:<game, activity>
A1:<person> was in A6:<location>
A1:<person> was suspended by A7:<org, person>
Actor Instances:
A1: {Murray, Morgan, Governor Bush, Martin, Nelson}
A2: {test}
A3: {season, year, week, month, night}
A4: {cocaine, drug, gasoline, vodka, sedative}
A5: {violation, game, abuse, misfeasance, riding}
A6: {desert, Simsbury, Albany, Damascus, Akron}
A7: {Fitch, NBA, Bud Selig, NFL, Gov Jeb Bush}
Table 1: An event schema produced by our system, rep-
resented as a set of (Actor,Rel, Actor) triples, and a set
of instances for each actor A1, A2, etc. For clarity we
show unstemmed verbs.
Until recently, all event schemas in use in NLP
were hand-engineered, e.g., the MUC templates and
ACE event relations (ARPA, 1991; ARPA, 1998;
Doddington et al, 2004). This led to technology that
could only focus on specific domains of interest and
has not been applicable more broadly.
The seminal work of Chambers and Jurafsky
(2009) showed that event schemas can also be in-
duced automatically from text corpora. Instead of
labeled roles these schemas have a set of relations
and actors that serve as arguments.1 Their system
is fully automatic, domain-independent, and scales
to large text corpora.
However, we identify several limitations in the
schemas produced by their system.2 Their schemas
1In the rest of this paper we use event schemas to refer to
these automatically induced schemas with actors and relations.
2Available at http://www.usna.edu/Users/cs/
nchamber/data/schemas/acl09
1721
Actor Rel Actor
A1 caused A2
A2 spread A1
A2 burned A1
- extinguished A1
A1 broke out -
- put out A1
Actor Instances:
A1: {fire, aids, infection, disease}
A2: {virus, bacteria, disease, urushiol, drug}
Table 2: An event schema from Chambers? system that
mixes the events of fire spreading and disease spreading.
often lack coherence: mixing unrelated events and
having actors whose entities do not play the same
role in the schema. Table 2 shows an event schema
from Chambers that mixes the events of fire spread-
ing and disease spreading.
Much of the incoherence of Chambers? schemas
can be traced to their representation that uses pairs
of elements from an assertion, thus, treating subject-
verb and verb-object separately. This often leads to
subject-verb-object triples that do not make sense in
the real world. For example, the assertions ?fire
caused virus? and ?bacteria burned AIDS? are im-
plicit in Table 2.
Another limitation in schemas Chambers released
is that they restrict schemas to two actors, which can
result in combining different actors. Table 4 shows
an example of combining perpeterators and victims
into a single actor.
1.1 Contributions
We present an event schema induction algorithm that
overcomes these weaknesses. Our basic represen-
tation is triples of the form (Arg1, Relation, Arg2),
extracted from a text corpus using Open Information
Extraction (Mausam et al, 2012). The use of triples
aids in agreement between subject and object of a
relation. The use of Open IE leads to more expres-
sive relation phrases (e.g., with prepositions). We
also assign semantic types to arguments, both to al-
leviate data sparsity and to produce coherent actors
for our schemas.
Table 1 shows an event schema generated by our
system. It has six relations and seven actors. The
schema makes several related assertions about a per-
son using a drug, failing a test, and getting sus-
pended. The main actors in the schema include the
person who failed the test, the drug used, and the
agent that suspended the person.
Our first step in creating event schemas is to tab-
ulate co-occurrence of tuples in a database that we
call Rel-grams (relational n-grams) (Sections 3, 5.1).
We then perform analysis on a graph induced from
the Rel-grams database and use this to create event
schemas (Section 4).
We compared our event schemas with those of
Chambers on several metrics including whether the
schema pertains to a coherent topic or event and
whether the actors play a coherent role in that event
(Section 5.2). Amazon Mechanical Turk workers
judged that our schemas have significantly better co-
herence ? 92% versus 82% have coherent topic and
81% versus 59% have coherent actors.
We release our open domain event schemas and
the Rel-grams database3 for further use by the NLP
community.
2 System Overview
Our approach to schema generation is based on the
idea that frequently co-occurring relations in text
capture relatedness of assertions about real-world
events. We begin by extracting a set of relational tu-
ples from a large text corpus and tabulate occurrence
of pairs of tuples in a database.
We then construct a graph from this database and
identify high-connectivity nodes (relational tuples)
in this graph as a starting point for constructing event
schemas. We use graph analysis to rank the tu-
ples and merge arguments to form the actors in the
schema.
3 Modeling Relational Co-occurrence
In order to tabulate pairwise occurences of relational
tuples we need a suitable relation-based represen-
tation. We now describe the extraction and rep-
resentation of relations, a database for storing co-
occurrence information, and our probabilistic model
for the co-occurrence. We call this model Rel-
grams, as it can be seen as a relational analog to the
n-grams language model.
3.1 Relations Extraction and Representation
We extract relational triples from each sentence in
a large corpus using the OLLIE Open IE system
3Available at http://relgrams.cs.washington.edu
1722
Tuples Table
Id Arg1 Rel Arg2 Count
... ... ... ... ...
13 bomb explode in <loc> 547
14 bomb explode in Baghdad 22
15 bomb explode in market 7
... ... ... ... ...
87 bomb kill <per> 173
... ... ... ... ...
92 <loc> be suburb of <loc> 1023
... ... ... ... ...
BigramCounts Table
T1 T2 Dist. Count E11 E12 E21 E22
... ... ... ... ... ... ... ...
13 87 1 27 25 0 0 0
13 87 2 35 33 0 0 0
... ... ... ... ... ... ... ...
13 87 10 62 59 0 0 0
87 13 1 6 0 0 0 0
... ... ... ... ... ... ... ...
92 13 1 12 0 0 12 0
... ... ... ... ... ... ... ...
Figure 1: Tables in the Rel-grams Database: Tuples maps tuples to unique identifiers, BigramCounts provides the
co-occurrence counts (Count) within various distances (Dist.), and four types of argument equality counts (E11-E22).
E11 is the number of times when T1.Arg1 = T2.Arg1, E12 is when T1.Arg1 = T2.Arg2 and so on.
(Mausam et al, 2012).4 This provides relational tu-
ples in the format (Arg1, Relation, Arg2) where each
tuple element is a phrase from the sentence. The
sentence ?He cited a new study that was released by
UCLA in 2008.? produces three tuples:
1. (He, cited, a new study)
2. (a new study, was released by, UCLA)
3. (a new study, was released in, 2008)
Relational triples provide a more specific repre-
sentation which is less ambiguous when compared
to (subj, verb) or (verb, obj) pairs. However, using
relational triples also increases sparsity. To reduce
sparsity and to improve generalization, we represent
the relation phrase by its stemmed head verb plus
any prepositions. The relation phrase may include
embedded nouns, in which case these are stemmed
as well. Moreover, tuple arguments are represented
as stemmed head nouns, and we also record seman-
tic types of the arguments.
We selected 29 semantic types from WordNet, ex-
amining the set of instances on a small development
set to ensure that the types are useful, but not overly
specific. The set of types are: person, organization,
location, time unit, number, amount, group, busi-
ness, executive, leader, effect, activity, game, sport,
device, equipment, structure, building, substance,
nutrient, drug, illness, organ, animal, bird, fish, art,
book, and publication.
To assign types to arguments, we apply Stanford
Named Entity Recognizer (Finkel et al, 2005)5, and
also look up the argument in WordNet 2.1 and record
4Available at: http://knowitall.github.io/
ollie/
5We used the system downloaded from: http://nlp.
stanford.edu/software/CRF-NER.shtml and used
the seven class CRF model distributed with it.
the first three senses if they map to our target se-
mantic types. We use regular expressions to recog-
nize dates and numeric expressions, and map per-
sonal pronouns to <person>. We associate all types
found by this mechanism with each argument. The
tuples in the example above are normalized to the
following:
1. (He, cite, study)
2. (He, cite, <activity>)
3. (<person>, cite, study)
4. (<person>, cite, <activity>)
5. (study, be release by, UCLA)
6. (study, be release by, <organization>)
7. (study, be release in, 2008)
8. (study, be release in, <time unit>)
9. (<activity>, be release by, UCLA)
...
In our preliminary experiments, we found that us-
ing normalized relation strings and semantic classes
for arguments results in a ten-fold increase in the
number of Rel-grams with a minimum support.
3.2 Co-occurrence Tabulation
We construct a database to hold co-occurrence
statistics for pairs of tuples found in each document.
Figure 1 shows examples for the types of statistics
contained in the database. The database consists
of two tables: 1) Tuples ? Maps each tuple to a
unique identifier and tabulates tuple counts. 2) Bi-
gramCounts ? Stores the directional co-occurrence
frequency, a count for tuple T followed by T ? at a
distance of k, and tabulates the number of times the
same argument was present in the pair of tuples.
Equality Constraints: Along with the co-
occurrence counts, we record the equality of argu-
ments in Rel-grams pairs. We assert an argument
1723
Table 3: Given a source tuple, the Rel-grams language
model estimates the probability of encountering other re-
lational tuples in a document. For clarity, we show the
unstemmed version.
Top tuples related to
(<person>, convicted of, murder)
1. (<person>, convicted in, <time unit>)
2. (<person>, sentenced to, death)
3. (<person>, sentenced to, year)
4. (<person>, convicted in, <location>)
5. (<person>, sentenced to, life)
6. (<person>, convicted in, <person>)
7. (<person>, convicted after, trial)
8. (<person>, sent to, prison)
pair is equal if they are from the same token se-
quence in the source sentence or one argument is a
co-referent mention of the other. We use the Stan-
ford Co-reference system (Lee et al, 2013)6 to de-
tect co-referring mentions. There are four possible
equalities depending on the specific pair of argu-
ments in the tuples are the same, shown as E11, E12,
E21 and E22 in Figure 1. For example, the E21 col-
umn has counts for the number of times the Arg2 of
T1 was determined to be the same as the Arg1 of T2.
Implementation and Query Language: We pop-
ulated the Rel-grams database using OLLIE extrac-
tions from a set of 1.8 Million New York Times arti-
cles drawn from the Gigaword corpus. The database
consisted of approximately 320K tuples that have
frequency ? 3 and 1.1M entries in the bigram table.
The Rel-grams database allows for powerful
querying using SQL. For example, Table 3 shows the
most frequent rel-grams associated with the query
tuple (<person>, convicted of, murder).
3.3 Rel-grams Language Model
From the tabulated co-occurrence statistics, we esti-
mate bi-gram conditional probabilities of tuples that
occur within a window of k tuples from each other.
Formally, we use Pk(T ?|T ) to denote the conditional
probability that T ? follows T within a window of k
tuples. To discount estimates from low-frequency
tuples, we use a ?-smoothed estimate:
6Available for download at: http://nlp.stanford.
edu/software/dcoref.shtml
Pk(T
?|T ) =
#(T, T ?, k) + ?
?
T ???V
#(T, T ??, k) + ? ? |V |
(1)
where, #(T, T ?, k) is the number of times T ? fol-
lows T within a window of k tuples. k = 1 in-
dicates adjacent tuples in the document. |V | is the
number of unique tuples in the corpus. For experi-
ments in this paper, we set ? to 0.05.
Co-occurrence within a small window is usu-
ally more reliable but is also sparse, whereas co-
occurrence within larger windows addresses sparsity
but may lead to topic drift. To leverage the bene-
fits of different window sizes, we also define a met-
ric with a weighted average of window sizes from 1
to 10, where the weight decays as window size in-
creases. For example, with ? set to 0.5 in equation
2, a window of k+1 has half the weight of a window
of k.
P (T ?|T ) =
?10
k=1 ?
kPk(T ?|T )
?10
k=1 ?
k
(2)
We believe that Rel-grams is a valuable source
of common-sense knowledge and may be useful for
several downstream tasks such as improving infor-
mation extractors, inference of implicit information,
etc. We assess its usefulness in the context of gener-
ating event schemas.
4 Schema Generation
We now use Rel-grams to identify relations and ac-
tors pertaining to a particular event. Our schema
generation consists of three steps. First, we build a
relation graph of tuples (G) using connections iden-
tified by Rel-grams. Second, we identify a set of
seed tuples as starting points for schemas. We use
graph analysis to find the tuples most related to each
seed. Finally, we merge the arguments in these tu-
ples to create actors and output the final schema.
Next we describe each of these steps in detail.
4.1 Rel-graph construction
We define a Rel-graph as an undirected weighted
graph G = (V,E), whose vertices (V ) are relation
tuples with edges (E), where an edge between ver-
tices T and T ? is weighted by the symmetric condi-
tional probability SCP (T, T ?) defined as
1724
SCP (T, T ?) = P (T |T ?)? P (T ?|T ) (3)
Both conditional probabilities are computed in
Equation 2. Figure 2 shows a portion of a Rel-graph
where the thickness of the edge indicates symmetric
conditional probability.
(bomb, explode at, <location>) 
(bomb, explode on,          <time_unit>) (bomb, kill,      <person>) 
(bomb, wound,       <person>) (<person>, plant,          bomb) 
(<organization>, claim, responsibility) 
Figure 2: Part of a Rel-graph showing tuples strongly
associated with (bomb, explode at, <location>). Undi-
rected edges are weighted by symmetric conditional
probability with line thickness indicating weight.
4.2 Finding Related Tuples
Our goal is to find closely related tuples that per-
tain to an event or topic. First, we locate high-
connectivity nodes in the Rel-graph to use as seeds.
We sort nodes by the sum of their top 25 edge
weights7 and take the top portion of this list after
filtering out redundant views of the same relation.
For each seed (Q), we find related tuples by ex-
tracting the sub-graph (GQ) from Q?s neighbors
(within two hops from Q) in the Rel-graph. Graph
analysis can detect the strongly connected nodes
within this sub-graph, representing tuples that fre-
quently co-occur in the context of the seed tuple.
Page rank is a well-known graph analysis algo-
rithm that uses graph connectivity to identify impor-
tant nodes within a graph (Brin and Page, 1998). We
are interested in connectivity within a subgraph with
respect to a designated query node (the seed). Con-
nection to a query node can help minimize concept
drift and ensure that the selected tuples are closely
related to the main topic of the sub-graph.
7Limiting to the top 25 edges avoids overly general tuples
that occur in many topics, which tend to have a large number of
weak edges.
In this work, we adapt the Personalized PageR-
ank algorithm (Haveliwala, 2002). The personalized
version of PageRank returns ranks of various nodes
with respect to a given query node and hence is more
appropriate for our task than the basic PageRank al-
gorithm. Within the subgraph GQ for a given seed
Q, we compute a solution to the following set of
PageRank Equations:
PRQ(T )
= (1? d) + d
X
T ?
SCP (T, T ?)PRQ(T
?) if T = Q
= d
X
T ?
SCP (T, T ?)PRQ(T
?) otherwise
Here PRQ(T ) denotes the page rank of a tuple T
personalized for the query tuple Q. It is a sum of
all its neighbors? page ranks, weighted by the edge
weights; d is the damping probability, which we set
to be 0.85 in our implementation.
The solution is computed iteratively by initializ-
ing the page rank of Q to 1 and all others to 0, then
recomputing page rank values until they converge to
within a small . This computation remains scalable,
since we restrict it to subgraphs a small number of
hops away from the query node. This is a standard
practice to handle large graphs (Agirre and Soroa,
2009; Mausam et al, 2010).
4.3 Creating Actors and Relations
We take the top n tuples from GQ according to
their Page rank scores. From each tuple T :
(Arg1, Rel, Arg2) in GQ, we record two actors
(A1, A2) corresponding to Arg1 and Arg2, and add
Rel to the list of relations that they participate in.
Then, we merge actors in two steps. First, we col-
lect the equality constraints for the tuples in GQ. If
the arguments corresponding to any pair of actors
have a non-zero equality constraint then we merge
them. Second, we merge actors that perform simi-
lar actions. A1 and A2 are merged if they are con-
nected to the same actor A3 through the same rela-
tion. For example, A1 and A2 in (A1:lawsuit, file by,
A3:company) and (A2:suit, file by, A3:company),
will be merged into a single actor. To avoid merging
distinct actors, we use a small list of rules that spec-
ify the semantic type pairs that cannot be merged
(e.g., location-date). Also, we do not merge two ac-
tors, if it can result in a relation where the same actor
1725
System A1 Rel A2
Relgrams {bomb, missile, grenade, device} explode in {city, Bubqua, neighborhood}
{bomb, missile, grenade, device} explode kill {people, civilian, lawmaker, owner, soldier}
{bomb, missile, grenade, device} explode on {Feb., Fri., Tues., Sun., Sept.}
{bomb, missile, grenade, device} explode wound {civilian, person, people, soldier, officer}
{bomb, missile, grenade, device} explode in {Feb., Beirut Monday, Sept., Aug.}
{bomb, missile, grenade, device} explode injure {woman, people, immigrant, policeman}
Chambers {bomb, explosion, blast, bomber, mine} explode {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} set off {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} kill {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} detonate {bomb, explosion, blast, bomber, mine, bombing}
{bomb, explosion, blast, bomber, mine} injure {soldier, child, civilian, bomber, palestinian}
{soldier, child, civilian, bomber, palestinian} plant {bomb, explosion, blast, bomber, mine, bombing}
Relgrams {Carey, John Anthony Volpe, Chavez, She } veto {legislation, bill, law, measure, version}
{legislation, bill, law, measure, version} be sign by {Carey, John Anthony Volpe, Chavez, She }
{legislation, bill, law, measure, version} be pass by {State Senate, State Assembly, House, Senate, Parliament}
{Carey, John Anthony Volpe, Chavez, She } sign into {law}
{Carey, John Anthony Volpe, Chavez, She } to sign {bill}
{Carey, John Anthony Volpe, Chavez, She } be governor of {Massachusetts, state, South Carolina, Texas, California}
Chambers {clinton, bush, bill, president, house} oppose {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} sign {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} approve {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house veto {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} support {bill, measure, legislation, plan, law}
{clinton, bush, bill, president, house} pass {bill, measure, legislation, plan, law}
Table 4: ?Bombing? and ?legislation? schema examples from Rel-grams and Chambers represented as a set of
(A1, Rel, A2) tuples, where the schema provides a set of instances for each actor A1 and A2. Relations and argu-
ments are in the stemmed form, e.g., ?explode kill? refers to ?exploded killing?. Instances in bold produce tuples that
are not valid in the real world.
is both the Arg1 and Arg2.
Finally, we generate an ordered list of tuples using
the final set of actors and their relations. The out-
put tuples are sorted by the average page rank of the
original tuples, thereby reflecting their importance
within the sub-graph GQ.
5 Evaluation
We present experiments to explore two main ques-
tions: How well do Rel-grams capture real world
knowledge, and what is the quality of event schemas
built using Rel-grams.
5.1 Evaluating Rel-grams
What sort of common-sense knowledge is encap-
sulated in Rel-grams? How often does it indicate
an implication between a pair of statements, and
how often does it indicate a common real-world
event or topic? To answer these questions, we con-
ducted an experiment to identify a subset of our Rel-
grams database with high precision for two forms of
common-sense knowledge:
? Implication: The Rel-grams express an im-
plication from T to T? or from T? to T, a
bi-directional form of the Recognizing Tex-
tual Entailment (RTE) guidelines (Dagan et al,
2005).
? Common Topic: Is there an underlying com-
mon topic or event to which both T and T? are
relevant?
We also evaluated whether both T and T? are valid
tuples that are well-formed and make sense in the
real world, a necessary pre-condition for either im-
plication or common topic.
We are particularly interested in the highest pre-
cision portion of our Rel-grams database. The
database has 1.1M entries with support of at least
three instances for each tuple. To find the highest
precision subset of these, we identified tuples that
have at least 25 Rel-grams, giving us 12,600 seed
tuples with a total of over 280K Rel-grams. Finally,
we sorted this subset by the total symmetrical con-
ditional probability of the top 25 Rel-grams for each
seed tuple.
We tagged a sample of this 280K set of Rel-grams
for valid tuples, implication between T and T?, and
common topic. We found that in the top 10% of this
set, 87% of the seed tuples were valid and 74% of the
Rel-grams had both tuples valid. Of the Rel-grams
1726
System Id A1 Rel A2
Relgrams R1 bomb explode in city
bomb explode kill people
bomb explode on Fri.
... ... ...
Chambers C1 blast explode child
child detonate blast
child plant bomb
... ... ...
Table 5: A grounded instantiation of the schemas from
Table 4, where each actor is represented as a randomly
selected instance.
with both tuples valid, 83% expressed an implication
between the tuples, and 90% had a common topic.
There were several reasons for invalid tuples ?
parsing errors; binary projections of inherently n-ary
relations, for example (<person>, put, <person>);
head-noun only representation omitting essential in-
formation; and incorrect semantic types, primarily
due to NER tagging errors.
While the Rel-grams suffer from noise in the tu-
ple validity, there is clearly strong signal in the data
about common topic and implication between tuples
in the Rel-grams. As we demonstrate in the follow-
ing section, an end task can use graph analysis tech-
niques to amplify this strong signal, producing high-
quality relational schemas.
5.2 Schemas Evaluation
In our schema evaluation, we are interested in
assessing how well the schemas correspond to
common-sense knowledge about real world events.
To this end, we focus on three measures, topical co-
herence, tuple validity, and actor coherence.
A good schema must be topically coherent, i.e.,
the relations and actors should relate to some real
world topic or event. The tuples that comprise a
schema should be valid assertions that make sense
in the real world. Finally, each actor in the schema
should belong to a cohesive set that plays a consis-
tent role in the relations. Since there are no good
automated ways to make such judgments, we per-
form a human evaluation using workers from Ama-
zon?s Mechanical Turk (AMT).
We compare Rel-grams schemas against the state-
of-the-art narrative schemas released by Cham-
bers (Chambers and Jurafsky, 2009).8 Chambers?
8Available at http://www.usna.edu/Users/cs/
System Id A1 Rel A2
Relgrams R11 bomb explode in city
missile explode in city
grenade explode in city
... ... ...
Relgrams R21 missile explode in city
missile explode in neighborhood
missile explode in front
... ... ...
Table 6: A schema instantiation used to test for actor co-
herence. Each of the top instances for A1 or A2 is pre-
sented, holding the relation and the other actor fixed.
schemas are less expressive than ours ? they do not
associate types with actors and each schema has a
constant pre-specified number of relations. For a
fair comparison we use a similarly expressive ver-
sion of our schemas that strips off argument types
and has the same number of relations per schema
(six) as their highest quality output set.
5.2.1 Evaluation Design
We created two tasks for AMT annotators. The
first task tests the coherence and validity of rela-
tions in a schema and the second does the same
for the schema actors. In order to make the tasks
understandable to unskilled AMT workers, we fol-
lowed the accepted practice of presenting them with
grounded instances of the schemas (Wang et al,
2013), e.g., instantiating a schema with a specific ar-
gument instead of showing the various possibilities
for an actor.
First, we collect the information in schemas as a
set of tuples: S = {T1, T2, ? ? ? , Tn}, where each tu-
ple is of the form T : (X,Rel, Y ), which conveys
a relationship Rel between actors X and Y . Each
actor is represented by its highest frequency exam-
ples (instances). Table 4 shows examples of schemas
from Chambers and Rel-grams represented in this
format. Then, we create grounded tuples by ran-
domly sampling from top instances for each actor.
Task I: Topical Coherence To test whether the re-
lations in a schema form a coherent topic or event,
we presented the AMT annotators with a schema as
a set of grounded tuples, showing each relation in
the schema, but randomly selecting one of the top 5
instances from each actor. We generated five such
nchamber/data/schemas/acl09
1727
Figure 3: (a) Has Topic: Percentage of schema instanti-
ations with a coherent topic. (b) Valid Tuples: Percent-
age of grounded statements that assert valid real-world
relations. (c) Valid + On Topic: Percentage of grounded
statements where 1) the instantiation has a coherent topic,
2) the tuple is valid and 3) the relation belongs to the
common topic. All differences are statistically significant
with a p-value < 0.01.
instantiations for each schema. An example instan-
tiation is shown in Table 5.
We ask three kinds of questions on each grounded
schema: (1) is each of the grounded tuples valid (i.e.
meaningful in the real world); (2) do the majority of
relations form a coherent topic; and (3) does each
tuple belong to the common topic. Similar to pre-
vious AMT studies we get judgments from multiple
(five) annotators on each task and use the majority
labels (Snow et al, 2008).
Our instructions specified that the annotators
should ignore grammar and focus on whether a tuple
may be interpreted as a real world statement. For ex-
ample, the first tuple in R1 in Table 5 is a valid state-
ment ? ?a bomb exploded in a city?, but the tuples
in C1 ?a blast exploded a child?, ?a child detonated
a blast?, and ?a child planted a blast? don?t make
sense.
Task II: Actor Coherence To test whether the in-
stances of an actor form a coherent set, we held the
relation and one actor fixed and presented the AMT
annotators with the top 5 instances for the other ac-
tor. The first example R11 in Table 6 holds the
relation ?explode in? fixed, and A2 is grounded to
the randomly selected instance ?city?. We present
grounded tuples by varying A1 and ask annotators to
judge whether these instances form a coherent topic
and whether each instance belongs to that common
topic. As with Task I, we create five random instan-
tiations for each schema.
Figure 4: Actor Coherence: Has Role bars compare the
percentage of tuples where the tested actors have a co-
herent role. Fits Role compares the percentage of top
instances that fit the specified role for the tested actors.
All differences are statistically significant with a p-value
< 0.01.
5.2.2 Results
We obtained a test set of 100 schemas per system
by randomly sampling from the top 500 schemas
from each system. We evaluate this test set using
Task I and II as described above. For both tasks we
obtained ratings from five turkers and use the major-
ity labels as the final annotation.
Does the schema belong to a single topic? The
Has Topic bars in Figure 3 show results for schema
coherence. Rel-grams has a higher proportion of
schemas with a coherent topic, 91% compared to
82% for Chambers?. This is a 53% reduction in in-
coherent schemas.
Do tuples assert valid real-world relations? The
Valid Tuples bars in Figure 3 compare the percent-
age of valid grounded tuples in the schema instan-
tiations. A tuple was labeled valid if a majority of
the annotators labeled it to be meaningful in the real
world. Here we see a dramatic difference ? Rel-
grams have 92% valid tuples, compared with Cham-
bers? 61%.
What proportion of tuples belong? The Valid +
On Topic bars in Figure 3 compare the percentage
of tuples that are both valid and on topic, i.e., fits
the main topic of the schema. Tuples from schema
instantiations that did not have a coherent topic were
labeled incorrect.
Rel-grams have a higher proportion of valid tu-
ples belonging to a common topic, 82% compared to
1728
58% for Chambers? schemas, a 56% error reduction.
This is the strictest of the experiments described thus
far ? 1) the schema must have a topic, 2) the tuple
must be valid, and 3) the tuple must belong to the
topic.
Do actors represent a coherent set of argu-
ments? We evaluated schema actors from the top
25 schemas in Chambers? and Rel-grams schemas,
using grounded instances such as those in Table 6.
Figure 4 compares the percentage of tuples where
the actors play a coherent role (Has Role), and the
percentage of instances that fit that role for the actor
(Fits Role). Rel-grams has much higher actor co-
herence than Chambers?, with 97% judged to have a
topic compared to 81%, and 81% of instances fitting
the common role compared with Chambers? 59%.
5.2.3 Error Analysis
The errors in both our schemas and those of
Chambers are primarily due to mismatched actors
and from extraction errors, although Chambers?
schemas have a larger number of actor mismatch er-
rors and the cause of the errors is different for each
system.
Examining the data published by Chambers, the
main source of invalid tuples are mismatch of sub-
ject and object for a given relation, which accounts
for 80% of the invalid tuples. We hypothesize that
this is due to the pair-wise representation that treats
subject-verb and verb-object separately, causing in-
consistent s-v-o tuples. An example is (boiler, light,
candle) where (boiler, light) and (light, candle) are
well-formed, yet the entire tuple is not. In addition,
43% of the invalid tuples seem to be from errors by
the dependency parser.
Our schemas also suffer from mismatched actors,
despite the semantic typing of the actors ? we found
a mismatch in 56% of the invalid tuples (5% of
all tuples). A general type such as <person> or
<organization> may still have an instance that does
not play the same role as other instances. For exam-
ple a relation (A1, graduated from, A2) has A2 that
is mostly school names, but also includes ?church?
which leads to an invalid tuple.
Extraction errors account for 47% of the invalid
tuples in our schemas, primarily errors that truncate
an n-ary relation as a binary tuple. For example, the
sentence ?Mr. Diehl spends more time ... than the
commissioner? is misanalysed by the Open IE ex-
tractor as (Mr. Diehl, spend than, commissioner).
6 Related Work
Prior work by Chambers and Jurafsky (2008;
2009; 2010) showed that event sequences (narrative
chains) mined from text can be used to induce event
schemas in a domain-independent fashion. How-
ever, our manual evaluation of their output showed
key limitations which may limit applicability.
As pointed out earlier, a major weakness in
Chambers? approach is the pair-wise representation
of subject-verb and verb-object. Also, their released
a set of schemas are limited to two actors, although
this number can be increased by setting a chain split-
ting parameter.
Chambers and Jurafsky (2011) extended schema
generation to learn domain-specific event templates
and associated extractors. In work parallel to ours,
Cheung et al (2013), developed a probabilistic so-
lution for template generation. However, their ap-
proach requires performing joint probability estima-
tion using EM, which can limit scaling to large cor-
pora.
In this work we developed an Open IE based
solution to generate schemas. Following prior
work (Balasubramanian et al, 2012), we use Open
IE triples for modeling relation co-occurrence. We
extend the triple representation with semantic types
for arguments to alleviate sparisty and to improve
coherence. We developed a page rank based schema
induction algorithm which results in more coherent
schemas with several actors. Unlike Chambers? ap-
proach this method does not require explicit param-
eter tuning for controlling the number of actors.
While our event schemas are close to being tem-
plates (because of associated types, and actor clus-
tering), they do not have associated extractors. Our
future work will focus on building extractors for
these. It will also be interesting to compare with
Cheung?s system on smaller focused corpora.
Defining representations for events is a topic of
active interest (Fokkens et al, 2013). In this work,
we use a simpler representation, defining event
schemas as a set of actors with associated types and
a set of roles they play.
1729
7 Conclusions
We present a system for inducing event schemas
from text corpora based on Rel-grams, a language
model derived from co-occurrence statistics of re-
lational triples (Arg1, Relation, Arg2) extracted by
a state-of-the-art Open IE system. By using triples
rather than a pair-wise representation of subject-verb
and verb-object, we achieve more coherent schemas
than Chambers and Jurafsky (2009). In particular,
our schemas have higher topic coherence (92% com-
pared to Chambers? 82%; make a higher percentage
of valid assertions (94% compared with 61%); and
have greater actor coherence (81% compared with
59%).
Our schemas are also more expressive than those
published by Chambers ? we have semantic typing
for the actors, we are not limited to two actors per
schema, and our relation phrases include preposi-
tions and are thus more precise and have higher cov-
erage of actors involved in the event.
Our future plans are to build upon our event
schemas to create an open-domain event extractor.
This will extend each induced schema to have asso-
ciated extractors. These extractors will operate on a
document and instantiate an instance of the schema.
We have created a Rel-grams database with 1.1M
entries and a set of over 2K event schemas from a
corpus of 1.8M New York Times articles. Both are
freely available to the research community9 and may
prove useful for a wide range of NLP applications.
Acknowledgments
We thank the anonymous reviewers, Tony Fader, and
Janara Christensen for their valuable feedback. This
paper was supported by Office of Naval Research
(ONR) grant number N00014-11-1-0294, Army Re-
search Office (ARO) grant number W911NF-13-
1-0246, Intelligence Advanced Research Projects
Activity (IARPA) via Air Force Research Lab-
oratory (AFRL) contract number FA8650-10-C-
7058, and Defense Advanced Research Projects
Agency (DARPA) via AFRL contract number AFRL
FA8750-13-2-0019. The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes notwithstanding any copyright
annotation thereon. The views and conclusions con-
tained herein are those of the authors and should
9available at http://relgrams.cs.washington.edu
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of ONR, ARO, IARPA, AFRL, or the U.S.
Government.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In 12th Con-
ference of the European Chapter of the Association for
Computational Linguistics (EACL), pages 33?41.
ARPA. 1991. Proc. 3rd Message Understanding Conf.
Morgan Kaufmann.
ARPA. 1998. Proc. 7th Message Understanding Conf.
Morgan Kaufmann.
Niranjan Balasubramanian, Stephen Soderland, Oren Et-
zioni, et al 2012. Rel-grams: a probabilistic model
of relations in text. In Proceedings of the Joint Work-
shop on Automatic Knowledge Base Construction and
Web-scale Knowledge Extraction, pages 101?105. As-
sociation for Computational Linguistics.
Sergey Brin and Lawrence Page. 1998. The anatomy of a
large-scale hypertextual web search engine. Computer
Networks, 30(1-7):107?117.
N. Chambers and D. Jurafsky. 2008. Unsupervised
learning of narrative event chains. In Proceedings of
ACL-08: HLT.
N. Chambers and D. Jurafsky. 2009. Unsupervised
learning of narrative schemas and their participants. In
Proceedings of ACL.
N. Chambers and D. Jurafsky. 2010. A database of nar-
rative schemas. In Proceedings of LREC.
N. Chambers and D. Jurafsky. 2011. Template-based
information extraction without the templates. In Pro-
ceedings of ACL.
J. Cheung, H. Poon, and L. Vandervende. 2013. Prob-
abilistic frame induction. In Proceedings of NAACL
HLT.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, , and R. Weischedel. 2004. The auto-
matic content extraction (ACE) program-tasks, data,
and evaluation. In Procs. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
1730
Antske Fokkens, Marieke van Erp, Piek Vossen, Sara
Tonelli, Willem Robert van Hage, BV SynerScope,
Luciano Serafini, Rachele Sprugnoli, and Jesper Hoek-
sema. 2013. Gaf: A grounded annotation framework
for events. NAACL HLT 2013, page 11.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW, pages 517?526.
Joseph Irwin, Mamoru Komachi, and Yuji Matsumoto.
2011. Narrative schema as world knowledge for
coreference resolution. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 86?92. Associ-
ation for Computational Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013.
Deterministic coreference resolution based on entity-
centric, precision-ranked rules. Computational Lin-
guistics, (Just Accepted):1?54.
Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld,
Kobi Reiter, Michael Skinner, Marcus Sammer, and
Jeff Bilmes. 2010. Panlingual lexical translation via
probabilistic inference. Artificial Intelligence Journal
(AIJ).
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP.
K. Owczarzak and H.T. Dang. 2010. Overview of the tac
2010 summarization track.
S. Patwardhan and E. Riloff. 2009. A unified model of
phrasal and sentential evidence for information extrac-
tion. In Proceedings of EMNLP 2009.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?
263. Association for Computational Linguistics.
A. Wang, C.D.V. Hoang, and M-Y. Kan. 2013. Perspec-
tives on crowdsourcing annotations for Natural Lan-
guage Processing. Language Resources and Evalua-
tion, 47:9?31.
1731
Proceedings of NAACL-HLT 2013, pages 1163?1173,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Coherent Multi-Document Summarization
Janara Christensen, Mausam, Stephen Soderland, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{janara,mausam,soderlan,etzioni}@cs.washington.edu
Abstract
This paper presents G-FLOW, a novel system
for coherent extractive multi-document sum-
marization (MDS).1 Where previous work on
MDS considered sentence selection and or-
dering separately, G-FLOW introduces a joint
model for selection and ordering that balances
coherence and salience. G-FLOW?s core rep-
resentation is a graph that approximates the
discourse relations across sentences based on
indicators including discourse cues, deverbal
nouns, co-reference, and more. This graph en-
ables G-FLOW to estimate the coherence of a
candidate summary.
We evaluate G-FLOW on Mechanical Turk,
and find that it generates dramatically bet-
ter summaries than an extractive summarizer
based on a pipeline of state-of-the-art sentence
selection and reordering components, under-
scoring the value of our joint model.
1 Introduction
The goal of multi-document summarization (MDS)
is to produce high quality summaries of collections
of related documents. Most previous work in ex-
tractive MDS has studied the problems of sentence
selection (e.g., (Radev, 2004; Haghighi and Vander-
wende, 2009)) and sentence ordering (e.g., (Lapata,
2003; Barzilay and Lapata, 2008)) separately, but
we believe that a joint model is necessary to produce
coherent summaries. The intuition is simple: if the
sentences in a summary are first selected?without
regard to coherence?then a satisfactory ordering of
the selected sentences may not exist.
1System and data at http://knowitall.cs.washington.edu/gflow/
doc1: Bomb-
ing in
Jerusalem
doc1: Anger
from Israelis
doc1: Suspen-
sion of peace
accord due to
bombing
doc2: Hamas
claims respon-
sibility
doc5: Pales-
tinians con-
demn attack
doc4: Mubarak
urges peace
accord
doc5: Pales-
tinians urge
peace accord
doc3: Clinton
urges peace
accord
Figure 1: An example of a discourse graph covering a
bombing and its aftermath, indicating the source docu-
ment for each node. A coherent summary should begin
with the bombing and then describe the reactions. Sen-
tences are abbreviated for compactness.
An extractive summary is a subset of the sen-
tences in the input documents, ordered in some
way.2 Of course, most possible summaries are in-
coherent. Now, consider a directed graph where the
nodes are sentences in the collection, and each edge
represents a pairwise ordering constraint necessary
for a coherent summary (see Figure 1 for a sample
graph). By definition, any coherent summary must
obey the constraints in this graph.
Previous work has constructed similar graphs au-
tomatically for single document summarization and
manually for MDS (see Section 2). Our system,
G-FLOW extends this research in two important
ways. First, it tackles automatic graph construction
for MDS, which requires novel methods for identi-
fying inter-document edges (Section 3). It uses this
2We focus exclusively on extractive summaries, so we drop
the word ?extractive? henceforth.
1163
State-of-the-art MDS system G-FLOW
? The attack took place Tuesday near Cailaco in East Timor, a
former Portuguese colony, according to a statement issued by the
pro-independence Christian Democratic Union of East Timor.
? The United Nations does not recognize Indonesian claims to East
Timor.
? In a decision welcomed as a landmark by Portugal, European Union
leaders Saturday backed calls for a referendum to decide the fate of East
Timor, the former Portuguese colony occupied by Indonesia since 1975.
? Indonesia invaded East Timor in 1975 and annexed it the following
year.
? Bhichai Rattakul, deputy prime minister and president of the
Bangkok Asian Games Organizing Committee, asked the Foreign
Ministry to urge the Saudi government to reconsider withdrawing
its 105-strong team.
? The games will be a success.
? Thailand won host rights for the quadrennial games in 1995, but
setbacks in preparations led officials of the Olympic Council of Asia late
last year to threaten to move the games to another country.
? Thailand showed its nearly complete facilities for the Asian Games to
a tough jury Thursday - the heads of the organizing committees from the
43 nations competing in the December event.
Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and
sentence orderer (Li et al, 2011a), and by G-FLOW.
graph to estimate coherence of a candidate summary.
Second, G-FLOW introduces a novel methodology
for joint sentence selection and ordering (Section 4).
It casts MDS as a constraint optimization problem
where salience and coherence are soft constraints,
and redundancy and summary length are hard con-
straints. Because this optimization problem is NP-
hard, G-FLOW uses local search to approximate it.
We report on a Mechanical Turk evaluation that
directly compares G-FLOW to state-of-the-art MDS
systems. Using DUC?04 as our test set, we com-
pare G-FLOW against a combination of an extractive
summarization system with state-of-the-art ROUGE
scores (Lin and Bilmes, 2011) followed by a state-
of-the-art sentence reordering scheme (Li et al,
2011a). We also compare G-FLOW to a combina-
tion of an extractive system with state-of-the-art co-
herence scores (Nobata and Sekine, 2004) followed
by the reordering system. In both cases participants
substantially preferred G-FLOW. Participants chose
G-FLOW 54% of the time when compared to Lin,
and chose Lin?s system 22% of the time. When com-
pared to Nobata, participants chose G-FLOW 60%
of the time, and chose Nobata only 20% of the time.
The remainder of the cases were judged equivalent.
A further analysis shows that G-FLOW?s sum-
maries are judged superior along several dimensions
suggested in the DUC?04 evaluation (including co-
herence, repetitive text, and referents). A compar-
ison against manually written, gold standard sum-
maries, reveals that while the gold standard sum-
maries are preferred in direct comparisons, G-FLOW
has nearly equivalent scores on almost all dimen-
sions suggested in the DUC?04 evaluation.
The paper makes the following contributions:
? We present G-FLOW, a novel MDS system that
jointly solves the sentence selection and order-
ing problems to produce coherent summaries.
? G-FLOW automatically constructs a domain-
independent graph of ordering constraints over
sentences in a document collection, based on
syntactic cues and redundancy across docu-
ments. This graph is the backbone for estimat-
ing the coherence of a summary.
? We perform human evaluation on blind test
sets and find that G-FLOW dramatically outper-
forms state-of-the-art MDS systems.
2 Related Work
Most existing research in multi-document summa-
rization (MDS) focuses on sentence selection for in-
creasing coverage and does not consider coherence
of the summary (Section 2.1). Although coherence
has been used in ordering of summary sentences
(Section 2.2), this work is limited by the quality of
summary sentences given as input. In contrast, G-
FLOW incorporates coherence in both selection and
ordering of summary sentences.
G-FLOW can be seen as an instance of discourse-
driven summarization (Section 2.3). There is prior
work in this area, but primarily for summarization of
single documents. There is some preliminary work
on the use of manually-created discourse models in
MDS. Our approach is fully automated.
2.1 Subset Selection in MDS
Most extractive summarization research aims to in-
crease the coverage of concepts and entities while
reducing redundancy. Approaches include the use of
maximum marginal relevance (Carbonell and Gold-
stein, 1998), centroid-based summarization (Sag-
gion and Gaizauskas, 2004; Radev et al, 2004), cov-
1164
ering weighted scores of concepts (Takamura and
Okumura, 2009; Qazvinian et al, 2010), formula-
tion as minimum dominating set problem (Shen and
Li, 2010), and use of submodularity in sentence se-
lection (Lin and Bilmes, 2011). Graph centrality has
also been used to estimate the salience of a sentence
(Erkan and Radev, 2004). Approaches to content
analysis include generative topic models (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010; Li et al, 2011b), and discriminative mod-
els (Aker et al, 2010).
These approaches do not consider coherence as
one of the desiderata in sentence selection. More-
over, they do not attempt to organize the selected
sentences into an intelligible summary. They are
often evaluted by ROUGE (Lin, 2004), which is
coherence-insensitive. In practice, these approaches
often result in incoherent summaries.
2.2 Sentence Reordering
A parallel thread of research has investigated taking
a set of summary sentences as input and reordering
them to make the summary fluent. Various algo-
rithms use some combination of topic-relatedness,
chronology, precedence, succession, and entity co-
herence for reordering sentences (Barzilay et al,
2001; Okazaki et al, 2004; Barzilay and Lapata,
2008; Bollegala et al, 2010). Recent work has also
used event-based models (Zhang et al, 2010) and
context analysis (Li et al, 2011a).
The hypothesis in this research is that a pipelined
combination of subset selection and reordering will
produce high-quality summaries. Unfortunately,
this is not true in practice, because sentences are se-
lected primarily for coverage without regard to co-
herence. This methodology often leads to an inad-
vertent selection of a set of disconnected sentences,
which cannot be put together in a coherent sum-
mary, irrespective of how the succeeding algorithm
reorders them. In our evaluation, reordering had lim-
ited impact on the quality of the summaries.
2.3 Coherence Models and Summarization
Research on discourse analysis of documents pro-
vides a basis for modeling coherence in a docu-
ment. Several theories have been developed for
modeling discourse, e.g., Centering Theory, Rhetor-
ical Structure Theory (RST), Penn Discourse Tree-
Bank (Grosz and Sidner, 1986; Mann and Thomp-
son, 1988; Wolf and Gibson, 2005; Prasad et al,
2008). Numerous discourse-guided summariza-
tion algorithms have been developed (Marcu, 1997;
Mani, 2001; Taboada and Mann, 2006; Barzilay and
Elhadad, 1997; Louis et al, 2010). However, these
approaches have been applied to single document
summarization and not to MDS.
Discourse models have seen some application to
summary generation in MDS, for example, using a
detailed semantic representation of the source texts
(McKeown and Radev, 1995; Radev and McKe-
own, 1998). A multi-document extension of RST
is Cross-document Structure Theory (CST), which
has been applied to MDS (Zhang et al, 2002; Jorge
and Pardo, 2010). However, these systems require
a stronger input, such as a manual CST-annotation
of the set of documents. Our work can be seen as
an instance of summarization based on lightweight
CST. However, a key difference is that our proposed
algorithm is completely automated and does not re-
quire any additional human annotation. Addition-
ally, while incorporating coherence into selection,
this work does not attempt to order the sentences
coherently, while our approach performs joint selec-
tion and ordering.
Discourse models have also been used for evalu-
ating summary quality (Barzilay and Lapata, 2008;
Louis and Nenkova, 2009; Pitler et al, 2010). Fi-
nally, there is work on generating coherent sum-
maries in specific domains, such as scientific articles
(Saggion and Lapalme, 2002; Abu-Jbara and Radev,
2011) using domain-specific cues like citations. In
contrast, our work generates summaries without any
domain-specific knowledge. Other research has fo-
cused on identifying coherent threads of documents
rather than sentences (Shahaf and Guestrin, 2010).
3 Discourse Graph
As described in Section 1, our goal is to identify
pairwise ordering constraints over a set of input sen-
tences. These constraints specify a multi-document
discourse graph, which is used by G-FLOW to eval-
uate the coherence of a candidate summary.
In this graph G, each vertex is a sentence and an
edge from si to sj indicates that sj can be placed
right after si in a coherent summary. In other words,
the two share a discourse relationship. In the fol-
1165
lowing three sentences (from possibly different doc-
uments) there should be an edge from s1 to s2, but
not between s3 and the other sentences:
s1 Militants attacked a market in Jerusalem.
s2 Arafat condemned the bombing.
s3 The Wye River Accord was signed in Oct.
Discourse theories have proposed a variety of re-
lationships between sentences such as background
and interpretation. RST has 17 such relations (Mann
and Thompson, 1988) and PDTB has 16 (Prasad et
al., 2008). While we seek to identify pairs of sen-
tences that have a relationship, we do not attempt to
label the edges with the exact relation.
We use textual cues from the discourse literature
in combination with the redundancy inherent in re-
lated documents to generate edges. Because this
methodology is noisy, the graph used by G-FLOW is
an approximation, which we refer to as an approx-
imate discourse graph (ADG). We first describe the
construction of this graph, and then discuss the use
of the graph for summary generation (Section 4).
3.1 Deverbal Noun Reference
Often, the main description of an event is mentioned
in a verbal phrase and subsequent references use
deverbal nouns (nominalization of verbs) (e.g., ?at-
tacked? and ?the attack?). In this example, the noun
is derivationally related to the verb, but that is not al-
ways the case. For example, ?bombing? in s2 above
refers to ?attacked? in s1.
We identify verb-noun pairs with this relationship
as follows. First, we locate a set of candidate pairs
from WordNet: for each verb v, we determine po-
tential noun references n using a path length of up to
two in WordNet (moving from verb to noun is pos-
sible via WordNet?s ?derivationally related? links).
This set captures verb-noun pairs such as (?to at-
tack?, ?bombing?), but also includes generic pairs
such as (?to act?, ?attack?). To filter such errors
we score the candidate references. Our goal is to
emphasize common pairs and to deemphasize pairs
with common verbs or verbs that map to many
nouns. To this end, we score pairs by (c/p) ? (c/q),
where c is the number of times the pair (v, n) ap-
pears in adjacent sentences, p is the number of times
the verb appears, and q is the number of times that
v appears with a different noun. We generate these
statistics over a background corpus of 60,000 arti-
cles from the New York Times and Reuters, and
filter out candidate pairs scoring below a threshold
identified over a small training set.
We construct edges in the ADG between pairs of
sentences containing these verb to noun mappings.
To our knowledge, we are the first to use deverbal
nouns for summarization.
3.2 Event/Entity Continuation
Our second indicator is related to lexical chains
(Barzilay and Lapata, 2008). We add an edge in
the ADG from a sentence si to sj if they contain
the same event or entity and the timestamp of si is
less than or equal to the timestamp of sj (timestamps
generated with (Chang and Manning, 2012)).
3.3 Discourse Markers
We use 36 explicit discourse markers (e.g., ?but?,
?however?, ?moreover?) to identify edges between
two adjacent sentences of a document (Marcu and
Echihabi, 2002). This indicator lets us learn an edge
from s4 to s5 below:
s4 Arafat condemned the bombing.
s5 However, Netanyahu suspended peace talks.
3.4 Inferred Edges
We exploit the redundancy of information in MDS
documents to infer edges to related sentences. An
edge (s, s??) can be inferred if there is an existing
edge (s, s?) and s? and s?? express similar informa-
tion. As an example, the edge (s6, s7) can be in-
ferred based on edge (s4, s5):
s6 Arafat condemned the attack.
s7 Netanyahu has suspended the talks.
To infer edges we need an algorithm to identify
sentences expressing similar information. To iden-
tify these pairs, we extract Open Information Extrac-
tion (Banko et al, 2007) relational tuples for each
sentence, and we mark any pair of sentences with
an equivalent relational tuple as redundant (see Sec-
tion 4.3). The inferred edges allow us to propagate
within-document discourse information to sentences
from other documents.
3.5 Co-referent Mentions
A sentence sj will not be clearly understood in iso-
lation and may need another sentence si in its con-
text, if sj has a general reference (e.g., ?the presi-
1166
dent?) pointing to a specific entity or event in si (e.g.,
?President Bill Clinton?). We construct edges based
on coreference mentions, as predicted by Stanford?s
coreference system (Lee et al, 2011). We are able
to identify syntactic edge (s8, s9):
s8 Pres. Clinton expressed sympathy for Israel.
s9 He said the attack should not derail the deal.
3.6 Edge Weights
We weight each edge in the ADG by adding the
number of distinct indicators used to construct that
edge ? if sentences s and s? have an edge because
of a discourse marker and a deverbal reference, the
edge weight wG(s, s?) will be two. We also include
negative edges in the ADG. wG(s, s?) is negative if
s? contains a deverbal noun reference, a discourse
marker, or a co-reference mention that is not fulfilled
by s. For example, if s? contains a discourse marker,
and s is neither the sentence directly preceding s?
and there is no inferred discourse link between s and
s?, then we will add a negative edge wG(s, s?).
3.7 Preliminary Graph Evaluation
We evaluated the quality of the ADG used by G-
FLOW, which is important not only for its use in
MDS, but also because the ADG may be used for
other applications like topic tracking and decompos-
ing an event into sub-events. One author randomly
chose 750 edges and labeled an edge correct if the
pair of sentences did have a discourse relationship
between them and incorrect otherwise. 62% of the
edges accurately reflected a discourse relationship.
Our ADG has on average 31 edges per sentence for
a dataset in which each document cluster has on av-
erage 253 sentences. This evaluation includes only
the positive edges.
4 Summary Generation
We denote a candidate summary X to be a sequence
of sentences ?x1, x2, . . . , x|X|?. G-FLOW?s summa-
rization algorithm searches through the space of or-
dered summaries and scores each candidate sum-
mary along the dimensions of coherence (Section
4.1), salience (Section 4.2) and redundancy (Section
4.3). G-FLOW returns the summary that maximizes
a joint objective function (Section 4.4).
weight feature
-0.037 position in document
0.033 from first three sentences
-0.035 number of people mentions
0.111 contains money
0.038 sentence length > 20
0.137 length of sentence
0.109 #sentences verbs appear in (any form)
0.349 #sentences common nouns appear in
0.355 #sentences proper nouns appear in
Table 2: Linear regression features for salience.
4.1 Coherence
G-FLOW estimates coherence of a candidate sum-
mary via the ADG. We define coherence as the sum
of edge weights between successive summary sen-
tences. For disconnected sentence pairs, the edge
weight is zero.
Coh(X) =
?
i=1..|X|?1
wG+(xi, xi+1) + ?wG?(xi, xi+1)
wG+ represents positive edges and wG? represents
negative edge weights. ? is a tradeoff coefficient for
positive and negative weights, which is tuned using
the methodology described in Section 4.4.
4.2 Salience
Salience is the inherent value of each sentence to
the documents. We compute salience of a summary
(Sal(X)) as the sum of the saliences of individual
sentences (
?
i Sal(xi)).
To estimate salience of a sentence, G-FLOW uses
a linear regression classifier trained on ROUGE
scores over the DUC?03 dataset. The classifier uses
surface features designed to identify sentences that
cover important concepts. The complete list of fea-
tures and learned weights is in Table 2. The clas-
sifier finds a sentence more salient if it mentions
nouns or verbs that are present in more sentences
across the documents. The highest ranked features
are the last three ? number of other sentences that
mention a noun or a verb in the given sentence. We
use the same procedure as in deverbal nouns for de-
tecting verb mentions that appear as nouns in other
sentences (Section 3.1).
4.3 Redundancy
We also wish to avoid redundancy. G-FLOW first
processes each sentence with a state-of-the-art Open
Information extractor OLLIE (Mausam et al, 2012),
which converts a sentence into its component re-
lational tuples of the form (arg1, relational phrase,
1167
arg2).3 For example, it finds (Militants, bombed, a
marketplace) as a tuple from sentence s12.
Two sentences will express redundant information
if they both contain the same or synonymous com-
ponent fact(s). Unfortunately, detecting synonymy
even at relational tuple level is very hard. G-FLOW
approximates this synonymy by considering two re-
lational tuples synonymous if the relation phrases
contain verbs that are synonyms of each other, have
at least one synonymous argument, and are times-
tamped within a day of each other. Because the in-
put documents cover related events, these relatively
weak rules provide good performance. The same
algorithm is used for inferring edges for the ADG
(Section 3.4). This algorithm can detect that the fol-
lowing sentences express redundant information:
s12 Militants bombed a marketplace in Jerusalem.
s13 He alerted Arafat after assailants attacked the
busy streets of Mahane Yehuda.
4.4 Objective Function
The objective function needs to balance coherence,
salience and redundancy and also honor the given
budget, i.e., maximum summary lengthB. G-FLOW
treats redundancy and budget as hard constraints and
coherence and salience as soft. Coherence is neces-
sarily soft as the graph is approximate. While previ-
ous MDS systems specifically maximized coverage,
in preliminary experiments on a development set, we
found that adding a coverage term did not improve
G-FLOW?s performance. We optimize:
maximize: F (x) , Sal(X) + ?Coh(X)? ?|X|
s.t.
?
i=1..|X| len(xi) < B
?xi, xj ? X : redundant(xi, xj) = 0
Here len refers to the sentence length. We add |X|
term (the number of sentences in the summary) to
avoid picking many short sentences, which may in-
crease coherence and salience scores at the cost of
overall summary quality.
The parameters ?, ? and ? (see Section 4.1) are
tuned automatically using a grid search over a de-
velopment set as follows. We manually generate ex-
tractive summaries for each document cluster in our
development set (DUC?03) and choose the parame-
ter setting that minimizes |F (XG-FLOW) ? F (X?)|
3Available from http://ollie.cs.washington.edu
summed over all document clusters. F is the objec-
tive function, XG-FLOW is the summary produced by
G-FLOW and X? is the manual summary.
This constraint optimization problem is NP hard,
which can be shown by using a reduction of the
longest path problem. For this reason, G-FLOW uses
local search to reach an approximation of the opti-
mum. G-FLOW employs stochastic hill climbing
with random restarts as the base search algorithm.
At each step, the search either adds a sentence, re-
moves a sentence, replaces a sentence by another, or
reorders a pair of sentences. The initial summary for
random restarts is constructed as follows. We first
pick the highest salience sentence with no incoming
negative edges as the first sentence. The following
sentences are probabilistically added one at a time
based on the summary score up to that sentence. The
initial summary is complete when there are no possi-
ble sentences left to fit within the budget. Intuitively,
this heuristic chooses a good starting point by se-
lecting a first sentence that does not rely on context
and subsequent sentences that build a high scoring
summary. As with all local search algorithms, this
algorithm is highly scalable and can easily apply to
large collections of related documents, but does not
guarantee global optima.
5 Experiments
Because summaries are intended for human con-
sumption we focused on human evaluations. We
hired workers on Amazon Mechanical Turk (AMT)
to evaluate the summaries. Our evaluation addresses
the following questions: (1) how do G-FLOW sum-
maries compare against the state-of-the-art in MDS
(Section 5.2)? (2) what is G-FLOW?s performance
along important summarization dimensions such as
coherence and redundancy (Section 5.3)? (3) how
does G-FLOW perform on coverage as measured
by ROUGE (Section 5.3.1)? (4) how much do the
components of G-FLOW?s objective function con-
tribute to performance (Section 5.4)? (5) how do G-
FLOW?s summaries compare to human summaries?
5.1 Data and Systems
We evaluated the systems on the Task 2 DUC?04
multi-document summarization dataset. This dataset
consists of 50 clusters of related documents, each of
which contains 10 documents. Each cluster of doc-
1168
uments also includes four gold standard summaries
used for evaluation. As in the DUC?04 competition,
we allowed 665 bytes for each summary including
spaces and punctuation. We used DUC?03 as our
development set, which contains 30 document clus-
ters, again with approximately 10 documents each.
We compared G-FLOW against four systems. The
first is a recent MDS extractive summarizer, which
we choose for its state-of-the-art ROUGE scores
(Lin and Bilmes, 2011).4 The second is a pipeline
of Lin?s system followed by a reimplementation of
a state-of-the-art sentence reordering system (Li et
al., 2011a). We refer to these systems as LIN and
LIN-LI, respectively. This second baseline allows
us to quantify the advantage of using coherence as a
factor in both sentence extraction and ordering.
We also compare against the system that had the
highest coherence ratings at DUC?04 (Nobata and
Sekine, 2004), which we refer to as NOBATA. As
this system did not preform sentence ordering on its
output, we also compare against a pipeline of No-
bata?s system and the sentence reordering system.
We refer to this system as NOBATA-LI.
Lastly, to evaluate how well the system performs
against human generated summaries, we compare
against the gold standard summaries provided by
DUC.
5.2 Overall Summary Quality
Following (Haghighi and Vanderwende, 2009) and
(Celikyilmaz and Hakkani-Tur, 2010), to compare
overall summary quality, we asked AMT workers
to compare two candidate system summaries. The
workers first read a gold standard summary, fol-
lowed by the two system summaries, and were then
asked to choose the better summary from the pair.
The system summaries were shown in a random or-
der to remove any bias.
To ensure that workers provided high quality data
we added two quality checks. First, we restricted
to workers who have an overall approval rating of
over 95% on AMT. Second, we asked the workers
to briefly describe the main events of the summary.
We manually filtered out work where this descrip-
tion was incorrect.
4We thank Lin and Bilmes for providing us with their code.
Unfortunately, we were unable to obtain other recent MDS sys-
tems from their authors.
Six workers compared each pair of summaries.
We recorded the scores for each cluster, and report
three numbers: the percentages of clusters where a
system is more often preferred over the other and the
percentage where the two systems are tied. G-FLOW
is preferred almost three times as often as LIN:
G-FLOW Indifferent LIN
56% 24% 20%
Next, we compared G-FLOW and LIN-LI. Sen-
tence reordering improves performance, but G-
FLOW is still overwhelmingly preferred:
G-FLOW Indifferent LIN-LI
54% 24% 22%
These results suggest that incorporating coher-
ence in sentence extraction adds significant value to
a summarization system. In these experiments, LIN
and LIN-LI are preferred in some cases. We an-
alyzed those summaries more carefully, and found
that occasionally, G-FLOW will sacrifice a small
amount of coverage for coherence, resulting in lower
performance in those cases (see Section 5.3.1).
We also compared LIN and LIN-LI, and found
that reordering does not improve performance by
much.
LIN-LI Indifferent LIN
32% 38% 30%
While the scores presented above represent com-
parisons between G-FLOW and a summarization
system with state-of-the-art ROUGE scores, we
also compared against a summarization system with
state-of-the-art coherence scores ? the system with
the highest coherence scores from DUC?04, (No-
bata and Sekine, 2004). We found that G-FLOW was
again preferred:
G-FLOW Indifferent NOBATA
68% 10% 22%
Adding in sentence ordering again improved the
scores for the comparison system somewhat:
G-FLOW Indifferent NOBATA-LI
60% 20% 20%
While these scores show a significant improve-
ment over previous sytems, they do not convey how
well G-FLOW compares to the gold standard ? man-
ually generated summaries. As a final experiment,
we compared G-FLOW and a second, manually gen-
erated summary:
1169
G-FLOW Indifferent Gold
14% 18% 68%
While we were pleased that in 32% of the cases,
Turkers either preferred G-FLOW or were indiffer-
ent, there is clearly a lot of room for improvement
despite the gains reported over previous sytems.
5.3 Comparison along Summary Dimensions
A high quality summary needs to be good along sev-
eral dimensions. We asked AMT workers to rate
summaries using the quality questions enumerated
in DUC?04 evaluation scheme.5 These questions
concern: (1) coherence, (2) useless, confusing, or
repetitive text, (3) redundancy, (4) nouns, pronouns,
and personal names that are not well-specified (5)
entities rementioned in an overly explicit way, (6)
ungrammatical sentences, and (7) formatting errors.
We evaluated G-FLOW LIN-LI and NOBATA-LI
against the gold standard summaries, using the same
AMT scheme as in the previous section. To assess
automated performance with respect to the standards
set by human summaries, we also evaluated a (dif-
ferent) gold standard summary for each document
cluster, using the same Mechanical Turk scheme as
in the previous section. The 50 summaries produced
by each system were evaluated by four workers. The
results are shown in Figure 2.
G-FLOW was rated significantly better than LIN-
LI in all categories except ?Redundancy? and signif-
icant better than NOBATA-LI on ?Coherence? and
?Referents?. The ratings for ?Coherence?, ?Refer-
ents?, and ?OverlyExplicit? are not surprising given
G-FLOW?s focus on coherence. The results for
?UselessText? may also be due to G-FLOW?s focus
on coherence which ideally prevents it from getting
off topic. Lastly, G-FLOW may perform better on
?Grammatical? and ?Formatting? because it tends to
choose longer sentences than other systems, which
are less likely to be sentence segmentation errors.
There may also be some bleeding from one dimen-
sion to the other ? if a worker likes one summary she
may score it highly for many dimensions.
Finally, somewhat surprisingly, we find G-
FLOW?s performance to be nearly that of human
summaries. G-FLOW is rated statistically signifi-
cantly lower than the gold summaries on only ?Re-
5http://duc.nist.gov/duc2004/quality.questions.txt
System R F
NOBATA 30.44 34.36
Best system in DUC-04 38.28 37.94
Takamura and Okumura (2009) 38.50 -
LIN 39.35 38.90
G-FLOW 37.33 37.43
Gold Standard Summaries 40.03 40.03
Table 3: ROUGE-1 recall and F-measure results (%) on
DUC-04. Some values are missing because not all sys-
tems reported both F-measure and recall.
dundancy?. Given the results from the previous sec-
tion, G-FLOW is likely performing worse on cate-
gories not conveyed in these scores, such as Cover-
age, which we examine next.
5.3.1 Coverage Evaluation using ROUGE
Most recent research has focused on the ROUGE
evaluation, and thus implicitly on coverage of in-
formation in a summary. To estimate the coverage
of G-FLOW, we compared the systems on ROUGE
(Lin, 2004). We calculated ROUGE-1 scores for
G-FLOW, LIN, and NOBATA.6 As sentence order-
ing does not matter for ROUGE, we do not include
LIN-LI or NOBATA-LI in this evaluation. Because
our algorithm does not explicitly maximize coverage
while LIN does, we expected G-FLOW to perform
slightly worse than LIN.
The ROUGE-1 scores for G-FLOW, LIN, NO-
BATA and other recent MDS systems are listed in Ta-
ble 3. We also include the ROUGE-1 scores for the
gold summaries (compared to the other gold sum-
maries). G-FLOW has slightly lower scores than
LIN and the gold standard summaries, but much
higher scores than NOBATA. G-FLOW only scores
significantly lower than LIN and the gold standard
summaries.
We can conclude that good summaries have both
the characteristics listed in the quality dimensions,
and good coverage. The gold standard summaries
outperform G-FLOW on both ROUGE scores and
the quality dimension scores, and therefore, out-
perform G-FLOW on overall comparison. How-
ever, G-FLOW is preferred to LIN-LI in addition to
NOBATA-LI indicating that its quality scores out-
weigh its ROUGE scores in that comparison. An
improvement to G-FLOW may focus on increasing
6ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n
4 -w 1.2
1170
Coherence UselessText Redundancy Referents OverlyExplicit Grammatical Formatting
Rat
ing
0
1
2
3
4
GoldG?FlowNobata?LiLin?Li
Figure 2: Ratings for the systems. 0 is the lowest possible score and 4 is the highest possible score. G-FLOW is rated
significantly higher than LIN-LI on all categories, except for ?Redundancy?, and significantly higher than NOBATA-LI
on ?Coherence? and ?Referents?. G-FLOW is only significantly lower than the gold standard on ?Redundancy?.
coverage while retaining strengths such as coher-
ence.
5.4 Ablation Experiments
In this ablation study, we evaluated the contribution
of the main components of G-FLOW ? coherence
and salience. The details of the experiments are the
same as in the experiment described in Section 5.2.
We first measured the importance of coherence in
summary generation. This system G-FLOW-SAL is
identical to the full system except that it does not
include the coherence term in the objective function
(see Section 4.4). The results show that coherence is
very important to G-FLOW?s performance:
G-FLOW Indifferent G-FLOW-SAL
54% 26% 20%
Similarly, we evaluated the contribution of
salience. This system G-FLOW-COH does not in-
clude the salience term in the objective function:
G-FLOW Indifferent G-FLOW-COH
60% 20% 20%
Without salience, the system produces readable,
but highly irrelevant summaries.
5.5 Agreement of Expert & AMT Workers
Because summary evaluation is a relatively complex
task, we compared AMT workers? annotations with
expert annotations from DUC?04. We randomly
selected ten summaries from each of the seven
DUC?04 annotators, and asked four Turk workers
to annotate them on the DUC?04 quality questions.
For each DUC?04 annotator, we selected all pairs
of summaries where one summary was judged more
than one point better than the other summary. We
compared whether the workers (voting as in Sec-
tion 5.2) likewise judged that summary better than
the second summary. We found that the annotations
agreed in 75% of cases. When we looked only at
pairs more than two points different, the agreement
was 80%. Thus, given the subjective nature of the
task, we feel reasonably confident that the AMT an-
notations are informative, and that the dramatic pref-
erence of G-FLOW over the baseline systems is due
to a substantial improvement in its summaries.
6 Conclusion
In this paper, we present G-FLOW, a multi-
document summarization system aimed at generat-
ing coherent summaries. While previous MDS sys-
tems have focused primarily on salience and cov-
erage but not coherence, G-FLOW generates an or-
dered summary by jointly optimizing coherence and
salience. G-FLOW estimates coherence by using
an approximate discourse graph, where each node
is a sentence from the input documents and each
edge represents a discourse relationship between
two sentences. Manual evaluations demonstrate that
G-FLOW generates substantially better summaries
than a pipeline of state-of-the-art sentence selec-
tion and reordering components. ROUGE scores,
which measure summary coverage, show that G-
FLOW sacrifices a small amount of coverage for
overall readability and coherence. Comparisons to
gold standard summaries show that G-FLOW must
improve in coverage to equal the quality of manu-
ally written summaries. We believe this research has
applications to other areas of summarization such as
update summarization and query based summariza-
tion, and we are interested in investigating these top-
ics in future work.
1171
Acknowledgements
We thank Luke Zettlemoyer, Lucy Vanderwende, Hal
Daume III, Pushpak Bhattacharyya, Chris Quirk, Erik
Frey, Tony Fader, Michael Schmitz, Alan Ritter, Melissa
Winstanley, and the three anonymous reviewers for help-
ful conversations and feedback on earlier drafts. We also
thank Lin and Bilmes for providing us with the code for
their system. This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-11-1-0294, and
DARPA contract FA8750-13-2-0019, and carried out at
the University of Washington?s Turing Center. This pa-
per was also supported in part by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via Air Force
Research Laboratory (AFRL) contract number FA8650-
10-C-7058. The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. The
views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either ex-
pressed or implied, of IARPA, AFRL, or the U.S. Gov-
ernment.
References
Amjad Abu-Jbara and Dragomir R. Radev. 2011. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of ACL 2011, pages 500?509.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using A * search and
discriminative training. In Proceedings of EMNLP
2010.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI 2007, pages 68?74.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay, Noemie Elhadad, and Kathleen R McK-
eown. 2001. Sentence ordering in multidocument
summarization. In Proceedings of HLT 2001, pages
1?7.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. Informa-
tion Process Management, 46(1):89?109.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?824.
Angel Chang and Christopher Manning. 2012. SU-
TIME: A library for recognizing and normalizing time
expressions. In Proceedings of LREC 2012.
Gunes Erkan and Dragomir R Radev. 2004. LexRank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
22(1):457?479.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Maria Lucia Castro Jorge and Thiago Alexan-
dre Salgueiro Pardo. 2010. Multi-Document
Summarization: Content Selection based on CST
Model (Cross-document Structure Theory). Ph.D.
thesis, Nu?cleo Interinstitucional de Lingu???stica
Computacional (NILC).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL 2003, pages 545?552.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In CoNLL 2011
Shared Task.
Peifeng Li, Guangxi Deng, and Qiaoming Zhu. 2011a.
Using context inference to improve sentence ordering
for multi-document summarization. In Proceedings of
IJCNLP 2011, pages 1055?1061.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011b.
Generating aspect-oriented multi-document summa-
rization with event-aspect model. In Proceedings of
EMNLP 2011, pages 1137?1146.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81.
Annie Louis and Ani Nenkova. 2009. Automatic sum-
mary evaluation without using human models. In Pro-
ceedings of EMNLP 2009, pages 306?314.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit discourse relations. In Proceedings of SIGDIAL
2010, pages 59?62.
1172
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Publishing Co, Amsterdam/Philadelphia.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL 2002, pages 368?375.
Daniel Marcu. 1997. From discourse structures to text
summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, pages 82?88.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP
2012, pages 523?534.
Kathleen McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of SIGIR 1995, pages 74?82.
Chikashi Nobata and Satoshi Sekine. 2004. Crl/nyu
summarization system at duc-2004. In Proceedings of
DUC 2004.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of COLING 2004,
pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of ACL
2010, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of LREC 2008.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of COLING
2010, pages 895?903.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):469?500.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938.
Dragomir R. Radev. 2004. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. Jour-
nal of Artificial Intelligence Research, 22(1):457?479.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. In Proceedings of DUC
2004.
Horacio Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting the
dots between news articles. In Proceedings of KDD
2010, pages 623?632.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proceed-
ings of Coling 2010, pages 984?992.
Maite Taboada and William C. Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse Studies,
8(4):567?588.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL 2009,
pages 781?789.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?288.
Zhu Zhang, Sasha Blair-Goldensohn, and Dragomir R.
Radev. 2002. Towards CST-enhanced summarization.
In Proceedings of AAAI 2002, pages 439?445.
Renxian Zhang, Li Wenjie, and Lu Qin. 2010. Sen-
tence ordering with event-enriched semantics and two-
layered clustering for multi-document news summa-
rization. In Proceedings of COLING 2010, pages
1489?1497.
1173
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 902?912,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical Summarization:
Scaling Up Multi-Document Summarization
Janara Christensen Stephen Soderland
Computer Science & Engineering
University of Washington
Seattle, USA
janara@cs.washington.edu
soderlan@cs.washington.edu
Gagan Bansal Mausam
Computer Science & Engineering
Indian Institute of Technology
Delhi, India
gaganbansal1993@gmail.com
mausam@cse.iitd.ac.in
Abstract
Multi-document summarization (MDS)
systems have been designed for short, un-
structured summaries of 10-15 documents,
and are inadequate for larger document
collections. We propose a new approach
to scaling up summarization called hierar-
chical summarization, and present the first
implemented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, in which the top level
provides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. SUMMA
optimizes for coherence as well as cover-
age of salient information. In an Amazon
Mechanical Turk evaluation, users pref-
ered SUMMA ten times as often as flat
MDS and three times as often as timelines.
1 Introduction
The explosion in the number of documents
on the Web necessitates automated approaches
that organize and summarize large document col-
lections on a complex topic. Existing methods
for multi-document summarization (MDS) are de-
signed to produce short summaries of 10-15 doc-
uments.
1
MDS systems do not scale to data sets
ten times larger and proportionately longer sum-
maries: they either cannot run on large input or
produce a disorganized summary that is difficult
to understand.
We present a novel MDS paradigm, hierarchi-
cal summarization, which operates on large doc-
ument collections, creating summaries that orga-
nize the information coherently. It mimics how
someone with a general interest in a complex topic
would learn about it from an expert ? first, the ex-
pert would provide an overview, and then more
1
In the DUC evaluations, summaries have a budget of 665
bytes and cover 10 documents.
Hierarchical Summarization: Scaling Up Multi-Document
Summarization
bstract
For topics that cover large amounts of in-
formation, simple, short summaries are in-
sufficient ? complex topics require more
information and more structure to under-
stand. We propose a new approach to scal-
ing up summarization called hierarchical
summarization, and present the first imple-
mented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, where the top level pro-
vides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. Com-
pared to flat multi-document summaries,
users prefer SUMMA ten times as often
and learn just as much, and compared to
timelines, users prefer SUMMA three t mes
as often and learn more in twice as many
cases.
1 Introduction
The explosion in the number of documents over
the Web necessitates automated approaches that
organize and summarize large document collec-
tions on a complex topic. Existing methods for
multi-document summarization (MDS) can handle
10-15 documents and create a short flat summary,
but are insufficient for large-scale summarization.
For large-scale summarization, we need summa-
rizers that organize the information coherently and
enable personalized interaction with the summary
so that users can explore the various aspects of in-
formation in different levels of detail based on in-
dividual interest.
To this end, we present a novel MDS paradigm,
hierarchical summarization. Hierarchical summa-
rization is designed to operate on large document
collections. It mimics how someone with a gen-
eral interest in a complex topic would learn about
x1,1
x1,2
x1,3
x4,1
x4,2
x4,3
x9,1
x9,2
x8,1
x8,2
x8,3
x7,1
x7,2
x3,1
x3,2
x3,3
x2,1
x2,2 x6,1
x6,2
x5,1
x5,2
x5,3
On Aug 7 1998, car bombs
exploded outside US em-
bassies in Kenya and Tanza-
nia. Several days later, the
US began investigations into
bombings. The US retali-
ated with missile strikes on
suspected terrorist camps
in Afghanistan and Sudan
on Aug 20.
Some questioned the timing
of Clinton?s decision to launch
strikes. On Aug 22, with bin
Laden having survived the
strikes, the US outlined other
efforts to damage his net-
work. Russia, Sudan, Pakistan,
and Afghanistan condemned the
strikes.
Clinton proposed meth-
ods to inflict financial
damage on bin Laden.
Another possibility is
for the United States to
negotiate with the Tal-
iban to surrender bin
Laden. But diplomats
who have dealt with
the Taliban doubt that
anything could come of
such negotiations.
Figure 1: An example of a hierarchical summary for the 1998
embassy bombings, with one branch of the hierarchy high-
lighted. Each rectangle represents a summary and each x
i,j
represents a sentence within a summary. The root summary
provides an overview of the events of August 1998. When the
last sentence is selected, a more detailed summary of the mis-
sile strikes is produced, and when the middle sentence of that
summary is selected, a more detailed summary bin Laden?s
escape is produced.
it from an expert ? first, the expert would give
an overview, and then more specific information
about various aspects. It has the following novel
characteristics:
i r A hierarchical summary of the 1998 embassy
bombings. Each rectangle represents a summary and each
x
i,j
is a sentenc within a summary. The root summary pro-
vides an overview of the events of August 1998. When the
third sentence is selected, a more detailed summary of the
missile strikes is displayed. Selecting the second sentence of
that summary produces a more detailed summary of the US?
options.
specific information about various aspects. Hi-
erarchical summarization has the following novel
characteristics:
? The summary is hierarchically organized
along one or more organizational principles
such as time, location, entities, or events.
? Each non-leaf summary is associated with a
set of child summaries where each gives de-
tails of an element (e.g. sentence) in the par-
ent summary.
? A user can navigate within the hierarchical
summary by clicking on an element of a par-
ent summary to view the associated child
summary.
For example, given the topic, ?1998 embassy
bombings,? the first summary (Figure 1) might
902
mention that the US retaliated by striking
Afghanistan and Sudan. The user can click on this
information to learn more about these attacks. In
this way, the system can present large amounts of
information without overwhelming the user, and
the user can tailor the output to their interests.
In this paper, we describe SUMMA, the first
hierarchical summarization system for multi-
document summarization.
2
It operates on a corpus
of related news articles. SUMMA hierarchically
clusters the sentences by time, and then summa-
rizes the clusters using an objective function that
optimizes salience and coherence.
We conducted an Amazon Mechanical Turk
(AMT) evaluation where AMT workers compared
the output of SUMMA to that of timelines and flat
summaries. SUMMA output was judged superior
more than three times as often as timelines, and
users learned more in twice as many cases. Users
overwhelmingly preferred hierarchical summaries
to flat summaries (92%) and learned just as much.
Our main contributions are as follows:
? We introduce and formalize the novel task of
hierarchical summarization.
? We present SUMMA, the first hierarchical
summarization system, which operates on
news corpora and summarizes over an or-
der of magnitude more documents than tra-
ditional MDS systems, producing summaries
an order of magnitude larger.
? We present a user study which demonstrates
the value of hierarchical summarization over
timelines and flat multi-document summaries
in learning about a complex topic.
In the next section, we formalize hierarchical
summarization. We then describe our methodol-
ogy to implement the SUMMA hierarchical sum-
marization system: hierarchical clustering in Sec-
tion 3 and creating summaries based on that clus-
tering in Section 4. We discuss our experiments in
Section 5, related work in Section 6, and conclu-
sions in Section 7.
2 Hierarchical Summarization
We propose a new task for large-scale summariza-
tion called hierarchical summarization. Input to a
hierarchical summarization system is a set of re-
lated documents D and a budget b for each sum-
mary within the hierarchy (in bytes, words, or sen-
tences). The output is the hierarchical summary
H , which we define formally as follows.
2
http://knowitall.cs.washington.edu/summa/
Definition A hierarchical summary H of a docu-
ment collection D is a set of summaries X orga-
nized into a hierarchy. The top of the hierarchy
is a summary X
1
representing all of D, and each
summary X
i
consists of summary units x
i,j
(e.g.
the jth sentence of summary i) that point to a child
summary, except at the leaf nodes of the hierarchy.
A child summary adds more detail to the infor-
mation in its parent summary unit. The child sum-
mary may include sub-events or background and
reactions to the event or topic in the parent.
We define several metrics in Section 4 for
a well-constructed hierarchical summary. Each
summary should maximize coverage of salient in-
formation; it should minimize redundancy; and
it should have intra-cluster coherence as well as
parent-to-child coherence.
Hierarchical summarization has two important
strengths in the context of large-scale summariza-
tion. First, the information presented at the start
is small and grows only as the user directs it, so
as not to overwhelm the user. Second, each user
directs his or her own experience, so a user inter-
ested in one aspect need only explore that section
of the data without having to view or understand
the entire summary. The parent-to-child links pro-
vide a means for a user to navigate, drilling down
for more details on topics of interest.
There are several possible organizing principles
for the hierarchy ? by date, by entities, by loca-
tions, or by events. Some organizing principles
will fit the data in a document collection better
than others. A system may select different orga-
nization for different portions of the hierarchy, for
example, organizing first by location or prominent
entity and then by date for the next level.
3 Hierarchical Clustering
Having defined the task, we now describe
the methodology behind our implementation,
SUMMA. In future work we intend to design a
system that dynamically selects the best organiz-
ing principle for each level of the hierarchy. In
this first implementation, we have opted for tem-
poral organization, since this is generally the most
appropriate for news events.
The problem of hierarchical summarization as
described in Section 2 has all of the requirements
of MDS, and additional complexities of inducing a
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
903
Hierarchical Clustering H
s1
. . .
sN
sj+1
. . .
sN
sl+1
. . .
sN
sj+1
. . .
sl
si+1
. . .
sj
s1
. . .
si
sk+1
. . .
si
s1
. . .
sk
Hierarchical Summary X
x1,1
x1,2
x1,3
x4,1
x4,2
x8,1
x8,2
x7,1
x7,2
x7,3
x3,1
x3,2
x3,3
x2,1
x2,2
x6,1
x6,2
x5,1
x5,2
x5,3
Figure 2: Examples of a hierarchical clustering and a hier-
archical summary, where the input sentences are s 2 S, the
number of input sentences is N , and the summary sentences
are x 2 X . The hierarchical clustering determines the struc-
ture of the hierarchical summary.
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the sentences in that cluster. More-
over, the number of sentences in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are interested in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each node (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For example, Figure 3 shows the number of
articles per day related to 1998 embassy bombings
published in the New York Times (identified using
a key word search). There were two main events
? on the 7th, the embassies were bombed and
on the 20th, US retaliated through missile strikes.
The figure shows a correspondence between these
events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters evenly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where C is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the tradeoff parameter.
B(C) =
X
c2C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first and second date of c:
burst(c) =
pub(d
i
) + pub(d
i+1
)
2
  pub(d
i 1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
Figure 2: Examples of input and output o hierarchical sum-
marization. The input sentences ar s ? S, the number of
input sentences is N , and the summary sentences are x ? X .
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the senten in that cluster. More-
over, the number of sent es in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are inter st d in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each no e (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For exampl , Figure 3 show the number of
articles per day related to the 1998 embassy bomb-
ings published in the New York Times (identified
using a key word search). There were two main
events ? on the 7th, the embassies were bombed
an on the 20th, the US retaliated through mis-
sile strikes Th figure shows a correspondence
between these events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters e enly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the trad off parameter.
B(C) =
?
c?C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first a d sec nd date of c:
burst(c) =
b(d
i
) + pub(d
i+1
)
2
? pub(d
i?1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
pub(d
i
) is the number of sentences published on
d
i
. The evenness of the split is measured by:
E(C) = min
c?C
size(c) (4)
where size(c) is the number of dates in cluster c.
We perform hierarchical clustering top-down, at
each point solving for Equation 1. ? was set using
a grid-search over a development set.
904
6 8 10 12 14 16 18 20 22 24
0
20
40
Day of Month
N
u
m
b
e
r
o
f
A
r
t
i
c
l
e
s
1
Figure 3: News coverage by date for the embassy bombings
in Tanzania and Kenya. There are spikes in the number of
articles published at the two major events.
3.2 Choosing the number of clusters
We cannot know a priori the number of clusters
for a given topic. However, when the number of
clusters is too large for the given summary budget,
the sentences will have to be too short, and when
the number of clusters is too small, we will not use
enough of the budget. We set the maximum num-
ber of clusters k
max
and minimum number of clus-
ters k
min
to be a function of the budget b and the
average sentence length in the cluster s
avg
, such
that k
max
? s
avg
? b and k
min
? s
avg
? b/2.
Given a maximum and minimum number of
clusters, we must determine the appropriate num-
ber of clusters. At each level, we cluster the sen-
tences by the method described above and choose
the number of clusters k according to the gap
statistic (Tibshirani et al, 2000). Specifically, for
each level, the algorithm will cluster repeatedly
with k varying from the minimum to the maxi-
mum. The algorithm will return the k that max-
imizes the gap statistic:
Gap
n
(k) = E
?
n
{log(W
k
)} ? log(W
k
) (5)
where W
k
is the score for the clusters computed
with Equation 1, and E
?
n
is the expectation under
a sample of size n from a reference distribution.
Ideally, the maximum depth of the clustering
would be a function of the number of sentences
in each cluster, but in our implementation, we set
the maximum depth to three, which works well for
the size of the datasets we use (300 articles).
4 Summarizing within the Hierarchy
After the sentences are clustered, we have a struc-
ture for the hierarchical summary that dictates the
number of summaries and the number of sentences
in each summary. We also have the set of sen-
tences from which each summary is drawn.
Intuitively, each cluster summary in the hierar-
chical summary should convey the most salient
information in that cluster. Furthermore, the hier-
archical summary should not include redundant
sentences. A hierarchical summary that is only
salient and nonredundant may still not be suitable
if the sentences within a cluster summary are dis-
connected or if the parent sentence for a summary
does not relate to the child summary. Thus, a hi-
erarchical summary must also have intra-cluster
coherence and parent-to-child coherence.
4.1 Salience
Salience is the value of each sentence to the topic
from which the documents are drawn. We measure
salience of a summary (Sal(X)) as the sum of the
saliences of individual sentences (
?
i
Sal(x
i
)).
Following previous research in MDS, we com-
puted individual saliences using a linear regres-
sion classifier trained on ROUGE scores over the
DUC?03 dataset (Lin, 2004; Christensen et al,
2013). This method finds those sentences more
salient that mention nouns or verbs that occur fre-
quently in the cluster.
In preliminary experiments, we noticed that
many sentences that were reaction sentences were
given a higher salience than action sentences. For
example, the reaction sentence, ?President Clinton
vowed to track down the perpetrators behind the
bombs that exploded outside the embassies in Tan-
zania and Kenya on Friday,? would have a higher
score than the action sentence, ?Bombs exploded
outside the embassies in Tanzania and Kenya on
Friday.? This problem occurs because the first sen-
tence has a higher ROUGE score (it covers more
important words than the second sentence). To ad-
just for this problem, we use only words identified
in the main clause (heuristically identified via the
parse tree) to compute our salience scores.
4.2 Redundancy
We identify redundant sentences using a linear
regression classifier trained on a manually la-
beled subset of the DUC?03 sentences. The fea-
tures include shared noun counts, sentence length,
TF*IDF cosine similarity, timestamp difference,
and features drawn from information extraction
such as number of shared tuples in Open IE
(Mausam et al, 2012).
905
4.3 Summary Coherence
We require two types of coherence: coherence be-
tween the parent and child summaries and coher-
ence within each summary X
i
.
We rely on the approximate discourse graph
(ADG) that was proposed in (Christensen et al,
2013) as the basis for measuring coherence. Each
node in the ADG is a sentence from the dataset.
An edge from sentence s
i
to s
j
with positive
weight indicates that s
j
may follow s
i
in a coher-
ent summary, e.g. continued mention of an event
or entity, or coreference link between s
i
and s
j
.
A negative edge indicates an unfulfilled discourse
cue or co-reference mention.
Parent-to-Child Coherence: Users navigate the
hierarchical summary from parent sentence to
child summary, so if the parent sentence bears no
relation to the child summary, the user will be un-
derstandably confused. The parent sentence must
have positive evidence of coherence with the sen-
tences in its child summary.
We estimate parent to child coherence as the co-
herence between a parent sentence and each sen-
tence in its child summary as:
PCoh(X) =
?
c?C
?
i=1..|X
c
|
w
G+
(x
p
c
, x
c,i
)) (6)
where x
p
c
is the parent sentence for cluster c and
w
G+
(x
p
c
, x
c,i
) is the sum of the positive edge
weights from x
p
c
to x
c,i
in the ADG G.
Intra-cluster Coherence: In traditional MDS, the
documents are usually quite focused, allowing for
highly focused summaries. In hierarchical sum-
marization, however, a cluster summary may span
hundreds of documents and a wide range of infor-
mation. For this reason, we may consider a sum-
mary acceptable even if it has limited positive evi-
dence of coherence in the ADG, as long as there
is no negative evidence in the form of negative
edges. For example, the following is a reasonable
summary for events spanning two weeks:
s
1
Bombs exploded at two US embassies.
s
2
US missiles struck in Afghanistan and Sudan.
Our measure of intra-cluster coherence mini-
mizes the number of missing references. These
are coreference mentions or discourse cues where
none of the sentences read before (either in an an-
cestor summary or in the current summary) con-
tain an antecedent:
CCoh(X) = ?
?
c?C
?
i=1..|X
c
|
#missingRef(x
c,i
) (7)
4.4 Objective Function
Having estimated salience, redundancy, and two
forms of coherence, we can now put this informa-
tion together into a single objective function that
measures the quality of a candidate hierarchical
summary.
Intuitively, the objective function should bal-
ance salience and coherence. Furthermore, the
summary should not contain redundant informa-
tion and each cluster summary should honor the
given budget, i.e., maximum summary length b.
We treat redundancy and budget as hard con-
straints and coherence and salience as soft con-
straints. Lastly, we require that sentences are
drawn from the cluster that they represent and that
the number of sentences in the summary corre-
sponding to each non-leaf cluster c is equivalent
to the number of child clusters of c. We optimize:
maximize: F (x) , Sal(X) + ?PCoh(X) + ?CCoh(X)
s.t. ?c ? C :
?
i=1..|X
c
|
len(x
c,i
) < b
?x
i
, x
j
? X : redundant(x
i
, x
j
) = 0
?c ? C, ?x
c
? X
c
: x
c
? c
?c ? C : |X
c
| = #children(c)
The tradeoff parameters ? and ? were set based
on a development set.
4.5 Algorithm
Optimizing this objective function is NP-hard, so
we approximate a solution by using beam search
over the space of partial hierarchical summaries.
Notice the contribution from a sentence depends
on individual salience, coherence (CCoh) based
on sentences visible on the user path down the hi-
erarchy to this sentence, and coherence (PCoh)
based on its parent sentence and its child sum-
mary. Since most of the sentence contributions de-
pend on the path from the root to the sentence, we
build our partial summary by incrementally adding
a sentence top-down in the hierarchy and from first
sentence to last within a cluster summary.
To account for PCoh, we estimate the contribu-
tion of the sentence by jointly identifying its best
child summary. However, we do not fix the child
summary at this time ? we simply use it to estimate
PCohwhen using that sentence. Since computing
the best child summary is also intractable we ap-
proximate a solution by a local search algorithm
over the child cluster.
Overall, our algorithm is a two level nested
search algorithm ? beam search in the outer loop to
906
search through the space of partial summaries and
local search (hill climbing with random restarts) in
the inner loop to pick the best sentence to add to
the existing partial summary. We use a beam of
size ten in our implementation.
5 Experiments
Our experiments are designed to evaluate how ef-
fective hierarchical summarization is in summa-
rizing a large, complex topic and how well this
helps users learn about the topic. Our evaluation
addresses the following questions:
? Do users prefer hierarchical summaries for
topic exploration? (Section 5.1)
? Are hierarchical summaries more effective
than other methods for learning about com-
plex events? (Section 5.2)
? How informative are the hierarchical sum-
maries compared to the other methods? (Sec-
tion 5.3)
? How coherent is the hierarchical structure in
the summaries? (Section 5.4)
We compared SUMMA against two baseline sys-
tems which represent the main NLP methods for
large-scale summarization: an algorithm for cre-
ating timelines over sentences (Chieu and Lee,
2004),
3
and a state-of-the-art flat MDS system
(Lin and Bilmes, 2011).
4
Each system was given
the same budget (over 10 times the traditional
MDS budget, which is 665 bytes).
We evaluated the questions on ten news topics,
representing a range of tasks: (1) Pope John Paul
II?s death and the 2005 Papal Conclave, (2) Bush v.
Gore, (3) the Tulip Revolution, (4) Daniel Pearl?s
kidnapping, (5) the Lockerbie bombing handover
of suspects, (6) the Kargil War, (7) NATO?s bomb-
ing of Yugoslavia in 1999, (8) Pinochet?s arrest in
London, (9) the 2005 London bombings, and (10)
the crash and investigation of SwissAir Flight 111.
We chose topics containing a set of related events
that unfolded over several months and were promi-
nent enough to be reported in at least 300 articles.
We drew our articles from the Gigaword corpus,
which contains articles from the New York Times
and other major newspapers. For each topic, we
used the 300 documents that best matched a key
3
Unfortunately, we were unable to obtain more recent
timeline systems from authors of the systems.
4
(Christensen et al, 2013) is a state-of-the-art coherent
MDS system, but does not scale to 300 documents.
word search. We selected topics which were be-
tween five and fifteen years old so that evaluators
would have relatively less pre-existing knowledge
about the topic.
5.1 User Preference
In our first experiment, we simply wished to eval-
uate which system users most prefer. We hired
Amazon Mechanical Turk (AMT) workers and as-
signed two topics to each worker. We paired up
workers such that one worker would see output
from SUMMA for the first topic and a competing
system for the second and the other worker would
see the reverse. For quality control, we asked
workers to complete a qualification task first, in
which they were required to write a short summary
of a news article. We also manually removed spam
from our results. Previous work has used AMT
workers for summary evaluations and has shown
high correlations with expert ratings (Christensen
et al, 2013). Five workers were hired to view each
topic-system pair.
We asked the workers to choose which format
they preferred and to explain why. The results are
as follows:
SUMMA 76% TIMELINE 24%
SUMMA 92% FLAT-MDS 8%
Users preferred the hierarchical summaries
three times more often than timelines and over
ten times more often than flat summaries. When
we examined the reasons given by the users, we
found that the people who preferred the hierar-
chical summaries liked that they gave a big pic-
ture overview and were then allowed to drill down
deeper. Some also explained that it was eas-
ier to remember information when presented with
the overview first. Typical responses included,
?Could gather and absorb the information at my
own pace,? and, ?Easier to follow and understand.?
When users preferred the timelines, they usually
remarked that it was more familiar, i.e. ?I liked
the familiarity of the format. I am used to these
timelines and they feel comfortable.? Users com-
plained that the flat summaries were disjointed,
confusing, and very frustrating to read.
5.2 Knowledge Acquisition
Evaluating how much a user learned is inherently
difficult, more so when the goal is to allow the user
the freedom to explore information based on indi-
vidual interest. For this reason, instead of asking a
set of predefined questions, we assess the knowl-
907
edge gain by following the methodology of (Sha-
haf et al, 2012) ? asking users to write a paragraph
summarizing the information learned.
Using the same setup as in the previous exper-
iment, for each topic, five AMT workers spent
three minutes reading through a timeline or sum-
mary and were then asked to write a description
of what they had learned. Workers were not al-
lowed to see the timeline or summary while writ-
ing. We collected five descriptions for each topic-
system combination. We then asked other AMT
workers to read and compare the descriptions writ-
ten by the first set of workers. Each evaluator was
presented with a corresponding Wikipedia article
and descriptions from a pair of users (timeline vs.
SUMMA or flat MDS vs. SUMMA). The descrip-
tions were randomly ordered to remove bias. The
workers were asked which user appeared to have
learned more and why. For each pair of descrip-
tions, four workers evaluated the pair. Standard
checks such as approval rating, location filtering,
etc. were used for removing spam. The results of
this experiment are as follows:
Prefer Indiff. Prefer
SUMMA 58% 17% TIMELINE 25%
SUMMA 40% 22% FLAT-MDS 38%
Descriptions written by workers using SUMMA
were preferred over twice as often as those from
timelines. We looked more closely at those cases
where the participants either preferred the time-
lines or were indifferent and found that this pref-
erence was most common when the topic was not
dominated by a few major events, but was instead
a series of similarly important events. For exam-
ple, in the kidnapping and beheading of Daniel
Pearl there were two or three obviously major
events, whereas in the Kargil War there were many
smaller important events. In latter cases, the hier-
archical summaries provided little advantage over
the timelines because it was more difficult to ar-
range the sentences hierarchically.
Since SUMMA was judged to be so much supe-
rior to flat MDS systems in Section 5.1, it is sur-
prising that users descriptions from flat MDS were
preferred nearly as often as those from SUMMA.
While the flat summaries were disjointed, they
were good at including salient information, with
the most salient tending to be near the start of the
summary. Thus, descriptions from both SUMMA
and flat MDS generally covered the most salient
information.
5.3 Informativeness
In this experiment, we assess the salience of the
information captured by the different systems, and
the ability of SUMMA to organize the information
so that more important information is placed at
higher levels.
ROUGE Evaluation: We first automatically
assessed informativeness by calculating the
ROUGE-1 scores of the output of each of the sys-
tems. For the gold standard comparison summary,
we use the Wikipedia articles for the topics.
5
Note that there is no good translation of ROUGE
for hierarchical summarization. Thus, we simply
use the traditional ROUGE metric, which will
not capture any of the hierarchical format. This
score will essentially serve as a rough measure of
coverage of the entire summary to the Wikipedia
article. The scores for each of the systems are as
follows:
P R F1
SUMMA 0.25 0.67 0.31
TIMELINE 0.28 0.65 0.33
FLAT-MDS 0.30 0.64 0.34
None of the differences are significant. From
this evaluation, one can gather that the systems
have similar coverage of the Wikipedia articles.
Manual Evaluation: While ROUGE serves as a
rough measure of coverage, we were interested in
gathering more fine-grained information on the in-
formativeness of each system. We performed an
additional manual evaluation that assesses the re-
call of important events for each system.
We first identified which events were most im-
portant in a news story. Because reading 300 arti-
cles per topic is impractical, we asked AMT work-
ers to read a Wikipedia article on the same topic
and then identify the three most important events
and the five most important secondary events. We
aggregated responses from ten workers per topic
and chose the three most common primary and five
most common secondary events.
One of the authors then manually identified the
presence of these events in the hierarchical sum-
maries, the timelines and the flat MDS summaries.
Below we show event recall (the percentage of the
events that were mentioned).
5
We excluded one topic (the handover of the Lockerbie
bombing suspects) because the corresponding Wikipedia ar-
ticle had insufficient information.
908
Events SUMMA TIMELINE FLAT-MDS
Prim. 96% 74% 93%
Sec. 76% 53% 64%
The difference in recall between SUMMA and
TIMELINE was significant in both cases, and the
difference between SUMMA and FLAT-MDS was
not. In general, the flat summaries were quite re-
dundant, which contributed to the slightly lower
event recall. The timelines, on the other hand,
were both incoherent and at the same time re-
ported less important facts.
We also evaluated at what level in the hierar-
chy the events were identified for the hierarchical
summaries. The event recall shows the percentage
of events mentioned at that level or above in the
hierarchical summary:
Events Level 1 Level 2 Level 3
Prim. 63% 81% 96%
Sec. 27% 51% 76%
81% of the primary events are present in the first
or second level, and 76% of the secondary events
are mentioned by the third level. While recog-
nizing primary events is relatively simple because
they are repeated frequently, identification of im-
portant secondary events often requires external
knowledge.
5.4 Parent-to-Child Coherence
We next tested the hierarchical coherence. One of
the authors graded how much each non-leaf sen-
tence in a summary was coherent with its child
summary on a scale of one to five, with one be-
ing incoherent and five being perfectly coherent.
We used the coherence scale from DUC?04.
6
Level 1 Level 2
Coherence 3.8 3.4
We found that for the top level of the summary,
the parent sentence generally represented the most
important event in the cluster and the child sum-
mary usually expressed details or reactions of the
event. The lower coherence scores were often the
result of too few lexical connections or lack of a
theme or story. While the facts of the sentences
made sense together, the summaries sometimes
did not read as if they were written by a human,
but as a series of disparate sentences.
For the second level, the problems were more
basic. The parent sentence occasionally expressed
a less important fact that the child summary did
6
http://duc.nist.gov/duc2004/quality.questions.txt
not then expand on or, more commonly, the child
summary was not focused enough. This result
stems from two problems in our algorithm. First,
summarizing sentences are rare, making good
choices for parent sentences difficult to find. The
second problem relates to the difficulty in identify-
ing whether two sentences are on the same topic.
For example, suppose the parent sentence is, ?A
Swissair plane Wednesday night crashed off Nova
Scotia, Canada.? A very good child sentence is,
?The airline confirmed that all passengers died.?
However, based on their surface features, the sen-
tence, ?A plane made an unscheduled landing after
a Swissair plane crashed off the coast of Canada,?
appears to be a better choice.
Even though there is scope for improvement, we
find these coherence scores encouraging for a first
algorithm for the task.
6 Related Work
Traditional approaches to large-scale summariza-
tion have included flat summaries and timelines.
There are two primary shortcomings to these ap-
proaches: first, they require the user to sort
through large amounts of potentially overwhelm-
ing information, and second, the output is static
? users with different interests will see the same
information. Below we describe related work on
traditional MDS, structured summaries, timelines,
discovering threads of documents and the uses of
hierarchies in generating summaries.
6.1 Traditional MDS
Traditionally, MDS systems have focused on three
to six sentence summaries covering 10-15 docu-
ments. Most extractive summarization research
aims to maximize coverage while reducing redun-
dancy (e.g. (Carbonell and Goldstein, 1998; Sag-
gion and Gaizauskas, 2004; Radev et al, 2004)).
Lin and Bilmes (2011) proposed a state-of-the-art
system that uses submodularity in sentence selec-
tion to accomplish these goals. Christensen et al
(2013) presented an algorithm for coherent MDS,
but it does not scale to larger output.
Structured Summaries: Some research has ex-
plored generating structured summaries. These
approaches attempt to identify major aspects of
a topic, but do not compile content to describe
those aspects. Rather, they rely on pre-existing, la-
beled paragraphs (for example, a paragraph titled,
?Symptoms of Meningitis?). Aspects are identi-
fied either by a training corpus of articles in the
909
same domain (Sauper and Barzilay, 2009), by an
entity-aspect LDA model (Li et al, 2010), or by
Wikipedia templates of related topics (Yao et al,
2011). These methods assume a common struc-
ture for all topics in a category, and do not allow
for more than two levels in the structure.
Timeline Generation: Recent papers in timeline
generation have emphasized the relationship with
summarization. Yan et al (2011b) balanced co-
herence and diversity to create timelines, Yan et
al. (2011a) used inter-date and intra-date sentence
dependencies, and Chieu and Lee (2004) used sen-
tence similarity. Others have emphasized identify-
ing important dates, primarily by bursts of news
(Swan and Allen, 2000; Akcora et al, 2010; Hu
et al, 2011; Kessler et al, 2012). While time-
lines can be useful for understanding events, they
do not generalize to other domains. Additionally,
long timelines can be overwhelming, short time-
lines have low information content, and there is
no method for personalized exploration.
Document Threads: A related track of research
investigates discovering threads of documents.
While we aim to summarize collections of infor-
mation, this track seeks to identify relationships
between documents. This research operates on the
document level, while ours operates on the sen-
tence level. Shahaf and Guestrin (2010) formal-
ized the characteristics of a good chain of articles
and proposed an algorithm to connect two speci-
fied articles. Gillenwater et al (2012) proposed
a probabilistic technique for extracting a diverse
set of threads from a given collection. Shahaf et
al. (2012) extended work on coherent threads to
finding coherent maps of documents, where a map
is set of intersecting threads representing how the
threads interact and relate.
Summarization and Hierarchies: A few papers
have examined the relationship between summa-
rization and hierarchies. Some focused on cre-
ating a hierarchical summary of a single docu-
ment (Buyukkokten et al, 2001; Otterbacher et
al., 2006), relying on the structure inherent in sin-
gle documents. Others investigated creating hier-
archies of words or phrases to organize documents
(Lawrie et al, 2001; Lawrie, 2003; Takahashi et
al., 2007; Haghighi and Vanderwende, 2009).
Other research identifies the hierarchical struc-
ture of the documents and generates a summary
that prioritizes more general information accord-
ing to the structure (Ouyang et al, 2009; Celikyil-
maz and Hakkani-Tur, 2010), or gains coverage by
drawing sentences from different parts of the hier-
archy (Yang and Wang, 2003; Wang et al, 2006).
7 Conclusions
We have introduced a new paradigm for large-
scale summarization called hierarchical summa-
rization, which allows a user to navigate a hier-
archy of relatively short summaries. We present
SUMMA, an implemented hierarchical news sum-
marization system,
7
and demonstrate its effective-
ness in a user study that compares SUMMA with
a timeline system and a flat MDS system. When
compared to timelines, users learned more with
SUMMA in twice as many cases, and SUMMA was
preferred more than three times as often. When
compared to flat summaries, users overwhelming
preferred SUMMA and learned just as much.
This first implementation performs temporal
clustering ? in future work, we will investigate dy-
namically selecting an organizing principle that is
best suited to the data at each level of the hierar-
chy: by entity, by location, by event, or by date.
We also intend to scale the system to even larger
document collections, and explore joint clustering
and summarization. Lastly, we plan to research
hierarchical summarization in other domains.
Acknowledgments
We thank Amitabha Bagchi, Niranjan Balasubra-
manian, Danish Contractor, Oren Etzioni, Tony
Fader, Carlos Guestrin, Prachi Jain, Lucy Van-
derwende, Luke Zettlemoyer, and the anonymous
reviewers for their helpful suggestions and feed-
back. We thank Hui Lin and Jeff Bilmes for
providing us with their code. This research was
supported in part by ARO contract W911NF-
13-1-0246, DARPA Air Force Research Labora-
tory (AFRL) contract FA8750-13-2-0019, UW-
IITD subcontract RP02815, and the Yahoo! Fac-
ulty Research and Engagement Award. This pa-
per is also supported in part by the Intelligence
Advanced Research Projects Activity (IARPA)
via AFRL contract number FA8650-10-C-7058.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, AFRL, or the U.S. Government.
7
http://knowitall.cs.washington.edu/summa/
910
References
C. G. Akcora, M. A. Bayir, M. Demirbas, and H. Fer-
hatosmanoglu. 2010. Identifying breakpoints in
public opinion. In 1st KDD Workshop on Social Me-
dia Analytics.
Berkhin Berkhin. 2006. A survey of clustering data
mining techniques. Grouping Multidimensional
Data, pages 25?71.
Orkut Buyukkokten, Hector Garcia-Molina, and An-
dreas Paepcke. 2001. Seeing the whole in parts:
Text summarization for web browsing on handheld
devices. In Proceedings of WWW 2001, pages 652?
662.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?
824.
Angel Chang and Christopher Manning. 2012. SU-
Time: A library for recognizing and normalizing
time expressions. In Proceedings of LREC 2012.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of SIGIR 2004, pages 425?432.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of
NAACL 2013.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proceedings of EMNLP-
CoNLL 2012, pages 710?720.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K.
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic
retrospection. In Proceedings of ICDM 2011.
Remy Kessler, Xavier Tannier, Caroline Hag`ege,
V?eronique Moriceau, and Andr?e Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
Proceedings of ACL 2012, pages 730?739.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423?430.
Jon Kleinberg. 2002. Bursty and hierarchical struc-
ture in streams. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?02, pages 91?
101.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of SIGIR ?01, pages 349?
357.
Dawn J. Lawrie. 2003. Language models for hierar-
chical summarization. Ph.D. thesis, University of
Massachusetts Amherst.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
ACL 2010, pages 640?649.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74?81.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of EMNLP 2012, pages 523?534.
Jahna Otterbacher, Dragomir Radev, and Omer Ka-
reem. 2006. News to go: Hierarchical text sum-
marization for mobile devices. In Proceedings of
SIGIR 2006, pages 589?596.
You Ouyang, Wenji Li, and Qin Lu. 2009. An
integrated multi-document summarization approach
based on word hierarchical representation. In Pro-
ceedings of the ACLShort 2009, pages 113?116.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
and Management, 40(6):919?938.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile rele-
vance and redundancy removal. In Proceedings of
DUC 2004.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of ACL 2009, pages
208?216.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the dots between news articles. In Proceedings of
KDD 2010, pages 623?632.
Dafna Shahaf, Carlos Guestrin, and Eric Horvitz.
2012. Trains of thought: Generating information
maps. In Proceedings of WWW 2012.
911
Russell Swan and James Allen. 2000. Automatic gen-
eration of overview timelines. In Proceedings of SI-
GIR 2000, pages 49?56.
Kou Takahashi, Takao Miura, and Isamu Shioya. 2007.
Hierarchical summarizing and evaluating for web
pages. In Proceedings of the 1st workshop on
emerging research opportunities for Web Data Man-
agement (EROW 2007).
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in
a dataset via the gap statistic. Journal of the Royal
Statistical Society, Series B, 32(2):411?423.
Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi.
2006. Multi-document summarization for terrorism
information extraction. In Proceedings of ISI?06.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of EMNLP 2011, pages
433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceeding
of SIGIR 2011, pages 745?754.
Christopher C. Yang and Fu Lee Wang. 2003. Fractal
summarization: summarization based on fractal the-
ory. In Proceedings of SIGIR 2003, pages 391?392.
Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and Hongyan Liu. 2011. Autopedia: Auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of WWW 2011, pages 161?162.
912
