Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 159?162,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation of Commonsense Knowledge with Mechanical Turk
Jonathan Gordon
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
jgordon@cs.rochester.edu
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Baltimore, MD, USA
vandurme@cs.jhu.edu
Lenhart K. Schubert
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
schubert@cs.rochester.edu
Abstract
Efforts to automatically acquire world knowl-
edge from text suffer from the lack of an easy
means of evaluating the resulting knowledge.
We describe initial experiments using Mechan-
ical Turk to crowdsource evaluation to non-
experts for little cost, resulting in a collection
of factoids with associated quality judgements.
We describe the method of acquiring usable
judgements from the public and the impact
of such large-scale evaluation on the task of
knowledge acquisition.
1 Introduction
The creation of intelligent artifacts that can achieve
human-level performance at problems like question-
answering ultimately depends on the availability of
considerable knowledge. Specifically, what is needed
is commonsense knowledge about the world in a form
suitable for reasoning. Open knowledge extraction
(Van Durme and Schubert, 2008) is the task of mining
text corpora to create useful, high-quality collections
of such knowledge.
Efforts to encode knowledge by hand, such as Cyc
(Lenat, 1995), require expensive man-hours of labor
by experts. Indeed, results from Project Halo (Fried-
land et al, 2004) suggest that properly encoding the
(domain-specific) knowledge from just one page of a
textbook can cost $10,000. OKE, on the other hand,
creates logical formulas automatically from existing
stores of human knowledge, such as books, newspa-
pers, and the Web. And while crowdsourced efforts to
gather knowledge, such as Open Mind (Singh, 2002),
learn factoids people come up with off the tops of
their heads to contribute, OKE learns from what peo-
ple normally write about and thus consider important.
Open knowledge extraction differs from open infor-
mation extraction (Banko et al, 2007) in the focus
on everyday, commonsense knowledge rather than
specific facts, and on the logical interpretability of
the outputs. While an OIE system might learn that
Tolstoy wrote using a dip pen, an OKE system would
prefer to learn that an author may write using a pen.
An example of an OKE effort is the KNEXT sys-
tem1 (Schubert, 2002), which uses compositional se-
mantic interpretation rules to produce logical formu-
las from the knowledge implicit in parsed text. These
formulas are then automatically expressed as English-
like ?factoids?, such as ?A PHILOSOPHER MAY HAVE
A CONVICTION? or ?NEGOTIATIONS CAN BE LIKELY
TO GO ON FOR SOME HOURS?.
While it is expected that eventually sufficiently
clean knowledge bases will be produced for infer-
ences to be made about everyday things and events,
currently the average quality of automatically ac-
quired knowledge is not good enough to be used in
traditional reasoning systems. An obstacle for knowl-
edge extraction is the lack of an easy method for
evaluating ? and thus improving ? the quality of re-
sults. Evaluation in acquisition systems is typically
done by human judging of random samples of output,
usually by the reporting authors themselves (e.g., Lin
and Pantel, 2002; Schubert and Tong, 2003; Banko et
al., 2007). This is time-consuming, and it has the po-
tential for bias: it would be preferable to have people
other than AI researchers label whether an output is
commonsense knowledge or not. We explore the use
of Amazon?s Mechanical Turk service, an online la-
bor market, as a means of acquiring many non-expert
judgements for little cost.
2 Related Work
While Open Mind Commons (Speer, 2007) asks users
to vote for or against commonsense statements con-
tributed by others users in order to come to a consen-
sus, we seek to evaluate an automatic system. Snow
et al (2008) compared the quality of labels produced
by non-expert Turkers against those made by experts
for a variety of NLP tasks and found that they re-
quired only four responses per item to emulate expert
annotations. Kittur et al (2008) describe the use and
1Public release of the basic KNEXT engine is forthcoming.
159
The statement above is a reasonably clear, entirely
plausible, generic claim and seems neither too spe-
cific nor too general or vague to be useful:
? I agree.
? I lean towards agreement.
? I?m not sure.
? I lean towards disagreement.
? I disagree.
Figure 1: Rating instructions and answers.
necessity of verifiable questions in acquiring accurate
ratings of Wikipedia articles from Mechanical Turk
users. These results contribute to our methods below.
3 Experiments
Previous evaluations of KNEXT output have tried to
judge the relative quality of knowledge learned from
different sources and by different techniques. Here
the goal is simply to see whether the means of evalu-
ation can be made to work reasonably, including at
what scale it can be done for limited cost. For these
experiments, we relied on $100 in credit provided by
Amazon as part of the workshop shared task. This
amount was used for several small experiments in or-
der to empirically estimate what $100 could achieve,
given a tuned method of presentation and evaluation.
We took a random selection of factoids generated
from the British National Corpus (BNC Consortium,
2001), split into sets of 20, and removed those most
easily filtered out as probably being of low quality
or malformed. We skipped the more stringent filters
(originally created for dealing with noisy Web text),
leaving more variety in the quality of the factoids
Turkers were asked to rate.
The first evaluation followed the format of previ-
ous, offline ratings. For each factoid, Turkers were
given the instructions and choices in Fig. 1, where
the options correspond in our analysis to the num-
bers 1?5, with 1 being agreement. To help Turkers
make such judgements, they were given a brief back-
ground statement: ?We?re gathering the sort of every-
day, commonsense knowledge an intelligent computer
system should know. You?re asked to rate several pos-
sible statements based on how well you think they
meet this goal.? Mason and Watts (2009) suggest that
while money may increase the number and speed of
responses, other motivations such as wanting to help
with something worthwhile or interesting are more
likely to lead to high-quality responses.
Participants were then shown the examples and
explanations in Fig. 2. Note that while they are told
some categories that bad factoids can fall into, the
Turkers are not asked to make such classifications
Examples of good statements:
? A SONG CAN BE POPULAR
? A PERSON MAY HAVE A HEAD
? MANEUVERS MAY BE HOLD -ED IN SECRET
It?s fine if verb conjugations are not attached or are a bit
unnatural, e.g. ?hold -ed? instead of ?held?.
Examples of bad statements:
? A THING MAY SEEK A WAY
This is too vague. What sort of thing? A way for/to
what?
? A COCKTAIL PARTY CAN BE AT
SCOTCH_PLAINS_COUNTRY_CLUB
This is too specific. We want to know that a cocktail
party can be at a country club, not at this particular one.
The underscores are not a problem.
? A PIG MAY FLY
This is not literally true even though it happens to be an
expression.
? A WORD MAY MEAN
This is missing information. What might a word mean?
Figure 2: The provided examples of good and bad factoids.
themselves, as this is a task where even experts have
low agreement (Van Durme and Schubert, 2008).
An initial experiment (Round 1) only required
Turkers to have a high (90%) approval rate. Under
these conditions, out of 100 HITs2, 60 were com-
pleted by participants whose IP addresses indicated
they were in India, 38 from the United States, and
2 from Australia. The average Pearson correlation
between the ratings of different Indian Turkers an-
swering the same questions was a very weak 0.065,
and between the Indian responders and those from
the US and Australia was 0.132. On the other hand,
the average correlation among non-Indian Turkers
was 0.508, which is close to the 0.6?0.8 range seen
between the authors in the past, and which can be
taken as an upper bound on agreement for the task.
Given the sometimes subtle judgements of mean-
ing required, being a native English speaker has pre-
viously been assumed to be a prerequisite. This differ-
ence in raters? agreements may thus be due to levels
of language understanding, or perhaps to different
levels of attentiveness to the task. However, it does
not seem to be the case that the Indian respondents
rushed: They took a median time of 201.5 seconds
(249.18 avg. with a high standard deviation of 256.3s
? some took more than a minute per factoid). The non-
Indian responders took a median time of just 115.5 s
(124.5 avg., 49.2 std dev.).
Regardless of the cause, given these results, we re-
stricted the availability of all following experiments
to Turkers in the US.Ideally we would include other
English-speaking countries, but there is no straight-
2Human Intelligence Tasks ? Mechanical Turk assignments.
In this case, each HIT was a set of twenty factoids to be rated.
160
All High Corr. (> 0.3)
Round Avg. Std. Dev. Avg. Std. Dev.
1 (BNC) 2.59 1.55 2.71 1.64
3 (BNC) 2.80 1.66 2.83 1.68
4 (BNC) 2.61 1.64 2.62 1.64
5 (BNC) 2.76 1.61 2.89 1.68
6 (Weblogs) 2.83 1.67 2.85 1.67
7 (Wikipedia) 2.75 1.64 2.75 1.64
Table 1: Average ratings for all responses and for highly
correlated responses. to other responses. Lower numbers
are more positive. Round 2 was withdrawn without being
completed.
forward way to set multiple allowable countries on
Mechanical Turk.When Round 2 was posted with
a larger set of factoids to be rated and the location
requirement, responses fell off sharply, leading us
to abort and repost with a higher payrate (7? for 20
factoids vs 5? originally) in Round 3.
To avoid inaccurate ratings, we rejected submis-
sions that were improbably quick or were strongly
uncorrelated with other Turkers? responses. We col-
lected five Turkers? ratings for each set of factoids,
and for each persons? response to a HIT computed
the average of their three highest correlations with
others? responses. We then rejected if the correla-
tions were so low as to indicate random responses.
The scores serve a second purpose of identifying a
more trustworthy subset of the responses. (A cut-off
score of 0.3 was chosen based on hand-examination.)
In Table 1, we can see that these more strongly corre-
lated responses rate factoids as slightly worse overall,
possibly because those who either casual or uncertain
are more likely to judge favorably on the assumption
that this is what the task authors would prefer, or they
are simply more likely to select the top-most option,
which was ?I agree?.
An example of a factoid that was labeled incor-
rectly by one of the filtered out users is ?A PER-
SON MAY LOOK AT SOME THING-REFERRED-TO OF
PRESS RELEASES?, for which a Turker from Madras
in Round 1 selected ?I agree?. Factoids containing
the vague ?THING-REFERRED-TO? are often filtered
out of our results automatically, but leaving them in
gave us some obviously bad inputs for checking Turk-
ers? responses. Another (US) Turker chose ?I agree?
when told ?TES MAY HAVE 1991ES? but ?I disagree?
when shown ?A TRIP CAN BE TO A SUPERMARKET?.
We are interested not only in whether there is a gen-
eral consensus to be found among the Turkers but also
how that consensus correlates with the judgements
of AI researchers. To this end, one of the authors
rated five sets (100 factoids) presented in Round 3,
0
500
1000
1500
2000
2500
0 1 2 3 4 5
Fr
eq
ue
nc
y
Rating
Figure 3: Frequency of ratings in the high-corr. results of
Round 3.
which yielded an average correlation between all the
Turkers and the author of 0.507, which rises slightly
to 0.532 if we only count those Turkers considered
?highly correlated? as described above.
As another test of agreement, for ten of the sets in
Round 3, two factoids were designated as fixpoints ?
the single best and worst factoid in the set, assigned
ratings 1 and 5 respectively. From the Turkers who
rated these factoids, 65 of the 100 ratings matched
the researchers? designations and 77 were within one
point of the chosen rating.3
A few of the Turkers who participated had fairly
strong negative correlations to the other Turkers, sug-
gesting that they may have misunderstood the task
and were rating backwards.4 Furthermore, one Turker
commented that she was unsure whether the state-
ment she was being asked to agree with (Fig. 1) ?was
a positive or negative?. To see how it would affect the
results, we ran (as Round 4) twenty sets of factoids,
asking simplified question ?Do you agree this is a
good statement of general knowledge?? The choices
were also reversed in order, running from ?I disagree?
to ?I agree? and color-coded, with agree being green
and disagree red. This corresponded to the coloring
of the good and bad examples at the top of the page,
which the Turkers were told to reread when they were
halfway through the HIT. The average correlation for
responses in Round 4 was 0.47, which is an improve-
ment over the 0.34 avg. correlation of Round 3.
Using the same format as Round 4, we ran factoids
from two other corpora. Round 6 consisted of 300 ran-
dom factoids taken from running KNEXT on weblog
data (Gordon et al, 2009) and Round 7 300 random
factoids taken from running KNEXT on Wikipedia.
3If we only look at the highly correlated responses, this in-
creases slightly to 68% exact match, 82% within one point.
4This was true for one Turker who completed many HITs, a
problem that might be prevented by accepting/rejecting HITs as
soon as all scores for that set of factoids were available rather
than waiting for the entire experiment to finish.
161
The average ratings for factoids from these sources
are lower than for the BNC, reflecting the noisy na-
ture of much writing on weblogs and the many overly
specific or esoteric factoids learned from Wikipedia.
The results achieved can be quite sensitive to the
display of the task. For instance, the frequency of
ratings in Fig. 3 shows that Turkers tended toward
the extremes: ?I agree? and ?I disagree? but rarely
?I?m not sure?. This option might have a negative
connotation (?Waffling is undesirable?) that another
phrasing would not. As an alternative presentation of
the task (Round 5), for 300 factoids, we asked Turk-
ers to first decide whether a factoid was ?incoher-
ent (not understandable)? and, otherwise, whether it
was ?bad?, ?not very good?, ?so-so?, ?not so bad?, or
?good? commonsense knowledge. Turkers indicated
factoids were incoherent 14% of the time, with a cor-
responding reduction in the number rated as ?bad?,
but no real increase in middle ratings. The average
ratings for the ?coherent? factoids are in Table 1.
4 Uses of Results
Beyond exploring the potential of Mechanical Turk
as a mechanism for evaluating the output of KNEXT
and other open knowledge extraction systems, these
experiments have two useful outcomes:
First, they give us a large collection of almost 3000
factoids that have associated average ratings and al-
low for the release of the subset of those factoids
that are believed to probably be good (rated 1?2).
This data set is being publicly released at http://
www.cs.rochester.edu/research/knext, and
it includes a wide range of factoids, such as ?A REP-
RESENTATION MAY SHOW REALITY? and ?DEMON-
STRATIONS MAY MARK AN ANNIVERSARY OF AN
UPRISING?.
Second, the factoids rated from Round 2 onward
were associated with the KNEXT extraction rules used
to generate them: The factoids generated by different
rules have average ratings from 1.6 to 4.8. We hope in
future to use this data to improve KNEXT?s extraction
methods, improving or eliminating rules that often
produce factoids judged to be bad. Inexpensive, fast
evaluation of output on Mechanical Turk could be a
way to measure incremental improvements in output
quality coming from the same source.
5 Conclusions
These initial experiments have shown that untrained
Turkers evaluating the natural-language verbaliza-
tions of an open knowledge extraction system will
generally give ratings that correlate strongly with
those of AI researchers. Some simple methods were
described to find those responses that are likely to
be accurate. This work shows promise for cheap and
quick means of measuring the quality of automati-
cally constructed knowledge bases and thus improv-
ing the tools that create them.
Acknowledgements
This work was supported by NSF grants IIS-0535105
and IIS-0916599.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the Web. In Proc. of IJCAI-07.
BNC Consortium. 2001. The British National Corpus,
v.2. Dist. by Oxford University Computing Services.
Noah S. Friedland et al. 2004. Project Halo: Towards a
digital Aristotle. AI Magazine, 25(4).
Jonathan Gordon, Benjamin Van Durme, and Lenhart K.
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In Proc. of K-CAP-09.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI ?08.
Douglas B. Lenat. 1995. Cyc: A Large-scale Investment
in Knowledge Infrastructure. Communications of the
ACM, 38(11):33?48.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of COLING-02.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the ?performance of crowds?. In Proc.
of HCOMP ?09.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extract-
ing and evaluating general world knowledge from the
Brown corpus. In Proc. of the HLT-NAACL Workshop
on Text Meaning.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. of HLT-02.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast ? but is it good? In
Proc. of EMNLP-08.
Robert Speer. 2007. Open mind commons: An inquisitive
approach to learning common sense. In Workshop on
Common Sense and Intelligent User Interfaces.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction through compositional lan-
guage processing. In Proc. of STEP 2008.
162
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 59?63,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Discovering Commonsense Entailment Rules Implicit in Sentences
Jonathan Gordon
Department of Computer Science
University of Rochester
Rochester, NY, USA
jgordon@cs.rochester.edu
Lenhart K. Schubert
Department of Computer Science
University of Rochester
Rochester, NY, USA
schubert@cs.rochester.edu
Abstract
Reasoning about ordinary human situations
and activities requires the availability of di-
verse types of knowledge, including expecta-
tions about the probable results of actions and
the lexical entailments for many predicates.
We describe initial work to acquire such a col-
lection of conditional (if?then) knowledge by
exploiting presuppositional discourse patterns
(such as ones involving ?but?, ?yet?, and ?hop-
ing to?) and abstracting the matched material
into general rules.
1 Introduction
We are interested, ultimately, in enabling an infer-
ence system to reason forward from facts as well
as backward from goals, using lexical knowledge to-
gether with world knowledge. Creating appropriate
collections of general world knowledge to support
reasoning has long been a goal of researchers in Arti-
ficial Intelligence. Efforts in information extraction,
e.g., Banko et al (2007), have focused on learning
base facts about specific entities (such as that Barack
Obama is president), and work in knowledge extrac-
tion, e.g., Van Durme and Schubert (2008), has found
generalizations (such as that a president may make
a speech). While the latter provides a basis for pos-
sibilistic forward inference (Barack Obama proba-
bly makes a speech at least occasionally) when its
meaning is sharpened (Gordon and Schubert, 2010),
these resources don?t provide a basis for saying what
we might expect to happen if, for instance, someone
crashes their car.
That the driver in a car crash might be injured
and the car damaged is a matter of common sense,
and, as such, is rarely stated directly. However, it
can be found in sentences where this expectation
is disconfirmed: ?Sally crashed her car into a tree,
but she wasn?t hurt.? We have been exploring the
use of lexico-syntactic discourse patterns indicating
disconfirmed expectations, as well as people?s goals
(?Joe apologized repeatedly, hoping to be forgiven?).
The resulting rules, expressed at this point in natural
language, are a first step toward obtaining classes of
general conditional knowledge typically not obtained
by other methods.
2 Related Work
One well-known approach to conditional knowledge
acquisition is that of Lin and Pantel (2001), where
inference rules are learned using distributional simi-
larity between dependency tree paths. These results
include entailment rules like ?x is the author of y ? x
wrote y? (which is true provided x is a literary work)
and less dependable ones like ?x caused y ? y is
blamed on x?. This work was refined by Pantel et al
(2007) by assigning the x and y terms semantic types
(inferential selectional preferences ? ISP) based on
lexical abstraction from empirically observed argu-
ment types. A limitation of the approach is that the
conditional rules obtained are largely limited to ones
expressing some rough synonymy or similarity re-
lation. Pekar (2006) developed related methods for
learning the implications of an event based on the
regular co-occurrence of two verbs within ?locally
coherent text?, acquiring rules like ?x was appointed
as y? suggests that ?x became y?, but, as in DIRT, we
lack information about the types of x and y, and only
acquire binary relations.
Girju (2003) applied Hearst?s (1998) procedure for
finding lexico-syntactic patterns to discover causal
relations between nouns, as in ?Earthquakes gener-
ate tsunami?. Chklovski and Pantel (2004) used pat-
59
(S < (NP $. (VP < (/,/ $. (S < (VP < (VBG <hoping)< (S < (VP < TO))))))))
(S < (NP $. (VP < ((CC < but) $.. (VP < (AUX < did) < (RB < /n[?o]t/))))))
(S < (NP $. (VP < (AUX $. (ADJP < (JJ $. ((CC < /(but|yet)/) $. JJ)))))))
(S < (NP $. (VP < (/,/ $. (S < (VP < ((VBG < expecting) $.
(S < (VP < TO)))))))))
Figure 1: Examples of TGrep2 patterns for finding parse
tree fragments that might be abstracted to inference rules.
See Rohde (2001) for an explanation of the syntax.
terns like ?x-ed by y-ing? (?obtained by borrowing?)
to get co-occurrence data on candidate pairs from the
Web. They used these co-occurrence counts to obtain
a measure of mutual information between pairs of
verbs, and hence to assess the strengths of the rela-
tions. A shortcoming of rules obtained in this way is
their lack of detailed predicative structure. For infer-
ence purposes, it would be insufficient to know that
?crashes cause injuries? without having any idea of
what is crashing and who or what is being injured.
Schoenmackers et al (2010) derived first-order
Horn clauses from the tuple relations found by TEXT-
RUNNER (Banko et al, 2007). Their system produces
rules like ?IsHeadquarteredIn(Company, State) :- Is-
BasedIn(Company, City) ? IsLocatedIn(City, State)?,
which are intended to improve inference for question-
answering. A limitation of this approach is that, op-
erating on the facts discovered by an information
extraction system, it largely obtains relations among
simple attributes like locations or roles rather than
consequences or reasons.
3 Method
Our method first uses TGrep2 (Rohde, 2001) to find
parse trees matching hand-authored lexico-syntactic
patterns, centered around certain pragmatically sig-
nificant cue words such as ?hoping to? or ?but didn?t?.
Some of the search patterns are in Figure 1. While
we currently use eight query patterns, future work
may add rules to cover more constructions.
The matched parse trees are filtered to remove
those unlikely to produce reasonable results, such
as those containing parentheses or quoted utterances,
and the trees are preprocessed in a top-down traversal
to rewrite or remove constituents that are usually
extraneous. For instance, the parse tree for
The next day he and another Bengali boy who
lives near by [sic] chose another way home,
hoping to escape the attackers.
is preprocessed to
People chose another way home, hoping to
escape the attackers.
Examples of the preprocessing rules include re-
moving interjections (INTJ) and some prepositional
phrases, heuristically turning long expressions into
keywords like ?a proposition?, abstracting named en-
tities, and reordering some sentences to be easier to
process. E.g., ?Fourteen inches from the floor it?s sup-
posed to be? is turned to ?It?s supposed to be fourteen
inches from the floor?.
The trees are then rewritten as conditional expres-
sions based on which semantic pattern they match,
as outlined in the following subsections. The sample
sentences are from the Brown Corpus (Kuc?era and
Francis, 1967) and the British National Corpus (BNC
Consortium, 2001), and the rules are those derived
by our current system.
3.1 Disconfirmed Expectations
These are sentences where ?but? or ?yet? is used to
indicate that the expected inference people would
make does not hold. In such cases, we want to flip the
polarity of the conclusion (adding or removing ?not?
from the output) so that the expectation is confirmed.
For instance, from
The ship weighed anchor and ran out her big
guns, but did not fire a shot.
we get that the normal case is the opposite:
If a ship weighs anchor and runs out her big
guns, then it may fire a shot.
Or for two adjectives, ?She was poor but proud?:
If a female is poor, then she may not be proud.
3.2 Contrasting Good and Bad
A different use of ?but? and ?yet? is to contrast some-
thing considered good with something considered
bad, as in ?He is very clever but eccentric?:
If a male is very clever,
then he may be eccentric.
If we were to treat this as a case of disconfirmed ex-
pectation as above, we would have claimed that ?If a
male is very clever, then he may not be eccentric?. To
identify this special use of ?but?, we consult a lexicon
of sentiment annotations, SentiWordNet (Baccianella
et al, 2010). Finding that ?clever? is positive while
?eccentric? is negative, we retain the surface polarity
in this case.
60
For sentences with full sentential complements for
?but?, recognizing good and bad items is quite difficult,
more often depending on pragmatic information. For
instance, in
Central government knew this would happen
but did not want to admit to it in its plans.
knowing something is generally good while being
unwilling to admit something is bad. At present, we
don?t deal with these cases.
3.3 Expected Outcomes
Other sentences give us a participant?s intent, and we
just want to abstract sufficiently to form a general
rule:
He stood before her in the doorway, evidently
expecting to be invited in.
If a male stands before a female in the
doorway, then he may expect to be invited in.
When we abstract from named entities (using a va-
riety of hand-built gazetteers), we aim low in the
hierarchy:
Elisabeth smiled, hoping to lighten the
conversational tone and distract the Colonel
from his purpose.
If a female smiles, then she may hope to
lighten the conversational tone.
While most general rules about ?a male? or ?a female?
could instead be about ?a person?, there are ones that
can?t, such as those about giving birth. We leave the
raising of terms for later work, following Van Durme
et al (2009).
4 Evaluation
Development was based on examples from the (hand-
parsed) Brown Corpus and the (machine-parsed)
British National Corpus, as alluded to above. These
corpora were chosen for their broad coverage of ev-
eryday situations and edited writing.
As the examples in the preceding subsections in-
dicate, rules extracted by our method often describe
complex consequences or reasons, and subtle rela-
tions among adjectival attributes, that appear to be
quite different from the kinds of rules targeted in pre-
vious work (as discussed earlier, or at venues such
as that of (Sekine, 2008)). While we would like to
evaluate the discovered rules by looking at inferences
made with them, that must wait until logical forms
are automatically created; here we judge the rules
themselves.
The statement above is a reasonably clear, entirely
plausible, generic claim and seems neither too specific
nor too general or vague to be useful:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for judging of unsharpened factoids.
Judge 1 Judge 2 Correlation
1.84 2.45 0.55
Table 1: Average ratings and Pearson correlation for rules
from the personal stories corpus. Lower ratings are better;
see Fig. 2.
For evaluation, we used a corpus of personal stories
from weblogs (Gordon and Swanson, 2009), parsed
with a statistical parser (Charniak, 2000). We sampled
100 output rules and rated them on a scale of 1?5
(1 being best) based on the criteria in Fig. 2. To
decide if a rule meets the criteria, it is helpful to
imagine a dialogue with a computer agent. Told an
instantiated form of the antecedent, the agent asks for
confirmation of a potential conclusion. E.g., for
If attacks are brief,
then they may not be intense,
the dialogue would go:
?The attacks (on Baghdad) were brief.?
?So I suppose they weren?t intense, were they??
If this is a reasonable follow-up, then the rule is prob-
ably good, although we also disprefer very unlikely
antecedents ? rules that are vacuously true.
As the results in Table 1 and Fig. 3 indicate, the
overall quality of the rules learned is good but there
is room for improvement. We also see a rather low
correlation between the ratings of the two judges,
indicating the difficulty of evaluating the quality of
the rules, especially since their expression in natural
language (NL) makes it tempting to ?fill in the blanks?
of what we understand them to mean. We hypothesize
that the agreement between judges will be higher
for rules in logical form, where malformed output
is more readily identified ? for instance, there is no
guessing about coreference or attachment.
Rules that both judges rated favorably (1) include:
If a pain is great, it may not be manageable.
If a person texts a male, then he-or-she may
get a reply.
61
020
40
60
80
1 2 3 4 5
Fr
eq
ue
nc
y
Rating
Figure 3: Counts for how many rules were assigned each
rating by judges. Lower ratings are better; see Fig. 2.
If a male looks around, then he may hope to
see someone.
If a person doesn?t like some particular store,
then he-or-she may not keep going to it.
While some bad rules come from parsing or pro-
cessing mistakes, these are less of a problem than
the heavy tail of difficult constructions. For instance,
there are idioms that we want to filter out (e.g., ?I?m
embarrassed but. . . ?) and other bad outputs show
context-dependent rather than general relations:
If a girl sits down in a common room, then she
may hope to avoid some pointless
conversations.
The sitting-down may not have been because she
wanted to avoid conversation but because of some-
thing prior.
It?s difficult to compare our results to other systems
because of the differences of representation, types of
rules, and evaluation methods. ISP?s best performing
method (ISP.JIM) achieves 0.88 specificity (defined as
a filter?s probability of rejecting incorrect inferences)
and 0.53 accuracy. While describing their SHERLOCK
system, Schoenmackers et al (2010) argue that ?the
notion of ?rule quality? is vague except in the context
of an application? and thus they evaluate the Horn
clauses they learn in the context of the HOLMES
inference-based QA system, finding that at precision
0.8 their rules allow the system to find twice as many
correct facts. Indeed, our weak rater agreement shows
the difficulty of judging rules on their own, and future
work aims to evaluate rules extrinsically.
5 Conclusion and Future Work
Enabling an inference system to reason about com-
mon situations and activities requires more types of
general world knowledge and lexical knowledge than
are currently available or have been targeted by previ-
ous work. We?ve suggested an initial approach to
acquiring rules describing complex consequences
or reasons and subtle relations among adjectival at-
tributes: We find possible rules by looking at interest-
ing discourse patterns and rewriting them as condi-
tional expressions based on semantic patterns.
A natural question is why we don?t use the
machine-learning/bootstrapping techniques that are
common in other work on acquiring rules. These tech-
niques are particularly successful when (a) they are
aimed at finding fixed types of relationships, such
as hyponymy, near-synonymy, part-of, or causal rela-
tions between pairs of lexical items (often nominals
or verbs); and (b) the fixed type of relationship be-
tween the lexical items is hinted at sufficiently often
either by their co-occurrence in certain local lexico-
syntactic patterns, or by their occurrences in simi-
lar sentential environments (distributional similarity).
But in our case, (a) we are looking for a broad range
of (more or less strong) consequence relationships,
and (b) the relationships are between entire clauses,
not lexical items. We are simply not likely to find
multiple occurrences of the same pair of clauses in
a variety of syntactic configurations, all indicating a
consequence relation ? you?re unlikely to find multi-
ple redundant patterns relating clauses, as in ?Went
up to the door but didn?t knock on it?.
There is more work to be done to arrive at a reli-
able, inference-ready knowledge base of such rules.
The primary desideratum is to produce a logical rep-
resentation for the rules such that they can be used in
the EPILOG reasoner (Schubert and Hwang, 2000).
Computing logical forms (as, e.g., in Bos (2008)) and
then deriving logically formulated rules from these
rather than deriving sentential forms directly from
text should also allow us to be more precise about
dropping modifiers, reshaping into generic present
tense from other tenses, and other issues that affect
the quality of the statements. We have a preliminary
version of a logical form generator that derives LFs
from TreeBank parses that can support this direc-
tion. Further filtering techniques (based both on the
surface form and the logical form) should keep the
desired inference rules while improving quality.
Acknowledgements
This work was supported by NSF grants IIS-
1016735 and IIS-0916599, and ONR STTR subcon-
tract N00014-10-M-0297.
62
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proc. of the Seventh Conference on International
Language Resources and Evaluation (LREC?10).
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the Web. In Proc. of the
Twentieth International Joint Conference on Artificial
Intelligence (IJCAI-07).
BNC Consortium. 2001. The British National Corpus, v.2.
Distributed by Oxford University Computing Services.
Johan Bos. 2008. Wide-coverage semantic analysis with
Boxer. In Proc. of the Symposium on Semantics in Text
Processing (STEP 2008).
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the First Annual Meeting of the North
American Chapter of the Association for Computational
Linguistics (NAACL 2000), pages 132?139.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb relations.
In Proc. of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-04).
Roxana Girju. 2003. Automatic detection of causal rela-
tions for question answering. In Proc. of the ACL 2003
Workshop on Multilingual Summarization and Question
Answering ? Machine Learning and Beyond.
Jonathan Gordon and Lenhart K. Schubert. 2010. Quan-
tificational sharpening of commonsense knowledge. In
Proc. of the AAAI 2010 Fall Symposium on Common-
sense Knowledge.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Proc.
of the Third International Conference on Weblogs and
Social Media (ICWSM), Data Challenge Workshop.
Marti Hearst. 1998. Automated discovery of WordNet
relations. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database and Some of Its Applica-
tions. MIT Press.
Henry Kuc?era and W. N. Francis. 1967. Computational
Analysis of Present-Day American English. Brown
University Press.
Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of inference rules from text. In Proc. of the ACM
Conference on Knowledge Discovery and Data Mining
(KDD).
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and Ed-
uard Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proc. of NAACL-HLT 2007.
Viktor Pekar. 2006. Acquisition of verb entailment from
text. In Proc. of HLT-NAACL 2006.
Doug Rohde. 2001. TGrep2 manual. Unpublished
manuscript, Brain & Cognitive Science Department,
MIT.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and
Jesse Davis. 2010. Learning first-order horn clauses
from Web text. In Proc. of EMNLP 2010.
Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic Logic Meets Little Red Riding Hood: A com-
prehensive, natural representation for language under-
standing. In L. Iwanska and S. C. Shapiro, editors,
Natural Language Processing and Knowledge Repre-
sentation: Language for Knowledge and Knowledge for
Language. MIT/AAAI Press.
Satoshi Sekine, editor. 2008. Notebook of the NSF Sympo-
sium on Semantic Knowledge Discovery, Organization,
and Use. New York University, 14?15 November.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction through compositional lan-
guage processing. In Proc. of the Symposium on Se-
mantics in Text Processing (STEP 2008).
Benjamin Van Durme, Phillip Michalak, and Lenhart K.
Schubert. 2009. Deriving Generalized Knowledge
from Corpora using WordNet Abstraction. In Proc. of
EACL 2009.
63
