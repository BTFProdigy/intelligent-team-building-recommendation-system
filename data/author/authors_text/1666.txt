Integrating compositional semantics into a verb lexicon 
Hoa Trang Dang, Karin Kipper and Martha Palmer 
Department of Computer and Information Sciences 
University of Pennsylvania 
200 South 33rd Street 
Philadelphia, PA 19104, USA 
{htd,kipper, mpahner} @linc.cis.upenn.edu 
Abstract 
We present a class-based approach to building a 
verb lexicon that makes explicit the close asso- 
ciation between syntax and semantics for Levin 
classes. We have used Lexicalized Tree Adjoin- 
ing Grammars to capture the syntax associated with 
each verb class and have augmented the trees to in- 
clude selectional restrictions. In addition, semantic 
predicates are associated with each tree, which al- 
low for a colnpositional interpretation. 
1 Introduction 
The difficulty o1' achieving adequate hand-crafted 
semantic representations has limited the lield of 
natural language t)rocessing to applications that 
can be contained within well-deIined sub-domains. 
Despite many different lexicon development ap- 
proaches (Mel'cuk, 1988; Copestakc and Sanfil- 
ippo, 1993; Lowe et al, 1997), the field has yet 
to develop a clear conseusus on guidelines for a 
colnputational lexicon. One of the most controver- 
sial areas in building such a lexicon is polyselny: 
how senses can be computationally distinguished 
and characterized. We address this problem hy em- 
ploying compositional semantics and the adjunction 
of syntactic l)hrases to support regular verb sense 
extensions. This differs l'rom the Lexical Concep- 
tual Structure (LCS) approach exemplified by Voss 
(1996), which requires a separate LCS representa- 
tion for each possible sense extension. In this pa- 
per we describe the construction of VerbNet, a verb 
lexicon with explicitly stated syntactic and seman- 
tic information for individual exical items, using 
Levin verb classes (Levin, 1993) to systematically 
construct lexical entries. We use Lexicalized Tree 
Adjoining Grammar (LTAG) (Joshi, 1987; Schabes, 
1990) to capture the syntax for each verb class, and 
associate semantic predicates with each tree. 
Althougla similar ideas have been explored for 
w:rb sense extension (Pusteiovsky, 1995; Goldberg, 
1995), our approach of applying LTAG to the prob- 
lem of composing and extending verb senses is 
novel. LTAGs have an extended omain of local- 
ity that captures the arguments of a verb in a local 
manner. The association of semantic predicates to a 
tree yields a complete semantics for the verb. More- 
ovel, the operation of adjunction in LTAGs provides 
a mechanism for extending verb senses. 
2 Levin classes 
Levin verb classes are based on the ability of a verb 
to occur in diathesis alternations, which are pairs 
of syntactic frames that are in some sense meaning 
preserving. The fundalnental ssulnption is that the 
syntactic frames arc a direct reflection of the under- 
lying semantics, ltowever, Levin classes exhibit in- 
consistencies that have hampered researchers' abil- 
ity to reference them directly in applications. Many 
verbs ale listed in multiple classes, some of which 
have confl icting sets of syntactic frames. Dang et al 
(1998) showed that multiple listings could in some 
cases be interpreted as regular sense xtensions, and 
defined intersective Levin classes, which are a more 
line-grained, syntactically and semantically coher- 
ent refinement of basic Levin classes. We represent 
these verb classes and their regular sense xtensions 
in the LTAG forlnalism. 
3 Lexicalized rI?ee Adjoining Grammars 
3.1 Overview of formalism 
Lexicatized Tree Adjoining Granunars consist of a 
finite set of initial and auxiliary elementary hees, 
and two operations to combine them. The min- 
imal, non-recursive linguistic structures of a lan- 
guage, such as a verb and its complements, are cap- 
tured by initial trees. Recursive structures of a lan- 
guage, such as prepositional modifiers which result 
in syntactically embedded VPs, are represented by 
auxiliary trees. 
1011 
Elementary trees are combined by the operations 
of substitution and adjunction. Substitution is a sim- 
ple operation that replaces a leaf of a tree with a new 
tree. Adjunction is a splicing operation that replaces 
an internal node of an elementary tree with an aux- 
iliary tree. Eveu tree is associated with a lexical 
item of the language, called the anchor of the tree. 
Tile tree represents the domain over which the lex- 
ical item can directly specify syntactic onstraints, 
such as subject-verb number agreement, or seman- 
tic constraints, uch as selectional restrictions, all of 
which are implemented as features. 
LTAGs are more powerful than context free gram- 
mars (CFG), allowing localization of so-called un- 
bounded dependencies that cannot be handled by 
CFGs. There are critical benefits to lexical seman- 
tics that are provided by the extended omain of 
locality of the lexicalized trees. Each lexical en- 
try corresponds to a tree. If the lexical item is a 
verb, the conesponding tree is a skeleton for an en- 
tire sentence with the verb already present, anchor- 
ing the tree as a terminal symbol. The other parts 
of the sentence will be substituted or adjoined in at 
appropriate places in the skeleton tree in the course 
of the derivation. The composition of trees during 
parsing is recorded in a derivation tree. The deriva- 
tion tree nodes correspond to lexically anchored el- 
ementary trees, and the arcs are labeled with infor- 
mation about how these trees were combined to pro- 
duce the parse. Since each lexically anchored initial 
tree corresponds to a semantic unit, the derivation 
tree closely resembles a semantic-dependency rep- 
resentation. 
3.2 Semantics for TAGs 
There is a range of previous work in incorporating 
semantics into TAG trees. Stone and Doran (1997) 
describe a system used for generation that simul- 
taneously constructs the semantics and syntax of 
a sentence using LTAGs. Joshi and Vijay-Shanker 
(1999), and Kallmeyer and Joshi (1999), describe 
the semantics of a derivation tree as a set of attach- 
ments of trees. The semantics of these attachments 
is given as a conjunction of formulae in a flat seman- 
tic representation. They provide a specific method- 
ology for composing semantic representations much 
like Candito and Kahane (1998), where the direc- 
tionality of dominance in the derivation tree should 
be interpreted according to the operations used to 
build it. Kallmeyer and Joshi also use a flat semantic 
representation to handle scope phenomena involv- 
ing quantifiers. 
4 Descr ip t ion  of  the verb lexicon 
VerbNet can be viewed in both a static and a dy- 
namic way. Tile static aspect refers to the verb and 
class entries and how they are organized, providing 
the characteristic descriptions of a verb sense or a 
verb class (Kipper et al, 2000). The dynamic as- 
pect of the lexicon constrains the entries to allow 
a compositional interpretation i LTAG derivation 
trees, representing extended verb meanings by in- 
corporating adjuncts. 
Verb classes allow us to capture generalizations 
about verb behavioL Each verb class lists the tlle- 
mafic roles that the predicate-argument structure of 
its members allows, and provides descriptions of 
the syntactic fi'ames conesponding to licensed con- 
structions, with selectional restrictions defined for 
each argument in each frame, l Each frame also 
includes semantic predicates describing the partic- 
ipants at various stages of the event described by 
the frame. 
Verb classes are hierarchically organized, ensur- 
ing that each class is coherent - that is, all its mem- 
bers have common semantic elements and share a 
common set of thematic roles and basic syntactic 
frames. This requires ome manual restructuring of 
the original Levin classes, which is facilitated by us- 
ing intersective Levin classes. 
5 Compositional Semantics 
We use TAG elementary trees for the description 
of allowable frames and associate semantic predi- 
cates with each tree, as was done by Stone and Do- 
ran. The semantic predicates are primitive enough 
so that many may be reused in different rees. By 
using TAGs we get the additional benefit of an ex- 
isting parser that yields derivations and derived trees 
fiom which we can construct the compositional se- 
mantics of a given sentence. 
We decompose each event E into a tripar- 
tite structure in a manner similar to Moens and 
Steedman (1988), introducing a time function for 
each predicate to specify whether the predicate is 
true in the preparatory (d~ring(E)), cuhnination 
(er~d(E)), or consequent (res~ll:(E)) stage of an 
event. 
hfitial trees capture tile semantics of the basic 
senses of verbs in each class. For example, many 
IThese restrictions are more like preferences that generate a 
preferred reading of a sentence. They may be relaxed epend- 
ing on the domain of a particular pplication. 
1012 
S \[ cvcnt=E \] S \[ event=E2 \]
NP.,.sH$ VP \[ cvcnt=E \] NParqo$ VP \[ event=E1 \] 
\[ +aninmtc \] \] \[ +animale \]
V V NI),~,.ql$ 
I \] \[ +animate \]
1"1.11| rU l l  
motion(during(E), Xa,.al ) motion(during(El), Xargl ) 
Figure 1 : Induced action alternation for the Run verbs 
verbs in the Run class can occur in the induced ac- 
tion alternation, in which the subject of the inmmsi- 
tive sentence has the same thematic role as the direct 
object in the transitive sentence. Figure l shows the 
initial trees for the transitive and intransitive vari- 
ants for the Run class, along with their semantic 
predicates. The entity in motion is given by argl, 
associated with the syntactic subject of the intransi- 
tive tree and the direct object of the transitive tree. 
The event denoted by the transitive variant is a com- 
position of two subevents: E1 refers to the event of 
av.ql running, and E2 refers to the event of an entity 
(argO) causing event E l .  
Predicates are associated with not only the verb 
trees, but also the auxiliary trees. We use a flat 
semantic rcpmsentatiou like that of Kalhncycr and 
Joshi, and the semantics of a sentence is the con- 
junction of the semantic predicates of the trees used 
to derive the sentence. Figure 2 shows au auxiliary 
tree for a path prepositional pllrase headed by "to", 
along with its associated semantic predicate. 
When the PP tree for "to the park" is adjoiued into 
the intransitive tree for "John ran", the semantic in- 
terpretation is the conjunction of the two predicates 
motion(during(E),john) A goal(end(E),john, park); 
adjunction into the transitive tree for "Bill ran 
the horse" yields cause(during(E2),bilI, El) A mo- 
tion(during(El), horse) A goal(end(El), horse, park). 
In both cases, the argument X,rs?0.,rgl (john or 
horse) for the anxiliary tree is noulocal and colnes 
from the adjunction site. 2 The arguments are re- 
covered from the derivation tree, following Candito 
and Kahane. When an initial tree is substituted into 
another tree, the dependency mirrors the derivation 
structure, so the variables associated with the sub- 
2X.,..qo,.,.ga is the variable associated with the cntity in mo- 
tion (ar91) in the tree to which tile PP a(Uoins (argO). 
stituting tree can be referenced as arguments in the 
host tree's predicates. When an auxiliary tree is 
adjoined, the dependency for the adjunction is re- 
versed, so that variables associated with the host 
tree can be referenced as arguments in the adjoin- 
ing tree's predicates. 
VP 
VPar:jO* PP 
\[ evc,~t=l~ \] 
I' NP.rql$ 
I 
lo 
qoal (end(E), X,.,.;jo.,,.r11, Xa,.~j1) 
Figure 2: Auxiliary path PP tree 
The tripartite vent structure allows us to express 
the semantics of classes of verbs like change of 
state verbs whose description requires reference to 
a complex event structure. In the case of a verb 
such as "break", it is important to make a distinc- 
tion between the state of the object before the end 
of the action and the new state that results after- 
wards. This event structure also handles the eona- 
tive construction, in which there is an intention of 
a goal during the event, that is not achieved at 
the end of the event. The example of the cona- 
rive construction shown in Figure 3 expresses the 
intention of hitting something. Because the in- 
tention is not satisfied the semantics do not in- 
clude the predicates manner(end(E),fi~rcefuI, X, rgo ) 
A conmct(end(E),X, rgo,Xc~rgO, that express the 
completion of the contact with impact event. 
The ability of verbs to take on extended senses 
in sentences based on their adjuncts is captured in a 
1013 
S \[ event=E \]
NPa~.qO$ VP \[ evcnt=E \]
V NPargl$ 
I 
hit 
manner(during(E), direetedmotion, Xa,..qo )A 
contact(end(E), Xar~O, Xar~l )A 
7naT,,nel' (end(E), f of'ee f '~l, Xar9O ) 
s \[ cvcnt=r~: \] 
NParq0$ VP 
V VP 
I 
hit V PP 
I 
I 
at 
manner (during(E), direct, edmotion, X~r:io )
Figure 3: Syntax and semantics of transitive and conative construction for Hit verbs 
natural way by the TAG operation of adjunction and 
our conjunction of semantic predicates. The orig- 
inal Hit verb class does not include movement of 
the direct object as part of the meaning of hit; only 
sudden contact has to be established. By adjoining 
a path PP such as "across NP", we get an extended 
meaning, and a change in Levin class membership 
to the Throw class. Figure 4 shows the class-specific 
auxiliary tree anchored by the preposition "across" 
together with its semantic predicates, introducing a
motion event that immediately follows (meets) the 
contact event. 
VP \[ evenI:E \] 
VP.rf/o*\[ cvcnt=EargO \] PP 
P NPargl.~ 
I 
aCI'OSS 
meets (E,,..,jo, E) A 
motion(during(E), X~m~,0.,,.91 )A 
via(during(E), X~r,jo.~r~l , Xa,.,j1) 
Figure 4: Auxiliary tree for "across" 
oll the LTAG formalism, for which we already have 
a large English grammar. Palmer et al (1998) de- 
fined compositional semantics for classes of verbs 
implemented in LTAG, representing general seman- 
tic components (e.g., motion, manner) as features 
on the nodes of the trees. Ore" use of separate log- 
ical forms gives a more detailed semantics for the 
sentence, so that for an event involving motion, it is 
possible to know not only that the event has a motion 
semantic omponent, but also which entity is actu- 
ally in motion. This level of detail is necessary for 
applications uch as animatiou of natural anguage 
instructions (Bindiganavale t al., 2000). Another 
important contribution of this work is that by divid- 
ing each event into a tripartite structure, we permit a 
more precise definition of the associated semantics. 
Finally, the operation of adjunction in TAGs pro- 
vides a principled approach to representing the type 
of regular polysemy that has been a major obstacle 
in buildiug verb lexicons. 
Researching whether a TAG grammar for Verb- 
Net can be automatically constructed by using de- 
velopment tools such as Xia et al (1999) or Candito 
(1996) is part of our next step. We also expect o be 
able to factor out some class-specific auxiliary trees 
to be used across several verb classes. 
6 Conclusion 
We have presented a class-based approach to build- 
ing a verb lexicon that makes explicit and imple- 
ments the close association between syntax and se- 
mantics, as postulated by Levin. The power of the 
lexicon comes from its dynamic aspect hat is based 
7 Acknowledgments 
The authors would like to thank the anonymous re- 
viewers for their valuable comments. This researeh 
was partially supported by NSF grants IIS-9800658 
and IIS-9900297 and CAPES grant 0914-95. 
1014 
References 
Ralna Bindiganawde, Willianl Schuler, Jan M. All- 
beck, Nornlan I. Badler, Aravind K. Joshi, and 
Martha Pahner. 2000. Dynamically Altering 
Agent Behaviors Using Natural Language In- 
structions. Fourth International Cot!ference on 
Autonomous Agents, June. 
Marie-Hdl~ne Candito and Sylwtin Kahane. 1998. 
Can the TAG derivation tree represent a senlan- 
tic graph? An answer ill the light of Meaning- 
Text Theory. In Piz)ceedhtgs of the Fourth 77~G+ 
Workshop, pages 21-24, Philadelphia, PA, Au- 
gust. 
Marie-Hdl~ne Candito. 1996. A Principle-Based 
Hierarchical Representation f LTAGs. In Pro- 
ceedings of COLING-96, Copenhagen, Denlnark. 
Aim Copestake and Antonio Sanfilippo. 1993. 
Multilingual exical representation. In Proceed- 
ings ,2.1" the AAAI Spring Symposium: Bttilding 
Lexicons.for Machine Translation, Stanford, Cal- 
ifornia. 
Hoa Trang l)ang, Karin Kipper, Martha Pahner, and 
Joseph Rosenzweig. 1998. hwestigating regu- 
lar sense extensions based on intersective Levin 
classes . In Proceedings of COLING-ACL98, 
Montreal, Canada, August. 
Adele E. Goldberg. 1995. C'onslruclions. A Con- 
struction Grammar Approach 1o k rgument Slrttc- 
lure. University of Chicago Press, Chicago, Ill. 
Aravind K. Joshi and K. Vijay-Shanker. 1999. 
Compositional semantics wilh Lexicalized 
Tlee-ac\[joilfing Granllnar: How nlueh under- 
specification is necessary? In Proceedings of 
the Third International Worksho I) on Conq)tt- 
rational Semantics (IWCS-3), pages 131-145, 
Tilburg, The Netherlands, January. 
Aravind K. Joshi. 1987. An introduction to tree ad- 
joining grannnars. In A. Manaster-Ramer, ditor, 
Mathematics of Language. Jolm 13elljamins, Am- 
sterdaln. 
Laura Kalhneyer and Anwind Joshi. 1999. Under- 
specified selnantics with LTAG. In Proceedhlgs 
of Amsterdam Colloquium on Semantics. 
Karin Kippm, Hoa Trang Dang, and Martha Palmer. 
2000. Class-based construction of a verb lexi- 
COll. Ill Pn)ceedings of the Seventh National Con- 
ference on Art!\[icial httelligence (AAAI-2000), 
Austin, TX, July-August. 
Beth Levin. 1993. English Verb Classes and A Iter- 
nation, A Preliminary hwestigation. Tile Univer- 
sity of Chicago Press. 
J.B. Lowe, C.E Baker, and C.J. Filhnore. 1997. A 
fnnne-semantic approach to semantic annotation. 
Ill Proceedings 1997 Siglex WorksholJ/ANLP97, 
Washington, I).C. 
I. A. Mel'cuk. 1988. Semantic description of lex- 
ical units ill an explanatory combinatorial dic- 
tionary: Basic plilmiples and heuristic criteria. 
International ,lournal of Lexicography, I:3:165- 
188. 
M. Moens and M. Steedman. 1988. Telnporal on- 
tology and tenlporal refel'ence. Computational 
Linguistics, 14:15-38. 
Martha l~allnel ", Joseph Rosenzweig, and Willialn 
Schuler. 1998. Capturing Motion Verb General- 
izations in Sylmhronous TAG. Ill Patrick Saint- 
l)izim, editol, Predicative Forms in Natural Lan- 
guage and in Lexical Knowledge Bases. Kluwer 
Press. 
James Pustejovsky. 1995. The Generative Lexicon. 
MIT Press, Cambridge, Massachusetts, USA. 
Yves Sehabes. 1990. Mathematical nd Computa- 
tional Aspects of Lexicalized Grammars. Ph.D. 
thesis, Compnter Science Department, University 
of Pennsylvania. 
Matthew Stone and Christine Doran. 1997. Sell- 
tence Plalming as l)escription using Tree Adjoin- 
ing Grammar. Ill Proceedings of ACL-EACL '97, 
Madrid, Spain. 
Clare Voss. 1996. lnlerlinxua-l)ased Machine 
7'ranslation o\[" &~atial Expressions. PI'LD. the- 
sis, University of Maryland, Depamnent of Com- 
puter Science. 
Fei Xia, Martha Pahnei, and K. Vijay-Shanker. 
1999. Toward senli-autolnating grannnar 
development. Ill Proceedings of the 51h 
Natural Language Proces'sing Pacific Rim 
3),mposium(NLPRS-99), Beijing, China. 
1015 
Simple Features for Chinese Word Sense Disambiguation
Hoa Trang Dang, Ching-yi Chia, Martha Palmer, and Fu-Dong Chiou
Department of Computer and Information Science
University of Pennsylvania
 
htd,chingyc,mpalmer,chioufd  @unagi.cis.upenn.edu
Abstract
In this paper we report on our experiments on au-
tomatic Word Sense Disambiguation using a max-
imum entropy approach for both English and Chi-
nese verbs. We compare the difficulty of the sense-
tagging tasks in the two languages and investigate
the types of contextual features that are useful for
each language. Our experimental results suggest
that while richer linguistic features are useful for
English WSD, they may not be as beneficial for Chi-
nese.
1 Introduction
Word Sense Disambiguation (WSD) is a central
open problem at the lexical level of Natural Lan-
guage Processing (NLP). Highly ambiguous words
pose continuing problems for NLP applications.
They can lead to irrelevant document retrieval in In-
formation Retrieval systems, and inaccurate transla-
tions in Machine Translation systems (Palmer et al,
2000). For example, the Chinese word (jian4) has
many different senses, one of which can be trans-
lated into English as ?see?, and another as ?show?.
Correctly sense-tagging the Chinese word in context
can prove to be highly beneficial for lexical choice
in Chinese-English machine translation.
Several efforts have been made to develop au-
tomatic WSD systems that can provide accurate
sense tagging (Ide and Veronis, 1998), with a cur-
rent emphasis on creating manually sense-tagged
data for supervised training of statistical WSD sys-
tems, as evidenced by SENSEVAL-1 (Kilgarriff and
Palmer, 2000) and SENSEVAL-2 (Edmonds and
Cotton, 2001). Highly polysemous verbs, which
have several distinct but related senses, pose the
greatest challenge for these systems (Palmer et al,
2001). Predicate-argument information and selec-
tional restrictions are hypothesized to be particu-
larly useful for disambiguating verb senses.
Maximum entropy models can be used to solve
any classification task and have been applied to a
wide range of NLP tasks, including sentence bound-
ary detection, part-of-speech tagging, and parsing
(Ratnaparkhi, 1998). Assigning sense tags to words
in context can be viewed as a classification task sim-
ilar to part-of-speech tagging, except that a separate
set of tags is required for each vocabulary item to be
sense-tagged. Under the maximum entropy frame-
work (Berger et al, 1996), evidence from different
features can be combined with no assumptions of
feature independence. The automatic tagger esti-
mates the conditional probability that a word has
sense  given that it occurs in context  , where 
is a conjunction of features. The estimated prob-
ability is derived from feature weights which are
determined automatically from training data so as
to produce a probability distribution that has max-
imum entropy, under the constraint that it is con-
sistent with observed evidence. With existing tools
for learning maximum entropy models, the bulk of
our work is in defining the types of features to look
for in the data. Our goal is to see if sense-tagging
of verbs can be improved by combining linguistic
features that capture information about predicate-
arguments and selectional restrictions.
In this paper we report on our experiments on au-
tomatic WSD using a maximum entropy approach
for both English and Chinese verbs. We compare
the difficulty of the sense-tagging tasks in the two
languages and investigate the types of contextual
features that are useful for each language. We find
that while richer linguistic features are useful for
English WSD, they do not prove to be as beneficial
for Chinese.
The maximum entropy system performed com-
petitively with the best systems on the English
verbs in SENSEVAL-1 and SENSEVAL-2 (Dang and
Palmer, 2002). However, while SENSEVAL-2 made
it possible to compare many different approaches
over many different languages, data for the Chinese
lexical sample task was not made available in time
for any systems to compete. Instead, we report on
two experiments that we ran using our own lexicon
and two separate Chinese corpora that are very sim-
ilar in style (news articles from the People?s Repub-
lic of China), but have different types and levels of
annotation ? the Penn Chinese Treebank (CTB)(Xia
et al, 2000), and the People?s Daily News (PDN)
corpus from Beijing University. We discuss the util-
ity of different types of annotation for successful au-
tomatic word sense disambiguation.
2 English Experiment
Our maximum entropy WSD system was de-
signed to combine information from many differ-
ent sources, using as much linguistic knowledge
as could be gathered automatically by current NLP
tools. In order to extract the linguistic features nec-
essary for the model, all sentences were first auto-
matically part-of-speech-tagged using a maximum
entropy tagger (Ratnaparkhi, 1998) and parsed us-
ing the Collins parser (Collins, 1997). In addi-
tion, an automatic named entity tagger (Bikel et al,
1997) was run on the sentences to map proper nouns
to a small set of semantic classes.
Chodorow, Leacock and Miller (Chodorow et al,
2000) found that different combinations of topical
and local features were most effective for disam-
biguating different words. Following their work, we
divided the possible model features into topical fea-
tures and several types of local contextual features.
Topical features looked for the presence of key-
words occurring anywhere in the sentence and any
surrounding sentences provided as context (usually
one or two sentences). The set of 200-300 keywords
is specific to each lemma to be disambiguated, and
is determined automatically from training data so
as to minimize the entropy of the probability of the
senses conditioned on the keyword.
The local features for a verb  in a particular sen-
tence tend to look only within the smallest clause
containing  . They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging (1), syntactic features that cap-
ture relations between the verb and its complements
(2-4), and semantic features that incorporate infor-
mation about noun classes for subjects and objects
(5-6):
1. the word  , the part of speech of  , the part
of speech of words at positions -1 and +1 rela-
tive to  , and words at positions -2, -1, +1, +2,
relative to 
2. whether or not the sentence is passive
3. whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree)
4. the words (if any) in the positions of subject,
direct object, indirect object, particle, preposi-
tional complement (and its object)
5. a Named Entity tag (PERSON, ORGANIZA-
TION, LOCATION) for proper nouns appear-
ing in (4)
6. WordNet synsets and hypernyms for the nouns
appearing in (4)
2.1 English Results
The maximum entropy system?s performance on
the verbs from the evaluation data for SENSEVAL-
1 (Kilgarriff and Rosenzweig, 2000) rivaled that
of the best-performing systems. We looked at the
effect of adding topical features to local features
that either included WordNet class features or used
just lexical and named entity features. In addition,
we experimented to see if performance could be
improved by undoing passivization transformations
to recover underlying subjects and objects. This
was expected to increase the accuracy with which
verb arguments could be identified, helping in cases
where selectional restrictions on arguments played
an important role in differentiating between senses.
The best overall variant of the system for verbs
did not use WordNet class features, but included
topical keywords and passivization transformation,
giving an average verb accuracy of 72.3%. If
only the best combination of feature sets for each
verb is used, then the maximum entropy mod-
els achieve 73.7% accuracy. These results are
not significantly different from the reported results
of the best-performing systems (Yarowsky, 2000).
Our system was competitive with the top perform-
ing systems even though it used only the training
data provided and none of the information from
the dictionary to identify multi-word constructions.
Later experiments show that the ability to correctly
identify multi-word constructions improves perfor-
mance substantially.
We also tested the WSD system on the verbs from
the English lexical sample task for SENSEVAL-2.1
1The verbs were: begin, call, carry, collaborate, develop,
Feature Type (local only) Accuracy Feature Type (local and topical) Accuracy
collocation 48.3 collocation 52.9
+ syntax 53.9 + syntax 54.2
+ syntax + semantics 59.0 + syntax + semantics 60.2
Table 1: Accuracy of maximum entropy system using different subsets of features for SENSEVAL-2 verbs.
In contrast to SENSEVAL-1, senses involving multi-
word constructions could be identified directly from
the sense tags themselves, and the head word and
satellites of multi-word constructions were explic-
itly marked in the training and test data. This addi-
tional annotation made it much easier to incorporate
information about the satellites, without having to
look at the dictionary (whose format may vary from
one task to another). All the best-performing sys-
tems on the English verb lexical sample task filtered
out possible senses based on the marked satellites,
and this improved performance.
Table 1 shows the performance of the system us-
ing different subsets of features. In general, adding
features from richer linguistic sources tended to im-
prove accuracy. Adding syntactic features to collo-
cational features proved most beneficial in the ab-
sence of topical keywords that could detect some
of the complements and arguments that would nor-
mally be picked up by parsing (complementizers,
prepositions, etc.). And while topical information
did not always improve results significantly, syntac-
tic features along with semantic class features al-
ways proved beneficial.
Incorporating topical keywords as well as col-
locational, syntactic, and semantic local features,
our system achieved 60.2% and 70.2% accuracy
using fine-grained and coarse-grained scoring, re-
spectively. This is in comparison to the next best-
performing system, which had fine- and coarse-
grained scores of 57.6% and 67.2% (Palmer et al,
2001). If we had not included a filter that only con-
sidered phrasal senses whenever there were satel-
lites of multi-word constructions marked in the test
data, our fine- and coarse-grained accuracy would
have been reduced to 57.5% and 67.2% (significant
at 	
 ).
3 Chinese Experiments
We chose 28 Chinese words to be sense-tagged.
Each word had multiple verb senses and possibly
draw, dress, drift, drive, face, ferret, find, keep, leave, live,
match, play, pull, replace, see, serve, strike, train, treat, turn,
use, wander, wash, work.
other senses for other parts of speech, with an av-
erage of 6 dictionary senses per word. The first
20 words were chosen by randomly selecting sev-
eral files totaling 5000 words from the 100K-word
Penn Chinese Treebank, and choosing only those
words that had more than one dictionary verb sense
and that occurred more than three times in these
files. The remaining 8 words were chosen by se-
lecting all words that had more than one dictio-
nary verb sense and that occurred more than 25
times in the CTB. The definitions for the words
were based on the CETA (Chinese-English Transla-
tion Assistance) dictionary (Group, 1982) and other
hard-copy dictionaries. Figure 1 shows an exam-
ple dictionary entry for the most common sense of
jian4. For each word, a sense entry in the lexi-
con included the definition in Chinese as well as
in English, the part of speech for the sense, a typ-
ical predicate-argument frame if the sense is for a
verb, and an example sentence. With these defini-
tions, each word was independently sense-tagged by
two native Chinese-speaking annotators in a double-
blind manner. Sense-tagging was done primarily us-
ing raw text, without segmentation, part of speech,
or bracketing information. After finishing sense tag-
ging, the annotators met to compare and to discuss
their results, and to modify the definitions if neces-
sary. The gold standard sense-tagged files were then
made after all this discussion.
In a manner similar to our English approach, we
included topical features as well as collocational,
syntactic, and semantic local features in the maxi-
mum entropy models. Collocational features could
be extracted from data that had been segmented into
words and tagged for part of speech:
 the target word
 the part of speech tag of the target word
 the words (if any) within 2 positions of the tar-
get word
 the part of speech of the words (if any) immedi-
ately preceding and following the target word
 whether the target word follows a verb
<entry id="00007" word=" " pinyin="jian4">
<wordsense id="00007-001">
         <definition id="chinese"> , , </definition>
         <definition id="english">to see, to perceive</definition>
         <pos>VV</pos>
         <pred-arg>NP0 NP1</pred-arg>
         <pred-arg>NP0 NP1 IP</pred-arg>
         <example> <word> </word> </example>
</wordsense>
</entry>
Figure 1: Example sense definition for jian4.
When disambiguating verbs, the following syn-
tactic local features were extracted from data brack-
eted according to the Penn Chinese Treebank guide-
lines:
 whether the verb has a surface subject
 the head noun of the surface subject of the verb
 whether the verb has an object (any phrase la-
beled with ?-OBJ?, such as NP-OBJ, IP-OBJ,
QP-OBJ)
 the phrase label of the object, if any
 the head noun of the object
 whether the verb has a VP complement
 the VP complement, if any
 whether the verb has an IP complement
 whether the verb has two NP complements
 whether the verb is followed by a predicate
(any phrase labeled with ?-PRD?)
Semantic features were generated by assigning a
HowNet2 noun category to each subject and object,
and topical keywords were extracted as for English.
Once all the features were extracted, a maximum
entropy model was trained and tested for each target
word. We used 5-fold cross validation to evaluate
the system on each word. Two methods were used
for partitioning a dataset of size  into five subsets:
Select  consecutive occurrences for each set, or
select every 5th occurrence for a set. In the end,
the choice of partitioning method made little differ-
ence in overall performance, and we report accuracy
as the precision using the latter (stratified) sampling
method.
2http://www.keenage.com/
Feature Type Acc Std Dev
collocation (no part of speech) 86.8 1.0
collocation 93.4 0.5
+ syntax 94.4 0.4
+ syntax + semantics 94.3 0.6
collocation + topic 90.3 1.0
+ syntax + topic 92.6 0.9
+ syntax + semantics + topic 92.8 0.9
Table 2: Overall accuracy of maximum entropy sys-
tem using different subsets of features for Penn Chi-
nese Treebank words (manually segmented, part-of-
speech-tagged, parsed).
3.1 Penn Chinese Treebank
All sentences containing any of the 28 target words
were extracted from the Penn Chinese Treebank,
yielding between 4 and 1143 occurrence (160 av-
erage) for each of the target words. The manual
segmentation, part-of-speech tags, and bracketing
of the CTB were used to extract collocational and
syntactic features.
The overall accuracy of the system on the 28
words in the CTB was 94.4% using local colloca-
tional and syntactic features. This is significantly
better than the baseline of 76.7% obtained by tag-
ging all instances of a word with the most frequent
sense of the word in the CTB. Considering only the
23 words for which more than one sense occurred in
the CTB, overall system accuracy was 93.9%, com-
pared with a baseline of 74.7%. Figure 2 shows the
results broken down by word.
As with the English data, we experimented with
different types of features. Table 2 shows the per-
formance of the system using different subsets of
features. While the system?s accuracy using syntac-
tic features was higher than using only collocational
features (significant at 
 ), the improve-
Word pinying (translation)               Events  Senses  Baseline  Acc.  Std Dev
-------------------------------------------------------------------------------
 biao3 shi4  (to indicate/express)   100     3       63.0      95.0  5.5
   chu1 (to go out/to come out)        34      5       50.0      50.0  11.1
   da2 (to reach a stage/to attain)    181     1       100       100   0.0
   dao3 (to come/to arrive)            219     10      36.5      82.7  7.1
 fa1 zhan3 (to develop/to grow)      437     3       65.2      97.0  1.2
   hui4 (will/be able to)              86      6       58.1      91.9  6.0
   jian4 (to see/to perceive)          4       2       75.0      25.0  38.7
 jie3 jue2 (to solve/to settle)      44      2       79.5      97.7  5.0
 jin4 xing2 (to be in progress)      159     3       89.3      95.6  2.5
   ke3 (may/can)                       57      1       100       100   0.0
   lai2 (to come/to arrive)            148     6       66.2      96.6  2.1
 li4 yong4 (to use/to utilize)       163     2       92.6      98.8  2.4
   rang4 (to let/to allow)             9       1       100       100   0.0
   shi3 (to make/to let)               89      1       100       100   0.0
   shuo1 (to say in spoken words)      306     6       86.9      95.1  2.0
   wan2 (to complete/to finish)        285     2       98.9      100.0 0.0
   wei2/wei4 (to be/to mean)           473     7       32.8      86.1  2.4
   xiang3 (to think/ponder/suppose)    8       3       62.5      50.0  50.0
 yin3 jin4 (to import/to introduce)  62      2       85.5      98.4  3.3
   zai4 (to exist/to be at(in, on))    1143    4       96.9      99.3  0.4
 fa1 xian4 (to discover/to realize)  37      3       59.5      100.0 0.0
 hui1 fu4 (to resume/to restore)     27      3       44.4      77.8  19.8
 kai1 fang4 (to open to investors)   122     5       74.6      96.7  3.0
 ke3 yi3 (may/can)                   32      1       100       100   0.0
 tong1 guo4 (to pass legislation)    81      5       66.7      95.1  2.5
 tou2 ru4 (to input money, etc.)     44      4       40.9      84.1  11.7
   yao4 (must/should/to intend to)     106     6       65.1      62.3  8.9
   yong4 (to use)                      41      2       58.5      100   0.0
-------------------------------------------------------------------------------
Overall                                  4497    3.5     76.7      94.4  0.4
Figure 2: Word, number of instances, number of senses in CTB, baseline accuracy, maximum entropy
accuracy and standard deviation using local collocational and syntactic features.
ment was not as substantial as for English, and this
was despite the fact that the Chinese bracketing was
done manually and should be almost error-free.
Semantic class information from HowNet yielded
no improvement at all. To see if using a differ-
ent ontology would help, we subsequently exper-
imented with the ROCLing conceptual structures
(Mo, 1992). In this case, we also manually added
unknown nouns from the corpus to the ontology and
labeled proper nouns with their conceptual struc-
tures, in order to more closely parallel the named
entity information used in the English experiments.
This resulted in a system accuracy of 95.0% (std.
dev. 0.6), which again is not significantly better than
omitting the noun class information.
3.2 People?s Daily News
Five of the CTB words (chu1, jian4, xiang3, hui1
fu4, yao4) had system performance of less than
80%, probably due to their low frequency in the
CTB corpus. These words were subsequently sense
tagged in the People?s Daily News, a much larger
corpus (about one million words) that has manual
segmentation and part-of-speech, but no bracketing
information.3 Those 5 words included all the words
for which the system performed below the baseline
3The PDN corpus can be found at
http://icl.pku.edu.cn/research/corpus/dwldform1.asp.
The annotation guidelines are not exactly the
same as for the Penn CTB, and can be found at
http://icl.pku.edu.cn/research/corpus/coprus-annotation.htm.
Feature Type Acc Std Dev
collocation (no part of speech) 72.3 2.2
collocation 70.3 2.9
+ syntax 71.7 3.0
+ syntax + semantics 72.7 3.1
collocation + topic 73.3 3.2
+ syntax + topic 72.6 3.9
+ syntax + semantics + topic 72.8 3.7
Table 3: Overall accuracy of maximum entropy sys-
tem using different subsets of features for People?s
Daily News words (automatically segmented, part-
of-speech-tagged, parsed).
Feature Type Acc Std Dev
collocation (no part of speech) 71.4 4.3
collocation 74.7 2.3
collocation + topic 72.1 3.1
Table 4: Overall accuracy of maximum entropy sys-
tem using different subsets of features for People?s
Daily News words (manually segmented, part-of-
speech-tagged).
in the CTB corpus. About 200 sentences for each
word were selected randomly from PDN and sense-
tagged as with the CTB.
We automatically annotated the PDN data to
yield the same types of annotation that had been
available in the CTB. We used a maximum-
matching algorithm and a dictionary compiled from
the CTB (Sproat et al, 1996; Xue, 2001) to do seg-
mentation, and trained a maximum entropy part-of-
speech tagger (Ratnaparkhi, 1998) and TAG-based
parser (Bikel and Chiang, 2000) on the CTB to do
tagging and parsing.4 Then the same feature extrac-
tion and model-training was done for the PDN cor-
pus as for the CTB.
The system performance is much lower for the
PDN than for the CTB, for several reasons. First,
the PDN corpus is more balanced than the CTB,
which contains primarily financial articles. A wider
range of usages of the words was expressed in PDN
than in CTB, making the disambiguation task more
difficult; the average number of senses for the PDN
words was 8.2 (compared to 3.5 for CTB), and the
4On held-out portions of the CTB, the accuracy of the seg-
mentation and part-of-speech tagging are over 95%, and the
accuracy of the parsing is 82%, which are comparable to the
performance of the English preprocessors. The performance
of these preprocessors is naturally expected to degrade when
transferred to a different domain.
baseline accuracy was 58.0% (compared to 76.7%
for CTB). Also, using automatically preprocessed
data for the PDN introduced noise that was not
present for the manually preprocessed CTB. Despite
these differences between PDN and CTB, the trends
in using increasingly richer linguistic preprocessing
are similar. Table 3 shows that adding more features
from richer levels of linguistic annotation yielded
no significant improvement over using only collo-
cational features. In fact, using only lexical collo-
cations from automatic segmentation was sufficient
to produce close to the best results. Table 4 shows
the system performance using the available manual
segmentation and part-of-speech tagging. While us-
ing part-of-speech tags seems to be better than us-
ing only lexical collocations, the difference is not
significant.
4 Conclusion
We have demonstrated the high performance of
maximum entropy models for word sense disam-
biguation in English, and have applied the same ap-
proach successfully to Chinese. While SENSEVAL-
2 showed that methods that work on English also
tend to work on other languages, our experiments
have revealed striking differences in the types of
features that are important for English and Chi-
nese WSD. While parse information seemed crucial
for English WSD, it only played a minor role in
Chinese; in fact, the improvement in Chinese per-
formance contributed by manual parse information
in the CTB disappeared altogether when automatic
parsing was done for the PDN. The fact that brack-
eting was more important for English than Chinese
WSD suggests that predicate-argument information
and selectional restrictions may play a more impor-
tant role in distinguishing English verb senses than
Chinese senses. Or, it may be the case that Chi-
nese verbs tend to be adjacent to their arguments,
so collocational information is sufficient to capture
the same information that would require parsing in
English. This is a question for further study.
The simpler level of linguistic processing re-
quired to achieve relatively high sense-tagging ac-
curacy in Chinese highlights an important differ-
ence between Chinese and English. Chinese is dif-
ferent from English in that much of Chinese linguis-
tic ambiguity occurs at the basic level of word seg-
mentation. Chinese word segmentation is a major
task in itself, and it seems that once this is accom-
plished little more needs to be done for sense dis-
ambiguation. Our experience in English has shown
that the ability to identify multi-word constructions
significantly improves sense-tagging performance.
Multi-character Chinese words, which are identified
by word segmentation, may be the analogy to En-
glish multi-word constructions.
5 Acknowledgments
This work has been supported by National Sci-
ence Foundation Grants, NSF-9800658 and NSF-
9910603, and DARPA grant N66001-00-1-8915 at
the University of Pennsylvania. The authors would
also like to thank the anonymous reviewers for their
valuable comments.
References
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Com-
putational Linguistics, 22(1).
Daniel M. Bikel and David Chiang. 2000. Two sta-
tistical parsing models applied to the chinese tree-
bank. In Proceedings of the Second Chinese Lan-
guage Processing Workshop, Hong Kong.
Daniel M. Bikel, Scott Miller, Richard Schwartz,
and Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing, Washington, DC.
Martin Chodorow, Claudia Leacock, and George A.
Miller. 2000. A topical/local classifier for word
sense identification. Computers and the Human-
ities, 34(1-2), April. Special Issue on SENSE-
VAL.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the As-
sociation for Computational Linguistics, Madrid,
Spain, July.
Hoa Trang Dang and Martha Palmer. 2002. Com-
bining contextual features for word sense disam-
biguation. In Proceedings of the Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia, PA.
Philip Edmonds and Scott Cotton. 2001.
SENSEVAL-2: Overview. In Proceedings
of SENSEVAL-2: Second International Work-
shop on Evaluating Word Sense Disambiguation
Systems, Toulouse, France, July.
Chinese-English Translation Assistance Group.
1982. Chinese Dictionaries: an Extensive Bib-
liography of Dictionaries in Chinese and Other
Languages. Greenwood Publishing Group.
Nancy Ide and Jean Veronis. 1998. Introduction to
the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1).
Adam Kilgarriff and Martha Palmer. 2000. In-
troduction to the special issue on SENSEVAL.
Computers and the Humanities, 34(1-2), April.
Special Issue on SENSEVAL.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for English SENSEVAL. Computers
and the Humanities, 34(1-2), April. Special Issue
on SENSEVAL.
Ruo-Ping Mo. 1992. A conceptual structure that is
suitable for analysing chinese. Technical Report
CKIP-92-04, Academia Sinica, Taipei, Taiwan.
M. Palmer, Chunghye Han, Fei Xia, Dania Egedi,
and Joseph Rosenzweig. 2000. Constraining lex-
ical selection across languages using tags. In
Anne Abeille and Owen Rambow, editors, Tree
Adjoining Grammars: formal, computational
and linguistic aspects. CSLI, Palo Alto, CA.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. En-
glish tasks: All-words and verb lexical sample.
In Proceedings of SENSEVAL-2: Second Interna-
tional Workshop on Evaluating Word Sense Dis-
ambiguation Systems, Toulouse, France, July.
Adwait Ratnaparkhi. 1998. Maximum Entropy
Models for Natural Language Ambiguity Resolu-
tion. Ph.D. thesis, University of Pennsylvania.
Richard Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-state
word segmentation algorithm for chinese. Com-
putational Linguistics, 22(3).
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Okurowski, John Kovarik, Fu-Dong Chiou,
Shizhe Huang, Tony Kroch, and Mitch Mar-
cus. 2000. Developing guidelines and ensuring
consistency for chinese text annotation. In Pro-
ceedings of the second International Conference
on Language Resources and Evaluation, Athens,
Greece.
Nianwen Xue. 2001. Defining and Automatically
Identifying Words in Chinese. Ph.D. thesis, Uni-
versity of Delaware.
David Yarowsky. 2000. Hierarchical decision lists
for word sense disambiguation. Computers and
the Humanities, 34(1-2), April. Special Issue on
SENSEVAL.
Combining Contextual Features for Word Sense Disambiguation
Hoa Trang Dang and Martha Palmer
Department of Computer and Information Sciences
University of Pennsylvania
Philadelphia, PA, USA, 19104
{htd,mpalmer}linc.cis.upenn.edu
Abstract
In this paper we present a maximum en-
tropy Word Sense Disambiguation system
we developed which performs competi-
tively on SENSEVAL-2 test data for En-
glish verbs. We demonstrate that using
richer linguistic contextual features sig-
nificantly improves tagging accuracy, and
compare the system?s performance with
human annotator performance in light
of both fine-grained and coarse-grained
sense distinctions made by the sense in-
ventory.
1 Introduction
Highly ambiguous words pose continuing problems
for Natural Language Processing (NLP) applica-
tions. They can lead to irrelevant document re-
trieval in IR systems, and inaccurate translations in
Machine Translation systems (Palmer et al, 2000).
While homonyms like bank are fairly tractable, pol-
ysemous words like run, with related but subtly dis-
tinct meanings, present the greatest hurdle for Word
Sense Disambiguation (WSD). SENSEVAL-1 and
SENSEVAL-2 have attempted to provide a frame-
work for evaluating automatic systems by creating
corpora tagged with fixed sense inventories, which
also enables the training of supervised WSD sys-
tems.
In this paper we describe a maximum entropy
WSD system that combines information from many
different sources, using as much linguistic knowl-
edge as can be gathered automatically by current
NLP tools. Maximum entropy models have been
applied to a wide range of classification tasks in
NLP (Ratnaparkhi, 1998). Our maximum entropy
system performed competitively with the best per-
forming systems on the English verb lexical sample
task in SENSEVAL-1 and SENSEVAL-2. We com-
pared the system performance with human annota-
tor performance in light of both fine-grained and
coarse-grained sense distinctions made by WordNet
in SENSEVAL-2, and found that many of the sys-
tem?s errors on fine-grained senses stemmed from
the same sources that caused disagreements between
human annotators. These differences were par-
tially resolved by backing off to more coarse-grained
sense-groups, which are sometimes necessary when
even human annotators cannot make the fine-grained
sense distinctions specified in the dictionary.
2 Related Work
While it is possible to build an automatic sense tag-
ger using only the dictionary definitions, the most
accurate systems tend to take advantage of super-
vised learning. The system with the highest overall
performance in SENSEVAL-1 used Yarowsky?s hier-
archical decision lists (Yarowsky, 2000); while there
is a large set of potential features, only a small num-
ber is actually used to determine the sense of any
given instance of a word. Chodorow, Leacock and
Miller (Chodorow et al, 2000) also achieved high
accuracy using naive bayesian models for WSD,
combining sets of linguistically impoverished fea-
tures that were classified as either topical or local.
Topical features consisted of a bag of open-class
words in a wide window covering the entire con-
text provided; local features were words and parts of
speech within a small window or at particular offsets
                     July 2002, pp. 88-94.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
from the target word. The system was configured to
use only local, only topical, or both local and topical
features for each word, depending on which configu-
ration produced the best result on a held-out portion
of the training data.
Previous experiments (Ng and Lee, 1996) have
explored the relative contribution of different knowl-
edge sources to WSD and have concluded that collo-
cational information is more important than syntac-
tic information. Additionally, Pedersen (Pedersen,
2001; Pedersen, 2000) has pursued the approach
of using simple word bigrams and other linguisti-
cally impoverished feature sets for sense tagging, to
establish upper bounds on the accuracy of feature
sets that do not impose substantial pre-processing
requirements. In contrast, we wish to demonstrate
that such pre-processing significantly improves ac-
curacy for sense-tagging English verbs, because we
believe that they allow us to extract a set of features
that more closely parallels the information humans
use for sense disambiguation.
3 System Description
We developed an automatic WSD system that uses
a maximum entropy framework to combine linguis-
tic contextual features from corpus instances of each
verb to be tagged. Under the maximum entropy
framework (Berger et al, 1996), evidence from dif-
ferent features can be combined with no assump-
tions of feature independence. The automatic tag-
ger estimates the conditional probability that a word
has sense x given that it occurs in context y, where
y is a conjunction of features. The estimated proba-
bility is derived from feature weights which are de-
termined automatically from training data so as to
produce a probability distribution that has maximum
entropy, under the constraint that it is consistent with
observed evidence.
In order to extract the linguistic features neces-
sary for the model, all sentences were first automat-
ically part-of-speech-tagged using a maximum en-
tropy tagger (Ratnaparkhi, 1998) and parsed using
the Collins parser (Collins, 1997). In addition, an
automatic named entity tagger (Bikel et al, 1997)
was run on the sentences to map proper nouns to a
small set of semantic classes. Following work by
Chodorow, Leacock and Miller, we divided the pos-
sible model features into topical and local contex-
tual features. Topical features looked for the pres-
ence of keywords occurring anywhere in the sen-
tence and any surrounding sentences provided as
context (usually one or two sentences). The set
of 200-300 keywords is specific to each lemma to
be disambiguated, and is determined automatically
from training data so as to minimize the entropy of
the probability of the senses conditioned on the key-
word.
The local features for a verb w in a particular sen-
tence tend to look only within the smallest clause
containing w. They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging (1), syntactic features that capture
relations between the verb and its complements (2-
4), and semantic features that incorporate informa-
tion about noun classes for objects (5-6):
1. the word w, the part of speech of w, and words
at positions -2, -1, +1, +2, relative to w
2. whether or not the sentence is passive
3. whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree)
4. the words (if any) in the positions of subject,
direct object, indirect object, particle, preposi-
tional complement (and its object)
5. a Named Entity tag (PERSON, ORGANIZA-
TION, LOCATION) for proper nouns appear-
ing in (4)
6. WordNet synsets and hypernyms for the nouns
appearing in (4)1
This set of local features relies on access to syntac-
tic structure as well as semantic class information,
and represents our move towards using richer syn-
tactic and semantic knowledge sources to model hu-
man performance.
1Nouns were not disambiguated in any way, and all possible
synsets and hypernyms for the noun were included. No separate
disambiguation of noun complements was done because, given
enough data, the maximum entropy model should assign high
weights to the correct semantic classes of the correct noun sense
if they represent defining selectional restrictions.
4 Evaluation
In this section we describe the system performance
on the verbs from SENSEVAL-1 and SENSEVAL-2.
The system was built after SENSEVAL-1 but before
SENSEVAL-2.2
SENSEVAL-1 SENSEVAL-1 used a DARPA-style
evaluation format where the participants were pro-
vided with hand-annotated training data and test
data. The lexical inventory used was the Hector lex-
icon, developed jointly by DEC and Oxford Univer-
sity Press (Kilgarriff and Rosenzweig, 2000). By
allowing for discussion and revision of confusing
lexical entries during tagging, before the final test
data was tagged, inter-annotator agreement of over
90% was eventually achieved. However, the Hector
lexicon was very small and under proprietary con-
straints, making it an unsuitable candidate for ap-
plications requiring a large-scale, publicly-available
dictionary.
SENSEVAL-2 The subsequent SENSEVAL-2 exer-
cise used a pre-release version of WordNet1.7 which
is much larger than Hector and is more widely used
in NLP applications. The average training set size
for verbs was only about half of that provided in
SENSEVAL-1, while the average polysemy of each
verb was higher3. Smaller training sets and the
use of a large-scale, publicly available dictionary ar-
guably make SENSEVAL-2 a more indicative evalu-
ation of WSD systems in the current NLP environ-
ment than SENSEVAL-1. The role of sense groups
was also explored as a way to address the pop-
ular criticism that WordNet senses are too vague
and fine-grained. During the data preparation for
SENSEVAL-2, previous WordNet groupings of the
verbs were carefully re-examined, and specific se-
mantic criteria were manually associated with each
group. This occasionally resulted in minor revisions
of the original groupings (Fellbaum et al, 2001).
This manual method of creating a more coarse-
grained sense inventory from WordNet contrasts
with automatic methods that rely on existing se-
2The system did not compete officially in SENSEVAL-2 be-
cause it was developed by people who were involved in coordi-
nating the English verbs lexical sample task.
3The average number of senses per verb in the training data
was 11.6 using the Hector dictionary in SENSEVAL-1, and 15.6
using WordNet1.7 in SENSEVAL-2.
mantic links in WordNet (Mihalcea and Moldovan,
2001), which can produce divergent dictionaries.
Our system performs competitively with the
best performing systems in SENSEVAL-1 and
SENSEVAL-2. Measuring accuracy as the recall
score (which is equal to precision in our case be-
cause the system assigns a tag to every instance), we
compare the system?s coarse-grained scores using
the revised groupings versus random groupings, and
demonstrate the coherence and utility of the group-
ings in reconciling apparent tagging disagreements.
4.1 SENSEVAL-1 Results
The maximum entropy WSD system?s perfor-
mance on the verbs from the evaluation data for
SENSEVAL-1 (Kilgarriff and Rosenzweig, 2000) ri-
valed that of the best-performing systems. Table 1
shows the performance of variants of the system us-
ing different subsets of possible features. In addition
to experimenting with different combinations of lo-
cal/topical features, we attempted to undo passiviza-
tion transformations to recover underlying subjects
and objects. This was expected to increase the accu-
racy with which verb arguments could be identified,
helping in cases where selectional restrictions on ar-
guments played an important role in differentiating
between senses.
The best overall variant of the system for verbs
did not use WordNet class features, but included
topical keywords and passivization transformation,
giving an average verb accuracy of 72.3%. This
falls between Chodorow, Leacock, and Miller?s ac-
curacy of 71.0%, and Yarowsky?s 73.4% (74.3%
post-workshop). If only the best combination of fea-
ture sets for each verb is used, then the maximum en-
tropy models achieve 73.7% accuracy. Even though
our system used only the training data provided and
none of the information from the dictionary itself,
it was still competitive with the top performing sys-
tems which also made use of the dictionary to iden-
tify multi-word constructions. As we show later,
using this additional piece of information improves
performance substantially.
In addition to the SENSEVAL-1 verbs, we ran the
system on the SENSEVAL-1 data for shake, which
contains both nouns and verbs. The system sim-
ply excluded verb complement features whenever
the part-of-speech tagger indicated that the word
task lex lex+topic lex+trans+topic wn wn+topic wn+trans+topic
amaze 0.957 0.928 0.942 0.957 0.899 0.913
bet-v 0.709 0.667 0.667 0.718 0.650 0.650
bother 0.866 0.852 0.847 0.837 0.828 0.823
bury 0.468 0.502 0.517 0.572 0.537 0.532
calculate 0.867 0.902 0.904 0.862 0.881 0.872
consume 0.481 0.492 0.508 0.454 0.503 0.454
derive 0.682 0.682 0.691 0.659 0.664 0.696
float-v 0.437 0.441 0.445 0.406 0.445 0.432
invade 0.560 0.522 0.531 0.580 0.551 0.536
promise-v 0.906 0.902 0.902 0.888 0.893 0.893
sack-v 0.972 0.972 0.972 0.966 0.966 0.966
scrap-v 0.812 0.866 0.871 0.796 0.876 0.882
seize 0.653 0.741 0.745 0.660 0.691 0.703
verbs 0.705 0.718 0.723 0.703 0.711 0.709
shake-p 0.744 0.725 0.742 0.767 0.770 0.758
Table 1: Accuracy of different variants of maximum entropy models on SENSEVAL-1 verbs. Only local in-
formation was used, unless indicated by ?+topic,? in which case the topical keyword features were included
in the model; ?wn? indicates that WordNet class features were used, while ?lex? indicates only lexical and
named entity tag features were used for the noun complements; ?+trans? indicates that an attempt was made
to undo passivization transformations.
to be sense-tagged was not a verb. Even on this
mix of nouns and verbs, the system performed
well compared with the best system for shake from
SENSEVAL-1, which had an accuracy of 76.5% on
the same task.
4.2 SENSEVAL-2 Results
We also tested the WSD system on the verbs from
the English lexical sample task for SENSEVAL-2.
In contrast to SENSEVAL-1, senses involving multi-
word constructions could be directly identified from
the sense tags themselves (through the WordNet
sense keys that were used as sense tags), and the
head word and satellites of multi-word construc-
tions were explicitly marked in the training and test
data. This additional annotation made it much eas-
ier for our system to incorporate information about
the satellites, without having to look at the dictio-
nary (whose format may vary from one task to an-
other). The best-performing systems on the English
verb lexical sample task (including our own) filtered
out possible senses based on the marked satellites,
and this improved performance.
Table 2 shows the performance of the system us-
ing different subsets of features. While we found lit-
tle improvement from transforming passivized sen-
tences into a more canonical form to recover under-
lying arguments, there is a clear improvement in per-
formance as richer linguistic information is incorpo-
rated in the model. Adding topical keywords also
helped.
Incorporating topical keywords as well as col-
locational, syntactic, and semantic local features,
our system achieved 59.6% and 69.0% accuracy
using fine-grained and coarse-grained scoring, re-
spectively. This is in comparison to the next best-
performing system, which had fine- and coarse-
grained scores of 57.6% and 67.2% (Palmer et al,
2001). Here we see the benefit from including a filter
that only considered phrasal senses whenever there
were satellites of multi-word constructions marked
in the test data; had we not included this filter, our
fine- and coarse-grained scores would have been
only 56.9% and 66.1%.
Table 3 shows a breakdown of the number of
senses and groups for each verb, the fine-grained
accuracy of the top three official SENSEVAL-2 sys-
tems, fine- and coarse-grained accuracy of our maxi-
Feature Type (local only) Accuracy Feature Type (local and topical) Accuracy
collocation 47.6 collocation 49.8
+ syntax 54.9 + syntax 57.1
+ syntax + transform 55.1 + syntax + transform 57.3
+ syntax + semantics 58.3 + syntax + semantics 59.6
+ syntax + semantics + transform 58.9 + syntax + semantics + transform 59.5
Table 2: Accuracy of maximum entropy system using different subsets of features for SENSEVAL-2 verbs.
Verb Senses Groups SMULS JHU KUNLP MX MX-c ITA ITA-c
begin 8 8 87.5 71.4 81.4 83.2 83.2 81.2 81.4
call 23 16 40.9 43.9 48.5 47.0 63.6 69.3 89.2
carry 27 17 39.4 51.5 45.5 37.9 48.5 60.7 75.3
collaborate 2 2 90.0 90.0 90.0 90.0 90.0 75.0 75.0
develop 15 5 36.2 42.0 42.0 49.3 68.1 67.8 85.2
draw 32 20 31.7 41.5 34.1 36.6 51.2 76.7 82.5
dress 14 8 57.6 59.3 71.2 61.0 89.8 86.5 100.0
drift 9 6 59.4 53.1 53.1 43.8 43.8 50.0 50.0
drive 15 10 52.4 42.9 54.8 59.5 78.6 58.8 71.7
face 7 4 81.7 80.6 82.8 81.7 90.3 78.6 97.4
ferret 1 1 100.0 100.0 100.0 100.0 100.0 100.0 100.0
find 17 10 29.4 26.5 27.9 27.9 39.7 44.3 56.9
keep 27 22 44.8 55.2 44.8 56.7 58.2 79.1 80.1
leave 14 10 47.0 51.5 50.0 62.1 66.7 67.2 80.5
live 10 8 67.2 59.7 59.7 68.7 70.1 79.7 87.2
match 8 4 40.5 52.4 52.4 47.6 69.0 56.5 82.6
play 25 18 50.0 45.5 37.9 50.0 51.5 * *
pull 33 28 48.3 55.0 45.0 53.3 68.3 68.1 72.2
replace 4 2 44.4 57.8 55.6 62.2 93.3 65.9 100.0
see 21 13 37.7 42.0 39.1 47.8 55.1 70.9 75.5
serve 12 7 49.0 54.9 68.6 68.6 72.5 90.8 93.2
strike 26 21 38.9 48.1 40.7 33.3 44.4 76.2 90.5
train 9 4 41.3 54.0 58.7 57.1 69.8 28.8 55.0
treat 6 5 63.6 56.8 56.8 56.8 63.6 96.9 97.5
turn 43 31 35.8 44.8 37.3 44.8 56.7 74.2 89.4
use 7 4 72.4 72.4 65.8 65.8 78.9 74.3 89.4
wander 4 2 74.0 78.0 82.0 82.0 90.0 65.0 90.0
wash 13 10 66.7 58.3 83.3 75.0 75.0 87.5 90.6
work 21 14 43.3 45.0 45.0 41.7 56.7 * *
TOTAL 15.6 10.7 56.3 56.6 57.6 59.6 69.0 71.3 82.0
Table 3: Number of senses and sense groups in training data for each SENSEVAL-2 verb; fine-grained
accuracy of top three competitors (JHU, SMULS, KUNLP) in SENSEVAL-2 English verbs lexical sample
task; fine-grained (MX) and coarse-grained accuracy (MX-c) of maximum entropy system; inter-tagger
agreement for fine-grained senses (ITA) and sense groups (ITA-c). *No inter-tagger agreement figures were
available for ?play? and ?work?.
mum entropy system, and human inter-tagger agree-
ment on fine-grained and coarse-grained senses.
Overall, coarse-grained evaluation using the groups
improved the system?s score by about 10%. This
is consistent with the improvement we found in
inter-tagger agreement for groups over fine-grained
senses (82% instead of 71%). As a base-line, to en-
sure that the improvement did not come simply from
the lower number of tag choices for each verb, we
created random groups. Each verb had the same
number of groups, but with the senses distributed
randomly. We found that these random groups pro-
vided almost no benefit to the inter-annotator agree-
ment figures (74% instead of 71%), confirming the
greater coherence of the manual groupings.
4.3 Analysis of errors
We found that the grouped senses for call substan-
tially improved performance over evaluating with
respect to fine-grained senses; the system achieved
63.6% accuracy with coarse-grained scoring using
the groups, as compared to 47.0% accuracy with
fine-grained scoring. When evaluated against the
fine-grained senses, the system got 35 instances
wrong, but 11 of the ?incorrect? instances were
tagged with senses that were actually in the same
group as the correct sense. This group of senses dif-
fers from others in the ability to take a small clause
as a complement, which is modeled as a feature in
our system. Here we see that the system benefits
from using syntactic features that are linguistically
richer than the features that have been used in the
past.
29% of errors made by the tagger on develop were
due to confusing Sense 1 and Sense 2, which are in
the same group. The two senses describe transitive
verbs that create new entities, characterized as either
?products, or mental or artistic creations: CREATE
(Sense 1)? or ?a new theory of evolution: CREATE
BY MENTAL ACT (Sense 2).? Instances of Sense 1
that were tagged as Sense 2 by the system included:
Researchers said they have developed a genetic en-
gineering technique for creating hybrid plants for
a number of key crops; William Gates and Paul
Allen developed an early language-housekeeper sys-
tem for PCs. Conversely, the following instances of
Sense 2 were tagged as Sense 1 by the tagger: A Pur-
due University team hopes to develop ways to mag-
netically induce cardiac muscle contractions; Kobe
Steel Ltd. adopted Soviet casting technology used it
until it developed its own system. Based on the direct
object of develop, the automatic tagger was hard-
pressed to differentiate between developing a tech-
nique/system (Sense 1) and developing a way/system
(Sense 2).
Analysis of inter-annotator disagreement between
two human annotators doing double-blind tagging
revealed similar confusion between these two senses
of develop; 25% of the human annotator disagree-
ments on develop involved determining which of
these two senses should be applied to phrases like
develop a better way to introduce crystallography
techniques. These instances that were difficult for
the automatic WSD system, were also difficult for
human annotators to differentiate consistently.
These different senses are clearly related, but the
relation is not reflected in their hypernyms, which
emphasize the differences in what is being high-
lighted by each sense, rather than the similarities.
Methods of evaluation that automatically back off
from synset to hypernyms (Lin, 1997) would fail
to credit the system for ?mistagging? an instance
with a closely related sense. Manually created sense
groups, on the other hand, can capture broader, more
underspecified senses which are not explicitly listed
and which do not participate in any of the WordNet
semantic relations.
5 Conclusion
We have demonstrated that our approach to disam-
biguating verb senses using maximum entropy mod-
els to combine as many linguistic knowledge sources
as possible, yields state-of-the-art performance for
English. This may be a language-dependent feature,
as other experiments indicate that additional linguis-
tic pre-processing does not necessarily improve tag-
ging accuracy for languages like Chinese (Dang et
al., 2002).
In examining the instances that proved trouble-
some to both the human taggers and the automatic
system, we found errors that were tied to subtle
sense distinctions which were reconciled by back-
ing off to the more coarse-grained sense groups.
Achieving higher inter-annotator agreement is nec-
essary in order to provide consistent training data
for supervised WSD systems. Lexicographers have
long recognized that many natural occurrences of
polysemous words are embedded in underspecified
contexts and could correspond to more than one spe-
cific sense. Annotators need the option of selecting,
as an alternative to an explicit sense, either a group
of specific senses or a single, broader sense, where
specific meaning nuances are subsumed. Sense
grouping, already present in a limited way in Word-
Net?s verb component, can be guided and enhanced
by the analysis of inter-annotator disagreements and
the development of explicit sense distinction criteria
that such an analysis provides.
6 Acknowledgments
This work has been supported by National Sci-
ence Foundation Grants, NSF-9800658 and NSF-
9910603, and DARPA grant N66001-00-1-8915 at
the University of Pennsylvania. The authors would
also like to thank the anonymous reviewers for their
valuable comments.
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1).
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: A high-
performance learning name-finder. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing, Washington, DC.
Martin Chodorow, Claudia Leacock, and George A.
Miller. 2000. A topical/local classifier for word sense
identification. Computers and the Humanities, 34(1-
2), April. Special Issue on SENSEVAL.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, Madrid, Spain, July.
Hoa Trang Dang, Ching yi Chia, Martha Palmer, and Fu-
Dong Chiou. 2002. Simple features for chinese word
sense disambiguation. In Proceedings of Coling-02,
Taipei, Taiwain.
Christiane Fellbaum, Martha Palmer, Hoa Trang Dang,
Lauren Delfs, and Susanne Wolf. 2001. Manual and
automatic semantic annotation with WordNet. In Pro-
ceedings of the Workshop on WordNet and Other Lex-
ical Resources, Pittsburgh, PA.
A. Kilgarriff and J. Rosenzweig. 2000. Framework and
results for English SENSEVAL. Computers and the
Humanities, 34(1-2), April. Special Issue on SENSE-
VAL.
Dekang Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Proceed-
ings of the 35th Annual Meeting of the ACL, Madrid,
Spain.
Rada Mihalcea and Dan I. Moldovan. 2001. Automatic
generation of a coarse grained WordNet. In Proceed-
ings of the Workshop on WordNet and Other Lexical
Resources, Pittsburgh, PA.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing multiple knowledge sources to disambiguate word
sense: An exemplar-based approach. In Proceedings
of the 34th Annual Meeting of the Association for
Computational Linguistics, Santa Cruz, CA, June.
M. Palmer, Chunghye Han, Fei Xia, Dania Egedi, and
Joseph Rosenzweig. 2000. Constraining lexical selec-
tion across languages using tags. In Anne Abeille and
Owen Rambow, editors, Tree Adjoining Grammars:
formal, computational and linguistic aspects. CSLI,
Palo Alto, CA.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In Proceed-
ings of SENSEVAL-2: Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
Ted Pedersen. 2000. A simple approach to building en-
sembles of naive bayesian classifiers for word sense
disambiguation. In Proceedings of the 1st Meeting
of the North American Chapter of the Association for
Computational Linguistics, Seattle, WA.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proceedings of
the 2nd Meeting of the North American Chapter of the
Association for Computational Linguistics, Pittsburgh,
PA.
Adwait Ratnaparkhi. 1998. Maximum Entropy Models
for Natural Language Ambiguity Resolution. Ph.D.
thesis, University of Pennsylvania.
David Yarowsky. 2000. Hierarchical decision lists for
word sense disambiguation. Computers and the Hu-
manities, 34(1-2), April. Special Issue on SENSE-
VAL.
Different Sense Granularities for Different Applications 
 
 
Martha Palmer, Olga Babko-Malaya, Hoa Trang Dang 
University of Pennsylvania 
{mpalmer/malayao/htd}@linc.cis.upenn.edu 
 
 
Abstract 
This paper describes an hierarchical approach 
to WordNet sense distinctions that provides 
different types of automatic Word Sense Dis-
ambiguation (WSD) systems, which perform 
at varying levels of accuracy.  For tasks where 
fine-grained sense distinctions may not be es-
sential, an accurate coarse-grained WSD sys-
tem may be sufficient. The paper discusses the 
criteria behind the three different levels of 
sense granularity, as well as the machine learn-
ing approach used by the WSD system. 
1 Introduction 
The difficulty of finding consistent criteria for making 
sense distinctions has been thoroughly attested to in the 
literature (Kilgarriff, ?97, Hanks, ?00).  Difficulties have 
been found with truth-theoretical criteria, linguistic crite-
ria and definitional criteria (Sparck-Jones, ?86, Geer-
aerts, ?93).  In spite of the proliferation of dictionaries, 
there is no methodology by which two lexicographers 
working independently are guaranteed to derive the same 
set of distinctions for a given word, with objects and 
events vying for which is the most difficult to character-
ize (Cruse, ?86, Apresjan, ?74, Pustejovsky, ?91, ?95).   
 
On the other hand, accurate Word Sense Disambiguation 
(WSD) could significantly improve the precision of In-
formation Retrieval by ensuring that the senses of verbs 
in the retrieved documents match the sense of the verb in 
the query.  For example, the two queries What do you 
call a successful movie? and Whom do you call for a 
successful movie? submitted to AskJeeves both retrieve 
the same set of documents, even though they are asking 
quite different questions, referencing very different 
senses of call.  The documents retrieved are also not very 
relevant, again because they do not distinguish which 
matches contain relevant senses and which do not. 
 
Tips on Being a Successful Movie Vampire ... I shall 
call the police. 
 
Successful Casting Call & Shoot for ``Clash of Em-
pires'' ... thank everyone for their participation in the 
making of yesterday's movie. 
 
Demme's casting is also highly entertaining, although I 
wouldn't go so far as to call it successful. This  movie's 
resemblance to its predecessor is pretty vague... 
 
VHS Movies: Successful Cold Call Selling: Over 100 
New Ideas, Scripts, and Examples from the Nation's 
Foremost Sales Trainer. 
 
The two senses of call in the two queries can be easily 
distinguished by their differing predicate-argument 
structures.  They are also separate senses in WordNet, 
but WordNet has an additional 26 senses for call, and the 
current best performance of an automatic Word Sense 
Disambiguation system this type of polysemous verb is 
only 60.2% (Dang and Palmer, 2002).  Is it possible that 
sense distinctions that are less fine-grained than Word-
Net?s distinctions could be made more reliably, and 
could still benefit this type of NLP application?   
 
The idea of underspecification as a solution to WSD has 
been proposed in Buitelaar 2000 (among others), who 
pointed out that for some applications, such as document 
categorization, information retrieval, and information 
extraction it may be sufficient to know if a given word 
belongs to a certain class of WordNet senses or under-
specified sense. On the other hand, there is evidence that 
machine translation of languages as diverse as Chinese 
and English will require all of the fine-grained sense 
distinctions that WordNet is capable of providing, and 
even more (Ng, et al2003, Palmer, et. al., to appear).   
 
An hierarchical approach to verb senses, of the type dis-
cussed in this paper, presents obvious advantages for the 
problem of word sense disambiguation. The human an-
notation task is simplified, since there are fewer choices 
at each level and clearer distinctions between them.  The 
automated systems can combine training data from 
closely related senses to overcome the sparse data prob-
lem, and both humans and systems can back off to a 
more coarse-grained choice when fine-grained choices 
prove too difficult. 
 
The approach to verb senses presented in this paper as-
sumes three different levels of sense distinctions: Prop-
Bank Framesets, WordNet groupings, and WordNet 
senses.  In a project for the semantic annotation of predi-
cate-argument structure, PropBank, we have made 
coarse-grained sense distinctions for the 700 most 
polysemous verbs in the Penn TreeBank (Kingsbury and 
Palmer, ?02).  These distinctions are based primarily on 
different subcategorization frames that require different 
argument label annotations. In a separate project, as dis-
cussed in Palmer et al2004, we have grouped 
SENSEVAL-2 verb senses (which came from WordNet 
1.7). These manual groupings were shown to reconcile a 
substantial portion of the manual and automatic tagging 
disagreements, showing that many of these disagree-
ments are fairly subtle (Palmer, et.al., ?04).   
 
The tree levels of sense distinctions form a continuum of 
granularity. Our criterion for the Framesets, being pri-
marily syntactic, is also the most clear cut. These distinc-
tions are based primarily on usages of a verb that have 
different numbers of predicate-arguments, however they 
also separate verb senses on semantic grounds, if these 
senses are not closely related. Sense groupings provide 
an intermediate level of hierarchy, where groups are 
distinguished by more fine-grained criteria.  Both 
Frameset and grouping distinctions can be made consis-
tently by humans and systems (over 90% accuracy for 
Framesets and 82% for groupings) and are surprisingly 
compatible; 95% of our groups map directly onto a sin-
gle PropBank sense.   
 
2  Background 
2.1 Propbank 
PropBank [Kingsbury & Palmer, 2002] is an annotation 
of the Wall Street Journal portion of the Penn Treebank 
II [Marcus, 1994] with dependency structures (or `predi-
cate-argument' structures), using sense tags for highly 
polysemous words and semantic role labels for each de-
pendency. An important goal is to provide consistent 
semantic role labels across different syntactic realiza-
tions of the same verb, as in the window in [ARG0 John] 
broke [ARG1 the window] and [ARG1 The window] broke. 
PropBank can provide frequency counts for (statistical) 
analysis or generation components in a machine transla-
tion system, but provides only a shallow semantic analy-
sis in that the annotation is close to the syntactic 
structure and each verb is its own predicate. 
 
In addition to the annotated corpus, PropBank provides a 
lexicon that lists, for each broad meaning of each anno-
tated verb, its Frameset, i.e., the possible arguments in 
the predicate and their labels and all possible syntactic 
realizations.  The notion of ``meaning'' used is fairly 
coarse-grained, and it is typically motivated from differ-
ing syntactic behavior.  The Frameset alo includes a 
``descriptor'' field for each role which is intended for use 
during annotation and as documentation, but which does 
not have any theoretical standing. The collection of 
Frameset entries for a verb is referred to as the verb's 
frame.  As an example of a PropBank entry, we give the 
frame for the verb leave below.  Currently, there are 
frames for over 3,000 verbs, with a total of just over 
4,300 Framesets described.  Of these 3,000 verb frames, 
only a small percentage 21.8 % (700) have more than 
one Frameset, with less than 100 verbs with 4 or more.  
The process of sense-tagging the PropBank corpus with 
the Frameset tags has just been completed. 
 
The criteria used for the Framesets are primarily syntac-
tic and clear cut. The guiding principle is that two verb 
meanings are distinguished as different framesets if they 
have distinct subcategorization frames. For example, the 
verb ?leave? has 2 framesets with the following frames, 
illustrated by the examples in (1) and (2): 
 
Frameset 1:  move away from 
Arg0:entity leaving 
Arg1:place left 
 
Frameset 2:  give  
Arg0:giver / leaver 
Arg1:thing given 
Arg2:benefactive / given-to 
 
(1) John left the room. 
(2) Mary left her daughter-in-law her pearls in her will 
 
2.2 WordNet  Sense Groupings 
In a separate project, as part of Senseval tagging exer-
cises, we have developed a lexicon with another level of 
coarse-grained distinctions, as described below. 
 
The Senseval-1 workshop (Kilgarriff and Palmer, 2000) 
provided convincing evidence that supervised automatic 
systems can perform word sense disambiguation (WSD) 
satisfactorily, given clear, consistent sense distinctions 
and suitable training data.  However, the Hector lexicon 
that was used as the sense inventory was very small and 
under proprietary constraints, and the question remained 
whether it was possible to have a publicly available, 
broad-coverage lexical resource for English and other 
languages, with the requisite clear, consistent sense dis-
tinctions. 
 
Subsequently, the Senseval-2 (Edmonds and Cotton, 
2001) exercise was run, which included WSD tasks for 
10 languages.  A concerted effort was made to use exist-
ing WordNets as sense inventories because of their 
widespread popularity and availability. Each language 
had a choice between the lexical sample task and the all-
words task.  The most polysemous words in the English 
Lexical Sample task are the 29 verbs, with an average 
polysemy of 16.28 senses using the pre-release version 
of WordNet 1.7.  Double blind annotation by two lin-
guistically trained annotators was performed on corpus 
instances, with a third linguist adjudicating between in-
ter-annotator differences to create the ?Gold Standard.?  
The average inter-annotator agreement rate was only 
71%, which is comparable to the 73% agreement for all 
words in SemCor, with a much lower average polysemy. 
However, a comparison of system performance on words 
of similar polysemy in Senseval-1 and Senseval-2 
showed very little difference in accuracy (Palmer et al, 
submitted).  In spite of the lower inter-annotator agree-
ment figures for Senseval-2, the double blind annotation 
and adjudication provided a reliable enough filter to en-
sure consistently tagged data with WordNet senses.  
Even so, the high polysemy of the WordNet 1.7 entries 
on average poses a challenge for automatic word sense 
disambiguation.  In addition, WordNet only gives a flat 
listing of alternative senses, unlike most standard dic-
tionaries which are more structured and often provide 
hierarchical entries. To address this lack, the verbs were 
grouped by two or more people, with differences being 
reconciled, and the sense groups were used for coarse-
grained scoring of the systems. 
 
The criteria used for groupings included syntactic and 
semantic ones. Syntactic structure performed two dis-
tinct functions in our groupings. Recognizable alterna-
tions with similar corresponding predicate-argument 
structures were often a factor in choosing to group 
senses together, as in the Levin classes and PropBank, 
whereas distinct subcategorization frames were also of-
ten a factor in putting senses in separate groups.  Fur-
thermore, senses were grouped together if they were 
more specialized versions of a general sense.  The se-
mantic criteria for grouping senses separately included 
differences in semantic classes of arguments (abstract 
versus concrete, animal versus human, animacy versus 
inanimacy, different instrument types...), differences in 
the number and type of arguments (often reflected in the 
subcategorization frame as discussed above), differences 
in entailments (whether an argument refers to a created 
entity or a resultant state), differences in the type of 
event (abstract, concrete, mental, emotional...), whether 
there is a specialized subject domain, etc.   
 
Senseval-2 verb inter-annotator disagreements were re-
duced by more than a third when evaluated against the 
groups, from 29% to 18%, and by over half in a separate 
study, from 28% to 12%.    A similar number of random 
groups provided almost no benefit to the inter-annotator 
agreement figures (74% instead of 71%), confirming the 
greater coherence of the manual groupings. 
3 Mapping of Sense Groups to Framesets  
Groupings of senses for Senseval-2, as discussed above, 
use both syntactic and semantic criteria.  Propbank, on 
the other hand, uses mostly syntactic cues to divide verb 
senses into framesets. As a result, framesets are more 
general than sense-groups and usually incorporate sev-
eral sense groups. We have been investigating whether 
or not the groups developed for SENSEVAL-2 can provide 
an intermediate level of hierarchy in between the Prop-
Bank Framesets and the WN 1.7 senses, and our initial 
results are promising.  Based on our existing WN 1.7 
tags and frameset tags of the Senseval2 verbs in the Penn 
TreeBank, 95% of the verb instances map directly from 
sense groups to framesets, with each frameset typically 
corresponding to two or more sense groups, as illustrated 
by the tables 1-4 for the verbs ?serve?, ?leave?, ?pull?, and 
?see?1  below. 
 
As the tables 1-4 illustrate, the criteria used to split the 
Framesets into groups are as follows:  
  
 1) Syntactic Frames. Most verb senses which allow syn-
tactic alternations (such as transitive/inchoative, unspeci-
fied object deletion, etc) are analyzed as one sense 
group. However, in some cases, as illustrated by the verb 
leave, intransitive and transitive uses are distinguished as 
different sense groups: 
 
Group 1: DEPART (Ship leaves at midnight) 
Group 2: LEAVE BEHIND (She left a mess.) 
 
The DEPART sense of the verb can be used transitively if 
the object specifies the place of departure. The LEAVE 
BEHIND sense is more general and allows syntactic varia-
tion as well as different semantic types of NPs. In Prop-
Bank, these groups are unified as one frameset (Frameset 
1 MOVE AWAY FROM). 
                                                          
1 All these verbs have one or more additional framesets, which 
correspond to one group or sense, and therefore are not in-
cluded here 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:   
WN1 (function) 
WN3(contribute to) 
WN12 (answer) 
 
His freedom served him well 
The scandal served to increase his popularity 
Nothing else will serve 
GROUP 2:   
WN2 (do duty) 
WN13 (do military service) 
 
She served in Congress 
She served in Vietnam 
GROUP 5:    
WN7 (devote one?s efforts) 
WN10 (attend to) 
 
She served the art of music 
May I serve you? 
serve 01:  Act, work 
 
Roles: 
Arg0:worker 
Arg1:job,  project 
Arg2:employer 
 
GROUP 3:   
WN4 (be used by) 
WN8 (serve well)                
WN14 (service) 
 
 
The garage served to shelter horses 
Art serves commerce 
Male animals serve the females for breeding 
purposes 
 
Table 1. Frameset  serve 01.
            
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 2:   
WN2 (leave behind)  
 WN12 (be survived by)  
 WN14  (forget) 
 
She left a mess 
He left six children I left my keys 
GROUP 1:   
WN1  (go away) 
 
WN5 (exit, go out) 
WN8 (depart) 
 
The ship leaves at midnight 
Leave the room 
The teenager left home 
GROUP 3:   
WN3 (to act)   
WN7 (result in) 
 
The inflation left them penniless 
Her blood left a stain on the napkin 
SINGLETON  
WN4 (leave behind) 
 
Leave it as is 
leave 02: Move away 
from  
 
Roles:  
Arg0:entity leaving 
Arg1:thing left 
Arg2 :attribute / sec-
ondary predication 
 
SINGLETON  
WN6 (allow for, provide) 
 
Leave lots of time for the trip 
 
Table 2. Frameset leave 02. 
 
 
 
2. Optional Arguments.  In PropBank verbs of manner 
of motion and verbs of directed motion are usually 
grouped into one frameset. For example, one of the 
framesets of the verb pull (TRY) TO CAUSE MOTION 
unifies the following two group senses: 
Group 1: MOVE ALONG (pull a sled) 
Group 2: MOVE INTO A CERTAIN DIRECTION (The van 
pulled up) 
    
Although the frame for the frameset 1 of the verb pull 
has a ?direction? argument, this argument does not 
have to be present (or implied), and verbs with this 
frame can also be understood as verbs of manner of 
motion in PropBank. 
 
3) Syntactic variation of arguments. Syntactic varia-
tion in objects can also be used to distinguish sense 
groups, but are not taken into consideration for distin-
guishing framesets.  Here both noun phrases and sen-
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (draw)  
WN4(apply force)  
WN9 (cause to move) 
WN10 (operate) 
WN13 (hit) 
 
Pull a sled 
Pull the rope 
A declining dollar pulled  down the export figures 
Pull the oars 
Pull the ball 
GROUP 2:   
WN2 (attract) 
WN12 (rip) 
 
The ad pulled in many potential customers 
 Pull the cooked chicken into strips 
GROUP 3:   
WN3 (move) 
WN7 (steer) 
 
The car pulls to the right 
 Pull the car over 
pull.01:  try to cause motion 
 
Roles:  
Arg0:puller 
Arg1:thing pulled 
Arg2: direction or predication 
Arg3:extent, distance moved 
  
 
GROUP 4:   
WN6 (pull out) 
WN15 (extract) 
WN17(take away) 
 
The mugger pulled a knife on his victim 
 Pull weeds 
 Pull the old soup cans from the shelf 
 
Table 3. Frameset pull 01. 
 
 
Frameset  Senseval-2 Groupings Examples from WordNet 
GROUP 1:  
WN1 (perceive by sight)  
WN7 (watch)  
WN19 (observe as if with an eye) 
 WN20 (examine)       
 
Can you the bird? 
See a movie 
The camera saw the burglary 
I must see your passport 
GROUP 3:   
WN3 (witness) 
 WN6 (learn) 
 
I want to see the results 
I see that you have been promoted 
GROUP 4:   
WN5 (consider) 
WN24 (interpret) 
 
I don?t see the situation quite as negatively 
What message do you see in this letter? 
GROUP 5:   
WN8 (determine) 
WN10 (check) 
WN14 (attend) 
 
See whether it works 
See that the curtains are closed 
Could you see about lunch? 
see.01: view  
 
Roles:  
Arg0:viewer 
Arg1:thing viewed 
Arg2:secondary attribute 
 
GROUP 6:   
WN11 (see a professional) 
WN15 (receive as a guest) 
 
You should see a lawyer 
The doctor will see you now 
 
Table 4. Frameset see 01. 
 
tential complements are contained in the same frame-
set.  These could also be distinguished by the type of 
event, a physical perception vs. an abstract or mental 
perception, but these would also not distinguished by 
PropBank. 
   
Group 1: PERCEIVE BY SIGHT (Can you see the bird?) 
Group 5: DETERMINE, CHECK (See whether it works) 
 
4) Semantic classes of arguments. Differences in se-
mantic classes of arguments, such as ANIMACY versus 
INANIMACY, are also not considered for distinguishing 
framesets. The verb serve, for example, has the follow-
ing group senses, the second of which requires an 
ANIMATE agent, which are unified as one frameset in 
PropBank: 
 
Group 1: FUNCTION (His freedom served him well) 
Group 2: WORK (He served in Congress)   
 
Most of the criteria which are used to split Framesets 
into groupings, as the tables above illustrate, are se-
mantic. These distinctions, although more fine-grained 
than Framesets, are still more easily distinguished than 
WordNet senses. 
 
Mismatches between Framesets and groupings usually 
occur for the following two reasons. First, some senses 
can be missing in the PropBank, if they do not occur in 
the corpus.  Second, given that PropBank is an annota-
tion of the Wall Street Journal, it often distinguishes 
obscure financial senses of the verb as separate senses.  
4 Experiments with Automatic WSD  
We have also been investigating the suitability of these 
distinctions for training automatic Word Sense 
Disambiguation systems.  The system that we used to 
tag verbs with their frameset is the same maximum 
entropy system as that of Dang and Palmer (2002), 
including both topical and local features. Topical 
features looked for the presence of keywords occurring 
anywhere in the sentence and any surrounding 
sentences provided as context (usually one or two 
sentences).  The set of keywords is specific to each 
lemma to be disambiguated, and is determined 
automatically from training data so as to minimize the 
entropy of the probability of the senses conditioned on 
the keyword.  
The local features for a verb w in a particular sentence 
tend to look only within the smallest clause containing 
w.  They include collocational features requiring no 
linguistic prepro essing beyond part-of-speech tagging 
(1), syntactic features that capture relations 
between the verb and its complements (2-4), and se-
mantic features that incorporate information about 
noun classes for objects (5-6): 
1) the word w, the part of speech of w, and 
words at positions -2, -1, +1, +2, relative to w 
2) whether or not the sentence is passive 
3) whether there is a subject, direct object, indi-
rect object, or clausal complement (a comple-
ment whose node label is S in the parse tree) 
4) the words (if any) in the positions of subject, 
direct object, indirect object, particle, preposi-
tional complement (and its object) 
5) a Named Entity tag (PERSON, 
ORGANIZATION, LOCATION) for proper 
nouns appearing in (4). 
6) all possible WordNet synsets and hypernyms 
for the nouns appearing in (4). 
The system performed well on the English verbs in 
Senseval-2, achieving an accuracy of 60.2% when tag-
ging verbs with their fine-grained WordNet senses, and 
70.2% when tagging with the more coarse-grained 
sense groups. 
 
 
Verb Framesets Instances Accuracy
call 11 522 0.835 
carry 4 195 0.933 
develop 2 240 0.938 
draw 3 94 0.926 
dress 3 15 0.800 
drive 2 99 0.808 
keep 5 136 0.919 
leave 3 147 0.762 
live 4 125 0.888 
play 5 98 0.806 
pull 6 88 0.784 
see 2 187 0.995 
serve 2 150 0.967 
strike 10 59 0.610 
train 2 17 0.941 
treat 2 51 0.863 
turn 14 141 0.638 
use 2 820 0.988 
wash 2 8 0.875 
work 7 398 0.955 
Table 5.  Frameset tagging results 
 
For frameset tagging, we collected a total of 3590 in-
stances of 20 verbs in the PropBank corpus that had 
been annotated with their framesets.  The verbs all had 
more than one possible frameset and were a subset of 
the ones used for the English lexical sample task of 
Senseval-2.  Local features for frameset taging were 
extracted using the gold-standard part-of-speech tags 
and bracketing of the Penn Treebank.  Table 5 shows 
the number of framesets, the number of instances, and 
the system accuracy for each verb using 10-fold cross-
validation. The overall accuracy of our automatic 
frameset tagging was 90.0%, compared to a baseline 
accuracy of 73.5% if verbs are tagged with their most 
frequent frameset. While the data is only a subset of 
that used in Senseval-2, it is clear that framesets can be 
much more reliably tagged than fine-grained WordNet 
senses and even sense groups. 
Conclusion 
This paper described an hierarchical approach to 
WordNet sense distinctions that provided different 
types of automatic Word Sense Disambiguation (WSD) 
systems, which perform at varying levels of accuracy. 
We have described three different levels of sense 
granularity, with PropBank Framesets being the most 
syntactic, the most coarse-grained, and most easily 
reproduced.  A set of manual groupings devised for 
Senseval2 provides a middle level of granularity that 
mediates between Framesets and WordNet.   For tasks 
where fine-grained sense distinctions may not be essen-
tial such as an AskJeeves information retrieval task, an 
accurate coarse-grained WSD system such as our 
Frameset tagger may be sufficient. There is evidence, 
however, that machine translation of languages as di-
verse as Chinese and English might require all of the 
fine-grained sense distinctions of WordNet, and even 
more (Ng, et al2003, Palmer, et. al., to appear).   
References 
 
Apresjan, J. D. .(1974) Regular polysemy, Linguistics, 
142:5?32. 
 
Atkins, S. (1993) Tools for computer-aided corpus 
lexicography: The Hector Project.  Actu Linguis-
tica Hunguricu, 41:5-72. 
 
Buitelaar, P.P (2000). Reducing Lexical Semantic 
Complexity with Systematic Polysemous Classes 
and Underspecification. In Poceedings of the 
ANLP Workshop on Syntactic and Semantic Com-
plexity in NLP Systems. Seattle, WA. 
 
Cruse, D. A., (1986), Lexical Semantics, Cambridge 
University Press, Cambridge, UK, 1986. 
 
Dang, H. T. and Palmer, M., (2002). Combining Con-
textual Features for Word Sense Disambiguation. 
In Proceedings of the Workshop on Word Sense 
Disambiguation: Recent Successes and Future Di-
rections, Philadelphia, Pa. 
 
Edmonds, P. and Cotton, S. (2001). SENSEVAL-2: 
Overview. In Proceedings of SENSEVAL-2: Sec-
ond International Workshop on Evaluating Word 
Sense Disambiguation Systems, ACL-SIGLEX, 
Toulouse, France. 
 
Hanks, P., (2000), Do word meanings exist? Com-
puters and the Humanities, Special Issue on 
SENSEVAL, 34(1-2). 
 
Geeraerts, D., (1993), Vagueness's puzzles, polysemy's 
vagaries, Cognitive Linguistics, 4. 
 
Kilgarriff, A., (1997), I don't believe in word senses, 
Computers and the Humanities, 31(2). 
 
Kilgarriff, A. and Palmer, M., (2000), Introduction to 
the special issue on Senseval, Computers and the 
Humanities, 34(1-2):1-13. 
 
Kingsbury, P., and Palmer, M, (2002), From TreeBank 
to PropBank, Third International Conference on 
Language  Resources and Evaluation, LREC-02, 
Las Palmas, Canary Islands, Spain, May 28- June 
3. 
 
Marcus, M, (1994), The Penn TreeBank: A revised 
corpus design for extracting predicate argument 
structure, In Proceedings of the ARPA Human 
Language Technology Workshop, Princeton, NJ.   
 
Ng, H. T., & Wang, B., & Chan, Y. S. (2003). 
Exploiting Parallel Texts for Word Sense Disam-
biguation: An Empirical Study. In the Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL-03). Sapporo, 
Japan, July. 
Palmer, M., Dang, H. T., and Fellbaum, C., (to appear, 
2004), Making fine-grained and coarse-grained 
sense distinctions, both manually and automati-
cally, under revision for Natural Language Engi-
neering. 
Pustejovsky, J. (1991) The Generative Lexicon,  in 
Computational Linguistics 17(4).   
Pustejovsky, J. (1995) The Generative Lexicon, Cam-
bridge, MIT Press, Mass. 
 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145?152
Manchester, August 2008
Mind the Gap: Dangers of Divorcing Evaluations of Summary Content
from Linguistic Quality
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland, USA
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, Maryland, USA
hoa.dang@nist.gov
Abstract
In this paper, we analyze the state of cur-
rent human and automatic evaluation of
topic-focused summarization in the Docu-
ment Understanding Conference main task
for 2005-2007. The analyses show that
while ROUGE has very strong correlation
with responsiveness for both human and
automatic summaries, there is a signifi-
cant gap in responsiveness between hu-
mans and systems which is not accounted
for by the ROUGE metrics. In addition
to teasing out gaps in the current auto-
matic evaluation, we propose a method
to maximize the strength of current auto-
matic evaluations by using the method of
canonical correlation. We apply this new
evaluation method, which we call ROSE
(ROUGE Optimal Summarization Evalua-
tion), to find the optimal linear combina-
tion of ROUGE scores to maximize corre-
lation with human responsiveness.
1 Introduction
ROUGE (Lin, 2004) and its linguistically-
motivated descendent, Basic Elements (BE) (Hovy
et al, 2005), evaluate a summary by computing its
overlap with a set of model (human) summaries;
ROUGE considers lexical n-grams as the unit
for comparing the overlap between summaries,
while Basic Elements uses larger units of com-
parison based on the output of syntactic parsers.
The ROUGE/BE toolkit has become the standard
automatic method for evaluating the content of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
machine-generated summaries, but the correlation
of these automatic scores with human evaluation
metrics has not always been consistent.
In this paper, we analyze the state of current
human and automatic evaluation of topic-focused
summarization. Using the results of the Document
Understanding Conference main task for 2005-
2007 we explore the correlation between variants
of ROUGE and the human metrics of responsive-
ness and linguistic quality. The analyses expose
a number of challenges and several surprising re-
sults. In particular, while ROUGE has very strong
correlation with responsiveness for both human
and system summaries, there is a significant gap
in responsiveness between humans and systems
which is not accounted for by the ROUGE metrics.
One cause of the gap is that many automatic sum-
marizers truncate the last sentence of their sum-
mary, which shows significant reduction in the re-
sponsiveness score but does not result in a statis-
tically significant drop in ROUGE scores. In ad-
dition to teasing out gaps in the current automatic
evaluation, we propose a method to maximize the
strength of current automatic evaluations by us-
ing the method of canonical correlation. We apply
this new evaluation method, which we call ROSE
(ROUGE Optimal Summarization Evaluation), to
find the optimal linear combination of ROUGE
metrics to maximize correlation with human re-
sponsiveness.
2 DUC 2005-2007 Task and Evaluation
The main task for DUC 2005-2007 was a com-
plex question-focused summarization task that re-
quired summarizers to piece together information
from multiple documents to answer a question or
set of questions as posed in a DUC topic state-
ment. The topic statement was a request for infor-
145
mation that could not be met by just stating a name,
date, quantity, etc. The summarization task was the
same for both human and automatic summarizers:
Given a topic statement and a set of 25-50 rele-
vant newswire documents, the summarization task
was to create from the documents a brief, well-
organized, fluent summary that answered the need
for information expressed in the topic statement.
The summary could be no longer than 250 words.
Summaries over the size limit were truncated, and
no bonus was given for creating a shorter sum-
mary.
NIST Assessors developed the DUC topics used
as test data. There were 50 DUC topics each year
in 2005-2006, and 45 topics in DUC 2007. Each
year, 10 NIST assessors produced a total of 4 hu-
man summaries for each of the topics. The asses-
sor who developed a particular topic always wrote
one of the 4 summaries for that topic.
NIST manually assessed each summary for both
content and readability. Readability was assessed
using a set of linguistic quality questions; sum-
mary content was assessed using the pseudo-
extrinsic measure of content responsiveness.
All summaries for a given topic were judged by
a single assessor who was usually the same as the
topic developer. In all cases, the assessor was one
of the summarizers for the topic. Assessors first
judged each summary for a topic for readability,
assigning a separate score for each of 5 linguis-
tic qualities; each summary for the topic was then
judged for content responsiveness. Each of these
manual evaluations was based on a five-point scale
(1=very poor, 5=very good), resulting in 6 scores
for each summary.
2.1 Evaluation of Readability
The readability of the summaries was assessed us-
ing five linguistic quality questions which mea-
sured qualities of the summary that did not involve
comparison with a reference summary or DUC
topic. The linguistic qualities measured were Q1:
Grammaticality, Q2: Non-redundancy, Q3: Refer-
ential clarity, Q4: Focus, and Q5: Structure and
coherence.
2.2 Evaluation of Content
NIST performed manual pseudo-extrinsic evalu-
ation of peer summaries in the form of assess-
ment of responsiveness. Responsiveness differs
from other measures of summary content such as
SEE coverage (Lin and Hovy, 2002) and Pyramid
scores (Nenkova and Passonneau, 2004) in that it
does not compare a peer summary against a set of
known human summaries. Rather, the assessor is
given a list of randomly ordered, unlabeled sum-
maries (both human and system-generated) for a
topic, and must assign a responsiveness score to
each summary (after having read all the summaries
first).
In DUC 2005-2007, NIST assessors assigned
a content responsiveness score to each summary;
content responsiveness indicated the amount of in-
formation in the summary that helped to satisfy the
information need expressed in the topic statement.
For content responsiveness, the linguistic quality
of the summary was to play a role in the assess-
ment only insofar as it interfered with the expres-
sion of information and reduced the amount of in-
formation that was conveyed.
In DUC 2006, assessors assigned an additional
overall responsiveness score, which was based on
both information content and readability. Asses-
sors judged overall responsiveness only after judg-
ing all their topics for readability and content re-
sponsiveness; however, they were not given di-
rect access to these previously assigned scores, but
were told to give their ?gut? reaction to the overall
responsiveness of each summary.
The content responsiveness score provides a
coarse manual measure of information coverage;
overall responsiveness reflects a combination of
readability and content. Content responsiveness
was largely responsible for determining how as-
sessors perceived the overall quality of a sum-
mary, but readability also played an important role.
While poor readability could downgrade the over-
all responsiveness of a summary that had very
good content responsiveness, very good readabil-
ity could sometimes bolster the overall responsive-
ness score of a less information-laden summary
(Dang, 2006). Attempts at greater readability in
2006 paid off among the peers with the best over-
all responsiveness scores. However, the automatic
peers generally had poor readability, and the aver-
age overall responsiveness for each peer was gen-
erally much lower than its average content respon-
siveness.
In addition to the human assessment of respon-
siveness, NIST computed three ?official? auto-
matic scores using ROUGE and Basic Elements:
ROUGE-2, ROUGE-SU4, and ROUGE-BE recall.
For the BE evaluation, summaries were parsed
146
withMinipar (Lin, 2005), and BE-F were extracted
and matched using the Head-Modifier criterion.
Jackknifing was used for each [peer, topic] pair
so that human and automatic peers could be com-
pared.
3 An Analysis of the Metrics
Figure 1 shows the average scores for each sum-
marizer for DUC 2005, 2006, and 2007. For
each year we report the Pearson correlation coeffi-
cient for ROUGE-2, ROUGE-SU4, and ROUGE-
BE (denoted ?
R2
, ?
SU4
and ?
BE
), against content
responsiveness. This correlation is computed in-
cluding just the systems as the human summariz-
ers are clearly distributed differently.
1
To high-
light the trend in the correlation we fit the systems
data using robust linear regression. This line could
be used to extrapolate the system performance if
ROUGE scores were to increase.
As seen in Figure 1, while both the manual
and the automatic ROUGE scores of the human
summarizers remained relatively constant over the
years, the systems made significant progress in
their automatic scores, with the top systems per-
forming within statistical confidence of the human
summarizers in the ROUGE metrics as reported by
Conroy et al (2007). While the content respon-
siveness scores of the systems also increased as a
group over the years, all systems performed signif-
icantly worse than humans in content responsive-
ness as measured by Tukey?s honestly significant
difference criterion (Conroy et al, 2007). Thus,
there is not only a gap in performance between
humans and systems on this task as measured
manually by content responsiveness, but there is
also a ?metric gap? in using any single variant of
ROUGE to predict content responsiveness. This
metric gap becomes more pronounced as system
performance improves to the point where ROUGE
is unable to distinguish between systems and hu-
mans.
We turn next to an analysis of sources of the per-
formance gap and ?metric gap?. Responsiveness
is a subjective measure, and because NIST uses
the same humans both to generate abstracts and to
evaluate the abstracts, there is the possibility that
humans may give high scores to their own abstract
1
One system each year in 2005-2007 had formatting prob-
lems in their summaries which resulted in abnormally low
ROUGE-BE scores. While these systems are included in the
scatter plot, they are not included in the correlation coefficient
computation.
Figure 1: Scatter plot of average manual con-
tent responsiveness vs. automatic ROUGE scores
(ROUGE-BE, ROUGE-2, and ROUGE-SU4) for
humans (filled points) and systems (unfilled
points), for DUC 2005-2007.
147
just because it not surprisingly ?says what they
would say.? To test this hypothesis we performed
one-side Student T?test, testing if the group of
?Self-Assessed? abstracts had significantly higher
responsiveness for DUC 2005-2007. Indeed, as
Table 1 shows in each year and for both content
and overall responsiveness, humans gave signifi-
cantly higher scores to their own abstracts than the
other human abstracts. This bias adds to the gap
in content responsiveness between the human and
automatic summarizers. Fortunately, this effect is
dampened by the fact that NIST used 10 asses-
sors and on average a human got to assess their
own abstract only 25% of the time. It is notewor-
thy to add that at the Multi-lingual Summarization
Evaluation of 2006, the human assessors were not
the abstractors. This and other factors, notably an
easier task, lead to there being no gap in perfor-
mance between the human and the top scoring sys-
tem (Schlesinger et al, 2008).
Table 1: Mean responsiveness assessment by hu-
mans for their own (Self) vs. Other abstracts.
Data Self Other Signif
DUC 2005 Content 4.88 4.61 0.00277
DUC 2006 Content 4.96 4.68 0.00052
DUC 2006 Overall 4.94 4.67 0.00326
DUC 2007 Content 4.87 4.65 0.01931
We next examine the correlation between re-
sponsiveness and each of the five (manual) met-
rics for linguistic quality. We divide the correlation
into three groups: Human (the group of 10 human
summarizers), Systems (the automatic systems en-
tered into DUC), and Combined (the union of these
two groups). Table 2 gives the Pearson correlation
coefficient and the p?value of statistical signifi-
cance between content responsiveness and each of
the five linguistic quality questions for DUC 2005-
2007. For DUC 2005 there is no significant corre-
lation between the average score of a human or au-
tomatic summarizer on linguistic questions and the
content responsiveness score. The fact that there is
a significant correlation in the ?Combined? case is
primarily due to the fact that the human summa-
rizers scored higher as a group than the systems in
the content metric as well as the linguistic metrics.
In DUC 2006 and 2007, the linguistic question
which rewards summaries for not having redun-
dancy (Q2) has a significant negative correlation
with content responsiveness in the group of sys-
tems. This negative correlation is due largely to the
fact that a number of low scoring systems (includ-
ing the baseline) have no significant redundancy.
Rarely does any system have sentences which are
near duplicates. However, many systems, even
those with relatively high responsiveness scores,
still suffer from clause level redundancy, much of
it in the form of noun phrases for which a human
summarizer would employ pronouns.
Table 3 gives additional correlations between
overall responsiveness and the linguistic questions
for DUC 2006. We contrast the correlations for
DUC 2006 in Table 2 vs. those in Table 3. Not sur-
prisingly, overall responsiveness, which intention-
ally penalizes summaries for linguistic problems,
does correlate more strongly with the linguistic
questions than content responsiveness. Also, we
note that the DUC 2007 correlations for content
responsiveness appear more like those for DUC
2006 overall responsiveness than the correspond-
ing correlations for DUC 2006 content responsive-
ness. NIST did not have sufficient time in 2007
to perform an overall responsiveness evaluation.
We hypothesize that the assessors, many of whom
worked on DUC 2006, may have inadvertently
taken linguistic quality into account more in 2007
than in 2006 for the content responsiveness, since
only one measure was done in 2007.
Finally, it was hypothesized at the DUC 2006
workshop
2
that the human assessors penalize sys-
tems in content responsiveness which end with a
sentence fragment, more than could be accounted
for by the missing content of the sentence frag-
ment. We tested the hypothesis by comparing
the average grammaticality (Q1), content respon-
siveness, and ROUGE scores of the 15 systems
in DUC 2007 that ended their summaries with a
complete sentence, against the 17 systems whose
summaries ended with a sentence fragment. Ta-
ble 4 gives a summary of the results. As measured
by a Student T?test, systems that ended their
summaries with a complete sentence had signif-
icantly higher content responsiveness scores than
those that did not; however, there was no signifi-
cant difference in ROUGE scores. The table lists
ROUGE-2 as an example; these results are consis-
tent with both ROUGE-BE and ROUGE-SU4.
Because linguistic quality clearly influences
content responsiveness, automatic methods of
evaluating summary content that try to maximize
2
Lucy Vanderwende, personal communication
148
Table 2: Correlation and p-values between Content Responsiveness and Linguistic Quality Questions,
DUC 2005-2007
Year Group Q1 Q2 Q3 Q4 Q5
Grammar Non-redund. Refer. Clarity Focus Structure/Coherence
2005 Humans -0.10( 0.78) 0.03( 0.94) 0.06( 0.87) 0.23( 0.53) 0.31( 0.39)
2005 Systems -0.05( 0.79) 0.15( 0.42) 0.19( 0.29) 0.30( 0.10) 0.08( 0.66)
2005 Combined 0.72( 0.00) 0.75( 0.00) 0.87( 0.00) 0.90( 0.00) 0.91( 0.00)
2006 Humans 0.26( 0.47) 0.15( 0.69) 0.04( 0.91) 0.64( 0.05) 0.40( 0.26)
2006 Systems 0.33( 0.05) -0.38( 0.03) 0.27( 0.11) 0.41( 0.01) 0.16( 0.35)
2006 Combined 0.74( 0.00) 0.68( 0.00) 0.86( 0.00) 0.87( 0.00) 0.89( 0.00)
2007 Humans 0.80( 0.01) 0.73( 0.02) 0.24( 0.51) 0.57( 0.09) 0.47( 0.17)
2007 Systems 0.60( 0.00) -0.43( 0.01) 0.59( 0.00) 0.71( 0.00) 0.49( 0.00)
2007 Combined 0.77( 0.00) 0.72( 0.00) 0.85( 0.00) 0.92( 0.00) 0.90( 0.00)
Table 3: Correlation and p-values between Overall Responsiveness and Linguistic Quality Questions,
DUC 2006
Group Q1 Q2 Q3 Q4 Q5
Grammar Non-redund. Refer. Clarity Focus Structure/Coherence
Humans 0.60( 0.06) 0.27( 0.45) 0.39( 0.26) 0.74( 0.01) 0.82( 0.00)
Systems 0.49( 0.00) -0.23( 0.19) 0.55( 0.00) 0.64( 0.00) 0.49( 0.00)
Combined 0.77( 0.00) 0.72( 0.00) 0.89( 0.00) 0.89( 0.00) 0.93( 0.00)
Table 4: Average scores of DUC 2007 systems
ending with a complete sentence vs. those ending
with a fragment.
Metric Sentence Fragment Signif
Grammaticality 3.88 3.24 0.011
Content Resp. 2.79 2.46 0.021
ROUGE-2 0.098 0.092 0.408
correlation with content responsiveness should at-
tempt to include some measures of linguistic qual-
ity. We hypothesize that different variants of
ROUGE may capture different qualities of a sum-
mary; for example, ROUGE-1 may be a good in-
dicator of the relevance of summary content, but
ROUGE variants that take into account larger con-
texts may capture linguistic qualities of the sum-
mary. Hence, a combination of scores (includ-
ing measures of linguistic quality) would be a bet-
ter predictor of ?content? responsiveness.
3
In the
3
An additional weakness in the automatic metrics, which
we do not attempt to address in our current work, is their in-
ability to adequately handle the generalizations that are often
made in model summaries (Dang, 2006), which are abstrac-
tive as opposed to the extractive summaries of most systems.
next section, we present a new evaluation metric
that finds a linear combination of ROUGE met-
rics which, in general, has stronger correlation
with content responsiveness than any of the cur-
rent ROUGE metrics.
4 ROSE: Un Melange de ROUGEs
We developed an automatic content evaluation
model which combines multiple ROUGE scores
using canonical correlation (Hotelling, 1935).
Canonical correlation finds the linear combination
of ROUGE scores that has maximum correlation
with human responsiveness on a given data set.
As this family of models is a ?blend? of ROUGE
scores we call this metric ROSE, for ROUGE Op-
timal Summarization Evaluation. We first apply
canonical correlation for each year of DUC using
a Monte Carlo method. We then report on pre-
liminary experiments that use ROSE models from
one year to predict content responsiveness in sub-
sequent years.
4.1 Blending ROUGE Scoring with a
Canonical Correlation Model
Suppose we are given a set of ROUGE scores and
the corresponding content responsiveness scores.
149
We let a
ij
, for i = 1, ...,m and j = 1, .., n, be the
ROUGE score of type j for the summarizer i, and
b
i
the human content evaluation metric. Canonical
correlation finds an n?long vector x such that
x = argmax ?(
n
?
j=1
a
ij
x
j
, b
i
), (1)
where ?(x, y) is the Pearson correlation between
x and y. A similar approach has been used by Liu
and Gildea (2007) in the application of machine
translation metrics, where they use a gradient opti-
mization method to solve the maximization prob-
lem.
Canonical correlation actually solves a more
general correlation optimization problem, where
the goal is to find two linear combinations of vari-
ables to maximize the correlation between two
sub-spaces. In the application of document sum-
marization, we may wish to consider a matrix B of
human evaluation metrics where b
ij
is the j?th hu-
man evaluation for the i?th summarizer. We could
include, for example, content and overall respon-
siveness or linguistic questions. Here we solve for
(x, y) in the equation below:
(x, y) = argmax ?(
n
?
j=1
a
ij
x
j
,
k
?
j=1
b
ij
y
j
). (2)
This maximization procedure can be solved via a
generalized eigenvalue problem, which we com-
puted in Matlab using a routine distributed by
Borga (2000). For the case studied here, as given
in Equation (1), the generalized eigenvalue reduces
to a linear least squares problem.
To find strong canonical correlations we decided
to explore a large space of metrics. To this end, we
included in our optimization 7 ROUGE automatic
metrics: ROUGE-1,2,3,4,L,SU4, and BE to pre-
dict content responsiveness and (for DUC 2006)
overall responsiveness. As our analyses of the pre-
vious section indicated for DUC 2006 and 2007
there was a significant correlation between the lin-
guistic questions and content responsiveness. We
add questions 1 and 4 to our canonical correla-
tion model to see to what extent these questions
could improve the correlation with content respon-
siveness. While the linguistic questions evaluation
scores are manually generated we combine them
with the automatic methods of ROUGE in an at-
tempt see to what extent these non-content scores
can better model both content and overall respon-
siveness. Thus, in all we consider 9 variables to
predict responsiveness. In order to perform an
evaluation that would avoid over-fitting the data
we used a Monte Carlo method of resampling to
evaluate which of the 2
9
? 1 = 511 combinations
of variables (canonical variates) to include in the
model.
4
In each experiment of the Monte Carlo method
we randomly held back 1/4 of the data (human and
system summarizers) for testing and used 3/4 of
the data to build a canonical variate model. We
found 4000 random samples sufficient to achieve
accuracy within at least 2 digits. For each of
the canonical variate models, 4000 trials are per-
formed and then the computed model is applied
to the held-back portion of the data and its Pear-
son correlation and p-value is reported. These
4000 correlations (and p-values) are then used to
estimate the median correlation for a canonical
variate. The median is computed from the sub-
set of 4000 experiments with statistically signif-
icant correlations on the testing data (95% con-
fidence, a p-value less then 0.05). The canoni-
cal variate with the highest estimated median cor-
relation is then compared with the best perform-
ing ROUGE method. We compare the best of
504=511-7 canonical variates with the best of the
7 ROUGE variants by using the Mann-Whitney U-
test, which tests for equal medians.
The procedure is then repeated using only the
systems to find the ROSE model that gives the best
prediction for just machine summarizers.
Table 5 gives the results of the Monte Carlo ex-
periments. In each case the best canonical variate
and the estimated median correlation are reported
over the set of ROUGE scores and the ROUGE
scores in union with the linguistic questions. As
these results are based on 4000 trials they are more
reliable than the simple correlation analysis done
using the three official DUC automatic metrics,
ROUGE-2, SU4, and BE. We note, in particular,
that occasionally ROUGE-1 and ROUGE-L were
found to be the best predictor even when linguistic
questions were allowed in the model. Not surpris-
ingly, the human evaluation of overall responsive-
ness was harder to predict and the optimal variants
included both linguistic questions 1 and 4.
The ROSE models give the best combinations
4
We also removed one system each year that had a poor
ROUGE-BE score due to formatting problems.
150
Table 5: Monte Carlo Results for Canonical Correlation Model. A * by a variant indicates that it differs
significantly from the best single ROUGE correlation with a p-value of 10
?7
or less as measured by a
Mann Whitney U-test.
Year Metric Summarizer Best ROUGE Corr. ROSE
ROUGE
Corr. ROSE
(ROUGE,Q)
Corr.
2005 Content All BE 0.976 R1,R2,R4,SU4,BE* 0.981 R1,R2,R3,RL,SU4,BE,Q4* 0.986
2005 Content Systems R2 0.939 R1,R2,RL 0.940 R2,RL,SU4,Q4 0.941
2006 Content All RL 0.928 R1,R2,R3,R4* 0.942 RL,Q1* 0.960
2006 Content Systems R1 0.900 R1 0.900 R1 0.900
2007 Content All BE 0.937 R1,R4,RL,BE 0.940 BE,Q4* 0.966
2007 Content Systems R3 0.906 RL,BE* 0.915 R1,RL,BE,Q1* 0.929
2006 Overall All BE 0.893 R1,R2,R3,R4* 0.913 R3,R4,Q1,Q4* 0.946
2006 Overall Systems RL 0.854 RL 0.854 R1,R3,SU4,Q1,Q4* 0.894
of ROUGE scores to give maximum correlation
with the human judgement of content or overall
responsiveness. The ROSE models based on just
ROUGE for the automatic summarizers are an ap-
propriate method to use to compare systems that
did not compete in DUC with those that did.
4.2 Applying ROSE across the Years
To further evaluate the generality of the ROSE
model we apply DUC 2005 canonical correlation
models to DUC 2006 and DUC 2007, and simi-
larly apply the DUC 2006 model to the DUC 2007
data. In these experiments we measure the stabil-
ity of a ROSE model from one year to the next.
(Note, we have also computed a model based on
the combined data of DUC 2005 and DUC 2006
for use with DUC 2007 and these results are com-
parable to those presented.) Here, for simplicity,
we restrict the ROSE model to use only the ?offi-
cial? ROUGE metrics to build a model based on a
given year and then evaluate that model on a subse-
quent year. Table 6 gives results for ROSE models
constructed from only ROUGE-2, ROUGE-SU4,
ROUGE-BE, and content responsiveness to create
the ROSE model for each year; results are also
given for ROSE models (ROSE
+Q1,4
) which also
includes the linguistic questions on grammaticality
(Q1) and focus (Q4).
The ROSE models built from only ROUGE
scores had mixed results, sometimes performing
worse than a single ROUGE score (e.g., the ROSE
model trained on DUC 2005 and evaluated on
DUC 2006), but in other cases performing as well
as or better than single ROUGE scores. These
preliminary results with ROSE illustrate the diffi-
culty in finding a single canonical variate that can
be used from year to year to build ROSE mod-
els based on previous years? data. We hypothesize
that the task is made more difficult due to humans
changing their criteria for judging content respon-
siveness over the years.
On the other hand, ROSE
+Q1,4
models that in-
cluded the linguistic questions Q1 and Q4 always
yielded the best correlation with content respon-
siveness both for the systems and for the group of
combined systems and human summarizers.
5 Conclusions
We analyzed the results of the topic-focused sum-
marization task using the data from DUC 2005-
2007. Our main concern was to expose causes of
the gap that currently exists between automatic and
human evaluation of summary content. As the au-
tomatic ROUGE scores of system summaries ap-
proaches that of human summaries, the disparity
between automatic and manual measures of sum-
mary content becomes a more important concern.
We find that there is a slight bias in the human eval-
uation: humans give their own summaries signifi-
cantly higher scores. Furthermore, the responsive-
ness metric appears to be time varying, i.e., the hu-
mans changed their standards for judging respon-
siveness over the years, making it difficult to use
automatic scores from one year to predict respon-
siveness in another year.
Assessors naturally tend toward taking linguis-
tic quality into account when assessing summaries.
The instructions for assessing content responsive-
ness implicitly acknowledges this; what is surpris-
ing is the extent to which linguistic quality does
influence content responsiveness. In particular, we
demonstrated that content responsiveness in DUC
2006 and 2007 correlated with the linguistic qual-
ity questions of grammar (Q1) and focus (Q4),
and that systems were significantly penalized in
content responsiveness when their summary ended
151
Table 6: Correlation and p-values between content responsiveness and various metrics for each ?Test?
year of DUC. ROSE models were constructed using DUC data from ?Train? year and evaluated on data
from ?Test? year.
Train/Test Summarizer R2 SU4 BE Q1 Q4 ROSE ROSE
+Q1,4
2005/2006 Humans 0.64(0.05) 0.69(0.03) 0.57(0.09) 0.26(0.47) 0.64(0.05) 0.59 (0.07) 0.61(0.06)
2005/2006 Systems 0.83(0.00) 0.85(0.00) 0.85(0.00) 0.33(0.06) 0.41(0.02) 0.83 (0.00) 0.85(0.00)
2005/2006 All 0.90(0.00) 0.88(0.00) 0.90(0.00) 0.74(0.00) 0.87(0.00) 0.90 (0.00) 0.93(0.00)
2005/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.53 (0.12) 0.57(0.09)
2005/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.90 (0.00) 0.92(0.00)
2005/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92 (0.00) 0.94(0.00)
2006/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.52(0.12) 0.67(0.03)
2006/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.89 (0.00) 0.90(0.00)
2006/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92( 0.00) 0.96(0.00)
with a sentence fragment even though the auto-
matic content measures did not show a statisti-
cally significant difference. The influence of lin-
guistic quality on ?content? responsiveness con-
tributes to the evaluation gap that we see between
ROUGE/BE and this coarse human measure of
summary content.
Automatic methods of evaluating summary con-
tent that try to maximize correlation with content
responsiveness should therefore attempt to include
some measures of linguistic quality. We found that
a blending of ROUGE scores using canonical cor-
relation gave higher correlations with content and
overall responsiveness. When the linguistic ques-
tions Q1 and Q4 were added to the ROSE model,
correlations of up to 0.96 were observed. This re-
sult leads to a natural question: What automatic
methods could be used to approximate the linguis-
tic questions? The work of Barzilay and Lapata
(2005) on local coherence might be a possible can-
didate for estimating focus (Q4), while an auto-
matic parser could be run on the summaries and
the induced score could be used as a surrogate for
grammaticality (Q1).
References
Barzilay, Regina and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
141?148, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Borga, Magnus. 2000. Matlab function cca().
Conroy, John M., Judith D. Schlesinger, and Dianne P.
O?Leary. 2007. CLASSY 2007 at DUC 2007. In
Proceedings of the Seventh Document Understand-
ing Conference (DUC), Rochester, New York.
Dang, Hoa Trang. 2006. Overview of DUC 2006. In
Proceedings of the Sixth Document Understanding
Conference (DUC), New York City, New York.
Hotelling, H. 1935. The most predictable criterion.
Journal of Educational Psychology, 26:139?142.
Hovy, Eduard, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating duc 2005 using Basic Elements. In Pro-
ceedings of the Fifth Document Understanding Con-
ference (DUC), Vancouver, Canada.
Lin, Chin-Yew and Eduard Hovy. 2002. Manual and
automatic evaluation of summaries. In Proceedings
of the ACL-02 Workshop on Automatic Summariza-
tion, Philadelphia, PA.
Lin, Chin-Yew. 2004. ROUGE: A package for
automatic evaluation of summaries. In Proceed-
ings of the ACL-04 Workshop: Text Summarization
Branches Out, pages 74?81, Barcelona, Spain.
Lin, Dekang. 2005. A dependency-base method for
evaluating broad-coverage parsers. In Proceedings
of the Nineteenth International Joint Conference on
Artificial Intelligence (IJCAI), Edinburgh, Scotland.
Liu, Ding and Daniel Gildea. 2007. Source-language
features and maximum correlation training for ma-
chine translation evaluation. In Proceedings of the
2007 Meeting of the North American chapter of the
Association for Computational Linguistics (NAACL-
07).
Nenkova, Ani and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 145?152, Boston, MA.
Schlesinger, Judith D., Dianne P. O?Leary, and John M.
Conroy. 2008. Arabic/English multi-document
summarization with CLASSY?the past and the fu-
ture. In Conference on Intelligent Text Processing
and Computational Linguistics 2008. Lecture Notes
in Computer Science, Springer-Verlag. to appear.
152
Proceedings of the 43rd Annual Meeting of the ACL, pages 42?49,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Role of Semantic Roles in Disambiguating Verb Senses
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
Martha Palmer
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
mpalmer@cis.upenn.edu
Abstract
We describe an automatic Word Sense
Disambiguation (WSD) system that dis-
ambiguates verb senses using syntactic
and semantic features that encode infor-
mation about predicate arguments and se-
mantic classes. Our system performs at
the best published accuracy on the English
verbs of Senseval-2. We also experiment
with using the gold-standard predicate-
argument labels from PropBank for dis-
ambiguating fine-grained WordNet senses
and course-grained PropBank framesets,
and show that disambiguation of verb
senses can be further improved with bet-
ter extraction of semantic roles.
1 Introduction
A word can have different meanings depending
on the context in which it is used. Word Sense
Disambiguation (WSD) is the task of determining
the correct meaning (?sense?) of a word in con-
text, and several efforts have been made to develop
automatic WSD systems. Early work on WSD
(Yarowsky, 1995) was successful for easily distin-
guishable homonyms like bank, which have multi-
ple unrelated meanings. While homonyms are fairly
tractable, highly polysemous verbs, which have re-
lated but subtly distinct senses, pose the greatest
challenge for WSD systems (Palmer et al, 2001).
Verbs are syntactically complex, and their syntax
is thought to be determined by their underlying se-
mantics (Grimshaw, 1990; Levin, 1993). Levin verb
classes, for example, are based on the ability of a
verb to occur in pairs of syntactic frames (diathe-
sis alternations); different senses of a verb belong to
different verb classes, which have different sets of
syntactic frames that are supposed to reflect under-
lying semantic components that constrain allowable
arguments. If this is true, then the correct sense of
a verb should be revealed (at least partially) in its
arguments.
In this paper we show that the performance of
automatic WSD systems can be improved by us-
ing richer linguistic features that capture informa-
tion about predicate arguments and their semantic
classes. We describe our approach to automatic
WSD of verbs using maximum entropy models to
combine information from lexical collocations, syn-
tax, and semantic class constraints on verb argu-
ments. The system performs at the best published
accuracy on the English verbs of the Senseval-2
(Palmer et al, 2001) exercise on evaluating au-
tomatic WSD systems. The Senseval-2 verb in-
stances have been manually tagged with their Word-
Net sense and come primarily from the Penn Tree-
bank WSJ. The WSJ corpus has also been manually
annotated for predicate arguments as part of Prop-
Bank (Kingsbury and Palmer, 2002), and the inter-
section of PropBank and Senseval-2 forms a corpus
containing gold-standard annotations of WordNet
senses and PropBank semantic role labels. This pro-
vides a unique opportunity to investigate the role of
predicate arguments in verb sense disambiguation.
We show that our system?s accuracy improves sig-
nificantly by adding features from PropBank, which
explicitly encodes the predicate-argument informa-
42
tion that our original set of syntactic and semantic
class features attempted to capture.
2 Basic automatic system
Our WSD system was built to combine information
from many different sources, using as much linguis-
tic knowledge as could be gathered automatically
by NLP tools. In particular, our goal was to see
the extent to which sense-tagging of verbs could be
improved by adding features that capture informa-
tion about predicate-arguments and selectional re-
strictions.
We used the Mallet toolkit (McCallum, 2002) for
learning maximum entropy models with Gaussian
priors for all our experiments. In order to extract
the linguistic features necessary for the models, all
sentences containing the target word were automat-
ically part-of-speech-tagged using a maximum en-
tropy tagger (Ratnaparkhi, 1998) and parsed using
the Collins parser (Collins, 1997). In addition, an
automatic named entity tagger (Bikel et al, 1997)
was run on the sentences to map proper nouns to a
small set of semantic classes.1
2.1 Topical features
We categorized the possible model features into top-
ical features and several types of local contextual
features. Topical features for a verb in a sentence
look for the presence of keywords occurring any-
where in the sentence and any surrounding sentences
provided as context (usually one or two sentences).
These features are supposed to show the domain in
which the verb is being used, since some verb senses
are used in only certain domains. The set of key-
words is specific to each verb lemma to be disam-
biguated and is determined automatically from train-
ing data so as to minimize the entropy of the proba-
bility of the senses conditioned on the keyword. All
alphabetic characters are converted to lower case.
Words occuring less than twice in the training data
or that are in a stoplist2 of pronouns, prepositions,
and conjunctions are ignored.
1The inclusion or omission of a particular company or prod-
uct implies neither endorsement nor criticism by NIST. Any
opinions, findings, and conclusions expressed are the authors?
own and do not necessarily reflect those of NIST.
2http://www.d.umn.edu/?tpederse/Group01/
WordNet/words.txt
2.2 Local features
The local features for a verb   in a particular sen-
tence tend to look only within the smallest clause
containing   . They include collocational features
requiring no linguistic preprocessing beyond part-
of-speech tagging, syntactic features that capture re-
lations between the verb and its complements, and
semantic features that incorporate information about
noun classes for subjects and objects:
Collocational features: Collocational features re-
fer to ordered sequences of part-of-speech tags or
word tokens immediately surrounding   . They in-
clude:
 unigrams: words   ,   ,  	 ,  
 ,  
 and
parts of speech   ,   ,   ,  
 ,  
 , where
  and   are at position  relative to  
 bigrams:    ,   
 ,  
 
 ;




, 




, 





 trigrams:     ,    
 ,
  
 

,
 
 
 
 ;       ,







, 







, 








Syntactic features: The system uses heuristics to
extract syntactic elements from the parse for the sen-
tence containing   . Let commander VP be the low-
est VP that dominates   and that is not immediately
dominated by another VP, and let head VP be the
lowest VP dominating   (See Figure 1). Then we
define the subject of   to be the leftmost NP sib-
ling of commander VP, and a complement of   to
be a node that is a child of the head VP, excluding
NPs whose head is a number or a noun from a list
of common temporal nouns (?week?, ?tomorrow?,
?Monday?, etc.). The system extracts the following
binary syntactic features:
 Is the sentence passive?
 Is there a subject, direct object (leftmost NP
complement of   ), indirect object (second left-
most NP complement of   ), or clausal comple-
ment (S complement of   )?
 What is the word (if any) that is the particle
or head of the subject, direct object, or indirect
object?
43
SNP
John
(commander) VP
VB
had
(head) VP
VB
pulled
NP
the blanket
PP
across the carpet
S
to create static
Figure 1: Example parse tree for   =?pulled?, from which is extracted the syntactic features: morph=normal
subj dobj sent-comp subj=john dobj=blanket prep=across across-obj=carpet.
 If there is a PP complement, what is the prepo-
sition, and what is the object of the preposition?
Semantic features:
 What is the Named Entity tag (PERSON, OR-
GANIZATION, LOCATION, UNKNOWN)
for each proper noun in the syntactic positions
above?
 What are the possible WordNet synsets and hy-
pernyms for each noun in the syntactic posi-
tions above? (Nouns are not explicitly disam-
biguated; all possible synsets and hypernyms
for the noun are included.)
This set of local features relies on access to syn-
tactic structure as well as semantic class informa-
tion, and attempts to model richer linguistic infor-
mation about predicate arguments. However, the
heuristics for extracting the syntactic features are
able to identify subjects and objects of only simple
clauses. The heuristics also do not differentiate be-
tween arguments and adjuncts; for example, the fea-
ture sent-comp is intended to identify clausal com-
plements such as in (S (NP Mary) (VP (VB called)
(S him a bastard))), but Figure 1 shows how a pur-
pose clause can be mistakenly labeled as a clausal
complement.
2.3 Evaluation
We tested the system on the 1806 test instances of
the 29 verbs from the English lexical sample task for
Senseval-2 (Palmer et al, 2001). Accuracy was de-
fined to be the fraction of the instances for which the
system got the correct sense. All significance testing
between different accuracies was done using a one-
tailed z-test, assuming a binomial distribution of the
successes; differences in accuracy were considered
to be significant if ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 768?775,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Different Structures for Evaluating Answers to Complex Questions:
Pyramids Won?t Topple, and Neither Will Human Assessors
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
Jimmy Lin
College of Information Studies
University of Maryland
College Park, MD 20742
jimmylin@umd.edu
Abstract
The idea of ?nugget pyramids? has re-
cently been introduced as a refinement to the
nugget-based methodology used to evaluate
answers to complex questions in the TREC
QA tracks. This paper examines data from
the 2006 evaluation, the first large-scale de-
ployment of the nugget pyramids scheme.
We show that this method of combining
judgments of nugget importance from multi-
ple assessors increases the stability and dis-
criminative power of the evaluation while in-
troducing only a small additional burden in
terms of manual assessment. We also con-
sider an alternative method for combining
assessor opinions, which yields a distinction
similar to micro- and macro-averaging in the
context of classification tasks. While the
two approaches differ in terms of underly-
ing assumptions, their results are neverthe-
less highly correlated.
1 Introduction
The emergence of question answering (QA) systems
for addressing complex information needs has ne-
cessitated the development and refinement of new
methodologies for evaluating and comparing sys-
tems. In the Text REtrieval Conference (TREC) QA
tracks organized by the U.S. National Institute of
Standards and Technology (NIST), improvements in
evaluation processes have kept pace with the evolu-
tion of QA tasks. For the past several years, NIST
has implemented an evaluation methodology based
on the notion of ?information nuggets? to assess an-
swers to complex questions. As it has become the
de facto standard for evaluating such systems, the
research community stands to benefit from a better
understanding of the characteristics of this evalua-
tion methodology.
This paper explores recent refinements to the
nugget-based evaluation methodology developed by
NIST. In particular, we examine the recent so-called
?pyramid extension? that incorporates relevance
judgments from multiple assessors to improve eval-
uation stability (Lin and Demner-Fushman, 2006).
We organize our discussion as follows: The next
section begins by providing a brief overview of
nugget-based evaluations and the pyramid exten-
sion. Section 3 presents results from the first large-
scale implementation of nugget pyramids for QA
evaluation in TREC 2006. Analysis shows that this
extension improves both stability and discriminative
power. In Section 4, we discuss an alternative for
combining multiple judgments that parallels the dis-
tinction between micro- and macro-averaging often
seen in classification tasks. Experiments reveal that
the methods yield almost exactly the same results,
despite operating on different granularities (individ-
ual nuggets vs. individual users).
2 Evaluating Complex Questions
Complex questions are distinguished from factoid
questions such as ?Who shot Abraham Lincoln?? in
that they cannot be answered by named entities (e.g.,
persons, organizations, dates, etc.). Typically, these
information needs are embedded in the context of a
scenario (i.e., user task) and often require systems to
768
synthesize information from multiple documents or
to generate answers that cannot be easily extracted
(e.g., by leveraging inference capabilities).
To date, NIST has already conducted several
large-scale evaluations of complex questions: def-
inition questions in TREC 2003, ?Other? ques-
tions in TREC 2004?2006, ?relationship? questions
in TREC 2005, and the complex, interactive QA
(ciQA) task in TREC 2006. Definition and Other
questions are similar in that they both request novel
facts about ?targets?, which can be persons, orga-
nizations, things, and events. Relationship ques-
tions evolved into the ciQA task and focus on in-
formation needs such as ?What financial relation-
ships exist between South American drug cartels and
banks in Liechtenstein?? Such complex questions
focus on ties (financial, military, familial, etc.) that
connect two or more entities. All of these evalua-
tions have employed the nugget-based methodology,
which demonstrates its versatility and applicability
to a wide range of information needs.
2.1 Basic Setup
In the TREC QA evaluations, an answer to a
complex question consists of an unordered set of
[document-id, answer string] pairs, where the strings
are presumed to provide some relevant information
that addresses the question. Although no explicit
limit is placed on the length of the answer, the final
metric penalizes verbosity (see below).
Evaluation of system output proceeds in two
steps. First, answer strings from all submissions
are gathered together and presented to a single as-
sessor. The source of each answer string is blinded
so that the assessor can not obviously tell which
systems generated what output. Using these an-
swers and searches performed during question de-
velopment, the assessor creates a list of relevant
nuggets. A nugget is a piece of information (i.e.,
?fact?) that addresses one aspect of the user?s ques-
tion. Nuggets should be atomic, in the sense that
an assessor should be able to make a binary de-
cision as to whether the nugget appears in an an-
swer string. Although a nugget represents a con-
ceptual entity, the assessor provides a natural lan-
guage description?primarily as a memory aid for
the subsequent evaluation steps. These descriptions
range from sentence-length document extracts to
r = # of vital nuggets returned
a = # of okay nuggets returned
R = # of vital nuggets in the answer key
l = # of non-whitespace characters in entire run
recall: R = r/R
allowance: ? = 100? (r + a)
precision: P =
{
1 if l < ?
1? l??l otherwise
F (?) = (?
2 + 1)? P ?R
?2 ? P +R
Figure 1: Official definition of F-score for nugget
evaluation in TREC.
key phrases to telegraphic short-hand notes?their
readability greatly varies from assessor to assessor.
The assessor also manually classifies each nugget
as either vital or okay (non-vital). Vital nuggets rep-
resent concepts that must be present in a ?good? an-
swer. Okay nuggets may contain interesting infor-
mation, but are not essential.
In the second step, the same assessor who cre-
ated the nuggets reads each system?s output in turn
and marks the appearance of the nuggets. An an-
swer string contains a nugget if there is a conceptual
match; that is, the match is independent of the partic-
ular wording used in the system?s output. A nugget
match is marked at most once per run?i.e., a sys-
tem is not rewarded for retrieving a nugget multiple
times. If the system?s output contains more than one
match for a nugget, the best match is selected and
the rest are left unmarked. A single [document-id,
answer string] pair in a system response can match
0, 1, or multiple nuggets.
The final F-score for an answer is calculated in the
manner described in Figure 1, and the final score of
a run is the average across the F-scores of all ques-
tions. The metric is a weighted harmonic mean be-
tween nugget precision and nugget recall, where re-
call is heavily favored (controlled by the ? parame-
ter, usually set to three). Nugget recall is calculated
solely on vital nuggets, while nugget precision is ap-
proximated by a length allowance based on the num-
ber of both vital and okay nuggets returned. In an
769
earlier pilot study, researchers discovered that it was
not possible for assessors to consistently enumer-
ate the total set of nuggets contained in an answer,
which corresponds to the denominator in a precision
calculation (Voorhees, 2003). Thus, a penalty for
verbosity serves as a surrogate for precision.
2.2 The Pyramid Extension
The vital/okay distinction has been identified as
a weakness in the TREC nugget-based evalua-
tion methodology (Hildebrandt et al, 2004; Lin
and Demner-Fushman, 2005; Lin and Demner-
Fushman, 2006). There do not appear to be any re-
liable indicators for predicting nugget importance,
which makes it challenging to develop algorithms
sensitive to this consideration. Since only vital
nuggets affect nugget recall, it is difficult for sys-
tems to achieve non-zero scores on topics with few
vital nuggets in the answer key. Thus, scores are
easily affected by assessor errors and other random
variations in evaluation conditions.
One direct consequence is that in previous TREC
evaluations, the median score for many questions
turned out to be zero. A binary distinction on nugget
importance is insufficient to discriminate between
the quality of runs that return no vital nuggets but
different numbers of okay nuggets. Also, a score
distribution heavily skewed towards zero makes
meta-analyses of evaluation stability difficult to per-
form (Voorhees, 2005).
The pyramid extension (Lin and Demner-
Fushman, 2006) was proposed to address the issues
mentioned above. The idea was relatively simple: by
soliciting vital/okay judgments from multiple asses-
sors (after the list of nuggets has been produced by
a primary assessor), it is possible to define nugget
importance with greater granularity. Each nugget is
assigned a weight between zero and one that is pro-
portional to the number of assessors who judged it
to be vital. Nugget recall from Figure 1 can be rede-
fined to incorporate these weights:
R =
?
m?Awm
?
n?V wn
Where A is the set of reference nuggets that are
matched in a system?s output and V is the set of all
reference nuggets; wm and wn are the weights of
nuggets m and n, respectively.1 The calculation of
nugget precision remains the same.
3 Nugget Pyramids in TREC 2006
Lin and Demner-Fushman (2006) present exper-
imental evidence in support of nugget pyramids
by applying the proposal to results from previous
TREC QA evaluations. Their simulation studies ap-
pear to support the assertion that pyramids address
many of the issues raised in Section 2.2. Based on
the results, NIST proceeded with a trial deployment
of nugget pyramids in the TREC 2006 QA track. Al-
though scores based on the binary vital/okay distinc-
tion were retained as the ?official? metric, pyramid
scores were simultaneously computed. This pro-
vided an opportunity to compare the two method-
ologies on a large scale.
3.1 The Data
The basic unit of evaluation for the main QA task
at TREC 2006 was the ?question series?. Each se-
ries focused on a ?target?, which could be a person,
organization, thing, or event. Individual questions
in a series inquired about different facets of the tar-
get, and were explicitly classified as factoid, list, or
Other. One complete series is shown in Figure 2.
The Other questions can be best paraphrased as ?Tell
me interesting things about X that I haven?t already
explicitly asked about.? It was the system?s task to
retrieve interesting nuggets about the target (in the
opinion of the assessor), but credit was not given
for retrieving facts already explicitly asked for in the
factoid and list questions. The Other questions were
evaluated using the nugget-based methodology, and
are the subject of this analysis.
The QA test set in TREC 2006 contained 75 se-
ries. Of the 75 targets, 19 were persons, 19 were
organizations, 19 were events, and 18 were things.
The series contained a total of 75 Other questions
(one per target). Each series contained 6?9 ques-
tions (counting the Other question), with most se-
ries containing 8 questions. The task employed the
AQUAINT collection of newswire text (LDC cat-
alog number LDC2002T31), consisting of English
data drawn from three sources: the New York Times,
1Note that this new scoring model captures the existing
binary vital/okay distinction in a straightforward way: vital
nuggets get a score of one, and okay nuggets zero.
770
147 Britain?s Prince Edward marries
147.1 FACTOID When did Prince Edward engage to marry?
147.2 FACTOID Who did the Prince marry?
147.3 FACTOID Where did they honeymoon?
147.4 FACTOID Where was Edward in line for the throne at the time of the wedding?
147.5 FACTOID What was the Prince?s occupation?
147.6 FACTOID How many people viewed the wedding on television?
147.7 LIST What individuals were at the wedding?
147.8 OTHER
Figure 2: Sample question series from TREC 2006.
Nugget 0 1 2 3 4 5 6 7 8
The couple had a long courtship 1 0 0 0 0 0 1 1 0
Queen Elizabeth II was delighted with the match 0 1 0 1 0 0 0 0 1
Queen named couple Earl and Contessa of Wessex 0 1 0 0 1 1 1 0 0
All marriages of Edward?s siblings ended in divorce 0 0 0 0 0 1 0 0 1
Edward arranged for William to appear more cheerful in photo 0 0 0 0 0 0 0 0 0
they were married in St. Georges Chapel, Windsor 1 1 1 0 1 0 1 1 0
Figure 3: Multiple assessors? judgments of nugget importance for Series 147 (vital=1, okay=0). Assessor 2
was the same as the primary assessor (assessor 0), but judgments were elicited at different times.
the Associated Press, and the Xinhua News Service.
There are approximately one million articles in the
collection, totaling roughly three gigabytes. In to-
tal, 59 runs from 27 participants were submitted to
NIST. For more details, see (Dang et al, 2006).
For the Other questions, nine sets of judgments
were elicited from eight judges (the primary assessor
who originally created the nuggets later annotated
the nuggets once again). Each assessor was asked to
assign the vital/okay label in a rapid fashion, without
giving each decision much thought. Figure 3 gives
an example of the multiple judgments for nuggets in
Series 147. There is variation in notions of impor-
tance not only between different assessors, but also
for a single assessor over time.
3.2 Results
After the human annotation process, nugget pyra-
mids were built in the manner described by Lin and
Demner-Fushman (2006). Two scores were com-
puted for each run submitted to the TREC 2006 main
QA task: one based on the vital/okay judgments of
the primary assessor (which we call the binary F-
score) and one based on the nugget pyramids (the
pyramid F-score). The characteristics of the pyra-
mid method can be inferred by comparing these two
sets of scores.
Figure 4 plots the average binary and average
pyramid F-scores for each run (which represents av-
erage performance across all series). Even though
the nugget pyramid does not represent any single
real user (a point we return to later), pyramid F-
scores do correlate highly with the binary F-scores.
The Pearson?s correlation is 0.987, with a 95% con-
fidence interval of [0.980, 1.00].
While the average F-score for a run is stable given
a sufficient number of questions, the F-score for
a single Other question exhibits greater variability
across assessors. This is shown in Figure 5, which
plots binary and pyramid F-scores for individual
questions from all runs. In this case, the Pearson
correlation is 0.870, with a 95% confidence interval
of [0.863, 1.00].
For 16.4% of all Other questions, the nugget pyra-
mid assigned a non-zero F-score where the origi-
nal binary F-score was zero. This can be seen in
the band of points on the left edge of the plot in
Figure 5. This highlights the strength of nugget
771
0.00
0.05
0.10
0.15
0.20
0.25
0.000.050.100.150.200.25
Aver
age 
binar
y F?
score
Average pyramid F?score
Figure 4: Scatter plot comparing the binary and
pyramid F-scores for each run.
pyramids?their ability to smooth out assessor dif-
ferences and more finely discriminate among sys-
tem outputs. This is a key capability that is useful
for system developers, particularly since algorithmic
improvements are often incremental and small.
Because it is more stable than the single-assessor
method of evaluation, the pyramid method also ap-
pears to have greater discriminative power. We fit
a two-way analysis of variance model with the se-
ries and run as factors, and the binary F-score as
the dependent variable. We found significant differ-
ences between series and between runs (p essentially
equal to 0 for both factors). To determine which runs
were significantly different from each other, we per-
formed a multiple comparison using Tukey?s hon-
estly significant difference criterion and controlling
for the experiment-wise Type I error so that the prob-
ability of declaring a difference between two runs to
be significant, when it is actually not, is at most 5%.
With 59 runs, there are C592 = 1711 different pairs
that can be compared. The single-assessor method
was able to declare one run to be significantly better
than the other in 557 of these pairs. Using the pyra-
mid F-scores, it was possible to find significant dif-
ferences in performance between runs in 617 pairs.
3.3 Discussion
Any evaluation represents a compromise between
effort (which correlates with cost) and insightful-
ness of results. The level of detail and meaning-
0.0
0.2
0.4
0.6
0.8
0.00.20.40.60.8
Bina
ry F?
scor
e
Pyramid F?score
Figure 5: Scatter plot comparing the binary and
pyramid F-scores for each Other question.
fulness of evaluations are constantly in tension with
the availability of resources. Modifications to exist-
ing processes usually come at a cost that needs to be
weighed against potential gains. Based on these con-
siderations, the balance sheet for nugget pyramids
shows a favorable orientation. In the TREC 2006
QA evaluation, soliciting vital/okay judgments from
multiple assessors was not very time-consuming (a
couple of hours per assessor). Analysis confirms
that pyramid scores confer many benefits at an ac-
ceptable cost, thus arguing for its adoption in future
evaluations.
Cost considerations precluded exploring other re-
finements to the nugget-based evaluation methodol-
ogy. One possible alternative would involve ask-
ing multiple assessors to create different sets of
nuggets from scratch. Not only would this be time-
consuming, one would then need to deal with the
additional complexities of aligning each assessor?s
nuggets list. This includes resolving issues such as
nugget granularity, overlap in information content,
implicature and other relations between nuggets, etc.
4 Exploration of Alternative Structures
Despite the demonstrated effectiveness of nugget
pyramids, there are a few potential drawbacks that
are worth discussing. One downside is that the
nugget pyramid does not represent a single assessor.
The nugget weights reflect the aggregation of opin-
ions across a sample population, but there is no guar-
772
antee that the method for computing those weights
actually captures any aspect of real user behavior.
It can be argued that the binary F-score is more re-
alistic since it reflects the opinion of a real user (the
primary assessor), whereas the pyramid F-score tries
to model the opinion of a mythical average user.
Although this point may seem somewhat counter-
intuitive, it represents a well-established tradition
in the information retrieval literature (Voorhees,
2002). In document retrieval, for example, relevance
judgments are provided by a single assessor?even
though it is well known that there are large indi-
vidual differences in notions of relevance. IR re-
searchers believe that human idiosyncrasies are an
inescapable fact present in any system designed for
human users, and hence any attempt to remove those
elements in the evaluation setup is actually undesir-
able. It is the responsibility of researchers to develop
systems that are robust and flexible. This premise,
however, does not mean that IR evaluation results
are unstable or unreliable. Analyses have shown
that despite large variations in human opinions, sys-
tem rankings are remarkably stable (Voorhees, 2000;
Sormunen, 2002)?that is, one can usually be confi-
dent about system comparisons.
The philosophy in IR sharply contrasts with work
in NLP annotation tasks such as parsing, word sense
disambiguation, and semantic role labeling?where
researchers strive for high levels of interannota-
tor agreement, often through elaborate guidelines.
The difference in philosophies arises because unlike
these NLP annotation tasks, where the products are
used primarily by other NLP system components, IR
(and likewise QA) is an end-user task. These sys-
tems are intended for real world use. Since people
differ, systems must be able to accommodate these
differences. Hence, there is a strong preference in
QA for evaluations that maintain a model of the in-
dividual user.
4.1 Micro- vs. Macro-Averaging
The current nugget pyramid method leverages mul-
tiple judgments to define a weight for each individ-
ual nugget, and then incorporates this weight into
the F-score computation. As an alternative, we pro-
pose another method for combining the opinions of
multiple assessors: evaluate system responses indi-
vidually against N sets of binary judgments, and
then compute the mean across those scores. We de-
fine the macro-averaged binary F-score over a set
A = {a1, ..., aN} of N assessors as:
F =
?
a?A Fa
N
Where Fa is the binary F-score according to the
vital/okay judgments of assessor a. The differ-
ences between the pyramid F-score and the macro-
averaged binary F-score correspond to the distinc-
tion between micro- and macro-averaging discussed
in the context of text classification (Lewis, 1991).
In those applications, both measures are mean-
ingful depending on focus: individual instances or
entire classes. In tasks where it is important
to correctly classify individual instances, micro-
averaging is more appropriate. In tasks where it
is important to correctly identify a class, macro-
averaging better quantifies performance. In classi-
fication tasks, imbalance in the prevalence of each
class can lead to large differences in macro- and
micro-averaged scores. Analogizing to our work,
the original formulation of nugget pyramids corre-
sponds to micro-averaging (since we focus on indi-
vidual nuggets), while the alternative corresponds to
macro-averaging (since we focus on the assessor).
We additionally note that the two methods en-
code different assumptions. Macro-averaging as-
sumes that there is nothing intrinsically interesting
about a nugget?it is simply a matter of a particular
user with particular needs finding a particular nugget
to be of interest. Micro-averaging, on the other hand,
assumes that some nuggets are inherently interest-
ing, independent of the particular interests of users.2
Each approach has characteristics that make it
desirable. From the perspective of evaluators, the
macro-averaged binary F-score is preferable be-
cause it models real users; each set of binary judg-
ments represents the information need of a real user,
each binary F-score represents how well an answer
will satisfy a real user, and the macro-averaged bi-
nary F-score represents how well an answer will sat-
isfy, on average, a sample population of real users.
From the perspective of QA system developers, the
micro-averaged nugget pyramid F-score is prefer-
able because it allows finer discrimination in in-
2We are grateful to an anonymous reviewer for this insight.
773
dividual nugget performance, which enables better
techniques for system training and optimization.
The macro-averaged binary F-score has the same
desirable properties as the micro-averaged pyramid
F-score in that fewer responses will have zero F-
scores as compared to the single-assessor binary F-
score. We demonstrate this as follows. Let X be a
response that receives a non-zero pyramid F-score.
Let A = {a1, a2, a3, ..., aN} be the set of N asses-
sors. Then it can be proven that X also receives a
non-zero macro-averaged binary F-score:
1. There exists some nugget v with weight greater
than 0, such that an answer string r in X
matches v. (def. of pyramid recall)
2. There exists some assessor ap ? Awhomarked
v as vital. (def. of pyramid nugget weight)
3. To show that X will also receive a non-zero
macro-averaged binary score, it is sufficient to
show that there is some assessor am ? A such
thatX receives a non-zero F-score when evalu-
ated using just the vital/okay judgments of am.
(def. of macro-averaged binary F-score)
4. But, such an assessor does exist, namely asses-
sor ap: Consider the binary F-score assigned
to X according to just assessor ap. The re-
call of X is greater than zero, since X contains
the response r that matches the nugget v that
was marked as vital by ap (from (2), (1), and
the def. of recall). The precision must also be
greater than zero (def. of precision). Therefore,
the macro-averaged binary F-score ofX is non-
zero. (def. of F-score)
4.2 Analysis from TREC 2006
While the macro-averaged method is guaranteed to
produce no more zero-valued scores than the micro-
averaged pyramid method, it is not guaranteed that
the scores will be the same for any given response.
What are the empirical characteristics of each ap-
proach? To explore this question, we once again ex-
amined data from TREC 2006.
Figure 6 shows a scatter plot of the pyramid F-
score and macro-averaged binary F-score for every
Other questions in all runs from the TREC 2006
QA track main task. Despite focusing on differ-
ent aspects of the evaluation setup, these measures
0.0
0.2
0.4
0.6
0.8
0.00.20.40.60.8
Pyra
mid 
F?sc
ore
Macro?averaged binary F?score
Figure 6: Scatter plot comparing the pyramid and
macro-averaged binary F-scores for all questions.
binary micro macro
binary 1.000/1.000 0.870/0.987 0.861/0.988
micro - 1.000/1.000 0.985/0.996
macro - - 1.000/1.000
Table 1: Pearson?s correlation of F-scores, by ques-
tion and by run.
are highly correlated, even at the level of individ-
ual questions. Table 1 provides a summary of the
correlations between the original binary F-score, the
(micro-averaged) pyramid F-score, and the macro-
averaged binary F-score. Pearson?s r is given for
F-scores at the individual question level (first num-
ber) and at the run level (second number). The cor-
relation between all three variants are about equal at
the level of system runs. At the level of individual
questions, the micro- and macro-averaged F-scores
(using multiple judgments) are still highly correlated
with each other, but each is less correlated with the
single-assessor binary F-score.
4.3 Discussion
The differences between macro- and micro-
averaging methods invoke a more general discus-
sion on notions of nugget importance. There are
actually two different issues we are attempting to
address with our different approaches: the first is
a more granular scale of nugget importance, the
second is variations across a population of users. In
774
the micro-averaged pyramid F-scores, we achieve
the first by leveraging the second, i.e., binary
judgments from a large population are combined
to yield weights for individual nuggets. In the
macro-averaged binary F-score, we focus solely on
population effects without addressing granularity of
nugget importance.
Exploring this thread of argument, we can for-
mulate additional approaches for tackling these is-
sues. We could, for example, solicit more granular
individual judgments on each nugget from each as-
sessor, perhaps on a Likert scale or as a continuous
quantity ranging from zero to one. This would yield
two more methods for computing F-scores, both a
macro-averaged and a micro-averaged variant. The
macro-averaged variant would be especially attrac-
tive because it reflects real users and yet individual
F-scores remain discriminative. Despite its possi-
ble advantages, this extension is rejected based on
resource considerations; making snap binary judg-
ments on individual nuggets is much quicker than a
multi-scaled value assignment?at least at present,
the additional costs are not sufficient to offset the
potential gains.
5 Conclusion
The important role that large-scale evaluations play
in guiding research in human language technologies
means that the community must ?get it right.? This
would ordinarily call for a more conservative ap-
proach to avoid changes that might have unintended
consequences. However, evaluation methodologies
must evolve to reflect the shifting interests of the re-
search community to remain relevant. Thus, orga-
nizers of evaluations must walk a fine line between
progress and chaos. Nevertheless, the introduction
of nugget pyramids in the TREC QA evaluation pro-
vides a case study showing how this fine balance can
indeed be achieved. The addition of multiple judg-
ments of nugget importance yields an evaluation that
is both more stable and more discriminative than the
original single-assessor evaluation, while requiring
only a small additional cost in terms of human labor.
We have explored two different methods for com-
bining judgments from multiple assessors to address
shortcomings in the original nugget-based evalua-
tion setup. Although they make different assump-
tions about the evaluation, results from both ap-
proaches are highly correlated. Thus, we can con-
tinue employing the pyramid-based method, which
is well-suited for developing systems, and still be as-
sured that the results remain consistent with an eval-
uation method that maintains a model of real indi-
vidual users.
Acknowledgments
This work has been supported in part by DARPA
contract HR0011-06-2-0001 (GALE). The second
author would like to thank Kiri and Esther for their
kind support.
References
H. Dang, J. Lin, and D. Kelly. 2006. Overview of the
TREC 2006 question answering track. In Proc. of
TREC 2006.
W. Hildebrandt, B. Katz, and J. Lin. 2004. Answering
definition questions with multiple knowledge sources.
In Proc. HLT/NAACL 2004.
D. Lewis. 1991. Evaluating text categorization. In Proc.
of the Speech and Natural Language Workshop.
J. Lin and D. Demner-Fushman. 2005. Automatically
evaluating answers to definition questions. In Proc. of
HLT/EMNLP 2005.
J. Lin and D. Demner-Fushman. 2006. Will pyramids
built of nuggets topple over? In Proc. of HLT/NAACL
2006.
E. Sormunen. 2002. Liberal relevance criteria of
TREC?counting on negligible documents? In Proc.
of SIGIR 2002.
E. Voorhees. 2000. Variations in relevance judgments
and the measurement of retrieval effectiveness. IP&M,
36(5):697?716.
E. Voorhees. 2002. The philosophy of information re-
trieval evaluation. In Proc. of CLEF Workshop.
E. Voorhees. 2003. Overview of the TREC 2003 ques-
tion answering track. In Proc. of TREC 2003.
E. Voorhees. 2005. Using question series to evaluate
question answering system effectiveness. In Proc. of
HLT/EMNLP 2005.
775
Proceedings of the Workshop on Task-Focused Summarization and Question Answering, pages 48?55,
Sydney, July 2006. c?2006 Association for Computational Linguistics
DUC 2005: Evaluation of Question-Focused Summarization Systems
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
100 Bureau Drive
Gaithersburg, MD, 20899
hoa.dang@nist.gov
Abstract
The Document Understanding Conference
(DUC) 2005 evaluation had a single user-
oriented, question-focused summarization
task, which was to synthesize from a set
of 25-50 documents a well-organized, flu-
ent answer to a complex question. The
evaluation shows that the best summariza-
tion systems have difficulty extracting rel-
evant sentences in response to complex
questions (as opposed to representative
sentences that might be appropriate to a
generic summary). The relatively gener-
ous allowance of 250 words for each an-
swer also reveals how difficult it is for
current summarization systems to produce
fluent text from multiple documents.
1 Introduction
The Document Understanding Conference (DUC)
is a series of evaluations of automatic text sum-
marization systems. It is organized by the Na-
tional Institute of Standards of Technology with
the goals of furthering progress in automatic sum-
marization and enabling researchers to participate
in large-scale experiments.
In DUC 2001-2004 a growing number of
research groups participated in the evaluation
of generic and focused summaries of English
newspaper and newswire data. Various target
sizes were used (10-400 words) and both single-
document summaries and summaries of multiple
documents were evaluated (around 10 documents
per set). Summaries were manually judged for
both content and readability. To evaluate content,
each peer (human or automatic) summary was
compared against a single model (human) sum-
mary using SEE (http://www.isi.edu/ cyl/SEE/)
to estimate the percentage of information in the
model that was covered in the peer. Addition-
ally, automatic evaluation of content coverage us-
ing ROUGE (Lin, 2004) was explored in 2004.
Human summaries vary in both writing style
and content. For example, (Harman and Over,
2004) noted that a human summary can vary in its
level of granularity, whether the summary has a
very high-level analysis or primarily contains de-
tails. They analyzed the effects of human vari-
aion in the DUC evaluations and concluded that
despite large variation in model summaries, the
rankings of the systems when compared against a
single model for each document set remained sta-
ble when averaged over a large number of docu-
ment sets and human assessors. The use of a large
test set to smooth over natural human variation is
not a new technique; it is the approach that has
been taken in TREC (Text Retrieval Conference)
for many years (Voorhees and Buckley, 2002).
While evaluators can achieve stable overall sys-
tem rankings by averaging scores over a large
number of document sets, system builders are still
faced with the challenge of producing a summary
for a given document set that is most likely to
satisfy any human user (since they cannot know
ahead of time which human will be using or judg-
ing the summary). Thus, system developers desire
an evaluation methodology that takes into account
human variation in summaries for any given doc-
ument set.
DUC 2005 marked a major change in direc-
tion from previous years. The road mapping com-
mittee had strongly recommended that new tasks
be undertaken that were strongly tied to a clear
user application. At the same time, the program
committee wanted to work on new evaluation
methodologies and metrics that would take into
48
account variation of content in human-authored
summaries.
Therefore, DUC 2005 had a single user-oriented
system task that allowed the community to put
some time and effort into helping with a new eval-
uation framework. The system task modeled real-
world complex question answering (Amigo et al,
2004). Systems were to synthesize from a set of
25-50 documents a brief, well-organized, fluent
answer to a need for information that could not
be met by just stating a name, date, quantity, etc.
Summaries were evaluated for both content and
readability.
The task design attempted to constrain two
parameters that could produce summaries with
widely different content: focus and granularity.
Having a question to focus the summary was in-
tended to improve agreement in content between
the model summaries. Additionally, the assessor
who developed each topic specified the desired
granularity (level of generalization) of the sum-
mary. Granularity was a way to express one type
of user preference; one user might want a general
background or overview summary, while another
user might want specific details that would allow
him to answer questions about specific events or
situations.
Because it is both impossible and unnatural to
eliminate all human variation, our assessors cre-
ated as many manual summaries as feasible for
each topic, to provide examples of the range of
normal human variability in the summarization
task. These multiple models would provide more
representative training data to system developers,
while enabling additional experiments to investi-
gate the effect of human variability on the evalua-
tion of summarization systems.
As in past DUCs, assessors manually evalu-
ated each summary for readability using a set
of linguistic quality questions. Summary con-
tent was manually evaluated using the pseudo-
extrinsic measure of responsiveness, which does
not attempt pairwise comparison of peers against
a model summary but gives a coarse ranking of
all the summaries based on responsiveness of
the summary to the topic. In parallel, ISI and
Columbia University led the summarization re-
search community in two exploratory efforts at in-
trinsic evaluation of summary content; these eval-
uations compared peer summaries against multiple
reference summaries, using Basic Elements at ISI
and Pyramids at Columbia University.
This paper describes the DUC 2005 task and the
results of our evaluations of summary content and
readability. (Hovy et al, 2005) and (Passonneau
et al, 2005) provide additional details and results
of the evaluations of summary content using Basic
Elements and Pyramids.
2 Task Description
The DUC 2005 task was a complex question-
focused summarization task that required summa-
rizers to piece together information from multiple
documents to answer a question or set of questions
as posed in a topic.
Assessors developed a total of 50 topics to be
used as test data. For each topic, the assessor se-
lected 25-50 related documents from the Los An-
geles Times and Financial Times of London and
formulated a topic statement, which was a request
for information that could be answered using the
selected documents. The topic statement could be
in the form of a question or set of related questions
and could include background information that the
assessor thought would help clarify his/her infor-
mation need.
The assessor also indicated the ?granularity? of
the desired response for each topic. That is, they
indicated whether they wanted the answer to their
question(s) to name specific events, people, places,
etc., or whether they wanted a general, high-level
answer. Only one value of granularity was given
for each topic, since the goal was not to measure
the effect of different granularities on system per-
formance for a given topic, but to provide addi-
tional information about the user?s preferences to
both human and automatic summarizers.
An example DUC topic follows:
num: D345
title: American Tobacco Companies Over-
seas
narr: In the early 1990?s, American to-
bacco companies tried to expand their busi-
ness overseas. What did these companies do
or try to do and where? How did their parent
companies fare?
granularity: specific
The summarization task was the same for both
human and automatic summarizers: Given a DUC
topic with granularity specification and a set of
documents relevant to the topic, the summariza-
tion task was to create from the documents a brief,
49
well-organized, fluent summary that answers the
need for information expressed in the topic, at
the specified level of granularity. The summary
could be no longer than 250 words (whitespace-
delimited tokens). Summaries over the size limit
were truncated, and no bonus was given for cre-
ating a shorter summary. No specific format-
ting other than linear was allowed. The summary
should include (in some form or other) all the
information in the documents that contributed to
meeting the information need.
Ten assessors produced a total of 9 human sum-
maries for each of 20 topics, and 4 human sum-
maries for each of the remaining 30 topics. The
summarization task was a relatively difficult task,
requiring about 5 hours to manually create each
summary. Thus, there would be a real benefit to
users if the task could be performed automatically.
3 Participants
There was much interest in the longer, question-
focused summaries required in the DUC 2005
task. 31 participants submitted runs to the evalua-
tion; they are identified by numeric Run IDs (2-32)
in the remainder of this paper. We also developed
a simple baseline system that returned the first 250
words of the most recent document for each topic
(Run ID = 1). In addition to the automatic peers,
there were 10 human peers, assigned alphabetic
Run IDs, A-J.
Most system developers treated the summariza-
tion task as a passage retrieval task. Sentences
were ranked according to relevance to the topic.
The most relevant sentences were then selected for
inclusion in the summary while minimizing redun-
dancy within the summary, up to the maximum
250-word allowance. A significant minority of
systems first decomposed the topic narrative into
a set of simpler questions, and then extracted sen-
tences to answer each subquestion. Systems dif-
fered in the approach taken to compute relevance
and redundancy, using similarity metrics ranging
from simple term frequency to semantic graph
matching. In order to include more relevant infor-
mation in the summary, systems attempted within-
sentence compression by removing phrases such
as parentheticals and relative clauses.
Many systems simply ignored the granularity
specification. The systems that addressed gran-
ularity did so by preferring to extract sentences
that contained proper names for topics with a ?spe-
cific? granularity but not for topics with ?general?
granularity.
Cross-sentence dependencies had to be handled,
including anaphora. Strategies for dealing with
pronouns that occurred in relevant sentences in-
cluded co-reference resolution, including the pre-
vious sentence for additional context, or simply
excluding all sentences containing any pronouns.
Most systems made no attempt to reword the ex-
tracted sentences to improve the readability of the
final summary. Although some systems grouped
related sentences together to improve cohesion,
the most common heuristic to improve readabil-
ity was simply to order the extracted sentences by
document date and position in the document. Sys-
tem 12 achieved high readability scores by choos-
ing a single representative document and extract-
ing sentences in the order of appearance in that
document. This approach is similar to the base-
line summarizer and produces summaries that are
more fluent than those constructed from multiple
documents.
4 Evaluation Results
Summaries were manually evaluated by 10 asses-
sors. All summaries for a given topic were judged
by a single assessor (who was usually the same as
the topic developer). In all cases, the assessor was
one of the summarizers for the topic. All sum-
maries for the topic (including the one written by
the assessor) were anonymously presented to the
assessor, in a random order, and the ssessor judged
each summary for readability and responsiveness
to the topic, giving separate scores for responsive-
ness and each of 5 linguistic qualities. This al-
lowed participants who could not work on opti-
mizing all 6 manual scores, to focus on only the
elements that they were interested in or had the re-
sources to address.
No single score was reported that reflected a
combination of readability and content. In pre-
vious years, responsiveness considered both the
content and readability of the summary. While it
tracked SEE coverage, responsiveness could not
be seen as a direct measure of content due to pos-
sible effects of readability on the score. Because
we needed an inexpensive manual measure of cov-
erage, we revised the definition of responsiveness
in 2005 so that it considered only the information
content and not the readability of the summary, to
the extent possible.
50
4.1 Evaluation of Readability
The readability of the summaries was assessed us-
ing five linguistic quality questions which mea-
sured qualities of the summary that do not in-
volve comparison with a reference summary or
DUC topic. The linguistic qualities measured
were Grammaticality, Non-redundancy, Referen-
tial clarity, Focus, and Structure and coherence.
Q1: Grammaticality The summary should
have no datelines, system-internal formatting, cap-
italization errors or obviously ungrammatical sen-
tences (e.g., fragments, missing components) that
make the text difficult to read.
Q2: Non-redundancy There should be no un-
necessary repetition in the summary. Unnecessary
repetition might take the form of whole sentences
that are repeated, or repeated facts, or the repeated
use of a noun or noun phrase (e.g., ?Bill Clinton?)
when a pronoun (?he?) would suffice.
Q3: Referential clarity It should be easy to
identify who or what the pronouns and noun
phrases in the summary are referring to. If a per-
son or other entity is mentioned, it should be clear
what their role in the story is. So, a reference
would be unclear if an entity is referenced but its
identity or relation to the story remains unclear.
Q4: Focus The summary should have a focus;
sentences should only contain information that is
related to the rest of the summary.
Q5: Structure and Coherence The summary
should be well-structured and well-organized. The
summary should not just be a heap of related infor-
mation, but should build from sentence to sentence
to a coherent body of information about a topic.
Each linguistic quality question was assessed on
a five-point scale:
1. Very Poor
2. Poor
3. Barely Acceptable
4. Good
5. Very Good
Table 1 shows the distribution of the scores
across all the summaries, broken down by the type
of summarizer (Human, Baseline, or Participants).
All summarizers generally performed well on the
first two linguistic qualities. The high scores on
non-redundancy show that most participants have
Humans
Q1
Fr
eq
ue
nc
y
1 2 3 4 5
0
50
10
0
15
0
20
0
25
0
Baseline
Q1
Fr
eq
ue
nc
y
1 2 3 4 5
0
5
10
15
20
Participants
Q1
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
0
30
0
50
0
Q1: Grammaticality
Humans
Q2
Fr
eq
ue
nc
y
1 2 3 4 5
0
50
10
0
15
0
20
0
25
0
Baseline
Q2
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
20
30
40
Participants
Q2
Fr
eq
ue
nc
y
1 2 3 4 5
0
20
0
40
0
60
0
80
0
10
00
Q2: Non-redundancy
Humans
Q3
Fr
eq
ue
nc
y
1 2 3 4 5
0
50
10
0
15
0
20
0
25
0
30
0
Baseline
Q3
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
20
30
Participants
Q3
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
0
20
0
30
0
40
0
Q3: Referential Clarity
Humans
Q4
Fr
eq
ue
nc
y
1 2 3 4 5
0
50
10
0
15
0
20
0
25
0
Baseline
Q4
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
20
30
Participants
Q4
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
0
20
0
30
0
40
0
50
0
Q4: Focus
Humans
Q5
Fr
eq
ue
nc
y
1 2 3 4 5
0
50
10
0
15
0
20
0
25
0
Baseline
Q5
Fr
eq
ue
nc
y
1 2 3 4 5
0
5
10
15
20
Participants
Q5
Fr
eq
ue
nc
y
1 2 3 4 5
0
10
0
30
0
50
0
Q5: Structure and Coherence
Table 1: Frequency of scores for each linguistic
quality, broken down by source of summary (Hu-
mans, Baseline, Participants).
51
successfully achieved this capability. Humans and
the baseline system also scored well on the last
3 linguistic qualities. The multi-document sum-
marization systems submitted by participants, on
the other hand, still struggle with referential clar-
ity and focus, and perform very poorly on structure
and coherence.
4.1.1 Comparison by system
For each linguistic quality question, we per-
formed a multiple comparison test between the
scores of all peers using Tukey?s honestly signif-
icant difference criterion. A multiple comparison
test between all human and automatic peers was
performed using the Kruskall-Wallis test, to see
how the individual automatic peers performed rel-
ative to human peers. For grammaticality, the best
human summarizer is significantly better than 28
of the 32 systems; the worst human summarizer
is better than 8 systems. For non-redundancy, the
two best humans are significantly better than 6 sys-
tems, and the two worst humans are not signifi-
cantly different from any system. For referential
clarity, all humans are significantly better than all
but 2 automatic peers (baseline and System 12).
For focus, the best human is significantly better
than all automatic peers except the baseline; all
other humans are significantly better than all au-
tomatic peers except the baseline and System 12.
For structure and coherence, the two best humans
are significantly better than 31 systems (all auto-
matic peers except the baseline); all humans are
better than 30 of the automatic peers (all automatic
peers except baseline and System 12).
4.2 Evaluation of Content
We performed manual pseudo-extrinsic evaluation
of peer summaries in the form of assessment of
responsiveness. Responsiveness is different from
SEE coverage in that it does not compare a peer
summary against a single reference; however, re-
sponsiveness tracked SEE coverage in DUC 2003
and 2004, and was used to provide a coarse-
grained measure of content in 2005. We also com-
puted ROUGE scores as was done in DUC 2004.
4.2.1 Responsiveness
Assessors assigned a raw responsiveness score
to each summary. The score provides a coarse
ranking of the summaries for each topic, according
to the amount of information in the summary that
helps to satisfy the information need expressed in
the topic statement, at the level of granularity re-
quested in the user profile. The score was an inte-
ger between 1 and 5, with 1 being least respon-
sive and 5 being most responsive. For a given
topic, some summary was required to receive each
of the five possible scores, but no distribution was
specified for how many summaries had to receive
each score. The number of human summaries
scored per topic also varied. Therefore, raw re-
sponsiveness scores should not be directly added
and compared across topics. Assigning respon-
siveness scores can be seen as a clustering task in
which peers are partitioned into exactly 5 clusters,
where members of a cluster are more similar to
each other in quality.
RunID
10 A
5 A
4 A B
15 A B C
29 A B C D
11 A B C D
17 A B C D
8 A B C D
7 A B C D E
14 A B C D E
6 A B C D E
28 A B C D E F
21 A B C D E F
19 A B C D E F
24 A B C D E F
9 A B C D E F
16 A B C D E F
32 A B C D E F
12 A B C D E F
25 A B C D E F
18 A B C D E F
27 A B C D E F
20 A B C D E F
3 A B C D E F
2 B C D E F
13 C D E F
30 D E F
22 E F
1 E F
26 F
31 F G
23 G
Table 2: Multiple comparison of systems based on
Friedman?s test on responsiveness
For each topic, we computed the scaled respon-
siveness score for each summary, such that the
sum of the scaled responsiveness score is propor-
tional to the number of summaries for the topic.
The scaled responsiveness is the rank of the sum-
mary based on the raw responsiveness score. We
computed the average scaled responsiveness score
of each summarizer across all topics. Since the
52
number of human summaries varied across topics,
we also computed the average scaled responsive-
ness score of only the automatic summaries (ig-
noring the human summaries in scaling respon-
siveness).
Table 2 shows the results of a multiple com-
parison of scaled responsiveness of the automatic
peers using Tukey?s honestly significant criterion
and Friedman?s test, with the best peers on top;
peers not sharing a common letter are significantly
different at the 95.5% confidence level. None of
the automatic peers performed significantly bet-
ter than the majority of the remaining peers, and
only eight of the automatic peers performed signif-
icantly better than the simple baseline. In multiple
comparison of all peers using the Kruskal-Wallis
test, all human peers were significantly better than
all the automatic peers.
4.2.2 ROUGE
We computed two ROUGE scores: ROUGE-2
and ROUGE-SU4 recall, both with stemming and
implementing jackknifing for each [peer, topic]
pair so that human and automatic peers could be
compared. Since the number of ROUGE evalu-
ations per topic varied depending on the number
of reference summaries, we computed a macro-
average of each score for each peer, where the
macro-average score is the mean over all topics of
the mean per-topic score for the peer.
Unlike responsiveness and linguistic quality
scores, which are ordinal data and are best suited
for non-parametric analyses, ROUGE scores, can
be measured on an interval scale and are suit-
able for parametric analysis. Analysis of variance
showed significant effects from peer and topic
(p = 0 for each factor) for both ROUGE-2 and
ROUGE-SU4 recall. To see which peers were
different, a multiple comparison of population
marginal means (PMM) was performed for each
type of ROUGE score. The population marginal
means remove any effect of an unbalanced design
(since not all human peers created summaries for
all topics) by fixing the values of the ?peer? factor,
and averaging out the effects of the ?topic? factor
as if each factor combination occurred the same
number of times.
Table 3 shows multiple comparison of all peers
based on ANOVA of ROUGE-2 recall (ROUGE-
SU4 shows similar results). ROUGE-2 and
ROUGE-SU4 both distinguish human peers from
automatic ones. The difference in the ROUGE-2
5 10 15 20 25 30 35
10
15
20
25
30
35
Average scaled responsiveness (primary)
Av
er
ag
e 
sc
al
ed
 re
sp
on
siv
en
es
s 
(se
co
nd
ary
)
Figure 1: Primary vs. secondary average scaled
responsiveness
score of the best system and worst human is not
considered significant (possibly due to the very
conservative nature of the multiple comparison
test) but is still relatively large. On the other
hand, ANOVA of ROUGE-2 found more signifi-
cant differences between the automatic peers than
did Friedman?s test of responsiveness.
4.3 Correlation
A metric must produce stable rankings of systems
in the face of human variation. Intrinsic measures
like ROUGE rely on multiple model summaries to
take into account human variation (although Pyra-
mids add another level of human variation in the
manual pyramid and peer annotation). For a met-
ric like responsiveness, which does not depend on
comparison of peer summaries against a model or
set of model summaries, it is appropriate to con-
sider the stability of the measure across different
assessors.
A secondary assessment was done on respon-
siveness for the 20 topics that had 9 summaries
each. The secondary assessor had written a sum-
mary for the topic but was generally not the same
person who developed the topic. As seen in Figure
1, average scaled responsiveness scores from the
two sets of assessments (averaged over the 20 top-
ics) track each other very well. The human sum-
maries are clustered on the upper right side of the
graph, while the automatic summaries form a sec-
ond cluster on the lower left side.
The actual responsiveness scores for each sys-
tem and each topic do vary between assessors, but
this variation in human judgment is smoothed out
by averaging over multiple topics. Table 4 shows
that the correlation between the primary and sec-
53
RunID PMM of R2
C 0.1172 A
A 0.1156 A B
I 0.1023 A B C
B 0.1014 A B C
J 0.1012 A B C
E 0.1009 A B C
D 0.0986 A B C
G 0.0970 B C
F 0.0947 C
H 0.0897 C D
15 0.0725 D E
17 0.0717 E
10 0.0698 E F
8 0.0696 E F
4 0.0686 E F G
5 0.0675 E F G
11 0.0643 E F G H
14 0.0635 E F G H I
16 0.0633 E F G H I
19 0.0632 E F G H I
7 0.0628 E F G H I J
9 0.0625 E F G H I J
29 0.0609 E F G H I J K
25 0.0609 E F G H I J K
6 0.0609 E F G H I J K
24 0.0597 E F G H I J K
28 0.0594 E F G H I J K
3 0.0594 E F G H I J K
21 0.0573 E F G H I J K
12 0.0563 F G H I J K
18 0.0553 F G H I J K L
26 0.0547 F G H I J K L
27 0.0546 F G H I J K L
32 0.0534 G H I J K L
20 0.0515 H I J K L
13 0.0497 H I J K L
30 0.0496 H I J K L
31 0.0487 I J K L
2 0.0478 J K L
22 0.0462 K L
1 0.0403 L M
23 0.0256 M
Table 3: Multiple comparison of all peers based on ANOVA of ROUGE-2 recall
54
Spearman Pearson
All peers 0.900 0.976 [0.960, 1.000]
Auto peers 0.775 0.822 [0.695, 1.000]
Table 4: Correlation between primary and sec-
ondary average scaled responsiveness (20 topics),
with 95% confidence intervals for Pearson?s r.
ondary average scaled responsiveness scores is re-
spectable despite the low number of topics. The
correlation suggests that responsiveness would
give a stable ranking of the systems when aver-
aged over the entire set of 50 topics.
Table 5 shows that there is high correlation
between macro-average ROUGE scores (intrin-
sic measures) and average scaled responsiveness
(a pseudo-extrinisic measure). The correlation is
high even when the human summaries are ignored.
Metric Spearman Pearson
ROUGE-2 (all) 0.951 0.972 [0.953, 1.000]
ROUGE-SU4 (all) 0.942 0.958 [0.930, 1.000]
ROUGE-2 (auto) 0.901 0.928 [0.872, 1.000]
ROUGE-SU4 (auto) 0.872 0.919 [0.855, 1.000]
Table 5: Correlation between average scaled re-
sponsiveness and macro-average ROUGE recall
over all topics and either all peers or only auto-
matic peers.
5 Conclusion
The DUC 2005 task was to summarize the answer
to a complex question, as found in a set of docu-
ments. The evaluation showed that only the top
systems are able to extract sentences whose in-
formation content is more responsive to the ques-
tion than a simple baseline. Additionally, systems
require much additional work to produce coher-
ent, well-structured text, which is apparent in the
longer summary sizes of DUC 2005. On the other
hand, systems do well on non-redundancy, since
text summarization has historically been formu-
lated as a text compression task. Since DUC 2005
is the first time question-focused summarization
has been evaluated on a large-scale, we have re-
peated the task in 2006, with some modifications.
We eliminated the ?granularity? specification in
DUC 2006. Assessors had appreciated the theory
behind the granularity specification, but found that
the size limit for the summaries was a much big-
ger factor in determining what information to in-
clude; some ?specific? summaries ended up being
very general given the large amount of informa-
tion and limited space allowed. From a human
perspective, the actual granularity of the resulting
summary mostly fell out naturally from the topic
question and the content that was available in the
source documents.
The definition of responsiveness scores was
meant to yield a coarse ranking of the peer sum-
maries into 5 ordered clusters. However, asses-
sors found it difficult to form these 5 clusters be-
cause of the large number (36+) of summaries that
needed to be compared with one another, and the
impression that many sets of human and automatic
summaries could not be separated into as many
as 5 groups. We therefore changed the scoring
of responsiveness in 2006 so that it is based on
the same scale as the linguistic quality questions;
this may reduce the discriminative power of the
responsiveness measure but should produce scores
that more accurately reflect the true differences be-
tween summaries.
References
Enrique Amigo, Julio Gonzalo, Victor Peinado,
Anselmo Penas, and Felisa Verdejo. 2004. An em-
pirical study of information synthesis tasks. In Pro-
ceedings of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 207?214,
Barcelona, Spain.
Donna Harman and Paul Over. 2004. The effects of
human variation in duc summarization evaluation.
In Proceedings of the ACL-04 Workshop: Text Sum-
marization Branches Out, pages 10?17, Barcelona,
Spain.
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating duc 2005 using basic elements. In Pro-
ceedings of the Fifth Document Understanding Con-
ference (DUC), Vancouver, Canada.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the
ACL-04 Workshop: Text Summarization Branches
Out, pages 74?81, Barcelona, Spain.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying the
pyramid method in duc 2005. In Proceedings of the
Fifth Document Understanding Conference (DUC),
Vancouver, Canada.
Ellen M. Voorhees and Chris Buckley. 2002. The
effect of topic set size on retrieval experiment er-
ror. In Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 316?323, Tam-
pere, Finland, August.
55
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 23?30,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Evaluation of automatic summaries: Metrics under varying data
conditions
Karolina Owczarzak and Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@nist.gov hoa.dang@nist.gov
Abstract
In evaluation of automatic summaries, it
is necessary to employ multiple topics and
human-produced models in order for the
assessment to be stable and reliable. How-
ever, providing multiple topics and models
is costly and time-consuming. This paper
examines the relation between the number
of available models and topics and the cor-
relations with human judgment obtained
by automatic metrics ROUGE and BE, as
well as the manual Pyramid method. Test-
ing all these methods on the same data set,
taken from the TAC 2008 Summarization
track, allows us to compare and contrast
the methods under different conditions.
1 Introduction
Appropriate evaluation of results is an important
aspect of any research. In areas such as automatic
summarization, the problem is especially complex
because of the inherent subjectivity in the task it-
self and its evaluation. There is no single objective
standard for a good quality summary; rather, its
value depends on the summary?s purpose, focus,
and particular requirements of the reader (Sp?arck
Jones, 2007). While the purpose and focus can
be set as constant for a specific task, the variabil-
ity of human judgment is more difficult to con-
trol. Therefore, in attempts to produce stable eval-
uations, it has become standard to use multiple
judges, not necessarily for parallel evaluation, but
in such a way that each judge evaluates a differ-
ent subset of the many summaries on which the
final system assessment is based. The incorpora-
tion of multiple points of view is also reflected in
automatic evaluation, where it takes the form of
employing multiple model summaries to which a
candidate summary is compared.
Since these measures to neutralize judgment
variation involve the production of multiple model
summaries, as well as multiple topics, evaluation
can become quite costly. Therefore, it is inter-
esting to examine how many models and topics
are necessary to obtain a relatively stable eval-
uation, and whether this number is different for
manual and automatic metrics. In their exami-
nation of summary evaluations, van Halteren and
Teufel (2003) suggest that it is necessary to use
at least 30 to 40 model summaries for a stable
evaluation; however, Harman and Over (2004) ar-
gue that a stable evaluation can be conducted even
with a single model, as long as there is an ade-
quate number of topics. This view is supported by
Lin (2004a), who concludes that ?correlations to
human judgments were increased by using multi-
ple references but using single reference summary
with enough number of samples was a valid al-
ternative?. Interestingly, similar conclusions were
also reached in the area of Machine Translation
evaluation; in their experiments, Zhang and Vogel
(2004) show that adding an additional reference
translation compensates the effects of removing
10?15% of the testing data, and state that, there-
fore, ?it seems more cost effective to have more
test sentences but fewer reference translations?.
In this paper, we look at how various metrics
behave with respect to a variable number of top-
ics and models used in the evaluation. This lets us
determine the stability of individual metrics, and
helps to illuminate the trade-offs inherent in de-
signing a good evaluation. For our experiments,
we used data from the Summarization track at the
Text Analysis Conference (TAC) 2008, where par-
ticipating systems were assessed on their summa-
rization of 48 topics, and the automatic metrics
ROUGE and BE, as well as the manual Pyramid
evaluation method, had access to 4 human mod-
els. TAC 2008 was the first task of the TAC/DUC
(Document Understanding Conference) series in
which the Pyramid method was used on all evalu-
ated data, making it possible to conduct a full com-
23
parison among the manual and automatic meth-
ods. Despite the lack of full Pyramid evaluation
in DUC 2007, we look at the remaining metrics
applied that year (ROUGE, BE, and Content Re-
sponsiveness), in order to see whether they con-
firm the insights gained from the TAC 2008 data.
2 Summary evaluation
The main evaluation at TAC 2008 was performed
manually, assessing the automatic candidate sum-
maries with respect to Overall Responsiveness,
Overall Readability, and content coverage accord-
ing to the Pyramid framework (Nenkova and Pas-
sonneau, 2004; Passonneau et al, 2005). Task par-
ticipants were asked to produce two summaries for
each of the 48 topics; the first (initial summary)
was a straightforward summary of 10 documents
in response to a topic statement, which is a request
for information about a subject or event; the sec-
ond was an update summary, generated on the ba-
sis of another set of 10 documents, which followed
the first set in temporal order and described further
developments in the given topic. The idea behind
the update summary was to avoid repeating all the
information included in the first set of documents,
on the assumption that the reader is familiar with
that information already.
The participating teams submitted up to three
runs each; however, only the first and second
runs were evaluated manually due to limited re-
sources. For each summary under evaluation, as-
sessors rated the summary from 1 (very poor) to
5 (very good) in terms of Overall Responsiveness,
which measures how well the summary responds
to the need for information expressed in the topic
statement and whether its linguistic quality is ad-
equate. Linguistic qualities such as grammatical-
ity, coreference, and focus were also evaluated as
Overall Readability, also on the scale from 1 to
5. Content coverage of each summary was evalu-
ated using the Pyramid framework, where asses-
sors create a list of information nuggets (called
Summary Content Units, or SCUs) from the set of
human-produced summaries on a given topic, then
decide whether any of these nuggets are present in
the candidate summary. All submitted runs were
evaluated with the automatic metrics: ROUGE
(Lin, 2004b), which calculates the proportion of
n-grams shared between the candidate summary
and the reference summaries, and Basic Elements
(Hovy et al, 2005), which compares the candidate
to the models in terms of head-modifier pairs.
2.1 Manual metrics
Evaluating Overall Responsiveness and Overall
Readability is a rather straightforward procedure,
as most of the complex work is done in the mind
of the human assessor. Each candidate summary
is given a single score, and the final score for
the summarization system is the average of all its
summary-level scores. The only economic factor
here is the number of topics, i.e. summaries per
system, that need to be judged in order to neutral-
ize both intra- and inter-annotator variability and
obtain a reliable assessment of the summarization
system.
When it comes to the Pyramid method, which
measures content coverage of candidate sum-
maries, the need for multiple topics is accompa-
nied by the need for multiple human model sum-
maries. First, independent human assessors pro-
duce summaries for each topic, guided by the topic
statement. Next, in the Pyramid creation stage,
an assessor reads all human-produced summaries
for a given topic and extracts all ?information
nuggets?, called Summary Content Units (SCUs),
which are short, atomic statements of facts con-
tained in the text. Each SCU has a weight which
is directly proportional to the number of model
summaries in which it appears, on the assumption
that the fact?s importance is reflected in how many
human summarizers decide to include it as rele-
vant in their summary. Once all SCUs have been
harvested from the model summaries, an assessor
then examines each candidate summary to see how
many of the SCUs from the list it contains. The fi-
nal Pyramid score for a candidate summary is its
total SCU weight divided by the maximum SCU
weight available to a summary of average length
(where the average length is determined by the
mean SCU count of the model summaries for this
topic). The final score for a summarization system
is the average score of all its summaries. In TAC
2008, the evaluation was conducted with 48 topics
and 4 human models for each topic.
We examined to what extent the number of
models and topics used in the evaluation can in-
fluence the Pyramid score and its stability. The
stability, similarly to the method employed by
Voorhees and Buckley (2002) for Information Re-
trieval, is determined by how well a system rank-
ing based on a small number of models/topics cor-
24
Models Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.8839 0.8032 0.7842 0.7680
2 0.8943 0.8200 0.7957 0.7983
3 0.8974* 0.8258 0.7999* 0.8098
4 (bootstr) 0.8972* 0.8310 0.8023* 0.8152
4 (actual) 0.8997 0.8302 0.8033 0.8171
Table 1: Mean correlations of Responsiveness and other met-
rics using 1, 2, 3, or 4 models for TAC 2008 initial summaries.
Values in each row are significantly different from each other at
95% level.
Models Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.9315 0.8861 0.8874 0.8716
2 0.9432 0.9013 0.8961 0.8978
3 0.9474* 0.9068* 0.8994 0.9076
4 (bootstr) 0.9481* 0.9079* 0.9023 0.9114
4 (actual) 0.9492 0.9103 0.9020 0.9132
Table 2: Mean correlations of Responsiveness and other met-
rics using 1, 2, 3, or 4 models for TAC 2008 update summaries.
Values in each row are significantly different from each other
at 95% level except ROUGE-2 and ROUGE-SU4 in 1-model
category.
Models ROUGE-2 ROUGE-SU4 BE
1 0.8789 0.8671 0.8553
2 0.8972 0.8803 0.8917
3 0.9036 0.8845 0.9048
4 (bootstr) 0.9082 0.8874 0.9107
4 (actual) 0.9077 0.8877 0.9123
Table 3: Mean correlations of 4-model Pyramid score and
other metrics using 1, 2, 3, or 4 models for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except ROUGE-2 and BE in 4-model
category.
Models ROUGE-2 ROUGE-SU4 BE
1 0.9179 0.9110 0.9016
2 0.9336 0.9199 0.9284
3 0.9392 0.9233 0.9383
4 (bootstr) 0.9443 0.9277 0.9436
4 (actual) 0.9429 0.9263 0.9446
Table 4: Mean correlations of 4-model Pyramid score and
other metrics using 1, 2, 3, or 4 models for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except ROUGE-2 and BE in 4-model
category.
relates with the ranking based on another set of
models/topics, where the two sets are randomly
selected and mutually exclusive. This methodol-
ogy allows us to check the correlations based on
up to half of the actual number of models/topics
only (because of the non-overlap requirement), but
it gives an indication of the general tendency. We
also look at the correlation between the Pyramid
score and Overall Responsiveness. We don?t ex-
pect a perfect correlation between Pyramid and
Responsiveness in the best of times, because Pyra-
mid measures content identity between the can-
didate and the model, and Responsiveness mea-
sures content relevance to topic as well as linguis-
tic quality. However, the degree of variation be-
tween the two scores depending on the number of
models/topics used for the Pyramid will give us
a certain indication of the amount of information
lost.
2.2 Automatic metrics
Similarly to the Pyramid method, ROUGE (Lin,
2004b) and Basic Elements (Hovy et al, 2005)
require multiple topics and model summaries to
produce optimal results. ROUGE is a collection
of automatic n-gram matching metrics, ranging
from unigram to four-gram. It also includes mea-
surements of the longest common subsequence,
weighted or unweighted, and the option to com-
pare stemmed versions of words and omit stop-
words. There is also the possibility of accept-
ing skip-n-grams, that is, counting n-grams as
matching even if there are some intervening non-
matching words. The skip-n-grams together with
stemming are the only ways ROUGE can acco-
modate alternative forms of expression and match
concepts even though they might differ in terms of
their syntactic or lexical form.
These methods are necessarily limited, and so
ROUGE relies on using multiple parallel model
summaries which serve as a source of lexi-
cal/syntactic variation in the comparison process.
The fewer models there are, the less reliable the
score. Our question here is not only what this rela-
tion looks like (as it was examined on the basis of
Document Understanding Conference data in Lin
(2004a)), but also how it compares to the reliabil-
ity of other metrics.
Basic Elements (BE), on the other hand, goes
beyond simple string matching and parses the syn-
tactic structure of the candidate and model to ob-
tain a set of head-modifier pairs for each, and then
compares the sets. A head-modifier pair consist of
the head of a syntactic unit (e.g. the noun in a noun
phrase), and the word which modifes the head (i.e.
a determiner in a noun phrase). It is also possible
to include the name of the relation which connects
them (i.e. subject, object, etc.). Since BEs reflect
thematic relations in a sentence rather than surface
word order, it should be possible to accommodate
certain differences of expression that might appear
between a candidate summary and a reference, es-
pecially as the words can be stemmed. This could,
in theory, allow us to use fewer models for the
evaluation. In practice, however, it fails to account
for the total possible variety, and, what is more,
25
the additional step of parsing the text can intro-
duce noise into the comparison.
TAC 2008 and DUC 2007 evaluations used
ROUGE-2 and ROUGE-SU4, which refer to the
recall of bigram and skip-bigram (with up to 4 in-
tervening words) matches on stemmed words, re-
spectively, as well as a BE score calculated on the
basis of stemmed head-modifier pairs without re-
lation labels. Therefore, these are the versions we
use in our comparisons.
3 Number of models
Since Responsiveness score does not depend on
the number of models, it serves as a reference
against which we compare the remaining metrics,
while we calculate their score with only 1, 2, 3, or
all 4 models. Given 48 topics in TAC 2008, and
4-model summaries for each topic, there are 4
48
possible combinations to derive the final score in
the single-model category, so to keep the experi-
ments simple we only selected 1000 random sam-
ples from that space. For 1000 repetitions, each
time we selected a random combination of model
summaries (only one model out of 4 available per
topic), against which we evaluated the candidate
summaries. Then, for each of the 1000 samples,
we calculated the correlation between the result-
ing score and Responsiveness. We then took the
1000 correlations produced in this manner, and
computed their mean. In the same way, we cal-
culated the scores based on 2 and 3 model sum-
maries, randomly selected from the 4 available for
each topic. The correlation means for all metrics
and categories are given in Table 1 for initial sum-
maries and Table 2 for update summaries. We also
ran a one-way analysis of variance (ANOVA) on
these correlations to determine whether the cor-
relation means were significantly different from
each other. For the 4-model category there was
only one possible sample for each metric, so in or-
der to perform ANOVA we bootstrapped this sam-
ple to produce 1000 samples. The actual value of
the 4-model correlation is given in the tables as 4
(actual), and the mean value of the bootstrapped
1000 correlations is given as 4 (bootstr).
Values for initial summaries are significantly
different from their counterparts for update sum-
maries at the 95% level. Pairwise testing of values
for statistically significant differences is shown
with symbols: in each column, the first value
marked with a particular symbol is not signifi-
cantly different from any subsequent value marked
with the same symbol.
We also examined the correlations of the met-
rics with the 4-model Pyramid score. Table 3
presents the correlation means for the initial sum-
maries, and Table 4 shows the correlation means
for the update summaries.
Since the Pyramid, contrary to Responsiveness,
makes use of multiple model summaries, we ex-
amine its stability given a decreased number of
models to rely on. For this purpose, we correlated
the Pyramid score based on randomly selected 2
models (half of the model pool) for each topic with
the score based on the remaining 2 models, and
repeated this 1000 times. We also looked at the
1-model category, where the Pyramid score cal-
culated on the basis of one model per topic was
correlated with the Pyramid score calculated on
the basis on another randomly selected model. In
both case we witness a very high mean correlation:
0.994 and 0.995 for the 2-model category, 0.982
and 0.985 for the 1-model category for TAC initial
and update summaries, respectively. As an illus-
tration, Figure1 shows the variance of correlations
for the initial summaries.
Figure 1: Correlations between Pyramid scores based on 1
or 2 model summaries for TAC 2008 initial summaries.
The variation in correlation levels between
other metrics and Pyramid and Responsiveness,
presented in Tables 3?4, is more visible in the
graph form. Figures 2-3 illustrate the mean
correlation values for TAC 2008 initial sum-
maries. While all the metrics record the steep-
est increase in correlation values with the addi-
tion of the second model, adding the third and
fourth model provides the metrics with smaller
but steady improvement, with the exception of
Pyramid-Responsiveness correlation in Figure 2.
The increase in correlation mean is most dramatic
for BE, which in all cases starts as the lowest-
26
correlating metric in the single-model category,
but by the 4-model point it outperforms one or
both versions of ROUGE. The Pyramid metric
achieves significantly higher correlations than any
other metric, independent of the number of mod-
els, which is perhaps unsurprising given that it is a
manual evaluation method. Of the two ROUGE
versions, ROUGE-2 seems consistently a better
predictor of both Responsiveness and the ?full? 4-
model Pyramid score than ROUGE-SU4.
Figure 2: Responsiveness vs. other metrics with 1, 2, 3, or
4 models for TAC 2008 initial summaries.
Figure 3: 4-model Pyramid vs. other metrics with 1, 2, 3,
or 4 models for TAC 2008 initial summaries.
Similar patterns appear in DUC 2007 data (Ta-
ble 5), despite the fact that the Overall Respon-
siveness of TAC 2008 is replaced with Content Re-
sponsiveness (ignoring linguistic quality), against
which we calculate all the correlations. Although
the increase in correlation means from 1- to 4-
models for the three automatic metrics is smaller
than for TAC 2008, the clearest rise occurs with
the addition of a second model, especially for BE,
and the subsequent additions change little. As in
the case of initial summaries 2008, ROUGE-2 out-
performs the remaining two metrics independently
of the number of models. However, most of the in-
creases are too small to be significant.
This comparison suggests diminishing returns
Models ROUGE-2 ROUGE-SU4 BE
1 0.8681 0.8254 0.8486
2 0.8747* 0.8291* 0.8577*
3 0.8766*? 0.8299*? 0.8599*
4 (bootstr) 0.8761*? 0.8305*? 0.8633
4 (actual) 0.8795 0.8301 0.8609
Table 5: Mean correlations of Content Responsiveness and
other metrics using 1, 2, 3, or 4 models for DUC 2007 sum-
maries. Values in each row are significantly different from
each other at 95% level.
with the addition of more models, as well as dif-
ferent reactions among the metrics to the presence
or absence of additional models. When correlating
with Responsiveness, the manual Pyramid metric
benefits very little from the fourth model, but au-
tomatic BE benefits most from almost every addi-
tion. ROUGE is situated somewhere between the
two, noting small but often significant increases.
On the whole, the use of multiple models (at least
two) seems supported, especially if we use auto-
matic metrics in our evaluation.
4 Number of topics
For the second set of experiments we kept all four
models, but varied the number of topics which
went into the final average system score. To deter-
mine the stability of Responsiveness and Pyramid
we looked at the correlations between the scores
based on smaller sets of topics. For 1000 rep-
etitions, we calculated Pyramid/Responsiveness
score based on a set of 1, 3, 6, 12, or 24 topics ran-
domly chosen from the pool of 48, and compared
the system ranking thus created with the ranking
based on another, equally sized set, such that the
sets did not contain common topics. Table 6 shows
the mean correlation for each case. Although such
comparison was only possible up to 24 topics (half
of the whole available topic pool), the numbers
suggest that at the level of 48 topics both Respon-
siveness and Pyramid are stable enough to serve as
reference for the automatic metrics.
Responsiveness Pyramid
Topics Initial Update Initial Update
1 0.182 0.196 0.333 0.267
3 0.405 0.404 0.439 0.520
6 0.581 0.586 0.608 0.690
12 0.738 0.738 0.761 0.816
24 0.849 0.866 0.851 0.901
Table 6: Mean correlations between Responsive-
ness/Pyramid scores based on 1, 3, 6, 12, and 24 topic sam-
ples for TAC 2008 initial and update summaries.
In a process which mirrored that described in
Section 3, we created 1000 random samples in
each of the n-topics category: 1, 3, 6, 12, 24, 36,
27
Topics Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.4219 0.4276 0.4375 0.3506
3 0.6204 0.5980 0.9016 0.5108
6 0.7274 0.6901 0.6836 0.6233
12 0.8159 0.7618 0.7456 0.7117
24 0.8679 0.8040 0.7809 0.7762
36 0.8890* 0.8208* 0.7951* 0.8017*
39 0.8927*? 0.8231*? 0.7967*? 0.8063*?
42 0.8954*?? 0.8258*?? 0.7958*?? 0.8102*??
45 0.8977*??? 0.8274*??? 0.8008*??? 0.8132???
48 (bootstr) 0.8972*??? 0.8302*??? 0.8046??? 0.8138???
48 (actual) 0.8997 0.8302 0.8033 0.8171
Table 7: Mean correlations of 48 topic Responsiveness and
other metrics using from 1 to 48 topics for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2, ROUGE-SU4 and
BE in 1-topic category, ROUGE-2 and ROUGE-SU4 in 3- and
6-topic category.
Topics Pyramid ROUGE-2 ROUGE-SU4 BE
1 0.5005 0.4882 0.5609 0.4011
3 0.7053 0.6862 0.7340 0.6097
6 0.8080 0.7850 0.8114 0.7274
12 0.8812 0.8498 0.8596 0.8188
24 0.9250 0.8882 0.8859 0.8774
36 0.9408* 0.9023* 0.8960* 0.8999*
39 0.9433*? 0.9045*? 0.8973*? 0.9037*?
42 0.9455*?? 0.9061*?? 0.8987*?? 0.9068*??
45 0.9474??? 0.9078*??? 0.8996*??? 0.9094???
48 (bootstr) 0.9481??? 0.9101??? 0.9015*??? 0.9111???
48 (actual) 0.9492 0.9103 0.9020 0.9132
Table 8: Mean correlations of 48 topic Responsiveness and
other metrics using from 1 to 48 topics for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except: Pyramid and ROUGE-2 in 1-
topic category, Pyramid and ROUGE-SU4 in 6-topic category,
ROUGE-2 and BE in 39-, 42-, and 48-topic category.
Topics ROUGE-2 ROUGE-SU4 BE
1 0.4693 0.4856 0.3888
3 0.6575 0.6684 0.5732
6 0.7577 0.7584 0.6960
12 0.8332 0.8245 0.7938
24 0.8805 0.8642 0.8684
36 0.8980* 0.8792* 0.8966*
39 0.9008*? 0.8812*? 0.9017*?
42 0.9033*?? 0.8839*?? 0.9058??
45 0.9052*??? 0.8853*??? 0.9093???
48 (bootstr) 0.9074??? 0.8877??? 0.9107???
48 (actual) 0.9077 0.8877 0.9123
Table 9: Mean correlations of 48 topic Pyramid score and
other metrics using from 1 to 48 topics for TAC 2008 initial
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2 and ROUGE-SU4
in the 6-topic category, ROUGE-2 and BE in 39- and 48-topic
category.
Topics ROUGE-2 ROUGE-SU4 BE
1 0.5026 0.5729 0.4094
3 0.7106 0.7532 0.6276
6 0.8130 0.8335 0.7512
12 0.8806 0.8834 0.8475
24 0.9196 0.9092 0.9063
36 0.9343* 0.9198* 0.9301*
39 0.9367*? 0.9213*? 0.9341*?
42 0.9386*?? 0.9227*?? 0.9376*??
45 0.9402*??? 0.9236*??? 0.9402???
48 (bootstr) 0.9430??? 0.9280? 0.9444??
48 (actual) 0.9429 0.9263 0.9446
Table 10: Mean correlations of 48 topic Pyramid score and
other metrics using from 1 to 48 topics for TAC 2008 update
summaries. Values in each row are significantly different from
each other at 95% level except: ROUGE-2 and ROUGE-SU4
in 12-topic category, ROUGE-2 and BE in 45-topic category.
39, 42, or 45. Within each of these categories, for
a thousand repetitions, we calculated the score for
automatic summarizers by averaging over n topics
randomly selected from the pool of 48 topics avail-
able in the evaluation. Again, we examined the
correlations between the metrics and the ?full? 48-
topic Responsiveness and Pyramid. As previously,
we then used ANOVA to determine whether the
correlation means differed significantly. Because
there was only one possible sample with all 48
topics for each metric, we bootstrapped this sam-
ple to provide 1000 new samples in the 48-topic
category, in order to perfom the ANOVA compari-
son of variance. Tables 7 and 8, as well as Figures
4 and 5, show the metrics? changing correlations
with Responsiveness. Tables 9 and 10, and Fig-
ures 6 and 7, show the correlations with the 48-
topic Pyramid score. Values for initial summaries
are significantly different from their counterparts
for update summaries at the 95% level.
In all cases, it becomes clear that the curves flat-
ten out and the correlations stop increasing almost
completely beyond the 36-topic mark. This means
that the scores for the automatic summarization
systems based on 36 topics will be on average
practically indistiguishable from the scores based
on all 48 topics, showing that beyond a certain
minimally necessary number of topics adding or
removing a few (or even ten) topics will not influ-
ence the system scores much. (However, we can-
not conclude that a further considerable increase in
the number of topics ? well beyond 48 ? would not
bring more improvement in the correlations, per-
haps increasing the stable ?correlation window? as
well.)
Topics ROUGE-2 ROUGE-SU4 BE
1 0.6157 0.6378 0.5756
3 0.7597 0.7511 0.7323
6 0.8168 0.7904 0.7957
12 0.8493 0.8123 0.8306
24 0.8690 0.8249* 0.8517*
36 0.8751* 0.8287*? 0.8580*?
39 0.8761*? 0.8295*?? 0.8592??
42 0.8768*?? 0.8299*??? 0.8602???
45 (bootstr) 0.8761*?? 0.8305??? 0.8627???
45 (actual) 0.8795 0.8301 0.8609
Table 11: Mean correlations of 45 topic Content Respon-
siveness and other metrics using from 1 to 45 topics for DUC
2007 summaries. Values in each row are significantly differ-
ent from each other at 95% level.
An interesting observation is that if we pro-
duce such limited-topic scores for the manual
metrics, Responsiveness and Pyramid, and corre-
late them with their own ?full? versions based on
28
Figure 4: Responsiveness vs. other metrics with 1 to 48 topics
for TAC 2008 initial summaries.
Figure 5: Responsiveness vs. other metrics with 1 to 48 topics
for TAC 2008 update summaries.
Figure 6: 48-topic Pyramid vs. other metrics with 1 to 48
topics for TAC 2008 initial summaries.
Figure 7: 48-topic Pyramid vs. other metrics with 1 to 48
topics for TAC 2008 update summaries.
all 48 topics, it appears that they are less stable
than the automatic metrics, i.e. there is a larger
gap between the worst and best correlations they
achieve.
1
The mean correlation between the ?full?
Responsiveness and that based on 1 topic is 0.443
and 0.448 for the initial and update summaries, re-
spectively; for that based on 3 topics, 0.664 and
0.667. Pyramid based on 1 topic achieves 0.467
for initial and 0.525 for update summaries; Pyra-
mid based on 3 topics obtains 0.690 and 0.742,
respectively. Some of these values, especially
for update summaries, are even lower than those
obtained by ROUGE in the same category, de-
spite the fact that 1- and 3-topic Responsiveness or
Pyramid is a proper subset of the 48-topic Respon-
siveness/Pyramid. On the other hand, ROUGE
achieves considerably worse correlations with Re-
sponsiveness than Pyramid when there are many
topics available. ROUGE-SU4 seems to be more
stable than ROUGE-2; in all cases ROUGE-2
starts with lower correlations than ROUGE-SU4,
but by the 12-topic mark its correlations increase
1
For reasons of space, these values are not included in the
tables, as they offer little insight besides what is mentioned
here.
above it.
Additionally, despite being an automatic metric,
BE seems to follow the same pattern as the manual
metrics. It is seriously affected by the decreasing
number of topics; in fact, if the number of topics
drops below 24, BE is the least reliable indicator
of either Responsiveness or Pyramid. However,
by the 48-topic mark it rises to levels comparable
with ROUGE-2.
As in the case of models, DUC 2007 data shows
mostly the same pattern as TAC 2008. Again, in
this data set, the increase in the correlation mean
with the addition of topics for each metric are
smaller than for either initial or update summaries
in TAC 2008, but the relative rate of increase re-
mains the same: BE gains most from additional
topics (+0.28 in DUC vs. +0.47 and +0.51 in
TAC), ROUGE-SU4 again shows the smallest in-
crease (+0.19 in DUC vs. +0.36 and +0.34 in
TAC), which means it is the most stable of the met-
rics across the variable number of topics.
2
2
The smaller total increase might be due to the smaller
number of available topics (45 in DUC vs. 48 in TAC), but
we have seen the same effect in Section 3 while discussing
models, so it might just be an accidental property of a given
data set.
29
5 Discussion and conclusions
As the popularity of shared tasks increases, task
organizers face an ever growing problem of pro-
viding an adequate evaluation to all participating
teams. Often, evaluation of multiple runs from the
same team is required, as a way to foster research
and development. With more and more system
submissions to judge, and the simultaneous need
for multiple topics and models in order to provide
a stable assessment, difficult decisions of cutting
costs and effort might sometimes be necessary. It
would be useful then to know where such deci-
sions will have the smallest negative impact, or at
least, what might be the trade-offs inherent in such
decisions.
From our experiments, it appears that manual
metrics such as Pyramid gain less from the addi-
tion of more model summaries than the automatic
metrics. A Pyramid score based on any two mod-
els correlates very highly with the score based on
any other two models. For the automatic metrics,
the largest gain is recorded with adding the sec-
ond model; afterwards the returns diminish. BE
seems to be the most sensitive metric to changes in
the number of models and topics; ROUGE-SU4,
on the other hand, is the least sensitive to such
changes and the most stable, but it does not ob-
tain the highest correlations when many models
and topics are available.
Whatever the number of models, manual Pyra-
mid considerably outperforms automatic metrics,
as can be expected, since human understanding is
not hampered by the possible differences in sur-
face expression between a candidate and a model.
But when it comes to decreased number of topics,
the inherent variability of human judgment shows
strongly, to the extent that, in extreme cases of
very few topics, it might be more prudent to use
ROUGE-SU4 than Pyramid or Responsiveness.
Lastly, we observe that, as with models, adding
one or two topics to the evaluation plays a great
role only if we have very few topics to start with.
Our experiments suggest that, as the number of
topics available for evaluation increases, so does
the number of additional topics necessary to make
a difference in the system ranking produced by
a metric. It seems that in the case of evaluation
based on 48 topics, as in the TAC Summarization
track, it would be possible to decrease the number
to about 36 without sacrificing much stability.
References
Donna Harman and Paul Over. 2004. The effects of
human variation in DUC summarization evaluation.
In Proceedings of the ACL-04 Workshop: Text Sum-
marization Branches Out, pages 10?17, Barcelona,
Spain.
Eduard Hovy, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating DUC 2005 using Basic Elements. In
Proceedings of the 5th Document Understanding
Conference (DUC).
Chin-Yew Lin. 2004a. Looking for a few good met-
rics: Automatic summarization evaluation - how
many samples are enough? In Proceedings of NT-
CIR Workshop 4, Tokyo, Japan.
Chin-Yew Lin. 2004b. ROUGE: A package for au-
tomatic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop: Text Summarization
Branches Out, pages 74?81.
Ani Nenkova and Rebecca J. Passonneau. 2004.
Evaluating content selection in summarization: The
Pyramid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 145?152, Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McK-
eown, and Sergey Sigelman. 2005. Applying
the Pyramid method in DUC 2005. In Proceed-
ings of the 5th Document Understanding Conference
(DUC), Vancouver, Canada.
Karen Sp?arck Jones. 2007. Automatic summarising:
The state of the art. Information Processing and
Management, 43(6):1449?1481.
Hans van Halteren and Simone Teufel. 2003. Examin-
ing the consensus between human summaries: Ini-
tial experiments with factoid analysis. In Proceed-
ings of the HLT-NAACL DUCWorkshop 2003, pages
57?64, Edmonton, Canada.
Ellen M. Voorhees and Chris Buckley. 2002. Effect of
topic set size on retrieval experiment error. In Pro-
ceedings of the 25th Annual International ACM SI-
GIR conference on Research and Development in In-
formation Retrieval, pages 317?323, Tampere, Fin-
land.
Ying Zhang and Stephan Vogel. 2004. Measuring
confidence intervals for the Machine Translation
evaluation metrics. In Proceedings of the 10th Con-
ference on Theoretical and Methodological Issues in
Machine Translation, pages 85?94, Baltimore, MD.
30
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?362,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Assessing the Effect of Inconsistent Assessors on Summarization Evaluation
Karolina Owczarzak
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@gmail.com
Peter A. Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroy@super.org
Abstract
We investigate the consistency of human as-
sessors involved in summarization evaluation
to understand its effect on system ranking and
automatic evaluation techniques. Using Text
Analysis Conference data, we measure anno-
tator consistency based on human scoring of
summaries for Responsiveness, Readability,
and Pyramid scoring. We identify inconsis-
tencies in the data and measure to what ex-
tent these inconsistencies affect the ranking
of automatic summarization systems. Finally,
we examine the stability of automatic metrics
(ROUGE and CLASSY) with respect to the
inconsistent assessments.
1 Introduction
Automatic summarization of documents is a re-
search area that unfortunately depends on human
feedback. Although attempts have been made at au-
tomating the evaluation of summaries, none is so
good as to remove the need for human assessors.
Human judgment of summaries, however, is not per-
fect either. We investigate two ways of measuring
evaluation consistency in order to see what effect it
has on summarization evaluation and training of au-
tomatic evaluation metrics.
2 Assessor consistency
In the Text Analysis Conference (TAC) Summariza-
tion track, participants are allowed to submit more
than one run (usually two), and this option is of-
ten used to test different settings or versions of the
same summarization system. In cases when the sys-
tem versions are not too divergent, they sometimes
produce identical summaries for a given topic. Sum-
maries are randomized within each topic before they
are evaluated, so the identical copies are usually in-
terspersed with 40-50 other summaries for the same
topic and are not evaluated in a row. Given that each
topic is evaluated by a single assessor, it then be-
comes possible to check assessor consistency, i.e.,
whether the assessor judged the two identical sum-
maries in the same way.
For each summary, assessors conduct content
evaluation according to the Pyramid framework
(Nenkova and Passonneau, 2004) and assign it Re-
sponsiveness and Readability scores1, so assessor
consistency can be checked in these three areas sep-
arately. We found between 230 (in 2009) and 430
(in 2011) pairs of identical summaries for the 2008-
2011 data (given on average 45 topics, 50 runs, and
two summarization conditions: main and update),
giving in effect anywhere from around 30 to 60 in-
stances per assessor per year. Using Krippendorff?s
alpha (Freelon, 2004), we calculated assessor con-
sistency within each year, as well as total consis-
tency over all years? data (for those assessors who
worked multiple years). Table 1 shows rankings of
assessors in 2011, based on their Readability, Re-
sponsiveness, and Pyramid judgments for identical
summary pairs (around 60 pairs per assessor).
Interestingly, consistency values for Readability
are lower overall than those for Responsiveness and
Pyramid, even for the most consistent assessors.
Given that Readability and Responsiveness are eval-
uated in the same way, i.e. by assigning a numeri-
cal score according to detailed guidelines, this sug-
1http://www.nist.gov/tac/2011/Summarization/Guided-
Summ.2011.guidelines.html
359
ID Read ID Resp ID Pyr
G 0.867 G 0.931 G 0.975
D 0.866 D 0.875 D 0.970
A 0.801 H 0.808 H 0.935
H 0.783 A 0.750 A 0.931
F 0.647 F 0.720 E 0.909
C 0.641 E 0.711 C 0.886
E 0.519 C 0.490 F 0.872
Table 1: Annotator consistency in assigning Readability
and Responsiveness scores and in Pyramid evaluation, as
represented by Krippendorff?s alpha for interval values,
on 2011 data.
gests that Readability as a quality of text is inher-
ently more vague and difficult to pinpoint.
On the other hand, Pyramid consistency values
are generally the highest, which can be explained
by how the Pyramid evaluation is designed. Even
if the assessor is inconsistent in selecting Sum-
mary Content Units (SCUs) across different sum-
maries, as long as the total summary weight is sim-
ilar, the summary?s final score will be similar, too.2
Therefore, it would be better to look at whether as-
sessors tend to find the same SCUs (information
?nuggets?) in different summaries on the same topic,
and whether they annotate them consistently. This
can be done using the ?autoannotate? function of
the Pyramid process, where all SCU contributors
(selected text strings) from already annotated sum-
maries are matched against the text of a candidate
(un-annotated) summary. The autoannotate func-
tion works fairly well for matching between extrac-
tive summaries, which tend to repeat verbatim whole
sentences from source documents.
For each summary in 2008-2011 data, we autoan-
notated it using all remaining manually-annotated
summaries from the same topic, and then we com-
pared the resulting ?autoPyramid? score with the
score from the original manual annotation for that
summary. Ideally, the autoPyramid score should
be lower or equal to the manual Pyramid score: it
would mean that in this summary, the assessor se-
lected as relevant all the same strings as s/he found
in the other summaries on the same topic, plus possi-
bly some more information that did not appear any-
2The final score is based on total weight of all SCUs found
in the summary, so the same weight can be obtained by select-
ing a larger number of lower-weight SCUs or a smaller number
of higher-weight SCUs (or the same number of similar-weight
SCUs which nevertheless denote different content).
Figure 1: Annotator consistency in selecting SCUs in
Pyramid evaluation, as represented by the difference be-
tween manual Pyramid and automatic Pyramid scores
(mP-aP), on 2011 data.
where else. If the autoPyramid score is higher than
the manual Pyramid score, it means that either (1)
the assessor missed relevant strings in this summary,
but found them in other summaries; or (2) the strings
selected as relevant elsewhere in the topic were acci-
dental, and as such not repeated in this summary. Ei-
ther way, if we then average out score differences for
all summaries for a given topic, it will give us a good
picture of the annotation consistency in this partic-
ular topic. Higher average autoPyramid scores sug-
gest that the assessor was missing content, or other-
wise making frequent random mistakes in assigning
content. Figure 1 shows the macro-average differ-
ence between manual Pyramid scores and autoPyra-
mid scores for each assessor in 2011.3 For the most
part, it mirrors the consistency ranking from Table
1, confirming that some assessors are less consistent
than others; however, certain differences appear: for
instance, Assessor A is one of the most consistent in
assigning Readability scores, but is not very good at
selecting SCUs consistently. This can be explained
by the fact that the Pyramid evaluation and assigning
Readability scores are different processes and might
require different skills and types of focus.
3 Impact on evaluation
Since human assessment is used to rank participat-
ing summarizers in the TAC Summarization track,
3Due to space constraints, we report figures for only 2011,
but the results for other years are similar.
360
Pearson?s r Spearman?s rho
-1 worst -2 worst -1 worst -2 worst
Readability 0.995 0.993 0.988 0.986
Responsiveness 0.996 0.989 0.986 0.946
Pyramid 0.996 0.992 0.978 0.960
mP-aP 0.996 0.987 0.975 0.943
Table 2: Correlation between the original summarizer
ranking and the ranking after excluding topics by one or
two worst assessors in each category.
we should examine the potential impact of incon-
sistent assessors on the overall evaluation. Because
the final summarizer score is the average over many
topics, and the topics are fairly evenly distributed
among assessors for annotation, excluding noisy
topics/assessors has very little impact on summa-
rizer ranking. As an example, consider the 2011 as-
sessor consistency data in Table 1 and Figure 1. If
we exclude topics by the worst performing assessor
from each of these categories, recalculate the sum-
marizer rankings, and then check the correlation be-
tween the original and newly created rankings, we
obtain results in Table 2.
Although the impact on evaluating automatic
summarizers is small, it could be argued that exclud-
ing topics with inconsistent human scoring will have
an impact on the performance of automatic evalua-
tion metrics, which might be unfairly penalized by
their inability to emulate random human mistakes.
Table 3 shows ROUGE-2 (Lin, 2004), one of the
state-of-the-art automatic metrics used in TAC, and
its correlations with human metrics, before and af-
ter exclusion of noisy topics from 2011 data. The
results are fairly inconclusive: it seems that in most
cases, removing topics does more harm than good,
suggesting that the signal-to-noise ratio is still tipped
in favor of signal. The only exception is Readability,
where ROUGE records a slight increase in correla-
tion; this is unsurprising, given that consistency val-
ues for Readability are the lowest of all categories,
and perhaps here removing noise has more impact.
In the case of Pyramid, there is a small gain when
we exclude the single worst assessor, but excluding
two assessors results in a decreased correlation, per-
haps because we remove too much valid information
at the same time.
A different picture emerges when we examine
how well ROUGE-2 can predict human scores on
the summary level. We pooled together all sum-
Readability Responsiveness Pyramid mP-aP
before 0.705 0.930 0.954 0.954
-1 worst 0.718 0.921 0.961 0.942
-2 worst 0.718 0.904 0.952 0.923
Table 3: Correlation between the summarizer rankings
according to ROUGE-2 and human metrics, before and
after excluding topics by one or two worst assessors in
that category.
Readability Responsiveness Pyramid mP-aP
before 0.579 0.694 0.771 0.771
-1 worst 0.626 0.695 0.828 0.752
-2 worst 0.628 0.721 0.817 0.741
Table 4: Correlation between ROUGE-2 and human met-
rics on a summary level before and after excluding topics
by one or two worst assessors in that category.
maries annotated by each particular assessor and cal-
culated the correlation between ROUGE-2 and this
assessor?s manual scores for individual summaries.
Then we calculated the mean correlation over all
assessors. Unsurprisingly, inconsistent assessors
tend to correlate poorly with automatic (and there-
fore always consistent) metrics, so excluding one
or two worst assessors from each category increases
ROUGE?s average per-assessor summary-level cor-
relation, as can be seen in Table 4. The only ex-
ception here is when we exclude assessors based on
their autoPyramid performance: again, because in-
consistent SCU selection doesn?t necessarily trans-
late into inconsistent final Pyramid scores, exclud-
ing those assessors doesn?t do much for ROUGE-2.
4 Impact on training
Another area where excluding noisy topics might be
useful is in training new automatic evaluation met-
rics. To examine this issue we turned to CLASSY
(Rankel et al, 2011), an automatic evaluation met-
ric submitted to TAC each year from 2009-2011.
CLASSY consists of four different versions, each
aimed at predicting a particular human evaluation
score. Each version of CLASSY is based on one
of three regression methods: robust regression, non-
negative least squares, or canonical correlation. The
regressions are calculated based on a collection of
linguistic and content features, derived from the
summary to be scored.
CLASSY requires two years of marked data to
score summaries in a new year. In order to predict
361
the human metrics in 2011, for example, CLASSY
uses the human ratings from 2009 and 2010. It first
considers each subset of the features in turn, and us-
ing each of the regression methods, fits a model to
the 2009 data. The subset/method combination that
best predicts the 2010 scores is then used to pre-
dict scores for 2011. However, the model is first re-
trained on the 2010 data to calculate the coefficients
to be used in predicting 2011.
First, we trained all four CLASSY versions on
all available 2009-2010 topics, and then trained
again excluding topics by the most inconsistent as-
sessor(s). A different subset of topics was ex-
cluded depending on whether this particular version
of CLASSY was aiming to predict Responsiveness,
Readability, or the Pyramid score. Then we tested
CLASSY?s performance on 2011 data, ranking ei-
ther automatic summarizers (NoModels case) or hu-
man and automatic summarizers together (AllPeers
case), separately for main and update summaries,
and calculated its correlation with the metrics it was
aiming to predict. Table 5 shows the result of this
comparison. For Pyramid, (a) indicates that ex-
cluded topics were selected based on Krippendorff?s
alpha, and (b) indicates that topics were excluded
based on their mean difference between manual and
automatic Pyramid scores.
The results are encouraging; it seems that remov-
ing noisy topics from training data does improve the
correlations with manual metrics in most cases. The
greatest increase takes place in CLASSY?s correla-
tions with Responsiveness for main summaries in
AllPeers case, and for correlations with Readabil-
ity. While none of the changes are large enough
to achieve statistical significance, the pattern of im-
provement is fairly consistent.
5 Conclusions
We investigated the consistency of human assessors
in the area of summarization evaluation. We con-
sidered two ways of measuring assessor consistency,
depending on the metric, and studied the impact of
consistent scoring on ranking summarization sys-
tems and on the performance of automatic evalu-
ation systems. We found that summarization sys-
tem ranking, based on scores for multiple topics,
was surprisingly stable and didn?t change signifi-
NoModels AllPeers
main update main update
Pyramid
CLASSY1 Pyr 0.956 0.898 0.945 0.936
CLASSY1 Pyr new (a) 0.950 0.895 0.932 0.955
CLASSY1 Pyr new (b) 0.960 0.900 0.940 0.955
Responsiveness
CLASSY2 Resp 0.951 0.903 0.948 0.963
CLASSY2 Resp new 0.954 0.907 0.973 0.950
CLASSY4 Resp 0.951 0.927 0.830 0.949
CLASSY4 Resp new 0.943 0.928 0.887 0.946
Readability
CLASSY3 Read 0.768 0.705 0.844 0.907
CLASSY3 Read new 0.793 0.721 0.858 0.906
Table 5: Correlations between CLASSY and human met-
rics on 2011 data (main and update summaries), before
and after excluding most inconsistent topic from 2009-
2010 training data for CLASSY.
cantly when several topics were removed from con-
sideration. However, on a summary level, remov-
ing topics scored by the most inconsistent assessors
helped ROUGE-2 increase its correlation with hu-
man metrics. In the area of training automatic met-
rics, we found some encouraging results; removing
noise from the training data allowed most CLASSY
versions to improve their correlations with the man-
ual metrics that they were aiming to model.
References
Deen G. Freelon. 2010. ReCal: Intercoder Reliability
Calculation as a Web Service. International Journal
of Internet Science, Vol 5(1).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
78?81. Barcelona, Spain.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better Metrics to Automatically
Predict the Quality of a Text Summary. Proceedings
of the SIAM Data Mining Text Mining Workshop 2012.
362
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 131?136,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Decade of Automatic Content Evaluation of News Summaries:
Reassessing the State of the Art
Peter A. Rankel
University of Maryland
rankel@math.umd.edu
John M. Conroy
IDA / Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
How good are automatic content metrics
for news summary evaluation? Here we
provide a detailed answer to this question,
with a particular focus on assessing the
ability of automatic evaluations to identify
statistically significant differences present
in manual evaluation of content. Using
four years of data from the Text Analysis
Conference, we analyze the performance
of eight ROUGE variants in terms of ac-
curacy, precision and recall in finding sig-
nificantly different systems. Our exper-
iments show that some of the neglected
variants of ROUGE, based on higher or-
der n-grams and syntactic dependencies,
are most accurate across the years; the
commonly used ROUGE-1 scores find
too many significant differences between
systems which manual evaluation would
deem comparable. We also test combina-
tions of ROUGE variants and find that they
considerably improve the accuracy of au-
tomatic prediction.
1 Introduction
ROUGE (Lin, 2004) is a suite of automatic eval-
uations for summarization and was introduced a
decade ago as a reasonable substitute for costly
and slow human evaluation. The scores it pro-
duces are based on n-gram or syntactic overlap be-
tween an automatic summary and a set of human
reference summaries. However, the field does not
have a good grasp of which of the many evalua-
tion scores is most accurate in replicating human
judgements. This state of uncertainty has led to
problems in comparing published work, as differ-
ent researchers choose to publish different variants
of scores.
In this paper we reassess the strengths of
ROUGE variants using the data from four years
of Text Analysis Conference (TAC) evaluations,
2008 to 2011. To assess the performance of the au-
tomatic evaluations, we focus on determining sta-
tistical significance1 between systems, where the
gold-standard comes from comparing the systems
using manual pyramid and responsiveness evalu-
ations. In this setting, computing correlation co-
efficients between manual and automatic scores is
not applicable as it does not take into account the
statistical significance of the differences nor does
it allow the use of more powerful statistical tests
which use pairwise comparisons of performance
on individual document sets. Instead, we report
on the accuracy of decisions on pairs of systems,
as well as the precision and recall of identifying
pairs of systems which exhibit statistically signifi-
cant differences in content selection performance.
2 Background
During 2008?2011, automatic summarization sys-
tems at TAC were required to create 100-word
summaries. Each year there were two multi-
document summarization sub-tasks, the initial
summary and the update summary, usually re-
ferred to as task A and task B, respectively. The
test inputs in each consisted of about 10 docu-
ments and the type of summary varied between
query-focused and guided. There are between 44
and 48 test inputs on which systems are compared
for each task.
In 2008 and 2009, task A was to produce a
1For the purpose of this study, we define a difference as
significant when the test statistic attains a value correspond-
ing to a p-value less than 0.05.131
query-focused summary in response to a user in-
formation need stated both as a brief statement
and a paragraph-long description of the informa-
tion the user seeks to find. In 2010 and 2011 task
A was ?guided summarization?, where the test in-
puts came from a small set of predefined domains.
These domains included accidents and natural dis-
asters, attacks, health and safety, endangered re-
sources, investigations and trials. Systems were
provided with a list of important aspects of infor-
mation for each domain and were asked to cover as
many of these aspects as possible. The writers of
the reference summaries for evaluation were given
similar instructions. In all four years, task B was
to produce an update summary for each of the in-
puts given in task A (query-focused or guided). In
each case, a new, subsequent set of documents re-
lated to the topic of the respective test set for task
A was provided to the system. The task was to
generate an update summary aimed at a user who
has already read all documents in the inputs for
task A.
The two manual evaluation approaches used in
TAC 2008?2011 are modified pyramid (Nenkova
et al, 2007) and overall responsiveness. The pyra-
mid method requires several reference summaries
for each input. These are manually analyzed to
discover content units based on meaning rather
than specific wording. Each content unit is as-
signed a weight equal to the number of reference
summaries that included that content unit. The
modified pyramid score is defined as the sum of
weights of the content units in the summary nor-
malized by the weight of an ideally informative
summary which expresses n content units, where
n is equal to the average of content units in the ref-
erence summaries. Responsiveness, on the other
hand, is based on direct human judgements, with-
out the need for reference summaries. Assessors
are presented with a statement of the user?s infor-
mation need and the summary they need to evalu-
ate. Then they rate how well they think the sum-
mary responds to the information need contained
in the topic statement. Responsiveness was rated
on a ten-point scale in 2009, and on a five-point
scale in all other years.
For each sub-task during 2008?2011, we ana-
lyze the performance of only the top 30 systems,
which roughly corresponds to the systems that per-
formed better than or around the median according
to each manual metric. Table 1 gives the number
of significant differences among the top 30 partici-
pating systems. We keep only the best performing
systems for the analysis because we are interested
in studying how well automatic evaluation metrics
can correctly compare very good systems.
Year Pyr A Pyr B Resp A Resp B
2008 82 109 68 105
2009 146 190 106 92
2010 165 139 150 128
2011 39 83 5 11
Table 1: Number of pairs of significantly different
systems among the top 30 across the years. There
is a total of 435 pairs in each year.
3 Which ROUGE is best?
In this section, we study the performance of
several ROUGE variants, including ROUGE-n,
for n = 1, 2, 3, 4, ROUGE-L, ROUGE-W-1.2,
ROUGE-SU4, and ROUGE-BE-HM (Hovy et al,
2006). ROUGE-n measures the n-gram recall of
the evaluated summary compared to the available
reference summaries. ROUGE-L is the ratio of
the number of words in the longest common sub-
sequence between the reference and the evaluated
summary and the number of words in the refer-
ence. ROUGE-W-1.2 is a weighted version of
ROUGE-L. ROUGE-SU4 is a combination of skip
bigrams and unigrams, where the skip bigrams are
formed for all words that appear in the text with
no more than four intervening words in between.
ROUGE-BE-HM computes recall of dependency
syntactic relations between the summary and the
reference.
To evaluate how well an automatic evalua-
tion metric reproduces human judgments, we use
prediction accuracy similar to Owczarzak et al
(2012). For each pair of systems in each subtask,
we compare the results of two Wilcoxon signed-
rank tests, one using the manual evaluation scores
for each system and one using the automatic evalu-
ation scores for each system (Rankel et al, 2011).2
The accuracy then is simply the percent agreement
between the results of these two tests.
2We use the Wilcoxon test as it was demonstrated by
Rankel et al (2011) to give more statistical power than un-
paired tests. As reported by Yeh (2000), other tests such as
randomized testing, may also be appropriate. There is con-
siderable variation in system performance for different inputs
(Nenkova and Louis, 2008) and paired tests remove the effect
of the input.132
Responsiveness Pyramid
Metric Acc P R BA Acc P R BA
R1 0.58 (0.61) 0.24 0.64 0.57 0.62 (0.66) 0.37 0.67 0.61
R2 0.64 (0.63) 0.28 0.60 0.59 0.68 (0.69) 0.43 0.63 0.64
R3 0.70 (0.63) 0.31 0.48 0.60 0.73 (0.68) 0.49 0.53 0.66
R4 0.73 (0.64) 0.33 0.40 0.60 0.74 (0.65) 0.50 0.45 0.65
RL 0.50 (0.59) 0.20 0.56 0.54 0.54 (0.63) 0.29 0.60 0.55
R-SU4 0.61(0.62) 0.26 0.61 0.58 0.65 (0.68) 0.40 0.65 0.63
R-W-1.2 0.52(0.62) 0.21 0.54 0.55 0.57(0.64) 0.32 0.62 0.57
R-BE-HM 0.70 (0.63) 0.30 0.49 0.59 0.74(0.68) 0.49 0.56 0.66
Table 2: Accuracy, Precision, Recall, and Balanced Accuracy of each ROUGE variant, averaged across
all eight tasks in 2008-2011, with and (without) significance.
As can be seen in Table 1, the manual evalua-
tion metrics often did not show many significant
differences between systems.3 Thus, it is clear
that the percent agreement will be high for an ap-
proach for automatic evaluation that always pre-
dicts zero significant differences. As traditionally
done when dealing which such skewed distribu-
tions of classes, we also examine the precision
and recall with respect to finding significant dif-
ferences of several ROUGE variants, to better as-
sess the quality of their prediction. To identify a
measure that is strong at both predicting signifi-
cant and non-significant differences we compute
balanced accuracy, the mean of the accuracy of
predicting significant differences and the accuracy
of predicting no significant difference.4
Each of these four measures for judging the per-
formance of ROUGE variants has direct intuitive
interpretation, unlike other opaque measures such
as correlation coefficients and F-measure which
have formal definitions which do not readily yield
to intuitive understanding.
3This is a somewhat surprising finding which may warrant
further investigation. One possible explanation is that differ-
ent systems generate similar summaries. Recent work has
shown that this is unlikely to be the case because the collec-
tion of summaries from several systems indicates better what
content is important than the single best summary (Louis and
Nenkova, 2013). The short summary length for which the
summarizers are compared may also contribute to the fact
that there are few significant difference. In early NIST eval-
uations manual evaluations could not distinguish automatic
and human summaries based on summaries of length 50 and
100 words and there were more significant differences be-
tween systems for 200-word summaries than for 100-word
summaries (Nenkova, 2005).
4More generally, one could define a utility function which
gives costs associated with errors and benefits to correct pre-
diction. Balanced accuracy weighs all errors as equally bad
and all correct prediction as equally good (von Neumann and
Morgenstern, 1953).
Few prior studies have taken statistical signifi-
cance into account during the assessment of auto-
matic metrics for evaluation. For this reason we
first briefly discuss ROUGE accuracy without tak-
ing significance into account. In this special case,
agreement simply means that the automatic and
manual evaluations agree on which of two systems
is better, based on each system?s average score for
all test inputs for a given task. It is very rare that
the average scores of two systems are equal, so
there is always a better system in each pair, and
random prediction would have 50% accuracy.
Many papers do not report the significance of
differences in ROUGE scores (for the ROUGE
variant of their choice), but simply claim that their
system X with higher average ROUGE score than
system Y is better than system Y . Table 2 lists
the average accuracy with significance taken into
account and then in parentheses, accuracy without
taking significance into account. The data demon-
strate that the best accuracy of the eight ROUGE
metrics is a meager 64% for responsiveness when
significance is not taken into account. So the con-
clusion about the relative merit of systems would
be different from that based on manual evaluation
in one out of three comparisons. However, the
best accuracy rises to 73% when significance is
taken into account; an incorrect conclusion will be
drawn in one out of four comparisons. The reduc-
tion in error is considerable.
Furthermore, ROUGE-3 and ROUGE-4, which
are rarely reported, are among the most accurate.
Note also, these results differ considerably from
those reported by Owczarzak et al (2012), where
ROUGE-2 was shown to have accuracy of 81% for
responsiveness and 89% for pyramid. The wide
differences are due to the fact we are only consid-133
ering systems which scored in the top 30. This il-
lustrates that our automatic metrics are not as good
at discriminating systems near the top. These find-
ings give strong support for the idea of requiring
authors to report the significance of the difference
between their summarization system and the cho-
sen baseline; the conclusions about relative merits
of the system would be more similar to those one
would draw from manual evaluation.
In addition to accuracy, Table 2 gives precision,
recall and balanced accuracy for each of the eight
ROUGE measures when significance is taken into
account. ROUGE-1 is arguably the most widely
used score in the literature and Table 2 reveals an
interesting property: ROUGE-1 has high recall but
low precision. This means that it reports many sig-
nificant differences, most of which do not exist ac-
cording to the manual evaluations.
Balanced accuracy helps us identify which
ROUGE variants are most accurate in finding
statistical significance and correctly predicting
that two systems are not significantly different.
For the pyramid evaluation, the variants with
best balanced accuracy (66%) are ROUGE-3 and
ROUGE-BE, with ROUGE-4 just a percent lower
at 65%. For responsiveness the configuration is
similar, with ROUGE-3 and ROUGE-4 tied for
best (60%), and ROUGE-BE just a percent lower.
The good performance of higher-order n-grams
is quite surprising because these are practically
never used for reporting results in the literature.
Based on our results however, they are much more
likely to accurately reproduce conclusions that
would have been drawn from manual evaluation
of top-performing systems.
4 Multiple hypothesis tests to combine
ROUGE variants
We now consider a method to combine multiple
evaluation scores in order to obtain a stronger en-
semble metric. The idea of combining ROUGE
variants has been explored in the prior litera-
ture. Conroy and Dang (2008), for example, pro-
posed taking linear combinations of ROUGE met-
rics. This approach was extended by Rankel et al
(2012) by including measures of linguistic quality.
Recently, Amigo? et al (2012) applied the ?hetero-
geneity principle? and combined ROUGE scores
to improve the precision relative to a human evalu-
ation metric. Their results demonstrate that a con-
sensus among ROUGE scores can predict more ac-
curately if an improvement in a human evaluation
metric will be achieved.
Along the lines of these investigations, we ex-
amine the performance of a simple combination
of variants: Call the difference between two sys-
tems significant only when all the variants in the
combination indicate significance. As in the sec-
tion above, a paired Wilcoxon signed-rank test is
used to determine the level of significance.
ROUGE Combination Acc Prec Rec BA
R1 R2 R4 RBE 0.76 0.77 0.36 0.76
R1 R4 RBE 0.76 0.76 0.36 0.76
R2 R4 RBE 0.76 0.74 0.40 0.75
R4 RBE 0.76 0.73 0.41 0.75
R1 R2 R4 0.76 0.71 0.40 0.74
R1 R4 0.75 0.70 0.40 0.73
R2 R4 0.75 0.68 0.44 0.73
R1 R2 RBE 0.75 0.66 0.48 0.72
R2 RBE 0.75 0.64 0.52 0.72
R4 0.74 0.62 0.47 0.70
R1 RBE 0.74 0.62 0.49 0.70
R1 R2 0.73 0.57 0.62 0.70
RBE 0.73 0.57 0.58 0.68
R2 0.71 0.53 0.69 0.68
R1 0.62 0.43 0.69 0.63
Table 3: Accuracy, Precision, Recall, and Bal-
anced Accuracy of each ROUGE combination on
TAC 2008-2010 pyramid.
We considered all possible combinations of four
ROUGE metrics that exhibited good properties
in the analyses presented so far: ROUGE-1 (be-
cause of its high recall), ROUGE-2 (because of
high accuracy when significance is not taken into
account) and ROUGE-4 and ROUGE-BE, which
showed good balanced accuracy.
The performance of these combinations for re-
producing the decisions in TAC 2008-2010 based
on the pyramid5 evaluation are given in Table 3.
The best balanced accuracy (76%) is for the com-
bination of all four variants. As more variants are
combined, precision increases but recalls drops.
5 Comparison with automatic
evaluations from AESOP 2011
In 2009-2011, TAC ran the task of Automatically
Evaluating Summaries of Peers (AESOP), to com-
5The ordering of the metric combinations relative to re-
sponsiveness was almost identical to the ordering relative to
the pyramid evaluation, and precision and recall exhibited the
same trend as more metrics were added to the combination.134
Pyramid A Pyramid B Responsiveness A Responsiveness B
Evaluation Metric Acc P R BA Acc P R BA Acc P R BA Acc P R BA
CLASSY1 0.60 0.02 0.60 0.50 0.84 0.03 0.18 0.50 0.61 0.14 0.64 0.54 0.70 0.21 0.22 0.52
DemokritosGR1 0.59 0.01 0.20 0.50 0.79 0.07 0.55 0.53 0.66 0.18 0.79 0.58 0.64 0.17 0.24 0.49
uOttawa3 0.44 0.01 0.60 0.50 0.48 0.02 0.36 0.50 0.52 0.13 0.77 0.55 0.43 0.13 0.36 0.46
DemokritosGR2 0.78 0.01 0.20 0.50 0.76 0.06 0.55 0.52 0.76 0.23 0.69 0.60 0.67 0.22 0.29 0.52
C-S-IIITH4 0.69 0.01 0.20 0.50 0.77 0.07 0.64 0.53 0.82 0.29 0.74 0.63 0.60 0.15 0.24 0.47
C-S-IIITH1 0.60 0.01 0.40 0.50 0.70 0.06 0.82 0.53 0.69 0.20 0.79 0.59 0.60 0.22 0.42 0.52
BEwT-E 0.73 0.01 0.20 0.50 0.80 0.01 0.09 0.49 0.79 0.25 0.72 0.61 0.72 0.31 0.39 0.58
R1-R2-R4-RBE 0.89 0.40 0.44 0.67 0.76 0.27 0.17 0.55 0.88 0.00 0.00 0.49 0.91 0.03 0.09 0.50
R1-R4-RBE 0.89 0.40 0.44 0.67 0.77 0.35 0.24 0.59 0.88 0.00 0.00 0.49 0.90 0.03 0.09 0.50
All ROUGEs 0.89 0.40 0.44 0.67 0.75 0.26 0.16 0.54 0.88 0.00 0.00 0.49 0.91 0.04 0.09 0.51
Table 4: Best performing AESOP systems from TAC 2011; Scores within the 95% confidence interval
of the best are in bold face.
pare automatic evaluation methods for automatic
summarization. Here we show how the submit-
ted AESOP metrics compare to the best ROUGE
variants that we have established so far. We report
the results on 2011 only, because even when the
same team participated in more than one year, the
metrics submitted were different and the 2011 re-
sults represent the best effort of these teams. How-
ever, as we saw in Table 1, in 2011 there were very
few significant differences between the top sum-
marization systems. In this sense the tasks that
year represent a challenging dataset for testing au-
tomatic evaluations.
The results for the best AESOP systems (ac-
cording to one or more measures), and the cor-
responding results for the ROUGE combinations
are shown in Table 4. These AESOP systems are:
CLASSY1 (Conroy et al, 2011; Rankel et al,
2012), DemokritosGR1 and 2 (Giannakopoulos et
al., 2008; Giannakopoulos et al, 2010), uOttawa3
(Kennedy et al, 2011), C-S-IITH1 and 4 (Kumar
et al, 2011; Kumar et al, 2012), and BEwT-E
(Tratz and Hovy, 2008).6 The combination metrics
achieve the highest accuracy by generally predict-
ing correctly when there are no significant differ-
ences between the systems. In addition, for 2008-
2010, where far more differences between systems
occur, the results of Table 3 show the combina-
tion metrics outperformed use of a single metric
and are competitive with the best metrics of AE-
SOP 2011. Thus, the combination metrics have
the ability to discriminate under both conditions
giving good prediction of human evaluation.
6To perform the comparison in the table the scores for
each system and document set were needed. Some systems
have changed after TAC 2011, but the data needed for these
comparisons were not available. BEwT-E did not participate
in AESOP 2011 and these data were provided by Stephen
Tratz. Special thanks to Stephen for providing these data.
6 Conclusion
We have tested the best-known automatic evalu-
ation metrics (ROUGE) on several years of TAC
data and compared their performance with re-
cently developed AESOP metrics. We discovered
that some of the rarely used variants of ROUGE
perform surprisingly well, and that by combin-
ing different ROUGEs together, one can create
an evaluation metric that is extremely competi-
tive with metrics submitted to the latest AESOP
task. Our results were reported in terms of sev-
eral different measures, and in each case, com-
pared how well the automatic metric predicted sig-
nificant differences found in manual evaluation.
We believe strongly that developers should include
statistical significance when reporting differences
in ROUGE scores of theirs and other systems,
as this improves the accuracy and credibility of
their results. Significant improvement in multi-
ple ROUGE scores is a significantly stronger in-
dicator that the developers have made a notewor-
thy improvement in text summarization. Systems
that report significant improvement using a com-
bination of ROUGE-BE (or its improved version
BEwT-E) in conjunction with ROUGE-1, 2, and
4, are more likely to give rise to summaries that
humans would judge as significantly better.
Acknowledgments
The authors would like to thank Ed Hovy who
raised the question ?How well do automatic met-
rics perform when comparing top systems?? Ed?s
comments helped motivate this work. In addition,
we would like to thank our anonymous referees for
their insightful comments, which contributed sig-
nificantly to this paper.
135
References
Enrique Amigo?, Julio Gonzalo, and Felisa Verdejo.
2012. The heterogeneity principle in evaluation
measures for automatic summarization. In Pro-
ceedings of Workshop on Evaluation Metrics and
System Comparison for Automatic Summarization,
pages 36?43, Montre?al, Canada, June. Association
for Computational Linguistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind
the gap: Dangers of divorcing evaluations of sum-
mary content from linguistic quality. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 145?152,
Manchester, UK, August. Coling 2008 Organizing
Committee.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2011. Nouveau-ROUGE: A Novelty Met-
ric for Update Summarization. Computational Lin-
guistics, 37(1):1?8.
George Giannakopoulos, Vangelis Karkaletsis,
George A. Vouros, and Panagiotis Stamatopoulos.
2008. Summarization system evaluation revisited:
N-gram graphs. TSLP, 5(3).
George Giannakopoulos, George A. Vouros, and Van-
gelis Karkaletsis. 2010. Mudos-ng: Multi-
document summaries using n-gram graphs (tech re-
port). CoRR, abs/1012.2042.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC?06), pages 899?902.
Alistair Kennedy, Anna Kazantseva Saif Mohammad,
Terry Copeck, Diana Inkpen, and Stan Szpakowicz.
2011. Getting emotional about news. In Fourth Text
Analysis Conference (TAC 2011).
Niraj Kumar, Kannan Srinathan, and Vasudeva Varma.
2011. Using unsupervised system with least linguis-
tic features for tac-aesop task. In Fourth Text Analy-
sis Conference (TAC 2011).
N. Kumar, K. Srinathan, and V. Varma. 2012. Us-
ing graph based mapping of co-occurring words and
closeness centrality score for summarization evalua-
tion. Computational Linguistics and Intelligent Text
Processing, pages 353?365.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Stan Szpakowicz
Marie-Francine Moens, editor, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2013. Automatically
assessing machine summary content without a gold
standard. Computational Linguistics, 39:267?300.
Ani Nenkova and Annie Louis. 2008. Can you sum-
marize this? identifying correlates of input difficulty
for multi-document summarization. In ACL, pages
825?833.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen
McKeown. 2007. The pyramid method: Incorpo-
rating human content selection variation in summa-
rization evaluation. TSLP, 4(2).
Ani Nenkova. 2005. Discourse factors in multi-
document summarization. In AAAI, pages 1654?
1655.
Karolina Owczarzak, John M. Conroy, Hoa Trang
Dang, and Ani Nenkova. 2012. An assessment
of the accuracy of automatic evaluation in summa-
rization. In Proceedings of Workshop on Evaluation
Metrics and System Comparison for Automatic Sum-
marization, pages 1?9, Montre?al, Canada, June. As-
sociation for Computational Linguistics.
Peter Rankel, John Conroy, Eric Slud, and Dianne
O?Leary. 2011. Ranking human and machine sum-
marization systems. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 467?473, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better metrics to automatically
predict the quality of a text summary. Algorithms,
5(4):398?420.
Stephen Tratz and Eduard Hovy. 2008. Summarisa-
tion evaluation using transformed basic elements. In
Proceedings TAC 2008. NIST.
John von Neumann and Oskar Morgenstern. 1953.
Theory of games and economic behavior. Princeton
Univ. Press, Princeton, NJ, 3. ed. edition.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, COLING ?00, pages 947?
953, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
136
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 263?274, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 7: The Joint Student Response Analysis and 8th
Recognizing Textual Entailment Challenge
Myroslava O. Dzikovska
School of Informatics, University of Edinburgh
Edinburgh, United Kingdom
m.dzikovska@ed.ac.uk
Rodney D. Nielsen
University of North Texas
Denton, TX, USA
Rodney.Nielsen@UNT.edu
Chris Brew
Nuance Communications
USA
cbrew@acm.org
Claudia Leacock
CTB McGraw-Hill
USA
claudia leacock@mheducation.com
Danilo Giampiccolo
CELCT
Italy
giampiccolo@celct.it
Luisa Bentivogli
CELCT and FBK
Italy
bentivo@fbk.eu
Peter Clark
Vulcan Inc.
USA
peterc@vulcan.com
Ido Dagan
Bar-Ilan University
Israel
dagan@cs.biu.ac.il
Hoa Trang Dang
NIST
hoa.dang@nist.gov
Abstract
We present the results of the Joint Student
Response Analysis and 8th Recognizing Tex-
tual Entailment Challenge, aiming to bring to-
gether researchers in educational NLP tech-
nology and textual entailment. The task of
giving feedback on student answers requires
semantic inference and therefore is related to
recognizing textual entailment. Thus, we of-
fered to the community a 5-way student re-
sponse labeling task, as well as 3-way and 2-
way RTE-style tasks on educational data. In
addition, a partial entailment task was piloted.
We present and compare results from 9 partic-
ipating teams, and discuss future directions.
1 Introduction
One of the tasks in educational NLP systems is pro-
viding feedback to students in the context of exam
questions, homework or intelligent tutoring. Much
previous work has been devoted to the automated
scoring of essays (Attali and Burstein, 2006; Sher-
mis and Burstein, 2013), error detection and correc-
tion (Leacock et al, 2010), and classification of texts
by grade level (Petersen and Ostendorf, 2009; Shee-
han et al, 2010; Nelson et al, 2012). In these appli-
cations, NLP methods based on shallow features and
supervised learning are often highly effective. How-
ever, for the assessment of responses to short-answer
questions (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Nielsen et al, 2008a; Mohler
et al, 2011) and in tutorial dialog systems (Graesser
et al, 1999; Glass, 2000; Pon-Barry et al, 2004; Jor-
dan et al, 2006; VanLehn et al, 2007; Dzikovska et
al., 2010) deeper semantic processing is likely to be
appropriate.
Since the task of making and testing a full edu-
cational dialog system is daunting, Dzikovska et al
(2012) identified a key subtask and proposed it as a
new shared task for the NLP community. Student
response analysis (henceforth SRA) is the task of
labeling student answers with categories that could
263
Example 1 QUESTION You used several methods to separate and identify the substances in mock rocks. How did you
separate the salt from the water?
REF. ANS. The water was evaporated, leaving the salt.
STUD. ANS. The water dried up and left the salt.
Example 2 QUESTION Georgia found one brown mineral and one black mineral. How will she know which one is harder?
REF. ANS. The harder mineral will leave a scratch on the less hard mineral. If the black mineral is harder, the
brown mineral will have a scratch.
STUD. ANS. The harder will leave a scratch on the other.
Figure 1: Example questions and answers
help a full dialog system to generate appropriate and
effective feedback on errors. System designers typi-
cally create a repertoire of questions that the system
can ask a student, together with reference answers
(see Figure 1 for an example). For each student an-
swer, the system needs to decide on the appropriate
tutorial feedback, either confirming that the answer
was correct, or providing additional help to indicate
how the answer is flawed and help the student im-
prove. This task requires semantic inference, for ex-
ample, to detect when the student answers are ex-
plaining the same content but in different words, or
when they are contradicting the reference answers.
Recognizing Textual Entailment (RTE) is a se-
ries of highly successful challenges used to evalu-
ate tasks related to semantic inference, held annually
since 2005. Initial challenges used examples from
information retrieval, question answering, machine
translation and information extraction tasks (Dagan
et al, 2006; Giampiccolo et al, 2008). Later chal-
lenges started to explore the applicability and im-
pact of RTE technology on specific application set-
tings such as Summarization and Knowledge Base
Population (Bentivogli et al, 2009; Bentivogli et al,
2010; Bentivogli et al, 2011). The SRA Task offers
a similar opportunity.
We therefore organized a joint challenge at
SemEval-2013, aiming to bring together the educa-
tional NLP and the semantic inference communities.
The goal of the challenge is to compare approaches
for student answer assessment and to evaluate the
methods typically used in RTE on data from educa-
tional applications.
We present the corpus used in the task (Section
2) and describe the Main task, including educational
NLP and textual entailment perspectives and data set
creation (Section 3). We discuss evaluation metrics
and results in Section 4. Section 5 describes the Pi-
lot task, including data set creation and evaluation
results. Section 6 presents conclusions and future
directions.
2 Student Response Analysis Corpus
We used the Student Response Analysis corpus
(henceforth SRA corpus) (Dzikovska et al, 2012)
as the basis for our data set creation. The corpus
contains manually labeled student responses to ex-
planation and definition questions typically seen in
practice exercises, tests, or tutorial dialogue.
Specifically, given a question, a known correct
?reference answer? and a 1- or 2-sentence ?student
answer?, each student answer in the corpus is label-
led with one of the following judgments:
? ?Correct?, if the student answer is a complete
and correct paraphrase of the reference answer;
? ?Partially correct incomplete?, if it is a par-
tially correct answer containing some but not
all information from the reference answer;
? ?Contradictory?, if the student answer explicitly
contradicts the reference answer;
? ?Irrelevant? if the student answer is talking
about domain content but not providing the
necessary information;
? ?Non domain? if the student utterance does not
include domain content, e.g., ?I don?t know?,
?what the book says?, ?you are stupid?.
The SRA corpus consists of two distinct subsets:
BEETLE data, based on transcripts of students in-
teracting with BEETLE II tutorial dialogue system
(Dzikovska et al, 2010), and SCIENTSBANK data,
264
based on the corpus of student answers to assess-
ment questions collected by Nielsen et al (2008b).
The BEETLE corpus consists of 56 questions in
the basic electricity and electronics domain requir-
ing 1- or 2- sentence answers, and approximately
3000 student answers to those questions. The SCI-
ENTSBANK corpus contains approximately 10,000
answers to 197 assessment questions in 15 different
science domains (after filtering, see Section 3.3)
Student answers in the BEETLE corpus were man-
ually labeled by trained human annotators using a
scheme that straightforwardly mapped into SRA an-
notations. The annotations in the SCIENTSBANK
corpus were converted into SRA labels from a sub-
stantially more fine-grained scheme by first auto-
matically labeling them using a set of question-
specific heuristics and then manually revising them
according to the class definitions (Dzikovska et al,
2012). We further filtered and transformed the cor-
pus to produce training and test data sets as dis-
cussed in the next section.
3 Main Task
3.1 Educational NLP perspective
The 5-way SRA task focuses on associating student
answers with categorical labels that can be used in
providing tutoring feedback. Most NLP research on
short answer scoring reports agreement with a nu-
meric score (Leacock and Chodorow, 2003; Pulman
and Sukkarieh, 2005; Mohler et al, 2011), which
is a potential contrast with our task. However, the
majority of the NLP work makes use of underlying
representations in terms of concepts, so the 5-way
task is still likely to mesh well with the available
technology. Research on tutorial dialog has empha-
sized generic methods that use latent semantic anal-
ysis or other machine learning methods to determine
when text strings express similar concepts (Hu et al,
2003; Jordan et al, 2004; VanLehn et al, 2007; Mc-
Carthy et al, 2008). Most of these methods, like
the NLP methods, (with the notable exception of
(Nielsen et al, 2008a)), are however strongly depen-
dent on domain expertise for the definitions of the
concepts. In educational applications, there would
be great value in a system that could operate more
or less unchanged across a range of domains and
question-types, requiring only a question text and a
reference answer supplied by the instructional de-
signers. Thus, the 5-way classification task at Se-
mEval was set up to evaluate the feasibility of such
answer assessment, either by adapting the existing
educational NLP methods to the categorical labeling
task or by employing the RTE approaches.
3.2 RTE perspective and 2- and 3-way Tasks
According to the standard definition of Textual En-
tailment, given two text fragments called Text (T)
and Hypothesis (H), it is said that T entails H if, typ-
ically, a human reading T would infer that H is most
likely true (Dagan et al, 2006).
In a typical answer assessment scenario, we ex-
pect that a correct student answer would entail the
reference answer, while an incorrect answer would
not. However, students often skip details that are
mentioned in the question or may be inferred from
it, while reference answers often repeat or make ex-
plicit information that appears in or is implied from
the question, as in Example 2 in Figure 1. Hence, a
more precise formulation of the task in this context
considers the entailing text T as consisting of both
the original question and the student answer, while
H is the reference answer.
We carried out a feasibility study to check how
well the entailment judgments in this formulation
align with the annotated response assessment, by an-
notating a sample of the data used in the SRA task
with entailment judgments. We found that some an-
swers labeled as ?correct? implied inferred or as-
sumed pieces of information not present in the text.
These reflected the teachers? assessment of student
understanding but would not be considered entailed
from the traditional RTE perspective. However, we
observed that in most such cases, a substantial part
of the hypothesis was still implied by the text. More-
over, answers assigned labels other than ?correct?
were always judged as ?not entailed?.
Overall, we concluded that the correlation be-
tween assessment judgments of the two types was
sufficiently high to consider an RTE approach. The
challenge for the textual entailment community was
to address the answer assessment task at varying
levels of granularity, using textual entailment tech-
niques, and explore how well these techniques can
help in this real-world educational setting.
In order to make the setup more similar to pre-
265
vious RTE tasks, we introduced 3-way and 2-way
versions of the task. The data for those tasks were
obtained by automatically collapsing the 5-way la-
bels. In the 3-way task, the systems were required to
classify the student answer as either (i) correct; (ii)
contradictory; or (iii) incorrect (combining the cat-
egories partially correct but incomplete, irrelevant
and not in the domain from the 5-way classification).
In the two-way task, the systems were required to
classify the student answer as either correct or in-
correct (combining the categories contradictory and
incorrect from the 3-way classification)
3.3 Data Preparation and Training Data
In preparation of the task four of the organizers ex-
amined all questions in the SRA corpus, and decided
that to remove some of the questions to make the
dataset more uniform.
We observed two main issues. First, a num-
ber of questions relied on external material, e.g.,
charts and graphs. In some cases, the information
in the reference answer was sufficient to make a rea-
sonable assessment of student answer correctness,
but in other cases the information contained in the
questions was deemed insufficient and the questions
were removed.
Second, some questions in the SCIENTSBANK
dataset could have multiple possible correct an-
swers, e.g., a question asking for any example out
of two or more unrelated possibilities. Such ques-
tions were also removed as they do not align well
with the RTE perspective.
Finally, parts of the data were re-checked for re-
liability. In BEETLE data, a second manual annota-
tion pass was carried out on a subset of questions
to check for consistency. In SCIENTSBANK, we
manually re-checked the test data. The automatic
conversion from the original SCIENTSBANK anno-
tations into SRA labels was not perfectly accurate
(Dzikovska et al, 2012). We did not have the re-
sources to check the entire data set. However, four of
the organizers jointly hand-checked approximately
100 examples to establish consensus, and then one
organizer hand-checked all of the test data set.
3.4 Test Data
We followed the evaluation methodology of Nielsen
et al (2008a) for creating the test data. Since our
goal is to support systems that generalize across
problems and domains (see Section 3.1), we created
three distinct test sets:
1. Unseen answers (UA): a held-out set to assess
system performance on the answers to ques-
tions contained in the training set (for which
the system has seen example student answers).
It was created by setting aside a subset if ran-
domly selected learner answers to each ques-
tion included in the training data set.
2. Unseen questions (UQ): a test set to assess
system performance on responses to previously
unseen questions but which still fall within the
application domains represented in the training
data. It was created by holding back all student
answers to a subset of randomly selected ques-
tions in each dataset.
3. Unseen domains (UD): a domain-independent
test set of responses to topics not seen in the
training data, available only in the SCIENTS-
BANK dataset. It was created by setting aside
the complete set of questions and answers from
three science modules from the fifteen modules
in the SCIENTSBANK data.
The final label distribution for train and test data
is shown in Table 1.
4 Main Task Results
4.1 Participants
The participants were invited to submit up to three
runs in any combination of the tasks. Nine teams
participated in the main task, most choosing to at-
tempt all subtasks (5-way, 3-way and 2-way), with
1 team entering only the 5-way and 1 team entering
only the 2-way task.
At least 6 (CNGL, CoMeT, CU, BIU, EHUALM,
LIMSI) of the 9 systems used some form of syn-
tactic processing, in most cases going beyond parts
of speech to dependencies or constituency structure.
CNGL emphasized this as an important aspect of the
system. At least 5 (CoMeT, CU, EHUALM, ETS
UKP) of the 9 systems used a system combination
approach, with several components feeding into a
final decision made by some form of stacked clas-
sifier. The majority of the systems used some kind
266
label BEETLE SCIENTSBANK
train (%) UA UQ Test-Total (%) train (%) UA UQ UD Test-Total (%)
correct 1665 (0.42) 176 344 520 (0.41) 2008 (0.40) 233 301 1917 2451 (0.42)
pc inc 919 (0.23) 112 172 284 (0.23) 1324 (0.27) 113 175 986 1274 (0.22)
contra 1049 (0.27) 111 244 355 (0.28) 499 (0.10) 58 64 417 539 (0.09)
irrlvnt 113 (0.03) 17 19 36 (0.03) 1115 (0.22) 133 193 1222 1548 (0.27)
non dom 195 (0.05) 23 40 63 (0.05) 23 (0.005) 3 0 20 23 (0.004)
incorr-3way 1227 (0.31) 152 231 383 (0.30) 2462 (0.495) 249 368 2228 2845 (0.49)
incorr-2way 2276 (0.58) 263 475 538 (0.59) 2961 (0.596) 307 432 2645 3384 (0.58)
Table 1: Label distribution. Percentages in parentheses. UA, UQ, UD correspond to individual test sets.
of measure of text-to-text similarity, whether the in-
spiration was LSA, MT measures such as BLEU
or in-house methods. These methods were em-
phasized as especially important by Celi, ETS and
SOFTCARDINALITY. These impressions are based
on short summaries sent to us by the participants
prior to the availability of the full system descrip-
tions. Check the individual system papers for detail.
4.2 Evaluation Metrics
For each evaluation data set (test set), we computed
the per-class precision, recall and F1 score. We also
computed three main summary metrics: accuracy,
macro-average F1 and weighted average F1.
Accuracy is the overall percentage of correctly
classified examples.
Macroaverage is the average value of each met-
ric (precision, recall, F1) across classes, without
taking class size into account. It is defined as
1/Nc
?
c metric(c), where Nc is the number of
classes (2, 3, or 5 depending on the task). Note
that in the 5-way SCIENTSBANK dataset the ?non-
domain? class is severely underrepresented, with
only 23 examples out of 4335 total (see Table 1).
Therefore, we calculated macro-averaged P/R/F1
over only 4 classes (i.e. excluding the ?non-domain?
class) for SCIENTSBANK 5-way data.
Weighted Average (or simply weighted) is the
average value for each metric weighted by class size,
defined as 1/N
?
c |c| ? metric(c) where N is the
total number of test items and |c| is the number of
items labeled as c in gold-standard data.1
1This metric is called microaverage in (Dzikovska et al,
2012). However, microaverage is used to define a different
metric in tasks where more than one label can be associated
with each data item (Tsoumakas et al, 2010). therefore, we use
weighted average to match the terminology used by the Weka
toolkit. The micro-average precision, recall and F1 computed
In general, macro-averaging favors systems that
perform well across all classes regardless of class
size. Accuracy and weighted average prefer systems
that perform best on the largest number of examples,
favoring higher performance on the most frequent
classes. In practice, only a small number of the sys-
tems were ranked differently by the different met-
rics. We discuss this further in Section 4.7. Results
for all metrics are available online, and this paper
focuses on two metrics for brevity: weighted and
macro-average F1 scores.
4.3 Results
The evaluation results for all metrics and all partic-
ipant runs are provided online.2 The tables in this
paper present the F1 scores for the best system runs.
Results are shown separately for each test set (TS),
with the simple mean over the five TSs reported in
the final column.
We used two baselines: the majority (most fre-
quent) class baseline and a lexical overlap baseline
described in detail in (Dzikovska et al, 2012). The
performance of the baselines is presented jointly
with system scores in the results tables.
For each participant, we report the single run with
the best average TS performance, identified by the
subscript in the run title, with the exception of ETS.
With all other participants, there was almost always
one run that performed best for a given metric on all
the TSs. In the small number of cases where another
run performed best on a given TS, we instead report
that value and indicate its run with a subscript (these
changes never resulted in meaningful changes in the
performance rankings). ETS, on the other hand, sub-
using the multi-label metric are all equal and mathematically
equivalent to accuracy.
2http://bit.ly/11a7QpP
267
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.423 0.386 0.372 0.389 0.367 0.387
CNGL2 0.547 0.469 0.266 0.297 0.294 0.375
CoMeT1 0.675 0.445 0.598 0.299 0.252 0.454
EHUALM2 0.566 0.4163 0.5253 0.446 0.437 0.471
ETS1 0.552 0.547 0.535 0.487 0.447 0.514
ETS2 0.705 0.614 0.625 0.356 0.434 0.547
LIMSIILES1 0.505 0.424 0.419 0.456 0.422 0.445
SoftCardinality1 0.558 0.450 0.537 0.492 0.471 0.502
UKP-BIU1 0.448 0.269 0.590 0.3972 0.407 0.418
Median 0.552 0.445 0.535 0.397 0.422 0.454
Baselines:
Lexical 0.483 0.463 0.435 0.402 0.396 0.436
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 2: Five-way task weighted-average F1
mitted results for systems that were substantially dif-
ferent from one another, with performance varying
from being the top rank to nearly the lowest. Hence,
it seemed more appropriate to report two separate
runs.3 In the rest of the discussion system is used to
refer to a row in the tables as just described.
Systems with performance that was not statisti-
cally different from the best results for a given TS
are all shown in bold (significance was not cal-
culated for the TS mean). Systems with perfor-
mance statistically better than the lexical baseline
are displayed in italics. Statistical significance tests
were conducted using approximate randomization
test (Yeh, 2000) with 10,000 iterations; p ? 0.05
was considered statistically significant.
4.4 Five-way Task
The results for the five-way task are shown in Tables
2 and 3.
Comparison to baselines All of the systems per-
formed substantially better than the majority class
baseline (?correct? for both BEETLE and SCIENTS-
BANK), on average exceeding it on the TS mean by
0.21 on the weighted F1 and 0.24 on the macro-
average F1. Six systems outperformed the lexical
baseline on the mean TS results for the weighted
F1 and five for the macro-average F1. Nearly all
of the top results on a given TS (shown in bold in
the tables) were statistically better than correspond-
ing lexical baselines according to significance tests
3In a small number of cases, ETS?s third run performed
marginally better, see full results online.
Dataset: BEETLE 5way SCIENTSBANK 4way
Run UA UQ UA UQ UD Mean
CELI1 0.315 0.300 0.278 0.286 0.269 0.270
CNGL2 0.431 0.382 0.252 0.262 0.239 0.274
CoMeT1 0.569 0.300 0.551 0.201 0.151 0.312
EHUALM2 0.526 0.3703 0.4473 0.353 0.340 0.382
ETS1 0.444 0.461 0.467 0.372 0.334 0.377
ETS2 0.619 0.552 0.581 0.274 0.339 0.428
LIMSIILES1 0.327 0.280 0.335 0.361 0.337 0.308
SoftCardinality1 0.455 0.436 0.474 0.384 0.375 0.389
UKP-BIU1 0.423 0.285 0.560 0.3252 0.348 0.364
Median 0.444 0.370 0.467 0.325 0.337 0.367
Baselines:
Lexical 0.424 0.414 0.375 0.329 0.311 0.333
Majority 0.114 0.118 0.151 0.146 0.148 0.129
Table 3: Five-way task macro-average F1
(indicated by italics in the tables).
Comparing UA and UQ/UD performance The
BEETLE UA (BUA) and SCIENTSBANK UA (SUA)
test sets represent questions with example answers
in training data, while the UQ and UD test sets repre-
sent transfer performance to new questions and new
domains respectively.
The top performers on UA test sets were CoMeT1
and ETS2, with the addition of UKP-BIU1 on SUA.
However, there was not a single best performer on
UQ and UD sets. ETS2 performed statistically bet-
ter than all other systems on BEETLE UQ (BUQ),
but it performed statistically worse than the lexical
baseline on SCIENTSBANK UQ (SUQ), resulting in
no overlap in the top performing systems on the two
UQ test sets. SoftCardinality1 performed statisti-
cally better than all other systems on SUD and was
among the three or four top performers on SUQ, but
was not a top performer on the other three TSs, gen-
erally not performing statistically better than the lex-
ical baseline on the BEETLE TSs.
Group performance The two UA TSs had more
systems that performed statistically better than the
lexical baseline (generally six systems) than did the
UQ TSs where on average only two systems per-
formed statistically better than the lexical baseline.
Over twice as many systems outperformed the lexi-
cal baseline on UD as on the UQ TSs. The top per-
forming systems according to the macro-average F1
were nearly identical to the top performing systems
according to the weighted F1.
268
4.5 Three-way Task
The results for the three-way task are shown in Ta-
bles 4 and 5.
Comparison to baselines All of the systems per-
formed substantially better than the majority base-
line (?correct? for BEETLE and ?incorrect? for SCI-
ENTSBANK), on average exceeding it on the TS
mean by 0.28 on the weighted F1 and 0.31 on the
macro-average F1. Five of the eight systems out-
performed the lexical baseline on the mean TS re-
sults for the weighted F1 and five on the macro-
average F1, and all top systems outperformed the
lexical baseline with statistical significance.
Comparing UA and UQ/UD performance The top
performers on both BUA and SUA were CoMeT1
and ETS2. As for the 5-way task there was no single
best performer for UQ and UD sets, and no overlap
in top performing systems on BUQ and SUQ test
sets, with ETS2 being the top performer on BUQ,
but statistically worse than the baseline on SUQ
and SUD. On the weighted F1, SoftCardinality1
performed statistically better than all other systems
on SUD and was among the two statistically best
systems on SUQ, but was not a top performer on
BUQ or BUA/SUA TSs. On the macro-average F1,
UKP-BIU1 became one of the statistically best per-
formers on all SCIENTSBANK TSs but, along with
SoftCardinality1, never performed statistically bet-
ter than the lexical baseline on the BEETLE TSs.
Group performance With the exception of SUA,
only around two systems performed statistically bet-
ter than the lexical baseline on each TS. The top per-
forming systems were nearly the same according to
the weighted F1 and the macro-average F1.
4.6 Two-way Task
The results for the two-way task are shown in Ta-
ble 6. Because the labels are roughly balanced in
the two-way task, the results on the weighted and
macro-average F1 are very similar and the top per-
forming systems are identical. Hence this section
will focus only on the macro-average F1.
As in the previous tasks, all of the systems per-
formed substantially better than the majority base-
line (?incorrect? for all sets), on average exceeding
it on the TS mean by 0.25 on the weighted F1 and
0.30 on the macro-average F1. However, just four of
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.519 0.463 0.500 0.555 0.534 0.514
CNGL2 0.592 0.471 0.383 0.367 0.360 0.435
CoMeT1 0.728 0.488 0.707 0.522 0.550 0.599
ETS1 0.619 0.542 0.603 0.631 0.600 0.599
ETS2 0.723 0.597 0.709 0.537 0.505 0.614
LIMSIILES1 0.587 0.454 0.532 0.553 0.564 0.538
SoftCardinality1 0.616 0.451 0.647 0.634 0.620 0.594
UKP-BIU1 0.472 0.313 0.670 0.573 0.5772 0.521
Median 0.604 0.467 0.625 0.554 0.557 0.566
Baselines:
Lexical 0.578 0.500 0.523 0.520 0.554 0.535
Majority 0.229 0.248 0.260 0.239 0.249 0.245
Table 4: Three-way task weighted-average F1
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.494 0.441 0.373 0.412 0.415 0.427
CNGL2 0.567 0.450 0.330 0.308 0.311 0.393
CoMeT1 0.715 0.466 0.640 0.380 0.404 0.521
ETS1 0.592 0.521 0.477 0.459 0.439 0.498
ETS2 0.710 0.585 0.643 0.389 0.367 0.539
LIMSIILES1 0.563 0.431 0.404 0.409 0.429 0.447
SoftCardinality1 0.596 0.439 0.555 0.469 0.486 0.509
UKP-BIU1 0.468 0.333 0.620 0.458 0.487 0.473
Median 0.580 0.446 0.516 0.411 0.422 0.485
Baselines:
Lexical 0.552 0.477 0.405 0.390 0.416 0.448
Majority 0.191 0.197 0.201 0.194 0.197 0.196
Table 5: Three-way task macro-average F1
the nine systems in the two-way task outperformed
the lexical baseline on the mean TS results. In fact,
the average performance fell below the lexical base-
line. The differences in the macro-average F1 be-
tween the top results on a SCIENTSBANK TS and
the corresponding lexical baselines were all statis-
tically significant. Two of the top results on BUA
were not statistically better than the lexical base-
line, and all systems performed below the baseline
on BUQ.
4.7 Discussion
All of the systems consistently outperformed the
most frequent class baseline. Beating the lexical
overlap baseline proved to be more challenging, be-
ing achieved by just over half of the results with
about half of those being statistically significant im-
provements. This underscores the fact that there is
still a considerable opportunity to improve student
269
Dataset: BEETLE SCIENTSBANK
Run UA UQ UA UQ UD Mean
CELI1 0.640 0.656 0.588 0.619 0.615 0.624
CNGL2 0.800 0.666 0.5911 0.561 0.556 0.635
CoMeT1 0.833 0.695 0.768 0.579 0.670 0.709
CU1 0.778 0.689 0.603 0.638 0.673 0.676
ETS1 0.802 0.720 0.705 0.688 0.683 0.720
ETS2 0.833 0.702 0.762 0.602 0.543 0.688
LIMSIILES1 0.723 0.641 0.583 0.629 0.648 0.645
SoftCardinality1 0.774 0.635 0.715 0.737 0.705 0.713
UKP-BIU1 0.608 0.481 0.726 0.669 0.6662 0.630
Median 0.778 0.666 0.705 0.629 0.666 0.676
Baselines:
Lexical 0.788 0.725 0.617 0.630 0.650 0.682
Majority 0.375 0.367 0.362 0.371 0.367 0.368
Table 6: Two-way task macro-average F1
response assessment systems.
The set of top performing systems on the
weighted F1 for a given TS were also always in the
top on the macro-average F1, but a small number of
additional systems joined the top performing set on
the macro-average F1. Specifically, one, three, and
two results joined the top set in the five-way, three-
way, and two-way tasks, respectively. In principle,
the metrics could differ substantially, because of the
treatment of minority classes, but in practice they
rarely did. Only one pair of participants swap adja-
cent TS mean rankings on the macro-average F1 rel-
ative to the weighted F1 on the two-way task. On the
five-way task, two pairs swap rankings and another
participant moved up two positions in the ranking,
ending at the median value.
Most (28/34) rank changes were only one position
and most (21/34) were in positions at or below the
median ranking. In the five-way task, a pair of sys-
tems, UKP-BIU1 and ETS1, had a meaningful per-
formance rank swap on the macro-average F1 rela-
tive to the weighted F1 on the UD test set. Specifi-
cally, UKP-BIU1 moved up four positions from rank
6, where it was not statistically better than the lexical
baseline, to the second best performance.
Not surprisingly, performance on UA was sub-
stantially higher than on UQ and UD, since the UA
is the only set which contains questions with exam-
ple answers in training data. Performance on BUA
was usually better than performance on SUA, most
likely because BUA contains more similar questions
and answers, focusing on a single science area, Elec-
tricity and Magnetism, compared to 12 distinct sci-
ence topics in SUA). In addition, the BEETLE study
participants may have used simpler language, since
they were aware that they were talking to a computer
system instead of writing down answers for human
teachers to assess as in SCIENTSBANK.
Performance on BUQ versus SUQ was much
more varied, presumably since there was no direct
training data for either TS. For the five-way task, the
best performance on the weighted F1 measure for
BUQ is 0.09 below the best result for BUA and the
analogous decrease from SUA to SUQ is 0.13, with
an additional 0.02 drop on SUD. On the two-way
task, the best weighted F1 for BUQ drops 0.11 from
the best BUA value, but the decrease from SUA to
SUQ is just 0.03, with another 0.03 drop to SUD.
While the drop in performance is fairly similar from
BUA to BUQ on all tasks and either metric, the de-
crease from SUA to SUQ seems to potentially be
dependent on the task, ranging from 0.13 on the five-
way task to 0.08 on the three-way task and 0.03 on
the two-way task.
5 Pilot Task on Partial Entailment
The SCIENTSBANK corpus was originally devel-
oped to assess student answers at a very fine-grained
level and contains additional annotations that break
down the answers into ?facets?, or low-level con-
cepts and relationships connecting them (hence-
forth, SCIENTSBANK Extra). This annotation aims
to support educational systems in recognizing when
specific parts of a reference answer are expressed
in the student answer, even if the reference answer
is not entailed as a whole (Nielsen et al, 2008b).
The task of recognizing such partial entailment rela-
tionships may also have various uses in applications
such as summarization or question answering, but it
has not been explored in previous RTE challenges.
Therefore, we proposed a pilot task on partial en-
tailment, in which systems are required to recognize
whether the semantic relation between specific parts
of the Hypothesis is expressed by the Text, directly
or by implication, even though entailment might not
be recognized for the Hypothesis as a whole, based
on the SCIENTSBANK facet annotation.
Each reference answer in SCIENTSBANK data is
broken down into facets, where a facet is a triplet
270
consisting of two key terms (both single words and
multi-words, e.g. carbon dioxide, each other, burns
out) and a relation linking them, as shown in Figure
2. The student answers were then annotated with
regards to each reference answer facet in order to
indicate whether the facet was (i) expressed, either
explicitly or by assumption or easy inference; (ii)
contradicted; or (iii) left unaddressed. Considering
the SCIENTSBANK reference answers as Hypothe-
ses, the facets capture their atomic components, and
facet annotations may correspond to the judgments
on the sub-parts of the H which are entailed by T.
We carried out a feasibility study to explore this
idea and to verify how well the facet annotations
align with traditional entailment judgments. We
focused on the reference answer facets labeled in
the gold standard annotation as Expressed or Unad-
dressed. The working hypothesis was that Expressed
labels assigned in SCIENTSBANK annotations cor-
responded to Entailed judgments in traditional tex-
tual entailment annotations, while Unaddressed la-
bels corresponded to No-entailment judgments.
Similarly to the feasibility study reported in Sec-
tion 3.2, we concluded that the correspondence be-
tween educational labels and entailment judgments
was not perfect due to the difference in educational
and textual entailment perspectives. Nevertheless,
the two classes of assessment appeared to be suffi-
ciently well correlated so as to offer a good testbed
for partial entailment in a natural setting.
5.1 Task Definition
Given (i) a text T, made up of a Question and a Stu-
dent Answer; (ii) a hypothesis H, i.e. the Reference
Answer for that question and (iii) a facet, i.e. a pair
of key terms in H, the task consists of determining
whether T expresses, either directly or by implica-
tion, the same relationship between the facet words
as in H. In other words, for each of H?s facets the
system assign one of the following judgments: Ex-
pressed, if the Student Answer expresses the same
relationship between the meaning of the facet terms
as in H; Unaddressed, if it does not.
Consider the example shown in Figure 2. For
facet 3, the system must decide whether the same re-
lation between the two terms ?contains? and ?seeds?
in H (the reference answer) is expressed, explicitly
or implicitly, in T (the combination of question and
student response). If the student answer is ?The part
of a plant you are observing is a fruit if it has seeds.?,
the answer to the question is ?yes? and the correct
judgment is ?Expressed?. But if the student says
?My rule is has to be sweet.?, T does not express
the same semantic relationship between ?contains?
and ?seeds? exhibited in H, thus the correct judgment
is ?Unaddressed?. Note that even though this is an
exercise in textual entailment, student response as-
sessment labels were used instead of traditional en-
tailment judgments, due to the partial mismatch be-
tween the two assessment classes found in the feasi-
bility study.
5.2 Dataset
We used a subset of the SCIENTSBANK Extra cor-
pus (Nielsen et al, 2008b) with the same problem-
atic questions filtered out as the main task (see Sec-
tion 3.3). We further filtered out all the student
answer facets which were labeled other than ?Ex-
pressed? or ?Unaddressed? in the gold standard an-
notation; the facets in which the relationship be-
tween the two key terms, as classified in the manual
annotation, proved to be problematic to define and
judge, namely Topic, Agent, Root, Cause, Quanti-
fier, Neg; and inter-propositional facets, i.e. facets
that expressed relations between higher-level propo-
sitions. Finally, the facet relations were removed
from the dataset, leaving the relationship between
the two facet terms unspecified so as to allow a more
fuzzy approach to the inference problem posed by
the exercise.
We used the same training/test split as reported in
Section 3.4. The training set created from the Train-
ing SCIENTSBANK Extra corpus contains 13,145
reference answer facets, 5,939 of which were la-
beled as ?Expressed? in the student answers and
7,206 as ?Unaddressed?. The Test set was created
from the SCIENTSBANK Extra unseen data and is
divided into the same subsets as the main task (Un-
seen Answers, Unseen Questions and Unseen Do-
mains). It contains 16,263 facets total, with 5,945
instances labeled as ?Expressed?, and 10,318 labeled
as ?Unaddressed?.
5.3 Evaluation Metrics and Baselines
The metrics used in the Pilot task were the same as in
the Main task, i.e. Overall Accuracy, Macroaverage
271
QUESTION: What is your ?rule? for deciding if the part of a plant you are observing is a fruit?
REFERENCE ANSWER: If a part of the plant contains seeds, that part is the fruit.
FACET 1: Relation NMod of Term1 part Term2 plant
FACET 2: Relation Theme Term1 contains Term2 part
FACET 3: Relation Material Term1 contains Term2 seeds
FACET 4: Relation Be Term1 fruit Term2 part
Figure 2: Example of facet annotations supporting the partial entailment task
Run UA UQ UD UA UQ UD
Weighted Averaged Macro Average
Run1 0.756 0.71 0.76 0.7370 0.686 0.755
Run 2 0.782 0.765 0.816 0.753 0.73 0.804
Run 3 0.744 0.733 0.77 0.719 0.7050 0.761
Baseline 0.54 0.547 0.478 0.402 0.404 0.384
Table 7: Weighted-average and macro-average F1 scores
(UA: Unseen Answers; UQ: Unseen Questions; UD Un-
seen Domains)
.
and Weighted Average Precision, Recall and F1, and
computed as described in Section 4.2. We used only
a majority class baseline, which labeled all facets
as ?Unaddressed?. Its performance is presented in
Section 5.4 jointly with the system results.
5.4 Participants and results
Only one participant, UKP-BIU, participated in the
Partial Entailment Pilot task. The UKP-BIU system
is a hybrid of two semantic relationship approaches,
namely (i) computing semantic textual similarity
by combining multiple content similarity measures
(Ba?r et al, 2012), and (ii) recognizing textual en-
tailment with BIUTEE (Stern and Dagan, 2011).
The two approaches are combined by generating in-
dicative features from each one and then applying
standard supervised machine learning techniques to
train a classifier. The system used several lexical-
semantic resources as part of the BIUTEE entail-
ment system, together with SCIENTSBANK depen-
dency parses and ESA semantic relatedness indexes
from Wikipedia.
The team submitted the maximum allowed of 3
runs. Table 7 shows Weighted Average and Macro
Average F1 scores respectively, also for the major-
ity baseline. The system outperformed the majority
baseline on both metrics. The best performance was
observed on Run 2, with the highest results on the
Unseen Domains test set.
6 Conclusions and Future Work
The Joint Student Response Analysis and 8th Rec-
ognizing Textual Entailment challenge has proven
to be a useful, interdisciplinary task using a realis-
tic dataset from the educational domain. In almost
all cases the best systems significantly outperformed
the lexical overlap baseline, sometimes by a large
margin, showing that computational linguistics ap-
proaches can contribute to educational tasks. How-
ever, the lexical baseline was not trivial to beat, par-
ticularly in the 2-way task. These results are consis-
tent with similar findings in previous RTE exercises.
Moreover, there is still significant room for improve-
ment in the absolute scores, reflecting the interesting
challenges that both educational data and RTE tasks
present to computational linguistics.
The educational setting places new stresses on
semantic inference technology because the educa-
tional notion of ?Expressed? and the RTE notion of
?Entailed? are slightly different. This raises the ed-
ucational question of whether RTE can work in this
setting, and the RTE question of whether this set-
ting is meaningful for evaluating RTE system per-
formance. The experimental results suggests that the
answer to both questions is ?yes?, a significant find-
ing for both educators and RTE technologists going
forward.
The Pilot task, aimed at exploring notions of par-
tial entailment, so far not explored in the series of
RTE challenges, has proven to be an interesting,
though challenging exercise. The novelty of the
task, namely performing textual entailment not on a
pair of full texts, but between a text and a hypothesis
consisting of a pair of words, may have represented
a more complex task than expected for some textual
entailment engines. Despite this, the encouraging
results obtained by the team which carried out the
exercise has shown that this partial entailment task
is worthy of further investigation.
272
Acknowledgments
The research reported here was supported by the US
ONR award N000141010085 and by the Institute of
Education Sciences, U.S. Department of Education,
through Grant R305A120808 to the University of
North Texas. The opinions expressed are those of
the authors and do not represent views of the Insti-
tute or the U.S. Department of Education. The RTE-
related activities were partially supported by the
Pascal-2 Network of Excellence, ICT-216886-NOE.
We would also like to acknowledge the contribution
of Alessandro Marchetti and Giovanni Moretti from
CELCT to the organization of the challenge.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning, and Assessment, 4(3), February.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In Proceedings of the 6th International Work-
shop on Semantic Evaluation, held in conjunction with
the 1st Joint Conference on Lexical and Computa-
tional Semantics, pages 435?440, Montreal, Canada,
June.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernando Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge. In
Proceedings of Text Analysis Conference (TAC) 2009.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. Thesixth PAS-
CAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2011. The seventh
PASCAL recognizing textual entailment challenge. In
Notebook papers and results, Text Analysis Confer-
ence (TAC).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment chal-
lenge. In J. Quin?onero-Candela, I. Dagan, B. Magnini,
and F. d?Alche? Buc, editors, Machine Learning Chal-
lenges, volume 3944 of Lecture Notes in Computer
Science. Springer.
Myroslava O. Dzikovska, Johanna D. Moore, Natalie
Steinhauser, Gwendolyn Campbell, Elaine Farrow,
and Charles B. Callaway. 2010. Beetle II: a system
for tutoring and computational linguistics experimen-
tation. In Proc. of ACL 2010 System Demonstrations,
pages 13?18.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: A dataset and baselines. In
Proc. of 2012 Conference of NAACL: Human Lan-
guage Technologies, pages 200?210.
Danilo Giampiccolo, Hoa Trang Dang, Bernardo
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth PASCAL recognizing textual entail-
ment challenge. In Proceedings of Text Analysis Con-
ference (TAC) 2008, Gaithersburg, MD, November.
Michael Glass. 2000. Processing language input in the
CIRCSIM-Tutor intelligent tutoring system. In Pa-
pers from the 2000 AAAI Fall Symposium, Available
as AAAI technical report FS-00-01, pages 74?79.
A. C. Graesser, K. Wiemer-Hastings, P. Wiemer-
Hastings, and R. Kreuz. 1999. Autotutor: A simu-
lation of a human tutor. Cognitive Systems Research,
1:35?51.
Xiangen Hu, Zhiqiang Cai, Max Louwerse, Andrew Ol-
ney, Phanni Penumatsa, and Art Graesser. 2003. A
revised algorithm for latent semantic analysis. In Pro-
ceedings of the 18th International Joint Conference on
Artificial intelligence (IJCAI?03), pages 1489?1491,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Pamela W. Jordan, Maxim Makatchev, and Kurt Van-
Lehn. 2004. Combining competing language under-
standing approaches in an intelligent tutoring system.
In Proc. of Intelligent Tutoring Systems Conference,
pages 346?357.
Pamela Jordan, Maxim Makatchev, Umarani Pap-
puswamy, Kurt VanLehn, and Patricia Albacete.
2006. A natural language tutorial dialogue system for
physics. In Proc. of 19th Intl. FLAIRS conference,
pages 521?527.
Claudia Leacock and Martin Chodorow. 2003. C-rater:
Automated scoring of short-answer questions. Com-
puters and the Humanities, 37(4):389?405.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammati-
cal Error Detection for Language Learners. Synthesis
Lectures on Human Language Technologies. Morgan
& Claypool Publishers.
Philip M. McCarthy, Vasile Rus, Scott A. Crossley,
Arthur C. Graesser, and Danielle S. McNamara. 2008.
Assessing forward-, reverse-, and average-entailment
indices on natural language input from the intelligent
tutoring system, iSTART. In Proc. of 21st Intl. FLAIRS
conference, pages 165?170.
Michael Mohler, Razvan Bunescu, and Rada Mihalcea.
2011. Learning to grade short answer questions using
273
semantic similarity measures and dependency graph
alignments. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 752?762, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and stu-
dent performance. Technical report, Student Achieve-
ment Partners. http://www.ccsso.org/
Documents/2012/Measures%20ofText%
20Difficulty_fina%l.2012.pdf.
Rodney D. Nielsen, Wayne Ward, and James H. Martin.
2008a. Learning to assess low-level conceptual under-
standing. In Proc. of 21st Intl. FLAIRS Conference,
pages 427?432.
Rodney D. Nielsen, Wayne Ward, James H. Martin, and
Martha Palmer. 2008b. Annotating students? under-
standing of science concepts. In Proceedings of the
Sixth International Language Resources and Evalua-
tion Conference, (LREC08), Marrakech, Morocco.
Sarah Petersen and Mari Ostendorf. 2009. A machine
learning approach to reading level assessment. Com-
puter, Speech and Language, 23(1):89?106.
Heather Pon-Barry, Brady Clark, Karl Schultz, Eliza-
beth Owen Bratt, and Stanley Peters. 2004. Advan-
tages of spoken language interaction in dialogue-based
intelligent tutoring systems. In Proc. of ITS-2004 Con-
ference, pages 390?400.
Stephen G Pulman and Jana Z Sukkarieh. 2005. Au-
tomatic short answer marking. In Proceedings of the
Second Workshop on Building Educational Applica-
tions Using NLP, pages 9?16, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Kathryn M. Sheehan, Irene Kostin, Yoko Futagi, and
Michael Flor. 2010. Generating automated text com-
plexity classifications that are aligned with targeted
text complexity standards. Technical Report RR-10-
28, Educational Testing Service.
Mark D. Shermis and Jill Burstein, editors. 2013. Hand-
book on Automated Essay Evaluation: Current Appli-
cations and New Directions. Routledge.
Asher Stern and Ido Dagan. 2011. A confidence
model for syntactically-motivated entailment proofs.
In Recent Advances in Natural Language Process-
ing (RANLP 2011), pages 455?462, Hissar, Bulgaria,
September.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vla-
havas. 2010. Mining multi-label data. In Oded
Maimon and Lior Rokach, editors, Data Mining
and Knowledge Discovery Handbook, pages 667?685.
Springer US.
Kurt VanLehn, Pamela Jordan, and Diane Litman. 2007.
Developing pedagogically effective tutorial dialogue
tactics: Experiments and a testbed. In Proc. of SLaTE
Workshop on Speech and Language Technology in Ed-
ucation, Farmington, PA, October.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proceed-
ings of the 18th International Conference on Compu-
tational linguistics (COLING 2000), pages 947?953,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
274
Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 25?32,
Portland, Oregon, June 23, 2011. c?2011 Association for Computational Linguistics
Who wrote What Where: Analyzing the content of human and automatic
summaries
Karolina Owczarzak and Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@nist.gov hoa.dang@nist.gov
Abstract
Abstractive summarization has been a long-
standing and long-term goal in automatic sum-
marization, because systems that can generate
abstracts demonstrate a deeper understanding
of language and the meaning of documents
than systems that merely extract sentences
from those documents. Genest (2009) showed
that summaries from the top automatic sum-
marizers are judged as comparable to manual
extractive summaries, and both are judged to
be far less responsive than manual abstracts,
As the state of the art approaches the limits
of extractive summarization, it becomes even
more pressing to advance abstractive summa-
rization. However, abstractive summarization
has been sidetracked by questions of what
qualifies as important information, and how do
we find it? The Guided Summarization task
introduced at the Text Analysis Conference
2010 attempts to neutralize both of these prob-
lems by introducing topic categories and lists
of aspects that a responsive summary should
address. This design results in more similar
human models, giving the automatic summa-
rizers a more focused target to pursue, and also
provides detailed diagnostics of summary con-
tent, which can can help build better meaning-
oriented summarization systems.
1 Introduction
What qualifies as important information and how do
we find it? These questions have been leading re-
search in automatic summarization since its begin-
nings, and we are still nowhere near a definitive
answer. Worse, experiments with humans subjects
suggest a definitive answer might not even exist.
With all their near-perfect language understanding
and world knowledge, two human summarizers will
still produce two different summaries of the same
text, simply because they will disagree on what?s
important. Fortunately, usually some of this infor-
mation will overlap. This is represented by the idea
behind the Pyramid evaluation framework (Nenkova
and Passonneau, 2004; Passonneau et al, 2005),
where different levels of the pyramid represent the
proportion of concepts (?Summary Content Units?,
or SCUs) mentioned by 1 to n summarizers in sum-
maries of the same text. Usually, there are very few
SCUs that are mentioned by all summarizers, a few
more that are mentioned by some of them, and the
greatest proportion are the SCUs that are mentioned
by individual summarizers only.
This variance in what should be a ?gold standard?
makes research in automatic summarization meth-
ods particularly difficult. How can we reach a goal
so vague and under-defined? Using term frequency
to determine important concepts in a text has proven
to be very successful, largely because of its simplic-
ity and universal applicability, but statistical meth-
ods can only provide the most basic level of perfor-
mance. On the other hand, there is no real motiva-
tion to use any deeper meaning-oriented text anal-
ysis if we are not even certain what information to
look for in order to produce a responsive summary.
To address these concerns, the Summarization
track at the 2010 Text Analysis Conference1 (TAC)
introduced a new summarization task ? Guided
Summarization ? in which topics are divided into
1All datasets available at http://www.nist.gov/tac/
25
narrow categories and a list of required aspects is
provided for each category. This serves two pur-
poses: first, it creates a more focused target for au-
tomatic summarizers, neutralizing human variance
and pointing to concrete types of information the
reader requires, and second, it provides a detailed
diagnostic tool to analyze the content of automatic
summaries, which can help build more meaning-
oriented systems. This paper shows how these ob-
jectives were achieved in TAC 2010, looking at the
similarity of human-crafted models, and then using
the category and aspect information to look in depth
at the differences between human and top automatic
summarizers, discovering strengths and weaknesses
of automatic systems and areas for improvement.
2 Topic-specific summarization
The idea that different types of stories might require
different approaches is not new, although the classi-
fication varies from task to task. Topic categories
were present in Document Understanding Confer-
ence2 (DUC) 2001, where topics were divided into:
single-event, single-subject, biographical, multiple
events of same type, and opinion. In their analy-
sis of these results, Nenkova and Louis (2008) find
that summaries of articles in what they call topic-
cohesive categories (single-event, single-subject, bi-
ography) are of higher quality than those in non-
cohesive categories (opinion, multiple event).
In essence, categorizing topics into types is based
on the assumption that stories of the same type fol-
low a specific template and include the same kinds
of facts, and this predictability might be employed
to improve the summarization process, since we at
least know what kinds of information are important
and what to look for. This was shown, among others,
by Bagga (1997), who analyzed source articles used
in the Message Understanding Conference (MUC)
and graphed the distribution of facts in articles on
air vehicle launches, terrorist attacks, joint ventures,
and corporate personnel changes, finding that the
same kinds of facts appeared repeatedly. A nat-
ural conclusion is that Information Extraction (IE)
methods might be helpful here, and in fact, White
et al (2001) presented an IE-based summarization
system for natural disasters, where they first filled
2http://www-nlpir.nist.gov/projects/duc/
an IE template with slots related to date, location,
type of disaster, damage (people, physical effects),
etc. Similarly, Radev and McKeown (1998) used IE
combined with Natural Language Generation (NLG)
in their SUMMON system.
There are two ways to classify stories: according
to their level of cohesiveness (to use the distinction
made by Nenkova and Louis (2008)), and accord-
ing to subject. The first classification could help
us determine which topics would be easier for au-
tomatic summarization, but the difficulty is related
purely to lexical characteristics of the text; as shown
in Louis and Nenkova (2009), source document sim-
ilarity in terms of word overlap is one of the pre-
dictive features of multi-document summary qual-
ity. The second classification, according to subject
matter, is what enables us to utilize more meaning-
oriented approaches such as IE and attempt a deeper
semantic analysis of the source text, and is what we
describe in this paper.
3 Guided summarization at TAC
The new guided summarization task in 2010 was
designed with the second classification in mind,
in order to afford the participants a chance to
explore deeper linguistic methods of text analy-
sis. There were five topic categories: (1) Acci-
dents and Natural Disasters, (2) Attacks (Crimi-
nal/Terrorist), (3) Health and Safety, (4) Endangered
Resources, and (5) Trials and Investigations (Crim-
inal/Legal/Other).3 In contrast to previous topic-
specific summarization tasks, the Guided Summa-
rization task also provided a list of required aspects,
which described the type of information that should
be included in the summary (if such information
could be found in source documents). Summariz-
ers also had the option of including any other in-
formation they deemed important to the topic. The
categories and their aspects, shown in Table 1, were
developed on the basis of past DUC and TAC topics
and model summaries from years 2001-2009.
Each topic came with 20 chronologically ordered
3In the remainder of this paper, the following short forms are
used for names of categories: Accidents = Accidents and Nat-
ural Disasters; Attacks = Attacks; Health = Health and Safety;
Resources = Endangered Resources; Trials = Trials and Inves-
tigations. Full description of the task is available at the TAC
website.
26
Accidents Attacks Health
what what what
when when who affected
where where how
why perpertrators why
who affected why countermeasures
damages who affected
countermeasures damages
countermeasures
Resources Trials
what who
importance who investigating
threats why
countermeasures charges
plead
sentence
Table 1: Categories and aspects in TAC 2010 Guided
Summarization task.
news articles. The initial summaries were to be pro-
duced on the basis of the first 10 documents. As
in TAC 2008 and 2009, the 2010 Summarization
task had an update component: using the second 10
documents, summarizers were to produce an update
summary under the assumption that the user had al-
ready read the first set of source documents. This
means that for the update part, there were two in-
teracting conditions, with the requirement for non-
redundancy taking priority over the requirement to
address all category aspects.
For each topic, four model summaries were writ-
ten by human assessors. All summaries were eval-
uated with respect to linguistic quality (Overall
Readability), content (Pyramid), and general quality
(Overall Responsiveness). Readability and Respon-
siveness were judged by human assessors on a scale
from 1 (very poor) to 5 (very good), while Pyramid
is a score between 0 and 1 (in very rare cases, it
exceeds 1, if the candidate summary contains more
SCUs than the average reference summary).
Since this was the first year of Guided Summa-
rization, only about half of the 43 participating sys-
tems made some use of the provided categories and
aspects, mostly using them and their synonyms as
query terms.
3.1 Model summaries across years
The introduction of categories, which implies tem-
plate story types, and aspects, which further nar-
rows content selection, resulted in the parallel model
summaries being much more similar to each other
than in previous years, as represented by the Pyra-
human automatic
initial update initial update
P
yr
am
id 2008 0.66 0.63 0.26 0.20
2009 0.68 0.60 0.26 0.20
2010 0.78 0.67 0.30 0.20
R
es
po
ns
.
2008 4.62 4.62 2.32 2.02
2009 4.66 4.48 2.32 2.17
2010 4.76 4.71 2.56 2.10
Table 2: Macro-average Pyramid and Responsiveness
scores for initial and update summaries for years 2008-
2010. Responsiveness scores for 2009 were scaled from
a ten-point to a five-point scale.
mid score, which measures information overlap be-
tween a candidate summary and a set of refer-
ence summaries. Table 2 shows the macro-averaged
Pyramid and Responsiveness scores for years 2008-
2010. Both initial and update human summaries
score higher for Pyramid in 2010, and also gain a lit-
tle in Responsiveness. The macro-averages for auto-
matic summarizers, on the other hand, increase only
for initial summaries, which we will discuss further
in Section 3.4. The similarity effect among model
summaries can be more clearly seen in Table 3,
which shows the percentage of Summary Content
Units (SCUs, information ?nuggets?or simple facts)
with different weights in Pyramids across the years
between 2008-2010. The weight of an SCU is sim-
ply the number of model summaries in which this
information unit appears. Pyramids in 2010 have
greater percentage of SCUs with weight > 1, and
their proportion of weight-1 SCUs is below half of
all SCUs. The difference is much more pronounced
for the initial summaries, since the update compo-
nent is restricted by the non-redundancy require-
ment, resulting in more variance in content selection
after the required aspects have been covered.4
3.2 Content coverage in TAC 2010
During the Pyramid creation process, assessors ex-
tracting SCUs from model summaries were asked to
mark the aspect(s) relevant to each SCU. This lets
us examine and compare the distribution of infor-
mation in human and automatic summaries. Table 4
shows macro-average SCU counts in Pyramids com-
4Each summary could be up to 100 words long, and no
incentive was given for writing summaries of shorter length;
therefore, the goal for both human and automatic summarizers
was to fit as much relevant information as possible in the 100-
word limit.
27
SCU
weight 2008 2009 2010
in
it
ia
l 4 9% 12% 22%
3 14% 13% 18%
2 22% 23% 24%
1 55% 52% 36%
up
da
te
4 8% 7% 11%
3 12% 12% 14%
2 21% 20% 26%
1 59% 62% 49%
Table 3: Percentage of SCUs with weights 1?4 in pyra-
mids for initial and update summaries for years 2008-
2010.
posed of four human summaries, and macro-average
counts of matching SCUs in the summaries of the
15 top-performing automatic summarizers (as deter-
mined by their Responsiveness rank on initial sum-
maries).5 Although automatic summaries find only
a small percentage of all available information (as
represented by the number of Pyramid SCUs), the
SCUs they find for the initial summaries are usually
those of the highest weight, i.e. encoding informa-
tion that is the most essential to the topic.
SCU distribution in human summaries is also in-
teresting: Health, Resources, and Trials all have
the expected pyramid shape, with many low-weight
SCUs at the base and few high-weight SCUs on top,
but for Attacks and Accidents, the usual pattern is
broken and we see an hourglass shape instead, re-
flecting the presence of many weight-4 SCUs. The
most likely explanation is that these two categories
are guided by a relatively long list of aspects (cf.
Table 1), many of which have unique answers in the
source text.
This is shown in more detail in Table 5, which
presents aspect coverage by Pyramids and top 15
automatic summarizers in terms of an average num-
ber of SCUs relevant to a given aspect and an aver-
age weight of an aspect-related SCU. Only Attack
and Accidents have aspects that tend to generate the
same answers from almost all human summarizers:
when, where in Accidents and what, when, where,
perpetrators, and who affected in Attacks all have
average weight of around 3. The patterns hold for
update summaries; although all values decrease and
5We chose to use the top 15 out of 43 participating systems
in order to exclude outliers like systems that returned empty
summaries, and to measure the state-of-the-art in the summa-
rization field.
SCU
weight initial update
pyramids automatic pyramids automatic
A
cc
id
en
ts
4 6.4 3.2 1.9 0.5
3 3.7 1 3.43 0.8
2 6.9 1.6 6.1 0.6
1 7.9 0.8 7.6 0.7
total 24.9 7.7 19.1 3.1
A
tt
ac
ks
4 7.7 4.9 3.7 1
3 3.1 0.8 3.7 0.8
2 5 1 5.3 0.8
1 5.6 0.5 9.4 0.7
total 21.4 9.1 22.1 3.9
H
ea
lt
h
4 4.9 1.8 1.6 0.4
3 4.2 0.8 2.6 0.7
2 5.3 0.6 4.9 0.8
1 10.6 0.9 12 0.8
total 25 5 21 3
R
es
ou
rc
es
4 4.2 1.5 1.1 0.6
3 5.1 1.3 2.7 0.5
2 5 1 5.9 1
1 9.5 0.7 12.4 1
total 23.8 5 22.1 3.4
T
ri
al
s
4 4.4 2.6 3.4 1.2
3 5.7 2 3.3 0.5
2 7.8 1.6 5.7 0.6
1 9.2 0.5 8.5 0.6
total 27.1 8.5 20.9 3.3
Table 4: Macro-average SCU counts with weights 1?4 in
pyramids and matching SCU counts in automatic sum-
maries, for initial and update summaries.
there is less overlap between models, answers to
these aspects are the most likely to occur in multi-
ple summaries.
The situation for top 15 automatic summarizers
is even more interesting: while they contain rela-
tively few matching SCUs, the SCUs they do find
are those of high weight, as can be seen by compar-
ing their SCU weight averages. Even for ?other?,
which covers ?all other information important for
the topic? and is therefore more dependent on sum-
mary writer?s subjective judgment and shows more
content diversity, resulting in low-weight SCUs in
the Pyramid, the top automatic summarizers find
those most weighted. It would seem, then, that the
content selection methods are able to identify some
of the most important facts; at the same time, the
density of information in automatic summaries is
much lower than in human summaries, indicating
that the automatic content is either not compressed
adequately, or that it includes non-relevant or re-
peated information.
28
Avg SCU weight (avg SCU count)
initial summaries update summaries
Pyramids automatic Pyramids automatic
A
cc
id
en
ts
what 2.4 (4.4) 3.1 (1.9) 2.5 (2.7) 2.87 (0.6)
when 3.6 (2.1) 3.7 (0.7) 3.7 (0.4) 4 (0.1)
where 3.0 (3.6) 3.2 (1.3) 2.1 (1.1) 2.58 (0.4)
why 2.6 (2.3) 3.1 (0.5) 2.4 (2.0) 3 (0.3)
who aff 2.3 (4.9) 2.8 (1.5) 2.0 (4.1) 2.45 (0.6)
damages 1.8 (2.4) 3.1 (0.5) 1.7 (1.9) 2.05 (0.2)
counterm 2.1 (8.0) 2.7 (1.2) 2.0 (8.1) 2.4 (0.9)
other 1.3 (0.4) 1.9 (0.1) 1.3 (0.6) 1 (0.0)
A
tt
ac
ks
what 2.9 (3.1) 3.7 (1.6) 2.0 (1.4) 2.8 (0.4)
when 3.4 (1.3) 3.8 (0.4) 2.4 (1.4) 2.2 (0.1)
where 2.7 (2.9) 3.7 (1.2) 2.5 (0.9) 3.8 (0.3)
perpetr 2.8 (3.6) 3.4 (1.0) 2.2 (3.0) 3.0 (0.9)
why 2.1 (3.4) 2.8 (0.9) 1.8 (1.3) 1.6 (0.2)
who aff 3.3 (4.0) 3.6 (1.7) 2.0 (2.0) 2.1 (0.3)
damages 2.2 (0.9) 3.0 (0.2) 3.4 (0.7) 4.0 (0.1)
counterm 2.3 (4.3) 2.8 (1.1) 2.1 (10.3) 2.6 (1.1)
other 1.7 (1.3) 2.2 (0.1) 1.6 (2.6) 1.7 (0.2)
H
ea
lt
h
what 2.4 (6.0) 3.1 (1.6) 2.4 (2.9) 3.0 (0.7)
who aff 2.0 (5.6) 2.6 (0.8) 1.8 (2.7) 2.0 (0.3)
how 2.4 (6.6) 3.1 (1.1) 1.6 (2.7) 2.4 (0.3)
why 2.2 (3.9) 2.9 (0.6) 1.7 (2.3) 2.1 (0.4)
counterm 2.0 (6.3) 2.7 (0.8) 1.7 (10.4) 2.2 (1.0)
other 1.1 (0.6) 1.9 (0.1) 1.2 (1.9) 1.6 (0.2)
R
es
ou
rc
es
what 2.3 (3.2) 2.9 (1.3) 1.6 (1.4) 2.6 (0.4)
importan 2.4 (3.1) 2.7 (0.3) 1.8 (1.9) 2.3 (0.2)
threats 2.3 (7.6) 2.8 (1.6) 1.6 (6.8) 2.0 (1.1)
counterm 2.0 (10.1) 2.8 (1.7) 1.7 (12.1) 2.2 (1.4)
other 1.4 (0.7) 2.9 (0.1) 1.8 (1.2) 2.5 (0.1)
T
ri
al
s
who 2.7 (3.5) 3.2 (1.7) 2.7 (2.3) 3.2 (0.4)
who inv 1.9 (5.5) 2.8 (0.8) 1.8 (3.3) 2.6 (0.5)
why 2.6 (6.3) 3.1 (2.2) 1.8 (2.4) 2.3 (0.3)
charges 2.7 (2.4) 3.2 (0.8) 2.4 (1.4) 2.5 (0.3)
plead 2.0 (5.0) 2.9 (0.9) 2.1 (3.5) 3.0 (0.5)
sentence 2.3 (2.7) 3.0 (0.5) 2.6 (6.0) 3.5 (0.8)
other 1.5 (3.2) 2.0 (0.3) 1.7 (4.8) 2.4 (0.6)
Table 5: Aspect coverage for Pyramids and top 15 auto-
matic summarizers in TAC 2010.
3.3 Effect of categories and aspects
Some categories in the Guided Summarization task
are defined in more detail than others, depending
on types of stories they represent. Stories about at-
tacks and accidents (and, to some extent, trials) tend
to follow more predictable and detailed templates,
which results in more similar models and better re-
sults for automatic summarizers. Figure 1 gives a
graphic representation of the macro-average Pyra-
mid and Responsiveness scores for human and top
15 automatic summarizers, with exact scores in Ta-
bles 6 and 7, where the first score marked with a
letter is not statistically significant from any subse-
quent score marked with the same letter, according
to ANOVA (p>0.05). Lack of significant difference
between human Responsiveness scores in Table 6
suggests that, for all categories, human summaries
are highly and equally responsive, but a look at their
Pyramid scores confirms that Attacks and Accidents
models tend to have more overlapping information.
For automatic summaries, their Pyramid and Re-
sponsiveness patterns are parallel. Here Attacks,
Accidents, and Trials contain on average more
matching SCUs than Health and Resources, making
these summaries more responsive. One reason for
these differences might be that many systems rely on
sentence positon in the extraction process, and first
sentences in these template stories often are a short
description of event including date, location, persons
involved, in effect giving systems the unique-answer
aspects mentioned in Section 3.2. Table 5 shows
this distribution of matching information in more de-
tail: for Attacks and Accidents, automatic summa-
rizers match relatively more SCUs for what, where,
when, who affected than for countermeasures, dam-
ages, or other. For Trials, again the easier aspects
are those that tend to appear at the beginning of
documents: who [is under investigation] and why.
Stories in Health and Resources, the weakest cate-
gories overall for automatic summarizers and with
the greatest amount of variance for human summa-
rizers, are non-events, instead being closer to what
in past DUC tasks was described as a ?multi-event?
or ?single subject? story type. Individual documents
within the source set might sometimes follow the
typical event template (e.g. describing individual
instances of coral reef destruction), but in general
these categories require much more abstraction and
render the opening-sentence extraction strategy less
effective.
If the higher averages are really due to the infor-
mation extracted with first sentences, then we would
also expect higher scores from Baseline 1, which
simply selected the opening sentences of the most
recent document, up to the 100-word limit. And in-
deed, as shown in Table 8, the partial Pyramid scores
for Baseline 1 are the highest for exactly these ?con-
crete? categories and aspects, mostly for Attacks and
Accidents, and aspects such aswhere, what, andwho
(the score of 1 for Accidents other is an outlier, since
there was only one SCU relevant for this calcula-
tion and the baseline happened to match it). On the
other hand, its lowest performance is mostly con-
centrated in Health and Resources, and in the more
?vague? aspects, like why, how, importance, coun-
29
Pyramid Responsiveness
in
it
ia
l
Attacks 0.857 A Trials 4.825 A
Accidents 0.812 AB Accidents 4.821 AB
Resources 0.773 AB Attacks 4.786 ABC
Health 0.767 AB Health 4.750 ABCD
Trials 0.751 B Resources 4.650 ABCD
up
da
te
Trials 0.749 A Attack 4.857 A
Attacks 0.745 AB Trials 4.825 AB
Accidents 0.700 AB Accidents 4.714 ABC
Health 0.610 C Health 4.625 ABCD
Resources 0.604 C Resources 4.600 ABCD
Table 6: Macro-average Pyramid and Responsiveness
scores per category for human summaries, comparison
across categories.
Pyramid Responsiveness
in
it
ia
l
Attacks 0.524 A Attacks 3.400 A
Trials 0.446 B Accidents 3.362 AB
Accidents 0.418 B Trials 3.167 ABC
Resources 0.323 C Resources 2.893 CD
Health 0.290 C Health 2.617 D
up
da
te
Resources 0.286 A Resources 2.520 A
Trials 0.261 AB Health 2.417 AB
Attacks 0.251 ABC Trials 2.380 ABC
Health 0.236 BCD Attacks 2.286 ABCD
Accidents 0.228 BCD Accidents 2.248 ABCD
Table 7: Macro-average Pyramid and Responsiveness
scores per category for top 15 automatic summaries, com-
parison across categories.
termeasures, and other. We can conclude that early
sentence position is not a good predictor of such in-
formation, and that automatic summarizers might do
well to diversify their methods of content identifi-
cation based on what type of information they are
looking for.
3.4 Initial and update summaries
While the initial component is only guided by
the categories and aspects, the update component
is placed under an overarching condition of non-
redundancy. Update summaries should not repeat
Highest Lowest
Category Aspect score Category Aspect score
(Accidents Other 1) Resources other 0
Attacks WHERE 0.66 Health other 0
Attacks WHAT 0.66 Attacks COUNTERM 0
Trials WHO 0.6 Attacks other 0
Attacks WHO AFF 0.56 Accidents WHY 0
Accidents WHERE 0.44 Health WHO AFF 0
Accidents WHAT 0.41 Trials SENTENCE 0.06
Trials WHY 0.38 Health WHY 0.06
Attacks PERP 0.34 Accidents DAMAGES 0.07
Trials WHO INV 0.33 Health HOW 0.08
Trials CHARGES 0.33 Resources IMPORTAN 0.09
Table 8: Top Pyramid scores for Baseline 1, per aspect,
for initial summaries.
Figure 1: Macro-average Pyramid and Responsiveness
scores in initial and update summaries, for humans and
top 15 automatic systems. In each group, columns from
left: Accidents, Attacks, Health, Resources, Trials. As-
terisk indicates significant drop from initial score.
any information that can be found in the initial doc-
ument set. This restriction narrows the pool of po-
tential summary elements to choose from. More im-
portantly, since the concrete aspects with unique an-
swers like what, where, and when are likely to be
mentioned in the first set of document (and, by ex-
tension, in the initial summaries), this shifts content
selection to aspects that generate more variance, like
why, countermeasures, or other. As shown in Fig-
ure 1, while Responsiveness remains high for hu-
man summarizers across categories, which means
the content is still relevant to the topic, the Pyramid
scores are lower in the update component, which
means the summarizers differ more in terms of what
information they extract from the source documents.
Note that this is not the case for Trials, where the
human performance for both Responsiveness and
Pyramid is practically identical for initial and up-
date summaries. The time course of trials is gener-
ally longer than those for accidents and attacks, and
many of the later-occurring aspects such as plea and
sentence are well-defined; hence the initial and up-
date human summaries have similar Pyramid scores.
Automatic summarizers, on the other hand, suffer
the greatest drop in those categories in which they
were the most successful before: Attacks, Acci-
dents, and Trials, in effect rendering their perfor-
mance across categories more or less even (cf. Fig-
30
ure 1).
A closer look at the aspect coverage in initial and
update components confirms the differences in as-
pect distribution. Figure 2 gives four columns for
each aspect: the first two columns represent initial
summaries, the second two represent update sum-
maries. Dark columns in each pair are human sum-
marizers, light columns are top 15 automatic sum-
marizers. For almost all aspects, humans find fewer
relevant (and new!) facts in the update documents,
with the exception of sentence in Trials, and coun-
termeasures and other in all categories. Logically,
once all the anchoring information has been given
(date, time, location, event), the only remaining rel-
evant content to focus on are consequences of the
event (countermeasures, sentence), and possibly up-
dates in victims and damages (who affected, dam-
ages) as well as any other information that might be
relevant. A similar (though less consistent) pattern
holds for automatic summarizers.
4 Summary and conclusions
Initial attempts at more complex treatments of any
subject often fail when faced with unrestricted, ?real
world? input. This is why almost all research in
summarization remains centered around relatively
simple extractive methods. Few developers try to
incorporate syntactic parsing to compress summary
sentences, and almost none want to venture into se-
mantic decompositon of source text, since the com-
plexity of these methods is the cause of potential
errors. Also, the tools might not deal particularly
well with different types of stories in the ?newswire?
genre. However, Genest (2009) showed the limits
of purely extractive summarization: their manual,
extractive summarizer (HexTac) performed much
worse than human abstractors, and comparably to
the top automatic summarizers in TAC 2009.
But if we want to see significant progress in ab-
stractive summarization, it?s important to provide a
more controlled environment for such experiments.
TAC 2010 results show that, first of all, by guid-
ing summary creation we end up with more similar
human abstracts than in previous tasks (partly due
to the choice of template-like categories, and partly
due to the further guiding role of aspects). Narrow-
ing down possible summary content, while exclud-
Figure 2: Average number of SCUs per aspect in initial
and update summaries in TAC 2010. Dark grey = Pyra-
mids, light grey = top 15 automatic summarizers. The
first pair of columns for each aspects shows initial sum-
maries, the second pair shows update summaries.
31
ing variance due to subjective opinions among hu-
man writers, creates in effect a more concrete in-
formation model, and a single, unified information
model is an easier goal to emulate than relying on
vague and subjective goals like ?importance?. Out
of five categories, Attacks and Accidents generated
the most similar models, mostly because they re-
quired concrete, unique-answer aspects like where
or when. In Health and Resources, the aspects were
more subjective in nature, and the resulting variance
was greater.
Moreover, the Guided Task provides a very valu-
able and detailed diagnostic tool for system devel-
opers: by looking at the system performance within
each aspect, we can find out which types of infor-
mation it is better able to identify. While the top au-
tomatic summarizers managed to retrieve less than
half of relevant information at the best of times, the
facts they did retrieve were highly-weighted. Their
better performance for certain aspects of Attacks,
Accidents, and Trials could be ascribed to the fact
that most of them rely on sentence position to deter-
mine important information in the source document.
A comparison of covered aspects suggests that sen-
tence position might be a better indicator for some
types of information than others.
Since it was the first year of the Guided Task, only
some of the teams used the provided category/aspect
information; as the task continues, we hope to see
more participants adopting categories and aspects
to guide their summarization. The predictable el-
ements of each category invite the use of differ-
ent techniques depending on the type of informa-
tion sought, perhaps suggesting the use of Infor-
mation Extraction methods. Some categories might
be easier to process than others, but even if the
information-mining approach cannot be extended to
all types of stories, at worst we will end up with
better summarization for event-type stories, like at-
tacks, accidents, or trials, which together comprise a
large part of reported news.
References
Amit Bagga and Alan W. Biermann. 1997. Analyzing
the Complexity of a Domain With Respect To An In-
formation Extraction Task. Proceedings of the tenth
International Conference on Research on Computa-
tional Linguistics (ROCLING X), 175?194.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-
Monod. 2009. HEXTAC: the Creation of a Manual
Extractive Run. Proceedings of the Text Analysis Con-
ference 2009.
Annie Louis and Ani Nenkova. 2009. Performance
confidence estimation for automatic summarization.
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, 541?548. Athens, Greece.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbia?s Newsblaster. Proceed-
ings of the Second International Conference on Hu-
man Language Technology Research, 280?285. San
Diego, California.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. Proceedings of
ACL-08: HLT, 825?833. Columbus, Ohio.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. Multidocument
summarization via information extraction. 2001. Pro-
ceedings of the First International Conference on Hu-
man Language Technology Research, 1?7. San Diego,
California.
32
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 1?9,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Assessment of the Accuracy of Automatic Evaluation in Summarization
Karolina Owczarzak
Information Access Division
National Institute of Standards and Technology
karolina.owczarzak@gmail.com
John M. Conroy
IDA Center for Computing Sciences
conroy@super.org
Hoa Trang Dang
Information Access Division
National Institute of Standards and Technology
hoa.dang@nist.gov
Ani Nenkova
University of Pennsylvania
nenkova@seas.upenn.edu
Abstract
Automatic evaluation has greatly facilitated
system development in summarization. At the
same time, the use of automatic evaluation
has been viewed with mistrust by many, as its
accuracy and correct application are not well
understood. In this paper we provide an as-
sessment of the automatic evaluations used for
multi-document summarization of news. We
outline our recommendations about how any
evaluation, manual or automatic, should be
used to find statistically significant differences
between summarization systems. We identify
the reference automatic evaluation metrics?
ROUGE 1 and 2?that appear to best emu-
late human pyramid and responsiveness scores
on four years of NIST evaluations. We then
demonstrate the accuracy of these metrics in
reproducing human judgements about the rel-
ative content quality of pairs of systems and
present an empirical assessment of the rela-
tionship between statistically significant dif-
ferences between systems according to man-
ual evaluations, and the difference according
to automatic evaluations. Finally, we present a
case study of how new metrics should be com-
pared to the reference evaluation, as we search
for even more accurate automatic measures.
1 Introduction
Automatic evaluation of content selection in sum-
marization, particularly the ROUGE evaluation
toolkit (Lin and Hovy, 2003), has been enthusias-
tically adopted by researchers since its introduction
in 2003. It is now standardly used to report results in
publications; however we have a poor understanding
of the accuracy of automatic evaluation. How often
do we publish papers where we report an improve-
ment according to automatic evaluation, but never-
theless, a standard manual evaluation would have led
us to different conclusions? In our work we directly
address this question, and hope that our encouraging
findings contribute to a better understanding of the
strengths and shortcomings of automatic evaluation.
The aim of this paper is to give a better assessment
of the automatic evaluation metrics for content se-
lection standardly used in summarization research.
We perform our analyses on data from the 2008-
2011 Text Analysis Conference (TAC)1 organized
by the National Institute of Standards and Technol-
ogy (NIST). We choose these datasets because in
early evaluation initiatives, the protocol for manual
evaluation changed from year to year in search of
stable manual evaluation approaches (Over et al,
2007). Since 2008, however, the same evaluation
protocol has been applied by NIST assessors and we
consider it to be the model that automatic metrics
need to emulate.
We start our discussion by briefly presenting the
manual procedure for comparing systems (Section
2) and how these scores should be best used to iden-
tify significant differences between systems over a
given test set (Section 3). Then, we embark on our
discussion of the accuracy of automatic evaluation
and its ability to reproduce manual scoring.
To begin our analysis, we assess the accuracy of
common variants of ROUGE on the TAC 2008-2011
datasets (Section 4.1). There are two aspects of eval-
uation that we pay special attention to:
Significant difference Ideally, all system compar-
isons should be performed using a test for sta-
1http://www.nist.gov/tac/
1
tistical significance. As both manual metrics
and automatic metrics are noisy, a statistical
hypothesis test is needed to estimate the prob-
ability that the differences observed are what
would be expected if the systems are compa-
rable in their performance. When this proba-
bility is small (by convention 0.05 or less) we
reject the null hypothesis that the systems? per-
formance is comparable.
It is important to know if scoring a system via
an automatic metric will lead to conclusions
about the relative merits of two systems differ-
ent from what one would have concluded on the
basis of manual evaluation. We report very en-
couraging results, showing that automatic met-
rics rarely contradict manual metrics, and some
metrics never lead to contradictions. For com-
pleteness, given that most papers do not report
significance, we also compare the agreement
between manual and automatic metrics without
taking significance into account.
Type of comparison Established manual evalua-
tions have two highly desirable properties: (1)
they can tell apart good automatic systems from
bad automatic systems and (2) they can differ-
entiate automatic summaries from those pro-
duced by humans with high accuracy. Both
properties are essential. Obviously, choosing
the better system in development cycles is key
in eventually improving overall performance.
Being able to distinguish automatic from man-
ual summaries is a general sanity test 2 that any
evaluation adopted for wide use is expected to
pass?it is useless to report system improve-
ments when it appears that automatic methods
are as good as human performance3. As we will
see, there is no single ROUGE variant that has
both of these desirable properties.
Finally, in Section 5, we discuss ways to compare
other automatic evaluation protocols with the refer-
2For now, automatic systems do not have the performance
of humans, thus, the ability to distinguish between human and
automatically generated summaries is an exemplar of the wider
problem of distinguishing high quality summaries from others.
3Such anomalous findings, when using automatic evalua-
tion, have been reported for some summarization genres such
as summarization of meetings (Galley, 2006).
ence ROUGE metrics we have established. We de-
fine standard tests for significance that would iden-
tify evaluations that are significantly more accurate
than the current reference measures, thus warrant-
ing wider adoption for future system development
and reporting of results. As a case study we apply
these to the TAC AESOP (Automatically Evaluating
Summaries of Peers) task which called for the devel-
opment of novel evaluation techniques that are more
accurate than ROUGE evaluations.
2 Manual evaluation
Before automatic evaluation methods are developed,
it is necessary to establish a desirable manual eval-
uation which the automatic methods will need to re-
produce. The type of summarization task must also
be precisely specified?single- or multi-document
summarization, summarization of news, meetings,
academic articles, etc. Saying that an automatic
evaluation correlates highly with human judgement
in general, is disturbingly incomplete, as the same
automatic metric can predict some manual evalu-
ation scores for some summarization tasks well,
while giving poor correlation with other manual
scores for certain tasks (Lin, 2004; Liu and Liu,
2010).
In our work, we compare automatic metrics with
the manual methods used at TAC: Pyramid and Re-
sponsiveness. These manual metrics primarily aim
to assess if the content of the summary is appro-
priately chosen to include only important informa-
tion. They do not deal directly with the linguistic
quality of the summary?how grammatical are the
sentences or how well the information in the sum-
mary is organized. Subsequently, in the experiments
that we present in later sections, we do not address
the assessment of automatic evaluations of linguistic
quality (Pitler et al, 2010), but instead analyze the
performance of ROUGE and other related metrics
that aim to score summary content.
The Pyramid evaluation (Nenkova et al, 2007) re-
lies on multiple human-written gold-standard sum-
maries for the input. Annotators manually identify
shared content across the gold-standards regardless
of the specific phrasing used in each. The pyra-
mid score is based on the ?popularity? of informa-
tion in the gold-standards. Information that is shared
2
across several human gold-standards is given higher
weight when a summary is evaluated relative to the
gold-standard. Each evaluated summary is assigned
a score which indicates what fraction of the most
important information for a given summary size is
expressed in the summary, where importance is de-
termined by the overlap in content across the human
gold-standards.
The Responsiveness metric is defined for query-
focused summarization, where the user?s informa-
tion need is clearly stated in a short paragraph. In
this situation, the human assessors are presented
with the user query and a summary, and are asked
to assign a score that reflects to what extent the sum-
mary satisfies the user?s information need. There are
no human gold-standards, and the linguistic quality
of the summary is to some extent incorporated in the
score, because information that is presented in a con-
fusing manner may not be seen as relevant, while it
could be interpreted by the assessor more easily in
the presence of a human gold-standard. Given that
all standard automatic evaluation procedures com-
pare a summary with a set of human gold-standards,
it is reasonable to expect that they will be more accu-
rate in reproducing results from Pyramid evaluation
than results from Responsiveness judgements.
3 Comparing systems
Evaluation metrics are used to determine the rela-
tive quality of a summarization system in compari-
son to one or more systems, which is either another
automatic summarizer, or a human reference sum-
marizer. Any evaluation procedure assigns a score
to each summary. To identify which of the two sys-
tems is better, we could simply average the scores
of summaries produced by each system in the test
set, and compare these averages. This approach is
straightforward; however, it gives no indication of
the statistical significance of the difference between
the systems. In system development, engineers may
be willing to adopt new changes only if they lead
to significantly better performance that cannot be at-
tributed to chance.
Therefore, in order to define more precisely what
it means for a summarization system to be ?bet-
ter? than another for a given evaluation, we employ
statistical hypothesis testing comparisons of sum-
marization systems on the same set of documents.
Given an evaluation of two summarization systems
A and B we have the following:
Definition 1. We say a summarizer A ?signifi-
cantly outperforms? summarizer B for a given
evaluation score if the null hypothesis of the fol-
lowing paired test is rejected with 95% confidence.
Given two vectors of evaluation scores x and y,
sampled from the corresponding random vari-
ables X and Y, measuring the quality of sum-
marizer A and B, respectively, on the same col-
lection of document sets, with the median of x
greater than the median of y,
H0 : The median of X ? Y is 0.
Ha : The median of X ? Y is not 0.
We apply this test using human evaluation met-
rics, such as pyramid and responsiveness, as well as
automatic metrics. Thus, when comparing two sum-
marization systems we can, for example, say system
A significantly outperforms system B in responsive-
ness if the null hypothesis can be rejected. If the null
hypothesis cannot be rejected, we say system A does
not significantly perform differently than system B.
A complicating factor when the differences be-
tween systems are tested for significance, is that
some inputs are simply much harder to summarize
than others, and there is much variation in scores
that is not due to properties of the summarizers
that produced the summaries but rather properties of
the input text that are summarized (Nenkova, 2005;
Nenkova and Louis, 2008).
Given this variation in the data, the most appropri-
ate approach to assess significance in the difference
between system is to use paired rank tests such as
a paired Wilcoxon rank-sum test, which is equiva-
lent to the Mann-Whitney U test. In these tests, the
scores of the two systems are compared only for the
same input and ranks are used instead of the actual
difference in scores assigned by the evaluation pro-
cedures. Prior studies have shown that paired tests
for significance are indeed able to discover consid-
erably more significant differences between systems
than non-paired tests, in which the noise of input dif-
ficulty obscures the actual difference in system per-
3
formance (Rankel et al, 2011). For this paper, we
perform all testing using the Wilcoxon sign rank test.
4 How do we identify a good metric?
If we treat manual evaluation metrics as our gold
standard, then we require that a good automatic met-
ric mirrors the distinctions made by such a man-
ual metric. An automatic metric for summarization
evaluation should reliably predict how well a sum-
marization system would perform relative to other
summarizers if a human evaluation were performed
on the summaries. An automatic metric would hope
to answer the question:
Would summarizer A significantly outper-
form summarizer B when evaluated by a
human?
We address this question by evaluating how well
an automatic metric agrees with a human metric in
its judgements in the following cases:
? all comparisons between different summariza-
tion systems
? all comparisons between systems and human
summarizers.
Depending on the application, we may record the
counts of agreements and disagreements or we may
normalize these counts to estimate the probability
that an automatic evaluation metric will agree with a
human evaluation metric.
4.1 Which is the best ROUGE variant
In this section, we set out to identify which of the
most widely-used versions of ROUGE have highest
accuracy in reproducing human judgements about
the relative merits of pairs of systems. We exam-
ine ROUGE-1, ROUGE-2 and ROUGE-SU4. For
all experiments we use stemming and for each ver-
sion we test scores produced both with and without
removing stopwords. This corresponds to six differ-
ent versions of ROUGE that we examine in detail.
ROUGE outputs several scores including preci-
sion, recall, and an F-measure. However, the most
informative score appears to be recall as reported
when ROUGE was first introduced (Lin and Hovy,
2003). Given that in the data we work with, sum-
maries are produced for a specified length in word
s (and all summaries are truncated to the predefined
length), recall on the task does not allow for artifi-
cially high scores which would result by producing
a summary of excessive length.
The goal of our analysis is to identify which of the
ROUGE variants is most accurate in correctly pre-
dicting which of two participating systems is the bet-
ter one according to the manual pyramid and respon-
siveness scores. We use the data for topic-focused
summarization from the TAC summarization track
in 2008-20114.
Table 1 gives the overview of the 2008-2011 TAC
Summarization data, including the number of top-
ics and participants. For each topic there were four
reference (model) summaries, written by one of the
eight assessors; as a result, there were eight human
?summarizers,? but each produced summaries only
for half of the topics.
year topics automatic human references/
summarizers summarizers topic
2008 48 58 8 4
2009 44 55 8 4
2010 46 43 8 4
2011 44 50 8 4
Table 1: Data in TAC 2008-2011 Summarization track.
We compare each pair of participating systems
based on the manual evaluation score. For each pair,
we are interested in identifying the system that is
better. We consider both the case when an appropri-
ate test for statistical significance has been applied to
pick out the better system as well as the case where
simply the average scores of systems over the test set
are compared. The latter use of evaluations is most
common in research papers on summarization; how-
ever, in summarization system development, testing
for significance is important because a difference in
summarizer scores that is statistically significant is
much more likely to reflect a true difference in qual-
ity between the two systems.
Therefore, we look at agreement between
ROUGE and manual metrics in two ways:
? agreement about significant differences be-
tween summarizers, according to a paired
4In all these years systems also competed on producing up-
date summaries. We do not report results on this task for the
sake of simplifying the discussion.
4
Auto only Human-Automatic
Pyr Resp Pyr Resp
diff no diff contr diff no diff contr diff no diff contr diff no diff contr
r1m 91 59 0.85 87 51 1.34 91 75 0.06 91 100 0.45
r1ms 90 59 0.83 84 50 3.01 91 75 0.06 90 100 0.45
r2m 91 68 0.19 88 60 0.47 75 75 0.62 75 100 1.02
r2ms 88 72 0 84 62 0.65 73 75 1.56 72 100 1.95
r4m 91 64 0.62 87 56 0.91 82 75 0.43 82 100 0.83
r4ms 90 64 0.04 85 55 1.15 83 75 0.81 83 100 1.20
Table 2: Average percentage agreement between ROUGE and manual metrics about significant differences on TAC
2008-2011 data. r1 = ROUGE-1, r2 = ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; diff =
agreement on significant differences, no diff = agreement on lack of significant differences, contr = contradictions.
Auto only Human-Automatic
Pyr Resp Pyr Resp
metric sig all sig all sig all sig all
r1m 77 87 70 82 90 99 90 99
r1ms 77 88 69 80 90 98 90 98
r2m 81 89 75 83 75 94 75 94
r2ms 81 89 74 81 72 93 72 93
r4m 80 88 73 82 82 96 82 96
r4ms 79 89 71 81 83 96 83 96
Table 3: Average agreement between ROUGE and manual metrics on TAC 2008-2011 data. r1 = ROUGE-1, r2 =
ROUGE-2, r4 = ROUGE-SU4, m = stemmed, s = stopwords removed; sig = agreement on significant differences, all
= agreement on all differences.
Wilcoxon test. No adjustments for multiple
comparisons are made.
? agreement about any differences between sum-
marizers (whether significant on not).
Agreements occur when the two evaluation met-
rics make the same distinction between System A
and System B: A is significantly better than B, A is
significantly worse than B, or A and B are not sig-
nificantly different from each other. Contradictions
occur when both metrics find a significant difference
between A and B, but in opposite directions; this is
a much more serious case than a mere lack of agree-
ment (i.e., when one metric says A and B are not
significantly different, and the other metric finds a
significant difference).
Table 2 shows the average percentage agreement
between ROUGE and Pyramid/Responsiveness
when it comes to identifying significant differences
or lack thereof. Column diff shows the recall
of significant differences between pairs of systems
(i.e., how many significant differences determined
by Pyramid/Responsiveness are found by ROUGE);
column no diff gives the recall of the cases where
there are no significant differences between two sys-
tems according to Pyramid/Responsiveness.
There are a few instances of contradictions, as
well, but their numbers are fairly small. ?Auto only?
refers to comparisons between automatic summariz-
ers only; ?Human-Automatic? refers to cases when
a human summarizer is compared to an automatic
summarizer. There are fewer human summarizers,
so there are fewer ?Human-Automatic? comparisons
than ?Auto only? ones.
There are a few exceptional cases where the hu-
man summarizer is not significantly better than the
automatic summarizers, even according to the man-
ual evaluation, which accounts for the uniform val-
ues in the ?no difference? column (this is proba-
bly because the comparison is performed for much
fewer test inputs).
Table 3 combines the number of agreements in
the ?difference? and ?no difference? columns from
Table 2 into the sig column, which shows accu-
racy: in checking system pairs for significant differ-
ences, in how many cases does ROUGE make the
same decision as the manual metric (there is/isn?t
a significant difference between A and B). Ta-
ble 3 also gives the number of agreements about
any differences between systems, not only those
that reached statistical significance; in other words,
agreements on system pairwise rankings. In both
5
tables we see that removing stopwords often de-
creases performance of ROUGE, although not al-
ways. Also, there is no clear winner in the ROUGE
comparison: while ROUGE-2 with stemming is the
best at distinguishing among automatic summariz-
ers, ROUGE-1 is the most accurate when it comes
to human?automatic comparisons. To reflect this,
we adopt both ROUGE-1 and ROUGE-2 (with stem-
ming, without removing stopwords) as our reference
automatic metrics for further comparisons.
Reporting pairwise accuracy of automatic evalua-
tion measures has several advantages over reporting
correlations between manual and automatic metrics.
In correlation analysis, we cannot obtain any sense
of how accurate the measure is in identifying statis-
tically significant differences. In addition, pairwise
accuracy is more interpretable than correlations and
gives some provisional indication about how likely
it is that we are drawing a wrong conclusion when
relying on automatic metric to report results.
Table 3 tells us that when statistical significance
is not taken into account, in 89% of cases ROUGE-
2 scores will lead to the same conclusion about the
relative merits of systems as the expensive Pyramid
evaluation. In 83% of cases the conclusions will
agree with the Responsiveness evaluation. The accu-
racy of identifying significant differences is worse,
dropping by about 10% for both Pyramid and Re-
sponsiveness.
Finally, we would like to get empirical estimates
of the relationship between the size of the difference
in ROUGE-2 scores between two systems and the
agreement between manual and ROUGE-2 evalua-
tion. The goal is to check if it is the case that if
one system scores higher than another by x ROUGE
points, then it would be safe to assume that a manual
evaluation would have led to the same conclusion.
Figure 1 shows a histogram of differences in
ROUGE-2 scores. The pairs for which this differ-
ence was significant are given in red and for those
where the difference is not significant are given in
blue. The histogram clearly shows that in general,
the size of improvement cannot be used to replace a
test for significance. Even for small differences in
ROUGE score (up to 0.007) there are about 15 pairs
out of 200 for which the difference is in fact signif-
icant according to Pyramid or Responsiveness. As
the difference in ROUGE-2 scores between the two
systems increases, there are more significant differ-
ences. For differences greater than 0.05, all differ-
ences are significant.
Figure 2 shows the histograms of differences in
ROUGE-2 scores, split into cases where the pairwise
ranking of systems according to ROUGE agrees
with manual evaluation (blue) and disagrees (red).
For score differences smaller than 0.013, about half
of the times ROUGE-2 would be wrong in identify-
ing which system in the pair is the better one accord-
ing to manual evaluations. For larger differences the
number of disagreements drops sharply. For this
dataset, a difference in ROUGE-2 scores of more
than 0.04 always corresponds to an improvement in
the same direction according to the manual metrics.
5 Looking for better metrics
In the preceding sections, we established that
ROUGE-2 is the best ROUGE variant for compar-
ing two automatic systems, and ROUGE-1 is best in
distinguishing between humans and machines. Ob-
viously, it is of great interest to develop even bet-
ter automatic evaluations. In this section, we out-
line a simple procedure for deciding if a new au-
tomatic evaluation is significantly better than a ref-
erence measure. For this purpose, we consider the
automatic metrics from the TAC 2011 AESOP task,
which called for the development of better automatic
metrics for summarization evaluation NIST ( 2011).
For each automatic evaluation metric, we estimate
the probability that it agrees with Pyramid or Re-
sponsiveness. Figure 3 gives the estimated proba-
bility of agreement with Pyramid and Overall Re-
sponsiveness for all AESOP 2011 metrics with an
agreement of 0.6 or more. The metrics are plot-
ted with error bars giving the 95% confidence in-
tervals for the probability of agreement with the
manual evaluations. The red-dashed line is the
performance of the reference automatic evaluation,
which is ROUGE-2 for machine only and ROUGE-
1 for comparing machines and human summariz-
ers. Metrics whose 95% confidence interval is be-
low this line are significantly worse (as measured
by the z-test approximation of a binomial test) than
the baseline. Conversely, those whose 95% con-
fidence interval is above the red line are signifi-
cantly better than the baseline. Thus, just ROUGE-
6
Figure 1: Histogram of the differences in ROUGE-2 score versus significant differences as determined by Pyramid
(left) or Responsiveness (right).
Figure 2: Histogram of the differences in ROUGE-2 score versus differences as determined by Pyramid (left) or
Responsiveness (right).
BE (the MINIPAR variant of ROUGE-BE), one of
NIST?s baselines for AESOP, significantly outper-
formed ROUGE-2 for predicting pyramid compar-
isons; and 4 metrics: ROUGE-BE, DemokritosGR2,
catholicasc1, and CLASSY1, all significantly out-
perform ROUGE-2 for predictiong responsiveness
comparisons. Descriptions of these metrics as well
as the other proposed metrics can be found in the
TAC 2011 proceedings (NIST, 2011).
Similarly, Figure 4 gives the estimated probabil-
ity when the comparison is made between human
and machine summarizers. Here, 10 metrics are sig-
nificantly better than ROUGE-1 in predicting com-
parisons between automatic summarization systems
and human summarizers in both pyramid and re-
sponsiveness. The ROUGE-SU4 and ROUGE-BE
baselines are not shown here but their performance
was approximately 57% and 46% respectively.
If we limit the comparisons to only those where
a significant difference was measured by Pyramid
and also Overall Responsiveness, we get the plots
given in Figure 5 for comparing automatic summa-
rization systems. (The corresponding plot for com-
parisons between machines and humans is omitted
as all differences are significant.) The results show
that there are 6 metrics that are significantly better
than ROUGE-2 for correctly predicting when a sig-
nificant difference in pyramid scores occurs, and 3
metrics that are significantly better than ROUGE-2
for correctly predicting when a significant difference
in responsiveness occurs.
6 Discussion
In this paper we provided a thorough assessment
of automatic evaluation in summarization of news.
We specifically aimed to identify the best variant
of ROUGE on several years of TAC data and dis-
covered that ROUGE-2 recall with stemming and
stopwords not removed, provides the best agreement
with manual evaluations. The results shed positive
light on the automatic evaluation, as we find that
ROUGE-2 agrees with manual evaluation in almost
90% of the case when statistical significance is not
computed, and about 80% when it is. However,
these numbers are computed in a situation where
many very different systems are compared?some
7
Figure 3: Pyramid and Responsiveness Agreement of AESOP 2011 Metrics for automatic summarizers.
Figure 4: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for all summarizers.
8
Figure 5: Pyramid and Responsiveness Significant Difference Agreement of AESOP 2011 Metrics for automatic
summarizers.
very good, others bad. We examine the size of dif-
ference in ROUGE score and identify that for differ-
ences less than 0.013 a large fraction of the conclu-
sions drawn by automatic evaluation will contradict
the conclusion drawn by a manual evaluation. Fu-
ture studies should be more mindful of these find-
ings when reporting results.
Finally, we compare several alternative automatic
evaluation measures with the reference ROUGE
variants. We discover that many new proposals are
better than ROUGE in distinguishing human sum-
maries from machine summaries, but most are the
same or worse in evaluating systems. The Basic El-
ements evaluation (ROUGE-BE) appears to be the
strongest contender for an automatic evaluation to
augment or replace the current reference.
References
Paul Over and Hoa Dang and Donna Harman. 2007.
DUC in context. Inf. Process. Manage. 43(6), 1506?
1520.
Chin-Yew Lin and Eduard H. Hovy. 2003. Auto-
matic Evaluation of Summaries Using N-gram Co-
occurrence Statistics. Proceeding of HLT-NAACL.
Michel Galley. 2006. A Skip-Chain Conditional Ran-
dom Field for Ranking Meeting Utterances by Impor-
tance. Proceeding of EMNLP, 364?372.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. Trans. Audio, Speech and Lang. Proc.,
187?196.
C.Y. Lin. 2004. Looking for a Few Good Metrics: Au-
tomatic Summarization Evaluation - How Many Sam-
ples are Enough? Proceedings of the NTCIR Work-
shop 4.
Ani Nenkova and Rebecca J. Passonneau and Kathleen
McKeown. 2007. The Pyramid Method: Incorporat-
ing human content selection variation in summariza-
tion evaluation. TSLP 4(2).
Emily Pitler and Annie Louis and Ani Nenkova. 2010.
Automatic Evaluation of Linguistic Quality in Multi-
Document Summarization. Proceedings of ACL, 544?
554.
Ani Nenkova. 2005. Automatic Text Summarization of
Newswire: Lessons Learned from the Document Un-
derstanding Conference. AAAI, 1436?1441.
Ani Nenkova and Annie Louis. 2008. Can You Summa-
rize This? Identifying Correlates of Input Difficulty
for Multi-Document Summarization. ACL, 825?833.
Peter Rankel and John M. Conroy and Eric Slud and Di-
anne P. O?Leary. 2011. Ranking Human and Machine
Summarization Systems. Proceedings of EMNLP,
467?473.
National Institute of Standards and Technology.
2011. Text Analysis Workshop Proceedings
http://www.nist.gov/tac/publications/index.html.
9
