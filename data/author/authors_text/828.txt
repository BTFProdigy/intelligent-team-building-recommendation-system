Looking Under the Hood: Tools for Diagnosing Your Question
Answering Engine
Eric Breck?, Marc Light?, Gideon S. Mann?, Ellen Riloff?,
Brianne Brown?, Pranav Anand?, Mats Rooth?, Michael Thelen?
? The MITRE Corporation, 202 Burlington Rd.,Bedford, MA 01730, {ebreck,light}@mitre.org
? Department of Computer Science, Johns Hopkins University, Baltimore, MD 21218, gsm@cs.jhu.edu
? School of Computing, University of Utah, Salt Lake City, UT 84112, {riloff,thelenm}@cs.utah.edu
? Bryn Mawr College, Bryn Mawr, PA 19010, bbrown@brynmawr.edu
? Department of Mathematics, Harvard University, Cambridge, MA 02138, anand@fas.harvard.edu
? Department of Linguistics, Cornell University, Ithaca, NY 14853, mr249@cornell.edu
Abstract
In this paper we analyze two question
answering tasks : the TREC-8 ques-
tion answering task and a set of reading
comprehension exams. First, we show
that Q/A systems perform better when
there are multiple answer opportunities
per question. Next, we analyze com-
mon approaches to two subproblems:
term overlap for answer sentence iden-
tification, and answer typing for short
answer extraction. We present general
tools for analyzing the strengths and
limitations of techniques for these sub-
problems. Our results quantify the limi-
tations of both term overlap and answer
typing to distinguish between compet-
ing answer candidates.
1 Introduction
When building a system to perform a task, the
most important statistic is the performance on
an end-to-end evaluation. For the task of open-
domain question answering against text collec-
tions, there have been two large-scale end-to-
end evaluations: (TREC-8 Proceedings, 1999)
and (TREC-9 Proceedings, 2000). In addition, a
number of researchers have built systems to take
reading comprehension examinations designed to
evaluate children?s reading levels (Charniak et al,
2000; Hirschman et al, 1999; Ng et al, 2000;
Riloff and Thelen, 2000; Wang et al, 2000). The
performance statistics have been useful for deter-
mining how well techniques work.
However, raw performance statistics are not
enough. If the score is low, we need to under-
stand what went wrong and how to fix it. If the
score is high, it is important to understand why.
For example, performance may be dependent on
characteristics of the current test set and would
not carry over to a new domain. It would also be
useful to know if there is a particular character-
istic of the system that is central. If so, then the
system can be streamlined and simplified.
In this paper, we explore ways of gaining
insight into question answering system perfor-
mance. First, we analyze the impact of having
multiple answer opportunities for a question. We
found that TREC-8 Q/A systems performed bet-
ter on questions that had multiple answer oppor-
tunities in the document collection. Second, we
present a variety of graphs to visualize and ana-
lyze functions for ranking sentences. The graphs
revealed that relative score instead of absolute
score is paramount. Third, we introduce bounds
on functions that use term overlap1 to rank sen-
tences. Fourth, we compute the expected score of
a hypothetical Q/A system that correctly identifies
the answer type for a question and correctly iden-
tifies all entities of that type in answer sentences.
We found that a surprising amount of ambiguity
remains because sentences often contain multiple
entities of the same type.
1Throughout the text, we use ?overlap? to refer to the
intersection of sets of words, most often the words in the
question and the words in a sentence.
2 The data
The experiments in Sections 3, 4, and 5 were per-
formed on two question answering data sets: (1)
the TREC-8 Question Answering Track data set
and (2) the CBC reading comprehension data set.
We will briefly describe each of these data sets
and their corresponding tasks.
The task of the TREC-8 Question Answering
track was to find the answer to 198 questions us-
ing a document collection consisting of roughly
500,000 newswire documents. For each question,
systems were allowed to return a ranked list of
5 short (either 50-character or 250-character) re-
sponses. As a service to track participants, AT&T
provided top documents returned by their retrieval
engine for each of the TREC questions. Sec-
tions 4 and 5 present analyses that use all sen-
tences in the top 10 of these documents. Each
sentence is classified as correct or incorrect auto-
matically. This automatic classification judges a
sentence to be correct if it contains at least half
of the stemmed, content-words in the answer key.
We have compared this automatic evaluation to
the TREC-8 QA track assessors and found it to
agree 93-95% of the time (Breck et al, 2000).
The CBC data set was created for the Johns
Hopkins Summer 2000 Workshop on Reading
Comprehension. Texts were collected from the
Canadian Broadcasting Corporation web page for
kids (http://cbc4kids.ca/). They are an average
of 24 sentences long. The stories were adapted
from newswire texts to be appropriate for ado-
lescent children, and most fall into the follow-
ing domains: politics, health, education, science,
human interest, disaster, sports, business, crime,
war, entertainment, and environment. For each
CBC story, 8-12 questions and an answer key
were generated.2 We used a 650 question sub-
set of the data and their corresponding 75 stories.
The answer candidates for each question in this
data set were all sentences in the document. The
sentences were scored against the answer key by
the automatic method described previously.
2This work was performed by Lisa Ferro and Tim Bevins
of the MITRE Corporation. Dr. Ferro has professional expe-
rience writing questions for reading comprehension exams
and led the question writing effort.
3 Analyzing the number of answer
opportunities per question
In this section we explore the impact of multiple
answer opportunities on end-to-end system per-
formance. A question may have multiple answers
for two reasons: (1) there is more than one differ-
ent answer to the question, and (2) there may be
multiple instances of each answer. For example,
?What does the Peugeot company manufacture??
can be answered by trucks, cars, or motors and
each of these answers may occur in many sen-
tences that provide enough context to answer the
question. The table insert in Figure 1 shows that,
on average, there are 7 answer occurrences per
question in the TREC-8 collection.3 In contrast,
there are only 1.25 answer occurrences in a CBC
document. The number of answer occurrences
varies widely, as illustrated by the standard devia-
tions. The median shows an answer frequency of
3 for TREC and 1 for CBC, which perhaps gives
a more realistic sense of the degree of answer fre-
quency for most questions.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 2 3 4 5 6 7 9 1 2 1 4 1 8 2 7 2 8 6 1 6 7
# Answers
%
 Q
ue
st
io
ns
TREC-8
5 0
3 5 2
7.04
3
12.94
CBC
2 1 9
2 7 4
1.25
1
0.61
# Questions
# Answers
Mean
Median
Standard Dev.
Figure 1: Frequency of answers in the TREC-8
(black bars) and CBC (white bars) data sets
To gather this data we manually reviewed 50
randomly chosen TREC-8 questions and identi-
fied all answers to these questions in our text col-
lection. We defined an ?answer? as a text frag-
ment that contains the answer string in a context
sufficient to answer the question. Figure 1 shows
the resulting graph. The x-axis displays the num-
ber of answer occurrences found in the text col-
lection per question and the y-axis shows the per-
3We would like to thank John Burger and John Aberdeen
for help preparing Figure 1.
centage of questions that had x answers. For ex-
ample, 26% of the TREC-8 questions had only
1 answer occurrence, and 20% of the TREC-8
questions had exactly 2 answer occurrences (the
black bars). The most prolific question had 67
answer occurrences (the Peugeot example men-
tioned above). Figure 1 also shows the analysis
of 219 CBC questions. In contrast, 80% of the
CBC questions had only 1 answer occurrence in
the targeted document, and 16% had exactly 2 an-
swer occurrences.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 1 0 2 0 3 0 4 0 5 0 6 0 7 0
# answers occurences per question
%
 o
f s
ys
te
m
s 
w
ith
 a
t l
ea
st
 o
ne
 c
or
re
ct
 re
sp
on
se
Point per question
Mean correct per occurrence #
Figure 2: Answer repetition vs. system response
correctness for TREC-8
Figure 2 shows the effect that multiple answer
opportunities had on the performance of TREC-8
systems. Each solid dot in the scatter plot repre-
sents one of the 50 questions we examined.4 The
x-axis shows the number of answer opportunities
for the question, and the y-axis represents the per-
centage of systems that generated a correct an-
swer5 for the question. E.g., for the question with
67 answer occurrences, 80% of the systems pro-
duced a correct answer. In contrast, many ques-
tions had a single answer occurrence and the per-
centage of systems that got those correct varied
from about 2% to 60%.
The circles in Figure 2 represent the average
percentage of systems that answered questions
correctly for all questions with the same number
of answer occurrences. For example, on average
about 27% of the systems produced a correct an-
swer for questions that had exactly one answer oc-
4We would like to thank Lynette Hirschman for suggest-
ing the analysis behind Figure 2 and John Burger for help
with the analysis and presentation.
5For this analysis, we say that a system generated a cor-
rect answer if a correct answer was in its response set.
currence, but about 50% of the systems produced
a correct answer for questions with 7 answer op-
portunities. Overall, a clear pattern emerges: the
performance of TREC-8 systems was strongly
correlated with the number of answer opportuni-
ties present in the document collection.
4 Graphs for analyzing scoring
functions of answer candidates
Most question answering systems generate sev-
eral answer candidates and rank them by defin-
ing a scoring function that maps answer candi-
dates to a range of numbers. In this section,
we analyze one particular scoring function: term
overlap between the question and answer can-
didate. The techniques we use can be easily
applied to other scoring functions as well (e.g.,
weighted term overlap, partial unification of sen-
tence parses, weighted abduction score, etc.). The
answer candidates we consider are the sentences
from the documents.
The expected performance of a system that
ranks all sentences using term overlap is 35% for
the TREC-8 data. This number is an expected
score because of ties: correct and incorrect can-
didates may have the same term overlap score. If
ties are broken optimally, the best possible score
(maximum) would be 54%. If ties are broken
maximally suboptimally, the worst possible score
(minimum) would be 24%. The corresponding
scores on the CBC data are 58% expected, 69%
maximum, and 51% minimum. We would like to
understand why the term overlap scoring function
works as well as it does and what can be done to
improve it.
Figures 3 and 4 compare correct candidates and
incorrect candidates with respect to the scoring
function. The x-axis plots the range of the scor-
ing function, i.e., the amount of overlap. The
y-axis represents Pr(overlap=x | correct) and
Pr(overlap=x | incorrect), where separate curves
are plotted for correct and incorrect candidates.
The probabilities are generated by normalizing
the number of correct/incorrect answer candidates
with a particular overlap score by the total number
of correct/incorrect candidates, respectively.
Figure 3 illustrates that the correct candidates
for TREC-8 have term overlap scores distributed
between 0 and 10 with a peak of 24% at an over-
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0 2 4 6 8 10 12 14 16 18 20N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
overlap
incorrect
correct
Figure 3: Pr(overlap=x|[in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 5 10 15 20 25 30No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
overlap
incorrect
correct
Figure 4: Pr(overlap=x|[in]correct) for CBC
lap of 2. However, the incorrect candidates have
a similar distribution between 0 and 8 with a peak
of 32% at an overlap of 0. The similarity of the
curves illustrates that it is unclear how to use the
score to decide if a candidate is correct or not.
Certainly no static threshold above which a can-
didate is deemed correct will work. Yet the ex-
pected score of our TREC term overlap system
was 35%, which is much higher than a random
baseline which would get an expected score of
less than 3% because there are over 40 sentences
on average in newswire documents.6
After inspecting some of the data directly, we
posited that it was not the absolute term overlap
that was important for judging candidate but how
the overlap score compares to the scores of other
candidates. To visualize this, we generated new
graphs by plotting the rank of a candidate?s score
6We also tried dividing the term overlap score by the
length of the question to normalize for query length but did
not find that the graph was any more helpful.
on the x-axis. For example, the candidate with
the highest score would be ranked first, the can-
didate with the second highest score would be
ranked second, etc. Figures 5 and 6 show these
graphs, which display Pr(rank=x | correct) and
Pr(rank=x | incorrect) on the y-axis. The top-
ranked candidate has rank=0.
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
-
10
00
-
90
0
-
80
0
-
70
0
-
60
0
-
50
0
-
40
0
-
30
0
-
20
0
-
10
0 0
N
or
m
al
iz
ed
 (+
/30
87
,-/5
70
73
) C
ou
nt
ranked overlap
incorrect
correct
Figure 5: Pr(rank=x | [in]correct) for TREC-8
0
0.05
0.1
0.15
0.2
0.25
0.3
-45 -40 -35 -30 -25 -20 -15 -10 -5 0No
rm
al
iz
ed
 (+
/13
11
,-/1
46
10
) C
ou
nt
ranked overlap
incorrect
correct
Figure 6: Pr(rank=x | [in]correct) for CBC
The ranked graphs are more revealing than the
graphs of absolute scores: the probability of a
high rank is greater for correct answers than in-
correct ones. Now we can begin to understand
why the term overlap scoring function worked as
well as it did. We see that, unlike classification
tasks, there is no good threshold for our scor-
ing function. Instead relative score is paramount.
Systems such as (Ng et al, 2000) make explicit
use of relative rank in their algorithms and now
we understand why this is effective.
Before we leave the topic of graphing scoring
functions, we want to introduce one other view of
the data. Figure 7 plots term overlap scores on
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
0 2 4 6 8 10 12 14
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
20000
lo
g-
od
ds
 o
f c
or
re
ct
ne
ss
m
a
ss
overlap
log-odds
mass curve
Figure 7: TREC-8 log odds correct given overlap
the x-axis and the log odds of being correct given
a score on the y-axis. The log odds formula is:
log Pr(correct|overlap)Pr(incorrect|overlap)
Intuitively, this graph shows how much more
likely a sentence is to be correct versus incorrect
given a particular score. A second curve, labeled
?mass,? plots the number of answer candidates
with each score. Figure 7 shows that the odds of
being correct are negative until an overlap of 10,
but the mass curve reveals that few answer candi-
dates have an overlap score greater than 6.
5 Bounds on scoring functions that use
term overlap
The scoring function used in the previous sec-
tion simply counts the number of terms shared
by a question and a sentence. One obvious mod-
ification is to weight some terms more heavily
than others. We tried using inverse document fre-
quence based (IDF) term weighting on the CBC
data but found that it did not improve perfor-
mance. The graph analogous to Figure 6 but with
IDF term weighting was virtually identical.
Could another weighting scheme perform bet-
ter? How well could an optimal weighting
scheme do? How poorly would the maximally
suboptimal scheme do? The analysis in this sec-
tion addresses these questions. In essence the an-
swer is the following: the question and the can-
didate answers are typically short and thus the
number of overlapping terms is small ? conse-
quently, many candidate answers have exactly the
same overlapping terms and no weighting scheme
could differentiate them. In addition, subset rela-
tions often hold between overlaps. A candidate
whose overlap is a subset of a second candidate
cannot score higher regardless of the weighting
scheme.7 We formalize these overlap set relations
and then calculate statistics based on them for the
CBC and TREC data.
Question: How much was Babe Belanger paid to play
amateur basketball?
S1: She was a member of the winningest
basketball team Canada ever had.
S2: Babe Belanger never made a cent for her
skills.
S3: They were just a group of young women
from the same school who liked to
play amateur basketball.
S4: Babe Belanger played with the Grads from
1929 to 1937.
S5: Babe never talked about her fabulous career.
MaxOsets : ( {S2, S4}, {S3} )
Figure 8: Example of Overlap Sets from CBC
Figure 8 presents an example from the CBC
data. The four overlap sets are (i) Babe Belanger,
(ii) basketball, (iii) play amateur basketball, and
(iv) Babe. In any term-weighting scheme with
positive weights, a sentence containing the words
Babe Belanger will have a higher score than sen-
tences containing just Babe, and sentences with
play amateur basketball will have a higher score
than those with just basketball. However, we can-
not generalize with respect to the relative scores
of sentences containing Babe Belanger and those
containing play amateur basketball because some
terms may have higher weights than others.
The most we can say is that the highest scor-
ing candidate must be a member of {S2, S4} or
{S3}. S5 and S1 cannot be ranked highest be-
cause their overlap sets are a proper subset of
competing overlap sets. The correct answer is
S2 so an optimal weighting scheme would have
a 50% chance of ranking S2 first, assuming that
it identified the correct overlap set {S2, S4} and
then randomly chose between S2 and S4. A max-
imally suboptimal weighting scheme could rank
S2 no lower than third.
We will formalize these concepts using the fol-
lowing variables:
7Assuming that all term weights are positive.
q: a question (a set of words)
s: a sentence (a set of words)
w,v: sets of intersecting words
We define an overlap set (ow,q) to be a set of
sentences (answer candidates) that have the same
words overlapping with the question. We define a
maximal overlap set (Mq) as an overlap set that is
not a subset of any other overlap set for the ques-
tion. For simplicity, we will refer to a maximal
overlap set as a MaxOset.
ow,q = {s|s ? q = w}
?q = all unique overlap sets for q
maximal(ow,q) if ?ov,q ? ?q, w 6? v
Mq = {ow,q ? ?q | maximal(ow,q)}
Cq = {s|s correctly answers q}
We can use these definitions to give upper
and lower bounds on the performance of term-
weighting functions on our two data sets. Table 1
shows the results. The max statistic is the per-
centage of questions for which at least one mem-
ber of its MaxOsets is correct. The min statis-
tic is the percentage of questions for which all
candidates of all of its MaxOsets are correct (i.e.,
there is no way to pick a wrong answer). Finally
the expectedmax is a slightly more realistic up-
per bound. It is equivalent to randomly choosing
among members of the ?best? maximal overlap
set, i.e., the MaxOset that has the highest percent-
age of correct members. Formally, the statistics
for a set of questions Q are computed as:
max = |{q|?o ? Mq,?s ? o s.t. s ? Cq}||Q|
min = |{q|?o ? Mq,?s ? o s ? Cq}||Q|
exp. max = 1|Q| ?
?
q?Q
max
o?Mq
|{s ? o and s ? Cq}|
|o|
The results for the TREC data are considerably
lower than the results for the CBC data. One ex-
planation may be that in the CBC data, only sen-
tences from one document containing the answer
are considered. In the TREC data, as in the TREC
task, it is not known beforehand which docu-
ments contain answers, so irrelevant documents
exp. max max min
CBC training 72.7% 79.0% 24.4%
TREC-8 48.8% 64.7% 10.1%
Table 1: Maximum overlap analysis of scores
may contain high-scoring sentences that distract
from the correct sentences.
In Table 2, we present a detailed breakdown
of the MaxOset results for the CBC data. (Note
that the classifications overlap, e.g., questions that
are in ?there is always a chance to get it right?
are also in the class ?there may be a chance to
get it right.?) 21% of the questions are literally
impossible to get right using only term weight-
ing because none of the correct sentences are in
the MaxOsets. This result illustrates that maxi-
mal overlap sets can identify the limitations of a
scoring function by recognizing that some candi-
dates will always be ranked higher than others.
Although our analysis only considered term over-
lap as a scoring function, maximal overlap sets
could be used to evaluate other scoring functions
as well, for example overlap sets based on seman-
tic classes rather than lexical items.
In sum, the upper bound for term weighting
schemes is quite low and the lower bound is
quite high. These results suggest that methods
such as query expansion are essential to increase
the feature sets used to score answer candidates.
Richer feature sets could distinguish candidates
that would otherwise be represented by the same
features and therefore would inevitably receive
the same score.
6 Analyzing the effect of multiple
answer type occurrences in a sentence
In this section, we analyze the problem of extract-
ing short answers from a sentence. Many Q/A
systems first decide what answer type a question
expects and then identify instances of that type in
sentences. A scoring function ranks the possible
answers using additional criteria, which may in-
clude features of the surrounding sentence such
as term overlap with the question.
For our analysis, we will assume that two short
answers that have the same answer type and come
from the same sentence are indistinguishable to
the system. This assumption is made by many
number of percentage
questions of questions
Impossible to get it wrong 159 24%
(?ow ? Mq, ?s ? ow, s ? Cq)
There is always a chance to get it right 45 7%
(?ow ? Mq, ?s ? ow s.t. s ? Cq)
There may be a chance to get it right 310 48%
(?ow ? Mq s.t. ?s ? ow s.t. s ? Cq)
The wrong answers will always be weighted too highly 137 21%
(?ow ? Mq, ?s ? ow, s 6? Cq)
There are no correct answers with any overlap with Q 66 10%
(?s ? d, s is incorrect or s has 0 overlap)
There are no correct answers (auto scoring error) 12 2%
(?s ? d, s is incorrect)
Table 2: Maximal Overlap Set Analysis for CBC data
Q/A systems: they do not have features that can
prefer one entity over another of the same type in
the same sentence.
We manually annotated data for 165 TREC-
9 questions and 186 CBC questions to indicate
perfect question typing, perfect answer sentence
identification, and perfect semantic tagging. Us-
ing these annotations, we measured how much
?answer confusion? remains if an oracle gives you
the correct question type, a sentence containing
the answer, and correctly tags all entities in the
sentence that match the question type. For exam-
ple, the oracle tells you that the question expects
a person, gives you a sentence containing the cor-
rect person, and tags all person entities in that sen-
tence. The one thing the oracle does not tell you
is which person is the correct one.
Table 3 shows the answer types that we used.
Most of the types are fairly standard, except for
the Defaultnp and Defaultvp which are default
tags for questions that desire a noun phrase or
verb phrase but cannot be more precisely typed.
We computed an expected score for this hy-
pothetical system as follows: for each question,
we divided the number of correct candidates (usu-
ally one) by the total number of candidates of the
same answer type in the sentence. For example,
if a question expects a Location as an answer and
the sentence contains three locations, then the ex-
pected accuracy of the system would be 1/3 be-
cause the system must choose among the loca-
tions randomly. When multiple sentences contain
a correct answer, we aggregated the sentences. Fi-
nally, we averaged this expected accuracy across
all questions for each answer type.
TREC CBC
Answer Type Score Freq Score Freq
defaultnp .33 47 .25 28
organization .50 1 .72 3
length .50 1 .75 2
thingname .58 14 .50 1
quantity .58 13 .77 14
agent .63 19 .40 23
location .70 24 .68 29
personname .72 11 .83 13
city .73 3 n/a 0
defaultvp .75 2 .42 15
temporal .78 16 .75 26
personnoun .79 7 .53 5
duration 1.0 3 .67 4
province 1.0 2 1.0 2
area 1.0 1 n/a 0
day 1.0 1 n/a 0
title n/a 0 .50 1
person n/a 0 .67 3
money n/a 0 .88 8
ambigbig n/a 0 .88 4
age n/a 0 1.0 2
comparison n/a 0 1.0 1
mass n/a 0 1.0 1
measure n/a 0 1.0 1
Overall .59 165 .61 186
Overall-dflts .69 116 .70 143
Table 3: Expected scores and frequencies for each
answer type
Table 3 shows that a system with perfect ques-
tion typing, perfect answer sentence identifica-
tion, and perfect semantic tagging would still
achieve only 59% accuracy on the TREC-9 data.
These results reveal that there are often multi-
ple candidates of the same type in a sentence.
For example, Temporal questions received an ex-
pected score of 78% because there was usually
only one date expression per sentence (the correct
one), while Default NP questions yielded an ex-
pected score of 25% because there were four noun
phrases per question on average. Some common
types were particularly problematic. Agent ques-
tions (most Who questions) had an answer con-
fusability of 0.63, while Quantity questions had a
confusability of 0.58.
The CBC data showed a similar level of an-
swer confusion, with an expected score of 61%,
although the confusability of individual answer
types varied from TREC. For example, Agent
questions were even more difficult, receiving a
score of 40%, but Quantity questions were easier
receiving a score of 77%.
Perhaps a better question analyzer could assign
more specific types to the Default NP and De-
fault VP questions, which skew the results. The
Overall-dflts row of Table 3 shows the expected
scores without these types, which is still about
70% so a great deal of answer confusion remains
even without those questions. The confusability
analysis provides insight into the limitations of
the answer type set, and may be useful for com-
paring the effectiveness of different answer type
sets (somewhat analogous to the use of grammar
perplexity in speech research).
Q1: What city is Massachusetts General Hospital located
in?
A1: It was conducted by a cooperative group of on-
cologists from Hoag, Massachusetts General Hospital
in Boston, Dartmouth College in New Hampshire, UC
San Diego Medical Center, McGill University in Montreal
and the University of Missouri in Columbia.
Q2: When was Nostradamus born?
A2: Mosley said followers of Nostradamus, who lived
from 1503 to 1566, have claimed ...
Figure 9: Sentences with Multiple Items of the
Same Type
However, Figure 9 shows the fundamental
problem behind answer confusability. Many sen-
tences contain multiple instances of the same
type, such as lists and ranges. In Q1, recognizing
that the question expects a city rather than a gen-
eral location is still not enough because several
cities are in the answer sentence. To achieve bet-
ter performance, Q/A systems need use features
that can more precisely target an answer.
7 Conclusion
In this paper we have presented four analyses of
question answering system performance involv-
ing: multiple answer occurence, relative score for
candidate ranking, bounds on term overlap perfor-
mance, and limitations of answer typing for short
answer extraction. We hope that both the results
and the tools we describe will be useful to others.
In general, we feel that analysis of good perfor-
mance is nearly as important as the performance
itself and that the analysis of bad performance can
be equally important.
References
E.J. Breck, J.D. Burger, L. Ferro, L. Hirschman, D. House,
M. Light, and I. Mani. 2000. How to Evaluate your
Question Answering System Every Day and Still Get
Real Work Done. In Proceedings of the Second Con-
ference on Language Resources and Evaluation (LREC-
2000).
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett, M. Kos-
mala, T. Moscovich, L. Pang, C. Pyo, Y. Sun, W. Wy,
Z. Yang, S. Zeller, and L. Zorn. 2000. Reading Compre-
hension Programs in a Statistical-Language-Processing
Class. In ANLP/NAACL Workshop on Reading Com-
prehension Tests as Evaluation for Computer-Based Lan-
guage Understanding Systems.
L. Hirschman, M. Light, E. Breck, and J. Burger. 1999.
Deep Read: A Reading Comprehension System. In Pro-
ceedings of the 37th Annual Meeting of the Association
for Computational Linguistics.
H.T. Ng, L.H. Teo, and J.L.P. Kwan. 2000. A Machine
Learning Approach to Answering Questions for Reading
Comprehension Tests. In Proceedings of EMNLP/VLC-
2000 at ACL-2000.
E. Riloff and M. Thelen. 2000. A Rule-based Question
Answering System for Reading Comprehension Tests.
In ANLP/NAACL Workshop on Reading Comprehension
Tests as Evaluation for Computer-Based Language Un-
derstanding Systems.
TREC-8 Proceedings. 1999. Proceedings of the Eighth
Text Retrieval Conference (TREC8). National Institute of
Standards and Technology, Special Publication 500-246,
Gaithersburg, MD.
TREC-9 Proceedings. 2000. Proceedings of the Ninth Text
Retrieval Conference (forthcoming). National Institute
of Standards and Technology, Special Publication 500-
XXX, Gaithersburg, MD.
W. Wang, Auer J., R. Parasuraman, I. Zubarev, D. Brandy-
berry, and M.P. Harper. 2000. A Question Answering
System Developed as a Project in a Natural Language
Processing Course. In ANLP/NAACL Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
Playing the Telephone Game: Determining the Hierarchical Structure of
Perspective and Speech Expressions
Eric Breck and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
USA
ebreck,cardie@cs.cornell.edu
Abstract
News articles report on facts, events, and opin-
ions with the intent of conveying the truth.
However, the facts, events, and opinions appear-
ing in the text are often known only second-
or third-hand, and as any child who has played
?telephone? knows, this relaying of facts often
garbles the original message. Properly under-
standing the information filtering structures that
govern the interpretation of these facts, then, is
critical to appropriately analyzing them. In this
work, we present a learning approach that cor-
rectly determines the hierarchical structure of
information filtering expressions 78.30% of the
time.
1 Introduction
Newswire text has long been a primary target for
natural language processing (NLP) techniques such
as information extraction, summarization, and ques-
tion answering (e.g. MUC (1998); NIS (2003);
DUC (2003)). However, newswire does not offer
direct access to facts, events, and opinions; rather,
journalists report what they have experienced, and
report on the experiences of others. That is, facts,
events, and opinions are filtered by the point of
view of the writer and other sources. Unfortu-
nately, this filtering of information through multiple
sources (and multiple points of view) complicates
the natural language interpretation process because
the reader (human or machine) must take into ac-
count the biases introduced by this indirection. It
is important for understanding both newswire and
narrative text (Wiebe, 1994), therefore, to appropri-
ately recognize expressions of point of view, and to
associate them with their direct and indirect sources.
This paper introduces two kinds of expression
that can filter information. First, we define a per-
spective expression to be the minimal span of text
that denotes the presence of an explicit opinion,
evaluation, emotion, speculation, belief, sentiment,
etc.1 Private state is the general term typically used
1Note that implicit expressions of perspective, i.e. Wiebe et
to refer to these mental and emotional states that
cannot be directly observed or verified (Quirk et al,
1985). Further, we define the source of a perspec-
tive expression to be the experiencer of that private
state, that is, the person or entity whose opinion
or emotion is being conveyed in the text. Second,
speech expressions simply convey the words of an-
other individual ? and by the choice of words, the
reporter filters the original source?s intent. Consider
for example, the following sentences (in which per-
spective expressions are denoted in bold, speech ex-
pressions are underlined, and sources are denoted in
italics):
1. Charlie was angry at Alice?s claim that Bob was
unhappy.
2. Philip Clapp, president of the National Environ-
ment Trust, sums up well the general thrust of the
reaction of environmental movements: ?There is no
reason at all to believe that the polluters are sud-
denly going to become reasonable.?
Perspective expressions in Sentence 1 describe the
emotions or opinion of three sources: Charlie?s
anger, Bob?s unhappiness, and Alice?s belief. Per-
spective expressions in Sentence 2, on the other
hand, introduce the explicit opinion of one source,
i.e. the reaction of the environmental movements.
Speech expressions also perform filtering in these
examples. The reaction of the environmental move-
ments is filtered by Clapp?s summarization, which,
in turn, is filtered by the writer?s choice of quotation.
In addition, the fact that Bob was unhappy is filtered
through Alice?s claim, which, in turn, is filtered by
the writer?s choice of words for the sentence. Sim-
ilarly, it is only according to the writer that Charlie
is angry.
The specific goal of the research described here
is to accurately identify the hierarchical structure of
perspective and speech expressions (pse?s) in text.2
al.?s (2003) ?expressive subjective elements? are not the subject
of study here.
2For the rest of this paper, then, we ignore the distinction
between perspective and speech expressions, so in future ex-
Given sentences 1 and 2 and their pse?s, for exam-
ple, we will present methods that produce the struc-
tures shown in Figure 1, which represent the multi-
stage information filtering that should be taken into
account in the interpretation of the text.
Sentence 1:
writer?s implicit speech event
claim
unhappy
angry
Sentence 2:
writer?s implicit speech event
sums up
reaction
Figure 1: Hierarchical structure of the perspective and
speech expressions in sentences 1 and 2
We propose a supervised machine learning ap-
proach to the problem that relies on a small set
of syntactically-based features. More specifically,
the method first trains a binary classifier to make
pairwise parent-child decisions among the pse?s in
the same sentence, and then combines the deci-
sions to determine their global hierarchical struc-
ture. We compare the approach to two heuristic-
based baselines ? one that simply assumes that ev-
ery pse is filtered only through the writer, and a
second that is based on syntactic dominance rela-
tions in the associated parse tree. In an evaluation
using the opinion-annotated NRRC corpus (Wiebe
et al, 2002), the learning-based approach achieves
an accuracy of 78.30%, significantly higher than
both the simple baseline approach (65.57%) and the
parse-based baseline (71.64%). We believe that this
study provides a first step towards understanding the
multi-stage filtering process that can bias and garble
the information present in newswire text.
The rest of the paper is organized as follows. We
present related work in Section 2 and describe the
machine learning approach in Section 3. The ex-
perimental methodology and results are presented
in Sections 4 and 5, respectively. Section 6 summa-
rizes our conclusions and plans for future work.
2 The Larger Problem and Related Work
This paper addresses the problem of identifying the
hierarchical structure of perspective and speech ex-
pressions. We view this as a necessary and im-
portant component of a larger perspective-analysis
amples, both types of pse appear in boldface. Note that the
acronym ?pse? has been used previously with a different mean-
ing (Wiebe, 1994).
pse class count
writer 9808
verb 7623
noun 2293
no parse 278
adjective 197
adverb 50
other 370
Table 1: Breakdown of classes of pse?s. ?writer? de-
notes pse?s with the writer as source. ?No parse? denotes
pse?s in sentences where the parse failed, and so the part
of speech could not be determined.
number of pse?s number of sentences
1 3612
2 3256
3 1810
4 778
5 239
>5 113
Table 2: Breakdown of number of pse?s per sentence
system. Such a system would be able to identify
all pse?s in a document, as well as identify their
structure. The system would also identify the direct
source of each pse. Finally, the system would iden-
tify the text corresponding to the content of a private
state or the speech expressed by a pse.3 Such a sys-
tem might analyze sentence 2 as follows:
(source: writer
pse: (implicit speech event)
content: Philip ... reasonable.?)
(source: clapp
pse: sums up
content: ?There ... reasonable.?)
(source: environmental movements
pse: reaction
content: (no text))
As far as we are aware, no single system ex-
ists that simultaneously solves all these problems.
There is, however, quite a bit of work that addresses
various pieces of this larger task, which we will now
survey.
Gerard (2000) proposes a computational model
of the reader of a news article. Her model provides
for multiple levels of hierarchical beliefs, such as
the nesting of a primary source?s belief within that
of a reporter. However, Gerard does not provide al-
gorithms for extracting this structure directly from
newswire texts.
Bethard et al (2004) seek to extract propositional
3In (Wiebe, 2002), this is referred to as the inside.
opinions and their holders. They define an opinion
as ?a sentence, or part of a sentence that would an-
swer the question ?How does X feel about Y?? ? A
propositional opinion is an opinion ?localized in the
propositional argument? of certain verbs, such as
?believe? or ?realize?. Their task then corresponds
to identifying a pse, its associated direct source, and
the content of the private state. However, they con-
sider as pse?s only verbs, and further restrict atten-
tion to verbs with a propositional argument, which
is a subset of the perspective and speech expressions
that we consider here. Table 1, for example, shows
the diversity of word classes that correspond to pse?s
in our corpus. Perhaps more importantly for the
purposes of this paper, their work does not address
information filtering issues, i.e. problems that arise
when an opinion has been filtered through multiple
sources. Namely, Bethard et al (2004) do not con-
sider sentences that contain multiple pse?s, and do
not, therefore, need to identify any indirect sources
of opinions. As shown in Table 2, however, we
find that sentences with multiple non-writer pse?s
(i.e. sentences that contain 3 or more total pse?s)
comprise a significant portion (29.98%) of our cor-
pus. An advantage over our work, however, is that
Bethard et al (2004) do not require separate solu-
tions to pse identification and the identification of
their direct sources.
Automatic identification of sources has also
been addressed indirectly by Gildea and Jurafsky?s
(2002) work on semantic role identification in that
finding sources often corresponds to finding the
filler of the agent role for verbs. Their methods then
might be used to identify sources and associate them
with pse?s that are verbs or portions of verb phrases.
Whether their work will also apply to pse?s that are
realized as other parts of speech is an open question.
Wiebe (1994), studies methods to track the
change of ?point of view? in narrative text (fiction).
That is, the ?writer? of one sentence may not corre-
spond to the writer of the next sentence. Although
this is not as frequent in newswire text as in fiction,
it will still need to be addressed in a solution to the
larger problem.
Bergler (1993) examines the lexical semantics of
speech event verbs in the context of generative lex-
icon theory. While not specifically addressing our
problem, the ?semantic dimensions? of reporting
verbs that she extracts might be very useful as fea-
tures in our approach.
Finally, Wiebe et al (2003) present preliminary
results for the automatic identification of perspec-
tive and speech expressions using corpus-based
techniques. While the results are promising (66% F-
was
Charlie angry
at
claim
?s
Alice
that
was
Bob unhappy
Figure 2: Dependency parse of sentence 1 according to
the Collins parser.
measure), the problem is still clearly unsolved. As
explained below, we will instead rely on manually
tagged pse?s for the studies presented here.
3 The Approach
Our task is to find the hierarchical structure among
the pse?s in individual sentences. One?s first im-
pression might be that this structure should be ob-
vious from the syntax: one pse should filter an-
other roughly when it dominates the other in a de-
pendency parse. This heuristic, for example, would
succeed for ?claim? and ?unhappy? in sentence 1,
whose pse structure is given in Figure 1 and parse
structure (as produced by the Collins parser) in Fig-
ure 2. 4
Even in sentence 1, though, we can see that
the problem is more complex: ?angry? dominates
?claim? in the parse tree, but does not filter it. Un-
fortunately, an analysis of the parse-based heuristic
on our training data (the data set will be described
in Section 4), uncovered numerous, rather than just
a few, sources of error. Therefore, rather than trying
to handcraft a more complex collection of heuris-
tics, we chose to adopt a supervised machine learn-
ing approach that relies on features identified in this
analysis. In particular, we will first train a binary
classifier to make pairwise decisions as to whether
a given pse is the immediate parent of another. We
then use a simple approach to combine these de-
cisions to find the hierarchical information-filtering
structure of all pse?s in a sentence.
We assume that we have a training corpus of
4For this heuristic and the features that follow, we will speak
of the pse?s as if they had a position in the parse tree. However,
since pse?s are often multiple words, and do not necessarily
form a constituent, this is not entirely accurate. The parse node
corresponding to a pse will be the highest node in the depen-
dency parse corresponding to a word in the pse. We consider
the writer?s implicit pse to correspond to the root of the parse.
sentences, annotated with pse?s and their hier-
archical pse structure (Section 4 describes the
corpus). Training instances for the binary clas-
sifier are pairs of pse?s from the same sentence,
?psetarget, pseparent?5. We assign a class value
of 1 to a training instance if pseparent is the
immediate parent of psetarget in the manually
annotated hierarchical structure for the sentence,
and 0 otherwise. For sentence 1, there are nine
training instances generated: ?claim,writer?,
?angry,writer?, ?unhappy, claim? (class 1),
?claim, angry?, ?claim, unhappy?, ?angry, claim?,
?angry, unhappy?, ?unhappy,writer?,
?unhappy, angry? (class 0). The features used
to describe each training instance are explained
below.
During testing, we construct the hierarchical pse
structure of an entire sentence as follows. For each
pse in the sentence, ask the binary classifier to judge
each other pse as a potential parent, and choose the
pse with the highest confidence6. Finally, join these
immediate-parent links to form a tree.7
One might also try comparing pairs of potential
parents for a given pse, or other more direct means
of ranking potential parents. We chose what seemed
to be the simplest method for this first attempt at the
problem.
3.1 Features
Here we motivate and describe the 23 features used
in our model. Unless otherwise stated, all features
are binary (1 if the described condition is true, 0
otherwise).
Parse-based features (6). Based on the perfor-
mance of the parse-based heuristic, we include a
pseparent-dominates-psetarget feature in our feature
set. To compensate for parse errors, however, we
also include a variant of this that is 1 if the parent of
pseparent dominates psetarget.
Many filtering expressions filter pse?s that occur
in their complements, but not in adjuncts. There-
fore, we add variants of the previous two syntax-
based features that denote whether the parent node
5We skip sentences where there is no decision to make (sen-
tences with zero or one non-writer pse). Since the writer pse is
the root of every structure, we do not generate instances with
the writer pse in the psetarget position.
6There is an ambiguity if the classifier assigns the same con-
fidence to two potential parents. For evaluation purposes, we
consider the classifier?s response incorrect if any of the highest-
scoring potential parents are incorrect.
7The directed graph resulting from flawed automatic pre-
dictions might not be a tree (i.e. it might be cyclic and discon-
nected). Since this occurs very rarely (5 out of 9808 sentences
on the test data), we do not attempt to correct any non-tree
graphs.
dominates psetarget, but only if the first dependency
relation is an object relation.
For similar reasons, we include a feature calculat-
ing the domination relation based on a partial parse.
Consider the following sentence:
3. He was criticized more than recognized for his
policy.
One of ?criticized? or ?recognized? will be the root
of this dependency parse, thus dominating the other,
and suggesting (incorrectly) that it filters the other
pse. Because a partial parse does not attach all con-
stituents, such spurious dominations are eliminated.
The partial parse feature is 1 for fewer instances
than pseparent-dominates-psetarget , but it is more
indicative of a positive instance when it is 1.
So that the model can adjust when the parse is
not present, we include a feature that is 1 for all
instances generated from sentences on which the
parser failed.
Positional features (5). Forcing the model to de-
cide whether pseparent is the parent of psetarget
without knowledge of the other pse?s in the sen-
tence is somewhat artificial. We therefore include
several features that encode the relative position of
pseparent and psetarget in the sentence. Specifi-
cally, we add a feature that is 1 if pseparent is the
root of the parse (and similarly for psetarget ). We
also include a feature giving the ordinal position of
pseparent among the pse?s in the sentence, relative
to psetarget (-1 means pseparent is the pse that im-
mediately precedes psetarget, 1 means immediately
following, and so forth). To allow the model to vary
when there are more potential parents to choose
from, we include a feature giving the total number
of pse?s in the sentence.
Special parents and lexical features (6). Some
particular pse?s are special, so we specify indicator
features for four types of parents: the writer pse,
and the lexical items ?said? (the most common non-
writer pse) and ?according to?. ?According to? is
special because it is generally not very high in the
parse, but semantically tends to filter everything else
in the sentence.
In addition, we include as features the part of
speech of pseparent and psetarget (reduced to noun,
verb, adjective, adverb, or other), since intuitively
we expected distinct parts of speech to behave dif-
ferently in their filtering.
Genre-specific features (6). Finally, journalistic
writing contains a few special forms that are not al-
ways parsed accurately. Examples are:
4. ?Alice disagrees with me,? Bob argued.
5. Charlie, she noted, dislikes Chinese food.
The parser may not recognize that ?noted? and
?argued? should dominate all other pse?s in sen-
tences 4 and 5, so we attempt to recognize when
a sentence falls into one of these two patterns.
For ?disagrees, argued? generated from sentence 4,
features pseparent-pattern-1 and psetarget-pattern-
1 would be 1, while for ?dislikes, noted? generated
from sentence 5, feature pseparent-pattern-2 would
be 1. We also add features that denote whether the
pse in question falls between matching quote marks.
Finally, a simple feature indicates whether pseparent
is the last word in the sentence.
3.2 Resources
We rely on a variety of resources to generate our fea-
tures. The corpus (see Section 4) is distributed with
annotations for sentence breaks, tokenization, and
part of speech information automatically generated
by the GATE toolkit (Cunningham et al, 2002).8
For parsing we use the Collins (1999) parser.9 For
partial parses, we employ CASS (Abney, 1997). Fi-
nally, we use a simple finite-state recognizer to iden-
tify (possibly nested) quoted phrases.
For classifier construction, we use the IND pack-
age (Buntine, 1993) to train decision trees (we use
the mml tree style, a minimum message length cri-
terion with Bayesian smoothing).
4 Data Description
The data for these experiments come from version
1.1 of the NRRC corpus (Wiebe et al, 2002).10. The
corpus consists of 535 newswire documents (mostly
from the FBIS), of which we used 66 (1375 sen-
tences) for developing the heuristics and features,
while keeping the remaining 469 (9808 sentences)
blind (used for 10-fold cross-validation).
Although the NRRC corpus provides annotations
for all pse?s, it does not provide annotations to de-
note directly their hierarchical structure within a
8GATE?s sentences sometimes extend across paragraph
boundaries, which seems never to be warranted. Inaccurately
joining sentences has the effect of adding more noise to our
problem, so we split GATE?s sentences at paragraph bound-
aries, and introduce writer pse?s for the newly created sen-
tences.
9We convert the parse to a dependency format that makes
some of our features simpler using a method similar to the one
described in Xia and Palmer (2001). We also employ a method
from Adam Lopez at the University of Maryland to find gram-
matical relationships between words (subject, object, etc.).
10The original corpus is available at http:
//nrrc.mitre.org/NRRC/Docs_Data/MPQA_
04/approval_mpqa.htm. Code and data used in our
experiments are available at http://www.cs.cornell.
edu/?ebreck/breck04playing/.
sentence. This structure must be extracted from
an attribute of each pse annotation, which lists the
pse?s direct and indirect sources. For example, the
?source chain? for ?unhappy? in sentence 1, would
be (writer, Alice, Bob). The source chains allow
us to automatically recover the hierarchical struc-
ture of the pse?s: the parent of a pse with source
chain (s0, s1, . . . sn?1, sn) is the pse with source
chain (s0, s1, . . . sn?1). Unfortunately, ambiguities
can arise. Consider the following sentence:
6. Bob said, ?you?re welcome? because he was glad
to see that Mary was happy.
Both ?said? and ?was glad? have the source chain
(writer, Bob),11 while ?was happy? has the source
chain (writer, Bob, Mary). It is therefore not clear
from the manual annotations whether ?was happy?
should have ?was glad? or ?said? as its parent.
5.82% of the pse?s have ambiguous parentage (i.e.
the recovery step finds a set of parents P (pse) with
|P (pse)| > 1). For training, we assign a class value
of 1 to all instances ?pse, par?, par ? P (pse). For
testing, if an algorithm attaches pse to any element
of P (pse), we score the link as correct (see Sec-
tion 5.1). Since ultimately our goal is to find the
sources through which information is filtered (rather
than the pse?s), we believe this is justified.
For training and testing, we used only those sen-
tences that contain at least two non-writer pse?s12
? for all other sentences, there is only one way to
construct the hierarchical structure. Again, Table 2
presents a breakdown (for the test set) of the num-
ber of pse?s per sentence ? thus we only use approx-
imately one-third of all the sentences in the corpus.
5 Results and Discussion
5.1 Evaluation
How do we evaluate the performance of an au-
tomatic method of determining the hierarchical
structure of pse?s? Lin (1995) proposes a method
for evaluating dependency parses: the score for
a sentence is the fraction of correct parent links
identified; the score for the corpus is the aver-
age sentence score. Formally, the score for a
11The annotators also performed coreference resolution on
sources.
12Under certain circumstances, such as paragraph-long
quotes, the writer of a sentence will not be the same as the
writer of a document. In such sentences, the NRRC corpus con-
tains additional pse?s for any other sources besides the writer of
the document. Since we are concerned in this work only with
one sentence at a time, we discard all such implicit pse?s be-
sides the writer of the sentence. Also, in a few cases, more than
one pse in a sentence was marked as having the writer as its
source. We believe this to be an error and so discarded all but
one writer pse.
metric size heurOne heurTwo decTree
Lin 2940 65.57% 71.64% 78.30%
perf 2940 36.02% 45.37% 54.52%
bin 21933 73.20% 77.73% 82.12%
bin + 7882 60.63% 66.94% 70.35%
bin ? 14051 80.24% 83.78% 88.72%
Table 3: Performance on test data. ?Lin? is Lin?s depen-
dency score, ?perf? is the fraction of sentences whose
structure was identified perfectly, and ?bin? is the perfor-
mance of the binary classifier (broken down for positive
and negative instances). ?Size? is the number of sen-
tences or pse pairs.
# pse?s # sents heurOne heurTwo decTree
3 1810 70.88% 75.41% 81.82%
4 778 59.17% 67.82% 74.38%
5 239 53.87% 61.92% 68.93%
>5 113 49.31% 58.03% 68.68%
Table 4: Performance by number of pse?s per sentence
method evaluated on the entire corpus (?Lin?) is
?
s?S
|{pse|pse?Non writer pse?s(s)?parent(pse)=autopar(pse))}|
|Non writer pse?s(s)|
|S| ,
where S is the set of all sentences in the corpus,
Non writer pse ?s(s) is the set of non-writer pse?s
in sentence s, parent(pse) is the correct parent
of pse, and autopar(pse) is the automatically
identified parent of pse.
We also present results using two other (related)
metrics. The ?perf? metric measures the fraction
of sentences whose structure is determined entirely
correctly (i.e. ?perf?ectly). ?Bin? is the accuracy of
the binary classifier (with a 0.5 threshold) on the in-
stances created from the test corpus. We also report
the performance on positive and negative instances.
5.2 Results
We compare the learning-based approach (decTree)
to the heuristic-based approaches introduced in Sec-
tion 3 ? heurOne assumes that all pse?s are at-
tached to the writer?s implicit pse; heurTwo is the
parse-based heuristic that relies solely on the domi-
nance relation13.
We use 10-fold cross-validation on the evalua-
tion data to generate training and test data (although
the heuristics, of course, do not require training).
The results of the decision tree method and the two
heuristics are presented in Table 3.
13That is, heurTwo attaches a pse to the pse most immedi-
ately dominating it in the dependency tree. If no other pse
dominates it, a pse is attached to the writer?s pse.
5.3 Discussion
Encouragingly, our machine learning method uni-
formly and significantly14 outperforms the two
heuristic methods, on all metrics and in sentences
with any number of pse?s. The difference is most
striking in the ?perf? metric, which is perhaps
the most intuitive. Also, the syntax-based heuris-
tic (heurTwo) significantly15 outperforms heurOne,
confirming our intuitions that syntax is important in
this task.
As the binary classifer sees many more negative
instances than positive, it is unsurprising that its per-
formance is much better on negative instances. This
suggests that we might benefit from machine learn-
ing methods for dealing with unbalanced datasets.
Examining the errors of the machine learning sys-
tem on the development set, we see that for half
of the pse?s with erroneously identified parents, the
parent is either the writer?s pse, or a pse like ?said?
in sentences 4 and 5 having scope over the entire
sentence. For example,
7. ?Our concern is whether persons used to the role
of policy implementors can objectively assess and
critique executive policies which impinge on hu-
man rights,? said Ramdas.
Our model chose the parent of ?assess and critique?
to be ?said? rather than ?concern.? We also see from
Table 4 that the model performs more poorly on sen-
tences with more pse?s. We believe that this reflects
a weakness in our decision to combine binary deci-
sions, because the model has learned that in general,
a ?said? or writer?s pse (near the root of the struc-
ture) is likely to be the parent, while it sees many
fewer examples of pse?s such as ?concern? that lie
in the middle of the tree.
Although we have ignored the distinction
throughout this paper, error analysis suggests
speech event pse?s behave differently than private
state pse?s with respect to how closely syntax re-
flects their hierarchical structure. It may behoove
us to add features to allow the model to take this
into account. Other sources of error include er-
roneous sentence boundary detection, parenthetical
statements (which the parser does not treat correctly
for our purposes) and other parse errors, partial quo-
tations, as well as some errors in the annotation.
Examining the learned trees is difficult because
of their size, but looking at one tree to depth three
14p < 0.01, using an approximate randomization test with
9,999 trials. See (Eisner, 1996, page 17) and (Chinchor et al,
1993, pages 430-433) for descriptions of this method.
15Using the same test as above, p < 0.01, except for the
performance on sentences with more than 5 pse?s, because of
the small amount of data, where p < 0.02.
reveals a fairly intuitive model. Ignoring the prob-
abilities, the tree decides pseparent is the parent
of psetarget if and only if pseparent is the writer?s
pse (and psetarget is not in quotation marks), or
if pseparent is the word ?said.? For all the trees
learned, the root feature was either the writer pse
test or the partial-parse-based domination feature.
6 Conclusions and Future Work
We have presented the concept of perspective and
speech expressions, and argued that determining
their hierarchical structure is important for natural
language understanding of perspective. We have
shown that identifying the hierarchical structure of
pse?s is amenable to automated analysis via a ma-
chine learning approach, although there is room for
improvement in the results.
In the future, we plan to address the related tasks
discussed in Section 2, especially identifying pse?s
and their immediate sources. We are also interested
in ways of improving the machine learning formu-
lation of the current task, such as optimizing the
binary classifier on the whole-sentence evaluation,
or defining a different binary task that is easier to
learn. Nevertheless, we believe that our results pro-
vide a step towards the development of natural lan-
guage systems that can extract and summarize the
viewpoints and perspectives expressed in text while
taking into account the multi-stage information fil-
tering process that can mislead more na??ve systems.
Acknowledgments
This work was supported in part by NSF Grant IIS-
0208028 and by an NSF Graduate Research Fellowship.
We thank Rebecca Hwa for creating the dependency
parses. We also thank the Cornell NLP group for help-
ful suggestions on drafts of this paper. Finally, we thank
Janyce Wiebe and Theresa Wilson for draft suggestions
and advice regarding this problem and the NRRC corpus.
References
Steven Abney. 1997. The SCOL manual. cass is avail-
able from http://www.vinartus.net/spa/scol1h.tar.gz.
Sabine Bergler. 1993. Semantic dimensions in the field
of reporting verbs. In Proceedings of the Ninth An-
nual Conference of the University of Waterloo Centre
for the New Oxford English Dictionary and Text Re-
search, Oxford, England, September.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic
extraction of opinion propositions and their holders.
In Working Notes of the AAAI Spring Symposium on
Exploring Attitude and Affect in Text: Theories and
Applications. March 22-24, 2004, Stanford.
Wray Buntine. 1993. Learning classification trees. In
D. J. Hand, editor, Artificial Intelligence frontiers in
statistics, pages 182?201. Chapman & Hall,London.
Available at http://ic.arc.nasa.gov/projects/bayes-
group/ind/IND-program.html.
Nancy Chinchor, Lynette Hirschman, and David Lewis.
1993. Evaluating message understanding systems:
An analysis of the third message understanding
conference (MUC-3). Computational Linguistics,
19(3):409?450.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Hamish Cunningham, Diana Maynard, Kalina Bont-
cheva, and Valentin Tablan. 2002. GATE: A frame-
work and graphical development environment for ro-
bust nlp tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for Com-
putational Linguistics (ACL ?02), Philadelphia, July.
2003. Proceedings of the Workshop on Text Summariza-
tion, Edmonton, Alberta, Canada, May. Presented at
the 2003 Human Language Technology Conference.
Jason Eisner. 1996. An empirical comparison of proba-
bility models for dependency grammar. Technical Re-
port IRCS-96-11, IRCS, University of Pennsylvania.
Christine Gerard. 2000. Modelling readers of news ar-
ticles using nested beliefs. Master?s thesis, Concordia
University, Montre?al, Que?bec, Canada.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In IJCAI, pages
1420?1427.
1998. Proceedings of the Seventh Message Understand-
ing Conference (MUC-7). Morgan Kaufman, April.
NIST. 2003. Proceedings of The Twelfth Text REtrieval
Conference (TREC 2003), Gaithersburg, MD, Novem-
ber. NIST special publication SP 500-255.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Grammar
of the English Language. Longman, New York.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, and T. Wil-
son. 2002. NRRC Summer Workshop on Multiple-
Perspective Question Answering Final Report. Tech
report, NRRC, Bedford, MA.
J. Wiebe, E. Breck, C. Buckley, C. Cardie, P. Davis,
B. Fraser, D. Litman, D. Pierce, E. Riloff, T. Wilson,
D. Day, and M. Maybury. 2003. Recognizing and Or-
ganizing Opinions Expressed in the World Press. In
Papers from the AAAI Spring Symposium on New Di-
rections in Question Answering (AAAI tech report SS-
03-07). March 24-26, 2003. Stanford.
Janyce Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233?287.
Janyce Wiebe. 2002. Instructions for annotating opin-
ions in newspaper articles. Technical Report TR-02-
101, Dept. of Comp. Sci., University of Pittsburgh.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Proc. of the
HLT Conference.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 431?439,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Joint Extraction of Entities and Relations for Opinion Recognition
Yejin Choi and Eric Breck and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,ebreck,cardie}@cs.cornell.edu
Abstract
We present an approach for the joint ex-
traction of entities and relations in the con-
text of opinion recognition and analysis.
We identify two types of opinion-related
entities ? expressions of opinions and
sources of opinions ? along with the link-
ing relation that exists between them. In-
spired by Roth and Yih (2004), we employ
an integer linear programming approach
to solve the joint opinion recognition task,
and show that global, constraint-based in-
ference can significantly boost the perfor-
mance of both relation extraction and the
extraction of opinion-related entities. Per-
formance further improves when a seman-
tic role labeling system is incorporated.
The resulting system achieves F-measures
of 79 and 69 for entity and relation extrac-
tion, respectively, improving substantially
over prior results in the area.
1 Introduction
Information extraction tasks such as recognizing
entities and relations have long been considered
critical to many domain-specific NLP tasks (e.g.
Mooney and Bunescu (2005), Prager et al (2000),
White et al (2001)). Researchers have further
shown that opinion-oriented information extrac-
tion can provide analogous benefits to a variety of
practical applications including product reputation
tracking (Morinaga et al, 2002), opinion-oriented
question answering (Stoyanov et al, 2005), and
opinion-oriented summarization (e.g. Cardie et
al. (2004), Liu et al (2005)). Moreover, much
progress has been made in the area of opinion ex-
traction: it is possible to identify sources of opin-
ions (i.e. the opinion holders) (e.g. Choi et al
(2005) and Kim and Hovy (2005b)), to determine
the polarity and strength of opinion expressions
(e.g. Wilson et al (2005)), and to recognize propo-
sitional opinions and their sources (e.g. Bethard
et al (2004)) with reasonable accuracy. To date,
however, there has been no effort to simultane-
ously identify arbitrary opinion expressions, their
sources, and the relations between them. Without
progress on the joint extraction of opinion enti-
ties and their relations, the capabilities of opinion-
based applications will remain limited.
Fortunately, research in machine learning has
produced methods for global inference and joint
classification that can help to address this defi-
ciency (e.g. Bunescu and Mooney (2004), Roth
and Yih (2004)). Moreover, it has been shown that
exploiting dependencies among entities and/or re-
lations via global inference not only solves the
joint extraction task, but often boosts performance
on the individual tasks when compared to clas-
sifiers that handle the tasks independently ? for
semantic role labeling (e.g. Punyakanok et al
(2004)), information extraction (e.g. Roth and Yih
(2004)), and sequence tagging (e.g. Sutton et al
(2004)).
In this paper, we present a global inference ap-
proach (Roth and Yih, 2004) to the extraction
of opinion-related entities and relations. In par-
ticular, we aim to identify two types of entities
(i.e. spans of text): entities that express opin-
ions and entities that denote sources of opinions.
More specifically, we use the term opinion expres-
sion to denote all direct expressions of subjectiv-
ity including opinions, emotions, beliefs, senti-
ment, etc., as well as all speech expressions that
introduce subjective propositions; and use the term
source to denote the person or entity (e.g. a re-
431
port) that holds the opinion.1 In addition, we
aim to identify the relations between opinion ex-
pression entities and source entities. That is, for
a given opinion expression Oi and source entity
Sj , we determine whether the relation Li,j def=
(Sj expresses Oi) obtains, i.e. whether Sj is the
source of opinion expression Oi. We refer to this
particular relation as the link relation in the rest
of the paper. Consider, for example, the following
sentences:
S1. [Bush](1) intends(1) to curb the increase in
harmful gas emissions and is counting on(1)
the good will(2) of [US industrialists](2) .
S2. By questioning(3) [the Imam](4)?s edict(4) [the
Islamic Republic of Iran](3) made [the people
of the world](5) understand(5)...
The underlined phrases above are opinion expres-
sions and phrases marked with square brackets are
source entities. The numeric superscripts on en-
tities indicate link relations: a source entity and
an opinion expression with the same number sat-
isfy the link relation. For instance, the source en-
tity ?Bush? and the opinion expression ?intends?
satisfy the link relation, and so do ?Bush? and
?counting on.? Notice that a sentence may con-
tain more than one link relation, and link relations
are not one-to-one mappings between sources and
opinions. Also, the pair of entities in a link rela-
tion may not be the closest entities to each other, as
is the case in the second sentence, between ?ques-
tioning? and ?the Islamic Republic of Iran.?
We expect the extraction of opinion relations to
be critical for many opinion-oriented NLP appli-
cations. For instance, consider the following ques-
tion that might be given to a question-answering
system:
? What is the Imam?s opinion toward the Islamic
Republic of Iran?
Without in-depth opinion analysis, the question-
answering system might mistake example S2 as
relevant to the query, even though S2 exhibits the
opinion of the Islamic Republic of Iran toward
Imam, not the other way around.
Inspired by Roth and Yih (2004), we model
our task as global, constraint-based inference over
separately trained entity and relation classifiers.
In particular, we develop three base classifiers:
two sequence-tagging classifiers for the extraction
1See Wiebe et al (2005) for additional details.
of opinion expressions and sources, and a binary
classifier to identify the link relation. The global
inference procedure is implemented via integer
linear programming (ILP) to produce an optimal
and coherent extraction of entities and relations.
Because many (60%) opinion-source relations
appear as predicate-argument relations, where the
predicate is a verb, we also hypothesize that se-
mantic role labeling (SRL) will be very useful for
our task. We present two baseline methods for
the joint opinion-source recognition task that use
a state-of-the-art SRL system (Punyakanok et al,
2005), and describe two additional methods for in-
corporating SRL into our ILP-based system.
Our experiments show that the global inference
approach not only improves relation extraction
over the base classifier, but does the same for in-
dividual entity extractions. For source extraction
in particular, our system achieves an F-measure of
78.1, significantly outperforming previous results
in this area (Choi et al, 2005), which obtained an
F-measure of 69.4 on the same corpus. In addition,
we achieve an F-measure of 68.9 for link relation
identification and 82.0 for opinion expression ex-
traction; for the latter task, our system achieves
human-level performance.2
2 High-Level Approach and Related
Work
Our system operates in three phases.
Opinion and Source Entity Extraction We
begin by developing two separate token-level
sequence-tagging classifiers for opinion expres-
sion extraction and source extraction, using linear-
chain Conditional Random Fields (CRFs) (Laf-
ferty et al, 2001). The sequence-tagging classi-
fiers are trained using only local syntactic and lex-
ical information to extract each type of entity with-
out knowledge of any nearby or neighboring enti-
ties or relations. We collect n-best sequences from
each sequence tagger in order to boost the recall of
the final system.
Link Relation Classification We also develop
a relation classifier that is trained and tested on
all pairs of opinion and source entities extracted
from the aforementioned n-best opinion expres-
sion and source sequences. The relation classifier
is modeled using Markov order-0 CRFs(Lafferty
2Wiebe et al (2005) reports human annotation agreement
for opinion expression as 82.0 by F1 measure.
432
et al, 2001), which are equivalent to maximum en-
tropy models. It is trained using only local syntac-
tic information potentially useful for connecting a
pair of entities, but has no knowledge of nearby or
neighboring extracted entities and link relations.
Integer Linear Programming Finally, we for-
mulate an integer linear programming problem for
each sentence using the results from the previous
two phases. In particular, we specify a number
of soft and hard constraints among relations and
entities that take into account the confidence val-
ues provided by the supporting entity and relation
classifiers, and that encode a number of heuristics
to ensure coherent output. Given these constraints,
global inference via ILP finds the optimal, coher-
ent set of opinion-source pairs by exploiting mu-
tual dependencies among the entities and relations.
While good performance in entity or relation
extraction can contribute to better performance of
the final system, this is not always the case. Pun-
yakanok et al (2004) notes that, in general, it is
better to have high recall from the classifiers in-
cluded in the ILP formulation. For this reason, it is
not our goal to directly optimize the performance
of our opinion and source entity extraction models
or our relation classifier.
The rest of the paper is organized as follows.
Related work is outlined below. Section 3 de-
scribes the components of the first phase of our
system, the opinion and source extraction classi-
fiers. Section 4 describes the construction of the
link relation classifier for phase two. Section 5
describes the ILP formulation to perform global
inference over the results from the previous two
phases. Experimental results that compare our ILP
approach to a number of baselines are presented in
Section 6. Section 7 describes how SRL can be in-
corporated into our global inference system to fur-
ther improve the performance. Final experimental
results and discussion comprise Section 8.
Related Work The definition of our source-
expresses-opinion task is similar to that of Bethard
et al (2004); however, our definition of opin-
ion and source entities are much more extensive,
going beyond single sentences and propositional
opinion expressions. In particular, we evaluate
our approach with respect to (1) a wide variety
of opinion expressions, (2) explicit and implicit3
sources, (3) multiple opinion-source link relations
3Implicit sources are those that are not explicitly men-
tioned. See Section 8 for more details.
per sentence, and (4) link relations that span more
than one sentence. In addition, the link rela-
tion model explicitly exploits mutual dependen-
cies among entities and relations, while Bethard
et al (2004) does not directly capture the potential
influence among entities.
Kim and Hovy (2005b) and Choi et al (2005)
focus only on the extraction of sources of
opinions, without extracting opinion expressions.
Specifically, Kim and Hovy (2005b) assume a pri-
ori existence of the opinion expressions and ex-
tract a single source for each, while Choi et al
(2005) do not explicitly extract opinion expres-
sions nor link an opinion expression to a source
even though their model implicitly learns approxi-
mations of opinion expressions in order to identify
opinion sources. Other previous research focuses
only on the extraction of opinion expressions (e.g.
Kim and Hovy (2005a), Munson et al (2005) and
Wilson et al (2005)), omitting source identifica-
tion altogether.
There have also been previous efforts to si-
multaneously extract entities and relations by ex-
ploiting their mutual dependencies. Roth and
Yih (2002) formulated global inference using a
Bayesian network, where they captured the influ-
ence between a relation and a pair of entities via
the conditional probability of a relation, given a
pair of entities. This approach however, could not
exploit dependencies between relations. Roth and
Yih (2004) later formulated global inference using
integer linear programming, which is the approach
that we apply here. In contrast to our work, Roth
and Yih (2004) operated in the domain of factual
information extraction rather than opinion extrac-
tion, and assumed that the exact boundaries of en-
tities from the gold standard are known a priori,
which may not be available in practice.
3 Extraction of Opinion and Source
Entities
We develop two separate sequence tagging classi-
fiers for opinion extraction and source extraction,
using linear-chain Conditional Random Fields
(CRFs) (Lafferty et al, 2001). The sequence tag-
ging is encoded as the typical ?BIO? scheme.4
Each training or test instance represents a sen-
tence, encoded as a linear chain of tokens and their
4
?B? is for the token that begins an entity, ?I? is for to-
kens that are inside an entity, and ?O? is for tokens outside an
entity.
433
associated features. Our feature set is based on
that of Choi et al (2005) for source extraction5,
but we include additional lexical and WordNet-
based features. For simplicity, we use the same
features for opinion entity extraction and source
extraction, and let the CRFs learn appropriate fea-
ture weights for each task.
3.1 Entity extraction features
For each token xi, we include the following fea-
tures. For details, see Choi et al (2005).
word: words in a [-4, +4] window centered on xi.
part-of-speech: POS tags in a [-2, +2] window.6
grammatical role: grammatical role (subject, ob-
ject, prepositional phrase types) of xi derived from
a dependency parse.7
dictionary: whether xi is in the opinion expres-
sion dictionary culled from the training data and
augmented by approximately 500 opinion words
from the MPQA Final Report8. Also computed
for tokens in a [-1, +1] window and for xi?s parent
?chunk? in the dependency parse.
semantic class: xi?s semantic class.9
WordNet: the WordNet hypernym of xi.10
4 Relation Classification
We also develop a maximum entropy binary clas-
sifier for opinion-source link relation classifica-
tion. Given an opinion-source pair, Oi-Sj , the re-
lation classifier decides whether the pair exhibits
a valid link relation, Li,j . The relation classifier
focuses only on the syntactic structure and lexical
properties between the two entities of a given pair,
without knowing whether the proposed entities are
correct. Opinion and source entities are taken from
the n-best sequences of the entity extraction mod-
els; therefore, some are invariably incorrect.
From each sentence, we create training and test
instances for all possible opinion-source pairings
that do not overlap: we create an instance for Li,j
only if the span of Oi and Sj do not overlap.
For training, we also filter out instances for
which neither the proposed opinion nor source en-
5We omit only the extraction pattern features.
6Using GATE: http://gate.ac.uk/
7Provided by Rebecca Hwa, based on the Collins parser:
ftp://ftp.cis.upenn.edu/pub/mcollins/PARSER.tar.gz
8https://rrc.mitre.org/pubs/mpqaFinalReport.pdf
9Using SUNDANCE: (http://www.cs.utah.edu/r?iloff/
publications.html#sundance)
10http://wordnet.princeton.edu/
tity overlaps with a correct opinion or source en-
tity per the gold standard. This training instance
filtering helps to avoid confusion between exam-
ples like the following (where entities marked in
bold are the gold standard entities, and entities
in square brackets represent the n-best output se-
quences from the entity extraction classifiers):
(1) [The president] s1 walked away from [the
meeting] o1, [ [revealing] o2 his disap-
pointment] o3 with the deal.
(2) [The monster] s2 walked away, [revealing] o4
a little box hidden underneath.
For these sentences, we construct training in-
stances for L1,1, L1,2, and L1,3, but not L2,4,
which in fact has very similar sentential structure
as L1,2, and hence could confuse the learning al-
gorithm.
4.1 Relation extraction features
The training and test instances for each (potential)
link Li,j (with opinion candidate entity Oi and
source candidate entity Sj) include the following
features.
opinion entity word: the words contained in Oi.
phrase type: the syntactic category of the con-
stituent in which the entity is embedded, e.g. NP
or VP. We encode separate features for Oi and Sj .
grammatical role: the grammatical role of the
constituent in which the entity is embedded.
Grammatical roles are derived from dependency
parse trees, as done for the entity extraction classi-
fiers. We encode separate features for Oi and Sj .
position: a boolean value indicating whether Sj
precedes Oi.
distance: the distance between Oi and Sj in num-
bers of tokens. We use four coarse categories: ad-
jacent, very near, near, far.
dependency path: the path through the depen-
dency tree from the head of Sj to the head of Oi.
For instance, ?subj?verb? or ?subj?verb?obj?.
voice: whether the voice of Oi is passive or active.
syntactic frame: key intra-sentential relations be-
tween Oi and Sj . The syntactic frames that we use
are:
? [E1:role] [distance] [E2:role], where distance
? {adjacent, very near, near, far}, and Ei:role
is the grammatical role of Ei. Either E1 is an
opinion entity and E2 is a source, or vice versa.
? [E1:phrase] [distance] [E2:phrase], where
Ei:phrase is the phrasal type of entity Ei.
434
? [E1:phrase] [E2:headword], where E2 must be
the opinion entity, and E1 must be the source en-
tity (i.e. no lexicalized frames for sources). E1
and E2 can be contiguous.
? [E1:role] [E2:headword], where E2 must be the
opinion entity, and E1 must be the source entity.
? [E1:phrase] NP [E2:phrase] indicates the
presence of specific syntactic patterns, e.g.
?VP NP VP? depending on the possible phrase
types of opinion and source entities. The three
phrases do not need to be contiguous.
? [E1:phrase] VP [E2:phrase] (See above.)
? [E1:phrase] [wh-word] [E2:phrase] (See
above.)
? Src [distance] [x] [distance] Op, where x ?
{by, of, from, for, between, among, and, have,
be, will, not, ], ?, . . . }.
When a syntactic frame is matched to a sen-
tence, the bracketed items should be instantiated
with particular values corresponding to the sen-
tence. Pattern elements without square brackets
are constants. For instance, the syntactic frame
?[E1:phrase] NP [E2:phrase]? may be instantiated
as ?VP NP VP?. Some frames are lexicalized with
respect to the head of an opinion entity to reflect
the fact that different verbs expect source enti-
ties in different argument positions (e.g. SOURCE
blamed TARGET vs. TARGET angered SOURCE).
5 Integer Linear Programming
Approach
As noted in the introduction, we model our task
as global, constraint-based inference over the sep-
arately trained entity and relation classifiers, and
implement the inference procedure as binary in-
teger linear programming (ILP) ((Roth and Yih,
2004), (Punyakanok et al, 2004)). ILP consists
of an objective function which is a dot product
between a vector of variables and a vector of
weights, and a set of equality and inequality con-
straints among variables. Given an objective func-
tion and a set of constraints, LP finds the opti-
mal assignment of values to variables, i.e. one that
minimizes the objective function. In binary ILP,
the assignments to variables must be either 0 or 1.
The variables and constraints defined for the opin-
ion recognition task are summarized in Table 1 and
explained below.
Entity variables and weights For each opinion
entity, we add two variables, Oi and O?i, where
Oi = 1 means to extract the opinion entity, and
Objective function f
=
?
i(woiOi) +
?
i(w?oiO?i)
+
?
j(wsjSj) +
?
j(w?sj S?j)
+
?
i,j(wli,jLi,j) +
?
i(w?li,j L?i,j)
?i, Oi + O?i = 1
?j, Sj + S?j = 1
?i, j, Li,j + L?i,j = 1
?i, Oi =
?
j Li,j
?j, Sj + Aj =
?
i Li,j
?j, Aj ? Sj ? 0
?i, j, i < j, Xi + Xj = 1,X ? {S,O}
Table 1: Binary ILP formulation
O?i = 1 means to discard the opinion entity. To
ensure coherent assignments, we add equality con-
straints ?i, Oi + O?i = 1. The weights woi and
w?oi for Oi and O?i respectively, are computed as
a negative conditional probability of the span of
an entity to be extracted (or suppressed) given the
labelings of the adjacent variables of the CRFs:
woi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xk = ?B?
& xm = ?I? for m ? [k + 1, l]
w?oi
def= ?P (xk, xk+1, ..., xl|xk?1, xl+1)
where xm = ?O? for m ? [k, l]
where xi is the value assigned to the random vari-
able of the CRF corresponding to an entity Oi.
Likewise, for each source entity, we add two vari-
ables Sj and S?j and a constraint Sj + S?j = 1. The
weights for source variables are computed in the
same way as opinion entities.
Relation variables and weights For each link
relation, we add two variables Li,j and L?i,j , and
a constraint Li,j + L?i,j = 1. By the definition of
a link, if Li,j = 1, then it is implied that Oi = 1
and Sj = 1. That is, if a link is extracted, then the
pair of entities for the link must be also extracted.
Constraints to ensure this coherency are explained
in the following subsection. The weights for link
variables are based on probabilities from the bi-
nary link classifier.
Constraints for link coherency In our corpus, a
source entity can be linked to more than one opin-
ion entity, but an opinion entity is linked to only
435
one source. Nonetheless, the majority of opinion-
source pairs involve one-to-one mappings, which
we encode as hard and soft constraints as follows:
For each opinion entity, we add an equality con-
straint Oi =
?
j Li,j to enforce that only one
link can emanate from an opinion entity. For each
source entity, we add an equality constraint and an
inequality constraint that together allow a source
to link to at most two opinions: Sj +Aj =
?
i Li,j
and Aj ? Sj ? 0, where Aj is an auxiliary vari-
able, such that its weight is some positive constant
value that suppresses Aj from being assigned to 1.
And Aj can be assigned to 1 only if Sj is already
assigned to 1. It is possible to add more auxiliary
variables to allow more than two opinions to link
to a source, but for our experiments two seemed to
be a reasonable limit.
Constraints for entity coherency When we use
n-best sequences where n > 1, proposed entities
can overlap. Because this should not be the case
in the final result, we add an equality constraint
Xi + Xj = 1, X ? {S,O} for all pairs of entities
with overlapping spans.
Adjustments to weights To balance the preci-
sion and recall, and to take into account the per-
formance of different base classifiers, we apply ad-
justments to weights as follows.
1) We define six coefficients cx and c?x, where
x ? {O,S,L} to modify a group of weights
as follows.
?i, x, wxi := wxi ? cx;
?i, x, w?xi := w?xi ? c?x;
In general, increasing cx will promote recall,
while increasing c?x will promote precision.
Also, setting co > cs will put higher confi-
dence on the opinion extraction classifier than
the source extraction classifier.
2) We also define one constant cA to set the
weights for auxiliary variable Ai. That is,
?i, wAi := cA.
3) Finally, we adjust the confidence of the link
variable based on n-th-best sequences of the en-
tity extraction classifiers as follows.
?i, wLi,j := wLi,j ? d
where d def= 4/(3 + min(m,n)), when Oi is
from an m-th sequence and Sj is from a n-th
sequence.11
11This will smoothly degrade the confidence of a link
based on the entities from higher n-th sequences. Values of d
decrease as 4/4, 4/5, 4/6, 4/7....
6 Experiments?I
We evaluate our system using the NRRC Multi-
Perspective Question Answering (MPQA) corpus
that contains 535 newswire articles that are man-
ually annotated for opinion-related information.
In particular, our gold standard opinion entities
correspond to direct subjective expression anno-
tations and subjective speech event annotations
(i.e. speech events that introduce opinions) in the
MPQA corpus (Wiebe et al, 2005). Gold stan-
dard source entities and link relations can be ex-
tracted from the agent attribute associated with
each opinion entity. We use 135 documents as a
development set and report 10-fold cross valida-
tion results on the remaining 400 documents in all
experiments below.
We evaluate entity and link extraction using
both an overlap and exact matching scheme.12 Be-
cause the exact start and endpoints of the man-
ual annotations are somewhat arbitrary, the over-
lap scheme is more reasonable for our task (Wiebe
et al, 2005). We report results according to both
matching schemes, but focus our discussion on re-
sults obtained using overlap matching.13
We use the Mallet14 implementation of CRFs.
For brevity, we will refer to the opinion extraction
classifier as CRF-OP, the source extraction classi-
fier as CRF-SRC, and the link relation classifier as
CRF-LINK. For ILP, we use Matlab, which pro-
duced the optimal assignment in a matter of few
seconds for each sentence. The weight adjustment
constants defined for ILP are based on the devel-
opment data.15
The link-nearest baselines For baselines, we
first consider a link-nearest heuristic: for each
opinion entity extracted by CRF-OP, the link-
nearest heuristic creates a link relation with the
closest source entity extracted by CRF-SRC. Re-
call that CRF-SRC and CRF-OP extract entities
from n-best sequences. We test the link-nearest
heuristic with n = {1, 2, 10} where larger n will
boost recall at the cost of precision. Results for the
12Given two links L1,1 = (O1, S1) and L2,2 = (O2, S2),
exact matching requires the spans of O1 and O2, and the
spans of S1 and S2, to match exactly, while overlap matching
requires the spans to overlap.
13Wiebe et al (2005) also reports the human annotation
agreement study via the overlap scheme.
14Available at http://mallet.cs.umass.edu
15co = 2.5, c?o = 1.0, cs = 1.5, c?s = 1.0, cL = 2.5, c?L =
2.5, cA = 0.2. Values are picked so as to boost recall while
reasonably suppressing incorrect links.
436
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
NEAREST-1 51.6 71.4 59.9 26.2 36.9 30.7
NEAREST-2 60.7 45.8 52.2 29.7 19.0 23.1
NEAREST-10 66.3 20.9 31.7 28.2 00.0 00.0
SRL 59.7 36.3 45.2 32.6 19.3 24.2
SRL+CRF-OP 45.6 83.2 58.9 27.6 49.7 35.5
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
Table 2: Relation extraction performance
NEAREST-n : link-nearest heuristic w/ n-best
SRL : all V-A0 frames from SRL
SRL+CRF-OP : all V-A0 filtered by CRF-OP
ILP-n : ILP applied to n-best sequences
link-nearest heuristic on the full source-expresses-
opinion relation extraction task are shown in the
first three rows of table 2. NEAREST-1 performs
the best in overlap-match F-measure, reaching
59.9. NEAREST-10 has higher recall (66.3%), but
the precision is really low (20.9%). Performance
of the opinion and source entity classifiers will be
discussed in Section 8.
SRL baselines Next, we consider two base-
lines that use a state-of-the-art SRL system (Pun-
yakanok et al, 2005). In many link relations,
the opinion expression entity is a verb phrase and
the source entity is in an agent argument posi-
tion. Hence our second baseline, SRL, extracts
all verb(V)-agent(A0) frames from the output of
the SRL system and provides an upper bound on
recall (59.7%) for systems that use SRL in isola-
tion for our task. A more sophisticated baseline,
SRL+CRF-OP, extracts only those V-A0 frames
whose verb overlaps with entities extracted by the
opinion expression extractor, CRF-OP. As shown
in table 2, filtering out V-A0 frames that are in-
compatible with the opinion extractor boosts pre-
cision to 83.2%, but the F-measure (58.9) is lower
than that of NEAREST-1.
ILP results The ILP-n system in table 2 de-
notes the results of the ILP approach applied to the
n-best sequences. ILP-10 reaches an F-measure
of 68.0, a significant improvement over the high-
est performing baseline16 , and also a substantial
improvement over ILP-1. Note that the perfor-
mance of NEAREST-10 was much worse than that
16Statistically significant by paired-t test, where p <
0.001.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
ILP-1 51.6 80.8 63.0 26.4 42.0 32.4
ILP-10 64.0 72.4 68.0 31.0 34.8 32.8
ILP+SRL-f -1 51.7 81.5 63.3 26.6 42.5 32.7
ILP+SRL-f -10 65.7 72.4 68.9 31.5 34.3 32.9
ILP+SRL-fc-10 64.0 73.5 68.4 28.4 31.3 29.8
Table 3: Relation extraction with ILP and SRL
ILP-n : ILP applied to n-best sequences
ILP+SRL-f -n : ILP w/ SRL features, n-best
ILP+SRL-fc-n : ILP w/ SRL features,
and SRL constraints, n-best
of NEAREST-1, because the 10-best sequences in-
clude many incorrect entities whereas the corre-
sponding ILP formulation can discard the bad en-
tities by considering dependencies among entities
and relations.17
7 Additional SRL Incorporation
We next explore two approaches for more directly
incorporating SRL into our system.
Extra SRL Features for the Link classifier We
incorporate SRL into the link classifier by adding
extra features based on SRL. We add boolean fea-
tures to check whether the span of an SRL argu-
ment and an entity matches exactly. In addition,
we include syntactic frame features as follows:
? [E1:srl-arg] [E2:srl-arg], where Ei:srl-arg indi-
cates the SRL argument type of entity Ei.
? [E1.srl-arg] [E1:headword] [E2:srl-arg], where
E1 must be an opinion entity, and E2 must be a
source entity.
Extra SRL Constraints for the ILP phase We
also incorporate SRL into the ILP phase of our
system by adding extra constraints based on SRL.
In particular, we assign very high weights for links
that match V-A0 frames generated by SRL, in or-
der to force the extraction of V-A0 frames.
17A potential issue with overlap precision and recall is that
the measures may drastically overestimate the system?s per-
formance as follows: a system predicting a single link rela-
tion whose source and opinion expression both overlap with
every token of a document would achieve 100% overlap pre-
cision and recall. We can ensure this does not happen by mea-
suring the average number of (source, opinion) pairs to which
each correct or predicted pair is aligned (excluding pairs not
aligned at all). In our data, this does not exceed 1.08, (except
for baselines), so we can conclude these evaluation measures
are behaving reasonably.
437
Opinion Source Link
r(%) p(%) f(%) r(%) p(%) f(%) r(%) p(%) f(%)
Before ILP CRF-OP/SRC/LINK with 1 best 76.4 88.4 81.9 67.3 81.9 73.9 60.5 50.5 55.0
merged 10 best 95.7 31.2 47.0 95.3 24.5 38.9 N/A
After ILP ILP-SRL-f -10 75.1 82.9 78.8 80.6 75.7 78.1 65.7 72.4 68.9
ILP-SRL-f -10 ? CRF-OP/SRC with 1 best 82.3 81.7 82.0 81.5 73.4 77.3 N/A
Table 4: Entity extraction performance (by overlap-matching)
8 Experiments?II
Results using SRL are shown in Table 3 (on the
previous page). In the table, ILP+SRL-f denotes
the ILP approach using the link classifier with
the extra SRL ?f ?eatures, and ILP+SRL-fc de-
notes the ILP approach using both the extra SRL
?f ?eatures and the SRL ?c?onstraints. For compar-
ison, the ILP-1 and ILP-10 results from Table 2
are shown in rows 1 and 2.
The F-measure score of ILP+SRL-f -10 is 68.9,
about a 1 point increase from that of ILP-10,
which shows that extra SRL features for the link
classifier further improve the performance over
our previous best results.18 ILP+SRL-fc-10 also
performs better than ILP-10 in F-measure, al-
though it is slightly worse than ILP+SRL-f -10.
This indicates that the link classifier with extra
SRL features already makes good use of the V-A0
frames from the SRL system, so that forcing the
extraction of such frames via extra ILP constraints
only hurts performance by not allowing the extrac-
tion of non-V-A0 pairs in the neighborhood that
could have been better choices.
Contribution of the ILP phase In order to
highlight the contribution of the ILP phase for our
task, we present ?before? and ?after? performance
in Table 4. The first row shows the performance
of the individual CRF-OP, CRF-SRC, and CRF-
LINK classifiers before the ILP phase. Without the
ILP phase, the 1-best sequence generates the best
scores. However, we also present the performance
with merged 10-best entity sequences19 in order
to demonstrate that using 10-best sequences with-
out ILP will only hurt performance. The precision
of the merged 10-best sequences system is very
low, however the recall level is above 95% for both
18Statistically significant by paired-t test, where p <
0.001.
19If an entity Ei extracted by the ith-best sequence over-
laps with an entity Ej extracted by the jth-best sequence,
where i < j, then we discard Ej . If Ei and Ej do not over-
lap, then we extract both entities.
CRF-OP and CRF-SRC, giving an upper bound for
recall for our approach. The third row presents
results after the ILP phase is applied for the 10-
best sequences, and we see that, in addition to the
improved link extraction described in Section 7,
the performance on source extraction is substan-
tially improved, from F-measure of 73.9 to 78.1.
Performance on opinion expression extraction de-
creases from F-measure of 81.9 to 78.8. This de-
crease is largely due to implicit links, which we
will explain below. The fourth row takes the union
of the entities from ILP-SRL-f -10 and the entities
from the best sequences from CRF-OP and CRF-
SRC. This process brings the F-measure of CRF-
OP up to 82.0, with a different precision-recall
break down from those of 1-best sequences with-
out ILP phase. In particular, the recall on opinion
expressions now reaches 82.3%, while maintain-
ing a high precision of 81.7%.
Overlap Match Exact Match
r(%) p(%) f(%) r(%) p(%) f(%)
DEV.CONF 65.7 72.4 68.9 31.5 34.3 32.9
NO.CONF 63.7 76.2 69.4 30.9 36.7 33.5
Table 5: Relation extraction with ILP weight ad-
justment. (All cases using ILP+SRL-f -10)
Effects of ILP weight adjustment Finally, we
show the effect of weight adjustment in the ILP
formulation in Table 5. The DEV.CONF row shows
relation extraction performance using a weight
configuration based from the development data.
In order to see the effect of weight adjustment,
we ran an experiment, NO.CONF, using fixed de-
fault weights.20 Not surprisingly, our weight ad-
justment tuned from the development set is not the
optimal choice for cross-validation set. Neverthe-
less, the weight adjustment helps to balance the
precision and recall, i.e. it improves recall at the
20To be precise, cx = 1.0, c?x = 1.0 for x ? {O, S, L},
but cA = 0.2 is the same as before.
438
cost of precision. The weight adjustment is more
effective when the gap between precision and re-
call is large, as was the case with the development
data.
Implicit links A good portion of errors stem
from the implicit link relation, which our system
did not model directly. An implicit link relation
holds for an opinion entity without an associated
source entity. In this case, the opinion entity is
linked to an implicit source. Consider the follow-
ing example.
? Anti-Soviet hysteria was firmly oppressed.
Notice that opinion expressions such as ?Anti-
Soviet hysteria? and ?firmly oppressed? do not
have associated source entities, because sources of
these opinion expressions are not explicitly men-
tioned in the text. Because our system forces
each opinion to be linked with an explicit source
entity, opinion expressions that do not have ex-
plicit source entities will be dropped during the
global inference phase of our system. Implicit
links amount to 7% of the link relations in our
corpus, so the upper bound for recall for our ILP
system is 93%. In the future we will extend our
system to handle implicit links as well. Note that
we report results against a gold standard that in-
cludes implicit links. Excluding them from the
gold standard, the performance of our final sys-
tem ILP+SRL-f -10 is 72.6% in recall, 72.4% in
precision, and 72.5 in F-measure.
9 Conclusion
This paper presented a global inference approach
to jointly extract entities and relations in the con-
text of opinion oriented information extraction.
The final system achieves performance levels that
are potentially good enough for many practical
NLP applications.
Acknowledgments We thank the reviewers for their
many helpful comments and Vasin Punyakanok for running
our data through his SRL system. This work was sup-
ported by the Advanced Research and Development Activity
(ARDA), by NSF Grants IIS-0535099 and IIS-0208028, and
by gifts from Google and the Xerox Foundation.
References
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou and
D. Jurafsky 2004. Automatic Extraction of Opin-
ion Propositions and their Holders. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text.
R. Bunescu and R. J. Mooney 2004. Collective In-
formation Extraction with Relational Markov Net-
works. In ACL.
C. Cardie, J. Wiebe, T. Wilson and D. Litman 2004.
Low-Level Annotations and Summary Representa-
tions of Opinions for Multi-Perspective Question
Answering. New Directions in Question Answering.
Y. Choi, C. Cardie, E. Riloff and S. Patwardhan 2005.
Identifying Sources of Opinions with Conditional
Random Fields and Extraction Patterns. In HLT-
EMNLP.
S. Kim and E. Hovy 2005. Automatic Detection of
Opinion Bearing Words and Sentences. In IJCNLP.
S. Kim and E. Hovy 2005. Identifying Opinion
Holders for Question Answering in Opinion Texts.
In AAAI Workshop on Question Answering in Re-
stricted Domains.
J. Lafferty, A. K. McCallum and F. Pereira 2001 Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In ICML.
B. Liu, M. Hu and J. Cheng 2005 Opinion Observer:
Analyzing and Comparing Opinions on the Web. In
WWW.
R. J. Mooney and R. Bunescu 2005 Mining Knowl-
edge from Text Using Information Extraction. In
SIGKDD Explorations.
S. Morinaga, K. Yamanishi, K. Tateishi and T.
Fukushima 2002. Mining product reputations on
the Web. In KDD.
M. A. Munson, C. Cardie and R. Caruana. 2005. Opti-
mizing to arbitrary NLP metrics using ensemble se-
lection. In HLT-EMNLP.
J. Prager, E. Brown, A. Coden and D. Radev 2000.
Question-answering by predictive annotation. In SI-
GIR.
V. Punyakanok, D. Roth and W. Yih 2005. General-
ized Inference with Multiple Semantic Role Label-
ing Systems (Shared Task Paper). In CoNLL.
V. Punyakanok, D. Roth, W. Yih and D. Zimak 2004.
Semantic Role Labeling via Integer Linear Program-
ming Inference. In COLING.
D. Roth and W. Yih 2004. A Linear Programming For-
mulation for Global Inference in Natural Language
Tasks. In CoNLL.
D. Roth and W. Yih 2002. Probabilistic Reasoning for
Entity and Relation Recognition. In COLING.
V. Stoyanov, C. Cardie and J. Wiebe 2005. Multi-
Perspective Question Answering Using the OpQA
Corpus. In HLT-EMNLP.
C. Sutton, K. Rohanimanesh and A. K. McCallum
2004. Dynamic Conditional Random Fields: Fac-
torized Probabilistic Models for Labeling and Seg-
menting Sequence Data. In ICML.
M. White, T. Korelsky, C. Cardie, V. Ng, D. Pierce and
K. Wagstaff 2001. Multi-document Summarization
via Information Extraction In HLT.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotat-
ing Expressions of Opinions and Emotions in Lan-
guage. In Language Resources and Evaluation, vol-
ume 39, issue 2-3.
T. Wilson, J. Wiebe and P. Hoffmann 2005. Recogniz-
ing Contextual Polarity in Phrase-Level Sentiment
Analysis. In HLT-EMNLP.
439
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 5?13,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
zymake: a computational workflow system for machine learning and
natural language processing
Eric Breck
Department of Computer Science
Cornell University
Ithaca, NY 14853
USA
ebreck@cs.cornell.edu
Abstract
Experiments in natural language processing
and machine learning typically involve run-
ning a complicated network of programs to
create, process, and evaluate data. Re-
searchers often write one or more UNIX shell
scripts to ?glue? together these various pieces,
but such scripts are suboptimal for several rea-
sons. Without significant additional work, a
script does not handle recovering from fail-
ures, it requires keeping track of complicated
filenames, and it does not support running pro-
cesses in parallel. In this paper, we present
zymake as a solution to all these problems.
zymake scripts look like shell scripts, but
have semantics similar to makefiles. Using
zymake improves repeatability and scalabil-
ity of running experiments, and provides a
clean, simple interface for assembling compo-
nents. A zymake script also serves as doc-
umentation for the complete workflow. We
present a zymake script for a published set
of NLP experiments, and demonstrate that it
is superior to alternative solutions, including
shell scripts and makefiles, while being far
simpler to use than scientific grid computing
systems.
1 Introduction
Running experiments in natural language process-
ing and machine learning typically involves a com-
plicated network of programs. One program might
extract data from a raw corpus, others might pre-
process it with various linguistic tools, before finally
the main program being tested is run. Further pro-
grams must evaluate the output, and produce graphs
and tables for inclusion in papers and presentations.
All of these steps can be run by hand, but a more typ-
ical approach is to automate them using tools such
as UNIX shell scripts. We argue that any approach
should satisfy a number of basic criteria.
Reproducibility At some future time, the original
researcher or other researchers ought to be able to
re-run the set of experiments and produce identical
results1. Such reproducibility is a cornerstone of sci-
entific research, and ought in principle to be easier
in our discipline than in a field requiring physical
measurements such as physics or chemistry.
Simplicity We want to create a system that we and
other researchers will find easy to use. A system
which requires significant overhead before any ex-
periment can be run can limit a researcher?s ability
to quickly and easily try out new ideas.
A realistic life-cycle of experiments A typical ex-
periment evolves in structure as it goes along - the
researcher may choose partway through to add new
datasets, new ranges of parameters, or new sets of
models to test. Moreover, a computational exper-
iment rarely works correctly the first time. Com-
ponents break for various reasons, a tool may not
perform as expected, and so forth. A usable tool
must be simple to use in the face of such repeated
re-execution.
Software engineering Whether writing shell
scripts, makefiles, or Java, one is writing code,
and software engineering concerns apply. One
key principle is modularity, that different parts of
1User input presents difficulties which we will not discuss.
5
training regime classes
two-way distinction A vs B+O
two-way distinction B vs A+O
three-way distinction A vs B vs O
baseline comparison A+B vs O
Table 1: Training regimes
a program should be cleanly separated. Another
is generality, creating solutions that are re-usable
in different specific cases. A usable tool must
encourage good software engineering.
Inherent support for the combinatorial nature of
our experiments Experiments in natural language
processing and machine learning typically compare
different datasets, different models, different feature
sets, different training regimes, and train and test on
a number of cross-validation folds. This produces a
very large number of files which any system must
handle in a clean way.
In this paper, we present zymake2, and argue that
is superior to several alternatives for the task of au-
tomating the steps in running an experiment in natu-
ral language processing or machine learning.
2 A Typical NLP Experiment
As a running example, we present the following
set of experiments (abstracted from (Breck et al,
2007)). The task is one of entity identification -
we have a large dataset in which two different types
of opinion entities are tagged, type A, and type B.
We will use a sequence-based learning algorithm to
model the entities, but we want to investigate the re-
lationship between the two types. In particular, will
it be preferable to learn a single model which pre-
dicts both entity type A and entity type B, or two
separate models, one predicting A, and one predict-
ing B. The former case makes a three-way distinc-
tion between entities of type A, of type B, and of
type O, all other words. The latter two models make
a distinction between type A and both other types
or between type B and both other types. Further-
2Any name consisting of a single letter followed by make
already refers to an existing software project. zymake is the
first pronouncable name consisting of a two letter prefix to
make, starting from the end of the alphabet. I pronounce ?zy-?
as in ?zydeco.?
more, prior work to which we wish to compare does
not distinguish at all between type A and type B, so
we also need a model which just predicts entities to
be of either type A or B, versus the background O.
These four training regimes are summarized in Ta-
ble 1.
Given one of these training regimes, the model
is trained and tested using 10-fold cross-validation,
and the result is evaluated using precision and re-
call. The evaluation is conducted separately for class
A, for class B, and for predicting the union of both
classes.
2.1 Approach 1: A UNIX Shell Script
Many researchers use UNIX shell scripts to co-
ordinate experiments3. Figure 1 presents a poten-
tial shell script for the experiments discussed in Sec-
tion 2. Shell scripting is familiar and widely used
for co-ordinating the execution of programs. How-
ever, there are three difficulties with this approach -
it is difficult to partially re-run, the specification of
the filenames is error-prone, and the script is badly
modularized.
Re-running the experiment The largest difficulty
with this script is how it handles errors - namely, it
does not. If some early processes succeed, but later
ones fail, the researcher can only re-run the entire
script, wasting the time spent on the previous run.
There are two common solutions to this problem.
The simplest is to comment out the parts of the script
which have succeeded, and re-run the script. This
is highly brittle and error-prone. More reliable but
much more complicated is to write a wrapper around
each command which checks whether the outputs
from the command already exist before running it.
Neither of these is desirable. It is also worth not-
ing that this problem can arise not just through error,
but when an input file changes, an experiment is ex-
tended with further processing, additional graphs are
added, further statistics are calculated, or if another
model is added to the comparison.
3Some researchers use more general programming lan-
guages, such as Perl, Python, or Java to co-ordinate their ex-
periments. While such languages may make some aspects of
co-ordination easier ? for example, such languages would not
have to call out to an external program to produce a range of in-
tegers as does the script in Figure 1 ? the arguments that follow
apply equally to these other approaches.
6
for fold in ?seq 0 9?; do
extract-test-data $fold raw-data $fold.test
for class in A B A+B; do
extract-2way-training $fold raw-data $class > $fold.$class.train
train $fold.$class.train > $fold.$class.model
predict $fold.$class.model $fold.test > $fold.$class.out
prep-eval-2way $fold.$class.out > $fold.eval-in
eval $class $fold.$class.eval-in > $fold.$class.eval
done
extract-3way-training $fold raw-data > $fold.3way.train
train $fold.3way.train > $fold.3way.model
predict $fold.3way.model $fold.test > $fold.3way.out
for class in A B A+B; do
prep-eval-3way $class $fold.3way.out > $fold.3way.$class.eval-in
eval $class $fold.3way.$class.eval-in > $fold.3way.$class.eval
done
done
Figure 1: A shell script
Problematic filenames In this example, a file-
name is a concatenation of several variable names -
e.g. $(fold).$(class).train. This is also
error-prone - the writer of the script has to keep
track, for each filename, of which attributes need to
be specified for a given file, and the order in which
they must be specified. Either of these can change
as an experiment?s design evolves, and subtle design
changes can require changes throughout the script of
the references to many filenames.
Bad modularization In this example, the eval
program is called twice, even though the input and
output files in each case are of the same format.
The problem is that the filenames are such that the
line in the script which calls eval needs to be in-
clude information about precisely which files (in
one case $fold.3way.$class, and in the other
$fold.$class) are being evaluated. This is irrel-
evant ? a more modular specification for the eval
program would simply say that it operates on a
.eval-in file and produces an .eval file. We
will see ways below of achieving exactly this.4
4One way of achieving this modularization with shell scripts
could involve defining functions. While this could be effective,
this greatly increases the complexity of the scripts.
%.model: %.train
train $< > $@
%.out: %.model %.test
predict $? > $@
Figure 2: A partial makefile
2.2 Approach 2: A makefile
One solution to the problems detailed above is to
use a makefile instead of a shell script. The make
program (Feldman, 1979) bills itself as a ?utility to
maintain groups of programs?5, but from our per-
spective, make is a declarative language for speci-
fying dependencies. This seems to be exactly what
we want, and indeed it does solve some of the prob-
lems detailed above. make has several new prob-
lems, though, which result in its being not an ideal
solution to our problem.
Figure 2 presents a portion of a makefile for this
task. For this part, the makefile ideally matches what
we want. It will pick up where it left off, avoiding
the re-running problem above. The question of file-
names is sidestepped, as we only need to deal with
the extensions here. And each command is neatly
5GNU make manpage.
7
partitioned into its own section, which specifies its
dependencies, the files created by each command,
and the shell command to run to create them. How-
ever, there are three serious problems with this ap-
proach.
Files are represented by strings The first prob-
lem can be seen by trying to write a similar line for
the eval command. It would look something like
this:
%.eval: %.eval-in
eval get-class $? > $@
However, it is hard to write the code represented
here as get-class. This code needs to examine
the filename string of $? or $@, and extract the class
from that. This is certainly possible using standard
UNIX shell tools or make extensions, but it is ugly,
and has to be written once for every time such a
field needs to be accessed. For example, one way of
writing get-class using GNU make extensions
would be:
GETCLASS = $(filter A B A+B,\
$(subst ., ,$(1)))
%.eval: %.eval-in
eval $(call GETCLASS,$@) $? > $@
The basic problem here is that to make, a
file is represented by a string, its filename.
For machine learning and natural language pro-
cessing experiments, it is much more natu-
ral to represent a file as a set of key-value
pairs. For example, the file 0.B.model might
be represented as { fold = 0, class = B,
filetype = model } .
Combinatorial dependencies The second prob-
lem with make is that it is very difficult to spec-
ify combinatorial dependencies. If one continued to
write the makefile above, one would eventually need
to write a final all target to specify all the files
which would need to be built. There are 60 such
files: one for each fold of the following set
$fold.3way.A.eval
$fold.3way.B.eval
$fold.3way.A+B.eval
$fold.A.eval
%.taggerA.pos: %.txt
tagger_A $? > $@
%.taggerB.pos: %.txt
tagger_B $? > $@
%.taggerC.pos: %.txt
tagger_C $? > $@
%.chunkerA.chk: %.pos
chunker_A $? > $@
%.chunkerB.chk: %.pos
chunker_B $? > $@
%.chunkerC.chk: %.pos
chunker_C $? > $@
%.parserA.prs: %.chk
parser_A $? > $@
%.parserB.prs: %.chk
parser_B $? > $@
%.parserC.prs: %.chk
parser_C $? > $@
Figure 3: A non-functional makefile for testing three in-
dependent decisions
$fold.B.eval
$fold.A+B.eval
There is no easy way in make of listing these 60
files in a natural manner. One can escape to a shell
script, or use GNU make?s foreach function, but
both ways are messy.
Non-representable dependency structures The
final problem with make also relates to dependen-
cies. It is more subtle, but it turns out that there are
some sorts of dependency structures which cannot
be represented in make. Suppose I want to com-
pare the effect of using one of three parsers, one of
three part-of-speech-taggers and one of three chun-
kers for a summarization experiment. This involves
three separate three-way distinctions in the makefile,
where for each, there are three different commands
that might be run. A non-working example is in Fig-
8
ure 3. The problem is that make pattern rules (rules
using the % character) can only match the suffix or
prefix of a filename6. This makefile does not work
because it requires the parser, chunker, and tagger
to all be the last part of the filename before the type
suffix.
2.3 Approach 3: zymake
zymake is designed to address the problems out-
lined above. The key principles of its design are as
follows:
? Like make, zymakefiles can be re-run multi-
ple times, each time picking up where the last
left off.
? Files are specified by key-value sets, not by
strings
? zymake includes a straightforward way of
handling combinatorial sets of files.
? zymake syntax is minimally different from
shell syntax.
Figure 4 presents a zymakefile which runs the run-
ning example experiment. Rather than explaining
the entire file at once, we will present a series of in-
creasingly complex parts of it.
Figure 5 presents the simplest possible zymake-
file, consisting of one rule, which describes how to
create a $().test file, and one goal, which lists
what files should be created by this file. A rule is
simply a shell command7, with some number of in-
terpolations8. An interpolation is anything between
the characters $( and the matching ). This is the
only form of interpolation done by zymake, so as
to minimally conflict with other interpolations done
by the shell, scripting languages such as Perl, etc.
6Thus, if we were only comparing two sets of items ? e.g.
parsers and taggers but not chunkers ? we could write this set
of dependencies by using a prefix to distinguish one set and a
suffix to distinguish the other. This is hardly pretty, though, and
does not extend to more than two sets.
7Users who are familiar with UNIX shells will find it use-
ful to be able to use input/output redirection and pipelines in
zymakefiles. Knowledge of advanced shell programming is not
necessary to use zymake, however.
8This term is used in Perl; it is sometimes referred to in other
languages as ?substitution? or ?expansion.?
extract-test-data $(fold) raw-data
$(>).test
extract-2way-training $(fold) raw-data
$(class) > $(train="2way").train
extract-3way-training $(fold) raw-data
> $(train="3way").train
train $().train > $().model
predict $().model $().test > $().out
prep-eval-3way $(class) $().out >
$(train="3way").eval-in
prep-eval-2way $().out >
$(train="2way").eval-in
eval $(class) $().eval-in > $().eval
classes = A B A+B
ways = 2way 3way
: $(fold = *(range 0 9)
class = *classes
train = *ways).eval
Figure 4: An example zymakefile. The exact commands
run by this makefile are presented in Appendix A.
extract-test-data raw-data $(>).test
: $().test
Figure 5: Simple zymakefile #1
extract-test-data $(fold) raw-data
$(>).test
: $(fold=0).test $(fold=1).test
Figure 6: Simple zymakefile #2
9
extract-test-data $(fold) raw-data
$(>).test
folds = 0 1
: $(fold=*folds).test
Figure 7: Simple zymakefile #3
The two interpolations in this example are file in-
terpolations, which are replaced by zymake with a
generated filename. Files in zymake are identified
not by a filename string but by a set of key-value
pairs, along with a suffix. In this case, the two in-
terpolations have no key-value pairs, and so are only
represented by a suffix. Finally, there are two kinds
of file interpolations - inputs, which are files that are
required to exist before a command can be run, and
outputs, which are files created by a command9. In
this case, the interpolation $(>).test is marked
as an output by the > character10, while $().test
is an input, since it is unmarked.
The goal of this program is to create a file match-
ing the interpolation $().test. The single rule
does create a file matching that interpolation, and so
this program will result in the execution of the fol-
lowing single command:
extract-test-data raw-data .test
Figure 6 presents a slightly more complex zy-
makefile. In this case, there are two goals - to create
a .test file with the key fold having the value
0, and another .test file with fold equal to 1.
We also see that the rule has become slightly more
complex ? there is now another interpolation. This,
however, is not a file interpolation, but a variable in-
terpolation. $(fold) will be replaced by the value
of fold.
9Unlike make, zymake requires that each command ex-
plicitly mention an interpolation corresponding to each input
or output file. This restriction is caused by the merging of the
command part of the rule with the dependency part of the rule,
which are separate in make. We felt that this reduced redun-
dancy and clutter in the zymakefiles, but this may occasionally
require writing a wrapper around a program which does not be-
have in this manner.
10zymakewill also infer that any file interpolation following
the > character, representing standard output redirection in the
shell, is an output
Executing this zymakefile results in the execution
of two commands:
extract-test-data 0 raw-data 0.test
extract-test-data 1 raw-data 1.test
Note that the output files are now not just .test
but include the fold number in their name. This is
because zymake infers that the fold key, mentioned
in the extract rule, is needed to distinguish the two
test files. In general the user should specify as few
keys as possible for each file interpolation, and allow
zymake to infer the exact set of keys necessary to
distinguish each file from the rest11.
Figure 7 presents a small refinement to the zy-
makefile in Figure 6. The commands that will be run
are the same, but instead of separately listing the two
test files to be created, we create a variable folds
which is a list of all the folds we want, and use a
splat to create multiple goals. A splat is indicated
by the asterisk character, and creates one copy of the
file interpolation for each value in the variable?s list.
Figure 4 is now a straightforward extension of the
example we have seen so far. It uses a few more
features of zymake that we will not discuss, such
as string-valued keys, and the range function, but
further documentation is available on the zymake
website. zymake wants to create the goals at the
end, so it examines all the rules and constructs a di-
rected acyclic graph, or DAG, representing the de-
pendencies among the files. It then executes the
commands in some order based on this DAG ? see
Section 3 for discussion of execution order.
2.4 Benefits of zymake
zymake satisfies the criteria set out above, and han-
dles the problems discussed with other systems.
? Reproducibility. By providing a single file
which can be re-executed many times, zymake
encourages a development style that encodes
all information about a workflow in a single
file. This also serves as documentation of the
complete workflow.
11Each file will be distinguished by all and only the keys
needed for the execution of the command that created it, and
the commands that created its inputs. A unique, global ordering
of keys is used along with a unique, global mapping of filename
components to key, value pairs so that the generated filename
for each file uniquely maps to the appropriate set of key, value
pairs.
10
? Simplicity. zymake only requires writing a set
of shell commands, annotated with interpola-
tions. This allows researchers to quickly and
easily construct new and more complex exper-
iments, or to modify existing ones.
? Experimental life-cycle. zymake can re-
execute the same file many times when com-
ponents fail, inputs change, or the workflow is
extended.
? Software engineering. Each command in a
zymakefile only needs to describe the inputs
and outputs relevant for that command, making
the separate parts of the file quite modular.
? Combinatorial experiments. zymake includes
a built-in method for specifying that a particu-
lar variable needs to range over several possi-
bilities, such as a set of models, parameter val-
ues, or datasets.
2.5 Using zymake
Beginning to use zymake is as simple as download-
ing a single binary from the website12. Just as with
a shell script or makefile, the user then writes a sin-
gle textual zymakefile, and passes it to zymake for
execution. Typical usage of zymake will be in an
edit-run development cycle.
3 Parallel Execution
For execution of very large experiments, efficient
use of parallelism is necessary. zymake offers a
natural way of executing the experiment in a maxi-
mally parallel manner. The default serial execution
does a topological sort of the DAG, and executes
the components in that order. To execute in paral-
lel, zymake steps through the DAG starting at the
roots, starting any command which does not depend
on a command which has not yet executed.
To make this practical, of course, remote execu-
tion must be combined with parallel execution. The
current implementation provides a simple means of
executing a remote job using ssh, combined with
a simple /proc-based measure of remote cpu uti-
lization to find the least-used remote cpu from a
12Binaries for Linux, Mac OS X, and Windows, as well
as full source code, are available at http://www.cs.
cornell.edu/?ebreck/zymake/.
provided set. We are currently looking at extend-
ing zymake to interface it with the Condor sys-
tem (Litzkow et al, 1988). Condor?s DAGMan
is designed to execute a DAG in parallel on a set
of remote machines, so it should naturally fit with
zymake. Interfaces to other cluster software are
possible as well. Another important extension will
be to allow the system to throttle the number of con-
current jobs produced and/or collect smaller jobs to-
gether, to better match the available computational
resources.
4 Other approaches
Deelman et al (2004) and Gil et al (2007) describe
the Pegasus andWings systems, which together have
a quite similar goal to zymake. This system is de-
signed to manage large scientific workflows, with
both data and computation distributed across many
machines. A user describes their available data and
resources in a semantic language, along with an
abstract specification of a workflow, which Wings
then renders into a complete workflow DAG. This is
passed to Pegasus, which instantiates the DAG with
instances of the described resources and passes it to
Condor for actual execution. The system has been
used for large-scale scientific experiments, such as
earthquake simulation. However, we believe that
the added complexity of the input that a user has
to provide over zymake?s simple shell-like syntax
will mean a typical machine learning or natural lan-
guage processing researcher will find zymake eas-
ier to use.
The GATE and UIMA architectures focus specif-
ically on the management of components for lan-
guage processing (Cunningham et al, 2002; Fer-
rucci and Lally, 2004). While zymake knows noth-
ing about the structure of the files it manages, these
systems provide a common format for textual an-
notations which all components must use. GATE
provides a graphical user interface for running com-
ponents and for viewing and producing annotations.
UIMA provides a framework not just for running ex-
periments but for data analysis and application de-
ployment. Compared to writing a zymake script,
however, the requirements for using these systems
to manage an experiment are greater. In addition,
both these architectures most naturally support com-
11
ponents written in Java (and in the case of UIMA,
C++). zymake is agnostic as to the source language
of each component, making it easier to include pro-
grams written by third parties or by researchers who
prefer different languages.
make, despite dating from 1979, has proved its
usefulness over time, and is still widely used. Many
other systems have been developed to replace it,
including ant13, SCons14, maven15, and others.
However, so far as we are aware, none of these sys-
tems solves the problems we have described with
make. As with make and shell scripts, running
experiments is certainly possible using these other
tools, but we believe they are far more complex and
cumbersome than zymake.
5 Future Extensions
There are a number of extensions to zymake which
could make it even more useful. One is to allow the
dependency DAG to vary during the running of the
experiment. At the moment, zymake requires that
the entire DAG be known before any processes can
run. As an example of when this is less than ideal,
consider early-stopping an artificial neural network.
One way of doing this is train the network to full
convergence, and output predictions from the inter-
mediate networks at some fixed interval of epochs.
We would like then to evaluate all these predictions
on held-out data (running one process for each of
them) and then to choose the point at which this
score is maximized (running one process for the
whole set). Since the number of iterations to con-
vergence is not known ahead of time, at the moment
we cannot support this structure in zymake. We
plan, however, to allow the structure of the DAG to
vary at run-time, allowing such experiments.
We are also interested in other extensions, includ-
ing an optional textual or graphical progress bar,
providing a way for the user to have more control
over the string filename produced from a key-value
set16, and keeping track of previous versions of cre-
ated files, to provide a sort of version control of the
output files.
13http://ant.apache.org/.
14http://www.scons.org/.
15http://maven.apache.org/.
16This will better allow zymake to interact with other work-
flows.
6 Conclusion
Most experiments in machine learning and natu-
ral language processing involve running a complex,
interdependent set of processes. We have argued
that there are serious difficulties with common ap-
proaches to automating these experiments. In their
place, we offer zymake, a new scripting language
with shell-like syntax but make-like semantics. We
hope our community will find it as useful as we have.
Acknowledgements
We thank Yejin Choi, Alex Niculescu-Mizil, David
Pierce, the Cornell machine learning discussion
group, and the anonymous reviewers for helpful
comments on earlier drafts of this paper.
A Output of Figure 4
We present here the commands run by zymake
when presented with the file in Figure 4. We present
only the commands run for fold 0, not for all 10
folds. Also, in actual execution zymake adds a pre-
fix to each filename based on the name of the zy-
makefile, so as to separate different experiments. Fi-
nally, note that this is only one possible order that the
commands could be run in.
extract-2way-training 0 raw-data A > A.0.2way.train
train A.0.2way.train > A.0.2way.model
extract-2way-training 0 raw-data B > B.0.2way.train
train B.0.2way.train > B.0.2way.model
extract-2way-training 0 raw-data A+B > AB.0.2way.train
train AB.0.2way.train > AB.0.2way.model
extract-3way-training 0 raw-data > 0.3way.train
train 0.3way.train > 0.3way.model
extract-test-data 0 raw-data 0.test
predict A.0.2way.model 0.test > A.0.2way.out
prep-eval-2way A.0.2way.out > A.0.2way.eval-in
eval A A.0.2way.eval-in > A.0.2way.eval
predict B.0.2way.model 0.test > B.0.2way.out
prep-eval-2way B.0.2way.out > B.0.2way.eval-in
eval B B.0.2way.eval-in > B.0.2way.eval
predict AB.0.2way.model 0.test > AB.0.2way.out
prep-eval-2way AB.0.2way.out > AB.0.2way.eval-in
eval A+B AB.0.2way.eval-in > AB.0.2way.eval
predict 0.3way.model 0.test > 0.3way.out
prep-eval-3way A 0.3way.out > A.0.3way.eval-in
eval A A.0.3way.eval-in > A.0.3way.eval
prep-eval-3way B 0.3way.out > B.0.3way.eval-in
eval B B.0.3way.eval-in > B.0.3way.eval
prep-eval-3way A+B 0.3way.out > AB.0.3way.eval-in
eval A+B AB.0.3way.eval-in > AB.0.3way.eval
12
References
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of the Twentieth International Joint Confer-
ence on Artificial Intelligence (IJCAI-2007), Hyder-
abad, India, January.
Hamish Cunningham, Diana Maynard, Kalina Bont-
cheva, and Valentin Tablan. 2002. GATE: A frame-
work and graphical development environment for ro-
bust NLP tools and applications. In Proceedings of the
40th Anniversary Meeting of the Association for Com-
putational Linguistics (ACL ?02), Philadelphia, July.
Ewa Deelman, James Blythe, Yolanda Gil, Carl Kessel-
man, Gaurang Mehta, Sonal Patil, Mei-Hui Su, Karan
Vahi, and Miron Livny. 2004. Pegasus : Mapping
scientific workflows onto the grid. In Across Grids
Conference, Nicosia, Cyprus.
Stuart I. Feldman. 1979. Make-a program for maintain-
ing computer programs. Software - Practice and Ex-
perience, 9(4):255?65.
David Ferrucci and Adam Lally. 2004. UIMA: an archi-
tectural approach to unstructured information process-
ing in the corporate research environment. Nat. Lang.
Eng., 10(3-4):327?348.
Yolanda Gil, Varun Ratnakar, Ewa Deelman, Gaurang
Mehta, and Jihie Kim. 2007. Wings for pegasus: Cre-
ating large-scale scientific applications using semantic
representations of computational workflows. In Pro-
ceedings of the 19th Annual Conference on Innovative
Applications of Artificial Intelligence (IAAI), Vancou-
ver, British Columbia, Canada, July.
Michael Litzkow, Miron Livny, and Matthew Mutka.
1988. Condor - a hunter of idle workstations. In Pro-
ceedings of the 8th International Conference of Dis-
tributed Computing Systems, June.
13
