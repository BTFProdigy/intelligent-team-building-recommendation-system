Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1500?1510, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Supervised Text-based Geolocation
Using Language Models on an Adaptive Grid
Stephen Roller? Michael Speriosu ? Sarat Rallapalli ?
Benjamin Wing ? Jason Baldridge ?
?Department of Computer Science, University of Texas at Austin
?Department of Linguistics, University of Texas at Austin
{roller, sarat}@cs.utexas.edu, {speriosu, jbaldrid}@utexas.edu, ben@benwing.com
Abstract
The geographical properties of words have re-
cently begun to be exploited for geolocating
documents based solely on their text, often in
the context of social media and online content.
One common approach for geolocating texts is
rooted in information retrieval. Given training
documents labeled with latitude/longitude co-
ordinates, a grid is overlaid on the Earth and
pseudo-documents constructed by concatenat-
ing the documents within a given grid cell;
then a location for a test document is chosen
based on the most similar pseudo-document.
Uniform grids are normally used, but they are
sensitive to the dispersion of documents over
the earth. We define an alternative grid con-
struction using k-d trees that more robustly
adapts to data, especially with larger training
sets. We also provide a better way of choosing
the locations for pseudo-documents. We eval-
uate these strategies on existing Wikipedia and
Twitter corpora, as well as a new, larger Twit-
ter corpus. The adaptive grid achieves com-
petitive results with a uniform grid on small
training sets and outperforms it on the large
Twitter corpus. The two grid constructions
can also be combined to produce consistently
strong results across all training sets.
1 Introduction
The growth of the Internet in recent years has
provided unparalleled access to informational re-
sources. It is often desirable to extract summary
metadata from such resources, such as the date of
writing or the location of the author ? yet only a
small portion of available documents are explicitly
annotated in this fashion. With sufficient training
data, however, it is often possible to infer this infor-
mation directly from a document?s text. For exam-
ple, clues to the geographic location of a document
may come from a variety of word features, e.g. to-
ponyms (Toronto), geographic features (mountain),
culturally local features (hockey), and stylistic or di-
alectical differences (cool vs. kewl vs. kool).
This article focuses on text-based document ge-
olocation, the prediction of the latitude and lon-
gitude of a document. Among the uses for this
are region-based search engines; tracing the sources
of historical documents; location attribution while
summarizing large documents; tailoring of ads while
browsing; phishing detection when a user account is
accessed from an unexpected location; and ?activist
mapping? (Cobarrubias, 2009), as in the Ushahidi
project.1 Geolocation has also been used as a fea-
ture in automatic news story identification systems
(Sankaranarayanan et al 2009).
One of the first works on document geolocation is
Ding et al(2000), who attempt to automatically de-
termine the geographic scope of web pages. They
focus on named locations, e.g. cities and states,
found in gazetteers. Locations are predicted based
on toponym detection and heuristic resolution al-
gorithms. A related, recent effort is Cheng et al
(2010), who geolocate Twitter users by resolving
their profile locations against a gazetteer of U.S.
cities and training a classifier to identify geographi-
cally local words.
An alternative to using a discrete set of locations
from a gazetteer is to use information retrieval (IR)
techniques on a set of geolocated training docu-
ments. A new test document is compared with each
1http://ushahidi.com/
1500
training document and a location chosen based on
the location(s) of the most similar training docu-
ment(s). For image geolocation, Chen and Grauman
(2011) perform mean-shift clustering over training
images to discretize locations, then estimate a test
image?s location with weighted voting from the k
most similar documents. For text, both Serdyukov
et al(2009) and Wing and Baldridge (2011) use a
similar approach, but compute document similarity
based on language models rather than image fea-
tures. Additionally, they group documents via a uni-
form geodesic grid rather than a clustered set of lo-
cations. This reduces the number of similarity com-
putations and removes the need to perform location
clustering altogether, but introduces a new param-
eter controlling the granularity of the grid. Kinsella
et al(2011) predict the locations of tweets and users
by comparing text in tweets to language models as-
sociated with zip codes and broader geopolitical en-
closures. Sadilek et al(2012) discretize by simply
clustering data points within a small distance thresh-
old, but only perform geolocation within fixed city
limits.
While the above approaches discretize the contin-
uous surface of the earth, Eisenstein et al(2010)
predict locations based on Gaussian distributions
over the earth?s surface as part of a hierarchical
Bayesian model. This model has many advantages
(e.g. the ability to compute a complete probability
distribution over locations), but we suspect it will be
difficult to scale up to the large document collections
needed for high accuracy.
We build on the IR approach with grids while ad-
dressing some of the shortcomings of a uniform grid.
Uniform grids are problematic in that they ignore the
geographic dispersion of documents and forgo the
possibility of greater-granularity geographic resolu-
tion in document-rich areas. Instead, we construct
a grid using a k-d tree, which adapts to the size of
the training set and the geographic dispersion of the
documents it contains. This can better benefit from
more data, since it enables the training set to support
more pseudo-documents when there is sufficient ev-
idence to do so, while still ensuring that all pseudo-
documents contain comparable amounts of data. It
also has the desirable property of generally requiring
fewer active cells than a uniform grid, drastically re-
ducing the computation time required to label a test
document.
We show that consistently strong results, robust
across both Wikipedia and Twitter datasets, are ob-
tained from the union of the pseudo-documents from
a uniform and adaptive grid. In addition, a sim-
ple difference in the choice of location for a given
grid cell ? the centroid of the training documents
in the cell, rather than the cell midpoint ? results
in across-the-board improvements. We also con-
struct and evaluate on a much larger dataset of ge-
olocated tweets than has been used in previous pa-
pers, demonstrating the scalability and robustness of
our methods and confirming the ability of the adap-
tive grid to more effectively use larger datasets.
2 Data
We work with three datasets: a corpus of geotagged
Wikipedia articles and two corpora of geotagged
tweets.
GEOWIKI is a collection of 1,019,490 geotagged
English articles from Wikipedia. The dump from
Wikimedia requires significant processing to obtain
article text and location, so we rely on the prepro-
cessed data used by Wing and Baldridge (2011).
GEOTEXT is a small dataset consisting of
377,616 messages from 9,475 users tweeting across
48 American states, compiled by Eisenstein et al
(2010). A document in this dataset is the concate-
nation of all tweets by a single user, with a location
derived from the earliest tweet with specific, GPS-
assigned latitude/longitude coordinates.
UTGEO2011 is a new dataset designed to ad-
dress the sparsity problems resulting from the size
of the previous dataset. It is based on 390 mil-
lion tweets collected across the entire globe be-
tween September 4th and November 29th, 2011, us-
ing the publicly available Twitter Spritzer feed and
global search API. Not all collected tweets were
geotagged. To be comparable to GEOTEXT, we
discarded tweets outside of North America (out-
side of the bounding box with latitude/longitude
corners at (25,?126) and (49,?60)). Following
Eisenstein et al(2010), we consider all tweets
of a user concatenated as a single document, and
use the earliest collected GPS-assigned location as
the gold location. Users without a gold location
were discarded. To remove many spammers and
1501
robots, we only kept users following 5 to 1000
people, followed by at least 5 users, and author-
ing no more than 1000 tweets in the three month
period. The resulting dataset contains 38 million
tweets from 449,694 users, or roughly 85 tweets
per user on average. We randomly selected 10,000
users each for development and held-out test eval-
uation. The remaining 429,694 users serve as a
training set termed UTGEO2011-LARGE. We also
randomly selected a 10,000 user training subset
(UTGEO2011-SMALL) to facilitate comparisons
with GEOTEXT and allow us to investigate the rel-
ative improvements for different models with more
training data.
Our code and the UTGEO2011 data set are both
available for download.2
3 Model
Assume we have a collection d of documents and
their associated location labels l. These docu-
ments may be actual texts, or they can be pseudo-
documents comprised of a number of texts grouped
via some algorithm (such as the grids discussed in
the next section).
For a test document di, its similarity to each la-
beled document is computed, and the location of the
most similar document assigned to di. Given an ab-
stract function sim that can be instantiated with an
appropriate similarity function (e.g. cosine distance
or Kullback-Leibler divergence),
loc(di) = loc(argmax
dj?d
sim(di, dj)).
This is a winner-takes-all strategy, which we follow
in this paper. In related work on image geoloca-
tion, Hays and Efros (2008) use the same general
framework, but compute the location based on the
k-nearest neighbors (kNN) rather than the top one.
They compute a distribution from the 120 nearest
neighbors using mean shift clustering (Comaniciu
and Meer, 2002) and choose the cluster with the
most members. This produced slightly better re-
sults than choosing only the closest image. In future
work, we will explore the kNN approach to see if it
is more effective for text geolocation.
2https://github.com/utcompling/
textgrounder/wiki/RollerEtAl_EMNLP2012
Following previous work in document geoloca-
tion, particularly Serdyukov et al(2009) (hence-
forth SMvZ) and Wing and Baldridge (2011)
(henceforth W&B), we geolocate texts using a lan-
guage modeling approach to information retrieval
(Ponte and Croft, 1998; Zhai and Lafferty, 2001).
For each document di, we construct a unigram prob-
ability distribution ?di over the vocabulary.
We smooth documents using the pseudo-Good-
Turing method of W&B, a nonparametric discount-
ing model that backs off from the unsmoothed distri-
bution ??di of the document to the unsmoothed distri-
bution ??D of all documents. A general discounting
model is as follows:
P (w|?di) =
{
(1? ?di)P (w|??di), if P (w|??di) > 0
?di
P (w|??D)
Udi
, otherwise,
where Udi = 1 ?
?
w?di
P (w|??D) is a normaliza-
tion factor that is precomputed when the distribution
for di is constructed. The discount factor ?di indi-
cates how much probability mass to reserve for un-
seen words. For pseudo-Good-Turing, it is
?di =
|w ? di s.t. count(w ? di) = 1|
|w ? di|
,
i.e. the fraction of words seen once in di.
We experimented with other smoothing methods,
including Jelinek-Mercer and Dirichlet smoothing.
A disadvantage of these latter two methods is that
they have an additional tuning parameter to which
their performance is highly sensitive, and even with
optimal parameter settings neither consistently out-
performed pseudo-Good-Turing. We also found no
consistent improvement from using interpolation in
place of backoff.
We also follow W&B in using Kullback-Leibler
(KL) divergence as the similarity metric, since it out-
performed both naive Bayes classification probabil-
ity and cosine similarity:
KL(?di ||?dj ) =
?
k
?di(k) log
?di(k)
?dj (k)
.
The motivation for computing similarity with KL is
that it is a measure of how well each document in
the labeled set explains the word distribution found
in the test document.
1502
4 Collapsing Documents with an Adaptive
Grid
In the previous section, we used the term ?docu-
ment? loosely when speaking of training documents.
A simplistic approach might indeed involve com-
paring a test document to each training document.
However, in the winner-takes-all model described
above, we can rely only on the result of comparing
with the single best training document, which may
not contain enough information to make a good pre-
diction.
A standard strategy to deal with this problem is
to collapse groups of geographically nearby docu-
ments into larger pseudo-documents. This also has
the advantage of reducing the computation time,
as fewer training documents need to be compared
against. Formally, this involves partitioning the
training documents into a set of sets of documents
G = {g1 . . . gn}. A collection d? of pseudo-
documents is formed from this set, such that the
pseudo-document for a particular group gi is simply
the concatenation of the documents in the group:
d?gi =
?
dj?gi
dj .
A location must be associated with each pseudo-
document. This can be chosen based on the parti-
tioning function itself or the locations of the docu-
ments in each group.
Both W&B and SMvZ use uniform grids consist-
ing of cells of equal degree size to partition doc-
uments. We explore an alternative that uses k-d
(k-dimensional) trees to construct a non-uniform
grid that adapts to training sets of different sizes
more gracefully. It ensures a roughly equal num-
ber of documents in each cell, which means that all
pseudo-documents compete on similar footing with
respect to the amount of training data.
W&B define the location for a cell to be its ge-
ographic center, while SMvZ only perform error
analysis in terms of choosing the correct cell. We
obtain consistently improved results using the cen-
troid of the cell?s documents, which takes into ac-
count where the documents are concentrated.
4.1 k-d Trees
A k-d tree is a space-partitioning data structure for
storing points in k-dimensional space, which groups
nearby points into buckets. As one moves down the
tree, the space is split into smaller regions along
chosen dimensions. In this way, it is a generaliza-
tion of a binary search tree to multiple dimensions.
The k-d tree was first introduced by Bentley (1975)
and has since been applied to numerous problems,
e.g. Barnes-Hut simulation (Anderson, 1999) and
nearest-neighbors search (Friedman et al 1977).
Partitioning geolocated documents using a k-d
tree provides finer granularity in dense regions and
coarser granularity elsewhere. For example, doc-
uments from Queens and Brooklyn may show sig-
nificant cultural distinctions, while documents sepa-
rated by the same distance in rural Montana may ap-
pear culturally identical. A uniform grid with large
cells will mash Queens and Brooklyn together, while
small cells will create unnecessarily sparse regions
in Montana.
An important parameter for a k-d tree is its bucket
size, which determines the maximum number of
points (documents in our case) that a cell may con-
tain. By varying the bucket size, the cells can be
made fine- or coarse-grained.
4.2 Partitioning with a k-d Tree
For geolocation, we consider the surface of earth to
be a 2-dimensional space (k=2) over latitude, longi-
tude pairs. We form a k-d tree by a recursive proce-
dure over the training data. Initially, all documents
are placed in the root node of the tree. If the number
of documents in the node exceeds the bucket size,
the node is split into two nodes along a chosen split
dimension and point. This procedure is recursively
called on each of the new child nodes, and repeats
until no node is overflowing. The resulting leaves of
the k-d tree form a patchwork of rectangles which
cover the entire earth.3
When splitting an overflowing node, the choice of
splitting dimension and point can greatly impact the
structure of the resulting k-d tree. Following Fried-
man et al(1977), we choose to always split a node
3We note that the grid ?rectangles? are actually trapezoids
due to the nature of the latitude/longitude coordinate system.
We assume the effect of this is negligible, since most documents
are away from the poles, where distortion is the most extreme.
1503
Figure 1: View of North America showing k-d leaves cre-
ated from GEOWIKI with a bucket size of 600 and the
MIDPOINT method, as visualized in Google Earth.
Figure 2: k-d leaves over the New York City and nearby
areas from the same dataset and parameter settings as in
Figure 1.
along the dimension exhibiting the greatest range of
values. However, there still exist multiple methods
for determining the split point, i.e. the point separat-
ing documents into ?left? and ?right? nodes. In this
paper, we consider two possibilities for selecting this
point: the MIDPOINT method, and the FRIEDMAN
method. The latter splits at the median of all the
points, resulting in an equal number of points in both
the left and right nodes and a perfectly balanced k-d
tree. The former splits at the midpoint between the
two furthest points, allowing for a greater difference
in the number of points in each bin. For geolocation,
the FRIEDMAN splitting method will likely lead to
less sparsity, and therefore more accurate cell selec-
tion. On the other hand, the MIDPOINT method is
likely to draw more geographically desirable bound-
aries.
Figure 1 shows the leaves of the k-d tree formed
over North America using the GEOWIKI dataset,
the MIDPOINT node division method, and a bucket
size of 600. Figure 2 shows the leaves over New
York City and its surrounding area for the same
dataset and settings. More densely populated ar-
eas of the earth (which in turn tend to have more
Wikipedia documents associated with them) contain
smaller and more numerous leaf cells. The cells
over Manhattan are significantly smaller than those
of Queens, the Bronx, and East Jersey, even at such
a coarse bucket size. Though the leaves of the k-d
tree implicitly cover the entire surface of the earth,
our illustrations limit the size of each box by its data,
leaving gaps where no training documents exist.
4.3 Selecting a Representative Location
W&B use the geographic center of a cell as the
geolocation for the pseudo-document it represents.
However, this ignores the fact that many cells will
have imbalances in the dispersion of the documents
they contain ? typically, they will be clumpy, with
documents clustering around areas of high popula-
tion or activity. An alternative is to select the cen-
troid of the locations of all the documents contained
within a cell. Uniform grids with small cells are
not especially sensitive to this choice since the abso-
lute distance between a center or centroid prediction
will not be great, and empty cells are simply dis-
carded. Nonetheless, using the centroid has the ben-
efit of making a uniform grid less sensitive to cell
size, such that larger cells can be used more reliably
? especially important when there are few training
documents.
In contrast, when choosing representative loca-
tions for the leaves of a k-d tree, it is quite important
to use the centroid because the leaves necessarily
span the entire earth and none are discarded (since
all have a roughly similar number of documents in
them). Some areas with low document density are
thus assigned very large cells, such as those over
the oceans, as seen in Figures 1 and 2. Using the
centroid allows these large leaves to be in the mix,
while still predicting the locations in them that have
the greatest document density.
5 Experimental Setup
Configurations. We experiment with several con-
figurations of grids and representative locations.
1504
0 200 400 600 8002
00
250
300
350
Bucket Size
Mean
 Erro
r (km)
ooo
o o
o o
o o
o o
xxx
x
x x
x x x x
xox MidpointFriedman
200 400 600 800 1000 1200
850
900
950
1000
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
200 400 600 800 1000 1200
1100
1120
1140
1160
1180
Bucket Size
Mean
 Erro
r (km)
MidpointFriedman
(a) (b) (c)
Figure 3: Development set comparisons for (a) GEOWIKI, (b) GEOTEXT, and (c) UTGEO2011-SMALL.
W&B refers to a uniform grid and geographic-
center location selection, UNIFCENTROID to a
uniform grid with centroid location selection,
KDCENTROID to a k-d tree grid with centroid
location selection, and UNIFKDCENTROID to
the union of pseudo-documents constructed by
UNIFCENTROID and KDCENTROID.
We also provide two baselines, both of which are
based on a uniform grid with centroid location selec-
tion. RANDOM predicts a grid cell chosen at random
uniformly; MOSTCOMMONCELL always predicts
the grid cell containing the most training documents.
Note that a most-common k-d leaf baseline does not
make sense, as all k-d leaves contain approximately
the same number of documents.
Evaluation. We use three metrics to measure ge-
olocation performance. The output of each exper-
iment is a predicted coordinate for each test docu-
ment. For each prediction, we compute the error dis-
tance along the surface of the earth to the gold coor-
dinate. We report the mean and median of all such
distances as in W&B and Eisenstein et al(2011).
We also report the fraction of error distances less
than 161 km, corresponding to Cheng et al(2010)?s
measure of predictions within 100 miles of the true
location. This third measure can reveal differences
between models not obvious from just mean and me-
dian.
6 Results
This section provides results for the datasets
described previously: GEOWIKI, GEOTEXT,
UTGEO2011-LARGE and UTGEO2011-SMALL.
We first give details for how we tuned parameters
and algorithmic choices using the development sets,
and then provide performance on the test sets based
on these determinations.
6.1 Tuning
The specific parameters are (1) the partition location
method; (2) the bucket size for k-d partitioning; (3)
the node division method for k-d partitioning; (4) the
degree size for uniform grid partitioning. We tune
with respect to mean error, like W&B.
Partition Location Method. Development set
results show that the centroid always performs bet-
ter than the center for all datasets, typically by a
wide margin (especially for large partition sizes). To
save space, we do not provide details, but point the
reader to the differences in test set results between
W&B and UNIFCENTROID (which are identical ex-
cept that the former uses the center and the latter
uses the centroid) in Tables 1 and 2. All further pa-
rameter tuning is done using the centroid method.
k-d Tree Bucket Size. Bucket size should not be
too large as a proportion of the total number of train-
ing documents. Larger bucket sizes tend to produce
larger leaves, so documents in a partition will have
a higher average distance to the center or centroid
point. This will result in predictions being made at
too coarse a granularity, greatly limiting obtainable
precision even when the correct leaf is chosen.
Conversely, small bucket sizes lead to fewer train-
ing documents per partition. A bucket size of one
reduces to the situation where no pseudo-documents
are used. While this might work well if location pre-
diction is done using the kNNs for a test document, it
1505
Test dataset GEOWIKI GEOTEXT
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 0.1? 7056 7145 0.3 5? 2008 1866 1.6
MOSTCOMMONCELL 0.1? 4265 2193 5.0 5? 1158 757 31.3
Eisenstein et al- - - - - 845 501 -
Wing & Baldridge 0.1? 221 11.8 - 5? 967 479 -
UNIFCENTROID 0.1? 181 11.0 90.3 5? 897 432 35.9
KDCENTROID B100, MIDPT. 192 22.5 87.9 B530, FRIED. 958 549 35.3
UNIFKDCENTROID 0.1?, B100, MIDPT. 176 13.4 90.3 5?, B530, FRIED. 890 473 34.1
Table 1: Performance on the held-out test sets of GEOWIKI and GEOTEXT, comparing to the results of Wing and
Baldridge (2011) and Eisenstein et al(2011).
is likely to perform very poorly for the 1NN rule we
adopt. It would also require efficient similarity com-
parisons, using techniques such as locality-sensitive
hashing (Kulis and Grauman, 2009).
The graphs in Figure 3 show development set per-
formance when varying bucket size. For GEOWIKI
and UTGEO2011-LARGE (not shown), increments
of 100 were used, but for the smaller GEOTEXT
and UTGEO2011-SMALL, more fine-grained incre-
ments of 10 were used. In the case of plateaus, as
was common with the FRIEDMAN method, we chose
the middle of the plateau as the bucket size. Overall,
we found optimal bucket sizes of 100 for GEOWIKI,
530 for GEOTEXT, 460 for UTGEO2011-SMALL,
and 1050 for UTGEO2011-LARGE. That the
Wikipedia data requires a smaller bucket size is un-
surprising: the documents themselves are generally
longer and there are many more of them, so a small
bucket size provides good coverage and granularity
without sacrificing the ability to estimate good lan-
guage models for each partition.
Node Division Method. The graphs in Fig-
ure 3 also display the difference between the
two splitting methods. MIDPOINT is clearly bet-
ter for GEOWIKI, while FRIEDMAN is better for
GEOTEXT in the range of bucket sizes produc-
ing the best results. FRIEDMAN is best for
UTGEO2011-LARGE (not shown), but MIDPOINT
is best for UTGEO2011-SMALL.
These results only partly confirm our expecta-
tions. We expected FRIEDMAN to perform bet-
ter on smaller datasets, as it distributes the doc-
uments evenly and avoids many sparsity issues.
We expected MIDPOINT to win on larger datasets,
where all nodes receive plentiful data and the k-d
tree would choose more representative geographical
boundaries.
Cell Size. Following W&B, we choose a
cell degree size of 0.1? for GEOWIKI, and a
cell degree size of 5.0? for GEOTEXT. For
UTGEO2011-LARGE and UTGEO2011-SMALL,
we follow the procedure of W&B, trying sizes
0.1?, 0.5?, 1.0?, 5.0?, and 10.0?, selecting the one
which performed best on the development set. For
UTGEO2011-SMALL, this resulted in coarse cells
of 10.0?, while for UTGEO2011-LARGE, cell sizes
of 0.1? were best.
With these tuned parameters, the average num-
ber of training tokens per k-d leaf was approx-
imately 26k for GEOWIKI, 197k for GEOTEXT,
250k for UTGEO2011-SMALL, and 954k for
UTGEO2011-LARGE.
6.2 Held-out Test Sets
Table 1 shows the performance on the test sets of
GEOWIKI and GEOTEXT of the different configu-
rations, along with that of W&B and Eisenstein et
al. (2011) where possible. The results obtained by
W&B on GEOWIKI are already very strong, but we
do see a clear improvement by changing from the
center-based locations for pseudo-documents they
used to the centroid-based locations we employ:
mean error drops from 221 km to 181 km, and me-
dian error from 11.8 km to 11.0 km. Also, we reduce
the mean error further to 176 km for the configu-
ration that combines the uniform grid and the k-d
partitions, though at the cost of increasing median
error somewhat. The 161 km accuracy is around
90% for all configurations, indicating that the gen-
eral language modeling approach we employ is very
1506
Test dataset UTGEO2011
Training dataset UTGEO2011-SMALL UTGEO2011-LARGE
Method Parameters Mean Med. Acc. Parameters Mean Med. Acc.
RANDOM 10? 1975 1833 2.3 0.1? 1627 1381 2.0
MOSTCOMMONCELL 10? 1522 1186 9.3 0.1? 1525 1185 11.8
Wing & Baldridge 10? 1223 825 3.4 0.1? 956 570 30.9
UNIFCENTROID 10? 1147 782 12.3 0.1? 956 570 30.9
KDCENTROID B460, MIDPT. 1098 733 18.1 B1050, FRIED. 860 463 34.6
UNIFKDCENTROID 10?, B460, MIDPT. 1080 723 18.1 0.1?, B1050, FRIED. 913 532 33.0
Table 2: Performance on the held-out test set of UTGEO2011 for different configurations trained on
UTGEO2011-SMALL (comparable in size to GEOTEXT) and UTGEO2011-LARGE. The numbers given for W&B
were produced from their implementation, and correspond to uniform grid partitioning with locations from centers
rather than centroids.
robust for fact-oriented texts that are rich in explicit
toponyms and geographically relevant named enti-
ties.
For GEOTEXT, the results show that the uniform
grid with centroid locations is the most effective of
our configurations. It improves on Eisenstein et al
(2011) by 69 km with respect to median error, but
has 52 km worse performance than their model with
respect to mean error. This indicates that our model
is generally more accurate, but that it is compara-
tively more wildly off on some documents. Their
model is a sophisticated one that attempts to build
detailed models of the geographic linguistic varia-
tion found in the dataset. Dialectal cues are actually
the most powerful ones in the GEOTEXT dataset,
and it seems our general approach of winner-takes-
all (1NN) hurts performance in this respect, espe-
cially with a very small training set.
Table 2 shows the performance on the test set of
UTGEO2011 with the UTGEO2011-SMALL and
UTGEO2011-LARGE training sets. (Performance
for W&B is obtained from their code.4) With
the small training set, error is worse than with
GEOTEXT, reflecting the wider geographic scope of
UTGEO2011. KDCENTROID is much more effec-
tive than the uniform grids, but combining it with the
uniform grid in UNIFKDCENTROID edges it out by
a small amount. More interestingly, KDCENTROID
is the strongest on all measures when using the large
training set, beating UNIFCENTROID by an even
larger margin for mean and median error than with
4https://bitbucket.org/utcompling/
textgrounder/wiki/WingBaldridge2011
the small training set. The bucket size used with the
large training set is double that for the small one,
but there are many more leaves created since there
are 42 times more training documents. With the ex-
tra data, the model is able to adapt better to the dis-
persion of documents and still have strong language
models for each leaf that work well even with our
greedy winner-takes-all decision method.
Note that the accuracy measurements for all
UTGEO2011 experiments are substantially lower
than those reported by Cheng et al(2010), who
report a best accuracy within 100 miles of 51%.
While UTGEO2011-LARGE contains a substan-
tially larger number of tweets, Cheng et al(2010)
limit themselves to users with at least 1,000
tweets, while we have an average of 85 tweets
per user. Their reported mean error distance of
862 km (versus our best mean of 860 km on
UTGEO2011-LARGE) indicates that their perfor-
mance is hurt by a relatively small number of ex-
tremely incorrect guesses, as ours appears to be.
Figure 4 provides a learning curve on
UTGEO2011?s development set for KDCENTROID.
Performance improves greatly with more data,
indicating that GEOTEXT performance would also
improve with more training data. Parameters, espe-
cially bucket size, need retuning as data increases,
which we hope to estimate automatically in future
work
Finally, we note that the KDCENTROID
method was faster than other methods. While
UNIFCENTROID took nearly 19 hours to com-
plete the test run on GEOWIKI (approximately
1507
0e+00 1e+05 2e+05 3e+05 4e+05
900
950
105
0
Training Set Size (# users)
Mea
n E
rror
 (km
)
l
l
l
l
l
l
l
Figure 4: Learning curve of KDCENTROID on the
UTGEO2011 development set.
1.38 seconds per test document), KDCENTROID
took only 80 minutes (.078 s/doc). Similarly,
UNIFCENTROID took about 67 minutes to
run on UTGEO2011-LARGE (0.34 s/doc), but
KDCENTROID took only 27 minutes (0.014 s/doc).
Generally, the KDCENTROID partitioning results
in fewer cells, and therefore fewer KL-divergence
comparisons. As expected, the UNIFKDCENTROID
model needs as much time as the two together,
taking roughly 21 hours for GEOWIKI (1.52 s/doc)
and 85 minutes for UTGEO2011-LARGE (0.36
s/doc).
7 Discussion
7.1 Error Analysis
We examine some of the greatest error distances
to better understand and improve our models. In
many cases, landmarks in Australia or New Zealand
are predicted in European locations with similarly-
named landmarks, or vice versa ? e.g. the Theatre
Royal, Hobart in Australia is predicted to be in Lon-
don?s theater district, and the Embassy of Australia,
Paris is predicted to be in the capital city of Aus-
tralia. Thus, our model may be inadvertently cap-
turing what Clements et al(2010) call wormholes,
places that are related but not necessarily adjacent.
Some of the other large errors stem from incorrect
gold labels, in particular due to sign errors in latitude
or longitude, which can place documents 10,000 or
more km from their correct locations.
Word Error Word Error
paramus 78 6100 130
ludlow 79 figueroa 133
355 99 dundas 138
ctfuuu 101 120th 139
74th 105 mississauga 140
5701 105 pulaski 144
bloomingdale 122 cucina 146
covina 133 56th 153
lawrenceville 122 403 157
ctfuuuu 124 428 161
Table 3: The 20 words with least average error
(km) in the UTGEO2011 development set, trained
on the UTGEO2011-SMALL training set, using the
KDCENTROID approach with our best parameters. Only
words that occur in at least 10 documents are shown.
Word Error Word Error
seniorpastor 1.1 KS01 2.4
prebendary 1.6 Keio 2.5
Wornham 1.7 Vrah 2.5
Owings 1.9 overspill 2.5
Londoners 2.0 Oriel 2.5
Sandringham 2.1 Holywell 2.6
Sheffield?s 2.2 \?vr&h 2.6
Oxford?s 2.2 operetta 2.6
Belair 2.3 Supertram 2.6
Beckton 2.4 Chanel 2.7
Table 4: Top 20 words with the least average er-
ror (km) in the GEOWIKI development set, using the
UNIFKDCENTROID approach with our best parameters.
Only words occurring in at least 10 documents are shown.
7.2 Most Predictive Words
Our approach relies on the idea that the use of certain
words correlates with a Twitter user or Wikipedia
article?s location. To investigate which words tend
to be good indicators of location, we computed, for
each word in a development set, the average error
distance of documents containing that word. Table 3
gives the 20 words with the least error, among
those that occur in at least 10 documents (users),
for the UTGEO2011 development set, trained on
UTGEO2011-SMALL.
Many of the best words are town names (paramus,
ludlow, bloomingdale), street names (74th, figueroa,
1508
120th), area codes (403), and street numbers (5701,
6100). All are highly locatable terms, as we would
expect. Many of the street addresses are due to
check-ins with the location-based social networking
service Foursquare (e.g. the tweet I?m at Starbucks
(7301 164th Ave NE, Redmond Town Center, Red-
mond)), where the user is literally broadcasting his
or her location. The token ctfuuu(u)?an elongation
of the internet abbreviation ctfu, or cracking the fuck
up?is a dialectal or stylistic feature highly indica-
tive of the Washington, D.C. area.
Similarly, several place names (Wornham, Belair,
Holywell) appear in GEOWIKI. Operettas are a cul-
tural phenomenon largely associated with France,
Germany, and England and particularly with specific
theaters in these countries. However, other highly
specific tokens such as KS01 have a very low aver-
age error because they occur in few documents and
are thus highly unambiguous indicators of location.
Other terms, like seniorpastor and \?vr&h, are due
to extraction errors in the dataset created by W&B,
and are carried along because of a high correlation
with specific documents.
8 Conclusion
We have shown how to construct an adaptive grid
with k-d trees that enables robust text geolocation
and scales well to large training sets. It will be inter-
esting to consider how it interacts with other strate-
gies for improving the IR-based approach. For ex-
ample, the pseudo-document word distributions can
be smoothed based on nearby documents or on the
structure of the k-d tree itself. Integrating our system
with topic models or Bayesian methods would likely
provide more insight with regard to the most dis-
criminative and geolocatable words. We also expect
predicting locations based on multiple most similar
documents (kNN) to be more effective in predict-
ing document location, as the second and third most
similar training documents together may sometimes
be a better estimation of its distribution than just the
first alone. Employing k Nearest Neighbors also al-
lows for more sophisticated methods of location es-
timation than a single leaf?s centroid. Other possi-
bilities include constructing multiple k-d trees using
random subsets of the training data to reduce sensi-
tivity to the bucket size.
In this article, we have considered each user in
isolation. However, Liben-Nowell et al(2005) show
that roughly 70% of social network links can be de-
scribed using geographic information and that the
probability of a social link is inversely proportional
to geographic distance. Backstrom et al(2010) ver-
ify these results on a much larger scale using ge-
olocated Facebook profiles: their algorithm geolo-
cates users with only the social graph and signif-
icantly outperforms IP-based geolocation systems.
Given that both Twitter and Wikipedia have rich,
linked document/user graphs, a natural extension to
our work here will be to combine text and network
prediction for geolocation. Sadilek et al(2012)
also show that a combination of textual and so-
cial data can accurately geolocate individual tweets
when scope is limited to a single city.
Tweets are temporally ordered and the geographic
distance between consecutive tweeting events is
constrained by the author?s movement. For tweet-
level geolocation, it will be useful to build on work
in geolocation that considers the temporal dimen-
sion (Chen and Grauman, 2011; Kalogerakis et al
2009; Sadilek et al 2012) to make better predictions
for documents/images that are surrounded by others
with excellent cues, but which are hard to resolve
themselves.
9 Acknowledgments
We would like to thank Matt Lease and the three
anonymous reviewers for their feedback. This re-
search was supported by a grant from the Morris
Memorial Trust Fund of the New York Community
Trust.
References
Richard J. Anderson. 1999. Tree data structures for
n-body simulation. SIAM Journal on Computing,
28(6):1923?1940.
Lars Backstrom, Eric Sun, and Cameron Marlow. 2010.
Find me if you can: improving geographical prediction
with social and spatial proximity. In Proceedings of
the 19th International Conference on World Wide Web,
pages 61?70.
Jon Louis Bentley. 1975. Multidimensional binary
search trees used for associative searching. Commu-
nications of the ACM, 18(9):509?517.
1509
Chao-Yeh Chen and Kristen Grauman. 2011. Clues from
the beaten path: Location estimation with bursty se-
quences of tourist photos. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recogni-
tion, pages 1569?1576.
Zhiyuan Cheng, James Caverlee, and Kyumin Lee. 2010.
You are where you tweet: A content-based approach
to geo-locating twitter users. In Proceedings of the
19th ACM International Conference on Information
and Knowledge Management, pages 759?768.
Martin Clements, Pavel Serdyukov, Arjen P. de Vries, and
Marcel J.T. Reinders. 2010. Finding wormholes with
flickr geotags. In Proceedings of the 32nd European
Conference on Information Retrieval, pages 658?661.
Sebastian Cobarrubias. 2009. Mapping machines: ac-
tivist cartographies of the border and labor lands of
Europe. Ph.D. thesis, University of North Carolina at
Chapel Hill.
Dorin Comaniciu and Peter Meer. 2002. Mean shift: a
robust approach toward feature space analysis. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 24(5):603?619.
Junyan Ding, Luis Gravano, and Narayanan Shivaku-
mar. 2000. Computing geographical scopes of web
resources. In Proceedings of the 26th International
Conference on Very Large Data Bases, pages 545?556.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model
for geographic lexical variation. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1277?1287.
Jacon Eisenstein, Ahmed Ahmed, and Eric P. Xing.
2011. Sparse additive generative models of text. In
Proceedings of the 28th International Conference on
Machine Learning, pages 1041?1048.
Jerome H. Friedman, Jon Louis Bentley, and Raphael Ari
Finkel. 1977. An algorithm for finding best matches
in logarithmic expected time. ACM Transactions on
Mathematical Software, 3:209?226.
James Hays and Alexei A. Efros. 2008. im2gps: esti-
mating geographic information from a single image.
In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 1?8.
Evangelos Kalogerakis, Olga Vesselova, James Hays,
Alexei Efros, and Aaron Hertzmann. 2009. Image se-
quence geolocation with human travel priors. In Pro-
ceedings of the IEEE 12th International Conference on
Computer Vision, pages 253?260.
Sheila Kinsella, Vanessa Murdock, and Neil O?Hare.
2011. ?I?m eating a sandwich in Glasgow?: Model-
ing locations with tweets. In Proceedings of the 3rd
International Workshop on Search and Mining User-
generated Contents, pages 61?68.
Brian Kulis and Kristen Grauman. 2009. Kernelized
locality-sensitive hashing for scalable image search.
In Proceedings of the 12th International Conference
on Computer Vision, pages 2130?2137.
David Liben-Nowell, Jasmine Novak, Ravi Kumar, Prab-
hakar Raghavan, and Andrew Tomkins. 2005. Geo-
graphic routing in social networks. Proceedings of the
National Academy of Sciences of the United States of
America, 102(33):11623?11628.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Adam Sadilek, Henry Kautz, and Jeffrey P. Bigham.
2012. Finding your friends and following them to
where you are. In Proceedings of the 5th ACM Inter-
national Conference on Web Search and Data Mining,
pages 723?732.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings
of the 17th ACM SIGSPATIAL International Confer-
ence on Advances in Geographic Information Systems,
pages 42?51.
Pavel Serdyukov, Vanessa Murdock, and Roelof van
Zwol. 2009. Placing flickr photos on a map. In Pro-
ceedings of the 32nd International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval, pages 484?491.
Benjamin Wing and Jason Baldridge. 2011. Simple su-
pervised document geolocation with geodesic grids.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 955?964.
Chengxiang Zhai and John Lafferty. 2001. A study of
smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334?342.
1510
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466?1476,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Text-Driven Toponym Resolution using Indirect Supervision
Michael Speriosu Jason Baldridge
Department of Linguistics
University of Texas at Austin
Austin, TX 78712 USA
{speriosu,jbaldrid}@utexas.edu
Abstract
Toponym resolvers identify the specific lo-
cations referred to by ambiguous place-
names in text. Most resolvers are based on
heuristics using spatial relationships be-
tween multiple toponyms in a document,
or metadata such as population. This pa-
per shows that text-driven disambiguation
for toponyms is far more effective. We ex-
ploit document-level geotags to indirectly
generate training instances for text classi-
fiers for toponym resolution, and show that
textual cues can be straightforwardly in-
tegrated with other commonly used ones.
Results are given for both 19th century
texts pertaining to the American Civil War
and 20th century newswire articles.
1 Introduction
It has been estimated that at least half of the
world?s stored knowledge, both printed and digi-
tal, has geographic relevance, and that geographic
information pervades many more aspects of hu-
manity than previously thought (Petras, 2004;
Skupin and Esperbe?, 2011). Thus, there is value
in connecting linguistic references to places (e.g.
placenames) to formal references to places (coor-
dinates) (Hill, 2006). Allowing for the querying
and exploration of knowledge in a geographically
informed way requires more powerful tools than a
keyword-based search can provide, in part due to
the ambiguity of toponyms (placenames).
Toponym resolution is the task of disambiguat-
ing toponyms in natural language contexts to geo-
graphic locations (Leidner, 2008). It plays an es-
sential role in automated geographic indexing and
information retrieval. This is useful for histori-
cal research that combines age-old geographic is-
sues like territoriality with modern computational
tools (Guldi, 2009), studies of the effect of histor-
ically recorded travel costs on the shaping of em-
pires (Scheidel et al, 2012), and systems that con-
vey the geographic content in news articles (Teitler
et al, 2008; Sankaranarayanan et al, 2009) and
microblogs (Gelernter and Mushegian, 2011).
Entity disambiguation systems such as those of
Kulkarni et al (2009) and Hoffart et al (2011)
disambiguate references to people and organiza-
tions as well as locations, but these systems do not
take into account any features or measures unique
to geography such as physical distance. Here we
demonstrate the utility of incorporating distance
measurements in toponym resolution systems.
Most work on toponym resolution relies on
heuristics and hand-built rules. Some use sim-
ple rules based on information from a gazetteer,
such as population or administrative level (city,
state, country, etc.), resolving every instance of
the same toponym type to the same location re-
gardless of context (Ladra et al, 2008). Others use
relationships between multiple toponyms in a con-
text (local or whole document) and look for con-
tainment relationships, e.g. London and England
occurring in the same paragraph or as the bigram
London, England (Li et al, 2003; Amitay et al,
2004; Zong et al, 2005; Clough, 2005; Li, 2007;
Volz et al, 2007; Jones et al, 2008; Buscaldi and
Rosso, 2008; Grover et al, 2010). Still others first
identify unambiguous toponyms and then disam-
biguate other toponyms based on geopolitical re-
lationships with or distances to the unambiguous
ones (Ding et al, 2000). Many favor resolutions of
toponyms within a local context or document that
cover a smaller geographic area over those that are
more dispersed (Rauch et al, 2003; Leidner, 2008;
Grover et al, 2010; Loureiro et al, 2011; Zhang
et al, 2012). Roberts et al (2010) use relation-
ships learned between people, organizations, and
locations from Wikipedia to aid in toponym reso-
lution when such named entities are present, but
do not exploit any other textual context.
1466
Most of these approaches suffer from a major
weakness: they rely primarily on spatial relation-
ships and metadata about locations (e.g., popu-
lation). As such, they often require nearby to-
ponyms (including unambiguous or containing to-
ponyms) to resolve ambiguous ones. This reliance
can result in poor coverage when the required in-
formation is missing in the context or when a doc-
ument mentions locations that are neither nearby
geographically nor in a geopolitical relationship.
There is a clear opportunity that most ignore:
use non-toponym textual context. Spatially rel-
evant words like downtown that are not explicit
toponyms can be strong cues for resolution (Hol-
lenstein and Purves, 2012). Furthermore, the con-
nection between non-spatial words and locations
has been successfully exploited in data-driven
approaches to document geolocation (Eisenstein
et al, 2010, 2011; Wing and Baldridge, 2011;
Roller et al, 2012) and other tasks (Hao et al,
2010; Pang et al, 2011; Intagorn and Lerman,
2012; Hecht et al, 2012; Louwerse and Benesh,
2012; Adams and McKenzie, 2013).
In this paper, we learn resolvers that use all
words in local or document context. For example,
the word lobster appearing near the toponym Port-
land indicates the location is Portland in Maine
rather than Oregon or Michigan. Essentially, we
learn a text classifier per toponym. There are no
massive collections of toponyms labeled with lo-
cations, so we train models indirectly using geo-
tagged Wikipedia articles. Our results show these
text classifiers are far more accurate than algo-
rithms based on spatial proximity or metadata.
Furthermore, they are straightforward to combine
with such algorithms and lead to error reductions
for documents that match those algorithms? as-
sumptions.
Our primary focus is toponym resolution, so we
evaluate on toponyms identified by human anno-
tators. However, it is important to consider the
utility of an end-to-end toponym identification and
resolution system, so we also demonstrate that
performance is still strong when toponyms are de-
tected with a standard named entity recognizer.
We have implemented all the models discussed
in this paper in an open source software package
called Fieldspring, which is available on GitHub:
http://github.com/utcompling/fieldspring
Explicit instructions are provided for preparing
data and running code to reproduce our results.
Figure 1: Points representing the United States.
2 Data
2.1 Gazetteer
Toponym resolvers need a gazetteer to obtain can-
didate locations for each toponym. Additionally,
many gazetteers include other information such as
population and geopolitical hierarchy information.
We use GEONAMES, a freely available gazetteer
containing over eight million entries worldwide.1
Each location entry contains a name (sometimes
more than one) and latitude/longitude coordinates.
Entries also include the location?s administrative
level (e.g. city or state) and its position in the
geopolitical hierarchy of countries, states, etc.
GEONAMES gives the locations of regional
items like states, provinces, and countries as single
points. This is clearly problematic when we seek
connections between words and locations: e.g. we
might learn that many words associated with the
USA are connected to a point in Kansas. To get
around this, we represent regional locations as a
set of points derived from the gazetteer. Since re-
gional locations are named in the entries for loca-
tions they contain, all locations contained in the
region are extracted (in some cases over 100,000
of them) and then k-means is run to find a smaller
set of spatial centroids. These act as a tractable
proxy for the spatial extent of the entire region. k
is set to the number of 1? by 1? grid cells covered
by that region. Figure 1 shows the points com-
puted for the United States.2 A nice property of
this representation is that it does not involve re-
gion shape files and the additional programming
infrastructure they require.
1Downloaded April 16, 2013 from www.geonames.
org.
2The representation also contains three points each in
Hawaii and Alaska not shown in Figure 1.
1467
Corpus docs toks types tokstop typestop ambavg ambmax
TRC-DEV 631 136k 17k 4356 613 15.0 857
TRC-DEV-NER - - - 3165 391 18.2 857
TRC-TEST 315 68k 11k 1903 440 13.7 857
TRC-TEST-NER - - - 1346 305 15.7 857
CWAR-DEV 228 33m 200k 157k 850 29.9 231
CWAR-TEST 113 25m 305k 85k 760 31.5 231
Table 1: Statistics of the corpora used for evaluation. Columns subscripted by top give figures for
toponyms. The last two columns give the average number of candidate locations per toponym token and
the number of candidate locations for the most ambiguous toponym.
A location for present purposes is thus a set of
points on the earth?s surface. The distance be-
tween two locations is computed as the great circle
distance between the closest pair of representative
points, one from each location.
2.2 Toponym Resolution Corpora
We need corpora with toponyms identified and re-
solved by human annotators for evaluation. The
TR-CONLL corpus (Leidner, 2008) contains 946
REUTERS news articles published in August
1996. It has about 204,000 words and articles
range in length from a few hundred words to sev-
eral thousand words. Each toponym in the corpus
was identified and resolved by hand.3 We place
every third article into a test portion (TRC-TEST)
and the rest in a development portion. Since our
methods do not learn from explicitly labeled to-
ponyms, we do not need a training set.
The Perseus Civil War and 19th Century Amer-
ican Collection (CWAR) contains 341 books (58
million words) written primarily about and during
the American Civil War (Crane, 2000). Toponyms
were annotated by a semi-automated process: a
named entity recognizer identified toponyms, and
then coordinates were assigned using simple rules
and corrected by hand. We divide CWAR into de-
velopment (CWAR-DEV) and test (CWAR-TEST)
sets in the same way as TR-CONLL.
Table 1 gives statistics for both corpora, includ-
ing the number and ambiguity of gold standard
toponyms for both as well as NER identified to-
3We found several systematic types of errors in the origi-
nal TR-CONLL corpus, such as coordinates being swapped
for some locations and some longitudes being zero or the neg-
ative of their correct values. We repaired many of these er-
rors, though some more idiosyncratic mistakes remain. We,
along with Jochen Leidner, will release this updated version
shortly and will link to it from our Fieldspring GitHub page.
ponyms for TR-CONLL.4 We use the pre-trained
English NER from the OpenNLP project.5
2.3 Geolocated Wikipedia Corpus
The GEOWIKI dataset contains over one million
English articles from the February 11, 2012 dump
of Wikipedia. Each article has human-annotated
latitude/longitude coordinates. We divide the cor-
pus into training (80%), development (10%), and
test (10%) at random and perform preprocessing
to remove markup in the same manner as Wing
and Baldridge (2011). The training portion is used
here to learn models for text-driven resolvers.
3 Toponym Resolvers
Given a set of toponyms provided via annotations
or identified using NER, a resolver must select a
candidate location for each toponym (or, in some
cases, a resolver may abstain). Here, we describe
baseline resolvers, a heuristic resolver based on
the usual cues used in most toponym resolvers,
and several text-driven resolvers. We also discuss
combining heuristic and text-driven resolvers.
3.1 Baseline Resolvers
RANDOM For each toponym, the RANDOM re-
solver randomly selects a location from those as-
sociated in the gazetteer with that toponym.
POPULATION The POPULATION resolver se-
lects the location with the greatest population for
each toponym. It is generally quite effective, but
when a toponym has several locations with large
populations, it is often wrong. Also, it can only be
used when such information is available, and it is
4States and countries are not annotated in CWAR, so we
do not evaluate end-to-end using NER plus toponym resolu-
tion for it as there are many (falsely) false positives.
5opennlp.apache.org
1468
less effective if the population statistics are from a
time period different from that of the corpus.
3.2 SPIDER
Leidner (2008) describes two general and useful
minimality properties of toponyms:
? one sense per discourse: multiple tokens of
a toponym in the same text generally do not
refer to different locations in the same text
? spatial minimality: different toponyms in a
text tend refer to spatially near locations
Many toponym resolvers exploit these (Smith and
Crane, 2001; Rauch et al, 2003; Leidner, 2008;
Grover et al, 2010; Loureiro et al, 2011; Zhang
et al, 2012). Here, we define SPIDER (Spatial
Prominence via Iterative Distance Evaluation and
Reweighting) as a strong representative of such
textually unaware approaches. In addition to cap-
turing both minimality properties, it also identifies
the relative prominence of the locations for each
toponym in a given corpus.
SPIDER resolves each toponym by finding the
location for each that minimizes the sum distance
to all locations for all other toponyms in the same
document. On the first iteration, it tends to select
locations that clump spatially: if Paris occurs with
Dallas, it will choose Paris, Texas even though the
topic may be a flight from Texas to France. Further
iterations bring Paris, France into focus by captur-
ing its prominence across the corpus. The key in-
tuition is that most documents will discuss Paris,
France and only a small portion of these mention
places close to Paris, Texas; thus, Paris, France
will be selected on the first iteration for many
documents (though not for the Dallas document).
SPIDER thus assigns each candidate location a
weight (initialized to 1.0), which is re-estimated
on each iteration. The adjusted distance between
two locations is computed as the great circle dis-
tance divided by the product of the two locations?
weights. At the end of an iteration, each candi-
date location?s weight is updated to be the frac-
tion of the times it was chosen times the number
of candidates for that toponym. The weights are
global, with one for each location in the gazetteer,
so the same weight vector is used for each token
of a given toponym on a given iteration.
For example, if after the first iteration Paris,
France is chosen thrice, Paris, Texas once, and
Paris, Arkansas never, the global weights of these
locations are (3/4)?3=2.25, (1/4)?3=.75, and
(0/4)?3=0, respectively (assume, for the exam-
ple, there are no other locations named Paris). The
sum of the weights remains equal to the number
of candidate locations. The updated weights are
used on the next iteration, so Paris, France will
seem ?closer? since any distance computed to it
is divided by a number greater than one. Paris,
Texas will seem somewhat further away, and Paris,
Arkansas infinitely far away. The algorithm con-
tinues for a fixed number of iterations or until the
weights do not change more than some thresh-
old. Here, we run SPIDER for 10 iterations; the
weights have generally converged by this point.
When only one toponym is present in a doc-
ument, we simply select the candidate with the
greatest weight. When there is no such weight in-
formation, such as when the toponym does not co-
occur with other toponyms anywhere in the cor-
pus, we select a candidate at random.
SPIDER captures prominence, but we stress it
is not our main innovation: its purpose is to be a
benchmark for text-driven resolvers to beat.
3.3 Text-Driven Resolvers
The text-driven resolvers presented in this section
all use local context windows, document context,
or both, to inform disambiguation.
TRIPDL We use a document geolocator
trained on GEOWIKI?s document location labels.
Others?such as Smith and Crane (2001)?have
estimated a document-level location to inform
toponym resolution, but ours is the first we are
aware of to use training data from a different
domain to build a document geolocator that uses
all words (not only toponyms) to estimate a
document?s location. We use the document geolo-
cation method of Wing and Baldridge (2011). It
discretizes the earth?s surface into 1? by 1? grid
cells and assigns Kullback-Liebler divergences to
each cell given a document, based on language
models learned for each cell from geolocated
Wikipedia articles. We obtain the probability of a
cell c given a document d by the standard method
of exponentiating the negative KL-divergence and
normalizing these values over all cells:
P (c|d) = exp(?KL(c, d))?
c? exp(?KL(c?, d))
This distribution is used for all toponyms t in d
to define distributions PDL(l|t, d) over candidate
1469
locations of t in document d to be the portion of
P (c|d) consistent with the t?s candidate locations:
PDL(l|t, d) =
P (cl|d)?
l??G(t) P (cl? |d)
where G(t) is the set of the locations for t in the
gazetteer, and cl is the cell containing l. TRIPDL
(Toponym Resolution Informed by Predicted Doc-
ument Locations) chooses the location that maxi-
mizes PDL.
WISTR While TRIPDL uses an off-the-shelf
document geolocator to capture the geographic
gist of a document, WISTR (Wikipedia Indirectly
Supervised Toponym Resolver) instead directly
targets each toponym. It learns text classifiers
based on local context window features trained on
instances automatically extracted from GEOWIKI.
To create the indirectly supervised training data
for WISTR, the OpenNLP named entity recog-
nizer detects toponyms in GEOWIKI, and can-
didate locations for each toponym are retrieved
from GEONAMES. Each toponym with a loca-
tion within 10km of the document location is con-
sidered a mention of that location. For example,
the Empire State Building Wikipedia article has a
human-provided location label of (40.75,-73.99).
The toponym New York is mentioned several times
in the article, and GEONAMES lists a New York at
(40.71,-74.01). These points are 4.8km apart, so
each mention of New York in the document is con-
sidered a reference to New York City.
Next, context windows w of twenty words to
each side of each toponym are extracted as fea-
tures. The label for a training instance is the
candidate location closest to the document loca-
tion. We extract 1,489,428 such instances for to-
ponyms relevant to our evaluation corpora. These
instances are used to train logistic regression clas-
sifiers P (l|t, w) for location l and toponym t. To
disambiguate a new toponym, WISTR chooses
the location that maximizes this probability.
Few such probabilistic toponym resolvers ex-
ist in the literature. Li (2007) builds a probabil-
ity distribution over locations for each toponym,
but still relies on nearby toponyms that could refer
to regions that contain that toponym and requires
hand construction of distributions. Other learn-
ing approaches to toponym resolution (e.g. Smith
and Mann (2003)) require explicit unambiguous
mentions like Portland, Maine to construct train-
ing instances, while our data gathering methodol-
ogy does not make such an assumption. Overell
and Ru?ger (2008) and Overell (2009) only use
nearby toponyms as features. Mani et al (2010)
and Qin et al (2010) use other word types but
only in a local context, and they require toponym-
labeled training data. Our approach makes use of
all words in local and document context and re-
quires no explicitly labeled toponym tokens.
TRAWL We bring TRIPDL, WISTR, and
standard toponym resolution cues about ad-
ministrative levels together with TRAWL (To-
ponym Resolution via Administrative levels and
Wikipedia Locations). The general form of a prob-
abilistic resolver that utilizes such information to
select a location l? for a toponym t in document d
may be defined as
l? = argmaxl P (l, al|t, d).
where al is the administrative level (country, state,
city) for l in the gazetteer. This captures the fact
that countries (like Sudan) tend to be referred to
more often than small cities (like Sudan, Texas).
The above term is simplified as follows:
P (l, al|t, d) = P (al|t, d)P (l|al, t, d)
? P (al|t)P (l|t, d)
where we approximate the administrative level
prediction as independent of the document, and
the location as independent of administrative level.
The latter term is then expressed as a linear combi-
nation of the local context (WISTR) and the doc-
ument context (TRIPDL):
P (l|t, d) = ?tP (l|t, ct) + (1??t)PDL(l|t, d).
?t, the weight of the local context distribution, is
set according to the confidence that a prediction
based on local context is correct:
?t = f(t)f(t)+C ,
where f(t) is the fraction of training instances
of toponym t of all instances extracted from
GEOWIKI. C is set experimentally; C=.0001 was
the optimal value for CWAR-DEV. Intuitively, the
larger C is, the greater f(t) must be for the local
context to be trusted over the document context.
We define P (a|t), the administrative level com-
ponent, to be the fraction of representative points
for a location l? out of the number of representa-
tives points for all candidate locations l ? t,
||Rl?||?
l??t ||Rl? ||
1470
where ||Rl|| is the number of representative points
of l. This boosts states and countries since higher
probability is assigned to locations with more
points (and cities have just one point).
Taken together, the above definitions yield the
TRAWL resolver, which selects the optimal can-
didate location l? according to
l? = argmaxl P (al|t)(?tP (l|t, ct) + (1??t)PDL(l|t, d)).
3.4 Combining Resolvers and Backoff
SPIDER begins with uniform weights for each
candidate location of each toponym. WISTR
and TRAWL both output distributions over these
locations based on outside knowledge sources,
and can be used as more informed initializa-
tions of SPIDER than the uniform ones. We
call these combinations WISTR+SPIDER and
TRAWL+SPIDER.6
WISTR fails to predict when encountering a
toponym it has not seen in the training data, and
TRIPDL fails when a toponym only has locations
in cells with no probability mass. TRAWL fails
when both of these are true. In these cases, we
select the candidate location geographically clos-
est to the most likely cell according to TRIPDL?s
P (c|d) distribution.
3.5 Document Size
For SPIDER, runtime is quadratic in the size
of documents, so breaking up documents vastly
reduces runtime. It also restricts the minimal-
ity heuristic?appropriately?to smaller spans of
text. For resolvers that take into account the sur-
rounding document when determining how to re-
solve a toponym, such as TRIPDL and TRAWL,
it can often be beneficial to divide documents into
smaller subdocuments in order to get a better esti-
mate of the overall geographic prominence of the
text surrounding a toponym, but at a more coarse-
grained level than the local context models pro-
vide. For these reasons, we simply divide each
book in the CWAR corpus into small subdocu-
ments of at most 20 sentences.
4 Evaluation
Many prior efforts use a simple accuracy metric:
the fraction of toponyms whose predicted location
6We scale each toponym?s distribution as output by
WISTR or TRAWL by the number of candidate locations
for that toponym, since the total weight for each toponym in
SPIDER is the number of candidate locations, not 1.
is the same as the gold location. Such a met-
ric can be problematic, however. The gazetteer
used by a resolver may not contain, for a given
toponym, a location whose latitude and longitude
exactly match the gold label for the toponym (Lei-
dner, 2008). Also, some errors are worse than oth-
ers, e.g. predicting a toponym?s location to be on
the other side of the world versus predicting it to
be a different city in the same country?accuracy
does not reflect this difference.
We choose a metric that instead measures the
distance between the correct and predicted loca-
tion for each toponym and compute the mean and
median of all such error distances. This is used
in document geolocation work (Eisenstein et al,
2010, 2011; Wing and Baldridge, 2011; Roller
et al, 2012) and is related to the root mean squared
distance metric discussed by Leidner (2008).
It is important to understand performance on
plain text (without gold toponyms), which is the
typical use case for applications using toponym
resolvers. Both the accuracy metric and the error-
distance metric encounter problems when the set
of predicted toponyms is not the same as the set
of gold toponyms (regardless of locations), e.g.
when a named entity recognizer is used to iden-
tify toponyms. In this case, we can use precision
and recall, where a true positive is defined as the
prediction of a correctly identified toponym?s lo-
cation to be as close as possible to its gold la-
bel, given the gazetteer used. False positives oc-
cur when the NER incorrectly predicts a toponym,
and false negatives occur when it fails to predict a
toponym identified by the annotator. When a cor-
rectly identified toponym receives an incorrect lo-
cation prediction, this counts as both a false nega-
tive and a false positive. We primarily present re-
sults from experiments with gold toponyms but in-
clude an accuracy measure for comparability with
results from experiments run on plain text with
a named entity recognizer. This accuracy met-
ric simply computes the fraction of toponyms that
were resolved as close as possible to their gold la-
bel given the gazetteer.
5 Results
Table 2 gives the performance of the resolvers
on the TR-CONLL and CWAR test sets when
gold toponyms are used. Values for RANDOM
and SPIDER are averaged over three trials. The
ORACLE row gives results when the candidate
1471
Resolver TRC-TEST CWAR-TEST
Mean Med. A Mean Med. A
ORACLE 105 19.8 100.0 0.0 0.0 100.0
RANDOM 3915 1412 33.5 2389 1027 11.8
POPULATION 216 23.1 81.0 1749 0.0 59.7
SPIDER10 2180 30.9 55.7 266 0.0 57.5
TRIPDL 1494 29.3 62.0 847 0.0 51.5
WISTR 279 22.6 82.3 855 0.0 69.1
WISTR+SPIDER10 430 23.1 81.8 201 0.0 85.9
TRAWL 235 22.6 81.4 945 0.0 67.8
TRAWL+SPIDER10 297 23.1 80.7 148 0.0 78.2
Table 2: Accuracy and error distance metrics on test sets with gold toponyms.
Figure 2: Visualization of how SPIDER clumps
most predicted locations in the same region
(above), on the CWAR-DEV corpus. TRAWL?s
output (below) is much more dispersed.
from GEONAMES closest to the annotated loca-
tion is always selected. The ORACLE mean and
median error values on TR-CONLL are nonzero
due to errors in the annotations and inconsisten-
cies stemming from the fact that coordinates from
GEONAMES were not used in the annotation of
TR-CONLL.
On both datasets, SPIDER achieves errors and
accuracies much better than RANDOM, validating
the intuition that authors tend to discuss places
near each other more often than not, while some
locations are more prominent in a given corpus
despite violating the minimality heuristic. The
text-driven resolvers vastly outperform SPIDER,
showing the effectiveness of textual cues for to-
ponym resolution.
The local context resolver WISTR is very
effective: it has the highest accuracy for
TR-CONLL, though two other text-based re-
solvers also beat the challenging POPULATION
baseline?s accuracy. TRAWL achieves a better
mean distance metric for TR-CONLL, and when
used to seed SPIDER, it obtains the lowest mean
error on CWAR by a large margin. SPIDER
seeded with WISTR achieves the highest accu-
racy on CWAR. The overall geographic scope
of CWAR, a collection of documents about the
American Civil War, is much smaller than that of
TR-CONLL (articles about international events).
This makes toponym resolution easier overall (es-
pecially error distances) for minimality resolvers
like SPIDER, which primarily seek tightly clus-
tered sets of locations. This behavior is quite
clear in visualizations of predicted locations such
as Figure 2.
On the CWAR dataset, POPULATION performs
relatively poorly, demonstrating the fragility of
population-based decisions for working with his-
torical corpora. (Also, we note that POPULATION
is not a resolver per se since it only ever predicts
one location for a given toponym, regardless of
context.)
Table 3 gives results on TRC-TEST when NER-
identified toponyms are used. In this case, the
ORACLE results are less than 100% due to the lim-
itations of the NER, and represent the best possible
results given the NER we used.
When resolvers are run on NER-identified to-
ponyms, the text-driven resolvers that use lo-
cal context again easily beat SPIDER. WISTR
achieves the best performance. The named en-
tity recognizer is likely better at detecting com-
mon toponyms than rare toponyms due to the na-
1472
Resolver P R F
ORACLE 82.6 59.9 69.4
RANDOM 25.1 18.2 21.1
POPULATION 71.6 51.9 60.2
SPIDER10 40.5 29.4 34.1
TRIPDL 51.8 37.5 43.5
WISTR 73.9 53.6 62.1
WISTR+SPIDER10 73.2 53.1 61.5
TRAWL 72.5 52.5 60.9
TRAWL+SPIDER10 72.0 52.2 60.5
Table 3: Precision, recall, and F-score of resolvers
on TRC-TEST with NER-identified toponyms.
ture of its training data, and many more local con-
text training instances were extracted from com-
mon toponyms than from rare ones in Wikipedia.
Thus, our model that uses only these local context
models does best when running on NER-identified
toponyms. We also measured the mean and me-
dian error distance for toponyms correctly identi-
fied by the named entity recognizer, and found that
they tended to be 50-200km worse than for gold
toponyms. This also makes sense given the named
entity recognizer?s tendency to detect common to-
ponyms: common toponyms tend to be more am-
biguous than others.
Results on TR-CONLL indicate much higher
performance than the resolvers presented by Lei-
dner (2008), whose F-scores do not exceed 36.5%
with either gold or NER toponyms.7 TRC-TEST
is a subset of the documents Leidner uses (he did
not split development and test data), but the results
still come from overlapping data. The most direct
comparison is SPIDER?s F-score of 39.7% com-
pared to his LSW03 algorithm?s 35.6% (both are
minimality resolvers). However, our evaluation is
more penalized since SPIDER loses precision for
NER?s false positives (Jack London as a location)
while Leidner only evaluated on actual locations.
It thus seems fair to conclude that the text-driven
classifiers, with F-scores in the mid-50?s, are much
more accurate on the corpus than previous work.
6 Error Analysis
Table 4 shows the ten toponyms that caused the
greatest total error distances from TRC-DEV with
gold toponyms when resolved by TRAWL, the re-
solver that achieves the lowest mean error on that
7Leidner (2008) reports precision, recall, and F-score val-
ues even with gold toponyms, since his resolvers can abstain.
dataset among all our resolvers.
Washington, the toponym contributing the most
total error, is a typical example of a toponym that
is difficult to resolve, as there are two very promi-
nent locations within the United States with the
name. Choosing one when the other is correct re-
sults in an error of over 4000 kilometers. This oc-
curs, for example, when TRAWL chooses Wash-
ington state in the phrase Israel?s ambassador to
Washington, where more knowledge about the
status of Washington, D.C. as the political cen-
ter of the United States (e.g. in the form of more
or better contextual training instances) could over-
turn the administrative level component?s prefer-
ence for states.
An instance of California in a baseball-related
news article is incorrectly predicted to be the town
California, Pennsylvania. The context is: ...New
York starter Jimmy Key left the game in the first
inning after Seattle shortstop Alex Rodriguez lined
a shot off his left elbow. The Yankees have lost
12 of their last 19 games and their lead in the AL
East over Baltimore fell to five games. At Califor-
nia, Tim Wakefield pitched a six-hitter for his third
complete game of the season and Mo Vaughn and
Troy O?Leary hit solo home runs in the second in-
ning as the surging Boston Red Sox won their third
straight 4-1 over the California Angels. Boston
has won seven of eight and is 20-6... The pres-
ence of many east coast cues?both toponym and
otherwise?make it unsurprising that the resolver
would predict California, Pennsylvania despite the
administrative level component?s heavier weight-
ing of the state.
The average errors for the toponyms Australia
and Russia are fairly small and stem from differ-
ences in how countries are represented across dif-
ferent gazetteers, not true incorrect predictions.
Table 5 shows the toponyms with the great-
est errors from CWAR-DEV with gold toponyms
when resolved by WISTR+SPIDER. Rome is
sometimes predicted as cities in Italy and other
parts of Europe rather than Rome, Georgia, though
it correctly selects the city in Georgia more often
than not due to SPIDER?s preference for tightly
clumped sets of locations. Mexico, however, fre-
quently gets incorrectly selected as a city in Mary-
land near many other locations in the corpus when
TRAWL?s administrative level component is not
present. Many other of the toponyms contributing
to the total error such as Jackson and Lexington are
1473
Toponym N Mean Total
Washington 25 3229 80717
Gaza 12 5936 71234
California 8 5475 43797
Montana 3 11635 34905
WA 3 11221 33662
NZ 2 14068 28136
Australia 88 280 24600
Russia 72 260 18712
OR 2 9242 18484
Sydney 12 1422 17067
Table 4: Toponyms with the greatest total error
distances in kilometers from TRC-DEV with gold
toponyms resolved by TRAWL. N is the number
of instances, and the mean error for each toponym
type is also given.
Toponym N Mean Total
Mexico 1398 2963 4142102
Jackson 2485 1210 3007541
Monterey 353 2392 844221
Haymarket 41 15663 642170
McMinnville 145 3307 479446
Alexandria 1434 314 450863
Eastport 184 2109 388000
Lexington 796 442 351684
Winton 21 15881 333499
Clinton 170 1401 238241
Table 5: Top errors from CWAR-DEV resolved by
TRAWL+SPIDER.
simply the result of many American towns sharing
the same names and a lack of clear disambiguating
context.
7 Conclusion
Our text-driven resolvers prove highly effective
for both modern day newswire texts and 19th cen-
tury texts pertaining to the Civil War. They eas-
ily outperform standard minimality toponym re-
solvers, but can also be combined with them. This
strategy works particularly well when predicting
toponyms on a corpus with relatively restricted
geographic extents. Performance remains good
when resolving toponyms identified automatically,
indicating that end-to-end systems based on our
models may improve the experience of digital hu-
manities scholars interested in finding and visual-
izing toponyms in large corpora.
Acknowledgements
We thank: the three anonymous reviewers, Grant
DeLozier, and the UT Austin Natural Language
Learning reading group, for their helpful feed-
back; Ben Wing, for his document geoloca-
tion software; Jochen Leidner, for providing the
TR-CONLL corpus as well as feedback on earlier
versions of this paper; and Scott Nesbit, for pro-
viding the annotations for the CWAR corpus. This
research was supported by a grant from the Morris
Memorial Trust Fund of the New York Commu-
nity Trust.
References
B. Adams and G. McKenzie. Inferring thematic
places from spatially referenced natural lan-
guage descriptions. Crowdsourcing Geographic
Knowledge, pages 201?221, 2013.
E. Amitay, N. Har?El, R. Sivan, and A. Soffer.
Web-a-Where: geotagging web content. In Pro-
ceedings of the 27th annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 273?280, 2004.
D. Buscaldi and P. Rosso. A conceptual density-
based approach for the disambiguation of to-
ponyms. International Journal of Geographical
Information Science, 22(3):301?313, 2008.
P. Clough. Extracting metadata for spatially-
aware information retrieval on the internet. In
Proceedings of the 2005 workshop on Ge-
ographic information retrieval, pages 25?30.
ACM, 2005.
G. Crane. The Perseus Digital Library, 2000. URL
http://www.perseus.tufts.edu.
J. Ding, L. Gravano, and N. Shivakumar. Comput-
ing geographical scopes of web resources. In
Proceedings of the 26th International Confer-
ence on Very Large Data Bases, pages 545?556,
2000.
J. Eisenstein, B. O?Connor, N. Smith, and E. Xing.
A latent variable model for geographic lexical
variation. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1277?1287, 2010.
J. Eisenstein, A. Ahmed, and E. Xing. Sparse ad-
ditive generative models of text. In Proceedings
of the 28th International Conference on Ma-
chine Learning, pages 1041?1048, 2011.
1474
J. Gelernter and N. Mushegian. Geo-parsing mes-
sages from microtext. Transactions in GIS, 15
(6):753?773, 2011.
C. Grover, R. Tobin, K. Byrne, M. Woollard,
J. Reid, S. Dunn, and J. Ball. Use of the Ed-
inburgh geoparser for georeferencing digitized
historical collections. Philosophical Transac-
tions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 368(1925):
3875?3889, 2010.
J. Guldi. The spatial turn. Spatial Humanities: a
Project of the Institute for Enabling, 2009.
Q. Hao, R. Cai, C. Wang, R. Xiao, J. Yang,
Y. Pang, and L. Zhang. Equip tourists with
knowledge mined from travelogues. In Pro-
ceedings of the 19th international conference on
World wide web, pages 401?410, 2010.
B. Hecht, S. Carton, M. Quaderi, J. Scho?ning,
M. Raubal, D. Gergle, and D. Downey. Ex-
planatory semantic relatedness and explicit spa-
tialization for exploratory search. In Proceed-
ings of the 35th international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, pages 415?424. ACM, 2012.
L. Hill. Georeferencing: The Geographic Associ-
ations of Information. MIT Press, 2006.
J. Hoffart, M. Yosef, I. Bordino, H. Fu?rstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. Robust disambiguation of named
entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 782?792. Association
for Computational Linguistics, 2011.
L. Hollenstein and R. Purves. Exploring place
through user-generated content: Using Flickr
tags to describe city cores. Journal of Spatial
Information Science, (1):21?48, 2012.
S. Intagorn and K. Lerman. A probabilistic ap-
proach to mining geospatial knowledge from
social annotations. In Conference on Infor-
mation and Knowledge Management (CIKM),
2012.
C. Jones, R. Purves, P. Clough, and H. Joho. Mod-
elling vague places with knowledge from the
web. International Journal of Geographical In-
formation Science, 2008.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. Collective annotation of
Wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining,
pages 457?466. ACM, 2009.
S. Ladra, M. Luaces, O. Pedreira, and D. Seco. A
toponym resolution service following the OGC
WPS standard. In Web and Wireless Geograph-
ical Information Systems, volume 5373, pages
75?85. 2008.
J. Leidner. Toponym resolution in text: Anno-
tation, Evaluation and Applications of Spatial
Grounding of Place Names. Universal Press,
Boca Raton, FL, USA, 2008.
H. Li, R. Srihari, C. Niu, and W. Li. InfoXtract lo-
cation normalization: a hybrid approach to geo-
graphic references in information extraction. In
Proceedings of the HLT-NAACL 2003 workshop
on Analysis of geographic references - Volume
1, pages 39?44, 2003.
Y. Li. Probabilistic toponym resolution and geo-
graphic indexing and querying. Master?s thesis,
The University of Melbourne, Melbourne, Aus-
tralia, 2007.
V. Loureiro, I. Anasta?cio, and B. Martins. Learn-
ing to resolve geographical and temporal ref-
erences in text. In Proceedings of the 19th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 349?352, 2011.
M. Louwerse and N. Benesh. Representing spatial
structure through maps and language: Lord of
the Rings encodes the spatial structure of Mid-
dle Earth. Cognitive science, 36(8):1556?1569,
2012.
I. Mani, C. Doran, D. Harris, J. Hitzeman,
R. Quimby, J. Richer, B. Wellner, S. Mardis,
and S. Clancy. SpatialML: annotation scheme,
resources, and evaluation. Language Resources
and Evaluation, 44(3):263?280, 2010.
S. Overell. Geographic Information Retrieval:
Classification, Disambiguation and Modelling.
PhD thesis, Imperial College London, 2009.
S. Overell and S. Ru?ger. Using co-occurrence
models for placename disambiguation. Inter-
national Journal of Geographical Information
Science, 22:265?287, 2008.
Y. Pang, Q. Hao, Y. Yuan, T. Hu, R. Cai, and
L. Zhang. Summarizing tourist destinations
by mining user-generated travelogues and pho-
1475
tos. Computer Vision and Image Understand-
ing, 115(3):352 ? 363, 2011.
V. Petras. Statistical analysis of geographic and
language clues in the MARC record. Technical
report, The University of California at Berkeley,
2004.
T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang.
An efficient location extraction algorithm by
leveraging web contextual information. In Pro-
ceedings of the 18th SIGSPATIAL International
Conference on Advances in Geographic Infor-
mation Systems, pages 53?60. ACM, 2010.
E. Rauch, M. Bukatin, and K. Baker. A
confidence-based framework for disambiguat-
ing geographic terms. In Proceedings of the
HLT-NAACL 2003 workshop on Analysis of ge-
ographic references - Volume 1, pages 50?54,
2003.
K. Roberts, C. Bejan, and S. Harabagiu. Toponym
disambiguation using events. In Proceedings of
the 23rd International Florida Artificial Intelli-
gence Research Society Conference, pages 271?
276, 2010.
S. Roller, M. Speriosu, S. Rallapalli, B. Wing, and
J. Baldridge. Supervised text-based geolocation
using language models on an adaptive grid. In
Proceedings of EMNLP 2012, 2012.
J. Sankaranarayanan, H. Samet, B. Teitler,
M. Lieberman, and J. Sperling. TwitterStand:
news in tweets. In Proceedings of the 17th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 42?51, 2009.
W. Scheidel, E. Meeks, and J. Weiland. ORBIS:
The Stanford geospatial network model of the
roman world. 2012.
A. Skupin and A. Esperbe?. An alternative map
of the United States based on an n-dimensional
model of geographic space. Journal of Vi-
sual Languages & Computing, 22(4):290?304,
2011.
D. Smith and G. Crane. Disambiguating geo-
graphic names in a historical digital library. In
Proceedings of the 5th European Conference on
Research and Advanced Technology for Digital
Libraries, pages 127?136, 2001.
D. Smith and G. Mann. Bootstrapping toponym
classifiers. In Proceedings of the HLT-NAACL
2003 workshop on Analysis of geographic ref-
erences - Volume 1, pages 45?49, 2003.
B. Teitler, M. Lieberman, D. Panozzo, J. Sankara-
narayanan, H. Samet, and J. Sperling. News-
Stand: a new view on news. In Proceedings of
the 16th ACM SIGSPATIAL international con-
ference on Advances in geographic information
systems, page 18. ACM, 2008.
R. Volz, J. Kleb, and W. Mueller. Towards
ontology-based disambiguation of geographical
identifiers. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, 2007.
B. Wing and J. Baldridge. Simple supervised doc-
ument geolocation with geodesic grids. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Hu-
man Language Technologies, pages 955?964,
2011.
Q. Zhang, P. Jin, S. Lin, and L. Yue. Extracting
focused locations for web pages. In Web-Age
Information Management, volume 7142, pages
76?89. 2012.
W. Zong, D. Wu, A. Sun, E. Lim, and D. Goh. On
assigning place names to geography related web
pages. In Proceedings of the 5th ACM/IEEE-
CS joint conference on Digital libraries, pages
354?362, 2005.
1476
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 53?63,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Twitter Polarity Classification with Label Propagation
over Lexical Links and the Follower Graph
Michael Speriosu
University of Texas at Austin
speriosu@mail.utexas.edu
Nikita Sudan
University of Texas at Austin
nsudan@utexas.edu
Sid Upadhyay
University of Texas at Austin
sid.upadhyay@utexas.edu
Jason Baldridge
University of Texas at Austin
jbaldrid@mail.utexas.edu
Abstract
There is high demand for automated tools that
assign polarity to microblog content such as
tweets (Twitter posts), but this is challenging
due to the terseness and informality of tweets
in addition to the wide variety and rapid evolu-
tion of language in Twitter. It is thus impracti-
cal to use standard supervised machine learn-
ing techniques dependent on annotated train-
ing examples. We do without such annota-
tions by using label propagation to incorpo-
rate labels from a maximum entropy classifier
trained on noisy labels and knowledge about
word types encoded in a lexicon, in combina-
tion with the Twitter follower graph. Results
on polarity classification for several datasets
show that our label propagation approach ri-
vals a model supervised with in-domain an-
notated tweets, and it outperforms the nois-
ily supervised classifier it exploits as well as
a lexicon-based polarity ratio classifier.
1 Introduction
Twitter is a microblogging service where users post
messages (?tweets?) of no more than 140 charac-
ters. With around 200 million users generating 140
million tweets per day, Twitter represents one of the
largest and most dynamic datasets of user generated
content. Along with other social networking web-
sites such as Facebook, the content on Twitter is real
time: tweets about everything from a friend?s birth-
day to a devastating earthquake can be found posted
during and immediately after an event in question.
This vast stream of real time data has major im-
plications for any entity interested in public opin-
ion and even acting on what is learned and engag-
ing with the public directly. Companies have the
opportunity to examine what customers and poten-
tial customers are saying about their products and
services without costly and time-consuming surveys
or explicit requests for feedback. Political organi-
zations and candidates might be able to determine
what issues the public is most interested in, as well
as where they stand on those issues. Manual inspec-
tion of tweets can be useful for many such analyses,
but many applications and questions require real-
time analysis of massive amounts of social media
content. Computational tools that automatically ex-
tract and analyze relevant information about opinion
expressed on Twitter and other social media sources
are thus in high demand.
Full sentiment analysis for a given question or
topic requires many stages, including but not lim-
ited to: (1) extraction of tweets based on an ini-
tial query, (2) filtering out spam and irrelevant items
from those tweets, (3) identifying subjective tweets,
and (4) identifying the polarity of those tweets. Like
most work in sentiment analysis, we focus on the
last stage, polarity classification. The simplest ap-
proaches are based on the presence of words or
emoticons that are indicators of positive or nega-
tive polarity (e.g. Twitter?s own API, O?Connor
et al (2010)), or calculating a ratio of positive to
negative terms (Choi and Cardie, 2009). Though
these are a useful first pass, the nuance of lan-
guage often defeats them (Pang and Lee, 2008).
Tweets provide additional challenges compared to
edited text; e.g. they are short and include infor-
mal/colloquial/abbreviated language.
53
Standard supervised classification methods im-
prove the situation somewhat (Pang et al, 2002),
but these require texts labeled with polarity as in-
put and they do not adapt to changes in language
use. One way around this is to use noisy labels (also
referred to as ?distant supervision?), e.g. by tak-
ing emoticons like ?:)? as positive and ?:(? as neg-
ative, and train a standard classifier (Read, 2005; Go
et al, 2009).1 Semi-supervised methods can also
reduce dependence on labeled texts: for example,
Sindhwani and Melville (2008) use a polarity lexi-
con combined with label propagation. Several have
used label propagation starting with a small number
of hand-labeled words to induce a lexicon for use
in polarity classification (Blair-Goldensohn et al,
2008; Rao and Ravichandran, 2009; Brody and El-
hadad, 2010).
In this paper, we bring together several of the
above approaches via label propagation using modi-
fied adsorption (Talukdar and Crammer, 2009). This
also allows us to explore the possibility of exploit-
ing the Twitter follower graph to improve polarity
classification, under the assumption that people in-
fluence one another or have shared affinities about
topics. We construct a graph that has users, tweets,
word unigrams, word bigrams, hashtags, and emoti-
cons as its nodes; users are connected based on the
Twitter follower graph, users are connected to the
tweets they created, and tweets are connected to
the unigrams, bigrams, hashtags and emoticons they
contain. We seed the graph using the polarity values
in the OpinionFinder lexicon (Wilson et al, 2005),
the known polarity of emoticons, and a maximum
entropy classifier trained on 1.8 million tweets with
automatically assigned labels based on the presence
of positive and negative emoticons, like Read (2005)
and Go et al (2009).
We compare the label propagation approach to
the noisily supervised classifier itself and to a stan-
dard lexicon-based method using positive/negative
ratios. Evaluation is performed on several datasets
of tweets that have been annotated for polarity: the
Stanford Twitter Sentiment set (Go et al, 2009),
1Davidov et al (2010) use 15 emoticons and 50 Twitter
hashtags as proxies for sentiment in a similar manner, but their
evaluation is indirect. Rather than predicting gold standard sen-
timent labels, they instead predict whether those same emoti-
cons and hashtags would be appropriate for other tweets.
tweets from the 2008 debate between Obama and
McCain (Shamma et al, 2009), and a new dataset
of tweets about health care reform that we have cre-
ated. In addition to performing standard per-tweet
accuracy, we also measure per-target accuracy (for
health care reform) and an aggregate error metric
over all users in our test set that captures how simi-
lar predicted positivity of each user is to their actual
positivity. Across all datasets and measures, we find
that label propagation is consistently better than the
noisily supervised classifier, which in turn outper-
forms the lexicon-based method. Additionally, for
the health care reform dataset, the label propagation
approach?which uses no gold labeled tweets, just a
hand-created lexicon?outperforms a maximum en-
tropy classifier trained on gold labels. However, we
do not find the follower graph to improve perfor-
mance with our current implementation.
2 Datasets
We use several different Twitter datasets as train-
ing or evaluation resources. From the annotated
datasets, only tweets with positive or negative polar-
ity are used, so neutral tweets are ignored. While im-
portant, subjectivity detection is largely a different
problem from polarity classification. For example,
Pang and Lee (2004) use minimum cuts in graphs
for the former and machine-learned text classifica-
tion for the latter. We also do not give any special
treatment to retweets, though doing so is a possible
future improvement.
2.1 Emoticon-based training set (EMOTICON)
Emoticons are commonly exploited as noisy in-
dicators of polarity?including by Twitter?s own
advanced search ?with positive/negative attitude.?
While imperfect, there is potential for millions of
tweets containing emoticons to serve as a source
of noisy training material for a supervised classi-
fier. We create such a training set from a sample
of the ?garden hose?2 Twitter feed, from September
to December, 2009. At the time of collection, this
included up to 15% of all tweets worldwide.
From this feed, 6,265,345 tweets containing at
least one of the emoticons listed in Table 1 are ex-
tracted; 5,156,277 contain a positive emoticon and
2http://dev.twitter.com/pages/streaming_api
54
+ :) :D =D =) :] =] :-) :-D :-] ;) ;D ;] ;-) ;-D ;-]
? :( =( :[ =[ :-( :-[ :?( :?[ D:
Table 1: Positive and negative emoticons.
+ #ff, congrats, gracias, yay, thx, smile,
awesome, hello, excited, moon, loving, glad,
sweet, wonderful, birthday, enjoy, goodnight,
amazing, cute, bom
? nickjonas, murphy, brittany, rip, triste, sad,
hurts, died, snow, huhu, headache, upset,
crying, throat, poor, sucks, ugh, sakit,
stomach, horrible
Table 2: Top 20 most predictive common unigram fea-
tures for the positive and negative classes, in order from
more predictive to less predictive.
1,109,068 contain a negative emoticon. A small
number of tweets contain both negative and posi-
tive emoticons. These are permitted to appear twice,
once for each label. Then, a balanced ratio of
positive/negative labels is obtained by keeping only
1,109,068 of the positive tweets. Finally, a large pro-
portion of non-English tweets are excluded by a fil-
ter that requires a tweet to have at least two words
(with at least two characters) from the CMU Pro-
nouncing Dictionary.3 A few non-English tweets
pass through this filter and some English tweets
with very unusual words or incorrect spelling are
dropped, but this simple strategy works well over-
all. The final training set contains 1,839,752 tweets,
still balanced for positive and negative emoticons.
Table 2 shows the 20 most predictive unigram
features of each class in the EMOMAXENT classi-
fier (described below) that are among the 1000 most
common unigrams in this dataset and are not them-
selves emoticons. A few non-English (but polar-
ized) words (e.g. gracias, bom, triste) make it past
our simple language filter and onto these lists, but
the majority of the most predictive words are En-
glish. Other highly predictive words are artifacts
of the particular tweet sample that comprises the
EMOTICON dataset, such as ?nickjonas,? ?brittany,?
and ?murphy,? the latter two explained by the abun-
3The dictionary contains 133k English words, including in-
flected forms and proper nouns. http://www.speech.
cs.cmu.edu/cgi-bin/cmudict
Dataset Use Size % Pos
STS dev 183 59.0
OMD dev 1898 73.1
HCR-TRAIN train 488 43.2
HCR-DEV dev 534 32.2
HCR-TEST test 396 38.6
Table 3: Basic properties of the annotated datasets used
in this paper.
dance of negative tweets after actress Brittany Mur-
phy?s death. Most others are intuitively good mark-
ers of positive or negative polarity.
2.2 Datasets with polarity annotations
Three annotated datasets, summarized in Table 3 and
described below, are used for training, development,
or evaluation of polarity classifiers.
Stanford Twitter Sentiment (STS). Go et al
(2009) created a collection of 216 annotated tweets
on various topics.4 Of these, 108 tweets are positive
and 75 are negative.
Obama-McCain Debate (OMD). Shamma et al
(2009) used Amazon Mechanical Turk to annotate
3,269 tweets posted during the presidential debate
on September 26, 2008 between Barack Obama and
John McCain. Each tweet was annotated by one
or more Turkers for the categories positive, nega-
tive, mixed, or other. We filter this dataset with two
constraints in order to ensure high inter-annotator
agreement. First, at least three votes must have
been provided for a tweet to be included. Second,
more than half of the votes must have been posi-
tive or negative; the majority label is taken as the
gold standard for that tweet. This results in a set of
1,898 tweets. Of these, 705 had positive gold labels
and 1192 had negative gold labels, and the average
inter-annotator agreement of the Turk votes for these
tweets was 83.7%. To our knowledge, we are the
first to perform automatic polarity classification on
this dataset.
Health Care Reform (HCR). We create a new
annotated dataset based on tweets about health care
reform in the USA. This was a strongly debated
4http://twittersentiment.appspot.com/
55
topic that created a large number of polarized tweets,
especially in the run up to the signing of the health
care bill on March 23, 2010. We extract tweets con-
taining the health care reform hashtag ?#hcr? from
early 2010; a subset of these are annotated by us and
colleagues for polarity (positive, negative, neutral,
irrelevant) and polarity targets (health care reform,
Obama, Democrats, Republicans, Tea Party, conser-
vatives, liberals, and Stupak). These are separated
into training, dev and test sets. As with the other
datasets, we restrict attention in this paper only to
positive and negative tweets.5
2.3 The Twitter follower graph
One of the key ideas we test in this paper is whether
social connections can be used to improve polarity
classification for individual tweets and users. We
construct the Twitter follower graphs for the users in
the above datasets in stages using publicly available
data from the Twitter API. From the full list of each
user?s followers, we retain only followers found
within the datasets; this prunes unknown users who
did not tweet about the topic and thus are unlikely to
provide useful information. This method for graph
construction offers nearly complete graphs, but has
two main disadvantages. First, many users have
raised their privacy levels over time, which hinders
the ability to view their follower graph. In these
cases only their tweet information is known. Sec-
ondly, due to the rapid pace of growth on Twit-
ter, user graphs tend to grow quickly; thus our con-
structed graph is a representation of the user?s cur-
rent social graph and not the exact graph that existed
at the time of the tweet.
3 Approach
We compare three main approaches: using lexicon-
based positive/negative ratios, maximum entropy
classification and label propagation.
3.1 Lexicon-based baseline (LEXRATIO)
A reasonable baseline to use in polarity classifica-
tion is to count the number of positive and negative
terms in a tweet and pick the category with more
terms (O?Connor et al, 2010). This actually uses
5A public release of this data, along with our code, is avail-
able at https://bitbucket.org/speriosu/updown.
supervision at the level of word types. Like most
others, we use the OpinionFinder subjectivity lexi-
con,6 which contains 2,304 words annotated as pos-
itive and 4,153 words as negative. If the number of
positive and negative words in a tweet is equal (in-
cluding zero for both), the label is chosen at random.
3.2 Maximum entropy classifier (MAXENT)
The OpenNLP Maximum Entropy package7 is used
to train polarity classifiers using either EMOTICON
or HCR-TRAIN, henceforth referred to as EMO-
MAXENT and GOLDMAXENT, respectively. After
tokenizing on whitespace, unigram and bigram
features are extracted. All characters are lowercased
and non-alphanumeric characters are trimmed from
the left and right sides of tokens. However, tokens
that contain no alphanumeric characters are not
trimmed. Stop words8 are excluded as unigram
features. However, bigram features are extracted be-
fore stop words are removed since many stop words
are informative in the context of content words: e.g.,
contrast shit (negative) from the shit (very positive).
The beginning and end of tweets are indicated by
?$? in bigram features. Thus, the full feature set for
the tweet I love my new iPod Touch! :D is [love,
ipod, touch, $ i, i love, love my,
my ipod, ipod touch, touch :D, :D
$]. The same tokenization method is used for all
datasets in this paper.
3.3 Label Propagation (LPROP)
Tweets are not created in isolation?each tweet is
linked to other tweets by the same author, and each
author is influenced by the tweets of those he or she
follows. Common vocabulary and topics of discus-
sion also connect tweets to each other. Graph-based
methods such as label propagation (Zhu and Ghahra-
mani, 2002; Baluja et al, 2008; Talukdar and Cram-
mer, 2009) provide a natural means to represent and
exploit such relationships in order to improve classi-
fication, often while requiring less supervision than
with standard classification. Label propagation al-
gorithms spread label distributions from a small set
6http://www.cs.pitt.edu/mpqa/
opinionfinderrelease/
7http://incubator.apache.org/opennlp/
8Taken from: http://www.ranks.nl/resources/
stopwords.html
56
Opinion Finder
N-Grams
Hashtags
Emoticons
U1
Ahhh 
#Obamacare
I Love 
#NY!
{Tweet3} {Tweetn}
U2
We can't pass 
this :( #Killthebill 
love
hate
enjoy
i love
love ny
we can't
#ny
#obamacare
#killthebill
:):( =]
U3
{Tweetn}
Un
{Tweetn}
...
                  EmoMaxent Seeds
                  Labeled Seeds
                  Unseeded
...
...
. . .
...
Figure 1: An illustration of our graph with All-edges and Noisy-seed (see text for description).
of nodes seeded with some initial label information
(always noisy, heuristic information rather than gold
instance labels in our case) throughout the graph.
Label distributions are spread across a graph G =
{V,E,W} where V is the set of n nodes, E is a set
ofm edges andW is an n?nmatrix of weights, with
wij as the weight of edge (i, j). We use Modified
Adsorption (MAD) (Talukdar and Crammer, 2009)
over a graph with nodes representing tweets, authors
and features, while varying the seed information and
the construction of the edge sets. The spreading of
the label distributions can be viewed as a controlled
random walk with three possible actions: (i) inject-
ing a seeded node with its seed label, (ii) continu-
ing the walk from the current node to a neighbor-
ing node, and (iii) abandoning the walk. MAD takes
three parameters, ?1, ?2 and ?3, which control the
relative importance of each of these actions, respec-
tively. We use the Junto Label Propagation Toolkit?s
implementation of MAD in this paper.9
Modified Adsorption requires some nodes in the
graph to have seed distributions, which can come for
a variety of knowledge sources. We consider the fol-
lowing variants for seeding the graph:
? Maxent-seed: EMOMAXENT is trained on the
EMOTICON dataset; every tweet node is seeded
9http://code.google.com/p/junto/
with its polarity predictions for the tweet.
? Lexicon-seed: Nodes are created for every word
in the OpinionFinder lexicon. Positive words are
seeded as 90% positive if they are strongly subjec-
tive and 80% positive if weakly subjective; simi-
larly and conversely for negative words. Every
tweet is connected by an edge to every word in
the polarity lexicon it contains, using the weight-
ing scheme discussed with Feature-edges below.
? Emoticon-seed: Nodes are created for emoticons
from Table 1 and seeded as 90% positive or nega-
tive depending on their polarity.
? Annotated-seed: The annotations in HCR-
TRAIN are used to seed the tweets from that
dataset as 100% positive or negative, in accor-
dance with the label.
We use Noisy-seed as a collective term for all of the
above seed sets except Annotated-seed.
The other main aspect of graph construction is
specifying edges and their weights. We consider the
following variants:
? Follower-edges: When a user A follows another
user B, we add an edge from A to B with a weight
of 1.0, a weight that is comparable to that of a
moderately frequent word in Feature-edges below.
? Feature-edges: Nodes are added for hashtags and
the features described in ?3.2 and connected to the
57
tweets that contain them. An edge connecting a
tweet t to a feature f has weight wtf using rel-
ative frequency ratios of the feature between the
dataset d in question and the EMOTICON dataset
as a reference corpus r:
wtf =
{
log Pd(f)Pr(f) if Pd(f) > Pr(f)
0 o.w.
(1)
We use All-edges when combining both edge sets.
Figure 1 illustrates the connections for All-edges
and Noisy-seed by example. Each user un is at-
tached to anyone who follows them or who they
follow. Each user is also connected to the tweets
they authored. Words from OpinionFinder are con-
nected to tweets that contain those words, and sim-
ilarly for hashtags, emoticons, unigrams, and bi-
grams. Emoticons and words from OpinionFinder
are seeded according to the explanation above. All
edges other than Feature-edges are given a weight of
1.0.
4 Results
4.1 Parameter tuning
We evaluated our models on the STS, OMD, and
HCR-DEV datasets during development and kept
HCR-TEST as a final held-out test set used once, af-
ter all relevant parameters had been set. For Mod-
ified Adsorption, 100 iterations were used, and a
seed injection parameter ?1 of .005 gave the best
balance of allowing seed distributions to affect other
nodes without overwhelming them. The Junto de-
fault value of .01 was used for both ?2 and ?3.
4.2 Per-tweet accuracy
Table 4 shows the per-tweet accuracy results of
the random baseline, the LEXRATIO baseline, the
EMOMAXENT classifier alone, the LPROP classifier
run only on Follower-edges with Maxent-seed, the
LPROP classifier run on the full graph from Figure 1
only seeded with Lexicon-seed, and the LPROP clas-
sifier run on All-edges and Noisy-seed.
For all datasets, LPROP with Feature-edges and
Noisy-seed outperforms or matches all other meth-
ods. For STS, our best result of 84.7% accu-
racy beats Go et al (2009)?s reported best result
Classifier MSE
Random .167
LEXRATIO .170
EMOMAXENT .233
LPROP (Follower-edges, Maxent-seed) .233
LPROP (All-edges, Lexicon-seed) .187
LPROP (Feature-edges, Noisy-seed) .148
LPROP (All-edges, Noisy-seed) .148
Table 5: Mean squared error (MSE) per-user on HCR-
TEST, for users with at least 3 tweets
of 82.7%. Their approach uses a Maxent classifier
trained on a noisily labeled emoticon training set
similar to our EMOTICON dataset. Note that they
also remove neutral tweets from the test set.
Our semi-supervised label propagation method
compares favorably to fully supervised approaches.
For example, a graph with Feature-edges seeded
with gold labels from HCR-TRAIN (i.e. Annotated-
seed) obtains only 64.6% per-tweet accuracy on
HCR-TEST. A maximent entropy classifier trained
on HCR-TRAIN achieves 66.7%. Our best label
propagation approach surpasses both of these at
71.2%.
We find that in general Follower-edges are not
helpful as implemented here. Further work is needed
to explore more nuanced ways of modeling the so-
cial graph, such as allowing leaders to influence fol-
lowers more than vice versa.
4.3 Per-user error
In many sentiment analysis applications, it is of in-
terest to know what the polarity of a given individual
or the overall polarity toward a particular product is.
Here we compare the positivity ratio predicted by
our methods to that in the gold standard labels on a
per-user basis, using the mean squared error between
the predicted positivity ratios ppr and the actual ra-
tios apr for all users:
MSE(ppr, apr) =
?
i
(apri ? ppri)
2
Where apri and ppri are the actual and predicted
positivity ratios of the ith user.
Table 5 gives MSE results on HCR-TEST for
users with at least 3 tweets. LPROP (Feature-edges,
58
Classifier STS OMD HCR-DEV HCR-TEST
Random 50.0 50.0 50.0 50.0
LEXRATIO 72.1 59.1 54.3 58.1
EMOMAXENT 83.1 61.3 58.6 62.9
LPROP (Follower-edges, Maxent-seed) 83.1 61.2 57.9 62.9
LPROP (All-edges, Lexicon-seed) 70.0 62.6 64.6 64.6
LPROP (Feature-edges, Noisy-seed) 84.7 66.7 65.7 71.2
LPROP (All-edges, Noisy-seed) 84.7 66.5 65.2 71.0
Table 4: Per-tweet accuracy percentages. The models and parameters were developed while tracking performance on
STS, OMD, and HCR-DEV, and HCR-TEST results were obtained from a single, blind run.
+ pow pow, good debate, hack the, hack
$ barackobama, barackobama, the vp,
good job, to vote, john is, is to, obama did,
they both, gergen, knowledge, voting for,
for veterans, the veterans, america, will take
? language, this was, drinking, terrorists,
government, china, obama i, that we, father,
obama in, mc, diplomacy, wars, afghanistan,
debt, simply, financial, the spin, the bottom,
bottom
Table 7: Top 20 most positive and most negative n-grams
in OMD after running LPROP with All-edges and Noisy-
seed. Note that ?$? indicates the beginning or end of a
tweet.
Noisy-seed) and LPROP (All-edges, Noisy-seed) are
tied for the lowest error.
4.4 Per-target accuracy
Table 6 gives results on a per-target basis for the
five most common targets in the HCR-TEST dataset,
in order from most common to least common: hcr,
dems, obama, gop, and conservatives. The per-
centages reflect the fraction of tweets correctly la-
beled for each target. These distributions are highly
skewed: the hcr target covers about 69% of the
tweets, while the conservatives target covers only
about 5%. Thus performance on the hcr target
tweets is most important for overall accuracy.
5 Discussion
Polar language An attractive property of label
propagation algorithms is that label distributions can
be obtained for nodes other than the tweets (and im-
+ human, stupak, you do, sunday, fired
vote for, yes on, $ we, vote yes, to vote,
vote on, goal, nation, do it, up to, ago, votes,
this #hcr, #hcr is, on #hcr
? gop, #tlot #hcr, #tcot #tlot, 12, #topprog,
medicare, #tlot, #tlot $, #ocra, cbo,
tea party, tea, passes, #hhrs, $ dems, #hc,
#obamacare, #sgp, dems, do not
Table 8: Top 20 most positive and most negative n-grams
in HCR-TEST after running LPROP with All-edges and
Noisy-seed.
portantly, nodes that were unseeded). For example,
all of the feature nodes?unigrams, bigrams, and
hashtags?have a loading for the positive and neg-
ative labels. These could be used for various vi-
sualizations of the results of the polarity classifica-
tion, including terms that are the most positive and
negative and also highlighting or bolding such terms
when showing a user individual tweets.
Table 7 shows the 20 unigrams and bigrams with
the highest and lowest ratio of positive label prob-
ability to negative label probability after running
LPROP with All-edges and Noisy-seed. These lists
are restricted to terms that had an edge weight of at
least 1.0, i.e. that were twice as frequent in OMD
compared to the reference corpus, that had a raw
count of at least 5 in OMD, and that didn?t al-
ready appear in the OpinionFinder lexicon. Some of
the terms are intuitively positive and negative, e.g.
good job and wars. Others reflect more specific as-
pects of the OMD dataset, such as good debate and
afghanistan.
Table 8 shows the top 20 for HCR-TEST. Many
59
hcr dems obama gop conservatives
Classifier (274) (27) (26) (22) (20)
LEXRATIO 58.0 64.8 69.2 50.0 52.5
EMOMAXENT 62.4 66.7 73.1 68.2 60.0
LPROP (Follower-edges, Maxent-seed) 62.4 66.7 73.1 68.2 60.0
LPROP (All-edges, Lexicon-seed) 60.6 85.2 73.1 86.4 60.0
LPROP (Feature-edges, Noisy-seed) 69.0 81.5 80.8 86.4 70.0
LPROP (All-edges, Noisy-seed) 69.0 77.8 80.8 86.4 70.0
Table 6: Per-target accuracy percentages for HCR-TEST. The number of tweets for each target is given in parentheses.
terms simply reflect a rallying to either pass or defeat
the healthcare reform bill (vote for, do not). Other
positive words represent more abstract concepts pro-
ponents of the bill may be expressing (human, goal).
Conversely, opponents such as those who would at-
tend a tea party are concerned about what they call
#obamacare.
Domain differences There are several reasons
why performance is much lower on both the OMD
and HCR datasets than on STS. First, both the
EMOTICON (noisy) training set and the STS dev set
are general in topic. Correct estimations of the posi-
tivity and negativity of general words in the training
set like yay and upset are more likely to be useful
in a broad-domain evaluation set, whereas misesti-
mations of the weights of more specific words and
bigrams are likely to be washed out. In contrast,
the OMD and HCR datasets contain a very differ-
ent vocabulary distribution from the STS set. Words
and phrases referring to specific political issues like
health care and iraq war have frequencies that are
orders of magnitude higher than either the EMOTI-
CON training set or the STS dev set. Thus, misesti-
mations of the positivity or negativity of these fea-
tures will be amplified in evaluation. Lastly, expres-
sion of political opinions tends to be more nuanced
than the general opinions and feelings, simply due
to the complex nature of political issues. Everyone
agrees that a sore throat is bad, while it is less ob-
vious how much government involvement in health
care is beneficial.
LEXRATIO vs. EMOMAXENT LEXRATIO has
low coverage for words that tend to indicate positive
and negative sentiment in particular domains. For
example, STS has the tweet In montreal for a long
weekend of R&R. Much needed, with a positive gold
label. The only word in this tweet in the Opinion-
Finder lexicon is long, which is labeled as negative.
Thus, LEXRATIO incorrectly classifies the tweet as
negative. EMOMAXENT correctly labels this tweet
positive due to features like weekend being strong
indicators of the positive class. Similarly, the tweet
Booz Allen Hamilton has a bad ass homegrown so-
cial collaboration platform. Way cool! #ttiv is la-
beled negative by LEXRATIO due to the presence of
bad. While EMOMAXENT has a negative preference
for both bad and ass, it has a strong positive prefer-
ence for bad ass, as well as both cool and way cool.
EMOMAXENT vs. LPROP As seen from the per-
tweet and per-user results, LPROP does consistently
better than MAXENT. We now discuss one example
of this improvement from the OMD set. One user
authored the following four tweets:
? t1: obama +3 the conspicuousness of their pres-
ence is only matched by our absence #tweetdebate
? t2: Fundamentally, if McCain fundamentally uses
?fundamental? one more time, I?m gonna go nuts.
#tweetdebate
? t3: McCain likes the bears in Montana joke too
much#tweetdebate #current
? t4: We are less respected now... Obama #current
#debate08 And I give credit to McCain... NOOO
The gold label for t1 is positive and the rest are nega-
tive. All of the LPROP classifiers correctly predicted
the labels for all four tweets. EMOMAXENT missed
t2 and t3, so this primarily negative user is incor-
rectly indicated as primarily positive by EMOMAX-
ENT. LPROP gets around this by propagating senti-
ment polarity through unigram features in this case.
60
The unigram mccain has an edge weight to tweets
that contain it of 8.6 for the OMD corpus, meaning
mccain is much more frequent in this corpus than
the reference corpus, so any sentiment associated
with mccain is propagated strongly. In this case, the
output of label propagation seeded with Noisy-seed
reveals that mccain has negative sentiment for this
dataset.
6 Related Work
Much work in sentiment analysis involves the use
and generation of dictionaries capturing the senti-
ment of words. These methods range from manual
approaches of developing domain-dependent lexi-
cons (Das and Chan, 2001) to semi-automated ap-
proaches (Hu and Liu, 2004) and fully automated
approaches (Turney, 2002). Melville et al (2009)
use a unified framework combining background lex-
ical information in terms of word-class associations
and refine this information for specific domains us-
ing any available training examples. They produce
better results than using either a lexicon or training.
O?Connor et al (2010) use the OpinionFinder
subjectivity lexicon to label the polarity of tweets
about Barack Obama and compare daily aggregate
sentiment scores to the Gallup poll time series of
manually gathered approval ratings of Obama. Even
with this simple polarity determination, they find
significant correlation between their predicted ag-
gregate sentiment per day and the Gallup poll.
Using the OMD dataset, Shamma et al (2009)
find that amount of Twitter activity is a good pre-
dictor of topic changes during the debate, and that
the content of concurrent tweets reflects a mix of
the current debate topic and Twitter users? reactions
to that topic. Diakopoulos and Shamma (2010) use
the same dataset to develop analysis and visualiza-
tion techniques to aid journalists and others in un-
derstanding the relationship between the live debate
event and the timestamped tweets.
Bollen et al (2010) perform aggregate sentiment
analysis on tweets over time, comparing predicted
sentiment to time series such as the stock market
and crude oil prices, as well as major events such
as election day and Thanksgiving. However, the au-
thors use hand-built rules for classification based on
the Profile of Mood States (POMS) and largely eval-
uate based on inspection.
7 Conclusion
We have improved upon existing tweet polarity clas-
sification methods by combining several knowledge
sources with a noisily supervised label propagation
algorithm. We show that a maximum entropy clas-
sifier trained with distant supervision works better
than a lexicon-based ratio predictor, improving the
accuracy for polarity classification on our held-out
test set from 58.1% to 62.9%. By using the predic-
tions of that classifier in combination with a graph
that incorporates tweets and lexical features, we ob-
tain even better accuracy of 71.2%.
We did not find overall gains from using the fol-
lower graph as implemented here. There is room
for improvement in the way the follower graph is
encoded in our graph, particularly with respect to
using asymmetric relationships rather than an undi-
rected graph, and in how follower relationships are
weighted.
Another source of information that could be used
to improve results is the text in pages that have
been linked to from a tweet. In many cases, it is
only possible to know what the polarity is by look-
ing at the page being linked to. Our label propa-
gation setup can incorporate this straightforwardly
by adding nodes for those pages plus edges between
them and all tweets that reference them.
Acknowledgments
This research was supported by a grant from the
Morris Memorial Trust Fund of the New York Com-
munity Trust. We thank Leif Johnson for providing
the tweets from the Twitter firehose for the EMOTI-
CON and HCR datasets, Partha Talukdar for the
Junto label propagation toolkit, and the UT Natural
Language Learning reading group for helpful feed-
back.
References
Shumeet Baluja, Rohan Seth, D. Sivakumar, Yushi Jing,
Jay Yagnik, Shankar Kumar, Deepak Ravichandran,
and Mohamed Aly. Video suggestion and discovery
for youtube: taking random walks through the view
graph. In WWW ?08: Proceeding of the 17th interna-
tional conference on World Wide Web, pages 895?904,
New York, NY, USA, 2008. ACM.
61
S. Blair-Goldensohn, K. Hannan, R. McDonald, T. Ney-
lon, G. Reis, and J. Reynar. Building a sentiment
summarizer for local service reviews. In WWW
Workshop on NLP in the Information Explosion Era
(NLPIX), 2008. URL http://www.ryanmcd.
com/papers/local_service_summ.pdf.
J. Bollen, A. Pepe, and H. Mao. Modeling public mood
and emotion: Twitter sentiment and socio-economic
phenomena. In Proceedings of the 19th International
World Wide Web Conference, 2010.
Samuel Brody and Noemie Elhadad. An unsupervised
aspect-sentiment model for online reviews. In Hu-
man Language Technologies: The 2010 Annual Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA, 2010. As-
sociation for Computational Linguistics. ISBN 1-
932432-65-5. URL http://portal.acm.org/
citation.cfm?id=1857999.1858121.
Yejin Choi and Claire Cardie. Adapting a polar-
ity lexicon using integer linear programming for
domain-specific sentiment classification. In Proceed-
ings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 590?
598. Association for Computational Linguistics, 2009.
URL http://www.aclweb.org/anthology/
D/D09/D09-1062.
S. Das and M. Chan. Extracting market sentiment from
stock message boards. Asia Pacific Finance Associa-
tion, 2001, 2001.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. En-
hanced sentiment learning using twitter hashtags and
smileys. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, 2010.
Nicholas A. Diakopoulos and David A. Shamma. Char-
acterizing debate performance via aggregated twitter
sentiment. In Proceedings of the 28th international
conference on Human factors in computing systems,
pages 1195?1198, 2010.
Alec Go, Richa Bhayani, and Lei Huang. Twitter senti-
ment classification using distant supervision. Unpub-
lished manuscript. Stanford University, 2009.
Minqing Hu and Bing Liu. Mining and summarizing cus-
tomer reviews. In KDD ?04: Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177, New
York, NY, USA, 2004. ACM. ISBN 1-58113-888-1.
doi: http://doi.acm.org/10.1145/1014052.1014073.
Prem Melville, Wojciech Gryc, and Richard D.
Lawrence. Sentiment analysis of blogs by combin-
ing lexical knowledge with text classification. In KDD
?09: Proceedings of the 15th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 1275?1284, New York, NY, USA, 2009.
ACM. ISBN 978-1-60558-495-9. doi: http://doi.acm.
org/10.1145/1557019.1557156.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the International AAAI
Conference on Weblogs and Social Media, 2010.
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?:
sentiment classification using machine learning tech-
niques. In Proceedings of the ACL-02 conference on
Empirical methods in natural language processing-
Volume 10, pages 79?86, 2002.
Bo Pang and Lillian Lee. A sentimental education: senti-
ment analysis using subjectivity summarization based
on minimum cuts. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, 2004.
Bo Pang and Lillian Lee. Opinion mining and sentiment
analysis. Foundations and Trends in Information Re-
trieval, 2(1-2):1?135, 2008.
Delip Rao and Deepak Ravichandran. Semi-supervised
polarity lexicon induction. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 675?682. Association for Com-
putational Linguistics, 2009. URL http://www.
aclweb.org/anthology/E09-1077.
Jonathon Read. Using emoticons to reduce dependency
in machine learning techniques for sentiment classifi-
cation. In Proceedings of the ACL Student Research
Workshop, ACLstudent ?05, pages 43?48, Strouds-
burg, PA, USA, 2005. Association for Computational
Linguistics. URL http://portal.acm.org/
citation.cfm?id=1628960.1628969.
David A. Shamma, Lyndon Kennedy, and Elizabeth F.
Churchill. Tweet the debates: understanding commu-
nity annotation of uncollected sources. In Proceedings
of the first SIGMM workshop on Social media, pages
3?10, 2009.
Vikas Sindhwani and Prem Melville. Document-word co-
regularization for semi-supervised sentiment analysis.
In Proceedings of IEEE International Conference on
Data Mining (ICDM-08), 2008.
Partha Talukdar and Koby Crammer. New regularized
algorithms for transductive learning. In Wray Bun-
tine, Marko Grobelnik, Dunja Mladenic, and John
Shawe-Taylor, editors, Machine Learning and Knowl-
edge Discovery in Databases, volume 5782, pages
442?457. Springer Berlin / Heidelberg, 2009.
62
P. D. Turney. Thumbs up or thumbs down? semantic ori-
entation applied to unsupervised classification of re-
views. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
417?424, 2002.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. Opin-
ionFinder: A system for subjectivity analysis. In Proc.
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP-2005) Companion Volume
(software demonstration), 2005.
Xiaojin Zhu and Zoubin Ghahramani. Learning from la-
beled and unlabeled data with label propagation. Tech-
nical Report CMU-CALD-02-107, Carnegie Mellon
University, 2002.
63
