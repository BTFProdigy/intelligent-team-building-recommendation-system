Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70?75,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages 
 
 
Marta Recasens, Toni Mart?, Mariona Taul? Llu?s M?rquez, Emili Sapena 
Centre de Llenguatge i Computaci? (CLiC) TALP Research Center,  
University of Barcelona Technical University of Catalonia 
Gran Via de les Corts Catalanes 585 
08007 Barcelona 
Jordi Girona Salgado 1-3 
08034 Barcelona 
{mrecasens, amarti, mtaule} 
@ub.edu 
{lluism, esapena} 
@lsi.upc.edu 
 
 
 
Abstract 
This paper presents the task ?Coreference 
Resolution in Multiple Languages? to be run 
in SemEval-2010 (5th International Workshop 
on Semantic Evaluations). This task aims to 
evaluate and compare automatic coreference 
resolution systems for three different lan-
guages (Catalan, English, and Spanish) by 
means of two alternative evaluation metrics, 
thus providing an insight into (i) the portabil-
ity of coreference resolution systems across 
languages, and (ii) the effect of different scor-
ing metrics on ranking the output of the par-
ticipant systems. 
1 Introduction 
Coreference information has been shown to be 
beneficial in many NLP applications such as In-
formation Extraction (McCarthy and Lehnert, 
1995), Text Summarization (Steinberger et al, 
2007), Question Answering (Morton, 2000), and 
Machine Translation. In these systems, there is a 
need to identify the different pieces of information 
that refer to the same discourse entity in order to 
produce coherent and fluent summaries, disam-
biguate the references to an entity, and solve ana-
phoric pronouns.  
Coreference is an inherently complex phenome-
non. Some of the limitations of the traditional rule-
based approaches (Mitkov, 1998) could be over-
come by machine learning techniques, which allow 
automating the acquisition of knowledge from an-
notated corpora. 
 
This task will promote the development of lin-
guistic resources ?annotated corpora1? and ma-
chine-learning techniques oriented to coreference 
resolution. In particular, we aim to evaluate and 
compare coreference resolution systems in a multi-
lingual context, including Catalan, English, and 
Spanish languages, and by means of two different 
evaluation metrics.  
By setting up a multilingual scenario, we can 
explore to what extent it is possible to implement a 
general system that is portable to the three lan-
guages, how much language-specific tuning is nec-
essary, and the significant differences between 
Romance languages and English, as well as those 
between two closely related languages such as 
Spanish and Catalan. Besides, we expect to gain 
some useful insight into the development of multi-
lingual NLP applications.  
As far as the evaluation is concerned, by em-
ploying B-cubed (Bagga and Baldwin, 1998) and 
CEAF (Luo, 2005) algorithms we can consider 
both the advantages and drawbacks of using one or 
the other scoring metric. For comparison purposes, 
the MUC score will also be reported. Among oth-
ers, we are interested in the following questions: 
Which evaluation metric provides a more accurate 
picture of the accuracy of the system performance? 
Is there a strong correlation between them? Can 
                                                           
1 Corpora annotated with coreference are scarce, especially for 
languages other than English.  
70
statistical systems be optimized under both metrics 
at the same time? 
The rest of the paper is organized as follows. 
Section 2 describes the overall task. The corpora 
and the annotation scheme are presented in Section 
3. Conclusions and final remarks are given in Sec-
tion 4. 
 
2 Task description  
The SemEval-2010 task ?Coreference Resolution 
in Multiple Languages? is concerned with auto-
matic coreference resolution for three different 
languages: Catalan, English, and Spanish.  
2.1 Specific tasks  
Given the complexity of the coreference phenom-
ena, we will concentrate only in two tractable as-
pects, which lead to the two following subtasks for 
each of the languages: 
i) Detection of full coreference chains, com-
posed by named entities, pronouns, and full 
noun phrases (NPs). 
ii) Pronominal resolution, i.e. finding the antece-
dents of the pronouns in the text.  
 
 
The example in Figure 1 illustrates the two sub-
tasks.2 Given a text in which NPs are identified and 
indexed (including elliptical subjects, represented 
as ?), the goal of (i) is to extract all coreference 
chains: 1?5?6?30?36, 9?11, and 7?18; while the 
goal of (ii) is to identify the antecedents of pro-
nouns 5 and 6, which are 1 and 5 (or 1), respec-
tively. Note that (b) is a simpler subtask of (a) and 
that for a given pronoun there can be multiple an-
tecedents (e.g. both 1 and 5 are correct antecedents 
for 6).  
We restrict the task to solving ?identity? rela-
tions between NPs (coreference chains), and be-
tween pronouns and antecedents. Nominal 
predicates and appositions as well as NPs with a 
non-nominal antecedent (discourse deixis) will not 
been taken into consideration in the recognition of 
coreference chains (see Section 3.1 for more in-
formation about decisions concerning the annota-
tion scheme). 
Although we target at general systems address-
ing the full multilingual task, we will allow taking 
part on any subtask of any language in order to 
promote participation. 
 
 
Figure 1.  NPs in a sample from the Catalan training 
data (left) and the English translation (right). 
                                                           
2 The example in Figure 1 is a simplified version of the anno-
tated format. See Section 2.2 for more details. 
[The beneficiaries of [[spouse?s]3 pensions]2]1 will 
be able to keep [the payment]4 even if [they]5 re-
marry provided that [they]6 fulfill [a series of [con-
ditions]8]7, according to [the royal decree approved 
yesterday by [the Council of Ministers]10]9.  
[The new rule]11 affects [the recipients of [a 
[spouse?s]13 pension]12 [that]14 get married after 
[January_1_,_2002]16]17. 
[The first of [the conditions]18]19 is being older 
[than 61 years old]20 or having [an officially rec-
ognized permanent disability [that]22 makes one 
disabled for [any [profession]24 or [job]25]23]21. 
[The second one]26 requires that [the pension]27 be 
[the main or only source of [the [pensioner?s]30 in-
come]29]28, and provided that [the annual amount 
of [the pension]32]31 represents, at least, [75% of 
[the total [yearly income of [the pen-
sioner]36]35]34]33. 
[Els beneficiaris de [pensions de [viudetat]3]2]1 po-
dran conservar [la paga]4 encara_que [?]5 es tornin 
a casar si [?]6 compleixen [una s?rie de [condi-
cions]8]7 , segons [el reial decret aprovat ahir pel 
[Consell_de_Ministres]10]9 .  
[La nova norma]11 afecta [els perceptors d' [una 
pensi? de [viudetat]13]12 [que]14 contreguin [matri-
moni]15 a_partir_de [l' 1_de_gener_del_2002]16]17 .  
[La primera de [les condicions]18]19 ?s tenir [m?s 
de 61 anys]20 o tenir reconeguda [una incapacitat 
permanent [que]22 inhabiliti per a [tota [professi?]24 
o [ofici]25]23]21. 
[La segona]26 ?s que [la pensi?]27 sigui [la principal 
o ?nica font d' [ingressos del [pensionista]30]29]28 , i 
sempre_que [l' import anual de [la mateixa pen-
si?]32]31 representi , com_a_m?nim , [el 75% del 
[total dels [ingressos anuals del [pensionis-
ta]36]35]34]33.  
 
71
2.2 Evaluation  
2.1.1 Input information 
The input information for the task will consist of: 
word forms, lemmas, POS, full syntax, and seman-
tic role labeling. Two different scenarios will be 
considered regarding the source of the input infor-
mation: 
 
i) In the first one, gold standard annotation will 
be provided to participants. This input annota-
tion will correctly identify all NPs that are part 
of coreference chains. This scenario will be 
only available for Catalan and Spanish. 
ii) In the second, state-of-the-art automatic lin-
guistic analyzers for the three languages will 
be used to generate the input annotation of the 
data. The matching between the automatically 
generated structure and the real NPs interven-
ing in the chains does not need to be perfect in 
this setting. 
  
By defining these two experimental settings, we 
will be able to check the performance of corefer-
ence systems when working with perfect linguistic 
(syntactic/semantic) information, and the degrada-
tion in performance when moving to a more realis-
tic scenario with noisy input annotation.  
2.1.2 Closed/open challenges 
In parallel, we will also consider the possibility of 
differentiating between closed and open chal-
lenges, that is, when participants are allowed to use 
strictly the information contained in the training 
data (closed) and when they make use of some ex-
ternal resources/tools (open). 
2.1.3 Scoring measures 
Regarding evaluation measures, we will have spe-
cific metrics for each of the subtasks, which will be 
computed by language and overall.  
Several metrics have been proposed for the task 
of coreference resolution, and each of them pre-
sents advantages and drawbacks. For the purpose 
of the current task, we have selected two of them ? 
B-cubed and CEAF ? as the most appropriate ones. 
In what follows we justify our choice.  
The MUC scoring algorithm (Vilain et al, 1995) 
has been the most widely used for at least two rea-
sons. Firstly, the MUC corpora and the MUC 
scorer were the first available systems. Secondly, 
the MUC scorer is easy to understand and imple-
ment. However, this metric has two major weak-
nesses: (i) it does not give any credit to the correct 
identification of singleton entities (chains consist-
ing of one single mention), and (ii) it intrinsically 
favors systems that produce fewer coreference 
chains, which may result in higher F-measures for 
worse systems. 
A second well-known scoring algorithm, the 
ACE value (NIST, 2003), owes its popularity to 
the ACE evaluation campaign. Each error (a miss-
ing element, a misclassification of a coreference 
chain, a mention in the response not included in the 
key) made by the response has an associated cost, 
which depends on the type of entity (e.g. person, 
location, organization) and on the kind of mention 
(e.g. name, nominal, pronoun). The fact that this 
metric is entity-type and mention-type dependent, 
and that it relies on ACE-type entities makes this 
measure inappropriate for the current task. 
The two measures that we are interested in com-
paring are B-cubed (Bagga and Baldwin, 1998) 
and CEAF (Luo, 2005). The former does not look 
at the links produced by a system as the MUC al-
gorithm does, but looks at the presence/absence of 
mentions for each entity in the system output. Pre-
cision and recall numbers are computed for each 
mention, and the average gives the final precision 
and recall numbers.  
CEAF (Luo, 2005) is a novel metric for evaluat-
ing coreference resolution that has already been 
used in some published papers (Ng, 2008; Denis 
and Baldridge, 2008). It mainly differs from B-
cubed in that it finds the best one-to-one entity 
alignment between the gold and system responses 
before computing precision and recall. The best 
mapping is that which maximizes the similarity 
over pairs of chains. The CEAF measure has two 
variants: a mention-based, and an entity-based one. 
While the former scores the similarity of two 
chains as the absolute number of common men-
tions between them, the latter scores the relative 
number of common mentions. 
Luo (2005) criticizes the fact that a response 
with all mentions in the same chain obtains 100% 
B-cubed recall, whereas a response with each men-
tion in a different chain obtains 100% B-cubed 
72
precision. However, precision will be penalized in 
the first case, and recall in the second case, each 
captured by the corresponding F-measure. Luo?s 
entity alignment might cause that a correctly iden-
tified link between two mentions is ignored by the 
scoring metric if that entity is not aligned. Finally, 
as far as the two CEAF metrics are concerned, the 
entity-based measure rewards alike a correctly 
identified one-mention entity and a correctly iden-
tified five-mention entity, while the mention-based 
measure takes into account the size of the entity. 
Given this series of advantages and drawbacks, 
we opted for including both B-cubed and CEAF 
measures in the final evaluation of the systems. In 
this way we will be able to perform a meta-
evaluation study, i.e. to evaluate and compare the 
performance of metrics with respect to the task 
objectives and system rankings. It might be inter-
esting to break B-cubed and CEAF into partial re-
sults across different kinds of mentions in order to 
get a better understanding of the sources of errors 
made by each system. Additionally, the MUC met-
ric will also be included for comparison purposes 
with previous results.  
Finally, for the setting with automatically gener-
ated input information (second scenario in Section 
2.1.1), it might be desirable to devise metric vari-
ants accounting for partial matches of NPs. In this 
case, capturing the correct NP head would give 
most of the credit. We plan to work in this research 
line in the near future.  
Official scorers will be developed in advance 
and made available to participants when posting 
the trial datasets. The period in between the release 
of trial datasets and the start of the full evaluation 
will serve as a test for the evaluation metrics. De-
pending on the feedback obtained from the partici-
pants we might consider introducing some 
improvements in the evaluation setting.  
3 AnCora-CO corpora  
The corpora used in the task are AnCora-CO, 
which are the result of enriching the AnCora cor-
pora (Taul? et al, 2008) with coreference informa-
tion. AnCora-CO is a multilingual corpus 
annotated at different linguistic levels consisting of 
400K words in Catalan3, 400K words in Spanish2, 
                                                           
3 Freely available for research purposes from the following 
URL: http://clic.ub.edu/ancora 
and 120K words in English. For the purpose of the 
task, the corpora are split into a training (85%) and 
test (15%) set. Each file corresponds to one news-
paper text.  
AnCora-CO consists mainly of newspaper and 
newswire articles: 200K words from the Spanish 
and Catalan versions of El Peri?dico newspaper, 
and 200K words from the EFE newswire agency in 
the Spanish corpus, and from the ACN newswire 
agency in the Catalan corpus. The source corpora 
for Spanish and Catalan are the AnCora corpora, 
which were annotated by hand with full syntax 
(constituents and functions) as well as with seman-
tic information (argument structure with thematic 
roles, semantic verb classes, named entities, and 
WordNet nominal senses). The annotation of 
coreference constitutes an additional layer on top 
of the previous syntactic-semantic information. 
The English part of AnCora-CO consists of a se-
ries of documents of the Reuters newswire corpus 
(RCV1 version).4 The RCV1 corpus does not come 
with any syntactic nor semantic annotation. This is 
why we only count with automatic linguistic anno-
tation produced by statistical taggers and parsers 
on this corpus. 
Although the Catalan, English, and Spanish cor-
pora used in the task all belong to the domain of 
newspaper texts, they do not form a three-way par-
allel corpus. 
3.1 Coreference annotation 
The annotation of a corpus with coreference in-
formation is highly complex due to (i) the lack of 
information in descriptive grammars about this 
topic, and (ii) the difficulty in generalizing the in-
sights from one language to another. Regarding (i), 
a wide range of units and relations occur for which 
it is not straightforward to determine whether they 
are or not coreferent. Although there are theoretical 
studies for English, they cannot always be ex-
tended to Spanish or Catalan since coreference is a 
very language-specific phenomenon, which ac-
counts for (ii). 
In the following we present some of the linguis-
tic issues more problematic in relation to corefer-
ence annotation, and how we decided to deal with 
them in AnCora-CO (Recasens, 2008). Some of 
them are language dependent (1); others concern 
                                                           
4 Reuters Corpus RCV1 is distributed by NIST at the follow-
ing URL: http://trec.nist.gov/data/reuters/reuters.html 
73
the internal structure of the mentions (2), or the 
type of coreference link (3). Finally, we present 
those NPs that were left out from the annotation 
for not being referential (4). 
 
1. Language-specific issues 
- Since Spanish and Catalan are pro-drop 
languages, elliptical subjects were intro-
duced in the syntactic annotation, and they 
are also annotated with coreference.  
- Expletive it pronouns, which are frequent 
in English and to a lesser extent in Spanish 
and Catalan are not referential, and so they 
do not participate in coreference links. 
- In Spanish, clitic forms for pronouns can 
merge into a single word with the verb; in 
these cases the whole verbal node is anno-
tated for coreference. 
2. Issues concerning the mention structure  
- In possessive NPs, only the reference of 
the thing possessed (not the possessor) is 
taken into account. For instance, su libro 
?his book? is linked with a previous refer-
ence of the same book; the possessive de-
terminer su ?his? does not constitute an NP 
on its own. 
- In the case of conjoined NPs, three (or 
more) links can be encoded: one between 
the entire NPs, and additional ones for 
each of the constituent NPs. AnCora-CO 
captures links at these different levels. 
3. Issues concerning types of coreference links 
- Plural NPs can refer to two or more ante-
cedents that appear separately in the text. 
In these cases an entity resulting from the 
addition of two or more entities is created.  
- Discourse deixis is kept under a specific 
link tag because not all coreference resolu-
tion systems can handle such relations. 
- Metonymy is annotated as a case of iden-
tity because both mentions pragmatically 
corefer. 
4. Non-referential NPs 
- In order to be linguistically accurate (van 
Deemter and Kibble, 2000), we distinguish 
between referring and attributive NPs: 
while the first point to an entity, the latter 
express some of its properties. Thus, at-
tributive NPs like apposition and predica-
tive phrases are not treated as identity 
coreference in AnCora-CO (they are kept 
distinct under the ?predicative link? tag).  
- Bound anaphora and bridging reference go 
beyond coreference and so are left out 
from consideration. 
The annotation process of the corpora is outlined in 
the next section. 
3.2 Annotation process 
The Ancora coreference annotation process in-
volves: (a) marking of mentions, and (b) marking 
of coreference chains (entities). 
(a) Referential full NPs (including proper nouns) 
and pronouns (including elliptical and clitic pro-
nouns) are the potential mentions of a coreference 
chain.  
(b) In the current task only identity relations 
(coreftype=?ident?) will be considered, which link 
referential NPs that point to the same discourse 
entity. Coreferent mentions are annotated with the 
attribute entity. Mentions that point to the same 
entity share the same entity number. In Figure 1, 
for instance, el reial decret aprovat ahir pel Con-
sell_de_Ministres ?the royal decree approved yes-
terday by the Council of Ministers? is 
entity=?entity9? and la nova norma ?the new rule? 
is also entity=?entity9? because they corefer. 
Hence, mentions referring to the same discourse 
entity all share the same entity number.  
The corpora were annotated by a total of seven 
annotators (qualified linguists) using the An-
CoraPipe annotation tool (Bertran et al, 2008), 
which allows different linguistic levels to be anno-
tated simultaneously and efficiently. AnCoraPipe 
supports XML in-line annotations.  
An initial reliability study was performed on a 
small portion of the Spanish AnCora-CO corpus. 
In that study, eight linguists annotated the corpus 
material in parallel. Inter-annotator agreement was 
computed with Krippendorff?s alpha, achieving a 
result above 0.8. Most of the problems detected 
were attributed either to a lack of training of the 
coders or to ambiguities that are left unresolved in 
the discourse itself. After carrying out this reliabil-
ity study, we opted for annotating the corpora in a 
two-stage process: a first pass in which all mention 
attributes and coreference links were coded, and a 
second pass in which the already annotated files 
were revised. 
 
74
4 Conclusions 
The SemEval-2010 multilingual coreference reso-
lution task has been presented for discussion.  
Firstly, we aim to promote research on coreference 
resolution from a learning-based perspective in a 
multilingual scenario in order to: (a) explore port-
ability issues; (b) analyze language-specific tuning 
requirements; (c) facilitate cross-linguistic com-
parisons between two Romance languages and be-
tween Romance languages and English; and (d) 
encourage researchers to develop linguistic re-
sources ? annotated corpora ? oriented to corefer-
ence resolution for other languages. 
Secondly, given the complexity of the corefer-
ence phenomena we split the coreference resolu-
tion task into two (full coreference chains and 
pronominal resolution), and we propose two dif-
ferent scenarios (gold standard vs. automatically 
generated input information) in order to evaluate to 
what extent the performance of a coreference reso-
lution system varies depending on the quality of 
the other levels of information. 
Finally, given that the evaluation of coreference 
resolution systems is still an open issue, we are 
interested in comparing different coreference reso-
lution metrics: B-cubed and CEAF measures. In 
this way we will be able to evaluate and compare 
the performance of these metrics with respect to 
the task objectives and system rankings. 
Acknowledgments 
This research has been supported by the projects 
Lang2World (TIN2006-15265-C06), TEXT-MESS 
(TIN2006-15265-C04), OpenMT (TIN2006-
15307-C03-02), AnCora-Nom (FFI2008-02691-E), 
and the FPU grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and the 
funding given by the government of the Generalitat 
de Catalunya.  
References  
Bagga, Amit and Breck Baldwin. 1998. Algorithms for 
scoring coreference chains. In Proceedings of Lan-
guage Resources and Evaluation Conference. 
Bertran, Manuel, Oriol Borrega, Marta Recasens, and 
B?rbara Soriano. 2008. AnCoraPipe: A tool for mul-
tilevel annotation, Procesamiento del Lenguaje Natu-
ral, n. 41: 291-292, SEPLN. 
Denis, Pascal and Jason Baldridge. 2008. Specialized 
models and ranking for coreference resolution. Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). 
Luo, Xiaoqiang. 2005. On coreference resolution per-
formance metrics. Proceedings of HLT/NAACL 2005. 
McCarthy Joseph and Wendy Lehnert.  1995. Using 
decision trees for coreference resolution. Proceed-
ings of the Fourteenth International Joint Conference 
on Artificial Intelligence. 
Mitkov, Ruslan. 1998. Robust pronoun resolution with 
limited knowledge. Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics, and 17th International Conference on Com-
putational Linguistics (COLING-ACL98). 
Morton, Thomas. 2000. Using coreference for question 
answering. Proceedings of the 8th Text REtrieval 
Conference (TREC-8). 
Ng, Vincent. 2008. Unsupervised models for corefer-
ence resolution. Proceedings of the Empirical Meth-
ods in Natural Language Processing (EMNLP 2008). 
NIST. 2003. In Proceedings of ACE 2003 workshop. 
Booklet, Alexandria, VA. 
Recasens, Marta. 2008. Towards Coreference Resolu-
tion for Catalan and Spanish. Master Thesis. Univer-
sity of Barcelona.  
Steinberger, Josef, Massimo Poesio, Mijail Kabadjov, 
and Karel Jezek. 2007. Two uses of anaphora resolu-
tion in summarization. Information Processing and 
Management, 43:1663?1680.  
Taul?, Mariona, Ant?nia Mart?, and Marta Recasens. 
2008. AnCora: Multilevel corpora with coreference 
information for Spanish and Catalan. In Proceedings 
of the Language Resources and Evaluation Confer-
ence (LREC 2008). 
van Deemter, Kees and Rodger Kibble. 2000. Squibs 
and Discussions: On coreferring: coreference in 
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637. 
Vilain, Marc, John Burger, John Aberdeen, Dennis 
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings 
of MUC-6. 
 
75
Coling 2010: Poster Volume, pages 1086?1094,
Beijing, August 2010
A Global Relaxation Labeling Approach to Coreference Resolution
Emili Sapena, Llu??s Padro? and Jordi Turmo?
TALP Research Center
Universitat Polite`cnica de Catalunya
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper presents a constraint-based
graph partitioning approach to corefer-
ence resolution solved by relaxation label-
ing. The approach combines the strengths
of groupwise classifiers and chain forma-
tion methods in one global method. Ex-
periments show that our approach signifi-
cantly outperforms systems based on sep-
arate classification and chain formation
steps, and that it achieves the best results
in the state of the art for the same dataset
and metrics.
1 Introduction
Coreference resolution is a natural language pro-
cessing task which consists of determining the
mentions that refer to the same entity in a text
or discourse. A mention is a noun phrase refer-
ring to an entity and includes named entities, def-
inite noun phrases, and pronouns. For instance,
?Michael Jackson? and ?the youngest of Jackson
5? are two mentions referring to the same entity.
A typical machine learning-based coreference
resolution system usually consists of two steps:
(i) classification, where the system evaluates the
coreferentiality of each pair or group of mentions,
and (ii) formation of chains, where given the con-
fidence values of the previous classifications the
system forms the coreference chains.
?Research supported by the Spanish Science and In-
novation Ministry, via the KNOW2 project (TIN2009-
14715-C04-04) and from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) under Grant
Agreement number 247762 (FAUST)
Regarding the classification step, pioneer sys-
tems developed were based on pairwise classi-
fiers. Given a pair of mentions, the process gen-
erates a feature vector and feeds it to a classi-
fier. The resolution is done by considering each
mention of the document as anaphor1 and look-
ing backward until the antecedent is found or
the beginning of the document is reached (Aone
and Bennett, 1995; McCarthy and Lehnert, 1995;
Soon et al, 2001).
A first approach towards groupwise classifiers
is the twin-candidate model (Yang et al, 2003).
The model faces the problem as a competition be-
tween two candidates to be the antecedent of the
anaphor into account. Each candidate mention is
compared with all the others in a round robin con-
test. Following the groupwise approach, rankers
consider all the possible antecedent mentions at
once (Denis and Baldridge, 2008). Rankers can
obtain more accurate results due to a more in-
formed context where all candidate mentions are
considered at the same time.
Coreference chains are formed after classifi-
cation. Many systems form the chains by join-
ing each positively-classified pair (i.e. single-
link) or with simple improvements such as linking
an anaphor only to its antecedent with maximum
confidence value (Ng and Cardie, 2002).
Some works propose more elaborated methods
than single-link for chain formation. The ap-
proaches used are Integer Linear Programming
1Typically a pair of coreferential mentions mi and mj
(i < j) are called antecedent and anaphor respectively,
though mj may not be anaphoric.
1086
(ILP) (Denis and Baldridge, 2007; Klenner and
Ailloud, 2009; Finkel and Manning, 2008), graph
partitioning (Nicolae and Nicolae, 2006), and
clustering (Klenner and Ailloud, 2008). The main
advantage of these types of post-processes is the
enforcement of transitivity sorting out the con-
tradictions that the previous classification process
may introduce.
Although chain formation processes search for
global consistency, the lack of contextual infor-
mation in the classification step is propagated for-
ward. Few works try to overcome the limita-
tions of keeping classification and chain formation
apart. Luo et al (2004) search the most proba-
ble path comparing each mention with the partial-
entities formed so far using a Bell tree struc-
ture. McCallum and Wellner (2005) propose a
graph partitioning cutting by distances, with the
peculiarity that distances are learned considering
coreferential chains of the labeled data instead of
pairs. Culotta et al (2007) combine a groupwise
classifier with a clustering process in a First-Order
probabilistic model.
The approach presented in this paper follows
the same research line of joining group classifi-
cation and chain formation in the same step. Con-
cretely, we propose a graph representation of the
problem solved by a relaxation labeling process,
reducing coreference resolution to a graph par-
titioning problem given a set of constraints. In
this manner, decisions are taken considering the
whole set of mentions, ensuring consistency and
avoiding that classification decisions are indepen-
dently taken. Our experimental results on the
ACE dataset show that our approach outperforms
systems based on separate classification and chain
formation steps, and that it achieves the best re-
sults in the state of the art for the same dataset and
metrics.
The paper is organized as follows. Section 2 de-
scribes the graph representation of the task. Sec-
tion 3 explains the use of relaxation labeling algo-
rithm and the machine learning process. Finally,
experiments and results are explained in Section 4
before paper is concluded.
2 Graph Representation
Let G = G(V,E) be an undirected graph where
V is a set of vertices and E a set of edges. Let
m = (m1, ...,mn) be the set of mentions of a
document with n mentions to resolve. Each men-
tion mi in the document is represented as a vertex
vi ? V . An edge eij ? E is added to the graph for
pairs of vertices (vi, vj) representing the possibil-
ity that both mentions corefer. The list of adjacent
vertices of a vertex vi is A(vi).
Let C be our set of constraints. Given a pair of
mentions (mi, mj), a subset of constraints Cij ?
C restrict the compatibility of both mentions. Cij
is used to compute the weight value of the edge
connecting vi and vj . Let wij ? W be the weight
of the edge eij :
wij =
?
k?Cij
?kfk(mi,mj) (1)
where fk(?) is a function that evaluates the con-
straint k. And ?k is the weight associated to the
constraint k (?k and wij can be negative).
In our approach, each vertex (vi) in the graph
is a variable (vi) for the algorithm. Let Li be the
number of different values (labels) that are pos-
sible for vi. The possible labels of each variable
are the partitions that the vertex can be assigned.
Note that the number of partitions (entities) in a
document is unknown, but it is at most the num-
ber of vertices (mentions), because in a extreme
case, each mention in a document could be refer-
ring to a different entity. A vertex with index i can
be in the first i partitions (i.e. Li = i).
Each combination of labelings for the graph
vertices is a partitioning (?). The resolution pro-
cess searches the partitioning ?? which optimizes
the goodness function F (?,W ), which depends
on the edge weights W. In this manner, ?? is opti-
mal if:
F (??,W ) ? F (?,W ),?? (2)
The next section describes the algorithm used
in the resolution process.
3 Relaxation Labeling
Relaxation labeling (Relax) is a generic name for
a family of iterative algorithms which perform
1087
function optimization, based on local information.
The algorithm has been widely used to solve NLP
problems such as PoS-tagging (Ma`rquez et al,
2000), chunking, knowledge integration, and Se-
mantic Parsing (Atserias, 2006).
Relaxation labeling solves our weighted con-
straint satisfaction problem dealing with the edge
weights. In this manner, each vertex is assigned to
a partition satisfying as many constraints as pos-
sible. To do that, the algorithm assigns a pro-
bability for each possible label of each variable.
Let H = (h1,h2, . . . ,hn) be the weighted label-
ing to optimize, where each hi is a vector con-
taining the probability distribution of vi, that is:
hi = (hi1, hi2, . . . , hiLi). Given that the resolutionprocess is iterative, the probability for label l of
variable vi at time step t is hil(t), or simply hil
when the time step is not relevant.
The support for a pair variable-label (Sil) ex-
presses how compatible is the assignment of label
l to variable vi considering the labels of adjacent
variables and the edge weights. Although several
support functions may be used (Torras, 1989), we
chose the following one, which defines the sup-
port as the sum of the edge weights that relate
variable vi with each adjacent variable vj multi-
plied by the weight for the same label l of vj :
Sil =
?
j?A(vi)
wij ? hjl (3)
where wij is the edge weight obtained in Equa-
tion 1. In our version of the algorithm, A(vi) is
the list of adjacent vertices of vi but only includ-
ing the ones with an index k < i. Consequently,
the weights only have influence in one direction
which is equivalent to using a directed graph. Al-
though the proposed representation is based on
a general undirected graph, preliminary experi-
ments showed that using directed edges yields
higher perfomance in this particular problem.
The aim of the algorithm is to find a weighted
labeling such that global consistency is maxi-
mized. Maximizing global consistency is defined
as maximizing the average support for each vari-
able. Formally, H? is a consistent labeling if:
Initialize:
H := H0,
Main loop:
repeat
For each variable vi
For each possible label l for vi
Sil =
?
j?A(vi) wij ? h
j
l
End for
For each possible label l for vi
hil(t + 1) =
hil(t)?(1+Sil)?Li
k=1
hik(t)?(1+Sik)End for
End for
Until no more significant changes
Figure 1: Relaxation labeling algorithm
Li?
l=1
h?il ? Sil ?
Li?
l=1
hil ? Sil ?h,?i (4)
A partitioning ? is directly obtained from the
weighted labeling H assigning to each variable
the label with maximum probability. The sup-
ports and the weighted labeling depend on the
edge weights (Equation 3). To satisfy Equation
4 is equivalent to satisfy Equation 2. Many stud-
ies have been done towards the demonstration of
the consistency, convergence and cost reduction
advantages of the relaxation algorithm (Rosenfeld
et al, 1976; Hummel and Zucker, 1987; Pelillo,
1997). Although some of the conditions required
by the formal demonstrations are not fulfilled in
our case, the presented algorithm ?that forces a
stop after a number of iterations? has proven use-
ful for practical purposes.
Figure 1 shows the pseudo-code of the relax-
ation algorithm. The process updates the weights
of the labels in each step until convergence. The
convergence is met when no more significant
changes are done in an iteration. Specifically,
when the maximum change in an update step
(maxi,l(|hil(t+1)?hil(t)|)) is lower than a param-
eter , a small value (0.001 in our experiments),
or a fixed number of iterations is reached (2000 in
our experiments). Finally, the assigned label for a
variable is the one with the highest weight. Figure
2 shows a representation.
1088
Figure 2: Representation of Relax. The vertices represent-
ing mentions are connected by weighted edges eij . Each ver-
tex has a vector hi of probabilities to belong to different par-
titions. The figure shows h2, h3 and h4.
3.1 Constraints
The performance of the resolution process de-
pends on the edge weights obtained by a set of
weighted constraints (Equation 1). Any method
or combination of methods to generate constraints
can be used. For example, a set of constraints
handwritten by linguist experts can be added to
another automatically obtained set.
This section explains the automatic constraint
generation process carried out in this work, using
a set of feature functions and a training corpus.
Ma`rquez et al (2000) have successfully used sim-
ilar processes to acquire constraints for constraint
satisfaction algorithms.
Each pair of mentions (mi, mj) in a training
document is evaluated by a set of feature functions
(Figure 3). The values returned by these functions
form a positive example when the pair of men-
tions corefer, and a negative one otherwise. Three
specialized models are constructed depending on
the type of anaphor mention (mj) of the pair: pro-
noun, named entity or nominal.
For each specialized model, a decision tree
(DT) is generated and a set of rules is ex-
tracted with C4.5 rule-learning algorithm (Quin-
lan, 1993). These rules are our set of constraints.
The C4.5rules algorithm generates a set of rules
for each path from the learnt tree. It then general-
izes the rules by dropping conditions.
The weight assigned to a constraint (?k) is its
DIST: Distance betweenmi andmj in sentences: number
DIST MEN: Distance betweenmi andmj in mentions: number
APPOSITIVE: One mention is in apposition with the other: y,n
I/J IN QUOTES:mi/j is in quotes or inside a NP or a sentence
in quotes: y,n
I/J FIRST:mi/j is the first mention in the sentence: y,n
I/J DEF NP:mi/j is a definitive NP: y,n
I/J DEM NP:mi/j is a demonstrative NP: y,n
I/J INDEF NP:mi/j is an indefinite NP: y,n
STR MATCH: String matching ofmi andmj : y,n
PRO STR: Both are pronouns and their strings match: y,n
PN STR: Both are proper names and their strings match: y,n
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns: y,n
HEAD MATCH: String matching of NP heads: y,n
NUMBER: The number of both mentions match: y,n,u
GENDER: The gender of both mentions match: y,n,u
AGREEMENT: Gender and number of both
mentions match: y,n,u
I/J THIRD PERSON:mi/j is 3rd person: y,n
PROPER NAME: Both mentions are proper names: y,n,u
I/J PERSON:mi/j is a person (pronoun or
proper name in a list): y,n
ANIMACY: Animacy of both mentions match
(persons, objects): y,n
I/J REFLEXIVE:mi/j is a reflexive pronoun: y,n
I/J TYPE:mi/j is a pronoun (p), entity (e) or nominal (n)
NESTED: One mention is included in the other: y,n
MAXIMALNP: Both mentions have the same NP parent
or they are nested: y,n
I/J MAXIMALNP:mi/j is not included in any
other mention: y,n
I/J EMBEDDED:mi/j is a noun and is not a maximal NP: y,n
BINDING: Conditions B and C of binding theory: y,n
SEMCLASS: Semantic class of both mentions match: y,n,u
(the same as Soon et al (2001))
ALIAS: One mention is an alias of the other: y,n,u
(only entities, else unknown)
Figure 3: Feature functions used
precision over the training data (Pk), but shifted
to be zero-centered: ?k = Pk ? 0.5.
3.2 Pruning
Analyzing the errors of development experiments,
we have found two main error patterns that can be
solved by a pruning process. First, the contribu-
tion of the edge weights for the resolution depends
on the size of the document. And second, many
weak edge weights may sum up to produce a bias
in the wrong direction.
The weight of an edge depends on the weights
assigned for the constraints which apply to a pair
of mentions according to Equation 1. Each ver-
tex is adjacent to all the other vertices. This pro-
duces that the larger the number of adjacencies,
the smaller the influence of a constraint is. A con-
sequence is that resolution for large and short do-
cuments has different results.
Many works have to deal with similar prob-
lems, specially the ones looking backward for an-
tecedents. The larger the document, the more pos-
1089
sible antecedents the system has to classify. This
problem is usually solved looking for antecedents
in a window of few sentences, which entails an
evident limitation of recall.
Regarding the weak edge weights, it is notable
that some kind of mention pairs are very weakly
informative. For example, the pairs (pronoun,
pronoun). Many stories have a few main charac-
ters which monopolize the pronouns of the doc-
ument. This produces many positive training ex-
amples for pairs of pronouns matching in gender
and person, which may lead the algorithm to pro-
duce large coreferential chains joining all these
mentions even for stories where there are many
different characters. For example, we have found
in the results of some documents a huge corefer-
ence chain including every pronoun ?he?. This
is because a pair of mentions (?he?, ?he?) is usu-
ally linked with a small positive weight. Although
the highest adjacent edge weight of a ?he? men-
tion may link with the correct antecedent, the sum
of several edge weights linking the mention with
other ?he? causes the problem.
A pruning process is perfomed solving both
problems and reducing computational costs from
O(n3) to O(n2). For each vertex?s adjacency list
A(vi), only a maximum of N edges remain and the
others are pruned. Concretely, the N/2 edges with
largest positive weight and the N/2 with largest
negative weight. The value of N is empirically
chosen by maximizing performances over training
data. On the one hand, the pruning forces the max-
imum adjacency to be constant and the contribu-
tion of the edge weights does not depend on the
size of the document. On the other hand, most
edges of the less informative pairs are discarded
avoiding further confusion. There are no limita-
tions in distance or other restrictions which may
cause a loss of recall.
3.3 Initial State
The initial state of the vertices define the a pri-
ori probabilities for each vertex to be in each par-
tition. There are several possible initial states.
In the case where no prior information is avail-
able, a random or uniformly distributed state is
commonly used. However, a well-informed initial
state should drive faster the relaxation process to
a better solution. This section describes the well-
informed initial state chosen in our approach and
the random one. Both are compared in the exper-
iments (Section 4.2).
The well-informed initial state favors the cre-
ation of new chains. Variable vi has Li = i pos-
sible values while variable vi+1 has Li + 1. The
probability distribution of vi+1 is equiprobable for
values from 1 to Li but it is the double for the pro-
bability to start a new chain Li + 1.
hil = 1Li+1 , ?l = 1..Li ? 1
hiLi =
2
Li+1
Pronouns do not follow this distribution but a
totally equiprobable one, given that they are usu-
ally anaphoric.
hil = 1Li , ?l = 1..Li
This configuration enables the resolution pro-
cess to determine as singletons the mentions for
which little evidence is available. This small dif-
ference between initial probability weights is also
introduced in order to avoid exceptional cases
where all support values contribute with the same
value.
The random initial state is also used in our
experiments to test that our proposed configura-
tion is better-informed than random. Given the
equiprobability state, we add a random value to
each probability to be in a partition:
hil = 1Li + il, ?l = 0..Li
where il is a random value ?12Li ? il ? 12Li .These little random differences may help the algo-
rithm to avoid local minima.
3.4 Reordering
The vertices of the graph would usually be placed
in the same order as the mentions are found in the
document (chronological). In this manner, vi cor-
responds to mi. However, as suggested by Luo
(2007), there is no need to generate the model
following that order. In our approach, the first
variables have a lower number of possible labels.
Moreover, an error in the first variables has more
influence on the performance than an error in the
later ones. Placing named entities at the beginning
is reasonably to expect that is helpful for the al-
gorithm, given that named entities are usually the
most informative mentions.
1090
Tokens Mentions Entities
bnews train 66627 9937 4408
bnews test 17463 2579 1040
npaper train 68970 11283 4163
npaper test 17404 2483 942
nwire train 70832 10693 4297
nwire test 16772 2608 1137
Figure 4: Statistics about ACE-phase02
Suppose we have three mentions appearing in
this order somewhere in a document: ?A. Smith?,
?he?, ?Alice Smith?. For proximity, mention ?he?
may tend to link with ?A. Smith?. Then, the third
mention ?Alice Smith? clearly is the whole name
of ?A. Smith? but the gender with ?he? does not
agree. Given that our implementation acts like a
directed graph only looking backward (see Sec-
tion 3), mention ?he? won?t change its tendency
and it may cause a split in the ?Alice Smith? coref-
erence chain. However, having named entities in
first place and pronouns at the end, enables the
mention ?he? to determine that ?A. Smith? and
?Alice Smith? having the same label are not good
antecedents.
Reordering only affects on the number of pos-
sible labels of the variables and the list of adjacen-
cies A(vi). The chronological order of the docu-
ment is taken into account by the constraints re-
gardless of the graph representation. Our experi-
ments confirm (Section 4) that placing first named
entity mentions, then nominal mentions and fi-
nally the pronouns, the precision increases consid-
erably. Inside of each of these groups, the order is
the same order of the document.
4 Experiments and Results
We evaluate our approach to coreference res-
olution using ACE-phase02 corpus, which is
composed of three sections: Broadcast News
(BNEWS), Newswire (NWIRE) and Newspaper
(NPAPER). Each section is in turn composed of a
training set and a test set. Figure 4 shows some
statistics about this corpus.
In our experiments, we consider the true men-
tions of ACE. This is because our focus is on
evaluating pairwise approach versus the graph
partitioning approach and also comparing them
to some state-of-the-art approaches which also
use true mentions. Moreover, details on men-
tion identifier systems and their performances are
rarely published by the systems based on auto-
matic identification of mentions and it difficults
the comparison.
To evaluate our system we use CEAF (Luo,
2005) and B3 (Bagga and Baldwin, 1998). CEAF
is computed based on the best one-to-one map be-
tween key coreference chains and response ones.
We use the mention-based similarity metric which
counts the number of common mentions shared
by key coreference chains and response ones. As
we are using true mentions for the experiments,
precision, recall and F1 are the same value and
only F1 is shown. B3 scorer is used for com-
parison reasons. B3 algorithm looks at the pres-
ence/absence of mentions for each entity in the
system output. Precision and recall numbers are
computed for each mention, and the average gives
the final precision and recall numbers.
MUC scorer (Vilain et al, 1995) is not used
in our experiments. Although it has been widely
used in the state of the art, we consider the newer
metrics have overcome some MUC limitations
(Bagga and Baldwin, 1998; Luo, 2005; Klenner
and Ailloud, 2008; Denis and Baldridge, 2008).
Our preprocessing pipeline consists of
FreeLing (Atserias et al, 2006) for sentence
splitting and tokenization, SVMTool (Gimenez
and Marquez, 2004) for part of speech tagging
and BIO (Surdeanu et al, 2005) for named entity
recognition and classification. No lemmatization
neither syntactic analysis are used.
4.1 Baselines
4.1.1 DT with automatic feature selection
The baseline developed in our work is based on
Soon et al (2001) with the improvements of Ng
and Cardie (2002), which uses a Decision Tree
(DT). Many research works use the same refe-
rences in order to evaluate possible improvements
done by their new models or by the incorporation
of new features.
The features used in the baseline are the same
than those used in our proposed system (Figure
3). However, some features are noisy and many
others have redundancy which causes low perfor-
mances using DTs. In order to select the best set
1091
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
DT 60.6 57.8 60.5 59.7 61.0 74.1 66.9
DT Hill 67.8 61.6 65.0 64.8 74.7 69.8 72.2
Table 1: Results ACE-phase02. Comparing baselines based on Decision Trees.
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
DT 60.6 59.5 64.7 61.7 63.3 74.7 68.5
DT + ILP 62.8 60.3 63.7 62.5 72.4 69.2 70.7
DT Hill 67.8 63.2 67.2 66.5 76.8 71.0 73.8
DT Hill + ILP 67.6 63.5 66.7 66.3 80.0 68.3 73.7
Relax 69.5 68.3 73.0 70.4 86.5 67.9 76.1
Table 2: Results on documents shorter than 200 mentions of ACE-phase02
of features a Hill Climbing process has been per-
formed doing a five-fold cross-validation over the
training corpus. A similar feature selection pro-
cess has been done by Hoste (2005).
The Hill Climbing process starts using the
whole set of features. A cross-validation is done
(un)masking each feature. The (un)masked fea-
ture with more improvement is (added to) re-
moved from the set. The process is repeated until
an iteration without improvements is reached.
Note that this optimization process is biased by
the metric used to evaluate each feature combi-
nation. We use CEAF in our experiments, which
encourages precision and consistency.
4.1.2 Integer Linear Programming
The second baseline developed forms the coref-
erence chains given the output of the pair classi-
fication of the first baseline. A set of binary vari-
ables (xij) symbolize whether pairs of mentions
(mi,mj) corefer (xij = 1) or not (xij = 0). An
objective function is defined as follows:
min
?
i<j ?log(Pcij)xij ? log(1? Pcij)(1? xij)
where Pcij is the confidence value of mentions
mi and mj to corefer obtained by the pair clas-
sifier. The minimization of the objective func-
tion is done by Integer Linear Programming (ILP)
in a similar way to (Klenner, 2007; Denis and
Baldridge, 2007; Finkel and Manning, 2008). In
order to keep consistency in the results, which is
the goal of this post-process, a set of triangular
constraints is required. For each three mentions
with indexes i < j < k the corresponding vari-
ables have to satisfy three constraints:
? xik ? xij + xjk ? 1
? xij ? xik + xjk ? 1
? xjk ? xij + xik ? 1
This implies that this model needs, for a doc-
ument with n mentions, 12n(n ? 1) variables and1
2n(n ? 1)(n ? 2) constraints to assure consis-tency2. This is an important limitation with a view
to scalability. In our experiments only documents
shorter than 200 mentions can be solved by this
baseline due to its computational cost.
4.2 Experiments
Four experiments have been done in order to eval-
uate our proposed approach. This section de-
scribes and analyzes the results of each experi-
ment. Finally, our performances are compared
with the state of the art.
The first experiment compares the perfor-
mances of our baselines (Table 1). ?DT? is the
system based on Decision Tree using all the fea-
tures of Figure 3 and ?DT+Hill? is a DT using
the features selected by the Hill Climbing process
(Section 4.1.1). There is a significant improve-
ment in the performances (5.1 points with CEAF,
5.3 with B3) after the automatic feature selection
process is done.
2 1
6n(n ? 1)(n ? 2) for each one of the three triangularconstraints
1092
bnews npaper nwire Global
Metric: CEAF CEAF B3
Model F1 F1 F1 F1 P R F1
Relax 67.3 64.4 69.5 67.2 88.4 62.7 73.3
Relax pruning 68.6 65.2 70.1 68.0 82.3 66.9 73.8
Relax pruning & reorder 69.5 67.3 72.1 69.7 85.3 66.8 74.9
Relax random IS 68.2 66.1 71.0 68.5 83.5 66.7 74.2
MaxEnt+ILP (Denis, 2007) - - - 66.2 81.4 65.6 72.7
Rankers (Denis, 2007) 65.7 65.3 68.1 67.0 79.8 66.8 72.7
Table 3: Results ACE-phase02.
In the second experiment the ILP chain forma-
tion process is applied using the output of both
DTs. Results are shown in Table 2. Note that ILP
only applies to documents shorter than 200 men-
tions due to its excessive computational cost (Sec-
tion 4.1.2). Results for Relax applied to the same
documents are also included for comparison. ILP
forces consistency of the results producing an in-
crease in precision score with B3 metric in both
cases. However, ?DT+Hill? has been optimized
for CEAF metric which encourages precision and
consistency. For this, a post-process forcing con-
sistency seems unnecessary for a classifier already
optimized. Relax significantly outperforms all the
baselines.
The third experiment shows the improvements
achieved by the use of pruning and reordering
techniques (Sections 3.2 and 3.4). Table 3 shows
the results. Pruning improves performances with
both metrics. B3 precision is decreased but the
global F1 is increased due to a considerably im-
provement of recall. Reordering recovers the pre-
cision lost by the pruning without loosing recall,
which achieves the best performances of 69.7 with
CEAF and 74.9 with B3.
The fourth experiment evaluates the influence
of the initial state. A comparison is done with
the proposed initial state (Section 3.3) and the
random one. The results shown in Table 3
for random initial state are the average of 3
executions. The system called ?Relax random
IS? is using the same values for pruning and
reordering techniques than the best result of
previous experiment: ?Relax pruning & reorder?.
As expected, results with a well-informed initial
state outperform the random ones.
Finally, Relax performances are compared with
the best scores we have found using the same cor-
pora and metrics. We compare our approach with
specialized Rankers ?groupwise classifier?, and
a system using ILP not only forcing consistency
but also using information about anaphoricity and
named entities. Relax outperforms both systems
with both metrics (Table 3).
5 Conclusion
The approach for coreference resolution presented
in this paper is a constraint-based graph partition-
ing solved by relaxation labeling.
The decision to join or not a set of mentions
in the same entity is taken considering always the
whole set of previous mentions like in groupwise
classifiers. Contrarily to the approaches where
variables are the linkage of each pair of mentions,
in this model consistency is implicitly forced.
Moreover, the influence of the partial results of
the other mentions at the same time avoids that
decisions are independently taken.
The capacity to easily incorporate constraints
from different sources and using different know-
ledge is also remarkable. This flexibility gives
a great potencial to the approach. Anaphoricity
filtering is not needed given that the necessary
knowledge can be also introduced by constraints.
In addition, three tecniques to improve results
have been presented: reordering, pruning and fea-
ture selection by Hill Climbing. The experiments
confirm their utility.
The experimental results clearly outperform the
baselines with separate classification and chain
formaiton. The approach also outperforms oth-
ers in the state of the art using same corpora and
metrics.
1093
References
Aone, C. and S.W. Bennett. 1995. Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proceedings of the 33rd annual meeting on ACL, pages
122?129.
Atserias, J., B. Casas, E. Comelles, M. Gonza?lez, L. Padro?,
and M. Padro?. 2006. Freeling 1.3: Syntactic and semantic
services in an open-source nlp library. In Proceedings of
the fifth international conference on Language Resources
and Evaluation (LREC 2006), ELRA. Genoa, Italy.
Atserias, J. 2006. Towards Robustness in Natural Lan-
guage Understanding. Ph.D. Thesis, Dept. Lenguajes
y Sistemas Informa?ticos. Euskal Herriko Unibertsitatea.
Donosti. Spain.
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring
coreference chains. Proceedings of the Linguistic Coref-
erence Workshop at LREC, pages 563?566.
Culotta, A., M. Wick, and A. McCallum. 2007. First-Order
Probabilistic Models for Coreference Resolution. Pro-
ceedings of NAACL HLT, pages 81?88.
Denis, P. and J. Baldridge. 2007. Joint Determination of
Anaphoricity and Coreference Resolution using Integer
Programming. Proceedings of NAACL HLT, pages 236?
243.
Denis, P. and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. Proceedings of the
EMNLP, Hawaii, USA.
Denis, P. 2007. New Learning Models for Robust Refer-
ence Resolution. Ph.D. dissertation, University of Texas
at Austin.
Finkel, J.R. and C.D. Manning. 2008. Enforcing transitivity
in coreference resolution. In Proceedings of the 46th An-
nual Meeting of the ACL HLT: Short Papers, pages 45?48.
Association for Computational Linguistics.
Gimenez, J. and L. Marquez. 2004. Svmtool: A general pos
tagger generator based on support vector machines. In
Proceedings of the 4th International Conference on Lan-
guage Resources and Evaluation, pages 43?46.
Hoste, V. 2005. Optimization Issues in Machine Learning of
Coreference Resolution. PhD thesis.
Hummel, R. A. and S. W. Zucker. 1987. On the foundations
of relaxation labeling processes. pages 585?605.
Klenner, M. and E?. Ailloud. 2008. Enhancing Coreference
Clustering. In Proceedings of the Second Workshop on
Anaphora Resolution. WAR II.
Klenner, M. and E. Ailloud. 2009. Optimization in Corefer-
ence Resolution Is Not Needed: A Nearly-Optimal Algo-
rithm with Intensional Constraints. In Proceedings of the
12th Conference of the EACL.
Klenner, M. 2007. Enforcing consistency on coreference
sets. In Recent Advances in Natural Language Processing
(RANLP), pages 323?328.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of 42nd ACL, page 135.
Luo, X. 2005. On coreference resolution performance met-
rics. Proc. of HLT-EMNLP, pages 25?32.
Luo, X. 2007. Coreference or not: A twin model for coref-
erence resolution. In Proceedings of NAACL HLT, pages
73?80.
Ma`rquez, L., L. Padro?, and H. Rodr??guez. 2000. A ma-
chine learning approach for pos tagging. Machine Learn-
ing Journal, 39(1):59?91.
McCallum, A. and B. Wellner. 2005. Conditional models
of identity uncertainty with application to noun corefer-
ence. Advances in Neural Information Processing Sys-
tems, 17:905?912.
McCarthy, J.F. and W.G. Lehnert. 1995. Using decision
trees for coreference resolution. Proceedings of the Four-
teenth International Conference on Artificial Intelligence,
pages 1050?1055.
Ng, V. and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. Proceedings of the
40th Annual Meeting on Association for Computational
Linguistics, pages 104?111.
Nicolae, C. and G. Nicolae. 2006. Best Cut: A Graph Al-
gorithm for Coreference Resolution. Proceedings of the
2006 Conference on EMNLP, pages 275?283.
Pelillo, M. 1997. The dynamics of nonlinear relaxation la-
beling processes. Journal of Mathematical Imaging and
Vision, 7(4):309?323.
Quinlan, J.R. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
Rosenfeld, R., R. A. Hummel, and S. W. Zucker. 1976.
Scene labelling by relaxation operations. IEEE Transac-
tions on Systems, Man and Cybernetics, 6(6):420?433.
Soon, W.M., H.T. Ng, and D.C.Y. Lim. 2001. A Machine
Learning Approach to Coreference Resolution of Noun
Phrases. Computational Linguistics, 27(4):521?544.
Surdeanu, M., J. Turmo, and E. Comelles. 2005. Named
Entity Recognition from Spontaneous Open-Domain
Speech. In Ninth European Conference on Speech Com-
munication and Technology. ISCA.
Torras, C. 1989. Relaxation and neural learning: Points
of convergence and divergence. Journal of Parallel and
Distributed Computing, 6:217?244.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. Proceedings of the 6th conference on Mes-
sage understanding, pages 45?52.
Yang, X., G. Zhou, J. Su, and C.L. Tan. 2003. Coreference
resolution using competition learning approach. In ACL
?03: Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
1094
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens
?
Llu??s M
`
arquez
?
Emili Sapena
?
M. Ant
`
onia Mart??
?
Mariona Taul
?
e
?
V
?
eronique Hoste
?
Massimo Poesio

Yannick Versley
??
?: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
?: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
?: University College Ghent, veronique.hoste@hogent.be
: University of Essex/University of Trento, poesio@essex.ac.uk
??: University of T?ubingen, versley@sfs.uni-tuebingen.de
Abstract
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
1 Introduction
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
(1) Major League Baseball sent its head of se-
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al, 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al, 2004), and ARE
(Orasan et al, 2008), yet many questions remain
open:
? To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
? How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
? How (dis)similar are different coreference
evaluation metrics?MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains?composed of named entities
(NEs), pronouns, and full noun phrases?in four
different scenarios. For more information, the
reader is referred to the task website.
1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
2 Linguistic Resources
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
1
http://stel.ub.edu/semeval2010-coref
1
Training Development Test
#docs #sents #tokens #docs #sents #tokens #docs #sents #tokens
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
Table 1: Size of the task datasets.
2.1 Source Corpora
Catalan and Spanish The AnCora corpora (Re-
casens and Mart??, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri?odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al, 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.
2
German The T?uBa-D/Z corpus (Hinrichs et al,
2005) is a newspaper treebank based on data taken
from the daily issues of ?die tageszeitung? (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.
2
Italian The LiveMemories corpus (Rodr??guez
et al, 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
2
Free user license agreements for the English and German
task datasets were issued to the task participants.
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.
3
2.2 Preprocessing Systems
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing
4
for
Catalan/Spanish and SVMTagger
5
for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.
6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al, 1999), and de-
pendency information by the Alpino parser (van
Noord et al, 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,
7
and dependency information by Malt-
Parser.
8
3
The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4
http://www.lsi.upc.es/ nlp/freeling
5
http://www.lsi.upc.edu/ nlp/SVMTool
6
http://www.lsi.upc.edu// xlluis/?x=cat:5
7
http://textpro.fbk.eu
8
http://maltparser.org
2
3 Task Description
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,
9
thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
? Only NP constituents and possessive deter-
miners can be mentions.
? Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
? Singletons are also considered as entities
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
3.1 Data Format
The task datasets as well as the participants?
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).
10
Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by ?#begin document ID? and ?#end doc-
ument ID? comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by ?\t? characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
9
Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
10
http://www.cnts.ua.ac.be/conll2008
ID Token Intermediate columns Coref
1 Major . . . (1
2 League . . .
3 Baseball . . . 1)
4 sent . . .
5 its . . . (1)|(2
6 head . . .
7 of . . .
8 security . . . (3)|2)
9 to . . .
. . . . . . . . . . . .
27 The . . . (1
28 league . . . 1)
29 is . . .
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (?0? if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (?sentence? if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are ?gold-
standard columns,? whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted?using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
3
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
3.2 Evaluation Settings
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
Gold-standard versus Regular setting. Only
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
3.3 Evaluation Metrics
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B
3
(Bagga and Baldwin, 1998) and
CEAF-?
3
(Luo, 2005), and the two link-based
metrics MUC (Vilain et al, 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F
1
. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
4 Participating Systems
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
5 Results and Evaluation
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B
3
for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,
11
thus making it difficult to draw firm conclu-
11
Only 45 entries in Table 5 from 192 potential cases.
4
System Architecture ML Methods External Resources
BART
(Broscheit et al, 2010) Closest-first with entity-
mention model (English),
Closest-first model (German,
Italian)
MaxEnt (English, Ger-
man), Decision trees
(Italian)
GermaNet & gazetteers (Ger-
man), I-Cab gazetteers (Italian),
Berkeley parser, Stanford NER,
WordNet, Wikipedia name list,
U.S. census data (English)
Corry
(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-
Net, U.S. census data
RelaxCor
(Sapena et al, 2010) Graph partitioning (solved by
relaxation labeling)
Decision trees, Rules WordNet
SUCRE
(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-
tional database model, Regular
feature definition language
Decision trees, Naive
Bayes, SVM, MaxEnt
?
TANL-1
(Attardi et al, 2010) Highest entity-mention simi-
larity
MaxEnt PoS tagger (Italian)
UBIU
(Zhekova and K?ubler, 2010) Pairwise model MBL ?
Table 3: Main characteristics of the participating systems.
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear ?win-
ners.? Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closed?regular for English, German, and Italian,
although it never outperforms the CEAF or B
3
sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B
3
. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:open?gold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B
3
(Pearson?s r = 0.91,
p< 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son?s r = 0.44, p< 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F
1
points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closed?gold to open?gold.
6 Conclusions
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
5
CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
Table 4: Baseline scores.
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
Acknowledgments
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph?ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes?us Gim?enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu??s (UPC), Montse
Nofre (UB), Llu??s Padr?o (UPC), Kepa Joseba
Rodr??guez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
6
Mention detection CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P F
1
R P Blanc
Catalan
closed?gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed?regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open?gold
open?regular
Dutch
closed?gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed?regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open?gold
open?regular
English
closed?gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed?regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open?gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open?regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed?gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed?regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open?gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open?regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed?gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed?regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open?gold
open?regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed?gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed?regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open?gold
open?regular
Table 5: Official results of the participating systems for all languages, settings, and metrics.
7
References
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodr??guez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program ? Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837?840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47?54.
Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13?20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition ? Version 3.0.
In Proceedings of MUC-7.
V?eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432?
1437.
Hamidreza Kobdani and Hinrich Sch?utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050?1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85?89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant?onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517?526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodr??guez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157?163.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777?784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47?50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663?1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52.
Desislava Zhekova and Sandra K?ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
8
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 88?91,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
RelaxCor: A Global Relaxation Labeling Approach to Coreference
Resolution
Emili Sapena, Llu??s Padr
?
o and Jordi Turmo
TALP Research Center
Universitat Polit`ecnica de Catalunya
Barcelona, Spain
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper describes the participation
of RelaxCor in the Semeval-2010 task
number 1: ?Coreference Resolution in
Multiple Languages?. RelaxCor is a
constraint-based graph partitioning ap-
proach to coreference resolution solved by
relaxation labeling. The approach com-
bines the strengths of groupwise classifiers
and chain formation methods in one global
method.
1 Introduction
The Semeval-2010 task is concerned with intra-
document coreference resolution for six different
languages: Catalan, Dutch, English, German, Ital-
ian and Spanish. The core of the task is to iden-
tify which noun phrases (NPs) in a text refer to the
same discourse entity (Recasens et al, 2010).
RelaxCor (Sapena et al, 2010) is a graph rep-
resentation of the problem solved by a relaxation
labeling process, reducing coreference resolution
to a graph partitioning problem given a set of con-
straints. In this manner, decisions are taken con-
sidering the whole set of mentions, ensuring con-
sistency and avoiding that classification decisions
are independently taken.
The paper is organized as follows. Section 2 de-
scribes RelaxCor, the system used in the Semeval
task. Next, Section 3 describes the tuning needed
by the system to adapt it to different languages and
other task issues. The same section also analyzes
the obtained results. Finally, Section 4 concludes
the paper.
2 System Description
This section briefly describes RelaxCor. First, the
graph representation is presented. Next, there is
an explanation of the methodology used to learn
constraints and train the system. Finally, the algo-
rithm used for resolution is described.
2.1 Problem Representation
LetG = G(V,E) be an undirected graph where V
is a set of vertices and E a set of edges. Let m =
(m
1
, ...,m
n
) be the set of mentions of a document
with n mentions to resolve. Each mention m
i
in
the document is represented as a vertex v
i
? V
in the graph. An edge e
ij
? E is added to the
graph for pairs of vertices (v
i
, v
j
) representing the
possibility that both mentions corefer.
Let C be our set of constraints. Given a pair of
mentions (m
i
, m
j
), a subset of constraints C
ij
?
C restrict the compatibility of both mentions. C
ij
is used to compute the weight value of the edge
connecting v
i
and v
j
. Let w
ij
? W be the weight
of the edge e
ij
:
w
ij
=
?
k?C
ij
?
k
f
k
(m
i
,m
j
) (1)
where f
k
(?) is a function that evaluates the con-
straint k and ?
k
is the weight associated to the
constraint. Note that ?
k
and w
ij
can be negative.
In our approach, each vertex (v
i
) in the graph
is a variable (v
i
) for the algorithm. Let L
i
be the
number of different values (labels) that are possi-
ble for v
i
. The possible labels of each variable are
the partitions that the vertex can be assigned. A
vertex with index i can be in the first i partitions
(i.e. L
i
= i).
88
Distance and position:
DIST: Distance betweenm
i
andm
j
in sentences: number
DIST MEN: Distance betweenm
i
andm
j
in mentions: number
APPOSITIVE: One mention is in apposition with the other: y,n
I/J IN QUOTES:m
i/j
is in quotes or inside a NP or a sentence
in quotes: y,n
I/J FIRST:m
i/j
is the first mention in the sentence: y,n
Lexical:
I/J DEF NP:m
i/j
is a definitive NP: y,n
I/J DEM NP:m
i/j
is a demonstrative NP: y,n
I/J INDEF NP:m
i/j
is an indefinite NP: y,n
STR MATCH: String matching ofm
i
andm
j
: y,n
PRO STR: Both are pronouns and their strings match: y,n
PN STR: Both are proper names and their strings match: y,n
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns: y,n
HEAD MATCH: String matching of NP heads: y,n
Morphological:
NUMBER: The number of both mentions match: y,n,u
GENDER: The gender of both mentions match: y,n,u
AGREEMENT: Gender and number of both
mentions match: y,n,u
I/J THIRD PERSON:m
i/j
is 3rd person: y,n
PROPER NAME: Both mentions are proper names: y,n,u
I/J PERSON:m
i/j
is a person (pronoun or
proper name in a list): y,n
ANIMACY: Animacy of both mentions match
(persons, objects): y,n
I/J REFLEXIVE:m
i/j
is a reflexive pronoun: y,n
I/J TYPE:m
i/j
is a pronoun (p), entity (e) or nominal (n)
Syntactic:
NESTED: One mention is included in the other: y,n
MAXIMALNP: Both mentions have the same NP parent
or they are nested: y,n
I/J MAXIMALNP:m
i/j
is not included in any
other mention: y,n
I/J EMBEDDED:m
i/j
is a noun and is not a maximal NP: y,n
BINDING: Conditions B and C of binding theory: y,n
Semantic:
SEMCLASS: Semantic class of both mentions match: y,n,u
(the same as (Soon et al, 2001))
ALIAS: One mention is an alias of the other: y,n,u
(only entities, else unknown)
I/J SRL ARG: Semantic role ofm
i/j
: N,0,1,2,3,4,M,L
SRL SAMEVERB: Both mentions have a semantic role
for the same verb: y,n
Figure 1: Feature functions used.
2.2 Training Process
Each pair of mentions (m
i
, m
j
) in a training doc-
ument is evaluated by the set of feature functions
shown in Figure 1. The values returned by these
functions form a positive example when the pair
of mentions corefer, and a negative one otherwise.
Three specialized models are constructed depend-
ing on the type of anaphor mention (m
j
) of the
pair: pronoun, named entity or nominal.
A decision tree is generated for each specialized
model and a set of rules is extracted with C4.5
rule-learning algorithm (Quinlan, 1993). These
rules are our set of constraints. The C4.5rules al-
gorithm generates a set of rules for each path from
the learned tree. It then checks if the rules can be
generalized by dropping conditions.
Given the training corpus, the weight of a con-
straint C
k
is related with the number of exam-
ples where the constraint applies A
C
k
and how
many of them corefer C
C
k
. We define ?
k
as
the weight of constraint C
k
calculated as follows:
?
k
=
C
C
k
A
C
k
? 0.5
2.3 Resolution Algorithm
Relaxation labeling (Relax) is a generic name for
a family of iterative algorithms which perform
function optimization, based on local informa-
tion (Hummel and Zucker, 1987). The algorithm
solves our weighted constraint satisfaction prob-
lem dealing with the edge weights. In this manner,
each vertex is assigned to a partition satisfying as
many constraints as possible. To do that, the al-
gorithm assigns a probability for each possible la-
bel of each variable. Let H = (h
1
,h
2
, . . . ,h
n
) be
the weighted labeling to optimize, where each h
i
is a vector containing the probability distribution
of v
i
, that is: h
i
= (h
i
1
, h
i
2
, . . . , h
i
L
i
). Given that
the resolution process is iterative, the probability
for label l of variable v
i
at time step t is h
i
l
(t), or
simply h
i
l
when the time step is not relevant.
Initialize:
H := H
0
,
Main loop:
repeat
For each variable v
i
For each possible label l for v
i
S
il
=
?
j?A(v
i
)
w
ij
? h
j
l
End for
For each possible label l for v
i
h
i
l
(t + 1) =
h
i
l
(t)?(1+S
il
)
?
L
i
k=1
h
i
k
(t)?(1+S
ik
)
End for
End for
Until no more significant changes
Figure 2: Relaxation labeling algorithm
The support for a pair variable-label (S
il
) ex-
presses how compatible is the assignment of la-
bel l to variable v
i
taking into account the labels
of adjacent variables and the edge weights. The
support is defined as the sum of the edge weights
that relate variable v
i
with each adjacent variable
v
j
multiplied by the weight for the same label l of
variable v
j
: S
il
=
?
j?A(v
i
)
w
ij
? h
j
l
where w
ij
is
the edge weight obtained in Equation 1 and vertex
v
i
has |A(v
i
)| adjacent vertices. In our version of
the algorithm for coreference resolution A(v
i
) is
the list of adjacent vertices of v
i
but only consid-
ering the ones with an index k < i.
The aim of the algorithm is to find a weighted
labeling such that global consistency is maxi-
mized. Maximizing global consistency is defined
89
Figure 3: Representation of Relax. The vertices represent-
ing mentions are connected by weighted edges e
ij
. Each ver-
tex has a vector h
i
of probabilities to belong to different par-
titions. The figure shows h
2
, h
3
and h
4
.
as maximizing the average support for each vari-
able. The final partitioning is directly obtained
from the weighted labeling H assigning to each
variable the label with maximum probability.
The pseudo-code of the relaxation algorithm
can be found in Figure 2. The process updates
the weights of the labels in each step until con-
vergence, i.e. when no more significant changes
are done in an iteration. Finally, the assigned label
for a variable is the one with the highest weight.
Figure 3 shows an example of the process.
3 Semeval task participation
RelaxCor have participated in the Semeval task for
English, Catalan and Spanish. The system does
not detect the mentions of the text by itself. Thus,
the participation has been restricted to the gold-
standard evaluation, which includes the manual
annotated information and also provides the men-
tion boundaries.
All the knowledge required by the feature func-
tions (Figure 1) is obtained from the annota-
tions of the corpora and no external resources
have been used, with the exception of WordNet
(Miller, 1995) for English. In this case, the sys-
tem has been run two times for English: English-
open, using WordNet, and English-closed, without
WordNet.
3.1 Language and format adaptation
The whole methodology of RelaxCor including
the resolution algorithm and the training process
is totally independent of the language of the docu-
ment. The only parts that need few adjustments are
the preprocess and the set of feature functions. In
most cases, the modifications in the feature func-
tions are just for the different format of the data
for different languages rather than for specific lan-
guage issues. Moreover, given that the task in-
cludes many information about the mentions of the
documents such as part of speech, syntactic depen-
dency, head and semantic role, no preprocess has
been needed.
One of the problems we have found adapting the
system to the task corpora was the large amount
of available data. As described in Section 2.2,
the training process generates a feature vector for
each pair of mentions into a document for all
the documents of the training data set. However,
the great number of training documents and their
length overwhelmed the software that learns the
constraints. In order to reduce the amount of pair
examples, we run a clustering process to reduce
the number of negative examples using the posi-
tive examples as the centroids. Note that negative
examples are near 94% of the training examples,
and many of them are repeated. For each positive
example (a corefering pair of mentions), only the
negative examples with distance less than a thresh-
old d are included in the final training data. The
distance is computed as the number of different
values inside the feature vector. After some exper-
iments over development data, the value of d was
assigned to 3. Thus, the negative examples were
discarded when they have more than three features
different than any positive example.
Our results for the development data set are
shown in Table 1.
3.2 Results analysis
Results of RelaxCor for the test data set are shown
in Table 2. One of the characteristics of the sys-
tem is that the resolution process always takes
into account the whole set of mentions and avoids
any possible pair-linkage contradiction as well as
forces transitivity. Therefore, the system favors
the precision, which results on high scores with
metrics CEAF and B
3
. However, the system is
penalized with the metrics based on pair-linkage,
specially with MUC. Although RelaxCor has the
highest precision scores even for MUC, the recall
is low enough to finally obtain low scores for F
1
.
Regarding the test scores of the task comparing
with the other participants (Recasens et al, 2010),
RelaxCor obtains the best performances for Cata-
90
- CEAF MUC B
3
language R P F
1
R P F
1
R P F
1
ca 69.7 69.7 69.7 27.4 77.9 40.6 67.9 96.1 79.6
es 70.8 70.8 70.8 30.3 76.2 43.4 68.9 95.0 79.8
en-closed 74.8 74.8 74.8 21.4 67.8 32.6 74.1 96.0 83.7
en-open 75.0 75.0 75.0 22.0 66.6 33.0 74.2 95.9 83.7
Table 1: Results on the development data set
- CEAF MUC B
3
BLANC
language R P F
1
R P F
1
R P F
1
R P Blanc
Information: closed Annotation: gold
ca 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
es 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
en 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
Information: open Annotation: gold
en 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
Table 2: Results of the task
lan (CEAF and B
3
), English (closed: CEAF and
B
3
; open: B
3
) and Spanish (B
3
). Moreover, Relax-
Cor is the most precise system for all the metrics
in all the languages except for CEAF in English-
open and Spanish. This confirms the robustness of
the results of RelaxCor but also remarks that more
knowledge or more information is needed to in-
crease the recall of the system without loosing this
precision
The incorporation of WordNet to the English
run is the only difference between English-open
and English-closed. The scores are slightly higher
when using WordNet but not significant. Analyz-
ing the MUC scores, note that the recall is im-
proved, while precision decreases a little which
corresponds with the information and the noise
that WordNet typically provides.
The results for the test and development are
very similar as expected, except the Spanish (es)
ones. The recall considerably falls from develop-
ment to test. It is clearly shown in the MUC recall
and also is indirectly affecting on the other scores.
4 Conclusion
The participation of RelaxCor to the Semeval
coreference resolution task has been useful to eval-
uate the system in multiple languages using data
never seen before. Many published systems typi-
cally use the same data sets (ACE and MUC) and
it is easy to unintentionally adapt the system to the
corpora and not just to the problem. This kind of
tasks favor comparisons between systems with the
same framework and initial conditions.
The results obtained confirm the robustness of
the RelaxCor, and the performance is considerably
good in the state of the art. The system avoids con-
tradictions in the results which causes a high pre-
cision. However, more knowledge is needed about
the mentions in order to increase the recall without
loosing that precision. A further error analysis is
needed, but one of the main problem is the lack of
semantic information and world knowledge spe-
cially for the nominal mentions ? the mentions that
are NPs but not including named entities neither
pronouns?.
Acknowledgments
The research leading to these results has received funding
from the European Community?s Seventh Framework Pro-
gramme (FP7/2007-2013) under Grant Agreement number
247762 (FAUST), and from the Spanish Science and Inno-
vation Ministry, via the KNOW2 project (TIN2009-14715-
C04-04).
References
R. A. Hummel and S. W. Zucker. 1987. On the foundations
of relaxation labeling processes. pages 585?605.
G.A. Miller. 1995. WordNet: a lexical database for English.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learning.
Morgan Kaufmann.
M. Recasens, L. M`arquez, E. Sapena, M.A. Mart??, M. Taul?e,
V. Hoste, M. Poesio, and Y. Versley. 2010. SemEval-2010
Task 1: Coreference resolution in multiple languages. In
Proceedings of the 5th International Workshop on Seman-
tic Evaluations (SemEval-2010), Uppsala, Sweden.
E. Sapena, L. Padr?o, and J. Turmo. 2010. A Global Relax-
ation Labeling Approach to Coreference Resolution. Sub-
mitted.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A Machine
Learning Approach to Coreference Resolution of Noun
Phrases. Computational Linguistics, 27(4):521?544.
91
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 35?39,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
RelaxCor Participation in CoNLL Shared Task on Coreference Resolution
Emili Sapena, Llu??s Padro? and Jordi Turmo?
TALP Research Center
Universitat Polite`cnica de Catalunya
Barcelona, Spain
{esapena, padro, turmo}@lsi.upc.edu
Abstract
This paper describes the participation of
RELAXCOR in the CoNLL-2011 shared
task: ?Modeling Unrestricted Coreference in
Ontonotes?. RELAXCOR is a constraint-based
graph partitioning approach to coreference
resolution solved by relaxation labeling. The
approach combines the strengths of groupwise
classifiers and chain formation methods in one
global method.
1 Introduction
The CoNLL-2011 shared task (Pradhan et al, 2011)
is concerned with intra-document coreference reso-
lution in English, using Ontonotes corpora. The core
of the task is to identify which expressions (usually
NPs) in a text refer to the same discourse entity.
This paper describes the participation of RELAX-
COR and is organized as follows. Section 2 de-
scribes RELAXCOR, the system used in the task.
Next, Section 3 describes the tuning needed by the
system to adapt it to the task issues. The same sec-
tion also analyzes the obtained results. Finally, Sec-
tion 4 concludes the paper.
2 System description
RELAXCOR (Sapena et al, 2010a) is a coreference
resolution system based on constraint satisfaction.
It represents the problem as a graph connecting any
?Research supported by the Spanish Science and Innova-
tion Ministry, via the KNOW2 project (TIN2009-14715-C04-
04) and from the European Community?s Seventh Framework
Programme (FP7/2007-2013) under Grant Agreement number
247762 (FAUST)
pair of candidate coreferent mentions and applies re-
laxation labeling, over a set of constraints, to decide
the set of most compatible coreference relations.
This approach combines classification and cluster-
ing in one step. Thus, decisions are taken consider-
ing the entire set of mentions, which ensures consis-
tency and avoids local classification decisions. The
RELAXCOR implementation used in this task is an
improved version of the system that participated in
the SemEval-2010 Task 1 (Recasens et al, 2010).
The knowledge of the system is represented as a
set of weighted constraints. Each constraint has an
associated weight reflecting its confidence. The sign
of the weight indicates that a pair or a group of men-
tions corefer (positive) or not (negative). Only con-
straints over pairs of mentions were used in the cur-
rent version of RELAXCOR. However, RELAXCOR
can handle higher-order constraints. Constraints can
be obtained from any source, including a training
data set from which they can be manually or auto-
matically acquired.
The coreference resolution problem is represented
as a graph with mentions in the vertices. Mentions
are connected to each other by edges. Edges are as-
signed a weight that indicates the confidence that the
mention pair corefers or not. More specifically, an
edge weight is the sum of the weights of the con-
straints that apply to that mention pair. The larger
the edge weight in absolute terms, the more reliable.
RELAXCOR uses relaxation labeling for the res-
olution process. Relaxation labeling is an iterative
algorithm that performs function optimization based
on local information. It has been widely used to
solve NLP problems. An array of probability values
35
is maintained for each vertex/mention. Each value
corresponds to the probability that the mention be-
longs to a specific entity given all the possible enti-
ties in the document. During the resolution process,
the probability arrays are updated according to the
edge weights and probability arrays of the neighbor-
ing vertices. The larger the edge weight, the stronger
the influence exerted by the neighboring probability
array. The process stops when there are no more
changes in the probability arrays or the maximum
change does not exceed an epsilon parameter.
2.1 Attributes and Constraints
For the present study, all constraints were learned
automatically using more than a hundred attributes
over the mention pairs in the training sets. Usual at-
tributes were used for each pair of mentions (mi,mj)
?where i < j following the order of the document?
, like those in (Sapena et al, 2010b), but bina-
rized for each possible value. In addition, a set
of new mention attributes were included such as
SAME SPEAKER when both mentions have the
same speaker1 (Figures 1 and 2).
A decision tree was generated from the train-
ing data set, and a set of constraints was extracted
with the C4.5 rule-learning algorithm (Quinlan,
1993). The so-learned constraints are conjunctions
of attribute-value pairs. The weight associated with
each constraint is the constraint precision minus a
balance value, which is determined using the devel-
opment set. Figure 3 is an example of a constraint.
2.2 Training data selection
Generating an example for each possible pair of
mentions produces an unbalanced dataset where
more than 99% of the examples are negative (not
coreferent), even more considering that the mention
detection system has a low precision (see Section
3.1). So, it generates large amounts of not coref-
erent mentions. In order to reduce the amount of
negative pair examples, a clustering process is run
using the positive examples as the centroids. For
each positive example, only the negative examples
with distance equal or less than a threshold d are
included in the final training data. The distance is
computed as the number of different attribute values
1This information is available in the column ?speaker? of
the corpora.
Distance and position:
Distance between mi and mj in sentences:
DIST SEN 0: same sentence
DIST SEN 1: consecutive sentences
DIST SEN L3: less than 3 sentences
Distance between mi and mj in phrases:
DIST PHR 0, DIST PHR 1, DIST PHR L3
Distance between mi and mj in mentions:
DIST MEN 0, DIST MEN L3, DIST MEN L10
APPOSITIVE: One mention is in apposition with the other.
I/J IN QUOTES: mi/j is in quotes or inside a NP
or a sentence in quotes.
I/J FIRST: mi/j is the first mention in the sentence.
Lexical:
STR MATCH: String matching of mi and mj
PRO STR: Both are pronouns and their strings match
PN STR: Both are proper names and their strings match
NONPRO STR: String matching like in Soon et al (2001)
and mentions are not pronouns.
HEAD MATCH: String matching of NP heads
TERM MATCH: String matching of NP terms
I/J HEAD TERM: mi/j head matches with the term
Morphological:
The number of both mentions match:
NUMBER YES, NUMBER NO, NUMBER UN
The gender of both mentions match:
GENDER YES, GENDER NO, GENDER UN
Agreement: Gender and number of both mentions match:
AGREEMENT YES, AGREEMENT NO, AGREEMENT UN
Closest Agreement: mi is the first agreement found
looking backward from mj : C AGREEMENT YES,
C AGREEMENT NO, C AGREEMENT UN
I/J THIRD PERSON: mi/j is 3rd person
I/J PROPER NAME: mi/j is a proper name
I/J NOUN: mi/j is a common noun
ANIMACY: Animacy of both mentions match (person, object)
I/J REFLEXIVE: mi/j is a reflexive pronoun
I/J POSSESSIVE: mi/j is a possessive pronoun
I/J TYPE P/E/N: mi/j is a pronoun (p), NE (e) or nominal (n)
Figure 1: Mention-pair attributes (1/2).
inside the feature vector. After some experiments
over development data, the value of d was assigned
to 5. Thus, the negative examples were discarded
when they have more than five attribute values dif-
ferent than any positive example. So, in the end,
22.8% of the negative examples are discarded. Also,
both positive and negative examples with distance
zero (contradictions) are discarded.
2.3 Development process
The current version of RELAXCOR includes a pa-
rameter optimization process using the development
data sets. The optimized parameters are balance and
pruning. The former adjusts the constraint weights
to improve the balance between precision and re-
call as shown in Figure 4; the latter limits the num-
ber of neighbors that a vertex can have. Limiting
36
Syntactic:
I/J DEF NP: mi/j is a definite NP.
I/J DEM NP: mi/j is a demonstrative NP.
I/J INDEF NP: mi/j is an indefinite NP.
NESTED: One mention is included in the other.
MAXIMALNP: Both mentions have the same NP parent
or they are nested.
I/J MAXIMALNP: mi/j is not included in any other NP.
I/J EMBEDDED: mi/j is a noun and is not a maximal NP.
C COMMANDS IJ/JI: mi/j C-Commands mj/i.
BINDING POS: Condition A of binding theory.
BINDING NEG: Conditions B and C of binding theory.
I/J SRL ARG N/0/1/2/X/M/L/Z: Syntactic argument of mi/j .
SAME SRL ARG: Both mentions are the same argument.
I/J COORDINATE: mi/j is a coordinate NP
Semantic:
Semantic class of both mentions match
(the same as (Soon et al, 2001))
SEMCLASS YES, SEMCLASS NO, SEMCLASS UN
One mention is an alias of the other:
ALIAS YES, ALIAS NO, ALIAS UN
I/J PERSON: mi/j is a person.
I/J ORGANIZATION: mi/j is an organization.
I/J LOCATION: mi/j is a location.
SRL SAMEVERB: Both mentions have a semantic role
for the same verb.
SRL SAME ROLE: The same semantic role.
SAME SPEAKER: The same speaker for both mentions.
Figure 2: Mention-pair attributes (2/2).
DIST SEN 1 & GENDER YES & I FIRST &
I MAXIMALNP & J MAXIMALNP &
I SRL ARG 0 & J SRL ARG 0 &
I TYPE P & J TYPE P
Precision: 0.9581
Training examples: 501
Figure 3: Example of a constraint. It applies when the distance
between mi and mj is exactly 1 sentence, their gender match,
both are maximal NPs, both are argument 0 (subject) of their
respective sentences, both are pronouns, and mi is not the first
mention of its sentence. The final weight will be weight =
precision? balance.
the number of neighbors reduces the computational
cost significantly and improves overall performance
too. Optimizing this parameter depends on proper-
ties like document size and the quality of the infor-
mation given by the constraints.
The development process calculates a grid given
the possible values of both parameters: from 0 to 1
for balance with a step of 0.05, and from 2 to 14
for pruning with a step of 2. Both parameters were
empirically adjusted on the development set for the
evaluation measure used in this shared task: the un-
weighted average of MUC (Vilain et al, 1995), B3
(Bagga and Baldwin, 1998) and entity-based CEAF
(Luo, 2005).
Figure 4: Development process. The figure shows MUC?s pre-
cision (red), recall (green), and F1 (blue) for each balance value
with pruning adjusted to 6.
3 CoNLL shared task participation
RELAXCOR has participated in the CoNLL task in
the Closed mode. All the knowledge required by the
feature functions is obtained from the annotations
of the corpora and no external resources have been
used with the exception of WordNet (Miller, 1995),
gender and number information (Bergsma and Lin,
2006) and sense inventories. All of them are allowed
by the task organization and available in their web-
site.
There are many remarkable features that make
this task different and more difficult but realistic
than previous ones. About mention annotation, it
is important to emphasize that singletons are not an-
notated, mentions must be detected by the system
and the mapping between system and true mentions
is limited to exact matching of boundaries. More-
over, some verbs have been annotated as corefering
mentions. Regarding the evaluation, the scorer uses
the modification of (Cai and Strube, 2010), unprece-
dented so far, and the corpora was published very re-
cently and there are no published results yet to use as
reference. Finally, all the preprocessed information
is automatic for the test dataset, carring out some
noisy errors which is a handicap from the point of
view of machine learning.
Following there is a description of the mention de-
tection system developed for the task and an analysis
of the obtained results in the development dataset.
37
3.1 Mention detection system
The mention detection system extracts one mention
for every NP found in the syntactic tree, one for ev-
ery pronoun and one for every named entity. Then,
the head of every NP is determined using part-of-
speech tags and a set of rules from (Collins, 1999).
In case that some NPs share the same head, the
larger NP is selected and the rest discarded. Also the
mention repetitions with exactly the same bound-
aries are discarded. In addition, nouns with capital
letters and proper names not included yet, that ap-
pear two or more times in the document, are also in-
cluded. For instance, the NP ?an Internet business?
is added as a mention, but also ?Internet? itself is
added in the case that the word is found once again
in the document.
As a result, taking into account that just exact
boundary matching is accepted, the mention detec-
tion achieves an acceptable recall, higher than 90%,
but a low precision (see Table 1). The most typ-
ical error made by the system is to include ex-
tracted NPs that are not referential (e.g., predicative
and appositive phrases) and mentions with incorrect
boundaries. The incorrect boundaries are mainly
due to errors in the predicted syntactic column and
some mention annotation discrepancies. Further-
more, verbs are not detected by this algorithm, so
most of the missing mentions are verbs.
3.2 Results analysis
The results obtained by RELAXCOR can be found
in Tables 1 and 2. Due to the lack of annotated sin-
gletons, mention-based metrics B3 and CEAF pro-
duce lower scores ?near 60% and 50% respectively?
than the ones typically achieved with different anno-
tations and mapping policies ?usually near 80% and
70%. Moreover, the requirement that systems use
automatic preprocessing and do their own mention
detection increase the difficulty of the task which ob-
viously decreases the scores in general.
The measure which remains more stable on its
scores is MUC given that it is link-based and not
takes singletons into account anyway. Thus, it is the
only one comparable with the state of the art right
now. The results obtained with MUC scorer show an
improvement of RELAXCOR?s recall, a feature that
needed improvement given the previous published
Measure Recall Precision F1
Mention detection 92.45 27.34 42.20
mention-based CEAF 55.27 55.27 55.27
entity-based CEAF 47.20 40.01 43.31
MUC 54.53 62.25 58.13
B3 63.72 73.83 68.40
(CEAFe+MUC+B3)/3 - - 56.61
Table 1: Results on the development data set
Measure Recall Precision F1
mention-based CEAF 53.51 53.51 53.51
entity-based CEAF 44.75 38.38 41.32
MUC 56.32 63.16 59.55
B3 62.16 72.08 67.09
BLANC 69.50 73.07 71.10
(CEAFe+MUC+B3)/3 - - 55.99
Table 2: Official test results
results with a MUCs recall remarkably low (Sapena
et al, 2010b).
4 Conclusion
The participation of RELAXCOR to the CoNLL
shared task has been useful to evaluate the system
using data never seen before in a totally automatic
context: predicted preprocessing and system men-
tions. Many published systems typically use the
same data sets (ACE and MUC) and it is easy to un-
intentionally adapt the system to the corpora and not
just to the problem. This kind of tasks favor com-
parisons between systems with the same framework
and initial conditions.
The obtained performances confirm the robust-
ness of RELAXCOR and a recall improvement. And
the overall performance seems considerably good
taking into account the unprecedented scenario.
However, a deeper error analysis is needed, specially
in the mention detection system with a low precision
and the training data selection process which may
be discarding positive examples that could help im-
proving recall.
Acknowledgments
The research leading to these results has received funding from the
European Community?s Seventh Framework Programme (FP7/2007-
2013) under Grant Agreement number 247762 (FAUST), and from
the Spanish Science and Innovation Ministry, via the KNOW2 project
(TIN2009-14715-C04-04).
38
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the Lin-
guistic Coreference Workshop at LREC 98, pages 563?
566, Granada, Spain.
S. Bergsma and D. Lin. 2006. Bootstrapping path-based
pronoun resolution. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 33?40. Association
for Computational Linguistics.
Jie Cai and Michael Strube. 2010. Evaluation met-
rics for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36, University of
Tokyo, Japan.
M. Collins. 1999. Head-driven statistical models for
natural language parsing. Ph.D. thesis, University of
Pennsylvania.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Joint Con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing (HLT-
EMNLP 2005, pages 25?32, Vancouver, B.C., Canada.
G.A. Miller. 1995. WordNet: a lexical database for En-
glish.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
J.R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010a. A
Global Relaxation Labeling Approach to Coreference
Resolution. In Proceedings of 23rd International Con-
ference on Computational Linguistics, COLING, Bei-
jing, China, August.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010b. Re-
laxCor: A Global Relaxation Labeling Approach to
Coreference Resolution. In Proceedings of the ACL
Workshop on Semantic Evaluations (SemEval-2010),
Uppsala, Sweden, July.
W.M. Soon, H.T. Ng, and D.C.Y. Lim. 2001. A
Machine Learning Approach to Coreference Resolu-
tion of Noun Phrases. Computational Linguistics,
27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the Sixth Message Understanding Conference
(MUC-6), pages 45?52.
39
