SCANMail: Audio Navigation in the Voicemail Domain
Michiel Bacchiani Julia Hirschberg Aaron Rosenberg Steve Whittaker
Donald Hindle Phil Isenhour Mark Jones Litza Stark
Gary Zamchick
fmichiel,julia,aer,stevewg@research.att.com, dhindle@answerlogic.com, isenhour@vt.edu,
jones@research.att.com, litza@udel.edu, zamchick@attlabs.att.com
AT&T Labs ? Research
180 Park Avenue
Florham Park, NJ 07932-0971, USA
ABSTRACT
This paper describes SCANMail, a system that allows users to
browse and search their voicemail messages by content through
a GUI. Content based navigation is realized by use of automatic
speech recognition, information retrieval, information extraction
and human computer interaction technology. In addition to the
browsing and querying functionalities, acoustics-based caller ID
technology is used to proposes caller names from existing caller
acoustic models trained from user feedback. The GUI browser also
provides a note-taking capability. Comparing SCANMail to a regu-
lar voicemail interface in a user study, SCANMail performed better
both in terms of objective (time to and quality of solutions) as well
as subjective objectives.
1. INTRODUCTION
Increasing amounts of public, corporate, and private audio present
a major challenge to speech, information retrieval, and human-
computer interaction research: how can we help people to take ad-
vantage of these resources when current techniques for navigating
them fall far short of text-based search methods? In this paper,
we describe SCANMail, a system that employs automatic speech
recognition (ASR), information retrieval (IR), information extrac-
tion (IE), and human computer interaction (HCI) technology to per-
mit users to browse and search their voicemail messages by content
through a GUI interface. A CallerId server also proposes caller
names from existing caller acoustic models and is trained from
user feedback. An Email server sends the original message plus
its ASR transcription to a mailing address specified in the user?s
profile. The SCANMail GUI also provides note-taking capabilities
as well as browsing and querying features. Access to messages and
information about them is presented to the user via a Java applet
running under Netscape. Figure 1 shows the SCANMail GUI.
.
2. SYSTEM DESCRIPTION
In SCANMail, messages are first retrieved from a voicemail server,
then processed by the ASR server that provides a transcription. The
message audio and/or transcription are then passed to the IE, IR,
Email, and CallerId servers. The acoustic and language model of
the recognizer, and the IE and IR servers are trained on 60 hours of
a 100 hour voicemail corpus, transcribed and hand labeled for tele-
phone numbers, caller names, times, dates, greetings and closings.
The corpus includes approximately 10,000 messages from approx-
imately 2500 speakers. About 90% of the messages were recorded
from regular handsets, the rest from cellular and speaker-phones.
The corpus is approximately gender balanced and approximately
12% of the messages were from non-native speakers. The mean
duration of the messages was 36.4 seconds; the median was 30.0
seconds.
2.1 Automatic Speech Recognition
The baseline ASR system is a decision-tree based state-clustered
triphone system with 8k tied states. The emission probabilities of
the states are modeled by 12 component Gaussian mixture distribu-
tions. The system uses a 14k vocabulary, automatically generated
by the AT&T Labs NextGen Text To Speech system. The language
model is a Katz-style backoff trigram trained on 700k words from
the transcriptions of the 60 hour training set. The word-error rate
of this system on a 40 hour test set is 34.9%.
Since the messages come from a highly variable source both in
terms of speaker as well as channel characteristics, transcription ac-
curacy is significantly improved by application of various normal-
ization techniques, developed for Switchboard evaluations [9]. The
ASR server uses Vocal Tract Length Normalization (VTLN) [5],
Constrained Modelspace Adaptation (CMA) [3], Maximum Like-
lihood Linear Regression (MLLR) [6] and Semi-Tied Covariances
(STC) [4] to obtain progressively more accurate acoustic models
and uses these in a rescoring framework. In contrast to Switch-
board, voicemail messages are generally too short too allow direct
application of the normalization techniques. A novel message clus-
tering algorithm based on MLLR likelihood [1] is used to guarantee
sufficient data for normalization. The final transcripts, obtained af-
ter 6 recognition passes, have a word error rate of 28.7% ? a 6.2%
accuracy improvement. Gender dependency provides 1.6% of this
gain. VTLN then additively improves accuracy with 1.0% when
applied only on the test data and an additional 0.3% when sub-
sequently applied with a VTLN trained model. The use of STC
further improves accuracy with 1.2%. Finally CMA and MLLR
provide additive gains of 1.5% and 0.6% respectively. The ASR
Figure 1: The SCANMail User Interface
server, running on a 667 MHz 21264 Alpha processor, produces
the final transcripts in approximately 20 times real-time.
2.2 Information Retrieval
Messages transcripts are indexed by the IR server using the SMART
IR [8, 2] engine. SMART is based on the vector space model
of information retrieval. It generates weighted term (word) vec-
tors for the automatic transcriptions of the messages. SMART pre-
processes the automatic transcriptions of each new message by to-
kenizing the text into words, removing common words that appear
on its stop-list, and performing stemming on the remaining words
to derive a set of terms, against which later user queries can be
compared. When the IR server is used to execute a user query, the
query terms are also converted into weighted term vectors. Vector
inner-product similarity computation is then used to rank messages
in decreasing order of their similarity to the user query.
2.3 Information Extraction
Key information is extracted from the ASR transcription by the
IE server, which currently extracts any phone numbers identified
in the message. Currently, this is done by recognizing digit strings
and scoring them based on the sequence length. An improved ex-
traction algorithm, trained on our hand-labeled voicemail corpus,
employs a digit string recognizer combined with a trigram language
model, to recognize strings in their lexical contexts, e.g. <word>
<digit string> <word>.
2.4 Caller Identification
The CallerID server proposes caller names by matching mes-
sages against existing caller models; this module is trained from
user feedback. The caller identification capability is based on text
independent speaker recognition techniques applied to the processed
speech in the voicemail messages. A user may elect to label a mes-
sage he/she has reviewed with a caller name for the purpose of
creating a speaker model for that caller. When the cumulative du-
ration of such user-labeled messages is sufficient, a caller model
is constructed. Subsequent messages will be processed and scored
against this caller model and models for other callers the user may
have designated. If the best matching model score for an incom-
ing message exceeds a decision threshold, a caller name hypothesis
is sent to the GUI client; if there is no PBX-supplied identifica-
tion (i.e. caller name supplied from the owner of the extension for
calls internal to the PBX), the CallerId hypothesis is presented in
the message header, for either accepting or editing by the user; if
there is a PBX identification, the CallerId hypothesis appears as the
first item in a user ?contact menu?, together with all previously id?d
callers for that user. To optimize the use of the available speech
data, and to speed model-building, caller models are shared among
users. Details and a performance evaluation of the CallerId process
are described in [7].
2.5 Graphical User Interface
In the SCANMail GUI, users see message headers (callerid, time
and date, length in seconds, first line of any attached note, and
presence of extracted phone numbers) as well as a thumbnail and
the ASR transcription of the current message. Any note attached
to the current message is also displayed. A search panel permits
users to search the contents of their mailbox by inputting any text
query. Results are presented in a new search window, with key-
words color-coded in the query, transcript, and thumbnail.
2.6 User Studies
User studies compared SCANMail with a standard over-the-phone
voicemail access. Eight subjects performed a series of fact-finding,
relevance ranking, and summarization tasks on artificial mailboxes
of twenty messages each, using either SCANMail or phone access.
SCANMail showed advantages for fact-finding and relevance rank-
ing tasks in quality of solution normalized by time to solution, for
fact-finding in time to solution and in overall user preference. Nor-
malized performance scores are higher when subjects employ IR
searches that are successful (i.e. the queries they choose contain
words correctly recognized by the recognizer) and for subjects who
listen to less audio and rely more upon the transcripts. However, we
also found that SCANMail?s search capability can be misleading,
causing subjects to assume that they have found all relevant doc-
uments when in fact some are NOT retrieved, and that when sub-
jects rely upon the accuracy of the ASR transcript, they can miss
crucial but unrecognized information. A trial of 10 friendly users
is currently underway, with modifications to access functionality
suggested by our subject users. A larger trial of the system is be-
ing prepared, for more extensive testing of user behavior with their
own mailboxes over time.
Acknowledgements
The authors would like to thank Andrej Ljolje, S. Parthasarathy,
Fernando Pereira, and Amit Singhal for their help in developing
this application.
3. REFERENCES
[1] M. Bacchiani. Using maximum likelihood linear regression
for segment clustering and speaker identification. In
Proceedings of the Sixth International Conference on Spoken
Language Processing, volume 4, pages 536?539, Beijing,
2000.
[2] C. Buckley. Implementation of the SMART information
retrieval system. Technical Report TR85-686, Department of
Computer Science, Cornell University, Ithaca, NY 14853,
May 1985.
[3] M. J. F. Gales. Maximum likelihood linear transformations for
hmm-based speech recognition. Computer Speech and
Language, pages 75?90, 1998.
[4] M. J. F. Gales. Semi-tied covariance matrices for hidden
markov models. IEEE Transactions on Acoustics, Speech, and
Signal Processing, 7(3), 1999.
[5] T. Kamm, G. Andreou, and J. Cohen. Vocal tract
normalization in speech recognition: Compensating for
systematic speaker variability. In Proceedings of the 15th
Annual Speech Research Symposium, pages 161?167, Johns
Hopkins University, Baltimore, MD, 1995.
[6] C. J. Legetter and P. C. Woodland. Maximum likelihood linear
regression for speaker adaptation of continuous density hidden
markov models. Computer Speech and Language, pages
171?185, 1995.
[7] A. Rosenberg, S. Parthasarathy, J. Hirschberg, and
S. Whittaker. Foldering voicemail messages by caller using
text independent speaker recognition. In Proceedings of the
Sixth International Conference on Spoken Language
Processing, Beijing, 2000.
[8] G. Salton, editor. The SMART Retrieval System?Experiments
in Automatic Document Retrieval. Prentice Hall Inc.,
Englewood Cliffs, NJ, 1971.
[9] Proceedings of the Speech Transcription Workshop,
University of Maryland, May 2000.
Supervised and unsupervised PCFG adaptation to novel domains
Brian Roark and Michiel Bacchiani
AT&T Labs - Research
{roark,michiel}@research.att.com
Abstract
This paper investigates adapting a lexicalized
probabilistic context-free grammar (PCFG) to
a novel domain, using maximum a posteriori
(MAP) estimation. The MAP framework is gen-
eral enough to include some previous model
adaptation approaches, such as corpus mixing in
Gildea (2001), for example. Other approaches
falling within this framework are more effec-
tive. In contrast to the results in Gildea (2001),
we show F-measure parsing accuracy gains of as
much as 2.5% for high accuracy lexicalized pars-
ing through the use of out-of-domain treebanks,
with the largest gains when the amount of in-
domain data is small. MAP adaptation can also be
based on either supervised or unsupervised adap-
tation data. Even when no in-domain treebank is
available, unsupervised techniques provide a sub-
stantial accuracy gain over unadapted grammars,
as much as nearly 5% F-measure improvement.
1 Introduction
A fundamental concern for nearly all data-driven ap-
proaches to language processing is the sparsity of la-
beled training data. The sparsity of syntactically anno-
tated corpora is widely remarked upon, and some recent
papers present approaches to improving performance in
the absence of large amounts of annotated training data.
Johnson and Riezler (2000) looked at adding features to a
maximum entropy model for stochastic unification-based
grammars (SUBG), from corpora that are not annotated
with the SUBG, but rather with simpler treebank annota-
tions for which there are much larger treebanks. Hwa (2001)
demonstrated how active learning techniques can reduce
the amount of annotated data required to converge on the
best performance, by selecting from among the candidate
strings to be annotated in ways which promote more in-
formative examples for earlier annotation. Hwa (1999) and
Gildea (2001) looked at adapting parsing models trained on
large amounts of annotated data from outside of the domain
of interest (out-of-domain), through the use of a relatively
small amount of in-domain annotated data. Hwa (1999)
used a variant of the inside-outside algorithm presented
in Pereira and Schabes (1992) to exploit a partially labeled
out-of-domain treebank, and found an advantage to adapta-
tion over direct grammar induction. Gildea (2001) simply
added the out-of-domain treebank to his in-domain training
data, and derived a very small benefit for his high accuracy,
lexicalized parser, concluding that even a large amount of
out-of-domain data is of little use for lexicalized parsing.
Statistical model adaptation based on sparse in-domain
data, however, is neither a new problem nor unique to pars-
ing. It has been studied extensively by researchers work-
ing on acoustic modeling for automatic speech recognition
(ASR) (Legetter and Woodland, 1995; Gauvain and Lee,
1994; Gales, 1998; Lamel et al, 2002). One of the meth-
ods that has received much attention in the ASR literature is
maximum a posteriori (MAP) estimation (Gauvain and Lee,
1994). In MAP estimation, the parameters of the model are
considered to be random variables themselves with a known
distribution (the prior). The prior distribution and the max-
imum likelihood distribution based on the in-domain obser-
vations then give a posterior distribution over the parame-
ters, from which the mode is selected. If the amount of in-
domain (adaptation) data is large, the mode of the posterior
distribution is mostly defined by the adaptation sample; if
the amount of adaptation data is small, the mode will nearly
coincide with the mode of the prior distribution. The intu-
ition behind MAP estimation is that once there are sufficient
observations, the prior model need no longer be relied upon.
Bacchiani and Roark (2003) investigated MAP adapta-
tion of n-gram language models, in a way that is straight-
forwardly applicable to probabilistic context-free grammars
(PCFGs). Indeed, this approach can be used for any gen-
erative probabilistic model, such as part-of-speech taggers.
In their language modeling approach, in-domain counts are
mixed with the out-of-domain model, so that, if the num-
ber of observations within the domain is small, the out-
of-domain model is relied upon, whereas if the number of
observations in the domain is high, the model will move
toward a Maximum Likelihood (ML) estimate on the in-
domain data alone. The case of a parsing model trained via
relative frequency estimation is identical: in-domain counts
can be combined with the out-of-domain model in just such
a way. We will show below that weighted count merging
is a special case of MAP adaptation; hence the approach
of Gildea (2001) cited above is also a special case of MAP
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 126-133
                                                         Proceedings of HLT-NAACL 2003
adaptation, with a particular parameterization of the prior.
This parameterization is not necessarily the one that opti-
mizes performance.
In the next section, MAP estimation for PCFGs is pre-
sented. This is followed by a brief presentation of the PCFG
model that is being learned, and the parser that is used
for the empirical trials. We will present empirical results
for multiple MAP adaptation schema, both starting from
the Penn Wall St. Journal treebank and adapting to the
Brown corpus, and vice versa. We will compare our su-
pervised adaptation performance with the results presented
in Gildea (2001). In addition to supervised adaptation, i.e.
with a manually annotated treebank, we will present results
for unsupervised adaptation, i.e. with an automatically an-
notated treebank. We investigate a number of unsupervised
approaches, including multiple iterations, increased sample
sizes, and self-adaptation.
2 MAP estimation
In the maximum a posteriori estimation framework de-
scribed in detail in Gauvain and Lee (1994), the model pa-
rameters ? are assumed to be a random vector in the space
?. Given an observation sample x, the MAP estimate is ob-
tained as the mode of the posterior distribution of ? denoted
as g(. | x)
?MAP = argmax
?
g(? | x) = argmax
?
f(x | ?)g(?) (1)
In the case of n-gram model adaptation, as discussed in
Bacchiani and Roark (2003), the objective is to estimate
probabilities for a discrete distribution across words, en-
tirely analogous to the distribution across mixture compo-
nents within a mixture density, which is a common use for
MAP estimation in ASR. A practical candidate for the prior
distribution of the weights ?1, ?2, ? ? ? , ?K , is its conjugate
prior, the Dirichlet density,
g(?1, ?2, ? ? ? , ?K | ?1, ?2, ? ? ? , ?K) ?
K?
i=1
??i?1i (2)
where ?i > 0 are the parameters of the Dirichlet distribu-
tion. With such a prior, if the expected counts for the i-th
component is denoted as ci, the mode of the posterior distri-
bution is obtained as
??i =
(?i ? 1) + ci
?K
k=1(?k ? 1) +
?K
k=1 ck
1 ? i ? K. (3)
We can use this formulation to estimate the posterior, but we
must still choose the parameters of the Dirichlet. First, let
us introduce some notation. A context-free grammar (CFG)
G = (V, T, P, S?), consists of a set of non-terminal symbols
V , a set of terminal symbols T , a start symbol S? ? V , and
a set of rule productions P of the form: A ? ?, where
A ? V and ? ? (V ? T )?. A probabilistic context-free
grammar (PCFG) is a CFG with a probability assigned to
each rule, such that the probabilities of all rules expanding a
given non-terminal sum to one; specifically, each right-hand
side has a probability given the left-hand side of the rule1.
LetA denote the left-hand side of a production, and ?i the
i-th possible expansion of A. Let the probability estimate
for the production A ? ?i according to the out-of-domain
model be denoted as P?(?i | A) and let the expected adapta-
tion counts be denoted as c(A ? ?i). Then the parameters
of the prior distribution for left-hand side A are chosen as
?Ai = ?AP?(?i | A) + 1 1 ? i ? K. (4)
where ?A is the left-hand side dependent prior weighting pa-
rameter. This choice of prior parameters defines the MAP
estimate of the probability of expansion ?i from the left-
hand side A as
P?(?i | A) =
?AP?(?i | A) + c(A? ?i)
?A +
?K
k=1 c(A? ?k)
1 ? i ? K. (5)
Note that the MAP estimates with this parameterization re-
duce to the out-of-domain model parameters in the absence
of adaptation data.
Each left-hand side A has its own prior distribution, pa-
rameterized with ?A. This presents an over-parameterization
problem. We follow Gauvain and Lee (1994) in adopt-
ing a parameter tying approach. As pointed out in
Bacchiani and Roark (2003), two methods of parameter ty-
ing, in fact, correspond to two well known model mixing
approaches, namely count merging and model interpolation.
Let P? and c? denote the probabilities and counts from the
out-of-domain model, and let P and c denote the probabili-
ties and counts from the adaptation model (i.e. in-domain).
2.1 Count Merging
If the left-hand side dependent prior weighting parameter is
chosen as
?A = c?(A)
?
?
, (6)
the MAP adaptation reduces to count merging, scaling the
out-of-domain counts with a factor ? and the in-domain
counts with a factor ?:
P?(?i | A) =
c?(A)?? P?(?i | A) + c(A? ?i)
c?(A)?? + c(A)
=
?c?(A? ?i) + ?c(A? ?i)
?c?(A) + ?c(A)
(7)
1An additional condition for well-formedness is that the PCFG
is consistent or tight, i.e. there is no probability mass lost to in-
finitely large trees. Chi and Geman (1998) proved that this con-
dition is met if the rule probabilities are estimated using relative
frequency estimation from a corpus.
2.2 Model Interpolation
If the left-hand side dependent prior weighting parameter is
chosen as
?A =
{
c(A) ?1?? , 0 < ? < 1 if c(A) > 0
1 otherwise
(8)
the MAP adaptation reduces to model interpolation using
interpolation parameter ?:
P?(?i | A) =
c(A) ?1?? P?(?i | A) + c(A? ?i)
c(A) ?1?? + c(A)
=
?
1?? P?(?i | A) + P(?i | A)
?
1?? + 1
= ?P?(?i | A) + (1? ?)P(?i | A) (9)
2.3 Other Tying Candidates
While we will not be presenting empirical results for other
parameter tying approaches in this paper, we should point
out that the MAP framework is general enough to allow
for other schema, which could potentially improve perfor-
mance over simple count merging and model interpolation
approaches. For example, one may choose a more com-
plicated left-hand side dependent prior weighting parameter
such as
?A =
{
c(A) ?1?? , 0 < ? < 1 if c?(A) c(A) > ?
c?(A)?? otherwise
(10)
for some threshold ?. Such a schema may do a better job
of managing how quickly the model moves away from the
prior, particularly if there is a large difference in the respec-
tive sizes of the in-domain and out-of domain corpora. We
leave the investigation of such approaches to future research.
Before providing empirical results on the count merging
and model interpolation approaches, we will introduce the
parser and parsing models that were used.
3 Grammar and parser
For the empirical trials, we used a top-down, left-to-right
(incremental) statistical beam-search parser (Roark, 2001a;
Roark, 2003). We refer readers to the cited papers for de-
tails on this parsing algorithm. Briefly, the parser maintains
a set of candidate analyses, each of which is extended to
attempt to incorporate the next word into a fully connected
partial parse. As soon as ?enough? candidate parses have
been extended to the next word, all parses that have not
yet attached the word are discarded, and the parser moves
on to the next word. This beam search is parameterized
with a base beam parameter ?, which controls how many
or how few parses constitute ?enough?. Candidate parses
are ranked by a figure-of-merit, which promotes better can-
didates, so that they are worked on earlier. The figure-of-
merit consists of the probability of the parse to that point
times a look-ahead statistic, which is an estimate of how
much probability mass it will take to connect the parse with
the next word. It is a generative parser that does not require
any pre-processing, such as POS tagging or chunking. It has
been demonstrated in the above papers to perform compet-
itively on standard statistical parsing tasks with full cover-
age. Baseline results below will provide a comparison with
other well known statistical parsers.
The PCFG is a Markov grammar (Collins, 1997; Char-
niak, 2000), i.e. the production probabilities are estimated
by decomposing the joint probability of the categories on the
right-hand side into a product of conditionals via the chain
rule, and making a Markov assumption. Thus, for example,
a first order Markov grammar conditions the probability of
the category of the i-th child of the left-hand side on the cat-
egory of the left-hand side and the category of the (i-1)-th
child of the left-hand side. The benefits of Markov gram-
mars for a top-down parser of the sort we are using is de-
tailed in Roark (2003). Further, as in Roark (2001a; 2003),
the production probabilities are conditioned on the label of
the left-hand side of the production, as well as on features
from the left-context. The model is smoothed using standard
deleted interpolation, wherein a mixing parameter ? is esti-
mated using EM on a held out corpus, such that probability
of a production A ? ?, conditioned on j features from the
left context, Xj1 = X1 . . . Xj , is defined recursively as
P(A? ? | Xj1) = P(? | A,X
j
1) (11)
= (1? ?)P?(? | A,Xj1) + ?P(? | A,X
j?1
1 )
where P? is the maximum likelihood estimate of the condi-
tional probability. These conditional probabilities decom-
pose via the chain rule as mentioned above, and a Markov
assumption limits the number of previous children already
emitted from the left-hand side that are conditioned upon.
These previous children are treated exactly as other con-
ditioning features from the left context. Table 1 gives the
conditioning features that were used for all empirical trials
in this paper. There are different conditioning features for
parts-of-speech (POS) and non-POS non-terminals. Deleted
interpolation leaves out one feature at a time, in the reverse
order as they are presented in the table 1.
The grammar that is used for these trials is a PCFG that
is induced using relative frequency estimation from a trans-
formed treebank. The trees are transformed with a selec-
tive left-corner transformation (Johnson and Roark, 2000)
that has been flattened as presented in Roark (2001b). This
transform is only applied to left-recursive productions, i.e.
productions of the form A ? A?. The transformed trees
look as in figure 1. The transform has the benefit for a top-
down incremental parser of this sort of delaying many of
the parsing decisions until later in the string, without un-
duly disrupting the immediate dominance relationships that
provide conditioning features for the probabilistic model.
(a)
NP

NP

NP

NNP
Jim
bb
POS
?s
HHH
NN
dog
PPPP
PP
,
IN
with . . .
l
NP
(b)
NP

NNP
Jim
POS
?s
XXXXX
NP/NP

NN
dog
HHH
NP/NP
PP

IN
with . . .
l
NP
(c)
NP
      
NNP
Jim
!!!
POS
?s
l
NP/NP
NN
dog
``````
NP/NP
PP
,
IN
with . . .
l
NP
Figure 1: Three representations of NP modifications: (a) the original treebank representation; (b) Selective left-corner
representation; and (c) a flat structure that is unambiguously equivalent to (b)
Features for non-POS left-hand sides
0 Left-hand side (LHS)
1 Last child of LHS
2 2nd last child of LHS
3 3rd last child of LHS
4 Parent of LHS (PAR)
5 Last child of PAR
6 Parent of PAR (GPAR)
7 Last child of GPAR
8 First child of conjoined category
9 Lexical head of current constituent
Features for POS left-hand sides
0 Left-hand side (LHS)
1 Parent of LHS (PAR)
2 Last child of PAR
3 Parent of PAR (GPAR)
4 POS of C-Commanding head
5 C-Commanding lexical head
6 Next C-Commanding lexical head
Table 1: Conditioning features for the probabilistic CFG
used in the reported empirical trials
The parse trees that are returned by the parser are then de-
transformed to the original form of the grammar for evalua-
tion2.
For the trials reported in the next section, the base beam
parameter is set at ? = 10. In order to avoid being pruned, a
parse must be within a probability range of the best scoring
parse that has incorporated the next word. Let k be the num-
ber of parses that have incorporated the next word, and let p?
be the best probability from among that set. Then the prob-
ability of a parse must be above p?k
3
10? to avoid being pruned.
2See Johnson (1998) for a presentation of the transform/de-
transform paradigm in parsing.
4 Empirical trials
The parsing models were trained and tested on treebanks
from the Penn Treebank II. For the Wall St. Journal portion,
we used the standard breakdown: sections 2-21 were kept
training data; section 24 was held-out development data; and
section 23 was for evaluation. For the Brown corpus por-
tion, we obtained the training and evaluation sections used
in Gildea (2001). In that paper, no held-out section was used
for parameter tuning3, so we further partitioned the training
data into kept and held-out data. The sizes of the corpora
are given in table 2, as well as labels that are used to refer to
the corpora in subsequent tables.
4.1 Baseline performance
The first results are for parsing the Brown corpus. Table
3 presents our baseline performance, compared with the
Gildea (2001) results. Our system is labeled as ?MAP?. All
parsing results are presented as labeled precision and recall.
Whereas Gildea (2001) reported parsing results just for sen-
tences of length less than or equal to 40, our results are for
all sentences. The goal is not to improve upon Gildea?s
parsing performance, but rather to try to get more benefit
from the out-of-domain data. While our performance is 0.5-
1.5 percent better than Gildea?s, the same trends hold ? low
eighties in accuracy when using the Wall St. Journal (out-of-
domain) training; mid eighties when using the Brown corpus
training. Notice that using the Brown held out data with the
Wall St. Journal training improved precision substantially.
Tuning the parameters on in-domain data can make a big
difference in parser performance. Choosing the smoothing
parameters as Gildea did, based on the distribution within
the corpus itself, may be effective when parsing within the
same distribution, but appears less so when using the tree-
bank for parsing outside of the domain.
3According to the author, smoothing parameters for his parser
were based on the formula from Collins (1999).
Corpus;Sect Used for Sentences Words
WSJ;2-21 Training 39,832 950,028
WSJ;24 Held out 1,346 32,853
WSJ;23 Eval 2,416 56,684
Brown;T Training 19,740 373,152
Brown;H Held out 2,078 40,046
Brown;E Eval 2,425 45,950
Table 2: Corpus sizes
System Training Heldout LR LP
Gildea WSJ;2-21 80.3 81.0
MAP WSJ;2-21 WSJ;24 81.3 80.9
MAP WSJ;2-21 Brown;H 81.6 82.3
Gildea Brown;T,H 83.6 84.6
MAP Brown;T Brown;H 84.4 85.0
Table 3: Parser performance on Brown;E, baselines. Note
that the Gildea results are for sentences ? 40 words in
length.
Table 4 gives the baseline performance on section 23 of
the WSJ Treebank. Note, again, that the Gildea results are
for sentences ? 40 words in length, while all others are for
all sentences in the test set. Also, Gildea did not report per-
formance of a Brown corpus trained parser on the WSJ. Our
performance under that condition is not particularly good,
but again using an in-domain held out set for parameter tun-
ing provided a substantial increase in accuracy, somewhat
more in terms of precision than recall. Our baseline results
for a WSJ section 2-21 trained parser are slightly better than
the Gildea parser, at more-or-less the same level of perfor-
mance as Charniak (1997) and Ratnaparkhi (1999), but sev-
eral points below the best reported results on this task.
4.2 Supervised adaptation
Table 5 presents parsing results on the Brown;E test set for
models using both in-domain and out-of-domain training
data. The table gives the adaptation (in-domain) treebank
that was used, and the ?A that was used to combine the adap-
tation counts with the model built from the out-of-domain
treebank. Recall that ?c?(A) times the out-of-domain model
yields count merging, with ? the ratio of out-of-domain
to in-domain counts; and ?c(A) times the out-of-domain
model yields model interpolation, with ? the ratio of out-of-
domain to in-domain probabilities. Gildea (2001) merged
the two corpora, which just adds the counts from the out-of-
domain treebank to the in-domain treebank, i.e. ? = 1.
This resulted in a 0.25 improvement in the F-measure. In
our case, combining the counts in this way yielded a half
a point, perhaps because of the in-domain tuning of the
smoothing parameters. However, when we optimize ? em-
pirically on the held-out corpus, we can get nearly a full
point improvement. Model interpolation in this case per-
System Training Heldout LR LP
MAP Brown;T Brown;H 76.0 75.4
MAP Brown;T WSJ;24 76.9 77.1
Gildea WSJ;2-21 86.1 86.6
MAP WSJ;2-21 WSJ;24 86.9 87.1
Charniak (1997) WSJ;2-21 WSJ;24 86.7 86.6
Ratnaparkhi (1999) WSJ;2-21 86.3 87.5
Collins (1999) WSJ;2-21 88.1 88.3
Charniak (2000) WSJ;2-21 WSJ;24 89.6 89.5
Collins (2000) WSJ;2-21 89.6 89.9
Table 4: Parser performance on WSJ;23, baselines. Note
that the Gildea results are for sentences ? 40 words in
length. All others include all sentences.
forms nearly identically to count merging.
Adaptation to the Brown corpus, however, does not ad-
equately represent what is likely to be the most common
adaptation scenario, i.e. adaptation to a consistent domain
with limited in-domain training data. The Brown corpus is
not really a domain; it was built as a balanced corpus, and
hence is the aggregation of multiple domains. The reverse
scenario ? Brown corpus as out-of-domain parsing model
and Wall St. Journal as novel domain ? is perhaps a more
natural one. In this direction, Gildea (2001) also reported
very small improvements when adding in the out-of-domain
treebank. This may be because of the same issue as with the
Brown corpus, namely that the optimal ratio of in-domain to
out-of-domain is not 1 and the smoothing parameters need
to be tuned to the new domain; or it may be because the new
domain has a million words of training data, and hence has
less use for out-of-domain data. To tease these apart, we par-
titioned the WSJ training data (sections 2-21) into smaller
treebanks, and looked at the gain provided by adaptation as
the in-domain observations grow. These smaller treebanks
provide a more realistic scenario: rapid adaptation to a novel
domain will likely occur with far less manual annotation of
trees within the new domain than can be had in the full Penn
Treebank.
Table 6 gives the baseline performance on WSJ;23, with
models trained on fractions of the entire 2-21 test set. Sec-
tions 2-21 contain approximately 40,000 sentences, and we
partitioned them by percentage of total sentences. From ta-
ble 6 we can see that parser performance degrades quite dra-
matically when there is less than 20,000 sentences in the
training set, but that even with just 2000 sentences, the sys-
tem outperforms one trained on the Brown corpus.
Table 7 presents parsing accuracy when a model trained
on the Brown corpus is adapted with part or all of the WSJ
training corpus. From this point forward, we only present
results for count merging, since model interpolation con-
sistently performed 0.2-0.5 points below the count merging
System Training Heldout Adapt ?A Baseline Adapted ?F
LR LP F LR LP F
Gildea WSJ;2-21 Brown;T,H c?(A) 83.6 84.6 84.1 83.9 84.8 84.35 0.25
MAP WSJ;2-21 Brown;H Brown;T c?(A) 84.4 85.0 84.7 84.9 85.6 85.25 0.55
MAP WSJ;2-21 Brown;H Brown;T 0.25?c(A) 84.4 85.0 84.7 85.4 85.9 85.65 0.95
MAP WSJ;2-21 Brown;H Brown;T 0.20c(A) 84.4 85.0 84.7 85.3 85.9 85.60 0.90
Table 5: Parser performance on Brown;E, supervised adaptation
System Training % Heldout LR LP
MAP WSJ;2-21 100 WSJ;24 86.9 87.1
MAP WSJ;2-21 75 WSJ;24 86.6 86.8
MAP WSJ;2-21 50 WSJ;24 86.3 86.4
MAP WSJ;2-21 25 WSJ;24 84.8 85.0
MAP WSJ;2-21 10 WSJ;24 82.6 82.6
MAP WSJ;2-21 5 WSJ;24 80.4 80.6
Table 6: Parser performance on WSJ;23, baselines
approach4. The ?A mixing parameter was empirically opti-
mized on the held out set when the in-domain training was
just 10% of the total; this optimization makes over a point
difference in accuracy. Like Gildea, with large amounts of
in-domain data, adaptation improved our performance by
half a point or less. When the amount of in-domain data
is small, however, the impact of adaptation is much greater.
4.3 Unsupervised adaptation
Bacchiani and Roark (2003) presented unsupervised MAP
adaptation results for n-gram models, which use the same
methods outlined above, but rather than using a manually
annotated corpus as input to adaptation, instead use an auto-
matically annotated corpus. Their automatically annotated
corpus was the output of a speech recognizer which used the
out-of-domain n-gram model. In our case, we use the pars-
ing model trained on out-of-domain data, and output a set
of candidate parse trees for the strings in the in-domain cor-
pus, with their normalized scores. These normalized scores
(posterior probabilities) are then used to give weights to the
features extracted from each candidate parse, in just the way
that they provide expected counts for an expectation maxi-
mization algorithm.
For the unsupervised trials that we report, we collected
up to 20 candidate parses per string5. We were interested in
investigating the effects of adaptation, not in optimizing per-
formance, hence we did not empirically optimize the mixing
parameter ?A for the new trials, so as to avoid obscuring the
effects due to adaptation alone. Rather, we used the best
4This is consistent with the results presented in
Bacchiani and Roark (2003), which found a small but con-
sistent improvement in performance with count merging versus
model interpolation for n-gram modeling.
5Because of the left-to-right, heuristic beam-search, the parser
does not produce a chart, rather a set of completed parses.
performing parameter from the supervised trials, namely
0.20c?(A). Since we are no longer limited to manually anno-
tated data, the amount of in-domain WSJ data that we can
include is essentially unlimited. Hence the trials reported go
beyond the 40,000 sentences in the Penn WSJ Treebank, to
include up to 5 times that number of sentences from other
years of the WSJ.
Table 8 shows the results of unsupervised adaptation as
we have described it. Note that these improvements are had
without seeing any manually annotated Wall St. Journal
treebank data. Using the approximately 40,000 sentences
in f2-21, we derived a 3.8 percent F-measure improvement
over using just the out of domain data. Going beyond the
size of the Penn Treebank, we continued to gain in accuracy,
reaching a total F-measure improvement of 4.2 percent with
200 thousand sentences, approximately 5 million words. A
second iteration with this best model, i.e. re-parsing the 200
thousand sentences with the adapted model and re-training,
yielded an additional 0.65 percent F-measure improvement,
for a total F-measure improvement of 4.85 percent over the
baseline model.
A final unsupervised adaptation scenario that we inves-
tigated is self-adaptation, i.e. adaptation on the test set it-
self. Because this adaptation is completely unsupervised,
thus does not involve looking at the manual annotations at
all, it can be equally well applied using the test set as the un-
supervised adaptation set. Using the same adaptation proce-
dure presented above on the test set itself, i.e. producing the
top 20 candidates from WSJ;23 with normalized posterior
probabilities and re-estimating, we produced a self-adapted
parsing model. This yielded an F-measure accuracy of 76.8,
which is a 1.1 percent improvement over the baseline.
5 Conclusion
What we have demonstrated in this paper is that maximum a
posteriori (MAP) estimation can make out-of-domain train-
ing data beneficial for statistical parsing. In the most likely
scenario ? porting a parser to a novel domain for which there
is little or no annotated data ? the improvements can be quite
large. Like active learning, model adaptation can reduce the
amount of annotation required to converge to a best level
of performance. In fact, MAP coupled with active learning
may reduce the required amount of annotation further.
There are a couple of interesting future directions for this
System % of ?A Baseline Adapted ?F
WSJ;2-21 LR LP F LR LP F
Gildea 100 c?(A) 86.1 86.6 86.35 86.3 86.9 86.60 0.25
MAP 100 0.20?c(A) 86.9 87.1 87.00 87.2 87.5 87.35 0.35
MAP 75 0.20?c(A) 86.6 86.8 86.70 87.1 87.3 87.20 0.50
MAP 50 0.20?c(A) 86.3 86.4 86.35 86.7 86.9 86.80 0.45
MAP 25 0.20?c(A) 84.8 85.0 84.90 85.3 85.5 85.40 0.50
MAP 10 0.20?c(A) 82.6 82.6 82.60 84.3 84.4 84.35 1.75
MAP 10 c?(A) 82.6 82.6 82.60 83.2 83.4 83.30 0.70
MAP 5 0.20?c(A) 80.4 80.6 80.50 83.0 83.1 83.05 2.55
Table 7: Parser performance on WSJ;23, supervised adaptation. All models use Brown;T,H as the out-of-domain treebank.
Baseline models are built from the fractions of WSJ;2-21, with no out-of-domain treebank.
Adaptation Iter- LR LP F- ?F
Sentences ation measure
0 0 76.0 75.4 75.70
4000 1 78.6 77.9 78.25 2.55
10000 1 78.9 78.0 78.45 2.75
20000 1 79.3 78.5 78.90 3.20
30000 1 79.7 78.9 79.30 3.60
39832 1 79.9 79.1 79.50 3.80
100000 1 79.7 79.2 79.45 3.75
200000 1 80.2 79.6 79.90 4.20
200000 2 80.6 80.5 80.55 4.85
Table 8: Parser performance on WSJ;23, unsupervised
adaptation. For all trials, the base training is Brown;T, the
held out is Brown;H plus the parser output for WSJ;24, and
the mixing parameter ?A is 0.20c?(A).
research. First, a question that is not addressed in this paper
is how to best combine both supervised and unsupervised
adaptation data. Since each in-domain resource is likely to
have a different optimal mixing parameter, since the super-
vised data is more reliable than the unsupervised data, this
becomes a more difficult, multi-dimensional parameter op-
timization problem. Hence, we would like to investigate au-
tomatic methods for choosing mixing parameters, such as
EM. Also, an interesting question has to do with choosing
which treebank to use for out-of-domain data. For a new
domain, is it better to choose as prior the balanced Brown
corpus, or rather the more robust Wall St. Journal treebank?
Perhaps one could use several out-of-domain treebanks as
priors. Most generally, one can imagine using k treebanks,
some in-domain, some out-of-domain, and trying to find the
best mixture to suit the particular task.
The conclusion in Gildea (2001), that out-of-domain tree-
banks are not particularly useful in novel domains, was pre-
mature. Instead, we can conclude that, just as in other sta-
tistical estimation problems, there are generalizations to be
had from these out-of-domain trees, providing more robust
estimates, especially in the face of sparse training data.
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
language model adaptation. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intelli-
gence, pages 598?603.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Conference of the North
American Chapter of the Association for Computational
Linguistics, pages 132?139.
Zhiyi Chi and Stuart Geman. 1998. Estimation of proba-
bilistic context-free grammars. Computational Linguis-
tics, 24(2):299?305.
Michael J. Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the 35th
Annual Meeting of the Association for Computational
Linguistics, pages 16?23.
Michael J. Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Michael J. Collins. 2000. Discriminative reranking for nat-
ural language parsing. In The Proceedings of the 17th
International Conference on Machine Learning.
M. J. F. Gales. 1998. Maximum likelihood linear transfor-
mations for hmm-based speech recognition. Computer
Speech and Language, pages 75?98.
Jean-Luc Gauvain and Chin-Hui Lee. 1994. Maximum
a posteriori estimation for multivariate gaussian mixture
observations of markov chains. IEEE Transactions on
Speech and Audio Processing, 2(2):291?298.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the Sixth Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
01).
Rebecca Hwa. 1999. Supervised grammar induction us-
ing training data with limited constituent information. In
Proceedings of the 37th Annual Meeting of the Associa-
tion for Computational Linguistics.
Rebecca Hwa. 2001. On minimizing training corpus for
parser acquisition. In Proceedings of the Fifth Computa-
tional Natural Language Learning Workshop.
Mark Johnson and Stefan Riezler. 2000. Exploiting aux-
iliary distributions in stochastic unification-based gram-
mars. In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics.
Mark Johnson and Brian Roark. 2000. Compact non-left-
recursive grammars using the selective left-corner trans-
form and factoring. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics (COL-
ING), pages 355?361.
Mark Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24(4):617?636.
L. Lamel, J.-L. Gauvain, and G. Adda. 2002. Unsupervised
acoustic model training. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), pages 877?880.
C. J. Legetter and P.C. Woodland. 1995. Maximum like-
lihood linear regression for speaker adaptation of contin-
uous density hidden markov models. Computer Speech
and Language, pages 171?185.
Fernando C.N. Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora. In
Proceedings of the 30th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 128?135.
Adwait Ratnaparkhi. 1999. Learning to parse natural lan-
guage with maximum entropy models. Machine Learn-
ing, 34:151?175.
Brian Roark. 2001a. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Brian Roark. 2001b. Robust Probabilistic Predictive
Syntactic Processing. Ph.D. thesis, Brown University.
http://arXiv.org/abs/cs/0105019.
Brian Roark. 2003. Robust garden path parsing. Natural
Language Engineering, 9(2):1?24.
Language model adaptation with MAP estimation
and the perceptron algorithm
Michiel Bacchiani, Brian Roark and Murat Saraclar
AT&T Labs-Research, 180 Park Ave., Florham Park, NJ 07932, USA
{michiel,roark,murat}@research.att.com
Abstract
In this paper, we contrast two language model
adaptation approaches: MAP estimation and
the perceptron algorithm. Used in isolation, we
show that MAP estimation outperforms the lat-
ter approach, for reasons which argue for com-
bining the two approaches. When combined,
the resulting system provides a 0.7 percent ab-
solute reduction in word error rate over MAP
estimation alone. In addition, we demonstrate
that, in a multi-pass recognition scenario, it is
better to use the perceptron algorithm on early
pass word lattices, since the improved error rate
improves acoustic model adaptation.
1 Introduction
Most common approaches to language model adapta-
tion, such as count merging and model interpolation, are
special cases of maximum a posteriori (MAP) estima-
tion (Bacchiani and Roark, 2003). In essence, these ap-
proaches involve beginning from a smoothed language
model trained on out-of-domain observations, and adjust-
ing the model parameters based on in-domain observa-
tions. The approach ensures convergence, in the limit, to
the maximum likelihood model of the in-domain obser-
vations. The more in-domain observations, the less the
out-of-domain model is relied upon. In this approach, the
main idea is to change the out-of-domain model parame-
ters to match the in-domain distribution.
Another approach to language model adaptation would
be to change model parameters to correct the errors
made by the out-of-domain model on the in-domain data
through discriminative training. In such an approach,
the baseline recognizer would be used to recognize in-
domain utterances, and the parameters of the model ad-
justed to minimize recognition errors. Discriminative
training has been used for language modeling, using vari-
ous estimation techniques (Stolcke and Weintraub, 1998;
Roark et al, 2004), but language model adaptation to
novel domains is a particularly attractive scenario for dis-
criminative training, for reasons we discuss next.
A key requirement for discriminative modeling ap-
proaches is training data produced under conditions that
are close to testing conditions. For example, (Roark et al,
2004) showed that excluding an utterance from the lan-
guage model training corpus of the baseline model used
to recognize that utterance is essential to getting word
error rate (WER) improvements with the perceptron al-
gorithm in the Switchboard domain. In that paper, 28
different language models were built, each omitting one
of 28 sections, for use in generating word lattices for the
omitted section. Without removing the section, no benefit
was had from models built with the perceptron algorithm;
with removal, the approach yielded a solid improvement.
More time consuming is controlling acoustic model train-
ing. For a task such as Switchboard, on which the above
citation was evaluated, acoustic model estimation is ex-
pensive. Hence building multiple models, omitting var-
ious subsections is a substantial undertaking, especially
when discriminative estimation techniques are used.
Language model adaptation to a new domain, how-
ever, can dramatically simplify the issue of controlling
the baseline model for producing discriminative training
data, since the in-domain training data is not used for
building the baseline models. The purpose of this paper is
to compare a particular discriminative approach, the per-
ceptron algorithm, which has been successfully applied
in the Switchboard domain, with MAP estimation, for
adapting a language model to a novel domain. In addi-
tion, since the MAP and perceptron approaches optimize
different objectives, we investigate the benefit from com-
bination of these approaches within a multi-pass recogni-
tion system.
The task that we focus upon, adaptation of a general
voicemail recognition language model to a customer ser-
vice domain, has been shown to benefit greatly from
MAP estimation (Bacchiani and Roark, 2003). It is an
attractive test for studying language model adaptation,
since the out-of-domain acoustic model is matched to
the new domain, and the domain shift does not raise the
OOV rate significantly. Using 17 hours of in-domain
observations, versus 100 hours of out-of-domain utter-
ances, (Bacchiani and Roark, 2003) reported a reduction
in WER from 28.0% using the baseline system to 20.3%
with the best performing MAP adapted model. In this pa-
per, our best scenario, which uses MAP adaptation and
the perceptron algorithm in combination, achieves an ad-
ditional 0.7% reduction, to 19.6% WER.
The rest of the paper is structured as follows. In the
next section, we provide a brief background for both
MAP estimation and the perceptron algorithm. This is
followed by an experimental results section, in which we
present the performance of each approach in isolation, as
well as several ways of combining them.
2 Background
2.1 MAP language model adaptation
To build an adapted n-gram model, we use a count
merging approach, much as presented in (Bacchiani and
Roark, 2003), which is shown to be a special case of max-
imum a posteriori (MAP) adaptation. Let wO be the out-
of-domain corpus, and wI be the in-domain sample. Let
h represent an n-gram history of zero or more words. Let
ck(hw) denote the raw count of an n-gram hw in wk,
for k ? {O, I}. Let p?k(hw) denote the standard Katz
backoff model estimate of hw given wk. We define the
corrected count of an n-gram hw as:
c?k(hw) = |wk| p?k(hw) (1)
where |wk| denotes the size of the sample wk. Then:
p?(w | h) =
?hc?O(hw) + c?I(hw)
?h
?
w? c?O(hw
?) +
?
w? c?I(hw
?)
(2)
where ?h is a state dependent parameter that dictates how
much the out-of-domain prior counts should be relied
upon. The model is then defined as:
p?(w | h) =
{
p?(w | h) if cO(hw) + cI(hw) > 0
?p?(w | h?) otherwise
(3)
where ? is the backoff weight and h? the backoff history
for history h.
The principal difficulty in MAP adaptation of this sort
is determining the mixing parameters ?h in Eq. 2. Follow-
ing (Bacchiani and Roark, 2003), we chose a single mix-
ing parameter for each model that we built, i.e. ?h = ?
for all states h in the model.
2.2 Perceptron algorithm
Our discriminative n-gram model training approach uses
the perceptron algorithm, as presented in (Roark et al,
2004), which follows the general approach presented in
(Collins, 2002). For brevity, we present the algorithm,
not in full generality, but for the specific case of n-gram
model training.
The training set consists of N weighted word lattices
produced by the baseline recognizer, and a gold-standard
transcription for each of the N lattices. Following (Roark
et al, 2004), we use the lowest WER hypothesis in the
lattice as the gold-standard, rather than the reference tran-
scription. The perceptron model is a linear model with k
feature weights, all of which are initialized to 0. The al-
gorithm is incremental, i.e. the parameters are updated at
each example utterance in the training set in turn, and the
updated parameters are used for the next utterance. Af-
ter each pass over the training set, the model is evaluated
on a held-out set, and the best performing model on this
held-out set is the model used for testing.
For a given path pi in a weighted word lattice L, let
w[pi] be the cost of that path as given by the baseline rec-
ognizer. Let GL be the gold-standard transcription for
L. Let ?(pi) be the K-dimensional feature vector for pi,
which contains the count within the path pi of each fea-
ture. In our case, these are unigram, bigram and trigram
feature counts. Let ??t ? RK be the K-dimensional fea-
ture weight vector of the perceptron model at time t. The
perceptron model feature weights are updated as follows
1. For the example lattice L at time t, find p?it such that
p?it = argmin
pi?L
(w[pi] + ??(pi) ? ??t) (4)
where ? is a scaling constant.
2. For the 0 ? k ? K features in the feature weight
vector ??t,
??t+1[k] = ??t[k] + ?(p?it)[k] ? ?(GL)[k] (5)
Note that if p?it = GL, then the features are left un-
changed.
As shown in (Roark et al, 2004), the perceptron fea-
ture weight vector can be encoded in a deterministic
weighted finite state automaton (FSA), so that much of
the feature weight update involves basic FSA operations,
making the training relatively efficient in practice. As
suggested in (Collins, 2002), we use the averaged per-
ceptron when applying the model to held-out or test data.
After each pass over the training data, the averaged per-
ceptron model is output as a weighted FSA, which can be
used by intersecting with a lattice output from the base-
line system.
3 Experimental Results
We evaluated the language model adaptation algorithms
by measuring the transcription accuracy of an adapted
voicemail transcription system on voicemail messages re-
ceived at a customer care line of a telecommunications
network center. The initial voicemail system, named
Scanmail, was trained on general voicemail messages
collected from the mailboxes of people at our research
site in Florham Park, NJ. The target domain is also com-
posed of voicemail messages, but for a mailbox that re-
ceives messages from customer care agents regarding
network outages. In contrast to the general voicemail
messages from the training corpus of the Scanmail sys-
tem, the messages from the target domain, named SS-
NIFR, will be focused solely on network related prob-
lems. It contains frequent mention of various network
related acronyms and trouble ticket numbers, rarely (if at
all) found in the training corpus of the Scanmail system.
To evaluate the transcription accuracy, we used a multi-
pass speech recognition system that employs various
unsupervised speaker and channel normalization tech-
niques. An initial search pass produces word-lattice out-
put that is used as the grammar in subsequent search
passes. The system is almost identical to the one de-
scribed in detail in (Bacchiani, 2001). The main differ-
ences in terms of the acoustic model of the system are
the use of linear discriminant analysis features; use of a
100 hour training set as opposed to a 60 hour training set;
and the modeling of the speaker gender which in this sys-
tem is identical to that described in (Woodland and Hain,
1998). Note that the acoustic model is appropriate for ei-
ther domain as the messages are collected on a voicemail
system of the same type. This parallels the experiments
in (Lamel et al, 2002), where the focus was on AM adap-
tation in the case where the LM was deemed appropriate
for either domain.
The language model of the Scanmail system is a Katz
backoff trigram, trained on hand-transcribed messages of
approximately 100 hours of voicemail (1 million words).
The model contains 13460 unigram, 175777 bigram, and
495629 trigram probabilities. The lexicon of the Scan-
mail system contains 13460 words and was compiled
from all the unique words found in the 100 hours of tran-
scripts of the Scanmail training set.
For every experiment, we report the accuracy of the
one-best transcripts obtained at 2 stages of the recog-
nition process: after the first pass lattice construction
(FP), and after vocal tract length normalization and gen-
der modeling (VTLN), Constrained Model-space Adap-
tation (CMA), and Maximum Likelihood Linear regres-
sion adaptation (MLLR). Results after FP will be denoted
FP; results after VTLN, CMA and MLLR will be denoted
MP.
For the SSNIFR domain we have available a 1 hour
manually transcribed test set (10819 words) and approx-
imately 17 hours of manually transcribed adaptation data
(163343 words). In all experiments, the vocabulary of
the system is left unchanged. Generally, for a domain
shift this can raise the error rate significantly due to an
increase in the OOV rate. However, this increase in error
rate is limited in these experiments, because the majority
of the new domain-dependent vocabulary are acronyms
System FP MP
Baseline 32.7 28.0
MAP estimation 23.7 20.3
Perceptron (FP) 26.8 23.0
Perceptron (MP) ? 23.9
Table 1: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the two methods, versus the
baseline out-of-domain system.
which are covered by the Scanmail vocabulary through
individual letters. The OOV rate of the SSNIFR test set,
using the Scanmail vocabulary is 2%.
Following (Bacchiani and Roark, 2003), ?h in Eq. 2 is
set to 0.2 for all reported MAP estimation trials. Follow-
ing (Roark et al, 2004), ? in Eq. 4 is also (coincidentally)
set to 0.2 for all reported perceptron trials. For the percep-
tron algorithm, approximately 10 percent of the training
data is reserved as a held-out set, for deciding when to
stop the algorithm.
Table 1 shows the results using MAP estimation and
the perceptron algorithm independently. For the percep-
tron algorithm, the baseline Scanmail system was used to
produce the word lattices used in estimating the feature
weights. There are two ways to do this. One is to use the
lattices produced after FP; the other is to use the lattices
produced after MP.
These results show two things. First, MAP estimation
on its own is clearly better than the perceptron algorithm
on its own. Since the MAP model is used in the ini-
tial search pass that produces the lattices, it can consider
all possible hypotheses. In contrast, the perceptron algo-
rithm is limited to the hypotheses available in the lattice
produced with the unadapted model.
Second, training the perceptron model on FP lattices
and applying that perceptron at each decoding step out-
performed training on MP lattices and only applying the
perceptron on that decoding step. This demonstrates the
benefit of better transcripts for the unsupervised adapta-
tion steps.
The benefit of MAP adaptation that leads to its supe-
rior performance in Table 1 suggests a hybrid approach,
that uses MAP estimation to ensure that good hypotheses
are present in the lattices, and the perceptron algorithm
to further reduce the WER. Within the multi-pass recog-
nition approach, several scenarios could be considered to
implement this combination. We investigate two here.
For each scenario, we split the 17 hour adaptation set
into four roughly equi-sized sets. In a first scenario, we
produced a MAP estimated model on the first 4.25 hour
subset, and produced word lattices on the other three sub-
sets, for use with the perceptron algorithm. Table 2 shows
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
MAP estimation 25 25.6 21.5
Perceptron (FP) 25 23.8 20.5
Perceptron (MP) 25 ? 20.8
Table 2: Recognition on the 1 hour SSNIFR test set using
systems obtained by supervised LM adaptation on the 17
hour adaptation set using the first method of combination
of the two methods, versus the baseline out-of-domain
system.
the results for this training scenario.
A second scenario involves making use of all of the
adaptation data for both MAP estimation and the percep-
tron algorithm. As a result, it requires a more compli-
cated control of the baseline models used for producing
the word lattices for perceptron training. For each of the
four sub-sections of the adaptation data, we produced a
baseline MAP estimated model using the other three sub-
sections. Using these models, we produced training lat-
tices for the perceptron algorithm for the entire adaptation
data set. At test time, we used the MAP estimated model
trained on the entire adaptation set, as well as the percep-
tron model trained on the entire set. The results for this
training scenario are shown in table 3.
Both of these hybrid training scenarios demonstrate a
small improvement by using the perceptron algorithm on
FP lattices rather than MP lattices. Closely matching the
testing condition for perceptron training is important: ap-
plying a perceptron trained on MP lattices to FP lattices
hurts performance. Iterative training did not produce fur-
ther improvements: training a perceptron on MP lattices
produced by using both MAP estimation and a perceptron
trained on FP lattices, achieved no improvement over the
19.6 percent WER shown above.
4 Discussion
This paper has presented a series of experimental re-
sults that compare using MAP estimation for language
model domain adaptation to a discriminative modeling
approach for correcting errors produced by an out-of-
domain model when applied to the novel domain. Be-
cause the MAP estimation produces a model that is used
during first pass search, it has an advantage over the
perceptron algorithm, which simply re-weights paths al-
ready in the word lattice. In support of this argument, we
showed that, by using a subset of the in-domain adapta-
tion data for MAP estimation, and the rest for use in the
perceptron algorithm, we achieved results at nearly the
same level as MAP estimation on the entire adaptation
set.
System MAP Pct. FP MP
Baseline 0 32.7 28.0
MAP estimation 100 23.7 20.3
Perceptron (FP) 100 22.9 19.6
Perceptron (MP) 100 ? 19.9
Table 3: Recognition on the 1 hour SSNIFR test set us-
ing systems obtained by supervised LM adaptation on the
17 hour adaptation set using the second method of com-
bination of the two methods, versus the baseline out-of-
domain system.
With a more complicated training scenario, which used
all of the in-domain adaptation data for both methods
jointly, we were able to improve WER over MAP estima-
tion alone by 0.7 percent, for a total improvement over
the baseline of 8.4 percent.
Studying the various options for incorporating the per-
ceptron algorithm within the multi-pass rescoring frame-
work, our results show that there is a benefit from incor-
porating the perceptron at an early search pass, as it pro-
duces more accurate transcripts for unsupervised adapta-
tion. Furthermore, it is important to closely match testing
conditions for perceptron training.
References
Michiel Bacchiani and Brian Roark. 2003. Unsupervised
language model adaptation. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP), pages 224?227.
Michiel Bacchiani. 2001. Automatic transcription of
voicemail at AT&T. In Proceedings of the Interna-
tional Conference on Acoustics, Speech, and Signal
Processing (ICASSP).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?8.
L. Lamel, J.-L. Gauvain, and G. Adda. 2002. Unsuper-
vised acoustic model training. In Proceedings of the
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP), pages 877?880.
Brian Roark, Murat Saraclar, and Michael Collins. 2004.
Corrective language modeling for large vocabulary
ASR with the perceptron algorithm. In Proceedings
of the International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
A. Stolcke and M. Weintraub. 1998. Discriminitive lan-
guage modeling. In Proceedings of the 9th Hub-5
Conversational Speech Recog nition Workshop.
P.C. Woodland and T. Hain. 1998. The September 1998
HTK Hub 5E System. In The Proceedings of the 9th
Hub-5 Conversational Speech Recognition Workshop.
