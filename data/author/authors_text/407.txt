Explo i t ing a Probabi l ist ic  Hierarchical Mode l  for Generat ion 
Srinivas Bangalore and Owen Rambow 
AT&T Labs Research 
180 Park Avenue 
F lorham Park, NJ 07932 
{sr in?,  rambow}@research,  a r t .  com 
Abst ract  
Previous stochastic approaches to generation 
do not include a tree-based representation of
syntax. While this may be adequate or even 
advantageous for some applications, other ap- 
plications profit from using as much syntactic 
knowledge as is available, leaving to a stochas- 
tic model only those issues that are not deter- 
mined by the grammar. We present initial re- 
suits showing that a tree-based model derived 
from a tree-annotated corpus improves on a tree 
model derived from an unannotated corpus, and 
that a tree-based stochastic model with a hand- 
crafted grammar outpertbrms both. 
1 I n t roduct ion  
For many apt)lications in natural anguage gen~ 
eration (NLG), the range of linguistic expres- 
sions that must be generated is quite restricted, 
and a grammar tbr generation can be fltlly spec- 
ified by hand. Moreover, in ma W cases it; is very 
important not to deviate from certain linguis- 
tic standards in generation, in which case hand- 
crafted grammars give excellent control. How- 
ever, in other applications tbr NLG the variety 
of the output is much bigger, and the demands 
on the quality of the output somewhat less strin- 
gent. A typical example is NLG in the con- 
text of (interlingua- or transthr-based) machine 
translation. Another reason for reb~xing the 
quality of the output may be that not enough 
time is available to develop a flfll grammar tbr 
a new target language in NLG. In all these 
cases, stochastic ("empiricist") methods pro- 
vide an alternative to hand-crafted ("rational- 
ist") approaches to NLG. To our knowledge, the 
first to use stochastic techniques in NLG were 
Langkilde and Knight (1998a) and (1998b). In 
this paper, we present FERGUS (Flexible Em- 
piricist/Rationalist Generation Using Syntax). 
FErtGUS follows Langkilde and Knight's seminal 
work in using an n-gram language model, but; we 
augment it with a tree-based stochastic model 
and a traditional tree-based syntactic grammar. 
More recent work on aspects of stochastic gen- 
eration include (Langkilde and Knight, 2000), 
(Malouf, 1999) and (Ratnaparkhi, 2000). 
Betbre we describe in more detail how we use 
stochastic models in NLG, we recall the basic 
tasks in NLG (Rainbow and Korelsky, 1992; Re- 
iter, 1994). During text  p lanning,  content and 
structure of the target text; are determined to 
achieve the overall communicative goal. Dur- 
ing sentence planning, linguistic means - in 
particular, lexical and syntactic means are de- 
termined to convey smaller pieces of meaning. 
l)uring real izat ion,  the specification chosen in 
sentence planning is transtbrmed into a surface 
string, by line~rizing and intlecting words in the 
sentence (and typically, adding function words). 
As in the work by Langkilde and Knight, our 
work ignores the text planning stage, but it; does 
address the sentence, planning and the realiza- 
tion stages. 
The structure of the paper is as tbllows. In 
Section 2, we present he underlying rammat- 
ical tbrmalism, lexicalized tree-adjoining gram- 
mar (LTAG). In Section 3, we describe the ar- 
chitecture of the system, and some of the mod- 
ules. In Section 4 we discuss three experiments. 
In Section 5 we colnpare our work to that of 
Langkilde and Knight (1998a). We conclude 
with a summary of on-going work. 
2 Modeling Syntax 
In order to model syntax, we use an existing 
wide-coverage grammar of English, the XTAG 
grammar developed at the University of Peru> 
sylvania (XTAG-Gronp, 1999). XTAG is a tree~ 
adjoining grammar (TAG) (Joshi, 1987a). In 
42 
,,. . . . . . . . . . . . . . .  , . .  
T rees  used  in der ivat ion  " -~,- " "  
7 / / " ' ,P  A , / /  i P--,7 
N Aux 1) N / N l' NI' ~ / l) A N 
I I I I I I ".-JI I I 
there was n{} cost estimate i'of the second phase 
1 '{3 71 Y2 {z2 74  71 75  (z 1 
Other supertags for the loxemcs found in the training corpus: 
IIOI1C {Z4 {Z 1 {~ I (z 1 {z4 
{z 5 {z 2 7 2 {z 5 
5 more 11 more  4 more I(} more 
IlOIle {z 3 {z 2 
{z 1 3' 2 
5 IIIOrC 2 Ill{We 
Figure 1: An excerl}t from the Xq'AG gr~um\]lm" t(} derive Th,{"r('. wa.s u,o to.st {:stim,,tc .fi)r the .second 
phase.; dotted lines show t)ossit}le a{ljun('ti{ms that were not made 
a TAG, the elementary structures are \])hrase- 
structure trees which are comt)osed using two 
ot}er~tions , sut}stitui,ion (w\]fich al}i}{;n{ts one 
tree ~1; the fl:ontier of another) mtd a(tjnlmtio\]t 
(which ins{;rts one tree into the mi{l{ll{', of im- 
o|;her). In gral)hi{:al re i ) rese l l ta l ; i{} l l  , \ ] l o{tes  &I; 
which substitul;ion can take 1)lac{'~ are \]uarked 
with dow\]>arrows. In linguisI;ic uses (}f TAG, 
we asso{'ial;e one lexical item (its anchor) with 
each tree, and {}he or (typically) more trees with 
each lexical ire\]n; its a result we obtain a lexi- 
calized TAG or LTAG. Since ea{'h lexi{:al item 
is associated with a whole tree (rather than 
just a phrase-stru{'ture ule, tbr exa\]nl)le), we 
cm\] st)e(:i\[y t}oth the t)re{licate-argument struc- 
ture of the lexeme (t}y includillg nodes at which 
its arguments must sut}stitute) and morl)h{}- 
syntactic onstraints uch as sut}je('t-verb agree- 
men| within the sl;rucl;ure associated with the 
l exeme.  This property is retbrred to as TAG's 
cztcndcd domain of locality. N{)l;e that in an 
LTAG, I;here is no distinction betw{:en lexicon 
nnd grammar. A Smnl)le grammar is shown in 
F igure  1. 
-We depart fl:om XTAG in our treatment of 
|;rees tbr adjuncts (such as adverl}s), an{t in- 
stead tbllow McDonMd and Pusteiovsky (1985). 
While in XTAG the elementary tree for an ad- 
.iuncl; conl;ains 1)hrase sl;ru{:i;ure |;hat atta{:hes 
l,he adjmmt to ll{}(tes in another tree with the 
stag anchored by 
71 \])et 
72 N 
7:~ A ux 
7.t Pro, l} 
0 T 
1',~ A(lj 
adjoins to direction 
NP 
N 
S, VP 
NP, VP 
S 
N 
right 
right 
right 
h:ft 
right 
right 
Figure 2: Adjmmtion table tbr graamnar frag- 
lUe l l t  
sl)ecitie(1 label (say, VP) from the specified di- 
rection (say, fronl the left), in our systenl the 
trees for adjuncts imply express their active va- 
lency, trot 11o1\[; how they connect to the lexical 
item they modi\[y. This ilfl'ormal;ion is kept in 
the  adjunct|on table which is associated with the. 
grammar; an excerpt is shown in Figure 2. Trees 
t;hat can adjoin to other trees (and have entries 
in the adjunct|on table) ;~re called gamma-trees, 
the other trees (which can only t)e substituted 
into other trees) are alpha-trees. 
Note that we can refer to a tree by a combi- 
nation of its name, called its supertag, and its 
anchor. N)r example, (q is the supertag of an 
all)ha-tree anchored 1)y a noun that projects up 
to NP, wMle 72 is |;lie superi;ag of it gamma tree 
anchored by a noun that only t)rojects 1;{) N (we 
43 
assume adjectives are adjoined at N), and, as 
the adjunction table shows, can right-adjoin to 
an N. So that es t imate~ is a particular tree 
in our LTAG grammar. Another tree that a su- 
pertag can be associated with is ~t~, which rep- 
resents the predicative use of a noun.1 Not all 
nouns are associated with all nominal supertags: 
the expletive there is only an cq. 
When we derive a sentence using an LTAG, 
we combine elementary trees fl'om the grmnmar 
using adjunction and substitution. For extort- 
pie, to derive the sentence There was no cost 
estimate for the second phase from the gram- 
mar in Figure 1, we substitute the tree tbr there 
into the tree tbr estimate. We then adjoin in 
the trees tbr the auxiliary was, the determiner 
no, and the modit)ing noun cost. Note that 
these adjunctions occur at different nodes: at 
VP, NP~ and N, respectively. We then adjoin in 
the preposition, into which we substitute ph, ase, 
into which we adjoin the and second. Note that 
all adjunctions are by gamma trees, and all sub- 
stitution by alpha trees. 
If we want to represent this derivation graphi- 
cally, we can do so in a derivation tree, which we 
obtain as follows: whelmver we adjoin or sub- 
stitute a tree t~ into a tree t2, we add a new 
daughter labeled t~ to the node labeled tg. As 
explained above, the name of each tree used is 
the lexeme along with the supertag. (We omit 
the address at which substitution or adjunction 
takes place.) The derivation tree t br our deriva- 
tion is shown in Figure 3. As can be seen, this 
structure is a dependency tree and resembles a
representation of lexical argument structure. 
aoshi (1987b) claims that TAG's properties 
make it particularly suited as a syntactic rep- 
resentation tbr generation. Specifically, its ex- 
tended domain of locality is useflfl in genera- 
tion tbr localizing syntactic properties (includ- 
ing word order as well as agreement and other 
morphological processes), and lexicalization is 
useful tbr providing an interfime from seman- 
tics (the deriw~tion tree represent the sentence's 
predicate-argument structure). Indeed, LTAG 
has been used extensively in generation, start- 
ing with (McDonald and Pustejovsky, 1985). 
1Sentences such as Peter is a doctor can be analyzed 
with with be as the head, as is more usual, or with doctor 
as the head, as is done in XTAG 1)eeause the be really 
behaves like an auxiliary, not like a flfll verb. 
estimate 
there was no cost for 
74 
c~ 1 7 3 71 72  
phase c~ 
the second 
71 75 
Figure 3: Derivation tree tbr LTAG deriw~tion 
of There was no cost estimate for the second 
phase 
3 System Overv iew 
FERGUS is composed of three modules: the 2?ee 
Chooser, the Unraveler, and the Linear Prece- 
dence (LP) Chooser. The input to the system is 
a dependency tree as shown in Figm'e 4. Note 
that the nodes are labeled only with lexemes, 
not with supertags. 2 The Tree Chooser then 
uses a stochastic tree model to choose TAG 
trees fbr the nodes in the input structure. This 
step can be seen as analogous to "supertag- 
ging" (Bangalore and Joshi, 1999), except that 
now supertags (i.e., names of trees) must be 
fbund tbr words in a tree rather than tbr words 
in a linear sequence. The Unraveler then uses 
the XTAG grammar to produce a lattice of all 
possible linearizations that arc compatible with 
the supertagged tree and the XTAG. The LP 
Chooser then chooses the most likely traversal 
of this lattice, given a language model. We dis- 
cuss the three components in more detail. 
The Tree Chooser draws on a tree model, 
which is a representation of XTAG derivation 
tbr 1,000,000 words of the Wall Street Journal. a 
The ~IYee Chooser makes the simplifying as- 
2In the system that we used in the experiments de- 
scribed in Section 4, all words (including flmction words) 
need to be present in tt, e inlmt representation, flflly in- 
flected. This is of course unrealistic for applications. In 
this paper, we only aim to show that the use of a %'ee 
Model improves performance of a stochastic generator. 
See Section 6 for further discussion. 
3This was constructed from the Penn ~lS"ee Bank us- 
ing some heuristics, since the Pemt ~IYee Bank does not 
contain hill head-dependent infornlation; as a result of 
the use of heuristics, the Tree Model is not flflly correct. 
44 
estimate 
there was no cost for 
phase 
the second 
Figure 4: Inlmt to FEII.GUS 
Smnl)tions that the (:hoice of n tree. tbr ~t node 
dei)ends only on its daughter nodes, thus allow- 
ing \]'or a tot)-(lown dynamic l)rogrmnlning algo- 
ril;hln. St)ccifically, a node 'q in the intml; si;ru(:- 
ture is assigned ~t sui)e, rt;~g s so th;tt the 1)rol):t - 
|)ilil;y of fin(ling the treelet (;()m\])ose(t of ~1 with 
superta X ,~ ;rod :dl of its (l;mght(;rs (as foun(t 
in I;he ini)ut sl;rucl;ure) is m;rximiz(;d, and such 
l;ha, t .'~ is (:Oml)a, tit)le with 'q'~s mother ~tll(l her 
sut)e, rtag .sin. Here, "('omt)atible" l:nemis |;hat; 
the tree ret)resclfl;ed by .'~ can 1)e adjoined or 
substii;uted into the tree ret)resented by ,%~, :m- 
('or(ling to the XTAG gra, nmmr. For our exmn- 
t)le senl;en(:(;, the, ini)ui; 1;o the sysl,e,m is the t;ree 
shown in Figure d, and the oul;1)ul; fi'om l;he ~.l~ee 
(~hooser is the, tree. ;ts shown in \],'igure. 3. No(;c 
that while a (le, riw~tion tree in TAG fully Sl)(:(:- 
iiies a derivation and thus :t smTth,(:e, s(mte.n(:e, 
the oul;lmt fl:om the ~l~-ee Chooser (loes not;. 
There are two reasons. \]?irstly, as exi)laine.d at; 
the end of Section 2, fin: us trees (:orrespond- 
ing to adjuncts are underspe.(-itied with rest)ect 
to the adjunct ion site aat(t/or I;h(; a(ljmwl;ion 
direction (from left; or fl'Oln right) in the tree 
of the mother node, or they nmy 1)e m~orde.re(l 
with respc(:t o other ad.iun('ts (tbr ex~nni)l(; , the 
fmnous adjective ordering t)roblem). Secondly, 
Sul)ert;ags nl~y h~ve been (:hose.n incorre(:l;ly or 
not at ;ill. 
The Unr;~veler takes ;~s input the senti- 
specitied derivation tree, (Figure 3) ml(l 1)ro- 
duces a word lattice. Each node, in the deriw> 
tion tree consisl;s of ~t lexi(:al item m~d a su- 
pertag. The linear order ()f the dmlghte.rs with 
rest)cot to l;he he;td 1)osil;ion of ;t sut)ertng is 
st)ecilied in the Xrl'AG grmnmar. This informa- 
tion is (:onsulted to order the (laughter nodes 
I 
TAG I)eliwtlion Tree 
wilht,ut ,SIIpeltags 
"l'lCC (?h?~)scl / J -= - Tree 
\\  h 
I 
One siagle sealli specified \ \ 
'I'AG l)cdvalion \] lees \ 
I \[(h'alll,llill'\] 
W(nd l.altice 
Shillg 
Figure 5: Ar(:hii;e(:ture of FERGUS 
with rcsl)e(:t to the head at each le.vel of the 
(terival;ion tree. in cases where ~ daughter node 
C&ll \])(I ntta('hed at more thin1 ()lie t)lace in the 
head SUl)ertag (as is the (:;~se in our exmnt)le for 
"was and for), n disjunction of M1 these, positions 
are. assigned to the dmlghter node. A botton> 
up algorithm the.n constructs ~ lattice that ell- 
(;odes the strings rei)re.sented 1)y (;~(:1~ level of 
th(! derivation tr(x'.. The latti('e~ at the. root of 
the (teriwttion tr(w. is the result o171;\]m Um';tveler. 
'Fhe resulting l~ttti(:(; for the ('.Xaml)h'. s(ml;e.nce is 
shown in Figure 6. 
The \]~t;ti('.e. OUtlmt from the. Unra.veh'a" en- 
codes all t)ossible word sequences l)erniitted 
1)y the derivation strueialre. We rmlk these. 
word sequen(:es in the order of their likeli- 
hoo(l 1)y composing the lattice with a finite- 
state machine rel)rese.nting ~ trigrmn bmgu~Ge 
1no(tel. This mo(M has 1)ee.n ('onstructed froln 
1,000,0000 words of W~dl Stre, et Journal (:orpus. 
We 1)i(:k the 1)est path through the lattice, re- 
sulting from the comt)osition using the Viterl)i 
algorithm, ;m(t this to I) ranking word sequence 
is the outt)ut of the LP Chooser. 
4 Experiments and Results 
In order l:o show |;ll~tl; Lhe llSO, of ~t tl:ce lIlode\] 
trod a, grmmnar doe.s indeed hell) pe, rformmme, 
we pe.rforme, d three experiments: 
45 
Q 
Figure 6: Word lattice tbr example sentence 
after Tree Chooser and Unraveler using the 
supertag-based model 
? For the baseline experiment, we impose a 
random tree structure ibr each sentence of 
the cortms and build a Tree Model whose 
parameters consist of whether a lexeme l~t 
precedes or tbllows her mother lexeme lm. 
We call this the Baseline Left-Right (LR) 
Model. This model generates There was 
est imate for  phase the second no cost . for 
our example input. 
? In the second experiment, we derive the 
parmneters tbr the LR model fl'om an an- 
notated corpus, in particular, the XTAG 
derivation tree cortms. This model gener- 
ates Th, crc no est imate J'or the second phase 
was cost . tbr our example input. 
? In the third experiment, as described 
in Section 3, we employ the supertag-based 
tree model whose parameters consist of 
whether a lexeme l d with supertag Sd is zt 
dependent of Im with supertag sin. Fm'- 
thermore we use the supertag in~brmation 
provided by the XTAG grammar to or- 
der the dependents. This model generates 
Thcrc was no cost est imate for  the second 
phase . tbr our example input, which is in- 
deed the sentence ibund in the WSJ. 
As in the case of machine translation, evalu- 
ation in generation is a complex issue. We use 
two metrics suggested in the MT literature (A1- 
shawl et al, 1.998) based on string edit; distance 
t)etween the outtmt of the generation system 
and the reference corpus string front the WSJ. 
These metrics, simple accuracy and generation 
accuracy, allow us to evaluate without human 
intervention, automatically and objectively. 4 
Simple accuracy is the mnnber of insertion 
(I), deletion (D) and substitutions (S) errors 
between the target language strings in the test 
corpus and the strings produced by the genera- 
tion model. The metric is summarized in Equa- 
tion (1). R is the number of tokens in the target 
string. This metric is similar to the string dis- 
tance metric used for measuring speech recog- 
nition accuracy. 
I + D + .q 
S implcAccuracy  = (1 - - --) (1) 
R 
4\~7c do not address the issue of whether these metrics 
can be used for comparative valuation of other genera- 
tion systems. 
46 
Tree 
Model 
Simt)le Go, ner~rtion 
Ac(:ura('y Accuracy 
Average time 
per scnten(:(; 
Baseline LR Model 41.2% 56.2% 186ms 
~l?(;cbank derived LI/. Model 52.9% 66.8% 129ms 
Sut)ertag-bascd Model 58.!)% 72.4% 517ms 
Tabl(; 1: Performance results front the thre(', tree models. 
Unlike sl)eech recognition, the task of gener- 
ation involves reordering of tokens. The simple 
accuracy metric, however, penalizes a mist)lacc.d 
token twice, as a deletion from its c.xpo, ct('.d posi- 
tion and insertion at at different l)osition. Wc llSO 
~ second metric, Generation A(:(:ura('y, shown in 
Eqm~tion (2), which treats (hilt|ion of ~ token 
~tt OIIC location in 1;11(; string ~md th(; insertion 
of the same tok(m ~t anoth('a" location in tim 
string as one single mov('an(mt (;trot (M). This 
is in addition to the rem~fining insertions (1 t) 
and deletions (Dl). 
Ge'n(~'rationAcc',,racy = (1 - 
54 + I I + 1)' -t- ,q ) 
(2) 
The siml)lc, a(:cura('y, g(merntion a('(:ur;my a,n(l 
tim av(n:ag(~ time, ti)r goamration of (;a,(:h l;cst; s(~,u - 
t(m('c for tim tin'o,(', (}Xl)crinmnts ;~r(~ tabul~m,xl in 
%d)le 1. The test set consist(xl of 1 O0 r~m(tonfly 
(:hoscn WS.I s(mt(m(:(; with ml ~w(n:age l ngt;h of 
16 words. As can be seen, tim sut)crtng-1)ased 
mo(M |rot)roves over the LR model derived from 
mmotated ata ~md both models improv(; over 
the baseline LR mod(:l. 
Sul)ertngs incorl)or~te richer infbrmation 
st|oh as argunmnt mid a(tjunci: disl;in(:tion, and 
nmnbcr and types of argunmnts. YVe cxt)(;(:t to 
iml)rove the performance of the supcrtag-bas(;d 
model by taking these features into a(:(:ount. 
In ongoing work, we h~vc developed tree- 
based metrics in addition to the string-l)ased 
presented here, in order to ewfluate sto(:hastic 
gener~tion models. We h~vc also attempted to 
correlate these quantitative metrics with human 
(tualitativ(~ judgcnl(mts. Ado, tail(~d dis(:ussion 
of these experiments and results is t)r(',s(mto, d 
in (Bangalore (',|; al., 2000). 
5 Compar i son  w i th  Langk i lde  8z 
Kn ight  
Langkildc and Knight (1998a) use a hand- 
(:rafted grmmmu: that maps semantic represen- 
tations to sequences of words with lino, arization 
constraints. A COml)lex semantic st, ructur( ~, is 
trnnsl~ted to ~L lattice,, mid a bigrmn langunge 
mode,1 t;hell (:hoost~,s &lltOllg {;}lo, l)ossiblo, surface, 
strings (moo(led in the l~ttice. 
The system of Langkildc 8~ Knight, Nitrogen, 
is similar to FERGUS in that generation is di- 
vided into two phases, the first of which results 
in a lattice fl'om which a surNcc si;ring is chosen 
during the, s(;cond t)has(; using a language model 
(in our case a trigram model, in Nitrogen's case 
a. 1)igr~ml 1no(M). Ih)w(',ver, (;t1(; first t)hases nr(', 
quit(', ditf(;r(mt. In FEI(.GUS, we sI;m:i; with a lex- 
i(:~d pr(',dit:at(;-argulnent st;ru(;l;ur(~ while in Ni- 
trogen, a more s0,mantic intmt is used. FEII.GUS 
(:ould (',asily |)(; augm(;nt(;d with a t)r(;t)ro(:cssor 
l;h~d; maps a so, m;mti(: rc, t)ro, s(mtal;ion t;o ore: syn- 
ta(:ti(: inl)ut; this is not the focus of our r(~sc~u'ch. 
\[Iowev(',r, ther(~ are two more imt)orl,mfl; differ- 
(m('es. First, |;t1(; h~m(t-crafl;ed grmmnar in Ni- 
trogen maps dir(;(:tly from semantics to a linear 
r(~l)r(;sentation , skipping tho, nr|)or(;s(:(mt rcI)rc- 
sentation usually f~vore(t br the, rod)r(',s(mtn|;ion 
of syntax. There is no stochastic tree model, 
since, the, re, ~tr(', no trees. In FEI{GUS, in|tied 
('hoices arc, ma(tc stochastically t)ascd on tim 
tree rcl)rcscntation in the "I?ce Chooser. This 
allows us to capture stochastically certain long- 
(tisl;ance cfli',(:ts which n-grmns camlot, such as 
sct)~ration of p;n'ts of a collocations (such as 
peT:form an ope~ution) through interl)osing ad- 
juncts (John peT:formed a long, .somewhat e- 
dious, and quite frustrating opcration on hi,s 
border collie). Second, tim hand-('rafl;cd gram- 
ln;tr llSCd in FEll.(-IUS was crafted indel)endcntly 
fl'om the n(;('xl for gent, rat;ion and is a imrcly 
(l(;(:larative rcl)rcs(mtation of English syntax. As 
47 
such, we can use it to handle morphological ef- 
fects such as agreement, which cannot in gen- 
eral be clone by an n-gram model and which are, 
at; the same time, descriptively straightforward 
and which are handled by all non-stochastic 
generation modules. 
6 Conclus ion and Out look  
We have presented empirical evidence that us- 
ing a tree model in addition to a language model 
can improve stochastic NLG. 
FERGUS aS presented in this paper is not 
ready to be used as a module in applications. 
Specifically, we will add a morphological compo- 
nent, a component that handles flmction words 
(auxiliaries, determiners), and a component 
that handles imnctuation. In all three cases, we 
will provide both knowledge-based and stochas- 
tic components, with the aim of comparing their 
behaviors, and using one type as a back-up tbr 
the other type. Finally, we will explore FI;R- 
OUS when applied to a language tbr which a 
much more limited XTAG grammar is available 
(for example, specit\[ying only the basic sentence 
word order as, sw, SVO, and speci(ying subject- 
verb agreement). In the long run, we intend 
FEI/OUS to become a flexible system which will 
use hand-crafted knowledge as much as possible 
and stochastic models as much as necessary. 
References 
Hiyan Alshawi, Srinivas Bangalore, and Shona 
Douglas. 1998. Automatic acquisition of hi- 
erarchical transduction models tbr machine tr 
anslation. In Proceedings of the 36th Annual 
Meeting Association for Computational Lin- 
guistics, Montreal, Canada. 
Srinivas Bangalore and Aravind Joshi. 1999. 
Supertagging: An approach to ahnost pars- 
ing. Computational Linguistics, 25(2). 
Sriniw~s Bangalore, Owen Rainbow, and Steve 
Whittaker. 2000. Ewfluation Metrics for 
Generation. In Proceedings of International 
Cor~:ferenee on Natural Language Generation, 
Mitzpe Ramon. Isreal. 
Aravind K. Joshi. 1987a. An introduction to 
Tree Adjoining Grammars. In A. Manaster- 
Ramer, editor, Mathematics of Language, 
pages 87-115. John Benjamins, Amsterdam. 
Aravind K. Joshi. 1987b. Tlm relevance of 
tree adjoining grammar to generation. In 
Gerard Kempeu, editor, Natural Language 
Generation: New Results in Artificial In- 
teUigence, Psychology and Linguistics, pages 
233 252. Kluwer Academic Publishers, Dor- 
drecht /Boston /Lancaster. 
Irene Langkilde and Kevin Knight. 1998a. Gen- 
eration that exploits corpus-based statistical 
knowledge. In 36th Meeting of the Associa- 
tion .for Computational Linguistics and 17th 
International Cor~:\['crcnce on Computational 
Linguistics (COLING-A CL'98), pages 704- 
710, Montrdal, Canada. 
Irene Langkilde and Kevin Knight. 1998b. 
The practical value of n-grams in genera- 
tion. In Proceedings of the Ninth Interna- 
tional Natural Language Generation Work- 
shop (INLG'98), Niagara-on-the-Lake, On- 
tario. 
Irene Langkilde and Kevin Knight. 2000. 
Forest-based statistical sentence generation. 
In Proceedings of First North American A CL, 
Seattle, USA, May. 
Robert Malouf. 1999. Two methods tbr 1)re- 
dieting the order of prenonfinal t~djectives in 
english. In Pwceedings of CLINg9. 
David D. McDonMd and James D. Pusteiovsky. 
1985. %~gs as a grammatical formalism tbr 
generation. In 23rd Meeting of the Associa- 
tion for Computational Linguistics (A CL '85), 
pages 94 103, Chicago, IL. 
Owen l:\[ambow and Tany~ Korelsky. 1992. Ap- 
plied text generation. In Third Conference on 
Applied Natural Language Processing, pages 
40 47, %ento, Italy. 
Adwait t/.atllaparkhi. 2000. Trainable methods 
for surface natural language generation. In 
Proceedings of First North American ACL, 
Seattle, USA, May. 
Ehud Reiter. 1994. Has a consensus NL gen- 
eration architecture appeared, and is it psy- 
cholinguistically plausible? In Proceedings of 
the 7th International Workshop on Natural 
Language Generation, pages 163-170, Maine. 
The XTAG-Group. 1999. A lexicalized %'ee 
Adjoining Grammar for English. Technical 
Report ht tp  ://w~rw. c is .  upenn, edu/~xtag/ 
tech- repor t / tech- repor t  .htral, The Insti- 
tute for Research in Cognitive Science, Uni- 
versity of Pennsylvania. 
48 
Finite-state Multimodal Parsing and Understanding 
Michael Johnston 
AT&T Labs - Research 
Shannon Laboratory, 180 Park Ave 
FIorham Park, NJ 07932, USA 
j ohnston@research ,  a t t .  tom 
Srinivas Bangalore 
AT&T Labs - Research 
Shannon Laboratory, 180 Park Ave 
Florham Park, NJ 07932, USA 
s r in i@research ,  a r t .  tom 
Abstract 
Multimodal interfaces require effective parsing and 
nn(lerstanding of utterances whose content is dis- 
tributed across multiple input modes. Johnston 1998 
presents an approach in which strategies lbr mul- 
timodal integration are stated declaratively using a 
unification-based grammar that is used by a mnlti- 
dilnensional chart parser to compose inputs. This 
approach is highly expressive and supports a broad 
class of interfaces, but offers only limited potential 
for lnutual compensation among the input modes, is 
subject o signilicant concerns in terms o1' COml)uta- 
tional complexity, and complicates selection among 
alternative multimodal interpretations of the input. 
In tiffs papeh we l)resent an alternative approacla 
in which multimodal lmrsing and understanding are 
achieved using a weighted finite-state device which 
takes speech and gesture streams as inputs and out- 
puts their joint interpretation. This approach is sig- 
nificantly more efficienl, enables tight-coupling of 
multimodal understanding with speech recognition, 
and provides a general probabilistic fralnework for 
multimodal ambiguity resolution. 
1 Introduction 
Multimodal interfaces are systems that allow input 
and/or output o be conveyed over multiple different 
channels uch as speech, graphics, and gesture. They 
enable more natural and effective interaction since 
different kinds of content can be conveyed in the 
modes to which they are best suited (Oviatt, 1997). 
Our specific concern here is with multimodal inter- 
faces supporting input by speech, pen, and touch, but 
the approach we describe has far broader applicabil- 
ity. These interfaces stand to play a critical role in the 
ongoing migration of interaction fi'oln the desktop 
to wireless portable computing devices (PI)As, next- 
generation phones) that offer limited screen real es- 
tale, and other keyboard-less platforms uch as pub- 
lic information kiosks. 
To realize their full potential, multimodal inter- 
faces need to support not just input from multiple 
modes, but synergistic multimodal utterances opti- 
mally distributed over the available modes (John- 
ston et al, 1997). In order to achieve this, an e f  
fcctive method for integration of content fi'Oln dill 
ferent modes is needed. Johnston (1998b) shows 
how techniques from natural language processing 
(unification-based gramumrs and chart parsing) can 
be adapted to support parsing and interpretation of
utterances distributed over multiple modes. In that 
approach, speech and gesture recognition produce ~,- 
best lists of recognition results which are assigned 
typed feature structure representations (Carpenter, 
1992) and passed to a luultidimensioual chart parsel ? 
that uses a lnultimodal unification-based granunar to 
combine the representations assigned to the input el- 
ements. Possible multimodal interpretations are then 
ranked and the optimal interpretation is passed on 
for execution. This approach overcomes many of 
the limitations of previous approaches tomultimodal 
integration such as (Bolt, 1980; Neal and Shapiro, 
1991) (See (Johnston ct al., 1997)(1). 282)). It sup- 
ports speech with multiple gestures, visual parsing 
of unimodal gestures, and its dechu'ative nature fa- 
cilitates rapid l)rototyping and iterative develol)meut 
of multimodal systems. Also, the unification-based 
approach allows for mutual COlnpensatiou of recog- 
nition errors in the individual modalities (Oviatt, 
1999). 
However, the unification-based approach does not 
allow for tight-conpling of nmltimodal parsing with 
speech and gesture recognition. Compensation elL 
fects are dependent on the correct answer appear- 
ing in the ~;,-best list of interpretations a signed to 
each mode. Multimodal parsing cannot directly in- 
fluence the progress of speech or gesture recognition. 
The multidimensional parsing approach is also sub- 
ject to significant concerns in terms of computational 
complexity. In the worst case, the multidimensional 
parsing algorithm (Johnston, 1998b) (p. 626) is ex- 
ponential with respect o the number of input ele- 
ments. Also this approach does not provide a nat- 
ural fiamework for combining the probabilities of 
speech and gesture vents in order to select among 
multiple competing multimodal interpretations. Wu 
et.al. (1999) present a statistical approach for select- 
ing among multiple possible combinations of speech 
369 
and gesture. However; it is not clear how the ap- 
proach will scale to more complex verbal language 
and combinations of speech with multiple gestures. 
In this papm, we propose an alternative approach 
that addresses these limitations: parsing, understand- 
ing, and integration of speech and gesture am pe> 
formed by a single finite-state device. With certain 
simplifying assumptions, multidimensional parsing 
and understanding with multimodal grammars can 
be achieved using a weighted finite-state automa- 
ton (FSA) running on throe tapes which represent 
speech input (words), gesture input (gesture sym- 
bols and reference markers), and their combined in- 
terpretation. We have implemented our approach in 
the context of a multimodal messaging application 
in which users interact with a company directo W 
using synergistic ombinations of speech and pen 
input; a multimodal variant of VPQ (Buntschuh et 
al., 1998). For example, the user might say emai l  
this person  and this person and gesture 
with the pen on pictures of two people on a user inter- 
face display. In addition to the user interface client, 
the architecture contains peech and gesture recog- 
nition components which process incoming streams 
of speech and electronic ink, and a multimodal lan- 
guage processing component (Figure 1 ). 
u, \[ 
ASR ~ I ~esture Recognizer \[ 
Multimodal Parser/Understander \] 
Backend 
Figure 1: Multimodal alvhitecture 
Section 2 provides background on finite-state lan- 
guage processing. In Section 3, we define and exem- 
plify multimodal context-fiee grammars (MCFGS) 
and their approximation as multimodal FSAs. We 
describe our approach to finite-state representation 
of meaning and explain how the three-tape finite 
state automaton can be factored out into a number 
of finite-state transducers. In Section 4, we explain 
how these transducers can be used to enable tight- 
coupling of multimodal language processing with 
speech and gesture recognition. 
2 Finite-state Language Processing 
Finite-state transducers (FST) are finite-state au- 
tomata (FSA) where each transition consists of an 
input and an output symbol. The transition is tra- 
versed if its input symbol matches the current sym- 
bol in the input and generates the output symbol as- 
sociated with the transition. In other words, an FST 
can be regarded as a 2-tape FSA with an input tape 
from which the input symbols are read and an output 
tape where the output symbols are written. 
Finite-state machines have been extensively ap- 
plied to many aspects of language processing in- 
cluding, speech recognition (Pereira nd Riley, 1997; 
Riccardi et al, 1996), phonology (Kaplan and Kay, 
1994), morphology (Koskenniemi, 1984), chunk- 
ing (Abney, 1991; Joshi and Hopely, 1997; Ban- 
galore, 1997), parsing (Roche, 1999), and machine 
translation (Bangalore and Riccardi, 2000). 
Finite-state models are attractive n~echanisms for 
language processing since they are (a) efficiently 
learnable fiom data (b) generally effective for decod- 
ing and (c) associated with a calculus for composing 
machines which allows for straightforward integra- 
tion of constraints fl'om various levels of language 
processing. Furdmrmore, software implementing 
the finite-state calculus is available for research pur- 
poses (Mohri eta\[., 1998). Another motivation for 
our choice of finite-state models is that they enable 
tight integration of language processing with speech 
and gesture recognition. 
3 Finite-state MultimodalGrammars 
Multimodal integration involves merging semantic 
content fi'om multiple streams to build a joint inter- 
pretation for a inultimodal utterance. We use a finite- 
state device to parse multiple input strealns and to 
combine their content into a single semantic repre- 
sentation. For an interface with n inodes, a finite- 
state device operating over n+ 1 tapes is needed. The 
first n tapes represent the input streams and r~ + \] is 
an output stream representing their composition. In 
the case of speech and pen input there are three tapes, 
one for speech, one for pen gesture, and a third for 
their combined meaning. 
As an example, in the messaging application 
described above, users issue spoken commands 
such as emai l  this person and that 
organization and gestm'e on the appropriate 
person and organization on the screen. The struc- 
ture and interpretation of multimodal colnlnands of 
this kind can be captured eclaratively in a multi- 
modal context-free grammar. We present a fi'agment 
capable of handling such commands in Figure 2. 
370 
S .~ V NP g:c:\]) NP -+ I)ET N 
CONJ --4 and:E:, NP --+ I)ET N CONJ NP 
V -+ cmail:g:cmail(\[ DET --+ |his:g:c 
V -+ page:c:page(\[ I)ET --+ lhat:?:c 
N --:. person:Gp:person( ENTP, Y
N -4 organization:Go:org( ENTRY 
N --+ dcpartment:Gd:dept( ENTRY 
ENTRY -> C:el :el c:g:) 
ENTRY -> c:e2:e2 c:g:) 
ENTRY -4 c:ea:ea g:e:) 
ENTP, Y --+ ... 
Figure 2: Multimodal grammar fragment 
The non-terminals in the multimodal grammar are 
atomic symbols. The multimodal aspects el' the 
grammar become apparent in the terlninals. Each 
terminal contains three components W:G:M corre- 
sponding to the n q- 1 tapes, where W is for the spo- 
ken language stream, G is the gesture stream, and 
M is the combined meaning. The epsilon symbol is 
used to indicate when oue of these is empty in a given 
terminal. The symbols in W are woMs from the 
speech stream. The symbols in G are of two types. 
Symbols like Go indicate the presence of a particular 
kind of gesturc in the gesture stream, while those like 
et are used as references to entities referred to by the 
gesture (See Section 3.1). Simple deictic pointing 
gestures are assigned semantic types based on tl~e n- 
tities they are references to. Gp represents a gestural 
tel'erence to a person on the display, Go to an orga- 
nization, and Gd lo a department. Compared with 
a feature-based multimodal gralnlnar, these types 
constitute a set of atomic categories which make 
ltle relewmt dislinclions for gesture vents prcdicl- 
lug speech events and vice versa. For example, if 
the gesture is G,, then phrases like thLs  person  
aud him arc preferred speech events and vice versa. 
These categories also play a role in constraining the 
semantic representation when the speech is under- 
specified with respect o semantic type (e.g. emai l  
th i s  one). These gesture symbols can be orga- 
nized into a type hierarchy reflecting the ontology 
of the entities in the application domain. For exam- 
pie, there might be a general type G with subtypes 
Go and Gp, where G v has subtypes G,,,,~ and Gpf for 
male and female. 
A multimodal CFG (MCFG) can be defined fop 
really as quadruple < N, 7', P, S >. N is the set of 
nonterminals. 1 ~ is the set of productions of the form 
A -+ (~whereA E Nand,~, C (NUT)* .  S i s  
the start symbol for the grammar. 7' is the set ot' ter- 
minals of the l'orm (W U e) : (G U e) : M* where 
W is the vocabulary of speech, G is the vocabulary 
of gesture=GestureSymbols U EventSymbols; 
GcsturcSymbols ={G v, Go, Gpj', G~.., ...} and 
a finite collections of \],gventSymbols ={c,,c~, 
. . . ,  c,,}. M is the vocabulary to lel)rcsent meaning 
and includes event symbols (Evenl:Symbol.s C M). 
In general a context-free grammar can be approx- 
imated by an FSA (Pereira and Wright 1997, Neder- 
her 1997). The transition symbols of the approx- 
imated USA are the terminals of the context-fiee 
grammar and in the case of multimodal CFG as de- 
tined above, these terminals contain three compo- 
nents, W, G and M. The multimodal CFG fi'ag- 
merit in Figurc 2 translates into the FSA in Figure 3, 
a three-tape finite state device capable of composing 
two input streams into a single output semantic rep- 
resentation stream. 
Our approach makes certain simplil'ying assump- 
tions with respect o ternporal constraints. In multi- 
gesture utterances the primary flmction of tempo- 
ral constraints i to force an order on the gestures. 
If you say move th i s  here  and make two .ges- 
tures, the first corresponds toth i  s and the second to 
here. Our multimodal grammars encode order but 
do not impose explicit temporal constraints, ltow- 
ever, general temporal constraints between speech 
and the first gesture can be enforced belbrc the FSA 
is applied. 
3.1 Finite-state Meaning Representation 
A novel aspect of our approach is that in addition 
to capturing the structure of language with a finite 
state device, we also capture meaning. Tiffs is very 
important in nmltimodal language processing where 
the central goal is to capture how the multiple modes 
contribute to the combined interpretation. Ottr ba- 
sic approach is to write symbols onto the third tape, 
which when concatenated together yield the seman- 
tic representation l'or the multimodal utterance. It 
suits out" purposes here to use a simple logical repre- 
sentation with predicates pred(....) and lists la, b,...l. 
Many other kinds of semantic representation could 
be generated. In the fl'agment in Figure 2, the word 
ema?l contributes email(\[ to the semantics tape, 
and the list and predicate arc closed when the rule 
S --+ V NP e:z:\]) applies. The word person  
writes person( on the semantics tape. 
A signiiicant problem we face in adding mean- 
ing into the finite-state framework is how to reprc- 
sent all of the different possible specific values that 
can be contributed by a gesture. For deictic refer- 
ences a unique identitier is needed for each object in 
the interface that the user can gesture on. For ex- 
alnple, il' the interface shows lists of people, there 
needs to be a unique ideutilier for each person. As 
part of the composition process this identifier needs 
371 
departmcnl:Gd:dept( cps:cl :el 
or,mnization:Go:or-( tnat:eps:eps ~ z } ~ ~ . \ [  3 ~ eps:eZ:e2 ~_ 
/ / ~ e~q'e~ ~.\ " \]--.~cps:el~s:) 
+:?,,+. 
and:eps:, 
Figure 3: Multimodal three-tape FSA 
to be copied from the gesture stream into the seman- 
tic representation. In the unification-based approach 
to multimodal integration, this is achieved by fea- 
ture sharing (Johnston, 1998b). In the finite-state ap- 
proach, we would need to incorporate all of the dif- 
ferent possible IDs into the FSA. For a person with 
id objid345 you need an arc e:objid345:objid345 
to transfer that piece of information fiom the ges- 
ture tape to the lneaning tape. All of the arcs for 
different IDs would have to be repeated everywhere 
in the network where this transfer of information is 
needed. Furthermore, these arcs would have to be 
updated as the underlying database was changed or 
updated. Matters are even worse for more complex 
pen-based ata such as drawing lines and areas in an 
interactive map application (Cohen et al, 1998). In 
this case, the coordinate set from the gesture needs 
to be incorporated into the senmntic representation. 
It might not be practical to incorporate the vast nuln- 
bet of different possible coordinate sequences into an 
FSA. 
Our solution to this problem is to store these 
specific values associated with incoming gestures 
in a finite set of buffers labeled el,e,),ea . . . .  and 
in place of the specific content write in the nalne 
of the appropriate buffer on the gesture tape. In- 
stead of having the specific values in the FSA, we 
have the transitions E:C I :C \ ] ,  C :C2:C2 ,  s:e3:e:3.., in 
each location where content needs to be transferred 
from the gesture tape to the meaning tape (See Fig- 
ure 3). These are generated fi'om the ENTRY pro- 
ductions in the multilnodal CFG in Figure 2. The 
gesture interpretation module empties the buffers 
and starts back at el after each multimodal com- 
mand, and so we am limited to a finite set of ges- 
ture events in a single utterance. Returning to 
the example email this person and that 
organization, assume the user gestures on en- 
tities objid367 and objid893. These will be stored 
in buffers el and e2. Figure 4 shows the speech and 
gesture streams and the resulting combined meaning. 
The elements on the meaning tape are concate- 
nated and the buffer references are replaced to yield 
S: email this person and that organization 
G: Gp cl 'Go e2 
M: email(\[ person(ct) , org(c2) \]) 
Figure 4: Messaging domain example 
email(~)er.son(objid367), or.q(objidS93)\]). As 
more recursive semantic phenomena such as pos- 
sessives and other complex noun phrases are added 
to the grammar the resulting machines become 
larger. However, the computational consequences 
of this can be lessened by lazy ewfluation tech- 
niques (Mohri, 1997) and we believe that this finite- 
state approach to constructing semantic representa- 
tions is viable for a broad range of sophisticated lan- 
guage interface tasks. We have implemented a size- 
able multimodal CFG for VPQ (See Section 1): 417 
rules and a lexicon of 2388 words. 
3.2 Multimodal Finite-state Transducers 
While a three-tape finite-state automaton is feasi- 
ble in principle (Rosenberg, 1964), currently avail- 
able tools for finite-state language processing (Mohri 
et al, 1998) only support finite-state transducers 
(FSTs) (two tapes). Furthermore, speech recogniz- 
ers typically do not support ile use of a three-tape 
FSA as a language model. In order to implement our 
approach, we convert he three-tape FSA (Figme 3) 
into an FST, by decomposing the transition symbols 
into an input component (G x W) and output compo- 
nent M, thus resulting in a function, T:(G x W) --+ 
M. This corresponds to a transducer in which ges- 
ture symbols and words are on the :input ape and the 
meaning is on the output tape (Figure 6). The do- 
main of this function T can be further curried to re- 
sult in a transducer that maps 7~:G --> W (Figure 7). 
This transducer captures the constraints that gesture 
places on the speech stream and we use it as a Jan- 
guage model for constraining the speech recognizer 
based on the recognized gesture string. In the fop 
lowing section, we explain how "F and 7% are used in 
conjunction with the speech recognition engine and 
gesture recognizer and interpreter to parse and inter- 
372 
pret nmltimodal input. 
4 Applying Multimodal Transducers  
There arc number of different ways in which multi- 
modal finite-state transducers can be integrated with 
speech and gesture recognition. The best approach 
to take depends on the properties of the lmrticular 
interface to be supported. The approach we outline 
here involves recognizing esture ilrst then using the 
observed gestures to modify the language model for 
speech recognition. This is a good choice if there 
is limited ambiguity in gesture recognition, for ex- 
an@e, if lhe m~jority of gestures are unambiguous 
deictic pointing gestures. 
The first step is for the geslure recognition and 
interpretation module to process incoming pen ges- 
tures and construct a linite state machine GeslltVe 
corresponding tothe range of gesture interpretations. 
Ill our example case (Figure 4) tile gesture input is 
unambiguous and the Gestttre linite state machine 
will be as in Figure 5. \]f the gestural input involves 
gesture recognition or is otherwise ambiguous it is 
represented as a lattice indicating all of the possi- 
ble recognitions and interpretations o1' tile gesture 
stream. This allows speech to compensate for ges- 
ture errors and mutual compensation. 
Figure 5: (;eslttre linite-smte machine 
This Ge,s'lure linite state machine is then com- 
posed with the transducer "R, which represents the 
relationship between speech and gesture (Figure 7). 
The result of this composition is a transducer Gesl- 
Lang (Figure 8). This transducer represents the re- 
lationship between this particular sl.ream of gestures 
and all of the possible word sequences tlmt could co- 
occur with those oes" , rares. In order to use this in- 
lbnnation to guide the speech recognizer, we lhcn 
take a proiection on the output ape (speech) of Gesl- 
Lang to yield a finite-state machine which is used 
as a hmguage model for speech recognition (Fig- 
ure 9). Using this model enables the gestural in- 
formation to directly influence the speech recog- 
nizer's search. Speech recognition yields a lattice 
of possible word sequences. In our example case it 
yMds the wol~.t sequence mail this person 
and that organization (Figure 10). We 
now need to reintegrale the geslure inl'ormation that 
wc removed in the prqjection step before recog- 
nition. This is achieved by composing Gest- 
Lang (Figure 8) with the result lattice from speech 
recognition (Figure 10), yielding transducer Gesl~ 
&)eechFST (Figure 11). This transducer contains 
the information both from the speech stream and 
from the gesture stream. The next step is to gen- 
erate the Colnbined meaning representation. To 
achieve this Gest&)eechFST (G : W) is converted 
into an FSM GestSpeechFSM by combining out- 
put and input on one tape (G x W) (Figure 12). 
GestSk)eeckFSM is then composed with T (Fig- 
ure 6), which relates speech and gesture to mean- 
ing, yielding file result transducer Result (Figure 13). 
The meaning is lead from the output tape yield- 
ing cm,dl(\[perso,,,(ca), m'O(e2)\]). We have imple- 
mented lifts approach and applied it in a multimodal 
interface to VPQ on a wireless PDA. In prelilni- 
nary speech recognition experiments, our approach 
yielded an average o1' 23% relative sentence-level er- 
ror reduction on a corpus of 1000 utterances (John- 
ston and Bangalore, 2000). 
5 Conclusion 
We have presented here a novel approach to muI- 
timodal hmguage processing in which spoken lan- 
guage and gesture are parsed and integrated by a 
single weighted lhfite-state device. This device pro- 
vides language models for speech and gesture recog- 
nition alld colllposes content from speech and gcs- 
lure into a single semantic representalion. Our ap- 
proach is novel not just in addressing multimodal 
hmguage but also in the encoding of semantics as 
well as syntax in a finile-state device. 
Compared to previous al~proaches (Johnston el al., 
1997; Jolmston, 1998a; Wu et al, 1999) which com- 
pose elements from 'n.-best lists of recognition re- 
sults, our approach provides an unprecedenled po- 
tential for mutual compensation among the input 
modes. It enables gestural input to dynamically 
alter the hmguage model used tbr speech recogni- 
lion. Furthermore, our approach avoids the com- 
putational complexity of multidimensional multi- 
modal parsing and our system of weighted finite- 
stale transducers provides a well understood prob- 
abilistic framcwork for combining the probability 
distributions associated with speech and gesture in- 
put and selecting among multiple competing nmlti- 
modal interpretations. Since the finite-state approach 
is more lightweight in coml)utational needs, it can 
more readily be deployed on a broader ange of plat- 
forms. 
In ongoing research, we are collecting a corpus of 
multimodal data ill order to forlnally evahmte the ef- 
fectiveness of our approach and to train weights for 
1he multimodal inile-state transducers. While we 
have concentrated here on understanding, in princi- 
ple the same device could be applied to multimodal 
373 
Gd_dcpartnlcnt:dept( c I_cps:e 1
. ~ Go or,,anization:or,,( ells tnat:eps ~ z j - ~ ~ ~ a b__...______cz ps:cz 
, ~  op~y,:om~< ~._ :pg_*>~_>/  -_______/ -- - " 'W' : ' ,  
ells_and:, ~ -- 
ells:l) .? 
Figure 6: Transducer elating gesture and speech to meaning (7-':(G x W) - -  M) 
Gd:departmcnt e 1 :eps 
/f~'~/'~ Go:organization cps:that ~ z } "~ -~ }-........_.cz:cl)s 
~ . . " /~"MQI~S? ' - - J~- - J " JNN. .  e3:ep*-"-"'""~"s _.,,-((43) 
('7') ~p,:~ma,, ~ ~:y>. /  ~p,:a,,d " - - - - - - - - - -~ '~ 
-..,..j......__eps:pags_.......ac-..__J - __ 
Figure 7: Transducer elating gesture and speech (TE:G ---+ W) 
eps:elllail eps:lhal Gp:person 
Figure 8: GestLang Transducer 
(}o:lu'ganizalion 
u page ~ this - - 
Figure 9: Projection of Output tape of GestLang Transducer 
@ email " @  this . @  person .~@ and . @  that =@ 
Figure 10: Result from speech recognizer 
Figure 11: GestureSpeechFST 
Figure 12: GestureSpeech FSM 
organization ~ @  
organization _- 
Q ~,,se,l,a,l:Olll,i,~,~.q) ot,s_,,,is:e,,, >@ o,)-p~rso'l:,'e~go"~> G ~,?,,s:el >q)  
EllS_all(l:, 
Figure 13: Result Transducer 
eps:) ~, Q~ 
~i,.:) >(~) ep,:\]) >(~) 
374 
generation which we are currently investigating. We 
are also exploring teclmiques to extend compilation 
fi'om feature structures gralnnlars to FSTs (Johnson, 
19!)8) to nmltimodal unification-based grammars. 
References 
Steven Abney. 1991. Parsing by chunks. In Robert 
Berwick, Steven Abney, and Carol Tenny, editors, 
Principle-based palwing. Kluwer Academic Pub- 
lishers. 
Srinivas Bangalore and Giuseppe Riccardi. 2000. 
Stochastic lhfite-state models for spoken language 
machine translation. In Proceedings o/" the Work- 
shop on Embedded Machine Translation Systems. 
Srinivas Bangalore. 1997. ComplexiO, of Lexic.al 
Descriptions and its Relevance to Partial Pmw- 
ing. Ph.l). tlaesis, University of Pennsylwmia, 
t~hiladelphia, PA, August. 
Robert A. Bolt. 1980. "put-thal-there":voicc and 
gesture at the graphics interface. Computer 
Graphics, 14(3):262-270. 
Bruce Buntschuh, C. Kamm, G. DiFabbrizio, 
A. Abella, M. Mohri, S. Narayanan, I. Zel.ikovic, 
R.D. Sharp, J. Wright, S. Marcus, J. Shaffer, 
R. I)uncan, and J.G. Wilpon. 1998. Vpq: A 
spoken language interface to large scale directory 
information. In Proceedin,q,s o/' ICSLI', Sydney, 
Australia. 
Robert Carpenter. 1992. The logic qf OT)ed./~'alure 
structures. Cambridge University Press, England. 
Philip R. Cohen, M. Johnston, 1). McGee, S. L. 
Oviatt, J. Pittman, I. Smith, L. Chen, and 
J. Clew. 1998. Multimodal interaction for dis- 
tributed interactive simulation. In M. Maybury 
and W. Wahlster, editors, Readings itz Intelligent 
httelfiwes. Morgan Kaul'mann Publishers. 
Mark Jollnson. 1998. Finite-state approximation 
of constraint-based grammars using left-corner 
grammar transforms. In Proceedings q/'COLING- 
ACL, pages 619-623, Montreal, Canada. 
Michael Johnston and Srinivas Bangalore. 2000. 
Tight-coupling of multimodal language process- 
ing with speech recognition. Technical report, 
AT&T Labs - Reseamh. 
Michael Johnston, ER. Cohen, D. McGee, S.L. Ovi- 
att, J.A. Pittman, and 1. Smidl. 1997. Unilication- 
based multimodal integration. In Proceedings o/ 
lhe 35th ACL, pages 281-288, Madrid, Spain. 
Michael Johnston. 1998a. Mullimodal language 
processing. In Proceedings q/" ICSLP, Sydney, 
Australia. 
Michael Johnston. 1998b. Unification-based multi- 
modal parsing. In Proceedings of COLING-ACL, 
pages 624-630, Montreal, Canada. 
Aravind Joshi and Philip Hopely. 1997. A parser 
fiom antiquity. Natural Language Engilzeering, 
2(4). 
Ronald M. Kaplan and M. Kay. 1994. Regular mod- 
els of phonological rule systems. Computational 
Linguislics, 20(3):331-378. 
K. K. Koskenniemi. 1984. 7ire-level morphology: a
general computation model,for wordzform recog- 
nition and production. Ph.D. thesis, University of 
He\[sinki. 
Mehryar Mohri, Fernando C. N. Pereira, and 
Michael Riley. 1998. A rational design for a 
weighted .finite-state transducer librao,. Num- 
ber 1436 in Lecture notes in computer science. 
Springm; Berlin ; New York. 
Mehryar Mohri. 1997. Finite-state transducers in 
language and speech processing. (7Oml~utational 
Linguistics, 23(2):269-312. 
J. G. Neal and S. C. Shapiro. 1991. Intelligent multi- 
media interface technology. In J. W. Sulliwm and 
S. W. Tylm, editors, Intelligent User lnter\['aces, 
pages 45-68. ACM Press, Addison Wesley, New 
York. 
Sharon L. Oviatt. 1997. Multimodal interactive 
maps: l)esigning l'or human performance. In 
Hmmut-Computer Interaction, pages 93-129. 
Sharon L. Ovialt. 1999. Mutual disambiguation of
recognition errors in a inultimodal architecture. In 
Cltl '99, pages 576-583. ACM Press, New York. 
Fernando C.N. Pereira and Michael I). Riley. 1997. 
Speech recognition by composition of weighted fi- 
nite automata. In E. Roche and Schabes Y., ed- 
itors, Finite State Devices for Nalttral Language 
Processitlg, pages 431-456. MIT Press, Cam- 
bridge, Massachusetts. 
Giuseppe Riccardi, R. Pieraccini, and E. Bocchieri. 
1996. Stochastic Automata for Language Model- 
ing. Computer Speech and Language, 10(4):265- 
293. 
Emmanuel Roche. 1999.  Finite state transducers: 
parsing free and fl'ozen sentences. In Andrfis Ko- 
rnai, editol, Extended Finite State Models el'Lan- 
guage. Cambridge University Press. 
A.L .  Rosenberg. 1964. On n-tape finite state accep- 
ters. FOCS, pages 76-81. 
Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 
1999.  Multilnodal integration - a statistical view. 
IEEE Transactions on Multimedia, I (4):334-34 l,
I)ecember. 
375 
 	
 
ffTowards Automatic Generation of Natural Language Generation
Systems
John Chen?, Srinivas Bangalore?, Owen Rambow?, and Marilyn A. Walker?
Columbia University? AT&T Labs?Research?
New York, NY 10027 Florham Park, NJ 07932
{jchen,rambow}@cs.columbia.edu {srini,walker}@research.att.com
Abstract
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would be
desirable for a system developer if there were
an automatic method for creating natural lan-
guage generation components that can produce
quality output efficiently. We conduct experi-
ments that show that this goal appears to be
realizable. In particular we discuss a natural
language generation system that is composed of
SPoT, a trainable sentence planner, and FER-
GUS, a stochastic surface realizer. We show
how these stochastic NLG components can be
made to work together, that they can be ported
to new domains with apparent ease, and that
such NLG components can be integrated in a
real-time dialog system.
1 Introduction
Systems that interact with the user via natural
language are in their infancy. As these systems
mature and become more complex, it would
be desirable for a system developer if there
were automatic methods for creating natural
language generation (NLG) components that
can produce quality output efficiently. Stochas-
tic methods for NLG may provide such auto-
maticity, but most previous work (Knight and
Hatzivassiloglou, 1995), (Langkilde and Knight,
1998), (Oh and Rudnicky, 2000), (Uchimoto et
al., 2000), (Bangalore and Rambow, 2000) con-
centrate on the specifics of individual stochastic
methods, ignoring other issues such as integra-
bility, portability, and efficiency. In contrast,
this paper investigates how different stochastic
NLG components can be made to work together
effectively, whether they can easily be ported to
new domains, and whether they can be inte-
grated in a real-time dialog system.
Request(DEPART?DATE)
Surface Generator
FERGUS
TTS
SPoT
Dialog Manager
Sentence Planner
DM
Imp?conf(N)
soft?merge
Text to Speech
Implicit?confirm(NEWARK)
Implicit?confirm(DALLAS)
period
Imp?conf(D)
Flying from Newark to
Dallas.  What date would
you like to leave?
Request(D?D)
Figure 1: Components of an NLG system.
Recall the basic tasks in NLG. During text
planning, content and structure of the target
text are determined to achieve the overall com-
municative goal. During sentence planning, lin-
guistic means?in particular, lexical and syn-
tactic means?are determined to convey smaller
pieces of meaning. During realization, the spec-
ification chosen in sentence planning is trans-
formed into a surface string by linearizing and
inflecting words in the sentence (and typically,
adding function words). Figure 1 shows how
such components cooperate to generate text
corresponding to a set of communicative goals.
Our work addresses both the sentence plan-
ning stage and the realization stage. The sen-
tence planning stage is embodied by the SPoT
sentence planner (Walker et al, 2001), while
the surface realization stage is embodied by the
FERGUS surface realizer (Bangalore and Ram-
bow, 2000). We extend the work of (Walker et
al., 2001) and (Bangalore and Rambow, 2000)
in various ways. We show that apparently each
of SPoT and FERGUS can be ported to differ-
ent domains with little manual effort. We then
show that these two components can work to-
gether effectively. Finally, we show the on-line
integration of FERGUS with a dialog system.
2 Testing the Domain Independence
of Sentence Planning
In this section, we address the issue of the
amount of effort that is required to port a sen-
tence planner to new domains. In particular,
we focus on the SPoT sentence planner. The
flexibility of the training mechanism that SPoT
employs allows us to perform experiments that
provide evidence for its domain independence.
Being a sentence planner, SPoT chooses ab-
stract linguistic resources (meaning-bearing lex-
emes, syntactic constructions) for a text plan. A
text plan is a set of communicative goals which
is assumed to be output by a dialog manager of
a spoken dialog system. The output of SPoT is
a set of ranked sentence plans, each of which is
a binary tree with leaves labeled by the commu-
nicative goals of the text plan.
SPoT divides the sentence planning task into
two stages. First, the sentence-plan-generator
(SPG) generates 12-20 possible sentence plans
for a given input text plan. These are gener-
ated randomly by incrementally building each
sentence plan according to some probability
distribution. Second, the sentence-plan-ranker
(SPR) ranks the resulting set of sentence plans.
SPR is trained for this task via RankBoost (Fre-
und et al, 1998), a machine learning algorithm,
using as training data sets of sentence plans
ranked by human judges.
In porting SPoT to a new domain, this last
point seems to be a hindrance. New train-
ing data in the new domain ranked by hu-
man judges might be needed in order to train
SPoT. To the contrary, our experiments that
show that this need not be the case. We par-
tition the set of all features used by (Walker et
al., 2001) to train SPoT into three subsets ac-
cording to their level of domain and task de-
pendence. Domain independent features are
features whose names include only closed-class
words, e.g. ?in,? or names of operations that in-
crementally build the sentence plan, e.g. merge.
Domain-dependent, task-independent features
are those whose names include open class words
Features Used Mean Score S.D.
all 4.56 0.68
domain-independent 4.55 0.69
task-independent 4.20 0.99
task-dependent 3.90 1.19
Table 1: Results for subsets of features used to
train SPoT
specific to this domain, e.g. ?travel? or the
names of the role slots, e.g. $DEST-CITY. Do-
main dependent, task dependent features are
features whose names include the value of a role
filler for the domain, e.g. ?Albuquerque.?
We have trained and tested SPoT with these
different feature subsets using the air-travel do-
main corpus of 100 text plans borrowed from
(Walker et al, 2001), using five fold cross-
validation. Results are shown in Table 2 us-
ing t-tests with the modified Bonferroni statis-
tic for multiple comparisons. Scores can range
from 1.0 (worst) to 5.0 (best). The results in-
dicate that the domain independent feature set
performs as well as all the features (t = .168, p
= .87), but that both the task independent (t
= 6.25, p = 0.0) and the task dependent (t =
4.58, p = 0.0) feature sets perform worse.
3 Automation in Training a Surface
Realizer
As with the sentence planning task, there is the
possibility that the task of surface realization
may be made to work in different domains with
relatively little manual effort. Here, we perform
experiments using the FERGUS surface realizer
to determine whether this may be so. We re-
view the FERGUS architecture, enumerate re-
sources required to train FERGUS, recapitulate
previous experiments that indicate how these
resources can be automatically generated, and
finally show how similar ideas can be used to
port FERGUS to different domains with little
manual effort.
3.1 Description of the FERGUS
Surface Realizer
Given an underspecified dependency tree repre-
senting one sentence as input, FERGUS outputs
the best surface string according to its stochas-
tic modeling. Each node in the input tree corre-
sponds to a lexeme. Nodes that are related by
grammatical function are linked together. Sur-
face ordering of the lexemes remains unspecified
in the tree.
FERGUS consists of three models: tree
chooser, unraveler, and linear precedence
chooser. The tree chooser associates a su-
pertag (Bangalore and Joshi, 1999) from a tree-
adjoining grammar (TAG) with each node in
the underspecified dependency tree. This par-
tially specifies the output string?s surface order;
it is constrained by grammatical constraints en-
coded by the supertags (e.g. subcategorization
constraints, voice), but remains free otherwise
(e.g. ordering of modifiers). The tree chooser
uses a stochastic tree model (TM) to select a
supertag for each node in the tree based on lo-
cal tree context. The unraveler takes the re-
sulting semi-specified TAG derivation tree and
creates a word lattice corresponding to all of
the potential surface orderings consistent with
this tree. Finally, the linear precedence (LP)
chooser finds the best path through the word
lattice according to a trigram language model
(LM), specifying the output string completely.
Certain resources are required in order to
train FERGUS. A TAG grammar is needed?
the source of the supertags with which the
semi-specified TAG derivation tree is annotated.
There needs to be a treebank in order to ob-
tain the stochastic model TM driving the tree
chooser. There also needs to be a corpus of sen-
tences in order to train the language model LM
required for the LP chooser.
3.2 Labor-Minimizing Approaches to
Training FERGUS
The resources that are needed to train FER-
GUS seem quite labor intensive to develop. But
(Bangalore et al, 2001) show that automati-
cally generated version of these resources can
be used by FERGUS to obtain quality output.
Two kinds of TAG grammar are used in (Ban-
galore et al, 2001). One kind is a manually de-
veloped, broad-coverage grammar for English:
the XTAG grammar (XTAG-Group, 2001). It
consists of approximately 1000 tree frames. Dis-
advantages of using XTAG are the consider-
able amount of human labor expended in its
development and the lack of a treebank based
on XTAG?the only way to estimate parame-
ters in the TM is to rely on a heuristic map-
ping of XTAG tree frames onto a pre-existing
treebank (Bangalore and Joshi, 1999). Another
kind of grammar is a TAG automatically ex-
tracted from a treebank using the techniques of
(Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)).
These techniques extract a linguistically mo-
tivated TAG using heuristics programmed us-
ing a modicum of human labor. They nullify
the disadvantages of using the XTAG grammar,
but they introduce potential complications?
notably, an extracted grammar?s size is often
much larger than that of XTAG, typically more
than 2000 tree frames, potentially leading to a
larger sparse data problem, and also the result-
ing grammar is not hand-checked.
Two kinds of treebank are used in (Bangalore
et al, 2001). One kind is the Penn Treebank
(Marcus et al, 1993). It consists of approxi-
mately 1,000,000 words of hand-checked, brack-
eted text. The text consists of Wall Street Jour-
nal news articles. The other kind of treebank is
the BLLIP corpus (Charniak, 2000). It con-
sists of approximately 40,000,000 words of text
that has been parsed by a broad-coverage sta-
tistical parser. The text consists of Wall Street
Journal news and newswire articles. The ad-
vantage of the former is that it has been hand-
checked, whereas the latter has the advantage
of being easily produced and hence can easily
be enlarged.
(Bangalore et al, 2001) experimentally de-
termine how the quality and quantity of the
resources used in training FERGUS affect the
output quality of the generator. They find that
while a better quality annotated corpus (Penn
Treebank) results in better model accuracy than
a lower quality corpus (BLLIP) of the same size,
an (easily-obtained) larger lower quality corpus
results in a model that eclipses a smaller, better
quality treebank. Also, the model that is ob-
tained when using an automatically extracted
grammar yields comparable output quality to
the model that is obtained when using a hand-
crafted (XTAG) grammar.
3.3 Automating Adaptation of
FERGUS to a New Domain
This paper is about minimizing the amount of
manual labor that is required to port NLG com-
ponents to different domains. (Bangalore et
al., 2001) perform all of their experiments on
the same domain of Wall Street Journal news
articles. In contrast, in this section we show
that FERGUS can be adapted to the domain of
air-travel reservation dialogs with minimal hu-
man effort. We show that out-of-domain train-
ing data can be used instead of in-domain data
without drastically compromising output qual-
ity. We also show that partially parsed in-
domain training data can be effectively used
to train the TM. Finally, we show that using
an in-domain corpus to train the LM can help
the output quality, even if that corpus is of
small size. In this section, we first describe the
training resources that are used in these exper-
iments. We subsequently describe the experi-
ments themselves and their results.
Various corpora are used in these experi-
ments. For training, there are two distinct
corpora. First, there is the previously in-
troduced Penn Treebank (PTB). As the
alternative, there is a human-human corpus of
dialogs (HH) from Carnegie Mellon University.
The HH corpus consists of approximately
13,000 words in the air-travel reservation
domain. This is not exactly the target domain
because human-human interaction differs
from human-computer interaction which is
our true target domain. From this raw text,
an LDA parser (Bangalore and Joshi, 1999)
trained using the XTAG-based Penn Treebank
creates a partially-parsed, non-hand-checked
treebank. Test data consists of about 2,200
words derived from Communicator template
data. Communicator templates are hand-
crafted surface strings of words interspersed
with slot names. An example is ?What time
would you, traveling from $ORIG-CITY
to $DEST-CITY like to leave?? The test
data is derived from all strings like these, with
duplicates, in the Communicator system by
replacing the slot names with fillers according
to a probability distribution. Furthermore,
dependency parses are assigned to the resulting
strings by hand.
In the first series of experiments, we ascertain
the output quality of FERGUS using the XTAG
grammar on different training corpora. We vary
the TM?s training corpus to be either PTB or
HH. We do the same for the LM?s training cor-
pus. Assessing the output quality of a generator
is a complex issue. Here, we select as our met-
ric understandability accuracy, defined in (Ban-
galore et al, 2000) as quantifying the differ-
PTB TM HH TM
PTB LM 0.30 0.38
HH LM 0.37 0.41
Table 2: Average understandability accuracies
using XTAG-Based FERGUS for various kinds
of training data
PTB TM
PTB LM 0.39
HH LM 0.33
Table 3: Average understandability accuracies
using automatically-extracted grammar based
FERGUS for various kinds of training data
ence between the generator output, in terms of
both dependency tree and surface string, and
the desired reference output. (Bangalore et al,
2000) finds this metric to correlate well with hu-
man judgments of understandability and qual-
ity. Understandability accuracy varies between
a high score of 1.0 and a low score which may
be less than zero.
The results of our experiments are shown in
Table 2. We conclude that despite its smaller
size, and despite its being only automatically-
and partially- parsed, using the in-domain HH
is more effective than using the out-of-domain
PTB for training the TM. Similarly, HH is more
effective than PTB for training the LM. The
best result is obtained by using HH to train both
the TM and the LM; this result (0.41) is com-
parable to the result obtained by using matched
PTB training and test data (0.43) that is used
in (Bangalore et al, 2001).
The second series of experiments investi-
gates the output quality of FERGUS using
automatically-extracted grammars. In these ex-
periments, the TM is always trained on PTB
but not HH. It is the type of training data that
is used to train the LM, either PTB or HH,
that is varied. The results are shown in Ta-
ble 3. Note that these scores are in the same
range as those obtained when training FER-
GUS using XTAG. Also, these scores show that
when using automatically-extracted grammars,
training LM using a large, out-of-domain cor-
pus (PTB) is more beneficial than training LM
using a small, in-domain corpus (HH).
We can now draw various conclusions about
training FERGUS in a new domain. Con-
sider training the TM. It is not necessary
to use a handwritten TAG in the new do-
main; a broad-coverage hand-written TAG or
an automatically-extracted TAG will give com-
parable results. Also, instead of requiring a
hand-checked treebank in the new domain, par-
tially parsed data in the new domain is ade-
quate. Now consider training the LM. Our ex-
periments show that a small corpus in the new
domain is a viable alternative to a large corpus
that is out of the domain.
4 Integration of SPoT with
FERGUS
We have seen evidence that both SPoT and
FERGUS may be easily transferable to a new
domain. Because the output of a sentence plan-
ner usually becomes the input of a surface real-
izer, questions arise such as whether SPoT and
FERGUS can be made to work together in a
new domain and what is the output quality of
the combined system. We will see that an ad-
dition of a rule-based component to FERGUS
will be necessary in order for this integration
to occur. We will subsequently see that the
output quality of the resulting combination of
SPoT and FERGUS is quite good.
Integration of SPoT as described in Section 2
and FERGUS as described in Section 3 is not
automatic. The reason is that the output of
SPoT is a deep syntax tree (Mel?c?uk, 1998)
whereas hitherto the input of FERGUS has
been a surface syntax tree. The primary dis-
tinguishing characteristic of a deep syntax tree
is that it contains features for categories such as
definiteness for nouns, or tense and aspect for
verbs. In contrast, a surface syntax tree real-
izes these features as function words. However,
there is a one-to-one mapping from features of
a deep syntax tree to function words in the cor-
responding surface syntax tree. Therefore, inte-
grating SPoT with FERGUS is basically a mat-
ter of performing this mapping. We have added
a rule-based component (RB) as the new first
stage of FERGUS to do just that. Note that it
is erroneous to think that RB makes choices be-
tween different generation options because there
is a one-to-one mapping between features and
function words.
PTB TM HH TM
PTB LM 0.48 0.47
HH LM 0.73 0.68
Table 4: Average understandability accuracies
of SPoT-integrated, XTAG-Based FERGUS for
various kinds of training data
After the addition of RB to FERGUS, we
evaluate the output quality of the combination
of SPoT and FERGUS. Only the XTAG gram-
mar is used in this experiment. As in previous
experiments with the XTAG grammar, there is
either the option of training using HH or PTB
derived data for either the TM or LM, giving a
total of four possibilities.
Test data is obtained by output strings that
are produced by the combination of SPoT and
the RealPro surface realizer (Lavoie and Ram-
bow, 1998). RealPro has the advantage of pro-
ducing high quality surface strings, but at the
cost of having to be hand-tuned to a particu-
lar domain. It is this cost we are attempting to
minimize by using FERGUS. Only those sen-
tence plans produced by SPoT ranked 3.0 or
greater by human judges are used. The surface
realization of these sentence plans yields a test
corpus of 2,200 words.
As shown in Table 4, the performance of
SPoT and FERGUS combined is quite high.
Also note that in terms of training the LM, out-
put quality is markedly better when HH is used
rather than PTB. Furthermore, note that there
is a smaller difference between using PTB or
HH to train TM when compared to previous re-
sults shown in Table 2. This seems to indicate
that the TM?s effect on output quality dimin-
ishes because of addition of RB to FERGUS.
5 On-line Integration of FERGUS
with a Dialog System
Certain statistical natural language processing
systems can be quite slow, usually because of
the large search space that these systems must
explore. It is therefore uncertain whether a sta-
tistical NLG component can be integrated into a
real-time dialog system. Investigating the mat-
ter in FERGUS?s case, we have experimented
with integrating FERGUS into Communicator,
a mixed-initiative, airline travel reservation sys-
tem. We begin by explaining how Communica-
tor manages surface generation without FER-
GUS. We then delineate several possible kinds
of integration. Finally, we describe our experi-
ences with one kind of integration.
Communicator performs only a rudimentary
form of surface generation as follows. The
dialog manager of Communicator issues a set
of communicative goals that are to be realized.
Surface template strings are selected based
on this set, such as ?What time would you,
traveling from $ORIG-CITY to $DEST-CITY
like to leave?? The slot names in these
strings are then replaced with fillers according
to the dialog manager?s state. The resulting
strings are then piped to a text-to-speech
component (TTS) for output.
There are several possibilities as to how FER-
GUS may supplant this system. One possibility
is off-line integration. In this case, the set of all
possible sets communicative goals for which the
dialog manager requires realization are matched
with a set of corresponding surface syntax trees.
The latter set is input to FERGUS, which gen-
erates a set of surface template strings, which in
turn is used to replace the manually created sur-
face template strings that are an original part
of Communicator. Since these changes are pre-
compiled, the resulting version of Communica-
tor is therefore as fast the original. On the other
hand, off-line integration may be unmanageable
if the set of sets of communicative goals is very
large. In that case, only the alternative of on-
line integration is palatable. In this approach,
each surface template string in Communicator is
replaced with its corresponding surface syntax
tree. At points in a dialog where Communicator
requires surface generation, it sends the appro-
priate surface syntax trees to FERGUS, which
generates surface strings.
We have implemented the on-line integration
of FERGUS with Communicator. Our experi-
ments show that FERGUS is fast enough to be
used in for this purpose, the average time for
FERGUS to generate output strings for one di-
alog turn being only 0.28 seconds.
6 Conclusions and Future Work
We have performed experiments that provide
evidence that components of a statistical NLG
system may be ported to different domains
without a huge investment in manual labor.
These components include a sentence planner,
SPoT, and a surface realizer, FERGUS. SPoT
seems easily portable to different domains be-
cause it can be trained well using only domain-
independent features. FERGUS may also be
said to be easily portable because our experi-
ments show that the quality and quantity of in-
domain training data need not be high and plen-
tiful for decent results. Even if in-domain data
is not available, we show that out-of-domain
training data can be used with adequate results.
By integrating SPoT with FERGUS, we have
also shown that different statistical NLG com-
ponents can be made to work well together. In-
tegration was achieved by adding a rule-based
component to FERGUS which transforms deep
syntax trees into surface syntax trees. The com-
bination of SPoT and FERGUS performs with
high accuracy. Post-integration, there is a di-
minishing effect of TM on output quality.
Finally, we have shown that a statistical NLG
component can be integrated into a dialog sys-
tem in real time. In particular, we replace the
hand-crafted surface generation of Communica-
tor with FERGUS. We show that the resulting
system performs with low latency.
This work may be extended in different di-
rections. Our experiments showed promising re-
sults in porting to the domain of air travel reser-
vations. Although this is a reasonably-sized do-
main, it would be interesting to see how our
findings vary for broader domains. Our experi-
ments used a partially parsed version of the HH
corpus. We would like to compare its use as
TM training data in relation to using a fully
parsed version of HH, and also a hand-checked
treebank version of HH. We would also like to
investigate the possibility of interpolating mod-
els based on different kinds of training data in
order to ameliorate data sparseness. Our ex-
periments focused on integration between the
NLG components of sentence planning and sur-
face generation. We would like to explore the
possibility of further integration, in particular
integrating these components with TTS. This
would provide the benefit of enabling the use of
syntactic and semantic information for prosody
assignment. Also, although FERGUS was inte-
grated with SPoT relatively easily, it does not
necessarily follow that FERGUS can be inte-
grated easily with other kinds of components.
It may be worthwhile to envision a redesigned
version of FERGUS whose input can be flexibly
underspecified in order to accommodate differ-
ent kinds of modules.
7 Acknowledgments
This work was partially funded by DARPA un-
der contract MDA972-99-3-0003.
References
Srinivas Bangalore and A. K. Joshi. 1999. Su-
pertagging: An approach to almost parsing.
Computational Linguistics, 25(2).
Srinivas Bangalore and Owen Rambow. 2000.
Exploiting a probabilistic hierarchical model
for generation. In Proceedings of the 18th
International Conference on Computational
Linguistics (COLING 2000).
Srinivas Bangalore, Owen Rambow, and Steve
Whittaker. 2000. Evaluation metrics for gen-
eration. In Proceedings of the First Interna-
tional Conference on Natural Language Gen-
eration, Mitzpe Ramon, Israel.
Srinivas Bangalore, John Chen, and Owen
Rambow. 2001. Impact of quality and quan-
tity of corpora on stochastic generation. In
Proceedings of the 2001 Conference on Em-
pirical Methods in Natural Langauge Process-
ing, Pittsburgh, PA.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of First An-
nual Meeting of the North American Chap-
ter of the Association for Computational Lin-
guistics, Seattle, WA.
John Chen. 2001. Towards Efficient Statis-
tical Parsing Using Lexicalized Grammati-
cal Information. Ph.D. thesis, University of
Delaware.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining gram-
mar. In Proceedings of the the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 456?463, Hong Kong.
Yoav Freund, Raj Iyer, Robert E. Schapire, and
Yoram Singer. 1998. An efficient boosting
algorithm for combining preferences. In Ma-
chine Learning: Proceedings of the Fifteenth
International Conferece.
Kevin Knight and V. Hatzivassiloglou. 1995.
Two-level many-paths generation. In Pro-
ceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics,
Boston, MA.
Irene Langkilde and Kevin Knight. 1998. Gen-
eration that exploits corpus-based statisti-
cal knowledge. In Proceedings of the 17th
International Conference on Computational
Linguistics and the 36th Annual Meeting of
the Association for Computational Linguis-
tics, Montreal, Canada.
Benoit Lavoie and Owen Rambow. 1998. A
framework for customizable generation of
multi-modal presentations. In Proceedings of
the 17th International Conference on Com-
putational Linguistics and the 36th Annual
Meeting of the Association for Computational
Linguistics, Montreal, Canada.
Mitchell Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of english: the
penn treebank. Computational Linguistics,
19(2):313?330.
Igor A. Mel?c?uk. 1998. Dependency Syntax:
Theory and Practice. State University of New
York Press, New York, NY.
Alice H. Oh and Alexander I. Rudnicky.
2000. Stochastic language generation for spo-
ken dialog systems. In Proceedings of the
ANLP/NAACL 2000 Workshop on Conver-
sational Systems, pages 27?32, Seattle, WA.
Kiyotaka Uchimoto, Masaki Murata, Qing Ma,
Satoshi Sekine, and Hitoshi Isahara. 2000.
Word order acquisition from corpora. In Pro-
ceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
?00), Saarbru?cken, Germany.
Marilyn A. Walker, Owen Rambow, and Mon-
ica Rogati. 2001. Spot: A trainable sentence
planner. In Proceedings of the Second Meeting
of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages
17?24.
Fei Xia. 1999. Extracting tree adjoining gram-
mars from bracketed corpora. In Fifth Natu-
ral Language Processing Pacific Rim Sympo-
sium (NLPRS-99), Beijing, China.
The XTAG-Group. 2001. A Lexicalized
Tree Adjoining Grammar for English.
Technical report, University of Penn-
sylvania. Updated version available at
http://www.cis.upenn.edu/?xtag.
Creating a Finite-State Parser with Application Semantics
Owen Rambow
University of Pennsylvania
Philadelphia, PA 19104
USA
Srinivas Bangalore
AT&T Labs ? Research
Florham Park, NJ 07932
USA
Tahir Butt
Johns Hopkins University
Baltimore, MD 21218
USA
Alexis Nasr
Universite? Paris 7
75005 Paris
France
Richard Sproat
AT&T Labs ? Research
Florham Park, NJ 07932
USA
rambow@unagi.cis.upenn.edu
Abstract
Parsli is a finite-state (FS) parser which can be
tailored to the lexicon, syntax, and semantics
of a particular application using a hand-editable
declarative lexicon. The lexicon is defined in
terms of a lexicalized Tree Adjoining Grammar,
which is subsequently mapped to a FS represen-
tation. This approach gives the application de-
signer better and easier control over the natural
language understanding component than using
an off-the-shelf parser. We present results using
Parsli on an application that creates 3D-images
from typed input.
1 Parsing and Application-Specific
Semantics
One type of Natural Language Understanding
(NLU) application is exemplified by the database
access problem: the user may type in free source
language text, but the NLU component must
map this text to a fixed set of actions dictated
by the underlying application program. We
will call such NLU applications ?application-
semantic NLU?. Other examples of application-
semantic NLU include interfaces to command-
based applications (such as airline reservation
systems), often in the guise of dialog systems.
Several general-purpose off-the-shelf (OTS)
parsers have become widely available (Lin,
1994; Collins, 1997). For application-semantic
NLU, it is possible to use such an OTS parser in
conjunction with a post-processor which trans-
fers the output of the parser (be it phrase struc-
ture or dependency) to the domain semantics. In
addition to mapping the parser output to appli-
cation semantics, the post-processor often must
also ?correct? the output of the parser: the parser
may be tailored for a particular domain (such as
Wall Street Journal (WSJ) text), but the new do-
main presents linguistic constructions not found
in the original domain (such as questions). It
may also be the case that the OTS parser consis-
tently misanalyzes certain lexemes because they
do not occur in the OTS corpus, or occur there
with different syntactic properties. While many
of the parsers can be retrained, often an anno-
tated corpus is not available in the application
domain (since, for example, the application it-
self is still under development and there is not
yet a user community). The process of retraining
may also be quite complex in practice. A further
disadvantage of this approach is that the post-
processor must typically be written by hand, as
procedural code. In addition, the application-
semantic NLU may not even exploit the strengths
of the OTS parser, because the NLU required
for the application is not only different (ques-
tions), but generally simpler (the WSJ contains
very long and syntactically complex sentences
which are not likely to be found as input in in-
teractive systems, including dialog systems).
This discussion suggests that we (i) need an
easy way to specify application semantics for a
parser and (ii) that we do not usually need the full
power of a full recursive parser. In this paper, we
suggest that application-semantic NLP may be
better served by a lexicalized finite-state (FS)
parser. We present PARSLI, a FS parser which
can be tailored to the application semantics us-
ing a hand-editable declarative lexicon. This ap-
proach gives the application designer better and
easier control over the NLU component. Further-
more, while the finite-state approach may not be
sufficient for WSJ text (given its syntactic com-
plexity), it is sufficient for most interactive sys-
tems, and the advantage in speed offered by FS
approaches in more crucial in interactive appli-
cations. Finally, in speech-based systems, the
lattice that is output from the speech recognition
component can easily used as input to a FS-based
parser.
2 Sample Application: WORDSEYE
WORDSEYE (Coyne and Sproat, 2001) is a
system for converting English text into three-
dimensional graphical scenes that represent that
text. WORDSEYE performs syntactic and se-
mantic analysis on the input text, producing a
description of the arrangement of objects in a
scene. An image is then generated from this
scene description. At the core of WORDSEYE
is the notion of a ?pose?, which can be loosely
defined as a figure (e.g. a human figure) in a con-
figuration suggestive of a particular action.
For WORDSEYE, the NLP task is thus to
map from an input sentence to a representation
that the graphics engine can directly interpret in
terms of poses. The graphical component can
render a fixed set of situations (as determined by
its designer); each situation has several actors in
situation-specific poses, and each situation can
be described linguistically using a given set of
verbs. For example, the graphical component
may have a way of depicting a commercial trans-
action, with two humans in particular poses (the
buyer and the seller), the goods being purchased,
and the payment amount. In English, we have
different verbs that can be used to describe this
situation (buy, sell, cost, and so on). These verbs
have different mappings of their syntactic argu-
ments to the components in the graphical repre-
sentation. We assume a mapping from syntax to
domain semantics, leaving to lexical semantics
the question of how such a mapping is devised
and derived. (For many applications, such map-
pings can be derived by hand, with the seman-
tic representation an ad-hoc notation.) We show
a sample of such mapping in Figure 1. Here,
we assume that the graphics engine of WORD-
SEYE knows how to depict a TRANSACTION
when some of the semantic arguments of a trans-
action (such as CUSTOMER, ITEM, AMOUNT)
are specified.
We show some sample transductions in Fig-
ure 2. In the output, syntactic constituents are
bracketed. Following each argument is informa-
tion about its grammatical function (?GF=0? for
example) and about its semantic role (ITEM for
example). If a lexical item has a semantics of
its own, the semantics replaces the lexical item
(this is the case for verbs), otherwise the lexical
item remains in place. In the case of the transi-
tive cost, the verbal semantics in Figure 1 spec-
ifies an implicit CUSTOMER argument. This is
generated when cost is used transitively, as can
be seen in Figure 2.
3 Mapping Tree Adjoining Grammar
to Finite State Machines
What is crucial for being able to define a map-
ping from words to application semantics is a
very abstract notion of grammatical function: in
devising such a mapping, we are not interested
in how English realizes certain syntactic argu-
ments, i.e., in the phrase structure of the verbal
projection. Instead, we just want to be able to re-
fer to syntactic functions, such as subject or indi-
rect object. Tree Adjoining Grammar (TAG) rep-
resents the entire syntactic projection from a lex-
eme in its elementary structures in an elementary
tree; because of this, each elementary tree can
be associated with a lexical item (lexicalization,
(Joshi and Schabes, 1991)). Each lexical item
can be associated with one or more trees which
represent the lexeme?s valency; these trees are
referred to as its supertags. In a derivation, sub-
stituting or adjoining the tree of one lexeme into
that of another creates a direct dependency be-
tween them. The syntactic functions are labeled
with integers starting with zero (to avoid discus-
sions about names), and are retained across op-
erations such as topicalization, dative shift and
passivization.
A TAG consists of a set of elementary trees of
two types, initial trees and auxiliary trees. These
trees are then combined using two operations,
substitution and adjunction. In substitution, an
initial tree is appended to a specially marked
node with the same label as the initial tree?s root
node. In adjunction, a non-substitution node is
rewritten by an auxiliary tree, which has a spe-
cially marked frontier node called the footnode.
The effect is to insert the auxiliary tree into the
middle of the other tree.
We distinguish two types of auxiliary trees.
Adjunct auxiliary trees are used for adjuncts;
they have the property that the footnode is al-
Verb Supertag Verb semantics Argument semantics
paid A nx0Vnx1 transaction 0=Customer 1=Amount
cost A nx0Vnx1 transaction 0=Item 1=Amount Implicit=Customer
cost A nx0Vnx2nx1 transaction 0=Item 1=Amount 2=Customer
bought, purchased A nx0Vnx1 transaction 0=Customer 1=Item
socks A NXN none none
Figure 1: Sample entries for a commercial transaction situation
In: I bought socks
Out: ( ( I ) GF=0 AS=CUSTOMER TRANSACTION ( socks ) GF=1 AS=ITEM )
In:the pajamas cost my mother-in-law 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION ( ( my ) mother-in-law ) GF=2 AS=CUSTOMER ( (
12 ) dollars ) GF=1 AS=AMOUNT )
In: the pajamas cost 12 dollars
Out: ( ( ( the ) pajamas ) GF=0 AS=ITEM TRANSACTION IMP:CUSTOMER ( ( 12 ) dollars ) GF=1
AS=AMOUNT )
Figure 2: Sample transductions generated by Parsli (?GF? for grammatical function, ?AS? for argu-
ment semantics, ?Imp? for implicit argument)
ways a daughter node of the root node, and the
label on these nodes is not, linguistically speak-
ing, part of the projection of the lexical item of
that tree. For example, an adjective will project
to AdjP, but the root- and footnode of its tree will
be labeled NP, since an adjective adjoins to NP.
We will refer to the root- and footnode of an ad-
junct auxiliary tree as its passive valency struc-
ture. Note that the tree for an adjective also spec-
ifies whether it adjoins from the left (footnode
on right) or right (footnode on left). Predicative
auxiliary trees are projected from verbs which
subcategorize for clauses. Since a verb projects
to a clausal category, and has a node labeled with
a clausal category on its frontier (for the argu-
ment), the resulting tree can be interpreted as an
auxiliary tree, which is useful in analyzing long-
distance wh-movement (Frank, 2001).
To derive a finite-state transducer (FST) from
a TAG, we do a depth-first traversal of each ele-
mentary tree (but excluding the passive valency
structure, if present) to obtain a sequence of non-
terminal nodes. For predicative auxiliary trees,
we stop at the footnode. Each node becomes two
states of the FST, one state representing the node
on the downward traversal on the left side, the
other representing the state on the upward traver-
sal, on the right side. For leaf nodes, the two
states are juxtaposed. The states are linearly con-
nected with   -transitions, with the left node state
of the root node the start state, and its right node
state the final state (except for predicative auxil-
iary trees ? see above). To each non-leaf state,
we add one self loop transition for each tree in
the grammar that can adjoin at that state from
the specified direction (i.e., for a state represent-
ing a node on the downward traversal, the auxil-
iary tree must adjoin from the left), labeled with
the tree name. For each pair of adjacent states
representing a substitution node, we add transi-
tions between them labeled with the names of
the trees that can substitute there. We output the
number of the grammatical function, and the ar-
gument semantics, if any is specified. For the
lexical head, we transition on the head, and out-
put the semantics if defined, or simply the lex-
eme otherwise. There are no other types of leaf
nodes since we do not traverse the passive va-
lency structure of adjunct auxiliary tees. At the
beginning of each FST, an   -transition outputs an
open-bracket, and at the end, an   -transition out-
puts a close-bracket. The result of this phase of
the conversion is a set of FSTs, one per elemen-
tary tree of the grammar. We will refer to them
as ?elementary FSTs?.
0 1<epsilon>:( 2A_NXG:GF=0
A_NXN:GF=0
3<epsilon>:FE=Customer 4ordered:transaction 5A_NXG:GF=1
A_NXN:GF=1
6<epsilon>:FE=Item 7<epsilon>:)
Figure 4: FST corresponding to TAG tree in Figure 3
S
NP

Arg0
VP
V
ordered
NP

Arg1
Figure 3: TAG tree for word ordered; the dow-
narrow indicates a substitution node for the nom-
inal argument
4 Constructing the Parser
In our approach, each elementary FST describes
the syntactic potential of a set of (syntactically
similar) words (as explained in Section 3). There
are several ways of associating words with FSTs.
Since FSTs correspond directly to supertags (i.e.,
trees in a TAG grammar), the basic way to
achieve such a mapping is to list words paired
with supertags, along with the desired seman-
tic associated with each argument position (see
Figure 1). The parser can also be divided into
a lexical machine which transduces words to
classes, and a syntactic machine, which trans-
duces classes to semantics. This approach has
the advantage of reducing the size of the over-
all machine since the syntax is factored from the
lexicon.
The lexical machine transduces input words to
classes. To determine the mapping from word to
supertag, we use the lexical probability  	

where 	 is the word and  the class. These
are derived by maximum likelihood estimation
from a corpus. Once we have determined for all
words which classes we want to pair them with,
we create a disjunctive FST for all words associ-
ated with a given supertag machine, which trans-
duces the words to the class name. We replaces
the class?s FST (as determined by its associated
supertag(s)) with the disjunctive head FST. The
weights on the lexical transitions are the nega-
tive logarithm of the emit probability 	 
 (ob-
tained in the same manner as are the lexical prob-
abilities).
For the syntactic machine, we take each ele-
mentary tree machine which corresponds to an
initial tree (i.e., a tree which need not be ad-
joined) and form their union. We then perform
a series of iterative replacements; in each iter-
ation, we replace each arc labeled by the name
of an elementary tree machine by the lexicalized
version of that tree machine. Of course, in each
iteration, there are many more replacements than
in the previous iteration. We use 5 rounds of iter-
ation; obviously, the number of iterations restrict
the syntactic complexity (but not the length) of
recognized input. However, because we output
brackets in the FSTs, we obtain a parse with
full syntactic/lexical semantic (i.e., dependency)
structure, not a ?shallow parse?.
This construction is in many ways similar to
similar constructions proposed for CFGs, in par-
ticular that of (Nederhof, 2000). One difference
is that, since we start from TAG, recursion is al-
ready factored, and we need not find cycles in the
rules of the grammar.
5 Experimental Results
We present results in which our classes are de-
fined entirely with respect to syntactic behav-
ior. This is because we do not have available
an important corpus annotated with semantics.
We train on the Wall Street Journal (WSJ) cor-
pus. We evaluate by taking a list of 205 sen-
tences which are chosen at random from entries
to WORDSEYE made by the developers (who
were testing the graphical component using a dif-
ferent parser). Their average length is 6.3 words.
We annotated the sentences by hand for the de-
sired dependency structure, and then compared
the structural output of PARSLI to the gold stan-
dard (we disregarded the functional and seman-
tic annotations produced by PARSLI). We eval-
uate performance using accuracy, the ration of
n Correctness Accuracy Nb
2 1.00 1.00 12
4 0.83 0.84 30
6 0.70 0.82 121
8 0.62 0.80 178
12 0.59 0.79 202
16 0.58 0.79 204
20 0.58 0.78 205
Figure 5: Results for sentences with  or fewer
words; Nb refers to the number of sentences in
this category
n Correctness Accuracy
1 0.58 0.78
2 0.60 0.79
4 0.62 0.81
8 0.69 0.85
12 0.68 0.86
20 0.70 0.87
30 0.73 0.89
Figure 6: Results for  -best analyses
the number of dependency arcs which are cor-
rectly found (same head and daughter nodes) in
the best parse for each sentence to the number
of arcs in the entire test corpus. We also report
the percentage of sentences for which we find the
correct dependency tree (correctness). For our
test corpus, we obtain an accuracy of 0.78 and
a correctness of 0.58. The average transduction
time per sentence (including initialization of the
parser) is 0.29 s. Figure 5 shows the dependence
of the scores on sentence length. As expected,
the longer the sentence, the worse the score.
We can obtain the n-best paths through the
FST; the scores for n-best paths are summarized
in Figure 6. Since the scores keep increasing, we
believe that we can further improve our 1-best
results by better choosing the correct path. We
intend to adapt the FSTs to use probabilities of
attaching particular supertags to other supertags
(rather than uniform weights for all attachments)
in order to better model the probability of differ-
ent analyses. Another option, of course, is bilex-
ical probabilities.
6 Discussion and Outlook
We have presented PARSLI, a system that takes
a high-level specification of domain lexical se-
mantics and generates a finite-state parser that
transduces input to the specified semantics.
PARSLI uses Tree Adjoining Grammar as an in-
terface between syntax and lexical semantics.
Initial evaluation results are encouraging, and we
expect to greatly improve on current 1-best re-
sults by using probabilities of syntactic combi-
nation. While we have argued that many appli-
cations do not need a fully recursive parser, the
same approach to using TAG as an intermediate
between application semantics and syntax can be
used in a chart parser; for a chart parser using the
FS machines discussed in this paper, see (Nasr et
al., 2002).
References
Michael Collins. 1997. Three generative, lex-
icalised models for statistical parsing. In
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics, Madrid, Spain, July.
Bob Coyne and Richard Sproat. 2001. Word-
sEye: An automatic text-to-scene conversion
system. In SIGGRAPH 2001, Los Angeles,
CA.
Robert Frank. 2001. Phrase Structure Composi-
tion and Syntactic Dependencies. MIT Press,
Cambridge, Mass.
Aravind K. Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized gram-
mars. In Maurice Nivat and Andreas Podel-
ski, editors, Definability and Recognizability
of Sets of Trees. Elsevier.
Dekang Lin. 1994. PRINCIPAR?an efficient,
broad-coverage, principle-based parser. In
coling94, pages 482?488, Kyoto, Japan.
Alexis Nasr, Owen Rambow, John Chen, and
Srinivas Bangalore. 2002. Context-free pars-
ing of a tree adjoining grammar using finite-
state machines. In Proceedings of the Sixth
International Workshop on tree Adjoining
Grammar and related Formalisms (TAG+6),
Venice, Italy.
Mark-Jan Nederhof. 2000. Practical experi-
ments with regular approximation of context-
free languages. Computational Linguistics,
26(1):17?44.
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 264?273,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
HotSpots: Visualizing Edits to a Text
Srinivas Bangalore
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
David Smith
AT&T Labs ? Research
180 Park Ave
Florham Park, NJ 07932
dsmith@research.att.com
Abstract
Compared to the telephone, email based cus-
tomer care is increasingly becoming the pre-
ferred channel of communication for corpora-
tions and customers. Most email-based cus-
tomer care management systems provide a
method to include template texts in order to re-
duce the handling time for a customer?s email.
The text in a template is suitably modified
into a response by a customer care agent. In
this paper, we present two techniques to im-
prove the effectiveness of a template by pro-
viding tools for the template authors. First,
we present a tool to track and visualize the ed-
its made by agents to a template which serves
as a vital feedback to the template authors.
Second, we present a novel method that au-
tomatically extracts potential templates from
responses authored by agents. These meth-
ods are investigated in the context of an email
customer care analysis tool that handles over a
million emails a year.
1 Introduction
Email based customer care is increasingly becom-
ing the preferred channel of communication for cor-
porations and customers compared to the conven-
tional telephone-based customer care. For cus-
tomers, email channel offers several advantages ?
there are no tedious menus to navigate, there is no
waiting time to reach an operator, the request can
be formulated at the customer?s pace and additional
material supporting the case can be attached to the
email. There is also a record of the service re-
quest for the customer unlike the telephone-based
customer care. However, there are also limitations
of the email channel. The most significant one is
that the customer-agent interaction could be drawn
out over successive emails spanning over several
days as opposed to being resolved in one or two
telephone calls. For corporations, the asynchronous
nature of email-based customer care offers signifi-
cant opportunities to reduce operations cost by ef-
fective load balancing compared to telephone-based
customer care. It is quite common for an email cus-
tomer care agent to work on several cases simulta-
neously over a period of a few hours. Email chan-
nel also offers higher bandwidth for corporations to
send additional information in the form of web links,
images and video or audio instructions.
The effectiveness of customer care in the email
channel is measured using two competing metrics:
Average Handling Time (AHT) and Customer Ex-
perience Evaluation (CEE). AHT measures the time
taken from when a customer email is opened to the
time when the response is sent out. This time is typ-
ically averaged over a period of a week or a month
for reporting purposes. CEE measures customer sat-
isfaction through a survey of a random subset of cus-
tomers who have interacted with the email customer
care center. These surveys typically involve qual-
itative and quantitative questions and measure the
quality of the interactions along a number of differ-
ent dimensions. As is the case in many surveys the
population responding to such questionnaires is typ-
ically small and very often quite biased. We do not
use the CEE metric for the work we report in this
paper.
As is evident from the definitions of AHT and
CEE, it is in the interest of a corporation to minimize
AHT while maximizing CEE. In order to reduce
AHT, most email customer care systems (Kana,
2008; Genesys, 2008) provide a mechanism for an
agent to respond to a customer?s email by selecting
a predefined template text that can be quickly cus-
tomized to serve as the response. The template text
is usually associated with a problem category it is in-
tended to address and might even be suggested to the
agent automatically using classification techniques
264
applied to the customer?s email. Once the template
is selected, the agent edits the template text to per-
sonalize as well as add case specific details as part
of composing a response. Each of the text edits con-
tributes to the handling time of the email. Hence,
it is in the interest of the template designer to mini-
mize the number of edits of the template in order to
lower AHT.
Although most email management systems pro-
vide a mechanism to author the template text, there
is typically no mechanism to monitor and track how
these templates are modified by the agents when
they compose a response. This information is vital
to the template authors when creating new versions
of the templates that reduce the number of edits and
consequently reduce AHT.
In this paper, we present two methods for improv-
ing the templates in a principled manner. After de-
scribing the related work in Section 2, we present a
brief description of the email tracking tool we have
developed in Section 3. In Section 4, we present
a tool called HotSpots that helps visualize the edits
being made by the customer care agents to the tem-
plates. This tool provides a visual feedback to the
template authors and suggests means of improving
the template text based on the edits made by agents.
In Section 5, we present a new approach to automat-
ically identify emerging templates ? texts that are
repeatedly created by agents and are similar to each
other but distinct from the current template text. We
use AHT as the metric to minimize for automatic
identification of emerging templates. We discuss
some of the issues concerning this work in Section 6
and conclude in Section 7.
2 Related Work
There are few threads of research that are relevant to
the work presented in this paper. First, the topic of
email response generation in the context of customer
care has been investigated by (Coch, 1996; Lapalme
and Kosseim, 2003; Zukerman and Marom, 2007).
In (Coch, 1996), the authors model multi-sentence
generation of response letters to customer com-
plaints in French. The generation model is carefully
crafted for the domain using domain-specific rules
for conceptual planning, rhetorical relations and sur-
face word order operators. They show that their
approach performs better than predefined templates
and slightly worse than human generated responses.
In (Lapalme and Kosseim, 2003), the authors ex-
plore three different approaches based on classifica-
tion, case-based reasoning and question-answering
to compose responses to queries in an email cus-
tomer care application for the telecommunication in-
dustry. The case-based reasoning approach is the
most similar to the template approach we follow. In
(Zukerman and Marom, 2007), the authors investi-
gate an approach to assembling a response by first
predicting the clusters of sentences to be included in
the response text and then applying multi-document
summarization techniques to collate the representa-
tive sentences into a single response. In contrast, in
this paper, due to constraints from the deployment
environment, we rely on a template-based approach
to response generation. We focus on providing tools
for investigating how the templates are modified and
suggest techniques for evolving more effective tem-
plates based on quantitative criteria.
Another thread of relevant research are methods
for visualizing texts. There are several methods that
have been proposed to provide a visual map of a set
of text documents with the focus of illustrating the
relatedness of these texts (Card et al, 1999). Us-
ing a metric for comparing texts (e.g. n-gram over-
lap) , the texts are clustered and the resulting clus-
ters are visualized as two or three dimensional color
maps. These approaches are useful to depict similar-
ities in a static repository of documents or the return
results of a search query. These maps are primar-
ily designed for exploration and navigation through
the document space. While the underlying algorithm
we use to illustrate the text edits is similar to the one
used in text map visualizations, our focus in this pa-
per is to provide a mechanism for template designers
to quickly identify the variants of a template sen-
tence created by the agents.
A third thread is in the context of human-
assisted machine translation, where a human trans-
lator post-edits the output of a machine translation
system (Foster et al, 1997; Foster et al, 2002; Och
et al, 2003). In order to improve the efficiency of
a human translator, the k-best output of a translation
system could be displayed as word or phrase choices
which are color coded based on the confidence value
assigned by the translation model. While the ap-
265
proach we follow is partly motivated by the post-
editing paradigm, there are significant differences in
the context we apply this approach. In the context
of this paper, the template designer is presented a
summary of the set of variants created by each agent
for each sentence of the template. The task of the
template designer is to use this tool to select (or con-
struct) a new variant for the template sentence with
the aim of minimizing the need for editing that sen-
tence in future uses of the template.
3 Email Customer Care
Typically, a large email customer care management
center receives over 100,000 emails a month. These
centers typically use a customer care management
system that offer not only logging and tracking of
emails but also tools for improving the efficiency
of agents responding to emails. Usually, an incom-
ing customer email is categorized into a set of few
topics/issues. The categorization might be done au-
tomatically based on regular expressions involving
keywords in the email or using weighted classifiers
that are trained on data. In order for an agent to re-
spond to an incoming email, these systems provide a
text box which allows the agent to author a response
from scratch. However, most email customer care
systems offer the ability to store a prefabricated re-
sponse (also called templates), instead of agents hav-
ing to author a response from scratch. These tem-
plates are typically associated with a problem cate-
gory or an issue that they are intended to address.
A template helps an agent compose a well-formed
response quickly. It contains hints for information
that the agent should enter as well as indications of
where that information should be entered in the tem-
plate. The template might also contain helpful infor-
mation to the customer in addition to legal verbiage
that the customer needs to be aware of.
An agent receives a customer email and after
comprehending the issues and consulting the cus-
tomer records in the database, selects one of the pre-
defined set of templates that best addresses the is-
sues raised in the email. Less frequently, she might
even select more than one template to compose the
response. She then proceeds to edit and personal-
ize the chosen templates to better suit the customer?s
email. An example of a ?generic? template ? not as-
sociated with a specific problem category is shown
in Figure 1.
Greetings Contact.FirstName,
Thank you for your email in regard to
XXXXXXXX.
I will be happy to assist you with your in-
quiry.
XXX BODY XXX
If I can be of any further assistance,
please reply directly to this email.
Thank you for using our company.
We appreciate your business and contin-
ued loyalty.
Regards,
Agent.FirstName
Figure 1: An example of a generic template
The process of selecting an appropriate template
that addresses the customer?s inquiries could be
quite tedious when there are hundreds of templates.
Email management systems offer tools that suggest
appropriate template to use based on the content of
the customer?s email. These tools are trained using
classification techniques on previous email interac-
tions.
As mentioned earlier, there are two metrics that
are typically used to measure the effectiveness and
efficiency of email responses. Customers are sur-
veyed after their email interaction to assess their
level of satisfaction for the service they received.
This is usually called the Customer Experience
Evaluation (CEE) and includes an evaluation of the
customer?s total interaction experience with the cor-
poration, not just the last email interaction. A small
subset of customers who had an interaction with the
email center is randomly chosen (typically in the or-
der of about 10% of customers) and are invited to
take part in the follow-up survey. Typically, only
a small percent (about 10%) of the customers who
receive these invitations respond to the survey; ef-
fectively about 1% of the total emails have customer
survey scores.
A second metric that is also used to measure the
efficiency of an operation is called the average han-
dling time (AHT) which measures the average of
266
times taken by agents to respond to emails. The
handling time includes the time to comprehend the
email, the time for database lookup and the time for
response composition. It is in the interest of the
email customer care operation to minimize AHT and
maximize CEE scores.
3.1 Email Customer Care Analysis Tool
We have designed and developed an Email Customer
Care Analysis Tool (ECAT) to help analyze the op-
erations of the email care center. It provides an end-
to-end view from the activities involved in answer-
ing emails to the results of subsequent customer care
surveys. In addition, ECAT also provides insights
into how the agents are editing templates as well as
guides template authors in designing more effective
templates.
ECAT is a web-based tool and offers a birds-eye
summary of the operations aggregated by region,
the template used, and the customer satisfaction sur-
vey results. Using this tool, analysts can drill down
through a series of views until they are eventually
presented with the results of a single survey or a sin-
gle email interaction.
One of the most useful functions of the tool is that
it shows the extent to which agents edit the tem-
plates in the process of creating responses to cus-
tomer emails. The degree to which a template is
edited is based on Levenshtein string edit distance
metric (Levenshtein, 1966). This metric measures
the number of edits (substitution, deletion and inser-
tions) of words that are needed to transform a tem-
plate into a response. The number of edits is normal-
ized by the number of words in the template. These
morphing scores can be viewed for a single email or
averaged per agent or per template used. The scores
range from 100 to 0, with 100 representing a tem-
plate which hadn?t been edited at all.
The tool also allows the morphing score to be
viewed alongside the handling time for an email, in
other words the amount of time that the agent spends
gathering data and actually composing a response.
Handling time is an important metric since it is di-
rectly related to cost of operating the email customer
care center. The more editing an agent does, the
more time they take to respond to a customer. So,
the number of templates, their precise wording and
the ease with which agents can distinguish them ob-
viously have significant influences on overall han-
dling time.
Beyond the confines of the email centers them-
selves, the CEE is the most important elements in
gauging the effectiveness of the agent. The survey
asks customers to rate their overall satisfaction with
the email reply to their question. Five is the high-
est score which equates with ?Extremely Satisfied?
while a one equals ?Extremely Dissatisfied.? Cus-
tomers are also asked to rank the email in terms of
it?s content, clarity, professionalism and the length
of time it took to receive a reply. The customer is
also allowed to enter some free text so that they can
say how satisfied they were, or not, with how an in-
quiry or problem was dealt with. Customers can also
say whether they called the company using a tele-
phone channel before turning to the email channel.
The survey files, all of which can be accessed in
their entirety from within the ECAT tool, also con-
tain information on what templates were used when
replying to the customer. They also tell the analyst
who the replying agent was and whether this was
the first or a subsequent email in communications
between the customer and the company.
The ECAT tool juxtaposes this CEE score with
the template morphing score to show correlations
between customer satisfaction and the degree to
which the template had been edited. This data is
graphed so that the analyst can immediately see if
heavy editing of a template is leading to higher CEE.
Heavy editing with a low customer rating could
mean that the template is not helping the agent to
respond correctly to the customer.
4 HotSpots
We designed the HotSpots tool that provides in-
sights to the template authors on how templates are
being edited by the agents when creating responses.
It suggests methods for improving the next version
of the template so as to reduce edits by agents and
hence reduce the handling time for an email. In this
section, we discuss the algorithm and the visualiza-
tion of the information that aids template authors in
improving the efficacy of the templates.
The HotSpots algorithm proceeds in two steps as
shown in Algorithm 1. The first step creates an
alignment between the template string and the re-
267
Algorithm 1 Compute HotSpots for a template T
given a response set R
1: EdEv = ?
2: T = s1s2 . . . sn
3: Ts = {si|1 ? i ? n}
4: Rs = {r
j
i |Rj ? R, Rj = r
j
1r
j
2 . . . r
j
mj , 1 ? i ?
mj}
5: Index : {Ts ?Rs} ? I
6: Tin = Index(s1)Index(s2) . . . Index(sn)
7: for all R ? R do
8: R = r1r2 . . . rnR
9: Rin = Index(r1)Index(r2) . . . Index(rnR)
// compute distance with sentences as tokens
and return the alignment and score
10: (alignment, score) = IndDist(Tin, Rin)
// for each of the sentences in T, update its
map
11: for all si ? T do
12: in = Index(si)
13: if (si, ) ? alignment then
14: EdEv[in].map = EdEv[in].map ?
{?delete?}
15: else // (si, rj) ? alignment
16: EdEv[in].map = EdEv[in].map ?
{rj}
17: end if
18: end for
19: end for
// Cluster the response sentences aligned for
each template sentence
20: for all si ? T do
21: in = Index(si)
22: Cl = KmedianCl(EdEv[in].map, ncl)
23: end for
Algorithm 2KmedianCl: Compute k centroids for a
set of strings S using k-median clustering algorithm
1: cs = ? // centroid of string s?s cluster
2: cei = ? // centroid of cluster i
3: numcl = 0 // number of cluster created so far
4: Cli = ? // members of cluster i
5: while (numcl ? k) ? (numcl ? |S|) do
6: if numcl = 0 then
7: ce0 = argmin
c?S
?
s?S
Dist(c, s)
8: else // select the string (s) that is farthest from its
centroid (cs)
9: cenumcl = argmax
s?S
Dist(cs, s)
10: end if
// Move strings to the closest cluster and compute
centroids until the set of centroids don?t change
11: repeat
12: for all s ? S do
13: i? = argmin
0?i?numcl
Dist(cei, s)
14: cs = cei?
15: Cli? = Cli? ? {s}
// Computed the closest cluster centroid cei
to s.
16: end for
// Recompute the cluster centroids cei
17: for all i such that 0 ? i ? numcl do
18: cei = argmin
c ? Cli
?
s?Cli
Dist(c, s)
19: end for
20: until set of centroids does not change
21: numcl = numcl + 1 // new cluster added
22: end while
268
sponse string. For the purposes of this paper, we
consider the alignments between the template text
and the response text with sentences as tokens in-
stead of a word-based alignment. The rationale
for this tokenization is that for template develop-
ers the visualization of the edits is expected to be
more meaningful when aggregated at the sentence
level rather than at the word level. In the second
step, using the sentence-level alignment we compute
the edit events (insertion, deletion and substitution)
of the template sentences in order to create the re-
sponse. All the edits events associated with a tem-
plate sentence are then clustered into k clusters and
the centroids of the k clusters are displayed as the
potential changes to that sentence. We next describe
these two steps in detail as illustrated in Algorithm
1 and Algorithm 2.
Given a set of responses R that agents create us-
ing a template T , Algorithm 1 proceeds as follows.
Each of the sentences in the template and the set of
responses are mapped into an integer index (Line
1). The template T and each of the responses in
R are split into sentences and mapped into index
sequences (Line 6 and Line 9). The alignment be-
tween the two index strings is computed in Line
10. This is a dynamic programming algorithm sim-
ilar to computing Levenshtein distance between two
strings, except the cost function used to compute the
match between tokens is as shown below.
From the alignment that maps si to rj , we collect
the set of response sentences associated with each
template sentence (Line 13-16). These sentences are
then clustered using k-median clustering method (il-
lustrated in Algorithm 2) in Line 22.
In Algorithm 2, we illustrate the method of clus-
tering we use to summarize the set of sentences we
have collected for each template sentence after the
alignment step. The algorithm is similar to the k-
means algorithm (Duda et al, 2001), however, given
that we are clustering strings instead of real num-
bers (as is typical in applications of k-means), we re-
strict the centroid of a cluster to be one of the mem-
bers of the set being clustered, hence the name k-
median algorithm (Martnez-Hinarejos et al, 2003).
The distance function to measure the closeness of
two strings is instantiated to be an n-gram overlap
between the two strings.1
The algorithm iterates over three steps until the
data is partitioned into k clusters (Line 5). The first
step (Lines 6-10) is the initialization of a centroid
for a new cluster. Initially when the data is not parti-
tioned into any cluster, the median string of the data
set is used as the initial centroid. For subsequent
iterations, the farthest point from all the centroids
computed thus far is used as the centroid for the new
cluster. In the second step (Lines 11-16), each mem-
ber of the data set is assigned to the nearest clus-
ter based on its distance to that cluster?s centroid.
Finally, in the third step (Lines 17-20), the cluster
centroids are recomputed based on the new cluster
memberships. Steps two and three are repeated until
there are no changes in the cluster memberships and
cluster centroids. This completes the introduction of
a new cluster for the data.
For the purposes of our task, we use up to a
four-gram overlap to measure distance between two
strings and use k = 5 for clustering the data.
4.1 Visualizing the HotSpots
The HotSpots page was created within the ECAT
tool to surgically dissect the way in which templates
were being morphed. For a given template, as shown
in Figure 2, the analyst is presented with a copy of
the texts from the current and previous versions of
that template. Each sentence in the two versions
of the template are color coded to show how fre-
quently the agents have changed that sentence. This
involved running the HotSpots algorithm against ap-
proximately 1,000 emails per template version. A
sentence that is colored red is one that was changed
in over 50% of the emails that were responded to
using that template. An orange sentence is one that
was edited in between 30% and 50%, green is be-
tween 10% to 30% and blue is between 0% and 10%.
The more often a sentence is edited the ?hotter? the
color.
The analyst can see the typical substitutions for a
sentence by hovering the mouse over that sentence.
The typical sentences computed as the centroids of
the clusters created using Algorithm 2 are them-
selves color coded using the same identification sys-
1We have also experimented with a symmetric version of
Levenshtein distance, but we prefer the n-gram overlap score
due to its linear run time complexity.
269
Figure 2: Example of two versions of a template and the edit score (Avg. Morph. Score) and centroids associated with
each sentence of the template.
tem. A typical sentence that occurred in over 50%
of the emails is colored red. A typical sentence that
occurred in 30% to 50% of the emails was orange
and so on.
In seeing the two versions side by side, the an-
alyst can visually inspect the agents? edits on the
current version of a template relative to the previ-
ous version. If the previous version of the template
is a ?hotter? document (with more red sentences), it
means that the changes made to the template by the
author had led to less editing by agents thus speeding
up the process of creating a customer response. If
the current template looks hotter, it suggests that the
changes made to the template were increasing the
agents? edits and probably the email handling time.
5 Automatic Extraction of Potential
Templates
The goal of the template author is to minimize the
number of edits done to a template and thus in-
directly lowering the handling time for an email.
In the preceding section, we discussed a tool that
aids the template authors to identify sentences where
changes are most often made by agents to a tem-
plate. This information could be used by the tem-
plate authors to create a new version of the template
that achieve the goal.
In this section, we investigate a technique that au-
tomatically identifies a possible template with the
potential of directly minimizing the average han-
dling time for an email. We use the set of responses
created by the agents using a given template and se-
lect one of the responses to be generalized and stored
as a new template. The response to be converted
into a template is chosen so as to directly minimize
the average handling time. In essence, we seek to
partition the set of responses R generated from tem-
plate T into two clusters R1 and R2. These clus-
ters have centroids T (current template) and T ? (new
template) such that constraint shown in 1 holds.
(?r?R1AHT (T, r) < AHT (T
?, r)) ?
(?r?R2AHT (T
?, r) < AHT (T, r)) (1)
Now, the quantity AHT (T, r) is logged as part
of the email management system and corresponds
to the time taken to respond to a customer?s email.2
2Although typically this time includes the time to look up a
270
Cluster Number of Centroid (Template/Response)
members
1 1799 GREETINGSPHR, Thank you for your recent email.
On behalf of the company, I would like to extend my sincere
apology for the problems you encountered when (XXX over key
with appropriate response XXX). It is our goal to provide
excellent customer service, and I am sorry that we did not
meet that objective. Your input is very valuable, and we will
take your feedback into consideration. Regards, Agent.FirstName
2 206 GREETINGSPHR, Thank you for letting me know that you?ve been
unable to send an online order to upgrade your NAMEDENTITY service.
Please accept my apologies for any problems this issue may have caused
you. You?re a highly valued customer. I understand your
concerns and I?ll be happy to address them. I am investigating this
issue. I have already made a personal commitment to email you
tomorrow, with the resolution. Thank you for your patience and for
choosing the company. We appreciate your business and continued
loyalty. Sincerely, Agent.FirstName
Table 1: Result of clustering responses using the AHT model as the distance metric.
However, we do not have access to AHT (T ?, r) for
any T ? 6= T . We propose a model to estimate this in
the next section.
5.1 Modeling Average Handling Time
We model AHT as a linear combination of sev-
eral factors which we believe would influence the
handling time for an email. These factors in-
clude the length in words of the customer?s input
email (inplen), the length in words of the template
(templatelen), the length in words of the response
(resplen), the total number of edits between the
template and the response (edit), the normalized edit
score (nedit), the number of individual events of
the edit distance ? substitution (sub), insertion (ins),
deletion (del) and identity (id), the number of block
(contiguous) substitution (blksub), block insertion
(blkins) and block deletion (blkdel). Using these in-
dependent variables, we fit a linear regression model
using the AHT values for 6175 responses created
from one particular template (say G). The result of
the regression fit is shown in Equation 2 and the data
and error statistics are shown in Table 2. It must be
noted that the coefficients for the variables are not
necessarily reflective of the importance of the vari-
ables, since they compensate for the different ranges
in variable values. We have also tried several differ-
the customer?s account etc., we assume that time is quite similar
for all responses created from the same template.
ent regression fits with fewer variables, but find that
this fit gives us the best correlation with the data.
?AHT = 0.5314 ? inplen? 2.7648 ? templatelen
+1.9982 ? resplen? 0.5822 ? edit
+2900.5242 ? nedit
+4.7499 ? id? 1.6647 ? del
?1.6021 ? ins + 26.6704 ? blksub
?15.239 ? blkins + 24.3931 ? blkdel
?261.6627 (2)
Mean AHT 675.74 seconds
Median AHT 543 seconds
Mode AHT 366 seconds
Standard Deviation 487.72 seconds
Correlation coefficient 0.3822
Mean absolute error 320.2 seconds
Root mean squared error 450.64 seconds
Total Number of Instances 6175
Table 2: Data statistics and the goodness of the regression
model for 6175 AHT data points.
Based on the goodness statistics of the regression
fit, it is clear the AHT model could be improved
further. However, we acknowledge that AHT does
not depend solely on the editing of a template to a
271
response but involves several other components in-
cluding the user interface, the complexity of cus-
tomer?s email, the database retrieval to access the
customer?s account and so forth.
Nevertheless, we use this model to cluster a new
set of 2005 responses originating from the same
template (G), as shown in Equation 1. Using the
k-median clustering as described earlier, we parti-
tion the responses into two clusters. We restrict the
first cluster centroid to be the template and search
for the best centroid for the second cluster. The re-
sults are shown in Table 1. The centroid for clus-
ter 1 with 1799 members is the template itself while
the centroid for cluster 2 with 206 members is a re-
sponse that could be suitably generalized to serve as
a template. The overall AHT for the 2005 responses
using the template was 989.2 seconds, while the av-
erage AHT for the members of cluster 1 and 2 was
971.9 seconds and 1140 seconds, indicating that the
template had to be edited considerably to create the
members of cluster 2.
6 Discussion
For the purposes of this paper, it is assumed that
AHT is the same as or correlates well with the time
to compose a response for an email. However, in
most cases the email care agent might have to per-
form several verification, validation, and problem
resolution phases by consulting the specifics of a
customer account before formulating and compos-
ing a response. The time taken for each of these
phases typically varies depending on the customer?s
account and the problem category. Nevertheless, we
assume that the times for these phases is mostly a
constant for a given problem category, and hence the
results presented in this paper need to be interpreted
on a per problem category basis.
A second limitation of the approach presented in
this paper is that the metric used to measure the sim-
ilarity between strings (n-gram overlap) is only a
crude approximation of an ideal semantic similarity
metric. There are however other similarity metrics
(e.g. BLEU (Papineni et al, 2002)) which could be
used equally well. The purpose of this paper is to il-
lustrate the possibility of analysis of responses using
one particular instantiation of the similarity metric.
In spite of the several directions that this work can
be improved, the system and algorithms described
in this paper have been deployed in an operational
customer care center. The qualitative feedback we
have received are extremely positive and analysts
have greatly improved the efficiency of the opera-
tion using this tool.
7 Conclusions
In this paper, we have presented two approaches that
help template authors in designing effective tem-
plates for email customer care agents. In the first ap-
proach, we have presented details of a graphical tool
that provides vital feedback to the template authors
on how their templates are being modified by agents
when creating responses. The template authors can
accommodate this information when designing the
next version of the template. We also presented a
novel technique for identifying responses that can
potentially serve as templates and reduce AHT. To-
wards this end, we discussed a method to model
AHT based on the characteristics of the customer?s
email, the template text and the response text.
8 Acknowledgments
Wewould like to thanks Mazin Gilbert, Junlan Feng,
Narendra Gupta and Wenling Hsu for the discus-
sions during the course of this work. We also thank
the members who generously offered to their sup-
port to provide us with data used in this study with-
out which this work would not have been possible.
We thank the anonymous reviewers for their useful
suggestions in improving the quality of this paper.
References
S.K. Card, J. Mackinlay, and B. Shneiderman. 1999.
Readings in Information Visualization: Using Vision
to Think. Morgan Kaufmann.
J. Coch. 1996. Evaluating and comparing three text-
production techniques. In Proceedings of Coling-96,
pages 249?254, Copenhagen, Denmark.
R.O. Duda, P.E. Hart, and D.G. Stork. 2001. Pattern
Classification. Wiley, New York.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Tar-
get text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
G. Foster, P. Langlais, and G. Lampalme. 2002. User-
friendly text prediction for translators. In EMNLP-02,
pages 46?51, Philadelphia, USA.
Genesys. 2008. http://www.genesys.com. Genesys Cor-
poration.
272
Kana. 2008. http://www.kana.com. Kana Corporation.
G. Lapalme and L. Kosseim. 2003. Mercure: Towards
an automatic e-mail follow-up system. IEEE Compu-
tational Intelligence Bulletin, 2(1):14?18.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
C.D. Martnez-Hinarejos, A. Juan, and F. Casacuberta.
2003. Generalized k-medians clustering for strings.
Lecture Notes in Computer Science, 2652/2003:502?
509.
F.J. Och, R. Zens, and H. Ney. 2003. Efficient search for
interactive statistical machine translation. In EACL-
03.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting
of the Association of Computational Linguistics, pages
313?318, Philadelphia, PA, July.
I. Zukerman and Y. Marom. 2007. Evaluation of a large-
scale email response system. In Proceedings of IJ-
CAI07, Hyderabad, India.
273
Edit Machines for Robust Multimodal Language Processing
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Michael Johnston
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
johnston@research.att.com
Abstract
Multimodal grammars provide an expres-
sive formalism for multimodal integra-
tion and understanding. However, hand-
crafted multimodal grammars can be brit-
tle with respect to unexpected, erroneous,
or disfluent inputs. Spoken language
(speech-only) understanding systems have
addressed this issue of lack of robustness
of hand-crafted grammars by exploiting
classification techniques to extract fillers
of a frame representation. In this paper,
we illustrate the limitations of such clas-
sification approaches for multimodal in-
tegration and understanding and present
an approach based on edit machines that
combine the expressiveness of multimodal
grammars with the robustness of stochas-
tic language models of speech recognition.
We also present an approach where the
edit operations are trained from data using
a noisy channel model paradigm. We eval-
uate and compare the performance of the
hand-crafted and learned edit machines in
the context of a multimodal conversational
system (MATCH).
1 Introduction
Over the years, there have been several mul-
timodal systems that allow input and/or output
to be conveyed over multiple channels such as
speech, graphics, and gesture, for example, put
that there (Bolt, 1980), CUBRICON (Neal and
Shapiro, 1991), QuickSet (Cohen et al, 1998),
SmartKom (Wahlster, 2002), Match (Johnston et
al., 2002). Multimodal integration and interpre-
tation for such interfaces is elegantly expressed
using multimodal grammars (Johnston and Ban-
galore, 2000). These grammars support com-
posite multimodal inputs by aligning speech in-
put (words) and gesture input (represented as se-
quences of gesture symbols) while expressing the
relation between the speech and gesture input and
their combined semantic representation. In (Ban-
galore and Johnston, 2000; Johnston and Banga-
lore, 2005), we have shown that such grammars
can be compiled into finite-state transducers en-
abling effective processing of lattice input from
speech and gesture recognition and mutual com-
pensation for errors and ambiguities.
However, like other approaches based on hand-
crafted grammars, multimodal grammars can be
brittle with respect to extra-grammatical, erro-
neous and disfluent input. For speech recognition,
a corpus-driven stochastic language model (SLM)
with smoothing or a combination of grammar-
based and   -gram model (Bangalore and John-
ston, 2004; Wang et al, 2002) can be built in order
to overcome the brittleness of a grammar-based
language model. Although the corpus-driven lan-
guage model might recognize a user?s utterance
correctly, the recognized utterance may not be
assigned a semantic representation by the multi-
modal grammar if the utterance is not part of the
grammar.
There have been two main approaches to im-
proving robustness of the understanding compo-
nent in the spoken language understanding litera-
ture. First, a parsing-based approach attempts to
recover partial parses from the parse chart when
the input cannot be parsed in its entirety due to
noise, in order to construct a (partial) semantic
representation (Dowding et al, 1993; Allen et al,
2001; Ward, 1991). Second, a classification-based
approach views the problem of understanding as
extracting certain bits of information from the in-
put. It attempts to classify the utterance and iden-
tifies substrings of the input as slot-filler values
to construct a frame-like semantic representation.
Both approaches have shortcomings. Although in
the first approach, the grammar can encode richer
semantic representations, the method for combin-
ing the fragmented parses is quite ad hoc. In the
second approach, the robustness is derived from
training classifiers on annotated data, this data is
very expensive to collect and annotate, and the
semantic representation is fairly limited. Further-
more, it is not clear how to extend this approach to
apply on lattice input ? an important requirement
for multimodal processing.
361
An alternative to these approaches is to edit
the recognized string to match the closest string
that can be accepted by the grammar. Essentially
the idea is that, if the recognized string cannot
be parsed, then we determine which in-grammar
string it is most like. For example, in Figure 1, the
recognized string is mapped to the closest string in
the grammar by deletion of the words restaurants
and in.
ASR: show cheap restaurants thai places in in chelsea
Edits: show cheap  thai places in  chelsea
Grammar: show cheap thai places in chelsea
Figure 1: Editing Example
In this paper, we develop further this edit-based
approach to finite-state multimodal language un-
derstanding and show how when appropriately
tuned it can provide a substantial improvement in
concept accuracy. We also explore learning ed-
its from data and present an approach of model-
ing this process as a machine translation problem.
We learn a model to translate from out of grammar
or misrecognized language (such as ?ASR:? above)
to the closest language the system can understand
(?Grammar:? above). To this end, we adopt tech-
niques from statistical machine translation (Brown
et al, 1993; Och and Ney, 2003) and use statistical
alignment to learn the edit patterns. Here we eval-
uate these different techniques on data from the
MATCHmultimodal conversational system (John-
ston et al, 2002) but the same techniques are more
broadly applicable to spoken language systems in
general whether unimodal or multimodal.
The layout of the paper is as follows. In Sec-
tions 2 and 3, we briefly describe the MATCH
application and the finite-state approach to mul-
timodal language understanding. In Section 4,
we discuss the limitations of the methods used
for robust understanding in spoken language un-
derstanding literature. In Section 5 we present
our approach to building hand-crafted edit ma-
chines. In Section 6, we describe our approach to
learning the edit operations using a noisy channel
paradigm. In Section 7, we describe our experi-
mental evaluation.
2 MATCH: A Multimodal Application
MATCH (Multimodal Access To City Help) is a
working city guide and navigation system that en-
ables mobile users to access restaurant and sub-
way information for New York City and Washing-
ton, D.C. (Johnston et al, 2002). The user inter-
acts with an interface displaying restaurant list-
ings and a dynamic map showing locations and
street information. The inputs can be speech,
drawing/pointing on the display with a stylus, or
synchronous multimodal combinations of the two
modes. The user can ask for the review, cui-
sine, phone number, address, or other informa-
tion about restaurants and subway directions to lo-
cations. The system responds with graphical la-
bels on the display, synchronized with synthetic
speech output. For example, if the user says phone
numbers for these two restaurants and circles two
restaurants as in Figure 2 [A], the system will draw
a callout with the restaurant name and number and
say, for example Time Cafe can be reached at 212-
533-7000, for each restaurant in turn (Figure 2
[B]).
Figure 2: MATCH Example
3 Finite-state Multimodal Understanding
Our approach to integrating and interpreting mul-
timodal inputs (Johnston et al, 2002) is an exten-
sion of the finite-state approach previously pro-
posed in (Bangalore and Johnston, 2000; John-
ston and Bangalore, 2005). In this approach, a
declarative multimodal grammar captures both the
structure and the interpretation of multimodal and
unimodal commands. The grammar consists of
a set of context-free rules. The multimodal as-
pects of the grammar become apparent in the ter-
minals, each of which is a triple W:G:M, consist-
ing of speech (words, W), gesture (gesture sym-
bols, G), and meaning (meaning symbols, M). The
multimodal grammar encodes not just multimodal
integration patterns but also the syntax of speech
and gesture, and the assignment of meaning, here
represented in XML. The symbol SEM is used to
abstract over specific content such as the set of
points delimiting an area or the identifiers of se-
lected objects (Johnston et al, 2002). In Figure 3,
we present a small simplified fragment from the
MATCH application capable of handling informa-
tion seeking requests such as phone for these three
restaurants. The epsilon symbol (  ) indicates that
a stream is empty in a given terminal.
In the example above where the user says phone
for these two restaurants while circling two restau-
rants (Figure 2 [a]), assume the speech recognizer
returns the lattice in Figure 4 (Speech). The ges-
ture recognition component also returns a lattice
(Figure 4, Gesture) indicating that the user?s ink
362
CMD   :  :  cmd  INFO  :  :  /cmd 
INFO   :  :  type  TYPE  :  :  /type 
for:  :  :  :  obj  DEICNP  :  :  /obj 
TYPE  phone:  :phone  review:  :review
DEICNP  DDETPL  :area:  :sel:  NUM HEADPL
DDETPL  these:G: 	 those:G: 
HEADPL  restaurants:rest:  rest 
 :SEM:SEM
 :  :  /rest 
NUM  two:2:  three:3:  ... ten:10: 
Figure 3: Multimodal grammar fragment
Speech:
sel
locareaG
Gesture:
2
<rest>
Meaning:
<rest>
</type> <obj>
</cmd></info></obj></rest>r12,r15
phone
twotheseforphone
SEM(r12,r15)
restaurants
<type><info><cmd>
SEM(points...)
ten
Figure 4: Multimodal Example
is either a selection of two restaurants or a ge-
ographical area. In Figure 4 (Gesture) the spe-
cific content is indicated in parentheses after SEM.
This content is removed before multimodal pars-
ing and integration and replaced afterwards. For
detailed explanation of our technique for abstract-
ing over and then re-integrating specific gestural
content and our approach to the representation of
complex gestures see (Johnston et al, 2002). The
multimodal grammar (Figure 3) expresses the re-
lationship between what the user said, what they
drew with the pen, and their combined mean-
ing, in this case Figure 4 (Meaning). The mean-
ing is generated by concatenating the meaning
symbols and replacing SEM with the appropri-
ate specific content:  cmd  info  type 
phone  /type  obj  rest  [r12,r15]  /rest 
 /obj  /info  /cmd  .
For use in our system, the multimodal grammar
is compiled into a cascade of finite-state transduc-
ers (Johnston and Bangalore, 2000; Johnston et al,
2002; Johnston and Bangalore, 2005). As a result,
processing of lattice inputs from speech and ges-
ture processing is straightforward and efficient.
3.1 Meaning Representation for Concept
Accuracy
The hierarchically nested XML representation
above is effective for processing by the backend
application, but is not well suited for the auto-
mated determination of the performance of the
language understanding mechanism. We adopt an
approach, similar to (Ciaramella, 1993; Boros et
al., 1996), in which the meaning representation,
in our case XML, is transformed into a sorted flat
list of attribute-value pairs indicating the core con-
tentful concepts of each command. The example
above yields:
ffProceedings of the 12th Conference of the European Chapter of the ACL, pages 94?102,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Incremental Parsing Models for Dialog Task Structure
Srinivas Bangalore and Amanda J. Stent
AT&T Labs ? Research, Inc., 180 Park Avenue,
Florham Park, NJ 07932, USA
{srini,stent}@research.att.com
Abstract
In this paper, we present an integrated
model of the two central tasks of dialog
management: interpreting user actions and
generating system actions. We model the
interpretation task as a classi?cation prob-
lem and the generation task as a predic-
tion problem. These two tasks are inter-
leaved in an incremental parsing-based di-
alog model. We compare three alterna-
tive parsing methods for this dialog model
using a corpus of human-human spoken
dialog from a catalog ordering domain
that has been annotated for dialog acts
and task/subtask information. We contrast
the amount of context provided by each
method and its impact on performance.
1 Introduction
Corpora of spoken dialog are now widely avail-
able, and frequently come with annotations for
tasks/games, dialog acts, named entities and ele-
ments of syntactic structure. These types of infor-
mation provide rich clues for building dialog mod-
els (Grosz and Sidner, 1986). Dialog models can
be built of?ine (for dialog mining and summariza-
tion), or online (for dialog management).
A dialog manager is the component of a dia-
log system that is responsible for interpreting user
actions in the dialog context, and for generating
system actions. Needless to say, a dialog manager
operates incrementally as the dialog progresses. In
typical commercial dialog systems, the interpre-
tation and generation processes operate indepen-
dently of each other, with only a small amount of
shared context. By contrast, in this paper we de-
scribe a dialog model that (1) tightly integrates in-
terpretation and generation, (2) makes explicit the
type and amount of shared context, (3) includes
the task structure of the dialog in the context, (4)
can be trained from dialog data, and (5) runs in-
crementally, parsing the dialog as it occurs and in-
terleaving generation and interpretation.
At the core of our model is a parser that in-
crementally builds the dialog task structure as the
dialog progresses. In this paper, we experiment
with three different incremental tree-based parsing
methods. We compare these methods using a cor-
pus of human-human spoken dialogs in a catalog
ordering domain that has been annotated for dialog
acts and task/subtask information. We show that
all these methods outperform a baseline method
for recovering the dialog structure.
The rest of this paper is structured as follows:
In Section 2, we review related work. In Sec-
tion 3, we present our view of the structure of task-
oriented human-human dialogs. In Section 4, we
present the parsing approaches included in our ex-
periments. In Section 5, we describe our data and
experiments. Finally, in Section 6, we present con-
clusions and describe our current and future work.
2 Related Work
There are two threads of research that are relevant
to our work: work on parsing (written and spoken)
discourse, and work on plan-based dialog models.
Discourse Parsing Discourse parsing is the pro-
cess of building a hierarchical model of a dis-
course from its basic elements (sentences or
clauses), as one would build a parse of a sen-
tence from its words. There has now been con-
siderable work on discourse parsing using statisti-
cal bottom-up parsing (Soricut and Marcu, 2003),
hierarchical agglomerative clustering (Sporleder
and Lascarides, 2004), parsing from lexicalized
tree-adjoining grammars (Cristea, 2000), and rule-
based approaches that use rhetorical relations and
discourse cues (Forbes et al, 2003; Polanyi et al,
2004; LeThanh et al, 2004). With the exception of
Cristea (2000), most of this research has been lim-
ited to non-incremental parsing of textual mono-
logues where, in contrast to incremental dialog
parsing, predicting a system action is not relevant.
The work on discourse parsing that is most
similar to ours is that of Baldridge and Las-
carides (2005). They used a probabilistic head-
driven parsing method (described in (Collins,
2003)) to construct rhetorical structure trees for a
spoken dialog corpus. However, their parser was
94
Dialog
Task
Topic/SubtaskTopic/Subtask
Task Task
Clause
UtteranceUtteranceUtterance
Topic/Subtask
DialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?Args
Figure 1: A schema of a shared plan tree for a
dialog.
not incremental; it used global features such as the
number of turn changes. Also, it focused strictly
in interpretation of input utterances; it could not
predict actions by either dialog partner.
In contrast to other work on discourse parsing,
we wish to use the parsing process directly for di-
alog management (rather than for information ex-
traction or summarization). This in?uences our
approach to dialog modeling in two ways. First,
the subtask tree we build represents the functional
task structure of the dialog (rather than the rhetor-
ical structure of the dialog). Second, our dialog
parser must be entirely incremental.
Plan-Based Dialog Models Plan-based ap-
proaches to dialog modeling, like ours, operate di-
rectly on the dialog?s task structure. The process
of task-oriented dialog is treated as a special case
of AI-style plan recognition (Sidner, 1985; Litman
and Allen, 1987; Rich and Sidner, 1997; Carberry,
2001; Bohus and Rudnicky, 2003; Lochbaum,
1998). Plan-based dialog models are used for both
interpretation of user utterances and prediction of
agent actions. In addition to the hand-crafted mod-
els listed above, researchers have built stochastic
plan recognition models for interaction, includ-
ing ones based on Hidden Markov Models (Bui,
2003; Blaylock and Allen, 2006) and on proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this area, the work most closely related to
ours is that of Barrett and Weld (Barrett and Weld,
1994), who build an incremental bottom-up parser
Opening
Order Placement
Contact Info
Delivery InfoShipping Info
ClosingSummaryPayment InfoOrder Item
Figure 2: Sample output (subtask tree) from a
parse-based model for the catalog ordering do-
main.
to parse plans. Their parser, however, was not
probabilistic or targeted at dialog processing.
3 Dialog Structure
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree T that incorpo-
rates the task/subtask structure, dialog acts, syn-
tactic structure and lexical content of the dialog,
as shown in Figure 1. A task is a sequence of sub-
tasks ST ? S. A subtask is a sequence of dialog
acts DA ? D. Each dialog act corresponds to one
clause spoken by one speaker, customer (cu) or
agent (ca) (for which we may have acoustic, lexi-
cal, syntactic and semantic representations).
Figure 2 shows the subtask tree for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting can be as deep as ?ve lev-
els in our data. Most often the nesting is at the
leftmost or rightmost frontier of the subtask tree.
As the dialog proceeds, an utterance from a par-
ticipant is accommodated into the subtask tree in
an incremental manner, much like an incremen-
tal syntactic parser accommodates the next word
into a partial parse tree (Alexandersson and Rei-
thinger, 1997). An illustration of the incremental
evolution of dialog structure is shown in Figure 4.
However, while a syntactic parser processes in-
put from a single source, our dialog parser parses
user-system exchanges: user utterances are inter-
preted, while system utterances are generated. So
the steps taken by our dialog parser to incorpo-
rate an utterance into the subtask tree depend on
whether the utterance was produced by the agent
or the user (as shown in Figure 3).
User utterances Each user turn is split into
clauses (utterances). Each clause is supertagged
95
Interpretation of a user?s utterance:
DAC : daui = argmax
du?D
P (du|cui , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(1)
STC : stui = argmax
su?S
P (su|daui , c
u
i , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(2)
Generation of an agent?s utterance:
STP : stai = argmax
sa?S
P (sa|ST i?1i?k , DA
i?1
i?k, c
i?1
i?k)
(3)
DAP : daai = argmax
da?D
P (da|stai , ST
i?1
i?k , DA
i?1
i?k, c
i?1
i?k)
(4)
Table 1: Equations used for modeling dialog act and sub-
task labeling of agent and user utterances. cui /c
a
i = the
words, syntactic information and named entities associated
with the ith utterance of the dialog, spoken by user/agent
u/a. daui /da
a
i = the dialog act of the i
th utterance, spoken
by user/agent u/a. stui /st
a
i = the subtask label of the i
th ut-
terance, spoken by user/agent u/a. DAi?1i?k represents the
dialog act tags for utterances i? 1 to i? k.
and labeled with named entities1. Interpretation of
the clause (cui ) involves assigning a dialog act la-
bel (daui ) and a subtask label (st
u
i ). We use ST
i?1
i?k ,
DAi?1i?k, and c
i?1
i?k to represent the sequence of pre-
ceeding k subtask labels, dialog act labels and
clauses respectively. The dialog act label daui is
determined from information about the clause and
(a kth order approximation of) the subtask tree so
far (Ti?1 = (ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown in
Equation 1 (Table 1). The subtask label stui is de-
termined from information about the clause, its di-
alog act and the subtask tree so far, as shown in
Equation 2. Then, the clause is incorporated into
the subtask tree.
Agent utterances In contrast, a dialog sys-
tem starts planning an agent utterance by iden-
tifying the subtask to contribute to next, stai ,
based on the subtask tree so far (Ti?1 =
(ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown in Equation 3
(Table 1) . Then, it chooses the dialog act of the
utterance, daai , based on the subtask tree so far and
the chosen subtask for the utterance, as shown in
Equation 4. Finally, it generates an utterance, cai ,
to realize its communicative intent (represented
as a subtask and dialog act pair, with associated
named entities)2.
Note that the current clause cui is used in the
1This results in a syntactic parse of the clause and could
be done incrementally as well.
2We do not address utterance realization in this paper.
Figure 3: Dialog management process
conditioning context of the interpretation model
(for user utterances), but the corresponding clause
for the agent utterance cai is to be predicted and
hence is not part of conditioning context in the
generation model.
4 Dialog Parsing
A dialog parser can produce a ?shallow? or ?deep?
tree structure. A shallow parse is one in which
utterances are grouped together into subtasks, but
the dominance relations among subtasks are not
tracked. We call this model a chunk-based dia-
log model (Bangalore et al, 2006). The chunk-
based model has limitations. For example, dom-
inance relations among subtasks are important
for dialog processes such as anaphora resolu-
tion (Grosz and Sidner, 1986). Also, the chunk-
based model is representationally inadequate for
center-embedded nestings of subtasks, which do
occur in our domain, although less frequently than
the more prevalent ?tail-recursive? structures.
We use the term parse-based dialog model to
refer to deep parsing models for dialog which
not only segment the dialog into chunks but also
predict dominance relations among chunks. For
this paper, we experimented with three alternative
methods for building parse-based models: shift-
reduce, start-complete and connection path.
Each of these operates on the subtask tree for
the dialog incrementally, from left-to-right, with
access only to the preceding dialog context, as
shown in Figure 4. They differ in the parsing ac-
tions and the data structures used by the parser;
this has implications for robustness to errors. The
instructions to reconstruct the parse are either en-
tirely encoded in the stack (in the shift-reduce
method), or entirely in the parsing actions (in the
start-complete and connection path methods). For
each of the four types of parsing action required
to build the parse tree (see Table 1), we construct
96
......
Order Item Task
Opening
Hello Request(MakeOrder) Ack
number with area codesecond
please
Ack
Contact?Info
can i have your
home telephone thank you 
please
Ack
Contact?Info
thank
youto place an order secondfor calling
XYZ catalog
this is mary
how may I
help you
yes one yes one thank you 
for calling
XYZ catalog
this is mary
how may I
help you
Ack
Order Item Task
Opening
Hello Request(MakeOrder)
yes i would like
Opening
Hello Request(MakeOrder) Ack
thank you 
for calling
Order Item Task
yes please
Shipping?Address
can i have your
home telephone 
number with area code
......XYZ catalog
Contact?Info
to place an order
yes i would like
you
thank
.........
Ack
this is mary
how may I
help you
yes one 
second
please ......
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
you
thank
to place an order
yes i would like
Order Item Task
Opening
how may I
you
thank
Closing.........
may we deliver this
order to your home
yes i would like
help you
yes one 
second
please
Ack
to place an order
yes i would like
you
thank
to place an order
help you
yes one 
second
please
Ack
Contact?Info
may we deliver this
order to your home
......
yes please
how may I
Shipping?Address
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
can i have your
home telephone 
number with area code
......
Order Item Task
Opening
Shipping?Address
Request(MakeOrder) Ack
thank you 
for calling
XYZ catalog
this is mary
Hello
can i have your
home telephone 
number with area code
......
Order Item Task
Opening
how may I
you
thankyes i would like
help you
yes one 
second
please
Ack
Contact?Info
to place an order
Figure 4: An illustration of incremental evolution of dialog structure
a feature vector containing contextual information
for the parsing action (see Section 5.1). These fea-
ture vectors and the associated parser actions are
used to train maximum entropy models (Berger et
al., 1996). These models are then used to incre-
mentally incorporate the utterances for a new di-
alog into that dialog?s subtask tree as the dialog
progresses, as shown in Figure 3.
4.1 Shift-Reduce Method
In this method, the subtask tree is recovered
through a right-branching shift-reduce parsing
process (Hall et al, 2006; Sagae and Lavie, 2006).
The parser shifts each utterance on to the stack. It
then inspects the stack and decides whether to do
one or more reduce actions that result in the cre-
ation of subtrees in the subtask tree. The parser
maintains two data structures ? a stack and a tree.
The actions of the parser change the contents of
the stack and create nodes in the dialog tree struc-
ture. The actions for the parser include unary-
reduce-X, binary-reduce-X and shift, where X is
each of the non-terminals (subtask labels) in the
tree. Shift pushes a token representing the utter-
ance onto the stack; binary-reduce-X pops two to-
kens off the stack and pushes the non-terminal X;
and unary-reduce-X pops one token off the stack
and pushes the non-terminal X. Each type of re-
duce action creates a constituent X in the dialog
tree and the tree(s) associated with the reduced el-
ements as subtree(s) of X. At the end of the dialog,
the output is a binary branching subtask tree.
Consider the example subdialog A: would you
like a free magazine? U: no. The process-
ing of this dialog using our shift-reduce dialog
parser would proceed as follows: the STP model
predicts shift for sta; the DAP model predicts
YNP(Promotions) for daa; the generator outputs
would you like a free magazine?; and the parser
shifts a token representing this utterance onto the
stack. Then, the customer says no. The DAC
model classi?es dau as No; the STC model clas-
si?es stu as shift and binary-reduce-special-offer;
and the parser shifts a token representing the ut-
terance onto the stack, before popping the top two
elements off the stack and adding the subtree for
special-order into the dialog?s subtask tree.
4.2 Start-Complete Method
In the shift-reduce method, the dialog tree is con-
structed as a side effect of the actions performed
on the stack: each reduce action on the stack in-
troduces a non-terminal in the tree. By contrast,
in the start-complete method the instructions to
build the tree are directly encoded in the parser ac-
tions. A stack is used to maintain the global parse
state. The actions the parser can take are similar
to those described in (Ratnaparkhi, 1997). The
parser must decide whether to join each new termi-
nal onto the existing left-hand edge of the tree, or
start a new subtree. The actions for the parser in-
clude start-X, n-start-X, complete-X, u-complete-
X and b-complete-X, where X is each of the non-
terminals (subtask labels) in the tree. Start-X
pushes a token representing the current utterance
onto the stack; n-start-X pushes non-terminal X
onto the stack; complete-X pushes a token repre-
senting the current utterance onto the stack, then
97
pops the top two tokens off the stack and pushes
the non-terminal X; u-complete-X pops the top to-
ken off the stack and pushes the non-terminal X;
and b-complete-X pops the top two tokens off the
stack and pushes the non-terminal X. This method
produces a dialog subtask tree directly, rather than
producing an equivalent binary-branching tree.
Consider the same subdialog as before, A:
would you like a free magazine? U: no. The
processing of this dialog using our start-complete
dialog parser would proceed as follows: the STP
model predicts start-special-offer for sta; the DAP
model predicts YNP(Promotions) for daa; the gen-
erator outputs would you like a free magazine?;
and the parser shifts a token representing this ut-
terance onto the stack. Then, the customer says
no. The DAC model classi?es dau as No; the STC
model classi?es stu as complete-special-offer; and
the parser shifts a token representing the utter-
ance onto the stack, before popping the top two
elements off the stack and adding the subtree for
special-order into the dialog?s subtask tree.
4.3 Connection Path Method
In contrast to the shift-reduce and the start-
complete methods described above, the connec-
tion path method does not use a stack to track the
global state of the parse. Instead, the parser di-
rectly predicts the connection path (path from the
root to the terminal) for each utterance. The col-
lection of connection paths for all the utterances in
a dialog de?nes the parse tree. This encoding was
previously used for incremental sentence parsing
by (Costa et al, 2001). With this method, there
are many more choices of decision for the parser
(195 decisions for our data) compared to the shift-
reduce (32) and start-complete (82) methods.
Consider the same subdialog as before, A:
would you like a free magazine? U: no. The pro-
cessing of this dialog using our connection path
dialog parser would proceed as follows. First, the
STP model predicts S-special-offer for sta; the
DAP model predicts YNP(Promotions) for daa;
the generator outputs would you like a free mag-
azine?; and the parser adds a subtree rooted at
special-offer, with one terminal for the current ut-
terance, into the top of the subtask tree. Then,
the customer says no. The DAC model classi-
?es dau as No and the STC model classi?es stu
as S-special-offer. Since the right frontier of the
subtask tree has a subtree matching this path, the
Type Task/subtask labels
Call-level call-forward, closing, misc-other, open-
ing, out-of-domain, sub-call
Task-level check-availability, contact-info,
delivery-info, discount, order-change,
order-item, order-problem, payment-
info, related-offer, shipping-address,
special-offer, summary
Table 2: Task/subtask labels in CHILD
Type Subtype
Ask Info
Explain Catalog, CC Related, Discount, Order Info
Order Problem, Payment Rel, Product Info
Promotions, Related Offer, Shipping
Convers- Ack, Goodbye, Hello, Help, Hold,
-ational YoureWelcome, Thanks, Yes, No, Ack,
Repeat, Not(Information)
Request Code, Order Problem, Address, Catalog,
CC Related, Change Order, Conf, Credit,
Customer Info, Info, Make Order, Name,
Order Info, Order Status, Payment Rel,
Phone Number, Product Info, Promotions,
Shipping, Store Info
YNQ Address, Email, Info, Order Info,
Order Status,Promotions, Related Offer
Table 3: Dialog act labels in CHILD
parser simply incorporates the current utterance as
a terminal of the special-offer subtree.
5 Data and Experiments
To evaluate our parse-based dialog model, we used
817 two-party dialogs from the CHILD corpus of
telephone-based dialogs in a catalog-purchasing
domain. Each dialog was transcribed by hand;
all numbers (telephone, credit card, etc.) were
removed for privacy reasons. The average di-
alog in this data set had 60 turns. The di-
alogs were automatically segmented into utter-
ances and automatically annotated with part-of-
speech tag and supertag information and named
entities. They were annotated by hand for dia-
log acts and tasks/subtasks. The dialog act and
task/subtask labels are given in Tables 2 and 3.
5.1 Features
In our experiments we used the following features
for each utterance: (a) the speaker ID; (b) uni-
grams, bigrams and trigrams of the words; (c) un-
igrams, bigrams and trigrams of the part of speech
tags; (d) unigrams, bigrams and trigrams of the su-
pertags; (e) binary features indicating the presence
or absence of particular types of named entity; (f)
the dialog act (determined by the parser); (g) the
task/subtask label (determined by the parser); and
(h) the parser stack at the current utterance (deter-
98
mined by the parser). Each input feature vector for
agent subtask prediction has these features for up
to three utterances of left-hand context (see Equa-
tion 3). Each input feature vector for dialog act
prediction has the same features as for agent sub-
task prediction, plus the actual or predicted sub-
task label (see Equation 4). Each input feature
vector for dialog act interpretation has features a-
h for up to three utterances of left-hand context,
plus the current utterance (see Equation 1). Each
input feature vector for user subtask classi?cation
has the same features as for user dialog act inter-
pretation, plus the actual or classi?ed dialog act
(see Equation 2).
The label for each input feature vector is the
parsing action (for subtask classi?cation and pre-
diction) or the dialog act label (for dialog act clas-
si?cation and prediction). If more than one pars-
ing action takes place on a particular utterance
(e.g. a shift and then a reduce), the feature vec-
tor is repeated twice with different stack contents.
5.2 Training Method
We randomly selected roughly 90% of the dialogs
for training, and used the remainder for testing.
We separately trained models for: user dia-
log act classi?cation (DAC, Equation 1); user
task/subtask classi?cation (STC, Equation 2);
agent task/subtask prediction (STP, Equation 3);
and agent dialog act prediction (DAP, Equation 4).
In order to estimate the conditional distributions
shown in Table 1, we use the general technique of
choosing the MaxEnt distribution that properly es-
timates the average of each feature over the train-
ing data (Berger et al, 1996). We use the machine
learning toolkit LLAMA (Haffner, 2006), which
encodes multiclass classi?cation problems using
binary MaxEnt classi?ers to increase the speed of
training and to scale the method to large data sets.
5.3 Decoding Method
The decoding process for the three parsing meth-
ods is illustrated in Figure 3 and has four stages:
STP, DAP, DAC, and STC. As already explained,
each of these steps in the decoding process is mod-
eled as either a prediction task or a classi?ca-
tion task. The decoder constructs an input feature
vector depending on the amount of context being
used. This feature vector is used to query the ap-
propriate classi?er model to obtain a vector of la-
bels with weights. The parser action labels (STP
and STC) are used to extend the subtask tree. For
example, in the shift-reduce method, shift results
in a push action on the stack, while reduce-X re-
sults in popping the top two elements off the stack
and pushing X on to the stack. The dialog act la-
bels (DAP and DAC) are used to label the leaves
of the subtask tree (the utterances).
The decoder can use n-best results from the
classi?er to enlarge the search space. In order
to manage the search space effectively, the de-
coder uses a beam pruning strategy. The decod-
ing process proceeds until the end of the dialog is
reached. In this paper, we assume that the end of
the dialog is given to the decoder3.
Given that the classi?ers are error-prone in their
assignment of labels, the parsing step of the de-
coder needs to be robust to these errors. We ex-
ploit the state of the stack in the different meth-
ods to rule out incompatible parser actions (e.g. a
reduce-X action when the stack has one element,
a shift action on an already shifted utterance). We
also use n-best results to alleviate the impact of
classi?cation errors. Finally, at the end of the di-
alog, if there are unattached constituents on the
stack, the decoder attaches them as sibling con-
stituents to produce a rooted tree structure. These
constraints contribute to robustness, but cannot be
used with the connection path method, since any
connection path (parsing action) suggested by the
classi?er can be incorporated into the incremental
parse tree. Consequently, in the connection path
method there are fewer opportunities to correct the
errors made by the classi?ers.
5.4 Evaluation Metrics
We evaluate dialog act classi?cation and predic-
tion by comparing the automatically assigned di-
alog act tags to the reference dialog act tags.
For these tasks we report accuracy. We evaluate
subtask classi?cation and prediction by compar-
ing the subtask trees output by the different pars-
ing methods to the reference subtask tree. We
use the labeled crossing bracket metric (typically
used in the syntactic parsing literature (Harrison et
al., 1991)), which computes recall, precision and
crossing brackets for the constituents (subtrees) in
a hypothesized parse tree given the reference parse
tree. We report F-measure, which is a combination
of recall and precision.
For each task, performance is reported for 1, 3,
3This is an unrealistic assumption if the decoder is to
serve as a dialog model. We expect to address this limitation
in future work.
99
5, and 10-best dynamic decoding as well as oracle
(Or) and for 0, 1 and 3 utterances of context.
5.5 Results
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0
20
40
60
80
100
Number utterances history
Nbest
F
start?complete
connection?paths
shift?reduce
Figure 5: Performance of parse-based methods for
subtask tree building
Figure 5 shows the performance of the different
methods for determining the subtask tree of the di-
alog. Wider beam widths do not lead to improved
performance for any method. One utterance of
context is best for shift-reduce and start-join; three
is best for the connection path method. The shift-
reduce method performs the best. With 1 utter-
ance of context, its 1-best f-score is 47.86, as com-
pared with 34.91 for start-complete, 25.13 for the
connection path method, and 21.32 for the chunk-
based baseline. These performance differences are
statistically signi?cant at p < .001. However, the
best performance for the shift-reduce method is
still signi?cantly worse than oracle.
All of the methods are subject to some ?stick-
iness?, a certain preference to stay within the
current subtask rather than starting a new one.
Also, all of the methods tended to perform poorly
on parsing subtasks that occur rarely (e.g. call-
forward, order-change) or that occur at many dif-
ferent locations in the dialog (e.g. out-of-domain,
order-problem, check-availability). For example,
the shift-reduce method did not make many shift
errors but did frequently b-reduce on an incor-
rect non-terminal (indicating trouble identifying
subtask boundaries). Some non-terminals most
likely to be labeled incorrectly by this method
(for both agent and user) are: call-forward, order-
change, summary, order-problem, opening and
out-of-domain.
Similarly, the start-complete method frequently
mislabeled a non-terminal in a complete action,
e.g. misc-other, check-availability, summary or
contact-info. It also quite frequently mislabeled
nonterminals in n-start actions, e.g. order-item,
contact-info or summary. Both of these errors in-
dicate trouble identifying subtask boundaries.
It is harder to analyze the output from the con-
nection path method. This method is more likely
to mislabel tree-internal nodes than those imme-
diately above the leaves. However, the same
non-terminals show up as error-prone for this
method as for the others: out-of-domain, check-
availability, order-problem and summary.
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0.0
0.2
0.4
0.6
0.8
1.0
Number utterances history
Nbest
A
cc
u
ra
cy
start?complete
connection?paths
shift?reduce
Figure 6: Performance of dialog act assignment to
user?s utterances.
Figure 6 shows accuracy for classi?cation of
user dialog acts. Wider beam widths do not
lead to sign?cantly improved performance for any
method. Zero utterances of context gives the high-
est accuracy for all methods. All methods per-
form fairly well, but no method signi?cantly out-
performs any other: with 0 utterances of context,
1-best accuracy is .681 for the connection path
method, .698 for the start-complete method and
.698 for the shift-reduce method. We note that
these results are competitive with those reported
in the literature (e.g. (Poesio and Mikheev, 1998;
Sera?n and Eugenio, 2004)), although the dialog
corpus and the label sets are different.
The most common errors in dialog act classi?-
cation occur with dialog acts that occur 40 times
or fewer in the testing data (out of 3610 testing
utterances), and with Not(Information).
Figure 7 shows accuracy for prediction of agent
dialog acts. Performance for this task is lower than
100
Speaker Utterance Shift-Reduce Start-Complete Connection Path
A This is Sally shift, Hello start-opening, Hello opening S, Hello
A How may I help you shift, binary-reduce-out-of-
domain, Hello
complete-opening,
Hello
opening S, Hello
B Yes Not(Information), shift,
binary-reduce-out-of-domain
Not(Information),
complete-opening
Not(Information), open-
ing S
B Um I would like to place
an order please
Rquest(Make-Order), shift,
binary-reduce-opening
Rquest(Make-Order),
complete-opening,
n-start-S
Rquest(Make-Order),
opening S
A May I have your tele-
phone number with the
area code
shift, Acknowledge start-contact-info, Ac-
knowledge
contact-info S,
Request(Phone-Number)
B Uh the phone number is
[number]
Explain(Phone-Number),
shift, binary-reduce-contact-
info
Explain(Phone-
Number), complete-
contact-info
Explain(Phone-Number),
contact-info S
Table 4: Dialog extract with subtask tree building actions for three parsing methods
0 1
1
3 0 1
3
3 0 1
5
3 0 1
10
3 0 1
Or
3
0.0
0.2
0.4
0.6
0.8
1.0
Number utterances history
Nbest
A
cc
u
ra
cy
start?complete
connection?paths
shift?reduce
Figure 7: Performance of dialog act prediction
used to generate agent utterances.
that for dialog act classi?cation because this is a
prediction task. Wider beam widths do not gener-
ally lead to improved performance for any method.
Three utterances of context generally gives the
best performance. The shift-reduce method per-
forms signi?cantly better than the connection path
method with a beam width of 1 (p < .01), but not
at larger beam widths; there are no other signi?-
cant performance differences between methods at
3 utterances of context. With 3 utterances of con-
text, 1-best accuracies are .286 for the connection
path method, .329 for the start-complete method
and .356 for the shift-reduce method.
The most common errors in dialog act predic-
tion occur with rare dialog acts, Not(Information),
and the prediction of Acknowledge at the start of a
turn (we did not remove grounding acts from the
data). With the shift-reduce method, some YNQ
acts are commonly mislabeled. With all methods,
dialog acts pertaining to Order-Info and Product-
Info acts are commonly mislabeled, which could
potentially indicate that these labels require a sub-
tle distinction between information pertaining to
an order and information pertaining to a product.
Table 4 shows the parsing actions performed by
each of our methods on the dialog snippet pre-
sented in Figure 4. For this example, the connec-
tion path method?s output is correct in all cases.
6 Conclusions and Future Work
In this paper, we present a parsing-based model
of task-oriented dialog that tightly integrates in-
terpretation and generation using a subtask tree
representation, can be trained from data, and runs
incrementally for use in dialog management. At
the core of this model is a parser that incremen-
tally builds the dialog task structure as it interprets
user actions and generates system actions. We ex-
periment with three different incremental parsing
methods for our dialog model. Our proposed shift-
reduce method is the best-performing so far, and
performance of this method for dialog act classi?-
cation and task/subtask modeling is good enough
to be usable. However, performance of all the
methods for dialog act prediction is too low to be
useful at the moment. In future work, we will ex-
plore improved models for this task that make use
of global information about the task (e.g. whether
each possible subtask has yet been completed;
whether required and optional task-related con-
cepts such as shipping address have been ?lled).
We will also separate grounding and task-related
behaviors in our model.
101
References
J. Alexandersson and N. Reithinger. 1997. Learning
dialogue structures from a corpus. In Proceedings
of Eurospeech.
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse. In Proceedings
of CoNLL.
S. Bangalore, G. Di Fabbrizio, and A. Stent. 2006.
Learning the structure of task-driven human-human
dialogs. In Proceedings of COLING/ACL.
A. Barrett and D. Weld. 1994. Task-decomposition via
plan parsing. In Proceedings of AAAI.
A. Berger, S.D. Pietra, and V.D. Pietra. 1996. A Max-
imum Entropy Approach to Natural Language Pro-
cessing. Computational Linguistics, 22(1):39?71.
N. Blaylock and J. F. Allen. 2006. Hierarchical instan-
tiated goal recognition. In Proceedings of the AAAI
Workshop on Modeling Others from Observations.
D. Bohus and A. Rudnicky. 2003. RavenClaw: Dialog
management using hierarchical task decomposition
and an expectation agenda. In Proceedings of Eu-
rospeech.
H.H. Bui. 2003. A general model for online probabal-
istic plan recognition. In Proceedings of IJCAI.
S. Carberry. 2001. Techniques for plan recogni-
tion. User Modeling and User-Adapted Interaction,
11(1?2):31?48.
M. Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguis-
tics, 29(4):589?638.
F. Costa, V. Lombardo, P. Frasconi, and G. Soda. 2001.
Wide coverage incremental parsing by learning at-
tachment preferences. In Proceedings of the Con-
ference of the Italian Association for Artificial Intel-
ligence (AIIA).
D. Cristea. 2000. An incremental discourse parser ar-
chitecture. In Proceedings of the 2nd International
Conference on Natural Language Processing.
K. Forbes, E. Miltsakaki, R. Prasad, A. Sarkar,
A. Joshi, and B. Webber. 2003. D-LTAG system:
Discourse parsing with a lexicalized tree-adjoining
grammar. Journal of Logic, Language and Informa-
tion, 12(3):261?279.
B.J. Grosz and C.L. Sidner. 1986. Attention, inten-
tions and the structure of discourse. Computational
Linguistics, 12(3):175?204.
P. Haffner. 2006. Scaling large margin classi?ers for
spoken language understanding. Speech Communi-
cation, 48(3?4):239?261.
J. Hall, J. Nivre, and J. Nilsson. 2006. Discriminative
classi?ers for deterministic dependency parsing. In
Proceedings of COLING/ACL.
P. Harrison, S. Abney, D. Fleckenger, C. Gdaniec,
R. Grishman, D. Hindle, B. Ingria, M. Marcus,
B. Santorini, and T. Strzalkowski. 1991. Evaluating
syntax performance of parser/grammars of English.
In Proceedings of the Workshop on Evaluating Nat-
ural Language Processing Systems, ACL.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating discourse structures for written texts. In
Proceedings of COLING.
D. Litman and J. Allen. 1987. A plan recognition
model for subdialogs in conversations. Cognitive
Science, 11(2):163?200.
K. Lochbaum. 1998. A collaborative planning model
of intentional structure. Computational Linguistics,
24(4):525?572.
M. Poesio and A. Mikheev. 1998. The predictive
power of game structure in dialogue act recognition:
experimental results using maximum entropy esti-
mation. In Proceedings of ICSLP.
L. Polanyi, C. Culy, M. van den Berg, G. L. Thione, and
D. Ahn. 2004. A rule based approach to discourse
parsing. In Proceedings of SIGdial.
D.V. Pynadath and M.P. Wellman. 2000. Probabilistic
state-dependent grammars for plan recognition. In
Proceedings of UAI.
A. Ratnaparkhi. 1997. A linear observed time statis-
tical parser based on maximum entropy models. In
Proceedings of EMNLP.
C. Rich and C.L. Sidner. 1997. COLLAGEN: When
agents collaborate with people. In Proceedings of
the First International Conference on Autonomous
Agents.
K. Sagae and A. Lavie. 2006. A best-?rst proba-
bilistic shift-reduce parser. In Proceedings of COL-
ING/ACL.
R. Sera?n and B. Di Eugenio. 2004. FLSA: Extending
latent semantic analysis with features for dialogue
act classi?cation. In Proceedings of ACL.
C.L. Sidner. 1985. Plan parsing for intended re-
sponse recognition in discourse. Computational In-
telligence, 1(1):1?10.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL/HLT.
C. Sporleder and A. Lascarides. 2004. Combining hi-
erarchical clustering and machine learning to pre-
dict high-level discourse structure. In Proceedings
of COLING.
102
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 238?245,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Effects of Word Confusion Networks on Voice Search
Junlan Feng, Srinivas Bangalore
AT&T Labs-Research
Florham Park, NJ, USA
junlan,srini@research.att.com
Abstract
Mobile voice-enabled search is emerging
as one of the most popular applications
abetted by the exponential growth in the
number of mobile devices. The automatic
speech recognition (ASR) output of the
voice query is parsed into several fields.
Search is then performed on a text corpus
or a database. In order to improve the ro-
bustness of the query parser to noise in the
ASR output, in this paper, we investigate
two different methods to query parsing.
Both methods exploit multiple hypotheses
from ASR, in the form of word confusion
networks, in order to achieve tighter cou-
pling between ASR and query parsing and
improved accuracy of the query parser. We
also investigate the results of this improve-
ment on search accuracy. Word confusion-
network based query parsing outperforms
ASR 1-best based query-parsing by 2.7%
absolute and the search performance im-
proves by 1.8% absolute on one of our data
sets.
1 Introduction
Local search specializes in serving geographi-
cally constrained search queries on a structured
database of local business listings. Most text-
based local search engines provide two text fields:
the ?SearchTerm? (e.g. Best Chinese Restau-
rant) and the ?LocationTerm? (e.g. a city, state,
street address, neighborhood etc.). Most voice-
enabled local search dialog systems mimic this
two-field approach and employ a two-turn dia-
log strategy. The dialog system solicits from the
user a LocationTerm in the first turn followed by a
SearchTerm in the second turn (Wang et al, 2008).
Although the two-field interface has been
widely accepted, it has several limitations for mo-
bile voice search. First, most mobile devices are
location-aware which obviates the need to spec-
ify the LocationTerm. Second, it?s not always
straightforward for users to be aware of the dis-
tinction between these two fields. It is com-
mon for users to specify location information in
the SearchTerm field. For example, ?restaurants
near Manhattan? for SearchTerm and ?NY NY?
for LocationTerm. For voice-based search, it is
more natural for users to specify queries in a sin-
gle utterance1. Finally, many queries often con-
tain other constraints (assuming LocationTerm is a
constraint) such as that deliver in restaurants that
deliver or open 24 hours in night clubs open 24
hours. It would be very cumbersome to enumerate
each constraint as a different text field or a dialog
turn. An interface that allows for specifying con-
straints in a natural language utterance would be
most convenient.
In this paper, we introduce a voice-based search
system that allows users to specify search requests
in a single natural language utterance. The out-
put of ASR is then parsed by a query parser
into three fields: LocationTerm, SearchTerm,
and Filler. We use a local search engine,
http://www.yellowpages.com/, which accepts the
SearchTerm and LocationTerm as two query fields
and returns the search results from a business list-
ings database. We present two methods for pars-
ing the voice query into different fields with par-
ticular emphasis on exploiting the ASR output be-
yond the 1-best hypothesis. We demonstrate that
by parsing word confusion networks, the accuracy
of the query parser can be improved. We further
investigate the effect of this improvement on the
search task and demonstrate the benefit of tighter
coupling of ASR and the query parser on search
accuracy.
The paper outline is as follows. In Section 2, we
discuss some of the related threads of research rel-
evant for our task. In Section 3, we motivate the
need for a query parsing module in voice-based
search systems. We present two different query
parsing models in Section 4 and Section 5 and dis-
cuss experimental results in Section 6. We sum-
marize our results in Section 7.
1Based on the returned results, the query may be refined
in subsequent turns of a dialog.
238
2 Related Work
The role of query parsing can be considered as
similar to spoken language understanding (SLU)
in dialog applications. However, voice-based
search systems currently do not have SLU as a
separate module, instead the words in the ASR
1-best output are directly used for search. Most
voice-based search applications apply a conven-
tional vector space model (VSM) used in infor-
mation retrieval systems for search. In (Yu et al,
2007), the authors enhanced the VSM by deem-
phasizing term frequency in Listing Names and
using character level instead of word level uni/bi-
gram terms to improve robustness to ASR errors.
While this approach improves recall it does not
improve precision. In other work (Natarajan et
al., 2002), the authors proposed a two-state hidden
Markov model approach for query understanding
and speech recognition in the same step (Natarajan
et al, 2002).
There are two other threads of research liter-
ature relevant to our work. Named entity (NE)
extraction attempts to identify entities of interest
in speech or text. Typical entities include loca-
tions, persons, organizations, dates, times mon-
etary amounts and percentages (Kubala et al,
1998). Most approaches for NE tasks rely on ma-
chine learning approaches using annotated data.
These algorithms include a hidden Markov model,
support vector machines, maximum entropy, and
conditional random fields. With the goal of im-
proving robustness to ASR errors, (Favre et al,
2005) described a finite-state machine based ap-
proach to take as input ASR n-best strings and ex-
tract the NEs. Although our task of query segmen-
tation has similarity with NE tasks, it is arguable
whether the SearchTerm is a well-defined entity,
since a user can provide varied expressions as they
would for a general web search. Also, it is not
clear how the current best performing NE methods
based on maximum entropy or conditional ran-
dom fields models can be extended to apply on
weighted lattices produced by ASR.
The other related literature is natural language
interface to databases (NLIDBs), which had been
well-studied during 1960s-1980s (Androutsopou-
los, 1995). In this research, the aim is to map
a natural language query into a structured query
that could be used to access a database. However,
most of the literature pertains to textual queries,
not spoken queries. Although in its full general-
1?bestWCN Query
ParsedQueryParserSpeech SearchASR
Figure 1: Architecture of a voice-based search sys-
tem
ity the task of NLIDB is significantly more ambi-
tious than our current task, some of the challeng-
ing problems (e.g. modifier attachment in queries)
can also be seen in our task as well.
3 Voice-based Search System
Architecture
Figure 1 illustrates the architecture of our voice-
based search system. As expected the ASR and
Search components perform speech recognition
and search tasks. In addition to ASR and Search,
we also integrate a query parsing module between
ASR and Search for a number of reasons.
First, as can be expected the ASR 1-best out-
put is typically error-prone especially when a user
query originates from a noisy environment. How-
ever, ASR word confusion networks which com-
pactly encode multiple word hypotheses with their
probabilities have the potential to alleviate the er-
rors in a 1-best output. Our motivation to intro-
duce the understanding module is to rescore the
ASR output for the purpose of maximizing search
performance. In this paper, we show promising
results using richer ASR output beyond 1-best hy-
pothesis.
Second, as mentioned earlier, the query parser
not only provides the search engine ?what? and
?where? information, but also segments the query
to phrases of other concepts. For the example we
used earlier, we segment night club open 24 hours
into night club and open 24 hours. Query seg-
mentation has been considered as a key step to
achieving higher retrieval accuracy (Tan and Peng,
2008).
Lastly, we prefer to reuse an existing local
search engine http://www.yellowpages.com/, in
which many text normalization, task specific tun-
ing, business rules, and scalability issues have
been well addressed. Given that, we need a mod-
ule to translate ASR output to the query syntax that
the local search engine supports.
In the next section, we present our proposed ap-
proaches of how we parse ASR output including
ASR 1-best string and lattices in a scalable frame-
work.
239
4 Text Indexing and Search-based Parser
(PARIS)
As we discussed above, there are many potential
approaches such as those for NE extraction we can
explore for parsing a query. In the context of voice
local search, users expect overall system response
time to be similar to that of web search. Con-
sequently, the relatively long ASR latency leaves
no room for a slow parser. On the other hand,
the parser needs to be tightly synchronized with
changes in the listing database, which is updated
at least once a day. Hence, the parser?s training
process also needs to be quick to accomodate these
changes. In this section, we propose a probabilis-
tic query parsing approach called PARIS (parsing
using indexing and search). We start by presenting
a model for parsing ASR 1-best and extend the ap-
proach to consider ASR lattices.
4.1 Query Parsing on ASR 1-best output
4.1.1 The Problem
We formulate the query parsing task as follows.
A 1-best ASR output is a sequence of words:
Q = q1, q2, . . . , qn. The parsing task is to
segment Q into a sequence of concepts. Each
concept can possibly span multiple words. Let
S = s1, s2, . . . , sk, . . . , sm be one of the possible
segmentations comprising of m segments, where
sk = qij = qi, . . . qj , 1 ? i ? j ? n + 1. The
corresponding concept sequence is represented as
C = c1, c2, . . . , ck, . . . , cm.
For a given Q, we are interested in searching
for the best segmentation and concept sequence
(S?, C?) as defined by Equation 1, which is rewrit-
ten using Bayes rule as Equation 2. The prior
probability P (C) is approximated using an h-
gram model on the concept sequence as shown
in Equation 3. We model the segment sequence
generation probability P (S|C) as shown in Equa-
tion 4, using independence assumptions. Finally,
the query terms corresponding to a segment and
concept are generated using Equations 5 and 6.
(S?, C?) = argmax
S,C
P (S,C) (1)
= argmax
S,C
P (C) ? P (S|C) (2)
P (C) = P (c1) ?
m?
i
P (ci|c
i?h+1
i?1 ) (3)
P (S|C) =
m?
k=1
P (sk | ck) (4)
P (sk|ck) = P (q
i
j |ck) (5)
P (qij |ck) = Pck(qi) ?
j?
l=i+1
Pck(ql | q
l?k+1
l?1 ) (6)
To train this model, we only have access to text
query logs from two distinct fields (SearchTerm,
LocationTerm) and the business listing database.
We built a SearchTerm corpus by including valid
queries that users typed to the SearchTerm field
and all the unique business listing names in the
listing database. Valid queries are those queries
for which the search engine returns at least one
business listing result or a business category. Sim-
ilarly, we built a corpus for LocationTerm by con-
catenating valid LocationTerm queries and unique
addresses including street address, city, state, and
zip-code in the listing database. We also built a
small corpus for Filler, which contains common
carrier phrases and stop words. The generation
probabilities as defined in 6 can be learned from
these three corpora.
In the following section, we describe a scalable
way of implementation using standard text indexer
and searcher.
4.1.2 Probabilistic Parsing using Text Search
We use Apache-Lucene (Hatcher and Gospod-
netic, 2004), a standard text indexing and search
engines for query parsing. Lucene is an open-
source full-featured text search engine library.
Both Lucene indexing and search are efficient
enough for our tasks. It takes a few milliseconds
to return results for a common query. Indexing
millions of search logs and listings can be done
in minutes. Reusing text search engines allows
a seamless integration between query parsing and
search.
We changed the tf.idf based document-term
relevancy metric in Lucene to reflect P (qij |ck) us-
ing Relevancy as defined below.
P (qij |ck) = Relevancy(q
i
j , dk) =
tf(qij , dk) + ?
N
(7)
where dk is a corpus of examples we collected for
the concept ck; tf(qij , dk) is referred as the term
frequency, the frequency of qij in dk;N is the num-
ber of entries in dk; ? is an empirically determined
smoothing factor.
240
0 1
gary/0.323
cherry/4.104
dairy/1.442
jerry/3.956
2
crites/0.652
christ/2.857
creek/3.872
queen/1.439
kreep/4.540
kersten/2.045
3springfield/0.303in/1.346 4
springfield/1.367
_epsilon/0.294 5/1
missouri/7.021
Figure 2: An example confusion network for ?Gary crities Springfield Missouri?
Inputs:
? A set of K concepts:C = c1, c2, . . . , cK ,
in this paper, K = 3, c1 =
SearchTerm, c2 = LocationTerm,
c3 = Filler
? Each concept ck associates with a text
corpus: dk. Corpora are indexed using
Lucene Indexing.
? A given query: Q = q1, q2, . . . , qn
? A given maximum number of words in a
query segment: Ng
Parsing:
? Enumerate possible segments in Q up to
Ng words long: qij = qi, qi+1, . . . , qj ,
j >= i, |j ? i| < Ng
? Obtain P (qij |ck)) for each pair of ck and
qij using Lucene Search
? Boost P (qij |ck)) based on the position of
qij in the query P (q
i
j |ck) = P (q
i
j |ck) ?
boostck(i, j, n)
? Search for the best segment sequence
and concept sequence using Viterbi
search
Fig.3. Parsing procedure using Text Indexer and
Searcher
pck(q
i
j) =
tf(qii ? dis(i, j), dk) + ?
N ? shift
(8)
When tf(qij , dk) is zero for all concepts, we
loosen the phrase search to be proximity search,
which searches words in qij within a specific dis-
tance. For instance, ?burlington west virginia? ?
5 will find entries that include these three words
within 5 words of each other. tf(qij , dk) is dis-
counted for proximity search. For a given qij , we
allow a distance of dis(i, j) = (j ? i + shift)
words. shift is a parameter that is set empirically.
The discounting formula is given in 8.
Figure 3 shows the procedure we use for pars-
ing. It enumerates possible segments qij of a given
Q. It then obtains P (qij |ck) using Lucene Search.
We boost pck(q
i
j)) based on the position of q
i
j in
Q. In our case, we simply set: boostck(i, j, n) = 3
if j = n and ck = LocationTerm. Other-
wise, boostck(i, j, n) = 1. The algorithm searches
for the best segmentation using the Viterbi algo-
rithm. Out-of-vocabulary words are assigned to c3
(Filler).
4.2 Query Parsing on ASR Lattices
Word confusion networks (WCNs) is a compact
lattice format (Mangu et al, 2000). It aligns a
speech lattice with its top-1 hypothesis, yielding
a ?sausage?-like approximation of lattices. It has
been used in applications such as word spotting
and spoken document retrieval. In the following,
we present our use of WCNs for query parsing
task.
Figure 2 shows a pruned WCN example. For
each word position, there are multiple alternatives
and their associated negative log posterior proba-
bilities. The 1-best path is ?Gary Crites Spring-
field Missouri?. The reference is ?Dairy Queen
in Springfield Missouri?. ASR misrecognized
?Dairy Queen? as ?Gary Crities?. However, the
correct words ?Dairy Queen? do appear in the lat-
tice, though with lower probability. The challenge
is to select the correct words from the lattice by
considering both ASR posterior probabilities and
parser probabilities.
The hypotheses in WCNs have to be reranked
241
by the Query Parser to prefer those that have
meaningful concepts. Clearly, each business name
in the listing database corresponds to a single con-
cept. However, the long queries from query logs
tend to contain multiple concepts. For example, a
frequent query is ?night club for 18 and up?. We
know ?night club? is the main subject. And ?18
and up? is a constraint. Without matching ?night
club?, any match with ?18 and up? is meaning-
less. The data fortunately can tell us which words
are more likely to be a subject. We rarely see ?18
and up? as a complete query. Given these observa-
tions, we propose calculating the probability of a
query term to be a subject. ?Subject? here specif-
ically means a complete query or a listing name.
For the example shown in Figure 2, we observe the
negative log probability for ?Dairy Queen? to be a
subject is 9.3. ?Gary Crites? gets 15.3. We refer
to this probability as subject likelihood. Given a
candidate query term s = w1, w2, ..wm, we repre-
sent the subject likelihood as Psb(s). In our exper-
iments, we estimate Psb using relative frequency
normorlized by the length of s. We use the follow-
ing formula to combine it with posterior probabil-
ities in WCNs Pcf (s):
P (s) = Pcf (s) ? Psb(s)
?
Pcf (s) =
?
j=1,...,nw
Pcf (wi)
where ? is used to flatten ASR posterior proba-
bilities and nw is the number of words in s. In
our experiments, ? is set to 0.5. We then re-rank
ASR outputs based on P (s). We will report ex-
perimental results with this approach. ?Subject?
is only related to SearchTerm. Considering this,
we parse the ASR 1-best out first and keep the
Location terms extracted as they are. Only word
alternatives corresponding to the search terms are
used for reranking. This also improves speed,
since we make the confusion network lattice much
smaller. In our initial investigations, such an ap-
proach yields promising results as illustrated in the
experiment section.
Another capability that the parser does for both
ASR 1-best and lattices is spelling correction. It
corrects words such as restaurants to restaurants.
ASR produces spelling errors because the lan-
guage model is trained on query logs. We need
to make more efforts to clean up the query log
database, though progresses had been made.
5 Finite-state Transducer-based Parser
In this section, we present an alternate method for
parsing which can transparently scale to take as in-
put word lattices from ASR. We encode the prob-
lem of parsing as a weighted finite-state transducer
(FST). This encoding allows us to apply the parser
on ASR 1-best as well as ASR WCNs using the
composition operation of FSTs.
We formulate the parsing problem as associat-
ing with each token of the input a label indicating
whether that token belongs to one of a business
listing (bl), city/state (cs) or neither (null). Thus,
given a word sequence (W = w1, . . . , wn) output
from ASR, we search of the most likely label se-
quence (T = t1, . . . , tn), as shown in Equation 9.
We use the joint probability P (W,T ) and approx-
imate it using an k-gram model as shown in Equa-
tions 10,11.
T ? = argmax
T
P (T |W ) (9)
= argmax
T
P (W,T ) (10)
= argmax
T
n?
i
P (wi, ti | w
i?k+1
i?1 , t
i?k+1
i?1 )
(11)
A k-gram model can be encoded as a weighted
finite-state acceptor (FSA) (Allauzen et al, 2004).
The states of the FSA correspond to the k-gram
histories, the transition labels to the pair (wi, ti)
and the weights on the arcs are ?log(P (wi, ti |
wi?k+1i?1 , t
i?k+1
i?1 )). The FSA also encodes back-off
arcs for purposes of smoothing with lower order k-
grams. An annotated corpus of words and labels is
used to estimate the weights of the FSA. A sample
corpus is shown in Table 1.
1. pizza bl hut bl new cs york cs new cs
york cs
2. home bl depot bl around null
san cs francisco cs
3. please null show null me null indian bl
restaurants bl in null chicago cs
4. pediatricians bl open null on null
sundays null
5. hyatt bl regency bl in null honolulu cs
hawaii cs
Table 1: A Sample set of annotated sentences
242
The FSA on the joint alphabet is converted into
an FST. The paired symbols (wi, ti) are reinter-
preted as consisting of an input symbol wi and
output symbol ti. The resulting FST (M ) is used
to parse the 1-best ASR (represented as FSTs
(I)), using composition of FSTs and a search for
the lowest weight path as shown in Equation 12.
The output symbol sequence (pi2) from the lowest
weight path is T ?.
T ? = pi2(Bestpath(I ?M)) (12)
Equation 12 shows a method for parsing the 1-
best ASR output using the FST. However, a simi-
lar method can be applied for parsing WCNs. The
WCN arcs are associated with a posterior weight
that needs to be scaled suitably to be comparable
to the weights encoded in M . We represent the re-
sult of scaling the weights in WCN by a factor of
? asWCN?. The value of the scaling factor is de-
termined empirically. Thus the process of parsing
a WCN is represented by Equation 13.
T ? = pi2(Bestpath(WCN
? ?M)) (13)
6 Experiments
We have access to text query logs consisting of 18
million queries to the two text fields: SearchTerm
and LocationTerm. In addition to these logs, we
have access to 11 million unique business listing
names and their addresses. We use the combined
data to train the parameters of the two parsing
models as discussed in the previous sections. We
tested our approaches on three data sets, which in
total include 2686 speech queries. These queries
were collected from users using mobile devices
from different time periods. Labelers transcribed
and annotated the test data using SearchTerm and
LocationTerm tags.
Data Sets Number of WACC
Speech Queries
Test1 1484 70.1%
Test2 544 82.9%
Test3 658 77.3%
Table 2: ASR Performance on three Data Sets
We use an ASR with a trigram-based language
model trained on the query logs. Table 2 shows the
ASR word accuracies on the three data sets. The
accuracy is the lowest on Test1, in which many
users were non-native English speakers and a large
percentage of queries are not intended for local
search.
We measure the parsing performance in terms
of extraction accuracy on the two non-filler slots:
SearchTerm and LocationTerm. Extraction accu-
racy computes the percentage of the test set where
the string identified by the parser for a slot is ex-
actly the same as the annotated string for that slot.
Table 3 reports parsing performance using the
PARIS approach for the two slots. The ?Tran-
scription? columns present the parser?s perfor-
mances on human transcriptions (i.e. word ac-
curacy=100%) of the speech. As expected, the
parser?s performance heavily relies on ASR word
accuracy. We achieved lower parsing perfor-
mance on Test1 compared to other test sets due
to lower ASR accuracy on this test set. The
promising aspect is that we consistently improved
SearchTerm extraction accuracy when usingWCN
as input. The performance under ?Oracle path?
column shows the upper bound for the parser us-
ing the oracle path2 from the WCN. We pruned
the WCN by keeping only those arcs that are
within cthresh of the lowest cost arc between
two states. Cthresh = 4 is used in our experi-
ments. For Test2, the upper bound improvement
is 7.6% (82.5%-74.9%) absolute. Our proposed
approach using pruned WCN achieved 2.7% im-
provement, which is 35% of the maximum poten-
tial gain. We observed smaller improvements on
Test1 and Test3. Our approach did not take advan-
tage of WCN for LocationTerm extraction, hence
we obtained the same performance with WCNs as
using ASR 1-best.
In Table 4, we report the parsing performance
for the FST-based approach. We note that the
FST-based parser on a WCN also improves the
SearchTerm and LocationTerm extraction accu-
racy over ASR 1-best, an improvement of about
1.5%. The accuracies on the oracle path and the
transcription are slightly lower with the FST-based
parser than with the PARIS approach. The per-
formance gap, however, is bigger on ASR 1-best.
The main reason is PARIS has embedded a module
for spelling correction that is not included in the
FST approach. For instance, it corrects nieman to
neiman. These improvements from spelling cor-
rection don?t contribute much to search perfor-
2Oracle text string is the path in the WCN that is closest
to the reference string in terms of Levenshtein edit distance
243
Data Sets SearchTerm Extraction Accuracy LocationTerm Extraction Accuracy
Input ASR WCN Oracle Transcription ASR WCN Oracle Transcription
1-best Path 4 1best Path 4
Test1 60.0% 60.7% 67.9% 94.1% 80.6% 80.6% 85.2% 97.5%
Test2 74.9% 77.6% 82.5% 98.6% 89.0% 89.0% 92.8% 98.7%
Test3 64.7% 65.7% 71.5% 96.7% 88.8% 88.8% 90.5% 97.4%
Table 3: Parsing performance using the PARIS approach
Data Sets SearchTerm Extraction Accuracy LocationTerm Extraction Accuracy
Input ASR WCN Oracle Transcription ASR WCN Oracle Transcription
1-best Path 4 1best Path 4
Test1 56.9% 57.4% 65.6% 92.2% 79.8% 79.8% 83.8% 95.1%
Test2 69.5% 71.0% 81.9% 98.0% 89.4% 89.4% 92.7% 98.5%
Test3 59.2% 60.6% 69.3% 96.1% 87.1% 87.1% 89.3% 97.3%
Table 4: Parsing performance using the FST approach
mance as we will see below, since the search en-
gine is quite robust to spelling errors. ASR gen-
erates spelling errors because the language model
is trained using query logs, where misspellings are
frequent.
We evaluated the impact of parsing perfor-
mance on search accuracy. In order to measure
search accuracy, we need to first collect a ref-
erence set of search results for our test utter-
ances. For this purpose, we submitted the hu-
man annotated two-field data to the search engine
(http://www.yellowpages.com/ ) and extracted the
top 5 results from the returned pages. The re-
turned search results are either business categories
such as ?Chinese Restaurant? or business listings
including business names and addresses. We con-
sidered these results as the reference search results
for our test utterances.
In order to evaluate our voice search system, we
submitted the two fields resulting from the query
parser on the ASR output (1-best/WCN) to the
search engine. We extracted the top 5 results from
the returned pages and we computed the Precision,
Recall and F1 scores between this set of results
and the reference search set. Precision is the ra-
tio of relevant results among the top 5 results the
voice search system returns. Recall refers to the
ratio of relevant results to the reference search re-
sult set. F1 combines precision and recall as: (2
* Recall * Precision) / (Recall + Precision) (van
Rijsbergen, 1979).
In Table 5 and Table 6, we report the search per-
formance using PARIS and FST approaches. The
overall improvement in search performance is not
Data Sets Precision Recall F1
ASR Test1 71.8% 66.4% 68.8%
1-best
Test2 80.7% 76.5% 78.5%
Test3 72.9% 68.8% 70.8%
WCN
Test1 70.8% 67.2% 69.0%
Test2 81.6% 79.0% 80.3%
Test3 73.0% 69.1% 71.0%
Table 5: Search performances using the PARIS ap-
proach
Data Sets Precision Recall F1
ASR Test1 71.6% 64.3% 67.8%
1-best
Test2 79.6% 76.0% 77.7%
Test3 72.9% 67.2% 70.0%
WCN
Test1 70.5% 64.7% 67.5%
Test2 80.3% 77.3% 78.8%
Test3 72.9% 68.1% 70.3%
Table 6: Search performances using the FST ap-
proach
as large as the improvement in the slot accura-
cies between using ASR 1-best and WCNs. On
Test1, we obtained higher recall but lower preci-
sion with WCN resulting in a slight decrease in
F1 score. For both approaches, we observed that
using WCNs consistently improves recall but not
precision. Although this might be counterintu-
itive, given that WCNs improve the slot accuracy
overall. One possible explanation is that we have
observed errors made by the parser using WCNs
are more ?severe? in terms of their relationship to
the original queries. For example, in one particular
244
case, the annotated SearchTerm is ?book stores?,
for which the ASR 1-best-based parser returned
?books? (due to ASR error) as the SearchTerm,
while the WCN-based parser identified ?banks?
as the SearchTerm. As a result, the returned re-
sults from the search engine using the 1-best-based
parser were more relevant compared to the results
returned by the WCN-based parser.
There are few directions that this observation
suggests. First, the weights on WCNs may need
to be scaled suitably to optimize the search per-
formance as opposed to the slot accuracy perfor-
mance. Second, there is a need for tighter cou-
pling between the parsing and search components
as the eventual goal for models of voice search is
to improve search accuracy and not just the slot
accuracy. We plan to investigate such questions in
future work.
7 Summary
This paper describes two methods for query pars-
ing. The task is to parse ASR output including 1-
best and lattices into database or search fields. In
our experiments, these fields are SearchTerm and
LocationTerm for local search. Our first method,
referred to as PARIS, takes advantage of a generic
search engine (for text indexing and search) for
parsing. All probabilities needed are retrieved on-
the-fly. We used keyword search, phrase search
and proximity search. The second approach, re-
ferred to as FST-based parser, which encodes the
problem of parsing as a weighted finite-state trans-
duction (FST). Both PARIS and FST successfully
exploit multiple hypotheses and posterior proba-
bilities from ASR encoded as word confusion net-
works and demonstrate improved accuracy. These
results show the benefits of tightly coupling ASR
and the query parser. Furthermore, we evaluated
the effects of this improvement on search perfor-
mance. We observed that the search accuracy im-
proves using word confusion networks. However,
the improvement on search is less than the im-
provement we obtained on parsing performance.
Some improvements the parser achieves do not
contribute to search. This suggests the need of
coupling the search module and the query parser
as well.
The two methods, namely PARIS and FST,
achieved comparable performances on search.
One advantage with PARIS is the fast training
process, which takes minutes to index millions
of query logs and listing entries. For the same
amount of data, FST needs a number of hours to
train. The other advantage is PARIS can easily
use proximity search to loosen the constrain of N-
gram models, which is hard to be implemented
using FST. FST, on the other hand, does better
smoothing on learning probabilities. It can also
more directly exploit ASR lattices, which essen-
tially are represented as FST too. For future work,
we are interested in ways of harnessing the bene-
fits of the both these approaches.
References
C. Allauzen, M. Mohri, M. Riley, and B. Roark. 2004.
A generalized construction of speech recognition
transducers. In ICASSP, pages 761?764.
I. Androutsopoulos. 1995. Natural language interfaces
to databases - an introduction. Journal of Natural
Language Engineering, 1:29?81.
B. Favre, F. Bechet, and P. Nocera. 2005. Robust
named entity extraction from large spoken archives.
In Proceeding of HLT 2005.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Ac-
tion (In Action series). Manning Publications Co.,
Greenwich, CT, USA.
F. Kubala, R. Schwartz, R. Stone, and R. Weischedel.
1998. Named entity extraction from speech. In in
Proceedings of DARPA Broadcast News Transcrip-
tion and Understanding Workshop, pages 287?292.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: Word error minimiza-
tion and other applications of confusion networks.
Computation and Language, 14(4):273?400, Octo-
ber.
P. Natarajan, R. Prasad, R.M. Schwartz, and
J. Makhoul. 2002. A scalable architecture for di-
rectory assistance automation. In ICASSP 2002.
B. Tan and F. Peng. 2008. Unsupervised query seg-
mentation using generative language models and
wikipedia. In Proceedings of WWW-2008.
C.V. van Rijsbergen. 1979. Information Retrieval.
Boston. Butterworth, London.
Y. Wang, D. Yu, Y. Ju, and A. Alex. 2008. An intro-
duction to voice search. Signal Processing Magzine,
25(3):29?38.
D. Yu, Y.C. Ju, Y.Y. Wang, G. Zweig, and A. Acero.
2007. Automated directory assistance system - from
theory to practice. In Interspeech.
245
Natural Language Generation in Dialog Systems
Owen Rambow Srinivas Bangalore Marilyn Walker
AT&T Labs ? Research
Florham Park, NJ, USA
rambow@research.att.com
ABSTRACT
Recent advances in Automatic Speech Recognition technology have
put the goal of naturally sounding dialog systems within reach.
However, the improved speech recognition has brought to light
a new problem: as dialog systems understand more of what the
user tells them, they need to be more sophisticated at responding
to the user. The issue of system response to users has been ex-
tensively studied by the natural language generation community,
though rarely in the context of dialog systems. We show how re-
search in generation can be adapted to dialog systems, and how
the high cost of hand-crafting knowledge-based generation systems
can be overcome by employing machine learning techniques.
1. DIALOG SYSTEMS AND GENERATION
Recent advances in Automatic Speech Recognition (ASR) tech-
nology have put the goal of naturally sounding dialog systems within
reach.1 However, the improved ASR has brought to light a new
problem: as dialog systems understand more of what the user tells
them, they need to be more sophisticated at responding to the user.
If ASR is limited in quality, dialog systems typically employ a
system-initiative dialog strategy in which the dialog system prompts
the user for specific information and then presents some informa-
tion to the user. In this paradigm, the range of user input at any time
is limited (thus facilitating ASR), and the range of system output at
any time is also limited. However, such interactions are not very
natural. In a more natural interaction, the user can supply more and
different information at any time in the dialog. The dialog system
must then support a mixed-initiative dialog strategy. While this
strategy places greater requirements on ASR, it also increases the
range of system responses and the requirements on their quality in
terms of informativeness and of adaptation to the context.
For a long time, the issue of system response to users has been
studied by the Natural Language Generation (NLG) community,
though rarely in the context of dialog systems. What have emerged
from this work are a ?consensus architecture? [17] which modu-
larizes the large number of tasks performed during NLG in a par-
 
The work reported in this paper was partially funded by DARPA
contract MDA972-99-3-0003.
.
ticular way, and a range of linguistic representations which can be
used in accomplishing these tasks. Many systems have been built
using NLG technology, including report generators [8, 7], system
description generators [10], and systems that attempt to convince
the user of a particular view through argumentation [20, 4].
In this paper, we claim that the work in NLG is relevant to dia-
log systems as well. We show how the results can be incorporated,
and report on some initial work in adapting NLG approaches to di-
alog systems and their special needs. The dialog system we use is
the AT&T Communicator travel planning system.We use machine
learning and stochastic approaches where hand-crafting appears to
be too complex an option, but we also use insight gained during
previous work on NLG in order to develop models of what should
be learned. In this respect, the work reported in this paper differs
from other recent work on generation in the context of dialog sys-
tems [12, 16], which does not modularize the generation process
and proposes a single stochastic model for the entire process. We
start out by reviewing the generation architecture (Section 2). In
Section 3, we discuss the issue of text planning for Communicator.
In Section 4, we summarize some initial work in using machine
learning for sentence planning [19]. Finally, in Section 5 we sum-
marize work using stochastic tree models in generation [2].
2. TEXT GENERATION ARCHITECTURE
.
NLG is conceptualized as a process leading from a high-level
communicative goal to a sequence of communicative acts which
accomplish this communicative goal. A communicative goal is a
goal to affect the user?s cognitive state, e.g., his or her beliefs about
the world, desires with respect to the world, or intentions about
his or her actions in the world. Following (at least) [13], it has
been customary to divide the generation process into three phases,
the first two of which are planning phases. Reiter [17] calls this
architecture a ?consensus architecture? in NLG.
 During text planning, a high-level communicative goal is
broken down into a structured representation of atomic com-
municative goals, i.e., goals that can be attained with a single
communicative act (in language, by uttering a single clause).
The atomic communicative goals may be linked by rhetori-
cal relations which show how attaining the atomic goals con-
tributes to attaining the high-level goal.
 During sentence planning, abstract linguistic resources are
chosen to achieve the atomic communicative goals. This
includes choosing meaning-bearing lexemes, and how the
meaning-bearing lexemes are connected through abstract gram-
matical constructions (basically, lexical predicate-argument
Realizer
Sentence
Planner
Text
Manager
Dialog
Natural Language Generation
Planner
Prosody
Utterance
User
Utterance
System
Assigner
TTS
Natural Language Understanding ASR
Figure 1: Architecture of a dialog system with natural language generation
structure and modification). As a side-effect, sentence plan-
ning also determines sentence boundaries: there need not
be a one-to-one relation between elementary communicative
goals and sentences in the final text.
 During realization, the abstract linguistic resources chosen
during sentence planning are transformed into a surface lin-
guistic utterance by adding function words (such as auxil-
iaries and determiners), inflecting words, and determining
word order. This phase is not a planning phase in that it only
executes decisions made previously, by using grammatical
information about the target language. (Prosody assignment
can be treated as a separate module which follows realization
and which draws on all previous levels of representation. We
do not discuss prosody further in this paper.)
Note that sentence planning and realization use resources spe-
cific to the target-language, while text planning is language-independent
(though presumably it is culture-dependent).
In integrating this approach into a dialog system, we see that the
dialog manager (DM) no longer determines surface strings to send
to the TTS system, as is often the case in current dialog systems.
Instead, the DM determines high-level communicative goals which
are sent to the NLG component. Figure 1 shows a complete archi-
tecture. An advantage of such an architecture is the possibility for
extended plug-and-play: not only can the entire NLG system be
replaced, but also modules within the NLG system, thus allowing
researchers to optimize the system incrementally.
The main objection to the use of NLG techniques in dialog sys-
tems is that they require extensive hand-tuning of existing sys-
tems and approaches for new domains. Furthermore, because of
the relative sophistication of NLG techniques as compared to sim-
pler techniques such as templates, the hand-tuning requires spe-
cialized knowledge of linguistic representations; hand-tuning tem-
plates only requires software engineering skills. An approach based
on machine learning can provide a solution to this problem: it
draws on previous research in NLG and uses the same sophisti-
cated linguistic representations, but it learns the domain-specific
rules that use these representation automatically from data. It is the
goal of our research to show that for dialog systems, approaches
based on machine learning can do as well as or outperform hand-
crafted approaches (be they NLG- or template-based), while requir-
ing far less time for tuning. In the following sections, we summa-
rize the current state of our research on an NLG system for the
Communicator dialog system.
3. TEXT PLANNER
Based on observations from the travel domain of the Communi-
cator system, we have categorized system responses into two types.
The first type occurs during the initial phase when the system is
gathering information from the user. During this phase, the high-
level communicative goals that the system is trying to achieve are
fairly complex: the goals include getting the hearer to supply in-
formation, and to explicitly or implicitly confirm information that
the hearer has just supplied. (These latter goals are often motivated
by the still not perfect quality of ASR.) The second type occurs
when the system has obtained information that matches the user?s
requirements and the options (flights, hotel, or car rentals) need to
be presented to the user. Here, the communicative goal is mainly to
make the hearer believe a certain set of facts (perhaps in conjunc-
tion with a request for a choice among these options).
In the past, NLG systems typically have generated reports or
summaries, for which the high-level communicative goal is of the
type ?make the hearer/reader believe a given set of facts?, as it is
in the second type of system response discussed above. We believe
that NLG work in text planning can be successfully adapted to bet-
ter plan these system responses, taking into account not only the
information to be conveyed but also the dialog context and knowl-
edge about user preferences. We leave this to ongoing work.
In the first type of system response, the high-level communica-
tive goal typically is an unordered list of high-level goals, all of
which need to be achieved with the next turn of the system. An ex-
ample is shown in Figure 2. NLG work in text planning has not ad-
dressed such complex communicative goals in the past. However,
we have found that for the Communicator domain, no text planning
is needed, and that the sentence planner can act directly on a rep-
resentation of the type shown in Figure 2, because the number of
goals is limited (to five, in our studies). We expect that further work
in other dialog domains will require an extension of existing work
in text planning to account better for communicative goals other
than those that simply aim to affect the user?s (hearer?s) beliefs.
implicit-confirm(orig-city:NEWARK)
implicit-confirm(dest-city:DALLAS)
implicit-confirm(month:9)
implicit-confirm(day-number:1)
request(depart-time)
Figure 2: Sample text plan (communicative goals)
Realization Score
What time would you like to travel on September the 1st to Dallas from Newark? 5
Leaving on September the 1st. What time would you like to travel from Newark to Dallas? 4.5
Leaving in September. Leaving on the 1st. What time would you, traveling from Newark
to Dallas, like to leave?
2
Figure 3: Sample alternate realizations of the set of communicative goals shown in Figure 2 suggested by our sentence planner, with
human scores
-
--
-
SP
R
.
.
Sentence Planner
SP
G
H
RealPro
Realizer
Text Plan Chosen sp?tree with associated DSyntS
-
H
Sp?trees with associated DSyntSs
aDialog
System .
Figure 4: Architecture of our sentence planner
4. SENTENCE PLANNER
The principal challenge facing sentence planning for dialog sys-
tems is that there is no good corpus of naturally occurring interac-
tions of the type that need to occur between a dialog system and hu-
man users. This is because of the not-yet perfect ASR and the need
for implicitly or explicitly confirming most or all of the informa-
tion provided by the user. In conversations between two humans,
communicative goals such as implicit or explicit confirmations are
rare, and thus transcripts of human-human interactions in the same
domain cannot be used for the purpose of learning good strategies
to attain communicative goals. And of course we do not want to
use transcripts of existing systems, as we want to improve on their
performance, not mirror it.
We have therefore taken the approach of randomly generating a
set of solutions and having human judges score each of the options.
Each turn of the system is, as described in Section 3, characterized
by a set of high-level goals such as that shown in Figure 2. In the
turns we consider, no text planning is needed. To date, we have
concentrated on the issue of choosing abstract syntactic construc-
tions (rather than lexical choice), so we map each elementary com-
municative goal to a canonical lexico-syntactic structure (called a
DSyntS [11]). We then randomly combine these DSyntSs into
larger DSyntSs using a set of clause-combining operations iden-
tified previously in the literature [14, 18, 5], such as RELATIVE-
CLAUSE, CONJUNCTION, and MERGE.2 The way in which the ele-
mentary DSyntSs are combined is represented in a structure called
the sp-tree. Each sp-tree is then realized using an off-the-shelf re-
alizer, RealPro [9]. Some sample realizations for the same text plan
are shown in Figure 3, along with the average of the scores assigned
by two human judges.

MERGE identifies the verbs and arguments of two lexico-syntactic
structures which differ only in adjuncts. For example, you are flying
from Newark and you are flying on Monday are merged to you are
flying from Newark on Monday.
Using the human scores on each of the up to twenty variants per
turn, we use RankBoost [6] to learn a scoring function which uses
a large set of syntactic and lexical features. The resulting sentence
planner consists of two components: the sentence plan generator
(SPG) which generates candidate sentence plans and the sentence
plan ranker (SPR) which scores each one of them using the rules
learned by RankBoost and which then chooses the best sentence
plan. This architecture is shown in Figure 4.
We compared the performance of our sentence planner to a ran-
dom choice of sentence plans, and to the sentence plans chosen
as top-ranked by the human judges. The mean score of the turns
judged best by the human judges is 4.82 as compared with the
mean of 4.56 for the turns generated by our sentence planner, for
a mean difference of 0.26 (5%) on a scale of 1 to 5. The mean of
the scores of the turns picked randomly is 2.76, for a mean differ-
ence of 1.8 (36%). We validated these results in an independent
experiment in which 60 subjects evaluated different realizations for
a given turn [15]. (Recall that our trainable sentence planner was
trained on the scores of only two human judges.) This evaluation
revealed that the choices made by our trainable sentence planner
were not statistically distinguishable from the choices ranked at the
top by the two human judges. More importantly, they were also not
distinguishable statistically from the current hand-crafted template-
based output of the AT&T Communicator system, which has been
developed and fine-tuned over an extended period of time (the train-
able sentence planner is based on judgments that took about three
person-days to make).
5. REALIZER
At the level of the surface language, the difference in commu-
nicative intention between human-human travel advisory dialogs
and the intended dialogs is not as relevant: we can try and mimic
the human-human transcripts as closely as possible. To show this,
we have performed some initial experiments using FERGUS (Flex-
ible Empiricist-Rationalist Generation Using Syntax), a stochastic
surface realizer which incorporates a tree model and a linear lan-
guage model [2]. We have developed a metric which can be com-
puted automatically from the syntactic dependency structure of the
sentence and the linear order chosen by the realizer, and we have
shown that this metric correlates with human judgments of the fe-
licity of the sentence [3]. Using this metric, we have shown that the
use of both the tree model and the linear language model improves
the quality of the output of FERGUS over the use of only one or
the other of these resources.
FERGUS was originally trained on the Penn Tree Bank cor-
pus consisting of Wall Street Journal text (WSJ). The results on
an initial set of Communicator sentences were not encouraging,
presumably because there are few questions in the WSJ corpus,
and furthermore, specific constructions (including what as deter-
miner) appear to be completely absent (perhaps due to a newspaper
style file). In an initial experiment, we replaced the linear language
model (LM) trained on 1 million words of WSJ by an LM trained
on 10,000 words of human-human travel planning dialogs collected
at CMU. This resulted in a dramatic improvement, with almost all
questions being generated correctly. Since the CMU corpus is rel-
atively small for a LM, we intend to experiment with finding the
ideal combination of WSJ and CMU corpora. Furthermore, we are
currently in the process of syntactically annotating the CMU cor-
pus so that we can derive a tree model as well. We expect further
improvements in quality of the output, and we expect to be able
to exploit the kind of limited lexical variation allowed by the tree
model [1].
6. CONCLUSION
We have discussed how work in NLG can be applied in the
development of dialog systems, and we have presented two ap-
proaches to using stochastic models and machine learning in NLG.
Of course, the final justification for using a more sophisticated NLG
architecture must come from user trials of an integrated system.
However, we suspect that, as in the case of non-dialog NLG sys-
tems, the strongest arguments in favor of NLG often come from
software engineering issues of maintainability and extensibility, which
can be difficult to quantify in research systems.
7. REFERENCES
[1] S. Bangalore and O. Rambow. Corpus-based lexical choice
in natural language generation. In 38th Meeting of the
Association for Computational Linguistics (ACL?00), Hong
Kong, China, 2000.
[2] S. Bangalore and O. Rambow. Exploiting a probabilistic
hierarchical model for generation. In Proceedings of the 18th
International Conference on Computational Linguistics
(COLING 2000), Saarbru?cken, Germany, 2000.
[3] S. Bangalore, O. Rambow, and S. Whittaker. Evaluation
metrics for generation. In Proceedings of the First
International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel, 2000.
[4] G. Carenini and J. Moore. A strategy for generating
evaluative arguments. In Proceedings of the First
International Natural Language Generation Conference
(INLG2000), Mitzpe Ramon, Israel, 2000.
[5] L. Danlos. G-TAG: A lexicalized formalism for text
generation inspired by tree adjoining grammar. In A. Abeille?
and O. Rambow, editors, Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing. CSLI
Publications, 2000.
[6] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient
boosting algorithm for combining preferences. In Machine
Learning: Proceedings of the Fifteenth International
Conference, 1998. Extended version available from
http://www.research.att.com/ schapire.
[7] E. Goldberg, N. Driedger, and R. Kittredge. Using
natural-language processing to produce weather forecasts.
IEEE Expert, pages 45?53, 1994.
[8] K. Kukich. Knowledge-Based Report Generation: A
Knowledge Engineering Approach to Natural Language
Report Generation. PhD thesis, University of Pittsuburgh,
1983.
[9] B. Lavoie and O. Rambow. RealPro ? a fast, portable
sentence realizer. In Proceedings of the Conference on
Applied Natural Language Processing (ANLP?97),
Washington, DC, 1997.
[10] B. Lavoie, O. Rambow, and E. Reiter. Customizable
descriptions of object-oriented models. In Proceedings of the
Conference on Applied Natural Language Processing
(ANLP?97), Washington, DC, 1997.
[11] I. A. Mel?c?uk. Dependency Syntax: Theory and Practice.
State University of New York Press, New York, 1988.
[12] A. H. Oh and A. I. Rudnicky. Stochastic language generation
for spoken dialog systems. In Proceedings of the
ANL/NAACL 2000 Workshop on Conversational Systems,
pages 27?32, Seattle, 2000. ACL.
[13] O. Rambow and T. Korelsky. Applied text generation. In
Third Conference on Applied Natural Language Processing,
pages 40?47, Trento, Italy, 1992.
[14] O. Rambow and T. Korelsky. Applied text generation. In
Proceedings of the Third Conference on Applied Natural
Language Processing, ANLP92, pages 40?47, 1992.
[15] O. Rambow, M. Rogati, and M. Walker. A trainable sentence
planner for spoken dialogue systems. In 39th Meeting of the
Association for Computational Linguistics (ACL?01),
Toulouse, France, 2001.
[16] A. Ratnaparkhi. Trainable methods for surface natural
language generation. In Proceedings of First North American
ACL, Seattle, USA, May 2000.
[17] E. Reiter. Has a consensus NL generation architecture
appeared, and is it psycholinguistically plausible? In
Proceedings of the 7th International Workshop on Natural
Language Generation, pages 163?170, Maine, 1994.
[18] J. Shaw. Clause aggregation using linguistic knowledge. In
Proceedings of the 8th International Workshop on Natural
Language Generation, Niagara-on-the-Lake, Ontario, 1998.
[19] M. Walker, O. Rambow, and M. Rogati. A trainable sentence
planner for spoken dialogue systems. In 2nd Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL?01), Pittsburgh, PA,
2001.
[20] I. Zukerman, R. McConachy, and K. Korb. Bayesian
reasoning in an abductive mechanism for argument
generation and analysis. In AAAI98 Proceedings ? the
Fifteenth National Conference on Artificial Intelligence,
pages 833?838, Madison, Wisconsin, 1998.
Learning Dependency Translation Models 
as Collections of Finite-State Head 
Transducers 
Hiyan Alshawi* 
Shannon Laboratory, AT&T Labs 
Shona Douglas* 
Shannon Laboratory, AT&T Labs 
Srinivas Bangalore* 
Shannon Laboratory, AT&T Labs 
The paper defines weighted head transducers,finite-state machines that perform middle-out string 
transduction. These transducers are strictly more expressive than the special case of standard left- 
to-right finite-state transducers. Dependency transduction models are then defined as collections 
of weighted head transducers that are applied hierarchically. A dynamic programming search 
algorithm is described for finding the optimal transduction of an input string with respect to a 
dependency transduction model. A method for automatically training a dependency transduc- 
tion model from a set of input-output example strings is presented. The method first searches 
for hierarchical alignments of the training examples guided by correlation statistics, and then 
constructs the transitions of head transducers that are consistent with these alignments. Experi- 
mental results are given for applying the training method to translation from English to Spanish 
and Japanese. 
1. Introduction 
We will define a dependency transduction model in terms of a collection of weighted 
head transducers. Each head transducer is a finite-state machine that differs from 
"standard" finite-state transducers in that, instead of consuming the input string left 
to right, it consumes it "middle out" from a symbol in the string. Similarly, the output 
of a head transducer is built up middle out at positions relative to a symbol in the 
output string. The resulting finite-state machines are more expressive than standard 
left-to-right transducers. In particular, they allow long-distance movement with fewer 
states than a traditional finite-state ransducer, a useful property for the translation task 
to which we apply them in this paper. (In fact, finite-state head transducers are capable 
of unbounded movement with a finite number of states.) In Section 2, we introduce 
head transducers and explain how input-output positions on state transitions result 
in middle-out transduction. 
When applied to the problem of translation, the head transducers forming the de- 
pendency transduction model operate on input and output strings that are sequences 
of dependents of corresponding headwords in the source and target languages. The 
dependency transduction model produces ynchronized dependency trees in which 
each local tree is produced by a head transducer. In other words, the dependency 
* 180 Park Avenue, Florham Park, NJ 07932 
t 180 Park Avenue, Florham Park, NJ 07932 
180 Park Avenue, Florham Park, NJ 07932 
@ 2000 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 1 
model applies the head transducers ecursively, imposing a recursive decomposition 
of the source and target strings. A dynamic programming search algorithm finds op- 
timal (lowest total weight) derivations of target strings from input strings or word 
lattices produced by a speech recognizer. Section 3 defines dependency transduction 
models and describes the search algorithm. 
We construct the dependency transduction models for translation automatically 
from a set of unannotated examples, each example comprising a source string and a 
corresponding target string. The recursive decomposition of the training examples 
results from an algorithm for computing hierarchical alignments of the examples, 
described in Section 4.2. This alignment algorithm uses dynamic programming search 
guided by source-target word correlation statistics as described in Section 4.1. 
Having constructed a hierarchical alignment for the training examples, a set of 
head transducer t ansitions are constructed from each example as described in Sec- 
tion 4.3. Finally, the dependency transduction model is constructed by aggregating the 
resulting head transducers and assigning transition weights, which are log probabili- 
ties computed from the training counts by simple maximum likelihood estimation. 
We have applied this method of training statistical dependency transduction mod- 
els in experiments on English-to-Spanish and English-to-Japanese translations of tran- 
scribed spoken utterances. The results of these experiments are described in Section 5; 
our concluding remarks are in Section 6. 
2. Head Transducers 
2.1 Weighted Finite-State Head Transducers 
In this section we describe the basic structure and operation of a weighted head trans- 
ducer. In some respects, this description is simpler than earlier presentations (e.g., 
Alshawi 1996); for example, here final states are simply a subset of the transducer 
states whereas in other work we have described the more general case in which final 
states are specified by a probability distribution. The simplified escription is adequate 
for the purposes of this paper. 
Formally, aweighted head transducer is a 5-tuple: an alphabet W of input symbols; 
an alphabet V of output symbols; a finite set Q of states q0 . . . . .  qs; a set of final states 
F c Q; and a finite set T of state transitions. A transition from state q to state q' has 
the form 
(q,q',w,v,o~,fl, cl
where w is a member of W or is the empty string c; v is a member of V or ?; the integer 
o~ is the input position; the integer fl is the output position; and the real number c is 
the weight or cost of the transition. A transition in which oz = 0 and fl = 0 is called a 
head transition. 
The interpretation f q, q', w, and v in transitions i similar to left-to-right transduc- 
ers, i.e., in transitioning from state q to state qt, the transducer "reads" input symbol 
w and "writes" output symbol v, and as usual if w (or v) is e then no read (respec- 
tively write) takes place for the transition. The difference lies in the interpretation f 
the read position c~ and the write position ft. To interpret the transition positions as 
transducer actions, we consider notional input and output apes divided into squares. 
On such a tape, one square is numbered 0,and the other squares are numbered 1,2 . . . .  
rightwards from square 0, and -1 , -2  . . . .  leftwards from square 0 (Figure 1). 
A transition with input position ~ and output position fl is interpreted as reading 
w from square c~ on the input tape and writing v to square fl of the output tape; if 
square fl is already occupied, then v is written to the next empty square to the left of 
46 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
<q, q' ,  w, v, a, fl,, c> 
@ o p -@ 
C 
I lw l  w0 I I 
-4 -3a,=--2-1 0 1 2 3 4 
-4 -3 -2 -1 0 1 2 ,0=3 4 
Figure 1 
Transition symbols and positions. 
fl if fl < 0, or to the right of fl if fl > 0, and similarly, if input was already read from 
position a, w is taken from the next unread square to the left of a if a < 0 or to the 
right of c~ if a ~ 0. 
The operation of a head transducer is nondeterministic. It starts by taking a head 
transition 
{q, q', w0, v0, 0, 0, c} 
where w0 is one of the symbols (not necessarily the leftmost) in the input string. (The 
valid initial states are therefore implicitly defined as those with an outgoing head 
transition.) w0 is considered to be at square 0 of the input tape and v0 is output at 
square 0 of the output tape. Further state transitions may then be taken until a final 
state in F is reached. For a derivation to be valid, it must read each symbol in the 
input string exactly once. At the end of a derivation, the output string is formed by 
taking the sequence of symbols on the target ape, ignoring any empty squares on this 
tape. 
The cost of a derivation of an input string to an output string by a weighted 
head transducer is the sum of the costs of transitions taken in the derivation. We can 
now define the string-to-string transduction function for a head transducer to be the 
function that maps an input string to the output string produced by the lowest-cost 
valid derivation taken over all initial states and initial symbols. (Formally, the function 
is partial in that it is not defined on an input when there are no derivations or when 
there are multiple outputs with the same minimal cost.) 
In the transducers produced by the training method described in this paper, the 
source and target positions are in the set {-1,0,1},  though we have also used hand- 
coded transducers (Alshawi and Xia 1997) and automatically trained transducers (A1- 
shawl and Douglas 2000) with a larger range of positions. 
2.2 Relationship to Standard FSTs 
The operation of a traditional eft-to-right ransducer can be simulated by a head 
transducer by starting at the leftmost input symbol and setting the positions of the 
first transition taken to a = 0 and fl = 0, and the positions for subsequent transitions 
to o~ = 1 and fl = 1. However, we can illustrate the fact that head transducers are more 
47 
Computational Linguistics Volume 26, Number 1 
a:a  
a:a ~ b:b 
0:0 
Figure 2 
Head transducer to reverse an input string of arbitrary length in the alphabet {a, b}. 
expressive than left-to-right transducers by the case of a finite-state head transducer 
that reverses a string of arbitrary length. (This cannot be performed by a traditional 
transducer with a finite number of states.) 
For example, the head transducer described below (and shown in Figure 2) with 
input alphabet {a, b} will reverse an input string of arbitrary length in that alphabet. 
The states of the example transducer are Q = {ql, q2} and F = {q2}, and it has the 
following transitions (costs are ignored here): 
{ql, q2,a,a,O,O} 
<ql, q2, b, b, 0, 0> 
<q2,q2,a,a,-1,1} 
(q2, q2, b, b, -1,1} 
The only possible complete derivations of the transducer read the input string right 
to left, but write it left to right, thus reversing the string. 
Another similar example is using a finite-state head transducer to convert a palin- 
drome of arbitrary length into one of its component halves. This clearly requires the 
use of an empty string on some of the output transitions. 
3. Dependency Transduction Models 
3.1 Dependency Transduction using Head Transducers 
In this section we describe dependency transduction models, which can be used for 
machine translation and other transduction tasks. These models consist of a collection 
of head transducers that are applied hierarchically. Applying the machines hierarchi- 
cally means that a nonhead transition is interpreted not simply as reading an input- 
output pair (w, v), but instead as reading and writing a pair of strings headed by (w, v) 
according to the derivation of a subnetwork. 
For example, the head transducer shown in Figure 3 can be applied recursively in 
order to convert an arithmetic expression from infix to prefix (Polish) notation (as noted 
by Lewis and Stearns \[1968\], this transduction cannot be performed by a pushdown 
transducer). 
In the case of machine translation, the transducers derive pairs of dependency 
trees, a source language dependency tree and a target dependency tree. A dependency 
tree for a sentence, in the sense of dependency grammar (for example Hays \[1964\] and 
Hudson \[1984\]), is a tree in which the words of the sentence appear as nodes (we do 
not have terminal symbols of the kind used in phrase structure grammar). In such a 
tree, the parent of a node is its head and the child of a node is the node's dependent. 
The source and target dependency trees derived by a dependency transduction 
model are ordered, i.e., there is an ordering on the nodes of each local tree. This 
48 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
b b ~C~ b:b b:b 
Figure 3 
Dependency transduction network mapping bracketed arithmetic expressions from infix to 
prefix notation. 
I I want to make a collect call I 
? , 
\[ quiero hac~ una llamada de cobr~ I 
Figure 4 
Synchronized dependency trees derived for transducing I want to make a collect call into quiero 
hacer una llamada de cobrar. 
means, in particular, that the target sentence can be constructed directly by a simple 
recursive traversal of the target dependency tree. Each pair of source and target rees 
generated is synchronized in the sense to be formalized in Section 4.2. An example is 
given in Figure 4. 
Head transducers and dependency transduction models are thus related as fol- 
lows: Each pair of local trees produced by a dependency transduction derivation is the 
result of a head transducer derivation. Specifically, the input to such a head transducer 
is the string corresponding to the flattened local source dependency tree. Similarly, the 
output of the head transducer derivation is the string corresponding to the flattened 
local target dependency tree. In other words, the head transducer is used to convert 
a sequence consisting of a headword w and its left and right dependent words to a 
sequence consisting of a target word v and its left and right dependent words (Fig- 
ure 5). Since the empty string may appear in a transition in place of a source or target 
symbol, the number of source and target dependents can be different. 
The cost of a derivation produced by a dependency transduction model is the 
sum of all the weights of the head transducer derivations involved. When applying a 
dependency transduction model to language translation, we choose the target string 
obtained by flattening the target ree of the lowest-cost dependency derivation that 
also generates the source string. 
We have not yet indicated what weights to use for head transducer t ansitions. 
The definition of head transducers as such does not constrain these. However, for a 
dependency transduction model to be a statistical model for generating pairs of strings, 
we assign transition weights that are derived from conditional probabilities. Several 
49 
Computational Linguistics Volume 26, Number 1 
Iw1 ..- wk.ll ~?1 ..-'~nl 
Iv ,  
Figure 5 
Head transducer converts the sequences of left and right dependents (wl ... wk-l/ and 
(wk+i ? ? ? w,) of w into left and right dependents (vl... vj-1) and {Vj+I... Vp) of v. 
probabilistic parameterizations can be used for this purpose including the following 
for a transition with headwords w and v and dependent words w' and v': 
P(q', w', v', fllw, v, q). 
Here q and q' are the from-state and to-state for the transition and a and fl are the 
source and target positions, as before. We also need parameters P(q0, ql\]w, v) for the 
probability of choosing a head transition 
(qo, ql, w,v,O,O) 
given this pair of headwords. To start the derivation, we need parameters 
P(roots(wo, vo)) for the probability of choosing w0,v0 as the root nodes of the two 
trees. 
These model parameters can be used to generate pairs of synchronized epen- 
dency trees starting with the topmost nodes of the two trees and proceeding recur- 
sively to the leaves. The probability of such a derivation can be expressed as: 
P( oots(wo, vo) )P(Dwo,vo) 
where P(Dw,v) is the probability of a subderivation headed by w and v, that is 
P(Dw,v) = P(qo, qllw, v) H P(qi+l, Wi, Vi,~i, fli\]w,v, qi)P(Dwi,vl) 
1K i ln  
for a derivation in which the dependents of w and v are generated by n transitions. 
3.2 Transduction Algorithm 
To carry out translation with a dependency transduction model, we apply a dynamic 
programming search to find the optimal derivation. This algorithm can take as input 
either word strings, or word lattices produced by a speech recognizer. The algorithm 
is similar to those for context-free parsing such as chart parsing (Earley 1970) and 
the CKY algorithm (Younger 1967). Since word string input is a special case of word 
lattice input, we need only describe the case of lattices. 
We now present a sketch of the transduction algorithm. The algorithm works 
bottom-up, maintaining a set of configurations. A configuration has the form 
In1, n2, w, v, q, c, t\] 
corresponding to a bottom-up artial derivation currently in state q covering an input 
sequence between nodes nl and n2 of the input lattice, w and v are the topmost 
50 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
nodes in the source and target derivation trees. Only the target ree t is stored in the 
configuration. 
The algorithm first initializes configurations for the input words, and then per- 
forms transitions and optimizations to develop the set of configurations bottom-up: 
Initialization: For each word edge between odes n and n ~ in the lattice 
with source word w0, an initial configuration is constructed for any head 
transition of the form 
(q, q', w0, v0, 0, 0, c} 
Such an initial configuration has the form: 
\[n, n t, w0, v0, q~, c, v0\] 
Transition: We show the case of a transition in which a new configuration 
results from consuming a source dependent wl to the left of a headword 
w and adding the corresponding target dependent Vl to the right of the 
target head v. Other cases are similar. The transition applied is: 
(q, q~, Wl, Vl, -1,1, c'} 
It is applicable when there are the following head and dependent 
configurations: 
\[n2,n3,w,v,q,c,t\] 
\[nl, n2, Wl, Vl, qf, Cl, tl\] 
where the dependent configuration is in a final state qf. The result of 
applying the transition is to add the following to the set of 
configurations: 
In1, n3, w, v, q', c + Cl q- C', t'\] 
where Y is the target dependency tree formed by adding tl as the 
rightmost dependent of t. 
Optimization: We also require a dynamic programming condition to 
remove suboptimal (sub)derivations. Whenever there are two 
configurations 
\[n, n', w, v, q, Cl, tl\] 
\[n, n', w, v, q, C2, t2\] 
and c2 > Cl, the second configuration is removed from the set of 
configurations. 
If, after all applicable transitions have been taken, there are configurations span- 
ning the entire input lattice, then the one with the lowest cost is the optimal derivation. 
When there are no such configurations, we take a pragmatic approach in the trans- 
lation application and simply concatenate the lowest costing of the minimal length 
sequences of partial derivations that span the entire lattice. A Viterbi-like search of 
the graph formed by configurations i used to find the optimal sequence of deriva- 
tions. One of the advantages ofmiddle-out transduction is that robustness i improved 
through such use of partial derivations when no complete derivations are available. 
51 
Computational Linguistics Volume 26, Number 1 
4. Training Method 
Our training method for head transducer models only requires a set of training exam- 
ples. Each example, or bitext, consists of a source language string paired with a target 
language string. In our experiments, the bitexts are transcriptions of spoken English 
utterances paired with their translations into Spanish or Japanese. 
It is worth emphasizing that we do not necessarily expect he dependency repre- 
sentations produced by the training method to be traditional dependency structures 
for the two languages. Instead, the aim is to produce bilingual (i.e., synchronized, see 
below) dependency representations that are appropriate to performing the translation 
task for a specific language pair or specific bilingual corpus. For example, headwords 
in both languages are chosen to force a synchronized alignment (for better or worse) 
in order to simplify cases involving so-called head-switching. This contrasts with one 
of the traditional approaches (e.g., Dorr 1994; Watanabe 1995) to posing the transla- 
tion problem, i.e., the approach in which translation problems are seen in terms of 
bridging the gap between the most natural monolingual representations underlying 
the sentences of each language. 
The training method has four stages: (i) Compute co-occurrence statistics from the 
training data. (ii) Search for an optimal synchronized hierarchical alignment for each 
bitext. (iii) Construct a set of head transducers that can generate these alignments with 
transition weights derived from maximum likelihood estimation. 
4.1 Computing Pairing Costs 
For each source word w in the data set, assign a cost, the translation pairing cost 
c(w, v) for all possible translations v into the target language. These translations of the 
source word may be zero, one, or several target language words (see Section 4.4 for 
discussion of the multiword case). The assignment of translation pairing costs (effec- 
tively a statistical bilingual dictionary) may be done using various statistical measures. 
For this purpose, a suitable statistical function needs to indicate the strength of co- 
occurrence correlation between source and target words, which we assume is indicative 
of carrying the same semantic ontent. Our preferred choice of statistical measure for 
assigning the costs is the ~ correlation measure (Gale and Church 1991). We apply 
this statistic to co-occurrence of the source word with all its possible translations in 
the data set examples. We have found that, at least for our data, this measure leads to 
better performance than the use of the log probabilities of target words given source 
words (cf. Brown et al 1993). 
In addition to the correlation measure, the cost for a pairing includes a distance 
measure component that penalizes pairings proportionately to the difference between 
the (normalized) positions of the source and target words in their respective sentences. 
4.2 Computing Hierarchical Alignments 
As noted earlier, dependency transduction models are generative probabilistic models; 
each derivation generates a pair of dependency trees. Such a pair can be represented 
as a synchronized hierarchical alignment of two strings. A hierarchical alignment 
consists of four functions. The first two functions are an alignment mapping f from 
source words w to target words f(w) (which may be the empty string ~), and an 
inverse alignment mapping from target words v to source words fr(v). The inverse 
mapping is needed to handle mapping of target words to ~; it coincides wi thf  for pairs 
without source ~. The other two functions are a source head-map g mapping source 
dependent words w to their heads g(w) in the source string, and a target head-map 
h mapping target dependent words v to their headwords h(v) in the target string. An 
52 
Alshawi, Bangalore, and Douglas Leaning Dependency Translation Models 
g 
show me nonstop flights to boston 
muestreme los vuelos sin escalas a boston 
g 
show me z ' \  
muestr~me 
nonstop flights to boston 
los vuelos sin escalas a boston 
Figure 6 
A hierarchical alignment: alignment mappings f and f', and head-maps g and h. 
example hierarchical alignment is shown in Figure 6 (f and f '  are shown separately 
for clarity). 
A hierarchical alignment is synchronized (i.e., it corresponds to synchronized e- 
pendency trees) if these conditions hold: 
Nonover lap :  If wl # w2, thenf(wl) f(w2), and similarly, if Vl  V2, thenf'(vl) # 
d'(v2). 
Synchron izat ion :  if f (w) = v and v # e, then f(g(w)) = h(v), and f'(v) = w. 
Similarly, ifd'(v) = w and w # e, thend'(h(v)) = g(w), andf(w) = v. 
Phrase  cont igu i ty :  The image under f of the maximal substring dominated by a 
headword w is a contiguous egment of the target string. 
(Here w and v refer to word tokens not symbols (types). We hope that the context of 
discussion will make the type-token distinction clear in the rest of this article.) The 
hierarchical alignment in Figure 6 is synchronized. 
Of course, translations of phrases are not always transparently related by a hier- 
archical alignment. In cases where the mapping between a source and target phrase is 
unclear (for example, one of the phrases might be an idiom), then the most reasonable 
choice of hierarchical alignment may be for f and f '  to link the heads of the phrases 
only, all the other words being mapped to e, with no constraints on the monolingual 
head mappings h and g. (This is the approach we take to compound lexical pairings, 
discussed in Section 4.4.) 
In the hierarchical alignments produced by the training method described here, 
the source and target strings of a bitext are decomposed into three aligned regions, 
as shown in Figure 7: a head region consisting of headword w in the source and its 
corresponding targetf(w) in the target string, a left substring region consisting of the 
source substring to the left of w and its projection under f on the target string, and 
a right substring region consisting of the source substring to the right of w and its 
projection under f  on the target string. The decomposition is recursive in that the left 
substring region is decomposed around a left headword wl, and the right substring 
53 
Computational Linguistics Volume 26, Number 1 
\[ 
Figure 7 
Decomposing source and target strings around heads w and f(w). 
region is decomposed around a right headword Wr. This process of decomposition 
continues for each left and right substring until it only contains a single word. 
For each bitext there are, in general, multiple such recursive decompositions that 
satisfy the synchronization constraints for hierarchical alignments. We wish to find 
such an alignment hat respects the co-occurrence statistics of bitexts as well as the 
phrasal structure implicit in the source and target strings. For this purpose we define 
a cost function on hierarchical alignments. The cost function is the sum of three terms. 
The first term is the total of all the translation pairing costs c(w,f(w)) of each source 
word w and its translation f(w) in the alignment; the second term is proportional to 
the distance in the source string between dependents wd and their heads g(wa); and the 
third term is proportional to the distance in the target string between target dependent 
words va and their heads h(va). 
The hierarchical alignment hat minimizes this cost function is computed using 
a dynamic programming procedure. In this procedure, the pairing costs are first re- 
trieved for each possible source-target pair allowed by the example. Adjacent source 
substrings are then combined to determine the lowest-cost subalignments for suc- 
cessively larger substrings of the bitext satisfying the constraints tated above. The 
successively larger substrings eventually span the entire source string, yielding the 
optimal hierarchical alignment for the bitext. This procedure has O(n 6) complexity 
in the number of words in the source (or target) sentence. In Alshawi and Douglas 
(2000) we describe a version of the alignment algorithm in which heads may have 
an arbitrary number of dependents, and in which the hierarchical alignments for the 
training corpus are refined by iterative reestimation. 
4.3 Constructing Transducers 
Building a head transducer involves creating appropriate head transducer states and 
tracing hypothesized head transducer transitions between them that are consistent 
with the hierarchical alignment of a bitext. 
The main transitions that are traced in our construction are those that map heads, 
wl and Wr, of the right and left dependent phrases of w to their translations as indi- 
cated by the alignment function f in the hierarchical alignment. The positions of the 
dependents in the target string are computed by comparing the positions off(wt) and 
f(Wr) to the position of v = f(w). 
In order to generalize from instances in the training data, some model states aris- 
ing from different raining instances are shared. In particular, in the construction de- 
scribed here, for a given pair (w, v) there is only one final state. (We have also tried 
using automatic word-clustering techniques to merge states further, but for the lim- 
ited domain corpora we have used so far, the results are inconclusive.) To specify 
54 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
? \oo 
Figure 8 
+1 :+1 -1 :+ 1 
States and transitions constructed for the "swapping" decomposition shown in Figure 7. 
the sharing of states we make use of a one-to-one state-naming function ? from se- 
quences of strings to transducer states. The same state-naming function is used for 
all examples in the data set, ensuring that the transducer fragments recorded for 
the entire data set will form a complete collection of head transducer transition et- 
works. 
Figure 7 shows a decomposition i which w has a dependent to either side, v 
has both dependents to the right, and the alignment is "swapping" (f(wl) is to the 
right off(wr)). The construction for this decomposition case is illustrated in Figure 8 
as part of a finite-state transition diagram, and described in more detail below. (The 
other transition arrows shown in the diagram will arise from other bitext alignments 
containing (w,f(w)) pairings.) Other cases covered by our algorithm (e.g., a single left 
source dependent but no right source dependent, or target dependents on either side 
of the target head) are simple variants. 
The detailed construction is as follows: 
1. Construct a transition from sl = ?(initial) to S 2 = O ' (w, f (w) ,  head) mapping 
the source headword w to the target head f(w) at position 0 in source 
and target. (In our training construction there is only one initial state sl.) 
2. Since the target dependentf(wr) is to the left of target dependentf(wl) 
(and we are restricting positions to {-1, 0, +1}) the Wr transition is 
constructed first in order that the target dependent nearest he head is 
output first. 
Construct a transition from s2 to s3 = c~(w,f(w), swapping, Wr,f(Wr) 
mapping the source dependent Wr at position +1 to the target dependent 
f(Wr) at position +1. 
3. Construct a transition from s3 to s4 = cr(w,f(w),final) mapping the source 
dependent wl at position -1 to the target dependentf(wl) at position +1. 
If instead the alignment had been as in Figure 9, in which the source dependents 
are mapped to target dependents in a parallel rather than swapping configuration 
(the configuration of sin escalas and Boston around flights:los vuelos in Figure 6), the 
construction is the same, except for the following differences: 
. 
. 
Since the target dependentf(wl) is to the left of target dependentf(Wr), 
the wl transition is constructed first in order that the target dependent 
nearest he head is output first. 
The source and target positions are as shown in Figure 10. Instead of 
s ta te  s3, we use a different state ss = ?(w,f(w),parallel, wl,f(wl)). 
55 
Computational Linguistics Volume 26, Number 1 
\[ "'" l \ [ \ ] \ [  ..-4..- \] 
j J  
Figure 9 
Decomposing source and target strings around heads w and f(w)--"parallel'. 
w :f(w) ?\oo 
Figure 10 
-1 :+1 
w / f (w , ) 
+1 :+ 1 
States and transitions constructed for the "parallel" decomposition shown in Figure 9. 
Other states are the same as for the first case. The resulting states and transitions are 
shown in Figure 10. 
After the construction described above is applied to the entire set of aligned bi- 
texts in the training set, the counts for transitions are treated as event observation 
counts of a statistical dependency transduction model with the parameters described 
in Section 3.1. More specifically, the negated logs of these parameters are used as the 
weights for transducer t ansitions. 
4.4 Mult iword Pairings 
In the translation application, source word w and target word v are generalized so 
they can be short substrings (compounds) of the source and target strings. Exam- 
ples of such multiword pairs are show me:muestrdme and nonstop:sin escalas in Fig- 
ure 6. The cost for such pairings still uses the same ~ statistic, now taking the ob- 
servations to be the co-occurrences of the substrings in the training bitexts. How- 
ever, in order that these costs can be comparable to the costs for simple pairings, 
they are multiplied by the number of words in the source substring of the pair- 
ing. 
The use of compounds in pairings does not require any fundamental changes to 
the hierarchical lignment dynamic programming algorithm, which simply produces 
dependency trees with nodes that may be compounds. In the transducer construction 
phase of the training method, one of the words of a compound is taken to be the pri- 
mary or "real" headword. (In fact, we take the least common word of a compound to 
be its head.) An extra chain of transitions i constructed to transduce the other words 
of compounds, if necessary using transitions with epsilon strings. This compilation 
means that the transduction algorithm is unaffected by the use of compounds when 
aligning training data, and there is no need for a separate compound identification 
phase when the transduction algorithm is applied to test data. Some results for dif- 
ferent choices of substring lengths can be found in Alshawi, Bangalore, and Douglas 
(1998). 
56 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
5. Experiments 
5.1 Evaluation Method 
In order to reduce the time required to carry out training evaluation experiments, 
we have chosen two simple, string-based evaluation metrics that can be calculated 
automatically. These metrics, simple accuracy and translation accuracy, are used to 
compare the target string produced by the system against a reference human transla- 
tion from held-out data. 
Simple accuracy is computed by first finding a transformation f one string into 
another that minimizes the total weight of insertions, deletions, and substitutions. (We 
use the same weights for these operations as in the NIST ASR evaluation software 
\[National Institute of Standards and Technology 1997\].) Translation accuracy includes 
transpositions (i.e., movement) of words as well as insertions, deletions, and substi- 
tutions. We regard the latter metric as more appropriate for evaluation of translation 
systems because the simple metric would count a transposition as two errors: an in- 
sertion plus a deletion. (This issue does not arise for speech recognizers because these 
systems do not normally make transposition errors.) 
For the lowest edit-distance transformation between the reference translation and 
system output, if we write I for the number of insertions, D for deletions, S for substi- 
tutions, and R for number of words in the reference translation string, we can express 
simple accuracy as 
simple accuracy = 1 - ( I  + D + S) /R .  
Similarly, if T is the number of transpositions in the lowest weight transformation 
including transpositions, we can express translation accuracy as 
translation accuracy = 1 - ( I  ~ + D ~ + S + T) /R .  
Since a transposition corresponds toan insertion and a deletion, the values of I ~ and D ~ 
for translation accuracy will, in general, be different from I and D in the computation of
simple accuracy. For Spanish, the units for string operations in the evaluation metrics 
are words, whereas for Japanese they are Japanese characters. 
5.2 English-to-Spanish 
The training and test data for the English-to-Spanish experiments were taken from 
a set of transcribed utterances from the Air Travel Information System (ATIS) corpus 
together with a translation of each utterance to Spanish. An utterance is typically asin- 
gle sentence but is sometimes more than one sentence spoken in sequence. Alignment 
search and transduction training was carried out only on bitexts with sentences up 
to length 20, a total of 13,966 training bitexts. The test set consisted of 1,185 held-out 
bitexts at all lengths. Table 1 shows the word accuracy percentages ( ee Section 5.1) 
for the trained model, e2s, against he original held-out ranslations at various source 
sentence l ngths. Scores are also given for a "word-for-word" baseline, sww, in which 
each English word is translated by the most highly correlated Spanish word. 
5.3 English-to-Japanese 
The training and test data for the English-to-Japanese experiments was a set of tran- 
scribed utterances of telephone service customers talking to AT&T operators. These 
utterances, collected from real customer-operator interactions, tend to include frag- 
mented language, restarts, etc. Both training and test partitions were restricted to bi- 
texts with at most 20 English words, giving 12,226 training bitexts and 3,253 held-out 
test bitexts. In the Japanese text, we introduce "word" boundaries that are convenient 
57 
Computational Linguistics Volume 26, Number 1 
Table 1 
Simple accuracy/translation accuracy (percent) for the trained 
English-to-Spanish model (e2s) against he word-for-word baseline 
(sww). 
Length < 5 < 10 G 15 < 20 All 
sww 45.1/45.8 46.7/48.6 46.5/48.2 45.5/47.1 45.2/46.9 
e2s 75.4/75.8 76.3/78.0 75.4/77.0 74.4/76.0 73.3/75.0 
Table 2 
Simple accuracy/translation accuracy as percentages of Japanese 
characters, for the trained English-to-Japanese model (e2j) and the 
word-for-word baseline (jww). 
Length G 5 < 10 G 15 ~ 20 All 
jww 75.8/78.0 45.2/50.4 40.0/45.4 37.2/42.8 37.2/42.8 
e2j 89.2/89.7 74.0/76.6 68.6/72.2 66.4/70.1 66.4/70.1 
for the training process. These word boundaries are parasitic on the word boundaries 
in the English transcriptions: the translators are asked to insert such a word boundary 
between any two Japanese characters that are taken to have arisen from the translation 
of distinct English words. This results in bitexts in which the number of multichar- 
acter Japanese "words" is at most the number of English words. However, as noted 
above, evaluation of the Japanese output is done with Japanese characters, i.e., with 
the Japanese text in its natural format. Table 2 shows the Japanese character accuracy 
percentages for the trained English-to-Japanese model, e2j, and a baseline model, jww, 
which gives each English word its most highly correlated translation. 
5.4 Note on Experimental Setting 
The vocabularies in these English-Spanish and English-Japanese experiments are only 
a few thousand words; the utterances are fairly short (an average of 7.3 words per utter- 
ance) and often contain errors typical of spoken language. So while the domains may 
be representative of task-oriented ialogue settings, further experimentation would 
be needed to assess the effectiveness of our method in situations uch as translat- 
ing newspaper articles. In terms of the training data required, Tsukada et al (1999) 
provide indirect empirical evidence suggesting accuracy can be further improved by 
increasing the size of our training sets, though also suggesting that the learning curve 
is relatively shallow beyond the current size of corpus. 
6. Concluding Remarks 
Formalisms for finite-state and context-free transduction have a long history (e.g., 
Lewis and Stearns 1968; Aho and Ullman 1972), and such formalisms have been ap- 
plied to the machine translation problem, both in the finite-state case (e.g., Vilar et al 
1996) and the context-free case (e.g., Wu 1997). In this paper we have added to this 
line of research by providing a method for automatically constructing fully lexicalized 
statistical dependency transduction models from training examples. 
Automatically training a translation system brings important benefits in terms of 
maintainability, robustness, and reducing expert coding effort as compared with tra- 
58 
Alshawi, Bangalore, and Douglas Learning Dependency Translation Models 
ditional rule-based translation systems (a number of which are described in Hutchins 
and Somers \[1992\]). The reduction of effort results, in large part, from being able 
to do without artificial intermediate representations of meaning; we do not require 
the development of semantic mapping rules (or indeed any rules) or the creation of 
a corpus including semantic annotations. Compared with left-to-right ransduction, 
middle-out ransduction also aids robustness because, when complete derivations are 
not available, partial derivations tend to have meaningful headwords. 
At the same time, we believe our method has advantages over the approach de- 
veloped initially at IBM (Brown et al 1990; Brown et al 1993) for training translation 
systems automatically. One advantage is that our method attempts to model the nat- 
ural decomposition of sentences into phrases. Another is that the compilation of this 
decomposition i to lexically anchored finite-state head transducers produces imple- 
mentations that are much more efficient han those for the IBM model. In particular, 
our search algorithm finds optimal transductions of test sentences in less than "real 
time" on a 300MHz processor, that is, the time to translate an utterance is less than 
the time taken to speak it, an important consideration for our speech translation ap- 
plication. 
References 
Aho, Alfred V. and Jeffrey D. Ullman. 1972. 
The Theory o/Parsing, Translation, and 
Compiling. Prentice-Hall, Englewood 
Cliffs, NJ. 
Alshawi, H. 1996. Head automata for 
speech translation. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2360-2364, Philadelphia, 
PA. 
Alshawi, H., S. Bangalore, and S. Douglas. 
1998. Learning phrase-based head 
transduction models for translation of 
spoken utterances. In Proceedings ofthe 
International Conference on Spoken Language 
Processing, pages 2767-2770, Sydney, 
Australia. 
Alshawi, H. and S. Douglas. 2000. Learning 
dependency transduction models from 
unannotated examples. Philosophical 
Transactions ofthe Royal Society (Series A: 
Mathematical, Physical and Engineering 
Sciences). To appear. 
Alshawi, Hiyan and Fei Xia. 1997. 
English-to-Mandarin speech translation 
with head transducers. In Proceedings of
the Workshop on Spoken Language 
Translation, Madrid, Spain. 
Brown, P. J., J. Cocke, S. A. Della Pietra, 
V. J. Della Pietra, J. Lafferty, R. L. Mercer, 
and P. Rossin. 1990. A statistical approach 
to machine translation. Computational 
Linguistics, 16(2):79-85. 
Brown, P. J., S. A. Della Pietra, V. J. Della 
Pietra, and R. L. Mercer. 1993. The 
mathematics ofmachine translation: 
Parameter estimation. Computational 
Linguistics, 16(2):263-312. 
Dorr, B. J. 1994. Machine translation 
divergences: A formal description and 
proposed solution. Computational 
Linguistics, 20(4):597-634. 
Earley, J. 1970. An efficient context-free 
parsing algorithm. Communications of the 
ACM, 13(2):94-102. 
Gale, W. A. and K. W. Church. 1991. 
Identifying word correspondences in 
parallel texts. In Proceedings ofthe Fourth 
DARPA Speech and Natural Language 
Processing Workshop, ages 152-157, Pacific 
Grove, CA. 
Hays, D. G. 1964. Dependency theory: A 
formalism and some observations. 
Language, 40:511-525. 
Hudson, R. A. 1984. Word Grammar. 
Blackwell, Oxford. 
Hutchins, W. J. and H. L. Somers. 1992. An 
Introduction to Machine Translation. 
Academic Press, New York. 
Lewis, P. M. and R. E. Stearns. 1968. 
Syntax-directed transduction. Journal of the 
Association for Computing Machinery, 
15(3):465-488. 
National Institute of Standards and 
Technology. 1997. Spoken Natural 
Language Processing Group Web page. 
http://www.itl.nist.gov/div894. 
Tsukada, Hajime, Hiyan Alshawi, Shona 
Douglas, and Srinivas Bangalore. 1999. 
Evaluation of machine translation system 
based on a statistical method by using 
spontaneous speech transcription. In
Proceedings ofthe Fall Meeting of the 
Acoustical Society of Japan, pages 115-116, 
September. 
Vilar, J. M., V. M. Jim~nez, J. C. Amengual, 
A. Castellanos, D. Llorens, and E. Vidal. 
1996. Text and speech translation by 
59 
Computational Linguistics Volume 26, Number 1 
means of subsequential transducers. 
Natural Language Engineering, 2(4):351-354. 
Watanabe, Hideo. 1995. A model of a 
bi-directional transfer mechanism using 
rule combination. Machine Translation, 
10(4):269-291. 
Wu, Dekai.1997. Stochastic inversion 
transduction grammars and bilingual 
parsing of parallel corpora. Computational 
Linguistics, 23(3):377-404. 
Younger, D. 1967. Recognition and Parsing 
of Context-Free Languages in Time n 3. 
Information and Control, 10:189-208. 
60 
Robust Understanding in
Multimodal Interfaces
Srinivas Bangalore?
AT&T Labs ? Research
Michael Johnston??
AT&T Labs ? Research
Multimodal grammars provide an effective mechanism for quickly creating integration and
understanding capabilities for interactive systems supporting simultaneous use of multiple
input modalities. However, like other approaches based on hand-crafted grammars, multimodal
grammars can be brittle with respect to unexpected, erroneous, or disfluent input. In this article,
we show how the finite-state approach to multimodal language processing can be extended
to support multimodal applications combining speech with complex freehand pen input, and
evaluate the approach in the context of a multimodal conversational system (MATCH). We
explore a range of different techniques for improving the robustness of multimodal integration
and understanding. These include techniques for building effective language models for speech
recognition when little or no multimodal training data is available, and techniques for robust
multimodal understanding that draw on classification, machine translation, and sequence edit
methods. We also explore the use of edit-based methods to overcome mismatches between the
gesture stream and the speech stream.
1. Introduction
The ongoing convergence of the Web with telephony, driven by technologies such as
voice over IP, broadband Internet access, high-speed mobile data networks, and hand-
held computers and smartphones, enables widespread deployment of multimodal in-
terfaces which combine graphical user interfaces with natural modalities such as speech
and pen. The critical advantage of multimodal interfaces is that they allow user input
and system output to be expressed in the mode or modes to which they are best suited,
given the task at hand, user preferences, and the physical and social environment of
the interaction (Oviatt 1997; Cassell 2001; Andre? 2002; Wahlster 2002). There is also an
increasing body of empirical evidence (Hauptmann 1989; Nishimoto et al 1995; Cohen
et al 1998a; Oviatt 1999) showing user preference and task performance advantages of
multimodal interfaces.
In order to support effective multimodal interfaces, natural language processing
techniques, which have typically operated over linear sequences of speech or text,
? 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com.
?? 180 Park Avenue, Florham Park, NJ 07932. E-mail: johnston@research.att.com.
Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication:
11 July 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
need to be extended in order to support integration and understanding of multimodal
language distributed over multiple different input modes (Johnston et al 1997; Johnston
1998b). Multimodal grammars provide an expressive mechanism for quickly creating
language processing capabilities for multimodal interfaces supporting input modes
such as speech and gesture (Johnston and Bangalore 2000). They support composite
multimodal inputs by aligning speech input (words) and gesture input (represented
as sequences of gesture symbols) while expressing the relation between the speech
and gesture input and their combined semantic representation. Johnston and Bangalore
(2005) show that such grammars can be compiled into finite-state transducers, enabling
effective processing of lattice input from speech and gesture recognition and mutual
compensation for errors and ambiguities.
In this article, we show how multimodal grammars and their finite-state imple-
mentation can be extended to support more complex multimodal applications. These
applications combine speech with complex pen input including both freehand gestures
and handwritten input. More general mechanisms are introduced for representation of
gestures and abstraction over specific content in the gesture stream along with a new
technique for aggregation of gestures. We evaluate the approach in the context of the
MATCH multimodal conversational system (Johnston et al 2002b), an interactive city
guide. In Section 2, we present the MATCH application, the architecture of the system,
and our experimental method for collection and annotation of multimodal data. In
Section 3, we evaluate the baseline approach on the collected data.
The performance of this baseline approach is limited by the use of hand-crafted
models for speech recognition and multimodal understanding. Like other approaches
based on hand-crafted grammars, multimodal grammars can be brittle with respect to
extra-grammatical, erroneous, and disfluent input. This is particularly problematic for
multimodal interfaces if they are to be used in noisy mobile environments. To overcome
this limitation we explore a broad range of different techniques for improving the
robustness of both speech recognition and multimodal understanding components.
For automatic speech recognition (ASR), a corpus-driven stochastic languagemodel
(SLM) with smoothing can be built in order to overcome the brittleness of a grammar-
based language model. However, for multimodal applications there is often very little
training data available and collection and annotation of realistic data can be very
expensive. In Section 5, we examine and evaluate various different techniques for rapid
prototyping of the language model for the speech recognizer, including transforma-
tion of out-of-domain data, grammar sampling, adaptation from wide-coverage gram-
mars, and speech recognition models built on conversational corpora (Switchboard).
Although some of the techniques presented have been reported in the literature, we
are not aware of work comparing the effectiveness of these techniques on the same
domain and using the same data sets. Furthermore, the techniques are general enough
that they can be applied to bootstrap robust gesture recognition models as well. The
presentation here focuses on speech recognitionmodels, partly due to the greater impact
of speech recognition performance compared to gesture recognition performance on the
multimodal application described here. However, in Section 7 we explore the use of
robustness techniques on gesture input.
Although the use of an SLM enables recognition of out-of-grammar utterances,
resulting in improved speech recognition accuracy, this may not help overall system
performance unless the multimodal understanding component itself is made robust
to unexpected inputs. In Section 6, we describe and evaluate several different tech-
niques for making multimodal understanding more robust. Given the success of dis-
criminative classification models in related applications such as natural language call
346
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
routing (Haffner, Tur, and Wright 2003; Gupta et al 2004) and semantic role label-
ing (Punyakanok, Roth, and Yih 2005), we first pursue a purely data-driven approach
where the predicate of a multimodal command and its arguments are determined by
classifiers trained on an annotated corpus of multimodal data. However, given the
limited amount of data available, this approach does not provide an improvement over
the grammar-based approach. We next pursue an approach combining grammar and
data where robust understanding is viewed as a statistical machine translation problem
where out-of-grammar or misrecognized language must be translated to the closest
language the system can understand. This approach provides modest improvement
over the grammar-based approach. Finally we explore an edit-distance approach which
combines grammar-based understanding with knowledge derived from the underlying
application database. Essentially, if a string cannot be parsed, we attempt to identify
the in-grammar string that it is most similar to, just as in the translation approach. This
is achieved by using a finite-state edit transducer to compose the output of the ASR
with the grammar-based multimodal alignment and understanding models. We have
presented these techniques as methods for improving the robustness of the multimodal
understanding by processing the speech recognition output. Given the higher chance of
error in speech recognition compared to gesture recognition, we focus on processing the
speech recognition output to achieve robustmultimodal understanding. However, these
techniques are also equally applicable to gesture recognition output. In Section 7, we
explore the use of edit techniques on gesture input. Section 8 concludes and discusses
the implications of these results.
2. The MATCH Application
Urban environments present a complex and constantly changing body of informa-
tion regarding restaurants, cinema and theater schedules, transportation topology, and
timetables. This information is most valuable if it can be delivered effectively while mo-
bile, since users? needs change rapidly and the information itself is dynamic (e.g., train
times change and shows get cancelled). MATCH (Multimodal Access To City Help) is a
working city guide and navigation system that enables mobile users to access restaurant
and subway information for urban centers such as New York City and Washington,
DC (Johnston et al 2002a, 2002b). MATCH runs stand-alone on a tablet PC (Figure 1) or
in client-server mode across a wireless network. There is also a kiosk version of the
system (MATCHkiosk) (Johnston and Bangalore 2004) which incorporates a life-like
talking head. In this article, we focus on the mobile version of MATCH, in which the
user interacts with a graphical interface displaying restaurant listings and a dynamic
map showing locations and street information. The inputs can be speech, drawings on
the display with a stylus, or synchronous multimodal combinations of the two modes.
The user can ask for reviews, cuisine, phone number, address, or other information
about restaurants and for subway directions to restaurants and locations. The system
responds with graphical callouts on the display, synchronized with synthetic speech
output.
For example, a user can request to see restaurants using the spoken command show
cheap italian restaurants in chelsea. The system will then zoom to the appropriate map
location and show the locations of restaurants on the map. Alternatively, the user could
give the same command multimodally by circling an area on the map and saying show
cheap italian restaurants in this neighborhood. If the immediate environment is too noisy or
public, the same command can be given completely using a pen stylus as in Figure 2,
by circling an area and writing cheap and italian.
347
Computational Linguistics Volume 35, Number 3
Figure 1
MATCH on tablet.
Similarly, if the user says phone numbers for these two restaurants and circles two
restaurants as in Figure 3(a) [A], the system will draw a callout with the restaurant
name and number and say, for example, Time Cafe can be reached at 212-533-7000, for
each restaurant in turn (Figure 3(a) [B]). If the immediate environment is too noisy or
public, the same command can be given completely in pen by circling the restaurants
and writing phone (Figure 3(b)).
The system also provides subway directions. For example, if the user says How do I
get to this place? and circles one of the restaurants displayed on the map the system will
askWhere do you want to go from?. The user can then respond with speech (for example,
25th Street and 3rd Avenue), with pen by writing (for example, 25th St & 3rd Ave), or
multimodally (for example, from here, with a circle gesture indicating the location).
The system then calculates the optimal subway route and generates a multimodal
presentation coordinating graphical presentation of each stage of the route with spoken
instructions indicating the series of actions the user needs to take (Figure 4).
Map-based systems have been a common application area for exploringmultimodal
interaction techniques. One of the reasons for this is the effectiveness and naturalness
of combining graphical input to indicate spatial locations with spoken input to specify
commands. See Oviatt (1997) for a detailed experimental investigation illustrating the
Figure 2
Unimodal pen command.
348
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 3
(a) Two area gestures. (b) Phone command in pen.
Figure 4
Multimodal subway route.
advantages of multimodal input for map-based tasks. Previous map-based multimodal
prototypes can be broken down into two main task domains: map annotation tasks and
information search tasks. Systems such as QuickSet (Cohen et al 1998b) focus on the use
of speech and pen input in order to annotate the location of features on a map. Other
systems use speech and pen input to enable users to search and browse for information
through direct interaction with a map display. In the ADAPT system (Gustafson et al
2000), users browse for apartments using combinations of speaking and pointing. In the
Multimodal Maps system (Cheyer and Julia 1998), users perform travel planning tasks
such as searching for hotels and points of interest. MATCH is an information search
application providing local search capabilities combined with transportation directions.
As such it is most similar to the Multimodal Maps application, though it provides
more powerful and robust language processing and multimodal integration capabili-
ties, while the language processing in the Multimodal Maps application is limited to
simple Verb Object Argument constructions (Cheyer and Julia 1998).
In the next section we explain the underlying architecture and the series of compo-
nents which enable the MATCH user interface.
2.1 MATCH Multimodal Architecture
The underlying architecture that supports MATCH consists of a series of re-usable
components which communicate over IP through a facilitator (MCUBE) (Figure 5).
Figure 6 shows the flow of information among components in the system. In earlier
349
Computational Linguistics Volume 35, Number 3
Figure 5
Multimodal architecture.
versions of the system, communication was over socket connections. In later versions of
the system communication between components uses HTTP.
Users interact with the system through a Multimodal User Interface client (MUI)
which runs in a Web browser. Their speech is processed by the WATSON speech recog-
nition server (Goffin et al 2005) resulting in aweighted lattice of word strings.When the
user draws on the map their ink is captured and any objects potentially selected, such as
currently displayed restaurants, are identified. The electronic ink is broken into a lattice
of strokes and sent to both gesture and handwriting recognition components which
Figure 6
Multimodal architecture flowchart.
350
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
enrich this stroke lattice with possible classifications of strokes and stroke combinations.
The gesture recognizer uses a variant of the template matching approach described
by Rubine (1991). This recognizes symbolic gestures such as lines, areas, points, arrows,
and so on. The stroke lattice is then converted into an ink lattice which represents all of
the possible interpretations of the user?s ink as either symbolic gestures or handwritten
words. The word lattice and ink lattice are integrated and assigned a combinedmeaning
representation by the multimodal integration and understanding component (Johnston
and Bangalore 2000; Johnston et al 2002b). Because we implement this component
using finite-state transducers, we refer to this component as the Multimodal Finite State
Transducer (MMFST). The approach used in the MMFST component for integrating
and interpreting multimodal inputs (Johnston et al 2002a, 2002b) is an extension of
the finite-state approach previously proposed (Bangalore and Johnston 2000; Johnston
and Bangalore 2000, 2005). (See Section 3 for details.) This provides as output a
lattice encoding all of the potential meaning representations assigned to the user?s
input. The meaning is represented in XML, facilitating parsing and logging by other
system components. MMFST can receive inputs and generate outputs using multiple
communication protocols, including the W3C EMMA standard for representation of
multimodal inputs (Johnston et al 2007). Themeaning lattice is flattened to an n-best list
and passed to a multimodal dialog manager (MDM) (Johnston et al 2002b), which re-
ranks the possible meanings in accordance with the current dialogue state. If additional
information or confirmation is required, the MDM enters into a short information
gathering dialogue with the user. Once a command or query is complete, it is passed
to the multimodal generation component (MMGEN), which builds amultimodal score
indicating a coordinated sequence of graphical actions and TTS prompts. This score is
passed back to the MUI. The MUI then coordinates presentation of graphical content
with synthetic speech output using the AT&T Natural Voices TTS engine (Beutnagel
et al 1999). The subway route constraint solver (SUBWAY) is a backend server built for
the prototype which identifies the best route between any two points in the city.
In the given example where the user says phone for these two restaurantswhile circling
two restaurants (Figure 3(a) [A]), assume the speech recognizer returns the lattice in
Figure 7 (Speech). The gesture recognition component also returns a lattice (Figure 7,
Gesture) indicating that the user?s ink is either a selection of two restaurants or a geo-
graphical area. The multimodal integration and understanding component (MMFST)
combines these two input lattices into a lattice representing their combined meaning
(Figure 7, Meaning). This is passed to the multimodal dialog manager (MDM) and from
there to the MUI where it results in the display in Figure 3(a) [B] and coordinated TTS
output.
The multimodal integration and understanding component utilizes a declarative
multimodal grammar which captures both the structure and the interpretation of mul-
timodal and unimodal commands. This formalism and its finite-state implementation
for the MATCH system are explained in detail in Section 3.
This multimodal grammar is in part derived automatically by reference to an un-
derlying ontology of the different kinds of objects in the application. Specific categories
in the ontology, such as located entity, are associated with templates and macros that
are used to automatically generate the necessary grammar rules for the multimodal
grammar and to populate classes in a class-based language model (Section 5). For
example, in order to add support for a new kind of entity, for example, bars, a category
bar is added to the ontology as a subtype of located entity along with specification of the
head nouns used for this new category, the attributes that apply to it, the symbol to use
for it in the gesture representation, and a reference to the appropriate table to find bars
351
Computational Linguistics Volume 35, Number 3
Figure 7
Multimodal example.
in the underlying application database. The appropriate multimodal grammar rules are
then derived automatically as part of the grammar compilation process. Because the
new entity type bar is assigned the ontology category located entity, the grammar will
automatically support deictic reference to bars with expressions such as this place in
addition to the more specific this bar.
In the next section, we explain the data collection procedure we employed in order
to evaluate the system and provide a test set for experimentingwith different techniques
for multimodal integration and understanding.
2.2 Multimodal Data Collection
A corpus of multimodal data was collected in a laboratory setting from a gender-
balanced set of 16 first-time novice users. The subjects were AT&T personnel with
no prior knowledge of the system and no experience building spoken or multimodal
systems. A total of 833 user interactions (218 multimodal/491 speech-only/124 pen-
only) resulting from six sample task scenarios involving finding restaurants of various
types and getting their names, phones, addresses, or reviews, and getting subway
directions between locations were collected and annotated.
Figure 8 shows the experimental set-up. Subjects interacted with the system in a
soundproof room separated from the experimenter by one-way glass. Two video feeds
were recorded, one from a scan converter connected to the system, the other from a
camera located in the subject room, which captured a side-on view of the subject and the
display. The system ran on a Fujitsu tablet computer networked to a desktop PC logging
server located next to the experimenter. The subject?s audio inputs were captured using
both a close-talking headset microphone and a desktop microphone (which captured
both user input and system audio).
As the user interacted with the system a multimodal log in XML format was
captured on the logging server (Ehlen, Johnston, and Vasireddy 2002). The log contains a
detailed record of the subject?s speech and pen inputs and the system?s internal process-
ing steps and responses, with links to the relevant audio files and speech recognition
lattices.
352
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 8
Experimenter and subject set-up.
The experimenter started out each subject with a brief tutorial on the system, show-
ing them the pen and how to click on the display in order to turn on themicrophone. The
tutorial was intentionally vague and broad in scope so the subjects might overestimate
the system?s capabilities and approach problems in new ways. The experimenter then
left the subject to complete, unassisted, a series of six sample task scenarios of vary-
ing complexity. These involved finding restaurants of various types and getting their
names, phones, addresses, or reviews, and getting subway directions between locations.
The task scenarios were presented in a GUI on the tablet next to the map display. In
our pilot testing, we presented users with whole paragraphs describing scenarios. We
found that users would often just rephrase the wording given in the paragraph, thereby
limiting the utility of the data collection. Instead, in this data collection we presented
what the user had to find as a table (Table 1). This approach elicited a broader range of
inputs from users.
After completing the scenarios the user then completed an online questionnaire on
the tablet regarding their experience with the system. This consisted of a series of Likert
scale questions to measure user satisfaction (Walker, Passonneau, and Boland 2001).
After the questionnaire the experimenter came into the experiment room and conducted
an informal qualitative post-experiment feedback interview.
The next phase of the data collection process was to transcribe and annotate the
users? input. Transcription is more complex for multimodal systems than for speech-
only systems because the annotator needs not just to hear what the user said but also
to see what they did. The browser-based construction of the multimodal user interface
enabled us to rapidly build a custom version of the system which serves as an online
multimodal annotation tool (Figure 9). This tool extends the approach described in
Ehlen, Johnston, and Vasireddy (2002) with a graphical interface for construction of
Table 1
Example scenario.
Use MATCH to find the name, address, and phone number of a restaurant matching
the following criteria:
Food Type Location
Vegetarian Union Square
353
Computational Linguistics Volume 35, Number 3
Figure 9
Multimodal log annotation tool.
gesture annotations and a tool for automatically deriving the meaning annotation for
out-of-grammar examples. This tool allows the annotator to dynamically replay the
users? inputs and system responses on the interactive map system itself, turn by turn,
and add annotations to a multimodal log file, encoded in XML. The annotation utilizes
the map component of the system (Figure 9(1)). It provides coordinated playback of
the subject?s audio with their electronic ink, enabling the user to rapidly annotate
multimodal data without having to replay video of the interaction. The user interface
of the multimodal log viewer provides fields for the annotator to transcribe the speech
input, the gesture input, and the meaning. A series of buttons and widgets are provided
to enable the annotator to rapidly and accurately transcribe the user?s gesture and the
appropriate meaning representation without having to remember the specifics of the
gesture and meaning representations (Figure 9(2)).
After transcribing the speech and gesture, the annotator hits a button to confirm
these, and they are recorded in the log and copied down to a second field used for
annotating the meaning of the input (Figure 9(3)). It would be both time consuming and
error-prone to have the annotator code in the meaning representation for each input by
hand. Instead the multimodal understanding system is integrated into the multimodal
annotation tool directly. The interface allows the annotator to adjust the speech and
gesture inputs and send them through the multimodal understander until they get the
meaning they are looking for (Figure 9(4)). When the multimodal understander returns
multiple possibilities an n-best list is presented and the annotator hits the button next
to the appropriate interpretation in order to select it as the annotated meaning. We
found this to be a very effective method of annotating meaning, although it does require
the annotator to have some knowledge of what inputs are acceptable to the system. In
addition to annotating the speech, gesture, and meaning, annotators also checked off a
series of flags indicating various properties of the exchange, such as whether the input
was partial, whether there was a user error, and so on. The result of this effort was a
354
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
corpus of 833 user interactions all fully annotated with speech, gesture, and meaning
transcriptions.
3. Multimodal Grammars and Finite-State Multimodal Language Processing
One of the most critical technical challenges in the development of effective multimodal
systems is that of enabling multimodal language understanding; that is, determining the
user?s intent by integrating and understanding inputs distributed over multiple modes.
In early work on this problem (Neal and Shapiro 1991; Cohen 1991, 1992; Brison and
Vigouroux 1993; Koons, Sparrell, and Thorisson 1993; Wauchope 1994), multimodal un-
derstanding was primarily speech-driven,1 treating gesture as a secondary dependent
mode. In these systems, incorporation of information from the gesture input into the
multimodal meaning is triggered by the appearance of expressions in the speech input
whose reference needs to be resolved, such as definite and deictic noun phrases (e.g.,
this one, the red cube). Multimodal integration was essentially a procedural add-on to a
speech or text understanding system.
Johnston et al (1997) developed a more declarative approach where multimodal
integration is modeled as unification of typed feature structures (Carpenter 1992) as-
signed to speech and gesture inputs. Johnston (1998a, 1998b) utilized techniques from
natural language processing (unification-based grammars and chart parsers) to extend
the unification-based approach and enable handling of inputs with more than one
gesture, visual parsing, and more flexible and declarative encoding of temporal and
spatial constraints. In contrast to the unification-based approaches, which separate
speech parsing and multimodal integration into separate processing stages, Johnston
and Bangalore (2000, 2005) proposed a one-stage approach to multimodal understanding
in which a single grammar specified the integration and understanding of multimodal
language. This avoids the complexity of interfacing between separate speech under-
standing and multimodal parsing components. This approach is highly efficient and
enables tight coupling with speech recognition, because the grammar can be directly
compiled into a cascade of finite-state transducers which can compose directly with
lattices from speech recognition and gesture recognition components.
In this section, we explain how the finite-state approach to multimodal language
understanding can be extended beyond multimodal input with simple pointing ges-
tures made on a touchscreen (as in Johnston and Bangalore [2000, 2005]) to applica-
tions such as MATCH with complex gesture input combining freeform drawings with
handwriting recognition. This involves three significant extensions to the approach: the
development of a gesture representation language for complex pen input combining
freehand drawing with selections and handwriting (Section 3.1); a new more scalable
approach to abstraction over the specific content of gestures within the finite-state
mechanism (Section 3.3); and a new gesture aggregation algorithmwhich enables robust
handling of the integration of deictic phrases with a broad range of different selection
gestures (Section 3.4). In Section 3.2, we illustrate the use of multimodal grammars for
this application with a fragment of the multimodal grammar for MATCH and illustrate
how this grammar is compiled into a cascade of finite-state transducers. Section 3.5
addresses the issue of temporal constraints onmultimodal integration. In Section 3.6, we
describe the multimodal dialog management mechanism used in the system and how
1 To be more precise, they are ?verbal language?-driven, in that either spoken or typed linguistic
expressions are the driving force of interpretation.
355
Computational Linguistics Volume 35, Number 3
Figure 10
Speech lattice.
contextual resolution of deictic expressions is accounted for. In Section 3.7, we evaluate
the performance of this approach to multimodal integration and understanding using
the multimodal data collected as described in Section 2.2.
3.1 Lattice Representations for Gesture and Meaning
One of the goals of our approach to multimodal understanding is to allow for am-
biguities and errors in the recognition of the individual modalities to be overcome
through combination with the other mode (Oviatt 1999; Bangalore and Johnston 2000).
To maximize the potential for error compensation, we maintain multiple recognition
hypotheses by representing input modes as weighted lattices of possible recognition
strings. For speech input, the lattice is a network of word hypotheses with associated
weights. Figure 10 presents a simplified speech lattice from the MATCH application.2
Representation of Gesture. Like speech, gesture input can also be represented as a token
stream, but unlike speech there is no pre-established tokenization of gestures (words
of a gesture language) other than for handwritten words. We have developed a gesture
representation language for pen input which enables representation of symbolic ges-
tures such as areas, lines, and arrows, selection gestures, and handwritten words. This
language covers a broad range of pen-based input for interactive multimodal applica-
tions and can easily be extended to new domains with different gesture symbols. Each
gesture is represented as a sequence of symbols indicating different characteristics of the
gesture. These symbol sequences can be concatenated in order to represent sequences
of gestures and assembled into a lattice representation in order to represent a range of
possible segmentations and interpretations of a sequence of ink strokes. In the MATCH
system, when the user draws on the map, their ink points are captured along with in-
formation about potentially selected items, and these are passed to a gesture processing
component. First, the electronic ink is rotated and scaled and broken into a lattice of
strokes. This stroke lattice is processed by both gesture and handwriting recognizers
to identify possible pen gestures and handwritten words in the ink stream. The results
are combined with selection information to derive the gesture lattice representations
presented in this section. The gesture recognizer uses a variant of the trained template
matching approach described in Rubine (1991). The handwriting recognizer is neural-
network based. Table 2 provides the full set of eight gestures supported and the symbol
sequences used to represent them in the gesture lattice.
2 The lattices in the actual system are weighted but for ease of exposition here we leave out weights in
the figures.
356
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 2
Gesture inputs supported.
For symbolic gestures and selections, the gesture symbol complexes have the basic
form: G FORM MEANING (NUMBER TYPE) SEM. FORM indicates the physical form
of the gesture, and has values such as area, point, line, and arrow. MEANING provides
a rough characterization of the specific meaning of that form; for example, an area can
be either a loc (location) or a sel (selection), indicating the difference between gestures
which delimit a spatial location on the screen and gestures which select specific dis-
played icons. NUMBER and TYPE are only found with sel. They indicate the number
of entities selected (1, 2, 3, many) and the specific type of entity (e.g., rest (restaurant) or
thtr (theater)). The TYPE value mix is used for selections of entities of different types.
Recognition of inputs as handwritten words is also encoded in the gesture lattice. These
are indicated by the sequence G hwWORD. For example, if the user wrote phone number
the gesture sequence would be G hw phone G hw number.
As an example, if the user draws an area on the screen which contains two restau-
rants (as in Figure 3(a) [A]), and the restaurants have associated identifiers id1 and id2,
357
Computational Linguistics Volume 35, Number 3
the gesture lattice will be as in Figure 11. The first two paths through this gesture lattice
represent the ambiguity between the use of the gesture to indicate a spatial location
versus a selection of objects on the screen. As defined in the subsequent multimodal
grammar, if the speech is show me chinese restaurants in this neighborhood then the first
pathwill be chosen. If the speech is tell me about these two restaurants then the second path
will be chosen. The third path represents the recognition hypothesis from handwriting
recognition that this is a handwritten O. If instead the user circles a restaurant and a
theatre, the lattice would be as in Figure 12. If they say tell me about this theater, the third
path will be taken. If they say tell me about these two, the fourth path will be taken. This
allows for cases where a user circles several entities and selects a specific one by type.
The underlying ontology of the application domain plays a critical role in the han-
dling of multimodal expressions. For example, if place in tell me about this place can refer
to either a restaurant or a theatre, then it can be aligned with both gesture symbols in
the multimodal grammar. The noun place is associated in the lexicon with a general type
in the ontology: located entity. When the multimodal grammar is compiled, by virtue of
this type assignment, the expression this place is associated with gesture representations
for all of the specific subtypes of located entity in the ontology, such as restaurant and
theater. The approach also extends to support deictic references to collections of objects
of different types. For example, the noun building is associated in the lexicon with the
type building. In the grammar these buildings is associated with the gesture type building.
If the user selects a collection of objects of different types they are assigned the type
building in the gesture lattice and so the expression these buildingswill pick out that path.
In the application domain of our prototype, where restaurants and theaters are the only
selectable object types, we use a simpler ontology with a single general object type mix
for collections of objects as in Figure 12, and this integrates with spoken phrases such as
these places.
Representation of Meaning. Understanding multimodal language is about extracting the
meaning from multimodal utterances. Although there continue to be endless debates in
Figure 11
Gesture lattice G: Selection of two restaurants.
Figure 12
Gesture lattice G: Restaurant and theater.
358
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 13
Meaning lattice.
Figure 14
XML meaning representation.
linguistics, philosophy, psychology, and neuroscience on what constitutes the meaning
of a natural language utterance (Jackendoff 2002), for the purpose of human?computer
interactive systems, ?meaning? is generally regarded as a representation that can be
executed by an interpreter in order to change the state of the system.
Similar to the input speech and gesture representations, in our approach the output
meaning is also represented in a lattice format. This enables compact representation
of multiple possible interpretations of the user?s inputs and allows for later stages
of processing, such as the multimodal dialog manager, to use contextual information
to rescore the meaning lattice. In order to facilitate logging and parsing by other
components (dialog manager, backend servers), the meaning representation language
is encoded in XML.3 The meaning lattice resulting from combination of speech and
gesture is such that for every path through the lattice, the concatenation of symbols
from that path will result in a well-formed XML expression which can be evaluated with
respect to the underlying application semantics. In the city information application this
includes elements such as<show>which contains a specification of a kind of restaurant
to show, with elements <cuis> (cuisine), <loc> (location), and so on. Figure 13 shows
the meaning lattice that would result when the speech lattice (Figure 10) combines with
the gesture lattice (Figure 11).
The first path through the lattice results from the combination of the speech string
show chinese restaurants here with an area gesture. Concatenating the symbols on this
path, we have the well-formed XML expression in Figure 14.
3.2 Multimodal Grammars and Finite-State Understanding
Context-free grammars have generally been used to encode the sequences of input
tokens (words) in a language which are considered grammatical or acceptable for pro-
cessing in a single input stream. In some cases grammar rules are augmented with oper-
ations used to simultaneously build a semantic representation of an utterance (Ades and
3 In our earlier work (Johnston and Bangalore 2000, 2005), we generated a predicate logic representation,
for example: email([person(id1), organization(id2)]).
359
Computational Linguistics Volume 35, Number 3
Steedman 1982; Pollard and Sag 1994; van Tichelen 2004). Johnston and Bangalore (2000,
2005) present a multimodal grammar formalism which directly captures the relation-
ship between multiple input streams and their combined semantic representation. The
non-terminals in the multimodal grammar are atomic symbols. The multimodal aspects
of the grammar become apparent in the terminals. Each terminal contains three compo-
nentsW:G:M corresponding to the two input streams and one output stream, whereW
is for the spoken language input stream, G is for the gesture input stream, andM is for
the combined meaning output stream. These correspond to the three representations
described in Section 3.1. The epsilon symbol () is used to indicate when one of these
is empty within a given terminal. In addition to the gesture symbols (G area loc ...), G
contains a symbol SEM used as a placeholder for specific content (see Section 3.3).
In Figure 15, we present a fragment of the multimodal grammar used for the
city information application described in this article. This grammar is simplified for
ease of exposition. The rules capture spoken, multimodal, and pen-only commands for
showing restaurants (SHOW), getting information about them (INFO), requesting subway
directions (ROUTE), and zooming the map (ZOOM).
As in Johnston and Bangalore (2000, 2005), this multimodal grammar is com-
piled into a cascade of finite-state transducers. Finite-state machines have been exten-
sively applied to many aspects of language processing, including speech recognition
(Riccardi, Pieraccini, and Bocchieri 1996; Pereira and Riley 1997), phonology (Kartunnen
1991; Kaplan and Kay 1994), morphology (Koskenniemi 1984), chunking (Abney 1991;
Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine transla-
tion (Bangalore and Riccardi 2000). Finite-state models are attractive mechanisms for
language processing since they are (a) efficiently learnable from data; (b) generally
effective for decoding; and (c) associated with a calculus for composingmachines which
allows for straightforward integration of constraints from various levels of language
processing. Furthermore, software implementing the finite-state calculus is available
for research purposes (Noord 1997; Mohri, Pereira, and Riley 1998; Kanthak and Ney
2004; Allauzen et al 2007).
We compile the multimodal grammar into a finite-state device operating over two
input streams (speech and gesture) and one output stream (meaning). The transition
symbols of the FSA correspond to the terminals of the multimodal grammar. For the
sake of illustration here and in the following examples we will only show the portion of
the three-tape finite-state device which corresponds to theDEICNP rule in the grammar
in Figure 15. The corresponding finite-state device is shown in Figure 16. This three-tape
machine is then factored into two transducers: R:G ? W and T :(G?W)? M. The R
machine (e.g., Figure 17) aligns the speech and gesture streams through a composition
with the speech and gesture input lattices (G o (G:W oW)). The result of this operation
is then factored onto a single tape and composed with the T machine (e.g., Figure 18)
in order to map these composite gesture?speech symbols into their combined meaning
(G W:M). Essentially the three-tape transducer is simulated by increasing the alphabet
size by adding composite multimodal symbols that include both gesture and speech
information. A lattice of possible meanings is derived by projecting on the output of
G W:M.
Because the speech and gesture inputs to multimodal integration and under-
standing are represented as lattices, this framework enables mutual compensation
for errors (Johnston and Bangalore 2005); that is, it allows for information from one
modality to be used to overcome errors in the other. For example, a lower confidence
speech result may be selected through the integration process because it is semantically
compatible with a higher confidence gesture recognition result. It is even possible for
360
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 15
Multimodal grammar fragment.
Figure 16
Multimodal three-tape FSA.
the system to overcome errors in both modalities within a single multimodal utterance.
The multimodal composition process prunes out combinations of speech and gesture
which are not semantically compatible and through combination of weights from the
two different modalities it provides a ranking of the remaining semantically compatible
combinations. This aspect of the approach is not the focus of this article and for ease
361
Computational Linguistics Volume 35, Number 3
Figure 17
Gesture/speech alignment transducer.
Figure 18
Gesture/speech to meaning transducer.
of exposition we have left out weights from the examples given. For the sake of com-
pleteness, we provide a brief description of the treatment of weights in the multimodal
integration mechanism. The speech and gesture lattices contain weights. These weights
are combined through the process of finite-state composition, so the finite-state device
resulting from multimodal integration sums the weights from both the input lattices.
In order to account for differences in reliability between the speech lattice weights and
gesture lattice weights, the weights on the lattices are scaled according to a weighting
factor ? learned from held-out training data. The speech lattice is scaled by ? : 0 < ? < 1
and the gesture lattice by 1? ?. Potentially this scaling factor could be dynamically
adapted based on environmental factors and specific users? performance with the indi-
vidual modes, though in the system described here the scaling factor was fixed for the
duration of the experiment.
3.3 Abstraction over Specific Gesture Content
The semantic content associated with gesture inputs frequently involves specific infor-
mation such as a sequence of map coordinates (e.g., for area gestures) or the identities
of selected entities (e.g., restaurants or theaters). As part of the process of multimodal
integration and understanding this specific content needs to be copied from the gesture
stream into the resulting combined meaning. Within the finite-state mechanism, the
onlyway to copy content is to havematching symbols on the gesture input andmeaning
output tapes. It is not desirable and in some cases infeasible to enumerate all of the
different possible pieces of specific content (such as sequences of coordinates) so that
they can be copied from the gesture input tape to the meaning output tape. This will
significantly increase the size of themachine. In order to capturemultimodal integration
using finite-state methods, it is necessary to abstract over certain aspects of the gestural
content.
We introduce here an approach to abstraction over specific gesture content using
a number of additional finite-state operations. The first step is to represent the gesture
input as a transducer I:Gwhere the input side contains gesture symbols and the specific
content and the output side contains the same gesture symbols but a reserved symbol
SEM appears in place of any specific gestural content such as lists of points or entity
identifiers. The I:G transducer for the gesture lattice G in Figure 11 is as shown in
Figure 19.
362
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 19
I:G transducer: Two restaurants.
Figure 20
Gesture lattice G.
In any location in the multimodal grammar (Figure 15) and corresponding three-
tape finite-state device (Figure 16) where content needs to be copied from the gesture
input into the meaning, the transition :SEM:SEM is used. In the T :(G?W)? M
(Figure 17) transducer these transitions are labeled SEM :SEM.
For composition with the G:W gesture/speech alignment transducer (Figure 18) we
take a projection of the output of the I:G transducer. For the example I:G transducer
(Figure 19) the output projection G is as shown in Figure 20. This projection operation
provides the abstraction over the specific content.
After composing the G andWwith G:W, factoring this transducer into an FSA G W
and composing it with T :(G?W)? M, we are left with a transducer G W:M. This
transducer combines a meaning latticeMwith a specification of the gesture and speech
symbols and is used to determine the meaning of G W.
The next step is to factor out the speech information (W), resulting in a transducer
G:M which relates a meaning lattice M to the gestures involved in determining those
meaningsG. This machine can be composedwith the original I:G transducer (I:G o G:M),
yielding a transducer I:M. The final step is to read off meanings from the I:M transducer.
For each path through the meaning lattice we concatenate symbols from the output M
side, unless theM symbol is SEM in which case we take the input I symbol for that arc.
Essentially, the I:G transducer provides an index back from the gesture symbol sequence
associated with each meaning in the meaning lattice to the specific content associated
with each gesture.
For our example case, if the speech these two restaurants is aligned with the gesture
lattice (Figure 20) using R:G ? W (Figure 18) and the result is then factored and
composed with T :(G?W)? M (Figure 17), the resulting G W:M transducer is as in
Figure 21. This is then factored in the G:M transducer Figure 22 and composed with I:G
(Figure 19), yielding the I:M transducer shown in Figure 23.
Figure 21
G W:M transducer.
363
Computational Linguistics Volume 35, Number 3
Figure 22
G:M transducer.
Figure 23
I:M transducer.
The meaning is generated by reading off and concatenating meaning symbols from
the output of the I:M transducer, except for cases in which the output symbol is SEM,
where instead the input symbol is taken. Alternatively, for all arcs in the I:M transducer
where the output is SEM, the input and output symbols can be swapped (because the
input label represents the value of the SEM variable), and then all paths in M will be
the full meanings with the specific content. For our example case this results in the
following meaning representation: <rest> [r12,r15] </rest>. This example was only for
the DEICNP subgrammar. With the full string phone numbers for these two restaurants
the complete resulting meaning is:<cmd><info><type> phone</type><obj><rest>
[r12,r15] </rest> </obj> </info> </cmd>.
A critical advantage of this approach is that, because the gesture lattice itself is used
to store the specific contents, the retrieval mechanism scales as the size and complexity
of the gesture lattice increases. In the earlier approach more and more variable names
are required as lattices increase in size, and in all places in the grammar where content is
copied from gesture to meaning, arcs must be present for all of these variables. Instead
here we leverage the fact that the gesture lattice itself can be used as a data structure
from which the specific contents can be retrieved using the finite-state operation of
composing I:G and G:M. This has the advantage that the algorithms required for ab-
stracting over the specific contents and then reinserting the content are not required,
and these operations are instead captured within the finite-state mechanism. One of the
advantages of this representation of the abstraction is that it encodes not just the type of
each gesture but also its position within the gesture lattice.
3.4 Gesture Aggregation
Johnston (2000) identifies problems involved inmultimodal understanding and integra-
tion of deictic numeral expressions such as these three restaurants. The problem is that for
a particular spoken phrase there are a multitude of different lexical choices of gesture
and combinations of gestures that can be used to select the specified plurality of entities
and all of these need to be integrated with the spoken phrase. For example, as illustrated
in Figure 24, the user might circle all three restaurants with a single pen stroke, circle
each in turn, or circle a group of two and group of one.
In the unification-based approach to multimodal parsing (Johnston 1998b), captur-
ing all of these possibilities in the spoken language grammar significantly increases its
size and complexity and any plural expression is made massively ambiguous. The sug-
gested alternative in Johnston (2000) is to have the deictic numeral subcategorize for a
plurality of the appropriate number and predictively apply a set of gesture combination
rules in order to combine elements of gestural input into the appropriate pluralities.
364
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 24
Multiple ways to select.
In the finite-state approach described here this can be achieved using a process we
term gesture aggregation, which serves as a pre-processing phase on the gesture input
lattice. A gesture aggregation algorithm traverses the gesture input lattice and adds
new sequences of arcs which represent combinations of adjacent gestures of identical
type. The operation of the gesture aggregation algorithm is described in pseudo-code
in Algorithm 1. The function plurality() retrieves the number of entities in a selection
gesture; for example, for a selection of two entities g1, plurality(g1) = 2. The function
type() yields the type of the gesture; for example rest for a restaurant selection gesture.
The function specific content() yields the specific IDs.
Algorithm 1 Gesture aggregation.
P ? the list of all paths through the gesture lattice GL
while P = ? do
p ? pop(P)
G ?the list of gestures in path p
i ? 1
while i < length(G) do
if g[i] and g[i+ 1] are both selection gestures then
if type(g[i]) == type(g[i+ 1]) then
plurality ? plurality(g[i])+ plurality(g[i+ 1)
start ? start state(g[i])
end ? end state(g[i+ 1])
type ? type(g[i])
specific ? append(specific content(g[i]), specific content(g[i+ 1])
g? ? G area sel plurality type specific
Add g? to GL starting at state start and ending at state end
p? ? the path p but with the arcs from start to end replaced with g?
push p? onto P
i ? i+ 1
end if
end if
end while
end while
365
Computational Linguistics Volume 35, Number 3
Essentially what this algorithm does is perform closure on the gesture lattice of a
function which combines adjacent gestures of identical type. For each pair of adjacent
gestures in the lattice which are of identical type, a new gesture is added to the lattice.
This new gesture starts at the start state of the first gesture and ends at the end state of
the second gesture. Its plurality is equal to the sum of the pluralities of the combining
gestures. The specific content for the new gesture (lists of identifiers of selected objects)
results from appending the specific contents of the two combining gestures. This oper-
ation feeds itself so that sequences of more than two gestures of identical type can be
combined.
For our example case of three selection gestures on three different restaurants
as in Figure 24(2), the gesture lattice before aggregation is as in Figure 25(a). After
aggregation the gesture lattice is as in Figure 25(b). Three new sequences of arcs have
been added. The first, from state 3 to state 8, results from the combination of the first
two gestures; the second, from state 14 to state 24, from the combination of the last two
gestures; and the third, from state 3 to state 24, from the combination of all three ges-
tures. The resulting lattice after the gesture aggregation algorithm has applied is shown
in Figure 25(b). Note that minimization has been applied to collapse identical paths.
A spoken expression such as these three restaurants is aligned with the gesture
symbol sequence G area sel 3 rest SEM in the multimodal grammar. This will be able
to combine not just with a single gesture containing three restaurants but also with our
example gesture lattice, since aggregation adds the path: G area sel 3 rest [id1,id2,id3].
We term this kind of aggregation type specific aggregation. The aggregation process
can be extended to support type non-specific aggregation for cases where users refer to sets
of objects of mixed types and select them using multiple gestures. For example in the
case where the user says tell me about these two and circles a restaurant and then a theater,
non-type specific aggregation applies to combine the two gestures into an aggregate of
mixed type G area sel 2 mix [(id1,id2)] and this is able to combine with these two. For
applications with a richer ontology with multiple levels of hierarchy, the type non-specific
aggregation should assign to the aggregate to the lowest common subtype of the set
of entities being aggregated. In order to differentiate the original sequence of gestures
that the user made from the aggregate, paths added through aggregation are assigned
additional cost.
Figure 25(c) shows how these new processes of gesture abstraction and aggregation
integrate into the overall finite-state multimodal language processing cascade. Aggre-
gation applies to the I:G representation of the gesture. A projection G on the I:G is
composed with the gesture/speech alignment transducer R:G ? W, then the result is
composed with the speech lattice. The resulting G:W transducer is factored into an FSA
with a composite alphabet of symbols. This is then composed with the T :(G?W)? M
yielding a result transducer G W:M. The speech is factored out of the input yielding
G:M which can then be composed with I:G, yielding a transducer I:M from which the
final meanings can be read.
3.5 Temporal Constraints on Multimodal Integration
In the approach taken here, temporal constraints for speech and gesture alignment
are not needed within the multimodal grammar itself. Bellik (1995, 1997) provides
examples indicating the importance of precise temporal constraints for proper interpre-
tation of multimodal utterances. Critically, though, Bellik?s examples involve not single
366
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 25
(a) Three gestures. (b) Aggregated lattice. (c) Multimodal language processing cascade.
367
Computational Linguistics Volume 35, Number 3
multimodal utterances but sequences of two utterances.4 The multimodal integration
mechanism andmultimodal grammars described herein enumerate the content of single
turns of user input, be they unimodal or multimodal. The multimodal integration
component and multimodal grammars are not responsible for combination of content
from different modes that occur in separate dialog turns. This is treated as part of dialog
management and reference resolution. Temporal constraints do, however, play a role in
segmenting parallel multimodal input streams into single user turns. This is one of the
functions of the multimodal understanding component. In order to determine which
gestures and speech should be considered part of a single user utterance, a dynamic
timeout adaptation mechanismwas used. In initial versions of the system, fixed timeout
intervals were used on receipt of input from one modality to see if the input is in fact
unimodal or whether input in the other modality is forthcoming. In pilot studies we
determined that the system latency introduced by these timeouts could be significantly
reduced by making the timeouts sensitive to activity in the other mode. In addition
to messages containing the results of speech recognition and gesture processing, we
instrumented the multimodal understanding component (MMFST) to receive events
indicating when the pen first touches the screen (pen-down event) and when the click-
to-speak button is pressed (click-to-speak event). When theMMFST component receives
a speech lattice, if a gesture lattice has already been received then the two lattices are
processed immediately as a multimodal input. If gesture has not yet been received
and there is no pen-down event, the multimodal component waits for a short timeout
interval before interpreting the speech as a unimodal input. If gesture has not been
received, but there has been a pen-down event, the multimodal component will wait
for a longer timeout period for the gesture lattice message to arrive. Similarly, when
gesture is received, if the speech lattice has already been received the two are integrated
immediately. If speech has not yet arrived, and there was no click-to-speak event, then
the systemwill wait for a short timeout before processing the gesture lattice as unimodal
input. If speech has not yet arrived but the click-to-speak event has been received then
the component will wait for the speech lattice to arrive for a longer timeout period.
Longer timeouts are used instead of waiting indefinitely to account for cases where
the speech or gesture processing does not return a result. In pilot testing we determined
that with the adaptive mechanism the short timeouts could be kept as low as a second or
less, significantly reducing system latency for unimodal inputs. With the non-adaptive
mechanism we required timeouts of as much as two to three seconds. For the longer
timeouts we found 15 seconds to be an appropriate time period. A further extension of
this approach would be to make the timeout mechanism adapt to specific users, since
empirical studies have shown that users tend to fall into specific temporal integration
patterns (Oviatt, DeAngeli, and Kuhn 1997).
The adaptive timeout mechanism could also be used with other speech activation
mechanisms. In an ?open microphone? setting where there is no explicit click-to-speak
event, voice activity detection could be used to signal that a speech event is forthcom-
ing. For our application we chose a ?click-to-speak? strategy over ?open microphone?
because it is more robust to noise andmobile multimodal interfaces are intended for use
in environments subject to noise. The other alternative, ?click-and-hold,? where the user
has to hold down a button for the duration of their speech, is also problematic because
it limits the ability of the user to use pen input while they are speaking.
4 See Johnston and Bangalore (2005) for a detailed explanation.
368
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
3.6 Multimodal Dialog Management and Contextual Resolution
Themultimodal dialog manager (MDM) is based on previous work on speech-act based
models of dialog (Rich and Sidner 1998; Stent et al 1999). It uses a Java-based toolkit
for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al
1999). It includes several rule-based processes that operate on a shared state. The state
includes system and user intentions and beliefs, a dialog history and focus space, and
information about the speaker, the domain, and the available modalities. The processes
include interpretation, update, selection, and generation.
The interpretation process takes as input an n-best list of possible multimodal
interpretations for a user input from the MMFST. It rescores them according to a set
of rules that encode the most likely next speech act given the current dialogue context,
and picks the most likely interpretation from the result. The update process updates
the dialogue context according to the system?s interpretation of user input. It augments
the dialogue history, focus space, models of user and system beliefs, and model of user
intentions. It also alters the list of current modalities to reflect those most recently used
by the user.
The selection process determines the system?s next move(s). In the case of a com-
mand, request, or question, it first checks that the input is fully specified (using the
domain ontology, which contains information about required and optional roles for
different types of actions); if it is not, then the system?s next move is to take the
initiative and start an information-gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or answer the question; to do this,
MDM communicates directly with the UI.
The generation process performs template-based generation for simple responses
and updates the system?s model of the user?s intentions after generation. A text plan-
ning component (TEXTPLAN) is used for more complex generation, such as the gener-
ation of comparisons (Walker et al 2002, 2004).
In the case of a navigational query, such as the example in Section 2, MDM first
receives a route query in which only the destination is specified: How do I get to this
place?. In the selection phase it consults the domain ontology and determines that a
source is also required for a route. It adds a request to query the user for the source to the
system?s next moves. This move is selected and the generation process selects a prompt
and sends it to the TTS component. The system asks Where do you want to go from?. If
the user says or writes 25th Street and 3rd Avenue then the MMFST will assign this input
two possible interpretations: either this is a request to zoom the display to the specified
location or it is an assertion of a location. Because theMDMdialogue state indicates that
it is waiting for an answer of the type location, MDM reranks the assertion as the most
likely interpretation. A generalized overlay process (Alexandersson and Becker 2001)
is used to take the content of the assertion (a location) and add it into the partial route
request. The result is determined to be complete. The UI resolves the location to map
coordinates and passes on a route request to the SUBWAY component.
We found this traditional speech-act based dialogue manager worked well for our
multimodal interface. Critical in this was our use of a common semantic representation
across spoken, gestured, andmultimodal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users flexibility in the mode they choose
to advance the dialogue.
One of the roles of the multimodal dialog manager is to handle contextual res-
olution of deictic expressions. Because they can potentially be resolved either by in-
tegration with a gesture, or from context, deictic expressions such as this restaurant are
369
Computational Linguistics Volume 35, Number 3
ambiguous in the multimodal grammar. There will be one path through the grammar
where this expression is associated with a sequence of gesture symbols, such as G
area selection 1 rest r123, and another where it is not associated with any gesture sym-
bols and assigned a semantic representation which indicates that it must be resolved
from context: <rest><discourseref></discourseref></rest>. If at the multimodal un-
derstanding stage there is a gesture of the appropriate type in the gesture lattice,
then the first of these paths will be chosen and the identifier associated with the
gesture will be added to the semantics during the multimodal integration and under-
standing process: <rest>r123</rest>. If there is no gesture, then this restaurant will
be assigned the semantic representation <rest><discourseref></discourseref></rest>
and the dialog manager will attempt to resolve the gesture from the dialog context.
The update process in the multimodal dialog manager maintains a record in the fo-
cus space of the last mention of entities of each semantic type, and the last men-
tioned entity. When the interpretation process receives a semantic representation
containing the marker <rest><discourseref></discourseref></rest> it replaces <dis-
courseref></discourseref> with the identifier of the last-mentioned entity of the type
restaurant.
Cases where the gesture is a low-confidence recognition result, in fact, where
the gesture is spurious and not an intentional input, are handled using back-offs in
the multimodal grammar as follows: In the multimodal grammar, productions are
added which consume a gesture from the gesture lattice, but assign the semantics
<rest><discourseref></discourseref></rest>. Generally these are assigned a higher cost
than paths through the model where the gesture is meaningful, so that these back-
off paths will only be chosen if there is no alternative. In practice for speech and
pen systems of the kind described here, we have found that spurious gestures are
uncommon, though they are likely to be considerably more of a problem for other kinds
of modalities, such as freehand gesture recognized using computer vision.
3.7 Experimental Evaluation
To determine the baseline performance of the finite-state approach to multimodal in-
tegration and understanding, and to collect data for the experiments on multimodal
robustness described in this article, we collected and annotated a corpus of multimodal
data as described in Section 2.2. To enable this initial experiment and data collection,
because no corpus data had already been collected, to bootstrap the process we initially
used a handcrafted multimodal grammar using grammar templates combined with
data from the underlying application database. As shown in Figure 26, the multimodal
grammar can be used to create language models for ASR, align the speech and gesture
results from the respective recognizers, and transform the multimodal utterance to a
meaning representation. All these operations are achieved using finite-state transducer
operations.
For the 709 inputs that involve speech (491 unimodal speech and 218 multimodal)
we calculated the speech recognition accuracy (word and sentence level) for results
using the grammar-based language model projected from the multimodal grammar. We
also calculated a series of measures of concept accuracy on the meaning representations
resulting from taking the results from speech recognition and combining them with the
gesture lattice using the gesture speech alignment model, and then the multimodal un-
derstanding model. The concept accuracy measures: Concept Sentence Accuracy, Predicate
Sentence Accuracy, and Argument Sentence Accuracy are explained subsequently.
370
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 26
Multimodal grammar compilation for different processes of MATCH.
The hierarchically-nested XML representation described in Section 3.1 is effective
for processing by the backend application, but is not well suited for the automated
determination of the performance of the language understanding mechanism. We de-
veloped an approach, similar to Ciaramella (1993) and Boros et al (1996), in which
the meaning representation, in our case XML, is transformed into a sorted flat list of
attribute?value pairs indicating the core contentful concepts of each command. The
attribute?value meaning representation normalizes over multiple different XML rep-
resentations which correspond to the same underlying meaning. For example, phone
and address and address and phone receive different XML representations but the same
attribute?value representation. For the example phone number of this restaurant, the XML
representation is as in Figure 27, and the corresponding attribute?value representation
is as in Figure 28.
Figure 27
XML meaning representation.
cmd:info type:phone object:selection. (1)
Figure 28
Attribute?value meaning representation.
371
Computational Linguistics Volume 35, Number 3
Table 3
ASR and concept accuracy for the grammar-based finite-state approach (10-fold).
Speech recognition Word accuracy 41.6%
Sentence accuracy 38.0%
Understanding Concept sentence accuracy 50.7%
Predicate accuracy 67.2%
Argument accuracy 52.8%
This transformation of the meaning representation allows us to calculate the per-
formance of the understanding component using string-matching metrics parallel to
those used for speech recognition accuracy. Concept Sentence Accuracy measures
the number of user inputs for which the system got the meaning completely right.5
Predicate Sentence Accuracymeasures whether the main predicate of the sentence was
correct (similar to call type in call classification). Argument Sentence Accuracy is an
exact string match between the reference list of arguments and the list of arguments
identified for the command. Note that the reference and hypothesized argument se-
quences are lexicographically sorted before comparison so the order of the arguments
does not matter. We do not utilize the equivalent of word accuracy on the concept token
sequence. The concept-level equivalent of word accuracy is problematic because it can
easily be manipulated by increasing or decreasing the number of tokens used in the
meaning representation.
To provide a baseline for the series of techniques explored in the rest of the article,
we performed recognition and understanding experiments on the same 10 partitions
of the data as in Section 4. The numbers are all averages over all 10 partitions. Table 3
shows the speech recognition accuracy using the grammar-based language model pro-
jected from the multimodal grammar. It also shows the concept accuracy results for the
multimodal?grammar-based finite-state approach to multimodal understanding.
The multimodal grammars described here provide an expressive mechanism for
quickly creating language processing capabilities for multimodal interfaces support-
ing input modes such as speech and pen, but like other approaches based on hand-
crafted grammars, multimodal grammars are brittle with respect to extra-grammatical
or erroneous input. The language model directly projected from the speech portion of
the hand-crafted multimodal grammar is not able to recognize any strings that are not
encoded in the grammar. In our data, 62% of user?s utterances were out of the multi-
modal grammar, a major problem for recognition (as illustrated in Table 3). The poor
ASR performance has a direct impact on concept accuracy. The fact that the score for
concept sentence accuracy is higher than that for sentence accuracy is not unexpected
since recognition errors do not always result in changes in meaning and also to a certain
extent the grammar-based language model will force fit out-of-grammar utterances to
similar in-grammar utterances.
4. Robustness in Multimodal Language Processing
A limitation of grammar-based approaches to (multimodal) language processing is
that the user?s input is often not covered by the grammar and hence fails to receive
an interpretation. This issue is present in grammar-based speech-only dialog systems
5 This metric is called Sentence Understanding in Ciaramella (1993).
372
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
as well. The lack of robustness in such systems is due to limitations in (a) language
modeling and (b) understanding of the speech recognition output.
The brittleness of using a grammar as a language model is typically alleviated by
building SLMs that capture the distribution of the user?s interactions in an application
domain. However, such SLMs are trained on large amounts of spoken interactions
collected in that domain?a tedious task in itself, in speech-only systems, but an often
insurmountable task in multimodal systems. The problem we face is how to make
multimodal systems more robust to disfluent or unexpected multimodal language in
applications for which little or no training data is available. The reliance on multimodal
grammars as a source of data is inevitable in such situations. In Section 5, we explore
and evaluate a range of different techniques for building effective SLMs for spoken
and multimodal systems under constraints of limited training data. The techniques are
presented in the context of SLMs, since spoken language interaction tends to be a dom-
inant mode in our application and has higher perplexity than the gesture interactions.
However, most of these techniques can also be applied to improve the robustness of the
gesture recognition component in applications with higher gesture language perplexity.
The second source of brittleness in a grammar-based multimodal/unimodal inter-
active system is in the assignment of meaning to the multimodal output. The grammar
typically encodes the relation between the multimodal inputs and their meanings. The
assignment of meaning to a multimodal output is achieved by parsing the utterance
using the grammar. In a grammar-based speech-only system, if the language model of
ASR is derived directly from the grammar, then every ASR output can be parsed and
assigned a meaning by the grammar. However, using an SLM results in ASR outputs
that may not be parsable by the grammar and hence cannot be assigned a meaning by
the grammar. Robustness in such cases is achieved by either (a) modifying the parser to
accommodate for unparsable substrings in the input (Ward 1991; Dowding et al 1993;
Allen et al 2001) or (b) modifying the meaning representation to make it learnable as
a classification task using robust machine learning techniques as is done in large scale
human-machine dialog systems (e.g., Gorin, Riccardi, and Wright 1997).
In our grammar-based multimodal system, the grammar serves as the speech-
gesture alignment model and assigns a meaning representation to the multimodal
input. Failure to parse amultimodal input implies that the speech and gesture inputs are
not fused together and consequently may not be assigned a meaning representation. In
order to improve robustness in multimodal understanding, more flexible mechanisms
must be employed in the integration and the meaning-assignment phases. In Section 6,
we explore and evaluate approaches that transform the multimodal input so as to
be parsable by the multimodal grammar, as well as methods that directly map the
multimodal input to the meaning representation without the use of the grammar. We
again present these approaches in the context of transformation of the ASR output,
but they are equally applicable to gesture recognition outputs. Transformation of the
multimodal inputs so as to be parsable by the multimodal grammar directly improves
robustness of multimodal integration and understanding.
Although some of the techniques presented in the next two sections are known
in the literature, they have typically been applied in the context of speech-only dialog
systems and on different application domains. As a result, comparing the strengths and
weaknesses of these techniques is very difficult. By evaluating them on the MATCH
domain, we are able to compare and extend these techniques for robust multimodal
understanding. Other factors such as contextual information including dialog context,
graphical display context, geographical context, as well as meta-information such as
user preferences and profiles, can be used to further enhance the robustness of a
373
Computational Linguistics Volume 35, Number 3
multimodal application. However, here we focus on techniques for improving robust-
ness of multimodal understanding that do not rely on such factors.
5. Robustness of Language Models for Speech Recognition
The problem of speech recognition can be succinctly represented as a search for the
most likely word sequence (w?) through the network created by the composition of a
language of acoustic observations (O), an acoustic model which is a transduction from
acoustic observations to phone sequences (A), a pronunciation model which is a trans-
duction from phone sequences to word sequences (L), and a language model acceptor
(G) (Pereira and Riley 1997) (Equation 2). The language model acceptor encodes the
(weighted) word sequences permitted in an application.
w? = argmax
w
?2(O ? A ? L ? G)(w) (2)
Typically, G is built using either a hand-crafted grammar or using a statistical lan-
guagemodel derived from a corpus of sentences from the application domain. Although
a grammar could bewritten so as to be easily portable across applications, it suffers from
being too prescriptive and has no metric for the relative likelihood of users? utterances.
In contrast, in the data-driven approach a weighted grammar is automatically induced
from a corpus and the weights can be interpreted as a measure of the relative likeli-
hoods of users? utterances. However, the reliance on a domain-specific corpus is one
of the significant bottlenecks of data-driven approaches, because collecting a corpus
specific to a domain is an expensive and time-consuming task, especially formultimodal
applications.
In this section, we investigate a range of techniques for producing a domain-specific
corpus using resources such as a domain-specific grammar as well as an out-of-domain
corpus. We refer to the corpus resulting from such techniques as a domain-specific de-
rived corpus in contrast to a domain-specific collected corpus. We are interested in techniques
that would result in corpora such that the performance of language models trained on
these corpora would rival the performance of models trained on corpora collected for
a specific domain. We investigate these techniques in the context of MATCH. We use
the notation Cd for the corpus, ?d for the language model built using the corpus Cd, and
G?d for the language model acceptor representation of the model ?d which can be used
in Equation (2).
5.1 Language Model Using In-Domain Corpus
We used the MATCH domain corpus from the data collection to build a class-based
trigram language model (?MATCH) using the 709 multimodal and speech-only utterances
as the corpus (CMATCH). We used the names of cuisine types, areas of interest, points of
interest, and neighborhoods as classes when building the trigram language model. The
trigram language model is represented as a weighted finite-state acceptor (Allauzen,
Mohri, and Roark 2003) for speech recognition purposes. The performance of this model
serves as the point of reference to compare the performance of language models trained
on derived corpora.
374
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
5.2 Grammar as Language Model
The multimodal context-free grammar (CFG; a fragment is presented in Section 2 and a
larger fragment is shown in Section 3.2) encodes the repertoire of language and gesture
commands allowed by the system and their combined interpretations. The CFG can
be approximated by a finite state machine (FSM) with arcs labeled with language,
gesture, and meaning symbols, using well-known compilation techniques (Nederhof
1997). Selecting the language symbol of each arc (projecting the FSM on the speech
component) results in an FSM that can be used as the language model acceptor (Ggram)
for speech recognition. Note that the resulting languagemodel acceptor is unweighted if
the grammar is unweighted and suffers from not being robust to language variations in
users? input. However, due to the tight coupling of the grammars used for recognition
and interpretation, every recognized string can be assigned a meaning representation
(though it may not necessarily be the intended interpretation).
5.3 Grammar-Based n-gram Language Model
As mentioned earlier, a hand-crafted grammar typically suffers from the problem of
being too restrictive and inadequate to cover the variations and extra-grammaticality of
users? input. In contrast, an n-gram languagemodel derives its robustness by permitting
all strings over an alphabet, albeit with different likelihoods. In an attempt to provide
robustness to the grammar-based model, we created a corpus (Cgram) of k sentences by
randomly sampling the set of paths of the grammar (Ggram)
6 and built a class-based
n-gram language model (?gram) using this corpus. Although this corpus does not rep-
resent the true distribution of sentences in the MATCH domain, we are able to derive
some of the benefits of n-gram language modeling techniques. Similar approaches have
been presented in Galescu, Ringger, and Allen (1998) and Wang and Acero (2003).
5.4 Combining Grammar and Corpus
A straightforward extension of the idea of sampling the grammar in order to create a
corpus is to select those sentences out of the grammar which make the resulting corpus
?similar? to the corpus collected in the pilot studies. In order to create this corpus Cclose,
we choose the k most likely sentences as determined by a language model (?MATCH)
built using the collected corpus. A mixture model (?mix) with mixture weight (?) is
built by interpolating the model trained on the corpus of extracted sentences (?close)
and the model trained on the collected corpus (?MATCH). This method is summarized
in Equation (4), where L(M) represents the language recognized by the multimodal
grammar (M).
Cclose = {S1, . . . Sk|Si ? L(M) ? Pr?MATCH (Si) > Pr?MATCH (Si+1)
?  ? j Pr?MATCH (Si) > Pr?MATCH (Sj) > Pr?MATCH (Si+1) (3)
?mix = ? ? ?close + (1? ?) ? ?MATCH (4)
6 We can also randomly sample a sub-network without expanding the k paths.
375
Computational Linguistics Volume 35, Number 3
5.5 Class-Based Out-of-Domain Language Model
An alternative to using in-domain corpora for building language models is to ?migrate?
a corpus of a different domain to our domain. The process of migrating a corpus
involves suitably generalizing the corpus to remove information that is specific only
to the other domain and instantiating the generalized corpus to our domain. Although
there are a number of ways of generalizing the out-of-domain corpus, the generalization
we have investigated involved identifying linguistic units, such as noun and verb
chunks, in the out-of-domain corpus and treating them as classes. These classes are
then instantiated to the corresponding linguistic units from the MATCH domain. The
identification of the linguistic units in the out-of-domain corpus is done automatically
using a supertagger (Bangalore and Joshi 1999). We use a corpus collected in the context
of a software help-desk application as an example out-of-domain corpus. In cases where
the out-of-domain corpus is closely related to the domain at hand, a more semantically
driven generalization might be more suitable. Figure 29 illustrates the process of mi-
grating data from one domain to another.
5.6 Adapting the Switchboard Language Model
We investigated the performance of a large-vocabulary conversational speech recogni-
tion system when applied to a specific domain such as MATCH. We used the Switch-
board corpus (Cswbd) as an example of a large-vocabulary conversational speech corpus.
We built a trigrammodel (?swbd) using the 5.4-million-word corpus and investigated the
effect of adapting the Switchboard language model given k in-domain untranscribed
speech utterances ({OiM}). The adaptation is done by first recognizing the in-domain
speech utterances and then building a language model (?adapt) from the corpus of
recognized text (Cadapt). This bootstrapping mechanism can be used to derive a domain-
specific corpus and language model without any transcriptions. Similar techniques for
Figure 29
A method for migration of data from one domain to another domain.
376
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
unsupervised language model adaptation are presented in Bacchiani and Roark (2003)
and Souvignier and Kellner (1998).
Cadapt = {S1,S2, . . . ,Sk} (5)
Si = argmax
S
?2(O
i
M ? A ? L ? Gswbd)(S)
5.7 Adapting a Wide-Coverage Grammar
There have been a number of computational implementations of wide-coverage,
domain-independent, syntactic grammars for English in various grammar formalisms
(Flickinger, Copestake, and Sag 2000; XTAG 2001; Clark and Hockenmaier 2002). Here,
we describe a method that exploits one such grammar implementation in the Lexical-
ized Tree-Adjoining Grammar (LTAG) formalism, for deriving domain-specific corpora.
An LTAG consists of a set of elementary trees (supertags) (Bangalore and Joshi 1999)
each associated with a lexical item (the head). Supertags encode predicate?argument
relations of the head and the linear order of its arguments with respect to the head.
In Figure 30, we show the supertag associated with the word show in an imperative
sentence such as show the Empire State Building. A supertag can be represented as a
finite-state machine with the head and its arguments as arc labels (Figure 31). The set
of sentences generated by an LTAG can be obtained by combining supertags using
substitution and adjunction operations. In related work (Rambow et al 2002), it has
been shown that for a restricted version of LTAG, the combinations of a set of supertags
can be represented as an FSM. This FSM compactly encodes the set of sentences gen-
erated by an LTAG grammar. It is composed of two transducers, a lexical FST, and a
syntactic FSM.
The lexical FST transduces input words to supertags. We assume that as input to the
construction of the lexical machine we have a list of words with their parts-of-speech.
Once we have determined for each word the set of supertags they should be associated
Figure 30
Supertag tree for the word show. The NP nodes permit substitution of all supertags with root
node labeled NP.
Figure 31
FSM for the word show. The ?NP arc permits replacement with FSMs representing NP.
377
Computational Linguistics Volume 35, Number 3
with, we create a disjunctive finite-state transducer (FST) for all words which transduces
the words to their supertags.
For the syntactic FSM, we take the union of all the FSMs for each supertag which
corresponds to an initial tree (i.e., a tree which need not be adjoined). We then perform
a series of iterative replacements: In each iteration, we replace each arc labeled by a
supertag by its lexicalized version of that supertag?s FSM. Of course, in each iteration,
there are many more replacements than in the previous iteration. Based on the syntactic
complexity in our domain (such as number of modifiers, clausal embedding, and prepo-
sitional phrases), we use five rounds of iteration. The number of iterations restricts the
syntactic complexity but not the length of the input. This construction is in many ways
similar to constructions proposed for CFGs, in particular that of Nederhof (1997). One
difference is that, because we start from TAG, recursion is already factored, andwe need
not find cycles in the rules of the grammar.
We derive a MATCH domain-specific corpus by constructing a lexicon consisting
of pairings of words with their supertags that are relevant to this domain. We then
compile the grammar to build an FSM of all sentences up to a given depth of recursion.
We sample this FSM and build a language model as discussed in Section 5.3. Given
untranscribed utterances from a specific domain, we can also adapt the language model
as discussed in Section 5.6.
5.8 Speech Recognition Experiments
We describe a set of experiments to evaluate the performance of the language model
in the MATCH multimodal system. We use word accuracy and string accuracy for
evaluating ASR output. All results presented in this section are based on 10-fold cross-
validation experiments run on the 709 spoken andmultimodal exchanges collected from
the pilot study described in Section 2.2.
Table 4 presents the performance results for ASR word and sentence accuracy using
language models trained on the collected in-domain corpus as well as on corpora
derived using the different methods discussed in Sections 5.2?5.7. For the class-based
models mentioned in the table, we defined different classes based on areas of interest
(e.g., riverside park, turtle pond), points of interest (e.g., Ellis Island, United Nations Build-
ing), type of cuisine (e.g., Afghani, Indonesian), price categories (e.g., moderately priced,
expensive), and neighborhoods (e.g., Upper East Side, Chinatown).
It is immediately apparent that the hand-crafted grammar as a language model
performs poorly and a language model trained on the collected domain-specific corpus
performs significantly better than models trained on derived data. However, it is en-
couraging to note that a model trained on a derived corpus (obtained from combining
the migrated out-of-domain corpus and a corpus created by sampling the in-domain
grammar) is within 10% word accuracy as compared to the model trained on the col-
lected corpus. There are several other noteworthy observations from these experiments.
The performance of the languagemodel trained on data sampled from the grammar
is dramatically better as compared to the performance of the hand-crafted grammar.
This technique provides a promising direction for authoring portable grammars that can
be sampled subsequently to build robust language models when no in-domain corpora
are available. Furthermore, combining grammar and in-domain data, as described in
Section 5.4, outperforms all other models significantly.
For the experiment on the migration of an out-of-domain corpus, we used a corpus
from a software help-desk application. Table 4 shows that the migration of data using
378
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 4
Performance results for ASR word and sentence accuracy using models trained on data derived
from different methods of bootstrapping domain-specific data.
Scenario ASR Word Sentence
Accuracy Accuracy
1 Grammar-based Grammar as language model 41.6 38.0
(Section 5.2)
Class-based n-gram language 60.6 42.9
model (Section 5.3)
2 In-domain Data Class-based n-gram model 73.8 57.1
(Section 5.1)
3 Grammar+ Class-based n-gram model 75.0 59.5
In-domain Data (Section 5.4)
4 Out-of-domain n-gram model 17.6 17.5
(Section 5.5) Class-based n-gram model 58.4 38.8
Class-based n-gram model
with Grammar-based n-gram
Language Model 64.0 45.4
5 Switchboard n-gram model 43.5 25.0
(Section 5.6) Language model trained on
recognized in-domain data 55.7 36.3
6 Wide-coverage n-gram model 43.7 24.8
Grammar Language model trained on
(Section 5.7) recognized in-domain data 55.8 36.2
linguistic units as described in Section 5.5 significantly outperforms a model trained
only on the out-of-domain corpus. Also, combining the grammar sampled corpus with
the migrated corpus provides further improvement.
The performance of the Switchboard model on the MATCH domain is presented
in the fifth row of Table 4. We built a trigram model using a 5.4-million-word Switch-
board corpus and investigated the effect of adapting the resulting language model on
in-domain untranscribed speech utterances. The adaptation is done by first running
the recognizer on the training partition of the in-domain speech utterances and then
building a language model from the recognized text. We observe that although the
performance of the Switchboard language model on the MATCH domain is poorer than
the performance of a model obtained by migrating data from a related domain, the
performance can be significantly improved using the adaptation technique.
The last row of Table 4 shows the results of using the MATCH specific lexicon to
generate a corpus using a wide-coverage grammar, training a language model, and
adapting the resulting model using in-domain untranscribed speech utterances as was
done for the Switchboard model. The class-based trigrammodel was built using 500,000
randomly sampled paths from the network constructed by the procedure described in
Section 5.7. It is interesting to note that the performance is very similar to the Switch-
board model given that the wide-coverage grammar is not designed for conversational
speech unlike models derived from Switchboard data. The data from the domain has
some elements of conversational-style speech which the Switchboard model models
well, but it also has syntactic constructions that are adequately modeled by the wide-
coverage grammar.
379
Computational Linguistics Volume 35, Number 3
In this section, we have presented a range of techniques to build language models
for speech recognition which are applicable at different development phases of an
application. Although the utility of in-domain data cannot be obviated, we have shown
that there are ways to approximate this data with a combination of grammar and out-of-
domain data. These techniques are particularly useful in the initial phases of application
development when there is very little in-domain data. The technique of authoring a
domain-specific grammar that is sampled for n-gram model building presents a good
trade-off between time-to-create and the robustness of the resulting language model.
Thismethod can be extended by incorporating suitably generalized out-of-domain data,
in order to approximate the distribution of n-grams in the in-domain data. If time to
develop is of utmost importance, we have shown that using a large out-of-domain
corpus (Switchboard) or a wide-coverage domain-independent grammar can yield a
reasonable language model.
6. Robust Multimodal Understanding
In Section 3, we showed how multimodal grammars can be compiled into finite-state
transducers enabling effective processing of lattice input from speech and gesture
recognition and mutual compensation for errors and ambiguities. However, like other
approaches based on hand-crafted grammars, multimodal grammars can be brittle
with respect to extra-grammatical, erroneous, and disfluent input. Also, the primary
applications of multimodal interfaces include use in noisymobile environments and use
by inexperienced users (for whom they provide a more natural interaction); therefore it
is critical that multimodal interfaces provide a high degree of robustness to unexpected
or ill-formed inputs.
In the previous section, we presented methods for bootstrapping domain-specific
corpora for the purpose of training robust languagemodels used for speech recognition.
Thesemethods overcome the brittleness of a grammar-based languagemodel. Although
the corpus-driven language model might recognize a user?s utterance correctly, the
recognized utterance may not be assigned a semantic representation by the multimodal
grammar if the utterance is not part of the grammar.
In this section, we address the issue of robustness in multimodal understanding.
Robustness in multimodal understanding results from improving robustness to speech
recognition and gesture recognition errors. Although the techniques in this section are
presented as applying to the output of a speech recognizer, they are equally applicable to
the output of a gesture recognizer.We chose to focus on robustness to speech recognition
errors because the errors in a gesture recognizer are typically smaller than in a speech
recognizer due to smaller vocabulary and lower perplexity.
There have been two main approaches to improving robustness of the under-
standing component in the spoken language understanding literature. First, a parsing-
based approach attempts to recover partial parses from the parse chart when the
input cannot be parsed in its entirety due to noise, in order to construct a (partial)
semantic representation (Ward 1991; Dowding et al 1993; Allen et al 2001). Second,
a classification-based approach, adopted from the Information Extraction literature,
views the problem of understanding as extracting certain bits of information from the
input. It attempts to classify the utterance and identifies substrings of the input as slot-
filler values to construct a frame-like semantic representation. Both approaches have
limitations. Although in the first approach the grammar can encode richer semantic
representations, the method for combining the fragmented parses is quite ad hoc. In the
380
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
second approach, the robustness is derived from training classifiers on annotated data;
this data is very expensive to collect and annotate, and the semantic representation
is fairly limited. There is some more recent work on using structured classification
approaches to transduce sentences to logical forms (Papineni, Roukos, and Ward 1997;
Thompson and Mooney 2003; Zettlemoyer and Collins 2005). However, it is not clear
how to extend these approaches to apply to lattice input?an important requirement
for multimodal processing.
6.1 Evaluation Issue
Before we present the methods for robust understanding, we discuss the issue of data
partitions to evaluate these methods on. Due to the limited amount of data, we run
cross-validation experiments in order to arrive at reliable performance estimates for
these methods. However, we have a choice in terms of how the data is split into training
and test partitions for the cross-validation runs. We could randomly split the data for
an n-fold (for example, 10-fold) cross-validation test. However, the data contain several
repeated attempts by users performing the six scenarios. A random partitioning of
these data would inevitably have the same multimodal utterances in training and test
partitions. We believe that this would result in an overly optimistic estimate of the
performance. In order to address this issue, we run 6-fold cross-validation experiments
by using five scenarios as the training set and the sixth scenario as the test set. This way
of partitioning the data overly handicaps data-driven methods because the distribution
of data in the training and test partitions for each cross-validation run would be sig-
nificantly different. In the experiment results for each method, we present 10-fold and
6-fold cross-validation results where appropriate in order to demonstrate the strengths
and limitations of each method. For all the experiments in this section, we used a data-
driven language model for ASR. The word accuracy of the ASR is 73.8%, averaged over
all scenarios and all speakers.
6.2 Classification-Based Approach
In this approach we view robust multimodal understanding as a sequence of classifica-
tion problems in order to determine the predicate and arguments of an utterance. The
set of predicates are the same set of predicates used in the meaning representation.
The meaning representation shown in Figure 28 consists of a predicate (the command
attribute) and a sequence of one or more argument attributes which are the parame-
ters for the successful interpretation of the user?s intent. For example, in Figure 28,
cmd:info is the predicate and type:phone object:selection are the arguments to the
predicate.
We determine the predicate (c?) for a N token multimodal utterance (SN1 ) by
searching for the predicate (c) that maximizes the posterior probability as shown in
Equation (6).
c? = argmax
c
P(c | SN1 ) (6)
We view the problem of identifying and extracting arguments from a multimodal
input as a problem of associating each token of the input with a specific tag that encodes
381
Computational Linguistics Volume 35, Number 3
the label of the argument and the span of the argument. These tags are drawn from
a tagset which is constructed by extending each argument label by three additional
symbols I,O,B, following Ramshaw and Marcus (1995). These symbols correspond to
cases when a token is inside (I) an argument span, outside (O) an argument span, or at
the boundary of two argument spans (B) (See Table 5).
Given this encoding, the problem of extracting the arguments amounts to a search
for the most likely sequence of tags (T?) given the input multimodal utterance SN1
as shown in Equation (7). We approximate the posterior probability P(T | SN1 ) using
independence assumptions to include the lexical context in an n-word window and the
preceding two tag labels, as shown in Equation (8).
T? = argmax
T
P(T | SN1 ) (7)
? argmax
T
?
i
P(ti | Sii?n,S
i+n+1
i+1 , ti?1, ti?2) (8)
Owing to the large set of features that are used for predicate identification and
argument extraction, which typically result in sparseness problems for generative mod-
els, we estimate the probabilities using a classification model. In particular, we use
the Adaboost classifier (Schapire 1999) wherein a highly accurate classifier is built by
combining many ?weak? or ?simple? base classifiers fi, each of which may only be
moderately accurate. The selection of the weak classifiers proceeds iteratively, picking
the weak classifier that correctly classifies the examples that are misclassified by the
previously-selected weak classifiers. Each weak classifier is associated with a weight
(wi) that reflects its contribution towards minimizing the classification error. The pos-
terior probability of P(c | x) is computed as in Equation 9. For our experiments, we use
simple n-grams of the multimodal utterance to be classified as weak classifiers.
P(c | x) = 1
(1+ e?2?
?
i wi?fi(x) )
(9)
For the experiments presented subsequently we use the data collected from the
domain to train the classifiers. However, the data could be derived from an in-domain
grammar using techniques similar to those presented in Section 5.
Table 5
The {I,O,B} encoding for argument extraction.
User cheap thai upper west side
Utterance
Argument <price> cheap </price> <cuisine>
Annotation thai </cuisine> <place> upper west
side </place>
IOB cheap price<B> thai cuisine<B>
Encoding upper place<I> west place<I>
side place<I>
382
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
6.2.1 Experiments and Results. We used a total of 10 predicates such as help, assert,
inforequest, and 20 argument types such as cuisine, price, location for our experiments.
These were derived from our meaning representation language. We used unigrams,
bigrams, and trigrams appearing in the multimodal utterance as weak classifiers for
the purpose of predicate classification. In order to predict the tag of a word for ar-
gument extraction, we used the left and right trigram context and the tags for the
preceding two tokens as weak classifiers. The results are presented in Table 6. We
present the concept sentence accuracy and the predicate and argument string accuracy
of the grammar-based understandingmodel and the classification-based understanding
model. The corresponding accuracy results on the 10-fold cross-validation experiments
are shown in parentheses. As can be seen, the grammar-based model significantly out-
performs the classification-based approach on the 6-fold cross-validation experiments
and the classification-based approach outperforms the grammar-based approach on the
10-fold cross-validation experiments. This is to be expected since the classification-
based approach needs to generalize significantly from the training set to the test set,
and these have different distributions of predicates and arguments in the 6-fold cross-
validation experiments.
A significant shortcoming of the classification approach is that it does not exploit
the semantic grammar from the MATCH domain to constrain the possible choices
from the classifier. Also, the classifier is trained using the data that is collected in
this domain. However, the grammar is a rich source of distribution-free data. It is
conceivable to sample the grammar in order to increase the training examples for the
classifier, in the same spirit as was done for building a language model using the
grammar (Section 5.3). Furthermore, knowledge encoded in the grammar and data
can be combined by techniques presented in Schapire et al (2002) to improve classifier
performance.
Another limitation of this approach is that it is unclear how to extend it to apply
to speech and gesture lattices. As shown in earlier sections, multimodal understanding
receives ambiguous speech and gesture inputs encoded as lattices. Mutual disambigua-
tion between these two modalities needs to be exploited. Although the classification
approach can be extended to apply to n-best lists of speech and gesture inputs, we prefer
an approach that can apply to lattice inputs directly.
6.3 Noisy Channel Model for Error Correction
In order to address the limitations of the classification-based approach, we explore an
alternate method for robust multimodal understanding. We translate the user?s input
to a string that can be assigned a meaning representation by the grammar. We can
Table 6
Concept accuracy results from classification-based model using data-driven language model for
ASR. (Numbers are percent for 6-fold cross validation by scenario. Corresponding percent for
10-fold cross validation are given in parentheses.)
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy % Accuracy % Accuracy %
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)
383
Computational Linguistics Volume 35, Number 3
apply this technique on a user?s gesture input as well in order to compensate for
gesture recognition errors.We couch the problem of error correction in the noisy channel
modeling framework. In this regard, we follow Ringger and Allen (1996) and Ristad
and Yianilos (1998); however, we encode the error correction model as a weighted FST
so we can directly edit speech/gesture input lattices. As mentioned earlier, we rely on
integrating speech and gesture lattices to avoid premature pruning of admissible so-
lutions for robust multimodal understanding. Furthermore, unlike Ringger and Allen,
the language grammar from our application filters out edited strings that cannot be
assigned an interpretation by the multimodal grammar. Also, whereas in Ringger and
Allen the goal is to translate to the reference string and improve recognition accuracy, in
our approach the goal is to translate the input in order to assign the reference meaning
and improve concept accuracy.
We let Sg be the string that can be assigned a meaning representation by the
grammar and Su be the user?s input utterance. If we consider Su to be the noisy version
of the Sg, we view the decoding task as a search for the string S
?
g as shown in Equa-
tion (10). Note we formulate this as a joint probability maximization as in Equation (11).
Equation (12) expands the sequence probability by the chain rule where Siu and S
i
g
are the ith tokens from Su and Sg respectively. We use a Markov approximation (limiting
the dependence on the history to the past two time steps: trigram assumption for our
purposes) to compute the joint probability P(Su,Sg), shown in Equation (13).
S?g = argmax
Sg
P(Sg|Su) (10)
= argmax
Sg
P(Sg,Su) (11)
= argmax
Sg
P(S0u,S
0
g) ? P(S
1
u,S
1
g|S
0
u,S
0
g) . . . ? P(S
n
u,S
n
g |S
0
u,S
0
g, . . . ,S
n?1
u ,S
n?1
g ) (12)
S?g = argmax
Sg
?
P(Siu,S
i
g|S
i?1
u ,S
i?2
u ,S
i?1
g ,S
i?2
g ) (13)
where Su = S
1
uS
2
u . . . S
n
u and Sg = S
1
gS
2
g . . . S
m
g .
In order to compute the joint probability, we need to construct an alignment
between tokens (Siu,S
i
g). We use the Viterbi alignment provided by the GIZA++
toolkit (Och and Ney 2003) for this purpose. We convert the Viterbi alignment into a
bilanguage representation that pairs words of the string Su with words of Sg. A few
examples of bilanguage strings are shown in Figure 32. We compute the joint n-gram
model using a language modeling toolkit (Goffin et al 2005). Equation (13) thus allows
us to edit a user?s utterance to a string that can be interpreted by the grammar.
Figure 32
A few examples of bilanguage strings.
384
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
6.3.1 Deriving a Translation Corpus. Because our multimodal grammar is implemented
as a finite-state transducer it is fully reversible and can be used not just to provide a
meaning for input strings but can also be run in reverse to determine possible input
strings for a given meaning. Our multimodal corpus was annotated for meaning using
themultimodal annotation tools described in Section 2.2. In order to train the translation
model we built a corpus that pairs the reference speech string for each utterance in
the training data with a target string. The target string is derived in two steps. First,
the multimodal grammar is run in reverse on the reference meaning yielding a lattice
of possible input strings. Second, the closest string (as defined by Levenshtein edit-
distance [Levenshtein 1966]) in the lattice to the reference speech string is selected as
the target string.
6.3.2 FST-Based Decoder. In order to facilitate editing of ASR lattices, we represent the
n-gram translation model as a weighted finite-state transducer (Bangalore and Riccardi
2002). We first represent the joint n-gram model as a finite-state acceptor (Allauzen
et al 2004). We then interpret the symbols on each arc of the acceptor as having two
components?a word from the user?s utterance (input) and a word from the edited
string (output). This transformation makes a transducer out of an acceptor. In doing
this, we can directly compose the editing model (?MT) with ASR lattices (?S) to produce
a weighted lattice of edited strings. We further constrain the set of edited strings to those
that are interpretable by the grammar. We achieve this by composing with the language
finite-state acceptor derived from the multimodal grammar (?G) and searching for the
best edited string, as shown in Equation (14).
S?MT = argmax
S
?S ? ?MT ? ?G (14)
If we were to apply this approach to input gesture lattices, then the translation
model would be built from pairings of the gesture recognition output and the corre-
sponding gesture string that would be interpretable by the grammar. Typical errors in
gesture input could include misrecognition of a spurious gesture that ought to have
been treated as noise (caused, for example, by improper detection of a pen-down event)
and non-recognition of pertinent gestures due to early end-pointing of ink input.
Figure 33 shows two examples. In the first example, the unimodal speech utterance
was edited by the model to produce a string that was correctly interpreted by the
multimodal grammar. In the second example, the speech and gesture integration failed
and resulted in an empty meaning representation. However, after the edit on the speech
string, the multimodal utterance was correctly interpreted.
6.3.3 Experiments and Results. Table 7 summarizes the results of the translation model
(TM) and compares its accuracy to a grammar-based model. We provide concept ac-
curacy and predicate and argument string accuracy of the translation models applied
to one-best and lattice ASR input. We also provide concept accuracy results on the
lattice of edited strings resulting from applying the translation models to the user?s
input. As can be seen from the table, the translation models outperform the grammar-
based models significantly in all cases. It is also interesting to note that there is some
improvement in concept accuracy using an ASR lattice over a one-best ASR output with
one-best translation output. However, the improvement is much more significant using
a lattice output from the translation model, suggesting that delaying selection of the
edited string until the grammar/domain constraints are applied is paying off.
385
Computational Linguistics Volume 35, Number 3
Figure 33
Sample inputs and the edited outputs from the translation model.
One of the limitations of the translation model is that the edit operations that are
learned are entirely driven by the parallel data. However, when the data are limited,
as is the case here, the edit operations learned are also restricted. We would like to
incorporate domain-specific edit operations in addition to the ones that are reflected in
the data. In the next section, we explore this approach.
6.4 Edit-Based Approach
In this section, we extend the approach of translating the ?noisy? version of the user?s
input to the ?clean? input to incorporate arbitrary editing operations. We encode the
possible edits on the input string as an edit FST with substitution, insertion, deletion,
and identity arcs. These operations could be either word-based or phone-based and
are associated with a cost. This allows us to incorporate, by hand, a range of edits
that may not have been observed in the data used in the noisy?channel-based error-
correction model. The edit transducer coerces the set of strings (S) encoded in the lattice
resulting from ASR (?S ) to the closest strings in the grammar that can be assigned
an interpretation. We are interested in the string with the least-cost sequence of edits
Table 7
Concept sentence accuracy of grammar-based and translation-based models with data-driven
language model for ASR. (Numbers are percent for 6-fold cross validation by scenario.
Corresponding percent for 10-fold cross validation are given in parentheses.)
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
ASR 1-best/1-best TM 46.1 (61.6) 68.0 (70.5) 47.0 (62.6)
ASR 1-best/Lattice TM 50.3 (61.6) 70.5 (70.5) 51.2 (62.6)
ASR Lattice/1-best TM 46.7 (61.9) 70.8 (69.9) 47.1 (63.3)
ASR Lattice/Lattice TM 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)
386
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 34
Basic edit machine.
(argmin) that can be assigned an interpretation by the grammar.7 This can be achieved
by composition (?) of transducers followed by a search for the least-cost path through a
weighted transducer as shown in Equation (15).
s? = argmin
s?S
?S ? ?edit ? ?g (15)
We first describe the machine introduced in Bangalore and Johnston (2004) (Basic
edit) then go on to describe a smaller edit machine with higher performance (4-edit) and
an edit machine which incorporates additional heuristics (Smart edit).
6.4.1 Basic Edit. The edit machine described in Bangalore and Johnston (2004) is es-
sentially a finite-state implementation of the algorithm to compute the Levenshtein
distance. It allows for unlimited insertion, deletion, and substitution of any word for
another (Figure 34). The costs of insertion, deletion, and substitution are set as equal,
except for members of classes such as price (expensive), cuisine (turkish), and so on,
which are assigned a higher cost for deletion and substitution.
6.4.2 Limiting the Number of Edits. Basic edit is effective in increasing the number of
strings that are assigned an interpretation (Bangalore and Johnston 2004) but is quite
large (15Mb, 1 state, 978,120 arcs) and adds an unacceptable amount of latency (5 sec-
onds on average) in processing one-best input and is computationally prohibitive to use
on lattices. In order to overcome these performance limitations, we experimented with
revising the topology of the edit machine so that it allows only a limited number of edit
operations (e.g., at most four edits) and removed the substitution arcs, because they
give rise to O(|
?
|2) arcs, where
?
is the vocabulary. Substitution is still possible but
requires one delete and one insert. For the same grammar, the resulting edit machine is
about 300Kb with 4 states and 16,796 arcs. The topology of the 4-editmachine is shown
in Figure 35. In addition to 4-edit, we also investigated 6-edit and 8-edit machines whose
results we report in the subsequent sections.
There is a significant savings in bounding the number of edits on the number of
paths in the resulting lattice. After composing with the basic edit machine, the lattice
would contain O(n ? |
?
|) arcs where n is the length of the input being edited. For
the bounded k-edit machines this reduces to O(k ? |?|) arcs and O(|?|k) paths for a
constant k.
7 Note that the closest string according to the edit metric may not be the closest string in meaning.
387
Computational Linguistics Volume 35, Number 3
Figure 35
4-edit machine.
6.4.3 Smart Edit. We incorporate a number of additional heuristics and refinements to
tune the 4-edit machine based on the underlying application database.
i. Deletion of SLM-only wordsWe add arcs to the edit transducer to allow for free
deletion of words in the SLM training data which are not found in the grammar: for
example, listings in thai restaurant listings in midtown? thai restaurant in midtown.
ii. Deletion of doubled words A common error observed in SLM output was dou-
bling of monosyllabic words: for example, subway to the cloisters recognized as subway
to to the cloisters. We add arcs to the edit machine to allow for free deletion of any short
word when preceded by the same word.
iii. Extended variable weighting of words Insertion and deletion costs were further
subdivided from two to three classes: a low cost for ?dispensable? words, (e.g., please,
would, looking, a, the), a high cost for special words (slot fillers, e.g., chinese, cheap,
downtown), and a medium cost for all other words, (e.g., restaurant, find).
iv. Auto completion of place names It is unlikely that grammar authors will include
all of the different ways to refer to named entities such as place names. For example, if
the grammar includes metropolitan museum of art the user may just say metropolitan mu-
seum. These changes can involve significant numbers of edits. A capability was added
to the edit machine to complete partial specifications of place names in a single edit.
This involves a closed world assumption over the set of place names. For example, if
the onlymetropolitan museum in the database is themetropolitan museum of artwe assume
that we can insert of art after metropolitan museum. The algorithm for construction of
these auto-completion edits enumerates all possible substrings (both contiguous and
non-contiguous) for place names. For each of these it checks to see if the substring is
found in more than one semantically distinct member of the set. If not, an edit sequence
is added to the edit machine which freely inserts the words needed to complete the
place name. Figure 36 illustrates one of the edit transductions that is added for the place
name metropolitan museum of art. The algorithm which generates the autocomplete edits
also generates new strings to add to the place name class for the SLM (expanded class).
In order to limit over-application of the completion mechanism, substrings starting
in prepositions (of art ? metropolitan museum of art) or involving deletion of parts of
abbreviations are not considered for edits (b c building? n b c building).
Note that the application-specific structure and weighting of Smart edit (iii, iv) can
be derived automatically: We use the place-name list for auto completion of place names
and use the domain entities, as determined by which words correspond to fields in the
underlying application database, to assign variable costs to different entities.
Figure 36
Auto-completion edits.
388
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 8
Concept accuracy for different edit models on 6-fold cross-validation experiments using a
data-driven language model for ASR.
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 40.3 40.7
(No edits)
Basic edit 51.5 63.1 52.6
4-edit 53.0 62.6 53.9
6-edit 58.2 74.7 59.0
8-edit 57.8 75.7 58.6
Smart 4-edit 60.2 69.9 60.9
Smart 6-edit 60.2 73.7 61.3
Smart 8-edit 60.9 76.0 61.9
Smart edit (exp) 59.7 70.8 60.5
Smart edit (exp, lattice) 62.0 73.1 63.0
Smart edit (lattice) 63.2 73.7 64.0
6.4.4 Experiments and Results.We summarize the concept accuracy results from the 6-fold
cross-validation experiments using the different edit machines previously discussed.
We also repeat the concept accuracy results from the grammar-based model with no
edits for a point of comparison. When compared to the baseline of 38.9% concept
accuracy without edits (No edits), Basic edit gave a relative improvement of 32%,
yielding 51.5% concept accuracy (Table 8). Interestingly, by limiting the number of edit
operations as in 4-edit, we improved the concept accuracy (53%) compared to Basic edit.
The reason for this improvement is that for certain input utterances, the Basic editmodel
creates a very large edited lattice and the composition with the grammar fails due to
memory restrictions.8 We also show improvement in concept accuracy by increasing
the number of allowable edit operations (up to 8-edit). The concept accuracy improves
with increasing number of edits but with diminishing relative improvements.
The heuristics in Smart edit clearly improve on the concept accuracy of the basic
edit models with a relative improvement of 55% over the baseline. Smart edit (exp)
shows the concept accuracy of Smart edit running on input from an ASR model with the
expanded classes required for auto completion of place names. Inspection of individual
partitions showed that, while the expanded classes did allow for the correction of
errors on place names, the added perplexity in the ASR model from expanding classes
resulted in errors elsewhere and an overall drop in concept accuracy of 0.5% compared
to Smart edit without expanded classes. Using ASR lattices as input to the edit models
further improved the accuracy to the best concept sentence accuracy score of 63.2%, a
relative improvement of 62.5% over the no-edit model. Lattice input also improved the
performance of Smart edit with the expanded classes from 59.7% to 62%.
To summarize, in Table 9 we tabulate the concept accuracy results from the best
performing configuration of each of the robust understandingmethods discussed in this
section. It is clear that the techniques such as translation-based edit and Smart edit that can
exploit the domain-specific grammar improve significantly over the classification-based
8 However, the concept accuracy for the 70% of utterances which are assigned a meaning using the basic
edit model was about 73%.
389
Computational Linguistics Volume 35, Number 3
Table 9
Summary of concept accuracy results from the different robust understanding techniques using
data-driven language models for ASR.
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
(No edits)
Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)
Translation-based edit 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)
Smart edit 63.2 (68.4) 73.7 (73.8) 64.0 (69.4)
approach. Furthermore, the heuristics encoded in the smart edit technique that exploit
the domain constraints outperform the translation-based edit technique that is entirely
data-dependent.
We also show the results from the 10-fold cross-validation experiments in the
table. As can be seen there is a significant improvement in concept accuracy for data-
driven techniques (classification and translation-based edit) compared to the 6-fold
cross-validation experiments. This is to be expected because the distributions estimated
from the training set fit the test data better in the 10-fold experiments as against 6-fold
experiments.
Based on the results we have presented in this section, it would be pragmatic to
rapidly build a hand-crafted grammar-based conversational system that can be made
robust using stochastic language modeling techniques and edit-based understanding
techniques. Once the system is deployed and data collected, then a judicious balance
of data-driven and grammar-based techniques would maximize the performance of the
system.
7. Robust Gesture Processing
Gesture recognition has a lower error rate than speech recognition in this application.
Even so, gesture misrecognitions and incompleteness of the multimodal grammar in
specifying speech and gesture combinations contribute to the number of utterances not
being assigned a meaning. We address the issue of robustness to gesture errors in this
section.
We adopted the edit-based technique used on speech utterances to improve ro-
bustness of multimodal understanding. However, unlike a speech utterance, a gesture
string has a structured representation. The gesture string is represented as a sequence of
attribute?values (e.g., gesture type takes values from {area, line, point, handwriting}) and
editing a gesture representation implies allowing for replacements within the value set.
We adopted a simple approach that allows for substitution and deletion of values for
each attribute, in addition to the deletion of any gesture. We did not allow for insertions
of gestures as it is not clear what specific content should be assigned to an inserted
gesture. One of the problems is that if you have, for example, a selection of two items
and you want to increase it to three selected items, it is not clear a priori which entity to
add as the third item. We encoded the edit operations for gesture editing as a finite-state
transducer just as we did for editing speech utterances. Figure 37 illustrates the gesture
edit transducer with delc representing the delection cost and substc the substitution
cost. This method of manipulating the gesture recognition lattice is similar to gesture
390
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 37
A finite-state transducer for editing gestures.
aggregation, introduced in Section 3.4. In contrast to substitution and deletion of ges-
tures, gesture aggregation involves insertion of new gestures into the lattice; however,
each introduced gesture has a well-definedmeaning based on the combination of values
of the gestures being aggregated.
We evaluated the effectiveness of the gesture edit machine on the MATCH data set.
The data consisted of 174 multimodal utterances that were covered by the grammar. We
used the transcribed speech utterance and the gesture lattice from the gesture recognizer
as inputs to the multimodal integration and understanding system. For 55.4% of the
utterances, we obtained the identical attribute?value meaning representation as the
human-transcribed meaning representation.
Applying the gesture edit transducer on the gesture recognition lattices, and then
integrating the result with the transcribed speech utterance produced a significant
improvement in the accuracy of the attribute?value meaning representation. For 68.9%
of the utterances, we obtained the identical attribute?value meaning representation
as the human-transcribed meaning representation, a 22.5% absolute improvement in
the robustness of the system that can be directly attributed to robustness in gesture
integration and understanding. In future work, we would like to explore learning from
data how to balance gesture editing and speech editing based on the relative reliabilities
of the two modalities.
8. Conclusion
We view the contributions of the research presented in this article from two perspec-
tives. First, we have shown how the finite-state approach to multimodal language
processing (Johnston and Bangalore 2005) can be extended to support applications
with complex pen input and how the approach can be made robust through coupling
with a stochastic speech recognition model using translation techniques or finite-state
edit machines. We have investigated the options available for bootstrapping domain-
specific corpora for language models by exploiting domain-specific and wide-coverage
grammars, linguistic generalization of out-of-domain data, and adapting domain-
independent corpora. We have shown that such techniques can closely approximate
the accuracy of speech recognizers trained on domain-specific corpora. For robust
391
Computational Linguistics Volume 35, Number 3
multimodal understanding we have presented and comparatively evaluated three dif-
ferent techniques based on discriminative classification, statistical translation, and edit
machines. We have investigated the strengths and limitations of these approaches
in terms of their ability to process lattice input, their ability to exploit constraints
from a domain-specific grammar, and their ability to utilize domain knowledge from
the underlying application database. The best performing multimodal understanding
system, using a stochastic ASR model coupled with the smart 4-edit transducer on
lattice input, is significantly more robust than the grammar-based system, achieving
68.4% concept sentence accuracy (10-fold) on data collected from novice first time users
of a multimodal conversational system. This is a substantial 35% relative improve-
ment in performance compared to 50.7% concept sentence accuracy (10-fold) using the
grammar-based language and multimodal understanding models without edits. In our
exploration of applying edit techniques to the gesture lattices we saw a 22.5% absolute
improvement in robustness.
The second perspective on the work views it as an investigation of a range of
techniques that balance the robustness provided by data-driven techniques and the
flexibility provided by grammar-based approaches. In the past four decades of speech
and natural language processing, both data-driven approaches and rule-based ap-
proaches have been prominent at different periods in time. Moderate-sized rule-based
spoken language models for recognition and understanding are easy to develop and
provide the ability to rapidly prototype conversational applications. However, scalabil-
ity of such systems is a bottleneck due to the heavy cost of authoring and maintenance
of rule sets and inevitable brittleness due to lack of coverage. In contrast, data-driven
approaches are robust and provide a simple process of developing applications given
availability of data from the application domain. However, this reliance on domain-
specific data is also one of the significant bottlenecks of data-driven approaches. Devel-
opment of conversational systems using data-driven approaches cannot proceed until
data pertaining to the application domain is available. The collection and annotation
of such data is extremely time-consuming and tedious, which is aggravated by the
presence of multiple modalities in the user?s input, as in our case. Also, extending an
existing application to support an additional feature requires adding additional data
sets with that feature. We have shown how a balanced approach where statistical lan-
guage models are coupled with grammar-based understanding using edit machines can
be highly effective in a multimodal conversational system. It is important to note that
these techniques are equally applicable for speech-only conversational systems as well.
Given that the combination of stochastic recognition models with grammar-based
understanding models provides robust performance, the question which remains is,
after the initial bootstrapping phase, as more data becomes available, should this
grammar-based approach be replaced with a data-driven understanding component?
There are a number of advantages to the hybrid approach we have proposed which
extend beyond the initial deployment of an application.
1. The expressiveness of the multimodal grammar allows us to specify
any compositional relationships and meaning that we want. The range
of meanings and their relationship to the input string can be arbitrarily
simple or complex.
2. The multimodal grammar provides an alignment between speech
and gesture input and enables multimodal integration of content
from different modes.
392
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
3. With the grammar-based approach it is straightforward to quickly add
support for new commands to the grammar or change the representation
of existing commands. The only retraining that is needed is for the ASR
model, and data for the ASR model can either be migrated from another
related domain or derived through grammar sampling.
4. Most importantly, this approach has the significant advantage that it
does not require annotation of speech data with meaning representations
and alignment of the meaning representations with word strings. This
can be complex and expensive, involving a detailed labeling guide and
instructions for annotators. In contrast in this approach, if data is used, all
that is needed is transcription of the audio, a far more straightforward
annotation task. If no data is used then grammar sampling can be used
instead and no annotation of data is needed whatsoever.
5. Although data-driven approaches to understanding are commonplace
in research, rule-based techniques continue to dominate in much of the
industry (Pieraccini 2004). See, for example, the W3C SRGS standard
(www.w3.org/TR/speech-grammar/).
Acknowledgments
We dedicate this article to the memory of
Candy Kamm whose continued support
for multimodal research made this work
possible. We thank Patrick Ehlen, Helen
Hastie, Preetam Maloor, Amanda Stent,
Gunaranjan Vasireddy, Marilyn Walker,
and Steve Whittaker for their contributions
to the MATCH system. We also thank
Richard Cox and Mazin Gilbert for their
ongoing support of multimodal research at
AT&T Labs?Research. We would also like
to thank the anonymous reviewers for
their many helpful comments and
suggestions for revision.
References
Abney, S. P. 1991. Parsing by chunks. In
R. Berwick, S. Abney, and C. Tenny,
editors, Principle-Based Parsing. IEEE,
Los Alamitos, CA, pages 257?278.
Ades, A. E. and M. Steedman. 1982. On the
order of words. Linguistics and Philosophy,
4:517?558.
Alexandersson, J. and T. Becker. 2001.
Overlay as the basic operation for
discourse processing in a multimodal
dialogue system. In Proceedings of the IJCAI
Workshop: Knowledge and Reasoning in
Practical Dialogue Systems, pages 8?14,
Seattle, WA.
Allauzen, C., M. Mohri, M. Riley, and
B. Roark. 2004. A generalized construction
of speech recognition transducers. In
International Conference on Acoustics, Speech,
and Signal Processing, pages 761?764,
Montreal.
Allauzen, C., M. Mohri, and B. Roark. 2003.
Generalized algorithms for constructing
statistical language models. In Proceedings
of the Association for Computational
Linguistics, pages 40?47, Sapporo.
Allauzen, C., M. Riley, J. Schalkwyk,
W. Skut, and M. Mohri. 2007. Openfst:
A general and efficient weighted
finite-state transducer library. In
Proceedings of the Ninth International
Conference on Implementation and Application
of Automata, (CIAA 2007), Lecture Notes in
Computer Science Vol. 4783, pages 11?23.
Springer, Berlin, Heidelberg.
Allen, J., D. Byron, M. Dzikovska,
G. Ferguson, L. Galescu, and A. Stent.
2001. Towards conversational
human-computer interaction.
AI Magazine, 22(4):27?38.
Andre?, E. 2002. Natural language in
multimedia/multimodal systems.
In R. Mitkov, editor, Handbook of
Computational Linguistics. Oxford
University Press, Oxford,
pages 650?669.
Bacchiani, M. and B. Roark. 2003.
Unsupervised language model
adaptation. In Proceedings of the
International Conference on Acoustics,
Speech, and Signal Processing,
pages 224?227, Hong Kong.
Bangalore, S. 1997. Complexity of Lexical
Descriptions and its Relevance to Partial
393
Computational Linguistics Volume 35, Number 3
Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Bangalore, S. and M. Johnston. 2000.
Tight-coupling of multimodal language
processing with speech recognition.
In Proceedings of the International Conference
on Spoken Language Processing,
pages 126?129, Beijing.
Bangalore, S. and M. Johnston. 2004.
Balancing data-driven and rule-based
approaches in the context of a
multimodal conversational system.
In Proceedings of the North American
Association for Computational Linguistics/
Human Language Technology, pages 33?40,
Boston, MA.
Bangalore, S. and A. K. Joshi. 1999.
Supertagging: An approach to almost
parsing. Computational Linguistics,
25(2):237?265.
Bangalore, S. and G. Riccardi. 2000.
Stochastic finite-state models for spoken
language machine translation. In
Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Bangalore, S. and G. Riccardi. 2002.
Stochastic finite-state models of spoken
language machine translation.Machine
Translation, 17(3):165?184.
Bellik, Y. 1995. Interface Multimodales:
Concepts, Mode`les et Architectures. Ph.D.
thesis, University of Paris XI (Orsay),
France.
Bellik, Y. 1997. Media integration in
multimodal interfaces. In Proceedings
of the IEEE Workshop on Multimedia
Signal Processing, pages 31?36,
Princeton, NJ.
Beutnagel, M., A. Conkie, J. Schroeter,
Y. Stylianou, and A. Syrdal. 1999. The
AT&T next-generation TTS. In Joint
Meeting of ASA; EAA and DAGA,
pages 18?24, Berlin.
Boros, M., W. Eckert, F. Gallwitz, G. Go?rz,
G. Hanrieder, and H. Niemann.
1996. Towards understanding
spontaneous speech: word accuracy
vs. concept accuracy. In Proceedings of
the International Conference on Spoken
Language Processing, pages 41?44,
Philadelphia, PA.
Brison, E. and N. Vigouroux. 1993.
Multimodal references: A generic fusion
process. Technical report, URIT-URA
CNRS, Universit Paul Sabatier, Toulouse.
Carpenter, R. 1992. The Logic of Typed Feature
Structures. Cambridge University Press,
Cambridge.
Cassell, J. 2001. Embodied conversational
agents: Representation and intelligence in
user interface. AI Magazine, 22:67?83.
Cheyer, A. and L. Julia. 1998. Multimodal
Maps: An Agent-Based Approach. Lecture
Notes in Computer Science, 1374:103?113.
Ciaramella, A. 1993. A prototype
performance evaluation report. Technical
Report WP8000-D3, Project Esprit 2218,
SUNDIAL.
Clark, S. and J. Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
In Proceedings of the LREC 2002, Beyond
Parseval Workshop, pages 60?66,
Las Palmas.
Cohen, P. R. 1991. Integrated interfaces
for decision support with simulation.
In Proceedings of the Winter Simulation
Conference, pages 1066?1072, Phoenix, AZ.
Cohen, P. R. 1992. The role of natural
language in a multimodal interface.
In Proceedings of the User Interface Software
and Technology, pages 143?149,
Monterey, CA.
Cohen, P. R., M. Johnston, D. McGee, S. L.
Oviatt, J. Clow, and I. Smith. 1998a. The
efficiency of multimodal interaction: A
case study. In Proceedings of the International
Conference on Spoken Language Processing,
pages 249?252, Sydney.
Cohen, P. R., M. Johnston, D. McGee, S. L.
Oviatt, J. Pittman, I. Smith, L. Chen, and
J. Clow. 1998b. Multimodal interaction for
distributed interactive simulation. In
M. Maybury and W. Wahlster, editors,
Readings in Intelligent Interfaces. Morgan
Kaufmann Publishers, San Francisco, CA,
pages 562?571.
Dowding, J., J. M. Gawron, D. E. Appelt,
J. Bear, L. Cherny, R. Moore, and D. B.
Moran. 1993. GEMINI: A natural
language system for spoken-language
understanding. In Proceedings of the
Association for Computational Linguistics,
pages 54?61, Columbus, OH.
Ehlen, P., M. Johnston, and G. Vasireddy.
2002. Collecting mobile multimodal
data for MATCH. In Proceedings of the
International Conference on Spoken Language
Processing, pages 2557?2560, Denver, CO.
Flickinger, D., A. Copestake, and I. Sag.
2000. HPSG analysis of English.
In W. Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer?Verlag, Berlin, pages 254?263.
Galescu, L., E. K. Ringger, and J. F. Allen.
1998. Rapid language model development
for new task domains. In Proceedings of the
ELRA First International Conference on
394
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Language Resources and Evaluation (LREC),
pages 807?812, Granada.
Goffin, V., C. Allauzen, E. Bocchieri,
D. Hakkani-Tur, A. Ljolje, S. Parthasarathy,
M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&T WATSON speech
recognizer. In Proceedings of the
International Conference on Acoustics, Speech,
and Signal Processing, pages 1033?1036,
Philadelphia, PA.
Gorin, A. L., G. Riccardi, and J. H. Wright.
1997. How May I Help You? Speech
Communication, 23(1-2):113?127.
Gupta, N., G. Tur, D. Tur, S. Bangalore,
G. Riccardi, and M. Rahim. 2004. The
AT&T spoken language understanding
system. IEEE Transactions on Speech and
Audio Processing, 14(1):213?222.
Gustafson, J., L. Bell, J. Beskow, J. Boye,
R. Carlson, J. Edlund, B. Granstro?m,
D. House, and M. Wirn. 2000. Adapt?
a multimodal conversational dialogue
system in an apartment domain. In
International Conference on Spoken Language
Processing, pages 134?137, Beijing.
Haffner, P., G. Tur, and J. Wright. 2003.
Optimizing SVMs for complex call
classification. In International Conference on
Acoustics, Speech, and Signal Processing,
pages 632?635, Hong Kong.
Hauptmann, A. 1989. Speech and gesture
for graphic image manipulation. In
Proceedings of CHI ?89, pages 241?245,
Austin, TX.
Jackendoff, R. 2002. Foundations of Language:
Brain, Meaning, Grammar, and Evolution
Chapter 9. Oxford University Press,
New York.
Johnston, M. 1998a. Multimodal language
processing. In Proceedings of the
International Conference on Spoken Language
Processing, pages 893?896, Sydney.
Johnston, M. 1998b. Unification-based
multimodal parsing. In Proceedings of the
Association for Computational Linguistics,
pages 624?630, Montreal.
Johnston, M. 2000. Deixis and conjunction in
multimodal systems. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 362?368,
Saarbru?cken.
Johnston, M., P. Baggia, D. C. Burnett,
J. Carter, D. Dahl, G. McCobb, and
D. Raggett. 2007. EMMA: Extensible
MultiModal Annotation markup language.
Technical report, W3C Candidate
Recommendation.
Johnston, M. and S. Bangalore. 2000.
Finite-state multimodal parsing and
understanding. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 369?375,
Saarbru?cken.
Johnston, M. and S. Bangalore. 2004.
Matchkiosk: A multimodal interactive city
guide. In Proceedings of the Association of
Computational Linguistics (ACL) Poster and
Demonstration Session, pages 222?225,
Barcelona.
Johnston, M. and S. Bangalore. 2005.
Finite-state multimodal integration and
understanding. Journal of Natural Language
Engineering, 11(2):159?187.
Johnston, M., S. Bangalore, A. Stent,
G. Vasireddy, and P. Ehlen. 2002a.
Multimodal language processing for
mobile information access. In Proceedings
of the International Conference on Spoken
Language Processing, pages 2237?2240,
Denver, CO.
Johnston, M., S. Bangalore, G. Vasireddy,
A. Stent, P. Ehlen, M. Walker, S. Whittaker,
and P. Maloor. 2002b. MATCH: An
architecture for multimodal dialog
systems. In Proceedings of the Association of
Computational Linguistics, pages 376?383,
Philadelphia, PA.
Johnston, M., P. R. Cohen, D. McGee, S. L.
Oviatt, J. A. Pittman, and I. Smith.
1997. Unification-based multimodal
integration. In Proceedings of the
Association of Computational Linguistics,
pages 281?288, Madrid.
Joshi, A. and P. Hopely. 1997. A parser from
antiquity. Journal of Natural Language
Engineering, 2(4):6?15.
Kanthak, S. and H. Ney. 2004. FSA: An
Efficient and Flexible C++ Toolkit
for Finite State Automata Using
On-Demand Computation. In Proceedings
of the Association for Computational
Linguistics Conference, pages 510?517,
Barcelona.
Kaplan, R. M. and M. Kay. 1994. Regular
models of phonological rule systems.
Computational Linguistics, 20(3):331?378.
Kartunnen, L. 1991. Finite-state constraints.
In Proceedings of the International
Conference on Current Issues in
Computational Linguistics, Universiti
Sains Malaysia, Penang.
Koons, D. B., C. J. Sparrell, and K. R.
Thorisson. 1993. Integrating simultaneous
input from speech, gaze, and hand
gestures. In M. T. Maybury, editor,
Intelligent Multimedia Interfaces. AAAI
Press/MIT Press, Cambridge, MA,
pages 257?276.
395
Computational Linguistics Volume 35, Number 3
Koskenniemi, K. K. 1984. Two-level
Morphology: A General Computation Model
for Word-form Recognition and Production.
Ph.D. thesis, University of Helsinki.
Larsson, S., P. Bohlin, J. Bos, and D. Traum.
1999. TrindiKit manual. Technical report,
TRINDI Deliverable D2.2, Gothenburg
University, Sweden.
Levenshtein, V. I. 1966. Binary codes capable
of correcting deletions, insertion and
reversals. Soviet Physics Doklady,
10:707?710.
Mohri, M., F. C. N. Pereira, and M. Riley.
1998. A rational design for a weighted
finite-state transducer library. Lecture Notes
in Computer Science, 1436:144?158.
Neal, J. G. and S. C. Shapiro. 1991. Intelligent
multi-media interface technology. In J. W.
Sullivan and S. W. Tyler, editors, Intelligent
User Interfaces. Addison Wesley, New York,
pages 45?68.
Nederhof, M. J. 1997. Regular
approximations of CFLs: A grammatical
view. In Proceedings of the International
Workshop on Parsing Technology,
pages 159?170, Boston, MA.
Nishimoto, T., N. Shida, T. Kobayashi,
and K. Shirai. 1995. Improving human
interface in drawing tool using speech,
mouse, and keyboard. In Proceedings of the
4th IEEE International Workshop on Robot
and Human Communication, ROMAN95,
pages 107?112, Tokyo.
Noord, G. 1997. FSA utilities: A toolbox
to manipulate finite-state automata.
Lecture Notes in Computer Science,
1260:87?108.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Oviatt, S., A. DeAngeli, and K. Kuhn. 1997.
Integration and synchronization of
input modes during multimodal
human-computer interaction. In CHI ?97:
Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems,
pages 415?422, New York, NY.
Oviatt, S. L. 1997. Multimodal interactive
maps: Designing for human performance.
Human-Computer Interaction, 12(1):93?129.
Oviatt, S. L. 1999. Mutual disambiguation
of recognition errors in a multimodal
architecture. In Proceedings of the
Conference on Human Factors in Computing
Systems: CHI?99, pages 576?583,
Pittsburgh, PA.
Papineni, K. A., S. Roukos, and T. R. Ward.
1997. Feature-based language
understanding. In Proceedings of
European Conference on Speech
Communication and Technology,
pages 1435?1438, Rhodes.
Pereira, F. C. N. and M. D. Riley. 1997.
Speech recognition by composition of
weighted finite automata. In E. Roche
and Y. Schabes, editors, Finite State
Devices for Natural Language Processing.
MIT Press, Cambridge, MA, USA,
pages 431?456.
Pieraccini, R. 2004. Spoken language
understanding: The research/industry
chasm. In HLT-NAACL 2004 Workshop
on Spoken Language Understanding for
Conversational Systems and Higher Level
Linguistic Information for Speech Processing,
Boston, MA.
Pollard, C. and I. A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Center
for the Study of Language and
Information, University of Chicago
Press, IL.
Punyakanok, V., D. Roth, and W. Yih. 2005.
Generalized inference with multiple
semantic role labeling systems shared
task paper. In Proceedings of the Annual
Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Rambow, O., S. Bangalore, T. Butt, A. Nasr,
and R. Sproat. 2002. Creating a finite-state
parser with application semantics. In
Proceedings of the International Conference on
Computational Linguistics (COLING 2002),
pages 1?5, Taipei.
Ramshaw, L. and M. P. Marcus. 1995. Text
chunking using transformation-based
learning. In Proceedings of the Third
Workshop on Very Large Corpora,
pages 82?94, Cambridge, MA.
Riccardi, G., R. Pieraccini, and E. Bocchieri.
1996. Stochastic automata for language
modeling. Computer Speech and Language,
10(4):265?293.
Rich, C. and C. Sidner. 1998. COLLAGEN:
A collaboration manager for software
interface agents. User Modeling and
User-Adapted Interaction, 8(3?4):315?350.
Ringger, E. K. and J. F. Allen. 1996. A
fertility channel model for post-correction
of continuous speech recognition.
In International Conference on Spoken
Language Processing, pages 897?900,
Philadelphia, PA.
Ristad, E. S. and P. N. Yianilos. 1998.
Learning string-edit distance. IEEE
Transaction on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
396
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Roche, E. 1999. Finite-state transducers:
parsing free and frozen sentences. In
A. Kornai, editor, Extended Finite-State
Models of Language. Cambridge University
Press, Cambridge, pages 108?120.
Rubine, D. 1991. Specifying gestures by
example. Computer Graphics, 25(4):329?337.
Schapire, R., M. Rochery, M. Rahim, and
N. Gupta. 2002. Incorporating prior
knowledge in boosting. In Proceedings
of the Nineteenth International Conference
on Machine Learning, pages 538?545,
Sydney.
Schapire, R. E. 1999. A brief introduction to
boosting. In Proceedings of International
Joint Conference on Artificial Intelligence,
pages 1401?1406, Stockholm.
Souvignier, B. and A. Kellner. 1998. Online
adaptation for language models in
spoken dialogue systems. In International
Conference on Spoken Language Processing,
pages 2323?2326, Sydney.
Stent, A., J. Dowding, J. Gawron, E. Bratt,
and R. Moore. 1999. The CommandTalk
spoken dialogue system. In Proceedings of
the 27th Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
College Park, MD.
Thompson, C. A. and R. J. Mooney. 2003.
Acquiring word-meaning mappings for
natural language interfaces. Journal of
Artificial Intelligence Research, 18:1?44.
van Tichelen, L. 2004. Semantic
interpretation for speech recognition.
Technical Report W3C.
Wahlster, W. 2002. SmartKom: Fusion and
fission of speech, gestures, and facial
expressions. In Proceedings of the 1st
International Workshop on Man-Machine
Symbiotic Systems, pages 213?225, Kyoto.
Walker, M., R. Passonneau, and J. Boland.
2001. Quantitative and qualitative
evaluation of DARPA Communicator
spoken dialogue systems. In Proceedings
of the 39rd Annual Meeting of the
Association for Computational Linguistics
(ACL/EACL-2001), pages 515?522,
Toulouse.
Walker, M. A., S. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2004. Generation and
evaluation of user tailored responses in
multimodal dialogue. Cognitive Science,
28(5):811?840.
Walker, M. A., S. J. Whittaker, P. Maloor,
J. D. Moore, M. Johnston, and
G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in
spoken dialogue. In Proceedings of the
International Natural Language
Generation Conference, pages 73?80,
Ramapo, NY.
Wang, Y. and A. Acero. 2003. Combination of
CFG and n-gram modeling in semantic
grammar learning. In Proceedings of the
Eurospeech Conference, pages 2809?2812,
Geneva.
Ward, W. 1991. Understanding spontaneous
speech: The Phoenix system. In
Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing,
pages 365?367, Washington, DC.
Wauchope, K. 1994. Eucalyptus: Integrating
natural language input with a graphical
user interface. Technical Report
NRL/FR/5510?94-9711, Naval
Research Laboratory, Washington, DC.
XTAG. 2001. A lexicalized tree-adjoining
grammar for English. Technical report,
University of Pennsylvania. Available at
www.cis.upenn.edu/?xtag/
gramrelease.html.
Zettlemoyer, L. S. and M. Collins. 2005.
Learning to map sentences to logical
form: Structured classification with
probabilistic categorial grammars.
In Proceedings of the Twenty-First
Conference on Uncertainty in Artificial
Intelligence (UAI-05), pages 658?66,
Arlington, VA.
397

  
	
	 Balancing Data-driven and Rule-based Approaches in the Context of a
Multimodal Conversational System
Srinivas Bangalore
AT&T Labs-Research
180 Park Avenue
Florham Park, NJ 07932
srini@research.att.com
Michael Johnston
AT&T Labs-Research
180 Park Avenue
Florham Park, NJ 07932
johnston@research.att.com
Abstract
Moderate-sized rule-based spoken language
models for recognition and understanding are
easy to develop and provide the ability to
rapidly prototype conversational applications.
However, scalability of such systems is a bot-
tleneck due to the heavy cost of authoring and
maintenance of rule sets and inevitable brittle-
ness due to lack of coverage in the rule sets.
In contrast, data-driven approaches are robust
and the procedure for model building is usu-
ally simple. However, the lack of data in a par-
ticular application domain limits the ability to
build data-driven models. In this paper, we ad-
dress the issue of combining data-driven and
grammar-based models for rapid prototyping
of robust speech recognition and understanding
models for a multimodal conversational sys-
tem. We also present methods that reuse data
from different domains and investigate the lim-
its of such models in the context of a particular
application domain.
1 Introduction
In the past four decades of speech and natural language
processing, both data-driven approaches and rule-based
approaches have been prominent at different periods in
time. In the recent past, rule-based approaches have
fallen into disfavor due to their brittleness and the sig-
nificant cost of authoring and maintaining complex rule
sets. Data-driven approaches are robust and provide a
simple process of developing applications given the data
from the application domain. However, the reliance on
domain-specific data is also one of the significant bottle-
necks of data-driven approaches. Development of a con-
versational system using data-driven approaches cannot
proceed until data pertaining to the application domain is
available. The collection and annotation of such data is
extremely time-consuming and tedious, which is aggra-
vated by the presence of multiple modalities in the user?s
input, as in our case. Also, extending an existing applica-
tion to support an additional feature requires adding ad-
ditional data sets with that feature.
In this paper, we explore various methods for combin-
ing rule-based and in-domain data for rapid prototyping
of speech recognition and understanding models that are
robust to ill-formed or unexpected input in the context
of a multimodal conversational system. We also investi-
gate approaches to reuse out-of-domain data and compare
their performance against the performance of in-domain
data-driven models.
We investigate these issues in the context of a multi-
modal application designed to provide an interactive city
guide: MATCH. In Section 2, we present the MATCH
application, the architecture of the system and the appa-
ratus for multimodal understanding. In Section 3, we dis-
cuss various approaches to rapid prototyping of the lan-
guage model for the speech recognizer and in Section 4
we present two approaches to robust multimodal under-
standing. Section 5 presents the results for speech recog-
nition and multimodal understanding using the different
approaches we consider.
2 The MATCH application
MATCH (Multimodal Access To City Help) is a work-
ing city guide and navigation system that enables mo-
bile users to access restaurant and subway information
for New York City (NYC) (Johnston et al, 2002b; John-
ston et al, 2002a). The user interacts with a graphical in-
terface displaying restaurant listings and a dynamic map
showing locations and street information. The inputs can
be speech, drawing on the display with a stylus, or syn-
chronous multimodal combinations of the two modes.
The user can ask for the review, cuisine, phone number,
address, or other information about restaurants and sub-
way directions to locations. The system responds with
graphical callouts on the display, synchronized with syn-
thetic speech output. For example, if the user says phone
numbers for these two restaurants and circles two restau-
rants as in Figure 1 [a], the system will draw a callout
with the restaurant name and number and say, for exam-
ple Time Cafe can be reached at 212-533-7000, for each
restaurant in turn (Figure 1 [b]). If the immediate en-
vironment is too noisy or public, the same command can
be given completely in pen by circling the restaurants and
writing phone.
Figure 1: Two area gestures
2.1 MATCH Multimodal Architecture
The underlying architecture that supports MATCH con-
sists of a series of re-usable components which commu-
nicate over sockets through a facilitator (MCUBE) (Fig-
ure 2). Users interact with the system through a Multi-
modal User Interface Client (MUI). Their speech and ink
are processed by speech recognition (Sharp et al, 1997)
(ASR) and handwriting/gesture recognition (GESTURE,
HW RECO) components respectively. These recognition
processes result in lattices of potential words and ges-
tures. These are then combined and assigned a mean-
ing representation using a multimodal finite-state device
(MMFST) (Johnston and Bangalore, 2000; Johnston et
al., 2002b). This provides as output a lattice encoding all
of the potential meaning representations assigned to the
user inputs. This lattice is flattened to an N-best list and
passed to a multimodal dialog manager (MDM) (John-
ston et al, 2002b), which re-ranks them in accordance
with the current dialogue state. If additional informa-
tion or confirmation is required, the MDM enters into a
short information gathering dialogue with the user. Once
a command or query is complete, it is passed to the mul-
timodal generation component (MMGEN), which builds
a multimodal score indicating a coordinated sequence of
graphical actions and TTS prompts. This score is passed
back to the Multimodal UI (MUI). The Multimodal UI
coordinates presentation of graphical content with syn-
thetic speech output using the AT&T Natural Voices TTS
engine (Beutnagel et al, 1999). The subway route con-
straint solver (SUBWAY) identifies the best route be-
tween any two points in New York City.
Figure 2: Multimodal Architecture
2.2 Multimodal Integration and Understanding
Our approach to integrating and interpreting multimodal
inputs (Johnston et al, 2002b; Johnston et al, 2002a) is
an extension of the finite-state approach previously pro-
posed (Bangalore and Johnston, 2000; Johnston and Ban-
galore, 2000). In this approach, a declarative multimodal
grammar captures both the structure and the interpreta-
tion of multimodal and unimodal commands. The gram-
mar consists of a set of context-free rules. The multi-
modal aspects of the grammar become apparent in the
terminals, each of which is a triple W:G:M, consisting
of speech (words, W), gesture (gesture symbols, G), and
meaning (meaning symbols, M). The multimodal gram-
mar encodes not just multimodal integration patterns but
also the syntax of speech and gesture, and the assignment
of meaning. The meaning is represented in XML, facil-
itating parsing and logging by other system components.
The symbol SEM is used to abstract over specific content
such as the set of points delimiting an area or the identi-
fiers of selected objects. In Figure 3, we present a small
simplified fragment from the MATCH application capa-
ble of handling information seeking commands such as
phone for these three restaurants. The epsilon symbol (   )
indicates that a stream is empty in a given terminal.
CMD    :   :  cmd  INFO   :   :  /cmd 
INFO    :   :  type  TYPE   :   :  /type 
for:   :    :   :  obj  DEICNP   :   :  /obj 
TYPE  phone:   :phone  review:   :review
DEICNP  DDETPL   :area:    :sel:   NUM HEADPL
DDETPL  these:G:    those:G:  
HEADPL  restaurants:rest:  rest  SEM:SEM:    :   :  /rest 
NUM  two:2:    three:3:   ... ten:10:  
Figure 3: Multimodal grammar fragment
Speech:    
Gesture:
<type><info><cmd>
SEM(points...)
phone
<rest>
Meaning: 
<rest>
<obj></type>
ten
2
sel
locareaG
SEM(r12,r15)
restaurantstwotheseforphone
</obj></rest>r12,r15 </info> </cmd>
Figure 4: Multimodal Example
In the example above where the user says phone for
these two restaurants while circling two restaurants (Fig-
ure 1 [a]), assume the speech recognizer returns the lat-
tice in Figure 4 (Speech). The gesture recognition com-
ponent also returns a lattice (Figure 4, Gesture) indicat-
ing that the user?s ink is either a selection of two restau-
rants or a geographical area. The multimodal grammar
(Figure 3) expresses the relationship between what the
user said, what they drew with the pen, and their com-
bined meaning, in this case Figure 4 (Meaning). The
meaning is generated by concatenating the meaning sym-
bols and replacing SEM with the appropriate specific con-
tent:  cmd  info  type  phone  /type  obj 
 rest  [r12,r15]  /rest 	 /obj 
 /info  /cmd  .
For the purpose of evaluation of concept accuracy, we
developed an approach similar to (Boros et al, 1996)
in which computing concept accuracy is reduced to com-
paring strings representing core contentful concepts. We
extract a sorted flat list of attribute value pairs that repre-
sents the core contentful concepts of each command from
the XML output. The example above yields the following
meaning representation for concept accuracy.
 
		Proceedings of NAACL HLT 2007, pages 1?8,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Exploiting acoustic and syntactic features for prosody labeling in
a maximum entropy framework
Vivek Rangarajan, Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
Viterbi School of Electrical Engineering
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Research Labs
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
In this paper we describe an automatic
prosody labeling framework that exploits
both language and speech information.
We model the syntactic-prosodic informa-
tion with a maximum entropy model that
achieves an accuracy of 85.2% and 91.5%
for pitch accent and boundary tone la-
beling on the Boston University Radio
News corpus. We model the acoustic-
prosodic stream with two different mod-
els, one a maximum entropy model and
the other a traditional HMM. We finally
couple the syntactic-prosodic and acoustic-
prosodic components to achieve signifi-
cantly improved pitch accent and bound-
ary tone classification accuracies of 86.0%
and 93.1% respectively. Similar experimen-
tal results are also reported on Boston Di-
rections corpus.
1 Introduction
Prosody refers to intonation, rhythm and lexical
stress patterns of spoken language that convey lin-
guistic and paralinguistic information such as em-
phasis, intent, attitude and emotion of a speaker.
Prosodic information associated with a unit of
speech, say, syllable, word, phrase or clause, influ-
ence all the segments of the unit in an utterance. In
this sense they are also referred to as suprasegmen-
tals (Lehiste, 1970). Prosody in general is highly
dependent on individual speaker style, gender, di-
alect and other phonological factors. The difficulty in
reliably characterizing suprasegmental information
present in speech has resulted in symbolic and para-
meteric prosody labeling standards like ToBI (Tones
and Break Indices) (Silverman et al, 1992) and Tilt
model (Taylor, 1998) respectively.
Prosody in spoken language can be characterized
through acoustic features or lexical features or both.
Acoustic correlates of duration, intensity and pitch,
like syllable nuclei duration, short time energy and
fundamental frequency (f0) are some acoustic fea-
tures that are perceived to confer prosodic promi-
nence or stress in English. Lexical features like parts-
of-speech, syllable nuclei identity, syllable stress of
neighboring words have also demonstrated high de-
gree of discriminatory evidence in prosody detection
tasks.
The interplay between acoustic and lexical fea-
tures in characterizing prosodic events has been suc-
cessfully exploited in text-to-speech synthesis (Bu-
lyko and Ostendorf, 2001; Ma et al, 2003), speech
recognition (Hasegawa-Johnson et al, 2005) and
speech understanding (Wightman and Ostendorf,
1994). Text-to-speech synthesis relies on lexical fea-
tures derived predominantly from the input text to
synthesize natural sounding speech with appropri-
ate prosody. In contrast, output of a typical auto-
matic speech recognition (ASR) system is noisy and
hence, the acoustic features are more useful in pre-
dicting prosody than the hypothesized lexical tran-
script which may be erroneous. Speech understand-
ing systems model both the lexical and acoustic fea-
tures at the output of an ASR to improve natural
language understanding. Another source of renewed
interest has come from spoken language translation
(No?th et al, 2000; Agu?ero et al, 2006). A pre-
requisite for all these applications is accurate prosody
detection, the topic of the present work.
In this paper, we describe our framework for build-
ing an automatic prosody labeler for English. We
report results on the Boston University (BU) Ra-
dio Speech Corpus (Ostendorf et al, 1995) and
Boston Directions Corpus (BDC) (Hirschberg and
Nakatani, 1996), two publicly available speech cor-
pora with manual ToBI annotations intended for ex-
periments in automatic prosody labeling. We con-
dition prosody not only on word strings and their
parts-of-speech but also on richer syntactic informa-
tion encapsulated in the form of Supertags (Banga-
lore and Joshi, 1999). We propose a maximum en-
tropy modeling framework for the syntactic features.
We model the acoustic-prosodic stream with two dif-
ferent models, a maximum entropy model and a more
traditional hidden markov model (HMM). In an au-
tomatic prosody labeling task, one is essentially try-
1
ing to predict the correct prosody label sequence for
a given utterance and a maximum entropy model of-
fers an elegant solution to this learning problem. The
framework is also robust in the selection of discrim-
inative features for the classification problem. So,
given a word sequence W = {w1, ? ? ? , wn} and a set
of acoustic-prosodic features A = {o1, ? ? ? , oT }, the
best prosodic label sequence L? = {l1, l2, ? ? ? , ln} is
obtained as follows,
L? = argmax
L
P (L|A,W ) (1)
= argmax
L
P (L|W ).P (A|L,W ) (2)
? argmax
L
P (L|?(W )).P (A|L,W ) (3)
where ?(W ) is the syntactic feature encoding of the
word sequence W . The first term in Equation (3)
corresponds to the probability obtained through our
maximum entropy syntactic model. The second term
in Equation (3), computed by an HMM corresponds
to the probability of the acoustic data stream which
is assumed to be dependent only on the prosodic la-
bel sequence.
The paper is organized as follows. In section 2
we describe related work in automatic prosody la-
beling followed by a description of the data used in
our experiments in section 3. We present prosody
prediction results from off-the-shelf synthesizers in
section 4. Section 5 details our proposed maximum
entropy syntactic-prosodic model for prosody label-
ing. In section 6, we describe our acoustic-prosodic
model and discuss our results in section 7. We finally
conclude in section 8 with directions for future work.
2 Related work
Automatic prosody labeling has been an active re-
search topic for over a decade. Wightman and Os-
tendorf (Wightman and Ostendorf, 1994) developed
a decision-tree algorithm for labeling prosodic pat-
terns. The algorithm detected phrasal prominence
and boundary tones at the syllable level. Bulyko
and Ostendorf (Bulyko and Ostendorf, 2001) used
a prosody prediction module to synthesize natural
speech with appropriate prosody. Verbmobil (No?th
et al, 2000) incorporated prosodic labeling into a
translation framework for improved linguistic analy-
sis and speech understanding.
Prosody has typically been represented either sym-
bolically, e.g., ToBI (Silverman et al, 1992) or
parametrically, e.g., Tilt Intonation Model (Tay-
lor, 1998). Parametric approaches either restrict
the variants of prosody by definition or automati-
cally learn prosodic patterns from data (Agu?ero et
al., 2006). The BU corpus is a widely used cor-
pus with symbolic representation of prosody. The
hand-labeled ToBI annotations make this an attrac-
tive corpus to perform prosody labeling experiments.
The main drawback of this corpus is that it com-
prises only read speech. Prosody labeling on sponta-
neous speech corpora like Boston Directions corpus
(BDC), Switchboard (SWBD) has garnered atten-
tion in (Hirschberg and Nakatani, 1996; Gregory and
Altun, 2004).
Automatic prosody labeling has been achieved
through various machine learning techniques, such
as decision trees (Hirschberg, 1993; Wightman and
Ostendorf, 1994; Ma et al, 2003), rule-based sys-
tems (Shimei and McKeown, 1999), bagging and
boosting on CART (Sun, 2002), hidden markov
models (Conkie et al, 1999), neural networks
(Hasegawa-Johnson et al, 2005),maximum-entropy
models (Brenier et al, 2005) and conditional ran-
dom fields (Gregory and Altun, 2004).
Prosody labeling of the BU corpus has been re-
ported in many studies (Hirschberg, 1993; Hasegawa-
Johnson et al, 2005; Ananthakrishnan and
Narayanan, 2005). Hirschberg (Hirschberg, 1993)
used a decision-tree based system that achieved
82.4% speaker dependent accent labeling accuracy
at the word level on the BU corpus using lexical fea-
tures. (Ross and Ostendorf, 1996) also used an ap-
proach similar to (Wightman and Ostendorf, 1994)
to predict prosody for a TTS system from lexical fea-
tures. Pitch accent accuracy at the word-level was
reported to be 82.5% and syllable-level accent accu-
racy was 80.2%. (Hasegawa-Johnson et al, 2005)
proposed a neural network based syntactic-prosodic
model and a gaussian mixture model based acoustic-
prosodic model to predict accent and boundary tones
on the BU corpus that achieved 84.2% accuracy in
accent prediction and 93.0% accuracy in intonational
boundary prediction. With syntactic information
alone they achieved 82.7% and 90.1% for accent and
boundary prediction, respectively. (Ananthakrish-
nan and Narayanan, 2005) modeled the acoustic-
prosodic information using a coupled hidden markov
model that modeled the asynchrony between the
acoustic streams. The pitch accent and boundary
tone detection accuracy at the syllable level were
75% and 88% respectively. Our proposed maximum
entropy syntactic model outperforms previous work.
On the BU corpus, with syntactic information alone
we achieve pitch accent and boundary tone accuracy
of 85.2% and 91.5% on the same training and test
sets used in (Chen et al, 2004; Hasegawa-Johnson
et al, 2005). Further, the coupled model with both
acoustic and syntactic information results in accura-
cies of 86.0% and 93.1% respectively. On the BDC
corpus, we achieve pitch accent and boundary tone
accuracies of 79.8% and 90.3%.
3 Data
The BU corpus consists of broadcast news stories in-
cluding original radio broadcasts and laboratory sim-
2
BU BDC
Corpus statistics f2b f1a m1b m2b h1 h2 h3 h4
# Utterances 165 69 72 51 10 9 9 9
# words (w/o punc) 12608 3681 5058 3608 2234 4127 1456 3008
# pitch accents 6874 2099 2706 2016 1006 1573 678 1333
# boundary tones (w IP) 3916 1059 1282 1023 498 727 361 333
# boundary tones (w/o IP) 2793 684 771 652 308 428 245 216
Table 1: BU and BDC dataset used in experiments
ulations recorded from seven FM radio announcers.
The corpus is annotated with orthographic transcrip-
tion, automatically generated and hand-corrected
part-of-speech tags and automatic phone alignments.
A subset of the corpus is also hand annotated with
ToBI labels. In particular, the experiments in this
paper are carried out on 4 speakers similar to (Chen
et al, 2004), 2 male and 2 female referred to here-
after asm1b, m2b, f1a and f2b. The BDC corpus is
made up of elicited monologues produced by subjects
who were instructed to perform a series of direction-
giving tasks. Both spontaneous and read versions of
the speech are available for four speakers h1, h2, h3
and h4 with hand-annotated ToBI labels and auto-
matic phone alignments, similar to the BU corpus.
Table 1 shows some of the statistics of the speakers
in the BU and BDC corpora.
In Table 1, the pitch accent and boundary tone
statistics are obtained by decomposing the ToBI la-
bels into binary classes using the mapping shown in
Table 2.
BU Labels Intermediate Mapping Coarse Mapping
H*,!H*
L* Single Accent
*,*?,X*? accent
H+!H*,L+H*,L+!H* Bitonal Accent
L*+!H,L*+H
L-L%,!H-L%,H-L%
H-H% Final Boundary tone
L-H%
%?,X%?,%H btone
L-,H-,!H- Intermediate Phrase (IP) boundary
-X?,-?
<,>,no label none none
Table 2: ToBI label mapping used in experiments
In all our prosody labeling experiments we adopt
a leave-one-out speaker validation similar to the
method in (Hasegawa-Johnson et al, 2005) for the
four speakers with data from one speaker for testing
and from the other three for training. For the BU
corpus, f2b speaker was always used in the training
set since it contains the most data. In addition to
performing experiments on all the utterances in BU
corpus, we also perform identical experiments on the
train and test sets reported in (Chen et al, 2004)
which is referred to as Hasegawa-Johnson et al set.
4 Baseline Experiments
We present three baseline experiments. One is sim-
ply based on chance where the majority class label is
predicted. The second is a baseline only for pitch ac-
cents derived from the lexical stress obtained through
look-up from a pronunciation lexicon labeled with
stress. Finally, the third and more concrete base-
line is obtained through prosody detection in current
speech synthesis systems.
4.1 Prosody labels derived from lexical
stress
Pitch accents are usually carried by the stressed syl-
lable in a particular word. Lexicons with phonetic
transcription and lexical stress are available in many
languages. Hence, one can use these lexical stress
markers within the syllables and evaluate the corre-
lation with pitch accents. Eventhough the lexicon
has a closed vocabulary, letter-to-sound rules can be
derived from it for unseen words. For each word car-
rying a pitch accent, we find the particular syllable
where the pitch accent occurs from the manual anno-
tation. For the same syllable, we predict pitch accent
based on the presence or absence of a lexical stress
marker in the phonetic transcription. The results are
presented in Table 3.
4.2 Prosody labeling with Festival and
AT&T Natural Voices R? Speech
Synthesizer
Festival (Black et al, 1998) and AT&T Natural
Voices R? (NV) speech synthesizer (att, ) are two
publicly available speech synthesizers that have a
prosody prediction module available. We performed
automatic prosody labeling using the two synthesiz-
ers to get a baseline.
4.2.1 AT&T Natural Voices R? Speech
Synthesizer
The AT&T NV R? speech synthesizer is a half
phone speech synthesizer. The toolkit accepts
an input text utterance and predicts appropriate
ToBI pitch accent and boundary tones for each of
3
Pitch accent Boundary tone
Corpus Speaker Set Prediction Module Chance Accuracy Chance Accuracy
Lexical stress 54.33 72.64 - -
Entire Set AT&T Natural Voices 54.33 81.51 81.14 89.10
Festival 54.33 69.55 81.14 89.54
Lexical stress 56.53 74.10 - -
BU Hasegawa-Johnson et al set AT&T Natural Voices 56.53 81.73 82.88 89.67
Festival 56.53 68.65 82.88 90.21
Lexical stress 57.60 67.42 - -
BDC Entire Set AT&T Natural Voices 57.60 68.49 88.90 84.90
Festival 57.60 64.94 88.90 85.17
Table 3: Classification results of pitch accents and boundary tones (in %) using Festival and AT&T NV R? synthesizer
the selected units (in this case, a pair of phones)
from the database. We reverse mapped the se-
lected half phone units to words, thus obtaining
the ToBI labels for each word in the input utter-
ance. The toolkit uses a rule-based procedure to
predict the ToBI labels from lexical information.
The pitch accent labels predicted by the toolkit are
Laccent  {H?,L?,none} and the boundary tones
are Lbtone  {L-L%,H-H%,L-H%,none}.
4.2.2 Festival Speech Synthesizer
Festival (Black et al, 1998) is an open-source unit
selection speech synthesizer. The toolkit includes
a CART-based prediction system that can predict
ToBI pitch accents and boundary tones for the input
text utterance. The pitch accent labels predicted by
the toolkit are Laccent  {H?,L+H?, !H?,none}
and the boundary tones are
Lbtone  {L-L%,H-H%,L-H%,none}. The
prosody labeling results obtained through both the
speech synthesis engines are presented in Table
3. The chance column in Table 3 is obtained by
predicting the most frequent label in the data set.
In the next sections, we describe our proposed
maximum entropy based syntactic model and HMM
based acoustic-prosodic model for automatic prosody
labeling.
5 Syntactic-prosodic Model
We propose a maximum entropy approach to model
the words, syntactic information and the prosodic
labels as a sequence. We model the prediction prob-
lem as a classification task as follows: given a se-
quence of words wi in a sentence W = {w1, ? ? ? , wn}
and a prosodic label vocabulary (li  L), we need
to predict the best prosodic label sequence L? =
{l1, l2, ? ? ? , ln}. We approximate the conditional
probability to be within a bounded n-gram context.
Thus,
L? = argmax
L
P (L|W,T, S) (4)
? argmax
L
n?
i
p(li|w
i+k
i?k, t
i+k
i?k, s
i+k
i?k) (5)
where W = {w1, ? ? ? , wn} is the word sequence and
T = {t1, ? ? ? , tn}, S = {s1, ? ? ? , sn} are the corre-
sponding part-of-speech and additional syntactic in-
formation sequences. The variable k controls the
context.
The BU corpus is automatically labeled (and
hand-corrected) with part-of-speech (POS) tags.
The POS inventory is the same as the Penn treebank
which includes 47 POS tags: 22 open class categories,
14 closed class categories and 11 punctuation labels.
We also automatically tagged the utterances using
the AT&T POS tagger. The POS tags were mapped
to function and content word categories 1 which was
added as a discrete feature. In addition to the POS
tags, we also annotate the utterance with Supertags
(Bangalore and Joshi, 1999). Supertags encapsulate
predicate-argument information in a local structure.
They are composed with each other using substi-
tution and adjunction operations of Tree-Adjoining
Grammars (TAGs) to derive a dependency analysis
of an utterance and its predicate-argument structure.
Even though there is a potential to exploit the de-
pendency structure between supertags and prosody
labels as demonstrated in (Hirschberg and Rambow,
2001), for this paper we use only the supertag labels.
Finally, we generate one feature vector (?) for
each word in the data set (with local contextual fea-
tures). The best prosodic label sequence is then,
L? = argmax
L
n?
i
P (li|?) (6)
To estimate the conditional distribution P (li|?) we
use the general technique of choosing the maximum
entropy (maxent) distribution that estimates the av-
erage of each feature over the training data (Berger
et al, 1996). This can be written in terms of Gibbs
distribution parameterized with weights ?, where V
is the size of the prosodic label set. Hence,
P (li|?) =
e?li .?
?V
l=1 e
?li .?
(7)
1function and content word features were obtained
through a look-up table based on POS
4
k=3
Corpus Speaker Set Syntactic features accent btone
correct POS tags 84.75 91.39
Entire Set AT&T POS + supertags 84.59 91.34
BU Joint Model (w AT&T POS + supertags) 84.60 91.36
correct POS tags 85.22 91.33
Hasegawa-Johnson et al set AT&T POS + supertags 84.95 91.21
Joint Model (w AT&T POS + supertags) 84.78 91.54
BDC Entire Set AT&T POS + supertags 79.81 90.28
Joint Model (w AT&T POS + supertags) 79.57 89.76
Table 4: Classification results (%) of pitch accents and boundary tones for different syntactic representation (k = 3)
We use the machine learning toolkit LLAMA
(Haffner, 2006) to estimate the conditional distribu-
tion using maxent. LLAMA encodes multiclass max-
ent as binary maxent to increase the training speed
and to scale the method to large data sets. Each of
the V classes in the label set L is encoded as a bit
vector such that, in the vector for class i, the ith bit
is one and all other bits are zero. Finally, V one-
versus-other binary classifiers are used as follows.
P (y|?) = 1? P (y?|?) =
e?y.?
e?y.? + e?y?.?
(8)
where ?y? is the parameter vector for the anti-label y?.
To compute P (li|?), we use the class independence
assumption and require that yi = 1 and for all j 6=
i, yj = 0.
P (li|?) = P (yi|?)
V?
j 6=i
P (yj |?) (9)
5.1 Joint Modeling of Accents and
Boundary Tones
Prosodic prominence and phrasing can also be
viewed as joint events occurring simultaneously. Pre-
vious work by (Wightman and Ostendorf, 1994) sug-
gests that a joint labeling approach may be more
beneficial in prosody labeling. In this scenario,
we treat each word to have one of the four labels
li  L = {accent-btone, accent-none, none-
btone, none-none}. We trained the classifier on
the joint labels and then computed the error rates for
individual classes. The results of prosody prediction
using the set of syntactic-prosodic features for k = 3
is shown in Table 4. The joint modeling approach
provides a marginal improvement in the boundary
tone prediction but is slightly worse for pitch accent
prediction.
5.2 Supertagger performance on
Intermediate Phrase boundaries
Perceptual experiments have indicated that inter-
annotator agreement for ToBI intermediate phrase
boundaries is very low compared to full-intonational
boundaries (Syrdal and McGory, 2000). Interme-
diate phrasing is important in TTS applications to
synthesize appropriate short pauses to make the ut-
terance sound natural. The significance of syntactic
features in the boundary tone prediction prompted
us to examine the effect of predicting intermediate
phrase boundaries in isolation. It is intuitive to ex-
pect supertags to perform well in this task as they
essentially form a local dependency analysis on an
utterance and provide an encoding of the syntactic
phrasal information. We performed this task as a
three way classification where li  L = {btone, ip,
none}. The results of the classifier on IPs is shown
in Table 5.
Model Syntactic features IP accuracy
correct POS tags 83.25
k=2 (bigram context) AT&T POS tags 83.32
supertags 83.37
correct POS tags 83.30
k=3 (trigram context) AT&T POS tags 83.46
supertags 83.74
Table 5: Accuracy (in %) obtained by leave-one out
speaker validation using IPs as a separate class on
entire speaker set
6 Acoustic-prosodic model
We propose two approaches to modeling the
acoustic-prosodic features for prosody prediction.
First, we propose a maximum entropy framework
similar to the syntactic model where we quantize
the acoustic features and model them as discrete
sequences. Second, we use a more traditional ap-
proach where we train continuous observation den-
sity HMMs to represent pitch accents and bound-
ary tones. We first describe the features used in the
acoustic modeling followed by a more detailed de-
scription of the acoustic-prosodic model.
6.1 Acoustic-prosodic features
The BU corpus contains the corresponding acoustic-
prosodic feature file for each utterance. The f0, RMS
energy (e) of the utterance along with features for
5
Pitch accent Boundary tone
Corpus Speaker Set Model Acoustics Acoustics+syntax Acoustics Acoustics+syntax
Entire Set Maxent acoustic model 80.09 84.53 84.10 91.56
HMM acoustic model 70.58 85.13 71.28 92.91
BU Hasegawa-Johnson et al set Maxent acoustic model 80.12 84.84 82.70 91.76
HMM acoustic model 71.42 86.01 73.43 93.09
BDC Entire Set Maxent acoustic model 74.51 78.64 83.53 90.49
Table 6: Classification results of pitch accents and boundary tones (in %) with acoustics only and acoustics+syntax
using both our models
distinction between voiced/unvoiced segment, cross-
correlation values at estimated f0 value and ratio of
first two cross correlation values are computed over
10 msec frame intervals. In our experiments, we use
these values rather than computing them explicitly
which is straightforward with most audio toolkits.
Both the energy and the f0 levels were normalized
with speaker specific means and variances. Delta
and acceleration coefficients were also computed for
each frame. The final feature vector is 6-dimensional
comprising of f0, ?f0, ?2f0, e, ?e, ?2e per frame.
6.2 Maximum Entropy acoustic-prosodic
model
We propose a maximum entropy modeling frame-
work to model the continuous acoustic-prosodic ob-
servation sequence as a discrete sequence through
the means of quantization. The quantized acoustic
stream is then used as a feature vector and the condi-
tional probabilities are approximated by an n-gram
model. This is equivalent to reducing the vocabu-
lary of the acoustic-prosodic features and hence of-
fers better estimates of the conditional probabilities.
Such an n-gram model of quantized continuous fea-
tures is similar to representing the set of features
with a linear fit as done in the tilt intonational model
(Taylor, 1998).
The quantized acoustic-prosodic feature stream is
modeled with a maxent acoustic-prosodic model sim-
ilar to the one described in section 5. Finally, we ap-
pend the syntactic and acoustic features to model the
combined stream with the maxent acoustic-syntactic
model, where the objective criterion for maximiza-
tion is Equation (1). The pitch accent and bound-
ary tone prediction accuracies for quantization per-
formed by considering only the first decimal place
is reported in Table 6. As expected, we found the
classification accuracy to drop with increasing num-
ber of bins used in the quantization due to the small
amount of training data.
6.3 HMM acoustic-prosodic model
We also investigated the traditional HMM approach
to model the high variability exhibited by the
acoustic-prosodic features. First, we trained sepa-
rate context independent single state Gaussian mix-
ture density HMMs for pitch accents and boundary
tones in a generative framework. The label sequence
was decoded using the viterbi algorithm. Next, we
trained HMMs with 3 state left-to-right topology
with uniform segmentation. The segmentations need
to be uniform due to lack of an acoustic-prosodic
model trained on the features pertinent to our task
to obtain forced segmentation.
The final label sequence using the maximum en-
tropy syntactic-prosodic model and the HMM based
acoustic-prosodic model was obtained by combin-
ing the syntactic and acoustic probabilities shown in
Equation (3). The syntactic-prosodic maxent model
outputs a posterior probability for each class per
word. We formed a lattice out of this structure and
composed it with the lattice generated by the HMM
acoustic-prosodic model. The best path was chosen
from the composed lattice through a Viterbi search.
The acoustic-prosodic probability P (A|L,W ) was
raised by a power of ? to adjust the weighting be-
tween the acoustic and syntactic model. The value of
? was chosen as 0.008 and 0.015 for pitch accent and
boundary tone respectively, by tuning on the train-
ing set. The results of the acoustic-prosodic model
and the coupled model are shown in Table 6.
7 Discussion
The baseline experiment with lexical stress obtained
from a pronunciation lexicon for prediction of pitch
accent yields substantially higher accuracy than
chance. This could be particularly useful in resource-
limited languages where prosody labels are usually
not available but one has access to a reasonable lex-
icon with lexical stress markers. Off-the-shelf speech
synthesizers like Festival and AT&T speech synthe-
sizer perform reasonably well in pitch accent and
boundary tone prediction. AT&T speech synthesizer
performs better than Festival in pitch accent predic-
tion and the latter performs better in boundary tone
prediction. This can be attributed to better rules
in the AT&T synthesizer for pitch accent prediction.
Boundary tones are usually highly correlated with
punctuation and Festival seems to capture this well.
However, both these synthesizers generate a high de-
6
gree of false alarms.
Our syntactic-prosodic maximum entropy model
proposed in section 5 outperforms previously re-
ported results on pitch accent and boundary tone
classification. Much of the gain comes from the ro-
bustness of the maximum entropy modeling in cap-
turing the uncertainty in the classification task. Con-
sidering the inter-annotator agreement for ToBI la-
bels is only about 81% for pitch accents and 93% for
boundary tones, the maximum entropy framework is
able to capture the uncertainty present in manual an-
notation. The supertag feature offers additional dis-
criminative information over the part-of-speech tags
(also as shown by (Hirschberg and Rambow, 2001).
The maximum entropy acoustic-prosodic model
discussed in section 6.2 performs reasonably well in
isolation. This is a simple method and the quantiza-
tion resolution can be adjusted based on the amount
of data available for training. However, the model
does not perform as well when combined with the
syntactic features. We conjecture that the gener-
alization provided by the acoustic HMM model is
complementary to that provided by the maximum
entropy model, resulting in better accuracy when
combined together as compared to that of a maxent-
based acoustic and syntactic model.
The weighted maximum entropy syntactic-
prosodic model and HMM acoustic-prosodic model
performs the best in pitch accent and boundary tone
classification. The classification accuracies are as
good as the inter-annotator agreement for the ToBI
labels. Our HMM acoustic-prosodic model is a gen-
erative model and does not assume the knowledge
of word boundaries in predicting the prosodic labels
as in most approaches (Hirschberg, 1993; Wightman
and Ostendorf, 1994; Hasegawa-Johnson et al,
2005). This makes it possible to have true parallel
prosody prediction during speech recognition. The
weighted approach also offers flexibility in prosody
labeling for either speech synthesis or speech recog-
nition. While the syntactic-prosodic model would
be more discriminative for speech synthesis, the
acoustic-prosodic model is more appropriate for
speech recognition.
8 Conclusions and Future Work
In this paper, we described a maximum entropy
modeling framework for automatic prosody label-
ing. We presented two schemes for prosody label-
ing that utilize the acoustic and syntactic informa-
tion from the input utterance, a maximum entropy
model that models the acoustic-syntactic informa-
tion as a sequence and the other that combines the
maximum entropy syntactic-prosodic model and a
HMM based acoustic-prosodic model. We also used
enriched syntactic information in the form of su-
pertags in addition to POS tags. The supertags
provide an improvement in both the pitch accent
and boundary tone classification. Especially, in the
case where the input utterance is automatically POS
tagged (and not hand-corrected), supertags provide
a marginal but definite improvement in prosody la-
beling. The maximum entropy syntactic-prosodic
model alone resulted in pitch accent and bound-
ary tone accuracies of 85.2% and 91.5% on training
and test sets identical to (Chen et al, 2004). As
far as we know, these are the best results on the
BU corpus using syntactic information alone and a
train-test split that does not contain the same speak-
ers. The acoustic-syntactic maximum entropy model
performs better than its syntactic-prosodic counter-
part for the boundary tone case but is slightly worse
for pitch accent scenario partly due to the approx-
imation involved in quantization. But these results
are still better than the baseline results from out-
of-the-box speech synthesizers. Finally, our com-
bined maximum entropy syntactic-prosodic model
and HMM acoustic-prosodic model performs the best
with pitch accent and boundary tone labeling accu-
racies of 86.0% and 93.1% respectively.
As a continuation of our work, we are incorpo-
rating our automatic prosody labeler in a speech-
to-speech translation framework. Typically, state-
of-the-art speech translation systems have a source
language recognizer followed by a machine transla-
tion system. The translated text is then synthesized
in the target language with prosody predicted from
text. In this process, some of the critical prosodic
information present in the source data is lost during
translation. With reliable prosody labeling in the
source language, one can transfer the prosody to the
target language (this is feasible for languages with
phrase level correspondence). The prosody labels by
themselves may or may not improve the translation
accuracy but they provide a framework where one
can obtain prosody labels in the target language from
the speech signal rather than depending on a lexical
prosody prediction module in the target language.
Acknowledgements
We would like to thank Vincent Goffin, Stephan
Kanthak, Patrick Haffner, Enrico Bocchieri for their
support with acoustic modeling tools. We are also
thankful to Alistair Conkie, Yeon-Jun Kim, Ann
Syrdal and Julia Hirschberg for their help and guid-
ance with the synthesis components and ToBI label-
ing standard.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006.
Prosody generation for speech-to-speech transla-
7
tion. In Proceedings of ICASSP, Toulouse, France,
May.
S. Ananthakrishnan and S. Narayanan. 2005. An au-
tomatic prosody recognizer using a coupled multi-
stream acoustic model and a syntactic-prosodic
language model. In In Proceedings of ICASSP,
Philadelphia, PA, March.
AT&T Natural Voices speech synthesizer.
http://www.naturalvoices.att.com.
S. Bangalore and A. K. Joshi. 1999. Supertagging:
An approach to almost parsing. Computational
Linguistics, 25(2), June.
A. Berger, S. D. Pietra, and V. D. Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
A. W. Black, P. Taylor, and R. Caley.
1998. The Festival speech synthesis system.
http://festvox.org/festival.
J. M. Brenier, D. Cer, and D. Jurafsky. 2005. The
detection of emphatic words using acoustic and
lexical features. In In Proceedings of Eurospeech.
I. Bulyko and M. Ostendorf. 2001. Joint prosody
prediction and unit selection for concatenative
speech synthesis. In Proc. of ICASSP.
K. Chen, M. Hasegawa-Johnson, and A. Cohen.
2004. An automatic prosody labeling system using
ANN-based syntactic-prosodic model and GMM-
based acoustic-prosodic model. In Proceedings of
ICASSP.
A. Conkie, G. Riccardi, and R. C. Rose. 1999.
Prosody recognition from speech utterances using
acoustic and linguistic based models of prosodic
events. In Proc. Eurospeech, pages 523?526, Bu-
dapest, Hungary.
M. Gregory and Y. Altun. 2004. Using conditional
random fields to predict pitch accent in conver-
sational speech. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL).
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Commu-
nication, 48(iv):239?261.
M. Hasegawa-Johnson, K. Chen, J. Cole, S. Borys,
S. Kim, A. Cohen, T. Zhang, J. Choi, H. Kim,
T. Yoon, and S. Chavara. 2005. Simultaneous
recognition of words and prosody in the boston
university radio speech corpus. Speech Communi-
cation, 46:418?439.
J. Hirschberg and C. Nakatani. 1996. A prosodic
analysis of discourse segments in direction-giving
monologues. In Proceedings of the 34th confer-
ence on Association for Computational Linguis-
tics, pages 286?293.
J. Hirschberg and O. Rambow. 2001. Learning
prosodic features using a tree representation. In
Proceedings of Eurospeech, pages 1175?1180, Aal-
borg.
J. Hirschberg. 1993. Pitch accent in context: Pre-
dicting intonational prominence from text. Artifi-
cial Intelligence, 63(1-2).
I. Lehiste. 1970. Suprasegmentals. MIT Press, Cam-
bridge, MA.
X. Ma, W. Zhang, Q. Shi, W. Zhu, and L. Shen.
2003. Automatic prosody labeling using both
text and acoustic information. In Proceedings of
ICASSP, volume 1, pages 516?519, April.
E. No?th, A. Batliner, A. Kie?ling, R. Kompe, and
H. Niemann. 2000. VERBMOBIL: The use of
prosody in the linguistic components of a speech
understanding system. IEEE Transactions on
Speech and Audio processing, 8(5):519?532.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical Report ECS-95-001, Boston University,
March.
K. Ross and M. Ostendorf. 1996. Prediction of ab-
stract prosodic labels for speech synthesis. Com-
puter Speech and Language, 10:155?185, Oct.
P. Shimei and K. McKeown. 1999. Word infor-
mativeness and automatic pitch accent modeling.
In In Proceedings of EMNLP/VLC, College Park,
Maryland.
K. Silverman, M. Beckman, J. Pitrelli, M. Osten-
dorf, C. Wightman, P. Price, J. Pierrehumbert,
and J. Hirschberg. 1992. ToBI: A standard for la-
beling English prosody. In Proceedings of ICSLP,
pages 867?870.
X. Sun. 2002. Pitch accent prediction using ensem-
ble machine learning. In Proc. of ICSLP.
A. K. Syrdal and J. McGory. 2000. Inter-transcriber
reliability of tobi prosodic labeling. In Proc. IC-
SLP, pages 235?238, Beijing, China.
P. Taylor. 1998. The tilt intonation model. In Proc.
ICSLP, volume 4, pages 1383?1386.
C. W. Wightman and M. Ostendorf. 1994. Auto-
matic labeling of prosodic patterns. IEEE Trans-
actions on Speech and Audio Processing, 2(3):469?
481.
8
Proceedings of NAACL HLT 2009: Short Papers, pages 185?188,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
MICA: A Probabilistic Dependency Parser
Based on Tree Insertion Grammars
Application Note
Srinivas Bangalore Pierre Boulllier
AT&T Labs ? Research INRIA
Florham Park, NJ, USA Rocquencourt, France
srini@research.att.com Pierre.Boullier@inria.fr
Alexis Nasr Owen Rambow Beno??t Sagot
Aix-Marseille Universite? CCLS, Columbia Univserity INRIA
Marseille, France New York, NY, USA Rocquencourt, France
alexis.nasr@lif.univ-mrs.fr rambow@ccls.columbia.edu benoit.sagot@inria.fr
Abstract
MICA is a dependency parser which returns
deep dependency representations, is fast, has
state-of-the-art performance, and is freely
available.
1 Overview
This application note presents a freely avail-
able parser, MICA (Marseille-INRIA-Columbia-
AT&T).1 MICA has several key characteristics that
make it appealing to researchers in NLP who need
an off-the-shelf parser.
? MICA returns a deep dependency parse, in
which dependency is defined in terms of lex-
ical predicate-argument structure, not in terms
of surface-syntactic features such as subject-verb
agreement. Function words such as auxiliaries
and determiners depend on their lexical head, and
strongly governed prepositions (such as to for give)
are treated as co-heads rather than as syntactic heads
in their own right. For example, John is giving books
to Mary gets the following analysis (the arc label is
on the terminal).
giving
John
arc=0
is
arc=adj
books
arc=1
to
arc=co-head
Mary
arc=2
The arc labels for the three arguments John,
books, and Mary do not change when the sentence
is passivized or Mary undergoes dative shift.
1We would like to thank Ryan Roth for contributing the
MALT data.
? MICA is based on an explicit phrase-structure
tree grammar extracted from the Penn Treebank.
Therefore, MICA can associate dependency parses
with rich linguistic information such as voice, the
presence of empty subjects (PRO), wh-movement,
and whether a verb heads a relative clause.
?MICA is fast (450 words per second plus 6 sec-
onds initialization on a standard high-end machine
on sentences with fewer than 200 words) and has
state-of-the-art performance (87.6% unlabeled de-
pendency accuracy, see Section 5).
? MICA consists of two processes: the supertag-
ger, which associates tags representing rich syntac-
tic information with the input word sequence, and
the actual parser, which derives the syntactic struc-
ture from the n-best chosen supertags. Only the su-
pertagger uses lexical information, the parser only
sees the supertag hypotheses.
? MICA returns n-best parses for arbitrary n;
parse trees are associated with probabilities. A
packed forest can also be returned.
? MICA is freely available2, easy to install under
Linux, and easy to use. (Input is one sentence per
line with no special tokenization required.)
There is an enormous amount of related work,
and we can mention only the most salient, given
space constraints. Our parser is very similar to the
work of (Shen and Joshi, 2005). They do not em-
ploy a supertagging step, and we do not restrict our
trees to spinal projections. Other parsers using su-
pertagging include the LDA of Bangalore and Joshi
(1999), the CCG-based parser of Clark and Curran
(2004), and the constraint-based approach of Wang
2http://www1.ccls.columbia.edu/?rambow/mica.html
185
and Harper (2004). Widely used dependency parsers
which generate deep dependency representations in-
clude Minipar (Lin, 1994), which uses a declarative
grammar, and the Stanford parser (Levy and Man-
ning, 2004), which performs a conversion from a
standard phrase-structure parse. All of these systems
generate dependency structures which are slightly
different from MICA?s, so that direct comparison
is difficult. For comparison purposes, we therefore
use the MALT parser generator (Nivre et al, 2004),
which allows us to train a dependency parser on our
own dependency structures. MALT has been among
the top performers in the CoNLL dependency pars-
ing competitions.
2 Supertags and Supertagging
Supertags are elementary trees of a lexicalized
tree grammar such as a Tree-Adjoining Gram-
mar (TAG) (Joshi, 1987). Unlike context-free gram-
mar rules which are single level trees, supertags are
multi-level trees which encapsulate both predicate-
argument structure of the anchor lexeme (by includ-
ing nodes at which its arguments must substitute)
and morpho-syntactic constraints such as subject-
verb agreement within the supertag associated with
the anchor. There are a number of supertags for each
lexeme to account for the different syntactic trans-
formations (relative clause, wh-question, passiviza-
tion etc.). For example, the verb give will be associ-
ated with at least these two trees, which we will call
tdi and tdi-dat. (There are also many other trees.)
tdi tdi-dat
S
NP0 ? VP
V? NP1 ? PP
P
to
NP2 ?
S
NP0 ? VP
V? NP2 ?NP1 ?
Supertagging is the task of disambiguating among
the set of supertags associated with each word in
a sentence, given the context of the sentence. In
order to arrive at a complete parse, the only step
remaining after supertagging is establishing the at-
tachments among the supertags. Hence the result of
supertagging is termed as an ?almost parse? (Banga-
lore and Joshi, 1999).
The set of supertags is derived from the Penn
Treebank using the approach of Chen (2001). This
extraction procedure results in a supertag set of
4,727 supertags and about one million words of su-
pertag annotated corpus. We use 950,028 annotated
words for training (Sections 02-21) and 46,451 (Sec-
tion 00) annotated words for testing in our exper-
iments. We estimate the probability of a tag se-
quence directly as in discriminative classification
approaches. In such approaches, the context of the
word being supertagged is encoded as features for
the classifier. Given the large scale multiclass la-
beling nature of the supertagging task, we train su-
pertagging models as one-vs-rest binary classifica-
tion problems. Detailed supertagging experiment re-
sults are reported in (Bangalore et al, 2005) which
we summarize here. We use the lexical, part-of-
speech attributes from the left and right context
in a 6-word window and the lexical, orthographic
(e.g. capitalization, prefix, suffix, digit) and part-
of-speech attributes of the word being supertagged.
Crucially, this set does not use the supertags for the
words in the history. Thus during decoding the su-
pertag assignment is done locally and does not need
a dynamic programming search. We trained a Max-
ent model with such features using the labeled data
set mentioned above and achieve an error rate of
11.48% on the test set.
3 Grammars and Models
MICA grammars are extracted in a three steps pro-
cess. In a first step, a Tree Insertion Grammar (TIG)
(Schabes and Waters, 1995) is extracted from the
treebank, along with a table of counts. This is the
grammar that is used for supertagging, as described
in Section 2. In a second step, the TIG and the count
table are used to build a PCFG. During the last step,
the PCFG is ?specialized? in order to model more
finely some lexico-syntactic phenomena. The sec-
ond and third steps are discussed in this section.
The extracted TIG is transformed into a PCFG
which generates strings of supertags as follows. Ini-
tial elementary trees (which are substituted) yield
rules whose left hand side is the root category of
the elementary tree. Left (respectively right) aux-
iliary trees (the trees for which the foot node is the
186
left (resp. right) daughter of the root) give birth to
rules whose left-hand side is of the form Xl (resp.
Xr), where X is the root category of the elementary
tree. The right hand side of each rule is built during
a top down traversal of the corresponding elemen-
tary tree. For every node of the tree visited, a new
symbol is added to the right hand side of rule, from
left to right, as follows:
? The anchor of the elementary tree adds the su-
pertag (i.e., the name of the tree), which is a terminal
symbol, to the context-free rule.
? A substitution node in the elementary tree adds
its nonterminal symbol to the context-free rule.
? A interior node in the elementary tree at which
adjunction may occur adds to the context-free rule
the nonterminal symbol X ?r or X ?l , where X is the
node?s nonterminal symbol, and l (resp. r) indicates
whether it is a left (resp. right) adjunction. Each
interior node is visited twice, the first time from the
left, and then from the right. A set of non-lexicalized
rules (i.e., rules that do not generate a terminal sym-
bol) allow us to generate zero or more trees anchored
by Xl from the symbol X ?l . No adjunction, the first
adjunction, and the second adjunction are modeled
explicitly in the grammar and the associated prob-
abilistic model, while the third and all subsequent
adjunctions are modeled together.
This conversion method is basically the same as
that presented in (Schabes and Waters, 1995), ex-
cept that our PCFG models multiple adjunctions at
the same node by positions (a concern Schabes and
Waters (1995) do not share, of course). Our PCFG
construction differs from that of Hwa (2001) in that
she does not allow multiple adjunction at one node
(Schabes and Shieber, 1994) (which we do since we
are interested in the derivation structure as a repre-
sentation of linguistic dependency). For more in-
formation about the positional model of adjunction
and a discussion of an alternate model, the ?bigram
model?, see (Nasr and Rambow, 2006).
Tree tdi from Section 2 gives rise to the following
rule (where tdi and tCO are terminal symbols and
the rest are nonterminals): S ? S?l NP VP?l V?l tdiV?r NP PP?l P?l tCO P?r NP PP?r VP?r S?r
The probabilities of the PCFG rules are estimated
using maximum likelihood. The probabilistic model
refers only to supertag names, not to words. In the
basic model, the probability of the adjunction or sub-
stitution of an elementary tree (the daughter) in an-
other elementary tree (the mother) only depends on
the nonterminal, and does not depend on the mother
nor on the node on which the attachment is per-
formed in the mother elementary tree. It is well
known that such a dependency is important for an
adequate probabilistic modelling of syntax. In order
to introduce such a dependency, we condition an at-
tachment on the mother and on the node on which
the attachment is performed, an operation that we
call mother specialization. Mother specialization is
performed by adding to all nonterminals the name of
the mother and the address of a node. The special-
ization of a grammar increase vastly the number of
symbols and rules and provoke severe data sparse-
ness problems, this is why only a subset of the sym-
bols are specialized.
4 Parser
SYNTAX (Boullier and Deschamp, 1988) is a sys-
tem used to generate lexical and syntactic analyzers
(parsers) (both deterministic and non-deterministic)
for all kind of context-free grammars (CFGs) as
well as some classes of contextual grammars. It
has been under development at INRIA for several
decades. SYNTAX handles most classes of determin-
istic (unambiguous) grammars (LR, LALR, RLR)
as well as general context-free grammars. The
non-deterministic features include, among others,
an Earley-like parser generator used for natural lan-
guage processing (Boullier, 2003).
Like most SYNTAX Earley-like parsers, the archi-
tecture of MICA?s PCFG-based parser is the follow-
ing:
? The Earley-like parser proper computes a shared
parse forest that represents in a factorized (polyno-
mial) way all possible parse trees according to the
underlying (non-probabilistic) CFG that represents
the TIG;
? Filtering and/or decoration modules are applied
on the shared parse forest; in MICA?s case, an n-
best module is applied, followed by a dependency
extractor that relies on the TIG structure of the CFG.
The Earley-like parser relies on Earley?s algo-
rithm (Earley, 1970). However, several optimiza-
tions have been applied, including guiding tech-
niques (Boullier, 2003), extensive static (offline)
187
computations over the grammar, and efficient data
structures. Moreover, Earley?s algorithm has been
extended so as to handle input DAGs (and not only
sequences of forms). A particular effort has been
made to handle huge grammars (over 1 million
symbol occurrences in the grammar), thanks to ad-
vanced dynamic lexicalization techniques (Boullier
and Sagot, 2007). The resulting efficiency is satisfy-
ing: with standard ambiguous NLP grammars, huge
shared parse forest (over 1010 trees) are often gener-
ated in a few dozens of milliseconds.
Within MICA, the first module that is applied on
top of the shared parse forest is SYNTAX?s n-best
module. This module adapts and implements the al-
gorithm of (Huang and Chiang, 2005) for efficient
n-best trees extraction from a shared parse forest. In
practice, and within the current version of MICA,
this module is usually used with n = 1, which iden-
tifies the optimal tree w.r.t. the probabilistic model
embedded in the original PCFG; other values can
also be used. Once the n-best trees have been ex-
tracted, the dependency extractor module transforms
each of these trees into a dependency tree, by ex-
ploiting the fact that the CFG used for parsing has
been built from a TIG.
5 Evaluation
We compare MICA to the MALT parser. Both
parsers are trained on sections 02-21 of our de-
pendency version of the WSJ PennTreebank, and
tested on Section 00, not counting true punctuation.
?Predicted? refers to tags (PTB-tagset POS and su-
pertags) predicted by our taggers; ?Gold? refers to
the gold POS and supertags. We tested MALT using
only POS tags (MALT-POS), and POS tags as well
as 1-best supertags (MALT-all). We provide unla-
beled (?Un?) and labeled (?Lb?) dependency accu-
racy (%). As we can see, the predicted supertags do
not help MALT. MALT is significantly slower than
MICA, running at about 30 words a second (MICA:
450 words a second).
MICA MALT-POS MALT-all
Pred Gold Pred Gold Pred Gold
Lb 85.8 97.3 86.9 87.4 86.8 96.9
Un 87.6 97.6 88.9 89.3 88.5 97.2
References
Srinivas Bangalore and Aravind Joshi. 1999. Supertag-
ging: An approach to almost parsing. Computational
Linguistics, 25(2):237?266.
Srinivas Bangalore, Patrick Haffner, and Gae?l Emami.
2005. Factoring global inference by enriching local rep-
resentations. Technical report, AT&T Labs ? Reserach.
Pierre Boullier and Philippe Deschamp.
1988. Le syste`me SYNTAXTM ? manuel
d?utilisation et de mise en ?uvre sous UNIXTM.
http://syntax.gforge.inria.fr/syntax3.8-manual.pdf.
Pierre Boullier and Beno??t Sagot. 2007. Are very large
grammars computationnaly tractable? In Proceedings of
IWPT?07, Prague, Czech Republic.
Pierre Boullier. 2003. Guided Earley parsing. In Pro-
ceedings of the 7th International Workshop on =20 Pars-
ing Technologies, pages 43?54, Nancy, France.
John Chen. 2001. Towards Efficient Statistical Parsing
Using Lexicalized Grammatical Information. Ph.D. the-
sis, University of Delaware.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL?04.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communication of the ACM, 13(2):94?102.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT?05, Vancouver, Canada.
Rebecca Hwa. 2001. Learning Probabilistic Lexicalized
Grammars for Natural Language Processing. Ph.D. the-
sis, Harvard University.
Aravind K. Joshi. 1987. An introduction to Tree Ad-
joining Grammars. In A. Manaster-Ramer, editor, Math-
ematics of Language. John Benjamins, Amsterdam.
Roger Levy and Christopher Manning. 2004. Deep de-
pendencies from context-free statistical parsers: Correct-
ing the surface dependency approximation. In ACL?04.
Dekang Lin. 1994. PRINCIPAR?an efficient, broad-
coverage, principle-based parser. In Coling?94.
Alexis Nasr and Owen Rambow. 2006. Parsing with
lexicalized probabilistic recursive transition networks. In
Finite-State Methods and Natural Language Processing,
Springer Verlag Lecture Notes in Commputer Science.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In CoNLL-2004.
Yves Schabes and Stuart Shieber. 1994. An alternative
conception of tree-adjoining derivation. Computational
Linguistics, 1(20):91?124.
Yves Schabes and Richard C. Waters. 1995. Tree Inser-
tion Grammar. Computational Linguistics, 21(4).
Libin Shen and Aravind Joshi. 2005. Incremental ltag
parsing. In HLT-EMNLP?05.
Wen Wang and Mary P. Harper. 2004. A statistical con-
straint dependency grammar (CDG) parser. In Proceed-
ings of the ACL Workshop on Incremental Parsing.
188
Proceedings of NAACL HLT 2009: Short Papers, pages 281?284,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tightly coupling Speech Recognition and Search
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Abstract
In this paper, we discuss the benefits of tightly
coupling speech recognition and search com-
ponents in the context of a speech-driven
search application. We demonstrate that by in-
corporating constraints from the information
repository that is being searched not only im-
proves the speech recognition accuracy but
also results in higher search accuracy.
1 Introduction
With the exponential growth in the use of mobile de-
vices in recent years, the need for speech-driven in-
terfaces is becoming apparent. The limited screen
space and soft keyboards of mobile devices make it
cumbersome to type in text input. Furthermore, by
the mobile nature of these devices, users often would
like to use them in hands-busy environments, ruling
out the possibility of typing text.
In this paper, we focus on the problem of speech-
driven search to access information repositories us-
ing mobile devices. Such an application typically
uses a speech recognizer (ASR) for transforming the
user?s speech input to text and a search component
that uses the resulting text as a query to retrieve
the relevant documents from the information reposi-
tory. For the purposes of this paper, we use the busi-
ness listings containing the name, address and phone
number of businesses as the information repository.
Most of the literature on speech-driven search ap-
plications that are available in the consumer mar-
ket (Acero et al, 2008; Bacchiani et al, 2008;
VLingo FIND, 2009) have quite rightly emphasized
the importance of the robustness of the ASR lan-
guage model and the data needed to build such a ro-
bust language model. We acknowledge that this is a
significant issue for building such systems, and we
provide our approach to creating a language model.
However, in contrast to most of these systems that
treat speech-driven search to be largely an ASR
problem followed by a Search problem, in this pa-
per, we show the benefits of tightly coupling ASR
and Search tasks and illustrate techniques to im-
prove the accuracy of both components by exploit-
ing the co-constraints between the two components.
The outline of the paper is as follows. In Sec-
tion 2, we discuss the set up of our speech-driven
application. In Section 3, we discuss our method to
integrating the speech and search components. We
present the results of the experiments in Section 4
and conclude in Section 5.
2 Speech-driven Search
We describe the speech-driven search application in
this section. The user of this application provides
a speech utterance to a mobile device intending to
search for the address and phone number of a busi-
ness. The speech utterance typically contains a busi-
ness name, optionally followed by a city and state
to indicate the location of the business (e.g. pizza
hut near urbana illinois.). User input with a busi-
ness category (laundromats in madison) and without
location information (hospitals) are some variants
supported by this application. The result of ASR is
used to search a business listing database of over 10
million entries to retrieve the entries pertinent to the
user query.
The ASR used to recognize these utterances in-
corporates an acoustic model adapted to speech col-
lected from mobile devices and a trigram language
model that is built from over 10 million text query
logs obtained from the web-based text-driven ver-
sion of this application. The 1-best speech recogni-
tion output is used to retrieve the relevant business
listing entries.
281
3 Tightly coupling ASR and Search
As mentioned earlier, most of the speech-driven
search systems use the the 1-best output from the
ASR as the query for the search component. Given
that ASR 1-best output is likely to be erroneous,
this serialization of the ASR and search components
might result in sub-optimal search accuracy. As will
be shown in our experiments, the oracle word/phrase
accuracy using n-best hypotheses is far greater than
the 1-best output. However, using each of the n-
best hypothesis as a query to the search compo-
nent is computationally sub-optimal since the strings
in the n-best hypotheses usually share large subse-
quences with each other. A lattice representation
of the ASR output, in particular, a word-confusion
network (WCN) transformation of the lattice, com-
pactly encodes the n-best hypothesis with the flexi-
bility of pruning alternatives at each word position.
An example of a WCN is shown in Figure 1. In or-
der to obtain a measure of the ambiguity per word
position in the WCN, we define the (average) arc
density of a WCN as the ratio of the total number
of arcs to the number of states in the WCN. As can
be seen, with very small increase in arc density, the
number of paths that are encoded in the WCN can
be increased exponentially. In Figure 2, we show
the improvement in oracle-path word and phrase ac-
curacies as a function of the arc density for our data
set. Oracle-path is a path in the WCN that has the
least edit-distance (Levenshtein, 1966) to the refer-
ence string. It is interesting to note that the oracle
accuracies can be improved by almost 10% absolute
over the 1-best accuracy with small increase in the
arc density.
0
1
ball
ys/0
.
317
aud
i/2.1
26
ball
ew
/4.7
04
ball
y/3.
625
ellie
s/4.
037
ellio
t/4.3
72
ellio
tt/4.
513
2/1
aut
om
obil
es/6
.
735
Figure 1: A sample word confusion network
3.1 Representing Search Index as an FST
In order to exploit WCNs for Search, we have im-
plemented our own search engine instead of using an
1 1.17 1.26 1.37 1.53 1.72 1.935456
5860
6264
6668
7072
74
Arc Densities
Accurac
y (in %)
 
 Word accuracyPhrase accuracy
Figure 2: Oracle accuracy graph for the WCNs at differ-
ent arc densities
0
audi
:aud
i_rep
air/c
1
audi
:aud
i_au
tomo
bile_
deale
rs/c2
auto
mob
ile:a
utom
obile
_salv
age/c
3
auto
mob
ile:a
udi_
auto
mob
ile_d
ealer
s/c4
bally
s:bal
lys_h
otel/
c5
bally
s:bal
lys_f
itnes
s/c6
bally
s:bal
lys_f
itnes
s/c6
Figure 3: An example of an FST representing the search
index
off-the-shelf search engine such as Lucene (Hatcher
and Gospodnetic., 2004). We index each business
listing (d) in our data that we intend to search using
the words (wd) in that listing. The pair (wd, d) is
assigned a weight (c(wd,d)) using different metrics,including the standard tf ? idf , as explained below.
This index is represented as a weighted finite-state
transducer (SearchFST) as shown in Figure 3 where
wd is the input symbol, d is the output symbol and
c(wd,d) is the weight of that arc.
3.2 Relevance Metrics
In this section, we describe six different weighting
metrics used to determine the relevance of a docu-
ment for a given query word that we have experi-
mented with in this paper.
idfw: idfw refers to the inverse document fre-
quency of the word, w, which is computed as
ln(D/dw), where D refers to the total number
of documents in the collection, and dw refers to
the total number of documents in the collection
that contain the word, w (Robertson and Jones,
1997; Robertson, 2004).
atfw: atfw refers to average term frequency, which
is computed as cfw/dw (Pirkola et al, 2002).
cfw ? idfw: Here cfw refers to the collection fre-
quency, which is simply the total number of oc-
currences of the word, w in the collection.
282
atfw ? idfw: (Each term as described above).
? fw,d
|dw| ? idfw: Here fw,d refers to the frequency of
the word, w, in the document, d, whereas |dw|
is the length of the document, d, in which the
word, w, occurs.
cfw?
|dw| ? idfw: (Each term as described above).
3.3 Search
By composing a query (Qfst) (either a 1-best
string represented as a finite-state acceptor, or a
WCN), with the SearchFST, we obtain all the arcs
(wq, dwq , c(wq ,dwq )) where wq is a query word, dwqis a listing with the query word and, c(wq ,dwq ) is theweight associated with that pair. Using this informa-
tion, we aggregate the weight for a listing (dq) across
all query words and rank the retrieved listings in the
descending order of this aggregated weight. We se-
lect the top N listings from this ranked list. The
query composition, listing weight aggregation and
selection of top N listings are computed with finite-
state transducer operations.
In Figure 4, we illustrate the result of reranking
the WCN shown in Figure 1 using the search rele-
vance weights of each word in the WCN. It must be
noted that the least cost path1 for the WCN in Fig-
ure 1 is ballys automobiles while the reranked 1-best
output in Figure 4 is audi automobiles. Given that
the user voice query was audi automobiles, the list-
ings retrieved from the 1-best output after reranking
are much more relevant than those retrieved before
reranking, as shown in Table 1.
0
1
aud
i/2.1
00
ball
ys/2
.
276
2/4
aut
om
obil
es/0
.
251
Figure 4: A WCN rescored using word-level search rele-
vance weights.
4 Experiments and Results
We took 852 speech queries collected from users us-
ing a mobile device based speech search application.
We ran the speech recognizer on these queries us-
ing the language model described in Section 2 and
created word-confusion networks such as those il-
lustrated in Figure 1. These 852 utterances were
divided into 300 utterances for the development set
and 552 for the test set.
1We transform the scores into costs and search for minimum
cost paths.
Before rescoring After rescoring
ballys intl auburn audi repair
los angeles ca auburn wa
ballys las vegas audi bellevue repair
las vegas nv bellevue wa
ballys las health spa university audi seattle wa
las vegas nv
ballys cleaners beverly hills audi
palm desert ca los angeles ca
ballys brothers audi independent repairs
yorba linda ca by eurotech livermore ca
Table 1: Listings retrieved for query audi automobiles
before and after ASR WCNs were rescored using search
relevance weights.
4.1 ASR Experiments
The baseline ASR word and sentence (complete
string) accuracies on the development set are 63.1%
and 57.0% while those on the test set are 65.1% and
55.3% respectively.
Metric Word Sent. Scaling AD
Acc. Acc. Factor
idfw 63.1 57.0 10?3 all
cfw ? idfw 63.5 58.3 15 ? 10?4 1.37
atfw 63.6 57.3 1 all
atfw ? idf 63.1 57.0 10?3 all? fw,d
|dfw| ? idf 63.9 58.3 15 ? 10?4 1.25
cfw?
|dfw|
? idfw 63.5 57.3 1 all
Table 2: Performance of the metrics used for rescoring
the WCNs output by ASR. (AD refers to arc density.)
In Table 2, we summarize the improvements ob-
tained by rescoring the ASRWCNs based on the dif-
ferent metrics used for computing the word scores
according to the search criteria. The largest im-
provement in word and sentence accuracies is ob-
tained by using the rescoring metric: ? fw,d|dfw| ? idf .The word-level accuracy improved from the baseline
accuracy of 63.1% to 63.9% after rescoring while
the sentence-level accuracy improved from 57.0%
to 58.3%. Thus, this rescoring metric, and the cor-
responding pruning AD and the scaling factor was
used to rerank the 552 WCNs in the test set. After
rescoring, on the test set, the word-level accuracy
improved from 65.1% to 65.9% and sentence-level
accuracy improved from 55.3% to 56.2%.
283
Number of Scores Baseline Rerankeddocuments
All
Precision 0.708 0.728
Documents
Recall 0.728 0.742
F-Score 0.718 0.735
Table 3: Table showing the relevancy of the search results
obtained by the baseline ASR output compared to those
obtained by the reranked ASR output.
4.2 Search Experiments
To analyze the Search accuracy of the baseline ASR
output in comparison to the ASR output, reranked
using the ? fw,d|dfw| ? idf reranking metric, we usedeach of the two sets of ASR outputs (i.e., base-
line and reranked) as queries to our search engine,
SearchFST (described in Section 3). For the search
results produced by each set of queries, we com-
puted the precision, recall, and F-score values of the
listings retrieved with respect to the listings retrieved
by the set of human transcribed queries (Reference).
The precision, recall, and F-scores for the baseline
ASR output and the reranked ASR output, averaged
across each set, is presented in Table 3. For the pur-
poses of this experiment, we assume that the set re-
turned by our SearchFST for the human transcribed
set of queries is the reference search set. This is
however an approximation for a human annotated
search set.
In Table 3, by comparing the search accuracy
scores corresponding to the baseline ASR output to
those corresponding to the reranked ASR output, we
see that reranking the ASR output using the informa-
tion repository produces a substantial improvement
in the accuracy of the search results.
It is interesting to note that even though the
reranking of the ASR as shown in Table 2 is of the
order of 1%, the improvement in Search accuracy is
substantially higher. This indicates to the fact that
exploiting constraints from both components results
in improving the recognition accuracy of that subset
of words that are more relevant for Search.
5 Conclusion
In this paper, we have presented techniques for
tightly coupling ASR and Search. The central idea
behind these techniques is to rerank the ASR out-
put using the constraints (encoded as relevance met-
rics) from the Search task. The relevance metric that
best improved accuracy is ? fw,d|dw| ? idfw, as deter-
mined on our development set. Using this metric
to rerank the ASR output of our test set, we im-
proved ASR accuracy from 65.1% to 65.9% at the
word-level and from 55.3% to 56.2% at the phrase
level. This reranking also improved the F-score of
the search component from 0.718 to 0.735. These
results bear out our expectation that tightly coupling
ASR and Search can improve the accuracy of both
components.
Encouraged by the results of our experiments, we
plan to explore other relevance metrics that can en-
code more sophisticated constraints such as the rel-
ative coherence of the terms within a query.
Acknowledgments
The data used in this work is partly derived from the
Speak4It voice search prototype. We wish to thank
every member of that team for having deployed that
voice search system.
References
A. Acero, N. Bernstein, R.Chambers, Y. Ju, X. Li,
J. Odell, O. Scholtz P. Nguyen, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proceedings of ICASSP 2008, Las Ve-
gas.
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,
and B. Strope. 2008. Deploying GOOG-411: Early
lesstons in data, measurement and testing. In Proceed-
ings of ICASSP 2008, Las Vegas.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Action
(In Action series). Manning Publications Co., Green-
wich, CT, USA.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
A. Pirkola, E. Lepaa?nen, and K. Ja?rvelin. 2002. The
?ratf? formula (kwok?s formula): exploiting average
term frequency in cross-language retrieval. Informa-
tion Research, 7(2).
S. E. Robertson and K. Sparck Jones. 1997. Simple
proven approaches to text retrieval. Technical report,
Cambridge University.
Stephen Robertson. 2004. Understanding inverse doc-
ument frequency: On theoretical arguments for idf.
Journal of Documentation, 60.
VLingo FIND, 2009.
http://www.vlingomobile.com/downloads.html.
284
 
		MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
Compiling Boostexter Rules into a Finite-state Transducer
Srinivas Bangalore
AT&T Labs?Research
180 Park Avenue
Florham Park, NJ 07932
Abstract
A number of NLP tasks have been effectively mod-
eled as classification tasks using a variety of classi-
fication techniques. Most of these tasks have been
pursued in isolation with the classifier assuming un-
ambiguous input. In order for these techniques to be
more broadly applicable, they need to be extended
to apply on weighted packed representations of am-
biguous input. One approach for achieving this is
to represent the classification model as a weighted
finite-state transducer (WFST). In this paper, we
present a compilation procedure to convert the rules
resulting from an AdaBoost classifier into an WFST.
We validate the compilation technique by applying
the resulting WFST on a call-routing application.
1 Introduction
Many problems in Natural Language Processing
(NLP) can be modeled as classification tasks either
at the word or at the sentence level. For example,
part-of-speech tagging, named-entity identification
supertagging1 , word sense disambiguation are tasks
that have been modeled as classification problems at
the word level. In addition, there are problems that
classify the entire sentence or document into one of
a set of categories. These problems are loosely char-
acterized as semantic classification and have been
used in many practical applications including call
routing and text classification.
Most of these problems have been addressed in
isolation assuming unambiguous (one-best) input.
Typically, however, in NLP applications these mod-
ules are chained together with each module intro-
ducing some amount of error. In order to alleviate
the errors introduced by a module, it is typical for a
module to provide multiple weighted solutions (ide-
ally as a packed representation) that serve as input
to the next module. For example, a speech recog-
nizer provides a lattice of possible recognition out-
puts that is to be annotated with part-of-speech and
1associating each word with a label that represents the syn-
tactic information of the word given the context of the sentence.
named-entities. Thus classification approaches need
to be extended to be applicable on weighted packed
representations of ambiguous input represented as a
weighted lattice. The research direction we adopt
here is to compile the model of a classifier into a
weighted finite-state transducer (WFST) so that it
can compose with the input lattice.
Finite state models have been extensively ap-
plied to many aspects of language processing in-
cluding, speech recognition (Pereira and Riley,
1997), phonology (Kaplan and Kay, 1994), mor-
phology (Koskenniemi, 1984), chunking (Abney,
1991; Bangalore and Joshi, 1999), parsing (Roche,
1999; Oflazer, 1999) and machine translation (Vilar
et al, 1999; Bangalore and Riccardi, 2000). Finite-
state models are attractive mechanisms for language
processing since they (a) provide an efficient data
structure for representing weighted ambiguous hy-
potheses (b) generally effective for decoding (c)
associated with a calculus for composing models
which allows for straightforward integration of con-
straints from various levels of speech and language
processing.2
In this paper, we describe the compilation pro-
cess for a particular classifier model into an WFST
and validate the accuracy of the compilation pro-
cess on a one-best input in a call-routing task. We
view this as a first step toward using a classification
model on a lattice input. The outline of the paper is
as follows. In Section 2, we review the classifica-
tion approach to resolving ambiguity in NLP tasks
and in Section 3 we discuss the boosting approach
to classification. In Section 4 we describe the com-
pilation of the boosting model into an WFST and
validate the result of this compilation using a call-
routing task.
2 Resolving Ambiguity by Classification
In general, we can characterize all these tagging
problems as search problems formulated as shown
2Furthermore, software implementing the finite-state calcu-
lus is available for research purposes.
in Equation (1). We notate   to be the input vocab-
ulary,  to be the vocabulary of  tags, an  word
input sequence as  (    ) and tag sequence as 
( 	  ). We are interested in 
 , the most likely tag
sequence out of the possible tag sequences (  ) that
can be associated to  .




ffMATCHKiosk: A Multimodal Interactive City Guide
Michael Johnston
AT&T Research
180 Park Avenue
Florham Park, NJ 07932
johnston@research.att.com
Srinivas Bangalore
AT&T Research
180 Park Avenue
Florham Park, NJ 07932
srini@research.att.com
Abstract
Multimodal interfaces provide more flexible and
compelling interaction and can enable public infor-
mation kiosks to support more complex tasks for
a broader community of users. MATCHKiosk is
a multimodal interactive city guide which provides
users with the freedom to interact using speech,
pen, touch or multimodal inputs. The system re-
sponds by generating multimodal presentations that
synchronize synthetic speech with a life-like virtual
agent and dynamically generated graphics.
1 Introduction
Since the introduction of automated teller machines
in the late 1970s, public kiosks have been intro-
duced to provide users with automated access to
a broad range of information, assistance, and ser-
vices. These include self check-in at airports, ticket
machines in railway and bus stations, directions and
maps in car rental offices, interactive tourist and vis-
itor guides in tourist offices and museums, and more
recently, automated check-out in retail stores. The
majority of these systems provide a rigid structured
graphical interface and user input by only touch or
keypad, and as a result can only support a small
number of simple tasks. As automated kiosks be-
come more commonplace and have to support more
complex tasks for a broader community of users,
they will need to provide a more flexible and com-
pelling user interface.
One major motivation for developing multimodal
interfaces for mobile devices is the lack of a key-
board or mouse (Oviatt and Cohen, 2000; Johnston
and Bangalore, 2000). This limitation is also true of
many different kinds of public information kiosks
where security, hygiene, or space concerns make a
physical keyboard or mouse impractical. Also, mo-
bile users interacting with kiosks are often encum-
bered with briefcases, phones, or other equipment,
leaving only one hand free for interaction. Kiosks
often provide a touchscreen for input, opening up
the possibility of an onscreen keyboard, but these
can be awkward to use and occupy a considerable
amount of screen real estate, generally leading to a
more moded and cumbersome graphical interface.
A number of experimental systems have inves-
tigated adding speech input to interactive graphi-
cal kiosks (Raisamo, 1998; Gustafson et al, 1999;
Narayanan et al, 2000; Lamel et al, 2002). Other
work has investigated adding both speech and ges-
ture input (using computer vision) in an interactive
kiosk (Wahlster, 2003; Cassell et al, 2002).
We describe MATCHKiosk, (Multimodal Access
To City Help Kiosk) an interactive public infor-
mation kiosk with a multimodal interface which
provides users with the flexibility to provide in-
put using speech, handwriting, touch, or composite
multimodal commands combining multiple differ-
ent modes. The system responds to the user by gen-
erating multimodal presentations which combine
spoken output, a life-like graphical talking head,
and dynamic graphical displays. MATCHKiosk
provides an interactive city guide for New York
and Washington D.C., including information about
restaurants and directions on the subway or metro.
It develops on our previous work on a multimodal
city guide on a mobile tablet (MATCH) (Johnston
et al, 2001; Johnston et al, 2002b; Johnston et al,
2002a). The system has been deployed for testing
and data collection in an AT&T facility in Wash-
ington, D.C. where it provides visitors with infor-
mation about places to eat, points of interest, and
getting around on the DC Metro.
2 The MATCHKiosk
The MATCHKiosk runs on a Windows PC mounted
in a rugged cabinet (Figure 1). It has a touch screen
which supports both touch and pen input, and also
contains a printer, whose output emerges from a slot
below the screen. The cabinet alo contains speak-
ers and an array microphone is mounted above the
screen. There are three main components to the
graphical user interface (Figure 2). On the right,
there is a panel with a dynamic map display, a
click-to-speak button, and a window for feedback
on speech recognition. As the user interacts with
the system the map display dynamically pans and
zooms and the locations of restaurants and other
points of interest, graphical callouts with informa-
tion, and subway route segments are displayed. In
Figure 1: Kiosk Hardware
the top left there is a photo-realistic virtual agent
(Cosatto and Graf, 2000), synthesized by concate-
nating and blending image samples. Below the
agent, there is a panel with large buttons which en-
able easy access to help and common functions. The
buttons presented are context sensitive and change
over the course of interaction.
Figure 2: Kiosk Interface
The basic functions of the system are to enable
users to locate restaurants and other points of inter-
est based on attributes such as price, location, and
food type, to request information about them such
as phone numbers, addresses, and reviews, and to
provide directions on the subway or metro between
locations. There are also commands for panning and
zooming the map. The system provides users with
a high degree of flexibility in the inputs they use
in accessing these functions. For example, when
looking for restaurants the user can employ speech
e.g. find me moderately priced italian restaurants
in Alexandria, a multimodal combination of speech
and pen, e.g. moderate italian restaurants in this
area and circling Alexandria on the map, or solely
pen, e.g. user writes moderate italian and alexan-
dria. Similarly, when requesting directions they can
use speech, e.g. How do I get to the Smithsonian?,
multimodal, e.g. How do I get from here to here?
and circling or touching two locations on the map,
or pen, e.g. in Figure 2 the user has circled a loca-
tion on the map and handwritten the word route.
System output consists of coordinated presenta-
tions combining synthetic speech with graphical ac-
tions on the map. For example, when showing a
subway route, as the virtual agent speaks each in-
struction in turn, the map display zooms and shows
the corresponding route segment graphically. The
kiosk system also has a print capability. When a
route has been presented, one of the context sensi-
tive buttons changes to Print Directions. When this
is pressed the system generates an XHTML doc-
ument containing a map with step by step textual
directions and this is sent to the printer using an
XHTML-print capability.
If the system has low confidence in a user in-
put, based on the ASR or pen recognition score,
it requests confirmation from the user. The user
can confirm using speech, pen, or by touching on
a checkmark or cross mark which appear in the bot-
tom right of the screen. Context-sensitive graphi-
cal widgets are also used for resolving ambiguity
and vagueness in the user inputs. For example, if
the user asks for the Smithsonian Museum a small
menu appears in the bottom right of the map en-
abling them to select between the different museum
sites. If the user asks to see restaurants near a partic-
ular location, e.g. show restaurants near the white
house, a graphical slider appears enabling the user
to fine tune just how near.
The system also features a context-sensitive mul-
timodal help mechanism (Hastie et al, 2002) which
provides assistance to users in the context of their
current task, without redirecting them to separate
help system. The help system is triggered by spoken
or written requests for help, by touching the help
buttons on the left, or when the user has made sev-
eral unsuccessful inputs. The type of help is chosen
based on the current dialog state and the state of the
visual interface. If more than one type of help is ap-
plicable a graphical menu appears. Help messages
consist of multimodal presentations combining spo-
ken output with ink drawn on the display by the sys-
tem. For example, if the user has just requested to
see restaurants and they are now clearly visible on
the display, the system will provide help on getting
information about them.
3 Multimodal Kiosk Architecture
The underlying architecture of MATCHKiosk con-
sists of a series of re-usable components which
communicate using XML messages sent over sock-
ets through a facilitator (MCUBE) (Figure 3). Users
interact with the system through the Multimodal UI
displayed on the touchscreen. Their speech and
ink are processed by speech recognition (ASR) and
handwriting/gesture recognition (GESTURE, HW
RECO) components respectively. These recogni-
tion processes result in lattices of potential words
and gestures/handwriting. These are then com-
bined and assigned a meaning representation using a
multimodal language processing architecture based
on finite-state techniques (MMFST) (Johnston and
Bangalore, 2000; Johnston et al, 2002b). This pro-
vides as output a lattice encoding all of the potential
meaning representations assigned to the user inputs.
This lattice is flattened to an N-best list and passed
to a multimodal dialog manager (MDM) (Johnston
et al, 2002b) which re-ranks them in accordance
with the current dialogue state. If additional infor-
mation or confirmation is required, the MDM uses
the virtual agent to enter into a short information
gathering dialogue with the user. Once a command
or query is complete, it is passed to the multimodal
generation component (MMGEN), which builds a
multimodal score indicating a coordinated sequence
of graphical actions and TTS prompts. This score
is passed back to the Multimodal UI. The Multi-
modal UI passes prompts to a visual text-to-speech
component (Cosatto and Graf, 2000) which com-
municates with the AT&T Natural Voices TTS en-
gine (Beutnagel et al, 1999) in order to coordinate
the lip movements of the virtual agent with synthetic
speech output. As prompts are realized the Multi-
modal UI receives notifications and presents coordi-
nated graphical actions. The subway route server is
an application server which identifies the best route
between any two locations.
Figure 3: Multimodal Kiosk Architecture
4 Discussion and Related Work
A number of design issues arose in the development
of the kiosk, many of which highlight differences
between multimodal interfaces for kiosks and those
for mobile systems.
Array Microphone While on a mobile device a
close-talking headset or on-device microphone can
be used, we found that a single microphone had very
poor performance on the kiosk. Users stand in dif-
ferent positions with respect to the display and there
may be more than one person standing in front. To
overcome this problem we mounted an array micro-
phone above the touchscreen which tracks the loca-
tion of the talker.
Robust Recognition and Understanding is par-
ticularly important for kiosks since they have so
many first-time users. We utilize the techniques
for robust language modelling and multimodal
understanding described in Bangalore and John-
ston (2004).
Social Interaction For mobile multimodal inter-
faces, even those with graphical embodiment, we
found there to be little or no need to support so-
cial greetings and small talk. However, for a public
kiosk which different unknown users will approach
those capabilities are important. We added basic
support for social interaction to the language under-
standing and dialog components. The system is able
to respond to inputs such as Hello, How are you?,
Would you like to join us for lunch? and so on.
Context-sensitive GUI Compared to mobile sys-
tems, on palmtops, phones, and tablets, kiosks can
offer more screen real estate for graphical interac-
tion. This allowed for large easy to read buttons
for accessing help and other functions. The sys-
tem alters these as the dialog progresses. These but-
tons enable the system to support a kind of mixed-
initiative in multimodal interaction where the user
can take initiative in the spoken and handwritten
modes while the system is also able to provide
a more system-oriented initiative in the graphical
mode.
Printing Kiosks can make use of printed output
as a modality. One of the issues that arises is that
it is frequently the case that printed outputs such as
directions should take a very different style and for-
mat from onscreen presentations.
In previous work, a number of different multi-
modal kiosk systems supporting different sets of
input and output modalities have been developed.
The Touch-N-Speak kiosk (Raisamo, 1998) com-
bines spoken language input with a touchscreen.
The August system (Gustafson et al, 1999) is a mul-
timodal dialog system mounted in a public kiosk.
It supported spoken input from users and multi-
modal output with a talking head, text to speech,
and two graphical displays. The system was de-
ployed in a cultural center in Stockholm, enabling
collection of realistic data from the general public.
SmartKom-Public (Wahlster, 2003) is an interactive
public information kiosk that supports multimodal
input through speech, hand gestures, and facial ex-
pressions. The system uses a number of cameras
and a video projector for the display. The MASK
kiosk (Lamel et al, 2002) , developed by LIMSI and
the French national railway (SNCF), provides rail
tickets and information using a speech and touch in-
terface. The mVPQ kiosk system (Narayanan et al,
2000) provides access to corporate directory infor-
mation and call completion. Users can provide in-
put by either speech or touching options presented
on a graphical display. MACK, the Media Lab
Autonomous Conversational Kiosk, (Cassell et al,
2002) provides information about groups and indi-
viduals at the MIT Media Lab. Users interact us-
ing speech and gestures on a paper map that sits be-
tween the user and an embodied agent.
In contrast to August and mVPQ, MATCHKiosk
supports composite multimodal input combining
speech with pen drawings and touch. The
SmartKom-Public kiosk supports composite input,
but differs in that it uses free hand gesture for point-
ing while MATCH utilizes pen input and touch.
August, SmartKom-Public, and MATCHKiosk all
employ graphical embodiments. SmartKom uses
an animated character, August a model-based talk-
ing head, and MATCHKiosk a sample-based video-
realistic talking head. MACK uses articulated
graphical embodiment with ability to gesture. In
Touch-N-Speak a number of different techniques
using time and pressure are examined for enabling
selection of areas on a map using touch input. In
MATCHKiosk, this issue does not arise since areas
can be selected precisely by drawing with the pen.
5 Conclusion
We have presented a multimodal public informa-
tion kiosk, MATCHKiosk, which supports complex
unstructured tasks such as browsing for restaurants
and subway directions. Users have the flexibility to
interact using speech, pen/touch, or multimodal in-
puts. The system responds with multimodal presen-
tations which coordinate synthetic speech, a virtual
agent, graphical displays, and system use of elec-
tronic ink.
Acknowledgements Thanks to Eric Cosatto,
Hans Peter Graf, and Joern Ostermann for their help
with integrating the talking head. Thanks also to
Patrick Ehlen, Amanda Stent, Helen Hastie, Guna
Vasireddy, Mazin Rahim, Candy Kamm, Marilyn
Walker, Steve Whittaker, and Preetam Maloor for
their contributions to the MATCH project. Thanks
to Paul Burke for his assistance with XHTML-print.
References
S. Bangalore and M. Johnston. 2004. Balancing
Data-driven and Rule-based Approaches in the
Context of a Multimodal Conversational System.
In Proceedings of HLT-NAACL, Boston, MA.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&T Next-
Generation TTS. In In Joint Meeting of ASA;
EAA and DAGA.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao,
Y. Nakano, K. Ryokai, D. Tversky, C. Vaucelle,
and H. Vilhjalmsson. 2002. MACK: Media lab
autonomous conversational kiosk. In Proceed-
ings of IMAGINA02, Monte Carlo.
E. Cosatto and H. P. Graf. 2000. Photo-realistic
Talking-heads from Image Samples. IEEE Trans-
actions on Multimedia, 2(3):152?163.
J. Gustafson, N. Lindberg, and M. Lundeberg.
1999. The August spoken dialogue system. In
Proceedings of Eurospeech 99, pages 1151?
1154.
H. Hastie, M. Johnston, and P. Ehlen. 2002.
Context-sensitive Help for Multimodal Dialogue.
In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, pages 93?
98, Pittsburgh, PA.
M. Johnston and S. Bangalore. 2000. Finite-
state Multimodal Parsing and Understanding. In
Proceedings of COLING 2000, pages 369?375,
Saarbru?cken, Germany.
M. Johnston, S. Bangalore, and G. Vasireddy. 2001.
MATCH: Multimodal Access To City Help. In
Workshop on Automatic Speech Recognition and
Understanding, Madonna di Campiglio, Italy.
M. Johnston, S. Bangalore, A. Stent, G. Vasireddy,
and P. Ehlen. 2002a. Multimodal Language Pro-
cessing for Mobile Information Access. In Pro-
ceedings of ICSLP 2002, pages 2237?2240.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. 2002b. MATCH: An Architecture for Mul-
timodal Dialog Systems. In Proceedings of ACL-
02, pages 376?383.
L. Lamel, S. Bennacef, J. L. Gauvain, H. Dartigues,
and J. N. Temem. 2002. User Evaluation of
the MASK Kiosk. Speech Communication, 38(1-
2):131?139.
S. Narayanan, G. DiFabbrizio, C. Kamm,
J. Hubbell, B. Buntschuh, P. Ruscitti, and
J. Wright. 2000. Effects of Dialog Initiative and
Multi-modal Presentation Strategies on Large
Directory Information Access. In Proceedings of
ICSLP 2000, pages 636?639.
S. Oviatt and P. Cohen. 2000. Multimodal Inter-
faces That Process What Comes Naturally. Com-
munications of the ACM, 43(3):45?53.
R. Raisamo. 1998. A Multimodal User Interface
for Public Information Kiosks. In Proceedings of
PUI Workshop, San Francisco.
W. Wahlster. 2003. SmartKom: Symmetric Multi-
modality in an Adaptive and Reusable Dialogue
Shell. In R. Krahl and D. Gunther, editors, Pro-
ceedings of the Human Computer Interaction Sta-
tus Conference 2003, pages 47?62.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 201?208,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning the Structure of Task-driven Human-Human Dialogs
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Giuseppe Di Fabbrizio
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
pino@research.att.com
Amanda Stent
Dept of Computer Science
Stony Brook University
Stony Brook, NY
stent@cs.sunysb.edu
Abstract
Data-driven techniques have been used
for many computational linguistics tasks.
Models derived from data are generally
more robust than hand-crafted systems
since they better reflect the distribution
of the phenomena being modeled. With
the availability of large corpora of spo-
ken dialog, dialog management is now
reaping the benefits of data-driven tech-
niques. In this paper, we compare two ap-
proaches to modeling subtask structure in
dialog: a chunk-based model of subdialog
sequences, and a parse-based, or hierarchi-
cal, model. We evaluate these models us-
ing customer agent dialogs from a catalog
service domain.
1 Introduction
As large amounts of language data have become
available, approaches to sentence-level process-
ing tasks such as parsing, language modeling,
named-entity detection and machine translation
have become increasingly data-driven and empiri-
cal. Models for these tasks can be trained to cap-
ture the distributions of phenomena in the data
resulting in improved robustness and adaptabil-
ity. However, this trend has yet to significantly
impact approaches to dialog management in dia-
log systems. Dialog managers (both plan-based
and call-flow based, for example (Di Fabbrizio and
Lewis, 2004; Larsson et al, 1999)) have tradition-
ally been hand-crafted and consequently some-
what brittle and rigid. With the ability to record,
store and process large numbers of human-human
dialogs (e.g. from call centers), we anticipate
that data-driven methods will increasingly influ-
ence approaches to dialog management.
A successful dialog system relies on the syn-
ergistic working of several components: speech
recognition (ASR), spoken language understand-
ing (SLU), dialog management (DM), language
generation (LG) and text-to-speech synthesis
(TTS). While data-driven approaches to ASR and
SLU are prevalent, such approaches to DM, LG
and TTS are much less well-developed. In on-
going work, we are investigating data-driven ap-
proaches for building all components of spoken
dialog systems.
In this paper, we address one aspect of this prob-
lem ? inferring predictive models to structure task-
oriented dialogs. We view this problem as a first
step in predicting the system state of a dialog man-
ager and in predicting the system utterance during
an incremental execution of a dialog. In particular,
we learn models for predicting dialog acts of ut-
terances, and models for predicting subtask struc-
tures of dialogs. We use three different dialog act
tag sets for three different human-human dialog
corpora. We compare a flat chunk-based model
to a hierarchical parse-based model as models for
predicting the task structure of dialogs.
The outline of this paper is as follows: In Sec-
tion 2, we review current approaches to building
dialog systems. In Section 3, we review related
work in data-driven dialog modeling. In Section 4,
we present our view of analyzing the structure of
task-oriented human-human dialogs. In Section 5,
we discuss the problem of segmenting and label-
ing dialog structure and building models for pre-
dicting these labels. In Section 6, we report ex-
perimental results on Maptask, Switchboard and a
dialog data collection from a catalog ordering ser-
vice domain.
2 Current Methodology for Building
Dialog systems
Current approaches to building dialog systems
involve several manual steps and careful craft-
ing of different modules for a particular domain
or application. The process starts with a small
scale ?Wizard-of-Oz? data collection where sub-
jects talk to a machine driven by a human ?behind
the curtains?. A user experience (UE) engineer an-
alyzes the collected dialogs, subject matter expert
interviews, user testimonials and other evidences
(e.g. customer care history records). This hetero-
geneous set of information helps the UE engineer
to design some system functionalities, mainly: the
201
semantic scope (e.g. call-types in the case of call
routing systems), the LG model, and the DM strat-
egy. A larger automated data collection follows,
and the collected data is transcribed and labeled by
expert labelers following the UE engineer recom-
mendations. Finally, the transcribed and labeled
data is used to train both the ASR and the SLU.
This approach has proven itself in many com-
mercial dialog systems. However, the initial UE
requirements phase is an expensive and error-
prone process because it involves non-trivial de-
sign decisions that can only be evaluated after sys-
tem deployment. Moreover, scalability is compro-
mised by the time, cost and high level of UE know-
how needed to reach a consistent design.
The process of building speech-enabled auto-
mated contact center services has been formalized
and cast into a scalable commercial environment
in which dialog components developed for differ-
ent applications are reused and adapted (Gilbert
et al, 2005). However, we still believe that ex-
ploiting dialog data to train/adapt or complement
hand-crafted components will be vital for robust
and adaptable spoken dialog systems.
3 Related Work
In this paper, we discuss methods for automati-
cally creating models of dialog structure using di-
alog act and task/subtask information. Relevant
related work includes research on automatic dia-
log act tagging and stochastic dialog management,
and on building hierarchical models of plans using
task/subtask information.
There has been considerable research on statis-
tical dialog act tagging (Core, 1998; Jurafsky et
al., 1998; Poesio and Mikheev, 1998; Samuel et
al., 1998; Stolcke et al, 2000; Hastie et al, 2002).
Several disambiguation methods (n-gram models,
hidden Markov models, maximum entropy mod-
els) that include a variety of features (cue phrases,
speaker ID, word n-grams, prosodic features, syn-
tactic features, dialog history) have been used. In
this paper, we show that use of extended context
gives improved results for this task.
Approaches to dialog management include
AI-style plan recognition-based approaches (e.g.
(Sidner, 1985; Litman and Allen, 1987; Rich
and Sidner, 1997; Carberry, 2001; Bohus and
Rudnicky, 2003)) and information state-based ap-
proaches (e.g. (Larsson et al, 1999; Bos et al,
2003; Lemon and Gruenstein, 2004)). In recent
years, there has been considerable research on
how to automatically learn models of both types
from data. Researchers who treat dialog as a se-
quence of information states have used reinforce-
ment learning and/or Markov decision processes
to build stochastic models for dialog management
that are evaluated by means of dialog simulations
(Levin and Pieraccini, 1997; Scheffler and Young,
2002; Singh et al, 2002; Williams et al, 2005;
Henderson et al, 2005; Frampton and Lemon,
2005). Most recently, Henderson et al showed
that it is possible to automatically learn good dia-
log management strategies from automatically la-
beled data over a large potential space of dialog
states (Henderson et al, 2005); and Frampton and
Lemon showed that the use of context informa-
tion (the user?s last dialog act) can improve the
performance of learned strategies (Frampton and
Lemon, 2005). In this paper, we combine the use
of automatically labeled data and extended context
for automatic dialog modeling.
Other researchers have looked at probabilistic
models for plan recognition such as extensions of
Hidden Markov Models (Bui, 2003) and proba-
bilistic context-free grammars (Alexandersson and
Reithinger, 1997; Pynadath and Wellman, 2000).
In this paper, we compare hierarchical grammar-
style and flat chunking-style models of dialog.
In recent research, Hardy (2004) used a large
corpus of transcribed and annotated telephone
conversations to develop the Amities dialog sys-
tem. For their dialog manager, they trained sepa-
rate task and dialog act classifiers on this corpus.
For task identification they report an accuracy of
85% (true task is one of the top 2 results returned
by the classifier); for dialog act tagging they report
86% accuracy.
4 Structural Analysis of a Dialog
We consider a task-oriented dialog to be the re-
sult of incremental creation of a shared plan by
the participants (Lochbaum, 1998). The shared
plan is represented as a single tree that encap-
sulates the task structure (dominance and prece-
dence relations among tasks), dialog act structure
(sequences of dialog acts), and linguistic structure
of utterances (inter-clausal relations and predicate-
argument relations within a clause), as illustrated
in Figure 1. As the dialog proceeds, an utterance
from a participant is accommodated into the tree in
an incremental manner, much like an incremental
syntactic parser accommodates the next word into
a partial parse tree (Alexandersson and Reithinger,
1997). With this model, we can tightly couple
language understanding and dialog management
using a shared representation, which leads to im-
proved accuracy (Taylor et al, 1998).
In order to infer models for predicting the struc-
ture of task-oriented dialogs, we label human-
human dialogs with the hierarchical information
shown in Figure 1 in several stages: utterance
segmentation (Section 4.1), syntactic annotation
(Section 4.2), dialog act tagging (Section 4.3) and
202
subtask labeling (Section 5).
Dialog
Task
Topic/SubtaskTopic/Subtask
Task Task
Clause
UtteranceUtteranceUtterance
Topic/Subtask
DialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?Args
Figure 1: Structural analysis of a dialog
4.1 Utterance Segmentation
The task of ?cleaning up? spoken language utter-
ances by detecting and removing speech repairs
and dysfluencies and identifying sentence bound-
aries has been a focus of spoken language parsing
research for several years (e.g. (Bear et al, 1992;
Seneff, 1992; Shriberg et al, 2000; Charniak and
Johnson, 2001)). We use a system that segments
the ASR output of a user?s utterance into clauses.
The system annotates an utterance for sentence
boundaries, restarts and repairs, and identifies
coordinating conjunctions, filled pauses and dis-
course markers. These annotations are done using
a cascade of classifiers, details of which are de-
scribed in (Bangalore and Gupta, 2004).
4.2 Syntactic Annotation
We automatically annotate a user?s utterance with
supertags (Bangalore and Joshi, 1999). Supertags
encapsulate predicate-argument information in a
local structure. They are composed with each
other using the substitution and adjunction oper-
ations of Tree-Adjoining Grammars (Joshi, 1987)
to derive a dependency analysis of an utterance
and its predicate-argument structure.
4.3 Dialog Act Tagging
We use a domain-specific dialog act tag-
ging scheme based on an adapted version of
DAMSL (Core, 1998). The DAMSL scheme is
quite comprehensive, but as others have also found
(Jurafsky et al, 1998), the multi-dimensionality
of the scheme makes the building of models from
DAMSL-tagged data complex. Furthermore, the
generality of the DAMSL tags reduces their util-
ity for natural language generation. Other tagging
schemes, such as the Maptask scheme (Carletta et
al., 1997), are also too general for our purposes.
We were particularly concerned with obtaining
sufficient discriminatory power between different
types of statement (for generation), and to include
an out-of-domain tag (for interpretation). We pro-
vide a sample list of our dialog act tags in Table 2.
Our experiments in automatic dialog act tagging
are described in Section 6.3.
5 Modeling Subtask Structure
Figure 2 shows the task structure for a sample di-
alog in our domain (catalog ordering). An order
placement task is typically composed of the se-
quence of subtasks opening, contact-information,
order-item, related-offers, summary. Subtasks can
be nested; the nesting structure can be as deep as
five levels. Most often the nesting is at the left or
right frontier of the subtask tree.
Opening
Order Placement
Contact Info
Delivery InfoShipping Info
ClosingSummaryPayment InfoOrder Item
Figure 2: A sample task structure in our applica-
tion domain.
Contact Info Order Item Payment Info Summary Closing
Shipping Info Delivery Info
Opening
Figure 3: An example output of the chunk model?s
task structure
The goal of subtask segmentation is to predict if
the current utterance in the dialog is part of the cur-
rent subtask or starts a new subtask. We compare
two models for recovering the subtask structure
? a chunk-based model and a parse-based model.
In the chunk-based model, we recover the prece-
dence relations (sequence) of the subtasks but not
dominance relations (subtask structure) among the
subtasks. Figure 3 shows a sample output from the
chunk model. In the parse model, we recover the
complete task structure from the sequence of ut-
terances as shown in Figure 2. Here, we describe
our two models. We present our experiments on
subtask segmentation and labeling in Section 6.4.
5.1 Chunk-based model
This model is similar to the second one described
in (Poesio and Mikheev, 1998), except that we
use tasks and subtasks rather than dialog games.
We model the prediction problem as a classifica-
tion task as follows: given a sequence of utter-
ances   in a dialog   	
 			
  and a
203
subtask label vocabulary  ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 152?159,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Statistical Machine Translation through Global Lexical Selection and
Sentence Reconstruction
Srinivas Bangalore, Patrick Haffner, Stephan Kanthak
AT&T Labs - Research
180 Park Ave, Florham Park, NJ 07932
{srini,haffner,skanthak}@research.att.com
Abstract
Machine translation of a source language
sentence involves selecting appropriate tar-
get language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this pa-
per, we present a novel approach to lexical
selection where the target words are associ-
ated with the entire source sentence (global)
without the need to compute local associa-
tions. Further, we present a technique for
reconstructing the target language sentence
from the selected words. We compare the re-
sults of this approach against those obtained
from a finite-state based statistical machine
translation system which relies on local lex-
ical associations.
1 Introduction
Machine translation can be viewed as consisting of
two subproblems: (a) lexical selection, where appro-
priate target language lexical items are chosen for
each source language lexical item and (b) lexical re-
ordering, where the chosen target language lexical
items are rearranged to produce a meaningful target
language string. Most of the previous work on statis-
tical machine translation, as exemplified in (Brown
et al, 1993), employs word-alignment algorithm
(such as GIZA++ (Och and Ney, 2003)) that pro-
vides local associations between source and target
words. The source-to-target word alignments are
sometimes augmented with target-to-source word
alignments in order to improve precision. Further,
the word-level alignments are extended to phrase-
level alignments in order to increase the extent of
local associations. The phrasal associations compile
some amount of (local) lexical reordering of the tar-
get words ? those permitted by the size of the phrase.
Most of the state-of-the-art machine translation sys-
tems use phrase-level associations in conjunction
with a target language model to produce sentences.
There is relatively little emphasis on (global) lexical
reordering other than the local reorderings permit-
ted within the phrasal alignments. A few exceptions
are the hierarchical (possibly syntax-based) trans-
duction models (Wu, 1997; Alshawi et al, 1998;
Yamada and Knight, 2001; Chiang, 2005) and the
string transduction models (Kanthak et al, 2005).
In this paper, we present an alternate approach to
lexical selection and lexical reordering. For lexical
selection, in contrast to the local approaches of as-
sociating target to source words, we associate tar-
get words to the entire source sentence. The intu-
ition is that there may be lexico-syntactic features of
the source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might be
difficult to exactly associate a target word to a source
word in many situations ? (a) when the translations
are not exact but paraphrases (b) when the target lan-
guage does not have one lexical item to express the
same concept that is expressed by a source word.
Extending word to phrase alignments attempts to ad-
dress some of these situations while alleviating the
noise in word-level alignments.
As a consequence of this global lexical selection
approach, we no longer have a tight association be-
tween source and target language words. The re-
sult of lexical selection is simply a bag of words in
the target language and the sentence has to be recon-
structed using this bag of words. The words in the
bag, however, might be enhanced with rich syntactic
information that could aid in reconstructing the tar-
get sentence. This approach to lexical selection and
152
Translation modelWFSA
BilanguagePhrase Segmented
FSA to FST
Bilanguage
WFSTTransformation
Bilanguage
Reordering
Local Phrase Joint Language
Modeling
Joint Language
Alignment
WordAlignmentSentence AlignedCorpus
Figure 1: Training phases for our system
ConstructionPermutation
Permutation Lattice
Lexical Choice 
FST Composition
Decoding
SourceSentence/
WeightedLattice
Target
Decoding Lexical Reodering
 CompositionFSA Sentence
Model
Translation ModelLanguage
Target
Figure 2: Decoding phases for our system
sentence reconstruction has the potential to circum-
vent limitations of word-alignment based methods
for translation between languages with significantly
different word order (e.g. English-Japanese).
In this paper, we present the details of training
a global lexical selection model using classifica-
tion techniques and sentence reconstruction mod-
els using permutation automata. We also present a
stochastic finite-state transducer (SFST) as an exam-
ple of an approach that relies on local associations
and use it to compare and contrast our approach.
2 SFST Training and Decoding
In this section, we describe each of the components
of our SFST system shown in Figure 1. The SFST
approach described here is similar to the one de-
scribed in (Bangalore and Riccardi, 2000) which has
subsequently been adopted by (Banchs et al, 2005).
2.1 Word Alignment
The first stage in the process of training a lexical se-
lection model is obtaining an alignment function (f )
that given a pair of source (s1s2 . . . sn) and target
(t1t2 . . . tm) language sentences, maps source lan-
guage word subsequences into target language word
subsequences, as shown below.
?i?j(f(si) = tj ? f(si) = ?) (1)
For the work reported in this paper, we have used
the GIZA++ tool (Och and Ney, 2003) which im-
plements a string-alignment algorithm. GIZA++
alignment however is asymmetric in that the word
mappings are different depending on the direction
of alignment ? source-to-target or target-to-source.
Hence in addition to the functions f as shown in
Equation 1 we train another alignment function g :
?j?i(g(tj) = si ? g(tj) = ?) (2)
English: I need to make a collect call
Japanese: ?H ???? ?ff?k $*d ?^%ffcW2
Alignment: 1 5 0 3 0 2 4
Figure 3: Example bilingual texts with alignment in-
formation
I:?H need:?^%ffcW2 to:? make:?ff?k
a:? collect ???? call $*d
Figure 4: Bilanguage strings resulting from align-
ments shown in Figure 3.
2.2 Bilanguage Representation
From the alignment information (see Figure 3), we
construct a bilanguage representation of each sen-
tence in the bilingual corpus. The bilanguage string
consists of source-target symbol pair sequences as
shown in Equation 3. Note that the tokens of a bilan-
guage could be either ordered according to the word
order of the source language or ordered according to
the word order of the target language.
Bf = bf1 bf2 . . . bfm (3)
bfi = (si?1; si, f(si)) if f(si?1) = ?
= (si, f(si?1); f(si)) if si?1 = ?
= (si, f(si)) otherwise
Figure 4 shows an example alignment and the
source-word-ordered bilanguage strings correspond-
ing to the alignment shown in Figure 3.
We also construct a bilanguage using the align-
ment function g similar to the bilanguage using the
alignment function f as shown in Equation 3.
Thus, the bilanguage corpus obtained by combin-
ing the two alignment functions is B = Bf ?Bg.
2.3 Bilingual Phrases and Local Reordering
While word-to-word translation only approximates
the lexical selection process, phrase-to-phrase map-
ping can greatly improve the translation of colloca-
tions, recurrent strings, etc. Using phrases also al-
lows words within the phrase to be reordered into the
correct target language order, thus partially solving
the reordering problem. Additionally, SFSTs can
take advantage of phrasal correlations to improve the
computation of the probability P (WS ,WT ).
The bilanguage representation could result in
some source language phrases to be mapped to ?
153
(empty target phrase). In addition to these phrases,
we compute subsequences of a given length k on the
bilanguage string and for each subsequence we re-
order the target words of the subsequence to be in
the same order as they are in the target language sen-
tence corresponding to that bilanguage string. This
results in a retokenization of the bilanguage into to-
kens of source-target phrase pairs.
2.4 SFST Model
From the bilanguage corpus B, we train an n-gram
language model using standard tools (Goffin et al,
2005). The resulting language model is represented
as a weighted finite-state automaton (S ? T ?
[0, 1]). The symbols on the arcs of this automaton
(si ti) are interpreted as having the source and target
symbols (si:ti), making it into a weighted finite-state
transducer (S ? T?[0, 1]) that provides a weighted
string-to-string transduction from S into T :
T ? = argmax
T
P (si, ti|si?1, ti?1 . . . si?n?1, ti?n?1)
2.5 Decoding
Since we represent the translation model as a
weighted finite-state transducer (TransFST ), the
decoding process of translating a new source in-
put (sentence or weighted lattice (Is)) amounts to
a transducer composition (?) and selection of the
best probability path (BestPath) resulting from the
composition and projecting the target sequence (pi1).
T ? = pi1(BestPath(Is ? TransFST )) (4)
However, we have noticed that on the develop-
ment corpus, the decoded target sentence is typically
shorter than the intended target sentence. This mis-
match may be due to the incorrect estimation of the
back-off events and their probabilities in the train-
ing phase of the transducer. In order to alleviate
this mismatch, we introduce a negative word inser-
tion penalty model as a mechanism to produce more
words in the target sentence.
2.6 Word Insertion Model
The word insertion model is also encoded as a
weighted finite-state automaton and is included in
the decoding sequence as shown in Equation 5. The
word insertion FST has one state and |?T | number
of arcs each weighted with a ? weight representing
the word insertion cost. On composition as shown
in Equation 5, the word insertion model penalizes or
rewards paths which have more words depending on
whether ? is positive or negative value.
T ? = pi1(BestPath(Is?TransFST?WIP )) (5)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2Figure 5: Locally constraint permutation automatonfor a sentence with 4 words and window size of 2.
2.7 Global Reordering
Local reordering as described in Section 2.3 is re-
stricted by the window size k and accounts only for
different word order within phrases. As permuting
non-linear automata is too complex, we apply global
reordering by permuting the words of the best trans-
lation and weighting the result by an n-gram lan-
guage model (see also Figure 2):
T ? = BestPath(perm(T ?) ? LMt) (6)
Even the size of the minimal permutation automa-
ton of a linear automaton grows exponentially with
the length of the input sequence. While decoding by
composition simply resembles the principle of mem-
oization (i.e. here: all state hypotheses of a whole
sentence are kept in memory), it is necessary to ei-
ther use heuristic forward pruning or constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)). We have cho-
sen to constrain permutations here. Figure 5 shows
the resulting minimal permutation automaton for an
input sequence of 4 words and a window size of 2.
Decoding ASR output in combination with global
reordering uses n-best lists or extracts them from lat-
tices first. Each entry of the n-best list is decoded
separately and the best target sentence is picked
from the union of the n intermediate results.
3 Discriminant Models for Lexical
Selection
The approach from the previous section is a genera-
tive model for statistical machine translation relying
on local associations between source and target sen-
tences. Now, we present our approach for a global
lexical selection model based on discriminatively
trained classification techniques. Discriminant mod-
eling techniques have become the dominant method
for resolving ambiguity in speech and other NLP
tasks, outperforming generative models. Discrimi-
native training has been used mainly for translation
model combination (Och and Ney, 2002) and with
the exception of (Wellington et al, 2006; Tillmann
and Zhang, 2006), has not been used to directly train
parameters of a translation model. We expect dis-
criminatively trained global lexical selection models
154
to outperform generatively trained local lexical se-
lection models as well as provide a framework for
incorporating rich morpho-syntactic information.
Statistical machine translation can be formulated
as a search for the best target sequence that maxi-
mizes P (T |S), where S is the source sentence and
T is the target sentence. Ideally, P (T |S) should
be estimated directly to maximize the conditional
likelihood on the training data (discriminant model).
However, T corresponds to a sequence with a ex-
ponentially large combination of possible labels,
and traditional classification approaches cannot be
used directly. Although Conditional Random Fields
(CRF) (Lafferty et al, 2001) train an exponential
model at the sequence level, in translation tasks such
as ours the computational requirements of training
such models are prohibitively expensive.
We investigate two approaches to approximating
the string level global classification problem, using
different independence assumptions. A comparison
of the two approaches is summarized in Table 1.
3.1 Sequential Lexical Choice Model
In the first approach, we formulate a sequential lo-
cal classification problem as shown in Equations 7.
This approach is similar to the SFST approach in
that it relies on local associations between the source
and target words(phrases). We can use a conditional
model (instead of a joint model as before) and the
parameters are determined using discriminant train-
ing which allows for richer conditioning context.
P (T |S) =
?N
i=1 P (ti|?(S, i)) (7)
where ?(S, i) is a set of features extracted from the
source string S (shortened as ? in the rest of the
section).
3.2 Bag-of-Words Lexical Choice Model
The sequential lexical choice model described in
the previous section treats the selection of a lexical
choice for a source word in the local lexical context
as a classification task. The data for training such
models is derived from word alignments obtained
by e.g. GIZA++. The decoded target lexical items
have to be further reordered, but for closely related
languages the reordering could be incorporated into
correctly ordered target phrases as discussed previ-
ously.
For pairs of languages with radically different
word order (e.g. English-Japanese), there needs to
be a global reordering of words similar to the case
in the SFST-based translation system. Also, for such
differing language pairs, the alignment algorithms
such as GIZA++ perform poorly.
These observations prompted us to formulate the
lexical choice problem without the need for word
alignment information. We require a sentence
aligned corpus as before, but we treat the target sen-
tence as a bag-of-words or BOW assigned to the
source sentence. The goal is, given a source sen-
tence, to estimate the probability that we find a given
word in the target sentence. This is why, instead of
producing a target sentence, what we initially obtain
is a target bag of words. Each word in the target vo-
cabulary is detected independently, so we have here
a very simple use of binary static classifiers. Train-
ing sentence pairs are considered as positive exam-
ples when the word appears in the target, and neg-
ative otherwise. Thus, the number of training ex-
amples equals the number of sentence pairs, in con-
trast to the sequential lexical choice model which
has one training example for each token in the bilin-
gual training corpus. The classifier is trained with n-
gram features (BOgrams(S)) from the source sen-
tence. During decoding the words with conditional
probability greater than a threshold ? are considered
as the result of lexical choice decoding.
BOW ?T = {t|P (t|BOgrams(S)) > ?} (8)
For reconstructing the proper order of words in
the target sentence we consider all permutations of
words in BOW ?T and weight them by a target lan-
guage model. This step is similar to the one de-
scribed in Section 2.7. The BOW approach can also
be modified to allow for length adjustments of tar-
get sentences, if we add optional deletions in the fi-
nal step of permutation decoding. The parameter ?
and an additional word deletion penalty can then be
used to adjust the length of translated outputs. In
Section 6, we discuss several issues regarding this
model.
4 Choosing the classifier
This section addresses the choice of the classifi-
cation technique, and argues that one technique
that yields excellent performance while scaling well
is binary maximum entropy (Maxent) with L1-
regularization.
4.1 Multiclass vs. Binary Classification
The Sequential and BOW models represent two dif-
ferent classification problems. In the sequential
model, we have a multiclass problem where each
class ti is exclusive, therefore, all the classifier out-
puts P (ti|?) must be jointly optimized such that
155
Table 1: A comparison of the sequential and bag-of-words lexical choice models
Sequential Lexical Model Bag-of-Words Lexical Model
Output target Target word for each source position i Target word given a source sentence
Input features BOgram(S, i? d, i+ d) : bag of n-grams BOgram(S, 0, |S|): bag of n-grams
in source sentence in the interval [i? d, i+ d] in source sentence
Probabilities P (ti|BOgram(S, i? d, i+ d)) P (BOW (T )|BOgram(S, 0, |S|))
Independence assumption between the labels
Number of classes One per target word or phrase
Training samples One per source token One per sentence
Preprocessing Source/Target word alignment Source/Target sentence alignment
?
i P (ti|?) = 1. This can be problematic: with
one classifier per word in the vocabulary, even allo-
cating the memory during training may exceed the
memory capacity of current computers.
In the BOW model, each class can be detected
independently, and two different classes can be de-
tected at the same time. This is known as the 1-vs-
other scheme. The key advantage over the multiclass
scheme is that not all classifiers have to reside in
memory at the same time during training which al-
lows for parallelization. Fortunately for the sequen-
tial model, we can decompose a multiclass classifi-
cation problem into separate 1-vs-other problems. In
theory, one has to make an additional independence
assumption and the problem statement becomes dif-
ferent. Each output label t is projected into a bit
string with components bj(t) where probability of
each component is estimated independently:
P (bj(t)|?) = 1? P (b?j(t)|?) = 11 + e?(?j??j?)??
In practice, despite the approximation, the 1-vs-
other scheme has been shown to perform as well as
the multiclass scheme (Rifkin and Klautau, 2004).
As a consequence, we use the same type of binary
classifier for the sequential and the BOW models.
The excellent results recently obtained with the
SEARN algorithm (Daume et al, 2007) also sug-
gest that binary classifiers, when properly trained
and combined, seem to be capable of matching more
complex structured output approaches.
4.2 Geometric vs. Probabilistic Interpretation
We separate the most popular classification tech-
niques into two broad categories:
? Geometric approaches maximize the width of
a separation margin between the classes. The
most popular method is the Support Vector Ma-
chine (SVM) (Vapnik, 1998).
? Probabilistic approaches maximize the con-
ditional likelihood of the output class given
the input features. This logistic regression is
also called Maxent as it finds the distribution
with maximum entropy that properly estimates
the average of each feature over the training
data (Berger et al, 1996).
In previous studies, we found that the best accuracy
is achieved with non-linear (or kernel) SVMs, at the
expense of a high test time complexity, which is un-
acceptable for machine translation. Linear SVMs
and regularized Maxent yield similar performance.
In theory, Maxent training, which scales linearly
with the number of examples, is faster than SVM
training, which scales quadratically with the num-
ber of examples. In our first experiments with lexi-
cal choice models, we observed that Maxent slightly
outperformed SVMs. Using a single threshold with
SVMs, some classes of words were over-detected.
This suggests that, as theory predicts, SVMs do not
properly approximate the posterior probability. We
therefore chose to use Maxent as the best probability
approximator.
4.3 L1 vs. L2 regularization
Traditionally, Maxent is regularized by imposing a
Gaussian prior on each weight: this L2 regulariza-
tion finds the solution with the smallest possible
weights. However, on tasks like machine translation
with a very large number of input features, a Lapla-
cian L1 regularization that also attempts to maxi-
mize the number of zero weights is highly desirable.
A new L1-regularized Maxent algorithms was
proposed for density estimation (Dudik et al, 2004)
and we adapted it to classification. We found this al-
gorithm to converge faster than the current state-of-
the-art in Maxent training, which is L2-regularized
L-BFGS (Malouf, 2002)1. Moreover, the number of
trained parameters is considerably smaller.
5 Data and Experiments
We have performed experiments on the IWSLT06
Chinese-English training and development sets from
1We used the implementation available at
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
156
Table 2: Statistics of training and development data from 2005/2006 (? = first of multiple translations only).
Training (2005) Dev 2005 Dev 2006
Chinese English Chinese English Chinese English
Sentences 46,311 506 489
Running Words 351,060 376,615 3,826 3,897 5,214 6,362?
Vocabulary 11,178 11,232 931 898 1,136 1,134?
Singletons 4,348 4,866 600 538 619 574?
OOVs [%] - - 0.6 0.3 0.9 1.0
ASR WER [%] - - - - 25.2 -
Perplexity - - 33 - 86 -
# References - - 16 7
2005 and 2006. The data are traveler task ex-
pressions such as seeking directions, expressions in
restaurants and travel reservations. Table 2 presents
some statistics on the data sets. It must be noted
that while the 2005 development set matches the
training data closely, the 2006 development set has
been collected separately and shows slightly differ-
ent statistics for average sentence length, vocabulary
size and out-of-vocabulary words. Also the 2006
development set contains no punctuation marks in
Chinese, but the corresponding English translations
have punctuation marks. We also evaluated our
models on the Chinese speech recognition output
and we report results using 1-best with a word er-
ror rate of 25.2%.
For the experiments, we tokenized the Chinese
sentences into character strings and trained the mod-
els discussed in the previous sections. Also, we
trained a punctuation prediction model using Max-
ent framework on the Chinese character strings in
order to insert punctuation marks into the 2006 de-
velopment data set. The resulting character string
with punctuation marks is used as input to the trans-
lation decoder. For the 2005 development set, punc-
tuation insertion was not needed since the Chinese
sentences already had the true punctuation marks.
In Table 3 we present the results of the three dif-
ferent translation models ? FST, Sequential Maxent
and BOW Maxent. There are a few interesting ob-
servations that can be made based on these results.
First, on the 2005 development set, the sequential
Maxent model outperforms the FST model, even
though the two models were trained starting from
the same GIZA++ alignment. The difference, how-
ever, is due to the fact that Maxent models can cope
with increased lexical context2 and the parameters
of the model are discriminatively trained. The more
surprising result is that the BOW Maxent model sig-
nificantly outperforms the sequential Maxent model.
2We use 6 words to the left and right of a source word for
sequential Maxent, but only 2 preceding source and target words
for FST approach.
The reason is that the sequential Maxent model re-
lies on the word alignment, which, if erroneous, re-
sults in incorrect predictions by the sequential Max-
ent model. The BOW model does not rely on the
word-level alignment and can be interpreted as a dis-
criminatively trained model of dictionary lookup for
a target word in the context of a source sentence.
Table 3: Results (mBLEU) scores for the three dif-
ferent models on the transcriptions for development
set 2005 and 2006 and ASR 1-best for development
set 2006.
Dev 2005 Dev 2006
Text Text ASR 1-best
FST 51.8 19.5 16.5
Seq. Maxent 53.5 19.4 16.3
BOW Maxent 59.9 19.3 16.6
As indicated in the data release document, the
2006 development set was collected differently com-
pared to the one from 2005. Due to this mis-
match, the performance of the Maxent models are
not very different from the FST model, indicating
the lack of good generalization across different gen-
res. However, we believe that the Maxent frame-
work allows for incorporation of linguistic features
that could potentially help in generalization across
genres. For translation of ASR 1-best, we see a sys-
tematic degradation of about 3% in mBLEU score
compared to translating the transcription.
In order to compensate for the mismatch between
the 2005 and 2006 data sets, we computed a 10-fold
average mBLEU score by including 90% of the 2006
development set into the training set and using 10%
of the 2006 development set for testing, each time.
The average mBLEU score across these 10 runs in-
creased to 22.8.
In Figure 6 we show the improvement of mBLEU
scores with the increase in permutation window size.
We had to limit to a permutation window size of 10
due to memory limitations, even though the curve
has not plateaued. We anticipate using pruning tech-
niques we can increase the window size further.
157
 0.46
 0.48
 0.5
 0.52
 0.54
 0.56
 0.58
 0.6
 6  6.5  7  7.5  8  8.5  9  9.5  10Permutation Window SizeFigure 6: Improvement in mBLEU score with the
increase in size of the permutation window
5.1 United Nations and Hansard Corpora
In order to test the scalability of the global lexical
selection approach, we also performed lexical se-
lection experiments on the United Nations (Arabic-
English) corpus and the Hansard (French-English)
corpus using the SFST model and the BOW Maxent
model. We used 1,000,000 training sentence pairs
and tested on 994 test sentences for the UN corpus.
For the Hansard corpus we used the same training
and test split as in (Zens and Ney, 2004): 1.4 million
training sentence pairs and 5432 test sentences. The
vocabulary sizes for the two corpora are mentioned
in Table 4. Also in Table 4, are the results in terms of
F-measure between the words in the reference sen-
tence and the decoded sentences. We can see that the
BOW model outperforms the SFST model on both
corpora significantly. This is due to a systematic
10% relative improvement for open class words, as
they benefit from a much wider context. BOW per-
formance on close class words is higher for the UN
corpus but lower for the Hansard corpus.
Table 4: Lexical Selection results (F-measure) on
the Arabic-English UN Corpus and the French-
English Hansard Corpus. In parenthesis are F-
measures for open and closed class lexical items.
Corpus Vocabulary SFST BOW
Source Target
UN 252,571 53,005 64.6 69.5
(60.5/69.1) (66.2/72.6)
Hansard 100,270 78,333 57.4 60.8
(50.6/67.7) (56.5/63.4)
6 Discussion
The BOW approach is promising as it performs rea-
sonably well despite considerable losses in the trans-
fer of information between source and target lan-
guage. The first and most obvious loss is about word
position. The only information we currently use to
restore the target word position is the target language
model. Information about the grammatical role of a
word in the source sentence is completely lost. The
language model might fortuitously recover this in-
formation if the sentence with the correct grammat-
ical role for the word happens to be the maximum
likelihood sentence in the permutation automaton.
We are currently working toward incorporating
syntactic information on the target words so as to be
able to recover some of the grammatical role infor-
mation lost in the classification process. In prelimi-
nary experiments, we have associated the target lex-
ical items with supertag information (Bangalore and
Joshi, 1999). Supertags are labels that provide linear
ordering constraints as well as grammatical relation
information. Although associating supertags to tar-
get words increases the class set for the classifier, we
have noticed that the degradation in the F-score is
on the order of 3% across different corpora. The su-
pertag information can then be exploited in the sen-
tence construction process. The use of supertags in
phrase-based SMT system has been shown to im-
prove results (Hassan et al, 2006).
A less obvious loss is the number of times a word
or concept appears in the target sentence. Func-
tion words like ?the? and ?of? can appear many
times in an English sentence. In the model dis-
cussed in this paper, we index each occurrence of the
function word with a counter. In order to improve
this method, we are currently exploring a technique
where the function words serve as attributes (e.g.
definiteness, tense, case) on the contentful lexical
items, thus enriching the lexical item with morpho-
syntactic information.
A third issue concerning the BOW model is the
problem of synonyms ? target words which translate
the same source word. Suppose that in the training
data, target words t1 and t2 are, with equal probabil-
ity, translations of the same source word. Then, in
the presence of this source word, the probability to
detect the corresponding target word, which we as-
sume is 0.8, will be, because of discriminant learn-
ing, split equally between t1 and t2, that is 0.4 and
0.4. Because of this synonym problem, the BOW
threshold ? has to be set lower than 0.5, which is
observed experimentally. However, if we set the
threshold to 0.3, both t1 and t2 will be detected in
the target sentence, and we found this to be a major
source of undesirable insertions.
The BOW approach is different from the pars-
ing based approaches (Melamed, 2004; Zhang and
Gildea, 2005; Cowan et al, 2006) where the transla-
tion model tightly couples the syntactic and lexical
items of the two languages. The decoupling of the
158
two steps in our model has the potential for gener-
ating paraphrased sentences not necessarily isomor-
phic to the structure of the source sentence.
7 Conclusions
We view machine translation as consisting of lexi-
cal selection and lexical reordering steps. These two
steps need not necessarily be sequential and could be
tightly integrated. We have presented the weighted
finite-state transducer model of machine translation
where lexical choice and a limited amount of lexical
reordering are tightly integrated into a single trans-
duction. We have also presented a novel approach
to translation where these two steps are loosely cou-
pled and the parameters of the lexical choice model
are discriminatively trained using a maximum en-
tropy model. The lexical reordering model in this
approach is achieved using a permutation automa-
ton. We have evaluated these two approaches on the
2005 and 2006 IWSLT development sets and shown
that the techniques scale well to Hansard and UN
corpora.
References
H. Alshawi, S. Bangalore, and S. Douglas. 1998. Automatic
acquisition of hierarchical transduction models for machine
translation. In ACL, Montreal, Canada.
R.E. Banchs, J.M. Crego, A. Gispert, P. Lambert, and J.B.
Marino. 2005. Statistical machine translation of euparl data
by using bilingual n-grams. In Workshop on Building and
Using Parallel Texts. ACL.
S. Bangalore and A. K. Joshi. 1999. Supertagging: An ap-
proach to almost parsing. Computational Linguistics, 25(2).
S. Bangalore and G. Riccardi. 2000. Stochastic finite-state
models for spoken language machine translation. In Pro-
ceedings of the Workshop on Embedded Machine Transla-
tion Systems, pages 52?59.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent.
1996. A Maximum Entropy Approach to Natural Language
Processing. Computational Linguistics, 22(1):39?71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The
Mathematics of Machine Translation: Parameter Estimation.
Computational Linguistics, 16(2):263?312.
D. Chiang. 2005. A hierarchical phrase-based model for statis-
tical machine translation. In Proceedings of the ACL Con-
ference, Ann Arbor, MI.
B. Cowan, I. Kucerova, and M. Collins. 2006. A discrimi-
native model for tree-to-tree translation. In Proceedings of
EMNLP.
H. Daume, J. Langford, and D. Marcu. 2007. Search-based
structure prediction. submitted to Machine Learning Jour-
nal.
M. Dudik, S. Phillips, and R.E. Schapire. 2004. Perfor-
mance Guarantees for Regularized Maximum Entropy Den-
sity Estimation. In Proceedings of COLT?04, Banff, Canada.
Springer Verlag.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tur, A. Ljolje,
S. Parthasarathy, M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&T WATSON Speech Recognizer. In Pro-
ceedings of ICASSP, Philadelphia, PA.
H. Hassan, M. Hearne, K. Sima?an, and A. Way. 2006. Syntac-
tic phrase-based statistical machine translation. In Proceed-
ings of IEEE/ACL first International Workshop on Spoken
Language Technology (SLT), Aruba, December.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005.
Novel reordering approaches in phrase-based statistical ma-
chine translation. In Proceedings of the ACL Workshop on
Building and Using Parallel Texts, pages 167?174, Ann Ar-
bor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In Proceedings of ICML, San Fran-
cisco, CA.
R. Malouf. 2002. A comparison of algorithms for maximum
entropy parameter estimation. In Proceedings of CoNLL-
2002, pages 49?55. Taipei, Taiwan.
I. D. Melamed. 2004. Statistical machine translation by pars-
ing. In Proceedings of ACL.
F. J. Och and H. Ney. 2002. Discriminative training and max-
imum entropy models for statistical machine translation. In
Proceedings of ACL.
F.J. Och and H. Ney. 2003. A systematic comparison of vari-
ous statistical alignment models. Computational Linguistics,
29(1):19?51.
Ryan Rifkin and Aldebaro Klautau. 2004. In defense of one-
vs-all classification. Journal of Machine Learning Research,
pages 101?141.
C. Tillmann and T. Zhang. 2006. A discriminative global train-
ing algorithm for statistical mt. In COLING-ACL.
V.N. Vapnik. 1998. Statistical Learning Theory. John Wiley &
Sons.
B. Wellington, J. Turian, C. Pike, and D. Melamed. 2006. Scal-
able purely-discriminative training for word and tree trans-
ducers. In AMTA.
D. Wu. 1997. Stochastic Inversion Transduction Grammars
and Bilingual Parsing of Parallel Corpora. Computational
Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based statistical
translation model. In Proceedings of 39th ACL.
R. Zens and H. Ney. 2004. Improvements in phrase-based sta-
tistical machine translation. In Proceedings of HLT-NAACL,
pages 257?264, Boston, MA.
H. Zhang and D. Gildea. 2005. Stochastic lexicalized inver-
sion transduction grammar for alignment. In Proceedings of
ACL.
159
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 225?228,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Enriching spoken language translation with dialog acts
Vivek Kumar Rangarajan Sridhar
Shrikanth Narayanan
Speech Analysis and Interpretation Laboratory
University of Southern California
vrangara@usc.edu,shri@sipi.usc.edu
Srinivas Bangalore
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932, U.S.A.
srini@research.att.com
Abstract
Current statistical speech translation ap-
proaches predominantly rely on just text tran-
scripts and do not adequately utilize the
rich contextual information such as conveyed
through prosody and discourse function. In
this paper, we explore the role of context char-
acterized through dialog acts (DAs) in statis-
tical translation. We demonstrate the integra-
tion of the dialog acts in a phrase-based statis-
tical translation framework, employing 3 lim-
ited domain parallel corpora (Farsi-English,
Japanese-English and Chinese-English). For
all three language pairs, in addition to produc-
ing interpretable DA enriched target language
translations, we also obtain improvements in
terms of objective evaluation metrics such as
lexical selection accuracy and BLEU score.
1 Introduction
Recent approaches to statistical speech translation
have relied on improving translation quality with
the use of phrase translation (Och and Ney, 2003;
Koehn, 2004). The quality of phrase translation
is typically measured using n-gram precision based
metrics such as BLEU (Papineni et al, 2002) and
NIST scores. However, in many dialog based speech
translation scenarios, vital information beyond what
is robustly captured by words and phrases is car-
ried by the communicative act (e.g., question, ac-
knowledgement, etc.) representing the function of
the utterance. Our approach for incorporating di-
alog act tags in speech translation is motivated by
the fact that it is important to capture and convey
not only what is being communicated (the words)
but how something is being communicated (the con-
text). Augmenting current statistical translation
frameworks with dialog acts can potentially improve
translation quality and facilitate successful cross-
lingual interactions in terms of improved informa-
tion transfer.
Dialog act tags have been previously used in the
VERBMOBIL statistical speech-to-speech transla-
tion system (Reithinger et al, 1996). In that work,
the predicted DA tags were mainly used to improve
speech recognition, semantic evaluation, and infor-
mation extraction modules. Discourse information
in the form of speech acts has also been used in in-
terlingua translation systems (Mayfield et al, 1995)
to map input text to semantic concepts, which are
then translated to target text.
In contrast with previous work, in this paper we
demonstrate how dialog act tags can be directly ex-
ploited in phrase based statistical speech translation
systems (Koehn, 2004). The framework presented
in this paper is particularly suited for human-human
and human-computer interactions in a dialog set-
ting, where information loss due to erroneous con-
tent may be compensated to some extent through the
correct transfer of the appropriate dialog act. The
dialog acts can also be potentially used for impart-
ing correct utterance level intonation during speech
synthesis in the target language. Figure 1 shows an
example where the detection and transfer of dialog
act information is beneficial in resolving ambiguous
intention associated with the translation output.
Figure 1: Example of speech translation output enriched with
dialog act
The remainder of this paper is organized as fol-
lows: Section 2 describes the dialog act tagger used
in this work, Section 3 formulates the problem, Sec-
tion 4 describes the parallel corpora used in our ex-
periments, Section 5 summarizes our experimental
results and Section 6 concludes the paper with a
brief discussion and outline for future work.
2 Dialog act tagger
In this work, we use a dialog act tagger trained on
the Switchboard DAMSL corpus (Jurafsky et al,
225
1998) using a maximum entropy (maxent) model.
The Switchboard-DAMSL (SWBD-DAMSL) cor-
pus consists of 1155 dialogs and 218,898 utterances
from the Switchboard corpus of telephone conver-
sations, tagged with discourse labels from a shal-
low discourse tagset. The original tagset of 375
unique tags was clustered to obtain 42 dialog tags
as in (Jurafsky et al, 1998). In addition, we also
grouped the 42 tags into 7 disjoint classes, based
on the frequency of the classes and grouped the re-
maining classes into an ?Other? category constitut-
ing less than 3% of the entire data. The simplified
tagset consisted of the following classes: statement,
acknowledgment, abandoned, agreement, question,
appreciation, other.
We use a maximum entropy sequence tagging
model for the automatic DA tagging. Given a se-
quence of utterances U = u1, u2, ? ? ? , un and a
dialog act vocabulary (di ? D, |D| = K), we
need to assign the best dialog act sequence D? =
d1, d2, ? ? ? , dn. The classifier is used to assign to
each utterance a dialog act label conditioned on a
vector of local contextual feature vectors comprising
the lexical, syntactic and acoustic information. We
used the machine learning toolkit LLAMA (Haffner,
2006) to estimate the conditional distribution using
maxent. The performance of the maxent dialog act
tagger on a test set comprising 29K utterances of
SWBD-DAMSL is shown in Table 1.
Accuracy (%)
Cues used (current utterance) 42 tags 7 tags
Lexical 69.7 81.9
Lexical+Syntactic 70.0 82.4
Lexical+Syntactic+Prosodic 70.4 82.9
Table 1: Dialog act tagging accuracies for various cues on the
SWBD-DAMSL corpus.
3 Enriched translation using DAs
If Ss, Ts and St, Tt are the speech signals and equiv-
alent textual transcription in the source and target
language, and Ls the enriched representation for the
source speech, we formalize our proposed enriched
S2S translation in the following manner:
S?t = argmaxSt
P (St|Ss) (1)
P (St|Ss) =
?
Tt,Ts,Ls
P (St, Tt, Ts, Ls|Ss) (2)
?
?
Tt,Ts,Ls
P (St|Tt, Ls).P (Tt, Ts, Ls|Ss) (3)
where Eq.(3) is obtained through conditional inde-
pendence assumptions. Even though the recogni-
tion and translation can be performed jointly (Ma-
tusov et al, 2005), typical S2S translation frame-
works compartmentalize the ASR, MT and TTS,
with each component maximized for performance
individually.
max
St
P (St|Ss) ? maxSt P (St|T
?
t , L?s)
?max
Tt
P (Tt|T ?s , L?s) (4)
?max
Ls
P (Ls|T ?s , Ss)?maxTs P (Ts|Ss)
where T ?s , T ?t and S?t are the arguments maximiz-
ing each of the individual components in the transla-
tion engine. L?s is the rich annotation detected from
the source speech signal and text, Ss and T ?s respec-
tively. In this work, we do not address the speech
synthesis part and assume that we have access to the
reference transcripts or 1-best recognition hypothe-
sis of the source utterances. The rich annotations
(Ls) can be syntactic or semantic concepts (Gu et
al., 2006), prosody (Agu?ero et al, 2006), or, as in
this work, dialog act tags.
3.1 Phrase-based translation with dialog acts
One of the currently popular and predominant
schemes for statistical translation is the phrase-
based approach (Koehn, 2004). Typical phrase-
based SMT approaches obtain word-level align-
ments from a bilingual corpus using tools such as
GIZA++ (Och and Ney, 2003) and extract phrase
translation pairs from the bilingual word alignment
using heuristics. Suppose, the SMT had access to
source language dialog acts (Ls), the translation
problem may be reformulated as,
T ?t = argmaxTt
P (Tt|Ts, Ls)
= argmax
Tt
P (Ts|Tt, Ls).P (Tt|Ls) (5)
The first term in Eq.(5) corresponds to a dialog act
specific MT model and the second term to a dia-
log act specific language model. Given sufficient
amount of training data such a system can possibly
generate hypotheses that are more accurate than the
scheme without the use of dialog acts. However, for
small scale and limited domain applications, Eq.(5)
leads to an implicit partitioning of the data corpus
226
Training Test
Farsi Eng Jap Eng Chinese Eng Farsi Eng Jap Eng Chinese Eng
Sentences 8066 12239 46311 925 604 506
Running words 76321 86756 64096 77959 351060 376615 5442 6073 4619 6028 3826 3897
Vocabulary 6140 3908 4271 2079 11178 11232 1487 1103 926 567 931 898
Singletons 2819 1508 2749 1156 4348 4866 903 573 638 316 600 931
Table 2: Statistics of the training and test data used in the experiments.
and might generate inferioir translations in terms of
lexical selection accuracy or BLEU score.
A natural step to overcome the sparsity issue is
to employ an appropriate back-off mechanism that
would exploit the phrase translation pairs derived
from the complete data. A typical phrase transla-
tion table consists of 5 phrase translation scores for
each pair of phrases, source-to-target phrase transla-
tion probability (?1), target-to-source phrase transla-
tion probability (?2), source-to-target lexical weight
(?3), target-to-word lexical weight (?4) and phrase
penalty (?5= 2.718). The lexical weights are the
product of word translation probabilities obtained
from the word alignments. To each phrase trans-
lation table belonging to a particular DA-specific
translation model, we append those entries from the
baseline model that are not present in phrase table
of the DA-specific translation model. The appended
entries are weighted by a factor ?.
(Ts ? Tt)L?s = (Ts ? Tt)Ls ? {?.(Ts ? Tt)
s.t. (Ts ? Tt) 6? (Ts ? Tt)Ls} (6)
where (Ts ? Tt) is a short-hand1 notation for a
phrase translation table. (Ts ? Tt)Ls is the DA-
specific phrase translation table, (Ts ? Tt) is the
phrase translation table constructed from entire data
and (Ts ? Tt)L?s is the newly interpolated phrase
translation table. The interpolation factor ? is used
to weight each of the four translation scores (phrase
translation and lexical probabilities for the bilan-
guage) with the phrase penalty remaining a con-
stant. Such a scheme ensures that phrase translation
pairs belonging to a specific DA model are weighted
higher and also ensures better coverage than a parti-
tioned data set.
4 Data
We report experiments on three different paral-
lel corpora: Farsi-English, Japanese-English and
1(Ts ? Tt) represents the mapping between source alpha-
bet sequences to target alhabet sequences, where every pair
(ts1, ? ? ? , tsn, tt1, ? ? ? , ttm) has a weight sequence ?1, ? ? ? , ?5
(five weights).
Chinese-English. The Farsi-English data used in
this paper was collected for human-mediated doctor-
patient mediated interactions in which an English
speaking doctor interacts with a Persian speaking
patient (Narayanan et al, 2006). We used a subset
of this corpus consisting of 9315 parallel sentences.
The Japanese-English parallel corpus is a part
of the ?How May I Help You? (HMIHY) (Gorin
et al, 1997) corpus of operator-customer conversa-
tions related to telephone services. The corpus con-
sists of 12239 parallel sentences. The conversations
are spontaneous even though the domain is lim-
ited. The Chinese-English corpus corresponds to the
IWSLT06 training and 2005 development set com-
prising 46K and 506 sentences respectively (Paul,
2006).
5 Experiments and Results
In all our experiments we assume that the same di-
alog act is shared by a parallel sentence pair. Thus,
even though the dialog act prediction is performed
for English, we use the predicted dialog act as the di-
alog act for the source language sentence. We used
the Moses2 toolkit for statistical phrase-based trans-
lation. The language models were trigram models
created only from the training portion of each cor-
pus. Due to the relatively small size of the corpora
used in the experiments, we could not devote a sep-
arate development set for tuning the parameters of
the phrase-based translation scheme. Hence, the ex-
periments are strictly performed on the training and
test sets reported in Table 23.
The lexical selection accuracy and BLEU scores
for the three parallel corpora is presented in Table 3.
Lexical selection accuracy is measured in terms of
the F-measure derived from recall ( |Res?Ref ||Ref | ? 100)
and precision ( |Res?Ref ||Res| ? 100), where Ref is the
set of words in the reference translation and Res is
2http://www.statmt.org/moses
3A very small subset of the data was reserved for optimizing
the interpolation factor (?) described in Section 3.1
227
F-score (%) BLEU (%)
w/o DA tags w/ DA tags w/o DA tags w/ DA tags
Language pair 7tags 42tags 7tags 42tags
Farsi-English 56.46 57.32 57.74 22.90 23.50 23.75
Japanese-English 79.05 79.40 79.51 54.15 54.21 54.32
Chinese-English 65.85 67.24 67.49 48.59 52.12 53.04
Table 3: F-measure and BLEU scores with and without use of dialog act tags.
the set of words in the translation output. Adding di-
alog act tags (either 7 or 42 tag vocabulary) consis-
tently improves both the lexical selection accuracy
and BLEU score for all the language pairs. The im-
provements for Farsi-English and Chinese-English
corpora are more pronounced than the improve-
ments in Japanese-English corpus. This is due to the
skewed distribution of dialog acts in the Japanese-
English corpus; 80% of the test data are statements
while other and questions category make up 16%
and 3.5% of the data respectively. The important
observation here is that, appending DA tags in the
form described in this work, can improve translation
performance even in terms of conventional objective
evaluation metrics. However, the performance gain
measured in terms of objective metrics that are de-
signed to reflect only the orthographic accuracy dur-
ing translation is not a complete evaluation of the
translation quality of the proposed framework. We
are currently planning of adding human evaluation
to bring to fore the usefulness of such rich anno-
tations in interpreting and supplementing typically
noisy translations.
6 Discussion and Future Work
It is important to note that the dialog act tags used
in our translation system are predictions from the
maxent based DA tagger described in Section 2. We
do not have access to the reference tags; thus, some
amount of error is to be expected in the DA tagging.
Despite the lack of reference DA tags, we are still
able to achieve modest improvements in the trans-
lation quality. Improving the current DA tagger and
developing suitable adaptation techniques are part of
future work.
While we have demonstrated here that using dia-
log act tags can improve translation quality in terms
of word based automatic evaluation metrics, the real
benefits of such a scheme would be attested through
further human evaluations. We are currently work-
ing on conducting subjective evaluations.
References
P. D. Agu?ero, J. Adell, and A. Bonafonte. 2006. Prosody
generation for speech-to-speech translation. In Proc.
of ICASSP, Toulouse, France, May.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May I
Help You? Speech Communication, 23:113?127.
L. Gu, Y. Gao, F. H. Liu, and M. Picheny. 2006.
Concept-based speech-to-speech translation using
maximum entropy models for statistical natural con-
cept generation. IEEE Transactions on Audio, Speech
and Language Processing, 14(2):377?392, March.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239?261.
D. Jurafsky, R. Bates, N. Coccaro, R. Martin, M. Meteer,
K. Ries, E. Shriberg, S. Stolcke, P. Taylor, and C. Van
Ess-Dykema. 1998. Switchboard discourse language
modeling project report. Technical report research
note 30, Johns Hopkins University, Baltimore, MD.
P. Koehn. 2004. Pharaoh: A beam search decoder for
phrasebased statistical machine translation models. In
Proc. of AMTA-04, pages 115?124.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proc. of Eurospeech.
L. Mayfield, M. Gavalda, W. Ward, and A. Waibel.
1995. Concept-based speech translation. In Proc. of
ICASSP, volume 1, pages 97?100, May.
S. Narayanan et al 2006. Speech recognition engineer-
ing issues in speech to speech translation system de-
sign for low resource languages and domains. In Proc.
of ICASSP, Toulose, France, May.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. Technical report, IBM T.J. Watson Re-
search Center.
M. Paul. 2006. Overview of the IWSLT 2006 Evaluation
Campaign. In Proc. of the IWSLT, pages 1?15, Kyoto,
Japan.
N. Reithinger, R. Engel, M. Kipp, and M. Klesen. 1996.
Predicting dialogue acts for a speech-to-speech trans-
lation system. In Proc. of ICSLP, volume 2, pages
654?657, Oct.
228
Stochastic Finite-State models for Spoken Language Machine 
': anslation 
Sr in ivas  Banga lore  G iuseppe R iccard i  
AT&T Labs - Research 
180  Park  Avenue 
F lorham Park,  NJ  07932 
{srini, ds'.p3}?research, att. tom 
Abst ract  
Stochastic finite-state models are efficiently learn- 
able from data, effective for decoding and are asso- 
ciated with a calculus for composing models which 
allows for tight integration of constraints frora var- 
ious levels of language processing. In this paper, 
we present a method for stochastic finite-state ma- 
chine translation that is trained automatically from 
pairs of source and target utterances. We use this 
method to develop models for English-Japanese and 
Japanese-English translation. We have embedded 
the Japanese-English translation system in a call 
routing task of unconstrained speech utterances. We 
evaluate the efficacy of the translation system :in the 
context of this application. 
1 In t roduct ion  
Finite state models have been extensively applied 
to many aspects of language processing including, 
speech recognition (Pereira nd Riley, 1997; Riccardi 
et al, 1996), phonology (Kaplan and Kay, 1994), 
morphology (Koskenniemi, 1984), chunking (Abney, 
1991; Srinivas, 1997) and parsing (Roche, 1.999). 
Finite-state models are attractive mechanisms for 
language processing since they are (a) efficiently 
learnable from data (b) generally effective for de- 
coding (c) associated with a calculus for composing 
models which allows for straightforward integration 
of constraints from various levels of language pro- 
cessing. I 
In this paper, we develop stochastic finite-state 
models (SFSM) for statistical machine transla- 
tion (SMT) and explore the performance limits of 
such models in the context of translation in limited 
domains. We are also interested in these models 
since they allow for a tight integration with a speech 
recognizer for speech-to-speech translation. In par- 
ticular we are interested in one-pass decoding and 
translation of speech as opposed to the more preva- 
lent approach of translation of speech lattices. 
The problem of machine translation can be viewed 
as consisting of two phases: (a) lexical choice phase 
1 Furthermore, software implementing the finite-stal;e cal- 
culus is available for research purposes. 
where appropriate target language lexical items are 
chosen for each source language lexical item and (b) 
reordering phase where the chosen target language 
lexical items are reordered to produce a meaning- 
ful target language string. In our approach, we will 
represent hese two phases using stochastic finite- 
state models which can be composed together to 
result in a single stochastic finite-state model for 
SMT. Thus our method can be viewed as a direct 
translation approach of transducing strings of the 
source language to strings of the target language. 
There are other approaches to statistical machine 
translation where translation is achieved through 
transduction of source language structure to tar- 
get language structure (Alshawi et al, 1998b; Wu, 
1997). There are also large international multi-site 
projects such as VERBMOBIL (Verbmobil, 2000) 
and CSTAR (Woszczyna et al, 1998; Lavie et al, 
1999) that are involved in speech-to-speech trans- 
lation in limited domains. The systems developed 
in these projects employ various techniques ranging 
from example-based to interlingua-based translation 
methods for translation between English, French, 
German, Italian, Japanese, and Korean. 
Finite-state models for SMT have been previ- 
ously suggested in the literature (Vilar et al, 1999; 
Knight and A1-Onaizan, 1998). In (Vilar et al, 
1999), a deterministic transducer is used to imple- 
ment an English-Spanish speech translation system. 
In (Knight and A1-Onaizan, 1998), finite-state ma- 
chine translation is based on (Brown et al, 1993) 
and is used for decoding the target language string. 
However, no experimental results are reported using 
this approach. 
? Our approach differs from the previous approaches 
in both the lexical choice and the reordering phases. 
Unlike the previous approaches, the lexical choice 
phase in our approach is decomposed into phrase- 
level and sentence-level translation models. The 
phrase-level translation is learned based on joint en- 
tropy reduction of the source and target languages 
and a variable length n-gram model (VNSA) (Ric- 
cardi et al, 1995; Riccardi et al, 1996) is learrmd 
for the sentence-level translation. For the construc- 
52 
tion of the bilingual exicon needed for lexical choice, 
we use the alignment algorithm presented in (A1- 
shawl et al, 1998b) which takes advantage of hi- 
erarchical decomposition of strings and thus per- 
forms a structure-based alignment. In the previ- 
ous approaches, a bilingual lexicon is constructed 
using a string-based alignment. Another difference 
between our approach and the previous approaches 
is in the reordering of the target language lexical 
items. In (Knight and A1-Onaizan, 1998), an FSM 
that represents all strings resulting from the per- 
mutations of the lexical items produced by lexical 
choice is constructed and the most likely translation 
is retrieved using a target language model. In (Vilar 
et al, 1999), the lexical items are associated with 
markers that allow for reconstruction of the target 
language string. Our reordering step is similar to 
that proposed in (Knight and A1-Onalzan, 1998) but 
does not incur the expense of creating a permutation 
lattice. We use a phrase-based VNSA target lan- 
guage model to retrieve the most likely translation 
from the lattice. 
In addition, we have used the resulting finite- 
state translation method to implement an English- 
Japanese speech and text translation system and 
a Japanese-English text translation system. We 
present evaluation results for these systems and dis- 
cuss their limitations. We also evaluate the efficacy 
of this translation model in the context of a telecom 
application such as call routing. 
The layout of the paper is as follows. In Section 2 
we discuss the architecture of the finite-state trans- 
lation system. We discuss the algorithm for learning 
lexical and phrasal translation in Section 3. The de- 
tails of the translation model are presented in Sec- 
tion 4 and our method for reordering the output 
is presented in Section 5. In Section 6 we discuss 
the call classification application and present moti- 
vations for embedding translation in such an applica- 
tion. In Section 6.1 we present he experiments and 
evaluation results for the various translation systems 
on text input. 
2 Stochastic Machine Translation 
In machine translation, the objective is to map a 
source symbol sequence Ws = wx,...,WNs (wi E 
Ls) into a target sequence WT = xl, . . . ,  XNT (Xi E 
LT). The statistical machine translation approach 
is based on the noisy channel paradigm and the 
Maximum-A-Posteriori decoding algorithm (Brown 
et al, 1993). The sequence Ws is thought as a noisy 
version of WT and the best guess I)d~ is then com- 
puted as 
^ 
W~ = argmax P(WT{Ws) wT 
= argmax P(WslWT)P(WT) (1) wT 
In (Brown et al, 1993) they propose a method for 
maximizing P(WTIWs) by estimating P(WT) and 
P(WsIWT) and solving the problem in equation 1. 
Our approach to statistical machine translation dif- 
fers from the model proposed in (Brown et al, 1993) 
in that: 
? We compute the joint model P(Ws, WT) from 
the bilanguage corpus to account for the direct 
mapping of the source sentence Ws into the tar- 
get sentence I?VT that is ordered according to the 
? source language word order. The target string 
IfV~ is then chosen from all possible reorderings 2 
of 
I?VT = argmax P(Ws, WT) (2) 
WT 
\[TV~ = arg max P(I~VT IAT) (3) 
WTE~W T
where AT is the target language model and AWT 
are the different reorderings of WT. 
? We decompose the translation problem into 
local (phrase-level) and global (sentence-level) 
source-target s ring transduction. 
? We automatically learn stochastic automata 
and transducers to perform the sentence-level 
and phrase-level translation. 
As shown in Figure 1, the stochastic machine 
translation system consists of two phases, the lexical 
choice phase and the reordering phase. In the next 
sections we describe the finite-state machine com- 
ponents and the operation cascade that implements 
this translation algorithm. 
3 Acquir ing Lexical Translations 
In the problem of speech recognition the alignment 
between the words and their acoustics is relatively 
straightforward since the words appear in the same 
order as their corresponding acoustic events. In con- 
trast, in machine" translation, the linear order of 
words in the source language, in general is not main- 
tained in the target language. 
The first stage in the process of bilingual phrase 
acquisition is obtaining an alignment function that 
given a pair of source and target language sentences, 
maps source language word subsequences into target 
language word subsequences. For this purpose, we 
use the alignment algorithm described in (Alshawi et 
2 Note that computing the exact set of all possible reorder- 
ings is computationally expensive. In Section 5 we discuss 
an approximation for the set of all possible reorderings that 
serves for our application. 
53  
max P(Ws,W r ) 
WT 
Reorder 
Figure 1: A block diagram of the stochastic machine translation system 
English: I need to make a collect call 
Japanese: ~l~ ~lzP  b ~--Jt~ 
Alignment: 1 5 0 3 0 2 4 
English: A T and T calling card 
Japanese: ~ 4 ~ -~ -- 7" Y F 
Alignment: 123456 
English: I'd like to charge this to my home phone 
Japanese: ~./J2 ~ $J,?~ ~69 ~C.  -~-~--~ 
Alignment: 170620345 
Table 1: Example bitexts and with alignment information 
al., 1998a). The result of the alignment procedure 
is shown in Table 1.3 
Although the search for bilingual phrases of length 
more than two words can be incorporated in a 
straight-forward manner in the alignment module, 
we find that doing so is computationally prohibitive. 
We first transform the output of the alignment 
into a representation conducive for further manip- 
ulation. We call this a bilanguage TB. A string 
R E TB is represented as follows: 
R = Wl-Z l ,  W2_Z2,. . .  , WN-ZN (4) 
an example alignment and the source-word-ordered 
bilanguage strings corresponding to the alignment 
shown in Table 1. 
Having transformed the alignment for each sen- 
tence pair into a bilanguage string (source word- 
ordered or target word-ordered), we proceed to seg- 
ment the corpus into bilingual phrases which can be 
acquired from the corpus TB by minimizing the joint 
entropy H(Ls, LT) ~ -1 /M log P(TB). The proba- 
bility P(Ws, WT) = P(R)  is computed in the same 
way as n-gram model: 
where wl E LsUe, zi E LTUe, e is the empty 
string and wi_zi is the symbol pair (colons are the 
delimiters) drawn from the source and target lan- 
guage. 
A string in a bilanguage corpus consists of se- 
quences of tokens where each token (wi-xi) is repre- 
sented with two components: a source word (\]possi- 
bly an empty word) as the first component and the 
target word (possibly an empty word) that is the 
translation of the source word as the second com- 
ponent. Note that the tokens of a bilanguage could 
be either ordered according to the word order of the 
source language or ordered according to the word 
order of the target language. Thus an alignment 
of a pair of source and target language sentences 
will result in two bilanguage strings. Table 2 shows 
3The Japanese string was translated and segmented so 
that a token boundary in Japanese corresponds tosome token 
boundary in English. 
P(R) = Il l 
i 
(5) 
Using the phrase segmented corpus, we construct 
a phrase-based variable n-gram translation model as 
discussed in the following section. 
4 Learn ing  Phrase-based  Var iab le  
N-gram Trans la t ion  Mode ls  
Our approach to stochastic language modeling is 
based on the Variable Ngram Stochastic Automaton 
(VNSA) representation and learning algorithms 
introduced in (Riccardi et al, 1995; Pdccardi et al, 
1996). A VNSA is a non-deterministic Stochastic 
Finite-State Machine (SFSM) that allows for pars- 
ing any possible sequence of words drawn from a 
given vocabulary 12. In its simplest implementation 
the state q in the VNSA encapsulates the lexical 
(word sequence) history of a word sequence. Each 
54 
I_i./Ji need_~,~5 9 ~ ~ to_%EPS% make_~--It, ~" a_%EPS% collect_ ~ I /2 b call_hi l~ 
I'd_$L~2c like_ L 1",: ~ ~ '~ '~ to_%EPS% charge_-~-'v - ~ this_,._ ~ ~ to_%EPS% my_*.L~ home. .~ phone_~-~b= 
A_m4 T_-~ 4 -- and_T:/V T_~ ~ -- calling_K-- ~) Z/Y" card_2-- V 
Table 2: Bilanguage strings resulting from alignments shown in Table 1. 
(%EPS% represents he null symbol c). 
state recognizes asymbol wi E lZU {e}, where e is the 
empty string. The probability of going from state qi 
to qj (and recognizing the symbol associated to qj) 
is given by the state transition probability, P(qj \[qi). 
Stochastic finite-state machines represent m a 
compact way the probability distribution over all 
possible word sequences. The probability of a word 
sequence W can be associated to a state sequence 
~Jw = ql , . . . ,  qj and to the probability P(~Jw)" For 
a non-deterministic f nite-state machine the prob- 
ability of W is then given by P(W) = ~j  P((Jw). 
Moreover, by appropriately defining the state space 
to incorporate l xical and extra-lexical information, 
the VNSA formalism can generate a wide class of 
probability distribution (i.e., standard word n-gram, 
class-based, phrase-based, etc.) (Riccardi et al, 
1996; Riccardi et al, 1997; Riccardi and Bangalore, 
1998). In Fig. 2, we plot a fragment of a VNSA 
trained with word classes and phrases. State 0 is 
the initial state and final states are double circled. 
The e transition from state 0 to state 1 carries 
the membership probability P(C), where the class 
C contains the two elements {collect, ca l l ing  
card}. The c transition from state 4 to state 6 
is a back-off transition to a lower order n-gram 
probability. State 2 carries the information about 
the phrase ca l l ing  card. The state transition 
function, the transition probabilities and state 
space are learned via the self-organizing algorithms 
presented in (Riccardi et al, 1996). 
4.1 Extending VNSAs to Stochastic 
Transducers 
Given the monolingual corpus T, the VNSA learning 
algorithm provides an automaton that recognizes an 
input string W (W E yY) and computes P(W) ? 0 
for each W. Learning VNSAs from the bilingual cor- 
pus TB leads to the notion of stochastic transducers 
rST. Stochastic transducers ST : Ls ? LT ~ \[0, 1\] 
map the string Ws E Ls into WT E LT and assign 
a probability to the transduction Ws ~--~ WT. In 
our case, the VNSA's model will estimate P(Ws ~-~.~" 
WT) : P(Ws, WT) and the symbol pair wi : xi 
will be associated to each transducer state q with 
input label wi and output label xl. The model 
rST provides a sentence-level transduction from Ws 
into WT. The integrated sentence and phrase-level 
transduction is then trained directly on the phrase- 
segmented corpus 7~ described in section 3. 
5 Reorder ing  the  output  
The stochastic transducers TST takes as input a sen- 
tence Ws and outputs a set of candidate strings in 
the target language with source language word or- 
der. Recall that the one-to-many mapping comes 
from the non-determinism of VST. The maximiza- 
tion step in equation 2is carried out with Viterbi al- 
gorithm over the hypothesized strings in LT and I~VT 
is selected. The last step to complete the translation 
process is to apply the monolingual target language 
model A T to re-order the sentence I?VT to produce 
^ 
W~. The re-order operation is crucial especially 
in the case the bilanguage phrases in 7~ are not 
sorted in the target language. For the re-ordering 
operation, the exact approach would be to search 
through all possible permutations of the words in 
ITVT and select the most likely. However, that op- 
eration is computationally very expensive. To over- 
come this problem, we approximate the set of the 
permutations with the word lattice AWT represent- 
ing (xl I x2 I . . .  XN) N, where xi are the words in 
ITVT. The most likely string ~V~ in the word lattice 
is then decoded as follows: 
^ 
W~ = argmax(~T o ~WT) 
= arg max P(~VT I)~T) 
(6) 
Where o is the composition operation defined for 
weighted finite-state machines (Pereira and Riley, 
1997). The complete operation cascade for the ma- 
chine translation process is shown in Figure 3. 
6 Embedd ing  Trans la t ion  in an  
App l i ca t ion  
In this section, we describe an application in 
which we have embedded our translation model and 
present some of the motivations for doing so. The 
application that we are interested in is a call type 
classification task called How May I Help You (Gorin 
et al, 1997). The goal is to sufficiently understand 
55 
collect/0.5 
ycs/O.8 ~ ~ ' ~ . _ ~ ~  
calYl 
please/1 < 
Figure 2: Example of a Variiable Ngram Stochastic Automaton (VNSA). 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
zsr 2,- 
m xe(v ,w ) \] w"  
Reorder 
Figure 3: The Machine Translation architecture 
caller's responses to the open-ended prompt How 
May I Help You? and route such a call based on the 
meaning of the response. Thus we aim at extracting 
a relatively small number of semantic actions from 
the utterances ofa very large set of users who are not 
trained to the system's capabilities and limitations. 
The first utterance of each transaction has been 
transcribed and marked with a call-type by label- 
ers. There are 14 call-types plus a class other for 
the complement class. In particular, we focused our 
study on the classification of the caller's first utter- 
ance in these dialogs. The spoken sentences vary 
widely in duration, with a distribution distinctively 
skewed around a mean value of 5.3 seconds corre- 
sponding to 19 words per utterance. Some examples 
of the first utterances are given below: 
? Yes ma'am where is area code two zero 
one? 
? I'm tryn'a call and I can't get i~ to 
go through I wondered if you could try 
it for me please? 
? Hel lo 
We trained a classifer on the training Set of En- 
glish sentences each of which was annotated with a 
call type. The classifier searches for phrases that are 
strongly associated with one of the call types (Gorin 
et al, 1997) and in the test phase the classifier ex- 
tracts these phrases from the output of the speech 
recognizer and classifies the user utterance. '\]?his 
how the system works when the user speaks English. 
However, if the user does not speak the language 
that the classifier is trained on, English, in our 
case, the system is unusable. We propose to solve 
this problem by translating the user's utterance, 
Japanese, in our case, to English. This extends the 
usability of the system to new user groups. 
An alternate approach could be to retrain the 
classifier on Japanese text. However, this approach 
would result in replicating the system for each pos- 
sible input language, a very expensive proposition 
considering, in general, that the system could have 
sophisticated natural language understanding and 
dialog components which would have to be repli- 
cated also. 
6.1 Experiments and Evaluation 
In this section, we discuss issues concerning evalu- 
ation of the translation system. The data for the 
experiments reported in this section were obtained 
from the customer side of operator-customer con- 
versations, with the customer-caxe application de- 
scribed above and detailed in (Riccardi and Gorin, 
January 2000; Gorin et al, 1997). Each of the cus- 
tomer's utterance transcriptions were then manually 
translated into Japanese. A total of 15,457 English- 
Japanese sentence pairs was split into 12,204 train- 
ing sentence pairs and 3,253 test sentence pairs. 
The objective of this experiment is to measure 
the performance of a translation system in the con- 
text of an application. In an automated call router 
there axe two important performance measures. The 
first is the probability of false rejection, where a 
call is falsely rejected. Since such calls would be 
transferred to a human agent, this corresponds to 
a missed opportunity for automation. The second 
56 
measure is the probability of correct classification. 
Errors in this dimension lead to misinterpretations 
that must be resolved by a dialog manager (Abella 
and Gorin, 1997). 
Using our approach described in the previous 
sections, we have trained a unigram, bigram and 
trigram VNSA based translation models with and 
without phrases. Table 3 shows lexical choice (bag- 
of-tokens) accuracy for these different ranslation 
models measured in terms of recall, precision and 
F-measure. 
In order to measure the effectiveness of our trans- 
lation models for this task we classify Japanese ut- 
terances based on their English translations. Fig- 
ure 4 plots the false rejection rate against the correct 
classification rate of the classifier on the English gen- 
erated by three different Japanese to English trans- 
lation models for the set of Japanese test sentences. 
The figure also shows the performance ofthe classi- 
fier using the correct English text as input. 
There are a few interesting observations to be 
made from the Figure 4. Firstly, the task per- 
formance on the text data is asymptotically simi- 
lar to the task performance on the translation out- 
put. In other words, the system performance is not 
significantly affected by the translation process; a 
Japanese transcription would most often be associ- 
ated with the same call type after translation as if 
the original were English. This result is particu- 
larly interesting inspite of the impoverished reorder- 
ing phase of the target language words. We believe 
that this result is due to the nature of the application 
where the classifier is mostly relying on the existence 
of certain key words and phrases, not necessarily in
any particular order. 
The task performance improved from the 
unigram-based translation model to phrase unigram- 
based translation model corresponding to the im- 
provement in the lexical choice accuracy in Table 3. 
Also, at higher false rejection rates, the task perfor- 
mance is better for trigram-based translation model 
than the phrase trigram-based translation model 
since the precision of lexical choice is better than 
that of the phrase trigram-based model as shown in 
Table 3. This difference narrows at lower false rejec- 
tion rate. 
We are currently working on evaluating the 
translation system in an application independent 
method and developing improved models of reorder- 
ing needed for better translation system. 
7 Conclusion 
We have presented an architecture for speech trans- 
lation in limited domains based on the simple ma- 
chinery of stochastic finite-state transducers. We 
have implemented stochastic finite-state models for 
English-Japanese and Japanese-English translation 
in limited domains. These models have been trained 
automatically from source-target utterance pairs. 
We have evaluated the effectiveness ofsuch a transla- 
tion model in the context of a call-type classification 
task. 
Re ferences  
A. Abella and A. L. Gorin. 1997. Generating se- 
mantically consistent inputs to a dialog man- 
ager. In Proceedings of European Conference on 
Speech Communication and Technology, pages 
1879-1882. 
Steven Abney. 1991. Parsing by chunks. In Robert 
Berwick, Steven Abney, and Carol Tenny, editors, 
Principle-based parsing. Kluwer Academic Pub- 
lishers. 
H.'Alshawi, S. Bangalore, and S. Douglas. 1998a. 
Learning Phrase-based Head Transduction Mod- 
els for Translation of Spoken Utterances. In The 
fifth International Conference on Spoken Lan- 
guage Processing (ICSLP98), Sydney. 
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou- 
glas. 1998b. Automatic acquisition of hierarchi- 
cal transduction models for machine translation. 
In Proceedings of the 36 th Annual Meeting of the 
Association for Computational Linguistics, Mon- 
treal, Canada. 
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mer- 
cer. 1993. The Mathematics of Machine Transla- 
tion: Parameter Estimation. Computational Lin- 
guistics, 16(2):263-312. 
E. Giachin. 1995. Phrase Bigrams for Continuous 
Speech Recognition. In Proceedings of ICASSP, 
pages 225-228, Detroit. 
A. L. Gorin, G. Riccardi, and J. H Wright. 1997. 
How May I Help You? Speech Communication, 
23:113-127. 
R.M. Kaplan and M. Kay. 1994. Regular models 
of phonological rule systems. Computational Lin- 
guistics, 20(3):331-378. 
Kevin Knight and Y. A1-Onaizan. 1998. Transla- 
tion with finite-state devices. In Machine trans- 
lation and the information soup, Langhorne, PA, 
October. 
K. K. KoskenniemL 1984. Two-level morphology: a
general computation model for word-form recogni- 
tion and production. Ph.D. thesis, University of 
Helsinki. 
Alon Lavie, Lori Levin, Monika Woszczyna, Donna 
Gates, Marsal Gavalda, , and Alex Waibel. 
1999. The janus-iii translation system: Speech- 
to-speech translation in multiple domains. In 
Proceedings of CSTAR Workshop, Schwetzingen, 
Germany, September. 
Fernando C.N. Pereira and Michael D. Riley. 1997. 
Speech recognition by composition of weighted 
finite automata. In E. Roche and Schabes Y., 
5"7 
Trans Recall 
VNSA order (R) 
Unigram 24.5 
Bigram 55.3 
Trigram 61.8 
Phrase Unigram 43.7 
Phrase Bigram 62.5 
Phrase Trigram 65.5 
Precision 
(e) 
83.6 
F-Measure 
(2*P*R/(P+R)) 
37.9 
87.3 67.7 
86.4 72.1 
80.3 56.6 
86.3 72.5 
85.5 74.2 
Table 3: Lexical choice accuracy of the Japanese to English Translation System with and without phrases 
100 
ROC curve for English test set 
i 
95 
i i i Trigram ! Phrese-LJnlgram ~ i 
| i iTexl ; ,~'* i ' ~! i ; ~.~ 
i f  i i , -  i i i/ i 
i~  .................. i;; h~ ........................................... i ~ g .." i v i i 
1~ Unlgram 
8 . . . . . . . . . .  ; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8O 
75 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
7O 10 20 30 40 50 60 70 80 90 
False rejection rate (%) 
Figure 4: Plots for the false rejection rate against he correct classification rate of the classifier on the English 
generated by three different Japanese to EnglJish translation models 
editors, Finite State Devices for Natural Lan- 
guage Processing. MIT Press, Cambridge, Mas- 
sachusetts. 
G. Riccardi and S. Bangalore. 1998. Automatic ac- 
quisition of phrase grammars for stochastic lan- 
guage modeling. In Proceedings of A CL Workshop 
on Very Large Corpora, pages 188-196, Montreal. 
G. Riccardi and A.L. Gorin. January, 2000. 
Stochastic Language Adaptation over Time and 
State in Natural Spoken Dialogue Systems. IEEE 
Transactions on Speech and Audio, pages 3--10. 
G. Riccardi, E. Bocchieri, and It. Pieraccini. 1995. 
Non deterministic stochastic language models for 
speech recognition. In Proceedings of ICASSP, 
pages 247-250, Detroit. 
G. Riccardi, R. Pieraccini, and E. Bocchieri. 1996. 
Stochastic Automata for Language Modeling. 
Computer Speech and Language, 10(4):265-293. 
G. Riccardi, A. L. Gorin, A. Ljolje, and M. Riley. 
1997. A spoken language system for automated 
call routing. In Proceedings of ICASSP, pages 
1143-1146, Munich. 
K. Ries, F.D. BuO, and T. Wang. 1995. Improved 
Language Modeling by Unsupervised Acquisition 
of Structure. In Proceedings of ICASSP, pages 
193-196, Detroit. 
Emmanuel Roche. 1999. Finite state transducers: 
parsing free and frozen sentences. In Andr~ Ko- 
rnai, editor, Eztened Finite State Models of Lan- 
guage. Cambridge University Press. 
B. Srinivas. 1997. Complexity of Lexical Descrip- 
tions and its .Relevance to Partial Parsing. Ph.D. 
thesis, University of Pennsylvania, Philadelphia, 
PA, August. 
Verbmobil. 2000. Verbmobil Web page. 
http://verbmobil.dfki.de/. 
J. Vilar, V.M. Jim~nez, J. Amengual, A. Castel- 
lanos, D. Llorens, and E. VidM. 1999. Text 
and speech translation by means of subsequential 
transducers. In Andr~ Kornai, editor, Extened 
Finite State Models of Language. Cambridge Uni- 
versity Press. 
Monika Woszczyna, Matthew Broadhead, Donna 
Gates, Marsal Gavalda, Alon Lavie, Lori Levin, 
58 
and Alex Waibel. 1998. A modular approach to 
spoken language translation for large domains. In 
Proceedings of AMTA-98, Langhorne, Pennsylva- 
nia, October. 
Dekai Wu. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor- 
pora. Computational Linguistics, 23(3):377-404. 
59 
Evaluation Metrics for Generation 
Sr in ivas  Banga lore  and  Owen Rambow and Steve  Whi t taker  
AT&T Labs  - Research  
180 Park  Ave,  PO Box 971 
F lo rham Park ,  N J  07932-0971, USA 
{srini,r~mbow, stevew}@reSearch, art tom 
Abst rac t  
Certain generation applications may profit from the 
use of stochastic methods. In developing stochastic 
methods, it is crucial to be able to quickly assess 
the relative merits of different approaches or mod- 
els. In this paper, we present several types of in- 
trinsic (system internal) metrics which we have used 
for baseline quantitative assessment. This quanti- 
tative assessment should then be augmented to a 
fuller evaluation that examines qualitative aspects. 
To this end, we describe an experiment that tests 
correlation between the quantitative metrics and hu- 
man qualitative judgment. The experiment confirms 
that intrinsic metrics cannot replace human evalu- 
ation, but some correlate significantly with human 
judgments of quality and understandability and can 
be used for evaluation during development. 
1 In t roduct ion  
For many applications in natural language genera- 
tion (NLG), the range of linguistic expressions that 
must be generated is quite restricted, and a gram- 
mar for a surface realization component can be fully 
specified by hand. Moreover, iLL inany cases it is 
very important not to deviate from very specific out- 
put in generation (e.g., maritime weather reports), 
in which case hand-crafted grammars give excellent 
control. In these cases, evaluations of the generator 
that rely on human judgments (Lester and Porter, 
I997) or on human annotation of the test corpora 
(Kukich, 1983) are quite sufficient . . . .  
However. in other NLG applications the variety of 
the output is much larger, and the demands on 
the quality of the output are solnewhat less strin- 
gent. A typical example is NLG in the context of 
(interlingua- or transfer-based) inachine translation. 
Another reason for relaxing the quality of the out- 
put may be that not enough time is available to de- 
velop a full gramnlar for a new target, language in 
NLG. ILL all these cases, stochastic methods provide 
an alternative to hand-crafted approaches to NLG. 
1 
To our knowledge, the first to use stochastic tech- 
niques in an NLG realization module were Langkilde 
and Knight (1998a) and (~998b) (see also (Langk- 
ilde, 2000)). As is the case for stochastic approaches 
in natural anguage understanding, the research and 
development itself requires an effective intrinsic met- 
ric in order to be able to evaluate progress. 
In this paper, we discuss several evaluation metrics 
that we are using during the development of FERGUS 
(Flexible Empiricist/Rationalist Generation Using 
Syntax). FERCUS, a realization module, follows 
Knight and Langkilde's eminal work in using an 
n-gram language model, but we augment it with a 
tree-based stochastic model and a lexicalized syntac- 
tic grammar. The metrics are useful to us as rela- 
tive quantitative assessments of different models we 
experiment with; however, we do not pretend that 
these metrics in themselves have any validity. In- 
stead, we follow work done in dialog systems (Walker 
et al, 1997) and attempt o find metrics which on 
tim one hand can be computed easily but on the 
other hand correlate with empirically verified human 
judgments in qualitative categories such as readabil- 
ity. 
The structure of the paper is as follows. In Section 2, 
we briefly describe the architecture of FEacUS, and 
some of the modules. In Section 3 we present four 
metrics and some results obtained with these met- 
rics. In Section 4 we discuss the for experimental 
validation of the metrics using human judgments, 
and present a new metric based on the results of 
these experiments. In Section 5 we discuss some of 
the 'many problematic issues related to  the use  Of 
metrics and our metrics in particular, and discuss 
on-going work. 
2 Sys tem Overv iew 
FERGUS is composed of three mQdules: .the Tree 
Chooser, tile Unraveler, and the Linear Precedence 
(LP) Chooser (Figure 1). Tile input to the system is 
a dependency tree as shown in Figure 2. t Note that 
the nodes are unordered and are labeled only with 
lexemes, not with any sort of syntactic annotations. 2 
The Tree Chooser uses a stochastic tree model to 
choose syntactic properties (expressed as trees in a 
Tree Adjoining Grammar) for the nodes in the in- 
put structure. This step can be seen as analogous to 
"supertagging" -(Bangalore-und doshh 1:999);. except 
that now supertags (i.e., names of trees which en- 
code the syntactic properties of a lexical head) must 
be found for words in a tree rather than for words 
in a linear sequence. The Tree Chooser makes the 
siinplifying assumptions that the choice of a tree for 
a node depends only on its daughter nodes, thus al- 
lowing for a top-down algorithm. The Tree Chooser 
draws on a tree model, which is a analysis in terms 
of syntactic dependency for 1,000,000 words of the 
Wall Street Journal (WSJ). 3 
The supertagged tree which is output from the Tree 
Chooser still does not fully determine the surface 
string, because there typically are different ways to 
attach a daughter node to her mother (for example, 
an adverb can be placed in different positions with 
respect o its verbal head). The Unraveler therefore 
uses the XTAG grammar of English (XTAG-Group, 
1999) to produce a lattice of all possible lineariza- 
tions that are compatible with the supertagged tree. 
Specifically, the daughter nodes are ordered with re- 
spect to the head at each level of the derivation tree. 
In cases where the XTAG grammar allows a daugh- 
ter node to be attached at more than one place in 
the mother supertag (as is the case in our exam- 
ple for was and for; generaUy, such underspecifica- 
tion occurs with adjuncts and with arguments if their 
syntactic role is not specified), a disjunction of all 
these positions is assigned to the daughter node. A 
bottom-up algorithm then constructs a lattice that 
encodes the strings represented by each level of the 
derivation tree. The lattice at the root of the deriva- 
tion tree is the result of the Unraveler. 
Finally. the LP Chooser chooses the most likely 
traversal of this lattice, given a linear language 
1The sentence generated by this tree is a predicativenoun 
construction. The XTAG grammar analyzes these as being 
headed by the noun,rather-than by.the copula, and we fol- 
low the XTAG analysis. However, it would of course also be 
possible to use a graminar that allows for the copula-headed 
analysis. 
21n the system that we used in the experiments described 
in Section 3. all words (including function words) need to be 
present in the input representation, fully inflected. Further- 
more, there is no indication of syntactic role at all. This is of 
course unrealistic f~r applications see ,Section 5 for further 
renlarks. 
:3This wa~s constructed from the Penn Tree Bank using 
some heuristics, sirice the. l)enn Tree Bank does not contain 
full head-dependerit information; as a result of the tlse of 
heuristics, the Tree Model is tint fully correct. 
2 
I 
TAG Derivation Tree 
without Supertags 
i 
One single semi-specif ied~ 
TAG Deri~tion Trees 
Word Lattice 
i 
\[ cPc.oo=, \] 
l 
String 
Figure 1: Architecture of FERGUS 
estimate 
there was no cost for 
I 
phase 
the second 
Figure 2: Input to FERGUS 
model (n-gram). The lattice output from the Un- 
raveler encodes all possible word sequences permit- 
ted by the supertagged ependency structure. \Ve 
rank these word sequences in the order of their likeN- 
hood by composing the lattice with a finite-state ma- 
chine representing a trigram language model. This 
model has been constructed from the 1.000,0000 
words WSJ training corpus. We pick the best path 
through the lattice resulting from the composition 
using the Viterbi algorithm, and this top ranking 
word sequence is the output of the LP Chooser and 
the generator. 
When we tally the results we obtain the score shown 
in the first column of Table 1. 
Note that if there are insertions and deletions, the 
number of operations may be larger than the number 
of tokens involved for either one of the two strings. 
As a result, the simple string accuracy metric may 
3 Base l ine -Qua_nt i tmt ive ,Met r i cs  ...,:-~.--..~,-:..,be.:..~eg~i~ee (t:hoagk:it, As, nevel:-greater._than 1, of 
We have used four different baseline quantitative 
metrics for evaluating our generator. The first two 
metrics are based entirely on the surface string. The 
next two metrics are based on a syntactic represen- 
tation of the sentence. 
3.1 S t r ing -Based  Met r i cs  
We employ two metrics that measure the accuracy 
of a generated string. The first metric, s imple ac- 
curacy,  is the same string distance metric used for 
measuring speech recognition accuracy. This met- 
ric has also been used to measure accuracy of MT 
systems (Alshawi et al, 1998). It is based on string 
edit distance between the output of the generation 
system and the reference corpus string. Simple ac- 
curacy is the number of insertion (I), deletion (D) 
and substitutions (S) errors between the reference 
strings in the test corpus and the strings produced by 
the generation model. An alignment algorithm us- 
ing substitution, insertion and deletion of tokens as 
operations attempts to match the generated string 
with the reference string. Each of these operations 
is assigned a cost value such that a substitution op- 
eration is cheaper than the combined cost of a dele- 
tion and an insertion operation. The alignment al- 
gorithm attempts to find the set of operations that 
minimizes the cost of aligning the generated string 
to tile reference string. Tile metric is summarized 
in Equation (1). R is the number of tokens in the 
target string. 
course). 
The simple string accuracy metric penalizes a mis- 
placed token twice, as a deletion from its expected 
position and insertion at a different position. This is 
particularly worrisome in our case, since in our eval- 
uation scenario the generated sentence is a permuta- 
tion of the tokens in the reference string. We there- 
fore use a second metric, Generat ion  Str ing Ac- 
curacy,  shown in Equation (3), which treats dele- 
tion of a token at one location in the string and the 
insertion of the same token at another location in 
the string as one single movement error (M). This 
is in addition to the remaining insertions (I ') and 
deletions (D'). 
(3) Generat ion  St r ing  Accuracy  = 
( 1 -- M~-/~.P-~--~-~) 
In our example sentence (2), we see that the inser- 
tion and deletion of no can be collapsed into one 
move. However, the wrong positions of cost and of 
phase are not analyzed as two moves, since one takes 
the place of the other, and these two tokens still re- 
sult in one deletion, one substitution, and one inser- 
tion. 5 Thus, the generation string accuracy depe- 
nalizes simple moves, but still treats complex moves 
(involving more than one token) harshly. Overall, 
the scores for the two metrics introduced so far are 
shown in the first two columns of Table 1. 
3.2 Tree-Based Metr ics  
(1) Simple Str ing Accuracy  = (1 I+*)+s I? ) \Vhile tile string-b~u~ed metrics are very easy to ap- 
ply, they have the disadvantage that they do not 
reflect the intuition that all token moves are not Consider tile fifth)wing example. The target sentence 
is on top, tile generated sentence below. Tile third equally "bad". Consider the subphrase stimate for 
line represents the operation needed to. transfor m .. phase the second of the sentence in (2). \Vhile this is 
one sentence into another: a period is used t.o indi- bad; i t  seems better:tiara rt alternative such as es- 
cate that no operation is needed. 4 
(2) There was no cost estimate for tile 
There was estimate for l)hase tile 
d (1 i 
second phase 
second no cost  
i s 
? I Note that the metric is symmetric, 
timate phase for tile second. Tile difference between 
the two strings is that the first scrambled string, but 
not tile second,  can be read off fl'om tile dependency  
tree for the sentence (as shown ill Figure 2) with- 
out violation of projectivity, i.e., without (roughly 
STiffs shows the importance of the alignment algorithm in 
the definition of Ihese two metrics: had it. not, aligned phase 
and cost as a substitution (but each with an empty position 
in the other~string-:instead),, then ~khe simple string accuracy 
would have 6 errors instead of 5, but the generation string 
accuracy would have 3 errors instead of ,1, 
speaking) creating discontinuous constituents. It 
has long been observed (though informally) that the 
dependency trees of a vast majority of sentences in 
the languages of the world are projective (see e.g. 
(Mel'euk, 1988)), so that a violation of projectivity 
is presumably a more severe rror than a word order 
variation that does not violate projectivity. 
We designed thet ree-based ' -acet t rucymetr i cs  in 
order to account for this effect. Instead of compar- 
ing two strings directly, we relate the two strings 
to a dependency tree of the reference string. For 
each treelet (i.e., non-leaf node with all of its daugh- 
ters) of the reference dependency tree, we construct 
strings of the head and its dependents in the order 
they appear in the reference string, and in the order 
they appear in the result string. We then calculate 
the number of substitutions, deletions, and inser- 
tions as for the simple string accuracy, and the num- 
ber of substitutions, moves, and remaining deletions 
and insertions as for the generation string metrics, 
for all treelets that form the dependency tree. We 
sum these scores, and then use the values obtained 
in the formulas given above for the two string-based 
metrics, yielding the S imple  Tree Accuracy  and 
Generat ion  Tree Accuracy .  The scores for our 
example sentence are shown in the last two columns 
of Table 1. 
3.3 Eva luat ion  Resu l ts  
The simple accuracy, generation accuracy, simple 
tree accuracy and generation tree accuracy for the 
two experiments are tabulated in Table 2. The test 
corpus is a randomly chosen subset of 100 sentences 
from the Section 20 of WSJ. The dependency struc- 
tures for the test sentences were obtained automat- 
ically from converting the Penn TreeBank phrase 
structure trees, in the same way as was done to 
Create the training corpus. The average length of 
the test sentences i 16.7 words with a longest sen- 
tence being 24 words in length. As can be seen, the 
supertag-based model improves over the baseline LR 
model on all four baseline quantitative metrics. 
4 Qua l i ta t ive  Eva luat ion  o f  the  
Quant i ta t ive  Met r i cs  
4.1 The  Exper iments  
We have presented four metrics which we can com- 
pute automatically. In order to determine whether 
the metrics correlate with independent notions un- 
derstandability or quality, we have performed eval- 
uation experiments with human subjects. 
In the web-based experiment, we ask human sub- 
jects to read a short paragraph from the WSJ. We 
present hree or five variants of the last sentence of 
this paragraph on the same page, and ask the sub- 
ject to judge them along two dimensions: 
Here we summarize two experiments that we have 
performed that use different tree nmdels. (For a 
more detailed comparisons of different tree models, 
see (Bangalore and Rainbow, 2000).) 
o For the baseline experiment, we impose a ran- 
dom tree structure for each sentence of the cor- 
pus and build a Tree Model whose parameters 
consist of whether a lexeme ld precedes or fol- 
lows her mother lexeme \[ .... We call this the 
Baseline Left-Right (LR) Model. This model 
generates There was est imate for  phase the sec- 
ond no cost .  for our example input. 
o In the second experiment we use the-system 
as described in Section 2. We employ the 
supertag-based tree model whose parameters 
consist of whether a lexeme ld with supertag 
sd is a dependent of lexeme 1,,, with supertag 
s,,,. Furthermore we use the information pro- 
vided by the XTAG grammar to order the de- 
pendents. This model generates There was no 
cost est imate for" the second phase . for our ex- 
ample input, .which is indeed.the sentence found 
in the WS.I. 
o Unders tandab i l i ty :  How easy is this sentence 
to understand? Options range from "Extremely 
easy" (= 7) to "Just barely possible" (=4) to 
"Impossible" (=1). (Intermediate numeric val- 
ues can also be chosen but have no description 
associated with them.) 
o Qual i ty:  How well-written is this sentence? 
Options range from "Extremely well-written'" 
(= 7) to "Pretty bad" (=4) to "Horrible (=1). 
(Again. intermediate numeric values can also t)e 
chosen, but have no description associated with 
them.) 
The 3-5 variants of each of 6 base sentences are con- 
strutted by us (most of the variants lraxre not actu- 
ally been generated by FERGUS) to sample multiple 
values of each intrinsic metric as well as to contrast 
differences between the intrinsic measures. Thus for 
one sentence "tumble", two of the five variants have 
approximately identical values for each of the met- 
rics but with the absolute values being high (0.9) 
and medium (0.7) respectively. For two other sen- 
\[,('II('(}S ~ve have contrasting intrinsic values for tree 
trod string based measures. For .the final sentence 
we have contrasts between the string measures with 
Metric Simple Generation Simple Generation 
String Accuracy String Accuracy Tree Accuracy Tree Accuracy 
Total number of tokens 9 9 9 9 
Unchanged 
Substitutions 
Insertions 
Deletions 
Moves 
6 
1 
2 
2 
0 
6 
0 
3 
3 
O..  
6 
0 
0 
0 
.3 
Total number of problems 5 4 " 6 3 
Score 0.44 0.56 0.33 0.67 
Table 1: Scores for the sample sentence according to the four metrics 
Tree Model Simple Generation Simple Generation 
String Accuracy String Accuracy Tree Accuracy Tree Accuracy i
Baseline LR Model 0.41 0.56 0.41 0.63 
. . . . .  i 
Supertag-based Model 0.58 0.72 0.65 0.76 I 
Table 2: Performance results 
tree measures being approximately equal. Ten sub- 
jects who were researchers from AT&T carried out 
the experiment. Each subject made a total of 24 
judgments. 
Given the variance between subjects we first nor- 
malized the data. We subtracted the mean score 
for each subject from each observed score and then 
divided this by standard eviation of the scores for 
that subject. As expected our data showed strong 
correlations between ormalized understanding and 
quality judgments for each sentence variant (r(22) = 
0.94, p < 0.0001). 
Our main hypothesis i that the two tree-based met- 
rics correlate better with both understandability and 
quality than the string-based metrics. This was con- 
firmed. Correlations of the two string metrics with 
normalized understanding for each sentence variant 
were not significant (r(22) = 0.08 and rl.2.21 = 0.23, for 
simple accuracy and generation accuracy: for both 
p > 0.05). In contrast both of the tree metrics were 
significant (r(2.2) = 0.51 and r(22) = 0.48: for tree 
accuracy and generation tree accuracy, for both p 
< 0.05). Similar results were achieved--for thegor- 
realized quality metric: (r(.2.21 = 0.16 and r(221 = 
0,33: for simple accuracy and generation accuracy, 
for both p > 0.05), (r(ee) = 0.45 and r(.2.2) = 0.42, 
for tree accuracy and generation tree accuracy, for 
both p < 0.05). 
A second aim of ()Lit" qualitative valuation was to 
lest various models of the relationship between in- 
trinsic variables and qualitative user judgments. \Ve 
proposed a mmlber-of'models:in which various conL- 
from the two tree models 
binations of intrinsic metrics were used to predict 
user judgments of understanding and quality. .We 
conducted a series of linear regressions with nor- 
malized judgments of understanding and quality as 
the dependent measures and as independent mea- 
sures different combinations of one of our four met- 
rics with sentence length, and with the "problem" 
variables that we used to define the string metrics 
(S, I, D, M, I ' ,  D' - see Section 3 for definitions). 
One sentence variant was excluded from the data set, 
on the grounds that the severely "mangled" sentence 
happened to turn out well-formed and with nearly 
the same nleaning as the target sentence. The re- 
sults are shown in Table 3. 
We first tested models using one of our metrics as a 
single intrinsic factor to explain the dependent vari- 
able. We then added the "problem" variables. 6 and 
could boost tile explanatory power while maintain- 
ing significance. In Table 3, we show only some con> 
binations, which show that tile best results were ob- 
tained by combining the simple tree accuracy with 
the number of Substitutions (S) and the sentence 
length. As we can see, the number of substitutions 
..... has an.important effecVon explanatory.power,, while 
that of sentence length is much more modest (but 
more important for quality than for understanding). 
Furthermore, the number of substitutions has more 
explanatory power than the number of moves (and 
in fact. than any of the other "problem" variables). 
The two regressions for understanding and writing 
show very sinlilar results. Normalized understand- 
6None of tile "problem" variables have much explanatory 
power on their own (nor (lid they achieve significance). 
Model User Metric Explanatory Power Statistical Significance 
(R 2) (p value) 
Simple String Accuracy Understanding 0.02 0.571 
Simple String Accuracy Quality 0.00 0.953 
Generation String Accuracy 
Generation String Accuracy 
S imple  T ree  Accuracy . . . . . . .  . - ~,  
Simple Tree Accuracy 
Generation Tree Accuracy 
Generation Tree Accuracy 
Simple Tree Accuracy + S 
Simple Tree Accuracy + S 
Simple Tree Accuracy + M 
Simple Tree Accuracy + M 
Simple Tree Accuracy + Length 
Simple Tree Accuracy + Length 
Simple Tree Accuracy + S + Length 
Simple Tree Accuracy + S + Length 
Understanding 
Quality 
::Unders~aatdiag 
Quality 
Understanding 
Quality 
0.02 
0.05 
: . ,  . . . .  0.36  
0.34 
0.35 
0.35 
0.584 
0.327 
. . ? . . . . . . . . . .  ".0;003.. - . :  
0.003 
0.003 
0.003 
Understanding 0.48 0.001 
Quality 0.47 0.002 
Understanding 0.38 0.008 
Quality 0.34 0.015 
Understanding 0.40 0.006 
Quality 0.42 0.006 
0.51 
0.53 
Understanding 
Quality 
0.003 
0.002 
Table 3: Testing different models of user judgments (S is number of substitutions, M number of moved 
elements) 
ing was best modeled as: 
Normalized understanding = 1.4728*sim- 
ple tree accuracy - 0.1015*substitutions- 
0.0228 * length - 0.2127. 
This model was significant: F(3,1 .9  ) = 6.62, p < 0.005. 
Tile model is plotted in Figure 3. with the data point 
representing the removed outlier at the top of the 
diagram. 
This model is also intuitively plausible. The simple 
tree metric was designed to measure the quality of a 
sentence and it has a positive coefficient. A substitu- 
tion represents a case in the string metrics in which 
not only a word is in the wrong place, but the word 
that should have been in that place is somewhere 
else, Therefore, substitutions, more than moves or 
insertions or deletions, represent grave cases of word 
order anomalies. Thus, it is plausible to penalize 
them separately. (,Note that tile simple tree accuracy 
is bounded by 1, while the number of substitutions i
l/ounded by the length of the sentence. In practice, 
in our sentences S ranges between 0 and 10 with 
a mean of 1,583.) Finally, it is also plausible that 
longer sentem:es are more difficult to understand, so 
that length has a (small) negative coefficient. 
We now turn to model for quality, 
Normalized quality = 1.2134*simple tree 
accuracy- 0.0839*substitutions - 0.0280 * 
length - 0.0689. 
This model was also significant: F(3A9) = 7.23, p < 
0.005. The model is plotted in Figure 4, with the 
data point representing the removed outlier at the 
top of the diagram. The quality model is plausible 
for the same reasons that the understanding model 
is. 
L2 
PP 
j ,1"  
.i / 
. / /  
/ 
/ /  
? . , j -  
-05 O0 05 
I a728"SLmo~eTteeMel~ - 0 I015"S - 0 0228"lerN~h 
Figure 3: Regression for Understanding 
6 
o 
Du~h~ 
- (0  -O5 0.0 05  I 0 
1 4728*S,mpleT~eeMetr ? - 0 I015"S - 0 0228"len~l~h 
Figure 4: Regression for Quality (Well-Formedness) 
4.2 Two New Metr i cs  
A further goal of these experiments was to obtain 
one or two metrics which can be automatically com- 
puted, and which have been shown to significantly 
correlate with relevant human judgments? We use as 
a starting point the two linear models for normalized 
understanding and quality given above, but we make 
two changes. First, we observe that while it is plau- 
sible to model human judgments by penalizing long 
sentences, this seems unmotivated in an accuracy 
metric: we do not want to give a perfectly generated 
longer sentence a lower score than a perfectly gener- 
ated shorter sentence. We therefore use models that 
just use the simple tree accuracy and the number 
of substitutions as independent variables? Second, 
we note that once we have done so, a perfect sen- 
tence gets a score of 0.8689 (for understandability) 
or 0.6639 (for quality). We therefore divide by this 
score to assure that a perfect sentence gets a score 
of 1. (As for the previously introduced metrics, the 
scores may be less than 0.) 
\Ve obtain the following new metrics: 
(4) Unders tandab i l i ty  ? Accuracy  = 
(1.3147*simple tree accuracy 0.1039*sub- 
stitutions - 0.4458) / 0.8689 - 
(5) Qua l i ty  Accuracy  = (1.0192*simple tree ac- 
curacy-  0.0869*substitutions - 0.3553) / 0.6639 
\ \e  reevahtated our system and the baseline model 
using the new metrics, in order to veri(v whether 
the nloro motivated metrics we have developed still 
show that FER(;I:S improves l)erforniance over the 
baseline. This is indeed the  case: the resuhs are 
Slllnm.arized ill Tabh'-t.  
Tree Model Understandability Quality 
Accuracy Accuracy 
Baseline -0.08 -0.12 
Supertag-based 0.44 0.42 
. Table 4: Performance results from the .two tree mod- 
..... els:using the:new metrics . . . . . . .  
5 D iscuss ion  
We have devised the baseline quantitative metrics 
presented in this paper for internal use during re- 
search and development, in order to evaluate dif- 
ferent versions of FERGUS. However, the question 
also arises whether they can be used to compare two 
completely different realization modules. In either 
case, there are two main issues facing the proposed 
corpus-based quantitative valuation: does it gener- 
alize and is it fair? 
The problem in generalization is this: can we use 
this method to evaluate anything other than ver- 
sions of FERGUS which generate sentences from the 
WSJ? We claim that we can indeed use the quan- 
titative evaluation procedure to evaluate most real- 
ization modules generating sentences from any cor- 
pus of unannotated English text. The fact that the 
tree-based metrics require dependency parses of the 
corpus is not a major impediment. Using exist- 
ing syntactic parsers plus ad-hoc postprocessors as 
needed, one can create the input representations to
the generator as well as the syntactic dependency 
trees needed for the tree-based metrics. The fact 
that the parsers introduce errors should not affect 
the way the scores are used, namely as relative scores 
(they have no real value absolutely). Which realiza- 
tion modules can be evaluated? First, it is clear 
that our approach can only evaluate single-sentence 
realization modules which may perform some sen- 
tence planning tasks, but cruciaUy not including sen- 
tence scoping/aggregation. Second, this approach 
:only works for generators whose input representa- 
tion is fairly "syntactic". For example, it may be 
difficult to evaluate in this manner a generator that 
-uses semanzic roles in-its inpntrepresent~ion,  since 
we currently cannot map large corpora of syntac- 
tic parses onto such semantic representations, and 
therefore cannot create the input representation for 
the evaluation. 
The second question is that of fairness of the evalu- 
ation. FE\[,tGt.'S as described in this paper is of lim- 
ited use. since it only chooses word order (and, to a 
certain extent, syntactic structure). Other realiza- 
tion and sentence planning tin{ks-which are needed 
for most applications and which may profit from a 
stochastic model include lexical choice, introduction 
of function words and punctuation, and generation 
of morphology. (See (Langkilde and Knight, 1998a) 
for a relevant discussion. FERGUS currently can per- 
form punctuation and function word insertion, and 
morphology and lexical choice are under develop- 
ment.) The question arises whether our metrics will 
. fairly measure the:quality,~of,a, more comp!ete real~ .... 
ization module (with some sentence planning). Once 
the range of choices that the generation component 
makes expands, one quickly runs into the problem 
that, while the gold standard may be a good way of 
communicating the input structure, there are usu- 
ally other good ways of doing so as well (using other 
words, other syntactic constructions, and so on). 
Our metrics will penalize such variation. However, 
in using stochastic methods one is of course precisely 
interested in learning from a corpus, so that the fact 
that there may be other ways of expressing an input 
is less relevant: the whole point of the stochastic ap- 
proach is precisely to express the input in a manner 
that resembles as much as possible the realizations 
found in the corpus (given its genre, register, id- 
iosyncratic hoices, and so on). Assuming the test 
corpus is representative of the training corpus, we 
can then use our metrics to measure deviance from 
the corpus, whether it be merely in word order or in 
terms of more complex tasks such as lexical choice 
as well. Thus, as long as the goal of the realizer 
is to enmlate as closely as possible a given corpus 
(rather than provide a maximal range of paraphras- 
tic capability), then our approach can be used for 
evaluation, r 
As in the case of machine translation, evaluation in 
generation is a complex issue. (For a discussion, see 
(Mellish and Dale, 1998).) Presumably, the qual- 
ity of most generation systems can only be assessed 
at a system level in a task-oriented setting (rather 
than by taking quantitative measures or by asking 
humans for quality assessments). Such evaluations 
are costly, and they cannot be the basis of work in 
stochastic generation, for which evaluation is a fre- 
quent step in research and development. An advan- 
tage of our approach is that our quantitative metrics 
allow us to evaluate without human intervention, au- 
tomatically and objectively (objectively with respect 
to the defined metric,-that is).- Independently, the 
use of the metrics has been validated using human 
subjects (as discussed in Section 4): once this has 
happened, the researcher can have increased confi- 
dence that choices nlade in research and develop- 
ment based on the quantitative metrics will in fact 
7We could also assume a set of acceptable paraphrases for 
each sentence in the test corpus. Our metrics are run on all 
paraphrases, and the best score chosen. However. for many 
applications it will not be emsy to construct such paraphrase 
sets, be it by hand or automatically. 
8 
correlate with relevant subjective qualitative mea- 
sures. 
References  
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou- 
glas. 1998. Automatic acquisition of hierarchical 
~traalsduatian.:models :for ~machine. tr:anslation, tn 
Proceedings of the 36th Annual Meeting Association 
for Computational Linguistics, Montreal, Canada. 
Srinivas Bangalore and Aravind Joshi. 1999. Su- 
pertagging: An approach to almost parsing. Com- 
putational Linguistics, 25(2). 
Srinivas Bangalore and Owen Rambow. 2000. Ex- 
ploiting a probabilistic hierarchical model for gem 
eration. In Proceedings of the 18th International 
Conference on Computational Linguistics (COLING 
2000), Saarbriicken, Germany. 
Karen Kukich. 1983. Knowledge-Based Report Gen- 
eration: A Knowledge Engineering Approach to Nat- 
ural Language Report Generation. Ph.D. thesis, Uni- 
versity of Pittsuburgh. 
Irene Langkilde and Kevin Knight. 1998a. Gener- 
ation that exploits corpus-based statistical knowl- 
edge. In 36th Meeting of the Association for Com- 
putational Linguistics and 17th International Con- 
ference on Computational Linguistics (COLING- 
ACL'98), pages 704-710, Montreal, Canada. 
Irene Langkilde and Kevin Knight. 1998b. The 
practical value of n-grams in generation. In Proceed- 
ings of the Ninth International Natural Language 
Generation Workshop (INLG'98), Niagara-on-the- 
Lake, Ontario. 
Irene Langkilde. 2000. Forest-based statistical sen- 
tence generation. In 6th Applied Natural Language 
Processing Conference (ANLP'2000), pages 170- 
177, Seattle, WA. 
James C. Lester and Bruce W. Porter. 1997. De- 
veloping and empirically evaluating robust explana- 
tion generators: The KNIGHT experiments. Compu- 
tational Linguistics. 23(1):65-102. 
Igor A. Mel'~uk. 19S8. Dependency Syntax: Theory 
and Practice. State University of New ~%rk Press. 
New York. 
Chris Mellish and Robert Dale. 1998. Evahlation in 
the context of natural language generation. Corn= 
puter Speech and Language, 12:349-373. 
M. A. Walker, D. Litman, C. A. Kamm. and 
A. Abella. 1997. PARADISE: A general framework 
for evahlating spoken dialogue agents. In Proceed- 
ings of the 35th Annual Meeting of the Association 
of Computational Linguistics, A CL/EA CL 97. pages 
271-280. 
The XTAG-Group. 1999. A lexicalized Tree Adjoin- 
ing Gralnmar for English. Technical report, Insti- 
- tu;te for 1Research in Cognitive Science, University of 
Pennsylvania. 
 
		 
	Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 96?102,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Three models for discriminative machine translation using
Global Lexical Selection and Sentence Reconstruction
Sriram Venkatapathy
Language Technologies Research
Centre, IIIT-Hyderabad
Hyderabad - 500019, India.
sriram@research.iiit.ac.in
Srinivas Bangalore
AT&T Labs - Research
Florham Park, NJ 07932
USA
srini@research.att.com
Abstract
Machine translation of a source language
sentence involves selecting appropriate
target language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this
paper, we present a novel approach to lex-
ical selection where the target words are
associated with the entire source sentence
(global) without the need for local asso-
ciations. This technique is used by three
models (Bag?of?words model, sequential
model and hierarchical model) which pre-
dict the target language words given a
source sentence and then order the words
appropriately. We show that a hierarchi-
cal model performs best when compared
to the other two models.
1 Introduction
The problem of machine translation can be viewed
as consisting of two subproblems: (a) lexical se-
lection, where appropriate target language lexi-
cal items are chosen for each source language
lexical item and (b) lexical reordering, where
the chosen target language lexical items are rear-
ranged to produce a meaningful target language
string. Most of the previous work on statisti-
cal machine translation, as exemplified in (Brown
et al, 1993), employs word?alignment algorithm
(such as GIZA++ (Och et al, 1999)) that provides
local associations between source words and target
words. The source?to?target word?alignments are
sometimes augmented with target?to?source word
alignments in order to improve the precision of
these local associations. Further, the word?level
alignments are extended to phrase?level align-
ments in order to increase the extent of local asso-
ciations. The phrasal associations compile some
amount of (local) lexical reordering of the target
words?those permitted by the size of the phrase.
Most of the state?of?the?art machine translation
systems use these phrase?level associations in
conjunction with a target language model to pro-
duce the target sentence. There is relatively little
emphasis on (global) lexical reordering other than
the local re-orderings permitted within the phrasal
alignments. A few exceptions are the hierarchical
(possibly syntax?based) transduction models (Wu,
1997; Alshawi et al, 1998; Yamada and Knight,
2001; Chiang, 2005) and the string transduction
models (Kanthak et al, 2005).
In this paper, we present three models for doing
discriminative machine translation using global
lexical selection and lexical reordering.
1. Bag?of?Words model : Given a source sen-
tence, each of the target words are chosen by
looking at the entire source sentence. The
target language words are then permuted in
various ways and then, the best permutation
is chosen using the language model on the
target side. The size of the search space of
these permutations can be set by a parameter
called the permutation window. This model
does not allow long distance re-orderings of
target words unless a very large permutation
window chosen which is very expensive.
2. Sequential Lexical Choice model : Given
a source sentence, the target words are pre-
dicted in an order which is faithful to the or-
96
der of words in the source sentence. Now,
the number of permutations that need to be
examined to obtain the best target language
strings are much less when compared to the
Bag?of?Words model. This model is ex-
pected to give good results for language pairs
such as English?French for which only lo-
cal word order variations exist between sen-
tences.
3. Hierarchical lexical association and re-
ordering model : For language pairs such
as English?Hindi or English?Japanese where
there is a high degree of global reordering
(Figure 1), it is necessary to be able to handle
long distance movement of words/phrases.
In this approach, the target words predicted
through global lexical selection are associ-
ated with various nodes of the source depen-
dency tree and then, hierarchical reordering is
done to obtain the order of words in the tar-
get sentence. Hierarchical reordering allows
phrases to distort to longer distances than the
previous two models.
Figure 1: Sample distortion between En-
glish?Hindi
The outline of the paper is as follows. In Section
2, we talk about the global lexical selection. Sec-
tion 3 describes three models for global lexical se-
lection and reordering. In Section 4, we report the
results of the translation models on English?Hindi
language pair and contrast the strengths and limi-
tations of the models.
2 Global lexical selection
For global lexical selection, in contrast to the
local approaches of associating target words to
the source words, the target words are associated
to the entire source sentence. The intuition is
that there may be lexico?syntactic features of the
source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might
be difficult to exactly associate a target word to
a source sentence in many situations - (a) when
translations are not exact but paraphrases (b) the
target language does not have one lexical item
to express the same concept that is expressed in
the source word. The extensions of word align-
ments to phrasal alignments attempt to address
some of these situations in additional to alleviat-
ing the noise in word?level alignments.
As a consequence of the global lexical selection
approach, we no longer have a tight association
between source language words/phrases and tar-
get language words/phrases. The result of lexical
selection is simply a bag of words(phrases) in the
target language and the target sentence has to be
reconstructed using this bag of words.
The target words in the bag, however, might
be enhanced with rich syntactic information that
could aid in the reconstruction of the target sen-
tence. This approach to lexical selection and
sentence reconstruction has the potential to cir-
cumvent the limitations of word?alignment based
methods for translation between significantly dif-
ferent word order languages. However, in this pa-
per, to handle large word order variations, we asso-
ciate the target words with source language depen-
dency structures to enable long distance reorder-
ing.
3 Training the discriminative models for
lexical selection and reordering
In this section, we present our approach for a
global lexical selection model which is based on
discriminatively trained classification techniques.
Discriminant modeling techniques have become
the dominant method for resolving ambiguity in
speech and natural language processing tasks, out-
performing generative models for the same task.
We expect the discriminatively trained global lex-
ical selection models to outperform generatively
trained local lexical selection models as well as
provide a framework for incorporating rich mor-
pho?syntactic information.
Statistical machine translation can be formu-
lated as a search for the best target sequence that
maximizes P (T | S), where S is the source sen-
tence and T is the target sentence. Ideally, P (T |
S) should be estimated directly to maximize the
conditional likelihood on the training data (dis-
criminant model). However, T corresponds to
a sequence with a exponentially large combina-
tion of possible labels, and traditional classifica-
tion approaches cannot be used directly. Although
97
Conditional Random Fields (CRF) (Lafferty et al,
2001) train an exponential model at the sequence
level, in translation tasks such as ours the compu-
tational requirements of training such models are
prohibitively expensive.
3.1 Bag-of-Words Lexical Choice Model
This model doesn?t require the sentences to be
word aligned in order to learn the local associa-
tions. Instead, we take the sentence aligned cor-
pus as before but we treat the target sentence as a
bag?of?words or BOW assigned to the source sen-
tence. The goal is, given a source sentence S, to
estimate the probability that we find a given word
(tj) in its translation ie.., we need to estimate the
probabilities P (true|tj , S) and P (false|tj, S).
To train such a model, we need to build binary
classifiers for all the words in the target lan-
guage vocabulary. The probability distributions
of these binary classifiers are learnt using maxi-
mum entropy model (Berger et al, 1996; Haffner,
2006). For the word tj , the training sentence
pairs are considered as positive examples where
the word appears in the target, and negative other-
wise. Thus, the number of training examples for
each binary classifier equals the number of train-
ing examples. In this model, classifiers are train-
ing using n?gram features (BOgrams(S)).
During decoding, instead of producing the tar-
get sentence directly, what we initially obtain is
the target bag of words. Each word in the target
vocabulary is detected independently, so we have
here a very simple use of binary static classifiers.
Given a sentence S, the bag of words (BOW (T )
contains those words whose distributions have the
positive probability greater than a threshold (? ).
BOW (T ) = {t | P (true | t, BOgrams(S)) > ?}
(1)
In order to reconstruct the proper order of words
in the target sentence, we consider various permu-
tations of words in BOW (T ) and weight them by
a target language model. Considering all possible
permutations of the words in the target sentence
is computationally not feasible. But, the number
of permutations examined can be reduced by us-
ing heuristic forward pruning or by constraining
the permutations to be within a local window of
adjustable size (also see (Kanthak et al, 2005)).
We have chosen to constrain permutations here.
Constraining the permutation using a local win-
dow can provide us some very useful local re-
orderings.
The bag?of?words approach can also be modi-
fied to allow for length adjustments of target sen-
tences, if we add optional deletions in the final
step of permutation decoding. The parameter ?
and an additional word deletion penalty ? can then
be used to adjust the length of translated outputs.
3.2 Sequential Lexical Choice Model
The previous approach gives us a predetermined
order of words initially which are then permuted to
obtain the best target string. Given that we would
not be able to search the entire space, it would be a
helpful if we could start searching various permu-
tations using a more definite string. One such def-
inite order in which the target words can be placed
is the order of source words itself. In this model,
during the lexical selection, we try to place the
target words in an order which is faithful to the
source sentence.
This model associates sets of target words with
every position in the source sentence and yet re-
tains the power of global lexical selection. For
every position (i) of the source sentence, a prefix
string is formed which consists of the sequence of
words from positions 1 to i. Each of these prefix
strings are used to predict bags of target words us-
ing the global lexical selection. Now, these bags
generated using the prefix strings are processed in
the order of source positions. Let Ti be the bag of
target words generated by prefix string i (Figure
2).
T (i+1)
T (i)
i i+1
Figure 2: The generation of target bags associated
with source sentence position
The goal is to associate a set of target words
with every source position. A target word t
is attached to the ith source position if it is
present in Ti but not in Ti?1 and the probability
P (true|t, Ti) > ? . The intuition behind this ap-
proach is that a word t is associated with a position
i if there was some information present at the ith
source position that triggered the probability of the
t to exceed the threshold ? .
98
Hence, the initial target string is the sequence
of target language words associated with the se-
quence of source language positions. This string
is now permuted in all possible ways (section 3.1)
and the best target string is chosen using the lan-
guage model.
3.3 Hierarchical lexical association and
reordering model
The Sequential Lexical Choice Model presented in
the last section is expected to work best for lan-
guage pairs for which there are mostly local word
order variations. For language pairs with signifi-
cant word order variation, the search for the target
string may still fail examine the best target lan-
guage string given the source sentence. The model
proposed in this section should be able to handle
such long distance movement of words/phrases.
In this model, the goal is to search for the best
target string T which maximizes the probability
P (T |S,D(S)), where S is the source sentence
and D(S) is the dependency structure associated
with the source sentence S. The probabilities of
the target words given the source sentence are
estimated in the same way as the bag?of?words
model. The only main difference during the esti-
mation stage is that we consider the dependency
tree based features apart from the n-gram features.
The decoding of the source sentence S takes
place in three steps,
1. Predict the bag?of?words : Given a source
sentence S, predict the bag of words BOW(T)
whose distributions have a positive probabil-
ities greater than a threshold (? ).
2. Attachment to Source nodes : These target
words are now attached to the nodes of source
dependency trees. For making the attach-
ments, the probability distributions of target
words conditioned on features local to the
source nodes are used.
3. Ordering the target language words : Tra-
verse the source dependency tree in a bottom-
up fashion to obtain the best target string.
3.3.1 Predict the bag?of?words
Given a source sentence S, all the target words
whose positive probability distributions are above
? are included in the bag.
BOW (T ) = {t | P (true|t, f(S))} (2)
In addition to the n?gram features, this model uses
cues provided by the dependency structure to pre-
dict the target bag?of?words.
S1
S2
S3 S4
S5
Figure 3: Dependency tree of a source sentence
with words s1, s2, s3, s4 and s5
Hence, the features that we have considered in
the model are (Figure 3),
1. N-grams. For example, in Figure 2, ?s1?, ?s2
s3 s4?, ?s4 s5? etc.
2. Dependency pair (The pair of nodes and its
parents). Example in Figure 2., ?s2 s1?, ?s4
s2? etc.
3. Dependency treelet (The triplet of a node, it?s
parent and sibling). For example, ?s3 s2 s4?,
?s2 s1 s5? etc.
3.3.2 Attachment to Source nodes
For every target word tj in the bag, the most
likely source nodes are determined by measuring
the positive distribution of the word tj given the
features of the particular node (Figure 4). Let
S(tj) denote the set of source nodes to which the
word tj can be attached to, then S(tj) is deter-
mined as,
S1
S2
S3 S4
S5
T1          T2        T3          T4
Figure 4: Dependency tree of a source sentence
with words S1, S2, S3, S4 and S5
S(tj) = argmaxs(P (true|tj , f(s)) (3)
where f(s) denotes the features of S in which
only those features are active which contain the
99
lexical item representing the node s. The target
words are in the global bag are processed in the
order of their global probabilities p(t|S). While
attaching the target words, it is ensured that no
source node had more than ? target words attached
to it. Also, a target word should not be attached
to more to more than ? number of times. There
is another constraint that can be applied to ensure
that the ratio of the total target words (which are
attached to source nodes) to the total number of
words in the source sentence does exceed a value
(?).
3.4 Ordering the target language words
In this step, the source sentence dependency tree is
traversed in a bottom?up fashion. At every node,
the best possible order of target words associated
with the sub-tree rooted at the node is determined.
This string is then used as a cohesive unit by the
superior nodes.
S1
S2
S3 S4
S5
t1 t2 t3 t4 t5
t6 t7t1 t2 t3 t7 t4 t5 t6
Figure 5: The target string associated with node
S1 is determined by permuting strings attached to
the children (in rectangular boxes, to signify that
they are frozen) and the lexical items attached to
S1
For example, in Figure 5, let ?t1 t2 t3?, ?t4 t5?
be the best strings associated with the children of
nodes s2 and s3 respectively. Let t6 and t7 be the
words that are attached to node s1. The best string
for the node s1 is determined by permuting the
strings ?t1 t2 t3?, ?t4 t5?, ?t6? ?t7? in all possible
ways and then choosing the best string using the
language model.
4 Dataset
The language pair that we considered for our ex-
periments are English?Hindi. The training set
consists of 37967 sentence pairs, the development
set contains 819 sentence pairs and the test set
has 699 sentence pairs. The dataset is from the
newspaper domain with topics ranging from pol-
itics to tourism. The sentence pairs have a maxi-
mum source sentence length of 30 words. The av-
erage length of English sentences is 18 while that
of Hindi sentences is 20.
The source language vocabulary is 41017 and
target sentence vocabulary is 48576. The to-
ken/type ratio of English in the dataset is 16.70
and that of Hindi is 15.64. This dataset is rela-
tively sparse. So, the translation accuracies on this
dataset would be relatively less when compared to
those on much larger datasets. In the target side
of the development corpus, the percentage of un-
seen tokens is 13.48%(3.87% types) while in the
source side, the percentage of unseen tokens is
10.77%(3.20% types). On furthur inspection of
a small portion of the dataset, we found that the
maximum percentage of the unseen words on the
target side are the named entities.
5 Results
5.1 Bag-of-Words model
The quality of the bag?of?words obtained is gov-
erned by the parameter ? (probability threshold).
To determine the best ? value, we experiment with
various values of ? and measure the lexical accu-
racies (F-score) of the bags generated on the de-
velopment set (See Figure 6). The total number
of features used for training this model are 53166
(with count-cutoff of 2).
Figure 6: Lexical Accuracies of the Bags-of-
words
Now, we order the bags of words obtained
through global selection to get the target lan-
guage strings. While reordering using the lan-
guage model, some of the noisy words from the
bag can be deleted by setting a deletion cost (?).
We experimented with various deletion costs, and
tuned it according to the best BLEU score that we
100
obtained on the development set. Figure 7 shows
the best BLEU scores obtained by reordering the
bags associated with various threshold values.
Figure 7: Lexical Accuracies of the Bags-of-
words
We can see that we obtained the best BLEU
when we choose a threshold of 0.17 to obtain the
bag?of?words, when the deletion cost is set to 19.
The reference target strings of the development
set has 15986 tokens. So, while tuning the param-
eters, we should ensure that the bags (obtained us-
ing the global lexical selection) that we consider
have more tokens than 15986 to allow some dele-
tions during reordering, and in effect obtain the
target strings whose total token count is approx-
imately equal to 15986. Figure 8 shows the varia-
tion in BLEU scores for various deletion costs by
fixing the threshold at 0.17.
Figure 8: BLEU scores for various deletion costs
when the threshold for global lexical selection is
set to 0.17
On the test set, we now fix the threshold at 0.17
(? ) and the deletion cost (?) at 19 to obtain the
target language strings. The BLEU score that we
obtained for this set is 0.0428.
5.2 Sequential Lexical Choice Model
The lexical accuracy values of the sequence of
words obtained by the sequential lexical choice
model are comparable to those obtained using the
bag?of?words model. The real difference comes
for the BLEU score. The best BLEU score ob-
tained on the development set was 0.0586 when ?
was set to 0.14 and deletion cost was 15. On the
test set, the BLEU score obtained was 0.0473.
5.3 Tree based model
The lexical accuracy values of the words obtained
in this model are comparable to the lexical accu-
racy values of the bag of words model. The total
number of features used for training this model are
118839 (with count-cutoff of 2). On the develop-
ment set, we obtained a BLEU score of 0.0650 for
? set at 0.17 and the deletion cost set at 20. On
the test set, we obtained a BLEU score of 0.0498.
We can see that the BLEU scores are now bet-
ter than the ones obtained using any of the other
models discussed before. This is because the Tree
based model has both the strengths of the global
lexical selection that ensures high quality lexical
items in the target sentences and that of an efficient
reconstruction model which takes care of long dis-
tance reordering. The table summarizes the BLEU
scores obtained by the three models on the devel-
opment and test sets.
Devel. Set Test. Set
Bag-of-Words 0.0545 0.0428
Sequential 0.0586 0.0473
Hierarchical 0.0650 0.0498
Table 1: Summary of the results
6 Conclusion
In this paper, we present a novel approach to lex-
ical selection where the target words are associ-
ated with the entire source sentence (global) with-
out the need for local associations. This technique
is used by three models (Bag?of?words model, se-
quential model and hierarchical model) which pre-
dict the target language words given a source sen-
tence and then order the words appropriately. We
show that a hierarchical model performs best when
compared to the other two models. The hierar-
chical model presented in this paper has both the
strengths of the global lexical selection and effi-
cient reconstruction model.
101
In the future, we are planning to improve the hi-
erarchical model by making two primary additions
? Handling cases of structural non-
isomorphism between source and target
sentences.
? Obtaining K-best target string per node of the
source dependency tree instead of just one
per node. This would allow us to explore
more possibilities without having to compro-
mise much on computational complexity.
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 1998. Automatic acquisition of hierarchical
transduction models for machine translation. In Pro-
ceedings of the 36th Annual Meeting Association for
Computational Linguistics, Montreal, Canada.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vin-
cent. 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39?71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer.
1993. The Mathematics of Machine Translation:
Parameter Estimation. Computational Linguistics,
16(2):263?312.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
263?270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239?261.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 167?174, Ann Arbor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, San Francisco, CA.
Franz Och, Christoph Tillmann, and Herman Ney.
1999. Improved alignment models for statistical
machine translation. In In Proc. of the Joint Conf. of
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20?28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377?404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of 39th
ACL.
102
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 974?983, Dublin, Ireland, August 23-29 2014.
A Framework for Translating SMS Messages
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore, Ron Shacham
AT&T Labs
1 AT&T Way, Bedminster, NJ 07921
vkumar,jchen,srini,rshacham@research.att.com
Abstract
Short Messaging Service (SMS) has become a popular form of communication. While it is
predominantly used for monolingual communication, it can be extremely useful for facilitating
cross-lingual communication through statistical machine translation. In this work we present an
application of statistical machine translation to SMS messages. We decouple the SMS transla-
tion task into normalization followed by translation so that one can exploit existing bitext re-
sources and present a novel unsupervised normalization approach using distributed representa-
tion of words learned through neural networks. We describe several surrogate data that are good
approximations to real SMS data feeds and use a hybrid translation approach using finite-state
transducers. Both objective and subjective evaluation indicate that our approach is highly suitable
for translating SMS messages.
1 Introduction
The preferred form of communication has been changing over time with advances in communication
technology. The majority of the world?s population now owns a mobile device and an ever increasing
fraction of users are resorting to Short Message Service (SMS) as the primary form of communication.
SMS offers an easy, convenient and condensed form of communication that is being embraced by
the younger demographic. Due to the inherent limit in the length of a message that can be transmitted,
SMS users have adopted several shorthand notations to compress the message; some that have become
standardized and many that are invented constantly. While SMS is predominantly used in a monolingual
mode, it has the potential to connect people speaking different languages. However, translating SMS
messages has several challenges ranging from the procurement of data in this domain to dealing with
noisy text (abbreviations, spelling errors, lack of punctuation, etc.) that is typically detrimental to trans-
lation quality. In this work we address all the elements involved in building a cross-lingual SMS service
that spans data acquisition, normalization, translation modeling, messaging infrastructure and user trial.
The rest of the paper is organized as follows. In Section 4, we present a variety of channels through
which we compiled SMS data followed by a description of our pipeline in Section 5 that includes nor-
malization, phrase segmentation and machine translation. Finally, we describe a SMS translation service
built using our pipeline in Section 6 along with results from a user trial. We provide some discussion in
Section 7 and conclude in Section 8.
2 Related Work
One of the main challenges of building a machine translation system for SMS messages is the lack of
training data in this domain. Typically, there are several legal restrictions in using consumer SMS data
that precludes one from either using it completely or forces one to use it in limited capacity. Only a
handful of such corpora are publicly available on the Web (Chen and Kan, 2013; Fairon and Paumier,
2006; Treurniet et al., 2012; Sanders, 2012; Tagg, 2009); they are limited in size and restricted to a few
language pairs.
The NUS SMS corpus (Chen and Kan, 2013) is probably the largest English SMS corpus consisting of
around 41000 messages. However, these messages are characteristic of Singaporean chat lingo and not
an accurate reflection of SMS style in other parts of the world. A corpus of 30000 French SMS messages
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
974
was collected in (Fairon and Paumier, 2006) to study the idiosyncrasies of SMS language in comparison
with standard French. More recently, (Pennell and Liu, 2011) have used twitter data as a surrogate
for SMS messages. Most of these previous efforts have focused on normalization, i.e., translation of
SMS text to canonical text while we are interested in translating SMS messages from one language into
another (Eidelman et al., 2011).
Several works have addressed the problem of normalizing SMS text. A majority of these works have
used statistical machine translation (character-level) to translate SMS text into standard text (Pennell and
Liu, 2011; Aw et al., 2009; Kobus et al., 2008). (Beaufort et al., 2010) used a finite-state framework
to learn the mapping between SMS and canonical form. A beam search decoder for normalizing social
media text was presented in (Wang and Tou Ng, 2013). All these approaches rely on supervised train-
ing data to train the normalization model. In contrast, we use an unsupervised approach to learn the
normalization lexicon of word forms in SMS to standard text.
While several works have addressed the problem of normalizing SMS using machine translation, there
has been little to no work on the translation of SMS messages across languages on a large scale. Machine
translation of instant messages from English-to-Spanish was proposed in (Bangalore et al., 2002) where
multiple translation hypotheses from several off-the-shelf translation engines were combined using con-
sensus decoding. However, the approach did not consider any specific strategies for normalization and
the fidelity of training bitext is questionable since it was obtained using automatic machine translation.
Several products that enable multilingual communication with the aid of machine translation in con-
ventional chat, email, etc., are available in the market. However, most of these models are trained on
relatively clean bitext.
3 Problem Formulation
The objective in SMS translation is to translate a foreign sentence f
sms
= f
sms
1
, ? ? ? , f
sms
J
into target
(English) sentence e = e
I
1
= e
1
, ? ? ? , e
I
. In general it is hard to procure such SMS bitext due to lack
of data and high cost of annotation. However, we typically have access to bitext in non-SMS domain.
Let f = f
1
, ? ? ? , f
J
be the normalized version of the SMS input sentence. Given f
sms
, we choose the
sentence with highest probability among all possible target sentences,
?
e(f
sms
) = argmax
e
{P(e|f
sms
)} (1)
P (e|f
sms
) ? P (e)
?
f
P (f
sms
, f |e) (2)
= P (e)
?
f
P (f
sms
|f , e)P (f |e) (3)
If one applies the max-sum approximation and assumes that P (f
sms
|f , e) is independent of e,
?
e(f
sms
) = argmax
e
P (f
?
|e)P (e) (4)
where f
?
= argmax
f
P (f
sms
|f). Hence, the SMS translation problem can be decoupled into normal-
ization followed by statistical machine translation
1
.
4 Data
Typically, one has access to a large corpus of general bitext {f , e} while data from the SMS domain
{f
sms
, e} is sparse. Compiling a large corpus of SMS messages is not straightforward as there are
several restrictions on the use of consumer SMS data. We are not aware of any large monolingual or
bilingual corpus of true SMS messages besides those mentioned in Section 2. To compile a corpus of
SMS messages, we used three sources of data: transcriptions of speech-based SMS collected through
1
One can also use a lattice output from the normalization to jointly optimize over e and f
975
smartphones, data collected through Amazon Mechanical Turk
2
and Twitter
3
as a surrogate for SMS-
like messages. We describe the composition of each of these data sources in the following subsections.
Corpus Message #count Corpus Message #count
i love you 988157 ily2
hello 881635 n a meeting
hi 607536 Amazon Mechanical Turk check facebook N/A
how are you 470999 kewl
Speech SMS what?s up 251044 call u n a few
what are you doing 218289 lol 472556
where are you 191912 Twitter haha 232428
call 191430 lmao 102018
lol 105618 omg 709504
how?s it going 102977 thanks for the rt 300254
Table 1: Examples of English messages collected from various sources in this work
4.1 Speech-based SMS
In the absence of access to a real feed of SMS messages, we used transcription of speech-based SMS
messages collected through a smartphone application. A majority of these messages were collected
while the users used the application in their cars. We had access to a total of 41.3 million English and
2.4 million Spanish automatic transcriptions. To avoid the use of erroneous transcripts, we sorted the
messages by frequency and manually translated the top 40,000 English and 10,000 Spanish messages,
respectively. Our final English-Spanish bitext corpus from this source of data consisted of 50,000 parallel
sentences. Table 1 shows the high frequency messages in this dataset.
4.2 Amazon Mechanical Turk
The SMS messages from speech-based interaction does not consist of any shorthands or orthographic
errors as the decoding vocabulary of the automatic speech recognizer is fixed. We posted a task on
Amazon Mechanical Turk, where we took the speech-based SMS messages and asked the turkers to enter
three responses to each message as they would on a smartphone. We iteratively posted the responses from
the turkers as messages to obtain more messages. We obtained a total of 1000 messages in English and
Spanish, respectively. Unlike the speech data, the responses contained several shorthands.
4.3 Twitter
Twitter is used by a large number of users for broadcasting messages, opinions, etc. The language used in
Twitter is similar to SMS and contains plenty of shorthands, spelling errors even though it is typically not
directed towards another individual. We compiled a data set of Twitter messages that we subsequently
translated to obtain a bilingual corpus. We used the Twitter4j API
4
to stream Twitter data for a set of
keywords (function words) over a week. The raw data consisted of roughly 106 million tweets. Subse-
quently, we performed some basic normalization (removal of @user, #tags, filtering advertisements, web
addresses) to obtain SMS-like tweets. Finally, we sorted the data by frequency and picked the top 10000
tweets. Eliminating the tweets present in either of the two previous sources resulted in 6790 messages
that we manually translated.
5 Framework
The user input is first stripped of any accents (Spanish), segmented into short chunks using an automatic
punctuation classifier. Subsequently, any shorthand in the message is expanded out using expansion dic-
tionaries (constructed manually and automatically) and finally translated using a phrase-based translation
2
https://www.mturk.com
3
https://twitter.com
4
http://twitter4j.org/en/
976
model. Our framework allows the use of confusion networks in case of ambiguous shorthand expansions.
We describe each component of the pipeline in detail in the following sections.
5.1 Tokenization
Our initial analysis of SMS messages from users, especially in Spanish indicated that while some users
use accented characters in orthography, several others omit it for the sake of faster responses and con-
venience. Hence, we decided to train all our models on unaccented characters. Given a message, we
convert all accented characters to their corresponding unaccented forms, e.g., ba?no? bano, followed by
lowercasing of all characters. We do not perform any other kind of tokenization.
5.2 Unsupervised SMS Normalization
In Section 5.2, we described a static lookup table for expanding abbreviations and shorthands typically
encountered in SMS messages, e.g., 4ever?forever. While a static lookup table provides a reasonable
way of handling common SMS abbreviations, it has limited coverage. In order to build a larger nor-
malization lexicon, we used distributed representation of words to induce the lexicon in an unsupervised
manner. Distributed word representations (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al.,
2010) induced through deep neural networks have been shown to be useful in several natural language
processing applications. We use the notion of distributional similarity that is automatically induced
through the word representations for learning automatic normalization lexicons.
Canonical form Noisy form
love loveeee, loveeeee, looove, love, wuv, wove, love, laffff, love, wuvvv, luhhhh, love, luvvv, luv
starbucks starbs, sbucks
once oncee, 1ce
tomorrow tmrw, tomorrow, 2moro, tmrrw, tomarrow, tomoro, tomoz, 2mrw, tmr, tm, tmwr, 2mm, tmw, 2morro
forever foreva, 5ever, foreverrrr, forver, foreeverrr, 4ever, 5eva, 4eva, foreevaa, forevs, foreve
because cause, cos, coz, ?cos, ?cause, bc, because, becuz, bcuz, cuz, bcus, bcoz, because
homework hwk, hw, hmwk, hmwrk, hmw, homeworkk, homwork, hmk, honework, homeowork
igualmente igualmentee, igualment, iwalmente
siempre simpre, siempre, 100pre, siempre, ciempre, siempre, siiempre, siemore, siempr, siemre, siempe
adios adi, a10, adio
contigo contigoo, cntigo, conmigo, contigoooo, kontigo, conmigoo, conmiqo
demasiado demaciado, demasido, demasiademente, demasiao
Table 2: Examples from the unsupervised normalization lexicon induced through deep learning
We started with the 106 million tweets described in Section 4.3 and used a deep neural network iden-
tical to that used in (Collobert and Weston, 2008), i.e., the network consisted of a lookup table, hidden
layer with 100 nodes and a linear layer with one output. However, we used a context of 5 words and
corrupted the centre word instead of the last word to learn the distributed representations. We performed
stochastic gradient minimization over 1000 epochs on the twitter data. Subsequently, we took the En-
glish and Spanish vocabularies in our translation model and found the 50 nearest neighbors using cosine
distance for each word. We trained the above representations using the Torch toolkit (Collobert et al.,
2011).
Feature English Spanish
dimension Precision Recall Precision Recall
100 70.4 97.4 69.8 97.3
200 72.2 97.5 79.2 100
300 70.4 97.4 71.6 100
Table 3: Performance of the unsupervised normalization procedure. Only 1-best for each word was
considered.
Once we obtained the 50 nearest neighbors for each word in the clean vocabulary, we used a com-
bination of cosine metric threshold and Levenshtein distance (weighted equally) between the consonant
977
skeleton of the strings to construct the mapping lexicon. Finally, we inverted the table to obtain a nor-
malization lexicon. Our procedure currently finds only one-to-one mappings. We took 60 singleton
entries from the static normalization tables reported in Section 5.2 and evaluated the performance of our
approach. The results are shown in Table 3 and some examples of learned normalizations are shown in
Table 2.
5.3 Phrase Segmentation
In many SMS messages, multiple clauses may be concatenated without explicit punctuation. For exam-
ple, the message hi babe hope you?re well sorry i missed your call needs to be interpreted as hi babe.
hope you?re well. sorry, i missed your call. We perform phrase segmentation using an automatic punc-
tuation classifier trained on SMS messages with punctuation. The classifier learns how to detect end of
sentence markers, i.e. periods, as well as commas in the input stream of unpunctuated words.
An English punctuation classifier and a Spanish punctuation classifier was trained. The former was
trained on two million words of smartphone data described in Section 4.1 while the latter was trained
on 223,000 words of Spanish subtitles from the OpenSubtitles
5
corpus. From each of these data sets, a
maximum entropy classifier was trained. Both classifiers utilized both unigram word and part of speech
(POS) features of a window size of two words around the target word to be classified. A POS tagger
trained on the English Penn Treebank provided English POS tags. Likewise, a Spanish POS tagger
provided Spanish POS tags. The training data for the Spanish tagger, 1.6 million words in size, was
obtained by running the Spanish Freeling parser over the Spanish version of TED talk transcripts. Results
are shown in Table 4. Both phrase segmenters detect end of sentence well. The Spanish phrase segmenter
detects commas better than the English one. This might be due to differences in the training sets; commas
appear about 20 times more often in the Spanish data than in the English data.
Class Precision Recall F-measure
English period 89.7 90.9 90.3
comma 61.1 10.9 18.5
Spanish period 94.3 87.4 90.7
comma 74.2 37.4 49.7
Table 4: Performance of automatic phrase segmentation (numbers are in %)
5.4 Machine Translation
We used a phrase-based translation framework with the phrase table represented as a finite-state trans-
ducer (Rangarajan Sridhar et al., 2013). Our framework proceeds by using the standard procedure of
performing word alignment using GIZA++ (Och and Ney, 2003) and obtaining phrases from the word
alignment using heuristics (Zens and Ney, 2004) and subsequently scoring them. The phrase table is
then represented as a finite-state transducer (FST). The FST decoder was used with minimum error rate
training (MERT) to compute a set of weights for the log-linear model. It is important to note that the
cost of arcs of the FST is a composite score (dot product of scores and weights) and hence requires an
additional lookup during the N-best generation phase in MERT to obtain the component scores. The
model is equivalent to Moses (?) phrase translation without reordering.
We noticed from the data collected in Section 4 that in typical SMS scenarios, a lot of phrases are stock
phrases and hence caching these phrases may result in high accuracies instead of deriving the translation
using a statistical model. We took the data created in Section 4 and created a FST to represent the
sentences. The motivation is to increase the precision of common entries as well as reduce the latency
involved in retrieving a translation from a statistical model. An example of the FST translation paradigm
is shown in Figure 1
We experimented with the notion of using a consensus-based word alignment by combining the align-
ment obtained through different alignment tools. We used GIZA++ (Och and Ney, 2003), Berkeley
5
http://www.opensubtitles.org
978
step1.fsm0
1how:how 2how^are:how^are 3how^are^you:how^are^you
are:are are^you:are^you you:you

WIP

LM)
hello how are you
hello 
how are you


ex.fst
0
hello:holathanks:gracias
how^do^you^do:como^estas
Cached Table
Statistical Model
bestpath(
hola como estas
hello.fsm
0
hello:hello
ptable.fst0/0
how:que/1.822how:como/0.458how^are^you:como^estas/1.106how^are^you:como^esta^usted/2.358are^you:estan/1.998
are^you:estas/0.757you:que/1.460you:tu/0.757
Figure 1: Illustration of the hybrid translation approach using FSTs. WIP and LM refer to the finite state
automata for word insertion penalty and language model, respectively.
Alignment strategy en2es es2en
GIZA++ 28.45 31.83
Pialign 28.08 33.48
Berkeley aligner 27.82 32.01
Union 28.01 33.14
Majority voting 27.32 32.96
Table 5: BLEU scores obtained using different alignment strategies. Only the statistical translation model
was used in the evaluation.
aligner (Liang et al., 2006) and the Phrasal ITG aligner (Pialign) (Neubig et al., 2011). We combined the
alignments in two different ways, taking the union of alignments or majority vote for each target word.
For training the translation model, we used a total of 28.5 million parallel sentences obtained from the
following sources: Opensubtitles (Tiedemann and Lars Nygaard, 2004), Europarl (Koehn, 2005), TED
talks (Cettolo et al., 2012) and Web. The bitext was processed to eliminate spurious pairs by restricting
the English and Spanish vocabularies to the top 150k frequent words as evidenced in a large collection of
monolingual corpora. We also eliminated bitext with ratio of English to Spanish words less than 0.5. The
initial model was optimized using MERT over 1000 parallel sentences from the SMS domain. Results of
the machine translation experiments are shown in Table 5. The test set used was 456 messages collected
in a real SMS interaction (see Section 6.1). The results indicate that consensus alignment procedure is not
superior to the individual alignment outputs. Furthermore, the BLEU scores obtained through both the
consensus procedures are not statistically significant with respect to the BLEU score obtained from the
individual alignment tools. Hence, we used with the phrase translation table obtained using the Phrasal
ITG aligner in all our experiments.
6 SMS Translation Service
In order to test the SMS translation models described in the previous sections, we created the infrastruc-
ture to intercept SMS messages, translate and deliver them in the preferred language of the recipient. The
users were simply asked to register their numbers with a particular language through a Web portal and
subsequently, all messages received by a user would be in the registered language. Some screenshots of
interaction between users is shown in Figure 2. For the messages that are translated, we show both the
original and translated messages. In cases where the translated message is longer than the character limit
per message, we split the message over two message boxes.
979
6.1 User Evaluation
Figure 2: Screenshots of the SMS interface with translation
In order to test the SMS translation models described in the previous sections, we created the infras-
tructure to intercept SMS messages, translate and deliver them in the preferred language of the recipient.
For the messages that are translated, we show both the original and translated messages. In cases where
the translated message is longer than the character limit per message, we split the message over two
message boxes. As part of the study we enrolled 20 English and 5 Spanish participants. The Spanish
participants were bilingual while the English users had little to no knowledge of Spanish. Some of these
interactions turned out to be short while others were had a large number of turns. We collected the
messages exchanged over 2 days that amounted to 241 English and 215 Spanish messages.
0!5!
10!15!
20!25!
30!35!
40!45!
0!1!
2!3!
4!5!
6!7!
8!9!
All! Most! Much! Little! None!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Adequacy of Translation!
Figure 3: Subjective ratings regarding the adequacy of using SMS translation
We manually translated the 456 messages to create a test data set for evaluation purposes. In the
absence of real SMS feeds in training, this test set is the closest we have to real SMS field data. The BLEU
scores using the entire pipeline (normalization, punctuation, cached and statistical machine translation)
for English-Spanish and Spanish-English was 31.25 and 37.19, respectively. We also created a survey
for the participants to evaluate fluency and adequacy (LDC, 2005) Figures 3 and 4 show the survey
results for adequacy and fluency, respectively. The results indicate that a majority of the people found
the translation quality to be sufficiently adequate while the fluency was between good and non-native.
7 Discussion
The SMS bitext described in Section 4 consists of a total 58790 unique parallel sentences in the SMS
domain. While the bulk of the data (speech-based) does not contain abbreviations and spelling errors, it
980
0!10!
20!30!
40!50!
60!
0!2!
4!6!
8!10!
12!
Flawless! Good! Non-nativ
e! Disfluent! Incompreh
ensible!
Perce
ntage
 of pa
rticipa
nts!
Numb
er of p
articip
ants!
Fluency of Translation!
Figure 4: Subjective ratings regarding the fluency of using SMS translation
is highly representative of SMS messages and in fact is perfectly suited for statistical machine translation
that typically uses normalized and tokenized data. The iterative procedure using Amazon Mechanical
Turk is a good approach to procuring surrogate SMS data. We plan to continue harvesting data using this
approach.
The unsupervised normalization lexicon learning using deep learning performs a good job of learning
SMS shorthands. However, the induced lexicon contains only one-to-one word mappings. If one were
to form compound words for a given dataset, the procedure can be potentially used for learning many-
to-one and many-to-many mappings. Our framework also learns spelling errors rather well. It may also
be possible to use distributed representations learned through log-linear models (Mikolov et al., 2013)
for our task. However, this is beyond the scope of the work presented in this paper. Finally, we used
only 1-best match for the unsupervised lexicon used in this work. One can potentially use a confusion
network and compose it with the FST model to achieve higher accuracies. Our scheme results in fairly
high precision with almost no false negatives (recall is extremely high) and can be reliably applied for
normalization. The unsupervised normalization scheme did not yield significant improvements in BLEU
score since our test set contained only 4 instances where shorthands were used.
Conventionally, sentence segmentation has been useful in improving the quality of statistical machine
translation (Matusov et al., 2006; Matusov et al., 2005). Such segmentation, albeit into shorter phrases,
is also useful for SMS translation. In the absence of phrase segmentation, the BLEU scores for English-
Spanish and Spanish-English drop to 29.65 and 23.95, respectively. The degradation for Spanish-English
messages is quite severe (drop from 37.19 to 23.95) as the lack of segmentation greatly reduces the use of
the cached table. In the absence of segmentation, the cached table was used for 12.8% and 14.4% of the
total phrases for English-Spanish and Spanish-English, respectively. However, with phrase segmentation
the cached table was used for 29.2% and 39.2% of total phrases.
The subjective results obtained from the user trial augur well for the real use of translation technology
as a feature in SMS. One of the issues in the study was balancing the English and Spanish participants.
Since we had access to more English participants (20) in comparison with Spanish participants (5), the
rate of exchange was slow. However, since SMS messages are not required to be real-time, participants
still engaged in a meaningful conversation. Subjective evaluation results using LDC criteria indicate
that most users were happy with the adequacy of translation while the fluency was rated as average. In
general, SMS messages are not very fluent due to character limit imposed on the exchanges and hence
machine translation has to use potentially disfluent source text.
8 Conclusion
We presented an application of statistical machine translation for translating SMS messages. We decou-
pled SMS translation into normalization followed by translation. Our unsupervised SMS normalization
approach exploits the distributional similarity of words and learns SMS shorthands with good accuracy.
We used a hybrid translation approach to exploit the repetitive nature of high frequency SMS messages.
Both objective and subjective evaluation experiments indicate that our system generates translation with
high quality while addressing the idiosyncrasies of SMS messages.
981
References
A. Aw, M. Zhang, J. Xiao, and J. Su. 2009. A phrase-based statistical model for SMS text normalization. In
Proceedings of COLING, pages 33?40.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Bootstrapping bilingual data using consensus translation for a
multilingual instant messaging system. In Proceedings of COLING.
R. Beaufort, S. Roekhaut, L. A. Cougnon, and C. Fairon. 2010. A hybrid rule/model-based finite-state framework
for normalizing sms messages. In Proceedings of ACL, pages 770?779.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. Journal of
Machine Learning Research, 3:1137?1155.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
T. Chen and M. Y. Kan. 2013. Creating a live, public short message service corpus: the NUS SMS corpus.
Language Resources and Evaluation, 47(2):299?335.
R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks
with multitask learning. In Proceedings of ICML.
R. Collobert, K. Kavukcuoglu, and C. Farabet. 2011. Torch7: A matlab-like environment for machine learning.
In BigLearn, NIPS Workshop.
V. Eidelman, K. Hollingshead, and P. Resnik. 2011. Noisy SMS Machine Translation in Low-Density Languages.
In Proceedings of 6th Workshop on Statistical Machine Translation.
C. Fairon and S. Paumier. 2006. A translated corpus of 30,000 french SMS. In Proceedings of LREC.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normalizing sms: Are two metaphors better than one? In Proceedings
of COLING, pages 441?448.
P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit.
LDC. 2005. Linguistic data annotation specification: Assessment of fluency and adequacy in translations. Tech-
nical report, Revision 1.5.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of NAACL-HLT, pages
104?111.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005. Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT, pages 148?154.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for
spoken language translation. In Proceedings of IWSLT, pages 158?165.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word representations in vector space.
In Proceedings of Workshop at ICLR.
Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised
model for joint phrase alignment and extraction. In Proceedings of the ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Pennell and Y. Liu. 2011. A character-level machine translation approach for normalization of SMS abbrevia-
tions. In Proceedings of IJCNLP.
V. K. Rangarajan Sridhar, J. Chen, S. Bangalore, A. Ljolje, and R. Chengalvarayan. 2013. Segmentation strategies
for streaming speech translation. In Proceedings of NAACL-HLT.
E. Sanders. 2012. Collecting and analysing chats and tweets in SoNaR. In Proceedings of LREC.
C. Tagg. 2009. Across-frequency in convolutive blind source separation. dissertation, University of Birmingham.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS corpus - parallel & free. In Proceedings of LREC.
982
M. Treurniet, O. De Clercq, H. van den Heuvel, and N. Oostdijk. 2012. Collecting a corpus of Dutch SMS. In
Proceedings of LREC, pages 2268?2273.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semi-
supervised learning. In Proceedings of ACL.
P. Wang and H. Tou Ng. 2013. A beam-search decoder for normalization of social media text with application to
machine translation. In Proceedings of NAACL-HLT.
Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In In
Proceedings of HLT-NAACL, pages 257?264.
983
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 55?63,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Qme! : A Speech-based Question-Answering system on Mobile Devices
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ
srini@research.att.com
Abstract
Mobile devices are becoming the dominant
mode of information access despite being
cumbersome to input text using small key-
boards and browsing web pages on small
screens. We present Qme!, a speech-based
question-answering system that allows for
spoken queries and retrieves answers to the
questions instead of web pages. We present
bootstrap methods to distinguish dynamic
questions from static questions and we show
the benefits of tight coupling of speech recog-
nition and retrieval components of the system.
1 Introduction
Access to information has moved from desktop and
laptop computers in office and home environments
to be an any place, any time activity due to mo-
bile devices. Although mobile devices have small
keyboards that make typing text input cumbersome
compared to conventional desktop and laptops, the
ability to access unlimited amount of information,
almost everywhere, through the Internet, using these
devices have made them pervasive.
Even so, information access using text input on
mobile devices with small screens and soft/small
keyboards is tedious and unnatural. In addition, by
the mobile nature of these devices, users often like
to use them in hands-busy environments, ruling out
the possibility of typing text. We address this issue
by allowing the user to query an information repos-
itory using speech. We expect that spoken language
queries to be a more natural and less cumbersome
way of information access using mobile devices.
A second issue we address is related to directly
and precisely answering the user?s query beyond
serving web pages. This is in contrast to the current
approach where a user types in a query using key-
words to a search engine, browses the returned re-
sults on the small screen to select a potentially rele-
vant document, suitably magnifies the screen to view
the document and searches for the answer to her
question in the document. By providing a method
for the user to pose her query in natural language and
presenting the relevant answer(s) to her question, we
expect the user?s information need to be fulfilled in
a shorter period of time.
We present a speech-driven question answering
system, Qme!, as a solution toward addressing these
two issues. The system provides a natural input
modality ? spoken language input ? for the users
to pose their information need and presents a col-
lection of answers that potentially address the infor-
mation need directly. For a subclass of questions
that we term static questions, the system retrieves
the answers from an archive of human generated an-
swers to questions. This ensures higher accuracy
for the answers retrieved (if found in the archive)
and also allows us to retrieve related questions on
the user?s topic of interest. For a second subclass of
questions that we term dynamic questions, the sys-
tem retrieves the answer from information databases
accessible over the Internet using web forms.
The layout of the paper is as follows. In Section 2,
we review the related literature. In Section 3, we
illustrate the system for speech-driven question an-
swering. We present the retrieval methods we used
to implement the system in Section 4. In Section 5,
we discuss and evaluate our approach to tight cou-
pling of speech recognition and search components.
In Section 6, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. We conclude in Section 7.
2 Related Work
Early question-answering (QA) systems, such as
Baseball (Green et al, 1961) and Lunar (Woods,
1973) were carefully hand-crafted to answer ques-
tions in a limited domain, similar to the QA
components of ELIZA (Weizenbaum, 1966) and
SHRDLU (Winograd, 1972). However, there has
been a resurgence of QA systems following the
TREC conferences with an emphasis on answering
factoid questions. This work on text-based question-
answering which is comprehensively summarized
55
in (Maybury, 2004), range widely in terms of lin-
guistic sophistication. At one end of the spectrum,
There are linguistically motivated systems (Katz,
1997; Waldinger et al, 2004) that analyze the user?s
question and attempt to synthesize a coherent an-
swer by aggregating the relevant facts. At the other
end of the spectrum, there are data intensive sys-
tems (Dumais et al, 2002) that attempt to use the
redundancy of the web to arrive at an answer for
factoid style questions. There are also variants of
such QA techniques that involve an interaction and
use context to resolve ambiguity (Yang et al, 2006).
In contrast to these approaches, our method matches
the user?s query against the questions in a large cor-
pus of question-answer pairs and retrieves the asso-
ciated answer.
In the information retrieval community, QA sys-
tems attempt to retrieve precise segments of a doc-
ument instead of the entire document. In (To-
muro and Lytinen, 2004), the authors match the
user?s query against a frequently-asked-questions
(FAQ) database and select the answer whose ques-
tion matches most closely to the user?s question.
An extension of this idea is explored in (Xue et al,
2008; Jeon et al, 2005), where the authors match the
user?s query to a community collected QA archive
such as (Yahoo!, 2009; MSN-QnA, 2009). Our ap-
proach is similar to both these lines of work in spirit,
although the user?s query for our system originates
as a spoken query, in contrast to the text queries in
previous work. We also address the issue of noisy
speech recognition and assess the value of tight in-
tegration of speech recognition and search in terms
of improving the overall performance of the system.
A novelty in this paper is our method to address dy-
namic questions as a seamless extension to answer-
ing static questions.
Also related is the literature on voice-search ap-
plications (Microsoft, 2009; Google, 2009; Yellow-
Pages, 2009; vlingo.com, 2009) that provide a spo-
ken language interface to business directories and
return phone numbers, addresses and web sites of
businesses. User input is typically not a free flowing
natural language query and is limited to expressions
with a business name and a location. In our system,
users can avail of the full range of natural language
expressions to express their information need.
And finally, our method of retrieving answers to
dynamic questions has relevance to the database and
meta search community. There is growing interest
in this community to mine the ?hidden? web ? infor-
mation repositories that are behind web forms ? and
provide a unified meta-interface to such informa-
tion sources, for example, web sites related travel,
or car dealerships. Dynamic questions can be seen
as providing a natural language interface (NLI) to
such web forms, similar to early work on NLI to
databases (Androutsopoulos, 1995).
3 Speech-driven Question Retrieval
System
We describe the speech-driven query retrieval appli-
cation in this section. The user of this application
provides a spoken language query to a mobile device
intending to find an answer to the question. Some
example users? inputs are1 what is the fastest ani-
mal in water, how do I fix a leaky dishwasher, why
is the sky blue. The result of the speech recognizer
is used to search a large corpus of question-answer
pairs to retrieve the answers pertinent to the user?s
static questions. For the dynamic questions, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com for
movie information). The result from the speech rec-
ognizer can be a single-best string or a weighted
word lattice.2 The retrieved results are ranked using
different metrics discussed in the next section. In
Figure 2, we illustrate the answers that Qme!returns
for static and dynamic quesitons.
Lattice1?best
Q&A corpus
ASRSpeech
Dynamic
Classify
from WebRetrieve
Rank
Search
Ranked ResultsMatch
Figure 1: The architecture of the speech-driven question-
answering system
4 Methods of Retrieval
We formulate the problem of answering static
questions as follows. Given a question-answer
archive QA = {(q1, a1), (q2, a2), . . . , (qN , aN )}
1The query is not constrained to be of any specific question
type (for example, what, where, when, how).
2For this paper, the ASR used to recognize these utterances
incorporates an acoustic model adapted to speech collected
from mobile devices and a four-gram language model that is
built from the corpus of questions.
56
Figure 2: Retrieval results for static and dynamic ques-
tions using Qme!
of N question-answer pairs, and a user?s ques-
tion qu, the task is to retrieve a subset QAr =
{(qr1, a
r
1), (q
r
2, a
r
2), . . . , (q
r
M , a
r
M )} M << N us-
ing a selection function Select and rank the mem-
bers of QAr using a scoring function Score such
that Score(qu, (qri , a
r
i )) > Score(qu, (q
r
i+1, a
r
i+1)).
Here, we assume
Score(qu, (qri , a
r
i )) = Score(qu, q
r
i ).
The Select function is intended to select the
matching questions that have high ?semantic? simi-
larity to the user?s question. However, given there is
no objective function that measures semantic simi-
larity, we approximate it using different metrics dis-
cussed below.
Ranking of the members of the retrieved set can
be based on the scores computed during the selec-
tion step or can be independently computed based
on other criteria such as popularity of the question,
credibility of the source, temporal recency of the an-
swer, geographical proximity to the answer origin.
4.1 Question Retrieval Metrics
We retrieve QA pairs from the data repository based
on the similarity of match between the user?s query
and each of the set of questions (d) in the repos-
itory. To measure the similarity, we have experi-
mented with the following metrics.
1. TF-IDF metric: The user input query and the
document (in our case, questions in the repos-
itory) are represented as bag-of-n-grams (aka
terms). The term weights are computed using a
combination of term frequency (tf ) and inverse
document frequency (idf ) (Robertson, 2004).
If Q = q1, q2, . . . , qn is a user query, then the
aggregated score for a document d using a un-
igram model of the query and the document is
given as in Equation 1. For a given query, the
documents with the highest total term weight
are presented as retrieved results. Terms can
also be defined as n-gram sequences of a query
and a document. In our experiments, we have
used up to 4-grams as terms to retrieve and rank
documents.
Score(d) =
?
w?Q
tfw,d ? idfw (1)
2. String Comparison Metrics: Since the length
of the user query and the query to be retrieved
are similar in length, we use string compar-
ison methods such as Levenshtein edit dis-
tance (Levenshtein, 1966) and n-gram overlap
(BLEU-score) (Papineni et al, 2002) as simi-
larity metrics.
We compare the search effectiveness of these sim-
ilarity metrics in Section 5.3.
5 Tightly coupling ASR and Search
Most of the speech-driven search systems use the
1-best output from the ASR as the query for the
search component. Given that ASR 1-best output
is likely to be erroneous, this serialization of the
ASR and search components might result in sub-
optimal search accuracy. A lattice representation
of the ASR output, in particular, a word-confusion
network (WCN) transformation of the lattice, com-
pactly encodes the n-best hypothesis with the flexi-
bility of pruning alternatives at each word position.
An example of a WCN is shown in Figure 3. The
weights on the arcs are to be interpreted as costs and
the best path in the WCN is the lowest cost path
from the start state (0) to the final state (4). Note
that the 1-best path is how old is mama, while the
input speech was how old is obama which also is in
the WCN, but at a higher cost.
0 1how/0.001
who/6.292
2
old/0.006
does/12.63
late/14.14
was/14.43
_epsilon/5.010
3
is/0.000
a/12.60
_epsilon/8.369
4/1
obama/7.796
lil/7.796
obamas/13.35
mama/0.000
bottle/12.60
Figure 3: A sample word confusion network with arc
costs as negative logarithm of the posterior probabilities.
57
0how:qa25/c1
old:qa25/c2
is:qa25/c3
obama:qa25/c4
old:qa150/c5
how:qa12/c6
obama:qa450/c7
is:qa1450/c8
Figure 4: Example of an FST representing the search in-
dex.
5.1 Representing Search Index as an FST
Lucene (Hatcher and Gospodnetic., 2004) is an off-
the-shelf search engine that implements the TF-IDF
metric. But, we have implemented our own search
engine using finite-state transducers (FST) for this
reason. The oracle word/phrase accuracy using n-
best hypotheses of an ASR is usually far greater than
the 1-best output. However, using each of the n-best
(n > 1) hypothesis as a separate query to the search
component is computationally sub-optimal since the
strings in the n-best hypotheses usually share large
subsequences with each other. The FST representa-
tion of the search index allows us to efficiently con-
sider lattices/WCNs as input queries.
The FST search index is built as follows. We in-
dex each question-answer (QA) pair from our repos-
itory ((qi, ai), qai for short) using the words (wqi) in
question qi. This index is represented as a weighted
finite-state transducer (SearchFST) as shown in Fig-
ure 4. Here a word wqi (e.g old) is the input symbol
for a set of arcs whose output symbol is the index
of the QA pairs where old appears in the question.
The weight of the arc c(wqi ,qi) is one of the simi-
larity based weights discussed in Section 4.1. As
can be seen from Figure 4, the words how, old, is
and obama contribute a score to the question-answer
pair qa25; while other pairs, qa150, qa12, qa450 are
scored by only one of these words.
5.2 Search Process using FSTs
A user?s speech query, after speech recognition, is
represented as an FSA (either 1-best or WCN), a
QueryFSA. The QueryFSA (denoted as q) is then
transformed into another FSA (NgramFSA(q)) that
represents the set of n-grams of the QueryFSA.
Due to the arc costs from WCNs, the NgramFSA
for a WCN is a weighted FSA. The NgramFSA is
composed with the SearchFST and we obtain all
the arcs (wq, qawq , c(wq ,qawq )) where wq is a query
term, qawq is a QA index with the query term and,
c(wq ,qawq ) is the weight associated with that pair. Us-
ing this information, we aggregate the weight for a
QA pair (qaq) across all query words and rank the
retrieved QAs in the descending order of this aggre-
gated weight. We select the top N QA pairs from
this ranked list. The query composition, QA weight
aggregation and selection of top N QA pairs are
computed with finite-state transducer operations as
shown in Equations 2 to 5.3
D1 = pi2(NgramFSA(q) ? SearchFST ) (2)
R1 = fsmbestpath(D1, 1) (3)
D2 = pi2(NgramFSA(R1) ? SearchFST ) (4)
TopN = fsmbestpath(fsmdeterminize(D2), N)
(5)
The process of retrieving documents using the
Levenshtein-based string similarity metric can also
be encoded as a composition of FSTs.
5.3 Experiments and Results
We have a fairly large data set consisting of over a
million question-answer pairs collected by harvest-
ing the web. In order to evaluate the retrieval meth-
ods discussed earlier, we use two test sets of QA
pairs: a Seen set of 450 QA pairs and an Unseen set
of 645 QA pairs. The queries in the Seen set have
an exact match with some question in the database,
while the queries in the Unseen set may not match
any question in the database exactly. 4 The questions
in theUnseen set, however, like those in the Seen set,
also have a human generated answer that is used in
our evaluations.
For each query, we retrieve the twenty most rel-
evant QA pairs, ranked in descending order of the
value of the particular metric under consideration.
However, depending on whether the user query is a
seen or an unseen query, the evaluation of the rele-
vance of the retrieved question-answer pairs is dif-
ferent as discussed below.5
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
4There may however be semantically matching questions.
5The reason it is not a recall and precision curve is that, for
the ?seen? query set, the retrieval for the questions is a zero/one
boolean accuracy. For the ?unseen? query set there is no perfect
match with the input question in the query database, and so we
determine the closeness of the questions based on the closeness
of the answers. Coherence attempts to capture the homogen-
ity of the questions retrieved, with the assumption that the user
might want to see similar questions as the returned results.
58
5.3.1 Evaluation Metrics
For the set of Seen queries, we evaluate the rele-
vance of the retrieved top-20 question-answer pairs
in two ways:
1. Retrieval Accuracy of Top-N results: We eval-
uate whether the question that matches the user
query exactly is located in the top-1, top-5,
top-10, top-20 or not in top-20 of the retrieved
questions.
2. Coherence metric: We compute the coherence
of the retrieved set as the mean of the BLEU-
score between the input query and the set of
top-5 retrieved questions. The intuition is that
we do not want the top-5 retrieved QA pairs
to distract the user by not being relevant to the
user?s query.
For the set of Unseen queries, since there are no
questions in the database that exactly match the in-
put query, we evaluate the relevance of the top-20 re-
trieved question-answer pairs in the following way.
For each of the 645 Unseen queries, we know the
human-generated answer. We manually annotated
each unseen query with the Best-Matched QA pair
whose answer was the closest semantic match to the
human-generated answer for that unseen query. We
evaluate the position of the Best-Matched QA in the
list of top twenty retrieved QA pairs for each re-
trieval method.
5.3.2 Results
On the Seen set of queries, as expected the re-
trieval accuracy scores for the various retrieval tech-
niques performed exceedingly well. The unigram
based tf.idf method retrieved 93% of the user?s query
in the first position, 97% in one of top-5 positions
and 100% in one of top-10 positions. All the other
retrieval methods retrieved the user?s query in the
first position for all the Seen queries (100% accu-
racy).
In Table 1, we tabulate the results of the Coher-
ence scores for the top-5 questions retrieved using
the different retrieval techniques for the Seen set of
queries. Here, the higher the n-gram the more co-
herent is the set of the results to the user?s query. It
is interesting to note that the BLEU-score and Lev-
enshtein similarity driven retrieval methods do not
differ significantly in their scores from the n-gram
tf.idf based metrics.
Method Coherence Metric
for top-5 results
TF-IDF unigram 61.58
bigram 66.23
trigram 66.23
4-gram 69.74
BLEU-score 66.29
Levenshtein 67.36
Table 1: Coherence metric results for top-5 queries re-
trieved using different retrieval techniques for the seen
set.
In Table 2, we present the retrieval results using
different methods on the Unseen queries. For 240 of
the 645 unseen queries, the human expert found that
that there was no answer in the data repository that
could be considered semantically equivalent to the
human-generated response to that query. So, these
240 queries cannot be answered using the current
database. For the remaining 405 unseen queries,
over 60% have their Best-Matched question-answer
pair retrieved in the top-1 position. We expect the
coverage to improve considerably by increasing the
size of the QA archive.
Method Top-1 Top-20
TFIDF Unigram 69.13 75.81
Bigram 62.46 67.41
Trigram 61.97 65.93
4-gram 56.54 58.77
WCN 70.12 78.52
Levenshtein 67.9 77.29
BLEU-score 72.0 75.31
Table 2: Retrieval results for the Unseen queries
5.3.3 Speech-driven query retrieval
In Equation 6, we show the tight integration of
WCNs and SearchFST using the FST composition
operation (?). ? is used to scale the weights6 from
the acoustic/language models on the WCNs against
the weights on the SearchFST. As before, we use
Equation 3 to retrieve the top N QA pairs. The tight
integration is expected to improve both the ASR and
Search accuracies by co-constraining both compo-
nents.
D = pi2(Unigrams(WCN)
??SearchFST ) (6)
For this experiment, we use the speech utterances
corresponding to the Unseen set as the test set. We
use a different set of 250 speech queries as the
6fixed using the development set
59
development set. In Table 3, we show the Word
and Sentence Accuracy measures for the best path
in the WCN before and after the composition of
SearchFST with the WCN on the development and
test sets. We note that by integrating the constraints
from the search index, the ASR accuracies can be
improved by about 1% absolute.
Set # of Word Sentence
utterances Accuracy Accuracy
Dev Set 250 77.1(78.2) 54(54)
Test Set 645 70.8(72.1) 36.7(37.1)
Table 3: ASR accuracies of the best path before and after
(in parenthesis) the composition of SearchFST
Since we have the speech utterances of the Un-
seen set, we were also able to compute the search
results obtained by integrating the ASR WCNs with
the SearchFST, as shown in line 5 of Table 2. These
results show that the the integration of the ASR
WCNs with the SearchFST produces higher search
accuracy compared to ASR 1-best.
6 Dynamic and Static Questions
Storing previously answered questions and their an-
swers allows Qme!to retrieve the answers to a sub-
class of questions quickly and accurately. We term
this subclass as static questions since the answers
to these questions remain the same irrespective of
when and where the questions are asked. Examples
of such questions are What is the speed of light?,
When is George Washington?s birthday?. In con-
trast, there is a subclass of questions, which we term
dynamic questions, for which the answers depend
on when and where they are asked. For such ques-
tions the above method results in less than satisfac-
tory and sometimes inaccurate answers. Examples
of such questions are What is the stock price of Gen-
eral Motors?, Who won the game last night?, What
is playing at the theaters near me?.
We define dynamic questions as questions whose
answers change more frequently than once a year.
In dynamic questions, there may be no explicit ref-
erence to time, unlike the questions in the TERQAS
corpus (Radev and Sundheim., 2002) which explic-
itly refer to the temporal properties of the entities
being questioned or the relative ordering of past and
future events. The time-dependency of a dynamic
question lies in the temporal nature of its answer.
For example, consider the dynamic question, ?What
is the address of the theater ?White Christmas? is
playing at in New York??. White Christmas is a sea-
sonal play that plays in New York every year for a
few weeks in December and January, but it does not
necessarily at the same theater every year. So, de-
pending when this question is asked, the answer will
be different.
Interest in temporal analysis for question-
answering has been growing since the late 1990?s.
Early work on temporal expressions identifica-
tion using a tagger led to the development of
TimeML (Pustejovsky et al, 2001), a markup
language for annotating temporal expressions and
events in text. Other examples include QA-by-
Dossier with Constraints (Prager et al, 2004), a
method of improving QA accuracy by asking auxil-
iary questions related to the original question in or-
der to temporally verify and restrict the original an-
swer. (Moldovan et al, 2005) detect and represent
temporally related events in natural language using
logical form representation. (Saquete et al, 2009)
use the temporal relations in a question to decom-
pose it into simpler questions, the answers of which
are recomposed to produce the answers to the origi-
nal question.
6.1 Dynamic/Static Classification
We automatically classify questions as dynamic and
static questions. Answers to static questions can be
retrieved from the QA archive. To answer dynamic
questions, we query the database(s) associated with
the topic of the question through web forms on the
Internet. We use a topic classifier to detect the topic
of a question followed by a dynamic/static classifier
trained on questions related to a topic, as shown in
figure 5. Given the question what movies are play-
ing around me?, we detect it is a movie related dy-
namic question and query a movie information web
site (e.g. www.fandango.com) to retrieve the results
based on the user?s GPS information.
Figure 5: Chaining two classifiers
We used supervised learning to train the topic
60
classifier, since our entire dataset is annotated by hu-
man experts with topic labels. In contrast, to train a
dynamic/static classifier, we experimented with the
following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other sub-
strings such as cost of and how much is. A question
is considered static if it does not contain any such
words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001) is presented in Table 6 and illustrated in Fig-
ure 7(a). The benefit of self-training is that we can
build a better classifier than that built from the small
seed corpus by simply adding in the large unlabeled
corpus without requiring hand-labeling.
1. Create k bags of data, each of size |L|, by sampling
with replacement from labeled set L.
2. Train k classifiers; one classifier on each of k bags.
3. Each classifier predicts labels of the unlabeled set.
4. The N labeled instances that j of k classifiers agree
on with the highest average confidence is added to the
labeled set L, to produce a new labeled set L?.
5. Repeat all 5 steps until stopping criteria is reached.
Figure 6: Self-training with bagging
(a) (b)
Figure 7: (a) Self-training with bagging (b) Committee-
based active-learning
In order to prevent a bias towards the majority
class, in step 4, we ensure that the distribution of
the static and dynamic questions remains the same
as in the annotated seed corpus. The benefit of bag-
ging (Breiman, 1996) is to present different views of
the same training set, and thus have a way to assess
the certainty with which a potential training instance
can be labeled.
Active-learning: This is another popular method for
training classifiers when not much annotated data is
available. The key idea in active learning is to anno-
tate only those instances of the dataset that are most
difficult for the classifier to learn to classify. It is
expected that training classifiers using this method
shows better performance than if samples were cho-
sen randomly for the same human annotation effort.
Figure 7(b) illustrates the algorithm and Figure 8
describes the algorithm, also known as committee-
based active-learning (Banko and Brill, 2001).
1. Create k bags of data, each of size |L|, by sampling
with replacement from the labeled set L.
2. Train k classifiers, one on each bag of the k bags.
3. Each classifier predicts the labels of the unlabeled set.
4. Choose N instances from the unlabeled set for human
labeling. N/2 of the instances are those whose labels the
committee of classifiers have highest vote entropy (un-
certainity). The other N/2 of the instances are selected
randomly from the unlabeled set.
5. Repeat all 5 steps until stopping criteria is reached.
Figure 8: Active Learning algorithm
We used the maximum entropy classifier in
Llama (Haffner, 2006) for all of the above classi-
fication tasks.
6.2 Experiments and Results
6.2.1 Topic Classification
The topic classifier was trained using a training
set consisted of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the questions.
The test set consisted of 15,000 randomly selected
questions. Word trigrams of the question are used
as features for a MaxEnt classifier which outputs a
score distribution on all of the 104 possible topic
labels. The error rate results for models selecting
the top topic and the top two topics according to the
score distribution are shown in Table 4. As can be
seen these error rates are far lower than the baseline
model of selecting the most frequent topic.
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
Table 4: Results of topic classification
61
Figure 9: Change in classification results
6.2.2 Dynamic/static Classification
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluate these
methods on a 250 question test set drawn from the
broad topic of Movies. For the baseline model, we
used the words/phrases discussed earlier based on
temporal and spatial indexicals. For the ?super-
vised? model, we use the baseline model to tag 500K
examples and use the machine-annotated corpus to
train a MaxEnt binary classifier with word trigrams
as features. The error rate in Table 5 shows that it
performs better than the baseline model mostly due
to better lexical coverage contributed by the 500K
examples.
Training approach Lowest Error rate
Baseline 27.70%
?Supervised? learning 22.09%
Self-training 8.84%
Active-learning 4.02%
Table 5: Best Results of dynamic/static classification
In the self-training approach, we start with a small
seed corpus of 250 hand-labeled examples from the
Movies topic annotated with dynamic or static tags.
We used the same set of 500K unlabeled examples
as before and word trigrams from the question were
used as the features for a MaxEnt classifier. We used
11 bags in the bagging phase of this approach and
required that all 11 classifiers agree unanimously
about the label of a new instance. Of all such in-
stances, we randomly selected N instances to be
added to the training set of the next iteration, while
maintaining the distribution of the static and dy-
namic questions to be the same as that in the seed
corpus. We experimented with various values of N ,
the number of newly labeled instances added at each
iteration. The error rate at initialization is 10.4%
compared to 22.1% of the ?supervised? approach
which can be directly attributed to the 250 hand-
labeled questions. The lowest error rate of the self-
training approach, obtained at N=100, is 8.84%, as
shown in Table 5. In Figure 9, we show the change
in error rate for N=40 (line S1 in the graph) and
N=100 (line S2 in the graph).
For the active learning approach, we used the
same set of 250 questions as the seed corpus, the
same set of 500K unlabeled examples, the same test
set, and the same set of word trigrams features as in
the self-training approach. We used 11 bags for the
bagging phase and selected top 20 new unlabeled in-
stances on which the 11 classifiers had the greatest
vote entropy to be presented to the human labeler for
annotation. We also randomly selected 20 instances
from the rest of the unlabeled set to be presented for
annotation. The best error rate of this classifier on
the test set is 4.02%, as shown in Table 5. The error
rate over successive iterations is shown by line A1
in Figure 9.
In order to illustrate the benefits of selecting the
examples actively, we repeated the experiment de-
scribed above but with all 40 unlabeled instances se-
lected randomly for annotation. The error rate over
successive iterations is shown by line R1 in Fig-
ure 9. Comparing A1 to R1, we see that the error de-
creases faster when we select some of the unlabeled
instances for annotation actively at each iteration.
7 Conclusion
In this paper, we have presented a system Qme!,
a speech-driven question-answering system for mo-
bile devices. We have proposed a query retrieval
model for question-answering and demonstrated the
mutual benefits of tightly coupling the ASR and
Search components of the system. We have pre-
sented a novel concept of distinguishing questions
that need dynamic information to be answered from
those questions whose answers can be retrieved from
an archive. We have shown results on bootstrap-
ping such a classifier using semi-supervised learning
techniques.
62
References
L. Androutsopoulos. 1995. Natural language interfaces
to databases - an introduction. Journal of Natural Lan-
guage Engineering, 1:29?81.
M. Banko and E. Brill. 2001. Scaling to very very large
corpora for natural language disambiguation. In Pro-
ceedings of the 39th annual meeting of the association
for computational linguistics: ACL 2001, pages 26?
33.
L. Breiman. 1996. Bagging predictors. Machine Learn-
ing, 24(2):123?140.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng. 2002.
Web question answering: is more always better? In
SIGIR ?02: Proceedings of the 25th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 291?298, New
York, NY, USA. ACM.
Google, 2009. http://www.google.com/mobile.
B.F. Green, A.K. Wolf, C. Chomsky, and K. Laughery.
1961. Baseball, an automatic question answerer. In
Proceedings of the Western Joint Computer Confer-
ence, pages 219?224.
P. Haffner. 2006. Scaling large margin classifiers for spo-
ken language understanding. Speech Communication,
48(iv):239?261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Action
(In Action series). Manning Publications Co., Green-
wich, CT, USA.
J. Jeon, W. B. Croft, and J. H. Lee. 2005. Finding sim-
ilar questions in large question and answer archives.
In CIKM ?05: Proceedings of the 14th ACM interna-
tional conference on Information and knowledge man-
agement, pages 84?90, New York, NY, USA. ACM.
B. Katz. 1997. Annotating the world wide web using
natural language. In Proceedings of RIAO.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertion and reversals. Soviet Physics
Doklady, 10:707?710.
M. T.Maybury, editor. 2004. NewDirections in Question
Answering. AAAI Press.
Microsoft, 2009. http://www.live.com.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence, pages 1009?1104.
MSN-QnA, 2009. http://qna.live.com/.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: A method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meeting
of the Association of Computational Linguistics, pages
313?318, Philadelphia, PA, July.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574?581.
J. Pustejovsky, R. Ingria, R. Saur??, J. Casta no, J. Littman,
and R. Gaizauskas., 2001. The language of time: A
reader, chapter The specification languae ? TimeML.
Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in ques-
tion answering. Technical report, Brandies University.
S. Robertson. 2004. Understanding inverse document
frequency: On theoretical arguments for idf. Journal
of Documentation, 60.
E. Saquete, J. L. Vicedo, P. Mart??nez-Barco, R. Mu
noz, and H. Llorens. 2009. Enhancing qa sys-
tems with complex temporal question processing ca-
pabilities. Journal of Artificial Intelligence Research,
35:775?811.
N. Tomuro and S. L. Lytinen. 2004. Retrieval models
and Q and A learning with FAQ files. In New Direc-
tions in Question Answering, pages 183?202.
vlingo.com, 2009. http://www.vlingomobile.com/downloads.html.
R. J. Waldinger, D. E. Appelt, J. L. Dungan, J. Fry, J. R.
Hobbs, D. J. Israel, P. Jarvis, D. L. Martin, S. Riehe-
mann, M. E. Stickel, and M. Tyson. 2004. Deductive
question answering from multiple resources. In New
Directions in Question Answering, pages 253?262.
J. Weizenbaum. 1966. ELIZA - a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 1:36?45.
T. Winograd. 1972. Understanding Natural Language.
Academic Press.
W. A. Woods. 1973. Progress in natural language un-
derstanding - an application to lunar geology. In Pro-
ceedings of American Federation of Information Pro-
cessing Societies (AFIPS) Conference.
X. Xue, J. Jeon, and W. B. Croft. 2008. Retrieval models
for question and answer archives. In SIGIR ?08: Pro-
ceedings of the 31st annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 475?482, New York, NY, USA.
ACM.
Yahoo!, 2009. http://answers.yahoo.com/.
F. Yang, J. Feng, and G. DiFabbrizio. 2006. A data
driven approach to relevancy recognition for contex-
tual question answering. In HLT-NAACL 2006 Work-
shop on Interactive Question Answering, New York,
USA, June 8-9.
YellowPages, 2009. http://www.speak4it.com.
63
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 437?445,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Real-time Incremental Speech-to-Speech Translation of Dialogs
Srinivas Bangalore, Vivek Kumar Rangarajan Sridhar, Prakash Kolan
Ladan Golipour, Aura Jimenez
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932, USA
vkumar,srini,pkolan,ladan,aura@research.att.com
Abstract
In a conventional telephone conversation be-
tween two speakers of the same language, the
interaction is real-time and the speakers pro-
cess the information stream incrementally. In
this work, we address the problem of incre-
mental speech-to-speech translation (S2S) that
enables cross-lingual communication between
two remote participants over a telephone. We
investigate the problem in a novel real-time
Session Initiation Protocol (SIP) based S2S
framework. The speech translation is per-
formed incrementally based on generation of
partial hypotheses from speech recognition.
We describe the statistical models comprising
the S2S system and the SIP architecture for
enabling real-time two-way cross-lingual dia-
log. We present dialog experiments performed
in this framework and study the tradeoff in ac-
curacy versus latency in incremental speech
translation. Experimental results demonstrate
that high quality translations can be generated
with the incremental approach with approxi-
mately half the latency associated with non-
incremental approach.
1 Introduction
In recent years, speech-to-speech translation (S2S)
technology has played an increasingly important
role in narrowing the language barrier in cross-
lingual interpersonal communication. The improve-
ments in automatic speech recognition (ASR), statis-
tical machine translation (MT), and, text-to-speech
synthesis (TTS) technology has facilitated the serial
binding of these individual components to achieve
S2S translation of acceptable quality.
Prior work on S2S translation has primarily fo-
cused on providing either one-way or two-way trans-
lation on a single device (Waibel et al, 2003; Zhou
et al, 2003). Typically, the user interface requires
the participant(s) to choose the source and target lan-
guage apriori. The nature of communication, either
single user talking or turn taking between two users
can result in a one-way or cross-lingual dialog inter-
action. In most systems, the necessity to choose the
directionality of translation for each turn does take
away from a natural dialog flow. Furthermore, single
interface based S2S translation (embedded or cloud-
based) is not suitable for cross-lingual communica-
tion when participants are geographically distant, a
scenario more likely in a global setting. In such a
scenario, it is imperative to provide real-time and
low latency communication.
In a conventional telephone conversation between
two speakers of the same language, the interaction
is real-time and the speakers process the informa-
tion stream incrementally. Similarly, cross-lingual
dialog between two remote participants will greatly
benefit through incremental translation. While in-
cremental decoding for text translation has been
addressed previously in (Furuse and Iida, 1996;
Sankaran et al, 2010), we address the problem in
a speech-to-speech translation setting for enabling
real-time cross-lingual dialog. We address the prob-
lem of incrementality in a novel session initiation
protocol (SIP) based S2S translation system that en-
ables two people to interact and engage in cross-
lingual dialog over a telephone (mobile phone or
landline). Our system performs incremental speech
recognition and translation, allowing for low latency
interaction that provides an ideal setting for remote
dialog aimed at accomplishing a task.
We present previous work in this area in Section 2
and introduce the problem of incremental translation
in Section 3. We describe the statistical models used
in the S2S translation framework in Section 4 fol-
lowed by a description of the SIP communication
437
framework for real-time translation in Section 5. In
Section 6, we describe the basic call flow of our sys-
tem following which we present dialog experiments
performed using our framework in Section 8. Fi-
nally, we conclude in Section 9 along with directions
for future work.
2 Previous Work
Most previous work on speech-to-speech transla-
tion systems has focused on a single device model,
i.e., the user interface for translation is on one de-
vice (Waibel et al, 1991; Metze et al, 2002; Zhou
et al, 2003; Waibel et al, 2003). The device typi-
cally supports multiple source-target language pairs.
A user typically chooses the directionality of transla-
tion and a toggle feature is used to switch the direc-
tionality. However, this requires physical presence
of the two conversants in one location.
On the other hand, text chat between users over
cell phones has become increasingly popular in the
last decade. While the language used in the inter-
action is typically monolingual, there have been at-
tempts to use statistical machine translation to en-
able cross-lingual text communication (Chen and
Raman, 2008). But this introduces a significant
overhead as the users need to type in the responses
for each turn. Moreover, statistical translation sys-
tems are typically unable to cope with telegraphic
text present in chat messages. A more user friendly
approach would be to use speech as the modality for
communication.
One of the first attempts for two-way S2S trans-
lation over a telephone between two potentially re-
mote participants was made as part of the Verbmobil
project (Wahlster, 2000). The system was restricted
to certain topics and speech was the only modality.
Furthermore, the spontaneous translation of dialogs
was not incremental. One of the first attempts at in-
cremental text translation was demonstrated in (Fu-
ruse and Iida, 1996) using a transfer-driven machine
translation approach. More recently, an incremen-
tal decoding framework for text translation was pre-
sented in (Sankaran et al, 2010). To the best of
our knowledge, incremental speech-to-speech trans-
lation in a dialog setting has not been addressed in
prior work. In this work, we address this problem
using first of a kind SIP-based large vocabulary S2S
translation system that can work with both smart-
phones and landlines. The speech translation is per-
formed incrementally based on generation of partial
hypotheses from speech recognition. Our system
displays the recognized and translated text in an in-
cremental fashion. The use of SIP-based technology
also supports an open form of cross-lingual dialog
without the need for attention phrases.
3 Incremental Speech-to-Speech
Translation
In most statistical machine translation systems, the
input source text is translated in entirety, i.e., the
search for the optimal target string is constrained
on the knowledge of the entire source string. How-
ever, in applications such as language learning and
real-time speech-to-speech translation, incremen-
tally translating the source text or speech can pro-
vide seamless communication and understanding
with low latency. Let us assume that the input string
(either text or speech recognition hypothesis) is f =
f1, ? ? ? , fJ and the target string is e = e1, ? ? ? , eI .
Among all possible target sentences, we will choose
the one with highest probability:
e?(f) = argmax
e
Pr(e|f) (1)
In an incremental translation framework, we do not
observe the entire string f . Instead, we observe Qs
sequences, S = s1 ? ? ? sk ? ? ? sQs , i.e., each sequence
sk = [fjkfjk+1 ? ? ? fj(k+1)?1], j1 = 1, jQs+1 =
J + 11. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ? ? ? ei(k+1)?1], i1 =
1, iQs+1 = I+1. Given this setting, we can perform
decoding using three different approaches. Assum-
ing that each partial source input is translated inde-
pendently, i.e., chunk-wise translation, we get,
e?(f) = argmax
t1
Pr(t1|s1) ? ? ? argmax
tk
Pr(tk|sk)
(2)
We call the decoding in Eq. 2 as partial decoding.
The other option is to translate the partial source in-
1For simplicity, we assume that the incremental and non-
incremental hypotheses are equal in length
438
put conditioned on the history, i.e.,
e?(f) = argmax
t1
Pr(t1|s1) ? ? ?
argmax
tk
Pr(tk|s1, ? ? ? , sk, t
?
1, ? ? ? , t
?
k?1) (3)
where t?i denotes the best translation for source se-
quence si. We term the result obtained through Eq. 3
as continue-partial. The third option is to wait for
all the partials to be generated and then decode the
source string which we call complete decoding, i.e.,
e?(f) = argmax
e
Pr(e|s1, ? ? ? , sk) (4)
Typically, the hypothesis e? will be more accurate
than e? as the translation process is non-incremental.
In the best case, one can obtain e? = e?. While the de-
coding described in Eq. 2 has the lowest latency, it
is likely to result in inferior performance in compari-
son to Eq. 1 that will have higher latency. One of the
main issues in incremental speech-to-speech trans-
lation is that the translated sequences need to be im-
mediately synthesized. Hence, there is tradeoff be-
tween the amount of latency versus accuracy as the
synthesized audio cannot be revoked in case of long
distance reordering. In this work, we focus on incre-
mental speech translation and defer the problem of
incremental synthesis to future work. We investigate
the problem of incrementality using a novel SIP-
based S2S translation system, the details of which
we discuss in the subsequent sections.
4 Speech-to-Speech Translation
Components
In this section, we describe the training data, pre-
processing steps and statistical models used in the
S2S system.
4.1 Automatic Speech Recognition
We use the AT&T WATSONSM real-time speech
recognizer (Goffin et al, 2004) as the speech recog-
nition module. WATSONSM uses context-dependent
continuous density hidden Markov models (HMM)
for acoustic modeling and finite-state networks for
network optimization and search. The acoustic mod-
els are Gaussian mixture tied-state three-state left-
to-right HMMs. All the acoustic models in this work
were initially trained using the Maximum Likeli-
hood Estimation (MLE) criterion, and followed by
discriminative training through Minimum Phone Er-
ror (MPE) criterion. We also employed Gaussian
Selection (Bocchieri, 1993) to decrease the real-time
factor during the recognition procedure.
The acoustic models for English and Span-
ish were mainly trained on short utterances in
the respective language, acquired from SMS and
search applications on smartphones. The amount
of training data for the English acoustic model
is around 900 hours of speech, while the data
for training the Spanish is approximately half that
of the English model. We used a total of 107
phonemes for the English acoustic model, com-
posed of digit-specific, alpha-specific, and general
English phonemes. Digit-specific and alpha-specific
phonemes were applied to improve the recognition
accuracy of digits and alphas in the speech. The
number of phonemes for Spanish was 34, and, no
digit- or alpha-specific phonemes were included.
The pronunciation dictionary for English is a hand-
labeled dictionary, with pronunciation for unseen
words being predicted using custom rules. A rule-
based dictionary was used for Spanish.
We use AT&T FSM toolkit (Mohri et al, 1997)
to train a trigram language model (LM). The lan-
guage model was linearly interpolated from 18 and
17 components for English and Spanish, respec-
tively. The data for the the LM components was
obtained from several sources that included LDC,
Web, and monolingual portion of the parallel data
described in section 4.2. An elaborate set of lan-
guage specific tokenization and normalization rules
was used to clean the corpora. The normalization
included spelling corrections, conversion of numer-
als into words while accounting for telephone num-
bers, ordinal, and, cardinal categories, punctuation,
etc. The interpolation was performed by tuning the
language model weights on a development set us-
ing perplexity metric. The development set was 500
sentences selected randomly from the IWSLT cor-
pus (Paul, 2006). The training vocabulary size for
English acoustic model is 140k and for the language
model is 300k. For the Spanish model, the train-
ing vocabulary size is 92k, while for testing, the
language model includes 370k distinct words. In
our experiments, the decoding and LM vocabularies
439
were the same.
4.2 Machine Translation
The phrase-based translation experiments reported
in this work was performed using the Moses2
toolkit (Koehn et al, 2007) for statistical machine
translation. Training the translation model starts
from the parallel sentences from which we learn
word alignments by using GIZA++ toolkit (Och
and Ney, 2003). The bidirectional word alignments
obtained using GIZA++ were consolidated by us-
ing the grow-diag-final option in Moses. Subse-
quently, we learn phrases (maximum length of 7)
from the consolidated word alignments. A lexical-
ized reordering model (msd-bidirectional-fe option
in Moses) was used for reordering the phrases in
addition to the standard distance based reordering
(distortion-limit of 6). The language models were
interpolated Kneser-Ney discounted trigram models,
all constructed using the SRILM toolkit (Stolcke,
2002). Minimum error rate training (MERT) was
performed on a development set to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses.
The parallel corpus for phrase-based transla-
tion was obtained from a variety of sources: eu-
roparl (Koehn, 2005), jrc-acquis corpus (Steinberger
et al, 2006), opensubtitle corpus (Tiedemann and
Lars Nygaard, 2004), web crawling as well as hu-
man translation. The statistics of the data used for
English-Spanish is shown in Table 1. About 30% of
the training data was obtained from the Web (Ran-
garajan Sridhar et al, 2011). The development set
(identical to the one used in ASR) was used in
MERT training as well as perplexity based optimiza-
tion of the interpolated language model. The lan-
guage model for MT and ASR was constructed from
identical data.
4.3 Text-to-speech synthesis
The translated sentence from the machine trans-
lation component is synthesized using the AT&T
Natural VoicesTM text-to-speech synthesis en-
gine (Beutnagel et al, 1999). The system uses unit
selection synthesis with half phones as the basic
2http://www.statmt.org/moses
en-es
Data statistics en es
# Sentences 7792118 7792118
# Words 98347681 111006109
Vocabulary 501450 516906
Table 1: Parallel data used for training translation
models
units. The database was recorded by professional
speakers of the language. We are currently using fe-
male voices for English as well as Spanish.
5 SIP Communication Framework for
Real-time S2S Translation
The SIP communication framework for real-time
language translation comprises of three main com-
ponents. Session Initiation Protocol (SIP) is becom-
ing the de-facto standard for signaling control for
streaming applications such as Voice over IP. We
present a SIP communication framework that uses
Real-time Transport Protocol (RTP) for packetiz-
ing multimedia content and User Datagram Proto-
col (UDP) for delivering the content. In this work,
the content we focus on is speech and text infor-
mation exchanged between two speakers in a cross-
lingual dialog. For two users conversing in two dif-
ferent languages (e.g., English and Spanish), the me-
dia channels between them will be established as
shown in Figure 1. In Figure 1, each client (UA) is
responsible for recognition, translation, and synthe-
sis of one language input. E.g., the English-Spanish
UA recognizes English text, converts it into Spanish,
and produces output Spanish audio. Similarly, the
Spanish-English UA is responsible for recognition
of Spanish speech input, converting it into English,
and producing output English audio. We describe
the underlying architecture of the system below.
5.1 Architecture
1. End point SIP user agents: These are the SIP
end points that exchange SIP signaling mes-
sages with the SIP Application server (AS) for
call control.
2. SIP User Agents: Provide a SIP interface to the
core AT&T WATSONSM engine that incorpo-
rates acoustic and language models for speech
440
SIP UA
(en->es)
SIP UA
(es->en)
     APP    
 SERVER
   Caller 
(English)
  Callee 
(Spanish)
Caller Eng Audio
Callee English Audio (Translated)
Callee Spanish Audio
Caller Spanish Audio (Translated)
C
a
l
l
e
r
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
e
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
e
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
e
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
r
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
C
a
l
l
e
e
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
r
 
E
n
g
l
i
s
h
 
T
e
x
t
 
 
 
 
 
(
R
e
c
o
g
n
i
z
e
d
)
C
a
l
l
e
r
 
S
p
a
n
i
s
h
 
T
e
x
t
 
 
 
 
 
(
T
r
a
n
s
l
a
t
e
d
)
SIP Channel for Signaling Setup and Text (recognized + translated)
Media Channel for RTP Audio
Figure 1: SIP communication framework used for real-time speech-to-speech translation. The example
shows the setup between two participants in English(en) and Spanish (es)
recognition.
3. SIP Application Server (AS): A standard SIP
B2BUA (back to back user agent) that receives
SIP signaling messages and forwards them to
the intended destination. The machine transla-
tion component (server running Moses (Koehn
et al, 2007)) is invoked from the AS.
In our communication framework, the SIP AS re-
ceives a call request from the calling party. The AS
infers the language preference of the calling party
from the user profile database and forwards the call
to the called party. Based on the response, AS in-
fers the language preference of the called party from
the user profile database. If the languages of the
calling and called parties are different, the AS in-
vites two SIP UAs into the call context. The AS ex-
changes media parameters derived from the calling
and called party SIP messages with that of the SIP
UAs. The AS then forwards the media parameters
of the UAs to the end user SIP agents.
The AS, the end user SIP UAs, and the SIP UAs
are all RFC 3261 SIP standard compliant. The end
user SIP UAs are developed using PJSIP stack that
uses PJMedia for RTP packetization of audio and
network transmission. For our testing, we have
implemented the end user SIP UAs to run on Ap-
ple IOS devices. The AS is developed using E4SS
(Echarts for SIP Servlets) software and deployed on
Sailfin Java container. It is deployed on a Linux box
installed with Cent OS version 5. The SIP UAs are
written in python for interfacing with external SIP
devices, and use proprietary protocol for interfacing
with the core AT&T WATSONSM engine.
6 Typical Call Flow
Figure 2 shows the typical call flow involved in set-
ting up the cross-lingual dialog. The caller chooses
the number of the callee from the address book or
enters it using the keypad. Subsequently, the call is
initiated and the underlying SIP channels are estab-
lished to facilitate the call. The users can then con-
verse in their native language with the hypotheses
displayed in an IM-like fashion. The messages of
the caller appear on the left side of the screen while
those of the callee appear on the right. Both the
recognition and translation hypotheses are displayed
incrementally for each side of the conversation. In
our experiments, the caller and the callee naturally
followed a protocol of listening to the other party?s
synthesized output before speaking once they were
accustomed to the interface. One of the issues dur-
ing speech recognition is that, the user can poten-
tially start speaking as the TTS output from the other
441
Figure 2: Illustration of call flow. The call is established using SIP and the real-time conversation appears
in the bubbles in a manner similar to Instant Messaging. For illustration purposes, the caller (Spanish) and
callee (English) are assumed to have set their language preferences in the setup menu.
participant is being played. We address the feedback
problem from the TTS output by muting the micro-
phone when TTS output is played.
7 Dialog Data
The system described above provides a natural way
to collect cross-lingual dialog data. We used our
system to collect a corpus of 40 scripted dialogs in
English and Spanish. A bilingual (English-Spanish)
speaker created dialog scenarios in the travel and
hospitality domain and the scripted dialog was used
as reference material in the call. Two subjects partic-
ipated in the data collection, a male English speaker
and female Spanish speaker. The subjects were in-
structed to read the lines verbatim. However, due to
ASR errors, the subjects had to repeat or improvise
few turns (about 10%) to sustain the dialog. The av-
erage number of turns per scenario in the collected
corpus is 13; 6 and 7 turns per scenario for English
and Spanish, respectively. An example dialog be-
tween two speakers is shown in Table 2.
8 Experiments
In this section, we describe speech translation ex-
periments performed on the dialog corpus collected
through our system. We present baseline results fol-
lowed by results of incremental translation.
8.1 Baseline Experiments
The models described in Section 4 were used to es-
tablish baseline results on the dialog corpus. No
A: Hello, I am calling from room four twenty one
the T.V. is not working. Do you think you can send
someone to fix it please?
B: Si, Sen?or enseguida enviamos a alguien para que
la arregle. Si no le cambiaremos de habitacio?n.
A: Thank you very much.
B: Estamos aqu para servirle. Lla?menos si necesita
algo ma?s.
Table 2: Example of a sample dialog scenario.
contextual information was used in these experi-
ments, i.e., the audio utterances were decoded in-
dependently. The ASR WER for English and Span-
ish sides of the dialogs is shown in Figure 3. The
average WER for English and Spanish side of the
conversations is 27.73% and 22.83%, respectively.
The recognized utterances were subsequently trans-
lated using the MT system described above. The
MT performance in terms of Translation Edit Rate
(TER) (Snover et al, 2006) and BLEU (Papineni
et al, 2002) is shown in Figure 4. The MT per-
formance is shown across all the turns for both ref-
erence transcriptions and ASR output. The results
show that the performance of the Spanish-English
MT model is better in comparison to the English-
Spanish model on the dialog corpus. The perfor-
mance on ASR input drops by about 18% compared
to translation on reference text.
442
08.5
17.0
25.5
34.0
Reference ASR
23.87
28.21
26.96
33.58
B
L
E
U
Spanish-English
English-Spanish
0
17.5
35.0
52.5
70.0
Reference ASR
63.42
59.19
55.34
47.26
T
E
R
 
Spanish-English
English-Spanish
Figure 4: TER (%) and BLEU of English-Spanish and Spanish-English MT models on reference transcripts
and ASR output
Figure 3: WER (%) of English and Spanish acoustic
models on the dialog corpus
8.2 Segmentation of ASR output for MT
Turn taking in a dialog typically involves the sub-
jects speaking one or more utterances in a turn.
Since, machine translation systems are trained on
chunked parallel texts (40 words or less), it is ben-
eficial to segment the ASR hypotheses before trans-
lation. Previous studies have shown significant im-
provements in translation performance through the
segmentation of ASR hypotheses (Matusov et al,
2007). We experimented with the notion of seg-
mentation defined by silence frames in the ASR out-
put. A threshold of 8-10 frames (100 ms) was found
to be suitable for segmenting the ASR output into
sentence chunks. We did not use any lexical fea-
tures for segmenting the turns. The BLEU scores for
different silence thresholds used in segmentation is
shown in Figure 5. The BLEU scores improvement
for Spanish-English is 1.6 BLEU points higher than
the baseline model using no segmentation. The im-
provement for English-Spanish is smaller but statis-
tically significant. Analysis of the dialogs revealed
that the English speaker tended to speak his turns
without pausing across utterance chunks while the
Spanish speaker paused a lot more. The results in-
dicate that in a typical dialog interaction, if the par-
ticipants observe inter-utterance pause (80-100 ms)
within a turn, it serves as a good marker for segmen-
tation. Further, exploiting such information can po-
tentially result in improvements in MT performance
as the model is typically trained on sentence level
parallel text.
12.0
13.8
15.6
17.4
19.2
21.0
22.8
24.6
26.4
28.2
30.0
50 80 110 140 170 200 500
25.21 25.20
24.86
24.80
24.34
24.27
23.87
28.76 28.76
28.21
28.18
27.76
27.62
26.96
B
L
E
U
Silence threshold for segmentation (ms)
Figure 5: BLEU score of English-Spanish and
Spanish-English MT models on the ASR output us-
ing silence segmentation
8.3 Incremental Speech Translation Results
Figure 6 shows the BLEU score for incremental
speech translation described in Section 3. In the fig-
ure, partial refers to Eq. 2, continue-partial refers to
Eq. 3 and complete refers to Eq. 4. The continue-
partials option was exercised by using the continue-
443
02.78
5.56
8.33
11.11
13.89
16.67
19.44
22.22
25.00
10 20 30 40 50 60 70 80 90 100 200 300 400 500 600 700 800 900 1000
B
L
E
U
Speech Recognizer timeouts (msec)
Partial (Eq. 1)
Moses ?continue-partial? (Eq. 2)
Complete (Eq. 3)
Figure 6: BLEU score (Spanish-English) for incremental speech translation across varying timeout periods
in the speech recognizer
partial-translation parameter in Moses (Koehn et al,
2007). The partial hypotheses are generated as a
function of speech recognizer timeouts. Timeout is
defined as the time interval with which the speech
recognizer generates partial hypotheses. For each
timeout interval, the speech recognizer may or may
not generate a partial result based on the search path
at that instant in time. As the timeout interval in-
creases, the performance of incremental translation
approaches that of non-incremental translation. The
key is to choose an operating point such that the
user perception of latency is minimal with accept-
able BLEU score. It is interesting that very good
performance can be attained at a timeout of 500 ms
in comparison with non-incremental speech trans-
lation, i.e., the latency can be reduced in half with
acceptable translation quality. The continue-partial
option in Moses performs slightly better than the
partial case as it conditions the decision on prior
source input as well as translation.
In Table 3, we present the latency measurements
of the various components in our framework. We do
not have a row for ASR since it is not possible to get
the start time for each recognition run as the RTP
packets are continuously flowing in the SIP frame-
work. The latency between various system compo-
nents is very low (5-30 ms). While the average time
taken for translation (incremental) is ? 100 ms, the
TTS takes the longest time as it is non-incremental
in the current work. It can also been seen that the
average time taken for generating incremental MT
output is half that of TTS that is non-incremental.
The overall results show that the communication in
our SIP-based framework has low latency.
Components Caller Callee Average
ASR output to MT input 6.8 0.1 3.4
MT 100.4 108.8 104.6
MT output to TTS 22.1 33.1 27.6
TTS 246 160.3 203.1
Table 3: Latency measurements (in ms) for the S2S
components in the real-time SIP framework.
9 Conclusion
In this paper, we introduced the problem of incre-
mental speech-to-speech translation and presented
first of a kind two-way real-time speech-to-speech
translation system based on SIP that incorporates
the notion of incrementality. We presented details
about the SIP framework and demonstrated the typ-
ical call flow in our application. We also presented
a dialog corpus collected using our framework and
benchmarked the performance of the system. Our
framework allows for incremental speech transla-
tion and can provide low latency translation. We
are currently working on improving the accuracy of
incremental translation. We are also exploring new
algorithms for performing reordering aware incre-
mental speech-to-speech translation, i.e., translating
source phrases such that text-to-speech synthesis can
be rendered incrementally.
444
References
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and
A. Syrdal. 1999. The AT&T Next-Gen TTS sys-
tem. In Proceedings of Joint Meeting of ASA, EAA
and DEGA.
E. Bocchieri. 1993. Vector quantization for the efficient
computation of continuous density likelihoods. Pro-
ceedings of ICASSP.
Charles L. Chen and T. V. Raman. 2008. Axsjax: a talk-
ing translation bot using google im: bringing web-2.0
applications to life. In Proceedings of the 2008 inter-
national cross-disciplinary conference on Web acces-
sibility (W4A).
O. Furuse and H. Iida. 1996. Incremental translation uti-
lizing constituent boundary patterns. In Proc. of Col-
ing ?96.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani Tur, Andrej Ljolje, and Sarangarajan
Parthasarathy. 2004. The AT&T Watson Speech Rec-
ognizer. Technical report, September.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
Tu?r, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
F. Metze, J. McDonough, H. Soltau, A. Waibel, A. Lavie,
S. Burger, C. Langley, L. Levin, T. Schultz, F. Pianesi,
R. Cattoni, G. Lazzari, N. Mana, and E. Pianta. 2002.
The NESPOLE! speech-to-speech translation system.
M. Mohri, F. Pereira, and M. Riley. 1997. Att
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL.
M. Paul. 2006. Overview of the iwslt 2006 evaluation
campaign. In Proceedings of the International Work-
shop of Spoken Language Translation, Kyoto, Japan.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
B. Sankaran, A. Grewal, and A. Sarkar. 2010. Incre-
mental decoding for phrase-based statistical machine
translation. In Proceedings of the fifth Workshop on
Statistical Machine Translation and Metrics.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of AMTA.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of ICSLP.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of LREC.
Wolfgang Wahlster, editor. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer.
A. Waibel, A. N. Jain, A. E. McNair, H. Saito, A. G.
Hauptmann, and J. Tebelskis. 1991. JANUS: a
speech-to-speech translation system using connection-
ist and symbolic processing strategies. In Proceedings
of ICASSP, pages 793?796, Los Alamitos, CA, USA.
A. Waibel, A. Badran, A. W. Black, R. Frederk-
ing, G. Gates, A. Lavie, L. Levin, K. Lenzo,
L. M. Tomokiyo, J. Reichert, T. Schultz, W. Dorcas,
M. Woszczyna, and J. Zhang. 2003. Speechalator:
two-way speech-to-speech translation on a consumer
PDA. In Proceedings of the European Conference on
Speech Communication and Technology, pages 369?
372.
B. Zhou, Y. Gao, J. Sorenson, D. Dechelotte, and
M. Picheny. 2003. A hand-held speech-to-speech
translation system. In Proceedings of ASRU.
445
Proceedings of NAACL-HLT 2013, pages 230?238,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Segmentation Strategies for Streaming Speech Translation
Vivek Kumar Rangarajan Sridhar, John Chen, Srinivas Bangalore
Andrej Ljolje, Rathinavelu Chengalvarayan
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ 07932
vkumar,jchen,srini,alj,rathi@research.att.com
Abstract
The study presented in this work is a first ef-
fort at real-time speech translation of TED
talks, a compendium of public talks with dif-
ferent speakers addressing a variety of top-
ics. We address the goal of achieving a sys-
tem that balances translation accuracy and la-
tency. In order to improve ASR performance
for our diverse data set, adaptation techniques
such as constrained model adaptation and vo-
cal tract length normalization are found to be
useful. In order to improve machine transla-
tion (MT) performance, techniques that could
be employed in real-time such as monotonic
and partial translation retention are found to
be of use. We also experiment with inserting
text segmenters of various types between ASR
and MT in a series of real-time translation ex-
periments. Among other results, our experi-
ments demonstrate that a good segmentation
is useful, and a novel conjunction-based seg-
mentation strategy improves translation qual-
ity nearly as much as other strategies such
as comma-based segmentation. It was also
found to be important to synchronize various
pipeline components in order to minimize la-
tency.
1 Introduction
The quality of automatic speech-to-text and speech-
to-speech (S2S) translation has improved so signifi-
cantly over the last several decades that such systems
are now widely deployed and used by an increasing
number of consumers. Under the hood, the individ-
ual components such as automatic speech recogni-
tion (ASR), machine translation (MT) and text-to-
speech synthesis (TTS) that constitute a S2S sys-
tem are still loosely coupled and typically trained
on disparate data and domains. Nevertheless, the
models as well as the pipeline have been optimized
in several ways to achieve tasks such as high qual-
ity offline speech translation (Cohen, 2007; Kings-
bury et al, 2011; Federico et al, 2011), on-demand
web based speech and text translation, low-latency
real-time translation (Wahlster, 2000; Hamon et al,
2009; Bangalore et al, 2012), etc. The design of a
S2S translation system is highly dependent on the
nature of the audio stimuli. For example, talks, lec-
tures and audio broadcasts are typically long and re-
quire appropriate segmentation strategies to chunk
the input signal to ensure high quality translation.
In contrast, single utterance translation in several
consumer applications (apps) are typically short and
can be processed without the need for additional
chunking. Another key parameter in designing a
S2S translation system for any task is latency. In
offline scenarios where high latencies are permit-
ted, several adaptation strategies (speaker, language
model, translation model), denser data structures (N-
best lists, word sausages, lattices) and rescoring pro-
cedures can be utilized to improve the quality of
end-to-end translation. On the other hand, real-
time speech-to-text or speech-to-speech translation
demand the best possible accuracy at low latencies
such that communication is not hindered due to po-
tential delay in processing.
In this work, we focus on the speech translation
of talks. We investigate the tradeoff between accu-
racy and latency for both offline and real-time trans-
lation of talks. In both these scenarios, appropriate
segmentation of the audio signal as well as the ASR
hypothesis that is fed into machine translation is crit-
ical for maximizing the overall translation quality of
the talk. Ideally, one would like to train the models
on entire talks. However, such corpora are not avail-
able in large amounts. Hence, it is necessary to con-
230
form to appropriately sized segments that are similar
to the sentence units used in training the language
and translation models. We propose several non-
linguistic and linguistic segmentation strategies for
the segmentation of text (reference or ASR hypothe-
ses) for machine translation. We address the prob-
lem of latency in real-time translation as a function
of the segmentation strategy; i.e., we ask the ques-
tion ?what is the segmentation strategy that maxi-
mizes the number of segments while still maximiz-
ing translation accuracy??.
2 Related Work
Speech translation of European Parliamentary
speeches has been addressed as part of the TC-
STAR project (Vilar et al, 2005; Fu?gen et al, 2006).
The project focused primarily on offline translation
of speeches. Simultaneous translation of lectures
and speeches has been addressed in (Hamon et al,
2009; Fu?gen et al, 2007). However, the work fo-
cused on a single speaker in a limited domain. Of-
fline speech translation of TED1 talks has been ad-
dressed through the IWSLT 2011 and 2012 evalua-
tion tracks. The talks are from a variety of speakers
with varying dialects and cover a range of topics.
The study presented in this work is the first effort on
real-time speech translation of TED talks. In com-
parison with previous work, we also present a sys-
tematic study of the accuracy versus latency tradeoff
for both offline and real-time translation on the same
dataset.
Various utterance segmentation strategies for of-
fline machine translation of text and ASR output
have been presented in (Cettolo and Federico, 2006;
Rao et al, 2007; Matusov et al, 2007). The work
in (Fu?gen et al, 2007; Fu?gen and Kolss, 2007)
also examines the impact of segmentation on of-
fline speech translation of talks. However, the real-
time analysis in that work is presented only for
speech recognition. In contrast with previous work,
we tackle the latency issue in simultaneous transla-
tion of talks as a function of segmentation strategy
and present some new linguistic and non-linguistic
methodologies. We investigate the accuracy versus
latency tradeoff across translation of reference text,
utterance segmented speech recognition output and
1http://www.ted.com
partial speech recognition hypotheses.
3 Problem Formulation
The basic problem of text translation can be formu-
lated as follows. Given a source (French) sentence
f = fJ1 = f1, ? ? ? , fJ , we aim to translate it into
target (English) sentence e? = e?I1 = e?1, ? ? ? , e?I .
e?(f) = arg max
e
Pr(e|f) (1)
If, as in talks, the source text (reference or ASR hy-
pothesis) is very long, i.e., J is large, we attempt
to break down the source string into shorter se-
quences, S = s1 ? ? ? sk ? ? ? sQs , where each sequence
sk = [fjkfjk+1 ? ? ? fj(k+1)?1], j1 = 1, jQs+1 =
J + 1. Let the translation of each foreign sequence
sk be denoted by tk = [eikeik+1 ? ? ? ei(k+1)?1], i1 =
1, iQs+1 = I
?
+ 12. The segmented sequences can
be translated using a variety of techniques such as
independent chunk-wise translation or chunk-wise
translation conditioned on history as shown in Eqs. 2
and 3, respectively. In Eq. 3, t?i denotes the best
translation for source sequence si.
e?(f) = arg max
t1
Pr(t1|s1) ? ? ? arg max
tk
Pr(tk|sk)
(2)
e?(f) = arg max
t1
Pr(t1|s1) arg max
t2
Pr(t2|s2, s1, t
?
1)
? ? ? arg max
tk
Pr(tk|s1, ? ? ? , sk, t
?
1, ? ? ? , t
?
k?1)
(3)
Typically, the hypothesis e? will be more accurate
than e? for long texts as the models approximating
Pr(e|f) are conventionally trained on short text seg-
ments. In Eqs. 2 and 3, the number of sequences Qs
is inversely proportional to the time it takes to gen-
erate partial target hypotheses. Our main focus in
this work is to obtain a segmentation S such that the
quality of translation is maximized with minimal la-
tency. The above formulation for automatic speech
recognition is very similar except that the foreign
string f? = f?J1 = f?1, ? ? ? , f?J? is obtained by decoding
the input speech signal.
2The segmented and unsegmented talk may not be equal in
length, i.e., I 6= I
?
231
Model Language Vocabulary #words #sents Corpora
Acoustic Model en 46899 2611144 148460 1119 TED talks
ASR Language Model en 378915 3398460155 151923101 Europarl, WMT11 Gigaword, WMT11 News crawl
WMT11 News-commentary, WMT11 UN, IWSLT11 TED training
Parallel text en 503765 76886659 7464857 IWSLT11 TED training talks, Europarl, JRC-ACQUIS
Opensubtitles, Web data
MT es 519354 83717810 7464857
Language Model es 519354 83717810 7464857 Spanish side of parallel text
Table 1: Statistics of the data used for training the speech translation models.
4 Data
In this work, we focus on the speech translation
of TED talks, a compendium of public talks from
several speakers covering a variety of topics. Over
the past couple of years, the International Work-
shop on Spoken Language Translation (IWSLT) has
been conducting the evaluation of speech translation
on TED talks for English-French. We leverage the
IWSLT TED campaign by using identical develop-
ment (dev2010) and test data (tst2010). However,
English-Spanish is our target language pair as our
internal projects are cater mostly to this pair. As a
result, we created parallel text for English-Spanish
based on the reference English segments released as
part of the evaluation (Cettolo et al, 2012).
We also harvested the audio data from the TED
website for building an acoustic model. A total
of 1308 talks in English were downloaded, out of
which we used 1119 talks recorded prior to Decem-
ber 2011. We split the stereo audio file and dupli-
cated the data to account for any variations in the
channels. The data for the language models was also
restricted to that permitted in the IWSLT 2011 eval-
uation. The parallel text for building the English-
Spanish translation model was obtained from sev-
eral corpora: Europarl (Koehn, 2005), JRC-Acquis
corpus (Steinberger et al, 2006), Opensubtitle cor-
pus (Tiedemann and Lars Nygaard, 2004), Web
crawling (Rangarajan Sridhar et al, 2011) as well as
human translation of proprietary data. Table 1 sum-
marizes the data used in building the models. It is
important to note that the IWSLT evaluation on TED
talks is completely offline. In this work, we perform
the first investigation into the real-time translation of
these talks.
5 Speech Translation Models
In this section, we describe the acoustic, language
and translation models used in our experiments.
5.1 Acoustic and Language Model
We use the AT&T WATSONSM speech recog-
nizer (Goffin et al, 2004). The speech recogni-
tion component consisted of a three-pass decoding
approach utilizing two acoustic models. The mod-
els used three-state left-to-right HMMs representing
just over 100 phonemes. The phonemes represented
general English, spelled letters and head-body-tail
representation for the eleven digits (with ?zero? and
?oh?). The pronunciation dictionary used the appro-
priate phoneme subset, depending on the type of the
word. The models had 10.5k states and 27k HMMs,
trained on just over 300k utterances, using both of
the stereo channels. The baseline model training was
initialized with several iterations of ML training, in-
cluding two builds of context dependency trees, fol-
lowed by three iterations of Minimum Phone Error
(MPE) training.
The Vocal Tract Length Normalization (VTLN)
was applied in two different ways. One was esti-
mated on an utterance level, and the other at the talk
level. No speaker clustering was attempted in train-
ing. The performance at test time was comparable
for both approaches on the development set. Once
the warps were estimated, after five iterations, the
ML trained model was updated using MPE training.
Constrained model adaptation (CMA) was applied
to the warped features and the adapted features were
recognized in the final pass with the VTLN model.
All the passes used the same LM. For offline recog-
nition the warps, and the CMA adaptation, are per-
formed at the talk level. For the real-time speech
translation experiments, we used the VTLN model.
232
The English language model was built using the
permissible data in the IWSLT 2011 evaluation. The
texts were normalized using a variety of cleanup,
number and spelling normalization techniques and
filtered by restricting the vocabulary to the top
375000 types; i.e., any sentence containing a to-
ken outside the vocabulary was discarded. First, we
removed extraneous characters beyond the ASCII
range followed by removal of punctuations. Sub-
sequently, we normalized hyphenated words and re-
moved words with more than 25 characters. The re-
sultant text was normalized using a variety of num-
ber conversion routines and each corpus was fil-
tered by restricting the vocabulary to the top 150000
types; i.e., any sentence containing a token outside
the vocabulary was discarded. The vocabulary from
all the corpora was then consolidated and another
round of filtering to the top 375000 most frequent
types was performed. The OOV rate on the TED
dev2010 set is 1.1%. We used the AT&T FSM
toolkit (Mohri et al, 1997) to train a trigram lan-
guage model (LM) for each component (corpus). Fi-
nally, the component language models were interpo-
lated by minimizing the perplexity on the dev2010
set. The results are shown in Table 2.
Accuracy (%)
Model dev2010 test2010
Baseline MPE 75.5 73.8
VTLN 78.8 77.4
CMA 80.5 80.0
Table 2: ASR word accuracies on the IWSLT data
sets.3
5.2 Translation Model
We used the Moses toolkit (Koehn et al, 2007) for
performing statistical machine translation. Mini-
mum error rate training (MERT) was performed on
the development set (dev2010) to optimize the fea-
ture weights of the log-linear model used in trans-
lation. During decoding, the unknown words were
preserved in the hypotheses. The data used to train
the model is summarized in Table 1.
3We used the standard NIST scoring package as we did not
have access to the IWSLT evaluation server that may normalize
and score differently
We also used a finite-state implementation of
translation without reordering. Reordering can pose
a challenge in real-time S2S translation as the text-
to-speech synthesis is monotonic and cannot retract
already synthesized speech. While we do not ad-
dress the text-to-speech synthesis of target text in
this work, we perform this analysis as a precursor
to future work. We represent the phrase transla-
tion table as a weighted finite state transducer (FST)
and the language model as a finite state acceptor
(FSA). The weight on the arcs of the FST is the
dot product of the MERT weights with the transla-
tion scores. In addition, a word insertion penalty
was also applied to each word to penalize short hy-
potheses. The decoding process consists of compos-
ing all possible segmentations of an input sentence
with the phrase table FST and language model, fol-
lowed by searching for the best path. Our FST-based
translation is the equivalent of phrase-based transla-
tion in Moses without reordering. We present re-
sults using the independent chunk-wise strategy and
chunk-wise translation conditioned on history in Ta-
ble 3. The chunk-wise translation conditioned on
history was performed using the continue-partial-
translation option in Moses.
6 Segmentation Strategies
The output of ASR for talks is a long string of
words with no punctuation, capitalization or seg-
mentation markers. In most offline ASR systems,
the talk is first segmented into short utterance-like
audio segments before passing them to the decoder.
Prior work has shown that additional segmentation
of ASR hypotheses of these segments may be nec-
essary to improve translation quality (Rao et al,
2007; Matusov et al, 2007). In a simultaneous
speech translation system, one can neither find the
optimal segmentation of the entire talk nor tolerate
high latencies associated with long segments. Con-
sequently, it is necessary to decode the incoming au-
dio incrementally as well as segment the ASR hy-
potheses appropriately to maximize MT quality. We
present a variety of linguistic and non-linguistic seg-
mentation strategies for segmenting the source text
input into MT. In our experiments, they are applied
to different inputs including reference text, ASR 1-
best hypothesis for manually segmented audio and
233
incremental ASR hypotheses from entire talks.
6.1 Non-linguistic segmentation
The simplest method is to segment the incoming text
according to length in number of words. Such a pro-
cedure can destroy semantic context but has little to
no overhead in additional processing. We experi-
ment with segmenting the text according to word
window sizes of length 4, 8, 11, and 15 (denoted
as data sets win4, win8, win11, win15, respectively
in Table 3). We also experiment with concatenating
all of the text from one TED talk into a single chunk
(complete talk).
A novel hold-output model was also developed in
order to segment the input text. Given a pair of par-
allel sentences, the model segments the source sen-
tence into minimally sized chunks such that crossing
links and links of one target word to many source
words in an optimal GIZA++ alignment (Och and
Ney, 2003) occur only within individual chunks.
The motivation behind this model is that if a segment
s0 is input at time t0 to an incremental MT system,
it can be translated right away without waiting for a
segment si that is input at a later time ti, ti > 0. The
hold-output model detects these kinds of segments
given a sequence of English words that are input
from left to right. A kernel-based SVM was used to
develop this model. It tags a token t in the input with
either the label HOLD, meaning to chunk it with the
next token, or the label OUTPUT, meaning to output
the chunk constructed from the maximal consecutive
sequence of tokens preceding t that were all tagged
as HOLD. The model considers a five word and POS
window around the target token t. Unigram, bigram,
and trigram word and POS features based upon this
window are used for classification. Training and de-
velopment data for the model was derived from the
English-Spanish TED data (see Table 1) after run-
ning it through GIZA++. Accuracy of the model on
the development set was 66.62% F-measure for the
HOLD label and 82.75% for the OUTPUT label.
6.2 Linguistic segmentation
Since MT models are trained on parallel text sen-
tences, we investigate segmenting the source text
into sentences. We also investigate segmenting the
text further by predicting comma separated chunks
within sentences. These tasks are performed by
training a kernel-based SVM (Haffner et al, 2003)
on a subset of English TED data. This dataset con-
tained 1029 human-transcribed talks consisting of
about 103,000 sentences containing about 1.6 mil-
lion words. Punctuation in this dataset was normal-
ized as follows. Different kinds of sentence ending
punctuations were transformed into a uniform end of
sentence marker. Double-hyphens were transformed
into commas. Commas already existing in the input
were kept while all other kinds of punctuation sym-
bols were deleted. A part of speech (POS) tagger
was applied to this input. For speed, a unigram POS
tagger was implemented which was trained on the
Penn Treebank (Marcus et al, 1993) and used or-
thographic features to predict the POS of unknown
words. The SVM-based punctuation classifier relies
on a five word and POS window in order to classify
the target word. Specifically, token t0 is classified
given as input the window t?2t?1tot1t2. Unigram,
bigram, and trigram word and POS features based on
this window were used for classification. Accuracy
of the classifier on the development set was 60.51%
F-measure for sentence end detection and 43.43%
F-measure for comma detection. Subsequently, data
sets pred-sent (sentences) and pred-punct (comma-
separated chunks) were obtained. Corresponding to
these, two other data sets ref-sent and ref-punct were
obtained based upon gold-standard punctuations in
the reference.
Besides investigating the use of comma-separated
segments, we investigated other linguistically moti-
vated segments. These included conjunction-word
based segments. These segments are separated at
either conjunction (e.g. ?and,? ?or?) or sentence-
ending word boundaries. Conjunctions were iden-
tified using the unigram POS tagger. F-measure
performance for detecting conjunctions by the tag-
ger on the development set was quite high, 99.35%.
As an alternative, text chunking was performed
within each sentence, with each chunk correspond-
ing to one segment. Text chunks are non-recursive
syntactic phrases in the input text. We investi-
gated segmenting the source into text chunks us-
ing TreeTagger, a decision-tree based text chun-
ker (Schmid, 1994). Initial sets of text chunks
were created by using either gold-standard sentence
boundaries or boundaries detected using the punc-
tuation classifier, yielding the data sets chunk-ref-
234
Reference text ASR 1-best
BLEU Mean BLEU Mean
Segmentation Segmentation Independent chunk-wise chunk-wise #words Independent chunk-wise chunk-wise #words
type strategy FST Moses with history per segment FST Moses with history per segment
win4 22.6 21.0 25.5 3.9?0.1 17.7 17.1 20.0 3.9?0.1
win8 26.6 26.2 28.2 7.9?0.3 20.6 20.9 22.3 7.9?0.2
Non-linguistic win11 27.2 27.4 29.2 10.9? 0.3 21.5 21.8 23.1 10.9?0.4
win15 28.5 28.5 29.4 14.9?0.6 22.3 22.8 23.3 14.9?0.7
ref-hold 13.3 14.0 17.1 1.6?1.9 12.7 13.1 17.5 1.5?1.0
pred-hold 15.9 15.7 16.3 2.2?1.9 12.6 12.9 17.4 1.5?1.0
complete talk 23.8 23.9 ? 2504 18.8 19.2 ? 2515
ref-sent 30.6 31.5 30.5 16.7?11.8 24.3 25.1 24.4 17.0?11.6
ref-punct 30.4 31.5 30.3 7.1?5.3 24.2 25.1 24.1 8.7?6.1
pred-punct 30.6 31.5 30.4 8.7?8.8 24.1 25.0 24.0 8.8?6.8
conj-ref-eos 30.5 31.5 30.2 11.2?7.5 24.1 24.9 24.0 11.5?7.7
conj-pred-eos 30.3 31.2 30.3 10.9?7.9 24.0 24.8 24.0 11.4?8.5
chunk-ref-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.2 16.9 1.4?0.7
Linguistic lgchunk1-ref-punct 21.0 21.8 25.1 1.7?1.0 16.9 17.4 19.6 1.8?1.0
lgchunk2-ref-punct 22.4 23.1 26.0 2.1?1.1 17.9 18.4 20.4 2.1?1.1
lgchunk3-ref-punct 24.3 25.1 27.4 2.5?1.7 19.2 19.9 21.3 2.5?1.7
chunk-pred-punct 17.9 18.9 21.4 1.3?0.7 14.5 15.1 16.9 1.4?0.7
lgchunk1-pred-punct 21.2 21.9 25.2 1.8?1.0 16.7 17.2 19.7 1.8?1.0
lgchunk2-pred-punct 22.6 23.1 26.0 2.1?1.2 17.7 18.3 20.5 2.1?1.2
lgchunk3-pred-punct 24.5 25.3 27.4 2.6?1.8 19.1 20.0 21.3 2.5?1.7
Table 3: BLEU scores at the talk level for reference text and ASR 1-best for various segmentation strategies.
The ASR 1-best was performed on manually segmented audio chunks provided in tst2010 set.
punct and chunk-pred-punct. Chunk types included
NC (noun chunk), VC (verb chunk), PRT (particle),
and ADVC (adverbial chunk).
Because these chunks may not provide sufficient
context for translation, we also experimented with
concatenating neighboring chunks of certain types
to form larger chunks. Data sets lgchunk1 concate-
nate together neighboring chunk sequences of the
form NC, VC or NC, ADVC, VC, intended to cap-
ture as single chunks instances of subject and verb.
In addition to this, data sets lgchunk2 capture chunks
such as PC (prepositional phrase) and VC followed
by VC (control and raising verbs). Finally, data sets
lgchunk3 capture as single chunks VC followed by
NC and optionally followed by PRT (verb and its di-
rect object).
Applying the conjunction segmenter after the
aforementioned punctuation classifier in order to de-
tect the ends of sentences yields the data set conj-
pred-eos. Applying it on sentences derived from the
gold-standard punctuations yields the data set conj-
ref-eos. Finally, applying the hold-output model to
sentences derived using the punctuation classifier
produces the data set pred-hold. Obtaining English
sentences tagged with HOLD and OUTPUT directly
from the output of GIZA++ on English-Spanish sen-
tences in the reference produces the data set ref-hold.
The strategies containing the keyword ref for ASR
simply means that the ASR hypotheses are used in
place of the gold reference text.
 0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
 0
 100
0
 200
0
 300
0
 400
0
 500
0
 600
0
processing time per token (sec)
ASR
 tim
eou
t (m
s)ASR
+MT
 (BL
EU)
10.0
11.7
12.6
13.3
13.7
14.0
14.1
14.3
14.6
14.7
14.7
14.8
ASR
+Pu
nct 
Seg
+MT
 (BL
EU)
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
15.1
Figure 1: Latencies and BLEU scores for tst2010 set
using incremental ASR decoding and translation
We also performed real-time speech translation by
using incremental speech recognition, i.e., the de-
coder returns partial hypotheses that, independent of
235
the pruning during search, will not change in the
future. Figure 1 shows the plot for two scenarios:
one in which the partial hypotheses are sent directly
to machine translation and another where the best
segmentation strategy pred-punct is used to segment
the partial output before sending it to MT. The plot
shows the BLEU scores as a function of ASR time-
outs used to generate the partial hypotheses. Fig-
ure 1 also shows the average latency involved in in-
cremental speech translation.
7 Discussion
The BLEU scores for the segmentation strategies
over ASR hypotheses was computed at the talk level.
Since the ASR hypotheses do not align with the
reference source text, it is not feasible to evalu-
ate the translation performance using the gold refer-
ence. While other studies have used an approximate
edit distance algorithm for resegmentation of the hy-
potheses (Matusov et al, 2005), we simply concate-
nate all the segments and perform the evaluation at
the talk level.
The hold segmentation strategy yields the poor-
est translation performance. The significant drop in
BLEU score can be attributed to relatively short seg-
ments (2-4 words) that was generated by the model.
The scheme oversegments the text and since the
translation and language models are trained on sen-
tence like chunks, the performance is poor. For ex-
ample, the input text the sea should be translated
as el mar, but instead the hold segmenter chunks it
as the?sea which MT?s chunk translation renders as
el?el mar. It will be interesting to increase the span
of the hold strategy to subsume more contiguous se-
quences and we plan to investigate this as part of
future work.
The chunk segmentation strategy yields quite poor
translation performance. In general, it does not
make the same kinds of errors that the hold strat-
egy makes; for example, the input text the sea will
be treated as one NC chunk by the chunk seg-
mentation strategy, leading MT to translate it cor-
rectly as el mar. The short chunk sizes of chunk
lead to other kinds of errors. For example, the in-
put text we use will be chunked into the NC we
and the VC use, which will be translated incor-
rectly as nosotros?usar; the infinitive usar is se-
lected rather than the properly conjugated form us-
amos. However, there is a marked improvement in
translation accuracy with increasingly larger chunk
sizes (lgchunk1, lgchunk2, and lgchunk3). Notably,
lgchunk3 yields performance that approaches that of
win8 with a chunk size that is one third of win8?s.
The conj-pred-eos and pred-punct strategies work
the best, and it can be seen that the average seg-
ment length (8-12 words) generated in both these
schemes is very similar to that used for training the
models. It is also about the average latency (4-5
seconds) that can be tolerated in cross-lingual com-
munication, also known as ear-voice span (Lederer,
1978). The non-linguistic segmentation using fixed
word length windows also performs well, especially
for the longer length windows. However, longer
windows (win15) increase the latency and any fixed
length window typically destroys the semantic con-
text. It can also be seen from Table 3 that translat-
ing the complete talk is suboptimal in comparison
with segmenting the text. This is primarily due to
bias on sentence length distributions in the training
data. Training models on complete talks is likely to
resolve this issue. Contrasting the use of reference
segments as input to MT (ref-sent, ref-punct, conj-
ref-eos) versus the use of predicted segments (pred-
sent, pred-punct, conj-pred-eos, respectively), it is
interesting to note that the MT accuracies never dif-
fered greatly between the two, despite the noise in
the set of predicted segments.
The performance of the real-time speech transla-
tion of TED talks is much lower than the offline sce-
nario. First, we use only a VTLN model as perform-
ing CMA adaptation in a real-time scenario typically
increases latency. Second, the ASR language model
is trained on sentence-like units and decoding the en-
tire talk with this LM is not optimal. A language
model trained on complete talks will be more appro-
priate for such a framework and we are investigating
this as part of current work.
Comparing the accuracies of different speech
translation strategies, Table 3 shows that pred-punct
performs the best. When embedded in an incremen-
tal MT speech recognition system, Figure 1 shows
that it is more accurate than the system that sends
partial ASR hypotheses directly to MT. This advan-
tage decreases, however, when the ASR timeout pa-
rameter is increased to more than five or six sec-
236
onds. In terms of latency, Figure 1 shows that the
addition of the pred-punct segmenter into the incre-
mental system introduces a significant delay. About
one third of the increase in delay can be attributed
to merely maintaining the two word lookahead win-
dow that the segmenter?s classifier needs to make
decisions. This is significant because this kind of
window has been used quite frequently in previous
work on simultaneous translation (cf. (Fu?gen et al,
2007)), and yet to our knowledge this penalty asso-
ciated with this configuration was never mentioned.
The remaining delay can be attributed to the long
chunk sizes that the segmenter produces. An inter-
esting aspect of the latency curve associated with the
segmenter in Figure 1 is that there are two peaks at
ASR timeouts of 2,500 and 4,500 ms, and that the
lowest latency is achieved at 3,000 ms rather than at
a smaller value. This may be attributed to the fact
that the system is a pipeline consisting of ASR, seg-
menter, and MT, and that 3,000 ms is roughly the
length of time to recite comma-separated chunks.
Consequently, the two latency peaks appear to cor-
respond with ASR producing segments that are most
divergent with segments that the segmenter pro-
duces, leading to the most pipeline ?stalls.? Con-
versely, the lowest latency occurs when the timeout
is set so that ASR?s segments most resemble the seg-
menter?s output to MT.
8 Conclusion
We investigated various approaches for incremen-
tal speech translation of TED talks, with the aim
of producing a system with high MT accuracy and
low latency. For acoustic modeling, we found that
VTLN and CMA adaptation were useful for increas-
ing the accuracy of ASR, leading to a word accuracy
of 80% on TED talks used in the IWSLT evalua-
tion track. In our offline MT experiments retention
of partial translations was found useful for increas-
ing MT accuracy, with the latter being slightly more
helpful. We experimented with several linguistic
and non-linguistic strategies for text segmentation
before translation. Our experiments indicate that a
novel segmentation into conjunction-separated sen-
tence chunks resulted in accuracies almost as high
and latencies almost as short as comma-separated
sentence chunks. They also indicated that signifi-
cant noise in the detection of sentences and punc-
tuation did not seriously impact the resulting MT
accuracy. Experiments on real-time simultaneous
speech translation using partial recognition hypothe-
ses demonstrate that introduction of a segmenter in-
creases MT accuracy. They also showed that in or-
der to reduce latency it is important for buffers in dif-
ferent pipeline components to be synchronized so as
to minimize pipeline stalls. As part of future work,
we plan to extend the framework presented in this
work for performing speech-to-speech translation.
We also plan to address the challenges involved in
S2S translation across languages with very different
word order.
Acknowledgments
We would like to thank Simon Byers for his help
with organizing the TED talks data.
References
S. Bangalore, V. K. Rangarajan Sridhar, P. Kolan,
L. Golipour, and A. Jimenez. 2012. Real-time in-
cremental speech-to-speech translation of dialogs. In
Proceedings of NAACL:HLT, June.
M. Cettolo and M. Federico. 2006. Text segmentation
criteria for statistical machine translation. In Proceed-
ings of the 5th international conference on Advances
in Natural Language Processing.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web Inventory of Transcribed and Translated Talks. In
Proceedings of EAMT.
J. Cohen. 2007. The GALE project: A description and
an update. In Proceedings of ASRU Workshop.
M. Federico, L. Bentivogli, M. Paul, and S. Stu?ker. 2011.
Overview of the IWSLT 2011 evaluation campaign. In
Proceedings of IWSLT.
C. Fu?gen and M. Kolss. 2007. The influence of utterance
chunking on machine translation performance. In Pro-
ceedings of Interspeech.
C. Fu?gen, M. Kolss, D. Bernreuther, M. Paulik, S. Stuker,
S. Vogel, and A. Waibel. 2006. Open domain speech
recognition & translation: Lectures and speeches. In
Proceedings of ICASSP.
C. Fu?gen, A. Waibel, and M. Kolss. 2007. Simultaneous
translation of lectures and speeches. Machine Trans-
lation, 21:209?252.
V. Goffin, C. Allauzen, E. Bocchieri, D. Hakkani-Tu?r,
A. Ljolje, and S. Parthasarathy. 2004. The AT&T
Watson Speech Recognizer. Technical report, Septem-
ber.
237
P. Haffner, G. Tu?r, and J. Wright. 2003. Optimizing
svms for complex call classification. In Proceedings
of ICASSP?03.
O. Hamon, C. Fu?gen, D. Mostefa, V. Arranz, M. Kolss,
A. Waibel, and K. Choukri. 2009. End-to-end evalua-
tion in simultaneous translation. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), March.
B. Kingsbury, H. Soltau, G. Saon, S. Chu, Hong-Kwang
Kuo, L. Mangu, S. Ravuri, N. Morgan, and A. Janin.
2011. The IBM 2009 GALE Arabic speech translation
system. In Proceedings of ICASSP.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, Shen W.,
C. Moran, R. Zens, C. J. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit.
M. Lederer. 1978. Simultaneous interpretation: units of
meaning and other features. In D. Gerver and H. W.
Sinaiko, editors, Language interpretation and commu-
nication, pages 323?332. Plenum Press, New York.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn treebank. Computational Linguistics,
19(2):313?330.
E. Matusov, G. Leusch, O. Bender, and H. Ney. 2005.
Evaluating machine translation output with automatic
sentence segmentation. In Proceedings of IWSLT.
E. Matusov, D. Hillard, M. Magimai-Doss, D. Hakkani-
Tu?r, M. Ostendorf, and H. Ney. 2007. Improving
speech translation with automatic boundary predic-
tion. In Proceedings of Interspeech.
M. Mohri, F. Pereira, and M. Riley. 1997. At&t
general-purpose finite-state machine software tools,
http://www.research.att.com/sw/tools/fsm/.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
V. K. Rangarajan Sridhar, L. Barbosa, and S. Bangalore.
2011. A scalable approach to building a parallel cor-
pus from the Web. In Proceedings of Interspeech.
S. Rao, I. Lane, and T. Schultz. 2007. Optimizing sen-
tence segmentation for spoken language translation. In
Proceedings of Interspeech.
H. Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Pro-
cessing.
R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Er-
javec, and D. Tufis. 2006. The JRC-Acquis: A multi-
lingual aligned parallel corpus with 20+ languages. In
Proceedings of LREC.
J. Tiedemann and L. Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of LREC.
D. Vilar, E. Matusov, S. Hasan, R. Zens, and H. Ney.
2005. Statistical machine translation of European par-
liamentary speeches. In Proceedings of MT Summit.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
238
Proceedings of the ACL 2010 System Demonstrations, pages 60?65,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Speech-driven Access to the Deep Web on Mobile Devices
Taniya Mishra and Srinivas Bangalore
AT&T Labs - Research
180 Park Avenue
Florham Park, NJ 07932 USA.
{taniya,srini}@research.att.com.
Abstract
The Deep Web is the collection of infor-
mation repositories that are not indexed
by search engines. These repositories are
typically accessible through web forms
and contain dynamically changing infor-
mation. In this paper, we present a sys-
tem that allows users to access such rich
repositories of information on mobile de-
vices using spoken language.
1 Introduction
The World Wide Web (WWW) is the largest
repository of information known to mankind. It
is generally agreed that the WWW continues to
significantly enrich and transform our lives in un-
precedent ways. Be that as it may, the WWW that
we encounter is limited by the information that
is accessible through search engines. Search en-
gines, however, do not index a large portion of
WWW that is variously termed as the Deep Web,
Hidden Web, or Invisible Web.
Deep Web is the information that is in propri-
etory databases. Information in such databases is
usually more structured and changes at higher fre-
quency than textual web pages. It is conjectured
that the Deep Web is 500 times the size of the
surface web. Search engines are unable to index
this information and hence, unable to retrieve it
for the user who may be searching for such infor-
mation. So, the only way for users to access this
information is to find the appropriate web-form,
fill in the necessary search parameters, and use it
to query the database that contains the information
that is being searched for. Examples of such web
forms include, movie, train and bus times, and air-
line/hotel/restaurant reservations.
Contemporaneously, the devices to access infor-
mation have moved out of the office and home en-
vironment into the open world. The ubiquity of
mobile devices has made information access an
any time, any place activity. However, informa-
tion access using text input on mobile devices is te-
dious and unnatural because of the limited screen
space and the small (or soft) keyboards. In addi-
tion, by the mobile nature of these devices, users
often like to use them in hands-busy environments,
ruling out the possibility of typing text. Filling
web-forms using the small screens and tiny key-
boards of mobile devices is neither easy nor quick.
In this paper, we present a system, Qme!, de-
signed towards providing a spoken language inter-
face to the Deep Web. In its current form, Qme!
provides a unifed interface onn iPhone (shown in
Figure 1) that can be used by users to search for
static and dynamic questions. Static questions are
questions whose answers to these questions re-
main the same irrespective of when and where the
questions are asked. Examples of such questions
are What is the speed of light?, When is George
Washington?s birthday?. For static questions, the
system retrieves the answers from an archive of
human generated answers to questions. This en-
sures higher accuracy for the answers retrieved (if
found in the archive) and also allows us to retrieve
related questions on the user?s topic of interest.
Figure 1: Retrieval results for static and dynamic
questions using Qme!
Dynamic questions are questions whose an-
swers depend on when and where they are asked.
Examples of such questions are What is the stock
price of General Motors?, Who won the game last
night?, What is playing at the theaters near me?.
60
The answers to dynamic questions are often part of
the DeepWeb. Our system retrieves the answers to
such dynamic questions by parsing the questions
to retrieve pertinent search keywords, which are in
turn used to query information databases accessi-
ble over the Internet using web forms. However,
the internal distinction between dynamic and static
questions, and the subsequent differential treat-
ment within the system is seamless to the user. The
user simply uses a single unified interface to ask a
question and receive a collection of answers that
potentially address her question directly.
The layout of the paper is as follows. In Sec-
tion 2, we present the system architecture. In
Section 3, we present bootstrap techniques to dis-
tinguish dynamic questions from static questions,
and evaluate the efficacy of these techniques on a
test corpus. In Section 4, we show how our system
retrieves answers to dynamic questions. In Sec-
tion 5, we show how our system retrieves answers
to static questions. We conclude in Section 6.
2 Speech-driven Question Answer
System
Speech-driven access to information has been a
popular application deployed by many compa-
nies on a variety of information resources (Mi-
crosoft, 2009; Google, 2009; YellowPages, 2009;
vlingo.com, 2009). In this prototype demonstra-
tion, we describe a speech-driven question-answer
application. The system architecture is shown in
Figure 2.
The user of this application provides a spoken
language query to a mobile device intending to
find an answer to the question. The speech recog-
nition module of the system recognizes the spo-
ken query. The result from the speech recognizer
can be either a single-best string or a weighted
word lattice.1 This textual output of recognition is
then used to classify the user query either as a dy-
namic query or a static query. If the user query is
static, the result of the speech recognizer is used to
search a large corpus of question-answer pairs to
retrieve the relevant answers. The retrieved results
are ranked using tf.idf based metric discussed in
Section 5. If the user query is dynamic, the an-
swers are retrieved by querying a web form from
the appropriate web site (e.g www.fandango.com
for movie information). In Figure 1, we illustrate
the answers that Qme!returns for static and dy-
1For this paper, the ASR used to recognize these utter-
ances incorporates an acoustic model adapted to speech col-
lected from mobile devices and a four-gram language model
that is built from the corpus of questions.
namic questions.
Lattice1?best
Q&A corpus
ASRSpeech
Dynamic
Classify
from WebRetrieve
Rank
Search
Ranked ResultsMatch
Figure 2: The architecture of the speech-driven
question-answering system
2.1 Demonstration
In the demonstration, we plan to show the users
static and dynamic query handling on an iPhone
using spoken language queries. Users can use the
iphone and speak their queries using an interface
provided by Qme!. A Wi-Fi access spot will make
this demonstation more compelling.
3 Dynamic and Static Questions
As mentioned in the introduction, dynamic ques-
tions require accessing the hidden web through a
web form with the appropriate parameters. An-
swers to dynamic questions cannot be preindexed
as can be done for static questions. They depend
on the time and geographical location of the ques-
tion. In dynamic questions, there may be no ex-
plicit reference to time, unlike the questions in the
TERQAS corpus (Radev and Sundheim., 2002)
which explicitly refer to the temporal properties
of the entities being questioned or the relative or-
dering of past and future events.
The time-dependency of a dynamic question
lies in the temporal nature of its answer. For exam-
ple, consider the question, What is the address of
the theater White Christmas is playing at in New
York?. White Christmas is a seasonal play that
plays in New York every year for a few weeks
in December and January, but not necessarily at
the same theater every year. So, depending when
this question is asked, the answer will be differ-
ent. If the question is asked in the summer, the
answer will be ?This play is not currently playing
anywhere in NYC.? If the question is asked dur-
ing December, 2009, the answer might be different
than the answer given in December 2010, because
the theater at which White Christmas is playing
differs from 2009 to 2010.
There has been a growing interest in tempo-
ral analysis for question-answering since the late
1990?s. Early work on temporal expressions iden-
61
tification using a tagger culminated in the devel-
opment of TimeML (Pustejovsky et al, 2001),
a markup language for annotating temporal ex-
pressions and events in text. Other examples in-
clude, QA-by-Dossier with Constraints (Prager et
al., 2004), a method of improving QA accuracy by
asking auxiliary questions related to the original
question in order to temporally verify and restrict
the original answer. (Moldovan et al, 2005) detect
and represent temporally related events in natural
language using logical form representation. (Sa-
quete et al, 2009) use the temporal relations in a
question to decompose it into simpler questions,
the answers of which are recomposed to produce
the answers to the original question.
3.1 Question Classification: Dynamic and
Static Questions
We automatically classify questions as dynamic
and static questions. The answers to static ques-
tions can be retrieved from the QA archive. To an-
swer dynamic questions, we query the database(s)
associated with the topic of the question through
web forms on the Internet. We first use a topic
classifier to detect the topic of a question followed
by a dynamic/static classifier trained on questions
related to a topic, as shown in Figure 3. For the
question what movies are playing around me?,
we detect it is a movie related dynamic ques-
tion and query a movie information web site (e.g.
www.fandango.com) to retrieve the results based
on the user?s GPS information.
Dynamic questions often contain temporal in-
dexicals, i.e., expressions of the form today, now,
this week, two summers ago, currently, recently,
etc. Our initial approach was to use such signal
words and phrases to automatically identify dy-
namic questions. The chosen signals were based
on annotations in TimeML. We also included spa-
tial indexicals, such as here and other clauses that
were observed to be contained in dynamic ques-
tions such as cost of, and how much is in the list of
signal phrases. These signals words and phrases
were encoded into a regular-expression-based rec-
ognizer.
This regular-expression based recognizer iden-
tified 3.5% of our dataset ? which consisted of
several million questions ? as dynamic. The type
of questions identified were What is playing in
the movie theaters tonight?, What is tomorrow?s
weather forecast for LA?, Where can I go to get
Thai food near here? However, random samplings
of the same dataset, annotated by four independent
human labelers, indicated that on average 13.5%
of the dataset is considered dynamic. This shows
that the temporal and spatial indexicals encoded as
a regular-expression based recognizer is unable to
identify a large percentage of the dynamic ques-
tions.
This approach leaves out dynamic questions
that do not contain temporal or spatial indexicals.
For example, What is playing at AMC Loew?s?, or
What is the score of the Chargers and Dolphines
game?. For such examples, considering the tense
of the verb in question may help. The last two ex-
amples are both in the present continuous tense.
But verb tense does not help for a question such
as Who got voted off Survivor?. This question is
certainly dynamic. The information that is most
likely being sought by this question is what is the
name of the person who got voted off the TV show
Survivor most recently, and not what is the name
of the person (or persons) who have gotten voted
off the Survivor at some point in the past.
Knowing the broad topic (such as movies, cur-
rent affairs, and music) of the question may be
very useful. It is likely that there may be many
dynamic questions about movies, sports, and fi-
nance, while history and geography may have few
or none. This idea is bolstered by the following
analysis. The questions in our dataset are anno-
tated with a broad topic tag. Binning the 3.5%
of our dataset identified as dynamic questions by
their broad topic produced a long-tailed distribu-
tion. Of the 104 broad topics, the top-5 topics con-
tained over 50% of the dynamic questions. These
top five topics were sports, TV and radio, events,
movies, and finance.
Considering the issues laid out in the previ-
ous section, our classification approach is to chain
two machine-learning-based classifiers: a topic
classifier chained to a dynamic/static classifier, as
shown in Figure 3. In this architecture, we build
one topic classifier, but several dynamic/static
classifiers, each trained on data pertaining to one
broad topic.
Figure 3: Chaining two classifiers
We used supervised learning to train the topic
62
classifier, since our entire dataset is annotated by
human experts with topic labels. In contrast, to
train a dynamic/static classifier, we experimented
with the following three different techniques.
Baseline: We treat questions as dynamic if they
contain temporal indexicals, e.g. today, now, this
week, two summers ago, currently, recently, which
were based on the TimeML corpus. We also in-
cluded spatial indexicals such as here, and other
substrings such as cost of and how much is. A
question is considered static if it does not contain
any such words/phrases.
Self-training with bagging: The general self-
training with bagging algorithm (Banko and Brill,
2001). The benefit of self-training is that we can
build a better classifier than that built from the
small seed corpus by simply adding in the large
unlabeled corpus without requiring hand-labeling.
Active-learning: This is another popular method
for training classifiers when not much annotated
data is available. The key idea in active learning
is to annotate only those instances of the dataset
that are most difficult for the classifier to learn to
classify. It is expected that training classifiers us-
ing this method shows better performance than if
samples were chosen randomly for the same hu-
man annotation effort.
We used the maximum entropy classifier in
LLAMA (Haffner, 2006) for all of the above clas-
sification tasks. We have chosen the active learn-
ing classifier due to its superior performance and
integrated it into the Qme! system. We pro-
vide further details about the learning methods in
(Mishra and Bangalore, 2010).
3.2 Experiments and Results
3.2.1 Topic Classification
The topic classifier was trained using a training
set consisting of over one million questions down-
loaded from the web which were manually labeled
by human experts as part of answering the ques-
tions. The test set consisted of 15,000 randomly
selected questions. Word trigrams of the question
are used as features for a MaxEnt classifier which
outputs a score distribution on all of the 104 pos-
sible topic labels. The error rate results for models
selecting the top topic and the top two topics ac-
cording to the score distribution are shown in Ta-
ble 1. As can be seen these error rates are far lower
than the baseline model of selecting the most fre-
quent topic.
Model Error Rate
Baseline 98.79%
Top topic 23.9%
Top-two topics 12.23%
Table 1: Results of topic classification
3.2.2 Dynamic/static Classification
As mentioned before, we experimented with
three different approaches to bootstrapping a dy-
namic/static question classifier. We evaluated
these methods on a 250 question test set drawn
from the broad topic of Movies. The error rates
are summarized in Table 2. We provide further de-
tails of this experiment in (Mishra and Bangalore,
2010).
Training approach Lowest Error rate
Baseline 27.70%
?Supervised? learning 22.09%
Self-training 8.84%
Active-learning 4.02%
Table 2: Best Results of dynamic/static classifica-
tion
4 Retrieving answers to dynamic
questions
Following the classification step outlined in Sec-
tion 3.1, we know whether a user query is static or
dynamic, and the broad category of the question.
If the question is dynamic, then our system per-
forms a vertical search based on the broad topic
of the question. In our system, so far, we have in-
corporated vertical searches on three broad topics:
Movies, Mass Transit, and Yellow Pages.
For each broad topic, we have identified a few
trusted content aggregator websites. For example,
for dynamic questions related to Movies-related
dynamic user queries, www.fandango.com is
a trusted content aggregator website. Other such
trusted content aggregator websites have been
identified for Mass Transit related and for Yellow-
pages related dynamic user queries. We have also
identified the web-forms that can be used to search
these aggregator sites and the search parameters
that these web-forms need for searching. So, given
a user query, whose broad category has been deter-
mined and which has been classified as a dynamic
query by the system, the next step is to parse the
query to obtain pertinent search parameters.
The search parameters are dependent on the
broad category of the question, the trusted con-
tent aggregator website(s), the web-forms associ-
ated with this category, and of course, the content
63
of the user query. From the search parameters, a
search query to the associated web-form is issued
to search the related aggregator site. For exam-
ple, for a movie-related query, What time is Twi-
light playing in Madison, New Jersey?, the per-
tinent search parameters that are parsed out are
movie-name: Twilight, city: Madison, and state:
New Jersey, which are used to build a search string
that Fandango?s web-form can use to search the
Fandango site. For a yellow-pages type of query,
Where is the Saigon Kitchen in Austin, Texas?, the
pertinent search parameters that are parsed out are
business-name: Saigon Kitchen, city: Austin, and
state: Texas, which are used to construct a search
string to search the Yellowpages website. These
are just two examples of the kinds of dynamic user
queries that we encounter. Within each broad cat-
egory, there is a wide variety of the sub-types of
user queries, and for each sub-type, we have to
parse out different search parameters and use dif-
ferent web-forms. Details of this extraction are
presented in (Feng and Bangalore, 2009).
It is quite likely that many of the dynamic
queries may not have all the pertinent search pa-
rameters explicitly outlined. For example, a mass
transit query may be When is the next train to
Princeton?. The bare minimum search parameters
needed to answer this query are a from-location,
and a to-location. However, the from-location is
not explicitly present in this query. In this case,
the from-location is inferred using the GPS sensor
present on the iPhone (on which our system is built
to run). Depending on the web-form that we are
querying, it is possible that we may be able to sim-
ply use the latitude-longitude obtained from the
GPS sensor as the value for the from-location pa-
rameter. At other times, we may have to perform
an intermediate latitude-longitude to city/state (or
zip-code) conversion in order to obtain the appro-
priate search parameter value.
Other examples of dynamic queries in which
search parameters are not explicit in the query, and
hence, have to be deduced by the system, include
queries such as Where is XMen playing? and How
long is Ace Hardware open?. In each of these
examples, the user has not specified a location.
Based on our understanding of natural language,
in such a scenario, our system is built to assume
that the user wants to find a movie theatre (or, is
referring to a hardware store) nearwhere he is cur-
rently located. So, the system obtains the user?s
location from the GPS sensor and uses it to search
for a theatre (or locate the hardware store) within
a five-mile radius of her location.
In the last few paragraphs, we have discussed
how we search for answers to dynamic user
queries from the hidden web by using web-forms.
However, the search results returned by these web-
forms usually cannot be displayed as is in our
Qme! interface. The reason is that the results are
often HTML pages that are designed to be dis-
played on a desktop or a laptop screen, not a small
mobile phone screen. Displaying the results as
they are returned from search would make read-
ability difficult. So, we parse the HTML-encoded
result pages to get just the answers to the user
query and reformat it, to fit the Qme! interface,
which is designed to be easily readable on the
iPhone (as seen in Figure 1).2
5 Retrieving answers to static questions
Answers to static user queries ? questions whose
answers do not change over time ? are retrieved
in a different way than answers to dynamic ques-
tions. A description of how our system retrieves
the answers to static questions is presented in this
section.
0
how:qa25/c1
old:qa25/c2
is:qa25/c3
obama:qa25/c4
old:qa150/c5
how:qa12/c6
obama:qa450/c7
is:qa1450/c8
Figure 4: An example of an FST representing the
search index.
5.1 Representing Search Index as an FST
To obtain results for static user queries, we
have implemented our own search engine using
finite-state transducers (FST), in contrast to using
Lucene (Hatcher and Gospodnetic., 2004) as it is
a more efficient representation of the search index
that allows us to consider word lattices output by
ASR as input queries.
The FST search index is built as follows. We
index each question-answer (QA) pair from our
repository ((qi, ai), qai for short) using the words
(wqi) in question qi. This index is represented as
a weighted finite-state transducer (SearchFST) as
shown in Figure 4. Here a word wqi (e.g old) is the
input symbol for a set of arcs whose output sym-
bol is the index of the QA pairs where old appears
2We are aware that we could use SOAP (Simple Object
Access Protocol) encoding to do the search, however not all
aggregator sites use SOAP yet.
64
in the question. The weight of the arc c(wqi ,qi) is
one of the similarity based weights discussed in
Section 4.1. As can be seen from Figure 4, the
words how, old, is and obama contribute a score to
the question-answer pair qa25; while other pairs,
qa150, qa12, qa450 are scored by only one of
these words.
5.2 Search Process using FSTs
A user?s speech query, after speech recogni-
tion, is represented as a finite state automaton
(FSA, either 1-best or WCN), QueryFSA. The
QueryFSA is then transformed into another FSA
(NgramFSA) that represents the set of n-grams
of the QueryFSA. In contrast to most text search
engines, where stop words are removed from the
query, we weight the query terms with their idf val-
ues which results in a weighted NgramFSA. The
NgramFSA is composed with the SearchFST and
we obtain all the arcs (wq, qawq , c(wq ,qawq )) where
wq is a query term, qawq is a QA index with the
query term and, c(wq ,qawq ) is the weight associ-
ated with that pair. Using this information, we
aggregate the weight for a QA pair (qaq) across
all query words and rank the retrieved QAs in the
descending order of this aggregated weight. We
select the top N QA pairs from this ranked list.
The query composition, QA weight aggregation
and selection of top N QA pairs are computed
with finite-state transducer operations as shown
in Equations 1 and 23. An evaluation of this
search methodology on word lattices is presented
in (Mishra and Bangalore, 2010).
D = pi2(NgramFSA ? SearchFST ) (1)
TopN = fsmbestpath(fsmdeterminize(D), N)
(2)
6 Summary
In this demonstration paper, we have presented
Qme!, a speech-driven question answering system
for use on mobile devices. The novelty of this sys-
tem is that it provides users with a single unified
interface for searching both the visible and the hid-
den web using the most natural input modality for
use on mobile phones ? spoken language.
7 Acknowledgments
We would like to thank Junlan Feng, Michael
Johnston and Mazin Gilbert for the help we re-
ceived in putting this system together. We would
3We have dropped the need to convert the weights into the
real semiring for aggregation, to simplify the discussion.
also like to thank ChaCha for providing us the data
included in this system.
References
M. Banko and E. Brill. 2001. Scaling to very very
large corpora for natural language disambiguation.
In Proceedings of the 39th annual meeting of the as-
sociation for computational linguistics: ACL 2001,
pages 26?33.
J. Feng and S. Bangalore. 2009. Effects of word con-
fusion networks on voice search. In Proceedings of
EACL-2009, Athens, Greece.
Google, 2009. http://www.google.com/mobile.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239?261.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Ac-
tion (In Action series). Manning Publications Co.,
Greenwich, CT, USA.
Microsoft, 2009. http://www.live.com.
T. Mishra and S. Bangalore. 2010. Qme!: A speech-
based question-answering system on mobile de-
vices. In Proceedings of NAACL-HLT.
D. Moldovan, C. Clark, and S. Harabagiu. 2005. Tem-
poral context representation and reasoning. In Pro-
ceedings of the 19th International Joint Conference
on Artificial Intelligence, pages 1009?1104.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Ques-
tion answering using constraint satisfaction: Qa-by-
dossier-with-contraints. In Proceedings of the 42nd
annual meeting of the association for computational
linguistics: ACL 2004, pages 574?581.
J. Pustejovsky, R. Ingria, R. Saur??, J. Casta no,
J. Littman, and R. Gaizauskas., 2001. The language
of time: A reader, chapter The specification languae
? TimeML. Oxford University Press.
D. Radev and B. Sundheim. 2002. Using timeml in
question answering. Technical report, Brandies Uni-
versity.
E. Saquete, J. L. Vicedo, P. Mart??nez-Barco, R. Mu noz,
and H. Llorens. 2009. Enhancing qa systems with
complex temporal question processing capabilities.
Journal of Artificial Intelligence Research, 35:775?
811.
vlingo.com, 2009.
http://www.vlingomobile.com/downloads.html.
YellowPages, 2009. http://www.speak4it.com.
65
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 609?613,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Predicting Relative Prominence in Noun-Noun Compounds
Taniya Mishra
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
taniya@research.att.com
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Abstract
There are several theories regarding what in-
fluences prominence assignment in English
noun-noun compounds. We have developed
corpus-driven models for automatically pre-
dicting prominence assignment in noun-noun
compounds using feature sets based on two
such theories: the informativeness theory and
the semantic composition theory. The eval-
uation of the prediction models indicate that
though both of these theories are relevant, they
account for different types of variability in
prominence assignment.
1 Introduction
Text-to-speech synthesis (TTS) systems stand to
gain in improved intelligibility and naturalness if
we have good control of the prosody. Typically,
prosodic labels are predicted through text analysis
and are used to control the acoustic parameters for
a TTS system. An important aspect of prosody pre-
diction is predicting which words should be prosod-
ically prominent, i.e., produced with greater en-
ergy, higher pitch, and/or longer duration than the
neighboring words, in order to indicate the for-
mer?s greater communicative salience. Appropriate
prominence assignment is crucial for listeners? un-
derstanding of the intended message. However, the
immense prosodic variability found in spoken lan-
guage makes prominence prediction a challenging
problem. A particular sub-problem of prominence
prediction that still defies a complete solution is pre-
diction of relative prominence in noun-noun com-
pounds.
Noun-noun compounds such as White House,
cherry pie, parking lot, Madison Avenue, Wall
Street, nail polish, french fries, computer program-
mer, dog catcher, silk tie, and self reliance, oc-
cur quite frequently in the English language. In a
discourse neutral context, such constructions usu-
ally have leftmost prominence, i.e., speakers produce
the left-hand noun with greater prominence than the
right-hand noun. However, a significant portion ?
about 25% (Liberman and Sproat, 1992) ? of them
are assigned rightmost prominence (such as cherry
pie, Madison Avenue, silk tie, computer program-
mer, and self reliance from the list above). What
factors influence speakers? decision to assign left or
right prominence is still an open question.
There are several different theories about rela-
tive prominence assignment in noun-noun (hence-
forth, NN) compounds, such as the structural the-
ory (Bloomfield, 1933; Marchand, 1969; Heinz,
2004), the analogical theory (Schmerling, 1971;
Olsen, 2000), the semantic theory (Fudge, 1984;
Liberman and Sproat, 1992) and the informativeness
theory (Bolinger, 1972; Ladd, 1984).1 However, in
most studies, the different theories are examined and
applied in isolation, thus making it difficult to com-
pare them directly. It would be informative and il-
luminating to apply these theories to the same task
and the same dataset.
For this paper, we focus on two particular the-
ories, the informativeness theory and the seman-
tic composition theory. The informativeness theory
posits that the relatively more informative and un-
expected noun is given greater prominence in the
NN compound than the less informative and more
predictable noun. The semantic composition theory
posits that relative prominence assignment in NN
compounds is decided according to the semantic re-
lationship between the two nouns.
We apply these two theories to the task of pre-
dicting relative prominence in NN compounds via
statistical corpus-driven methods, within the larger
context of building a system that can predict appro-
priate prominence patterns for text-to-speech syn-
thesis. Here we are only focusing on predicting rela-
tive prominence of NN compounds in a neutral con-
text, where there are no pragmatic reasons (such as
contrastiveness or given/new distinction) for shifting
prominence.
1In-depth reviews of the different theories can be found in
Plag (2006) and Bell and Plag (2010).
609
2 Informativeness Measures
We used the following five metrics to capture the
individual and relative informativeness of nouns in
each NN compound:
? Unigram Predictability (UP): Defined as the
predictability of a word given a text corpus, it
is measured as the log probability of the word
in the text corpus. Here, we use the maximum
likelihood formulation of this measure.
UP = log
Freq(wi)
?
i Freq(wi)
(1)
This is a very simple measure of word informa-
tiveness that has been shown to be effective in
a similar task (Pan and McKeown, 1999).
? Bigram Predictability (BP): Defined as the pre-
dictability of a word given a previous word, it
is measured as the log probability of noun N2
given noun N1.
BP = log (Prob(N2 | N1)) (2)
? Pointwise Mutual Information (PMI): Defined
as a measure of how collocated two words are,
it is measured as the log of the ratio of probabil-
ity of the joint event of the two words occurring
and the probability of them occurring indepen-
dent of each other.
PMI = log
Prob(N1, N2)
Prob(N1)Prob(N2)
(3)
? Dice Coefficient (DC): Dice is another colloca-
tion measure used in information retrieval.
DC =
2? Prob(N1, N2)
Prob(N1) + Prob(N2)
(4)
? Pointwise Kullback-Leibler Divergence (PKL):
In this context, Pointwise Kullback-Leibler di-
vergence (a formulation of relative entropy)
measures the degree to which one over-
approximates the information content of N2 by
failing to take into account the immediately
preceding word N1. (PKL values are always
negative.) A high absolute value of PKL indi-
cates that there is not much information con-
tained in N2 if N1 is taken into account. We
define PKL as
Prob(N2 | N1) log
Prob(N2 | N1)
Prob(N2)
(5)
Another way to consider PKL is as PMI nor-
malized by the predictability of N2 given N1.
All except the first the aforementioned five infor-
mativeness measures are relative measures. Of
these, PMI and Dice Coefficient are symmetric mea-
sures while Bigram Predictability and PKL are non-
symmetric (unidirectional) measures.
3 Semantic Relationship Modeling
We modeled the semantic relationship between the
two nouns in the NN compound as follows. For
each of the two nouns in each NN compound, we
maintain a semantic category vector of 26 elements.
The 26 elements are associated with 26 semantic
categories (such as food, event, act, location, arti-
fact, etc.) assigned to nouns in WordNet (Fellbaum,
1998). For each noun, each element of the semantic
category vector is assigned a value of 1, if the lem-
matized noun (i.e., the associated uninflected dic-
tionary entry) is assigned the associated semantic
category by WordNet, otherwise, the element is as-
signed a value of 0. (If a semantic category vector is
entirely populated by zeros, then that noun has not
been assigned any semantic category information by
WordNet.) We expected the cross-product of the se-
mantic category vectors of the two nouns in the NN
compound to roughly encode the possible semantic
relationships between the two nouns, which ? fol-
lowing the semantic composition theory ? corre-
lates with prominence assignment to some extent.
4 Semantic Informativeness Features
For each noun in each NN compound, we also
maintain three semantic informativeness features:
(1) Number of possible synsets associated with the
noun. A synset is a set of words that have the same
sense or meaning. (2) Left positional family size and
(3) Right positional family size. Positional family
size is the number of unique NN compounds that in-
clude the particular noun, either on the left or on the
right (Bell and Plag, 2010). These features are ex-
tracted from WordNet as well.
The intuition behind extracting synset counts and
positional family size was, once again, to measure
the relative informativeness of the nouns in NN com-
pounds. Smaller synset counts indicate more spe-
cific meaning of the noun, and thus perhaps more
information content. Larger right (or left) posi-
tional family size indicates that the noun is present
610
in the right (left) position of many possible NN com-
pounds, and thus less likely to receive higher promi-
nence in such compounds.
These features capture type-based informative-
ness, in contrast to the measures described in Sec-
tion 2, which capture token-based informativeness.
5 Experimental evaluation
For our evaluation, we used a hand-labeled corpus
of 7831 NN compounds randomly selected from the
1990 Associated Press newswire, and hand-tagged
for leftmost or rightmost prominence (Sproat, 1994).
This corpus contains 64 pairs of NN compounds that
differ in terms of capitalization but not in terms of
relative prominence assignment. It only contains
four pairs of NN compounds that differ in terms of
capitalization and in terms of relative prominence
assignment. Since there is not enough data in this
corpus to consider capitalization as a feature, we re-
moved the case information (by lowercasing the en-
tire corpora), and removed any duplicates. Of the
four pairs that differed in terms of capitalization,
we only retained the lower-cased NN compounds.
By normalizing Sproat?s hand-labeled corpus in this
way, we created a slightly smaller corpus 7767 ut-
terances that was used for the evaluation.
For each of the NN compounds in this corpus, we
computed the three aforementioned feature sets. To
compute the informativeness features, we used the
LDC English Gigaword corpus. The semantic cate-
gory vectors and the semantic informativeness fea-
tures were obtained from Wordnet. Using each of
the three feature sets individually as well as com-
bined together, we built automatic relative promi-
nence prediction models using Boostexter, a dis-
criminative classification model based on the boost-
ing family of algorithms, which was first proposed
in Freund and Schapire (1996).
Following an experimental methodology similar
to Sproat (1994), we used 88% (6835 samples) of
the corpus as training data and the remaining 12%
(932 samples) as test data. For each test case, the
output of the prediction models was either a 0 (indi-
cating that the leftmost noun receive higher promi-
nence) or a 1 (indicating that the rightmost noun re-
ceive higher prominence). We estimated the model
error of the different prediction models by comput-
ing the relative error reduction from the baseline er-
ror. The baseline error was obtained by assigning
the majority class to all test cases. We avoided over-
fitting by using 5-fold cross validation.
5.1 Results
The results of the evaluation of the different models
are presented in Table 1. In this table, INF denotes
informativeness features (Sec. 2), SRF denotes se-
mantic relationship modeling features (Sec. 3) and
SIF denotes semantic informativeness features (Sec.
4). We also present the results of building prediction
models by combining different features sets.
These results show that each of the prediction
models reduces the baseline error, thus indicating
that the different types of feature sets are each cor-
related with prominence assignment in NN com-
pounds to some extent. However, it appears that
some feature sets are more predictive. Of the indi-
vidual feature sets, SRF and INF features appear to
be more predictive than the SIF features. Combined
together, the three feature sets are most predictive,
reducing model error over the baseline error by al-
most 33% (compared to 16-22% for individual fea-
ture sets), though combining INF with SRF features
almost achieves the same reduction in baseline error.
Note that none of the three types of feature sets
that we have defined contain any direct lexical infor-
mation such as the nouns themselves or their lem-
mata. However, considering that the lexical con-
tent of the words is a rich source of information that
could have substantial predictive power, we included
the lemmata associated with the nouns in the NN
compounds as additional features to each feature set
and rebuilt the prediction models. An evaluation of
these lexically-enhanced models is shown in Table
2. Indeed, addition of the lemmatized form of the
NN compounds substantially increases the predic-
tive power of all the models. The baseline error is
reduced by almost 50% in each of the models ?
the error reduction being the greatest (53%) for the
model built by combining all three feature sets.
6 Discussion and Conclusion
Several other studies have examined the main idea of
relative prominence assignment using one or more
of the theories that we have focused on in this paper
(though the particular tasks and terminology used
were different) and found similar results. For exam-
ple, Pan and Hirschberg (2000) have used some of
the same informativeness measures (denoted by INF
above) to predict pitch accent placement in word bi-
611
Feature Av. baseline Av. model % Error
Sets error (in %) error (in %) reduction
INF 29.18 22.85 21.69
SRF 28.04 21.84 22.00
SIF 29.22 24.36 16.66
INF-SRF 28.52 19.53 31.55
INF-SIF 28.04 21.25 24.33
SRF-SIF 29.74 21.30 28.31
All 28.98 19.61 32.36
Table 1: Results of prediction models
Feature Av. baseline Av. model % Error
Sets error (in %) error (in %) reduction
INF 28.6 14.67 48.74
SRF 28.34 14.29 49.55
SIF 29.48 14.85 49.49
INF-SRF 28.16 14.81 47.45
INF-SIF 28.38 14.16 50.03
SRF-SIF 29.24 14.51 50.30
All 28.12 13.19 52.95
Table 2: Results of lexically-enhanced prediction models
grams. Since pitch accents and perception of promi-
nence are strongly correlated, their conclusion that
informativeness measures are a good predictor of
pitch accent placement agrees with our conclusion
that informativeness measures are useful predictors
of relative prominence assignment. However, we
cannot compare their results to ours directly, since
their corpus and baseline error measurement2 were
different from ours.
Our results are more directly comparable to those
shown in Sproat (1994). For the same task as we
consider in this study, besides developing a rule-
based system, Sproat also developed a statistical
corpus-based model. His feature set was developed
to model the semantic relationship between the two
nouns in the NN compound, and included the lem-
mata related to the nouns. The model was trained
and tested on the same hand-labeled corpus that we
used for this study and the baseline error was mea-
sured in the same way. So, we can directly com-
pare the results of our lexically-enhanced SRF-based
models to Sproat?s corpus-driven statistical model.
2Pan and Hirschberg present error obtained by using a
unigram-based predictability model as baseline error. It is un-
clear what is the error obtained by assigning left prominence to
all words in their database, which was our baseline error.
In his work, Sproat reported a baseline error of 30%
and a model error of 16%. The reported relative im-
provement over the baseline error in Sproat?s study
was 46.6%, while our relative improvement using
the lexically enhanced SRF based model was 49.5%,
and the relative improvement using the combined
model is 52.95%.
Type-based semantic informativeness features of
the kind that we grouped as SIF were analyzed
in Bell and Plag (2010) as potential predictors of
prominence assignment in compound nouns. Like
us, they too found such features to be predictive
of prominence assignment and that combining them
with features that model the semantic relationship in
the NN compound makes them more predictive.
7 Conclusion
The goal of the presented work was predicting rel-
ative prominence in NN compounds via statistical
corpus-driven methods. We constructed automatic
prediction models using feature sets based on two
different theories about relative prominence assign-
ment in NN compounds: the informativeness theory
and the semantic composition theory. In doing so,
we were able to compare the two theories.
Our evaluation indicates that each of these theo-
ries is relevant, though perhaps to different degrees.
This is supported by the observation that the com-
bined model (in Table 1) is substantially more pre-
dictive than any of the individual models. This indi-
cates that the different feature sets capture different
correlations, and that perhaps each of the theories
(on which the feature sets are based) account for dif-
ferent types of variability in prominence assignment.
Our results also highlight the difference between
being able to use lexical information in prominence
prediction of NN compounds, or not. Using lexical
features, we can improve prediction over the default
case (i.e., assigning prominence to the left noun in
all cases) by over 50%. But if the given input is an
out-of-vocabulary NN compound, our non-lexically
enhanced best model can still improve prediction
over the default by about 33%.
Acknowledgment We would like to thank
Richard Sproat for freely providing the dataset on
which the developed models were trained and tested.
We would also like to thank him for his advice on
this topic.
612
References
M. Bell and I. Plag. 2010. Informativeness
is a determinant of compound stress in En-
glish. Submitted for publication. Obtained from
http://www2.uni-siegen.de/?engspra/
publicat.html on February 12, 2010.
L. Bloomfield. 1933. Language, Holt, New York.
D. Bolinger. 1972. Accent is predictable (if you?re a
mind-reader). Language 48.
C. Fellbaum (editor). 1998. WordNet: An Electronic
Lexical Database, The MIT Press, Boston.
Y. Freund and R. E. Schapire, 1996. Experiments with
a new boosting alogrithm. Machine Learning: Pro-
ceedings of the Thirteenth International Conference,
pp. 148-156.
E. Fudge. 1984. English Word-Stress, Allen and Unwin,
London and Boston.
H. J. Giegerich. Compound or phrase? English noun-
plus-noun constructions and the stress criterion. In
English Language and Linguistics, 8:1?24.
R. D. Ladd, 1984. English compound stress. In Dafydd
Gibbon and Helmut Richter (eds.) Intonation, Accent
and Rhythm: Studies in 1188 Discourse Phonology,
W de Gruyter, Berlin.
M. Liberman and R. Sproat. 1992. The Stress and Struc-
ture of Modified Noun Phrases in English. In I. Sag
(ed.), Lexical Matters, pp. 131?181, CSLI Publica-
tions, Chicago, University of Chicago Press.
H. Marchand. The categories and types of present-day
English word-formation, Beck, Munich.
S. Olsen. 2000. Compounding and stress in English: A
closer look at the boundary between morphology and
syntax. Linguistische Berichte, 181:55?70.
S. Pan and J. Hirschberg. 2000. Modeling local context
for pitch accent prediction. Proceedings of the 38th
Annual Conference of the Association for Computa-
tional Linguistics (ACL-00), pp. 233-240, Hong Kong.
ACL.
S. Pan and K. McKeown. 1999. Word informativeness
and automatic pitch accent modeling. Proceedings of
the Joint SIGDAT Conference on EMNLP and VLC,
pp. 148?157.
I. Plag. 2006. The variability of compound stress in En-
glish: structural, semantic and analogical factors. En-
glish Language and Linguistics, 10.1, pp. 143?172.
R. Sproat. 1994. English Noun-Phrase Accent Prediction
for Text-to-Speech. Computer Speech and Language,
8, pp. 79?94.
R.E. Schapire, A brief introduction to boosting. In Pro-
ceedings of IJCAI, 1999.
S. F. Schmerling. 1971. A stress mess. Studies in the
Linguistic Sciences, 1:52?65.
613
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109?113,
Dublin, Ireland, August 23-24, 2014.
AT&T: The Tag&Parse Approach to Semantic Parsing of Robot Spatial
Commands
Svetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas Bangalore
AT&T Labs Research
1 AT&T Way Bedminster NJ 07921
{sveta,hjung,jchen,srini}@research.att.com
Abstract
The Tag&Parse approach to semantic
parsing first assigns semantic tags to each
word in a sentence and then parses the
tag sequence into a semantic tree. We
use statistical approach for tagging, pars-
ing, and reference resolution stages. Each
stage produces multiple hypotheses which
are re-ranked using spatial validation. We
evaluate the Tag&Parse approach on a cor-
pus of Robotic Spatial Commands as part
of the SemEval Task6 exercise. Our sys-
tem accuracy is 87.35% and 60.84% with
and without spatial validation.
1 Introduction
In this paper we describe a system participating
in the SemEval2014 Task-6 on Supervised Seman-
tic Parsing of Robotic Spatial Commands. It pro-
duces a semantic parse of natural language com-
mands addressed to a robot arm designed to move
objects on a grid surface. Each command directs
a robot to change position of an object given a
current configuration. A command uniquely iden-
tifies an object and its destination, for example
?Move the turquoise pyramid above the yellow
cube?. System output is a Robot Control Lan-
guage (RCL) parse (see Figure 1) which is pro-
cessed by the robot arm simulator. The Robot Spa-
tial Commands dataset (Dukes, 2013) is used for
training and testing.
Our system uses a Tag&Parse approach which
separates semantic tagging and semantic parsing
stages. It has four components: 1) semantic tag-
ging, 2) parsing, 3) reference resolution, and 4)
spatial validation. The first three are trained using
LLAMA (Haffner, 2006), a supervised machine
learning toolkit, on the RCL-parsed sentences.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
For semantic tagging, we train a maximum en-
tropy sequence tagger for assigning a semantic la-
bel and value to each word in a sentence, such as
type cube or color blue. For parsing, we train a
constituency parser on non-lexical RCL semantic
trees. For reference resolution, we train a maxi-
mum entropy model that identifies entities for ref-
erence tags found by previous components. All of
these components can generate multiple hypothe-
ses. Spatial validation re-ranks these hypotheses
by validating them against the input spatial con-
figuration. The top hypothesis after re-ranking is
returned by the system.
Separating tagging and parsing stages has sev-
eral advantages. A tagging stage allows the system
flexibility to abstract from possible grammatical or
spelling errors in a command. It assigns a seman-
tic category to each word in a sentence. Words not
contributing to the semantic meaning are assigned
?O? label by the tagger and are ignored in the fur-
ther processing. Words that are misspelled can po-
tentially receive a correct tag when a word simi-
larity feature is used in building a tagging model.
This will be especially important when process-
ing output of spoken commands that may contain
recognition errors.
The remainder of the paper is organized thusly.
In Section 2 we describe each of the components
used in our system. In Section 3 we describe the
results reported for SemEval2014 and evaluation
of each system component. We summarize our
findings and present future work in Section 4.
2 System
2.1 Sequence Tagging
A sequence tagging approach is used for condi-
tional inference of tags given a word sequence.
It is used for many natural language tasks, such
as part of speech (POS) and named entity tag-
ging (Toutanova and others, 2003; Carreras et al.,
2003). We train a sequence tagger for assign-
109
Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.
Word index tag label
Move 1 action move
the 2 O -
turquoise 3 color cyan
pyramid 4 type prism
above 5 relation above
the 6 O -
yellow 7 color yellow
cube 8 type cube
Table 1: Tagging labels for a sentence Move the
turquoise pyramid above the yellow cube.
ing a combined semantic tag and label (such as
type cube) to each word in a command. The tags
used for training are extracted from the leaf-level
nodes of the RCL trees. Table 2 shows tags and
labels for a sample sentence ?Move the turquoise
pyramid above the yellow cube? extracted from
the RCL parse tree (see Figure 1). In some cases,
a label is the same as a word (yellow, cube) while
in other cases, it differs (turquoise - cyan, pyramid
- prism).
We train a sequence tagger using LLAMA max-
imum entropy (maxent) classification (Haffner,
2006) to predict the combined semantic tag and
label of each word. Neighboring words, immedi-
ately neighboring semantic tags, and POS tags are
used as features, where the POS tagger is another
sequence tagging model trained on the Penn Tree-
bank (Marcus et al., 1993). We also experimented
with a tagger that assigns tags and labels in sep-
arate sequence tagging models, but it performed
poorly.
2.2 Parsing
We use a constituency parser for building RCL
trees. The input to the parser is a sequence of
tags assigned by a sequence tagger, such as ?ac-
tion color type relation color type? for the exam-
ple in Figure 1.
The parser generates multiple RCL parse tree
hypotheses sorted in the order of their likelihood.
The likelihood of a tree T given a sequence of tags
T is determined using a probabilistic context free
grammar (PCFG) G:
P (T |S) =
?
r?T
P
G
(r) (1)
The n-best parses are obtained using the CKY
algorithm, recording the n-best hyperedge back-
pointers per constituent along the lines of (Huang
and Chiang, 2005). G was obtained and P
G
was
estimated from a corpus of non-lexical RCL trees
generated by removing all nodes descendant from
the tag nodes (action, color, etc.). Parses may con-
tain empty nodes not corresponding to any tag in
the input sequence. These are hypothesized by the
parser at positions in between input tags and in-
serted as edges according to the PCFG, which has
probabilistic rules for generating empty nodes.
2.3 Reference Resolution
Reference resolution identifies the most prob-
able antecedent for each anaphor within a
text (Hirschman and Chinchor, 1997). It applies
when multiple candidates antecedents are present.
For example, in a sentence ?Pick up the red cube
standing on a grey cube and place it on top of
the yellow one?, the anaphor it has two candidate
antecedents corresponding to entity segments the
red cube and a grey cube. In our system, anaphor
and antecedents are represented by reference tags
occurring in one sentence. A reference tag is ei-
ther assigned by a sequence tagger to one of the
words (e.g. to a pronoun) or is inserted into a
tree by the parser (e.g. ellipsis). We train a bi-
nary maxent model for this task using LLAMA.
The input is a pair consisting of an anaphor and
a candidate antecedent, along with their features.
110
Features that are used include the preceding and
following words as well as the tags/labels of both
the anaphor and candidate antecedent. The refer-
ence resolution component selects the antecedent
for which the model returns the highest score.
2.4 Spatial Validation
SemEval2014 Task6 provided a spatial planner
which takes an RCL command as an input and
determines if that command is executable in the
given spatial context. At each step described in
2.1?2.3, due to the statistical nature of our ap-
proach, multiple hypotheses can be easily com-
puted with different confidence values. We used
the spatial planner to validate the final output RCL
commands from the three steps by checking if the
RCLs are executable or not. We generate multi-
ple tagger output hypotheses. For each tagger out-
put hypothesis, we generate multiple parser out-
put hypotheses. For each parser output hypothe-
sis, we generate multiple reference resolution out-
put hypotheses. The resulting output hypotheses
are ranked in the order of confidence scores with
the highest tagging output scores ranked first, fol-
lowed by the parsing output scores, and, finally,
reference resolution output scores. The system re-
turns the result of the top scored command that is
valid according to the spatial validator.
In many applications, there can be a tool or
method to validate tag/parse/reference outputs
fully or partially. Note that in our system the val-
idation is performed after all output is generated.
Tightly coupled validation, such as checking va-
lidity of a tagged entity or a parse constituent,
could help in computing hypotheses at each step
(e.g., feature values based on possible entities or
actions) and it remains as future work.
3 Results
In this section, we present evaluation results on the
three subsets of the data summarized in Table 3. In
the TEST2500 data set, the models are trained on
the initial 2500 sentences of the Robot Commands
Treebank and evaluated on the last 909 sentences
(this corresponds to the data split of the SemEval
task). In TEST500 data set, the models are trained
on the initial 500 sentences of the training set and
evaluated on the last 909 test sentences. We re-
port these results to analyze the models? perfor-
mance on a reduced training size. In DEV2500
data set, models are trained on 90% of the initial
2500 sentences and evaluated on 10% of the 2500
# Dataset Avg # hyp Accuracy
1 TEST2500 1-best 1 86.0%
2 TEST2500 max-5 3.34 95.2%
3 TEST500 1-best 1 67.9%
4 TEST500 max-5 4.25 83.8%
5 DEV2500 1-best 1 90.8%
6 DEV2500 max-5 2.9 98.0%
Table 3: Tagger accuracy for 1-best and maximum
of 5-best hypotheses (max-5).
sentences using a random data split. We observe
that sentence length and standard deviation of test
sentences in the TEST2500 data set is higher than
on the training sentences while in the DEV2500
data set training and test sentence length and stan-
dard deviation are comparable.
3.1 Semantic Tagging
Table 3 presents sentence accuracy of the seman-
tic tagging stage. Tagging accuracy is evaluated
on 1-best and on max-5 best tagger outputs. In
the max-5 setting the number of hypotheses gen-
erated by the tagger varies for each input with the
average numbers reported in Table 3. Tagging ac-
curacy on TEST2500 using 1-best is 86.0%. Con-
sidering max-5 best tagging sequences, the accu-
racy is 95.2%. On the TEST500 data set tagging
accuracy is 67.9% and 83.8% on 1-best and max-
5 best sequences respectively, approximately 8%
points lower than on TEST2500 data set. On the
DEV2500 data set tagging accuracy is 90.8% and
98.0% on 1-best and max-5 best sequences, 4.8%
and 2.8% points higher than on the TEST2500
data set. The higher performance on DEV2500 in
comparison to the TEST2500 can be explained by
the higher complexity of the test sentences in com-
parison to the training sentences in the TEST2500
data set.
3.2 RCL Parsing
Parsing was evaluated using the EVALB scoring
metric (Collins, 1997). Its 1-best F-measure accu-
racy on gold standard TEST2500 and DEV2500
semantic tag sequences was 96.17% and 95.20%,
respectively. On TEST500, its accuracy remained
95.20%. On TEST2500 with system provided in-
put sequences, its accuracy was 94.79% for 869
out of 909 sentences that were tagged correctly.
3.3 System Accuracy
Table 4 presents string accuracy of automatically
generated RCL parse trees on each data set. The
111
Name Train #sent Train Sent. len. (stdev) Test #sent Test Sent. Len. (stdev)
TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)
TEST500 500 14.62(5.66) 909 13.96 (5.59)
DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)
Table 2: Number of sentences, average length and standard deviation of the data sets.
results are obtained by comparing system output
RCL parse string with the reference RCL parse
string. For each data set, we ran the system
with and without spatial validation. We ran RCL
parser and reference resolution on automatically
assigned semantic tags (Auto) and oracle tagging
(Orcl). We observed that some tag labels can be
verified systematically and corrected them with
simple rules: e.g., change ?front? to ?forward?
because relation specification in (Dukes, 2013)
doesn?t have ?front? even though annotations in-
cluded cases with ?front? as relation.
The system performance on TEST2500 data
set using automatically assigned tags and no spa-
tial validation is 60.84%. In this mode, the sys-
tem uses 1-best parser and 1-best tagger output.
With spatial validation, which allows the system to
re-rank parser and tagger hypotheses, the perfor-
mance increases by 27% points to 87.35%. This
indicates that the parser and the tagger component
often produce a correct output which is not ranked
first. Using oracle tags without / with spatial vali-
dation on TEST2500 data set the system accuracy
is 67.55% / 94.83%, 7% points above the accuracy
using predicted tags.
The system performance on TEST500 data set
using automatically assigned tags with / with-
out spatial validation is 48.95% / 74.92%, ap-
proximately 12% points below the performance
on TEST2500 (Row 1). Using oracle tags with-
out / with spatial validation the performance on
TEST500 data set is 63.89% / 94.94%. The per-
formance without spatial validation is only 4% be-
low TEST2500, while with spatial validation the
performance on TEST2500 and TEST500 is the
same. These results indicate that most perfor-
mance degradation on a smaller data set is due to
the semantic tagger.
The system performance on DEV2500 data set
using automatically assigned tags without / with
spatial validation is 68.0% / 96.80% (Row 5), 8%
points above the performance on TEST2500 (Row
1). With oracle tags, the performance is 69.60%
/ 98.0%, which is 2-3% points above TEST2500
(Row 2). These results indicate that most perfor-
mance improvement on a better balanced data set
# Dataset Tag Accuracy without / with
spatial validation
1 TEST2500 Auto 60.84 / 87.35
2 TEST2500 Orcl 67.55 / 94.83
3 TEST500 Auto 48.95 / 74.92
4 TEST500 Orcl 63.89 / 94.94
5 DEV2500 Auto 68.00 / 96.80
6 DEV2500 Orcl 69.60 / 98.00
Table 4: System accuracy with and without spatial
validation using automatically assigned tags and
oracle tags (OT).
DEV2500 is due to better semantic tagging.
4 Summary and Future Work
In this paper, we present the results of semantic
processing for natural language robot commands
using Tag&Parse approach. The system first tags
the input sentence and then applies non-lexical
parsing to the tag sequence. Reference resolution
is applied to the resulting parse trees. We com-
pare the results of the models trained on the data
sets of size 500 (TEST500) and 2500 (TEST2500)
sentences. We observe that sequence tagging
model degrades significantly on a smaller data set.
Parsing and reference resolution models, on the
other hand, perform nearly as well on both train-
ing sizes. We compare the results of the models
trained on more (DEV2500) and less (TEST2500)
homogeneous training/testing data sets. We ob-
serve that a semantic tagging model is more sen-
sitive to the difference between training and test
set than parsing model degrading significantly a
less homogeneous data set. Our results show that
1) both tagging and parsing models will benefit
from an improved re-ranking, and 2) our parsing
model is robust to a data size reduction while tag-
ging model requires a larger training data set.
In future work we plan to explore how
Tag&Parse approach will generalize in other do-
mains. In particular, we are interested in using
a combination of domain-specific tagging models
and generic semantic parsing (Das et al., 2010) for
processing spoken commands in a dialogue sys-
tem.
112
References
Xavier Carreras, Llu??s M`arquez, and Llu??s Padr?o.
2003. A Simple Named Entity Extractor Using Ad-
aBoost. In Proceedings of the CoNLL, pages 152?
157, Edmonton, Canada.
Michael Collins. 1997. Three Generative Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the ACL, pages 16?23.
Dipanjan Das, Nathan Schneider, Desai Chen, and
Noah A. Smith. 2010. Probabilistic Frame-
Semantic Parsing. In HLT-NAACL, pages 948?956.
Kais Dukes. 2013. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference (LTC).
Patrick Haffner. 2006. Scaling large margin classifiers
for spoken language understanding. Speech Com-
munication, 48(3-4):239?261.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition. In Proceedings of
the Message Understanding Conference (MUC-7).
Science Applications International Corporation.
Liang Huang and David Chiang. 2005. Better K-
best Parsing. In Proceedings of the Ninth Inter-
national Workshop on Parsing Technology, Parsing
?05, pages 53?64, Stroudsburg, PA, USA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of the 2003 Conference of the
NAACL on Human Language Technology - Volume
1, pages 173?180.
113
Referring Expression Generation Using Speaker-based Attribute Selection
and Trainable Realization (ATTR)
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
In the first REG competition, researchers
proposed several general-purpose algorithms
for attribute selection for referring expression
generation. However, most of this work did
not take into account: a) stylistic differences
between speakers; or b) trainable surface re-
alization approaches that combine semantic
and word order information. In this paper we
describe and evaluate several end-to-end re-
ferring expression generation algorithms that
take into consideration speaker style and use
data-driven surface realization techniques.
1 Introduction
There now exist numerous general-purpose algo-
rithms for attribute selection used in referring ex-
pression generation (e.g., (Dale and Reiter, 1995;
Krahmer et al, 2003; Belz and Gatt, 2007)). How-
ever, these algorithms by-and-large focus on the al-
gorithmic aspects of referring expression generation
rather than on psycholinguistic factors that influence
language production. For example, we know that
humans exhibit individual style differences during
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the lan-
guage production process is subject to lexical prim-
ing, which means that words and concepts that have
been used recently are likely to appear again (Levelt,
1989).
In this paper, we first explore the impact of indi-
vidual style and priming on attribute selection for
referring expression generation. To get an idea
of the potential improvement when modeling these
factors, we implemented a version of full brevity
search (Dale, 1992) that uses speaker-specific con-
straints, and another version that also uses recency
constraints. We found that using speaker-specific
constraints led to big performance gains for both
TUNA domains, while the use of recency constraints
was not as effective for TUNA-style tasks. We then
modified Dale and Reiter?s classic attribute selection
algorithm (Dale and Reiter, 1995) to model speaker-
specific constraints, and found performance gains in
this more greedy approach as well.
Then we looked at surface realization for referring
expression generation. There are several approaches
to surface realization described in the literature (Re-
iter and Dale, 2000) ranging from hand-crafted
template-based realizers to data-driven syntax-based
realizers (Langkilde and Knight, 2000; Bangalore
and Rambow, 2000). Template-based realization
involves the insertion of attribute values into pre-
determined templates. Data-driven syntax-based
methods use syntactic relations between words (in-
cluding long-distance relations) for word ordering.
Other data-driven techniques exhaustively generate
possible realizations with recourse to syntax in as
much as it is reflected in local n-grams. Such tech-
niques have the advantage of being robust although
they are inadequate to capture long-range depen-
dencies. In this paper, we explore three techniques
for the task of referring expression generation that
are different hybrids of hand-crafted and data-driven
methods.
The remainder of this paper is organized as fol-
lows: In Section 2, we present the algorithms for
attribute selection. The different methods for sur-
face realizers are presented in Section 3. The exper-
iments concerning the attribute selection and surface
realization are presented in Section 4 and Section 5.
The final remarks are discussed in Section 6.
2 Attribute Selection Algorithms
Full Brevity (FB) We implemented a version of
full brevity search (Dale, 1992). It does the follow-
211
ing: first, it constructs AS, the set of attribute sets
that uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on a selection criterion. The minimality (FB-
m) criterion selects from among the smallest ele-
ments of AS at random. The frequency (FB-f) cri-
terion selects from among the elements of AS the
one that occurred most often in the training data.
The speaker frequency (FB-sf) criterion selects
from among the elements of AS the one used most
often by this speaker in the training data, backing off
to FB-f if necessary. This criterion models speaker-
specific constraints. Finally, the speaker recency
(FB-sr) criterion selects from among the elements
of AS the one used most recently by this speaker in
the training data, backing off to FB-sf if necessary.
This criterion models priming and speaker-specific
constraints.
Dale and Reiter We implemented two variants of
the classic Dale & Reiter attribute selection (Dale
and Reiter, 1995) algorithm. For Dale & Reiter
basic (DR-b), we first build the preferred list of
attributes by sorting the most frequently used at-
tributes in the training set. We keep separate lists
based upon the ?+LOC? and ?-LOC? conditions
and backoff to a global preferred frequency list in
case the attributes are not covered in the current list
(merge and sort by frequency). Next, we iterate over
the list of preferred attributes and select the next one
that rules out at least one entity in the contrast set
until no distractors are left. The Dale & Reiter
speaker frequency (DR-sf) uses a speaker-specific
preferred list, backing off to the DR-b preferred list
if an attribute is not in the current speaker?s preferred
list. For this task, we ignored any further attribute
knowledge base or taxonomy abstraction.
3 Surface Realization Approaches
We summarize our approaches to surface realization
in this section. All three surface realizers have the
same four stages: (a) lexical choice of words and
phrases for the attribute values; (b) generation of a
space of surface realizations (T ); (c) ranking the set
of realizations using a language model (LM ); (d)
selecting the best scoring realization.
T ? = BestPath(Rank(T, LM)) (1)
Template-Based Realizer To construct our
template-based realizer, we extract the annotated
word string from each trial in the training data
and replace each annotated text segment with the
attribute type with which it is annotated. The key
for each template is the lexicographically sorted list
of attribute types it contains. Consequently, any
attribute lists not found in the training data cannot
be realized by the template-based realizer; however,
if there is a template for an input attribute list it is
quite likely to be coherent.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
fill in each possible template with each combina-
tion of the attribute realizations. We report results
for two versions of this realizer: one with speaker-
specific lexicon and templates (Template-S), and
one without (Template).
Dependency-Based Realizer To construct our
dependency-based realizer, we first parse all the
word strings from the training data using the depen-
dency parser described in (Bangalore et al, 2005;
Nasr and Rambow, 2004). Then, for every pair
of words wi, wj that occur in the same referring
expression (RE) in the training data, we compute:
freq(i < j), the frequency with which wi pre-
cedes wj in any RE; freq(i = j ? 1), the fre-
quency with which wi immediately precedes wj in
any RE; freq(dep(wi, wj) ? i < j), the frequency
with which wi depends on and precedes wj in any
RE, and freq(dep(wi, wj) ? j < i), the frequency
with which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute in the input attribute set, and
for each combination of attribute realizations, we
find the most likely set of dependencies and prece-
dences given the training data.
Permute and Rank In this method, the lexical
items associated with each of the attribute value to
be realized are treated as a disjunctive set of tokens.
This disjunctive set is represented as a finite-state
automaton with two states and transitions between
them labeled with the tokens of the set. The transi-
tions are weighted by the negative logarithm of the
probability of the lexical token (w) being associated
with that attribute value (attr): (?log(P (w|attr))).
These sets are treated as unordered bags of tokens;
we create permutations of these bags of tokens to
represent the set of possible surface realizations. We
then use the language model to rank this set of possi-
ble realizations and recover the highest scoring RE.
212
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
Unfortunately, the number of states of the min-
imal permutation automaton of even a linear au-
tomata (finite-state machine representation of a
string) grows exponentially with the number of
words of the string. So, instead of creating a full
permutation automaton, we choose to constrain per-
mutations to be within a local window of adjustable
size (also see (Kanthak et al, 2005)).
4 Attribute Selection Experiments
Data Preparation The training data were used to
build the models outlined above. The development
data were then processed one-by-one. For our final
submissions, we use training and development data
to build our models.
Results Table 1 shows the results for variations of
full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both cor-
pora, we see a large performance jump when we
use speaker constraints. However, when we incor-
porate recency constraints as well performance de-
clines slightly. We think this is due to two factors:
first, the speakers are not in a conversation, and self-
priming may have less impact; and second, we do
not always have the most recent prior utterance for a
given speaker in the training data.
Table 1 also shows the results for variations of
Dale and Reiter?s algorithm. When we incorpo-
String-Edit Dist. Accuracy
Furniture
DEV FB-sf DR-sf DEV FB-sf DR-sf
Permute&Rank 4.39 4.60 4.74 0.07 0.04 0.03
Dependency 3.90 4.25 5.50 0.14 0.06 0.03
Template 4.36 4.33 5.39 0.07 0.05 0.03
Template-S 3.52 3.81 5.16 0.28 0.20 0.04
People
Permute&Rank 6.26 6.46 7.01 0.01 0.01 0.00
Dependency 3.96 4.32 7.03 0.06 0.06 0.00
Template 5.16 4.62 7.26 0.03 0.06 0.00
Template-S 4.25 4.31 7.04 0.18 0.13 0.00
Overall
Permute&Rank 5.25 5.45 5.78 0.05 0.03 0.01
Dependency 3.93 4.28 6.20 0.07 0.06 0.01
Template 4.73 4.46 6.25 0.05 0.05 0.01
Template-S 3.86 4.04 6.03 0.23 0.17 0.02
Table 2: Results for realization
rate speaker constraints, we again see a performance
jump, although compared to the best possible case
(full brevity) there is still room for improvement.
Discussion We have shown that by using speaker
and recency constraints in standard algorithms, it
is possible to achieve performance gains on the at-
tribute selection task.
The most relevant previous research is the work of
(Gupta and Stent, 2005), who modified Dale and Re-
iter?s algorithm to model speaker adaptation in dia-
log. However, this corpus does not involve dialog so
there are no cross-speaker constraints, only within-
speaker constraints (style and priming).
5 Surface Realization Experiments
Data Preparation We first normalize the training
data to correct misspellings and remove punctuation
and capitalization. We then extract a phrasal lexi-
con. For each attribute value we extract the count of
all realizations of that value in the training data. We
treat locations as a special case, storing separately
the realizations of x-y coordinate pairs and single
x- or y-coordinates. We add a small number of re-
alizations to the lexicon by hand to cover possible
attribute values not seen in the training data.
Results Table 2 shows the evaluation results for
string-edit distance and string accuracy on the devel-
opment set with three different attributes sets: DEV
? attributes selected by the human test; FB-sf ? at-
tributes generated by the full brevity algorithm with
speaker frequency; and DR-sf ? attributes selected
213
by the Dale & Reiter algorithm with speaker fre-
quency.
For the TUNA realization task (DEV attributes),
our approaches work better for the furniture domain,
where there are fewer attributes, than for the people
domain. For the furniture domain, the Template-S
approach achieves lowest string-edit distance, while
for the people domain, the Dependency approach
achieves lowest string-edit distance. The latter
method was submitted for human evaluation.
When we consider the ?end-to-end? referring
expression generation task (FB-sf and DR-sf at-
tributes), the best overall performing system is the
speaker-based template generator with full-brevity
and speaker frequency attribute selection. In terms
of generated sentence quality, a preliminary and
qualitative analysis shows that the combination Per-
mute & Rank and DR-sf produces more naturalistic
phrases.
Discussion Although the Template-S approach
achieves the best string edit distance scores over-
all, it is not very robust. If no examples were found
in the training data neither Template approach will
produce no output. (This happens twice for each of
the domains on the development data.) The Depen-
dency approach achieves good overall performance
with more robustness.
The biggest cause of errors for the Permute
and Reorder approach was missing determiners and
missing modifiers. The biggest cause of errors for
the Dependency approach was missing determiners
and reordered words. The Template approach some-
times had repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
6 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches to
surface realization can work well using a range of
lexical, syntactic and semantic information.
In addition to individual style and priming, an-
other potentially fruitful area for exploration with
TUNA-style tasks is human visual search strategies
(Rayner, 1998). We leave this idea for future work.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
S. Bangalore and O. Rambow. 2000. Exploiting a prob-
abilistic hierarchical model for generation. In Proc.
COLING.
S. Bangalore, A. Emami, and P. Haffner. 2005. Factor-
ing global inference by enriching local representations.
Technical report, AT&T Labs-Research.
A. Belz and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proceedings of UCNLG+MT at MT Summit XI.
A. Belz. 2007. Probabilistic generation of weather fore-
cast texts. In Proceedings of NAACL/HLT.
R. Dale and E. Reiter. 1995. Computational interpreta-
tions of the Gricean maxims in the generation of refer-
ring expressions. Cognitive Science, 19(2).
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. MIT Press, Cambridge, MA.
S. Gupta and A. Stent. 2005. Automatic evaluation of
referring expression generation using corpora. In Pro-
ceedings of UCNLG.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
E. Krahmer, S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
I. Langkilde and K. Knight. 2000. Forest-based statisti-
cal sentence generation. In Proc. NAACL.
W. Levelt, 1989. Speaking: From intention to articula-
tion, pages 222?226. MIT Press.
A. Nasr and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop
on Tree Adjoining Grammar and Related Formalisms
(TAG+7).
K. Rayner. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psychologi-
cal Bulletin, 124(3).
E. Reiter and R. Dale. 2000. Building Natural Language
Generation Systems. Cambridge University Press.
214
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 151?158
Manchester, August 2008
Trainable Speaker-Based Referring Expression Generation
Giuseppe Di Fabbrizio and Amanda J. Stent and Srinivas Bangalore
AT&T Labs - Research, Inc.
180 Park Avenue
Florham Park, NJ 07932, USA
{pino,stent,srini}@research.att.com
Abstract
Previous work in referring expression gen-
eration has explored general purpose tech-
niques for attribute selection and surface
realization. However, most of this work
did not take into account: a) stylistic dif-
ferences between speakers; or b) trainable
surface realization approaches that com-
bine semantic and word order information.
In this paper we describe and evaluate sev-
eral end-to-end referring expression gener-
ation algorithms that take into considera-
tion speaker style and use data-driven sur-
face realization techniques.
1 Introduction
Natural language generation (NLG) systems have
typically decomposed the problem of generating
a linguistic expression from a conceptual specifi-
cation into three major steps: content planning,
text planning and surface realization (Reiter and
Dale, 2000). The task in content planning is to
select the information that is to be conveyed to
maximize communication efficiency. The task in
text planning and surface realization is to use the
available linguistic resources (words and syntax) to
convey the selected information using well-formed
linguistic expressions.
During a discourse (whether written or spoken,
monolog or dialog), a number of entities are in-
troduced into the discourse context shared by the
reader/hearer and the writer/speaker. Construct-
ing linguistic references to these entities efficiently
and effectively is a problem that touches on all
c? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
parts of an NLG system. Traditionally, this prob-
lem is split into two parts. The task of selecting
the attributes to use in referring to an entity is the
attribute selection task, performed during content
planning or sentence planning. The actual con-
struction of the referring expression is part of sur-
face realization.
There now exist numerous general-purpose al-
gorithms for attribute selection (e.g., (Dale and Re-
iter, 1995; Krahmer et al, 2003; Belz and Gatt,
2007; Siddharthan and Copestake, 2004)). How-
ever, these algorithms by-and-large focus on the
algorithmic aspects of referring expression gener-
ation rather than on psycholinguistic factors that
influence language production. For example, we
know that humans exhibit individual differences in
language production that can be quite pronounced
(e.g. (Belz, 2007)). We also know that the
language production process is subject to lexical
priming, which means that words and concepts that
have been used recently are likely to appear again
(Levelt, 1989).
In this paper, we look at attribute selection and
surface realization for referring expression gener-
ation using the TUNA corpus 1, an annotated cor-
pus of human-produced referring expressions that
describe furniture and people. We first explore
the impact of individual style and priming on at-
tribute selection for referring expression genera-
tion. To get an idea of the potential improvement
when modeling these factors, we implemented a
version of full brevity search that uses speaker-
specific constraints, and another version that also
uses recency constraints. We found that using
speaker-specific constraints led to big performance
gains for both TUNA domains, while the use of re-
1http://www.csd.abdn.ac.uk/research/tuna/
151
cency constraints was not as effective for TUNA-
style tasks. We then modified Dale and Reiter?s
classic attribute selection algorithm (Dale and Re-
iter, 1995) to model individual differences in style,
and found performance gains in this more greedy
approach as well.
Then, we look at surface realization for re-
ferring expression generation. There are sev-
eral approaches to surface realizations described
in the literature (Reiter and Dale, 2000) rang-
ing from hand-crafted template-based realizers to
data-driven syntax-based realizers (Langkilde and
Knight, 2000; Bangalore and Rambow, 2000).
Template-based realization provides a straightfor-
ward method to fill out pre-defined templates with
the current attribute values. Data-driven syntax-
based methods employ techniques that incorporate
the syntactic relations between words which can
potentially go beyond local adjacency relations.
Syntactic information also helps in eliminating un-
grammatical sentence realizations. At the other ex-
treme, there are techniques that exhaustively gen-
erate possible realizations with recourse to syntax
in as much as it is reflected in local n-grams. Such
techniques have the advantage of being robust al-
though they are inadequate to capture long-range
dependencies. We explore three techniques for
the task of referring expression generation that are
different hybrids of hand-crafted and data-driven
methods.
The layout of this paper is as follows: In Sec-
tion 2, we describe the TUNA data set and the task
of identifying target entities in the context of dis-
tractors. In Section 3, we present our algorithms
for attribute selection. Our algorithms for sur-
face realization are presented in Section 4. Our
evaluation of these methods for attribute selection
and surface realization are presented in Sections 5
and 6.
2 The TUNA Corpus
The TUNA corpus was constructed using a web-
based experiment. Participants were presented
with a sequence of web pages, on each of which
they saw displayed a selection of 7 pictures of ei-
ther furniture (e.g. Figure 1) or people (e.g. Fig-
ure 2) sparsely placed on a 3 row x 5 column
grid. One of the pictures (the target) was high-
lighted; the other 6 objects (the distractors) were
randomly selected from the object database. Par-
ticipants were told that they were interacting with a
computer system to remove all but the highlighted
picture from the screen. They entered a description
of the object using natural language to identify the
object to the computer system.
The section of the TUNA corpus we used was
that provided for the REG 2008 Challenge2. The
training data includes 319 referring expressions in
the furniture domain and 274 in the people domain.
The development data (which we used for testing)
includes 80 referring expressions in the furniture
domain and 68 in the people domain.
Figure 1: Example of data from the furniture do-
main (The red couch on top).
Figure 2: Example of data from the people domain
(The bald subject on the bottom with the white
beard).
3 Attribute Selection Algorithms
Given a set of entities with attributes appropriate
to a domain (e.g., cost of flights, author of a book,
2http://www.nltg.brighton.ac.uk/research/reg08/. Prelimi-
nary versions of these algorithms were used in this challenge
and presented at INLG 2008.
152
color of a car) that are in a discourse context, and a
target entity that needs to be identified, the task of
attribute selection is to select a subset of the at-
tributes that uniquely identifies the target entity.
(Note that there may be more than one such at-
tribute set.) The efficacy of attribute selection can
be measured based on the minimality of the se-
lected attribute set as well as its ability to deter-
mine the target entity uniquely. There are varia-
tions however in terms of what makes an attribute
set more preferable to a human. For example, in
a people identification task, attributes of faces are
generally more memorable than attributes pertain-
ing to outfits. In this paper, we demonstrate that
the attribute set is speaker dependent.
In this section, we present two different attribute
selection algorithms. The Full Brevity algorithm
selects the attribute set by exhaustively searching
through all possible attribute sets. In contrast, Dale
and Reiter algorithm orders the attributes based
on a heuristic (motivated by human preference)
and selects the attributes in that order until the tar-
get entity is uniquely determined. We elaborate on
these algorithms below.
Full Brevity (FB) We implemented a version of
full brevity search. It does the following: first,
it constructs AS, the set of attribute sets that
uniquely identify the referent given the distrac-
tors. Then, it selects an attribute set ASu ? AS
based on one of the following four criteria: 1) The
minimality (FB-m) criterion selects from among
the smallest elements of AS at random. 2) The
frequency (FB-f) criterion selects the element of
AS that occurred most often in the training data.
3) The speaker frequency (FB-sf) criterion se-
lects the element of AS used most often by this
speaker in the training data, backing off to FB-f if
necessary. This criterion models individual speak-
ing/writing style. 4) Finally, the speaker recency
(FB-sr) criterion selects the element of AS used
most recently by this speaker in the training data,
backing off to FB-sf if necessary. This criterion
models priming.
Dale and Reiter We implemented two variants
of the classic Dale & Reiter attribute selection
(Dale and Reiter, 1995) algorithm. For Dale &
Reiter basic (DR-b), we first build the preferred
list of attributes by sorting the attributes according
to frequency of use in the training data. We keep
separate lists based on the ?LOC? condition (if its
value was ?+LOC?, the participants were told that
they could refer to the target using its location on
the screen; if it was ?-LOC?, they were instructed
not to use location on the screen) and backoff to
a global preferred attribute list if necessary. Next,
we iterate over the list of preferred attributes and
select the next one that rules out at least one en-
tity in the contrast set until no distractors are left.
Dale & Reiter speaker frequency (DR-sf) uses
a different preferred attribute list for each speaker,
backing off to the DR-b preferred list if an attribute
has never been observed in the current speaker?s
preferred attribute list. For the purpose of this task,
we did not use any external knowledge (e.g. tax-
onomies).
4 Surface Realization Approaches
A surface realizer for referring expression genera-
tion transforms a set of attribute-value pairs into a
linguistically well-formed expression. Our surface
realizers, which are all data-driven, involve four
stages of processing: (a) lexical choice of words
and phrases to realize attribute values; (b) genera-
tion of a space of surface realizations (T ); (c) rank-
ing the set of realizations using a language model
(LM ); (d) selecting the best scoring realization.
In general, the best ranking realization (T?) is de-
scribed by equation 1:
T ? = Bestpath(Rank(T,LM)) (1)
We describe three different methods for creating
the search space of surface realizations ? Template-
based, Dependency-based and Permutation-based
methods. Although these techniques share the
same method for ranking, they differ in the meth-
ods used for generating the space of possible sur-
face realizations.
4.1 Generating possible surface realizations
In order to transform the set of attribute-value
pairs into a linguistically well-formed expression,
the appropriate words that realize each attribute
value need to be selected (lexical choice) and the
selected words need to be ordered according to
the syntax of the target language (lexical order).
We present different models for approximating the
syntax of the target language. All three models
tightly integrate the lexical choice and lexical re-
ordering steps.
153
4.1.1 Template-Based Realizer
In the template-based approach, surface realiza-
tions from our training data are used to infer a set
of templates. In the TUNA data, each attribute in
each referring expression is annotated with its at-
tribute type (e.g. in ?the large red sofa? the sec-
ond word is labeled ?size?, the third ?color? and
the fourth ?type?). We extract the annotated re-
ferring expressions from each trial in the training
data and replace each attribute value with its type
(e.g. ?the size color type?) to create a tem-
plate. Each template is indexed by the lexicograph-
ically sorted list of attribute types it contains (e.g.
color size type). If an attribute set is not
found in the training data (e.g. color size)
but a superset of that set is (e.g. color size
type), then the corresponding template(s) may be
used, with the un-filled attribute types deleted prior
to output.
At generation time, we find all possible realiza-
tions (l) (from the training data) of each attribute
value (a) in the input attribute set (AS), and fill in
each possible template (t) with each combination
of the attribute realizations. The space of possible
surface realizations is represented as a weighted
finite-state automaton. The weights are computed
from the prior probability of each template and
the prior probability of each lexical item realizing
an attribute (Equation 2). We have two versions
of this realizer: one with speaker-specific lexi-
cons and templates (Template-S), and one without
(Template). We report results for both.
P (T |AS) =
?
t
P (t|AS)?
?
a?t
?
l
P (l|a, t) (2)
4.1.2 Dependency-Based Realizer
To construct our dependency-based realizer, we
first parse all the word strings from the train-
ing data using the dependency parser described
in (Bangalore et al, 2005; Nasr and Rambow,
2004). Then, for every pair of words wi, wj that
occur in the same referring expression (RE) in the
training data, we compute: freq(i < j), the fre-
quency with which wi precedes wj in any RE;
freq(dep(wi, wj) ? i < j), the frequency with
which wi depends on and precedes wj in any RE,
and freq(dep(wi, wj)?j < i), the frequency with
which wi depends on and follows wj in any RE.
At generation time, we find all possible realiza-
tions of each attribute value in the input attribute
set, and for each combination of attribute realiza-
tions, we find the most likely set of dependencies
and precedences given the training data. In other
words, we bin the selected attribute realizations
according to whether they are most likely to pre-
cede, depend on and precede, depend on and fol-
low, or follow, the head word they are closest to.
The result is a set of weighted partial orderings on
the attribute realizations. As with the template-
based surface realizer, we implemented speaker-
specific and speaker-independent versions of the
dependency-based surface realizer. Once again,
we encode the space of possible surface realiza-
tions as a weighted finite-state automaton.
4.1.3 Permute and Rank Realizer
In this method, the lexical items associated with
each attribute value to be realized are treated as a
disjunctive set of tokens. This disjunctive set is
represented as a finite-state automaton with two
states and transitions between them labeled with
the tokens of the set. The transitions are weighted
by the negative logarithm of the probability of the
lexical token (l) being associated with that attribute
value (a): (?log(P (l|a))). These sets are treated
as bags of tokens; we create permutations of these
bags of tokens to represent the set of possible sur-
face realizations.
In general, the number of states of the minimal
permutation automaton of even a linear automaton
(finite-state representation of a string) grows expo-
nentially with the number of words of the string.
Although creating the full permutation automaton
for full natural language generation tasks could
be computationally prohibitive, most attribute sets
in our two domains contain no more than five at-
tributes. So we choose to explore the full permu-
tation space. A more general approach might con-
strain permutations to be within a local window of
adjustable size (also see (Kanthak et al, 2005)).
Figure 3 shows the minimal permutation au-
tomaton for an input sequence of 4 words and a
window size of 2. Each state of the automaton is
indexed by a bit vector of size equal to the number
of words/phrases of the target sentence. Each bit
of the bit vector is set to 1 if the word/phrase in
that bit position is used on any path from the initial
to the current state. The next word for permutation
from a given state is restricted to be within the win-
dow size (2 in our case) positions counting from
the first as-yet uncovered position in that state. For
example, the state indexed with vector ?1000? rep-
154
0000
10001
0100
2
1100
2
10103
1
1110
3
1101
4
1111
4
3
2
Figure 3: Locally constraint permutation automaton for a sentence with 4 positions and a window size
of 2.
resents the fact that the word/phrase at position 1
has been used. The next two (window=2) posi-
tions are the possible outgoing arcs from this state
with labels 2 and 3 connecting to state ?1100? and
?1010? respectively. The bit vectors of two states
connected by an arc differ only by a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
4.2 Ranking and Recovering a Surface
Realization
These three methods for surface realization create
a space of possible linguistic expressions given the
set of attributes to be realized. These expressions
are encoded as finite-state automata and have to be
ranked based on their syntactic well-formedness.
We approximate the syntactic well-formedness of
an expression by the n-gram likelihood score of
that expression. We use a trigram model trained
on the realizations in the training corpus. This
language model is also represented as a weighted
finite-state automaton. The automaton represent-
ing the space of possible realizations and the one
representing the language model are composed.
The result is an automaton that ranks the possible
realizations according to their n-gram likelihood
scores. We then produce the best-scoring realiza-
tion as the target realization of the input attribute
set.
We introduce a parameter ? which allows us
to control the importance of the prior score rela-
tive to the language model scores. We weight the
finite-state automata according to this parameter as
shown in Equation 3.
T ? = Bestpath(? ? T ? (1 ? ?) ? LM) (3)
DICE MASI Acc. Uniq. Min.
Furniture
FB-m .36 .16 0 1 1
FB-f .81 .58 .40 1 0
FB-sf .95 .87 .79 1 0
FB-sr .93 .81 .71 1 0
DR-b .81 .60 .45 1 0
DR-sf .86 .64 .45 1 .04
People
FB-m .26 .12 0 1 1
FB-f .58 .37 .28 1 0
FB-sf .94 .88 .84 1 .01
FB-sr .93 .85 .79 1 .01
DR-b .70 .45 .25 1 0
DR-sf .78 .55 .35 1 0
Overall
FB-m .32 .14 0 1 1
FB-f .70 .48 .34 1 0
FB-sf .95 .87 .81 1 .01
FB-sr .93 .83 .75 1 .01
DR-b .76 .53 .36 1 0
DR-sf .82 .60 .41 1 .02
Table 1: Results for attribute selection
5 Attribute Selection Experiments
Data Preparation The training data were used
to build the models outlined above. The develop-
ment data were then processed one-by-one.
Metrics We report performance using the met-
rics used for the REG 2008 competition. The
MASI metric is a metric used in summarization
that measures agreement between two annotators
(or one annotator and one system) on set-valued
items (Nenkova et al, 2007). Values range from
0 to 1, with 1 representing perfect agreement.
The DICE metric is also a measure of association
whose value varies from 0 (no association) to 1 (to-
tal association) (Dice, 1945). The Accuracy met-
ric is binary-valued: 1 if the attribute set is iden-
tical to that selected by the human, 0 otherwise.
The Uniqueness metric is also binary-valued: 1 if
the attribute set uniquely identifies the target refer-
ent among the distractors, 0 otherwise. Finally, the
Minimality metric is 1 if the selected attribute set
is as small as possible (while still uniquely identi-
fying the target referent), and 0 otherwise. We note
155
that attribute selection algorithms such as Dale &
Reiter?s are based on the observation that humans
frequently do not produce minimal referring ex-
pressions.
Results Table 1 shows the results for variations
of full brevity. As we would expect, all approaches
achieve a perfect score on uniqueness. For both
corpora, we see a large performance jump when
we use speaker constraints for all metrics other
than minimality. However, when we incorporate
recency constraints as well performance declines
slightly. We think this is due to two factors: first,
the speakers are not in a conversation, and self-
priming may have less impact than other-priming;
and second, we do not always have the most recent
prior utterance for a given speaker in the training
data.
Table 1 also shows the results for variations of
Dale & Reiter?s algorithm. When we incorporate
speaker constraints, we again see a performance
jump for most metrics, although compared to the
best possible case (full brevity) there is still room
for improvement.
We conclude that speaker constraints can be suc-
cessfully used in standard attribute selection algo-
rithms to improve performance on this task.
The most relevant previous research is the work
of (Gupta and Stent, 2005), who modified Dale
and Reiter?s algorithm to model speaker adaptation
in dialog. However, this corpus does not involve
dialog so there are no cross-speaker constraints,
only within-speaker constraints (speaker style and
priming).
6 Surface Realization Experiments
Data Preparation We first normalized the train-
ing data to correct misspellings and remove punc-
tuation and capitalization. We then extracted a
phrasal lexicon. For each attribute value we ex-
tracted the count of all realizations of that value in
the training data. We treated locations as a spe-
cial case, storing separately the realizations of x-
y coordinate pairs and single x- or y-coordinates.
We added a small number of realizations by hand
to cover possible attribute values not seen in the
training data.
Realization We ran two realization experiments.
In the first experiment, we used the human-
selected attribute sets in the development data as
the input to realization. If we want to maxi-
? SED ACC Bleu NIST
Furniture
Permute&Rank 0.01 3.54 0.14 0.311 3.87
Dependency 0.90 4.51 0.09 0.206 3.29
Dependency-S 0.60 4.30 0.11 0.232 3.91
Template 0.10 3.59 0.13 0.328 3.93
Template-S 0.10 2.80 0.28 0.403 4.67
People
Permute&Rank 0.04 4.37 0.10 0.227 3.15
Dependency 0.70 6.10 0.00 0.072 2.35
Dependency-S 0.50 5.84 0.02 0.136 3.05
Template 0.80 3.87 0.07 0.250 3.18
Template-S 0.70 3.79 0.15 0.265 3.59
Overall
Permute&Rank .01/.04 3.92 0.12 0.271 4.02
Dependency 0.9/0.7 5.24 0.05 0.146 3.23
Dependency-S 0.6/0.5 5.01 0.07 0.187 3.98
Template 0.1/0.8 3.77 0.10 0.285 4.09
Template-S 0.1/0.7 3.26 0.22 0.335 4.77
Table 2: Results for realization using speakers? at-
tribute selection (SED: String Edit Distance, ACC:
String Accuracy)
mize humanlikeness, then using these attribute sets
should give us an idea of the best possible perfor-
mance of our realization methods. In the second
experiment, we used the attribute sets output by
our best-performing attribute selection algorithms
(FB-sf and DR-sf) as the input to realization.
Metrics We report performance of our surface
realizers using the metrics used for the REG 2008
shared challenge and standard metrics used in the
natural language generation and machine trans-
lation communities. String Edit Distance (SED)
is a measure of the number of words that would
have to be added, deleted, or replaced in order to
transform the generated referring expression into
the one produced by the human. As used in the
REG 2008 shared challenge, it is unnormalized, so
its values range from zero up. Accuracy (ACC)
is binary-valued: 1 if the generated referring ex-
pression is identical to that produced by the hu-
man (after spelling correction and normalization),
and 0 otherwise. Bleu is an n-gram based met-
ric that counts the number of 1, 2 and 3 grams
shared between the generated string and one or
more (preferably more) reference strings (Papenini
et al, 2001). Bleu values are normalized and range
from 0 (no match) to 1 (perfect match). Finally,
the NIST metric is a variation on the Bleu met-
ric that, among other things, weights rare n-grams
higher than frequently-occurring ones (Dodding-
ton, 2002). NIST values are unnormalized.
156
SED ACC Bleu NIST
Furniture
FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf FB-sf DR-sf
Permute&Rank 3.97 4.22 0.09 0.06 .291 .242 3.82 3.32
Dependency 4.80 5.03 0.04 0.03 .193 .105 3.32 2.46
Dependency-S 4.71 4.88 0.06 0.04 .201 .157 3.74 3.26
Template 3.89 4.56 0.09 0.05 .283 .213 3.48 3.22
Template-S 3.26 3.90 0.19 0.12 .362 .294 4.41 4.07
People
Permute&Rank 4.75 5.82 0.09 0.03 .171 .110 2.70 2.31
Dependency 6.35 6.91 0.00 0.00 .068 .073 1.81 1.86
Dependency-S 5.94 6.18 0.01 0.00 .108 .113 2.73 2.41
Template 3.62 4.24 0.07 0.04 .231 .138 2.88 1.35
Template-S 3.76 4.38 0.12 0.06 .201 .153 2.76 1.88
Overall
Permute&Rank 4.33 4.96 0.09 0.05 .236 .235 3.73 3.72
Dependency 5.51 6.00 0.02 0.01 .136 .091 2.97 2.50
Dependency-S 5.36 5.67 0.04 0.02 .159 .136 3.77 3.25
Template 3.76 4.41 0.08 0.05 .258 .180 3.69 2.89
Template-S 3.48 4.12 0.16 0.09 .288 .229 4.15 3.58
Table 3: Results for realization with different attribute selection algorithms
Furniture People
FB-sf DR-sf FB-sf DR-sf
Permute&Rank .01 .05 .05 .04
Dependency .9 .9 .9 .1
Dependency-S .2 .2 .4 .4
Template .8 .8 .8 .8
Template-S .6 .8 .8 .8
Table 4: Optimal ? values with different attribute
selection algorithms
Results Our experimental results are shown in
Tables 2 and 3. (These results are the results
obtained with the language model weighting that
gives best performance; the weights are shown in
Tables 2 and 4.) Our approaches work better for
the furniture domain, where there are fewer at-
tributes, than for the people domain. For both
domains, for automatic and human attribute se-
lection, the speaker-dependent Template-based ap-
proach seems to perform the best, then the speaker-
independent Template-based approach, and then
the Permute&Rank approach. However, we find
automatic metrics for evaluating generation qual-
ity to be unreliable. We looked at the output of the
surface realizers for the two examples in Section 2.
The best output for the example in Figure 1 is from
the FB-sf template-based speaker-dependent algo-
rithm, which is the big red sofa. The worst out-
put is from the DR-sf dependency-based speaker-
dependent algorithm, which is on the left red chair
with three seats. The best output for the exam-
ple in Figure 2 is from the FB-sf template-based
speaker-independent algorithm, which is the man
with the white beard. The worst output is from the
FB-sf dependency-based speaker-dependent algo-
rithm, which is beard man white.
Discussion The Template-S approach achieves
the best string edit distance scores, but it is not very
robust. If no examples are found in the training
data that realize (a superset of) the input attribute
set, neither Template approach will produce any
output.
The biggest cause of errors for the Permute and
Reorder approach is missing determiners and miss-
ing modifiers. The biggest cause of errors for the
Dependency approach is missing determiners and
reordered words. The Template approach some-
times has repeated words (e.g. ?middle?, where
?middle? referred to both x- and y-coordinates).
Here we report performance using automatic
metrics, but we find these metrics to be unreliable
(particularly in the absence of multiple reference
texts). Also, we are not sure that people would ac-
cept from a computer system output that is very
human-like in this domain, as the human-like out-
put is often ungrammatical and telegraphic (e.g.
?grey frontal table?). We plan to do a human eval-
uation soon to better analyze our systems? perfor-
mance.
7 Conclusions
When building computational models of language,
knowledge about the factors that influence human
language production can prove very helpful. This
knowledge can be incorporated in frequentist and
heuristic approaches as constraints or features. In
the experiments described in this paper, we used
157
data-driven, speaker-aware approaches to attribute
selection and referring expression realization. We
showed that individual speaking style can be use-
fully modeled even for quite ?small? generation
tasks, and confirmed that data-driven approaches
to surface realization can work well using a range
of lexical, syntactic and semantic information.
We plan to explore the impact of human visual
search strategies (Rayner, 1998) on the referring
expression generation task. In addition, we are
planning a human evaluation of the generation sys-
tems? output. Finally, we plan to apply our algo-
rithms to a conversational task.
Acknowledgments
We thank Anja Belz, Albert Gatt, and Eric Kow
for organizing the REG competition and providing
data, and Gregory Zelinsky for discussions about
visually-based constraints.
References
Bangalore, S. and O. Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In
Proc. COLING.
Bangalore, S., A. Emami, and P. Haffner. 2005. Fac-
toring global inference by enriching local represen-
tations. Technical report, AT&T Labs-Research.
Belz, A. and A. Gatt. 2007. The attribute selection for
GRE challenge: Overview and evaluation results. In
Proc. UCNLG+MT at MT Summit XI.
Belz, A. 2007. Probabilistic generation of weather
forecast texts. In Proc. NAACL/HLT.
Dale, R. and E. Reiter. 1995. Computational interpre-
tations of the Gricean maxims in the generation of
referring expressions. Cognitive Science, 19(2).
Dice, L. 1945. Measures of the amount of ecologic
association between species. Ecology, 26.
Doddington, G. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. HLT.
Gupta, S. and A. Stent. 2005. Automatic evaluation
of referring expression generation using corpora. In
Proc. UCNLG.
Kanthak, S., D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proc. ACL Work-
shop on Building and Using Parallel Texts.
Krahmer, E., S. van Erk, and A. Verleg. 2003. Graph-
based generation of referring expressions. Computa-
tional Linguistics, 29(1).
Langkilde, I. and K. Knight. 2000. Forest-based statis-
tical sentence generation. In Proc. NAACL.
Levelt, W., 1989. Speaking: From intention to articu-
lation, pages 222?226. MIT Press.
Nasr, A. and O. Rambow. 2004. Supertagging and
full parsing. In Proc. 7th International Workshop on
Tree Adjoining Grammar and Related Formalisms
(TAG+7).
Nenkova, A., R. Passonneau, and K. McKeown. 2007.
The Pyramid method: incorporating human con-
tent selection variation in summarization evaluation.
ACM Transactions on speech and language process-
ing, 4(2).
Papenini, K., S. Roukos, T. Ward, andW.-J. Zhu. 2001.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proc. ACL.
Rayner, K. 1998. Eye movements in reading and infor-
mation processing: 20 years of research. Psycholog-
ical Bulletin, 124(3).
Reiter, E. and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
Siddharthan, A. and A. Copestake. 2004. Generat-
ing referring expressions in open domains. In Proc.
ACL.
158
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 34?42,
COLING 2010, Beijing, August 2010.
Phrase Based Decoding using a Discriminative Model
Prasanth Kolachina
LTRC, IIIT-Hyderabad
{prasanth k}@research.iiit.ac.in
Sriram Venkatapathy
LTRC, IIIT-Hyderabad
{sriram}@research.iiit.ac.in
Srinivas Bangalore
AT&T Labs-Research, NY
{srini}@research.att.com
Sudheer Kolachina
LTRC, IIIT-Hyderabad
{sudheer.kpg08}@research.iiit.ac.in
Avinesh PVS
LTRC, IIIT-Hyderabad
{avinesh}@research.iiit.ac.in
Abstract
In this paper, we present an approach to
statistical machine translation that com-
bines the power of a discriminative model
(for training a model for Machine Transla-
tion), and the standard beam-search based
decoding technique (for the translation of
an input sentence). A discriminative ap-
proach for learning lexical selection and
reordering utilizes a large set of feature
functions (thereby providing the power to
incorporate greater contextual and linguis-
tic information), which leads to an effec-
tive training of these models. This model
is then used by the standard state-of-art
Moses decoder (Koehn et al, 2007) for the
translation of an input sentence.
We conducted our experiments on
Spanish-English language pair. We used
maximum entropy model in our exper-
iments. We show that the performance
of our approach (using simple lexical
features) is comparable to that of the
state-of-art statistical MT system (Koehn
et al, 2007). When additional syntactic
features (POS tags in this paper) are used,
there is a boost in the performance which
is likely to improve when richer syntactic
features are incorporated in the model.
1 Introduction
The popular approaches to machine translation
use the generative IBM models for training
(Brown et al, 1993; Och et al, 1999). The param-
eters for these models are learnt using the stan-
dard EM Algorithm. The parameters used in these
models are extremely restrictive, that is, a simple,
small and closed set of feature functions is used
to represent the translation process. Also, these
feature functions are local and are word based. In
spite of these limitations, these models perform
very well for the task of word-alignment because
of the restricted search space. However, they per-
form poorly during decoding (or translation) be-
cause of their limitations in the context of a much
larger search space.
To handle the contextual information, phrase-
based models were introduced (Koehn et al,
2003). The phrase-based models use the word
alignment information from the IBM models and
train source-target phrase pairs for lexical se-
lection (phrase-table) and distortions of source
phrases (reordering-table). These models are still
relatively local, as the target phrases are tightly as-
sociated with their corresponding source phrases.
In contrast to a phrase-based model, a discrim-
inative model has the power to integrate much
richer contextual information into the training
model. Contextual information is extremely use-
ful in making lexical selections of higher quality,
as illustrated by the models for Global Lexical Se-
lection (Bangalore et al, 2007; Venkatapathy and
34
Bangalore, 2009).
However, the limitation of global lexical se-
lection models has been sentence construction.
In global lexical selection models, lattice con-
struction and scoring (LCS) is used for the pur-
pose of sentence construction (Bangalore et al,
2007; Venkatapathy and Bangalore, 2009). In our
work, we address this limitation of global lexi-
cal selection models by using an existing state-of-
art decoder (Koehn et al, 2007) for the purpose
of sentence construction. The translation model
used by this decoder is derived from a discrimina-
tive model, instead of the usual phrase-table and
reordering-table construction algorithms. This al-
lows us to use the effectiveness of an existing
phrase-based decoder while retaining the advan-
tages of the discriminative model. In this paper,
we compare the sentence construction accuracies
of lattice construction and scoring approach (see
section 4.1 for LCS Decoding) and the phrase-
based decoding approach (see section 4.2).
Another advantage of using a discriminative ap-
proach to construct the phrase table and the re-
ordering table is the flexibility it provides to in-
corporate linguistic knowledge in the form of ad-
ditional feature functions. In the past, factored
phrase-based approaches for Machine Translation
have allowed the use of linguistic feature func-
tions. But, they are still bound by the local-
ity of context, and definition of a fixed struc-
ture of dependencies between the factors (Koehn
and Hoang, 2007). Furthermore, factored phrase-
based approaches place constraints both on the
type and number of factors that can be incorpo-
rated into the training. In this paper, though we do
not extensively test this aspect, we show that us-
ing syntactic feature functions does improve the
performance of our approach, which is likely to
improve when much richer syntactic feature func-
tions (such as information about the parse struc-
ture) are incorporated in the model.
As the training model in a standard phrase-
based system is relatively impoverished with re-
spect to contextual/linguistic information, integra-
tion of the discriminative model in the form of
phrase-table and reordering-table with the phrase-
based decoder is highly desirable. We propose to
do this by defining sentence specific tables. For
example, given a source sentence s, the phrase-
table contains all the possible phrase-pairs condi-
tioned on the context of the source sentence s.
In this paper, the key contributions are,
1. We combine a discriminative training model
with a phrase-based decoder. We ob-
tained comparable results with the state-of-
art phrase-based decoder.
2. We evaluate the performance of the lattice
construction and scoring (LCS) approach to
decoding. We observed that even though the
lexical accuracy obtained using LCS is high,
the performance in terms of sentence con-
struction is low when compared to phrase-
based decoder.
3. We show that the incorporation of syntactic
information (POS tags) in our discriminative
model boosts the performance of translation.
In future, we plan to use richer syntactic fea-
ture functions (which the discriminative ap-
proach allows us to incorporate) to evaluate
the approach.
The paper is organized in the following sec-
tions. Section 2 presents the related work. In
section 3, we describe the training of our model.
In section 4, we present the decoding approaches
(both LCS and phrase-based decoder). We de-
scribe the data used in our experiments in section
5. Section 6 consists of the experiments and re-
sults. Finally we conclude the paper in section 7.
2 Related Work
In this section, we present approaches that are di-
rectly related to our approach. In Direct Trans-
lation Model (DTM) proposed for statistical ma-
chine translation by (Papineni et al, 1998; Och
and Ney, 2002), the authors present a discrimi-
native set-up for natural language understanding
(and MT). They use a slightly modified equation
(in comparison to IBM models) as shown in equa-
tion 1. In equation 1, they consider the translation
model from f ? e (p(e|f)), instead of the the-
oretically sound (after the application of Bayes?
rule), e ? f (p(f |e)) and use grammatical fea-
tures such as the presence of equal number of
35
verbs forms etc.
e? = argmax
e
pTM (e|f) ? pLM (e) (1)
In their model, they use generic feature func-
tions such as language model, cooccurence fea-
tures such as presence of a lexical relationship in
the lexicon. Their search algorithm limited the use
of complex features.
Direct Translation Model 2 (DTM2) (Itty-
cheriah and Roukos, 2007) expresses the phrase-
based translation task in a unified log-linear prob-
abilistic framework consisting of three compo-
nents:
1. a prior conditional distribution P0
2. a number of feature functions ?i() that cap-
ture the effects of translation and language
model
3. the weights of the features ?i that are esti-
mated using MaxEnt training (Berger et al,
1996) as shown in equation 2.
Pr(e|f) = P0(e, j|f)Z exp
?
i
?i?i(e, j, f) (2)
In the above equation, j is the skip reordering
factor for the phrase pair captured by?i() and rep-
resents the jump from the previous source word.
Z represents the per source sentence normaliza-
tion term (Hassan et al, 2009). While a uni-
form prior on the set of futures results in a max-
imum entropy model, choosing other priors out-
put a minimum divergence models. Normalized
phrase count has been used as the prior P0 in the
DTM2 model.
The following decision rule is used to obtain opti-
mal translation.
e? = argmax
e
Pr(e|f)
= argmax
e
M?
m=1
?m?m(f, e)
(3)
The DTM2 model differs from other phrase-
based SMT models in that it avoids the redun-
dancy present in other systems by extracting from
a word aligned parallel corpora a set of minimal
phrases such that no two phrases overlap with
each other (Hassan et al, 2009).
The decoding strategy in DTM2 (Ittycheriah
and Roukos, 2007) is similar to a phrase-based de-
coder except that the score of a particular transla-
tion block is obtained from the maximum entropy
model using the set of feature functions. In our
approach, instead of providing the complete scor-
ing function ourselves, we compute the parame-
ters needed by a phrase based decoder, which in
turn uses these parameters appropriately. In com-
parison with the DTM2, we also use minimal non-
overlapping blocks as the entries in the phrase ta-
ble that we generate.
Xiong et al (2006) present a phrase reordering
model under the ITG constraint using a maximum
entropy framework. They model the reordering
problem as a two-class classification problem, the
classes being straight and inverted. The model is
used to merge the phrases obtained from trans-
lating the segments in a source sentence. The
decoder used is a hierarchical decoder motivated
from the CYK parsing algorithm employing a
beam search algorithm. The maximum entropy
model is presented with features extracted from
the blocks being merged and probabilities are es-
timated using the log-linear equation shown in
(4). The work in addition to lexical features and
collocational features, uses an additional metric
called the information gain ratio (IGR) as a fea-
ture. The authors report an improvement of 4%
BLEU score over the traditional distance based
distortion model upon using the lexical features
alone.
p?(y|x) =
1
Z?(x)
exp(
?
i
?i?i(x, y)) (4)
3 Training
The training process of our approach has two
steps:
1. training the discriminative models for trans-
lation and reordering.
2. integrating the models into a phrase based
decoder.
36
The input to our training step are the word-
alignments between source and target sentences
obtained using GIZA++ (implementation of IBM,
HMM models).
3.1 Training discriminative models
We train two models, one to model the transla-
tion of source blocks, and the other to model the
reordering of source blocks. We call the transla-
tion model a ?context dependent block translation
model? for two reasons.
1. It is concerned with the translation of mini-
mal phrasal units called blocks.
2. The context of the source block is used dur-
ing its translation.
The word alignments are used to obtain the set
of possible target blocks, and are added to the tar-
get vocabulary. A target block b is a sequence of n
words that are paired with a sequence ofm source
words (Ittycheriah and Roukos, 2007). In our ap-
proach, we restrict ourselves to target blocks that
are associated with only one source word. How-
ever, this constraint can be easily relaxed.
Similarly, we call the reordering model, a ?con-
text dependent block distortion model?. For train-
ing, we use the maximum entropy software library
Llama presented in (Haffner, 2006).
3.1.1 Context Dependent Block Translation
Model
In this model, the goal is to predict a target
block given the source word and contextual and
syntactic information. Given a source word and its
lexical context, the model estimates the probabil-
ities of the presence or absence of possible target
blocks (see Figure 1).
The probabilities of the candidate target blocks
are obtained from the maximum entropy model.
The probability pei of a candidate target block ei
is estimated as given in equation 5
pei = P (true|ei, fj , C) (5)
where fj is the source word corresponding to ei
and C is its context.
Using the maximum entropy model, binary
classifiers are trained for every target block in the
context window
source word
word syntactically dependent
SOURCE SENTENCE
target word 1 prob p1
............
target word 2 prob p2
prob pKtarget word K
on source word
Figure 1: Word prediction model
vocabulary. These classifiers predict if a particu-
lar target block should be present given the source
word and its context. This model is similar to the
global lexical selection (GLS) model described in
(Bangalore et al, 2007; Venkatapathy and Banga-
lore, 2009) except that in GLS, the predicted tar-
get blocks are not associated with any particular
source word unlike the case here.
For the set of experiments in this paper, we used
a context of size 6, containing three words to the
left and three words to the right. We also used
the POS tags of words in the context window as
features. In future, we plan to use the words syn-
tactically dependent on a source word as global
context(shown in Figure 1).
3.1.2 Context Dependent Block Distortion
Model
An IBM model 3 like distortion model is
trained to predict the relative position of a source
word in the target given its context. Given a
source word and its context, the model estimates
the probability of particular relative position be-
ing an appropriate position of the source word in
the target (see Figure 2).
context window
source wordSOURCE SENTENCE
0p0
1p1 2p2 wpw?1p?1?2p?2?wp?w ...
...
word syntactically dependent
on source word
Figure 2: Position prediction model
Using a maximum entropy model similar to
37
the one described in the context dependent block
translation model, binary classifiers are trained
for every possible relative position in the target.
These classifiers output a probability distribution
over various relative positions given a source word
and its context.
The word alignments in the training corpus are
used to train the distortion model. While comput-
ing the relative position, the difference in sentence
lengths is also taken into account. Hence, the rela-
tive position of the target block located at position
i corresponding to the source word located at po-
sition j is given in equation 6.
r = round(i ? mn ? j) (6)
where, m is the length of source sentence and n is
the number of target blocks. round is the function
to compute the nearest integer of the argument. If
the source word is not aligned to any target word,
a special symbol ?INF? is used to indicate such a
case. In our model, this symbol is also a part of
the target distribution.
The features used to train this model are the
same as those used for the block translation
model. In order to use further lexical information,
we also incorporated information about the target
word for predicting the distribution. The informa-
tion about possible target words is obtained from
the ?context dependent block translation model?.
The probabilities in this case are measured as
shown in equation 7
pr,ei = P (true|r, ei, fj , C) (7)
3.2 Integration with phrase-based decoder
The discriminative models trained are sentence
specific, i.e. the context of the sentence is used
to make predictions in these models. Hence,
the phrase-based decoder is required to use in-
formation specific to a source sentence. In order
to handle this issue, a different phrase-table and
reordering-table are constructed for every input
sentence. The phrase-table and reordering-table
are constructed using the discriminative models
trained earlier.
In Moses (Koehn et al, 2007), the phrase-
table contains the source phrase, the target phrase
and the various scores associated with the phrase
pair such as phrase translation probability, lexical
weighting, inverse phrase translation probability,
etc.1
In our approach, given a source sentence, the
following steps are followed to construct the
phrase table.
1. Extract source blocks (?words? in this work)
2. Use the ?context dependent block translation
model? to predict the possible target blocks.
The set of possible blocks can be predicted
using two criteria, (1) Probability threshold,
and (2) K-best. Here, we use a threshold
value to prune the set of possible candidates
in the target vocabulary.
3. Use the prediction probabilities to assign
scores to the phrase pairs.
A similar set of steps is used to construct the
reordering-table corresponding to an input sen-
tence in the source language.
4 Decoding
4.1 Decoding with LCS Decoder
The lattice construction and scoring algorithm, as
the name suggests, consists of two steps,
1. Lattice construction
In this step, a lattice representing various
possible target sequences is obtained. In the
approach for global lexical selection (Banga-
lore et al, 2007; Venkatapathy and Banga-
lore, 2009), the input to this step is a bag of
words. The bag of words is used to construct
an initial sequence (a single path lattice). To
this sequence, deletion arcs are added to in-
corporate additional paths (at a cost) that fa-
cilitate deletion of words in the initial se-
quence. This sequence is permuted using a
permutation window in order to construct a
lattice representing possible sequences. The
permutation window is used to control the
search space.
In our experiments, we used a similar process
for sentence construction. Using the con-
text dependent block translation algorithm,
1http://www.statmt.org/moses/?n=FactoredTraining.ScorePhrases
38
we obtain a number of translation blocks for
every source word. These blocks are inter-
connected in order to obtain the initial lattice
(see figure 3).
f_(i?1) f_(i) f_(i+1)
t_(i?1,1)
t_(i?1,2)
t_(i?1,3)
t_(i,2)
t_(i,1) t_(i+1,1)
t_(i+1,2)
t_(i+1,3)
.... ...............
SOURCE SENTENCE
INTIAL TARGET LATTICE
Figure 3: Lattice Construction
To control deletions at various source posi-
tions, deletion nodes may be added to the
initial lattice. This lattice is permuted us-
ing a permutation window to construct a lat-
tice representing possible sequences. Hence,
the parameters that dictate lattice construc-
tion are, (1) Threshold for lexical selection,
(2) Using deletion arcs or not, and (3) Per-
mutation window.
2. Scoring
In this step, each of the paths in the lattice
constructed in the earlier step is scored us-
ing a language model (Haffner, 2006), which
is same as the one used in the sentence con-
struction in global lexical selection models.
It is to be noted that we do not use the dis-
criminative reordering model in this decoder,
and only the language model is used to score
various target sequences.
The path with the lowest score is considered
the best possible target sentence for the given
source sentence. Using this decoder, we con-
ducted experiments on the development set by
varying threshold values and the size of the per-
mutation window. The best parameter values ob-
tained using the development set were used for de-
coding the test corpus.
4.2 Decoding with Moses Decoder
In this approach, the phrase-table and the
reordering-table are constructed using the dis-
criminative model for every source sentence (see
section 3.2). These tables are then used by the
state-of-art Moses decoder to obtain correspond-
ing translations.
The various training and decoding parameters
of the discriminative model are computed by ex-
haustively exploring the parameter space, and cor-
respondingly measuring the output quality on the
development set. The best set of parameters were
used for decoding the sentences in the test corpus.
We modified the weights assigned by MOSES to
the translation model, reordering model and lan-
guage model. Experiments were conducted by
performing pruning on the options in the phrase
table and by using the word penalty feature in
MOSES.
We trained a language model of order 5 built on
the entire EUROPARL corpus using the SRILM
package. The method uses improved Kneser-Ney
smoothing algorithm (Chen and Goodman, 1999)
to compute sequence probabilities.
5 Dataset
The experiments were conducted on the Spanish-
English language pair. The latest version of the
Europarl corpus(version-5) was used in this work.
A small set of 200K sentences was selected from
the training set to conduct the experiments. The
test and development sets containing 2525 sen-
tences and 2051 sentences respectively were used,
without making any changes.
Corpus No. of sentences Source Target
Training 200000 59591 36886
Testing 2525 10629 8905
Development 2051 8888 7750
Monolingual 200000 n.a 36886
English (LM)
Table 1: Corpus statistics for Spanish-English cor-
pus.
6 Experiments and Results
The output of our experiments was evaluated us-
ing two metrics, (1) BLEU (Papineni et al, 2002),
and (2) Lexical Accuracy (LexAcc). Lexical ac-
curacy measures the similarity between the un-
ordered bag of words in the reference sentence
39
against the unordered bag of words in the hypoth-
esized translation. Lexical accuracy is a measure
of the fidelity of lexical transfer from the source
to the target sentence, independent of the syntax
of the target language (Venkatapathy and Banga-
lore, 2009). We report lexical accuracies to show
the performance of LCS decoding in comparison
with the baseline system.
We first present the results of the state-of-art
phrase-based model (Moses) trained on a paral-
lel corpus. We treat this as our baseline. The re-
ordering feature used is msd-bidirectional, which
allows for all possible reorderings over a speci-
fied distortion limit. The baseline accuracies are
shown in table 2.
Corpus BLEU Lexical Accuracy
Development 0.1734 0.448
Testing 0.1823 0.492
Table 2: Baseline Accuracy
We conduct two types of experiments to test our
approach.
1. Experiments using lexical features (see sec-
tion 6.1), and
2. Experiments using syntactic features (see
section 6.2).
6.1 Experiments using Lexical Features
In this section, we present results of our exper-
iments that use only lexical features. First, we
measure the translation accuracy using LCS de-
coding. On the development set, we explored the
set of decoding parameters (as described in sec-
tion 4.1) to compute the optimal parameter val-
ues. The best lexical accuracy obtained on the de-
velopment set is 0.4321 and the best BLEU score
obtained is 0.0923 at a threshold of 0.17 and a per-
mutation window size of value 3. The accuracies
corresponding to a few other parameter values are
shown in Table 3.
On the test data, we obtained a lexical accu-
racy of 0.4721 and a BLEU score of 0.1023. As
we can observe, the BLEU score obtained using
the LCS decoding technique is low when com-
pared to the BLEU score of the state-of-art sys-
tem. However, the lexical accuracy is comparable
Threshold Perm. Window LexAcc BLEU
0.16 3 0.4274 0.0914
0.17 3 0.4321 0.0923
0.18 3 0.4317 0.0918
0.16 4 0.4297 0.0912
0.17 4 0.4315 0.0915
Table 3: Lexical Accuracies of Lattice-Output us-
ing lexical features alone for various parameter
values
to the lexical accuracy of Moses. This shows that
the discriminative model provides good lexical se-
lection, while the sentence construction technique
does not perform as expected.
Next, we present the results of the Moses based
decoder that uses the discriminative model (see
section 3.2). In our experiments, we did not use
MERT training for tuning the Moses parameters.
Rather, we explore a set of possible parameter val-
ues (i.e. weights of the translation model, reorder-
ing model and the language model) to check the
performance. We show the BLEU scores obtained
on the development set using Moses decoder in
Table 4.
Reordering LM Translation BLEU
weight(d) weight(l) weight(t)
0 0.6 0.3 0.1347
0 0.6 0.6 0.1354
0.3 0.6 0.3 0.1441
0.3 0.6 0.6 0.1468
Table 4: BLEU for different weight values using
lexical features only
On the test set, we obtained a BLEU score of
0.1771. We observe that both the lexical accuracy
and the BLEU scores obtained using the discrim-
inative training model combined with the Moses
decoder are comparable to the state-of-art results.
The summary of the results obtained using three
approaches and lexical feature functions is pre-
sented in Table 5.
6.2 Experiments using Syntactic Features
In this section, we present the effect of incorpo-
rating syntactic features using our model on the
40
Approach BLEU LexAcc
State-of-art(MOSES) 0.1823 0.492
LCS decoding 0.1023 0.4721
Moses decoder trained
using a discriminative 0.1771 0.4841
model
Table 5: Translation accuracies using lexical fea-
tures for different approaches
translation accuracies. Table 6 presents the results
of our approach that uses syntactic features at dif-
ferent parameter values. Here, we can observe
that the translation accuracies (both LexAcc and
BLEU) are better than the model that uses only
lexical features.
Reordering LM Translation BLEU
weight(d) weight(l) weight(t)
0 0.6 0.3 0.1661
0 0.6 0.6 0.1724
0.3 0.6 0.3 0.1780
0.3 0.6 0.6 0.1847
Table 6: BLEU for different weight values using
syntactic features
Table 7 shows the comparative performance of
the model using syntactic as well as lexical fea-
tures against the one with lexical features func-
tions only.
Model BLEU LexAcc
Lexical features 0.1771 0.4841
Lexical+Syntactic 0.201 0.5431
features
Table 7: Comparison between translation accura-
cies from models using syntactic and lexical fea-
tures
On the test set, we obtained a BLEU score of
0.20 which is an improvement of 2.3 points over
the model that uses lexical features alone. We also
obtained an increase of 6.1% in lexical accuracy
using this model with syntactic features as com-
pared to the model using lexical features only.
7 Conclusions and Future Work
In this paper, we presented an approach to statisti-
cal machine translation that combines the power
of a discriminative model (for training a model
for Machine Translation), and the standard beam-
search based decoding technique (for the transla-
tion of an input sentence). The key contributions
are:
1. We incorporated a discriminative model in
a phrase-based decoder. We obtained com-
parable results with the state-of-art phrase-
based decoder (see section 6.1). The ad-
vantage in using our approach is that it has
the flexibility to incorporate richer contextual
and linguistic feature functions.
2. We show that the incorporation of syntac-
tic information (POS tags) in our discrimina-
tive model boosted the performance of trans-
lation. The lexical accuracy using our ap-
proach improved by 6.1% when syntactic
features were used in addition to the lexi-
cal features. Similarly, the BLEU score im-
proved by 2.3 points when syntactic features
were used compared to the model that uses
lexical features alone. The accuracies are
likely to improve when richer linguistic fea-
ture functions (that use parse structure) are
incorporated in our approach.
In future, we plan to work on:
1. Experiment with rich syntactic and structural
features (parse tree-based features) using our
approach.
2. Experiment on other language pairs such as
Arabic-English and Hindi-English.
3. Improving LCS decoding algorithm using
syntactic cues in the target (Venkatapathy
and Bangalore, 2007) such as supertags.
References
Bangalore, S., P. Haffner, and S. Kanthak. 2007. Statistical machine transla-
tion through global lexical selection and sentence reconstruction. In An-
nual Meeting-Association for Computational Linguistics, volume 45, page
152.
Berger, A.L., V.J.D. Pietra, and S.A.D. Pietra. 1996. A maximum en-
tropy approach to natural language processing. Computational linguistics,
22(1):39?71.
41
Brown, P.F., V.J.D. Pietra, S.A.D. Pietra, and R.L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter estimation. Computa-
tional linguistics, 19(2):263?311.
Chen, S.F. and J. Goodman. 1999. An empirical study of smoothing
techniques for language modeling. Computer Speech and Language,
13(4):359?394.
Haffner, P. 2006. Scaling large margin classifiers for spoken language under-
standing. Speech Communication, 48(3-4):239?261.
Hassan, H., K. Sima?an, and A. Way. 2009. A syntactified direct translation
model with linear-time decoding. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Processing: Volume 3-Volume
3, pages 1182?1191. Association for Computational Linguistics.
Ittycheriah, A. and S. Roukos. 2007. Direct translation model 2. In Proceed-
ings of NAACL HLT, pages 57?64.
Koehn, P. and H. Hoang. 2007. Factored translation models. In Pro-
ceedings of the 2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
Koehn, P., F.J. Och, and D. Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics on Human Language
Technology-Volume 1, pages 48?54. Association for Computational Lin-
guistics.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi,
B. Cowan, W. Shen, C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In Annual meeting-association
for computational linguistics, volume 45, page 2.
Och, F.J. and H. Ney. 2002. Discriminative training and maximum entropy
models for statistical machine translation. In Proceedings of ACL, vol-
ume 2, pages 295?302.
Och, F.J., C. Tillmann, H. Ney, et al 1999. Improved alignment models
for statistical machine translation. In Proc. of the Joint SIGDAT Conf.
on Empirical Methods in Natural Language Processing and Very Large
Corpora, pages 20?28.
Papineni, KA, S. Roukos, and RT Ward. 1998. Maximum likelihood and
discriminative training of directtranslation models. In Acoustics, Speech
and Signal Processing, 1998. Proceedings of the 1998 IEEE International
Conference on, volume 1.
Papineni, K., S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for
automatic evaluation of machine translation. In Proceedings of the 40th
annual meeting on association for computational linguistics, pages 311?
318. Association for Computational Linguistics.
Venkatapathy, S. and S. Bangalore. 2007. Three models for discriminative
machine translation using Global Lexical Selection and Sentence Recon-
struction. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on
Syntax and Structure in Statistical Translation, pages 96?102. Association
for Computational Linguistics.
Venkatapathy, Sriram and Srinivas Bangalore. 2009. Discriminative Machine
Translation Using Global Lexical Selection. ACM Transactions on Asian
Language Information Processing, 8(2).
Xiong, D., Q. Liu, and S. Lin. 2006. Maximum entropy based phrase reorder-
ing model for statistical machine translation. In Proceedings of the 21st
International Conference on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational Linguistics, page 528.
Association for Computational Linguistics.
42
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 51?56,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Reducing the Impact of Data Sparsity in Statistical Machine Translation
Karan Singla
1
, Kunal Sachdeva
1
, Diksha Yadav
1
, Srinivas Bangalore
2
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
AT&T Labs-Research
Abstract
Morphologically rich languages generally
require large amounts of parallel data to
adequately estimate parameters in a statis-
tical Machine Translation(SMT) system.
However, it is time consuming and expen-
sive to create large collections of parallel
data. In this paper, we explore two strate-
gies for circumventing sparsity caused by
lack of large parallel corpora. First, we ex-
plore the use of distributed representations
in an Recurrent Neural Network based lan-
guage model with different morphological
features and second, we explore the use of
lexical resources such as WordNet to over-
come sparsity of content words.
1 Introduction
Statistical machine translation (SMT) models es-
timate parameters (lexical models, and distortion
model) from parallel corpora. The reliability of
these parameter estimates is dependent on the size
of the corpora. In morphologically rich languages,
this sparsity is compounded further due to lack of
large parallel corpora.
In this paper, we present two approaches that
address the issue of sparsity in SMT models for
morphologically rich languages. First, we use an
Recurrent Neural Network (RNN) based language
model (LM) to re-rank the output of a phrase-
based SMT (PB-SMT) system and second we use
lexical resources such as WordNet to minimize the
impact of Out-of-Vocabulary(OOV) words on MT
quality. We further improve the accuracy of MT
using a model combination approach.
The rest of the paper is organized as follows.
We first present our approach of training the base-
line model and source side reordering. In Section
4, we present our experiments and results on re-
ranking the MT output using RNNLM. In Section
5, we discuss our approach to increase the cover-
age of the model by using synset ID?s from the
English WordNet (EWN). Section 6 describes our
experiments on combining the model with synset
ID?s and baseline model to further improve the
translation accuracy followed by results and obser-
vations sections.We conclude the paper with future
work and conclusions.
2 Related Work
In this paper, we present our efforts of re-
ranking the n-best hypotheses produced by a PB-
MT (Phrase-Based MT) system using RNNLM
(Mikolov et al., 2010) in the context of an English-
Hindi SMT system. The re-ranking task in ma-
chine translation can be defined as re-scoring the
n-best list of translations, wherein a number of
language models are deployed along with fea-
tures of source or target language. (Dungarwal
et al., 2014) described the benefits of re-ranking
the translation hypothesis using simple n-gram
based language model. In recent years, the use
of RNNLM have shown significant improvements
over the traditional n-gram models (Sundermeyer
et al., 2013). (Mikolov et al., 2010) and (Liu et
al., 2014) have shown significant improvements in
speech recognition accuracy using RNNLM . Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. We have
also explored the use of morphological features
(Hindi being a morphologically rich language) in
RNNLM and deduced that these features further
improve the baseline RNNLM in re-ranking the n-
best hypothesis.
Words in natural languages are richly diverse
so it is not possible to cover all source language
words when training an MT system. Untranslated
out-of-vocabulary (OOV) words tend to degrade
the accuracy of the output produced by an MT
model. Huang (2010) pointed to various types
of OOV words which occur in a data set ? seg-
51
mentation error in source language, named enti-
ties, combination forms (e.g. widebody) and ab-
breviations. Apart from these issues, Hindi being
a low-resourced language in terms of parallel cor-
pora suffers from data sparsity.
In the second part of the paper, we address the
problem of data sparsity with the help of English
WordNet (EWN) for English-Hindi PB-SMT. We
increase the coverage of content words (exclud-
ing Named-Entities) by incorporating sysnset in-
formation in the source sentences.
Combining Machine Translation (MT) systems
has become an important part of statistical MT in
past few years. Works by (Razmara and Sarkar,
2013; Cohn and Lapata, 2007) have shown that
there is an increase in phrase coverage when com-
bining different systems. To get more coverage of
unigrams in phrase-table, we have explored sys-
tem combination approaches to combine models
trained with synset information and without synset
information. We have explored two methodolo-
gies for system combination based on confusion
matrix(dynamic) (Ghannay et al., 2014) and mix-
ing models (Cohn and Lapata, 2007).
3 Baseline Components
3.1 Baseline Model and Corpus Statistics
We have used the ILCI corpora (Choudhary and
Jha, 2011) for our experiments, which contains
English-Hindi parallel sentences from tourism and
health domain. We randomly divided the data into
training (48970), development (500) and testing
(500) sentences and for language modelling we
used news corpus of English which is distributed
as a part of WMT?14 translation task. The data is
about 3 million sentences which also contains MT
training data.
We trained a phrase based (Koehn et al., 2003)
MT system using the Moses toolkit with word-
alignments extracted from GIZA++ (Och and Ney,
2000). We have used the SRILM (Stolcke and
others, 2002) with Kneser-Ney smoothing (Kneser
and Ney, 1995) for training a language model for
the first stage of decoding. The result of this base-
line system is shown in Table 1.
3.2 English Transformation Module
Hindi is a relatively free-word order language and
generally tends to follow SOV (Subject-Object-
Verb) order and English tends to follow SVO
(Subject-Verb-Object) word order. Research has
Number of Number of Number of
Training Development Evaluation BLEU
Sentences Sentences Sentences
48970 500 500 20.04
Table 1: Baseline Scores for Phrase-based Moses
Model
shown that pre-ordering source language to con-
form to target language word order significantly
improves translation quality (Collins et al., 2005).
We created a re-ordering module for transform-
ing an English sentence to be in the Hindi order
based on reordering rules provided by Anusaaraka
(Chaudhury et al., 2010). The reordering rules are
based on parse output produced by the Stanford
Parser (Klein and Manning, 2003).
The transformation module requires the text to
contain only surface form of words, however, we
extended it to support surface form along with its
factors such as lemma and Part of Speech (POS).
Input : the girl in blue shirt is my sister
Output : in blue shirt the girl is my sister.
Hindi : neele shirt waali ladki meri bahen hai (
blue) ( shirt) (Mod)(girl)(my)(sister)(Vaux)
With this transformation, the English sentence
is structurally closer to the Hindi sentence which
leads to better phrase alignments. The model
trained with the transformed corpus produces a
new baseline score of 21.84 BLEU score an
improvement over the earlier baseline of 20.04
BLEU points.
4 Re-Ranking Experiments
In this section, we describe the results of re-
ranking the output of the translation model us-
ing Recurrent Neural Networks (RNN) based lan-
guage models using the same data which is used
for language modelling in the baseline models.
Unlike traditional n-gram based discrete lan-
guage models, RNN do not make the Markov as-
sumption and potentially can take into account
long-term dependencies between words. Since the
words in RNNs are represented as continuous val-
ued vectors in low dimensions allowing for the
possibility of smoothing using syntactic and se-
mantic features. In practice, however, learning
long-term dependencies with gradient descent is
difficult as described by (Bengio et al., 1994) due
to diminishing gradients.
We have integrated the approach of re-scoring
52
100 200 300 400 500
22
24
26
28
30
Number of Hypotheses
B
L
E
U
s
c
o
r
e
s
Baseline
POS
NONE
Lemma
Oracle
All
Figure 1: BLEU Scores for Re-ranking experi-
ments with RNNLM using different feature com-
binations.
n-best output using RNNLM which has also been
shown to be helpful by (Liu et al., 2014). Shi
(2012) also showed the benefits of using RNNLM
with contextual and linguistic features. Follow-
ing their work, we used three type of features for
building an RNNLM for Hindi : lemma (root),
POS, NC (number-case). The data used was a
Wikipedia dump, MT training data, news arti-
cles which had approximately 500,000 Hindi sen-
tences. Features were extracted using paradigm-
based Hindi Morphological Analyzer
1
Figure 1 illustrates the results of re-ranking per-
formed using RNNLM trained with various fea-
tures. The Oracle score is the highest achievable
score in a re-ranking experiment. This score is
computed based on the best translation out of n-
best translations. The best translation is found us-
ing the cosine similarity between the hypothesis
and the reference translation. It can be seen from
Figure 1, that the LM with only word and POS in-
formation is inferior to all other models. However,
morphological features like lemma, number and
case information help in re-ranking the hypothesis
significantly. The RNNLM which uses all the fea-
tures performed the best for the re-ranking exper-
iments achieving a BLEU score of 26.91, after re-
scoring 500-best obtained from the pre-order SMT
model.
1
We have used the HCU morph-analyzer.
System BLEU
Baseline 21.84
Rescoring 500-best with RNNLM
Features
NONE 25.77
POS 24.36
Lemma(root) 26.32
ALL(POS+Lemma+NC) 26.91
Table 2: Rescoring results of 500-best hypotheses
using RNNLM with different features
5 Using WordNet to Reduce Data
Sparsity
We extend the coverage of our source data by us-
ing synonyms from the English WordNet (EWN).
Our main motivation is to reduce the impact of
OOV words on output quality by replacing words
in a source sentence with their corresponding
synset IDs. However, choosing the appropriate
synset ID based upon its context and morphologi-
cal information is important. For sense selection,
we followed the approach used by (Tammewar et
al., 2013), which is also described further in this
section in the context of our task. We ignored
words that are regarded as Named-Entities as in-
dicated by Stanford NER tagger, as they should
not have synonyms in any case.
5.1 Sense Selection
Words are ambiguous, independent of their sen-
tence context. To choose an appropriate sense ac-
cording to the context for a lexical item is a chal-
lenging task typically termed as word-sense dis-
ambiguation. However, the syntactic category of
a lexical item provides an initial cue for disam-
biguating a lexical item. Among the varied senses,
we filter out the senses that are not the same POS
tag as the lexical item. But words are not just am-
biguous across different syntactic categories but
are also ambiguous within a syntactic category. In
the following, we discuss our approaches to select
the sense of a lexical item best suited in a given
context within a given category. Also categories
were filtered so that only content words get re-
placed with synset IDs.
5.1.1 Intra-Category Sense Selection
First Sense: Among the different senses,we se-
lect the first sense listed in EWN corresponding to
the POS-tag of a given lexical item. The choice is
motivated by our observation that the senses of a
53
lexical item are ordered in the descending order of
their frequencies of usage in the lexical resource.
Merged Sense: In this approach, we merge all
the senses listed in EWN corresponding to the
POS-tag of the given lexical item. The motivation
behind this strategy is that the senses in the EWN
for a particular word-POS pair are too finely clas-
sified resulting in classification of words that may
represent the same concept, are classified into dif-
ferent synsets. For example : travel and go can
mean the same concept in a similar context but the
first sense given by EWN is different for these two
words. Therefore, we merge all the senses for a
word into a super sense ( synset ID of first word
occurred in data), which is given to all its syn-
onyms even if it occurs in different synset IDs.
5.2 Factored Model
Techniques such as factored modelling (Koehn
and Hoang, 2007) are quite beneficial for Trans-
lation from English to Hindi language as shown
by (Ramanathan et al., 2008). When we replace
words in a source sentence with the synset ID?as,
we tend to lose morphological information associ-
ated with that word. We add inflections as features
in a factored SMT model to minimize the impact
of this replacement.
We show the results of the processing steps on
an example sentence below.
Original Sentence : Ram is going to market to
buy apples
New Sentence : Ram is Synset(go.v.1)
to Synset(market.n.0) to Synset(buy.v.1)
Synset(apple.n.1)
Sentence with synset ID: Ram E is E
Synset(go.v.1) ing to E Synset(market.n.0) E
to E Synset(buy.v.1) E Synset(apple.n.1) s
Then English sentences were reordered to Hindi
word-order using the module discussed in Section
3.
Reordered Sentence: Ram E Synset(apple.n.1) s
Synset(buy.v.1) E to E Synset(market.n.0) E to E
Synset(go.v.1) ing is E
In Table 3, the second row shows the BLEU
scores for the models in which there are synset IDs
for the source side. It can be seen that the factored
model also shows significant improvement in the
results.
6 Combining MT Models
Combining Machine translation (MT) systems has
become an important part of Statistical MT in
the past few years. There are two dominant ap-
proaches. (1) a system combination approach
based on confusion networks (CN) (Rosti et al.,
2007), which can work dynamically in combin-
ing the systems. (2) Combine the models by lin-
early interpolating and then using MERT to tune
the combined system.
6.1 Combination based on confusion
networks
We used the tool MANY (Barrault, 2010) for sys-
tem combination. However, since the tool is con-
figured to work with TERp evaluation metric, we
modified it to use METEOR (Gupta et al., 2010)
metric since it has been shown by (Kalyani et al.,
2014), that METEOR evaluation metric is better
correlated to human evaluation for morphologi-
cally rich Indian Languages.
6.2 Linearly Interpolated Combination
In this approach, we combined phrase-tables of
the two models (Eng (sysnset) - Hindi and Base-
line) using linear interpolation. We combined the
two models with uniform weights ? 0.5 for each
model, in our case. We again tuned this model
with the new interpolated phrase-table using stan-
dard algorithm MERT.
7 Experiments and Results
As can be seen in Table 3, the model with synset
information led to reduction in OOV words. Even
though BLEU score decreased, but METEOR
score improved for all the experiments based on
using synset IDs in the source sentence, but it has
been shown by (Gupta et al., 2010) that METEOR
is a better evaluation metrics for morphologically
rich languages. Also, when synset ID?as are used
instead of words in the source language, the sys-
tem makes incorrect morphological choices. Ex-
ample : going and goes will be replaced by same
synset ID ?aSynset(go.v.1)?a, so this has lead to loss
of information in the phrase-table but METEOR
catches these complexities as it considers features
like stems, synonyms for its evaluation metrics
and hence showed better improvements compared
to BLEU metric. Last two rows of Table 3 show
results for combination experiments and Mixture
Model (linearly interpolated model) showed best
54
System #OOV words BLEU Meteor
Baseline 253 21.8 .492
Eng(Synset ID)-Hindi
Baseline 237 19.2 .494
*factor(inflections) 225 20.3 .506
Ensembled Decoding 213 21.0 .511
Mixture Model 210 21.2 .519
Table 3: Results for the model in which there were Synset ID?s instead of word in English data
results with significant reduction in OOV words
and also some gains in METEOR score.
8 Observations
In this section, we study the coverage of different
models by categorizing the OOV words into 5 cat-
egories.
? NE(Named Entities) : As the data was
from Health & Tourism domain, these words
were mainly the names of the places and
medicines.
? VB : types of verb forms
? NN : types of nouns and pronouns
? ADJ : all adjectives
? AD : adverbs
? OTH : there were some words which did not
mean anything in English
? SM : There were some occasional spelling
mistakes seen in the test data.
Note : There were no function words seen in the
OOV(un-translated) words
Cat. Baseline Eng(synset)-Hin MixtureModel
NE 120 121 115
VB 47 37 27
NN 76 60 47
ADJ 22 15 12
AD 5 5 4
OTH 2 2 2
SM 8 8 8
Table 4: OOV words in Different Models
As this analysis was done on a small dataset and
for a fixed domain, the OOV words were few in
number as it can be seen in Table 4. But the OOV
words across the different models reduced as ex-
pected. The NE words remained almost the same
for all the three models but OOV words from cate-
gory VB,NN,ADJ decreased for Eng(synset)-Hin
model and Mixture model significantly.
9 Future Work
In the future, we will work on using the two ap-
proaches discussed: Re-Ranking & using lexical
resources to reduce sparsity together in a system.
We will work on exploring syntax based features
for RNNLM and we are planning to use a better
method for sense selection and extending this con-
cept for more language pairs. Word-sense disam-
biguation can be used for choosing more appro-
priate sense when the translation model is trained
on a bigger data data set. Also we are looking for
unsupervised techniques to learn the replacements
for words to reduce sparsity and ways to adapt our
system to different domains.
10 Conclusions
In this paper, we have discussed two approaches
to address sparsity issues encountered in training
SMT models for morphologically rich languages
with limited amounts of parallel corpora. In the
first approach we used an RNNLM enriched with
morphological features of the target words and
show the BLEU score to improve by 5 points. In
the second approach we use lexical resource such
as WordNet to alleviate sparsity.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Sriram Chaudhury, Ankitha Rao, and Dipti M Sharma.
2010. Anusaaraka: An expert system based machine
translation system. In Natural Language Processing
55
and Knowledge Engineering (NLP-KE), 2010 Inter-
national Conference on, pages 1?6. IEEE.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd annual
meeting on association for computational linguis-
tics, pages 531?540. Association for Computational
Linguistics.
Piyush Dungarwal, Rajen Chatterjee, Abhijit Mishra,
Anoop Kunchukuttan, Ritesh Shah, and Pushpak
Bhattacharyya. 2014. The iit bombay hindi-english
translation system at wmt 2014. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 90?96, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Chung-chi Huang, Ho-ching Yen, and Jason S Chang.
2010. Using sublexical translations to handle the
oov problem in mt. In Proceedings of The Ninth
Conference of the Association for Machine Transla-
tion in the Americas (AMTA).
Aditi Kalyani, Hemant Kamud, Sashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt
systems for hindi to english translation. In In-
ternational Journal of Computer Applications, vol-
ume 89.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423?430. Asso-
ciation for Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In EMNLP-CoNLL, pages 868?876.
Citeseer.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
X Liu, Y Wang, X Chen, MJF Gales, and PC Wood-
land. 2014. Efficient lattice rescoring using recur-
rent neural network language models.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 440?447. Association for
Computational Linguistics.
Ananthakrishnan Ramanathan, Jayprasad Hegde,
Ritesh M Shah, Pushpak Bhattacharyya, and
M Sasikumar. 2008. Simple syntactic and morpho-
logical processing can help english-hindi statistical
machine translation. In IJCNLP, pages 513?520.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Yangyang Shi, Pascal Wiggers, and Catholijn M
Jonker. 2012. Towards recurrent neural networks
language models with linguistic and contextual fea-
tures. In INTERSPEECH.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Martin Sundermeyer, Ilya Oparin, J-L Gauvain, Ben
Freiberg, R Schluter, and Hermann Ney. 2013.
Comparison of feedforward and recurrent neural
network language models. In Acoustics, Speech and
Signal Processing (ICASSP), 2013 IEEE Interna-
tional Conference on, pages 8430?8434. IEEE.
Aniruddha Tammewar, Karan Singla, Srinivas Banga-
lore, and Michael Carl. 2013. Enhancing asr by
mt using semantic information from hindiwordnet.
In Proceedings of ICON-2013: 10th International
Conference on Natural Language Processing.
56
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85?91,
October 29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploring System Combination approaches for Indo-Aryan MT Systems
Karan Singla
1
, Nishkarsh Shastri
2
, Megha Jhunjhunwala
2
, Anupam Singh
3
,
Srinivas Bangalore
4
, Dipti Misra Sharma
1
1
LTRC IIIT Hyderabad,
2
IIT-Kharagpur,
3
NIT-Durgapur,
4
AT&T Labs-Research
Abstract
Statistical Machine Translation (SMT)
systems are heavily dependent on the qual-
ity of parallel corpora used to train transla-
tion models. Translation quality between
certain Indian languages is often poor due
to the lack of training data of good qual-
ity. We used triangulation as a technique
to improve the quality of translations in
cases where the direct translation model
did not perform satisfactorily. Triangula-
tion uses a third language as a pivot be-
tween the source and target languages to
achieve an improved and more efficient
translation model in most cases. We also
combined multi-pivot models using linear
mixture and obtained significant improve-
ment in BLEU scores compared to the di-
rect source-target models.
1 Introduction
Current SMT systems rely heavily on large quan-
tities of training data in order to produce good
quality translations. In spite of several initiatives
taken by numerous organizations to generate par-
allel corpora for different language pairs, train-
ing data for many language pairs is either not
yet available or is insufficient for producing good
SMT systems. Indian Languages Corpora Initia-
tive (ILCI) (Choudhary and Jha, 2011) is currently
the only reliable source for multilingual parallel
corpora for Indian languages however the number
of parallel sentences is still not sufficient to create
high quality SMT systems.
This paper aims at improving SMT systems
trained on small parallel corpora using various re-
cently developed techniques in the field of SMTs.
Triangulation is a technique which has been found
to be very useful in improving the translations
when multilingual parallel corpora are present.
Triangulation is the process of using an interme-
diate language as a pivot to translate a source lan-
guage to a target language. We have used phrase
table triangulation instead of sentence based tri-
angulation as it gives better translations (Utiyama
and Isahara, 2007). As triangulation technique ex-
plores additional multi parallel data, it provides
us with separately estimated phrase-tables which
could be further smoothed using smoothing meth-
ods (Koehn et al. 2003). Our subsequent approach
will explore the various system combination tech-
niques through which these triangulated systems
can be utilized to improve the translations.
The rest of the paper is organized as follows.
We will first talk about the some of the related
works and then we will discuss the facts about the
data and also the scores obtained for the baseline
translation model. Section 3 covers the triangu-
lation approach and also discusses the possibility
of using combination approaches for combining
triangulated and direct models. Section 4 shows
results for the experiments described in previous
section and also describes some interesting obser-
vations from the results. Section 5 explains the
conclusions we reached based on our experiments.
We conclude the paper with a section about our fu-
ture work.
2 Related Works
There are various works on combining the tri-
angulated models obtained from different pivots
with the direct model resulting in increased con-
fidence score for translations and increased cov-
erage by (Razmara and Sarkar, 2013; Ghannay et
al., 2014; Cohn and Lapata, 2007). Among these
techniques we explored two of the them. The first
one is the technique based on the confusion ma-
trix (dynamic) (Ghannay et al., 2014) and the other
one is based on mixing the models as explored
by (Cohn and Lapata, 2007). The paper also dis-
cusses the better choice of combination technique
85
among these two when we have limitations on
training data which in our case was small and re-
stricted to a small domain (Health & Tourism).
As suggested in (Razmara and Sarkar, 2013),
we have shown that there is an increase in phrase
coverage when combining the different systems.
Conversely we can say that out of vocabulary
words (OOV) always decrease in the combined
systems.
3 Baseline Translation Model
In our experiment, the baseline translation model
used was the direct system between the source and
target languages which was trained on the same
amount of data as the triangulated models. The
parallel corpora for 4 Indian languages namely
Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla
(bn) was taken from Indian Languages Corpora
Initiative (ILCI) (Choudhary and Jha, 2011) . The
parallel corpus used in our experiments belonged
to two domains - health and tourism and the train-
ing set consisted of 28000 sentences. The develop-
ment and evaluation set contained 500 sentences
each. We used MOSES (Koehn et al., 2007) to
train the baseline Phrase-based SMT system for all
the language pairs on the above mentioned paral-
lel corpus as training, development and evaluation
data. Trigram language models were trained using
SRILM (Stolcke and others, 2002). Table 1 below
shows the BLEU score for all the trained pairs.
Language Pair BLEU Score
bn-mt 18.13
mt-bn 21.83
bn-gj 22.45
gj-mt 23.02
gj-bn 24.26
mt-gj 25.5
hn-mt 30.01
hn-bn 32.92
bn-hn 34.99
mt-hn 36.82
hn-gj 40.06
gj-hn 43.48
Table 1: BLEU scores of baseline models
4 Triangulation: Methodology and
Experiment
We first define the term triangulation in our con-
text. Each source phrase s is first translated to an
intermediate (pivot) language i, and then to a tar-
get language t. This two stage translation process
is termed as triangulation.
Our basic approach involved making triangu-
lated models by triangulating through different
pivots and then interpolating triangulated models
with the direct source-target model to make our
combined model.
In line with various previous works, we will
be using multiple translation models to overcome
the problems faced due to data sparseness and in-
crease translational coverage. Rather than using
sentence translation (Utiyama and Isahara, 2007)
from source to pivot and then pivot to target, a
phrase based translation model is built.
Hence the main focus of our approach is on
phrases rather than on sentences. Instead of using
combination techniques on the output of several
translation systems, we constructed a combined
phrase table to be used by the decoder thus avoid-
ing the additional inefficiencies observed while
merging the output of various translation systems.
Our method focuses on exploiting the availability
of multi-parallel data, albeit small in size, to im-
prove the phrase coverage and quality of our SMT
system.
Our approach can be divided into different steps
which are presented in the following sections.
4.1 Phrase-table triangulation
Our emphasis is on building an enhanced phrase
table that incorporates the translation phrase tables
of different models. This combined phrase table
will be used by the decoder during translation.
Phrase table triangulation depends mainly on
phrase level combination of the two different
phrase based systems mainly source (src) - pivot
(pvt) and pivot (pvt) - target (tgt) using pivot lan-
guage as a basis for combination. Before stating
the mathematical approach for triangulation, we
present an example.
4.1.1 Basic methodology
Suppose we have a Bengali-Hindi phrase-table
(T
BH
) and a Hindi-Marathi phrase-table (T
HM
).
From these tables, we have to construct a Bengali-
Marathi phrase-table (T
BM
). For that we need
86
Triangulated
System
Full-Triangulation
(phrase-table length)
Triangulation with top 40
(Length of phrase table)
Full Triangulation
(BLEU Score)
Triangulation with top 40
(BLEU SCORE)
gj - hn - mt 3,585,450 1,086,528 24.70 24.66
gj - bn - mt 7,916,661 1,968,383 20.55 20.04
Table 2: Comparison between triangulated systems in systems with full phrase table and the other having
top 40 phrase-table entries
to estimate four feature functions: phrase trans-
lation probabilities for both directions ?(
?
b|m?)
and ?(m?|
?
b), and lexical translation probabilities
for both directions lex(
?
b|m?) and lex(m?|
?
b) where
?
b and m? are Bengali and Marathi phrases that
will appear in our triangulated Bengali-Marathi
phrase-table T
BM
.
?(
?
b|m?) =
?
?
h?T
BH
?T
HM
?(
?
b|
?
h)?(
?
h|m?) (1)
?(m?|
?
b) =
?
?
h?T
BH
?T
HM
?(m?|
?
h)?(
?
h|
?
b) (2)
lex(
?
b|m?) =
?
?
h?T
BH
?T
HM
lex(
?
b|
?
h)lex(
?
h|m?) (3)
lex(m?|
?
b) =
?
?
h?T
BH
?T
HM
lex(m?|
?
h)lex(
?
h|
?
b) (4)
In these equations a conditional independence
assumption has been made that source phrase
?
b
and target phrase m? are independent given their
corresponding pivot phrase(s)
?
h. Thus, we can
derive ?(
?
b|m?), ?(m?|
?
b), lex(
?
b|m?), lex(m?|
?
b) by as-
suming that these probabilities are mutually inde-
pendent given a Hindi phrase
?
h.
The equation given requires that all phrases in
the Hindi-Marathi bitext must also be present in
the Bengali-Hindi bitext. Clearly there would be
many phrases not following the above require-
ment. For this paper we completely discarded the
missing phrases. One important point to note is
that although the problem of missing contextual
phrases is uncommon in multi-parallel corpora, as
it is in our case, it becomes more evident when the
bitexts are taken out from different sources.
In general, wider range of possible translations
are found for any source phrase through triangula-
tion. We found that in the direct model, a source
phrase is aligned to three phrases then there is
high possibility of it being aligned to three phrases
in intermediate language. The intermediate lan-
guage phrases are further aligned to three or more
phrases in target language. This results in increase
in number of translations of each source phrase.
4.1.2 Reducing the size of phrase-table
While triangulation is intuitively appealing, it suf-
fers from a few problems. First, the phrasal trans-
lation estimates are based on noisy automatic word
alignments. This leads to many errors and omis-
sions in the phrase-table. With a standard source-
target phrase-table these errors are only encoun-
tered once, however with triangulation they are en-
countered twice, and therefore the errors are com-
pounded. This leads to much noisier estimates
than in the source-target phrase-table. Secondly,
the increased exposure to noise means that trian-
gulation will omit a greater proportion of large or
rare phrases than the standard method. An align-
ment error in either of the source-intermediate bi-
text or intermediate-target bitext can prevent the
extraction of a source-target phrase pair.
As will be explained in the next section, the sec-
ond kind of problem can be ameliorated by using
the triangulated phrase-based table in conjunction
with the standard phrase based table referred to as
direct src-to-pvt phrase table in our case.
For the first kind of problem, not only the com-
pounding of errors leads to increased complex-
ity but also results in an absurdly large triangu-
lated phrase based table. To tackle the problem of
unwanted phrase-translation, we followed a novel
approach.
A general observation is that while triangulat-
ing between src-pvt and pvt-tgt systems, the re-
sultant src-tgt phrase table formed will be very
large since for a translation s? to
?
i in the src-to-
pvt table there may be many translations from
?
i to
?
t1,
?
t2...
?
tn. For example, the Bengali-Hindi
phrase-table(T
BH
) consisted of 846,106 transla-
tions and Hindi-Marathi phrase-table(T
HM
) con-
sisted of 680,415 translations and after triangu-
lating these two tables our new Bengali-Marathi
triangulated table(T
BM
) consisted of 3,585,450
translations as shown in Table 2. Tuning with
such a large phrase-table is complex and time-
consuming. To reduce the complexity of the
phrase-table, we used only the top-40 transla-
87
tions (translation with 40 maximum values of
P (
?
f |e?) for every source phrase in our triangulated
phrase-table(T
BM
) which reduced the phrase table
to 1,086,528 translations.
We relied on P (
?
f |e?)(inverse phrase translation
probability) to choose 40 phrase translations for
each phrase, since in the direct model, MERT
training assigned the most weight to this param-
eter.
It is clearly evident from Table 2 that we have
got a massive reduction in the length of the phrase-
table after taking in our phrase table and still the
results have no significant difference in our output
models.
4.2 Combining different triangulated models
and the direct model
Combining Machine translation (MT) systems has
become an important part of Statistical MT in the
past few years. There have been several works by
(Rosti et al., 2007; Karakos et al., 2008; Leusch
and Ney, 2010);
We followed two approaches
1. A system combination based on confusion
network using open-source tool kit MANY
(Barrault, 2010), which can work dynami-
cally in combining the systems
2. Combine the models by linearly interpolating
them and then using MERT to tune the com-
bined system.
4.2.1 Combination based on confusion
matrix
MANY tool was used for this and initially it was
configured to work with TERp evaluation matrix,
but we modified it to work using METEOR-Hindi
(Gupta et al., 2010), as it has been shown by
(Kalyani et al., 2014), that METEOR evaluation
metric is closer to human evaluation for morpho-
logically rich Indian Languages.
4.2.2 Linearly Interpolated Models
We used two different approaches while merging
the different triangulated models and direct src-tgt
model and we observed that both produced com-
parable results in most cases. We implemented the
linear mixture approach, since linear mixtures of-
ten outperform log-linear ones (Cohn and Lapata,
2007). Note that in our combination approaches
the reordering tables were left intact.
1. Our first approach was to use linear interpola-
tion to combine all the three models (Bangla-
Hin-Marathi, Bangla-Guj-Marathi and di-
rect Bangla-Marathi models) with uniform
weights, i.e 0.3 each in our case.
2. In the next approach, the triangulated phrase
tables are combined first into a single trian-
gulated phrase-table using uniform weights.
The combined triangulated phrase-table and
direct src-tgt phrase table is then combined
using uniform weights. In other words, we
combined all the three systems, Ban-Mar,
Ban-Hin-Mar, and Ban-Guj-Mar with 0.5,
0.25 and 0.25 weights respectively. This
weight distribution reflects the intuition that
the direct model is less noisy than the trian-
gulated models.
In the experiments below, both weight settings
produced comparable results. Since we performed
triangulation only through two languages, we
could not determine which approach would per-
form better. An ideal approach will be to train the
weights for each system for each language pair
using standard tuning algorithms such as MERT
(Zaidan, 2009).
4.2.3 Choosing Combination Approach
In order to compare the approaches on our data,
we performed experiments on Hindi-Marathi pair
following both approaches discussed in Section
4.2.1 and 4.2.2. We also generated triangulated
models through Bengali and Gujarati as pivot lan-
guages.
Also, the approach presented in section 4.2.1
depends heavily on LM (Language Model).In or-
der to study the impact of size, we worked on
training Phrase-based SMT systems with subsets
of data in sets of 5000, 10000, 150000 sentences
and LM was trained for 28000 sentences for com-
paring these approaches. The combination results
were compared following the approach mentioned
in 4.2.1 and 4.2.2.
Table 3, shows that the approach discussed in
4.2.1 works better if there is more data for LM
but we suffer from the limitation that there is no
other in-domain data available for these languages.
From the Table, it can also be seen that combin-
ing systems with the approach explained in 4.2.2
can also give similar or better results if there is
scarcity of data for LM. Therefore we followed the
88
#Training #LM Data Comb-1 Comb-2
5000 28000 21.09 20.27
10000 28000 24.02 24.27
15000 28000 27.10 27.63
Table 3: BLEU scores for Hindi-Marathi Model
comparing approaches described in 3.2.1(Comb-
1) and 3.2.2(Comb-2)
approach from Section 4.2.2 for our experiments
on other language pairs.
5 Observation and Resuslts
Table 4, shows the BLEU scores of triangulated
models when using the two languages out of the
4 Indian languages Hin, Guj, Mar, Ban as source
and target and the remaining two as the pivot lan-
guage. The first row mentions the BLEU score
of the direct src-tgt model for all the language
pairs. The second and third rows provide the tri-
angulated model scores through pivots which have
been listed. The fourth and fifth rows show the
BLEU scores for the combined models (triangu-
lated+direct) with the combination done using the
first and second approach respectively that have
been elucidated in the Section 4.2.2
As expected, both the combined models have
performed better than the direct models in all
cases.
Figure 1: Phrase-table coverage of the evaluation
set for all the language pairs
Figure 1, shows the phrase-table coverage of the
evaluation set for all the language pairs. Phrase-
table coverage is defined as the percentage of un-
igrams in the evaluation set for which translations
are present in the phrase-table. The first bar cor-
responds to the direct model for each language
pair, the second and third bars show the cover-
age for triangulated models through the 2 piv-
ots, while the fourth bar is the coverage for the
combined model (direct+triangulated). The graph
clearly shows that even though the phrase table
coverage may increase or decrease by triangula-
tion through a single pivot the combined model
(direct+triangulated) always gives a higher cover-
age than the direct model.
Moreover, there exists some triangulation mod-
els whose coverage and subsequent BLEU scores
for translation is found to be better than that of the
direct model. This is a particularly interesting ob-
servation as it increases the probability of obtain-
ing better or at least comparable translation mod-
els even when direct source-target parallel corpus
is absent.
6 Discussion
Dravidian languages are different from Indo-aryan
languages but they are closely related amongst
themselves. So we explored similar experiments
with Malayalam-Telugu pair of languages with
similar parallel data and with Hindi as pivot.
The hypothesis was that the direct model for
Malayalam-Telegu would have performed better
due to relatedness of the two languages. However
the results via Hindi were better as can be seen in
Table 5.
As Malayalam-Telegu are comparatively closer
than compared to Hindi, so the results via Hindi
should have been worse but it seems more like a
biased property of training data which considers
that all languages are closer to Hindi, as the trans-
lation data was created from Hindi.
7 Future Work
It becomes increasingly important for us to im-
prove these techniques for such languages having
rare corpora. The technique discussed in the paper
is although efficient but still have scope for im-
provements.
As we have seen from our two approaches of
combining the phrase tables and subsequent in-
terpolation with direct one, the best combination
among the two is also not fixed. If we can find the
89
BLEU scores gj-mt mt-gj gj-hn hn-gj hn-mt mt-hn
Direct model 23.02 25.50 43.48 40.06 30.01 36.82
Triangulated
through pivots
hn 24.66 hn 27.09 mt 36.76 mt 33.69 gj 29.27 gj 33.86
bn 20.04 bn 22.02 bn 35.07 bn 32.66 bn 26.72 bn 31.34
Mixture-1 26.12 27.46 43.23 39.99 33.09 38.50
Mixture-2 26.25 27.32 44.04 41.45 33.36 38.44
(a)
BLEU scores bn-gj gj-bn bn-hn hn-bn mt-bn bn-mt
Direct model 22.45 24.26 34.99 32.92 21.83 18.13
Triangulated
through pivots
hn 23.97 hn 26.26 gj 31.69 gj 29.60 hn 23.80 hn 21.04
mt 20.70 mt 22.32 mt 28.96 mt 27.95 gj 22.41 gj 18.15
Mixture-1 25.80 27.45 35.14 34.77 24.99 22.16
Mixture-2 24.66 27.39 35.02 34.85 24.86 22.75
(b)
Table 4: Table (a) & (b) show results for all language pairs after making triangulated models and then
combining them with linear interpolation with the two approaches described in 3.2.2. In Mixture-1,
uniform weights were given to all three models but in Mixture-2, direct model is given 0.5 weight relative
to the other models (.25 weight to each)
System Blue Score
Direct Model 4.63
Triangulated via Hindi 14.32
Table 5: Results for Malayalam-Telegu Pair for
same data used for other languages
best possible weights to be assigned to each table,
then we can see improvement in translation. This
can be implemented by making the machine learn
from various iterations of combining and adjusting
the scores accordingly.(Nakov and Ng, 2012) have
indeed shown that results show significant devia-
tions associated with different weights assigned to
the tables.
References
Lo??c Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147?155.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Sahar Ghannay, France Le Mans, and Lo?c Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1?5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt sys-
tems for hindi to english translation. arXiv preprint
arXiv:1404.3992.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81?84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
90
Machine Translation and MetricsMATR, pages 315?
320. Association for Computational Linguistics.
Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich lan-
guages. Journal of Artificial Intelligence Research,
44(1):179?222.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252?
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Masao Utiyama and Hitoshi Isahara. 2007. A compari-
son of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484?491.
Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
91
