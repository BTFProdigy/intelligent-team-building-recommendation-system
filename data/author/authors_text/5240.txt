An interlingua aiming at communication on the Web: 
How language-independent can it be? 
Ronaldo Teixeira Martins 
ronaIdo @nilc.icmsc.sc.usp.br 
Lucia Helena Machado Rino 
Iucia @ dc. uf scar.br 
Maria das Graqas Volpe Nunes 
md gvnune @ icmc.sc, usp.br 
Gisele Montilha 
gisele @nilc. icmsc, sc. usp. br 
Osvaldo Novais de Oliveira Jr. 
chu@if.sc.usp.br 
Ndcleo Interinstitucional de Lingiiistica Computacional (NILC/Sio Carlos) 
http://nilc.icmsc.sc.usp.br 
CP 668 - ICMC-USP, 13560-970 Silo Carlos, SP, Brazil 
Abstract 
In this paper, we describe the Universal Networking Language, an interlingua 
to be plugged in a Web environment aiming at allowing for many-to-many 
information exchange, 'many' here referring to many natural anguages. The 
interlingua is embedded in a Knowledge-Base MT system whose language- 
dependent modules comprise an encoder, a decoder, and linguistic resources 
that have been developed by native speakers of each language involved in the 
project. Issues concerning both the interlingua formalism and its foundational 
issues are discussed. 
1. Introduction 
The widespread use of the Web and the 
growing Intemet facilities have sparked 
enormous interest in improving the ways 
people use to communicate. In this context 
multilingual Machine Translation systems 
become prominent, for they allow for a huge 
information flow. To date, MT systems have 
been built under limited conditions, of which 
we highlight two: i) in general, they mirror 
one-to-many(languages) or many(languages)- 
to-one approaches, often involving English at 
the "one" end; ii) communication is reduced 
to basic information exchange, ignoring 
richness and flexibility implied by human 
mind. The first limitation has been seldom 
overcome, since it requires a robust 
environment and research teams that can 
cope with knowledge of several anguages 1, 
to derive precise automatic language 
analyzers and synthesizers. The second 
limitation follows up the first: adding up 
communicative issues to linguistic 
processing/modeling makes still harder to 
overcome MT limitations. 
In this article, we elaborate on work using 
an interlingua conceived to overcome the first 
limitation, i.e., to allow for a many-to-many 
information exchange environment, which 
shall be plugged in a nontraditional Internet 
platform. The goal is to allow interlocutors to 
entangle communication even if they do not 
share the same mother tongue or the English 
Standing, most often, for natural language, or NL. 
24 
language, unlike MT systems that have just 
one language at one of their edges. As the 
main component of a Knowledge-Base MT 
system (hereafter, KBMT), the interlingua 
approach has been developed under the 
Universal Networking Language Project, or 
simply UNL Project. What makes the 
interlingua UNL special is its intended use: 
as an electronic language for networks, it has 
to allow for high quality 2 conversation 
systems involving many languages. As the 
main component of a KBMT system, it has to 
be sufficiently robust o ground research and 
development (R&D) of the language-specific 
modules to be attached to the system. It is 
this latter perspective that is undertaken here: 
from the viewpoint of R&D, we discuss how 
broad, or language-independent, he 
interlingua UNL is, especially focusing on its 
syntax and coverage. In addition to being 
consistent and complete to represent 
meaning, we also consider its sharing by 
researchers all around the world, which is an 
important bottleneck of the UNL Project, 
since information exchange by researchers 
during R&D brings about the problems 
introduced by the interlingua UNL itself, 
concerning both its formalism and 
foundational issues. Before discussing this 
topic in Section 5, we present an overview of 
the UNL Project (Section 2) and describe the 
main features of the interlingua UNL 
(Section 3). In Section 4, we describe the 
UNL system architecture. Hereafter, 
'interlingua UNL' will be simply referred to 
as UNL, the acronym for Universal 
Networking Language. Also, the viewpoint 
presented here is that of interlingua users 
who experience R&D for a given NL, and not 
of its authors. 
2. The UNL Project 
The UNL Project 3 has been launched by 
the United Nations University to foster and 
ease international web communication by 
means of NLP systems. Its main strength lies 
on the development of the UNL, as a unique 
semantic (or meaning) representation that can 
be interchanged with the various languages to 
be integrated in the KBMT system. In the 
UNL Project, plug-in software to encode NL 
texts onto UNL ones (NL-UNL encoders) 
and to decode UNL into NL texts (UNL-NL 
decoders) have been developed by R&D 
groups in their own native languages. The 
modules to process Brazilian Portuguese 4, for 
example, have been developed by a team of 
Portuguese native speakers that comprises 
linguists, computational linguists, and 
computer experts. Such packages will be 
made available in WWW servers and will be 
accessible by browsing through Internet, thus 
overcoming the need for people all around 
the world tO learn the language of their 
interlocutors. Several inguistic groups have 
signed to the. Project, namely: the Indo- 
European (Portuguese, Spanish, French, 
Italian, English, German, Russian, Latvian 
and Hindi), the Semitic (Arabic), the Sino- 
Tibetan (Chinese), the Ural-Altaic 
(Mongolian), the Malayan-Polynesian 
(Indonesian), and the Japanese. 
On the one hand, the main strength of 
the Project is that knowledgeable specialists 
address language-dependent issues of their 
mother tongue, most of which are related to 
R&D of the encoding and decoding modules 
and to the specification of the NL-UNL 
lexicon. On the other hand, this also 
represents a crucial problem faced by the 
project participants, for distinct groups may 
interpret the interlingua specification 
differently. There is thus the need for a 
consensus about the UNL formalism, 
2 By 'high quality' we mean 'at least allowing for 
readability and understandability by any user'. 
3 A description of both, the Project and the UNL itself, 
can be found in http://www.unl.ias.unu.edu/. 
4 Hereafter referred to as Portuguese or by its acronym, 
BP. 
25 
bringing about an assessment of its coverage, 
completeness, and consistency, all features 
that will be discussed shortly. 
3. The Universal Networking Language 
The UNL is a formal language designed 
for rendering automatic multilingual 
information exchange. It is intended to be a 
cross-linguistic semantic representation of 
NL sentence meaning, being the core of the 
UNL System, the KBMT system developed 
by H. Uchida (1996) at the Institute of 
Advanced- Studies, United Nations 
University, Tokyo; Japan. 
UNL subsumes a tridimensional theory of 
(sentence) meaning, whose components are 
defined according to one of the following sets 
(Martins et al, 1998a): concepts (e.g., "cat", 
"sit", "on", or "mat"), concept relations (e.g., 
"agent", "place", or "object"), and concept 
predicates (e.g., "past" or "definite"). Such 
components are formally and 
correspondingly represented by three 
different kinds of entities, namely: Universal 
Words (UWs), Relation Labels (RLs), and 
Attribute Labels (ALs). According to the 
UNL syntax, information conveyed by each 
sentence can be represented by a hypergraph 
whose nodes represent UWs and whose arcs 
represent RLs. To make symbol processing 
simpler, hypergraphs are often reduced to 
lists of ordered binary relations between 
concepts, as it is shown in Figure 1 for the 
sentence (1) The cat sat  on the mat. 5 
'sit', 'cat', 'on' and 'mat' are UWs; 'agt' (agent), 
'pie' (place) and 'obj' (object) are RLs; '@def, 
'@entry' and '@past' are ALs. 
Figure la: UNL hypergraph representation f the 
English sentence "The cat sat on the mat" 
agt(sit. @entry. @past,cat. @def) 
plc(sit. @entry. @past,on) 
obj(on,mat. @def) 
Figure lb :  UNL linear representation of the 
English sentence "The cat sat on the mat." 
UWs are labels for concept-like 
information, roughly corresponding to the 
lexical level in the sentence structure. They 
comprise an open large inventory, virtually 
capable of denoting every non-compositional 
meaning to be conveyed by any speaker of 
any language. For the sake of representation, 
these atomic semantic ontents are associated 
to English words and expressions, which play 
the role of semantic labels. However, there is 
no one-to-one mapping between the English 
vocabulary and the UNL lexicon, for UNL, 
as a multilingual representation code, is 
larger than the English vocabulary. To avoid 
unnecessary proliferation of the UNL 
vocabulary and to certify that standards be 
observed by UNL teams, control over the 
specification of the UW set is centered at the 
UNL Center, in Japan. 
Several semantic relationships hold 
between UWs, namely synonymy, antonymy, 
hyponymy, hypemymy and meronymy, 
which compose the UNL Ontology. Steady 
semantic valencies (such as agent and object 
features) can also be represented, forming the 
UNL Knowledge-Base. Both Ontology and 
Knowledge-Base aim at constraining the 
scope of UW labels, whenever ambiguity is 
to be avoided. The. UNL representation f 
sentence (1), for example, can be ambiguous 
26 
in Romance languages, for the translation of 
'cat' should make explicit the animal sex: if 
male, it would be "gato" (Portuguese and 
Spanish), "gatto" (Italian), "chat" (French), 
whereas different names would have to be 
used for the female cat. Instead of having a 
unique UW 'cat', it is thus quite feasible to 
have a whole structure in which 'cat' is only 
the hyper-ordinate option. 
For the English-UNL association ot to 
undermine the intended universality of the 
UW inventory, its semantic-orthograpical 
correspondence has to be considered rather 
incidental, or even. approximated. It is not 
always the case that extensions 6 of a UW 
label and of its corresponding English word 
coincide. The extension of the English word 
"mat", for example, does not exactly coincide 
with the extension of any Portuguese word, 
although we can find many overlaps between 
"mat" and, e.g., "capacho" (Portuguese). 
Portuguese speakers, however, would not say 
"capacho" for the ornamental dishmat, as 
would not English speakers use the word 
"mat" for a fawner (still "capacho" in 
Portuguese). Since each language categorizes 
the world in a very idiosyncratic way, it 
would be misleading to impose a 
straightforward correspondence between 
lexical items of two different languages. In 
UNL, this problem has been overcome by 
proposing a rather analogic lexicon, instead 
of a digital one. Although discrete, UWs 
convey continuous entities, in the sense that 
semantic gaps between concepts are fulfilled 
by the UNL Knowledge-Base, as it is shown 
for the UW 'mat' in Figure 2. Granularity 
thus plays an important role in UNL lexical 
organization and brings flexibility into cross- 
linguistic lexical matching. 
Cf. (Frege, 1892), extension here is used to establish 
the relationship between a word and the world, 
opposed to intension, referring to the relationship 
between aword and its meaning. 
icl 
Figure 2a: UNL hypergraph artial representation for 
the meaning denoted by the English word "mat" 
"mat" 
"mat(aoj>entity)" 
"mat(icl>event)" 
"mat(icl>frame)" 
"mat(icl>rug)" 
"mat(icl>state)" 
"mat(obi>entitv)" 
Figure 2b: UNL partial inear epresentation for 
the meaning denoted by the English word "mat" 
While lexical representation in UNL 
comprises a set of universal concepts 
signaled by UWs, the cross-lexical level 
involves a set of ordered binary relations 
between UWs, which are the Relation Labels 
(RLs). RLs specification are similar to 
Fillmore's semantic ases (1968), with RLs 
corresponding to semantic-value relations 
linking concept-like information. There are 
currently 44 RLs, but this set has been 
continuously modified by empirical evidence 
of lack, or redundancy, of relations. The 
inventory of RLs can be divided into three 
parts, according to the functional aspects of 
the related concepts: ontological, event-like 
and logical relations. Ontological relations 
are used as UW constraints in reducing 
lexical granularity or avoiding ambiguity as 
shown above, and they help positioning UWs 
in a UNL lexical structure. Five different 
labels are used to convey ontological 
relations: icl (hyponymy), equ (synonymy), 
ant (antonymy), pof (meronymy), and fld 
(semantic field). 
2"7 
UNL depicts sentence meaning as a fact 
composed by either a simple or a complex 
event, which is considered here the starting 
point of a UNL representation, i.e., its 
minimal complete semantic unit. Event-like 
relations are assigned by an event external or 
internal structure, or by both. An event 
external structure has to do nearly always 
with time and space boundaries. It can be 
referred to by a set of RLs signaling the event 
co-occurrent meanings, such as 7 its 
environment (scn); starting place (pl0, 
finishing p!ace (pit), or, simply, place (plc); 
range (fmt); starting time (tmf), finishing 
time (tmt), or, simply, time (tim); and 
duration (dur). Action modifiers, such as 
manner (man) and method (met) can also 
qualify this structure. An event internal 
structure is associated to one of the following 
simple frames: action, activity, movement, 
state, and process, each expressing different 
RLs in the event itself, including its actors 
and circumstances. 
Event actors are any animate or inanimate 
character playing any role in events, which 
can be the main or the coadjutant actors. 
There can be up to eight actors, signaled by 
the following RLs: agent (agt), co-agent 
(cag), object (obj), co-object (cob), object 
place (opl), beneficiary (ben), partner (ptn) 
and instrument (ins). They can also be 
coordinated through the RLs conjunction 
(and) and disjunction (or), or subordinated to 
each other by possession (pos), content (cnt), 
naming (nam), comparison (bas), proportion 
(per), and modification (mod). They can still 
be quantified (qua) or qualified by the RLs 
"property attribution" (aoj) and co-attribution 
(cao). It is possible to refer to an "initial 
actor" (src), a "final actor" (gol), or an 
"intermediary actor" (via). Finally, spatial 
relationships can also hold between actors: 
current place (plc), origin (firm), destination 
(to), and path (via). Besides single events, 
there can still be complex cross-event 
relationships which express either paralleled 
events - co-occurrence (coo), conjunction 
(and), and disjunction (or) - or hierarchically 
posed events - purpose (pur), reason (rsn), 
condition (con), and sequence (seq). They 
can all be referred to as logical relations, 
since they are often isomorphic to first-order 
logic predicates. 
According to the UNL authors, it is 
possible to codify any sentence written in any 
NL into a corresponding UNL text expressing 
the sentence meaning through the use of the 
above RLs. This is still a claim to be verified, 
since cases of superposition and competition 
between different RLs have been observed, 
as it is discussed in Section 5. 
In addition to UWs and RLs, UNL 
makes use of predicate-like information, or 
Attribute Labels (ALs), which are names for 
event and concept "transformations", in a 
sense very close to that intended by Chomsky 
(1957, 1965). They are not explicitly 
represented in a UNL hypergraph, although 
they are used to modify its nodes. ALs can 
convey information about concept intensions 
and extensions. In the former case, ALs name 
information about utterers' intensions over 
either specific parts of a sentence (focus, 
topic, emphasis, theme) or the whole 
structure (exclamation, interrogation, 
invitation, recommendation, obligation, etc.). 
In the latter case, ALs refer to spatial 
(definite, indefinite, generic, plural) or 
temporal (past, present, future)information, 
or still, temporal external (begin-soon, begin- 
just, end-soon, end-just) or intemal 
(perfecfive, progressive, imperfective, 
iterative) structures. To differentiate ALs 
from UWs, ALs are attached to UWs by the 
symbol ".@". The cOncept expressed by the 
UW 'sit' in "sit. @entry. @past", for example, 
is taken as the starting point (. @entry) of the 
corresponding hypergraph and it is to be 
modified by temporal information (.@past). 
7 RLs names are bracketed.  
28 
4. The UNL System 
The UNL system architecture consists of 
two main processes, the encoder and 
decoder, and several linguistic resources, 
each group of these corresponding to a NL 
embedded in the system, as depicted in 
Figure 3. 
~U~qL e language-to-~ dictionary 
UNL-t0-target-~ uage dictionary 
~source  I 
language I 
Encoder 
? r I 
1 
Decoder 
language I 
 s~t~CNL e language-to-~ grammar J 
Figure 3: The UNL System Architecture 
A source document (SLD) conveys 
written text on any subject, in any of the NLs 
considered. There is no constraint in the 
domain or structure of the SLD, but there is 
necessarily a loss of semantic expressiveness 
during NL-UNL encoding. The goal of the 
UNL is not, in principle, to fully preserve text 
meaning, but only its main components, i.e., 
those considered to be essential. However, 
there is no measurable account as to what is 
essential in the UNL Project. By convention, 
this is linked to what has been called the 
literal meaning, whi.ch is directly derived 
from interpreting the sentence surface 
structure. Therefore, there is no room to 
represent content hat is not directly mapped 
onto the NL syntactic-semantic licensed 
structures. 
The NL-UNL encoding tool, or UNL 
Encoder, is generic enough to handle all the 
29 
languages included in the Project. Apart from 
the (supposedly) universal knowledge-base, 
used to fill-in possible interlexical gaps when 
mapping is not precise, all other linguistic 
resources are language-dependent. The 
source grammar essentially guides the 
elicitation of the sentence semantic structure 
into its corresponding UNL structure, by 
determining RLs and ALs, always giving 
priority to information content. 
The UNL-NL decoding tool, or UNL 
Decoder, works in the opposite way to the 
Encoder. Besides the lexicon and the 
grammar, a cooccurrence dictionary is also 
used at this stage, to disentangle lexical 
choice. The target grammar is responsible for 
the semantic-syntactic mapping, now 
resolving semantic organization by making 
syntactic and dependence choices between 
UWs, taking RLs and ALs into account. 
5. Remarks on language-independence 
The main strength of the UNL Project 
rests on human expertise: language-specific 
aspects to be included in the multilingual 
KBMT system are handled by native 
speakers of that language, in an attempt o 
overcome the need of representing 
knowledge across several languages or 
cultures. It has been successful in developing 
NL-driven resources and processes by 
researchers all around the world. For 
example, the BP UNL lexicon has over 
65,000 entries that are categorized according 
to grammatical and some semantic features, 
and this will be extended considerably in the 
future to cover the Portuguese vocabulary to 
a greater extent. Up to the present ime, only 
decoding systems customized to each NL 
have been plugged into a general decoder 
skeleton (provided by the UNL Center) and 
have already been assessed, producing 
promising results. The BP decoder, for 
example, is able to produce outputs whose 
literal meaning is preserved in most cases 
(Martins et al, 1998b), using handcoded 
UNL expressions. Actually, to decode any 
UNL text, NL-UNL encoding has to be 
handmade, since customization of the UNL 
Encoder to each NL has not yet been 
undertaken in the project. In spite of the 
promising decoding results, a) output quality 
varies enormously with UNL sentences 
encoding, which can be different across 
distinct research groups; b) communicative 
aspects of information exchange on the web 
are not explored in depth, as it can be seen 
through the list of RLs or ALs. UNL is not 
knowledge intensive and there are no 
guidelines as to consistently recognize or 
extract such kind of information from the 
surface of the source texts. 
There are several reasons why 
interpretation and use of the UNL among the 
various teams are not uniform, including 
cultural aspects and syntax differences of the 
languages involved. Using English as the 
lingua franca for communication and 
cooperation among the research groups and 
as the knowledge representation language has 
also brought limitations into the Project, 
since it implies a non-desirable level of 
language-dependence. This is inevitable, 
however, for limitations definitely come 
along with the choice made. For example, 
attaching a NL word to a UW may be 
difficult, owing to the cross-references 
introduced by using English to convey UNL 
symbols. Resuming the example shown in 
Figure 1, this is the case of the UW "on" in 
(lb): the preposition 'on' fills in the position 
feature of the verb 'sit' and, thus, is 
represented in UNL correspondingly as the 
second term of the binary relation 'plc' and 
the first term of 'obj'. This, undoubtedly, is 
critical, for 'sit' can be juxtaposed to other 
prepositions leading to different meanings, 
which, in turn, may introduce different sets of 
binary relations, implying a high-level 
complexity in the UNL representation. As a 
result, languages whose syntactic structures 
deeply differ from the English ones may 
30 
present an additional level of complexity that 
makes mapping to/from UNL impossible or 
unrealistic. In this respect, we have not been 
facing many problems in fitting Portuguese 
structures with UNL ones, since Portuguese, 
like English, is an inflectional anguage that 
also employs prepositional constructions. 
However, prepositions in Portuguese may 
play considerably different roles compared to 
English. Various extensions of the English 
spatial prepositions "on", "over" and 
"above", for example, are subsumed in 
Portuguese by a single form "sobre" (which 
may also mean ..about). Therefore, in 
Portuguese, cats could be, at the same time, 
not only "on" but also "over" and "above" 
mats. Only world knowledge, associated to 
contextual indexes, both absent in the 
referred UNL hypergraph, could avoid the 
unsuited encodings The cat sat over the mat. 
or The cat sat above the mat. from the 
Portuguese sentence "O gato sentou sobre o 
tapete". 
Another problem related to the sentence 
The cat sat on the mat. refers to the existence 
of competing analyses: it is quite plausible 
that a UNL representation suggesting a noun 
phrase instead of a full sentence holds for this 
sentence. It so happens when the arc between 
'sitting' and 'cat' concepts are labeled by the 
RL 'obj', instead of the RL 'agt' in (1), as it 
is shown in Figure 1 a', yielding the UNL text 
shown in Figure lb'. 
o 
Figure la': UNL hypergraph representation f 
the English sentence "The cat sat on the mat." 
obj(sit. @entry. @past,cat. @def) 
plc(sit. @ entry. @ past,on) 
obj(on,mat. @def) 
Figure lb ' :  UNL linear representation f the 
English sentence "The cat sat on the mat." 
Both analyses are equally accurate and 
can lead to good NL surface expressions, 
although they refer to different semantic 
facts. Indeed, to define an object relationship 
between "sitting" and "cat" is to say that the 
cat was already sat before the beginning of 
the event (e,g., The cat sat on the mat ate the 
fish.). In this case, the animal does not 
actually perform the action, but is 
conditioned to it, the main performer position 
being empty, thus yielding the referred noun 
phrase. In Figure 1, instead, the cat on its 
own has taken the sitting position, therefore 
introducing an agent relationship. These two 
different semantic facts may correspond, in 
English, to a single surface structure. Indeed, 
(1) is orthographically identical to (1'). 
However, other languages (e.g., Portuguese) 
do behave differently. 
Although it is also possible to have, in 
Portuguese, the same surface structure 
corresponding to both UNL representations 
("sentado no tapete"), it is more feasible to 
have, for each case, completely different 
constructions. In the case depicted by Figure 
1, the UW "sit" would be associated to the 
verb "sentar" (corresponding to "to sit"). 
Thus, the generation result should be 
something like "O gato sentou no tapete" or 
"O gato sentado no tapete". On the other 
hand, for Figure 1', the same UW 'sit' would 
be generated in a completely different way, 
corresponding to the passive form of the 
Portuguese xpression "colocar sentado" (to 
be put in a sitted position), for which there is 
no adequate English surface xpression. 
Distinguishing such situations to cope 
with syntactic-semantic troublesome 
mappings, though interesting, is a highly 
31 
context-sensitive task, often surpassing 
sentence boundaries. UNL descriptions do 
not address such fine-grained level of 
meaning representation, being limited to 
meanings derived from context-free source 
sentences, even when context-freeness 
implies insufficient information. When this is 
not possible, UNL offers a default analysis 
for semantically ambiguous sentences, in 
which case we can say that the UNL 
representation is probabilistic, rather than 
deterministic. 
The _way we believe some of UNL 
limitations can-be  overcome and/or 
minimized is by designing a fully-fledged 
testing procedure to assess outputs of both 
decoder and encoder for the various 
languages. Since the same encoding and 
decoding procedures have been delivered to 
the UNL teams, it is possible that part of the 
set of rules or translation strategies of a given 
team may be interchangeable with another 
one from a different language. In this way, 
sharing procedures may become a warranty 
for common ground assessment of the varied 
models, in which case it may be possible to 
make eligible concurrent strategies equally 
available for the languages involved. 
Concerning the UNL means to 
disambiguate or proceed to reference 
resolution or other discourse figures, most of 
the troublesome occurrences are enclosed in 
the treatment issued by specialists and, thus, 
they are constrained to, and handled by, at the 
level of native speakers use. This measure 
can be somewhat fruitful, provided that each 
signatory of the Project finds a way to trace a 
UNL text back onto its own NL text or vice- 
versa, making a proper use of the UNL 
syntax or symbols. This, in fact, can be a 
good method to evaluate (de)coding: once a 
UNL code has been produced from any NL 
text, this code can be the source to decoding 
into the same NL, in order to compare the 
original NL text with the automatically 
generated one. Evaluation, in this case, can 
be carried out by the same research group 
responsible for both processes. 
Compared to other interlingua pproaches 
(e.g., Mikrokosmos, Gazelle, or Kant), the 
UNL Project is in a much earlier stage - most 
of those are over 10 years old, while the UNL 
one is about 3 years old - but it is much more 
ambitious than most of the current systems 
under construction. For UNL is actually a 
front-end to a many-to-many communication 
system, with no constraints that are normally 
inherent in MT systems. Since knowledge is 
specified by native speakers for each NL 
module, grammar, semantics and world 
knowledge can be well founded. Its 
limitations, from a conceptual viewpoint, are 
shared by most of its counterparts, as in 
treating text at the sentence level only. In 
addition, by no means is the UNL system 
committed to event replication as it is the 
case of human translation. Automatic 
strategies have no psychological motivation 
whatsoever and are solely based upon 
computer efficiency principles, namely time 
and space. 
Acknowledgments 
The development of resources for 
Brazilian Portuguese in the UNL Project has 
been sponsored by the Institute of Advanced 
Studies of the United Nations University. The 
authors are also grateful to CNPq and Finep 
(Brazil) for the financial support and to Mr. 
Tadao Takahashi, the coordinator of the 
Brazilian branch in the UNL Project. 
References 
Chomsky, N. (1957). Syntactic Structures. 
The Hague, Mouton. 
Chomsky, N. (1965). Aspects of the Theory of 
Syntax. MIT Press, Cambridge, MA. 
Fillmore, C. (1968). The case for case. In 
Bach, E. and Harms, R.T. (orgs.), 
Universals in linguistic theory, pp. 1-88. 
Rinehard and Winston, New York. 
32 
Frege, G. (1892). On Sinn and Bedeutung. In 
Beaney, M. (ed.), The Frege Reader. 
Blackwell Publishers, Malden, MA, 1997. 
Martins, R.T., Rino, L.H.M., Nunes, M.G.V. 
(1998a). As Regras Gramaticais para a 
Decodtfica~ao UNL-Portugu~s no Projeto 
UNL. Relat6rio T6cnico 67. Instituto de 
CiSncias Matem~iticas e da Computa~ao. 
Universidade de S~o Paulo, Sao Carlos. 
Martins, R.T.; Rino, L.H.M.; Nunes, 
M.G.V.; Oliveira Jr., O.N. (1998b). Can 
the syntactic realization be detached from 
the syntactic analysis during generation of 
natural ldnguage sentences? III Encontro 
para o processamento c mputacional da 
lingua portuguesa escrita e falada 
(PROPOR'98). Porto Alegre - RS. 
Novembro. 
Uchida, H. (1996). UNL: Universal 
Networking Language - An Electronic 
Language for Communication, 
Understanding, and Collaboration. 
UNU/IAS/UNL Center. Tokyo, Japan. 
33 
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 10?16,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
A Supervised Model for Extraction of Multiword Expressions Based on
Statistical Context Features
Meghdad Farahmand
The Computer Science Center
University of Geneva
Switzerland
meghdad.farahmand@unige.ch
Ronaldo Martins
UNDL Foundation
Geneva - Switzerland
r.martins@undl.ch
Abstract
We present a method for extracting Multi-
word Expressions (MWEs) based on the
immediate context they occur in, using a
supervised model. We show some of these
contextual features can be very discrim-
inant and combining them with MWE-
specific features results in a relatively ac-
curate extraction. We define context as
a sequential structure and not a bag of
words, consequently, it becomes much
more informative about MWEs.
1 Introduction
Multiword Expressions (MWEs) are an important
research topic in the area of Natural Language
Processing (NLP). Efficient and effective extrac-
tion and interpretation of MWEs is crucial in most
NLP tasks. They exist in many types of text and
cause major problems in all kinds of natural lan-
guage processing applications (Sag et al., 2002).
However, identifying and lexicalizing these im-
portant but hard to identify structures need to be
improved in most major computational lexicons
(Calzolari et al., 2002). Jackendoff (1997) esti-
mates that the number of MWEs is equal to the
number of single words in a speaker?s lexicon,
while Sag et al. (2002) believe that the number
is even greater than this. Moreover, as a lan-
guage evolves, the number of MWEs consistently
increases. MWEs are a powerful way of extending
languages? lexicons. Their role in language evolu-
tion is so important that according to Baldwin and
Kim (2010), ?It is highly doubtful that any lan-
guage would evolve without MWEs of some de-
scription?.
The efficient identification and extraction of
MWEs can positively influence many other NLP
tasks, e.g., part of speech tagging, parsing,
syntactic disambiguation, semantic tagging, ma-
chine translation, and natural language generation.
MWEs also have important applications outside
NLP. For instance in document indexing, informa-
tion retrieval (Acosta et al., 2011), and cross lin-
gual information retrieval (Hull and Grefenstette,
1996).
In this paper we present a method of extracting
MWEs which is relatively different from most of
the state of the art approaches. We characterize
MWEs based on the statistical properties of the
immediate context they occur in. For each pos-
sible MWE candidate we define a set of contex-
tual features (e.g., prefixes, suffixes, etc.). The
contextual feature vector is then enriched with a
few MWE-specific features such as the frequency
of its components, type frequency of the candi-
date MWE, and the association between these two
(which is learned by a supervised model). Subse-
quently the MWEhood of the extracted candidates
is predicted based on this feature representation,
using a Support Vector Machine (SVM). The sys-
tem reaches a relatively high accuracy of predict-
ing MWEs on unseen data.
1.1 Previous Work
Attempts to extract MWEs are of different types.
The most common techniques are primarily fo-
cused on collocations. Some of these techniques
are rule-based and symbolic e.g., (Seretan, 2011;
Goldman et al., 2001; Nerima et al., 2003; Bald-
win, 2005; Piao et al., 2003; McCarthy et al.,
2003; Jacquemin et al., 1997). Some rely on lexi-
cons (Michiels and Dufour, 1998; Li et al., 2003)
and (Pearce, 2001) that uses WordNet to evalu-
ate the candidate MWE based on anti-collocations.
Other approaches are hybrid in the sense that
they benefit from both statistical and linguistic
information. For instance (Seretan and Wehrli,
2006; Baldwin and Villavicencio, 2002; Piao and
McEnery, 2001; Dias, 2003).
There are also fully statistical approaches. For
instance (Pecina, 2010; Evert, 2005; Lapata and
10
Lascarides, 2003; Smadja et al., 1996), or the early
work Xtract (Smadja, 1993).
Other approaches consider all types of MWEs
(Zhang et al., 2006). Some of these approaches
build upon generic properties of MWEs, for in-
stance semantic non-compositionality (Van de
Cruys and Moir?on, 2007).
A different approach is presented in (Widdows
and Dorow, 2005). The authors present a graph-
based model to capture and assess fixed expres-
sions in form of Noun and/or Noun.
There are also bilingual models which are
mostly based on the assumption that a translation
of the MWE in a source language exists in a tar-
get language. For instance (de Medeiros Caseli
et al., 2010; Ren et al., 2009), and (Moir?on and
Tiedemann, 2006) which measures MWEs candi-
dates? idiomaticity based on translational entropy.
Another example is (Duan et al., 2009) which is
a hybrid model that aims at extracting bilingual
(English-Chinese) MWEs . It combines Multi-
ple Sequence Alignment Model with some filter-
ing based on hard rules to obtain an improved ex-
traction.
A more generic model is presented in (Ramisch,
2012) where the author develops a flexible plat-
form that can accept different types of criteria
(from statistical to deep linguistic) in order to ex-
tract and filter MWEs. However, in this work,
as the author claims, the quality of the extracted
MWEs is highly dependent on the level of deep
linguistic analysis, and thereby, the role of statisti-
cal criterion is less significant.
1.2 Motivation
We propose an original method to extract multi-
word expressions based on statistical contextual
features, e.g., a set of immediate prefixes, suffixes,
circumfixes, infixes to circumfixes, etc., (see Sec.
2). These features are used to form a feature repre-
sentation, which together with a set of annotations
train a supervised model in order to predict and
extract MWEs from a large corpus.
We observed some discriminant behavior in
contextual features (such as prefixes, suffixes, cir-
cumfixes, etc.) of a set of manually selected
MWEs. A supervised model is then applied to
learn MWEhood based on these features.
In general, modeling lexical and syntactic (and
not semantic) characteristics of continuous MWEs
is the focus of this paper. In order for the MWE de-
composability condition to hold, we consider bi-
grams and above (up to size 4). Idiomaticity at
some level is a necessary prerequisite of MWEs.
Hereby, we consider idiomaticity at lexical, syn-
tactic and statistical levels, and leave the semantic
idiomaticity to the future work.
Relatively similar models have been previously
applied to problems similar to MWEs, for instance
named entity recognition (Nadeau and Sekine,
2007; Ratinov and Roth, 2009).
The focus on contextual features allows some
degree of generalization, i.e., we can apply this
model to a family of languages.
1
However, this
work focuses only on English MWEs.
2 Proposed System
We prepared a corpus that comprises 100K
Wikipedia documents for each of the mentioned
languages.
1
After cleaning and segmenting the
corpus, we extracted all possible n-grams (up to
size 7) and their token and type frequencies. Then
two basic statistical filters were applied in order to
systematically decrease the size of our immense
n-gram set: (i) Frequency filter, where we filter
an n-gram if its frequency is less than the ratio
between tokens and types, where for a given size
of n-grams, the total number of n-grams and the
number of distinct n-grams of that size, are con-
sidered tokens and types, respectively. (ii) Redun-
dancy filter where we consider an n-gram to be
redundant if it is subsumed by any other n
?
-gram,
where n
?
> n. This gives us a pruned set of n-
grams which we refer to as the statistically signifi-
cant set. Table 1 presents a count-wise description
of the filtering results on the English corpus.
raw frq flt rdund flt
1-grams 1782993 64204 64204
2-grams 14573453 1117784 1085787
3-grams 38749315 3797456 3394414
4-grams 53023415 5409794 3850944
5-grams 53191941 2812650 2324912
6-grams 47249534 1384821 568645
7-grams 39991254 757606 757606
1
We are adapting our model so that it can handle clusters
of similar languages. So far we have processed the following
9 widely-spoken languages: English, German, Dutch, Span-
ish, French, Italian, Portuguese, Polish, and Russian. How-
ever, to study the efficiency of the presented model applied to
languages other than English, remains a future work.
11
Table 1: Number of extracted n-grams for EN.
First column indicates raw data, second and third
columns indicate the number of n-grams after fre-
quency and redundancy filters respectively.
For the set of significant n-grams a set of statis-
tical features are extracted which will be described
shortly. Fig. 1 illustrates the workflow of the sys-
tem.
 removing tags, 
cleaning, 
segmentation 
Language 
Model
100K
Wikipedia
docs
Corpus
cleaning;
extraction
n-grams
freq &
redundancy
Filters
statistically
significant
n-grams
indexing
Index
(MWE candidate
, {|f1|,|f2|,...)
feature 
extraction
This%set%is%used%in%
annotation%and%generation%
of%test/training%data%
Figure 1: Schematic of pre-processing, n-gram ex-
traction and filtering. Blended and plain nodes
represent resources, and operations respectively.
While studying the English corpus and different
MWEs therein, it was observed that often, MWEs
(as well as some other types of syntactic units)
are followed, preceded or surrounded by a lim-
ited number of high frequency significant n-gram
types. Moreover, our manual evaluation and con-
stituency tests reveal that generally when a fre-
quent significant prefix co-occurs with a frequent
significant suffix, they form a circumfix whose sig-
nificant infixes are (i) many, (ii) can mostly be con-
sidered syntactic unit, specifically when it comes
to bi/trigrams. Table 2 illustrates a randomly se-
lected sample of infixes of such circumfix (the..of).
Remarkably, the majority of them are idiomatic at
least at one level.
franz liszt academy official list
most important albums closest relatives
ministry of commerce protestant church
executive vice president peak period
famous italian architect manhattan school
blessed virgin mary rise and fall
world cup winner former head
Table 2: Examples of bi/trigrams surrounded by
the circumfix the..of
The immediate proximity of these particular con-
text features to MWEs keeps emerging while eval-
uating similar circumfixes. We believe it sug-
gests the presence of a discriminant attribute that
we model with features 5-8 (see Table 3) and
learn using a supervised model. Nevertheless,
the fact that MWEs share these features with
other types of syntactic units encourages introduc-
ing more MWE-specific features (namely, MWE?s
frequency, the frequency of its components, and
their associations), then enforcing the learning
model to recognize a MWE based on the combi-
nation of these two types of features. Note that
the association between the type frequency of a
MWE, and the frequency of its components is im-
plicitly learned by the supervised model through-
out the learning phase. A candidate MWE can be
represented as:
y = (x
1
, ..., x
m
, x
m+1
, ..., x
n
) ? N
0
(1)
Where x
1
, ..., x
m
are contextual, and
x
m+1
, ..., x
n
are specific features (m = 8,
and n = 11). These features are described in
Table 3.
contextual features
x
1
# set of all possible prefixes of y
x
2
# set of distinct prefixes of y
x
3
# set of all possible suffixes of y
x
4
# set of distinct suffixes of y
x
5
# set of all possible circumfixes of y
x
6
# set of distinct circumfixes of y (C)
x
7
# set of all possible infixes to members of C
x
8
# set of distinct infixes to members of C
specific features
x
9
the size of y
x
10
number of occurrences of y in the corpus
x
11
list of frequencies of the components of y
Table 3: Description of the extracted features
12
A prefix of y is the longest n-gram immediately
before y, if any or the boundary marker #, other-
wise. A suffix of y is the longest n-gram imme-
diately after y, if any or the boundary marker #,
otherwise. A circumfix (c
i
? C) of y is the pair
(p, s) where p and s are respectively the prefix and
the suffix of a given occurrence of y. An Infix of
c
i
is an n-gram that occurs between p and s.
Components to generate candidate MWEs, fil-
ter them and extract their relevant features were
very memory and CPU intensive. To address the
performance issues we implemented parallel pro-
grams and ran them on a high performance cluster.
3 Experimental Results
A set of ? 10K negative and positive English
MWE examples were annotated. This set does
not particularly belong in any specific genre, as
the examples were chosen randomly from across
a general-purpose corpus. This set comprises an
equal number of positive and negative annotations.
Part of it was annotated manually at UNDL foun-
dation,
2
and part of it was acquired from the man-
ually examined MWE lexicon presented in (Ner-
ima et al., 2003). The set of positive and negative
annotated n-grams is detailed in Table 4. The bias
toward bigrams is due to the fact that the majority
of manually verified MWEs that could be obtained
are bigrams.
size + examples ? examples
2-grams 4, 632 5, 173
3-grams 500 22
4-grams 68 15
Table 4: Annotations? statistics
This set was divided into 1/3 test and 2/3 train-
ing data, which were selected randomly but were
evenly distributed with respect to positive and neg-
ative examples. The test set remains completely
unseen to the model during the learning phase. We
then train a linear SVM:
h(y) = w
?
y + b (2)
Where h(y) is a discriminant hyperplane, w is
the weight vector, and y is a set of MWE exam-
ples, where each example is defined as: y
j
=
x
1
, ..., x
11
. Table 5 shows the results of the
model?s multiple runs on five different pairs of
training and test sets.
2
The Universal Networking Digital Language Founda-
tion: http://www.undlfoundation.org/
precision (%) recall (%) accuracy(%)
run 1 84.8 96.8 89.7
run 2 82.5 97.4 88.4
run 3 83.6 97.8 89.3
run 4 84.1 97.5 89.5
run 5 83.4 97.1 88.9
Table 5: Performance of the SVM which learns the
MWEhood based on contextual and specific fea-
tures (x
1
? x
11
)
Table 6 illustrates the trained model?s predic-
tions on a set of randomly selected test examples.
The overall performance of the model is shown in
the form of a precision-recall curve in Fig. 2.
n-grams classified as MWE
spend time genetically modified
hijack a plane fish tank
top dog toy car
factory outlet motorcycle racing
season nine vintage car
video conference chestnut tree
kill your entry fee
safety precaution quantum leap
version shown make an appeal
flood damage drug dealer
bargaining chip lung transplant
grant her tone like
postgraduate student make a phone call
raise the price ozone layer
n-grams classified as non-MWE
score is and dartmouth
the tabular capped a
on sale clarified his
liver was the cancan
the regulating an ending
the rabi warns the
this manuscript a few
an exponential an institution
the petal blades are
or ended difficulties he
and workmen the guidance
the eyelids the examined
the vices the episodes
they work monument is
Table 6: Sample SVM?s output on unseen data.
A t-test ranks the significance of the defined fea-
tures in classifying n-grams into MWE, and non-
MWE classes, as illustrated in Fig. 3. The most
13
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5
0.6
0.7
0.8
0.9
1
Recall
Preci
sion
Precision?recall curve
Figure 2: Precision-recall curve
important features are the size of examples (x
9
),
and the frequencies of their components (x
11
).
The significance of x
9
is due to the fact that in
the training set majority of MWEs are bigrams.
Therefore, by the SVM, being a bigram is consid-
ered as a substantial feature of MWEs. Neverthe-
less since the number of negative and positive ex-
amples which are bigrams are approximately the
same, the bias toward x
9
in discriminating MWEs
from non-MWE balances out. However its as-
sociation with other features which is implicitly
learned still has an impact on discriminating these
two classes. x
7
and x
8
are the next two important
features, as we expected. These two are the fea-
tures whose magnitude suggests the presence or
lack of contexts such as (the..of ).
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10        x11(avg rnk)0
5
10
15
20
Features
Ranks
Figure 3: Ranks of the features that represent their
discriminant impact.
The class separability of MWE (1), and non-
MWE (?1) examples can be seen in Fig. 4, where
the bidimentional projection of the examples of
two classes is visualized. A star plot of a sample
of 50 manually annotated examples is shown in
Fig. 5. In many cases, but not always, non-MWEs
can be discriminated from MWEs, in this eleven
dimensional visualization. Same pattern was ob-
served in the visualization of 500 examples (which
would be hard to demonstrate in the present pa-
per?s scale).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?3
?2
?1
0
1
2
3 x 106
t
f(t)
 
 ?11
Figure 4: Andrews curve for the training exam-
ples. Bold line in the middle, and bold dotted
line represent the median of MWE and non-MWE
classes respectively.
4 Conclusions and Future Work
We presented a method to extract MWEs based
on the immediate context they occur in, using
a supervised model. Several contextual features
were extracted from a large corpus. The size of
the corpus had a profound effect on the effective-
ness of these features. The presented MWE ex-
traction model reaches a relatively high accuracy
on an unseen test set. In future work, the effi-
ciency of this approach on languages other than
English will be studied. Furthermore, other fea-
tures - specifically deep linguistic ones e.g., de-
gree of constituency as described in (Ponvert et
al., 2011) or POS tags, will be added to the fea-
ture representation of MWE candidates. Finally
context-based probabilistic scores which are lin-
guistically motivated can be investigated and com-
pared with the supervised model. Another inter-
esting work would be to introduce kernels so that
we can go from statistics of contextual features to
training the supervised model directly on the tex-
tual context.
14
?1 1 ?1 1 1 ?1 ?1 1
1 ?1 1 ?1 ?1 ?1 1 1
1 1 ?1 ?1 1 ?1 1 ?1
?1 ?1 1 ?1 ?1 ?1 ?1 ?1
?1 1 1 ?1 1 1 1 1
?1 ?1 ?1 ?1 ?1 1 ?1 ?1
?1 1
Figure 5: Star plot of 50 MWE (1), and non-MWE
(?1) examples
References
Otavio Acosta, Aline Villavicencio, and Viviane Mor-
eira. 2011. Identification and treatment of multi-
word expressions applied to information retrieval.
Kordoni et al, pages 101?109.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. Handbook of Natural Language Pro-
cessing, second edition. Morgan and Claypool.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In proceedings of the 6th conference on
Natural language learning-Volume 20, pages 1?7.
Association for Computational Linguistics.
Timothy Baldwin. 2005. Deep lexical acquisition of
verb?particle constructions. Computer Speech &
Language, 19(4):398?414.
Nicoletta Calzolari, Charles J Fillmore, Ralph Gr-
ishman, Nancy Ide, Alessandro Lenci, Catherine
MacLeod, and Antonio Zampolli. 2002. Towards
best practice for multiword expressions in computa-
tional lexicons. In LREC.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expres-
sions. Language resources and evaluation, 44(1-
2):59?77.
Ga?el Dias. 2003. Multiword unit hybrid extrac-
tion. In Proceedings of the ACL 2003 workshop
on Multiword expressions: analysis, acquisition and
treatment-Volume 18, pages 41?48. Association for
Computational Linguistics.
Jianyong Duan, Mei Zhang, Lijing Tong, and Feng
Guo. 2009. A hybrid approach to improve bilin-
gual multiword expression extraction. In Advances
in Knowledge Discovery and Data Mining, pages
541?547. Springer.
Stefan Evert. 2005. The statistics of word cooccur-
rences. Ph.D. thesis, Dissertation, Stuttgart Univer-
sity.
Jean-Philippe Goldman, Luka Nerima, and Eric
Wehrli. 2001. Collocation extraction using a syn-
tactic parser. In Proceedings of the ACL Workshop
on Collocations, pages 61?66.
David A Hull and Gregory Grefenstette. 1996. Query-
ing across languages: a dictionary-based approach
to multilingual information retrieval. In Proceed-
ings of the 19th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, pages 49?57. ACM.
Ray Jackendoff. 1997. The architecture of the lan-
guage faculty. Number 28. MIT Press.
Christian Jacquemin, Judith L Klavans, and Evelyne
Tzoukermann. 1997. Expansion of multi-word
terms for indexing and retrieval using morphology
and syntax. In Proceedings of the eighth conference
on European chapter of the Association for Com-
putational Linguistics, pages 24?31. Association for
Computational Linguistics.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional ev-
idence. In Proceedings of the tenth conference on
European chapter of the Association for Computa-
tional Linguistics-Volume 1, pages 235?242. Asso-
ciation for Computational Linguistics.
Wei Li, Xiuhong Zhang, Cheng Niu, Yuankai Jiang,
and Rohini Srihari. 2003. An expert lexicon ap-
proach to identifying english phrasal verbs. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics-Volume 1, pages
513?520. Association for Computational Linguis-
tics.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Proceedings of the ACL 2003
workshop on Multiword expressions: analysis, ac-
quisition and treatment-Volume 18, pages 73?80.
Association for Computational Linguistics.
Archibald Michiels and Nicolas Dufour. 1998. Defi,
a tool for automatic multi-word unit recognition,
meaning assignment and translation selection. In
Proceedings of the first international conference on
language resources & evaluation, pages 1179?1186.
Begona Villada Moir?on and J?org Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL 2006
Workshop on Multi-wordexpressions in a multilin-
gual context, pages 33?40.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
15
Luka Nerima, Violeta Seretan, and Eric Wehrli. 2003.
Creating a multilingual collocation dictionary from
large text corpora. In Proceedings of the tenth con-
ference on European chapter of the Association for
Computational Linguistics-Volume 2, pages 131?
134. Association for Computational Linguistics.
Darren Pearce. 2001. Synonymy in collocation ex-
traction. In Proceedings of the Workshop on Word-
Net and Other Lexical Resources, Second meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 41?46. Citeseer.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language resources and
evaluation, 44(1-2):137?158.
Scott Songlin Piao and Tony McEnery. 2001. Multi-
word unit alignment in english-chinese parallel cor-
pora. In the Proceedings of the Corpus Linguistics
2001, pages 466?475.
Scott SL Piao, Paul Rayson, Dawn Archer, Andrew
Wilson, and Tony McEnery. 2003. Extracting mul-
tiword expressions with a semantic tagger. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 49?56. Association for Computa-
tional Linguistics.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw
text with cascaded finite state models. In ACL, pages
1077?1086.
Carlos Ramisch. 2012. A generic framework for mul-
tiword expressions treatment: from acquisition to
applications. In Proceedings of ACL 2012 Student
Research Workshop, pages 61?66. Association for
Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, pages 147?
155. Association for Computational Linguistics.
Zhixiang Ren, Yajuan L?u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, pages 47?54. Associa-
tion for Computational Linguistics.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1?15. Springer.
Violeta Seretan and Eric Wehrli. 2006. Accurate col-
location extraction using a multilingual parser. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 953?960. Association for Computa-
tional Linguistics.
Violeta Seretan. 2011. Syntax-based collocation ex-
traction, volume 44. Springer.
Frank Smadja, Kathleen R McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational linguistics, 22(1):1?38.
Frank Smadja. 1993. Retrieving collocations from
text: Xtract. Computational linguistics, 19(1):143?
177.
Tim Van de Cruys and Begona Villada Moir?on. 2007.
Semantics-based multiword expression extraction.
In Proceedings of the Workshop on a Broader Per-
spective on Multiword Expressions, pages 25?32.
Association for Computational Linguistics.
Dominic Widdows and Beate Dorow. 2005. Automatic
extraction of idioms using graph analysis and asym-
metric lexicosyntactic patterns. In Proceedings of
the ACL-SIGLEX Workshop on Deep Lexical Acqui-
sition, pages 48?56. Association for Computational
Linguistics.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated multiword ex-
pression prediction for grammar engineering. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, pages 36?44. Association for Computational
Linguistics.
16
