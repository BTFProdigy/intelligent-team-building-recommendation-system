239
240
241
242
The Automatic Generation of Formal Annotations in a Multimedia
Indexing and Searching Environment
Thierry Declerck
DFKI GmbH
Stuhlsatzenhausweg 3
D-66123 Saarbruecken
Germany
declerck@dfki.de
Peter Wittenburg
MPI for Psycholinguistics
Wundtlaan 1, PB 310
NL-6500 AH Nijmegen
The Netherlands
Peter.Wittenburg@mpi.nl
Hamish Cunningham
Dept. of Computer Science
University of Sheffield
Regent Court, 211 Portobello
GB-Sheffield S1 4DP
Great Britain
hamish@dcs.shef.ac.uk
Abstract
We describe in this paper the MU-
MIS Project (Multimedia Indexing and
Searching Environment)1 , which is
concerned with the development and in-
tegration of base technologies, demon-
strated within a laboratory prototype, to
support automated multimedia index-
ing and to facilitate search and retrieval
from multimedia databases. We stress
the role linguistically motivated annota-
tions, coupled with domain-specific in-
formation, can play within this environ-
ment. The project will demonstrate that
innovative technology components can
operate on multilingual, multisource,
and multimedia information and create
a meaningful and queryable database.
1 Introduction
MUMIS develops and integrates basic technolo-
gies, which will be demonstrated within a labora-
tory prototype, for the automatic indexing of mul-
timedia programme material. Various technology
components operating offline will generate for-
mal annotations of events in the data material pro-
cessed. These formal annotations will form the
basis for the integral online part of the MUMIS
project, consisting of a user interface allowing the
querying of videos. The indexing of the video ma-
terial with relevant events will be done along the
1MUMIS is an on-going EU-funded project within the
Information Society Program (IST) of the European Union,
section Human Language Technology (HLT). See for more
information http://parlevink.cs.utwente.nl/projects/mumis/.
line of time codes extracted from the various doc-
uments.
For this purpose the project makes use of data
from different media sources (textual documents,
radio and television broadcasts) in different lan-
guages (Dutch, English and German) to build a
specialized set of lexicons and an ontology for
the selected domain (soccer). It also digitizes
non-text data and applies speech recognition tech-
niques to extract text for the purpose of annota-
tion.
The core linguistic processing for the anno-
tation of the multimedia material consists of
advanced information extraction techniques for
identifying, collecting and normalizing signifi-
cant text elements (such as the names of players
in a team, goals scored, time points or sequences
etc.) which are critical for the appropriate anno-
tation of the multimedia material in the case of
soccer.
Due to the fact that the project is accessing and
processing distinct media in distinct languages,
there is a need for a novel type of merging tool in
order to combine the semantically related annota-
tions generated from those different data sources,
and to detect inconsistencies and/or redundancies
within the combined annotations. The merged an-
notations will be stored in a database, where they
will be combined with relevant metadata.2
Finally the project will develop a user interface
to enable professional users to query the database,
by selecting from menus based on structured an-
2We see in this process of merging extracted informa-
tions and their combination with metadata a fruitful base for
the identification and classification of content or knowledge
from distinct types of documents.
notations and metadata, and to view video frag-
ments retrieved to satisfy the query, offering thus
a tool to formulate queries about multimedia pro-
grammes and directly get interactive access to the
multimedia contents. This tool constitutes the on-
line component of the MUMIS environment.
2 State of the Art
MUMIS differs in many significant ways from ex-
isting technologies and already achieved or ad-
vanced projects3 . Most closely related to the the-
matic focus of MUMIS are the HLT projects Pop-
Eye [POP] and OLIVE [OLI]. Pop-Eye used sub-
titles to index video streams and offered time-
stamped texts to satisfy a user query, on request
displaying a storyboard or video fragment corre-
sponding to the text hit. OLIVE used automatic
speech recognition to generate transcriptions of
the sound tracks of news reports, which were then
indexed and used in ways similar to the Pop-Eye
project; both projects used fuzzy matching IR al-
gorithms to search and retrieve text, offering lim-
ited multilingual access to texts. Instead of using
IR methods to index and search the transcriptions,
MUMIS will create formal annotations to the in-
formation, and will fuse information annotations
from different media sources. The fusion result
is then used to direct retrieval, through interface
techniques such as pop-up menus, keyword lists,
and so on. Search takes the user direct to the sto-
ryboard and video clippings.
The Informedia project at Carnegie-Mellon-
University [INF] has a similar conceptual base-
line to MUMIS. The innovative contribution of
MUMIS is that it uses a variety of multilingual
information sources and fuses them on the ba-
sis of formal domain-specific annotations. Where
Informedia primarily focuses on special applica-
tions, MUMIS aims at the advancement and in-
tegratibility of HLT-enhanced modules to enable
information filtering beyond the textual domain.
Therefore, MUMIS can be seen as complemen-
tary to Informedia with extensions typical for Eu-
rope.
The THISL project [THI] is about spoken doc-
ument retrieval, i.e., automatic speech recognition
3We are aware of more related on-going projects, at least
within the IST program, but we can not compare those to
MUMIS now, since we still lack first reports.
is used to auto-transcribe news reports and then
information retrieval is carried out on this infor-
mation. One main focus of THISL is to improve
speech recognition. Compared to MUMIS it lacks
the strong language processing aspects, the fusion
of multilingual sources, and the multimedia deliv-
ery.
Columbia university is running a project [COL]
to use textual annotations of video streams to in-
dicate moments of interest, in order to limit the
scope of the video processing task which requires
extreme CPU capacities. So the focus is on find-
ing strategies to limit video processing. The Uni-
versity of Massachusetts (Amherst) is also run-
ning projects about video indexing [UMA], but
these focus on the combination of text and im-
ages. Associated text is used to facilitate indexing
of video content. Both projects are funded under
the NSF Stimulate programme [NSF].
Much work has been done on video and im-
age processing (Virage [VIR], the EUROMEDIA
project [EUR], Surfimage [SUR], the ISIS project
[ISI], IBM's Media Miner, projects funded under
the NSF Stimulate program [NSF], and many oth-
ers). Although this technology in general is in its
infancy, there is reliable technology to indicate,
for example, scene changes using very low-level
cues and to extract key frames at those instances
to form a storyboard for easy video access. Some
institutions are running projects to detect subtitles
in the video scene and create a textual annotation.
This task is very difficult, given a sequence of real
scenes with moving backgrounds and so on. Even
more ambitious tasks such as finding real patterns
in real movies (tracing the course of the ball in a
soccer match, for example) are still far from being
achieved.4
3 Formal Annotations for the Soccer
Domain
Soccer has been chosen as the domain to test and
apply the algorithms to be developed. There are a
number of reasons for this choice: availability of
people willing to help in analyzing user require-
ments, existence of many information sources in
4The URLs of the projects mentionned above are given
in the bibliography at the end of this paper.
several languages5 , and great economic and pub-
lic interest. The prototype will also be tested by
TV professionals and sport journalists, who will
report on its practicability for the creation and
management of their programme and information
material.
The principles and methods derived from this
domain can be applied to other as well. This has
been shown already in the context of text-based
Information Extraction (IE), for which method-
ologies for a fast adaptation to new domains
have been developed (see the MUC conferences
and (Neumann et al, 2000)). And generally
speaking the use of IE for automatic annotation
of multimedia document has the advantage of
providing, besides the results of the (shallow)
syntactic processing, accurate semantic (or con-
tent/conceptual) information (and thus potential
annotation) for specific predefined domains, since
a mapping from the linguistically analyzed rele-
vant text parts can be mapped onto an unambigu-
ous conceptual description6 . Thus in a sense it
can be assumed that IE is supporting the word
sense disambiguation task.
It is also commonly assumed (see among oth-
ers (Cunningham, 1999)) that IE occupies an in-
termediate place between Information Retrieval
(with few linguistic knowledge involved) and
Text Understanding (involving the full deep lin-
guistic analysis and being still not realized for the
time being.). IE being robust but offering only a
partial (but mostly accurate) syntactic and content
analysis, it can be said that this language technol-
ogy is actually filling the gap between available
low-level annotated/indexed documents and cor-
pora and the desirable full content annotation of
those documents and corpora. This is the reason
why MUMIS has chosen this technology for pro-
viding automatic annotation (at distinct linguistic
and domain-specific levels) of multimedia mate-
rial, allowing thus to add queryable ?content in-
formation? to this material.7
5We would like to thank at this place the various institu-
tions making available various textual, audio and video data.
6This topic has already been object of a workshop dis-
cussing the relations between IE and Corpus Linguistics
(McNaught, 2000).
7MUMIS was not explicitly designed for supporting
knowledge management tasks, but we assume that the mean-
ingful organization of domain-specific multimedia material
proposed by the project can be adapted to the organization of
4 The Multimedia Material in MUMIS
The MUMIS project is about automatic index-
ing of videos of soccer matches with formal an-
notations and querying that information to get
immediate access to interesting video fragments.
For this purpose the project chose the European
Football Championships 2000 in Belgium and the
Netherlands as its main database. A major project
goal is to merge the formal annotations extracted
from textual and audio material (including the au-
dio part of videos) on the EURO 2000 in three
languages: English, German, Dutch. The mate-
rial MUMIS has to process can be classified in
the following way:
1. Reports from Newspapers (reports about
specific games, general reports) which is
classified as free texts (FrT)
2. Tickers, close captions, Action-Databases
which are classified as semi-formal texts
(SFT)
3. Formal descriptions about specific games
which are classified as formal texts (FoT)
4. Audio material recorded from radio and TV
broadcasts
5. Video material recorded from TV broadcasts
1-4 will be used for automatically generating
formal annotations in order to index 5. MUMIS
is investigating the precise contribution of each
source of information for the overall goal of the
project.
Since the information contained in formal texts
can be considered as a database of true facts, they
play an important role within MUMIS. But never-
theless they contain only few information about a
game: the goals, the substitutions and some other
few events (penalties, yellow and red cards). So
there are only few time points available for in-
dexing videos. Semi-formal texts (SFT), like live
tickers on the web, are offering much more time
points sequences, related with a higher diversity
the distributed information of an enterprise and thus support
the sharing and access to companies expertise and know-
how.
of events (goals scenes, fouls etc,) and seem to of-
fer the best textual source for our purposes. Nev-
ertheless the quality of the texts of online tick-
ers is often quite poor. Free texts, like newspa-
pers articles, have a high quality but the extrac-
tion of time points and their associated events in
text is more difficult. Those texts also offer more
background information which might be interest-
ing for the users (age of the players, the clubs they
are normally playing for, etc.). Figures 1 and 2 in
section 8 show examples of (German) formal and
semi-formal texts on one and the same game.
5 Processing Steps in MUMIS
5.1 Media Pre-Processing
Media material has been delivered in various
formats (AudioDAT, AudioCassettes, Hi-8 video
cassettes, DV video cassettes etc) and qualities.
All audio signals (also those which are part of
the video recordings) are digitized and stored in
an audio archive. Audio digitization is done with
20 kHz sample frequency, the format generated is
according to the de-facto wav standard. For dig-
itization any available tool can be used such as
SoundForge.
Video information (including the audio compo-
nent) of selected games have been digitized into
MPEG1 streams first. Later it will be encoded in
MPEG2 streams. While the quality of MPEG1 is
certainly not satisfying to the end-user, its band-
width and CPU requirements are moderate for
current computer and network technology. The
mean bit rate for MPEG1 streams is about 1.5
Mbps. Current state-of-the-art computers can ren-
der MPEG1 streams in real time and many net-
work connections (Intranet and even Internet) can
support MPEG1. MPEG2 is specified for about 3
to 5 Mbps. Currently the top-end personal com-
puters can render MPEG2, but MPEG2 is not yet
supported for the most relevant player APIs such
as JavaMediaFramework or Quicktime. When
this support is given the MUMIS project will also
offer MPEG2 quality.
For all separate audio recordings as for ex-
ample from radio stations it has to be checked
whether the time base is synchronous to that one
of the corresponding video recordings. In case of
larger deviations a time base correction factor has
to be estimated and stored for later use. Given that
the annotations cannot be created with too high
accuracy a certain time base deviation will be ac-
cepted. For part of the audio signals manual tran-
scriptions have to be generated to train the speech
recognizers. These transcripts will be delivered in
XML-structured files.
Since keyframes will be needed in the user in-
terface, the MUMIS project will develop software
that easily can generate such keyframes around a
set of pre-defined time marks. Time marks will
be the result of information extraction processes,
since the corresponding formal annotations is re-
ferring to to specific moments in time. The soft-
ware to be written has to extract the set of time
marks from the XML-structured formal annota-
tion file and extract a set of keyframes from the
MPEG streams around those time marks. A set of
keyframes will be extracted around the indicated
moments in time, since the estimated times will
not be exact and since the video scenes at such
decisive moments are changing rapidly. There
is a chance to miss the interesting scene by us-
ing keyframes and just see for example specta-
tors. Taking a number of keyframes increases the
chance to grab meaningful frames.
5.2 Multilingual Automatic Speech
Recognition
Domain specific language models will be trained.
The training can be bootstrapped from written re-
ports of soccer matches, but substantial amounts
of transcribed recordings of commentaries on
matches are also required. Novel techniques
will be developed to interpolate the base-line lan-
guage models of the Automatic Speech Recogni-
tion (ASR) systems and the domain specific mod-
els. Moreover, techniques must be developed to
adapt the vocabularies and the language models
to reflect the specific conditions of a match (e.g.,
the names players have to be added to the vocabu-
lary, with the proper bias in the language model).
In addition, the acoustic models must be adapted
to cope with the background noise present in most
recordings.
Automatic speech recognition of the sound
tracks of television and (especially) radio pro-
grammes will make use of closed caption subtitle
texts and information extracted from formal texts
to help in finding interesting sequences and auto-
matically transcribing them. Further, the domain
lexicons will help with keyword and topic spot-
ting. Around such text islands ASR will be used
to transcribe the spoken soundtrack. The ASR
system will then be enriched with lexica contain-
ing more keywords, to increase the number of se-
quence types that can be identified and automati-
cally transcribed.
5.3 Multilingual Domain Lexicon Building
All the collected textual data for the soccer do-
main are used for building the multilingual do-
main lexicons. This data can be in XML, HTML,
plain text format, etc. A number of automatic
processes are used for the lexicon building, first
on a monolingual and secondly on a multilin-
gual level. Manual browsing and editing is tak-
ing place, mainly in order to provide the semantic
links to the terms, but also for the fine-tuning of
the lexicon according to the domain knowledge.
Domain lexicons are built for four lan-
guages, namely English, German, Dutch and
Swedish. The lexicons will be delivered in a
fully structured, XML-compliant, TMX-format
(Translation Memory eXchange format). For
more information about the TMX format see
http://www.lisa.org/tmx/tmx.htm.
We will also investigate how far
EUROWORDNET resources (see
http://www.hum.uva.nl/ ewn/) can be of use
for the organization of the domain-specific
terminology.
5.4 Building of Domain Ontology and Event
Table
The project is currently building an ontology for
the soccer domain, taking into consideration the
requirements of the information extraction and
merging components, as well as users require-
ments. The ontology will be delivered in an XML
format8.
8There are still on-going discussions within the
project consortium wrt the best possible encoding for-
mat for the domain ontology, the alternative being
reduced probably to RDFS, OIL and IFF, see respec-
tively, and among others, http://www.w3.org/TR/rdf-
schema/, http://www.oasis-open.org/cover/oil.html and
http://www.ontologos.org/IFF/The%20IFF%20Language.
html
In parallel to building the ontology an event ta-
ble is being described. It contains the major event
types that can occur in soccer games and their
attributes. This content of the table is matching
with the content of the ontology. The event ta-
ble is a flat structure and guides the information
extraction processes to generate the formal event
annotations. The formal event annotations build
the basis for answering user queries. The event
table is specified as an XML schema to constrain
the possibilities of annotation to what has been
agreed within the project consortium.
5.5 Generation of Formal Annotations
The formal annotations are generated by the IE
technology and are reflecting the typical output of
IE systems, i.e.instantiated domain-specific tem-
plates or event tables. The slots to be filled by
the systems are basically entities (player, teams
etc.), relations (player of, opponents etc.) and
events (goal, substitution etc.), which are all de-
rived from the current version of the domain on-
tology and can be queried for in the online com-
ponent of the MUMIS prototype. All the tem-
plates associated with an event are including a
time slot to be filled if the corresponding informa-
tion is available in a least one of the sources con-
sulted during the IE procedure. This time infor-
mation is necessary for the indexing of the video
material.
The IE systems are applying to distinct sources
(FoT, FrT etc.) but they are not concerned with
achieving consistency in the IE result on distinct
sources about the same event (game): this is the
task of the merging tools, described below.
Since the distinct textual sources are differ-
ently structured, from ?formal? to ?free? texts, the
IE systems involved have adopted a modular ap-
proach: regular expressions for the detection of
Named Entities in the case of formal texts, full
shallow parsing for the free texts. On the base of
the factual information extracted from the formal
texts, the IE systems are also building dynamic
databases on certain entities (like name and age
of the players, the clubs they are normally playing
for, etc.) or certain metadata (final score), which
can be used at the next level of processing.
5.6 The Merging Tool
The distinct formal annotations generated are
passed to a merging component, which is respon-
sible for avoiding both inconsistencies and redun-
dancies in the annotations generated on one event
(in our case a soccer game).
In a sense one can consider this merging
component as an extension of the so-called co-
reference task of IE systems to a cross-document
(and cross-lingual) reference resolution task. The
database generated during the IE process will help
here for operating reference resolution for more
?verbose? types of texts, which in the context
of soccer are quite ?poetic? with respect to the
naming of agents (the ?Kaiser? for Beckenbauer,
the ?Bomber? for Mueller etc...), which would
be quite difficult to achieve within the sole refer-
ential information available within the boundary
of one document. The project will also investi-
gate here the use of inferential mechanisms for
supporting reference resolution. So for example,
?knowing? from the formal texts the final score
of a game and the names of the scorers, follow-
ing formulation can be resolved form this kind
of formulation in a free text (in any language):
?With his decisive goal, the ?Bomber? gave the
victory to his team.?, whereas the special nam-
ing ?Bomber? can be further added to the entry
?Mueller?
The merging tools used in MUMIS will also
take into consideration some general representa-
tion of the domain-knowledge in order to filter out
some annotations generated in the former phases.
The use of general representations9 (like domain
frames), combined with inference mechanisms,
might also support a better sequential organiza-
tion of some event templates in larger scenarios.
It will also allow to induce some events which
are not explicitly mentioned in the sources under
consideration (or which the IE systems might not
have detected).
5.7 User Interface Building
The user first will interact with a web-portal to
start a MUMIS query session. An applet will be
9Like for example the Type Description Language
(TDL), a formalism supporting all kind of operations on
(typed) features as well as multiple inheritance, see (Krieger
and Schaefer, 1994).
down-line loaded in case of showing the MUMIS
demonstration. This applet mainly offers a query
interface. The user then will enter a query that
either refers to metadata, formal annotations, or
both. The MUMIS on-line system will search
for all formal annotations that meet the criteria
of the query. In doing so it will find the appro-
priate meta-information and/or moments in some
media recording. In case of meta-information it
will simply offer the information in scrollable text
widgets. This will be done in a structured way
such that different type of information can eas-
ily be detected by the user. In case that scenes of
games are the result of queries about formal anno-
tations the user interface will first present selected
video keyframes as thumbnails with a direct indi-
cation of the corresponding metadata.
The user can then ask for more metadata
about the corresponding game or for more media
data. It has still to be decided within the project
whether several layers of media data zooming in
and out are useful to satisfy the user or whether
the step directly to the corresponding video frag-
ment is offered. All can be invoked by simple
user interactions such as clicking on the presented
screen object. Playing the media means playing
the video and corresponding audio fragment in
streaming mode requested from a media server.
6 Standards for Multimedia Content
MUMIS is looking for a compliance with exist-
ing standards in the context of the processing of
multimedia content on the computer and so will
adhere to emerging standards such as MPEG4,
which defines how different media objects will be
decoded and integrated at the receiving station,
and MPEG7, which is about defining standards
for annotations which can be seen as multime-
dia objects. Further, MUMIS will also maintain
awareness of international discussions and devel-
opments in the aerea of multimedia streaming
(RTP, RTSP, JMF...), and will follow the discus-
sions within the W3C consortium and the EBU
which are also about standardizing descriptions of
media content.
7 Role of MUMIS for the Annotation of
Multimedia Content
To conclude, we would like to list the points
where we think MUMIS will, directly or indi-
rectly, contribute to extract and access multimedia
content:
  uses multimedia (MM) and multilingual in-
formation sources;
  carries out multimedia indexing by applying
information extraction to a well-delineated
domain and using already existing informa-
tion as constraints;
  uses and extends advanced language tech-
nology to automatically create formal anno-
tations for MM content;
  merges information from many sources
to improve the quality of the annotation
database;
  application of IE to the output of ASR and
the combination of this with already existing
knowledge;
  definition of a complex information annota-
tion structure, which is stored in a standard
document type definition (DTD);
  integration of new methods into a query in-
terface which is guided by domain knowl-
edge (ontology and multilingual lexica).
So in a sense MUMIS is contributing in defin-
ing semantic structures of multimedia contents,
at the level proposed by domain-specific IE anal-
ysis. The full machinery of IE, combined with
ASR (and in the future with Image Analysis)
can be used for multimedia contents development
and so efficiently support cross-media (and cross-
lingual) information retrieval and effective navi-
gation within multimedia information interfaces.
There seems thus that this technolgy can play a
highly relevant role for the purposes of knowl-
edge detection and management. This is prob-
ably specially valid for the merging component,
which is eliminating redundancies in the annota-
tions generated from sets of documents and estab-
lishing complex reference resolutions, thus sim-
plyfying the access to content (and knowledge)
distributed over multiple documents and media.
References
Doug E. Appelt. 1999. An introduction to information
extraction. AI Communications, 12.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech Com-
munication.
K. Bontcheva, H. Brugman, A. Russel, P. Wittenburg,
and H. Cunningham. 2000. An Experiment in
Unifying Audio-Visual and Textual Infrastructures
for Language Processing R&D. In Proceedings of
the Workshop on Using Toolsets and Architectures
To Build NLP Systems at COLING-2000, Luxem-
bourg. http://gate.ac.uk/.
Daan Broeder, Hamish Cunningham, Nancy Ide,
David Roy, Henry Thompson, and Peter Witten-
burg, editors. 2000. Meta-Descriptions and An-
notation Schemes for Multimodal/Multimedia Lan-
gauge Resources LREC-2000.
H. Brugman, K. Bontcheva, P. Wittenburg, and
H. Cunningham. 1999. Integrating Multimedia and
Textual Software Architectures for Language Tech-
nology. Technical report mpi-tg-99-1, Max-Planck
Institute for Psycholinguistics, Nijmegen, Nethed-
lands.
Hamish Cunningham. 1999. An introduction to infor-
mation extraction. Research memo CS - 99 - 07.
Thierry Declerck and G. Neumann. 2000. Using a pa-
rameterisable and domain-adaptive information ex-
traction system for annotating large-scale corpora?
In Proceedings of the Workshop Information Ex-
traction meets Corpus Linguistics, LREC-2000.
Kevin Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, H. Cunningham, and
Y. Wilks. 1998. University of sheffield:
Description of the lasie-ii system as used for
muc-7. In SAIC, editor, Proceedings of the
7th Message Understanding Conference, MUC-7,
http://www.muc.saic.com/. SAIC Information Ex-
traction.
Christopher Kennedy and B. Boguraev. 1996.
Anaphora for everyone: Pronominal anaphora res-
olution without a parser. In Proceedings of the
16th International Conference on Computational
Linguistics, COLING-96, pages 113?118, Copen-
hagen.
Hans-Ulrich Krieger and U. Schaefer. 1994.

?
a type description language for constraint-based
grammars. In Proceedings of the 15th Interna-
tional Conference on Computational Linguistics,
COLING-94, pages 893?899.
Shalom Lappin and H-H. Shih. 1996. A generalized
algorithm for ellipsis resolution. In Proceedings
of the 16th International Conference on Compu-
tational Linguistics, COLING-96, pages 687?692,
Copenhagen.
John McNaught, editor. 2000. Information Extraction
meets Corpus Linguistics, LREC-2000.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proceedings of the 17th In-
ternational Conference on Computational Linguis-
tics, COLING-98, pages 869?875, Montreal.
MUC, editor. 1995. Sixth Message Understanding
Conference (MUC-6). Morgan Kaufmann.
MUC, editor. 1998. Seventh Message Understanding
Conference (MUC-7), http://www.muc.saic.com/.
SAIC Information Extraction.
Guenter Neumann, R. Backofen, J. Baur, M. Becker,
and C. Braun. 1997. An information extrac-
tion core system for real world german text pro-
cessing. In Proceedings of the 5th Conference on
Applied Natural Language Processing, ANLP-97,
pages 209?216.
Guenter Neumann, C. Braun, and J. Piskorski. 2000.
A divide-and-conquer strategy for shallow parsing
of german free texts. In Proceedings of the 6th Con-
ference on Applied Natural Language Processing,
ANLP-00.
Jakub Piskorski and G. Neumann. 2000. An intel-
ligent text extraction and navigation system. In
Proceedings of the 6th Conference on Recherche
d'Information Assiste?e par Ordinateur, RIAO-2000.
Project URLs:
COL:  
			ffInteraction of Tools and Metadata-Descriptions1 for
Multimedia Language Resources
Daan Broeder
Max-Planck Institute for
Psycholinguistics
Wundtlaan1 6525 XD
Nijmegen
The Netherlands
daan.broeder@mpi.nl
Peter Wittenburg
Max-Planck Institute for
Psycholinguistics
Wundtlaan1 6525 XD
Nijmegen
The Netherlands
peter.wittenburg@mpi.nl
1We distinguish metadata from annotation data, knowing that many don?t make this difference. While metadata in this
context is meant to describe the whole language resource, the annotation is a time synchronous description of what is
happening and is spoken during a recording
Abstract
The increasing amount and complexity of
multi-media/multi-modal language
resources (MMLR) poses a problem in
many respects. This paper wants to discuss
metadata descriptions that can be used to
easy find and locate suitable MMLRs in the
Internet and how these descriptions may be
used to discover and apply suitable tools on
the data.
1 Introduction
We succeeded in reaching a consensus within a
representative part of the linguistic community
in Europe about a standard for such metadata
descriptions. A machine readable
implementation of this standard will then allow
us to build up a searchable and browsable space.
Our presentation is based on the work executed
within the framework of the international
EAGLES/ISLE [1,2] project that is named IMDI
(ISLE Metadata Initiative), on practical work
with meta descriptions at the MPI for
Psycholinguistics, on a collaborative enterprise
to create a browsable corpus demo of material 7
European institutes and on suggestions with
respect to metadata within the DOBES [3] and
the CGN [4] projects.
2 Metada for Language Resources
The idea of describing a whole document with
the help of a few characteristic metadata
elements is not new. Well-known corpora such
as Childes [5] have used header information to
describe the content, the speakers and the
language being spoken etc. The Text Encoding
Initiative [6] and the CES group [7] have
specified in detail the tag set with which a whole
text document can be described. However, all
early initiatives were not meant to be a general
standard for the description of MM LRs and
allow the formation of a searchable and
browsable space on the Internet IMDI desires.
This is what recent initiatives in other domains
such as Dublin Core (DC) [8] and MPEG7 [9]
want to achieve: XML-based machine-readable
information about certain documents that is
openly accessible in the net such that easy
retrieval is possible. New initiatives by the W3C
such as RDF [10] support these intentions.
Within IMDI we have made an overview about
header and metadata elements used so far by the
language resource community. This overview
and the concrete needs within large European
projects will be used to develop and test a first
proposal on the way to come to a hopefully
widely accepted standard. Compliance with the
standard has to guarantee that metadata
descriptions created by different people at
different locations adhere to the same syntax and
to the same semantic definitions of the metadata
elements included. The standard has to offer
possibilities of adding metadata elements
defined by sub communities, projects or even
individuals. From other initiatives we know that
these goals can only be achieved if the set of
metadata elements is not too exhaustive. This
does not mean that only limited information can
be stored. For instance the metadata description
standard certainly includes an element to enter
the name of the language spoken, but other very
elaborate information about that language can be
made available in other data types pointed to by
hyperlinks to other data perhaps conforming to
more specialised schemas.
IMDI is now entering a phase where the
metadata element categories and the metadata
elements to be included have been discussed
with interested members of the MMLR
community for about a year and become stable.
Two resource types were selected to start with:
(1) multimedia corpora and (2) lexicons, the
discussion about the lexicon resources is at the
moment less far developed than that concerning
the corpora. Within the IMDI initiative we
started the search for a suitable set of metadata
elements by trying to identify the characteristics
of such resources that people such as
researchers, developers, students, or even the
general public would choose to use to find
exactly those resources they are looking for.
Very helpful was the study of the creation
process and the construction of a structured
metadata set as a reflection of an ontology of
these resources. We know that resources
themselves are not openly available, but at least
the metadata description should inform the
community about their existence, about
intellectual property rights and modes of usage.
Two main categories of metadata can be
identified:
? Basic information on the content of the
resource: the content language of the
resource, and administrative information
about the resource.
? Resource descriptions that define the
type and structure of the resource.
A full listing of all IMDI elements is given in
Appendix A, but for definitions and
substructures we refer to [1]. The relevant
elements for the resources themselves are:
Table 1 elements
for media files
Table 2
elements for
annotation
units
C: constrained
OV: open vocabulary
CCV: closed constrained vocabulary
For annotation units multiple units may reside in
one file. The relevant elements for
characterising the resources in a way that is
important to tools (a discussion that we will
come to later) are:
o Reference to the resource itself
o Size (of media file, if the tool has a
limit)
o Format (for media files somewhat more
simple then for annotation units)
o Type (for annotation unit the type of
analysis result e.g. morphology,
phonetics ?)
o Different encodings
Resource Link (c)
Media Resource Link (c)
Annotator (string)
Date (c)
Type (ov)
Format (ov)
Content Encoding (string)
Character Encoding (c)
Resource Link (c)
Size (string)
Type (ccv)
Format (ov)
Quality (ccv)
Recording Conditions (string)
Position (c)
3 Strategies for Metadata Standards
The way IMDI has developed its metadata
vocabulary can be described as bottom up.
IMDI chose to try to first understand the
linguistic community?s needs by making an
overview of metadata used by different projects
and corpora, speak with representatives of many
institutions and try to distill a metadata set from
it that focuses on retrieval aspects. For IMDI the
needs of the creators are the start and end point
since the creators are also the major consumer
group of language resources. So the question for
IMDI posed itself was ?how to enable resource
discovery of useful language resources that can
be used for certain studies etc?. This approach
leads to a metadata set whose terminology fits
the domain and a vocabulary that is considerable
richer than the for instance the DC set.
Interesting enough another initiative named
OLAC [11] that wants to create metadata for
language resources has taken the DC set as a
starting point. The OLAC approach can be
called a top-down one and seems motivated by
the wish to join the ?very important? Open
Archives Initiative (OAI) [12] without having
too much work in mapping different metadata
sets. OLAC wants to use a slightly more
specialised version of the OAI metadata set and
because OAI uses Dublin Core as default
metadata set the choice of an extended DC set
for OLAC is understandable. Of course the
question remains if this is sufficient to
characterise language resources in a sufficient
specific manner.
The discussion showed that both approaches are
important especially when the ontology of the
domain is not very well understood. IMDI starts
with analyzing the domain and leads to a more
narrow and specialised categorization scheme.
DC on the other hand offers very broad
categories the semantics of which are often
sloppily defined. Both approaches lead to
specific inherent retrieval problems. We have to
consider two views; (1) People from inside the
domain searching for resources (2) People from
outside the domain. People from inside have
intimate knowledge of the domain ontology and
want more specific categories. People from
outside need broader categories to assure that
the resources they search fall in the larger ?hit
list?. The discussion about OLAC DC qualifiers
led partly to the same discussions that were
carried out in IMDI. This is not surprising, since
OLAC somehow has to address the needs of the
field and the participants at the meeting were
mainly linguists. The OLAC top-down approach
which starts with a smaller vocabulary than
IMDI will have less problems when addressing
the interoperability with metadata sets such as
DC. However this advantage disappears when
OLAC will add more specific elements and
qualifiers to accurately describe the domain.
4 Tools and Metadata
It was the initial idea that the metadata
descriptions could also have elements that
describe specific tools that can be used to act on
the resources themselves. However since
resources and tools form orthogonal dimensions
it is better to have the metadata description only
describe resources and not a set of tools that will
change in time anyway. A more elegant solution
is to describe the type and structure of the
resources in sufficient detail so that ?browser?
tools used to access the metadata description can
decide which ones of the available tools are
suitable to handle the data. This can be either
based on local user configurable information or
on some sort of remote tool registry. At the
moment IMDI is experimenting with a scheme
of (semi) mime-types to characterise language
resources. We foresee that users will want to
customise the mapping of tools to resource types
to their own taste just as they are able to do with
WWW-browsers.
Needed for such a scheme is that tool
repositories note the types (mime-types) and
encoding, character encoding for which the tools
are suitable (see the lists in table 1 and 2). It has
to be investigated in detail how far tool
registries and resource collections structured by
metadata descriptions can be created in a way
such that especially na?ve users can overcome
the frustrating problems of accessing the right
resources with suitable tools. This problem is
not solved and is one of the greatest obstacles
for increasing the reusability of the huge
treasure of resources. IMDI has taken limited
tests with a number of tools to study the
interaction between mime-type tagged resources
and selecting from a tool palette. We have no
doubt that this is the way to go.
A special question is the form of the
infrastructure. Where will we store the metadata
descriptions and/or resources and how are tool
registries such as from DFKI [13] made known
to the distributed resource universe? During the
IMDI project a preliminary solution is found for
creating a registry authority for metadata. This
registry authority has to build a web-portal,
check the quality of the produced meta
descriptions, create intuitively understandable
browsable hierarchies based on the meta
descriptions and link the meta descriptions to
other type of information and resources The
registry authority will also provide tools such as
a constrained editor that allows the user to create
meta descriptions and a suitable browser which
can operate on the metadata description files.
The IMDI project will also work on
requirements for the registry authority and the
metadata tools.
At the moment the time has come for IMDI to
investigate if and how the metadata description
browser can access remote software registries to
assist users in the choice of tools to use for
resources. This would be a logical extension to
the local configurable mapping of tools on
resource types that is needed anyway for non-
networked situations.
References
[1] http://www.mpi.nl/ISLE
[2] P. Wittenburg, D. Broeder & B. Sloman:
Meta-Descriptions for Language Resources
- EAGLES/ISLE - A Proposal for a Meta-
Description Standard for Language
Resources.
http://www.mpi.nl/ISLE/documents/papers/
white_paper_11.pdf.
[3] DOBES http://www.mpi.nl/DOBES
[4] Corpus Gesproken Nederlands
http://www.nwo.nl/gw/introductie/
[5] CHAT http://childes.psy.cmu.edu/
[6] TEI http://www.uic.edu/orgs/tei/
[7] CES http://www.cs.vassar.edu/CES
[8] DC http://purl.org/dc/documents/
[9] MPEG7 http://mpeg7
[10] RDF http://www.w3.org/RDF/
[11] OLAC http://www.language-archives.org/
[12] OAI http://www.openarchives.org/
[13] DFKI-softwareregistry
http://registry.dfki.de/
Appendix A: IMDI Metadata Vocabulary for Language Resources.
Session
Name (string) Resources (group)
Title (string) Media File + (group)
Date (c) Resource Link (c)
Continent (ccv) Size (string)
Country (ccv) Type (ccv)
Region (string) Format (ov)
Address (string) Quality (ccv)
Description + (sub) Recording Conditions (string)
Keys (sub) Position (c)
Project (group) Access (sub)
Name (string) Description + (sub)
Title (string) Annotation Unit + (group)
Id (string) Resource Link (c)
Contact (group) Media Resource Link (c)
Description + (group) Annotator (string)
Collector (group) Date (c)
Name (string) Type (ov)
Contact (sub) Format (ov)
Description + (sub) Content Encoding (string)
Content (group) Character Encoding (c)
Communication Context (group) Access (sub)
Interactivity (ccv) Language (sub)
Planning Type (ccv) Anonymous (ccv)
Involvement (ccv) Description + (sub)
Genre (group) Media Carrier +
Interactional (ovl) Id (string)
Discursive (ovl) Format (ov)
Performance (ovl) Quality (ccv)
Task (ocv) Position (c)
Modalities (ocv) Access (sub)
Languages (group) Description + (sub)
Description + (sub) Anonymous (group)
Language + (sub) Resource Link (c)
Description + (sub) Access (sub)
Keys (sub) References (group)
Participants (group) Description + (sub)
Description + (sub)
Participant (group)
Type (ov)
Name + (string)
Full name (string)
Role (ov)
Language + (sub)
Age (c)
Sex (ccv)
Education (string)
Anonymous (ccv)
Description + (sub)
Keys (sub)
Towards Metadata Interoperability 
Peter Wittenburg 
MPI for Psycholinguistics 
Wundtlaan 1 
6525 XD Nijmegen, Netherlands 
Peter.Wittenburg@mpi.nl 
Daan Broeder 
MPI for Psycholinguistics 
Wundtlaan 1 
6525 XD Nijmegen, Netherlands 
Daan.Broeder@mpi.nl 
Paul Buitelaar 
DFKI 
Stuhlsatzenhausweg 3 
D-66123 Saarbr?cken 
paulb@dfki.de 
 
Abstract 
Within two European projects metadata 
interoperability is one of the central top-
ics. While the INTERA project has as 
one of its goals to achieve an interopera-
bility between two widely used metadata 
sets for the domain of language re-
sources, the ECHO project created an in-
tegrated metadata domain of in total nine 
data providers from five different disci-
plines from the humanities. In both pro-
jects ad hoc techniques are used to 
achieve results. In the INTERA project, 
however, machine readable and ISO 
compliant concept definitions are created 
as a first step towards the Semantic Web. 
In the ECHO project a complex ontology 
was realized purely relying on XML. It is 
argued that concept definitions should be 
registered in open Data Category Reposi-
tories and that relations between them 
should be described as RDF assertions. 
Yet we are missing standards that would 
allow us to overcome the ad hoc solu-
tions. 
1 Introduction 
Metadata is a key source of information towards 
realization of the Semantic Web that could be 
exploited in many different ways. Several pro-
jects are starting to focus on exploiting rich 
metadata in and between projects and disciplines. 
For instance, the ECHO (European Cultural 
Heritage Online)1 project brings together meta-
data for resources from the History of Arts, His-
tory of Science, Linguistics, Ethnology and 
Philosophy. One aspect of the work in ECHO is 
to create a cross-disciplinary domain for resource 
discovery. In the INTERA (Integrated European 
Language Resource Area)2 project one of the 
                                                           
1 ECHO: http://www.mpi.nl/echo 
2 INTERA: http://www.elda.fr/rubrique22.html 
tasks is to establish a foundation for a more 
flexible definition and use of metadata for lan-
guage resources. 
 
We can distinguish two types of metadata. The 
first one concerns its use as ?data about data?. 
This definition of metadata includes for example 
text that describes images, sounds, videos and 
other texts. Such metadata can exist in different 
forms like complex annotations of media re-
cordings as discussed for example by Bird (2001) 
and Brugman (2001). A second type of metadata 
consists of keywords describing objects that form 
the catalogues of the increasingly large digital 
collections, e.g., of linguistic data. This type of 
metadata was introduced by initiatives such as 
Dublin Core3 for general type web-resources, 
OLAC4 for general type linguistic resources and 
IMDI5 for more elaborate linguistic resource de-
scriptions that are useful not only for discovery 
but also for management purposes.  
 
Although the first type of metadata is very im-
portant for the above mentioned use in content 
descriptions, in this paper we will focus on as-
pects that are related to the second, keyword type 
of metadata. It is obvious that this type of meta-
data  
? contains amongst others important informa-
tion about a resource that cannot be retrieved 
from its content; 
? are especially relevant for the discovery and 
management of multimedia resources since 
speech and image recognition are still far 
away from being applicable in most cases; 
? includes a reduced set of descriptive ele-
ments and requires classification such that 
content information in many cases is richer; 
? offers a limited set of semantically well-
defined data categories (ISO 12620) that can 
be related with other concepts. 
 
                                                           
3 Dublin Core: http://dublincore.org 
4 OLAC: http://www.language-archives.org 
5 IMDI: http://www.mpi.nl/IMDI 
In this paper we will describe the problems that 
we encountered in the INTERA and the ECHO 
projects to come to interoperable metadata do-
mains, the structural and semantic solutions that 
were chosen to solve the tasks and the solutions 
we are aiming at in the long run. In this context 
we will also refer to the intentions within ISO 
TC37/SC46. 
2 Current tasks 
The INTERA task 
One focus of the work in the INTERA project is 
on the integration of metadata elements that are 
used in describing language resources for open 
data category repositories. Two metadata sets are 
being used currently for the discovery and man-
agement of language resources. The OLAC set is 
used for discovery purposes and aims to be used 
for all kinds of language resources. The set was 
derived from the Dublin Core set, i.e., on pur-
pose it only includes a limited set of elements.  
 
The IMDI set was designed bottom-up and is 
used for discovery and management purposes. It 
is a rich and structured set especially derived for 
annotated resources and lexica. The distributed 
IMDI domain was extended in the INTERA and 
ECHO projects to more than 27 participating 
European institutions sees itself as an OLAC data 
provider, i.e., the OLAC harvester can read all 
IMDI records that are offered via the Open Ar-
chives Initiative metadata harvesting protocol7 
(OAI MHP). A wrapper is used to map the IMDI 
elements to the OLAC elements, i.e., the map-
                                                           
6 ISO TC37/SC4: http://www.tc37sc4.org 
7 OAI MHP: http:// www.ukoln.ac.uk/cd-
focus/presentations/ cldprac/sld020.htm 
ping relations are hardwired into a server-based 
program.  
 
Recently, a new version of the IMDI metadata 
set (version 3.0.3) was provided. In parallel, also 
the new version of the OLAC metadata set (Au-
gust 2003) was worked out. Both metadata sets 
are described by human readable definition docu-
ments available in the web. New mapping rules 
have to be constructed which for short-term 
needs will again be hard-wired into a server-
based program.  
 
But this is not seen as being sufficient to serve 
future needs. New ways have to be developed for 
making the mapping more transparent and to pre-
pare the metadata domain for Semantic Web ap-
plications. Therefore, as a first step, the IMDI 
metadata concepts are entered into the open data 
category registry that is currently emerging 
within ISO TC37/SC4.  
The ECHO task 
In the ECHO project one of the tasks is to create 
a metadata domain that covers five disciplines 
and several institutions within each discipline. In 
total we were confronted with nine different 
metadata sets.  
 
The table below gives an overview of the meta-
data types that we were confronted with. One of 
the sets is DC compliant, two produce descrip-
tions that are close to DC, two provide true OAI 
compliance including the delivery of DC records. 
Most of the data is extracted from relational da-
tabases, encoding other types of data as well. In 
many cases the elements used were not well de-
fined, possibly leading to differences in usage by 
the metadata creators. 
 
Domain ? Sub-domain size Type MD 
Formal 
State 
Harvesting 
Type Comment 
HoA - Fotothek very large MIDAS Iconclass 
non 
validated XML export from a database 
HoA - Lineamenta small close to DC non val XML export from a database 
HoA ? Maps of Rome small self-defined non val XML export from a database 
HoS ? Berlin Collection large close to DC validated XML export from a database 
HoS ? IMSS pot large DC non val XML export from a database 
E ? Ethnology Museum 
Leiden RMV very large 
OMV 
OMV Thesaurus validated OAI export from a database 
E ? NECEP database small self defined validated XML export from a database 
L ? IMDI Domain large IMDI set validated XML/OAI true XML domain 
P ? Collection of Texts small self defined non val XML XML texts 
History of arts (HoA), History of Science (HoS), Linguistics (L), Ethnology (E), Phylosophy (P) 
Also the way in which the content of resources is 
described differs substantially. In Fotothek the 
IconClass thesaurus is used to categorize the con-
tent of photos and images. In the RMV catalogue 
the OVM thesaurus is used which is similar to 
the AAT thesaurus. Some use the subject field 
from the DC element set with all its weaknesses, 
others have an unconstrained keyword field and 
the elaborate IMDI set has a couple of elements 
that describe the content such as ?task?, ?genre?, 
?subgenre?, ?language? and ?modalities?.  
 
A variety of description options is used for the 
indication of geographic regions. In the RMV 
case a geographic thesaurus is used. Others use 
descriptors such as ?country? and ?region?. In 
some instances language names have to be used 
to indicate a geographical overlap. 
 
When creating an interoperable metadata domain 
one has to cope with problems at each layer: 
character encoding, data harvesting, syntactical 
aspects and semantic integration. Only the last 
point is of relevance in the context of this paper.  
 
To enable semantic integration an ontology was 
built that covers  
 
? nine metadata repositories; 
? a file where all metadata concepts rele-
vant for the integrated domain ECHO 
domain are listed including their descrip-
tion in a number of major languages (the 
setup is similar to the one used within 
ISO TC37/SC4); 
? a file that includes all mappings between 
these concepts where each individual set 
presents a view that is mapped to all oth-
ers; 
? two geographic thesauri containing dif-
ferent types of geographic information 
with cross-links between them; 
? two category thesauri describing the con-
tent of the resources; 
? two mapping files containing one-
directional cross-links between the two 
thesauri; 
? a file that contains all content type of de-
scriptions that occur in the metadata re-
cords and which do not use one of the 
big thesauri with mappings to these two. 
 
As we are currently using the existing files sim-
ply as exchange formats they have been repre-
sented in XML (rather than RDF for instance). 
To implement fast search, specially optimized 
internal representations are chosen and combined 
with fast indexes. The representations are such 
that all occurring references are expanded in 
preparation time and not during execution time. 
A special engine was programmed that can oper-
ate on these extended representations. 
 
To illustrate this we use an example with geo-
graphic thesaurus information. A search for 
?Country=Italy? should result in hits for all ob-
jects that have to do with ?Italy? either as the 
creation site or as the site where the scene takes 
place. The metadata records are now extended 
such that for all locations that are within ?Italy? 
the nodes appearing higher up in the thesaurus 
hierarchy are added. This assures that a record 
containing for example ?Rome? will also be in-
dicated as a hit when ?Italy? was entered in the 
query. 
 
Exploiting all repositories during run-time by 
intelligent crawlers would require fast parallel 
algorithms. Only parallelism would yield the 
execution speed needed to satisfy the users.  
Relation types 
We have discovered different types of relations 
between the concepts used in the INTERA and 
ECHO projects.  
 
In the INTERA project we can indicate internal 
relations within the structured IMDI metadata 
set, i.e., structure conveys semantic relations. An 
example can be given by the many attributes of a 
participant. A certain participant has a ?name? as 
an identifier and various attributes such as ?age?, 
?role? and ?education?. Between the IMDI and 
OLAC concepts there are three types of relations: 
(1) For some concepts one can speak of equality 
and it was agreed that the controlled vocabularies 
will be unified where possible. (2) There are also 
hierarchical relations such as ?subClass? and 
?superClass? between some of the concepts. (3) 
There is a type of relation where we can speak 
about a semantic overlap that we cannot specify 
in more detail. Finally, there are concepts such as 
?age? or ?education? of a participant that do not 
map at all.  
 
For the mappings in ECHO we have identified 
four useful types of relations: (1) ?isEqualTo? 
defines semantic equivalence, (2) ?isSubclassOf? 
defines a hyponymy relation, (3) ?isSuperclas-
sOf? defines the inverse and (4) ?mapsTo? is 
used to express a semantic overlap. In most 
cases, the ?mapsTo? relation type was used ? a 
one-directional relation indicating semantic over-
lap that should be exploitable. It is not clear yet 
in how far it makes sense to define the fuzzy 
?mapsTo? relation in terms of the standard types 
provided by RDF(S)8 and/or OWL9. All concepts 
that do not map to others or that are too special 
(for example ?size of an image?) were excluded 
in the ontology definition process. 
Examples from ECHO 
Using the described ECHO interoperability 
framework a number of experiments were carried 
out for evaluation purposes. A few examples will 
be discussed here. 
 
Example 1 
Simple Search ?dogon? 
 1 match was found: NECEP: 1 
Complex Search ?dogon? 
 View NECEP - society name: 1 in NECEP 
 View IMSS - Ianguage: 1 in NECEP 
 View DC - language: 1 in NECEP 
 View Language - language: 1 in NECEP 
Complex Search ?mali? 
 View Language - country: 1 in NECEP 
 
This example demonstrates the effect of the 
mapping between the metadata sets and of the 
geographical thesaurus. The language element is 
mapped to the society name element in NECEP 
although this is semantically not correct. Enter-
ing ?mali? in the country specification yields a 
hit since ?mali? is seen as a superclass to 
?dogon?. Here a relation type such as 
?has_language? would be semantically more ap-
propriate.  
 
Example 2 
Simple Search ?inuit? 
 2 matches are found: Language: 1, NECEP: 1 
Complex Search ?inuit? 
 View Language - *: 0 in Language (could not be  
found in the Language domain) 
 View Language ? language: 1 in NECEP 
Complex Search ?greenland? 
 View Language ? language: 1 in NECEP 
 
The results are similar compared to example 1. It 
indicates that the element including ?inuit? in the 
language domain is not an element that is used 
for mapping. It was used as avalue of an optional 
                                                           
8 RDF: http://www.w3.org/RDF 
9 OWL: http://www.w3.org/2001/sw/WebOnt 
element by one specific researcher. This example 
shows that simple search covering all metadata 
elements can lead to improved results. 
 
Example 3 
Simple Search ?agriculture? 
 75 matches are found: Language: 73, Fotothek: 2 
Complex Search ?agriculture? 
 View Fotothek - iconography: 2 in Fotothek  
 View RMV ? content: 2 in Fotothek 
 View IMDI ? content: 2 in Fotothek 
 
These results are misleading and demonstrate the 
weakness of simple search. The 73 hits for lan-
guage result from matching with the recording 
place (?southern agriculture kindergarten?) and 
the affiliation of an actor (?ministry of agricul-
ture?). These results obviously do not refer to 
documents the user was serching for. In the case 
of Fotothek the hits make sense since it is about 
?harvesting?. The mapping in complex leads to 
the expected results, the misleading hits from the 
language domain are not found.  
 
Example 4 
Simple Search ?clothing?  
 22 matches: Language: 8, RMV: 8, Fotothek: 6 
Complex Search ?clothing? 
 View RMV ? content: 8 in RMV, 6 in Fotothek 
 View Fotothek ? iconography: 8 in RMV, 6 in  
Fotothek 
 View Language ? content: 8 in RMV, 6 in  
Fotothek 
 
Again the rich annotations that are used in vari-
ous free-text fields in the language domain lead 
to wrong hits. They are about chats at the bakery 
shop and the clothes people are wearing ? so it?s 
not about clothing as an object which may be 
intended by the person specifying the search. The 
results for complex search from different do-
mains shows the correctness of the mappings.  
 
Example 5 
Simple Search ?horses? 
 7 matches: Fotothek: 2, Language: 2, IMSS: 3 
Complex Search ?horses? 
 View Fotothek ? object title: 3 in IMSS 
 View Fotothek ? iconography: 2 in Fotothek 
 View Lineamenta ? title: 3 in IMSS 
 View Lineamenta ? keywords: 2 in Fotothek 
 View IMSS ? title: 3 in IMSS 
 View IMSS ?subject: 2 in Fotothek 
 View Language ? title: 3 in IMSS 
 View Language ? content: 2 in Fotothek 
 
This example clearly indicates the strength of 
simple search and the weakness of complex 
search. The pattern used by complex search can 
be compared with a narrow path in the complex 
semantic space. If selecting the title element the 
hits of IMSS are found, if the content element is 
chosen the Fotothek hits are found. Both, how-
ever, are leading to useful hits where ?horses? 
are central concepts in the resources. The reason 
for the indicated results are partly caused by very 
sparsely encoded metadata. In the case of IMSS 
the term ?horses? is only mentioned in the title, 
the content element is yet not used. In the lan-
guage case thesaurus information is used to infer 
from the string found in the title element (?spa-
tial layout task, farm scenarios?) to ?horses?.  
Summary 
Only the first three relations (equality, hypo-
nomy, hyperonomy) can be used in a strictly 
logical way. The fourth relation type is of a fuzzy 
nature but occurs most frequently. To prevent a 
semantic cycle during searching, the specially 
tailored inference engine is restricted to one in-
ference step over this fuzzy relation and exploits 
all relations only in one direction10. It is evident 
that the existing ontology does not describe a 
complete logical system. 
 
In case of the INTERA project we will continue 
to rely on a wrapper that will map IMDI to 
OLAC records to allow OAI style of harvesting. 
In the ECHO project we created optimized in-
dexes such that searching can be executed fast, 
i.e., the knowledge components in XML are sim-
ply used as interchange formats allowing for the 
easy identification of all structural components 
and for their validation. 
3 Foundation for Metadata-
Interoperability 
In the previous sections we described the current 
state of the practical work in two projects to 
achieve semantic interoperability. The way cho-
sen has a number of disadvantages in the long-
run: 
 
? In the ECHO project there are no con-
cept definitions that adhere to open and 
emerging standards such as ISO 11179 
and ISO 12620, and which are available 
in validated machine-readable registries. 
                                                           
10 It should be noted, however, that advanced infer-
ence systems can handle semantic cycles of this na-
ture. 
? The current definitions do not contain 
hierarchical relations, which could be 
part of the concept definitions if agreed 
upon by the community.  
? A contribution from other experts, for 
example to improve the definitions and 
to add other language specific aspects, is 
largely excluded. 
? The representation of the semantic rela-
tions between concepts is partly encapsu-
lated in a program preventing any 
flexibility. In the ECHO case they are 
structurally described with the help of 
XML tags, however, it would be much 
better to provide them in a way that in-
ference engines relying on RDF(S) and 
OWL could operate on them.  
 
From the practical work we learned that often the 
semantic scope of the metadata elements is not 
specified as precisely as seems possible and also 
necessary. This will allow for a spectrum of us-
age that will have effects not only on human in-
terpretation, but especially on the way of 
mapping relations to chose. It is obvious from 
this experience that users will not always agree 
on the interpretation of the definitions and on the 
types of mappings applied. At this moment we 
cannot make final statements in how far hierar-
chical relations will be effected by this that 
would constitute an implicit thesaurus as is ex-
pected within ISO TC37/SC4. 
Open Data Category Repositories 
Based on the experience so far it can be recom-
mended to include into open repositories only 
concepts that have been used for a while and 
therefore have shown their semantic stability 
within a certain community. For the area of lan-
guage resources ISO TC37/SC4 is on the way to 
create such a repository, which is compliant with 
widely recognized standards such as ISO 11179 
and ISO 12620. Therefore, it makes sense to reg-
ister all elements used within IMDI and OLAC 
as data categories in this repository.  
 
This will open up several new possibilities for 
projects and initiatives: (1) IMDI and OLAC can 
create schemas that define their sets by referring 
to machine-readable definitions. For instance, an 
equality relationship can be directly indicated by 
referring to the same data category registry 
(DCR) entry. Search engines could make use of 
this information. (2) It is our experience that pro-
jects often like to tailor their own metadata sets 
due to their specific needs. In this case an open 
registry would simply allow to create a new 
schema and to re-use existing definitions as 
much as possible11. By referring to DCR entries 
again a direct form of interoperability is 
achieved.  
 
We assume that we will have widely recognized 
DCRs as currently defined within ISO 
TC37/SC4. They should contain the concepts 
that are based on a wide agreement within com-
munities. However, due to the slow acceptance 
processes within standardization bodies and the 
different needs that result for example from dif-
ferent languages there could be a need for re-
searchers to set up their own temporary DCRs. 
We therefore foresee a large number of data 
category repositories.  
 
 
For the ECHO project the usage of an open DCR 
is not yet an option. To be of use for the commu-
nity there has to be a wide acceptance. The do-
main of ?cultural heritage? addressed within 
ECHO covers too many different disciplines and 
the concepts are semantically mostly too differ-
ent. Disciplines such as history of arts, history of 
science and ethnology have to start their disci-
pline oriented discussion process to define useful 
concepts and to start building widely recognized 
registries. What seems necessary is to start creat-
ing files with concept definitions that can be eas-
ily integrated later into open registries and that 
are compliant to emerging standards.  
Open Relation repositories 
Concept definitions in DCRs are one important 
aspect in defining metadata ontologies. Another 
aspect are repositories that store relations be-
tween these concepts. From our experience in the 
two projects mentioned, it seems required to 
separate these two types of information in order 
to achieve a high degree of independence and 
flexibility. However, other experiences as that of 
the GOLD initiative (Farrar, 2005) indicate that 
opinions on this vary largely. 
 
Theoretically, it is possible to include all infor-
mation that defines a concept into the DCR. The 
concept ?country? that is used within IMDI is 
                                                           
11 IMDI already provides a step towards this kind of 
flexibility by allowing projects to define profiles or 
individuals to define new key-value pairs. 
typically a sub-part of a ?continent?. However, 
the proper definition of the concept ?country? in 
the context of language resources is not depend-
ent on the availability of this hierarchical rela-
tion. But this again may be completely different 
for abstract linguistic concepts such as ?transitive 
verb? where we know that the class relation 
?transitive-verb isSubClassOf verb? is part of the 
definition.  
 
In general, we argue that whenever it is not 
strictly necessary for the proper definition of a 
concept, relation aspects should be kept outside 
of DCRs as much as possible, since they often 
form a constraint with only little agreement.  
 
For the represention of relations in a machine-
readable format, RDF(S) seems to be the most 
suitable choice. In RDF, all relations are repre-
sented as tertiary assertions as indicated in Figure 
1. Actually, each of these RDF assertions defines 
a relation between two resources, since the value 
can be an arbitrary web-resource as well. 
 
 
 
 
Figure 1 shows a basic RDF assertion specifying that 
a (web) resource identified by a URI has properties 
that may have values.  
 
Obviously, this simple mechanism allows us to 
create complex repositories of semantic relations. 
Since all objects of such an assertion can be web-
resources we can for example point to concepts 
defined in a DCR and relate them with each 
other.  
 
From the two mentioned projects we can give 
two typical examples. From the INTERA project 
we notice that according to our interpretation the 
concept ?IMDI:Participant.Role=Collector? is a 
sub-class of ?OLAC:Creator? (Figure 2).  
 
 
 
 
Figure 2 shows a typical relation that can be found in 
the INTERA project. 
 
 
 
 
Figure 3 shows a typical relation that can be found in 
the ECHO project. 
 
resource value 
property 
I:Participant O:Creator 
isSubClassOf 
I:Genre F:Iconography 
mapsTo 
In the ECHO project we can identify a semantic 
overlap between ?IMDI:Genre? and ?Foto-
thek:Iconography? (Figure 3).  
 
We can imagine that RDF will be used by some 
projects, initiatives and institutions to establish 
widely recognized and used repositories with 
mapping relations.  
 
We also assume that many persons, projects and 
institutions will create their own mappings to 
tune their operations like searching according to 
their specific needs, i.e., a large variety of ?prac-
tical ontologies? will emerge. These practical 
ontologies may re-use most of the semantics 
found in a repository, or they overwrite a certain 
number of relations or they introduce new rela-
tions that are not yet defined elsewhere. 
 
In contrast to the ISO data category repository 
that is based on the experiences of the work 
about ISO 11179 and ISO 12620, there is no 
work yet of how to represent relations for the 
domain of language resources. For INTERA this 
creates the need of using ad hoc solutions. ISO 
TC37/SC4 should urgently take up this issue. 
4 Registries and Engines 
Given the discussion above, we can expect the 
Semantic Web era to produce a large number of 
data category definitions stored in different 
DCRs and mapping relations between these 
stored in other repositories. Amongst these com-
ponents there will be some that deserve a larger 
interest by the language resource community, 
since they are maintained by recognized experts, 
but there will also be many others created within 
projects and institutions or even by individuals to 
satisfy only ad-hoc purposes. Therefore, we need 
an infrastructure for registering these compo-
nents for making them visible and searchable. 
 
Current inference engines such as provided by 
Jena12 assume that there is one database of mean-
ingful RDF triples. This would allow us to inte-
grate all our mapping relations from the INTERA 
or ECHO ontologies (such as ?Country isSub-
ClassOf Continent? and ?Place isSubClassOf 
Country?), that is currently part of an XML-
based thesaurus. To arrive at an RDF-based da-
tabase instead, we would need to harvest meta-
data from the XML-based thesaurus, i.e., we 
                                                           
12 Jena: http://jena.sourceforge.net 
would first have to write a wrapper that converts 
XML structure information into RDF assertions. 
 
Further, we would like to harvest RDF triples 
from different sites, since we need to integrate 
already existing knowledge. Two problems can 
be foreseen here: (1) How do we know where to 
find useful RDF triple instances? We need 
mechanisms to register the existence of sites with 
that type of information and to semi-formally 
describe the content. (2) When we harvest triples 
from such a site we may include knowledge ? 
metadata ontologies defined in RDF(S) - that is 
conflicting with what is already available. How 
can we deal with this and how can we be selec-
tive? 
 
Currently, there are no answers to these ques-
tions. But they have to be addressed soon. Also 
here ISO TC37/SC4 could play an important 
role, since it is about infrastructure aspects that 
have to be worked out for the language resource 
community. 
5 XML vs RDF 
We explained why XML was chosen in repre-
senting the knowledge involved in the projects 
mentioned. Mainly short-term arguments guided 
us to take this decision. This may not be the cor-
rect decision in the long-term. Nevertheless, also 
ISO TC37/SC4 has chosen to represent data 
category definitions as XML structures including 
hierarchical references needed to properly define 
a concept. 
 
The underlying data models of XML and RDF 
are very different. XML is based on a tree model, 
i.e., it has a strong bias towards hierarchies. All 
expressive power is gained from structural rela-
tions, which to a certain extent allow for the rep-
resentation of semantic relations. 
 
In contrast to this, RDF is based on a loose col-
lection of relations. It is therefore very simple to 
combine relations from different RDF reposito-
ries into larger collections. Although implicit 
hierarchies will be difficult to recover. 
 
Semantically, RDF Schema offers the user the 
option to define the value range of any user-
defined relation (property) used in an RDF file 
with user-defined classes, while XML only offers 
basic data types. OWL has even more expressive 
power. A good overview is given by Gil and 
Ratnaker, 2001.  
 
Summarizing, we would like to emphasize the 
following two points that need to be taken into 
account by any follow-up projects of INTERA 
and ECHO. Such a project should:  
 
? represent all concept definitions of a resource 
metadata set in an ISO DCR compliant way 
and turn them over to RDF-based reposito-
ries that may emerge within the disciplines in 
the coming years; 
? represent relations as much as possible in 
external RDF(S)-based metadata ontologies 
using all needed expressional power of 
RDF(S) and OWL so that users can easily 
add their own relations or reformulate exist-
ing ones. 
6 Conclusion 
The work on metadata interoperability in the two 
projects mentioned clearly indicate that this type 
of work is in its beginning phase. Ad hoc meth-
ods are used to achieve high speed and to guaran-
tee efficient exchange of knowledge components, 
but they form obstacles on the way towards a 
flexible and open Semantic Web type of infra-
structures. The examples indicate that the chosen 
mapping strategies lead to the expected results in 
many cases. They also indicate some of the prob-
lems that are associated with using specific ele-
ments for searching. Amongst others these are 
caused by sparsely filled in metadata descrip-
tions, unawareness about the underlying element 
semantics, insufficient mappings between meta-
data elements and thesaurus concepts. 
 
The usage of ISO 11179 and ISO 12620 compli-
ant open Data Category Registries for machine 
readable definitions of metadata concepts within 
INTERA is a first step in the right direction. 
However, other disciplines than linguistics lack 
such a widely agreed registry type. For building 
up and combining repositories of RDF-based 
relations between registered concepts there is yet 
no infrastructure. Even in the linguistics domain 
yet there is no suggestion for standards. ISO 
TC37/SC4 should take up this issue, since Data 
Category repositories with concept definitions 
and relation repositories are mutually dependent 
on each other to form exploitable knowledge 
bases. Due to the many contributions from pro-
jects, institutions and even individuals that will 
disagree with proposed definitions and relations 
we will need an efficient infrastructure for dis-
covering and combining useful knowledge com-
ponents. 
References 
S. Bird and M. Liberman. 2001. A formal framework 
for linguistic annotation. 
http://www.ldc.upenn.edu/Papers/CIS9901_1999/r
evised_13Aug99.pdf 
H. Brugman and P. Wittenburg. 2001. The application 
of annotation models for the construction of data-
bases and tools. 
http://www.ldc.upenn.edu/annotation/database/pap
ers/Brugman_Wittenburg/20.2.brugman.pdf 
S. Farrar and D.T. Langendoen. 2003. Markup and the 
GOLD Ontology. 
http://saussure.linguistlist.org/cfdocs/emeld/worksh
op/2003/paper-terry.html 
Y. Gil and V. Ratnaker. A Comparison of (Semantic) 
Markup Languages. In Proceedings of AAAI 2001. 
http://trellis.semanticweb.org.expect/web/semantic
web.comparison.html 
 
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 370?376,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Unsupervised Feature Learning for Visual Sign Language Identification
Binyam Gebrekidan Gebre
1
, Onno Crasborn
2
, Peter Wittenburg
1
,
Sebastian Drude
1
, Tom Heskes
2
1
Max Planck Institute for Psycholinguistics,
2
Radboud University Nijmegen
bingeb@mpi.nl,o.crasborn@let.ru.nl,peter.wittenburg@mpi.nl,
sebastian.drude@mpi.nl,t.heskes@science.ru.nl
Abstract
Prior research on language identification fo-
cused primarily on text and speech. In this
paper, we focus on the visual modality and
present a method for identifying sign lan-
guages solely from short video samples. The
method is trained on unlabelled video data (un-
supervised feature learning) and using these
features, it is trained to discriminate between
six sign languages (supervised learning). We
ran experiments on short video samples in-
volving 30 signers (about 6 hours in total). Us-
ing leave-one-signer-out cross-validation, our
evaluation shows an average best accuracy of
84%. Given that sign languages are under-
resourced, unsupervised feature learning tech-
niques are the right tools and our results indi-
cate that this is realistic for sign language iden-
tification.
1 Introduction
The task of automatic language identification is
to quickly identify the identity of the language
given utterances. Performing this task is key in
applications involving multiple languages such as
machine translation and information retrieval (e.g.
metadata creation for large audiovisual archives).
Prior research on language identification is
heavily biased towards written and spoken lan-
guages (Dunning, 1994; Zissman, 1996; Li et al,
2007; Singer et al, 2012). While language iden-
tification in signed languages is yet to be studied,
significant progress has been recorded for written
and spoken languages.
Written languages can be identified to about
99% accuracy using Markov models (Dunning,
1994). This accuracy is so high that current
research has shifted to related more challeng-
ing problems: language variety identification
(Zampieri and Gebre, 2012), native language iden-
tification (Tetreault et al, 2013) and identification
at the extremes of scales; many more languages,
smaller training data, shorter document lengths
(Baldwin and Lui, 2010).
Spoken languages can be identified to accura-
cies that range from 79-98% using different mod-
els (Zissman, 1996; Singer et al, 2003). The
methods used in spoken language identification
have also been extended to a related class of prob-
lems: native accent identification (Chen et al,
2001; Choueiter et al, 2008; Wu et al, 2010) and
foreign accent identification (Teixeira et al, 1996).
While some work exists on sign language
recognition
1
(Starner and Pentland, 1997; Starner
et al, 1998; Gavrila, 1999; Cooper et al, 2012),
very little research exists on sign language iden-
tification except for the work by (Gebre et al,
2013), where it is shown that sign language identi-
fication can be done using linguistically motivated
features. Accuracies of 78% and 95% are reported
on signer independent and signer dependent iden-
tification of two sign languages.
This paper has two goals. First, to present a
method to identify sign languages using features
learned by unsupervised techniques (Hinton and
Salakhutdinov, 2006; Coates et al, 2011). Sec-
ond, to evaluate the method on six sign languages
under different conditions.
Our contributions: a) show that unsupervised
feature learning techniques, currently popular in
many pattern recognition problems, also work for
visual sign languages. More specifically, we show
how K-means and sparse autoencoder can be used
to learn features for sign language identification.
b) demonstrate the impact on performance of vary-
ing the number of features (aka, feature maps or
filter sizes), the patch dimensions (from 2D to 3D)
and the number of frames (video length).
1
There is a difference between sign language recognition
and identification. Sign language recognition is the recogni-
tion of the meaning of the signs in a given known sign lan-
guage, whereas sign language identification is the recognition
of the sign language itself from given signs.
370
2 The challenges in sign language
identification
The challenges in sign language identification
arise from three sources as described below.
2.1 Iconicity in sign languages
The relationship between forms and meanings are
not totally arbitrary (Perniss et al, 2010). Both
signed and spoken languages manifest iconicity,
that is forms of words or signs are somehow mo-
tivated by the meaning of the word or sign. While
sign languages show a lot of iconicity in the lex-
icon (Taub, 2001), this has not led to a universal
sign language. The same concept can be iconi-
cally realised by the manual articulators in a way
that conforms to the phonological regularities of
the languages, but still lead to different sign forms.
Iconicity is also used in the morphosyntax and
discourse structure of all sign languages, however,
and there we see many similarities between sign
languages. Both real-world and imaginary objects
and locations are visualised in the space in front
of the signer, and can have an impact on the artic-
ulation of signs in various ways. Also, the use of
constructed action appears to be used in many sign
languages in similar ways. The same holds for the
rich use of non-manual articulators in sentences
and the limited role of facial expressions in the
lexicon: these too make sign languages across the
world very similar in appearance, even though the
meaning of specific articulations may differ (Cras-
born, 2006).
2.2 Differences between signers
Just as speakers have different voices unique to
each individual, signers have also different sign-
ing styles that are likely unique to each individual.
Signers? uniqueness results from how they articu-
late the shapes and movements that are specified
by the linguistic structure of the language. The
variability between signers either in terms of phys-
ical properties (hand sizes, colors, etc) or in terms
of articulation (movements) is such that it does not
affect the understanding of the sign language by
humans, but that it may be difficult for machines
to generalize over multiple individuals. At present
we do not know whether the differences between
signers using the same language are of a similar or
different nature than the differences between dif-
ferent languages. At the level of phonology, there
are few differences between sign languages, but
the differences in the phonetic realization of words
(their articulation) may be much larger.
2.3 Diverse environments
The visual ?activity? of signing comes in a context
of a specific environment. This environment can
include the visual background and camera noises.
The background objects of the video may also in-
clude dynamic objects ? increasing the ambiguity
of signing activity. The properties and configu-
rations of the camera induce variations of scale,
translation, rotation, view, occlusion, etc. These
variations coupled with lighting conditions may
introduce noise. These challenges are by no means
specific to sign interaction, and are found in many
other computer vision tasks.
3 Method
Our method performs two important tasks. First,
it learns a feature representation from patches of
unlabelled raw video data (Hinton and Salakhut-
dinov, 2006; Coates et al, 2011). Second, it looks
for activations of the learned representation (by
convolution) and uses these activations to learn a
classifier to discriminate between sign languages.
3.1 Unsupervised feature learning
Given samples of sign language videos (unknown
sign language with one signer per video), our sys-
tem performs the following steps to learn a feature
representation (note that these video samples are
separate from the video samples that are later used
for classifier learning or testing):
1. Extract patches. Extract small videos (here-
after called patches) randomly from any-
where in the video samples. We fix the
size of the patches such that they all have r
rows, c columns and f frames and we ex-
tract patches m times. This gives us X =
{x
(1)
, x
(1)
, . . . , x
(m)
}, where x
(i)
? R
N
and
N = r?c?f (the size of a patch). For our ex-
periments, we extract 100,000 patches of size
15 ? 15 ? 1 (2D) and 15 ? 15 ? 2 (3D).
2. Normalize the patches. There is evidence
that normalization and whitening (Hyv?arinen
and Oja, 2000) improve performance in un-
supervised feature learning (Coates et al,
2011). We therefore normalize every patch
x
(i)
by subtracting the mean and dividing by
371
Figure 1: Illustration of feature extraction: convolution and pooling.
the standard deviation of its elements. For vi-
sual data, normalization corresponds to local
brightness and contrast normalization.
3. Learn a feature-mapping. Our unsuper-
vised algorithm takes in the normalized and
whitened datasetX = {x
(1)
, x
(1)
, . . . , x
(m)
}
and maps each input vector x
(i)
to a new fea-
ture vector of K features (f : R
N
? R
K
).
We use two unsupervised learning algorithms
a) K-means b) sparse autoencoders.
(a) K-means clustering: we train K-means
to learns K c
(k)
centroids that mini-
mize the distance between data points
and their nearest centroids (Coates and
Ng, 2012). Given the learned centroids
c
(k)
, we measure the distance of each
data point (patch) to the centroids. Natu-
rally, the data points are at different dis-
tances to each centroid, we keep the dis-
tances that are below the average of the
distances and we set the other to zero:
f
k
(x) = max{0, ?(z)? z
k
} (1)
where z
k
= ||x? c
(k)
||
2
and ?(z) is the
mean of the elements of z.
(b) Sparse autoencoder: we train a sin-
gle layer autoencoder with K hid-
den nodes using backpropagation to
minimize squared reconstruction error.
At the hidden layer, the features are
mapped using a rectified linear (ReL)
function (Maas et al, 2013) as follows:
f(x) = g(Wx+ b) (2)
where g(z) = max(z, 0). Note that ReL
nodes have advantages over sigmoid or
tanh functions; they create sparse repre-
sentations and are suitable for naturally
sparse data (Glorot et al, 2011).
From K-means, we get K R
N
centroids and from
the sparse autoencoder, we get W ? R
KxN
and
b ? R
K
filters. We call both the centroids and
filters as the learned features.
3.2 Classifier learning
Given the learned features, the feature mapping
functions and a set of labeled training videos, we
extract features as follows:
1. Convolutional extraction: Extract features
from equally spaced sub-patches covering the
video sample.
2. Pooling: Pool features together over four
non-overlapping regions of the input video to
reduce the number of features. We perform
max pooling for K-means and mean pooling
for the sparse autoencoder over 2D regions
(per frame) and over 3D regions (per all se-
quence of frames).
3. Learning: Learn a linear classifier to predict
the labels given the feature vectors. We use
logistic regression classifier and support vec-
tor machines (Pedregosa et al, 2011).
The extraction of classifier features through
convolution and pooling is illustrated in figure 1.
372
4 Experiments
4.1 Datasets
Our experimental data consist of videos of 30
signers equally divided between six sign lan-
guages: British sign language (BSL), Danish
(DSL), French Belgian (FBSL), Flemish (FSL),
Greek (GSL), and Dutch (NGT). The data for the
unsupervised feature learning comes from half of
the BSL and GSL videos in the Dicta-Sign cor-
pus
2
. Part of the other half, involving 5 signers, is
used along with the other sign language videos for
learning and testing classifiers.
For the unsupervised feature learning, two types
of patches are created: 2D dimensions (15 ? 15)
and 3D (15 ? 15 ? 2). Each type consists of ran-
domly selected 100,000 patches and involves 16
different signers. For the supervised learning, 200
videos (consisting of 1 through 4 frames taken at a
step of 2) are randomly sampled per sign language
per signer (for a total of 6,000 samples).
4.2 Data preprocessing
The data preprocessing stage has two goals.
First, to remove any non-signing signals that re-
main constant within videos of a single sign lan-
guage but that are different across sign languages.
For example, if the background of the videos is
different across sign languages, then classifying
the sign languages could be done with perfection
by using signals from the background. To avoid
this problem, we removed the background by us-
ing background subtraction techniques and manu-
ally selected thresholds.
The second reason for data preprocessing is to
make the input size smaller and uniform. The
videos are colored and their resolutions vary from
320 ? 180 to 720 ? 576. We converted the videos
to grayscale and resized their heights to 144 and
cropped out the central 144 ? 144 patches.
4.3 Evaluation
We evaluate our system in terms of average accu-
racies. We train and test our system in leave-one-
signer-out cross-validation, where videos from
four signers are used for training and videos of the
remaining signer are used for testing. Classifica-
tion algorithms are used with their default settings
and the classification strategy is one-vs.-rest.
2
http://www.dictasign.eu/
5 Results and Discussion
Our best average accuracy (84.03%) is obtained
using 500 K-means features which are extracted
over four frames (taken at a step of 2). This ac-
curacy obtained for six languages is much higher
than the 78% accuracy obtained for two sign lan-
guages (Gebre et al, 2013). The latter uses lin-
guistically motivated features that are extracted
over video lengths of at least 10 seconds. Our sys-
tem uses learned features that are extracted over
much smaller video lengths (about half a second).
All classification accuracies are presented in ta-
ble 5 for 2D and table 5 for 3D. Classification con-
fusions are shown in table 5. Figure 2 shows fea-
tures learned by K-means and sparse autoencoder.
(a) K-means features (b) SAE features
Figure 2: All 100 features learned from 100,000
patches of size 15?15. K-means learned relatively
more curving edges than the sparse auto encoder.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 1
100 69.23 70.60 67.42 73.85 74.53 71.8
300 76.08 77.37 74.80 72.27 70.67 68.90
500 83.03 79.88 77.92 67.50 69.38 66.20
# of frames = 2
100 71.15 72.07 67.42 72.78 74.62 72.08
300 77.33 78.27 76.60 71.85 71.07 68.27
500 83.58 79.50 79.90 67.73 70.15 66.45
# of frames = 3
100 71.42 73.10 67.82 65.70 67.52 63.68
300 78.40 78.57 76.50 72.53 71.68 68.18
500 83.48 80.05 80.57 67.85 70.85 66.77
# of frames = 4
100 71.88 73.05 68.70 64.93 67.48 63.80
300 79.32 78.65 76.42 72.27 72.18 68.35
500 84.03 80.38 80.50 68.25 71.57 67.27
K = # of features, SVM = SVM with linear kernel
LR-L? = Logistic Regression with L1 and L2 penalty
Table 1: 2D filters (15?15): Leave-one-signer-out
cross-validation average accuracies.
373
1 2 3 4 5 6 7 8 9 1012345678910
BSL
1 2 3 4 5 6 7 8 9 1012345678910
DSL
1 2 3 4 5 6 7 8 9 1012345678910
FBSL
1 2 3 4 5 6 7 8 9 1012345678910
FSL
1 2 3 4 5 6 7 8 9 1012345678910
GSL
1 2 3 4 5 6 7 8 9 1012345678910
NGT
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 3: Visualization of coefficients of Lasso (logistic regression with L1 penalty) for each sign lan-
guage with respect to each of the 100 filters of the sparse autoencoder. The 100 filters are shown in figure
2(b). Each grid cell represents a frame and each filter is activated in 4 non-overlapping pooling regions.
K-means Sparse Autoencoder
K LR-L1 LR-L2 SVM LR-L1 LR-L2 SVM
# of frames = 2
100 70.63 69.62 68.87 67.40 66.53 65.73
300 73.73 74.05 73.03 72.83 73.48 70.52
500 75.30 76.53 75.40 72.28 74.65 68.72
# of frames = 3
100 72.48 73.30 70.33 68.68 67.40 68.33
300 74.78 74.95 74.77 74.20 74.72 70.85
500 77.27 77.50 76.17 72.40 75.45 69.42
# of frames = 4
100 74.85 73.97 69.23 68.68 67.80 68.80
300 76.23 76.58 74.08 74.43 75.20 70.65
500 79.08 78.63 76.63 73.50 76.23 70.53
Table 2: 3D filters (15?15?2): Leave-one-signer-
out cross-validation average accuracies.
BSL DSL FBSL FSL GSL NGT
BSL 56.11 2.98 1.79 3.38 24.11 11.63
DSL 2.87 92.37 0.95 0.46 3.16 0.18
FBSL 1.48 1.96 79.04 4.69 6.62 6.21
FSL 6.96 2.96 2.06 60.81 18.15 9.07
GSL 5.50 2.55 1.67 2.57 86.05 1.65
NGT 9.08 1.33 3.98 18.76 4.41 62.44
Table 3: Confusion matrix ? confusions averaged
over all settings for K-means and sparse autoen-
coder with 2D and 3D filters (i.e. for all # of
frames, all filter sizes and all classifiers).
Tables 5 and 5 indicate that K-means performs
better with 2D filters and that sparse autoencoder
performs better with 3D filters. Note that features
from 2D filters are pooled over each frame and
concatenated whereas, features from 3D filters are
pooled over all frames.
Which filters are active for which language?
Figure 3 shows visualization of the strength of fil-
ter activation for each sign language. The figure
shows what Lasso looks for when it identifies any
of the six sign languages.
6 Conclusions and Future Work
Given that sign languages are under-resourced,
unsupervised feature learning techniques are the
right tools and our results show that this is realis-
tic for sign language identification.
Future work can extend this work in two direc-
tions: 1) by increasing the number of sign lan-
guages and signers to check the stability of the
learned feature activations and to relate these to
iconicity and signer differences 2) by comparing
our method with deep learning techniques. In our
experiments, we used a single hidden layer of fea-
tures, but it is worth researching into deeper layers
to improve performance and gain more insight into
the hierarchical composition of features.
Other questions for future work. How good are
human beings at identifying sign languages? Can
a machine be used to evaluate the quality of sign
language interpreters by comparing them to a na-
tive language model? The latter question is partic-
ularly important given what happened at the Nel-
son Mandela?s memorial service
3
.
3
http://www.youtube.com/watch?v=X-DxGoIVUWo
374
References
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Tao Chen, Chao Huang, E. Chang, and Jingchun Wang.
2001. Automatic accent identification using gaus-
sian mixture models. In Automatic Speech Recog-
nition and Understanding, 2001. ASRU ?01. IEEE
Workshop on, pages 343?346.
Ghinwa Choueiter, Geoffrey Zweig, and Patrick
Nguyen. 2008. An empirical study of automatic ac-
cent classification. In Acoustics, Speech and Signal
Processing, 2008. ICASSP 2008. IEEE International
Conference on, pages 4265?4268. IEEE.
Adam Coates and Andrew Y Ng. 2012. Learn-
ing feature representations with k-means. In Neu-
ral Networks: Tricks of the Trade, pages 561?580.
Springer.
Adam Coates, Andrew Y Ng, and Honglak Lee. 2011.
An analysis of single-layer networks in unsuper-
vised feature learning. In International Conference
on Artificial Intelligence and Statistics, pages 215?
223.
H. Cooper, E.J. Ong, N. Pugeault, and R. Bowden.
2012. Sign language recognition using sub-units.
Journal of Machine Learning Research, 13:2205?
2231.
Onno Crasborn, 2006. Nonmanual structures in sign
languages, volume 8, pages 668?672. Elsevier, Ox-
ford.
T. Dunning. 1994. Statistical identification of lan-
guage. Computing Research Laboratory, New Mex-
ico State University.
Dariu M Gavrila. 1999. The visual analysis of human
movement: A survey. Computer vision and image
understanding, 73(1):82?98.
Binyam Gebrekidan Gebre, Peter Wittenburg, and Tom
Heskes. 2013. Automatic sign language identifica-
tion. In Proceedings of ICIP 2013.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Geoffrey E Hinton and Ruslan R Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507.
Aapo Hyv?arinen and Erkki Oja. 2000. Independent
component analysis: algorithms and applications.
Neural networks, 13(4):411?430.
Haizhou Li, Bin Ma, and Chin-Hui Lee. 2007. A
vector space modeling approach to spoken language
identification. Audio, Speech, and Language Pro-
cessing, IEEE Transactions on, 15(1):271?284.
Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.
2013. Rectifier nonlinearities improve neural net-
work acoustic models. In Proceedings of the ICML.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825?2830.
Pamela Perniss, Robin L Thompson, and Gabriella
Vigliocco. 2010. Iconicity as a general property
of language: evidence from spoken and signed lan-
guages. Frontiers in psychology, 1.
E. Singer, PA Torres-Carrasquillo, TP Gleason,
WM Campbell, and D.A. Reynolds. 2003. Acous-
tic, phonetic, and discriminative approaches to auto-
matic language identification. In Proc. Eurospeech,
volume 9.
E. Singer, P. Torres-Carrasquillo, D. Reynolds, A. Mc-
Cree, F. Richardson, N. Dehak, and D. Sturim.
2012. The mitll nist lre 2011 language recogni-
tion system. In Odyssey 2012-The Speaker and Lan-
guage Recognition Workshop.
Thad Starner and Alex Pentland. 1997. Real-time
american sign language recognition from video us-
ing hidden markov models. In Motion-Based Recog-
nition, pages 227?243. Springer.
Thad Starner, Joshua Weaver, and Alex Pentland.
1998. Real-time american sign language recogni-
tion using desk and wearable computer based video.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 20(12):1371?1375.
Sarah Taub. 2001. Language from the body: iconicity
and metaphor in American Sign Language. Cam-
bridge University Press, Cambridge.
C. Teixeira, I. Trancoso, and A. Serralheiro. 1996. Ac-
cent identification. In Spoken Language, 1996. IC-
SLP 96. Proceedings., Fourth International Confer-
ence on, volume 3, pages 1784?1787 vol.3.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill.
2013. A report on the first native language identi-
fication shared task. NAACL/HLT 2013, page 48.
Tingyao Wu, Jacques Duchateau, Jean-Pierre Martens,
and Dirk Van Compernolle. 2010. Feature subset
selection for improved native accent identification.
Speech Communication, 52(2):83?98.
Marcos Zampieri and Binyam Gebrekidan Gebre.
2012. Automatic identification of language vari-
eties: The case of portuguese. In Proceedings of
KONVENS, pages 233?237.
375
M.A. Zissman. 1996. Comparison of four approaches
to automatic language identification of telephone
speech. IEEE Transactions on Speech and Audio
Processing, 4(1):31?44.
376
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 216?223,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Improving Native Language Identification with TF-IDF Weighting
Binyam Gebrekidan Gebre1, Marcos Zampieri2, Peter Wittenburg1, Tom Heskes3
1Max Planck Institute for Psycholinguistics
2University of Cologne
3Radboud University
bingeb@mpi.nl,mzampier@uni-koeln.de,
peter.wittenburg@mpi.nl,t.heskes@science.ru.nl
Abstract
This paper presents a Native Language Iden-
tification (NLI) system based on TF-IDF
weighting schemes and using linear classi-
fiers - support vector machines, logistic re-
gressions and perceptrons. The system was
one of the participants of the 2013 NLI Shared
Task in the closed-training track, achieving
0.814 overall accuracy for a set of 11 native
languages. This accuracy was only 2.2 per-
centage points lower than the winner?s perfor-
mance. Furthermore, with subsequent evalua-
tions using 10-fold cross-validation (as given
by the organizers) on the combined training
and development data, the best average accu-
racy obtained is 0.8455 and the features that
contributed to this accuracy are the TF-IDF of
the combined unigrams and bigrams of words.
1 Introduction
Native Language Identification (NLI) is the task of
automatically identifying the native language of a
writer based on the writer?s foreign language pro-
duction. The task is modeled as a classification task
in which automatic methods have to assign class la-
bels (native languages) to objects (texts). NLI is by
no means trivial and it is based on the assumption
that the mother tongue influences Second Language
Acquisition (SLA) and production (Lado, 1957).
When an English native speaker hears someone
speaking English, it is not difficult for him/her to
identify if this person is a native speaker or not.
Moreover, it is, to some extent, possible to assert
the mother tongue of non-native speakers by his/hers
pronunciation patterns, regardless of their language
proficiency. In NLI, the same principle that seems
intuitive for spoken language, is applied to text. If
it is true that the mother tongue of an individual in-
fluences speech production, it should be possible to
identify these traits in written language as well.
NLI methods are particularly relevant for lan-
guages with a significant number of foreign speak-
ers, most notably, English. It is estimated that
the number of non-native speakers of English out-
numbers the number of native speakers by two to
one (Lewis et al, 2013). The written production
of non-native speakers is abundant on the Internet,
academia, and other contexts where English is used
as lingua franca.
This study presents the system that participated in
the 2013 NLI Shared Task (Tetreault et al, 2013)
under the name Cologne-Nijmegen. The novel as-
pect of the system is the use of TF-IDF weighting
schemes. For this study, we experimented with a
number of algorithms and features. Linear SVM and
logistic regression achieved the best accuracies on
the combined features of unigrams and bigrams of
words. The rest of the paper will explain in detail
the features, methods and results achieved.
2 Motivation
There are two main reasons to study NLI. On one
hand, there is a strong linguistic motivation, particu-
larly in the field of SLA and on the other hand, there
is the practical relevance of the task and its integra-
tion to a number of computational applications.
The linguistic motivation of NLI is the possibil-
ity of using classification methods to study the inter-
216
play between native and foreign language acquisi-
tion and performance (Wong and Dras, 2009). One
of the SLA theories that investigate these phenom-
ena is contrastive analysis, which is used to explain
why some structures of L2 are more difficult to ac-
quire than others (Lado, 1957).
Contrastive analysis postulates that the difficulty
in mastering L2 depends on the differences between
L1 and L2. In the process of acquiring L2, lan-
guage transfer (also known as L1 interference) oc-
curs and speakers apply knowledge from their na-
tive language to a second language, taking advan-
tage of their similarities. Computational methods
applied to L2 written production can function as a
corpus-driven method to level out these differences
and serve as a source of information for SLA re-
searchers. It can also be used to provide more tar-
geted feedback to language learners about their er-
rors.
NLI is also a relevant task in computational lin-
guistics and researchers have turned their attention
to it in the last few years. The task is often regarded
as a part of a broader task of authorship profiling,
which consists of the application of automatic meth-
ods to assert information about the writer of a given
text, such as age, gender as well native language.
Authorship profiling is particularly useful for foren-
sic linguistics.
Automatic methods of NLI may be integrated in
NLP applications such as spam detection or machine
translation. NLP tasks such as POS tagging and
parsing might also benefit from NLI, as these re-
sources are trained on standard language written by
native speakers. These tools can be more accurate to
tag non-native speaker?s text if trained with L2 cor-
pora.
3 Related Work
In the last years, a couple of attempts at identifying
native language have been described in the literature.
Tomokiyo and Jones (2001) uses a Naive Bayes al-
gorithm to classify transcribed data from three native
languages: Chinese, Japanese and English. The al-
gorithm reached 96% accuracy when distinguishing
native from non-native texts and 100% when distin-
guishing English native speakers from Chinese na-
tive speakers.
Koppel et al (2005) used machine learning to
identify the native languages of non-native English
speakers with five different mother tongues (Bul-
garian, Czech, French, Russian, and Spanish), us-
ing data retrieved from the International Corpus of
Learner English (ICLE) (Granger et al, 2009). The
features used in this study were function words,
character n-grams, and part-of-speech (POS) bi-
grams.
Tsur and Rappoport (2007) investigated the influ-
ence of the phonology of a writer?s mother tongue
through native language syllables modelled by char-
acter bigrams. Estival et al (2007) addressed NLI as
part of authorship profiling. Authors aim to attribute
10 different characteristics of writers by analysing
a set of English e-mails. The study reports around
84% accuracy in distinguishing e-mails written by
English Arabic and Spanish L1 speakers.
SVM, the algorithm that achieved the best results
in our experiments, was also previously used in NLI
(Kochmar, 2011). In this study, the author identi-
fied error types that are typical for speakers of differ-
ent native languages. She compiled a set of features
based on these error types to improve the classifica-
tion?s performance.
Recently, the TOEFL11 corpus was compiled to
serve as an alternative to the ICLE corpus (Tetreault
et al, 2012). Authors argue that TOEFL11 is more
suitable to NLI than ICLE. This study also experi-
mented with different features to increase results in
NLI and reports best accuracy results of 90.1% on
ICLE and 80.9% on TOEFL11.
4 Methods
We approach the task of native language identifica-
tion as a kind of text classification. In text classifica-
tion, decisions and choices have to be made at three
levels. First, how do we use the training and devel-
opment data? Second, what features do we extract
and how do we select the most informative ones?
Third, which machine learning algorithms perform
best and which parameters can we tune under the
constraints of memory and time? In the following
subsections, we answer these questions.
217
4.1 Dataset: TOEFL11
The dataset used for the shared task is called
TOEFL11 (Blanchard et al, 2013). It consists of
12,100 English essays (about 300 to 400 words long)
from the Test of English as a Foreign Language
(TOEFL). The essays are written by 11 native lan-
guage speakers (L1). Table 1 shows the 11 na-
tive languages. Each essay is labelled with an En-
glish language proficiency level (high, medium, or
low) based on the judgments of human assessment
specialists. We used 9,900 essays for training data
and 1,100 for development (parameter tuning). The
shared task organizers kept 1,100 essays for testing.
Table 1: TOEFL11
L1 languages Arabic, Chinese,
French, German,
Hindi, Italian,
Japanese, Korean,
Spanish, Telugu,
Turkish
# of essays per L1
900 for training
100 for validating
100 for testing
4.2 Features
We explored different kinds and combinations of
features that we assumed to be different for different
L1 speakers and that are also commonly used in the
NLI literature (Koppel et al, 2005; Tetreault et al,
2012). Table 2 shows the sources of the features we
considered. Unigrams and bigrams of words are ex-
plored separately and in combination. One through
four grams of part of speech tags have also been ex-
plored. For POS tagging of the essays, we applied
the default POS tagger from NLTK (Bird, 2006).
Spelling errors have also been treated as features.
We used the collection of words in Peter Norvig?s
website1 as a reference dictionary. The collection
consists of about a million words. It is a concate-
nation of several public domain books from Project
Gutenberg and lists of most frequent words from
Wiktionary and the British National Corpus.
Character n-grams have also been explored for
both the words in the essays and for words with
1http://norvig.com/spell-correct.html
spelling errors. The maximum n-gram size consid-
ered is six.
All features, consisting of either characters or
words or part-of-speech tags or their combinations,
are mapped into normalized numbers (norm L2).
For the mapping, we use TF-IDF, a weighting tech-
nique popular in information retrieval but which is
also finding its use in text classification. Features
that occurred in less than 5 of the essays or those
that occurred in more than 50% of the essays are
removed (all characters are in lower case). These
cut-off values are experimentally selected.
Table 2: A summary of features used in our experiments
Word n-grams Unigrams and bigrams of
words present in the es-
says.
POS n-grams One up to four grams of
POS tags present in the
essays; tagging is done
using default NLTK tag-
ger (Bird, 2006).
Character n-grams One up to six grams of
characters in each essay.
Spelling errors All words that are not
found in the dictionary
of Peter Norvig?s spelling
corrector.
4.2.1 Term Frequency (TF)
Term Frequency refers to the number of times a
particular term appears in an essay. In our experi-
ments, terms are n-grams of characters, words, part-
of-speech tags or any combination of them. The
intuition is that a term that occurs more frequently
identifies/specifies the essay better than another term
that occurs less frequently. This seems a useful
heuristic but what is the relationship between the fre-
quency of a term and its importance to the essay?
From among many relationships, we selected a log-
arithmic relationship (sublinear TF scaling) (Man-
ning et al, 2008):
wft,e =
{
1 + log(tft,e) if tft,e > 0
0 otherwise
(1)
218
where wft,e refers to weight and tft,e refers to the
frequency of term t in essay e.
The wft,e weight tells us the importance of a term
in an essay based on its frequency. But not all terms
that occur more frequently in an essay are equally
important. The effective importance of a term also
depends on how infrequent the term is in other es-
says and this intuition is handled by Inverse Docu-
ment Frequency(IDF).
4.2.2 Inverse Document Frequency(IDF)
Inverse Document Frequency (IDF) quantifies the
intuition that a term which occurs in many essays
is not a good discriminator, and should be given
less weight than one which occurs in fewer essays.
In mathematical terms, IDF is the log of the in-
verse probability of a term being found in any essay
(Salton and McGill, 1984):
idf(ti) = log
N
ni
, (2)
where N is the number of essays in the corpus,
and term ti occurs in ni of them. IDF gives a new
weight when combined with TF to form TF-IDF.
4.2.3 TF?IDF
TF?IDF combines the weights of TF and IDF
by multiplying them. TF gives more weight to a
frequent term in an essay and IDF downscales the
weight if the term occurs in many essays. Equation
3 shows the final weight that each term of an essay
gets before normalization.
wi,e = (1 + log(tft,e))? log(N/ni) (3)
Essay lengths are usually different and this has an
impact on term weights. To abstract from different
essay lengths, each essay feature vector is normal-
ized to unit length. After normalization, the result-
ing essay feature vectors are fed into classifiers.
4.3 Classifiers
We experimented with three linear classifiers - lin-
ear support vector machines, logistic regression and
perceptrons - all from scikit-learn (Pedregosa et al,
2011). These algorithms are suitable for high dimen-
sional and sparse data (text data is high dimensional
and sparse). In the following paragraphs, we briefly
describe the algorithms and the parameter values we
selected.
SVMs have been explored systematically for text
categorization (Joachims, 1998). An SVM classi-
fier finds a hyperplane that separates examples into
two classes with maximal margin (Cortes and Vap-
nik, 1995) (Multi-classes are handled by multi one-
versus-rest classifiers). Examples that are not lin-
early separable in the feature space are mapped to a
higher dimension using kernels. In our experiments,
we used a linear kernel and a penalty parameter of
value 1.0.
In its various forms, logistic regression is also
used for text classification (Zhang et al, 2003;
Genkin et al, 2007; Yu et al, 2011) and native
language identification (Tetreault et al, 2012). Lo-
gistic regression classifies data by using a decision
boundary, determined by a linear function of the fea-
tures. For the implementation of the algorithm, we
used the LIBLINEAR open source library (Fan et
al., 2008) from scikit-learn (Pedregosa et al, 2011)
and we fixed the regularization parameter to 100.0.
For baseline, we used a perceptron classifier
(Rosenblatt, 1957). Perceptron (or single layer net-
work) is the simplest form of neural network. It is
designed for linear separation of data and works well
for text classification. The number of iterations of
the training algorithm is fixed to 70 and the rest of
parameters are left with their default values.
5 Results and Discussion
For each classifier, we ran ten-fold cross-validation
experiments. We divided the training and develop-
ment data into ten folds using the same fold splitting
ids as requested by the shared task organizers and
also as used in (Tetreault et al, 2012). Nine of the
folds were used for training and the tenth for test-
ing the trained model. This was repeated ten times
with each fold being held out for testing. The per-
formance of the classifiers on different features are
presented in terms of average accuracy.
Table 3 gives the average accuracies based on
the TF-IDF of word and character n-grams. Lin-
ear SVM gives the highest accuracy of 84.55% us-
ing features extracted from unigrams and bigrams
of words. Logistic regression also gives comparable
accuracy of 84.45% on the same features.
219
Table 3: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
Words
1 74.73 74.18 65.45
2 80.91 80.27 75.45
1 and 2 84.55 84.45 78.82
(1 and 2)* 83.36 83.27 78.73
* minus country and language names
Characters
1 18.45 19.27 9.09
2 43.27 40.82 10.36
3 71.36 68.00 36.91
4 80.36 79.91 59.64
5 83.09 82.64 73.91
6 84.09 84.00 76.45
The size of the feature vector of unigrams and bi-
grams of words is 73,6262. For each essay, only a
few of the features have non-zero values. Which
features are active and most discriminating in the
classifiers? Table 4 shows the ten most informative
features for the 10th run in the cross-validation (as
picked up linear SVM).
Table 4: Ten most informative features for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI in china / hold / china / time on / may / taiwan / just /still / , the / . take
FRE french / conclude , / even if / in france / france / toconclude / indeed , / ... / . indeed / indeed
GER special / furthermore / might / germany / , because /have to / . but / - / often / , that
HIN which / and concept / various / hence / generation / &/ towards / then / its / as compared
ITA in italy / , for / infact / that a / italy / i think / in fact /italian / think that / :
JPN , and / i disagree / is because / . it / . if / i think /japan , / japanese / in japan / japan
KOR . however / however , / even though / however / thesedays / various / korea , / korean / in korea / korea
SPA an specific / because is / moment / , etc / going to / ,is / necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / with out / when compared / i conclude /
and also
TUR ages / istanbul / addition to / conditions / enough / inturkey / the life / ; / . because / turkey
The ten most informative features include coun-
2features that occur less than 5 times or that occur in more
than 50% of the essays are removed from the vocabulary
try and language names. For example, for Japanese
and Korean L1s, four of the ten top features include
Korea or Korean in the unigrams or bigrams. How
would the classification accuracy decrease if we re-
moved mentions of country or language names?
We made a list of the 11 L1 language names and
the countries where they are mainly spoken (for ex-
ample, German, Germany, French, France, etc.). We
considered this list as stop words (i.e. removed them
from corpus) and ran the whole classification exper-
iments. The new best accuracy is 83.36% ( a loss of
just 1.2% ). Table 3 shows the new accuracies for all
classifiers. The new top ten features mostly consist
of function words and some spelling errors. Table 5
shows all of the new top ten features.
The spelling errors seem to have been influenced
by the L1 languages, especially for French and
Spanish languages. The English words example
and developed have similar sounding/looking equiv-
alents in French (exemple and de?veloppe?) . Simi-
larly, the English words necessary and different have
similar sounding/looking words in Spanish (nece-
sario and diferente). These spelling errors made it
to the top ten features. But how discriminating are
they on their own?
Table 5: Ten most informative features (minus country
and language names) for each L1
ARA many reasons / from / self / advertisment / , and /statment / any / thier / alot of / alot
CHI and more / hold / more and / time on / taiwan / may /just / still / . take / , the
FRE conclude / exemple / developped / conclude , / evenif / to conclude / indeed , / ... / . indeed / indeed
GER has to / special / furthermore / might / , because /have to / . but / - / often / , that
HIN and concept / which / various / hence / generation / &/ towards / then / its / as compared
ITA possibility / probably / particular / , for / infact / thata / i think / in fact / think that / &
JPN i agree / the opinion / tokyo / two reasons / is because/ , and / i disagree / . it / . if / i think
KOR creative / , many / ?s / . also / . however / even though/ however , / various / however / these days
SPA activities / an specific / moment / , etc / going to / , is/ necesary / , and / diferent / , but
TEL
may not / the statement / every one / days / the above
/ where as / when compared / with out / i conclude /
and also
TUR enjoyable / being / ages / addition to / istanbul /enough / conditions / the life / ; / . because
We ran experiments with features extracted from
220
Table 6: Confusion matrix: Best accuracy is for German (95%) and the worst is for Hindi (72%)
ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR
ARA 83 1 4 1 1 3 1 2 3 1 0
CHI 0 88 2 0 2 0 2 5 1 0 0
FRE 3 0 88 2 1 2 0 1 2 0 1
GER 2 0 1 95 0 0 0 0 1 0 1
HIN 2 1 1 1 72 0 0 0 2 18 3
ITA 0 0 6 3 0 84 0 0 6 0 1
JPN 1 2 0 1 1 0 84 10 0 0 1
KOR 0 3 0 2 3 0 8 81 1 1 1
SPA 6 2 5 2 0 4 0 0 79 0 2
TEL 0 0 0 0 16 0 1 0 0 83 0
TUR 1 1 0 1 3 0 0 0 1 0 93
only spelling errors. For comparison, we also ran
experiments with POS tags with and without their
words. None of these experiments beat the best ac-
curacy obtained using unigram and bigram of words
- not even the unigram and bigram of POS tagged
words. See table 7 for the obtained results.
Table 7: Cross-validation results; accuracy in %
N-gram LinearSVM
Logistic
Regression Perceptron
POS
1 17.00 17.09 9.09
2 43.45 40.00 11.18
3 55.27 53.55 35.36
4 56.09 56.18 48.64
POS + Word
1 75.09 74.18 64.09
2 80.45 80.64 76.18
1 and 2 83.00 83.36 79.09
Spelling errors - characters
1 20.36 21.00 9.09
2 34.09 32.64 9.73
3 47.00 44.64 26.82
4 50.82 48.09 41.64
1?4 51.82 48.27 34.18
words 42.73 39.45 28.73
All our reported results so far have been global
classification results. Table 6 shows the confusion
matrix for each L1. The best accuracy is 95% for
German and the worst is for Hindi (72%). Hindi
is classified as Telugu (18%) of the times and Tel-
ugu is classified as Hindi 16% of the times and
only one Telugu essay is classified as any other than
Hindi. More generally, the confusion matrix seems
to suggest that geographically closer countries are
more confused with each other: Hindi and Telugu,
Japanese and Korean, Chinese and Korean.
The best accuracy (84.55%) obtained in our ex-
periments is higher than the state-of-the-art accuracy
reported in (Tetreault et al, 2012) (80.9%). But the
features we used are not different from those com-
monly used in the literature (Koppel et al, 2005;
Tetreault et al, 2012) (n-grams of characters or
words). The novel aspect of our system is the use
of TF-IDF weighting on all of the features including
on unigrams and bigrams of words.
TF-IDF weighting has already been used in na-
tive language identification (Kochmar, 2011; Ahn,
2011). But its importance has not been fully ex-
plored. Experiments in Kochmar (2011) were lim-
ited to character grams and in a binary classifica-
tion scenario. Experiments in Ahn (2011) applied
TF-IDF weighting to identify content words and
showed how their removal decreased performance
(Ahn, 2011). By contrast, in this paper, we applied
TF-IDF weighting consistently to all features - same
type features (e.g. unigrams) or combined features
(e.g. unigram and bigrams).
How would the best accuracy change if TF-IDF
weighting is not applied? Table 8 shows the changes
to the best average accuracies with and without
TF/IDF weighting for the three classifiers.
Table 8: The importance of TF-IDF weighting
TF IDF SVM LR Perceptron
Yes Yes 84.55 84.45 78.82
Yes No 80.82 80.73 63.18
No Yes 82.36 82.27 78.82
No No 79.18 78.55 56.36
221
6 Conclusions
This paper has presented the system that participated
in the 2013 NLI Shared Task in the closed-training
track. Cross-validation testing on the TOEFL11 cor-
pus showed that the system could achieve an accu-
racy of about 84.55% in categorizing unseen essays
into one of the eleven L1 languages.
The novel aspect of the system is the use
of TF-IDF weighting schemes on features ?
which could be any or combination of n-gram
words/characters/POS tags. The feature combina-
tion that gave the best accuracy is the TF-IDF of
unigrams and bigrams of words. The next best fea-
ture class is the TF-IDF of 6-gram characters , which
achieved 84.09%, very close to 84.55%. Both lin-
ear support vector machines and logistic regression
classifiers have performed almost equally.
To improve performance in NLI, future work
should examine new features that can classify ge-
ographically or typologically related languages such
as Hindi and Telugu. Future work should also ana-
lyze the information obtained in NLI experiments to
quantify and investigate differences in the usage of
foreign language lexicon or grammar according to
the individual?s mother tongue.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Commissions
7th Framework Program under grant agreement no
238405 (CLARA). The authors would like to thank
the organizers of the NLI Shared Task 2013 for pro-
viding prompt reply to all our inquiries and for coor-
dinating a very interesting and fruitful shared task.
References
Charles S. Ahn. 2011. Automatically detecting authors?
native language. Ph.D. thesis, Monterey, California.
Naval Postgraduate School.
Steven Bird. 2006. NLTK: the natural language toolkit.
In Proceedings of the COLING/ACL on Interactive
presentation sessions, pages 69?72. Association for
Computational Linguistics.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Ed-
ucational Testing Service.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273?297.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author Profiling
for English Emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING), pages 263?272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Alexander Genkin, David D Lewis, and David Madigan.
2007. Large-scale bayesian logistic regression for text
categorization. Technometrics, 49(3):291?304.
Sylviane Granger, Estelle Dagneaux, and Fanny Meu-
nier. 2009. International Corpus of Learner English.
Presses Universitaires de Louvain, Louvain-la-Neuve.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. Springer.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge, United Kingdom.
Moshe Koppel, Jonathan Schler, and Kfir Zigon. 2005.
Automatically determining an anonymous author?s na-
tive language. Lecture Notes in Computer Science,
3495:209?217.
Robert Lado. 1957. Applied Linguistics for Language
Teachers. University of Michigan Press.
Paul Lewis, Gary Simons, and Charles Fennig. 2013.
Ethnologue: Languages of the World, Seventeeth Edi-
tion. SIL International.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to information re-
trieval, volume 1. Cambridge University Press Cam-
bridge.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, Jake Vanderplas, Alexandre Passos,
David Cournapeau, Matthieu Brucher, Matthieu Per-
rot, and E?douard Duchesnay. 2011. Scikit-learn: Ma-
chine learning in Python. Journal of Machine Learn-
ing Research, 12:2825?2830.
Frank Rosenblatt. 1957. The perceptron, a perceiv-
ing and recognizing automaton Project Para. Cornell
Aeronautical Laboratory.
Gerard Salton and Michael McGill. 1984. Introduction
to Modern Information Retrieval. McGraw-Hill Book
Company.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
222
language identification. In Proceedings of COLING
2012, pages 2585?2602, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
Summary report on the first shared task on native lan-
guage identification. In Proceedings of the Eighth
Workshop on Building Educational Applications Us-
ing NLP, Atlanta, GA, USA, June. Association for
Computational Linguistics.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you?: Naive bayes
detection of non-native utterance text. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies (NAACL ?01).
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proceed-
ings of the Workshop on Cognitive Aspects of Compu-
tational Language Acquisition, pages 9?16.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Pro-
ceedings of the Australasian Language Technology As-
sociation Workshop, pages 53?61. Citeseer.
Hsiang-Fu Yu, Fang-Lan Huang, and Chih-Jen Lin.
2011. Dual coordinate descent methods for logistic
regression and maximum entropy models. Machine
Learning, 85(1-2):41?75.
Jian Zhang, Rong Jin, Yiming Yang, and Alexander G.
Hauptmann. 2003. Modified logistic regression: An
approximation to svm and its applications in large-
scale text categorization. In ICML, pages 888?895.
223
